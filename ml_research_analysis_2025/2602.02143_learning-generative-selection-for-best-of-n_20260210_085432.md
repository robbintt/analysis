---
ver: rpa2
title: Learning Generative Selection for Best-of-N
arxiv_id: '2602.02143'
source_url: https://arxiv.org/abs/2602.02143
tags:
- selection
- math
- genselect
- learning
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of Best-of-N selection quality
  in test-time scaling for LLM reasoning, where small models struggle to effectively
  select the best among multiple generated solutions. The core method trains 1.7B-parameter
  reasoning models using reinforcement learning (DAPO) to perform generative selection
  on math and code tasks.
---

# Learning Generative Selection for Best-of-N

## Quick Facts
- arXiv ID: 2602.02143
- Source URL: https://arxiv.org/abs/2602.02143
- Reference count: 34
- Primary result: RL-trained 1.7B reasoning models significantly outperform prompting baselines on Best-of-N selection for math and code tasks

## Executive Summary
This paper addresses the limitation of Best-of-N selection quality in test-time scaling for LLM reasoning, where small models struggle to effectively select the best among multiple generated solutions. The core method trains 1.7B-parameter reasoning models using reinforcement learning (DAPO) to perform generative selection on math and code tasks. The models are trained to select correct solutions from sets containing both correct and incorrect candidates synthesized from large-scale datasets with automatic verification. Across math benchmarks (AIME24, AIME25, HMMT25) and LiveCodeBench, the RL-trained models significantly outperform prompting and majority-voting baselines, often approaching or exceeding much larger models (4B-8B). Notably, these selection gains generalize to choosing outputs from stronger generation models despite training only on weaker model outputs.

## Method Summary
The paper trains 1.7B-parameter Qwen3 reasoning models using reinforcement learning (DAPO) to perform generative selection (GenSelect) on math and code tasks. The approach generates 2-16 candidate solutions per problem using a weaker model (Qwen3-1.7B), then filters to problems with at least one correct solution but no more than 50% correct candidates. The RL objective rewards selecting the correct solution index. Training uses on-policy sampling via VeRL framework with batch size 128, temperature 1.5, and learns a transferable notion of solution quality that generalizes to stronger generators despite training only on weaker model outputs.

## Key Results
- RL-trained selection models significantly outperform prompting and majority-voting baselines on math benchmarks (AIME24/25, HMMT25) and LiveCodeBench
- Selection gains generalize to choosing outputs from stronger generation models despite training only on weaker model outputs
- Cross-domain transfer shows asymmetric pattern: math-trained selectors transfer better to code than code-trained selectors transfer to math
- Math-trained models match or exceed out-of-the-box selection performance of much larger models (4B-8B)

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning with verifiable rewards teaches small models to specialize in comparative selection, overcoming their default tendency to overfit to generation tasks. The RL training loop provides dense, binary feedback on selection correctness, enabling the model to allocate reasoning capacity toward comparative analysis rather than defaulting to generation-style patterns. This specialization is stronger in smaller models due to their limited capacity to simultaneously excel at both generation and selection.

### Mechanism 2
Models trained via RL learn a transferable notion of solution quality that generalizes to out-of-distribution generators. The selection task forces the model to learn abstract features of solution correctness (reasoning structure, logical consistency, final answer plausibility) rather than memorizing generator-specific artifacts. When trained on weak model outputs with ground-truth correctness labels, the model internalizes general quality criteria that apply to stronger generators' outputs.

### Mechanism 3
Math-trained selectors transfer better to code than code-trained selectors transfer to math, due to differences in verification signal quality and semantic coverage requirements. Math verification provides cleaner binary signals than code verification, which suffers from false positives and incomplete coverage. Additionally, code selection requires judging broader semantic properties (algorithmic structure, edge cases, complexity) that are harder to learn from sparse correctness signals.

## Foundational Learning

- **Best-of-N Selection (Test-Time Scaling)**: The core problem formulation where generating multiple candidates and selecting the best is fundamentally limited by selector quality. Quick check: Given 8 candidate solutions where 2 are correct, explain why Pass@8 exceeds Majority@8, and how a generative selector differs from a pointwise reward model.

- **On-Policy Reinforcement Learning with Verifiable Rewards**: The paper uses DAPO with correctness as the reward. Understanding why on-policy sampling matters—generating selections during training to match the distribution the model will encounter—is critical. Quick check: Why would precomputing all selection rewards before training be easier than training a synthesis/aggregation model that generates new outputs requiring fresh verification?

- **Selection vs. Synthesis Paradigms**: The paper explicitly chooses selection over synthesis-based aggregation. Understanding this design choice clarifies why the approach is tractable and where it might fail. Quick check: Given candidate solutions S1, S2, S3, what is the output of a selector versus a synthesizer? Which requires re-running verification during training?

## Architecture Onboarding

- **Component map**: OpenMathReasoning / OpenCodeReasoning → Qwen3-1.7B candidate generation → Automatic verifier (Math-Verify / unit tests) → Filter (≥1 correct, ≤50% correct) → GenSelect prompt construction → VeRL framework → DAPO algorithm → On-policy rollout generation → Binary reward → Policy update

- **Critical path**: The filtering logic (≥1 correct AND ≤50% correct candidates) determines training data quality. If this is wrong, models either see trivial selection tasks (all correct) or impossible ones (no correct). The 16K token length filter also matters—longer prompts are discarded, potentially biasing toward simpler problems.

- **Design tradeoffs**: Temperature 1.5 for diverse selection reasoning but risks incoherent outputs; selection over synthesis for simpler reward computation but cannot produce novel solutions; domain-specific training with math model transferring better cross-domain than code model.

- **Failure signatures**: No improvement over prompting indicates selection tasks may be too easy or too hard; code model fails to converge suggests insufficient test coverage; no transfer to stronger generators indicates training distribution too narrow.

- **First 3 experiments**: 1) Baseline sanity check: Evaluate Pass@1, Majority@8, Pass@8, and GenSelect prompting before training; 2) Data quality audit: Manually verify correctness labels and non-triviality constraints on sample prompts; 3) Ablate candidate set size: Train and evaluate with N=4, 8, 16 candidates.

## Open Questions the Paper Calls Out

- **Can improved verification signals close the performance gap between code and math selection, or does code require fundamentally different selection reasoning?**: The paper notes that correctness signals from unit tests are noisier than symbolic math verification, but does not disentangle verification noise from intrinsic task difficulty.

- **How does selection accuracy scale when N grows beyond 16 candidates?**: The paper constructs "2–16 candidate solutions" per prompt but does not explore scaling behavior at larger N, leaving practical limits unknown.

- **Why does training on math selection transfer better to code than code-to-math transfer?**: The paper documents this asymmetric cross-domain transfer but offers no mechanistic explanation for why math training acquires transferable features that code training does not.

## Limitations
- Automatic verification signals, particularly for code tasks, may be noisy or incomplete, limiting learning signal quality
- The selection-only paradigm constrains the approach to choosing from existing candidates rather than generating novel solutions
- Temperature-1.5 generation for selector outputs lacks ablation to confirm optimal settings

## Confidence
- **High Confidence**: RL training successfully teaches small models to perform generative selection better than prompting baselines
- **Medium Confidence**: Transfer mechanism from weak to strong generators works in tested cases
- **Medium Confidence**: Domain asymmetry (math transfer > code transfer) is well-supported by verification quality differences

## Next Checks
1. **Verification Signal Quality Audit**: Systematically measure false positive/negative rates of automatic verifiers across the training corpus to quantify how verification noise affects learning signal quality and selection accuracy.

2. **Transfer Generalization Stress Test**: Evaluate trained selection models on outputs from generators that differ substantially in architecture, reasoning style, or solution formatting from training generators to identify boundaries of transfer capability.

3. **Synthesis vs Selection Capability Comparison**: Implement and compare a synthesis-based aggregation model against the selection approach on the same datasets to quantify performance gap and identify problem types where synthesis would be necessary.