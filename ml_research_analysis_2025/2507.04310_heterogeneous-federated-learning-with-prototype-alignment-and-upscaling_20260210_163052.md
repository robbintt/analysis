---
ver: rpa2
title: Heterogeneous Federated Learning with Prototype Alignment and Upscaling
arxiv_id: '2507.04310'
source_url: https://arxiv.org/abs/2507.04310
tags:
- prototype
- prototypes
- protonorm
- learning
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of heterogeneous federated
  learning (FL), where clients have varying data distributions and model architectures.
  The authors propose Prototype Normalization (ProtoNorm), a novel framework that
  improves prototype separation through two key components: Prototype Alignment (PA)
  and Prototype Upscaling (PU).'
---

# Heterogeneous Federated Learning with Prototype Alignment and Upscaling

## Quick Facts
- arXiv ID: 2507.04310
- Source URL: https://arxiv.org/abs/2507.04310
- Authors: Gyuejeong Lee; Jihwan Shin; Daeyoung Choi
- Reference count: 40
- Key outcome: ProtoNorm framework improves heterogeneous FL performance through prototype alignment and upscaling, achieving superior results on CIFAR-10/100, Flowers-102, and Tiny ImageNet

## Executive Summary
This paper addresses the challenge of heterogeneous federated learning where clients have varying data distributions and model architectures. The authors propose Prototype Normalization (ProtoNorm), a novel framework that improves prototype separation through two key components: Prototype Alignment (PA) and Prototype Upscaling (PU). PA optimizes global prototype configurations on a unit sphere using a gradient ascent-based Thomson problem solver to maximize angular separation. PU then increases prototype magnitudes to enhance separation in Euclidean space. The method achieves superior prototype separation while maintaining communication efficiency by sharing only class prototypes.

## Method Summary
ProtoNorm addresses heterogeneous FL challenges by introducing a two-stage prototype optimization approach. The framework first employs Prototype Alignment (PA) to optimize prototype positions on a unit sphere through gradient ascent, solving the Thomson problem to maximize angular separation between prototypes. This is followed by Prototype Upscaling (PU), which increases prototype magnitudes to enhance separation in Euclidean space. The approach maintains communication efficiency by transmitting only class prototypes between clients and server, with computationally intensive PA operations performed server-side. This design makes ProtoNorm particularly suitable for resource-constrained FL environments while achieving consistent performance improvements across benchmark datasets.

## Key Results
- ProtoNorm consistently outperforms existing HtFL approaches on CIFAR-10/100, Flowers-102, and Tiny ImageNet datasets
- Achieves superior prototype separation through combined PA and PU mechanisms
- Maintains communication efficiency by sharing only class prototypes while performing intensive PA operations server-side
- Demonstrates robust performance across varying levels of model heterogeneity

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual optimization strategy. By first positioning prototypes on a unit sphere to maximize angular separation (PA), it creates an optimal geometric arrangement that reduces prototype interference. The subsequent magnitude increase (PU) then amplifies these separations in Euclidean space, making the decision boundaries more robust to variations in client data distributions. This two-step approach addresses the fundamental challenge in heterogeneous FL where differing client architectures and data distributions can cause prototype overlap and degraded performance.

## Foundational Learning
- **Thomson Problem**: The challenge of optimally arranging points on a sphere to maximize minimum distance between them. This is crucial for understanding PA's optimization objective and ensuring maximum angular separation between prototypes.
- **Prototype-based Classification**: A method where class representations are stored as prototypes rather than learned parameters, enabling efficient communication in FL settings by transmitting only prototype vectors.
- **Gradient Ascent Optimization**: The iterative process used in PA to find optimal prototype positions, requiring careful consideration of learning rates and convergence criteria for stable optimization.
- **Federated Learning Heterogeneity**: The challenges arising from clients having different data distributions and model architectures, which ProtoNorm specifically addresses through its prototype separation strategy.

## Architecture Onboarding
- **Component Map**: Clients -> Local Training -> Prototype Update -> Server -> PA Optimization -> PU Scaling -> Global Prototypes
- **Critical Path**: Local client training → Prototype transmission → Server-side PA optimization → PU scaling → Updated prototype distribution
- **Design Tradeoffs**: PA operations are computationally intensive but performed server-side to maintain client efficiency; prototype-only communication reduces bandwidth but requires careful optimization to maintain accuracy.
- **Failure Signatures**: Prototype overlap due to insufficient PA optimization, convergence issues from aggressive PU scaling, communication bottlenecks from excessive prototype dimensions.
- **First Experiments**: 1) Benchmark ProtoNorm against FedAvg on CIFAR-10 with heterogeneous architectures, 2) Test PA sensitivity to learning rate variations, 3) Evaluate scalability with increasing number of classes.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on image classification tasks, with untested performance on non-vision domains like time-series or tabular data
- Sensitivity of PA optimization to hyperparameter choices (learning rate, iteration count) requires further exploration
- Scalability challenges for scenarios with large number of classes (>100) due to computational demands of Thomson problem solver

## Confidence
- High Confidence: Core algorithmic contribution of combining PA and PU for prototype separation is well-grounded
- Medium Confidence: Communication efficiency claims supported by theoretical arguments but real-world network variations not explicitly modeled
- Medium Confidence: Generalization to heterogeneous model architectures demonstrated empirically but robustness to extreme architectural differences needs validation

## Next Checks
1. Evaluate ProtoNorm's performance on non-image datasets, particularly in domains with inherent temporal or sequential characteristics, to assess cross-domain applicability.
2. Conduct ablation studies focusing on the PA optimization hyperparameters to establish sensitivity bounds and identify potential failure modes.
3. Test the framework's scalability by increasing the number of classes and clients while monitoring both convergence behavior and computational overhead, particularly for the server-side PA operations.