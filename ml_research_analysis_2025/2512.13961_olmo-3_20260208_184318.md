---
ver: rpa2
title: Olmo 3
arxiv_id: '2512.13961'
source_url: https://arxiv.org/abs/2512.13961
tags:
- data
- training
- math
- arxiv
- olmo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Olmo 3 is a family of state-of-the-art, fully-open language models
  at the 7B and 32B parameter scales, designed for long-context reasoning, function
  calling, coding, instruction following, general chat, and knowledge recall. The
  model construction targets a diverse set of capabilities through a multi-stage training
  pipeline including pretraining, midtraining, and long-context extension.
---

# Olmo 3

## Quick Facts
- arXiv ID: 2512.13961
- Source URL: https://arxiv.org/abs/2512.13961
- Reference count: 40
- Primary result: State-of-the-art fully-open language models at 7B and 32B parameters with strong performance across reasoning, coding, and instruction following

## Executive Summary
Olmo 3 is a family of state-of-the-art, fully-open language models at 7B and 32B parameter scales, designed for long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. The model construction targets a diverse set of capabilities through a multi-stage training pipeline including pretraining, midtraining, and long-context extension. The release includes the entire model flow, enabling customization at any development stage. The flagship model, Olmo 3.1 Think 32B, achieves the strongest performance among fully-open thinking models, narrowing the gap to top open-weight models like Qwen 3 32B while being trained on six times fewer tokens.

## Method Summary
Olmo 3 uses a multi-stage training pipeline: pretraining on 6T tokens (Dolma 3 Mix) for broad language understanding, midtraining on 100B tokens (Dolma 3 Dolmino Mix) for math, code, and reasoning capabilities, long-context extension on 50-100B tokens (Dolma 3 Longmino Mix) for extended sequence handling, and post-training refinement through supervised fine-tuning, preference optimization, and reinforcement learning. Key innovations include specialized datasets curated for each stage, a new evaluation suite (OlmoBaseEval) for compute-efficient base model development, and algorithmic advances in reinforcement learning (OlmoRL) with active sampling and multi-domain mixing.

## Key Results
- Olmo 3.1 Think 32B achieves strongest performance among fully-open thinking models, narrowing the gap to top open-weight models like Qwen 3 32B
- Olmo 3 Base is the best fully-open model at 32B parameters, outperforming Stanford Marin 32B and Apertus 70B
- Through midtraining, the models achieve double-digit improvements over other fully-open 32B models on math and code evaluation composites
- OlmoRL framework with active sampling and multi-domain mixing prevents reward over-optimization and maintains training stability

## Why This Works (Mechanism)

### Mechanism 1
A multi-stage training pipeline with domain-targeted data curation at each stage improves both general and specialized capabilities more efficiently than single-stage training. Pretraining on 6T tokens establishes broad language understanding; midtraining on 100B tokens targets math, code, reasoning with high-quality synthetic data; long-context extension on 50-100B tokens enables extended sequence handling; post-training (SFT → DPO → RL) refines reasoning and instruction-following. Core assumption: Domain-specific gains from later stages do not catastrophically regress general capabilities established in earlier stages; data quality signals are predictive of downstream task performance. Evidence: Olmo 3 Base achieves double-digit improvements over other fully-open 32B models on Math and Code composites; midtraining achieves improvements across target capability domains.

### Mechanism 2
Delta Learning—constructing preference pairs where the quality delta between chosen and rejected responses is maximized—enables preference tuning to improve model capabilities even when supervised fine-tuning on the same chosen responses provides no benefit. When SFT saturates, contrasting a strong response with a weak one creates a learnable delta that updates the policy to prefer stronger reasoning patterns without memorizing specific responses. Core assumption: The delta between responses captures a learnable capability gradient; the model can generalize from contrast rather than imitation. Evidence: Continued SFT on chosen responses from Dolci Think DPO hurts initial SFT model, but pairing these with rejected responses enables preference tuning to drive strong gains beyond initial SFT.

### Mechanism 3
Reinforcement Learning with Verifiable Rewards (RLVR) applied to a diverse mix of domains with active sampling and token-level loss stabilizes training and prevents reward over-optimization observed in single-domain RL. Active sampling filters zero-gradient groups, maintaining effective batch size and training stability; mixed-domain data prevents the policy from over-optimizing to a single reward signal; token-level loss avoids length bias. Core assumption: Verifiable rewards are sufficiently correlated with true task performance; multi-domain mixing provides sufficient regularization; infrastructure can handle long generation efficiently. Evidence: Active sampling maintains consistently full batch of completions with non-zero advantage, greatly reducing loss variance; training on mixed data prevents overfitting while single-domain training degrades other capabilities.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)** - A reinforcement learning algorithm that computes advantages relative to other samples in a group rather than against a value function baseline. Why needed: OlmoRL builds on GRPO with modifications (active sampling, token-level loss, no KL loss). Quick check: Given 8 samples from same prompt with rewards [0.8, 0.6, 0.8, 0.4, 0.8, 0.6, 0.4, 0.4], what would GRPO advantage be for each sample, and which samples would be filtered by zero-gradient filtering?

- **RoPE (Rotary Position Embedding)** - A positional encoding scheme that encodes position information through rotation matrices applied to query and key vectors. Why needed: Olmo 3 uses RoPE with YaRN scaling for long-context extension. Quick check: If model is pretrained with RoPE at context length 8192 and you want to extend to 65536, what are key parameters you might adjust? What does YaRN change compared to linear interpolation?

- **Document Packing with Intra-document Masking** - Concatenating multiple documents into fixed-length training sequences while ensuring attention only operates within document boundaries. Why needed: Long-context extension uses document packing to create longer training instances. Quick check: Given training sequence of length 65K containing 3 documents of lengths [20K, 30K, 15K], draw attention mask pattern ensuring tokens from document 1 cannot attend to documents 2 and 3.

## Architecture Onboarding

- **Component map**: Base Model (7B/32B params, 32/64 layers, 4096/5120 hidden dim) → Attention Pattern (Sliding window 4096 at 3/4 layers, full at remaining) → Training Infrastructure (OLMo-core for pretraining/midtraining/SFT; Open Instruct for DPO/RL; vLLM for inference) → Post-Training Pipeline (SFT → DPO → RLVR)

- **Critical path**: 1) Data Curation: Token-constrained mixing + quality-aware upsampling (pretrain); microanneals + integration tests (midtrain); long-context data with synthetic augmentation (extension); Dolci datasets for post-training 2) Base Training: Pretrain (5.9T tokens) → Midtrain (100B tokens) → Long-context (50-100B tokens) 3) Post-Training: SFT on reasoning traces → DPO with delta learning → RLVR with multi-domain mix + active sampling 4) Evaluation: OlmoBaseEval (base), post-training eval suite (SFT/DPO/RL checkpoints), decontamination checks

- **Design tradeoffs**: SWA vs. Full Attention: SWA reduces memory/compute for 4K local context but limits global information flow; full attention at last layer ensures global reasoning; 3/4 ratio balances efficiency and capability. GQA (32B) vs. MHA (7B): GQA reduces KV cache for efficient inference at larger scale but may slightly reduce expressiveness. DPO Beta (β=5): Higher β increases KL regularization, preventing rapid policy drift; trade-off is slower learning on preference signal. Multi-domain RL vs. Single-domain: Mixed training prevents overfitting but under-optimizes each domain. Response Length (8K-32K for RL): Longer traces enable more complex reasoning but increase inference cost.

- **Failure signatures**: Thinking traces emitted by Instruct model: If midtraining includes special tokens, base model emits them at inference; fix by using newline formatting only. Length explosion during DPO: Preference pairs with longer chosen responses bias model toward verbosity; filter to limit chosen-rejected token difference to ~100 tokens. RL instability / batch size decay: Without active sampling, groups with zero reward variance produce no gradient, shrinking effective batch size; implement active sampling to continuously resample. Long-context performance drop at short contexts: If extension uses only long documents without short-context mixing, short-context performance degrades; use 34% long / 66% short mix. Reward hacking on contaminated data: If training data contains eval instances, even random rewards improve performance; verify decontamination by training with spurious rewards.

- **First 3 experiments**: 1) Reproduce delta learning effect: Train DPO on preference pairs from strong model (chosen) vs. weak model (rejected), then compare to continued SFT on chosen responses alone. Expected: DPO improves, SFT hurts or has no effect. 2) Ablate active sampling in RLVR: Run RL-Zero with and without active sampling on math domain; measure batch size stability, loss variance, and AIME performance. Expected: Without active sampling, batch size decays and loss variance increases. 3) Test long-context extension components: Run 3 variants: (a) YaRN on all layers, (b) YaRN on full-attention only, (c) no YaRN; measure RULER scores at 4K-65K. Expected: YaRN on full-attention only yields best overall.

## Open Questions the Paper Calls Out

### Open Question 1
Can the inference-to-training compute ratio for the 32B learner in OlmoRL be significantly reduced through alternative sharding configurations? The authors suspect a "suboptimal sharding configuration for the 32B learner," noting that inference dominates computational costs to the extent that they use roughly 5x–14x as much compute for inference as for training. Future work can leverage profiling the 32B learner with alternative parallelism strategies to balance training speed against inference bottlenecks.

### Open Question 2
How do different domains (math, code, instruction following, chat) interact during multi-objective Reinforcement Learning with Verifiable Rewards (RLVR)? The authors state that future work can investigate the interactions between domains in multi-objective RLVR. Ablation studies using the Olmo 3 RL-Zero setup can analyze cross-domain performance gradients when varying domain weights in the RL mixture.

### Open Question 3
Is it possible to optimize the midtraining data mix to maximize reasoning capabilities without causing significant degradation in general knowledge (MCQA/GenQA) performance? The authors observe real tradeoffs when skewing data toward math/code (improved reasoning) versus general QA (improved knowledge). Experiments utilizing synthetic reasoning data or curriculum learning strategies can determine if reasoning gains can be disentangled from knowledge retention issues.

## Limitations
- Multi-stage training pipeline relies heavily on domain-specific data curation that may not generalize across different capability targets
- Delta Learning mechanism lacks theoretical grounding for why contrastive preference learning succeeds where direct imitation fails
- RLVR implementation achieves stability through active sampling and multi-domain mixing, but optimal balance between domains remains an empirical hyperparameter

## Confidence

- **High Confidence**: Base model architecture choices (Olmo 2 foundation, attention patterns, activation functions) and pretraining methodology (Dolma 3 Mix curation, token mixing) - these follow established best practices with clear implementation details
- **Medium Confidence**: Midaftering and long-context extension effectiveness - while improvements are documented, specific data curation strategies and their relative contributions are not fully isolated through ablation
- **Low Confidence**: Delta Learning mechanism and RLVR stability innovations - these represent novel algorithmic contributions with limited theoretical justification and sparse external validation

## Next Checks

1. **Delta Learning Mechanism Isolation**: Conduct controlled experiments comparing DPO with delta learning against standard DPO on identical preference pairs, and against SFT on chosen responses, across multiple capability domains to quantify the specific contribution of contrastive learning versus preference optimization generally

2. **Long-Context Extension Component Ablation**: Systematically vary YaRN application (all layers vs. full-attention only), document packing strategies, and synthetic data augmentation ratios to determine which components drive RULER improvements versus baseline long-context performance

3. **Multi-domain RL Optimization Study**: Evaluate domain-specific versus mixed-domain RL training across different capability profiles to identify optimal domain mixing ratios for various target applications, and test whether active sampling's benefits persist when varying group sizes and reward noise levels