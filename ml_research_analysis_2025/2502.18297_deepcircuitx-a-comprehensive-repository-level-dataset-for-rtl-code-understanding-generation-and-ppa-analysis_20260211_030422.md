---
ver: rpa2
title: 'DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code Understanding,
  Generation, and PPA Analysis'
arxiv_id: '2502.18297'
source_url: https://arxiv.org/abs/2502.18297
tags:
- code
- design
- dataset
- data
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepCircuitX is a comprehensive repository-level dataset for Register
  Transfer Level (RTL) code understanding, generation, and power-performance-area
  (PPA) analysis in hardware design automation. Unlike existing datasets that focus
  only on file-level RTL code or physical layouts, DeepCircuitX provides a holistic
  resource spanning repository, file, module, and block levels, along with synthesized
  netlists and PPA metrics.
---

# DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code Understanding, Generation, and PPA Analysis

## Quick Facts
- arXiv ID: 2502.18297
- Source URL: https://arxiv.org/abs/2502.18297
- Reference count: 40
- Key outcome: DeepCircuitX is a comprehensive repository-level dataset for Register Transfer Level (RTL) code understanding, generation, and power-performance-area (PPA) analysis in hardware design automation.

## Executive Summary
DeepCircuitX addresses the gap in RTL-focused machine learning datasets by providing a multi-level repository spanning over 4,000 RTL projects across 77 functional categories. Unlike existing datasets that focus on file-level code or physical layouts, DeepCircuitX offers hierarchical organization (repository → file → module → block) with synthesized netlists and PPA metrics. The dataset is enriched with Chain of Thought annotations generated by GPT-4 and Claude, enabling effective training of large language models for RTL understanding, completion, and generation tasks. Experimental results demonstrate significant improvements across BLEU, METEOR, ROUGE, and Pass@k metrics for various RTL tasks, establishing new benchmarks for hardware design automation.

## Method Summary
DeepCircuitX collects RTL code from GitHub using 222 keywords, organizing 4,000+ repositories into 77 functional categories. The dataset employs a Chain of Thought annotation pipeline using GPT-4 and Claude to generate detailed descriptions, comments, and question-answer pairs at multiple hierarchical levels. Logic synthesis via Synopsys Design Compiler produces netlists and PPA reports across multiple technology libraries. The dataset supports training and evaluation of LLMs (220M–16B parameters) for RTL understanding, completion, generation, and PPA prediction tasks, with benchmarks including RTLLM and VerilogEval.

## Key Results
- Fine-tuning LLMs on DeepCircuitX significantly improves BLEU-4 scores (e.g., CodeGen2.5: 0.1060 → 13.6858) for RTL code understanding tasks
- Pass@1 scores for RTL code generation increase substantially (e.g., CodeLlama-7B: 24.25% on RTLLM)
- PPA prediction accuracy improves with increased training data volume, with MAPE for area prediction decreasing from 1.9379 to 0.6564 as training data scales from 10% to 100%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on DeepCircuitX improves RTL code understanding, completion, and generation performance.
- Mechanism: The dataset's multi-level hierarchical structure (repository → file → module → block) provides training signals at multiple granularities, allowing models to learn both local code patterns (blocks, modules) and global structural relationships (repositories). This hierarchical exposure enables transfer of learned representations across scales, improving generalization on downstream RTL tasks.
- Core assumption: Models benefit from exposure to hierarchical context; RTL code understanding requires both local syntax/semantics and global architectural awareness.
- Evidence anchors:
  - [abstract] "DeepCircuitX provides a holistic, multilevel resource that spans repository, file, module, and block-level RTL code. This structure enables more nuanced training and evaluation of large language models (LLMs) for RTL-specific tasks."
  - [section IV-C, Table VI] Fine-tuned models show significant BLEU-4 improvements (e.g., CodeGen2.5: 0.1060 → 13.6858; CodeT5+: 0.1410 → 4.9067).
  - [corpus] Related work (RTLRepoCoder, RTLSquad) similarly leverages repository-level context for RTL tasks, suggesting this is an active research direction, though direct causal attribution remains empirically underexplored.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) annotations enhance LLM training data quality for RTL tasks.
- Mechanism: CoT annotations generated by GPT-4 and Claude provide structured reasoning traces (what the code does, how it achieves functionality, port descriptions, internal signal explanations). These detailed annotations serve as supervision signals that align model outputs with human-interpretable explanations, improving both understanding and generation capabilities.
- Core assumption: High-quality natural language annotations improve code-language model alignment; LLM-generated annotations are sufficiently accurate for training purposes.
- Evidence anchors:
  - [abstract] "DeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering detailed descriptions of functionality and structure at multiple levels."
  - [section III-B] "We propose a Chain of Thought (CoT) detailed annotation method to generate descriptions and comments for each of the four levels... Additionally, we generate question-answer pairs to help describe the functionality and key features of each code segment."
  - [section IV-B, Table V] Human evaluation shows annotations score 3.5–3.84/4 on Accuracy, Completeness, and Understandable Clarity metrics.

### Mechanism 3
- Claim: PPA prediction accuracy improves as training data volume increases.
- Mechanism: Learning-based PPA predictors (e.g., SNS, MasterRTL) trained on synthesized netlists and corresponding PPA metrics from the dataset can learn correlations between RTL-derived features and final PPA outcomes. Larger training sets provide more diverse circuit examples, improving model generalization.
- Core assumption: RTL-to-PPA correlations are learnable from synthesis reports; early-stage RTL features contain sufficient signal for accurate PPA estimation.
- Evidence anchors:
  - [abstract] "Additionally, the dataset includes synthesized netlists and PPA metrics, facilitating early-stage design exploration and enabling accurate PPA prediction directly from RTL code."
  - [section IV-E, Table VIII] MAPE for area prediction decreases from 1.9379 (10% data) to 0.6564 (100% data) for SNS model; similar scaling observed for MasterRTL.

## Foundational Learning

- Concept: Register Transfer Level (RTL) and Verilog/VHDL fundamentals
  - Why needed here: The entire dataset is built around RTL code understanding; without grasping hardware description languages, module hierarchies, and synthesizable constructs, you cannot meaningfully evaluate or extend this work.
  - Quick check question: Can you explain the difference between combinational and sequential logic blocks in Verilog, and identify which constructs are synthesizable?

- Concept: Logic synthesis and PPA metrics
  - Why needed here: The dataset's multimodal transformation relies on synthesis tools (Synopsys Design Compiler) to generate netlists and PPA reports. Understanding how synthesis optimizes area, power, and timing is essential for interpreting experimental results and designing PPA prediction tasks.
  - Quick check question: What trade-offs does a synthesis tool make between area, power, and timing, and how would you interpret a critical path delay report?

- Concept: Large Language Model fine-tuning paradigms
  - Why needed here: The paper's experiments involve fine-tuning CodeLlama, CodeT5+, CodeGen, and DeepSeek on domain-specific data. Understanding fine-tuning (vs. pre-training), evaluation metrics (BLEU, ROUGE, Pass@k), and domain adaptation is necessary to replicate or extend the results.
  - Quick check question: What is the difference between Pass@1 and Pass@5, and why might BLEU scores be insufficient for evaluating functional correctness of generated RTL code?

## Architecture Onboarding

- Component map: Data Collection Layer -> Annotation Layer -> Circuit Transformation Layer -> Training/Evaluation Layer -> Benchmarks
- Critical path: Data collection → Quality filtering/categorization → CoT annotation → Logic synthesis (multiple technology libraries: sky130, asap7, etc.) → PPA report extraction → Dataset packaging → Model fine-tuning → Benchmark evaluation. The synthesis step is bottleneck-prone due to tool licensing and runtime.
- Design tradeoffs:
  - Breadth vs. depth: Dataset covers 77 functional categories but may have uneven distribution across categories; some categories have more samples than others.
  - LLM annotation accuracy vs. cost: CoT annotations by GPT-4/Claude scale well but may introduce systematic errors (human eval scores 3.5–3.84/4, not perfect).
  - Technology library choice: PPA metrics are technology-dependent; predictors trained on sky130 may not generalize to asap7 (paper acknowledges this constraint).
  - Model scale vs. accessibility: 16B models show strong performance but require significant compute; 220M models offer accessibility with reduced capability.
- Failure signatures:
  - Synthesis failures: Some RTL repositories may not synthesize cleanly (missing dependencies, non-synthesizable constructs); these would be excluded from multimodal data.
  - Annotation hallucinations: LLM-generated descriptions may misdescribe functionality; human eval catches some but not all issues.
  - PPA prediction divergence: Delay prediction MAPE remains high (3.48–80.78), indicating current models struggle with timing estimation on complex designs.
  - Benchmark overfitting: Models may achieve high Pass@k on RTLLM/VerilogEval but fail on unseen industrial designs (not tested in paper).
- First 3 experiments:
  1. Replicate fine-tuning on a single model family (e.g., CodeT5+ 220M and 7B) using a subset of DeepCircuitX, comparing BLEU/ROUGE scores against baseline. This validates the dataset's training efficacy.
  2. Conduct an ablation study training models with vs. without CoT annotations to isolate the annotation quality contribution. The paper does not provide this; it would strengthen causal claims.
  3. Evaluate PPA prediction generalization by training on one technology library (e.g., sky130) and testing on another (e.g., asap7) to quantify cross-technology transferability and identify failure modes.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the accuracy of early-stage PPA prediction be improved for large-scale, practical RTL designs (over 10,000 cells) where current learning-based models currently struggle?
  - Basis in paper: [explicit] The authors state, "how to accurately predict PPA of practical designs remains an opening question, necessitating further exploration."
  - Why unresolved: Models that perform well on simple benchmarks exhibit significantly diminished accuracy when applied to complex, practical designs found in the DeepCircuitX dataset.
  - What evidence would resolve it: A learning-based model achieving low Mean Absolute Percentage Error (MAPE) across all Power, Performance, and Area metrics specifically on the >10k cell designs in the dataset.

- **Open Question 2**: To what extent can further model scaling and advanced training techniques improve the syntactic and functional accuracy of LLMs in RTL code generation tasks?
  - Basis in paper: [explicit] The paper concludes, "Future work could explore further model scaling and more advanced training techniques to push performance even higher."
  - Why unresolved: While fine-tuning on DeepCircuitX improved performance, the absolute Pass@1 scores (e.g., ~24%) indicate that reliability is insufficient for fully automating hardware design.
  - What evidence would resolve it: Demonstrating that larger parameter scales or novel training objectives yield significantly higher Pass@1 and Pass@5 scores on the established generation benchmarks.

- **Open Question 3**: What feature representations or architectures are required to accurately estimate timing characteristics from RTL code before logic synthesis optimization occurs?
  - Basis in paper: [explicit] The analysis notes, "estimating timing characteristics in the early stages using path features on RTL-based graphs is still difficult."
  - Why unresolved: Logic synthesis tools heavily optimize logic structures to minimize delay, making the raw features extracted from pre-synthesis RTL code a poor predictor of final timing.
  - What evidence would resolve it: A prediction framework that maintains high correlation with post-synthesis delay reports despite the aggressive logic optimizations performed by tools like Design Compiler.

## Limitations

- Cross-technology PPA generalization: Experimental results are not shown for cross-technology prediction, leaving open the question of real-world applicability when migrating designs across foundries.
- Annotation quality and bias: CoT annotations are generated by GPT-4/Claude, not humans, and evaluated at 3.5–3.84/4, with potential systematic hallucinations or style biases.
- Benchmark representativeness: RTLLM and VerilogEval benchmarks may not fully align with industrial RTL complexity, raising questions about real-world robustness.

## Confidence

- **High confidence**: The dataset construction methodology (hierarchical organization, CoT annotation pipeline, synthesis integration) is clearly specified and reproducible. The improvements in BLEU/METEOR/ROUGE for understanding and Pass@k for completion/generation are statistically significant and well-documented.
- **Medium confidence**: PPA prediction results show scaling trends with data volume, but the high MAPE for delay (3.48–80.78) and lack of cross-technology validation reduce confidence in generalizability.
- **Low confidence**: Causal attribution of performance gains to specific dataset features (e.g., multi-level hierarchy vs. CoT annotations) is not isolated via ablation studies; claims are correlative, not proven.

## Next Checks

1. **Ablation on annotation quality**: Retrain models with and without CoT annotations on the same data splits to isolate the contribution of high-quality descriptions to downstream task performance.
2. **Cross-technology PPA transfer**: Train PPA predictors on sky130 and evaluate on asap7/nangate to quantify domain shift and identify failure modes in technology-portable synthesis estimation.
3. **Industrial benchmark validation**: Test fine-tuned models on a held-out set of industrial RTL designs (if available) or on progressively larger and more complex modules to assess real-world robustness beyond academic benchmarks.