---
ver: rpa2
title: Pareto Optimal Algorithmic Recourse in Multi-cost Function
arxiv_id: '2502.07214'
source_url: https://arxiv.org/abs/2502.07214
tags:
- recourse
- uni00000003
- cost
- pareto
- paths
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of finding algorithmic recourse
  in multi-cost scenarios where individuals seek to alter features to achieve desired
  outcomes. The authors propose a novel framework that formulates recourse as a multi-objective
  optimization problem, allowing users to assign weights to different criteria based
  on their importance.
---

# Pareto Optimal Algorithmic Recourse in Multi-cost Function

## Quick Facts
- **arXiv ID:** 2502.07214
- **Source URL:** https://arxiv.org/abs/2502.07214
- **Reference count:** 40
- **Primary result:** Novel framework for finding Pareto optimal algorithmic recourse recommendations across multiple cost functions by formulating recourse as a multi-objective optimization problem on actionability graphs.

## Executive Summary
This paper addresses the challenge of finding algorithmic recourse in multi-cost scenarios where individuals seek to alter features to achieve desired outcomes. The authors propose a novel framework that formulates recourse as a multi-objective optimization problem, allowing users to assign weights to different criteria based on their importance. By constructing an actionability graph and modifying the shortest path algorithm, their method identifies Pareto optimal recourse recommendations. To handle scalability in large graphs, they incorporate the concept of ε-net to find approximated Pareto optimal actions. Experiments demonstrate the trade-offs between different criteria and show the method's scalability in large graphs. Compared to current heuristic practices, their approach provides a stronger theoretical foundation and better aligns recourse suggestions with real-world requirements, addressing concerns regarding interpretability, reliability, and transparency from the explainable AI perspective.

## Method Summary
The method constructs an actionability graph where nodes represent accessible data points and edges represent feasible actions with k cost values. A modified Bellman-Ford algorithm finds all Pareto-optimal paths by maintaining Pareto tables at each vertex and iteratively pruning dominated paths. For scalability, ε-net sampling reduces graph size while preserving approximation quality with high probability. The framework outputs Pareto-optimal recourse paths ending at data points with desired model predictions, allowing users to choose based on their cost preferences.

## Key Results
- Formulates algorithmic recourse as a multi-objective optimization problem on actionability graphs
- Provides theoretical guarantee of finding all Pareto-optimal paths via modified Bellman-Ford algorithm
- Demonstrates scalability through ε-net sampling with bounded approximation error
- Shows trade-offs between different cost criteria through experimental evaluation on MNIST and Adult datasets

## Why This Works (Mechanism)

### Mechanism 1: Multi-Cost Graph Formulation
- **Claim:** Representing recourse as paths on an actionability graph with multi-cost edges enables simultaneous optimization across non-differentiable cost functions.
- **Mechanism:** The method constructs a directed graph where nodes are accessible data points and edges represent feasible actions. Each edge carries k cost values (one per cost function). By defining domination (path P dominates Q iff all costs of P ≤ corresponding costs of Q), the problem reduces to finding Pareto-optimal paths rather than aggregating costs into a single scalar. This avoids the unrealistic requirement of pre-assigning weights to different cost criteria.
- **Core assumption:** Cost functions are metric functions; feasible actions can be determined pairwise between data points; the number of hops η is bounded (paths too long are considered non-interpretable).
- **Evidence anchors:**
  - [abstract]: "formulating recourse as a multi-objective optimization problem and assigning weights to different criteria based on their importance, our method identifies Pareto optimal recourse recommendations"
  - [Section 2]: Defines actionability graph construction, domination, and Pareto optimality formally
  - [corpus]: Related work (Dandl et al.) uses multi-objective evolutionary algorithms but focuses on specific criteria only; this work generalizes to arbitrary metric cost functions
- **Break condition:** If cost functions are not metrics, or if the actionability graph becomes fully connected with no pruning possible, the Pareto table size τ grows unboundedly, making the approach intractable.

### Mechanism 2: Pareto Table Propagation via Modified Bellman-Ford
- **Claim:** A modified dynamic programming algorithm can enumerate all Pareto-optimal paths in polynomial time relative to graph size and number of cost functions.
- **Mechanism:** Algorithm 1 maintains D^ℓ_v, a Pareto table at each vertex v storing non-dominated (cost vector, path) pairs at iteration ℓ. In each iteration, for each edge (u,v), the algorithm concatenates paths from u's Pareto table with edge costs W_uv, then prunes dominated entries. This extends Bellman-Ford's relaxation step to vector-valued costs. Theoretical guarantee: after η iterations (maximum path length), all Pareto-optimal paths from source to recourse points are found.
- **Core assumption:** The maximum Pareto table size τ is bounded; pruning correctly identifies dominated paths; the maximum degree γ and number of edges |E| do not explode combinatorially.
- **Evidence anchors:**
  - [Section 3, Algorithm 1]: Full pseudocode with initialization, dynamic programming loop, and Pareto table update
  - [Section 3, Analysis]: Time complexity O(γη log^{k-4} log log(γτ) · η|E|) with optimizations for k=2,3
  - [Theorem 1]: "Algorithm 1 finds all the Pareto optimal paths from the source to any endpoint t, where h(t) = 1" (proof in full version)
  - [corpus]: No direct comparison in corpus; gradient-based recourse methods lack this theoretical guarantee
- **Break condition:** If Pareto tables grow exponentially (e.g., all paths are incomparable), the algorithm degrades; if η is set too high, interpretability is lost and runtime increases linearly.

### Mechanism 3: Scalability via Epsilon-Net Graph Shrinking
- **Claim:** Random sampling with size dependent on VC-dimension and ε can provably approximate Pareto-optimal paths while reducing graph size.
- **Mechanism:** The approach shrinks the graph by identifying κ-shrinkable vertices (where merging vertex i into j preserves cost approximation within factor κ). To efficiently find a representative subgraph, it uses ε-net theory: a random sample of size O(|VC|*/ε log 1/ε) preserves approximation quality with high probability, where |VC|* is the maximum VC-dimension across cost functions. For costs satisfying discrete Lipschitz-like bounds (c(i,j) ≈ distance in data space), VC-dimension is O(d), making sampling practical.
- **Core assumption:** Cost functions have structure (e.g., discrete Lipschitz continuity) that bounds VC-dimension; shrinking preserves approximation within factor κ^ℓ where ℓ is path length; ε-net sampling is uniform random.
- **Evidence anchors:**
  - [Section 4, Definition 1-3]: Formal definitions of κ-shrinkable, shrunk graph, and ε-net in this context
  - [Section 4, Equation 1]: Haussler-Welzl theorem providing sample size bound O(|VC|/ε log 1/ε log 1/δ)
  - [Section 5.3, Figure 4]: Empirical validation showing sampling size ≥256 provides stable cost quality on MNIST
  - [corpus]: No comparable scalability solution in path-oriented recourse approaches according to authors
- **Break condition:** If cost functions are arbitrary (no Lipschitz-like structure), VC-dimension may be unbounded, making ε-net guarantees void; if κ^ℓ approximation is too loose, returned paths may be far from true Pareto-optimal.

## Foundational Learning

- **Concept: Pareto Optimality and Domination**
  - Why needed here: The core algorithm outputs Pareto-optimal paths, requiring understanding of when one solution dominates another across multiple objectives.
  - Quick check question: Given two paths with cost vectors (3, 10, 5) and (4, 8, 5), which dominates which, or are they incomparable?

- **Concept: Dynamic Programming for Shortest Paths (Bellman-Ford)**
  - Why needed here: Algorithm 1 is a modification of Bellman-Ford; understanding edge relaxation and iteration structure is prerequisite to grasping Pareto table updates.
  - Quick check question: In standard Bellman-Ford, what does the ℓ-th iteration compute, and why do we need |V|-1 iterations in the worst case?

- **Concept: Epsilon-Nets and VC-Dimension**
  - Why needed here: The scalability mechanism relies on ε-net theory to bound sample complexity; without this, the theoretical guarantee for approximation is not defensible.
  - Quick check question: What does it mean for a subset N to be an ε-net of set A with respect to range space R? If VC-dimension is d, what sample size suffices?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Actionability Graph Builder -> Pareto Path Engine -> Scalability Module (Optional) -> Recourse Presenter
- **Critical path:** Graph construction -> Pareto table initialization -> η iterations of (concatenate -> prune) -> filter recourse endpoints -> output Pareto-optimal paths. The prune step determines correctness; the iterate count η determines both completeness and interpretability.
- **Design tradeoffs:**
  - η (max path length): Lower = more interpretable but may miss valid recourse; higher = more complete but slower and less actionable
  - Number of cost functions k: More costs = richer trade-offs but Pareto tables grow (O(γ τ log^{k-4} ...))
  - ε-net sampling threshold: Aggressive shrinking = faster but looser approximation; conservative = better quality but may not scale
  - Graph connectivity (KNN k): Dense graphs = more paths but larger Pareto tables; sparse graphs = faster but may miss optimal recourse
- **Failure signatures:**
  - Empty output: Source has no path to any recourse point within η hops; check actionability constraints
  - Pareto table explosion: τ grows to O(|V|); indicates costs are nearly independent, many incomparable paths; increase pruning aggressiveness or reduce η
  - Paths violate domain constraints: Actionability graph construction error; verify immutable features and feasibility rules
  - Approximation quality degrades with sampling: ε-net size insufficient for data dimension; increase sample size or verify cost function has bounded VC-dimension
- **First 3 experiments:**
  1. **Validation on synthetic graph:** Construct small graph (20-50 nodes) with known Pareto-optimal paths; verify Algorithm 1 returns all and only non-dominated paths; compare against brute-force enumeration
  2. **Scalability benchmark:** Measure runtime and Pareto table size vs. |V| (128, 256, 512, 1024, 2048) on Adult dataset; plot scaling curve; identify knee point where ε-net becomes necessary
  3. **Cost trade-off visualization:** On MNIST or Adult, run with k=2 costs, plot Pareto frontier (cost_1 vs. cost_2), verify that relaxing one cost improves the other; compare paths selected by different users with different implicit weightings

## Open Questions the Paper Calls Out
- **Question:** Can an efficient, deterministic algorithm be developed to find the optimal minimum-cardinality shrunk graph ($G^*_S$) that eliminates the order-dependency of the current shrinking procedure?
  - **Basis in paper:** [explicit] The authors state that finding the subgraph with the smallest cardinality ($G^*_S$) is "highly non-trivial" because it depends on the order of vertices in the shrinking procedure, which led them to use the probabilistic $\epsilon$-net approach as a workaround.
  - **Why unresolved:** The paper utilizes random sampling ($\epsilon$-net) to approximate the shrinking, but does not provide a method to determine the optimal deterministic reduction of the graph.
  - **What evidence would resolve it:** An algorithm that identifies the optimal vertex set for shrinking in polynomial time, or a proof of NP-hardness showing that finding $G^*_S$ is computationally intractable.

## Limitations
- The approach's correctness depends critically on the actionability graph construction and Pareto table pruning efficiency
- Theoretical scalability guarantees assume cost functions have bounded VC-dimension, which may not hold for arbitrary metric costs
- The method's interpretability benefit hinges on users being able to understand and navigate Pareto frontiers

## Confidence
- **High Confidence:** The multi-cost graph formulation and modified Bellman-Ford algorithm are mathematically sound and well-defined
- **Medium Confidence:** The practical effectiveness depends on implementation details (feasibility rules, pruning optimizations) not fully specified in the paper
- **Low Confidence:** The framework's ability to handle very large graphs (>10K nodes) and high-dimensional cost spaces (k>10) remains unverified

## Next Checks
1. **Pareto Optimality Verification:** Construct small synthetic graphs (20-50 nodes) with known Pareto-optimal paths; run Algorithm 1 and verify it returns all and only non-dominated paths compared to exhaustive enumeration
2. **Scalability Benchmark:** Measure runtime and Pareto table sizes across graph sizes (128, 256, 512, 1024, 2048) on Adult dataset; plot scaling curves; identify threshold where epsilon-net sampling becomes necessary and verify approximation quality
3. **Cost Trade-off Analysis:** On MNIST or Adult with k=2 costs, plot Pareto frontiers and verify that relaxing one cost improves the other; test with different user-defined implicit weightings to ensure method returns diverse paths reflecting user preferences