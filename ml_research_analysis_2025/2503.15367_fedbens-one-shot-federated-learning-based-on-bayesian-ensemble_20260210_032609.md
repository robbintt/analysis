---
ver: rpa2
title: 'FedBEns: One-Shot Federated Learning based on Bayesian Ensemble'
arxiv_id: '2503.15367'
source_url: https://arxiv.org/abs/2503.15367
tags:
- learning
- federated
- one-shot
- global
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FedBEns, a one-shot federated learning method
  based on Bayesian inference that addresses the limitations of unimodal approximations
  used in prior work. The method leverages mixtures of Laplace approximations to capture
  multiple modes in local loss functions, enabling the server to construct a multimodal
  global posterior that is then used for Bayesian marginalization.
---

# FedBEns: One-Shot Federated Learning based on Bayesian Ensemble

## Quick Facts
- arXiv ID: 2503.15367
- Source URL: https://arxiv.org/abs/2503.15367
- Authors: Jacopo Talpini; Marco Savi; Giovanni Neglia
- Reference count: 21
- Primary result: Bayesian ensemble method achieves up to 10 percentage points higher accuracy than state-of-the-art one-shot federated learning baselines across FashionMNIST, SVHN, and CIFAR10 datasets

## Executive Summary
FedBEns introduces a one-shot federated learning method that leverages Bayesian inference with multimodal approximations to overcome the limitations of unimodal approaches. The method trains multiple local models per client, approximates their posteriors as mixtures of Laplace distributions, and aggregates these into a global multimodal posterior at the server. This enables Bayesian marginalization over multiple modes rather than relying on a single maximum a posteriori estimate, leading to improved generalization performance across heterogeneous data distributions.

## Method Summary
The method trains M independent models per client from different random initializations, then approximates each local posterior as a Gaussian mixture via Laplace approximation. The server receives these mixtures and computes a global posterior using the formula p(w|D₁,...,Dc) ∝ ∏c p(w|Dc) / p(w)^(C-1), which corrects for prior over-counting when multiplying posteriors. The global posterior is optimized using M independent SGD runs from median-initialized starting points, and predictions are made by averaging over the ensemble of posterior modes. The approach uses Kronecker factorization for Hessian approximation and cold temperature scaling (T=0.1) to improve approximation quality.

## Key Results
- FedBEns consistently outperforms state-of-the-art baselines (FedFisher, RegMean, DENSE, OTFusion, Fisher Merge) across all tested heterogeneity levels (α = 0.05, 0.1, 0.2, 0.4)
- Kronecker factorization achieves up to 10 percentage points higher accuracy than diagonal Hessian approximations
- Using just 2-5 mixture components provides significant accuracy gains over single-mode approaches
- FedBEns maintains competitive performance even when compared to FedFisher with 5 communication rounds
- Cold temperature scaling (T=0.1) and appropriate prior variance selection are critical for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal approximation of local posteriors captures diverse optima that unimodal methods miss, leading to better global model construction.
- Mechanism: Each client trains M independent models from different random initializations, then approximates their local posterior as a mixture of M Gaussians via Laplace approximation at each solution. The server receives these mixtures and aggregates them into a multimodal global posterior. This explicitly accounts for multiple local minima in the loss landscape, some of which may generalize better than others.
- Core assumption: Neural network loss functions exhibit meaningful multimodality where different local minima capture complementary information about the data distribution.
- Evidence anchors:
  - [abstract] "leverages the inherent multimodality of local loss functions to find better global models"
  - [section 1] "The figure illustrates that the local minimum corresponding to the broader, less sharply-peaked mode of client 1's local loss, is crucial for accurately reconstructing the optimal global model"
  - [corpus] Weak direct corpus support for multimodality mechanism; neighboring papers focus on feature statistics and adaptation rather than loss landscape multimodality
- Break condition: If local loss landscapes are effectively unimodal (e.g., convex or highly overparameterized networks with single basins), the mixture components collapse to redundant solutions, eliminating marginal benefit over unimodal approximations.

### Mechanism 2
- Claim: The exact posterior aggregation formula preserves all information from local datasets while enabling single-round communication.
- Mechanism: The global posterior is computed as p(w|D₁,...,Dc) ∝ ∏c p(w|Dc) / p(w)^(C-1). The denominator p(w)^(C-1) corrects for prior over-counting that would occur when multiplying C posteriors that each already incorporate the prior. This mathematical identity enables combining posteriors without accessing raw data.
- Core assumption: Clients share the same prior distribution, and datasets are conditionally independent given the model parameters.
- Evidence anchors:
  - [section 3.1] "combining the posteriors via Equation (2) provides the same information about the model as if it had been trained in the usual centralized setting"
  - [appendix A] Full proof provided for the posterior aggregation formula under conditional independence
  - [corpus] No direct corpus validation of this specific aggregation formula
- Break condition: If local datasets exhibit conditional dependence (e.g., overlapping data, shared temporal patterns), the factorization assumption fails and the aggregated posterior no longer equals the true global posterior.

### Mechanism 3
- Claim: Bayesian marginalization via ensembling outperforms MAP optimization by averaging over posterior modes.
- Mechanism: Rather than selecting a single optimal parameter setting, the server samples M modes from the global posterior (initialized from the median of client solutions) and averages their predictions: p(y|x,D) ≈ (1/M)∑ p(y|x,ŵm). Cold temperature scaling (T=0.1) concentrates the posterior to improve Laplace approximation quality.
- Core assumption: The ensemble of posterior modes provides a better approximation to the true predictive distribution than any single mode.
- Evidence anchors:
  - [section 4.2] "it is advantageous to consider a mixture of these solutions and use all of them to make predictions, in line with a Bayesian Model Averaging perspective"
  - [table 4] Ablation shows ensemble accuracy (60.08% FMNIST, α=0.05) exceeds best single component (59.98%)
  - [section 6.1] "significant boost observed even with just two or three components"
- Break condition: If posterior modes are highly correlated (provide similar predictions) or if the number of mixture components M is insufficient to capture the true posterior complexity, marginalization benefits diminish.

## Foundational Learning

- Concept: **Laplace Approximation**
  - Why needed here: Core technique for tractably approximating posterior distributions as Gaussians centered at MAP estimates using Hessian curvature. FedBEns requires understanding how the precision matrix Λ relates to the Hessian of the negative log-likelihood.
  - Quick check question: Given a neural network with d parameters, explain why computing the full Hessian is impractical and name two factorization approaches that reduce this cost.

- Concept: **Bayesian Model Averaging vs. MAP Estimation**
  - Why needed here: FedBEns fundamentally differs from prior work by marginalizing over the posterior rather than optimizing to find its maximum. Understanding why prediction via p(y|x,D) = ∫ p(y|x,w)p(w|D)dw outperforms p(y|x,w_MAP) is essential.
  - Quick check question: In a multimodal posterior, why might a high-probability mode generalize worse than a lower-probability but broader mode?

- Concept: **Kronecker-Factored Approximate Curvature (K-FAC)**
  - Why needed here: FedBEns uses Kronecker factorization as the primary Hessian approximation method (superior to diagonal in experiments). Understanding how the Hessian factorizes as a Kronecker product per layer (O(d₁² + d₂²) vs O((d₁·d₂)²)) explains the scalability.
  - Quick check question: For a layer with 512 input neurons and 256 output neurons, compare the storage requirements of full Hessian vs. Kronecker factorization.

## Architecture Onboarding

- Component map:
  - ClientTraining() -> Laplace Approximation Module -> Server Aggregation -> ServerTraining() -> Inference

- Critical path:
  1. Client-side: Train M models → Compute Laplace approximations per model → Transmit {ŵ, Λ} pairs
  2. Server-side: Aggregate mixtures into global posterior formula → Run M independent SGD runs (300 steps, Adam) from median initialization → Select best validation checkpoint per component
  3. Inference: Ensemble predictions via (1/M)∑ p(y|x,ŵglobal_m)

- Design tradeoffs:
  - M (mixture count): Higher M improves accuracy (Figure 2 shows near-monotonic gains) but increases communication O(M×model_size) and client compute O(M×training_cost). Paper shows M=2-5 sufficient.
  - Hessian approximation: Kronecker most accurate but ~3.9d parameters; Diagonal+Full ~2.7d; Diagonal cheapest but worst performance (Figure 3)
  - Temperature T: T=0.1 (cold) improves approximation quality; T>1 degrades performance significantly (Figure 4)

- Failure signatures:
  - High variance across seeds (Table 1 shows ±3-6% in high heterogeneity α=0.05) indicates sensitivity to initialization and data split
  - CIFAR10 performance drops sharply with more clients (Table 2: 61.49%→54.87% as C goes 5→40) suggesting small local datasets cause overfitting
  - If communication cost constraint exists and M>3, consider that FedFisher with 5 rounds may outperform FedBEns with 5 mixtures in some settings (Table 3: CIFAR10 α=0.05)

- First 3 experiments:
  1. **Sanity check**: Replicate FashionMNIST with C=5, α=0.1, M=2 using Kronecker factorization; target ~70% accuracy per Table 1. This validates the full pipeline with minimal compute.
  2. **Hessian ablation**: Compare Diagonal vs. Diagonal+Full vs. Kronecker on SVHN with M=3, α=0.2. Expect Kronecker to exceed diagonal by 5-10 percentage points per Figure 3.
  3. **Scaling test**: Run CIFAR10 with varying client counts (C=10, 20, 40) at α=0.3 to characterize performance degradation pattern. Monitor if accuracy drop matches Table 2's trend (~5-6% loss per 2× client increase).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational burden of training multiple local ensemble members be mitigated by approximating the mixture of Laplacians within a single model architecture?
- Basis in paper: [explicit] The Conclusion identifies computational cost as a "main limitation" and proposes exploring methods like "mimicking ensemble prediction within a single model" (citing Havasi et al., 2021) to reduce the cost of training M parallel models.
- Why unresolved: The current algorithm scales linearly with the number of mixture components M, potentially excluding resource-constrained clients.
- What evidence would resolve it: An adaptation of FedBEns that uses a single-model representation for the ensemble, demonstrating comparable accuracy to the multi-model approach with reduced local training time.

### Open Question 2
- Question: How does the transmission of local curvature information (Hessians) impact privacy leakage, and can Differential Privacy (DP) be effectively integrated?
- Basis in paper: [explicit] The Conclusion explicitly states: "we plan to study the privacy risks associated with the proposed approach and try to enhance it, e.g., through differential privacy techniques."
- Why unresolved: Sharing second-order information (precision matrices) alongside weights may reveal more information about local data distributions than standard gradient sharing, yet current experiments lack a formal privacy analysis.
- What evidence would resolve it: A theoretical analysis defining the privacy budget for sharing Laplace approximation parameters, or empirical results showing accuracy retention when DP noise is injected into the local precision matrices.

### Open Question 3
- Question: Is the proposed method robust in strictly data-free server settings where the temperature and prior variance cannot be tuned using a validation set?
- Basis in paper: [inferred] Section 2.2 critiques other methods for requiring server-side data, yet Section 5.2 reveals FedBEns uses a "small fraction of the training data... kept at the server as validation data" for hyperparameter tuning.
- Why unresolved: The method's reliance on a validation set to select critical hyperparameters (like the cold temperature T=0.1) creates a dependency on server-side data availability, which may not always be permissible.
- What evidence would resolve it: Ablation studies demonstrating that hyperparameters selected without a validation set (e.g., via heuristics or cross-validation on local client data) achieve statistically similar performance to the current validation-tuned results.

## Limitations

- The computational cost scales linearly with the number of mixture components M, potentially limiting applicability to resource-constrained clients
- Strong performance degradation with increasing client count suggests overfitting on small local datasets, particularly for complex datasets like CIFAR10
- Reliance on a validation set at the server for hyperparameter tuning creates a dependency on server-side data availability
- Multimodality mechanism lacks direct corpus evidence for federated settings and assumes meaningful multiple modes exist in local loss landscapes

## Confidence

**High Confidence**: The mathematical derivation of the posterior aggregation formula (Eq. 2) and the correctness of the Laplace approximation framework. The Kronecker factorization implementation and its computational advantages are well-established.

**Medium Confidence**: The empirical superiority of FedBEns over baselines across datasets and heterogeneity levels. The results are consistently better, but the confidence intervals (±3-6%) indicate substantial variance that could affect reproducibility.

**Low Confidence**: The core mechanism claim that capturing multiple modes via Laplace approximation is the primary driver of performance gains. While supported by ablation studies, this mechanism is not directly validated against alternatives like deeper ensembles or different posterior approximation methods.

## Next Checks

1. **Mechanism Isolation Test**: Run FedBEns with M=1 (single mode) vs. M=3 on FashionMNIST with varying α. If the performance gap narrows significantly at low heterogeneity, this would challenge the multimodality mechanism's importance.

2. **Alternative Posterior Approximation**: Replace the Laplace approximation with a variational inference approach or deeper ensemble training. Compare whether these achieve similar gains without relying on Hessian approximations, which would suggest the multimodality itself (not the approximation method) drives improvements.

3. **Client Dataset Size Sensitivity**: Systematically vary the number of local samples per client (e.g., 50, 100, 200) while holding C=10 fixed. This would clarify whether the accuracy degradation with more clients is primarily due to smaller datasets or increased heterogeneity.