---
ver: rpa2
title: 'From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token''s
  Nature'
arxiv_id: '2509.16591'
source_url: https://arxiv.org/abs/2509.16591
tags:
- tokens
- entropy
- advantage
- token
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of uniform optimization in
  existing reinforcement learning from human feedback (RLHF) methods for large language
  models (LLMs), which ignores the heterogeneous nature of tokens in reasoning processes.
  The authors introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive
  token-aware algorithm that dynamically adapts optimization based on token entropy.
---

# From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature

## Quick Facts
- arXiv ID: 2509.16591
- Source URL: https://arxiv.org/abs/2509.16591
- Reference count: 40
- Primary result: HAPO achieves 1.97-3.07 point accuracy gains on math reasoning benchmarks while maintaining exploration

## Executive Summary
This paper addresses the limitation of uniform optimization in existing RLHF methods for LLMs, which ignores the heterogeneous nature of tokens in reasoning processes. The authors introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. HAPO comprises four key innovations: Adaptive Temperature Sampling adjusts sampling temperature by token entropy; Token-Level Group Average normalizes advantages across tokens to balance positive and negative samples; Differential Advantage Redistribution modulates advantages using entropy and importance ratios for fine-grained optimization; and Asymmetric Adaptive Clipping applies entropy-conditioned boundaries to suppress noise and encourage exploration.

## Method Summary
HAPO is a token-aware policy optimization algorithm that recognizes tokens have heterogeneous natures requiring different optimization treatments. The method computes entropy statistics at the batch level and applies them throughout the optimization pipeline. It adjusts sampling temperature based on token entropy, normalizes advantages across all tokens rather than sequences, redistributes advantages using both entropy and importance ratios, and applies asymmetric clipping boundaries conditioned on entropy values. The algorithm is built on DPO with PPO-style clipping and introduces negligible computational overhead while consistently improving performance across multiple model scales on mathematical reasoning tasks.

## Key Results
- HAPO consistently outperforms DAPO across multiple model scales, achieving average accuracy gains ranging from 1.97 to 3.07 points on mathematical reasoning benchmarks
- The method maintains longer response lengths and higher entropy during training, indicating preserved exploration capabilities
- Token-level group averaging effectively balances positive and negative advantages, addressing the sequence-level normalization bias
- Adaptive temperature sampling achieves both higher accuracy and more critical tokens than fixed temperatures

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Temperature Sampling
- Claim: Dynamic temperature adjustment based on token entropy increases critical token generation while maintaining accuracy
- Mechanism: High-entropy tokens receive elevated temperature ($T_{high} > 1.0$) to encourage exploration at reasoning branch points; low-entropy tokens receive reduced temperature ($T_{low} < 1.0$) to preserve coherence. Temperature is computed per-token using normalized log-entropy: $T_{i,t} = T_{base} \cdot (1 + \frac{\log(H_{i,t}) - \rho_{\log(H)}}{\sigma_{\log(H)}} \cdot \tau)$
- Core assumption: Entropy measured at each position reflects genuine uncertainty about the next token, and increasing temperature specifically at high-entropy positions yields beneficial exploration rather than noise
- Evidence anchors:
  - [abstract]: "Adaptive Temperature Sampling adjusts sampling temperature by token entropy"
  - [section 4.1, Observation 4, Figure 9]: Shows accuracy decreases with temperature while critical token count increases; adaptive strategy achieves both higher accuracy and more critical tokens than fixed temperatures
  - [corpus]: Related work "Efficient Reinforcement Learning with Semantic and Token Entropy" addresses similar entropy dynamics but at sequence level; token-level temperature adjustment is distinct
- Break condition: If entropy measurements become unreliable (e.g., miscalibrated model) or if temperature adjustments produce incoherent outputs, the mechanism may degrade rather than help

### Mechanism 2: Token-Level Group Average with Differential Advantage Redistribution
- Claim: Normalizing advantages across all tokens (not sequences) balances positive/negative gradients; subsequent entropy-ratio modulation directs stronger updates toward tokens with clear optimization signals
- Mechanism: First, compute advantages across all tokens in group: $A_{i,t} = (a_{i,t} - \mu_{tok}) / \sigma_{tok}$, ensuring $\sum A_{i,t} = 0$. Then apply differential redistribution: high-entropy tokens with importance ratios outside the "neutral zone" ($r \notin [\gamma_L, \gamma_U]$) receive amplified advantages; low-entropy tokens with ratios inside the zone receive suppressed advantages
- Core assumption: Importance ratio deviation from 1.0 indicates the model has a clear update direction for that token, and this signal is more reliable than entropy alone
- Evidence anchors:
  - [abstract]: "Differential Advantage Redistribution modulates advantages using entropy and importance ratios for fine-grained optimization"
  - [section 4.2, Figure 11]: Shows sequence-level normalization biases toward negative advantages; token-level normalization restores balance
  - [section 4.2, Figure 14]: Importance ratio analysis reveals high-entropy tokens with high advantages show strong ratio-advantage correlation
  - [corpus]: "GTPO and GRPO-S" proposes token-level reward shaping but uses different credit assignment; ESPO uses entropy importance sampling at batch level rather than token-specific
- Break condition: If importance ratios become unstable during early training (ratios ≈ 1 for all tokens), the neutral zone classification becomes unreliable

### Mechanism 3: Asymmetric Adaptive Clipping
- Claim: Entropy-conditioned asymmetric clipping boundaries enable aggressive noise suppression for low-entropy tokens while permitting exploration for high-entropy tokens
- Mechanism: Low-entropy tokens receive expanded left boundary ($\epsilon_L^{low} > \epsilon_L^{high}$, e.g., 0.35 vs 0.2) allowing greater probability reduction. High-entropy tokens receive expanded right boundary ($\epsilon_R^{high} > \epsilon_R^{low}$, e.g., 0.35 vs 0.2) allowing greater probability increase. Boundaries scale continuously with normalized entropy
- Core assumption: Low-entropy tokens hitting left boundaries are predominantly noise (formatting, artifacts) that should be suppressed; high-entropy tokens hitting right boundaries include critical reasoning elements that should explore
- Evidence anchors:
  - [abstract]: "Asymmetric Adaptive Clipping applies entropy-conditioned boundaries to suppress noise and encourage exploration"
  - [section 4.3, Figures 16-17]: Analysis shows left-clipped tokens dominated by low-entropy formatting symbols; right-clipped tokens dominated by high-entropy reasoning elements
  - [corpus]: Archer applies similar entropy-based clipping but uses binary categorization and completely relaxes boundaries; this paper's continuous asymmetric approach differs
- Break condition: If semantic analysis assumptions are wrong (e.g., some low-entropy tokens are actually important), aggressive suppression could harm performance

## Foundational Learning

- Concept: **Importance Sampling Ratio** ($r_t(\theta) = \pi_\theta(a_t|s_t) / \pi_{\theta_{old}}(a_t|s_t)$)
  - Why needed here: Central to PPO-style algorithms; determines how much the policy has shifted. HAPO uses ratios to identify tokens with clear update directions (ratios far from 1.0)
  - Quick check question: If a token has ratio = 0.6, what does this mean about the current policy's probability versus the old policy's probability?

- Concept: **Advantage Estimation** (GAE vs. group normalization)
  - Why needed here: Determines which tokens/sequences receive positive or negative reinforcement. HAPO modifies advantage computation from sequence-level to token-level group averaging
  - Quick check question: Why might longer sequences with negative rewards dominate gradient computation in a token-mean loss framework?

- Concept: **Entropy as Uncertainty Signal**
  - Why needed here: HAPO uses entropy pervasively to classify tokens and modulate all pipeline stages. Understanding entropy-accuracy tradeoffs is essential
  - Quick check question: A token has entropy = 0.1. What does this suggest about the model's confidence, and how would HAPO treat it differently from a token with entropy = 1.5?

## Architecture Onboarding

- Component map: Rollout Generation: Prompt → [Adaptive Temperature Sampler] → Responses → Advantage Computation: Responses + Rewards → [Token-Level Group Average] → [Differential Advantage Redistribution] → Modulated Advantages → Loss Computation: Modulated Advantages + Log Probs → [Asymmetric Adaptive Clipping] → Policy Update
- Critical path: Entropy computation must happen first (per-token during rollout, batch-level for statistics). Token-level group averaging must precede differential redistribution. Clipping boundaries are computed per-token based on entropy before loss calculation
- Design tradeoffs:
  - Pre-norm vs. post-norm advantage redistribution: Paper tested both; post-norm (modulate after averaging) showed more stable training (Figure 21)
  - Binary vs. continuous entropy treatment: Binary provides analytical clarity; continuous avoids artificial discontinuities at thresholds
  - Neutral zone width: Set to half the clipping range ($[\gamma_L, \gamma_U] = [1-\epsilon_L/2, 1+\epsilon_R/2]$); narrower zones amplify more tokens
- Failure signatures:
  - Entropy collapse: If average entropy drops sharply during training, exploration has failed. Symptom: response length decreases rapidly (Figure 12 shows this with relaxed left clipping)
  - Advantage instability: If max advantage values fluctuate wildly, check token-level group averaging is correctly normalizing
  - Ratio-driven amplification of noise: If performance degrades, the neutral zone may be misclassifying tokens; check ratio distributions
- First 3 experiments:
  1. **Reproduce baseline entropy dynamics**: Run DAPO on a small subset, track entropy distribution, critical token ratio, and clipping patterns per Figure 8/16 to establish reference behavior
  2. **Ablate one component at a time**: Start with only Token-Level Group Average (Table 2, row with only B), verify advantage balance improves per Figure 11 before adding other components
  3. **Validate adaptive temperature in isolation**: Compare fixed temperatures {0.5, 0.8, 1.0, 1.1} vs. entropy-guided adaptive per Figure 10; expect ~2-3 point accuracy gain and longer responses if working correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does HAPO's entropy-guided exploration strategy degrade performance on tasks requiring deterministic outputs, such as code generation?
- Basis in paper: [inferred] The Introduction claims the design "ensures generalizability to other domains," yet all experiments (AIME, MATH, OlympiadBench) are restricted to mathematical reasoning
- Why unresolved: The "Adaptive Temperature Sampling" increases randomness for high-entropy tokens to encourage reasoning exploration. In code generation, high entropy often indicates syntax errors rather than useful exploration paths
- What evidence would resolve it: Benchmarking HAPO on code generation tasks (e.g., HumanEval) or translation tasks to verify if the heterogeneous strategy helps or harms structural accuracy

### Open Question 2
- Question: Is HAPO compatible with standard KL divergence regularization, or does it render such constraints redundant?
- Basis in paper: [explicit] Section 5.3 states, "Crucially, we excludes both KL divergence loss and entropy loss" from the training configuration
- Why unresolved: It is unclear if HAPO's asymmetric clipping and advantage redistribution implicitly perform the stability function of KL penalties, or if the methods would conflict if combined
- What evidence would resolve it: Ablation studies reintroducing the KL penalty term $\beta D_{KL}$ into the HAPO objective to measure the impact on training stability and final accuracy

### Open Question 3
- Question: Does the "Dual-Entropy Phenomenon" cause the model to inadvertently suppress contextually important tokens that happen to have low entropy?
- Basis in paper: [explicit] Section 3 identifies that critical reasoning tokens often have "twin siblings" with identical word stems but low entropy
- Why unresolved: HAPO applies stricter left clipping boundaries to low-entropy tokens to suppress noise. The paper does not verify if this mechanism accidentally suppresses the "low-entropy twin" of a critical reasoning token in a different context
- What evidence would resolve it: A fine-grained analysis of token-level gradient updates specifically for "Dual-Entropy" pairs to ensure they are treated according to their semantic role rather than just their instantaneous entropy value

## Limitations
- All reported experiments focus on mathematical reasoning tasks, limiting generalization claims to other domains
- Computational overhead claims lack quantitative measurements to validate "negligible" impact
- Modest accuracy improvements (1.97-3.07 points) lack statistical significance measures and effect size analysis

## Confidence
- **High confidence (✦✦✦✦✧)**: The core observation that tokens have heterogeneous natures requiring different optimization treatments is well-supported by empirical analysis
- **Medium confidence (✦✦✦✧✧)**: The adaptive temperature sampling mechanism shows promise but requires more validation; the differential advantage redistribution's reliance on importance ratios needs more theoretical justification
- **Low confidence (✦✧✧✧✧)**: Claims about negligible computational overhead lack quantitative support; generalization beyond mathematical reasoning remains unproven

## Next Checks
1. **Computational overhead quantification**: Measure and report wall-clock time, memory usage, and FLOPs for HAPO versus DAPO across different batch sizes and model scales, including breakdown of overhead by component
2. **Cross-domain generalization study**: Apply HAPO to at least two non-mathematical reasoning tasks (e.g., code generation, creative writing, summarization) with at least three different model scales, comparing performance against DAPO with statistical analysis
3. **Training stability and hyperparameter sensitivity analysis**: Conduct experiments varying critical hyperparameters across at least five runs per configuration, reporting convergence curves, variance in final performance, and identifying stable operating regions