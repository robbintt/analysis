---
ver: rpa2
title: 'INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems'
arxiv_id: '2507.21130'
source_url: https://arxiv.org/abs/2507.21130
tags:
- integral
- theta
- frac
- problems
- numerical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INTEGRALBENCH provides a specialized benchmark of 317 graduate-level
  definite integral problems with both symbolic and numerical ground truth solutions
  and manual difficulty annotations. The dataset is constructed through LLM-assisted
  curation from academic sources, with problems converted from images to LaTeX and
  parameters instantiated for problems containing free variables.
---

# INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems

## Quick Facts
- arXiv ID: 2507.21130
- Source URL: https://arxiv.org/abs/2507.21130
- Reference count: 40
- Primary result: 9 state-of-the-art LLMs achieve PASS@3 scores ranging from 4.11% to 44.48% on graduate-level definite integral problems

## Executive Summary
INTEGRALBENCH provides a specialized benchmark of 317 graduate-level definite integral problems with both symbolic and numerical ground truth solutions and manual difficulty annotations. The dataset is constructed through LLM-assisted curation from academic sources, with problems converted from images to LaTeX and parameters instantiated for problems containing free variables. Evaluation of nine state-of-the-art LLMs reveals significant performance gaps, with larger models generally performing better but architecture and training methodology proving equally critical. The 32B QwQ model outperforms several larger models, achieving 44.48% PASS@3 numerical accuracy. A strong negative correlation exists between problem difficulty and model accuracy, with performance dropping sharply on challenging problems. Inference-time scaling analysis shows models exhibit rapid early gains followed by plateauing, with varying "sweet spots" across architectures. Failure mode analysis reveals critical weaknesses including output truncation, circular reasoning patterns, format violations, and symbolic-numerical inconsistency.

## Method Summary
INTEGRALBENCH evaluates LLMs on 317 graduate-level definite integral problems using a standardized pipeline. Each problem includes LaTeX integrand, symbolic answer, numerical answer (10 decimal places), and difficulty rating (1-5). Models receive a standardized prompt template requesting both symbolic and numerical answers in JSON format. The evaluation uses PASS@N and ALL@N metrics, where PASS@3 requires at least one correct answer in three attempts and ALL@3 requires all three attempts to be correct. Numerical verification uses an absolute error threshold of 10^-6, while symbolic answers are verified through LLM-generated Python code execution. The benchmark tests nine state-of-the-art LLMs including QwQ-32B, DeepSeek-R1, Claude-3.5-Sonnet, and others, with results stratified by difficulty level and token consumption patterns.

## Key Results
- Performance varies widely across models, with QwQ-32B achieving 44.48% PASS@3 while some models score below 10%
- Strong negative correlation exists between problem difficulty and model accuracy, with near-zero performance on difficulty 4-5 problems
- Symbolic accuracy consistently exceeds numerical accuracy across all models, indicating computational reliability issues
- Inference-time scaling shows rapid early accuracy gains followed by plateauing, with distinct "sweet spots" for different architectures
- Failure modes include output truncation, circular reasoning patterns, format violations, and symbolic-numerical inconsistency

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Calibrated Evaluation Stratification
Manual difficulty annotations (1-5 scale) enable fine-grained capability differentiation that aggregate metrics obscure. Problems are clustered by expert-rated complexity (involving special functions, nested integrals, multi-step decomposition), creating a monotonic difficulty gradient. Models degrade predictably along this gradient, with near-perfect accuracy on difficulty 1-2 and near-zero on difficulty 4-5. Core assumption: Human difficulty ratings correlate with computational steps and symbolic reasoning depth required.

### Mechanism 2: Dual Verification (Symbolic + Numerical Consistency)
Requiring both symbolic and numerical correctness surfaces a distinct failure mode: symbolic-numerical inconsistency. Models generate symbolic expressions, then numerically evaluate them. Disagreement between the symbolic result's numerical evaluation and the stated numerical answer reveals computational unreliability even when reasoning is correct. Core assumption: Numerical error threshold of 10^-6 is appropriate for floating-point validation without false negatives.

### Mechanism 3: Inference-Time Scaling Saturation
Extended token budgets yield diminishing returns beyond model-specific "sweet spots," suggesting architecture-dependent reasoning efficiency. Models accumulate correct answers rapidly in early tokens, then plateau. Kimi-K1.5 peaks at ~20% token ratio; DeepSeek-R1 improves beyond 80%. This indicates some models extract information early while others require extended reasoning chains. Core assumption: Token consumption correlates with reasoning steps rather than verbose repetition.

## Foundational Learning

- **Definite Integral Components**
  - Why needed here: Understanding integrand, limits, and integration variable structure is required to parse problem format and validate solution completeness.
  - Quick check question: Can you identify the integrand, bounds, and variable in `∫₀¹ x²·ln(x)dx`?

- **PASS@N vs ALL@N Metrics**
  - Why needed here: PASS@3 measures peak capability (≥1 correct in 3 attempts); ALL@3 measures consistency (all 3 correct). The gap reveals inference stochasticity.
  - Quick check question: If a model gets 2/3 attempts correct, what is its PASS@3 and ALL@3?

- **Symbolic-Numerical Bridge**
  - Why needed here: The pipeline evaluates symbolic answers by generating Python code (mpmath) to numerically compute them, comparing to stated numerical answers.
  - Quick check question: Why might `π²/6` evaluate to a different float than expected due to precision settings?

## Architecture Onboarding

- **Component map:** Problem ingestion (OCR validation → LaTeX correctness check) → Parameter instantiation (for free variables → concrete values) → Model inference (standardized prompt, max_tokens per model) → Dual verification (symbolic parsing → code execution → numerical comparison) → Metric aggregation (stratified by difficulty, model, token consumption)

- **Critical path:** 1. Problem ingestion (OCR validation → LaTeX correctness check) 2. Parameter instantiation (for free variables → concrete values) 3. Model inference (standardized prompt, max_tokens per model) 4. Dual verification (symbolic parsing → code execution → numerical comparison) 5. Metric aggregation (stratified by difficulty, model, token consumption)

- **Design tradeoffs:** Cost vs. Scale: Human annotation ($479.93) dominates LLM API costs ($5.00); scaling requires automation. Coverage vs. Contamination: Graduate-level sources ensure difficulty but risk training overlap. Precision vs. False Negatives: 10^-6 threshold catches subtle errors but susceptible to floating-point instability.

- **Failure signatures:** Output truncation: verbose reasoning hits max_tokens before final answer. Circular reasoning: identical expressions repeated (e.g., `2^{k+2}` → `2^{k+2}` → ...). Format violations: correct math embedded in invalid JSON structure. Refusal: "No simple closed-form solution" when symbolic answer exists.

- **First 3 experiments:** 1. Baseline calibration: Run all 9 models on difficulty-1 problems only to establish upper-bound consistency; expect PASS@3 >90% for all. 2. Token budget sweep: For QwQ-32B and DeepSeek-R1, vary max_tokens (25%, 50%, 75%, 100%) to identify actual plateau points beyond the observed "sweet spots." 3. Failure mode injection: Intentionally truncate 5% of outputs mid-generation to validate that truncation detection logic correctly identifies this failure type vs. format violations.

## Open Questions the Paper Calls Out

### Open Question 1
Can automated expansion methods scale INTEGRALBENCH beyond 317 problems while preserving mathematical rigor? Basis in paper: [explicit] The authors state in the Discussion that "future work will focus on developing more automated expansion methods... to scale the dataset efficiently while maintaining mathematical rigor and quality." Why unresolved: The current dataset relies heavily on costly and time-consuming manual verification to ensure correctness. What evidence would resolve it: A methodology that generates valid integral problems with ground truth solutions at scale without human intervention.

### Open Question 2
To what extent can fine-tuning on INTEGRALBENCH optimize models for integral computation compared to general mathematical training? Basis in paper: [explicit] The authors note the dataset "can serve as a training resource to fine-tune LLMs or specialized agents... to optimize models for integral computation specifically." Why unresolved: The paper currently utilizes the dataset exclusively for evaluating pre-trained models rather than as a training corpus. What evidence would resolve it: Performance benchmarks of models fine-tuned on INTEGRALBENCH compared to their base versions on unseen integrals.

### Open Question 3
Does expanding evaluation to PASS@16 better characterize model potential by mitigating the inference randomness observed in PASS@3? Basis in paper: [explicit] The Limitations section suggests that "higher sampling rates (e.g., PASS@16) would provide more robust estimates... better characterizing the range of LLM performance under inference randomness." Why unresolved: The gap between PASS@3 and ALL@3 indicates high variance, but higher N metrics were not computed. What evidence would resolve it: Comparative data showing if PASS@16 scores stabilize and reveal capabilities hidden by lower sampling rates.

## Limitations

- **Contamination risk**: Graduate-level sources create potential overlap with model training corpora, particularly affecting easier problems
- **Numerical precision sensitivity**: 10^-6 threshold may be overly conservative, rejecting mathematically correct answers due to floating-point instability
- **Annotation subjectivity**: Human-assigned difficulty ratings may not perfectly correlate with actual computational complexity

## Confidence

- **High confidence**: Current LLMs struggle with graduate-level integration problems, evidenced by PASS@3 scores below 45% even for the best-performing model
- **Medium confidence**: Difficulty stratification claims, as the negative correlation between difficulty and accuracy is observed but could be influenced by training contamination or annotation bias
- **Low confidence**: Absolute numerical threshold of 10^-6 as the sole arbiter of correctness, given systematic gap between symbolic and numerical accuracy

## Next Checks

1. **Training data overlap audit**: Perform detailed analysis of model training corpora against benchmark sources to quantify contamination risk, including checking for exact problem matches and similar problem patterns

2. **Threshold sensitivity analysis**: Systematically vary the numerical precision threshold (10^-4 to 10^-8) and observe changes in symbolic-numerical consistency rates to determine if the current threshold is too strict

3. **Difficulty calibration validation**: Conduct blind difficulty rating by independent experts and compare against original annotations, additionally analyzing whether difficulty ratings correlate with measurable features (number of integration steps, special functions required, nesting depth)