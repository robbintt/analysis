---
ver: rpa2
title: A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using
  Large Language Models
arxiv_id: '2504.15552'
source_url: https://arxiv.org/abs/2504.15552
tags:
- qinqiang
- script
- arxiv
- opera
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a multi-agent framework that automates the
  complete production of Qinqiang opera scripts, visuals, and performances using AI
  technologies. Three specialized agents collaborate: one generates culturally accurate
  scripts via LLM prompt engineering, another creates stage visuals through image
  generation models, and a third produces expressive vocal performances using TTS
  synthesis.'
---

# A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models

## Quick Facts
- arXiv ID: 2504.15552
- Source URL: https://arxiv.org/abs/2504.15552
- Reference count: 27
- Primary result: Multi-agent AI pipeline generates Qinqiang opera scripts, visuals, and performances with expert ratings of 3.6/5 overall, outperforming single-agent baseline by 0.3 points

## Executive Summary
This paper presents a multi-agent framework that automates the complete production of Qinqiang opera scripts, visuals, and performances using AI technologies. Three specialized agents collaborate: one generates culturally accurate scripts via LLM prompt engineering, another creates stage visuals through image generation models, and a third produces expressive vocal performances using TTS synthesis. Tested on Dou E Yuan, the system achieved expert ratings of 3.8/5 for script fidelity, 3.5/5 for visual coherence, and 3.8/5 for speech accuracy, with an overall score of 3.6/5—outperforming a single-agent baseline by 0.3 points. Ablation studies confirmed that removing any agent degraded performance, validating the modular collaboration approach. This work demonstrates how AI can preserve and scale traditional performing arts while pointing to future enhancements in cross-modal alignment and emotional nuance.

## Method Summary
The framework employs a sequential pipeline of three specialized agents: Agent1 (script generation) uses GPT-4o with hierarchical prompting to generate structured JSON scripts following Qinqiang conventions; Agent2 (visual generation) employs DALL-E to create stage visuals and character illustrations from scene descriptions; Agent3 (voice synthesis) uses TTS to produce expressive vocal performances with emotion annotations. The system relies entirely on prompt engineering rather than fine-tuning, passing structured JSON outputs between agents. Expert evaluation on a 5-point scale assessed script fidelity, visual coherence, speech accuracy, and overall performance across the Dou E Yuan opera.

## Key Results
- Expert ratings achieved: 3.8/5 for script fidelity, 3.5/5 for visual coherence, 3.8/5 for speech accuracy, 3.6/5 overall
- Outperformed single-agent baseline by 0.3 points on overall score
- Ablation studies confirmed each agent's contribution—removing any agent degraded performance
- Successfully generated complete Qinqiang opera production including script, visuals, and vocal performance

## Why This Works (Mechanism)
The multi-agent approach works by decomposing the complex task of opera production into specialized subtasks handled by purpose-built AI models. Agent1's LLM prompt engineering ensures cultural authenticity and structural adherence to Qinqiang conventions, while Agent2 and Agent3 transform textual outputs into visual and auditory modalities respectively. The modular design allows for parallel optimization of each component while maintaining overall coherence through structured data passing. The sequential pipeline prevents modality misalignment during generation, though it introduces potential cascading errors if early agents produce flawed outputs.

## Foundational Learning
- **Prompt Engineering for Cultural Tasks:** Critical for guiding LLMs to generate culturally authentic content without fine-tuning; quick check involves testing prompt variations on obscure cultural references
- **Multi-Agent System Architecture:** Enables specialization and parallel optimization while maintaining coordination through structured data formats; quick check is running ablation studies to measure individual agent contributions
- **Cross-Modal Generation Pipeline:** Sequential processing ensures modality alignment during generation but may propagate errors; quick check involves comparing emotional consistency across generated modalities
- **Expert Evaluation Methodology:** Essential for validating culturally sensitive AI outputs; quick check involves establishing clear rubric criteria and inter-rater reliability metrics
- **JSON Structured Data Passing:** Provides consistent format for agent communication and error isolation; quick check is schema validation at each pipeline stage
- **Traditional Opera Domain Knowledge:** Required for effective prompt engineering and evaluation; quick check involves consultation with domain experts on generated outputs

## Architecture Onboarding

- **Component Map:** Agent1 (LLM) -> Agent2 (Image Generator) -> Agent3 (TTS)
- **Critical Path:**
  1. User provides high-level inputs (theme, characters) to Agent1
  2. Agent1 generates detailed, structured script in JSON format
  3. Script is parsed to feed separate inputs to Agent2 (scene descriptions) and Agent3 (dialogue/lyrics)
  4. Agent2 and Agent3 generate visual and audio outputs respectively
  5. Final output combines script, visual assets, and audio file

- **Design Tradeoffs:**
  - **Modularity vs. Cohesion:** Modular design enables easy model upgrades but lacks shared context that unified models might have; agents operate in isolation potentially missing cross-modal alignment
  - **Prompt Engineering vs. Fine-tuning:** Prompt engineering is faster and cheaper but may be less robust for capturing highly nuanced cultural data not present in pre-training
  - **Sequential vs. Parallel Processing:** Sequential pipeline ensures modality alignment during generation but introduces cascading error potential

- **Failure Signatures:**
  - **Cascading Errors:** Script inaccuracies amplify through visual and audio generation stages
  - **Modality Misalignment:** Emotional dissonance between visual and audio outputs due to lack of shared context
  - **Cultural Hallucination:** LLM generates plausible but incorrect cultural details that downstream agents faithfully reproduce

- **First 3 Experiments:**
  1. **End-to-End Test with Ablation:** Run full pipeline on Dou E Yuan scene, then disable Agent2 and Agent3 separately to replicate ablation study findings
  2. **Single-Agent Baseline Comparison:** Implement single-agent approach generating script, visual description, and audio cues simultaneously to test 0.3-point improvement claim
  3. **Cultural Fidelity Stress Test:** Provide Agent1 with obscure cultural prompts and manually inspect script for historical accuracy and symbolic correctness

## Open Questions the Paper Calls Out
- **Open Question 1:** How can TTS architectures be refined to capture genre-specific vocal ornaments, such as Qinqiang's unique vibrato and nuanced modulations?
  - Basis: Results section notes "unique vibrato and nuanced modulations still need refinement"
  - Why unresolved: Standard TTS models optimized for natural speech rather than stylized opera vocalizations
  - Evidence needed: Expert ratings specifically on "vocal ornamentation" and "style fidelity" metrics

- **Open Question 2:** What mechanisms can improve cross-modal alignment between generated visuals and audio?
  - Basis: Conclusion states intent to "deepen cross-modal alignment between text, imagery, and audio"
  - Why unresolved: Sequential pipeline lacks feedback loops to correct inconsistencies
  - Evidence needed: Quantitative metric for cross-modal consistency showing improvement over sequential baseline

- **Open Question 3:** Is the prompt engineering framework transferable to other traditional opera genres?
  - Basis: Abstract and Conclusion identify "support for additional opera genres" as necessary future direction
  - Why unresolved: System tested exclusively on Dou E Yuan; Qinqiang-specific prompts may not generalize
  - Evidence needed: Successful application to distinct genre (e.g., Peking Opera) with comparable expert ratings

## Limitations
- Cultural authenticity evaluation criteria remain underspecified, limiting assessment of true adherence to Qinqiang conventions
- Cross-modal emotional alignment between visuals and audio is imperfect due to decoupled agent operation
- Unclear TTS model specification—paper mentions "Whisper" but this is an ASR model, not TTS

## Confidence
- **High Confidence:** Modular pipeline architecture validated by ablation studies; clear sequential design and reproducible API usage
- **Medium Confidence:** Quantitative evaluation results plausible but exact rubric and inter-rater reliability not detailed
- **Low Confidence:** Single-agent baseline comparison insufficiently detailed—no implementation description or evaluation process provided

## Next Checks
1. **Cross-Modal Emotion Consistency Test:** Run pipeline on Qinqiang scene with annotated emotional arcs and compare emotional tone consistency across modalities
2. **Cultural Hallucination Audit:** Generate scripts for obscure Qinqiang themes and cross-reference with domain experts to measure hallucination rates
3. **TTS Model Clarification and Replication:** Identify exact TTS system used, implement with emotion-conditioned parameters, and verify reported speech accuracy performance