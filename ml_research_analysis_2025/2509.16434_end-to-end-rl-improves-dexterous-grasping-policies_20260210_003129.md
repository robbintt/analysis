---
ver: rpa2
title: End-to-end RL Improves Dexterous Grasping Policies
arxiv_id: '2509.16434'
source_url: https://arxiv.org/abs/2509.16434
tags:
- policies
- which
- policy
- simulation
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling up image-based end-to-end
  reinforcement learning for dexterous grasping with a robot arm and hand system.
  The authors identify that vision-based RL is memory inefficient and results in low
  batch sizes, which is problematic for algorithms like PPO.
---

# End-to-end RL Improves Dexterous Grasping Policies

## Quick Facts
- arXiv ID: 2509.16434
- Source URL: https://arxiv.org/abs/2509.16434
- Reference count: 33
- Primary result: Vision-based teachers lead to better performance than state-based teachers when distilled into vision policies, achieving 93% real-world grasping success rate

## Executive Summary
This paper addresses the challenge of scaling up image-based end-to-end reinforcement learning for dexterous grasping with a robot arm and hand system. The authors identify that vision-based RL is memory inefficient and results in low batch sizes, which is problematic for algorithms like PPO. They propose a disaggregated simulation and RL framework that separates the simulator and RL training onto separate GPUs, allowing them to double the number of environments that can be simulated on the same hardware compared to traditional data parallelism. This increased efficiency enables training end-to-end depth policies for dexterous grasping, which are then distilled into stereo RGB policies.

## Method Summary
The paper introduces a disaggregated simulation and RL framework that addresses memory efficiency issues in vision-based reinforcement learning for dexterous grasping. The key innovation is separating the simulator and RL training onto different GPUs, which allows doubling the number of parallel environments compared to traditional data parallelism approaches. This framework enables training end-to-end depth policies, which are subsequently distilled into stereo RGB policies. The authors demonstrate that vision-based teachers (depth policies) outperform state-based teachers when used for distillation, both in simulation and real-world experiments, achieving a 93% success rate on a real-world grasping task.

## Key Results
- Vision-based teachers (depth policies) lead to better performance than state-based teachers when distilled into vision policies
- Best model achieved 93% success rate on real-world grasping task
- Disaggregated simulation framework doubles the number of environments that can be simulated on the same hardware compared to traditional data parallelism
- Outperformed previous state-of-the-art vision-based methods

## Why This Works (Mechanism)
The disaggregated simulation framework addresses the fundamental bottleneck in vision-based RL: memory efficiency and batch size limitations. By separating simulator and RL training onto different GPUs, the approach enables scaling to more parallel environments, which is crucial for effective PPO training. The success of vision-based teachers over state-based teachers suggests that the visual representation learned through end-to-end training captures task-relevant features that transfer better to vision-based policies during distillation.

## Foundational Learning

1. **Reinforcement Learning with PPO**: Why needed - PPO is the algorithm used for training the policies; quick check - understand advantage estimation and clipping mechanism
2. **Simulation-to-Real Transfer**: Why needed - policies are trained in simulation and deployed on real robots; quick check - understand domain randomization and transfer challenges
3. **Policy Distillation**: Why needed - transferring knowledge from depth-based policies to RGB-based policies; quick check - understand teacher-student framework and KL divergence
4. **Multi-GPU Parallelization**: Why needed - scaling RL training to more environments; quick check - understand data parallelism vs disaggregated approaches
5. **Dexterous Grasping Control**: Why needed - the task involves controlling a robot hand and arm; quick check - understand degrees of freedom and control frequency
6. **Depth vs RGB Vision**: Why needed - comparing different visual input modalities; quick check - understand advantages and limitations of each modality

## Architecture Onboarding

Component Map: GPU-1 (Simulator) <-> GPU-2 (RL Trainer) -> Policy Network -> Robot Control Interface

Critical Path: The simulation generates observations and rewards, which are sent to the RL trainer. The trainer updates the policy parameters, which are then used by the simulator for the next step. This loop continues until training converges.

Design Tradeoffs: The disaggregated approach trades increased communication overhead between GPUs for better memory efficiency and the ability to scale to more parallel environments. This is particularly beneficial for vision-based RL where each environment requires significant memory for image buffers.

Failure Signatures: Performance degradation could indicate communication bottlenecks between GPUs, insufficient batch size despite parallelization, or poor transfer from simulation to reality. Visual artifacts in training could suggest issues with image preprocessing or network architecture.

First Experiments:
1. Verify basic PPO training with the disaggregated framework on a simple task to confirm the communication pipeline works
2. Compare batch size and memory usage between disaggregated and traditional data parallelism approaches
3. Test policy performance with synthetic depth data before moving to real depth sensors

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Heavy reliance on simulation-to-real transfer without extensive real-world training data raises questions about policy robustness to real-world variations
- Claims about the disaggregated framework doubling efficiency lack quantitative comparison to alternative parallelization strategies
- Evaluation focuses primarily on grasping success rates without comprehensive analysis of failure modes or policy generalization across diverse object types

## Confidence

- High confidence: The disaggregated simulation framework concept and its implementation details
- Medium confidence: Real-world grasping success rates and comparison to previous vision-based methods
- Low confidence: Claims about the superiority of vision-based teachers over state-based teachers, as the comparison methodology could be influenced by architectural differences

## Next Checks

1. Conduct ablation studies comparing the disaggregated framework against standard data parallelism with matched computational resources to quantify the claimed efficiency gains
2. Test policy performance on a diverse set of objects beyond the benchmark set used in the paper, including objects with varying textures, sizes, and materials
3. Evaluate policy robustness by introducing systematic variations in lighting, camera calibration, and object placement to assess real-world generalization limits