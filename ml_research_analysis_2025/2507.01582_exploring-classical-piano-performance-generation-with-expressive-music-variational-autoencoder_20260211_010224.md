---
ver: rpa2
title: Exploring Classical Piano Performance Generation with Expressive Music Variational
  AutoEncoder
arxiv_id: '2507.01582'
source_url: https://arxiv.org/abs/2507.01582
tags:
- music
- performance
- expressive
- decoder
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating classical piano
  performances from scratch by emulating both composer and performer roles. The authors
  introduce the Expressive Compound Word (ECP) representation to capture metrical
  structure and expressive nuances, then propose the Expressive Music Variational
  AutoEncoder (XMVAE) with two branches: a VQ-VAE Composer branch for generating scores
  and a vanilla VAE Pianist branch for producing expressive parameters.'
---

# Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder

## Quick Facts
- arXiv ID: 2507.01582
- Source URL: https://arxiv.org/abs/2507.01582
- Authors: Jing Luo; Xinyu Yang; Jie Wei
- Reference count: 23
- Key outcome: XMVAE outperforms state-of-the-art models across six of seven objective metrics and shows superior subjective quality in listening tests

## Executive Summary
This paper addresses the challenge of generating classical piano performances from scratch by introducing the Expressive Music Variational AutoEncoder (XMVAE). The model employs a dual-branch architecture that separately handles score composition and performance expression, using the Expressive Compound Word (ECP) representation to capture metrical structure and expressive nuances. The VQ-VAE Composer branch generates discrete score tokens while the vanilla VAE Pianist branch produces continuous expressive parameters. Through comprehensive evaluation on the ATEPP dataset, XMVAE demonstrates superior performance across multiple objective metrics and subjective listening tests compared to baseline models.

## Method Summary
XMVAE uses a dual-branch architecture with a VQ-VAE Composer branch for score generation and a vanilla VAE Pianist branch for expressive parameter generation. The ECP representation encodes musical information into 8 sub-tokens per timestep (4 score tokens: Family, Beat-Position, Pitch, Duration; 4 performance tokens: Beat-Period, Velocity, Timing, Articulation). The model employs a multiscale encoder with beat-level attention to capture rhythmic context, and orthogonal Transformer decoders that process compound tokens along temporal and sub-token axes separately. Training involves joint optimization of both branches with reconstruction and KL divergence losses, optionally including pretraining of the Composer branch on additional musical scores.

## Key Results
- XMVAE achieves UPC of 7.37 versus 7.93-8.06 for baselines, indicating better pitch diversity
- Performance Rank (PR) reaches 56.14 versus 45.42-56.09 for baselines, showing superior pitch distribution
- Average Pitch Span (APS) of 10.92 versus 10.24-11.43 for baselines demonstrates better expressive range
- Downbeat Stability (DSTD) of 0.2486 versus 0.2289-0.3922 for baselines indicates improved rhythmic consistency
- Inter-Onset Interval (IOI) of 0.1473 versus 0.1087-0.1396 for baselines shows better timing stability
- Articulation Variation Index (AVI) of 8.28 versus 6.84-8.20 for baselines demonstrates superior dynamics

## Why This Works (Mechanism)

### Mechanism 1: Role-Separated Dual-Branch Architecture
The dual-branch architecture separates composition (score generation) from performance (expressive parameter generation), reducing model burden and improving output quality. The VQ-VAE Composer branch generates discrete score tokens at the note level, while the vanilla VAE Pianist branch produces continuous expressive parameters using the score codes as conditioning. This separation assumes score content and performance style are partially independent factors that benefit from specialized processing pathways.

### Mechanism 2: Beat-Constrained Multiscale Encoding
The multiscale encoder combines standard self-attention with beat-level self-attention that limits each time step's attention range to its corresponding beat (24 ticks per beat). This beat-constrained attention improves rhythmic coherence by enabling localized rhythmic context learning, leveraging the hierarchical organization of classical piano performance around metrical beats.

### Mechanism 3: Orthogonal Axis Decoding for Compound Tokens
The orthogonal Transformer decoder processes compound tokens along separate temporal and sub-token axes. The Temporal Decoder processes along the time axis to produce hidden states, while the SubToken Decoder processes along the sub-token axis using these hidden states as memory. This approach assumes sub-tokens within each compound token have interdependencies that benefit from explicit modeling.

## Foundational Learning

- **Variational Autoencoders (VAE) and Vector Quantized VAE (VQ-VAE)**: Essential for understanding latent space sampling, KL divergence regularization, and discrete codebook quantization. Quick check: Can you explain why VQ-VAE uses a discrete codebook while vanilla VAE uses continuous latent space?

- **Transformer attention mechanisms and positional encoding**: Both encoder and decoder are Transformer-based, requiring understanding of self-attention, cross-attention, and position encoding. Quick check: How does beat-level self-attention differ from standard global self-attention?

- **Symbolic music representations (MIDI, REMI, Compound Word)**: The ECP representation extends CP; understanding note events, timing quantization, and expressive parameters is prerequisite. Quick check: What are the four score sub-tokens and four performance sub-tokens in ECP?

## Architecture Onboarding

- **Component map**: Input ECP tokens -> Embedding summation -> Multiscale Encoder -> VQ quantization (z_s) + VAE sampling (z_p) -> Temporal Decoder -> SubToken Decoder -> Output token predictions

- **Critical path**: Input ECP tokens are converted to embeddings, processed through the multiscale encoder, quantized via VQ and sampled via VAE, then decoded through orthogonal Transformers to produce final token predictions.

- **Design tradeoffs**: Discrete scores vs continuous expressive parameters (VQ-VAE vs vanilla VAE), joint training vs sequential training (enables gradient flow but increases memory), beat-level attention window (improves rhythmic coherence but may miss long-range dependencies), pretraining on score-only data (improves performance but requires additional data curation).

- **Failure signatures**: High UPC (>8.0) + low PR (<50) indicates repetitive, narrow-range pitch sequences; high DSTD (>0.35) shows rhythmic instability; low AVI (<7.5) reveals dynamics lacking expressiveness; missing beat-level attention causes rhythmic metrics to degrade.

- **First 3 experiments**:
  1. Reproduce baseline comparison: Train MT and ECPT with identical hyperparameters on aligned dataset; verify metrics fall within reported ranges before implementing full XMVAE.
  2. Ablate multiscale encoder: Train XMVAE without beat-level attention; expect UPC to increase (~7.9) and rhythmic metrics to degrade per Table II.
  3. Test pretraining effect: Pretrain Composer branch on Musescore scores (1,189 files), then fine-tune on aligned dataset; expect improvement across most metrics, especially UPC and PR.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can XMVAE effectively perform score-to-performance rendering by feeding known score latent sequences directly to the Performance Decoder? The paper observes potential for this application but has not tested it.

- **Open Question 2**: Can the vanilla VAE's latent variable z_p enable controllable style transfer or manipulation of performance characteristics across different performers? The paper samples z_p randomly but has not tested whether it meaningfully disentangles performer-specific style attributes.

- **Open Question 3**: Would training on larger, higher-quality aligned score-performance datasets significantly close the quality gap between generated and real music? The current dataset is relatively small (282 scores) and the paper suggests data scarcity may limit performance.

## Limitations

- Data Coverage: The ATEPP dataset subset (282 scores, 787 performances) is relatively small for classical music generation, potentially limiting generalization across diverse interpretations.
- Architecture Generalizability: The beat-level attention mechanism and compound token design may not transfer effectively to genres with ambiguous meter, free rhythm, or non-piano instruments.
- Evaluation Scope: Objective metrics may not fully capture musical qualities like voice leading, harmonic progression, or structural coherence in multi-voice classical compositions.

## Confidence

**High Confidence Claims:**
- XMVAE outperforms baseline models on seven specified objective metrics
- Dual-branch architecture provides measurable improvements over single-branch approaches
- Beat-level attention contributes positively to rhythmic coherence
- Pretraining Composer branch on additional scores improves performance

**Medium Confidence Claims:**
- Orthogonal decoder architecture is superior to flattened decoding approaches
- ECP representation effectively captures metrical structure and expressive nuances
- Model successfully emulates both composer and performer roles
- Subjective quality ratings reflect genuine improvements in musical quality

**Low Confidence Claims:**
- Model's ability to generalize to classical music beyond training distribution
- Claims about computational efficiency compared to alternative architectures
- Long-term stability of generated performances beyond 256-note segments
- Effectiveness for interactive or controllable music generation

## Next Checks

1. **Ablation of SubToken Decoder**: Train XMVAE without the SubToken Decoder component to directly measure its contribution. Based on reported ablation (PR drops from 56.14 to 47.83, UPC rises to 8.06), removing this component should significantly degrade PR while increasing UPC.

2. **Genre Transfer Experiment**: Apply trained XMVAE to generate performances for a different musical genre (e.g., jazz or contemporary classical) without fine-tuning. Measure whether beat-level attention and compound token structure introduce harmful bias when metrical structure differs from classical piano.

3. **Expert Musician Evaluation**: Conduct blind listening test with professional classical pianists or composers who rate generated performances on compositional quality, voice leading, harmonic sophistication, and stylistic authenticity. This addresses the limitation of general listener evaluations and provides domain-expert validation.