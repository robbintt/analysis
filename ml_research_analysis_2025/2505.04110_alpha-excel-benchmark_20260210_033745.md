---
ver: rpa2
title: Alpha Excel Benchmark
arxiv_id: '2505.04110'
source_url: https://arxiv.org/abs/2505.04110
tags:
- excel
- challenges
- performance
- benchmark
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel benchmark for evaluating Large Language
  Models (LLMs) using 113 Financial Modeling World Cup Excel challenges converted
  to JSON format. The benchmark tests models across categories like financial modeling,
  game simulations, and data analysis, revealing that GPT-4o-mini achieved the highest
  overall accuracy, especially on structured financial tasks (20% accuracy).
---

# Alpha Excel Benchmark

## Quick Facts
- arXiv ID: 2505.04110
- Source URL: https://arxiv.org/abs/2505.04110
- Authors: David Noever; Forrest McKee
- Reference count: 3
- Key outcome: GPT-4o-mini achieved highest accuracy on structured financial tasks (>20% accuracy), with models performing better on financial/data analysis than complex game simulations requiring multi-step reasoning and state tracking

## Executive Summary
This study introduces a benchmark for evaluating Large Language Models (LLMs) using 113 Financial Modeling World Cup Excel challenges converted to JSON format. The benchmark tests models across categories like financial modeling, game simulations, and data analysis, revealing that GPT-4o-mini achieved the highest overall accuracy, especially on structured financial tasks (>20% accuracy). Models performed better on financial and data analysis challenges than on complex game simulations requiring multi-step reasoning and state tracking. The benchmark provides a practical, business-oriented evaluation framework bridging academic AI metrics with real-world Excel usage, highlighting numerical reasoning and rule application as key limitations for current LLMs.

## Method Summary
The benchmark converts 113 Financial Modeling World Cup Excel challenges into JSON format with standardized schema including challenge metadata, problem statements, input data, and structured question-answer pairs. Models are evaluated via API with temperature=0.2, 60-second timeout, randomized challenge order, and minimal few-shot examples. The JSON schema preserves core reasoning requirements while eliminating Excel-specific implementation details. Evaluation includes accuracy percentage by category and partial credit scoring to distinguish minor calculation errors from conceptual misunderstandings.

## Key Results
- GPT-4o-mini achieved highest overall accuracy, particularly excelling on structured financial tasks (>20% accuracy)
- Models performed significantly better on financial and data analysis challenges than on complex game simulations requiring multi-step reasoning and state tracking
- Numerical reasoning capabilities represent fundamental limitations, with calculation mistakes propagating through extended calculation chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting spreadsheet challenges to structured JSON preserves core cognitive demands while enabling automated LLM evaluation.
- Mechanism: The JSON schema captures challenge metadata, problem statements, input data, and structured question-answer pairs that mirror what human competitors receive, eliminating Excel-specific implementation details while retaining the reasoning requirements.
- Core assumption: The cognitive load of problem-solving transfers across representation formats (spreadsheet → JSON → natural language prompt).
- Evidence anchors:
  - [abstract] "We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats"
  - [Methods] "This translation process required balancing between preserving problem complexity and eliminating Excel-specific implementation details that would obscure rather than illuminate LLM reasoning capabilities"
  - [corpus] Weak direct support; related benchmarks (ELAIPBench, EDINET-Bench) use similar domain-to-evaluable-format transformations but for different domains
- Break condition: If challenge solutions depend critically on spatial/spreadsheet-specific features (cell navigation, visual scanning), the JSON representation underrepresents authentic task difficulty.

### Mechanism 2
- Claim: LLM performance degrades systematically with increasing state-tracking and rule-synthesis requirements.
- Mechanism: Game simulation challenges require maintaining accurate representations of evolving scenarios through multiple state transitions; models show "marked degradation in accuracy as simulation steps accumulated" due to architectural limitations in maintaining context through extended reasoning chains.
- Core assumption: Performance gaps reflect architectural constraints in working memory/state maintenance rather than insufficient training data alone.
- Evidence anchors:
  - [Results] "Models performed better on financial and data analysis challenges than on complex game simulations requiring multi-step reasoning and state tracking"
  - [Discussion] "State tracking deficiencies became particularly evident in game simulation challenges where models needed to maintain accurate representations of evolving scenarios through multiple state transitions"
  - [corpus] ACADREASON paper examines reasoning limits on complex problems, supporting multi-step degradation pattern
- Break condition: If failures stem primarily from ambiguous problem statements rather than state-tracking limits, improving prompt clarity would close the gap.

### Mechanism 3
- Claim: Numerical errors propagate through calculation chains, amplifying small mistakes into significantly divergent final answers.
- Mechanism: Challenges requiring "extended calculation chains" show compounding errors where "small errors propagated through subsequent steps, culminating in significantly divergent final answers"—suggesting current LLM architectures lack verification mechanisms for intermediate computational steps.
- Core assumption: Error propagation is a structural property of how LLMs generate sequential outputs, not a training data issue.
- Evidence anchors:
  - [Results] "Calculation mistakes, arithmetic errors, or flawed mathematical operations... frequently occurred in challenges requiring extended calculation chains"
  - [Discussion] "Numerical reasoning capabilities represent perhaps the most fundamental limitation revealed by our study"
  - [corpus] FLAWS benchmark examines error identification in scientific contexts but doesn't directly address numerical propagation mechanisms
- Break condition: If introducing external symbolic verification (e.g., calculator tools, code execution) eliminates the propagation effect, the mechanism is tooling-addressable rather than architectural.

## Foundational Learning

- Concept: **JSON schema design for evaluable benchmarks**
  - Why needed here: The benchmark's validity depends on preserving problem semantics while enabling programmatic scoring; poor schema design either loses task fidelity or makes evaluation inconsistent.
  - Quick check question: Can you map a sample FMWC challenge to a JSON structure that a) preserves the reasoning required, b) enables automated answer matching, and c) handles both numerical and categorical responses?

- Concept: **Temperature calibration for deterministic vs. creative outputs**
  - Why needed here: The paper uses temperature 0.2 to balance "deterministic precision necessary for numerical correctness with sufficient creative flexibility for novel problem-solving approaches."
  - Quick check question: At what temperature would you expect numerical tasks to become unreliable, and what metric would you use to detect this threshold?

- Concept: **Partial credit scoring for multi-step problems**
  - Why needed here: Binary correctness obscures capability differences; the benchmark uses "a scoring system that avoids binary correctness assessments" with provisions for distinguishing minor calculation errors from conceptual misunderstandings.
  - Quick check question: Design a rubric for a 5-step financial modeling problem that awards partial credit—what constitutes a "minor calculation error" vs. a "fundamental conceptual misunderstanding"?

## Architecture Onboarding

- Component map: [FMWC Excel Challenges] → [JSON Schema Transformation] → [Prompt Generator] → [LLM API Layer] → [Response Normalizer] → [Scoring Engine] → [Results Dashboard]
- Critical path: The prompt generation and response normalization stages determine evaluation fairness. The paper emphasizes that "semantically equivalent answers may appear in syntactically diverse forms, requiring both exact matching for precision-critical elements and semantic evaluation for conceptual responses."
- Design tradeoffs:
  - **Abstraction vs. authenticity**: JSON format enables automated evaluation but "sacrifices important visual and spatial aspects of spreadsheet work"
  - **Timeout calibration**: 60-second threshold balances model deliberation against practical constraints; diminishing returns observed beyond this threshold
  - **Single-turn vs. multi-turn evaluation**: Current design presents all questions simultaneously; future work suggests iterative refinement protocols would better mirror human problem-solving
- Failure signatures:
  - **Rule application errors**: Models apply individual rules correctly in isolation but fail to maintain logical consistency across rule interactions (common in game simulations)
  - **Context misunderstanding**: Solutions address different problems than presented, indicating extraction failures from natural language descriptions
  - **Calculation propagation**: Early numerical errors compound through multi-step chains
- First 3 experiments:
  1. Replicate the GPT-4o-mini vs. Mistral vs. Qwen2.5 comparison on a 20-challenge subset, confirming the >20% financial task accuracy and lower game simulation scores
  2. Test temperature sensitivity: run the same challenges at temperature 0.0, 0.2, 0.5, and 0.8 to validate the paper's claim that 0.2 balances precision and flexibility
  3. Isolate the state-tracking mechanism: create simplified versions of game simulation challenges with fewer state transitions to identify the step-count threshold where performance degrades sharply

## Open Questions the Paper Calls Out

- **Can hybrid architectures combining Large Language Models with symbolic mathematics engines significantly reduce the numerical reasoning errors observed in game simulation challenges?**
  - Basis: The authors suggest "future LLM architectures would benefit from specialized components dedicated to precise calculation verification" and "hybrid approaches that combine language model capabilities with specialized numerical reasoning components."
  - Why unresolved: Current models exhibit calculation mistakes and state tracking deficiencies that worsen with complexity, indicating probabilistic token prediction alone is insufficient for precise, multi-step arithmetic.
  - What evidence would resolve it: A comparative study benchmarking a hybrid model against standalone LLMs, showing a statistically significant reduction in "calculation mistakes" and "rule application errors" on the FMWC dataset.

- **To what extent do multi-turn, agentic evaluation protocols improve model performance on complex Excel tasks compared to the single-turn methodology used in this study?**
  - Basis: The paper states that "Multi-turn evaluation protocols represent a particularly promising direction" that could mirror how human experts refine solutions, contrasting with the current single-turn approach.
  - Why unresolved: The current methodology relies on a single response within a 60-second window, which fails to capture the iterative debugging and refinement process characteristic of authentic Excel usage.
  - What evidence would resolve it: A re-evaluation of the benchmark using a multi-turn framework where models can query feedback and refine answers, resulting in higher accuracy scores for complex challenges.

- **Can collaborative benchmarks accurately quantify the utility of LLMs as assistive tools for human users with varying levels of Excel expertise?**
  - Basis: The authors propose that "Collaborative benchmarks assessing how effectively models assist human users with Excel tasks would provide perhaps the most ecologically valid performance metrics."
  - Why unresolved: Current benchmarks measure absolute correctness in isolation, whereas real-world value often lies in the "complementary relationship between human and AI approaches" rather than autonomous problem-solving.
  - What evidence would resolve it: User studies measuring the efficiency and accuracy of human-AI pairs (novices and experts) solving FMWC challenges compared to humans or AI working alone.

## Limitations
- The JSON transformation process may introduce fidelity loss by eliminating spreadsheet-specific spatial features that contribute to task difficulty
- The scoring methodology remains underspecified, particularly regarding partial credit allocation and numerical tolerance thresholds
- The 60-second timeout may artificially truncate complex reasoning chains, conflating architectural limitations with evaluation constraints

## Confidence
- **High confidence**: GPT-4o-mini's superior performance on structured financial tasks (>20% accuracy advantage), the systematic degradation in game simulation performance, and the general pattern of numerical reasoning limitations are well-supported by the results section and corroborated by similar findings in the broader literature (ACADREASON, FLAWS benchmarks).
- **Medium confidence**: The mechanism linking JSON representation to preserved cognitive load relies on reasonable but untested assumptions about task transfer across formats. The temperature calibration claim (0.2 optimal balance) is based on practical experimentation but lacks systematic sensitivity analysis.
- **Low confidence**: The specific architectural explanation for state-tracking failures - that they reflect fundamental working memory constraints rather than training data or prompt quality issues - remains speculative without ablation studies comparing different model architectures or iterative refinement protocols.

## Next Checks
1. Test the JSON representation fidelity by having human FMWC competitors solve both original Excel challenges and their JSON translations, measuring correlation in completion rates and solution accuracy.
2. Compare performance across different model families (transformers vs. state-tracking enhanced architectures) on the same game simulation challenges to determine whether state-tracking failures are universal or architecture-specific.
3. Evaluate whether integrating external computational verification (code execution, calculator plugins) eliminates the numerical propagation effect, distinguishing between architectural limitations and tooling gaps.