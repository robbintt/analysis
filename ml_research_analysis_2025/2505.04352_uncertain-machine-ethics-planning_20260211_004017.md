---
ver: rpa2
title: Uncertain Machine Ethics Planning
arxiv_id: '2505.04352'
source_url: https://arxiv.org/abs/2505.04352
tags:
- moral
- worth
- policy
- state
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses ethical decision-making under uncertainty
  by developing a planning framework that handles multiple moral theories and non-moral
  goals. The core method introduces Multi-Moral Markov Decision Processes (MMMDPs)
  and Multi-Moral Stochastic Shortest Path Problems (MMSSPs) that incorporate lexicographic
  preferences between moral theories.
---

# Uncertain Machine Ethics Planning

## Quick Facts
- arXiv ID: 2505.04352
- Source URL: https://arxiv.org/abs/2505.04352
- Reference count: 40
- This paper develops a planning framework for ethical decision-making under uncertainty using Multi-Moral Markov Decision Processes (MMMDPs) and Multi-Moral Stochastic Shortest Path Problems (MMSSPs) that incorporate lexicographic preferences between moral theories.

## Executive Summary
This paper addresses ethical decision-making under uncertainty by developing a planning framework that handles multiple moral theories and non-moral goals. The core method introduces Multi-Moral Markov Decision Processes (MMMDPs) and Multi-Moral Stochastic Shortest Path Problems (MMSSPs) that incorporate lexicographic preferences between moral theories. A heuristic planning algorithm based on Multi-Objective AO* is developed, utilizing Machine Ethics Hypothetical Retrospection (MEHR) for ethical reasoning. The algorithm explores the state space to find Pareto undominated policies, then applies MEHR argumentation to select the most ethically preferable policy. Experiments on a "Lost Insulin" case study show the system produces ethically sensitive decisions sensitive to moral theory configurations, though with exponential complexity in the number of histories, making it suitable for small-scale problems requiring careful ethical scrutiny.

## Method Summary
The method extends finite-horizon MDPs with multiple moral considerations by introducing MMMDPs and MMSSPs. The planner uses a heuristic variant of Multi-Objective AO* that maintains worth-vector sets per state-time, performing post-order DFS backups with Pareto pruning and budget-based pruning for MMSSPs. Moral considerations are encoded as modules implementing worth types, judgements, aggregation, preferences, and consistency relations. After finding Pareto undominated policies, the system extracts all possible histories and applies MEHR argumentation to compute non-acceptability measures. The final policy selection minimizes non-acceptability across attacked arguments, with ties broken by expected cost for MMSSPs.

## Key Results
- The "Lost Insulin" case study successfully demonstrates ethical sensitivity to moral theory configurations
- MEHR argumentation produces non-acceptability measures that guide policy selection (e.g., policy C0,H0 has N=0.0015 vs C1,H0 with N=0.5 in one configuration)
- The framework handles both MMMDPs and MMSSPs, with budget constraints successfully limiting policy space
- Policy selection is sensitive to lexicographic ordering of moral theories and their relative importance
- The system correctly identifies non-stationary policies as necessary when positive moral loops exist within budget constraints

## Why This Works (Mechanism)
The approach works by combining multi-objective planning with ethical argumentation. The MMMDP/MMSSP framework allows encoding multiple moral theories as distinct objectives with lexicographic preferences, ensuring strict prioritization of moral considerations. The heuristic planner explores the state space to find Pareto optimal policies without scalarization, preserving the distinct moral dimensions. MEHR argumentation then evaluates each policy by examining all possible histories through the lens of each moral theory, computing attack relations and non-acceptability measures. This creates a two-stage filtering process: first eliminating dominated policies through multi-objective optimization, then selecting the most ethically preferable among the remaining candidates based on retrospective argumentation.

## Foundational Learning
- **Concept: Finite-horizon Markov Decision Processes (MDPs)** - Why needed: The MMMDP extends finite-horizon MDPs with multiple moral considerations; understanding states, actions, transitions, and Bellman backups is prerequisite to following the algorithm. Quick check: Can you write the Bellman optimality equation for a finite-horizon MDP with horizon H?
- **Concept: Multi-Objective MDPs and Pareto dominance** - Why needed: The planner uses a multi-objective formulation with worth-vectors and Pareto pruning; knowing why scalarization is avoided is essential. Quick check: When does a vector v dominate vector u in a minimization context with three objectives?
- **Concept: Argumentation frameworks and attack relations** - Why needed: MEHR is built on value-based argumentation; understanding arguments, attacks, and conflict resolution is required to interpret non-acceptability. Quick check: In an abstract argumentation framework, what does it mean for an argument to be "accepted" under preferred semantics?

## Architecture Onboarding
- **Component map**: Domain specification (S, A, P, M, C, L) → Moral consideration modules → Planner core (Multi-Objective AO* variant) → History extractor → MEHR argumentation engine → Policy selector
- **Critical path**: 1) Encode ethical planning problem as MMMDP/MMSSP, 2) Implement moral consideration modules, 3) Run heuristic planner to convergence, 4) Extract Pareto-undominated policies and history distributions, 5) Run MEHR argumentation to compute N(π), 6) Select optimal moral policy
- **Design tradeoffs**: Expressiveness vs. tractability (rich moral worth types increase fidelity but cause exponential blowup), Lexicographic vs. scalarized preferences (prioritize theories strictly but may ignore tradeoffs), Stationary vs. non-stationary policies (non-stationary allows handling moral loops within budget but increases complexity), Offline vs. online planning (current algorithm is offline)
- **Failure signatures**: Non-termination or slow convergence (likely due to weak heuristic or many undominated policies), Unexpected policy selection (verify attack relations Ψ and critical questions), Budget violations (ensure costs are strictly positive), Empty policy set (may indicate no proper policy under budget)
- **First 3 experiments**: 1) Reproduce "Lost Insulin" MMMDP case with five configurations and verify non-acceptability values, 2) Convert to MMSSP by adding costs and budget to confirm budget-feasible policies, 3) Introduce Virtue Ethics moral consideration and verify integration without algorithmic changes

## Open Questions the Paper Calls Out
- **Open Question 1**: Can n-step lookahead or Monte Carlo history sampling effectively approximate MEHR argumentation while maintaining ethical fidelity? Current algorithm evaluates all histories exhaustively, causing exponential time and space complexity. Evidence: Empirical comparison showing Monte Carlo sampling achieves comparable policy selections with significant speedup.
- **Open Question 2**: How can moral theories that preserve full state-probability information be represented tractably? One theory could back-propagate all state-probability information instead of reducing to a single expected worth, but this represents exponential blowup. Evidence: A modified aggregation function that maintains partial distributional information without exponential state space growth.
- **Open Question 3**: Can positive moral loops be eliminated to enable stationary policies without sacrificing ethical constraints? Current formalism requires non-stationary policies to allow finite cycles within budget. Evidence: A loop-detection or cycle-elimination mechanism that preserves budget-optimal ethical behavior while allowing stationary policies.

## Limitations
- The paper delegates critical implementation details (Pareto pruning algorithm and full MEHR argumentation) to external references without providing specifications
- Exponential growth in history enumeration severely limits scalability, making the approach practical only for small problems requiring careful scrutiny
- The current algorithm is offline and cannot handle dynamic environments or online decision-making scenarios

## Confidence
- **High Confidence**: The MMMDP/MMSSP formalism and general architecture combining multi-objective planning with MEHR argumentation are well-specified and internally consistent
- **Medium Confidence**: The heuristic planning algorithm structure and lexicographic preference handling are clearly described, though exact implementation details for pruning and policy extraction are missing
- **Low Confidence**: The MEHR argumentation specifics and full behavior of the combined system under various moral configurations cannot be verified without the external references

## Next Checks
1. Implement the full Lost Insulin case study with all five moral configurations and verify that computed non-acceptability values and selected policies match reported results in Section 8
2. Extend implementation to MMSSP by adding cost functions and budgets, then confirm that planner produces budget-feasible policies and that cost incorporation changes policy selection as claimed
3. Add a third moral consideration (e.g., Virtue Ethics with ordered categorical worth) and test whether planner correctly integrates it without algorithmic changes while observing impact on convergence behavior and policy outcomes