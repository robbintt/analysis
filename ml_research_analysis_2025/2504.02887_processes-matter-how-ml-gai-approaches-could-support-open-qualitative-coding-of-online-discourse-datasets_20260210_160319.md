---
ver: rpa2
title: 'Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding
  of Online Discourse Datasets'
arxiv_id: '2504.02887'
source_url: https://arxiv.org/abs/2504.02887
tags:
- codes
- coding
- human
- open
- coders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Open coding is a key inductive step in qualitative research that
  discovers and constructs concepts from human datasets. While some studies explore
  machine learning (ML)/Generative AI (GAI)'s potential for open coding, few evaluation
  studies exist.
---

# Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets

## Quick Facts
- arXiv ID: 2504.02887
- Source URL: https://arxiv.org/abs/2504.02887
- Reference count: 3
- One-line primary result: Item-level ML/GAI approaches effectively identify content-based codes in open coding, while humans excel at interpreting conversational dynamics.

## Executive Summary
This study systematically evaluates five recently published ML/GAI approaches against human coders for open qualitative coding of online discourse datasets. Using a dataset of 127 chat messages from a mobile learning software platform, the research reveals that while line-by-line AI approaches effectively identify content-based codes, humans uniquely excel at interpreting conversational dynamics. The findings suggest that rather than replacing humans, ML/GAI should be integrated according to researchers' analytical processes, functioning as parallel co-coders that complement human strengths.

## Method Summary
The study compares five ML/GAI approaches (BERTopic+LLM, Chunk-Level LLM, Chunk-Level Structured, Item-Level, and Item-Level with Verb Phrases) against four human coders using GPT-4o-0513 for all AI approaches. The dataset consists of 127 timestamped chat messages from a mobile learning platform's first two months. Messages were chunked via signal processing on timestamp intervals, then coded without a predefined codebook. A code merging algorithm using semantic embeddings and hierarchical clustering combined results, with human validation determining coverage and contribution potential (Little/Minor/Substantial Gain).

## Key Results
- Item-level approaches produced 240-282 codes versus 47-60 for chunk-level approaches, demonstrating finer granularity
- Verb-phrase constrained item-level approach yielded 10 substantial-gain codes versus 2 for plain item-level
- Machine coders contributed 11/13 substantial-gain codes from content; humans contributed 6/7 from conversational dynamics
- Cohen's Kappa for inter-coder reliability was 0.68 on 81 merged codes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Item-level LLM coding produces finer-grained open codes than chunk-level approaches because the constraint forces granular attention
- Mechanism: Constraining the coding unit to single messages prevents LLMs from defaulting to high-level thematic summarization, yielding more codes per data volume
- Core assumption: Granularity effect is causal, not merely correlational
- Evidence anchors: Item-level approaches produced 240-282 codes vs. 47-60 for chunk-level; Table 3 demonstrates finer-grained codes
- Break condition: If chunk-level approaches with explicit "generate N codes per message" instructions matched item-level granularity

### Mechanism 2
- Claim: Instructing LLMs to use verb phrases for code labels improves alignment with grounded theory's expectation of action/experience-focused codes
- Mechanism: Syntactic constraints bias semantic interpretation toward agentive, process-oriented readings rather than static topic labels
- Core assumption: Improvement is specifically due to verb-phrase constraint, not longer instructions
- Evidence anchors: Verb-phrase approach contributed 10 substantial-gain codes vs. 2 for plain item-level
- Break condition: If non-verb syntactic constraints produced equivalent improvements

### Mechanism 3
- Claim: Humans and LLMs exhibit complementary strengths because they access different signal sources: LLMs leverage message-internal semantics; humans infer conversational dynamics from sequential context and social norms
- Mechanism: LLMs lack robust mechanisms for tracking multi-turn discourse functions that humans naturally infer through pragmatic reasoning
- Core assumption: Performance gap reflects fundamental architectural difference, not just current model limitations
- Evidence anchors: Machine coders' substantial contributions were 11/13 from content; humans' were 6/7 from conversational dynamics; even o1-preview failed to produce conversational-dynamics gains
- Break condition: If models with conversation-graph structures matched human performance on conversational-dynamics codes

## Foundational Learning

- Concept: Open coding vs. thematic coding distinction
  - Why needed here: Paper's central claim hinges on item-level approaches better matching grounded theory's open coding expectations (exhaustive, fine-grained) versus chunk-level approaches that produce themes
  - Quick check question: Can you explain why "feature prioritization" is a theme rather than an open code, and why "manage user expectations" better fits the latter?

- Concept: Groundedness vs. exhaustiveness in qualitative evaluation
  - Why needed here: Evaluation framework operationalizes contribution potential through groundedness and analytical value, not ICR-style agreement metrics
  - Quick check question: Why would high inter-coder reliability between human and machine coders be insufficient for evaluating open coding quality?

- Concept: Conversational dynamics as a signal class
  - Why needed here: Paper identifies this as humans' unique strength; understanding what counts as conversational dynamics versus content is essential for designing complementary workflows
  - Quick check question: For the message "Don't aim for completeness, it should be categorized and refined one by one," what makes "understanding designer's situation" a conversational-dynamics code rather than a content code?

## Architecture Onboarding

- Component map: Dataset preparation (chunking) -> Five coding approaches -> Code merging pipeline (embedding-based hierarchical clustering) -> Human-AI validation interface -> Contribution taxonomy (Groundedness check → Coverage analysis → Gain classification → Source coding)

- Critical path: 1. Dataset preparation (chunking) → 2. Parallel coding (humans + 5 ML approaches) → 3. First-pass validation (groundedness/broadness) → 4. Focus on item-level approaches → 5. Algorithmic merging → 6. Human coverage decisions → 7. Contribution analysis → 8. Source attribution

- Design tradeoffs:
  - Item-level vs. chunk-level: Granularity vs. efficiency; item-level produces 5-6× more codes but requires more downstream processing
  - Verb-phrase constraint: Higher substantial-gain codes (10 vs. 2) but may over-emphasize action framing at expense of descriptive codes
  - Algorithmic merging threshold: Strict definitions yield more "uniquely covered" codes but increase manual validation burden

- Failure signatures:
  - Overly broad codes: "user engagement" applied to most messages indicates insufficient granularity constraint
  - Ungrounded codes: Topic modeling on oversized clusters produces labels with no data connection
  - Conversational-dynamics blindspot: Models consistently miss codes like "topic change without response" even with explicit prompts

- First 3 experiments:
  1. Ablation on verb-phrase constraint: Run item-level coding with (a) verb phrases, (b) noun phrases, (c) no syntactic constraint
  2. Conversation-graph augmentation: Provide LLMs with explicit response-timestamp adjacency information
  3. Temperature/model robustness check: Replicate across multiple models and temperatures to distinguish stable architectural gaps from model-specific noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-stage or layered prompting strategies enable LLMs to identify the "intentions and consequences" of actions in discourse, which current single-pass approaches miss?
- Basis in paper: Authors suggest "better approach may benefit from more sophisticated coding processes into layered prompts" to examine intentions, but did not test this hypothesis
- Why unresolved: Study only tested single-pass approaches; proposed layered solution remains speculative
- What evidence would resolve it: Experiment comparing single-prompt coding against multi-stage prompt chain requiring models to first identify actions then infer intent in second pass

### Open Question 2
- Question: Can specific prompt engineering or model architectures be developed to allow LLMs to identify codes grounded in conversational dynamics as effectively as humans?
- Basis in paper: Authors found machine coders "relatively weaker when identifying novel codes grounded in conversation dynamics," and even advanced models failed to improve results
- Why unresolved: Authors tested multiple models and prompts without success, indicating fundamental gap in how current LLMs process discourse structure versus content
- What evidence would resolve it: Development and testing of prompt that explicitly encodes conversation trees or turn-taking structures, evaluated against human-coded conversational dynamics

### Open Question 3
- Question: To what extent do findings regarding the "Item-Level with Verb Phrases" approach generalize to qualitative datasets beyond online chat messages?
- Basis in paper: Authors explicitly state "We only examined one dataset, one research question... There is no guarantee that our findings will automatically generalize to other datasets"
- Why unresolved: Study was limited to specific dataset of 127 online chat messages regarding physics software
- What evidence would resolve it: Replication study applying same five ML/GAI approaches to diverse qualitative datasets (e.g., semi-structured interviews, focus groups)

## Limitations
- Results based on single dataset of 127 chat messages from one educational software context, limiting generalizability
- Exact prompt templates for all five ML/GAI approaches were not provided, creating reproduction challenges
- Evaluation framework prioritizes groundedness and analytical value over inter-coder reliability, which may not translate to all evaluation contexts

## Confidence
- High confidence: The complementary strength finding (AI excels at content codes, humans at conversational dynamics) is well-supported by multiple evidence sources
- Medium confidence: Mechanisms explaining granularity effects and verb-phrase benefits are plausible but lack direct experimental validation
- Low confidence: Claim that conversational dynamics represents fundamental architectural gap rather than model limitation is weakly supported

## Next Checks
1. Ablation study on syntactic constraints: Test whether verb-phrase benefits persist when replacing with noun-phrase or process-focused constraints
2. Conversation-graph augmentation: Provide models with explicit response-timestamp adjacency information to test whether conversational-dynamics performance improves
3. Cross-dataset robustness: Apply methodology to different discourse type (e.g., clinical interviews or social media threads) to assess generalizability beyond educational chat contexts