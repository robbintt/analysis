---
ver: rpa2
title: 'Prism: Mining Task-aware Domains in Non-i.i.d. IMU Data for Flexible User
  Perception'
arxiv_id: '2501.01598'
source_url: https://arxiv.org/abs/2501.01598
tags:
- prism
- data
- dataset
- domains
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of flexible user perception (FUP)
  on mobile devices, where machine learning models need to perform well on non-i.i.d.
  inertial measurement unit (IMU) data collected from diverse sources.
---

# Prism: Mining Task-aware Domains in Non-i.i.d. IMU Data for Flexible User Perception

## Quick Facts
- arXiv ID: 2501.01598
- Source URL: https://arxiv.org/abs/2501.01598
- Reference count: 40
- Primary result: Achieves up to 16.79% F1 score improvement on non-i.i.d. IMU tasks with <60ms latency on mobile devices

## Executive Summary
This paper addresses the challenge of flexible user perception (FUP) on mobile devices using inertial measurement unit (IMU) data. The core contribution is Prism, a method that automatically discovers task-aware domains within non-i.i.d. IMU datasets and trains specialized models for each identified domain. By leveraging an expectation-maximization (EM) algorithm to identify latent domains and selecting the optimal model for each test sample, Prism achieves state-of-the-art performance while maintaining low inference latency suitable for mobile deployment.

## Method Summary
Prism addresses flexible user perception by automatically discovering task-aware domains embedded in non-i.i.d. IMU data using an expectation-maximization (EM) algorithm. The method first estimates latent domains with respect to specific perception tasks, then trains domain-specific models for each identified domain. During inference, Prism selects the best-fit model for each test sample by comparing it to all identified domains in feature space. This approach enables state-of-the-art FUP performance with low latency, making it suitable for deployment on typical mobile devices including low-end smartphones.

## Key Results
- Achieves up to 16.79% improvement in F1 scores for non-i.i.d. tasks compared to existing methods
- Maintains inference latency under 60ms, enabling real-time deployment on mobile devices
- Demonstrates effectiveness across multiple public IMU datasets including UCI, HHAR, Motion, and SHL for both activity recognition and user authentication tasks

## Why This Works (Mechanism)
Prism works by addressing the fundamental challenge that IMU data collected from diverse sources exhibits non-i.i.d. characteristics. Traditional machine learning approaches struggle with this heterogeneity, leading to poor generalization across different users, devices, or usage contexts. By automatically discovering task-aware domains, Prism can create specialized models that capture the unique patterns within each domain while maintaining flexibility to handle diverse inputs. The EM-based domain discovery algorithm effectively partitions the data into coherent groups that share similar characteristics, allowing for more accurate and robust predictions when the appropriate domain-specific model is selected.

## Foundational Learning

1. **Non-i.i.d. Data Distribution** - Why needed: IMU data from different users/devices exhibits varying statistical properties that violate independent and identically distributed assumptions. Quick check: Verify that training data shows clear distributional differences across collection sources.

2. **Expectation-Maximization Algorithm** - Why needed: EM provides a principled way to discover latent domains without explicit domain labels. Quick check: Monitor log-likelihood convergence during EM iterations to ensure proper model fitting.

3. **Task-aware Domain Discovery** - Why needed: Different perception tasks may benefit from different domain partitions. Quick check: Compare domain discovery results when optimizing for different target tasks.

4. **Domain-specific Model Selection** - Why needed: Different domains may require different model architectures or parameters for optimal performance. Quick check: Evaluate model selection accuracy by comparing against oracle selection.

## Architecture Onboarding

**Component Map:** IMU Data -> Feature Extraction -> EM Domain Discovery -> Domain-specific Models -> Model Selection -> Prediction

**Critical Path:** The most time-critical path is Feature Extraction -> Model Selection -> Prediction, as this must complete within the 60ms latency budget for real-time deployment.

**Design Tradeoffs:** The method trades model complexity (multiple domain-specific models) for improved accuracy and flexibility. This increases storage requirements but enables better handling of non-i.i.d. data patterns.

**Failure Signatures:** Performance degradation occurs when: 1) EM algorithm fails to discover meaningful domains, 2) Test samples fall between discovered domains, 3) Domain-specific models overfit to their respective domains.

**First 3 Experiments:**
1. Test domain discovery on a synthetic dataset with known ground truth domains to validate EM algorithm performance
2. Evaluate model selection accuracy on a held-out test set with known domain labels
3. Measure inference latency on target mobile device hardware to verify deployment feasibility

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content.

## Limitations

- Requires labeled training data for each domain, which may be impractical in real-world deployment scenarios
- EM algorithm convergence properties and sensitivity to initialization parameters are not thoroughly analyzed
- Performance on extremely small or highly imbalanced datasets is not explicitly evaluated

## Confidence

- High confidence in mobile deployment and latency results (directly measured and reproducible)
- Medium confidence in F1 score improvements (dependent on dataset quality and preprocessing choices)
- Low confidence in claims about robustness to unknown domains (evaluation focuses on known domain partitions)

## Next Checks

1. Test Prism's performance when trained on datasets with significant label noise or missing values to evaluate robustness
2. Evaluate the method's ability to handle completely unseen device types or wearing configurations not present in training data
3. Conduct ablation studies to isolate the contribution of the EM-based domain discovery versus the task-aware model selection components