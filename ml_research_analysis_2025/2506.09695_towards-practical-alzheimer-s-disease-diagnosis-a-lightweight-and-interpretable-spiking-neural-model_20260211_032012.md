---
ver: rpa2
title: 'Towards Practical Alzheimer''s Disease Diagnosis: A Lightweight and Interpretable
  Spiking Neural Model'
arxiv_id: '2506.09695'
source_url: https://arxiv.org/abs/2506.09695
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000024
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early Alzheimer's disease
  (AD) diagnosis, particularly at the mild cognitive impairment (MCI) stage, by proposing
  a lightweight and interpretable spiking neural network called FasterSNN. The model
  integrates Leaky Integrate-and-Fire (LIF) neurons, region-adaptive 3D convolution,
  and a multi-scale spiking attention mechanism to efficiently process MRI data while
  maintaining high diagnostic accuracy.
---

# Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model

## Quick Facts
- arXiv ID: 2506.09695
- Source URL: https://arxiv.org/abs/2506.09695
- Authors: Changwei Wu; Yifei Chen; Yuxin Du; Jinying Zong; Jie Dong; Mingxuan Liu; Feiwei Qin; Yong Peng; Jin Fan; Changmiao Wang
- Reference count: 40
- Primary result: Achieves 0.8325 accuracy and 0.9294 AUC on ADNI/AIBL datasets with only 1.74 J energy consumption

## Executive Summary
This study addresses the challenge of early Alzheimer's disease diagnosis, particularly at the mild cognitive impairment (MCI) stage, by proposing a lightweight and interpretable spiking neural network called FasterSNN. The model integrates Leaky Integrate-and-Fire (LIF) neurons, region-adaptive 3D convolution, and a multi-scale spiking attention mechanism to efficiently process MRI data while maintaining high diagnostic accuracy. Experimental results on ADNI and AIBL datasets show that FasterSNN achieves an accuracy of 0.8325 and AUC of 0.9294 with only two inference time steps and an energy consumption of 1.74 J, outperforming existing deep learning and SNN models. The approach balances computational efficiency, accuracy, and interpretability, offering a promising solution for scalable, cost-effective AD screening in resource-limited clinical settings.

## Method Summary
FasterSNN is a 3-class classification model (AD/MCI/CN) that processes 3D MRI brain scans using a spiking neural network architecture. The method employs LIF neurons with decay factor 0.9 and threshold 1.0, operating over T=2 time steps. The core architecture consists of 4 FasterSNN blocks with channels 64→128→256→512, each containing region-adaptive convolution (standard 3×3×3 in core region, depthwise separable in edges), batch normalization, GELU activation, and a Spiking Weighted Attention (SWA) module. A Multi-Scale Fusion (MSF) module aggregates features from all blocks using learnable weights. The model is trained with Adam optimizer (lr=1e-3, weight_decay=1e-3), batch_size=16, mixed-precision training, STBP with surrogate gradient, and ReduceLROnPlateau. Data is preprocessed by resampling to 64×64×64, Z-score normalization, and generating temporal sequences with Gaussian noise.

## Key Results
- Achieves 0.8325 accuracy and 0.9294 AUC on ADNI/AIBL datasets
- Energy consumption of only 1.74 J with 2 inference time steps
- Outperforms existing deep learning and SNN models in accuracy-energy trade-off
- Ablation studies show MSF module contributes 6.89% accuracy improvement

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Sparse-Adaptive Processing
Integrating LIF neurons with region-adaptive convolution reduces energy consumption while preserving feature extraction capability. LIF neurons introduce sparsity by firing only when membrane potential exceeds threshold, while the FasterSNN Block applies full 3×3×3 convolution to the core region and cheaper depthwise separable convolution to edge regions, with residual connections for stability.

### Mechanism 2: Multi-Scale Spiking Attention (SWA)
The SWA mechanism enhances detection of subtle pathological cues by applying attention weighted by spiking dynamics. It uses parallel channel and spatial attention branches with LIF neurons discretizing the attention mapping process, enforcing sparsity to filter noise and focus on faint pathological features.

### Mechanism 3: Hierarchical Multi-Scale Fusion (MSF)
A pyramid fusion strategy captures AD pathology spanning macroscopic and microscopic changes. Four-level pyramid extracts features at increasing depths (64 to 512 channels), with learnable weights dynamically weighting each scale's contribution before temporal averaging, validated by 6.89% accuracy drop when removed.

## Foundational Learning

- **Leaky Integrate-and-Fire (LIF) Neuron Dynamics**: Core compute unit replacing standard ReLU; membrane potential decays over time ($V_t = \lambda V_{t-1} + \dots$) and triggers binary spike ($S_t$). Quick check: If input intensity is constant but low (below threshold), how does decay factor $\lambda$ influence whether a spike eventually occurs?

- **Surrogate Gradient Training**: Spike generation function is non-differentiable; rectangular approximation propagates gradients back through network (STBP algorithm). Quick check: Why can't you use standard backpropagation directly through spike thresholding operation $S_t = \mathbf{1}_{V_t \ge V_{th}}$?

- **3D Region-Adaptive Convolution**: Efficiency relies on not treating all voxels equally; masking mechanism ($M_{i,j,k}$) separates tensor into core and edge regions. Quick check: In a 64³ input, which region receives standard 3×3×3 convolution and which receives depthwise separable convolution?

## Architecture Onboarding

- **Component map**: Input (3D MRI 64³) -> Stem (3D Conv -> LIF) -> 4×FasterSNN Blocks -> MSF Module -> Temporal Averaging -> Linear Classifier

- **Critical path**: 
  1. Data Prep: Verify 64³ resampling and Z-score normalization pipeline
  2. Block Logic: Ensure spatial masking correctly splits feature map for dual-convolution strategy
  3. Temporal Flow: Track LIF neuron state across 2 time steps; final prediction averages over steps

- **Design tradeoffs**: 
  - Accuracy vs. Energy: Only 2 time steps and 64³ resolution provide extreme efficiency (1.74 J) but sacrifice granularity of higher resolutions
  - Stability vs. Sparsity: Ablation shows removing LIF improves training time slightly but hurts accuracy (~3%) and energy efficiency

- **Failure signatures**: 
  - Mode Collapse/Noise Sensitivity: Removing MSF causes significant accuracy drop (~7%), indicating failure to integrate coarse/fine features
  - Overfitting on Small Data: 43.11 M parameters may overfit without proper augmentation/regularization
  - Interpretability Disconnect: Attention maps highlighting non-brain regions suggest focus on artifacts

- **First 3 experiments**:
  1. Sanity Check (Ablation): Run model with/without MSF module on validation set; confirm ~6-7% accuracy gap
  2. Resolution Sensitivity: Test 64³ vs 96³; quantify accuracy gain vs ~4x energy cost increase
  3. Spike Visualization: Visualize spike raster plots in first vs last block to verify sparsity increases with depth

## Open Questions the Paper Calls Out

1. **Multimodal Integration**: How can FasterSNN extend to integrate multimodal neuroimaging data (PET, DTI) without compromising low energy consumption? The current single-modality MRI reliance limits capacity to capture complex pathological mechanisms.

2. **Quantitative Interpretability**: To what extent do spiking attention maps quantitatively align with established anatomical priors and expert-segmented ROIs? Current interpretability remains primarily qualitative lacking quantitative alignment mechanism.

3. **Harmonization for Generalization**: Does advanced harmonization (ComBat correction, MNI registration) significantly improve generalization across heterogeneous clinical sites? Current reliance on basic Z-score normalization may fail to remove scanner-specific biases.

4. **Dynamic Temporal Modeling**: Can dynamic temporal modeling strategies improve classification for complex/ambiguous MCI cases versus current fixed time-step approach? Fixed T=2 setting provides optimal trade-off for average cases but may lack temporal depth for subtle pathology.

## Limitations

- Energy efficiency claim relies on specific 2 time steps and 64³ resolution; scaling to higher resolutions or longer temporal windows could significantly increase energy costs
- Ablation study validates MSF importance but didn't compare learnable fusion weights against alternative fusion strategies
- Interpretability claims based on attention maps and visualizations not validated through neurologist review or comparison to established radiological markers

## Confidence

- **High Confidence**: Core technical claims about FasterSNN architecture (LIF neurons, region-adaptive convolution, SWA module) are well-specified with direct ablation evidence
- **Medium Confidence**: Performance metrics reported with cross-validation standard deviations, but comparison to other SNN methods is limited and energy efficiency based on specific inference setup
- **Low Confidence**: Interpretability and clinical utility assertions not validated through expert review or established radiological marker comparison

## Next Checks

1. **Clinical Relevance Audit**: Have neurologist review attention maps and spike visualizations to confirm they highlight anatomically relevant AD pathology versus artifacts

2. **Energy Scalability Test**: Measure energy consumption at higher resolutions (96³ or 128³) and longer time steps (T=5, T=10) to quantify accuracy-energy trade-off

3. **Fusion Strategy Comparison**: Implement and compare MSF module against simpler fusion strategies (fixed averaging, max pooling) to isolate contribution of learnable weights versus multi-scale architecture itself