---
ver: rpa2
title: Detecting OOD Samples via Optimal Transport Scoring Function
arxiv_id: '2502.16115'
source_url: https://arxiv.org/abs/2502.16115
tags:
- otod
- detection
- auroc
- methods
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTOD, a novel post hoc method for out-of-distribution
  (OOD) detection that leverages optimal transport theory, specifically Wasserstein-1
  distance, to calculate OOD scores without requiring in-distribution data during
  testing. The method uses features, logits, and softmax probability from a pre-trained
  neural network, combining them with temperature scaling to enhance performance.
---

# Detecting OOD Samples via Optimal Transport Scoring Function

## Quick Facts
- **arXiv ID:** 2502.16115
- **Source URL:** https://arxiv.org/abs/2502.16115
- **Authors:** Heng Gao; Zhuolin He; Jian Pu
- **Reference count:** 26
- **Primary result:** Outperforms state-of-the-art OOD detection methods with 7.19-12.51% improvements in mean FPR@95 on CIFAR benchmarks

## Executive Summary
This paper proposes OTOD, a post hoc method for out-of-distribution (OOD) detection that leverages optimal transport theory without requiring in-distribution data during testing. The method computes Wasserstein-1 distances between normalized feature, logit, and probability representations of test samples and fixed reference vectors (mean and zero). By combining multi-scale information with temperature scaling, OTOD achieves state-of-the-art performance on CIFAR-10 and CIFAR-100 benchmarks, improving upon existing methods by 7.19-12.51% in FPR@95.

## Method Summary
OTOD is a post-hoc OOD detection method that operates on pre-trained neural networks without requiring additional training or in-distribution data at test time. It extracts three representations from the model: penultimate layer features, logits, and softmax probabilities. Each representation is L2-normalized, and Wasserstein-1 distance is computed between each normalized vector and two fixed reference points (mean and zero vectors). The final OOD score combines these three Wasserstein-1 scores with equal weights and applies temperature scaling to the logits before softmax normalization. The method demonstrates improved detection performance through ablation studies confirming the effectiveness of multi-scale information fusion.

## Key Results
- Achieves 7.19-12.51% improvements in mean FPR@95 compared to GEN on CIFAR benchmarks
- Ablation study shows AUROC improves from 74.54% (features only) to 80.02% (all three combined), a 5.48% gain
- Temperature sensitivity analysis reveals optimal settings differ by dataset: T=3 for CIFAR-100, T=10 for CIFAR-10
- Outperforms state-of-the-art methods across multiple OOD datasets including MNIST, SVHN, Texture, and Places365

## Why This Works (Mechanism)

### Mechanism 1: Wasserstein-1 Distance Scoring via Fixed Reference Points
The Wasserstein-1 distance enables OOD scoring by measuring optimal transport cost between a sample's representation and two fixed reference vectors (mean and zero). The score $\hat{S}_{W_1}(\hat{f}) = -\min[W_1(\hat{f}, u), W_1(\hat{f}, o)]$ leverages a dual-reference ensemble effect where lower (more negative) scores indicate higher OOD likelihood. This works under the assumption that class-conditional feature distributions follow multivariate Gaussian distributions, with OOD samples exhibiting larger transport costs to reference points compared to ID samples. The theoretical mean discrepancy upper bound is proportional to $||\mu_i - \mu_{ood}||_{TV}$, making pronounced distribution shifts necessary for effectiveness.

### Mechanism 2: Multi-Scale Information Fusion
OTOD integrates features, logits, and softmax probability spaces to provide complementary OOD signals that improve detection beyond any single modality. The final score $S_{W_1}(\hat{f}, \hat{l}, \tilde{p}) = \alpha_1 \hat{S}_{W_1}(\hat{f}) + \alpha_2 \hat{S}_{W_1}(\hat{l}) + \alpha_3 \hat{S}_{W_1}(\tilde{p})$ applies W1 scoring independently to each normalized representation with equal weights. Each representation level captures distinct geometric cues that previous post hoc methods failed to utilize jointly, as evidenced by the 5.48% AUROC improvement when combining all three components.

### Mechanism 3: Temperature-Softened Probability Discrimination
Temperature scaling enlarges the separability between ID and OOD softmax distributions by softening probability outputs. The temperature $T$ divides logits before softmax: $\tilde{p} = \text{normalize}(\text{SoftMax}(l/T))$. Higher $T$ values create smoother distributions that may amplify distributional differences. An optimal temperature exists that maximizes ID-OOD separability, with experiments showing T=3 for CIFAR-100 and T=10 for CIFAR-10. Performance degrades at temperature extremes, indicating the importance of this hyperparameter.

## Foundational Learning

- **Concept: Wasserstein Distance (Optimal Transport)**
  - Why needed here: Core mathematical foundation enabling distance computation between distributions without parametric assumptions
  - Quick check question: Why does the Kantorovich-Rubinstein dual formulation $W_1(\mu, \nu) = \sup_{||\psi||_L \leq 1} \left[\int \psi d\mu - \int \psi d\nu\right]$ allow scoring without estimating full ID distributions?

- **Concept: Post Hoc OOD Detection Paradigm**
  - Why needed here: OTOD's design constraint—model-agnostic, no training modifications—defines its applicability scope
  - Quick check question: What trade-offs exist between post hoc methods (like OTOD) and training-based outlier exposure approaches in terms of deployment flexibility versus detection power?

- **Concept: Temperature Scaling in Neural Networks**
  - Why needed here: Critical hyperparameter affecting softmax sharpness and OOD signal quality
  - Quick check question: As $T \to \infty$, softmax approaches uniform distribution—how would this theoretically affect the Wasserstein-1 score for OOD detection?

## Architecture Onboarding

- **Component map:**
  Pre-trained model -> Feature/Logit/Probability extraction -> L2 normalization -> W1 scoring (mean/zero refs) -> Temperature scaling -> Weighted aggregation -> OOD score

- **Critical path:**
  1. Feature extraction from frozen backbone
  2. L2 normalization of all three representations
  3. Per-representation W1 distance computation
  4. Weighted aggregation

- **Design tradeoffs:**
  - Reference vectors: Mean vector captures central tendency; zero vector provides boundary reference. Paper doesn't ablate this design choice.
  - Equal weights ($\alpha_i = 1/3$): Simple but potentially suboptimal; dataset-specific tuning could improve performance.
  - Temperature selection: Requires validation data; $T=3$ (CIFAR-100) vs $T=10$ (CIFAR-10) suggests sensitivity to dataset complexity.

- **Failure signatures:**
  - High FPR@95 on near-distribution OOD (e.g., CIFAR-100 as OOD for CIFAR-10 shows higher FPR than distant datasets like Texture)
  - Degraded performance when $||\mu_i - \mu_{ood}||_{TV}$ is small (Theorem 1 bound tightens)
  - Architecture mismatch: Optimal hyperparameters differ between ResNet-18 and WideResNet-28

- **First 3 experiments:**
  1. Reproduce baseline comparison on CIFAR-10 with ResNet-18 using provided code, verifying FPR@95 improvements over GEN (claimed 7.19-12.51%).
  2. Ablation study replicating Table III to validate multi-scale fusion contribution—start with features only, then add logits, then probabilities.
  3. Temperature sensitivity analysis on your target architecture/dataset by sweeping $T \in \{1, 3, 5, 10, 20, 50\}$ to identify optimal setting, as Figure 3 shows non-monotonic behavior.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the theoretical mean discrepancy upper bound be extended to the full multi-scale OTOD score, rather than just the feature-based component? The authors state they provide theoretical guarantees "partially... for the feature part of OTOD" but leave the logits and softmax components theoretically ungrounded. Resolution would require a derivation for the full equation $\tilde{S}_{W1}(\hat{f}, \hat{l}, \hat{p})$.

- **Open Question 2:** Is the uniform weighting strategy ($\alpha_1=\alpha_2=\alpha_3=1/3$) optimal across diverse datasets, or does it require tuning for different distribution shifts? The implementation specifies fixed equal weights without justification or ablation study. A sensitivity analysis on the coefficients $\alpha$ demonstrating performance variance across different weighting schemes would resolve this.

- **Open Question 3:** Does the OTOD method scale effectively to high-resolution datasets (e.g., ImageNet) given its reliance on optimal transport calculations on normalized vectors? The paper claims the method is "model agnostic" but limits experimental validation to CIFAR-10 and CIFAR-100 benchmarks. Benchmark results on ImageNet-1k as the in-distribution dataset would verify scalability.

## Limitations

- Dependence on mean discrepancy bound introduces limitations when OOD distributions have small distributional shifts from ID data
- Assumes multivariate Gaussian distributions for feature spaces without empirical validation across diverse datasets
- Equal weighting of feature, logit, and probability components appears arbitrary without justification for why these modalities should contribute equally
- Wasserstein-1 distance computation method is underspecified - exact computation, dual formulation, or approximation unclear

## Confidence

- **High confidence**: Performance improvements over baselines (7.19-12.51% FPR@95 reduction) - supported by direct experimental comparison on CIFAR benchmarks
- **Medium confidence**: Temperature scaling mechanism - theoretical justification exists but corpus validation is limited
- **Low confidence**: Gaussian distribution assumption for feature spaces - stated but not empirically verified

## Next Checks

1. **Distribution validation**: Empirically verify the multivariate Gaussian assumption for penultimate layer features across multiple datasets using statistical tests (e.g., Mardia's test for multivariate normality)

2. **Reference vector ablation**: Systematically test alternative reference vector designs beyond the mean-zero ensemble (e.g., class-specific means, learned reference points)

3. **Near-distribution robustness**: Evaluate OTOD performance when OOD data is semantically similar to ID (e.g., CIFAR-100 as OOD for CIFAR-10) to test the bound's practical implications