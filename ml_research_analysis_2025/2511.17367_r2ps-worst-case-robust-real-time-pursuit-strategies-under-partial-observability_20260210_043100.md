---
ver: rpa2
title: 'R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability'
arxiv_id: '2511.17367'
source_url: https://arxiv.org/abs/2511.17367
tags:
- policy
- evader
- pursuers
- pursuit
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of computing real-time pursuit
  strategies in graph-based pursuit-evasion games under partial observability, where
  traditional methods become computationally expensive. The authors propose a novel
  approach that combines theoretical analysis of dynamic programming (DP) algorithms
  with reinforcement learning (RL) to create robust, generalizable pursuit policies.
---

# R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability

## Quick Facts
- arXiv ID: 2511.17367
- Source URL: https://arxiv.org/abs/2511.17367
- Reference count: 23
- Primary result: RL policies achieve robust zero-shot generalization to unseen real-world graphs, outperforming standard game RL methods like PSRO

## Executive Summary
This paper addresses real-time pursuit strategies in graph-based pursuit-evasion games under partial observability, where traditional dynamic programming (DP) methods become computationally expensive. The authors propose a novel approach combining theoretical analysis of DP algorithms with reinforcement learning (RL) to create robust, generalizable pursuit policies. They prove that DP algorithms remain optimal under asynchronous moves by the evader and extend these strategies to partially observable settings using a belief preservation mechanism. The method integrates this mechanism into the Equilibrium Policy Generalization (EPG) framework, enabling cross-graph RL training against asynchronous-move DP evaders. Experiments show their approach achieves robust zero-shot generalization to unseen real-world graphs, consistently outperforming standard game RL methods like PSRO.

## Method Summary
The approach consists of three main components: (1) DP algorithm precomputation to compute distance tables D and optimal policies for training graphs, (2) belief preservation mechanism to handle partial observability by maintaining probability distributions over possible evader positions, and (3) cross-graph adversarial RL training using SAC with KL guidance from belief-averaged DP policies against asynchronous DP evaders. The GNN policy architecture uses 6-layer self-attention with pointer network output. Training is performed on 150 synthetic + 150 real-world urban graphs (max 500 nodes), with inference complexity O(n²m) that scales to large graphs.

## Key Results
- R2PS achieves 0.95 success rate against DPasync on Times Square vs PSRO's 0.04
- Inference time <0.01s on 1800-node graphs vs 101s for DP recomputation
- DP_belief consistently outperforms DP_pos (Grid Map: 0.78 vs 0.59; Downtown: 0.90 vs 0.73)
- Performance degrades sharply when belief update frequency reduced from every-step to every-2-steps

## Why This Works (Mechanism)

### Mechanism 1: Belief Preservation for Partial Observability
- **Claim:** Belief preservation enables effective pursuit under limited observation by maintaining probability distributions over possible evader positions.
- **Mechanism:** Initialize belief at evader's starting position; update possible positions Pos = Neighbor(Pos_old) minus observable positions; propagate belief using transition model; use belief-weighted distance estimates; collapse belief when evader re-observed.
- **Core assumption:** Evader moves predictably enough that graph-based belief propagation provides useful information.
- **Evidence anchors:** Table 1 shows DPbelief consistently outperforms DPPos; belief update frequency critically affects performance (Table 4).
- **Break condition:** Performance drops sharply when belief update frequency reduced; improves with known evader transition model.

### Mechanism 2: Asynchronous-Move DP Policy Optimality
- **Claim:** DP distance table D remains optimal when evader predicts pursuer actions and moves asynchronously.
- **Mechanism:** Compute D where D(s) = worst-case timesteps to capture; pursuer policy µ* = argmin_neighbor{max{D(np, ne)}}; evader policy ν* = argmax_neighbor{D(np, ne)} using actual pursuer move.
- **Core assumption:** Finite state space and existence of pure-strategy Nash equilibrium.
- **Evidence anchors:** Theorem 2 guarantees both policies optimal under asynchronous play; Corollary 1 states both µ* and ν* are optimal for states with D(s) < ∞.
- **Break condition:** If D(s) = ∞, Theorem 3 guarantees evader never captured.

### Mechanism 3: Cross-Graph Adversarial RL for Zero-Shot Generalization
- **Claim:** Training GNN policy against provably optimal DP evaders across diverse graphs produces robust zero-shot generalization.
- **Mechanism:** Construct training corpus (150 synthetic + 150 real-world); pre-compute DP policies; train with SAC using belief-augmented state and KL guidance from reference policy; GNN with self-attention + pointer network.
- **Core assumption:** Graph structures share learnable pursuit-evasion patterns allowing policy intersection across graphs.
- **Evidence anchors:** Table 2 shows R2PS consistently outperforms PSRO on test graphs; inference scales effectively to large graphs (Table 3).
- **Break condition:** Performance degrades on large-diameter, few-small-cycle graphs; no reference guidance (β=0) leads to slower, less efficient training.

## Foundational Learning

- **Concept: Graph-Based Pursuit-Evasion Games**
  - **Why needed here:** Entire framework operates on graph representations; pursuit success depends on topology.
  - **Quick check question:** Given Grid Map (all minimal cycles length 4) vs. Hollywood Walk of Fame (diameter 31, few small cycles), which should favor pursuit and why?

- **Concept: Minimax and Nash Equilibrium in Zero-Sum Games**
  - **Why needed here:** DP algorithm computes minimax solutions; optimality proofs rely on Nash equilibrium; asynchronous extension preserves equilibrium properties.
  - **Quick check question:** If evader observes pursuer's action before moving, why does µ* = argmin{max{D(np, ne)}} remain optimal for pursuer?

- **Concept: Graph Neural Networks with Attention**
  - **Why needed here:** Policy implemented as GNN with multi-head self-attention for permutation-invariant processing; pointer network handles variable neighbor counts.
  - **Quick check question:** Why use pointer network for action output rather than fixed-size layer? How does masked attention enforce graph structure?

## Architecture Onboarding

- **Component map:** Offline: [Training Graphs] → [DP Precomputation] → [Distance Tables + Policies (µ*, ν*)]; Training: [Graph Sampler] → [Environment with DP Evader ν*] → [State: (sp, Pos, belief)] → [GNN Policy πθ] → [Action] → [Reference µ*] → [KL Guidance] → [SAC Loss] → [Update]; Inference: [Observation] → [Belief Update O(|V|)] → [GNN Forward O(n²m)] → [Action]

- **Critical path:** DP precomputation (offline, minutes for large graphs, one-time cost); Belief maintenance (runtime, every step, O(|V|) update); GNN inference (runtime, every step, O(n²m) forward pass).

- **Design tradeoffs:** Belief representation (full distribution vs position set); reference weight β (higher = more guidance, less exploration); training corpus (synthetic vs real-world); observation range (train at minimum, deploy at any higher range).

- **Failure signatures:** Belief drift (evader exploits non-uniform assumptions); topology mismatch (large-diameter, few-cycle graphs); scale blowup (nodes >500).

- **First 3 experiments:** (1) Implement DP_belief and DP_pos on Grid Map; test vs stationary and DPasync evaders; (2) Train R2PS on synthetic-only; zero-shot evaluate on Downtown and Eiffel Tower; compare vs PSRO; (3) Train at range=2; evaluate at ranges 2-6 on same graphs.

## Open Questions the Paper Calls Out

- **Can adaptive or learned evader policy estimation during pursuit significantly improve success rates compared to the uniform distribution assumption in belief updates?**
  - Basis in paper: The authors state belief preservation "always employs a uniform evader policy ν since we could not access prior information about the true opponent" and show known opponent information improves success rates.
  - Why unresolved: While the paper demonstrates that using known opponent information improves performance, it does not explore methods for learning or adapting the evader policy estimate online during pursuit.
  - What evidence would resolve it: A comparative study showing success rates with learned/adaptive evader models versus uniform assumptions on the same test graphs.

- **How does R2PS scale with larger pursuer teams (m > 2), given the O(n²m) inference complexity and exponential DP complexity?**
  - Basis in paper: The paper only evaluates m=2 pursuers, justified by the theoretical result that "3 pursuers with full observations can always capture the evader in any planar graph," but does not empirically validate scaling to larger teams.
  - Why unresolved: Larger pursuer teams introduce multi-agent coordination challenges not examined in the experiments, and the GNN architecture's parameter-sharing may face limitations with heterogeneous agent roles.
  - What evidence would resolve it: Experiments on the same test graphs with m=3,4,5 pursuers showing success rates and inference times against best-responding evaders.

- **Can the approach be extended to multi-exit pursuit-evasion games (network security games) where the evader has explicit escape goals?**
  - Basis in paper: The conclusion states belief preservation provides an efficient way to handle partial observability for "no-exit PEGs," and the related work section distinguishes these from "multi-exit PEGs... sometimes referred to as network security games."
  - Why unresolved: Multi-exit PEGs have fundamentally different game dynamics with escape zones and different Nash equilibrium structures; the distance table D may not directly apply.
  - What evidence would resolve it: A modified algorithm applying belief preservation to multi-exit scenarios with comparative performance against existing network security game solvers.

## Limitations

- Limited evaluation scope: All experiments use 2 pursuers vs 1 evader; results may not generalize to different pursuer/evader ratios or larger teams.
- Fixed observation range: Training at observation range=2 with test deployment at higher ranges shows improvement, but the method's limits at extremely low observability (range=1) remain untested.
- Graph size distribution: Training graphs capped at 500 nodes; performance on significantly larger graphs (>1000 nodes) is unverified.

## Confidence

- **High confidence:** DP algorithm optimality under asynchronous moves (Theorem 2) and belief preservation mechanism effectiveness (Table 1 comparison of DP_pos vs DP_belief).
- **Medium confidence:** Zero-shot generalization claims (Table 2 cross-graph results) - while consistent, the specific training-test graph combinations could influence results.
- **Medium confidence:** Asynchronous evader modeling assumptions - the proof relies on finite state spaces and pure-strategy Nash equilibrium existence, which may not hold in all graph topologies.

## Next Checks

1. **Belief update frequency sensitivity:** Systematically vary belief update intervals (every step, every 2 steps, every 4 steps) on Grid Map and Big Ben to quantify performance degradation curve.

2. **Scale-up performance:** Evaluate R2PS on graphs with 1000+ nodes (beyond training distribution) to measure inference time scaling and success rate degradation.

3. **Team size generalization:** Test R2PS with 3+ pursuers against 1-2 evaders on the same graph corpus to verify strategy transferability beyond the 2v1 setting.