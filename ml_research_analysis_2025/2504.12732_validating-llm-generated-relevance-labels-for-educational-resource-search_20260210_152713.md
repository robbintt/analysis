---
ver: rpa2
title: Validating LLM-Generated Relevance Labels for Educational Resource Search
arxiv_id: '2504.12732'
source_url: https://arxiv.org/abs/2504.12732
tags:
- relevance
- search
- judgements
- educational
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigated how Large Language Models (LLMs) can effectively
  assess the relevance of educational resources. Using a dataset of 401 human relevance
  judgements from a teacher user study, the authors compared three prompt frameworks
  for guiding LLMs: a generic two-aspect baseline, a comprehensive 12-dimensional
  rubric from educational literature, and a 10-dimension framework directly informed
  by study participants.'
---

# Validating LLM-Generated Relevance Labels for Educational Resource Search

## Quick Facts
- arXiv ID: 2504.12732
- Source URL: https://arxiv.org/abs/2504.12732
- Authors: Ratan J. Sebastian; Anett Hoppe
- Reference count: 16
- Primary result: Domain-specific frameworks significantly outperform baseline, with participant-derived criteria achieving strong agreement with human judgments (Cohen's κ up to 0.639)

## Executive Summary
This study investigates how Large Language Models (LLMs) can assess the relevance of educational resources for professional search tasks. Using 401 human relevance judgments from a teacher user study, the authors compare three prompt frameworks for guiding LLMs: a generic baseline, a comprehensive literature-derived rubric, and a framework directly informed by study participants. Domain-specific frameworks significantly outperform the baseline, with participant-derived criteria achieving strong agreement with human judgments. GPT-3.5 shows the best performance among evaluated LLMs, demonstrating that LLMs can effectively evaluate educational resources when prompted with domain-specific criteria.

## Method Summary
The study uses a dataset of 401 human relevance judgments (353 documents, 19 topics) from a teacher user study. Documents are preprocessed into HEAD (first 5,000 characters) and SKIM (ten 1,000-character segments from first 50,000 characters) representations. Five prompt frameworks are tested: M0 (baseline with trustworthiness and content match), M1 (12-dimensions from literature), M2 (5 grouped dimensions from M1), and M3 variants (participant-derived dimensions). LLMs including GPT-3.5, Claude, Llama-3.3-70B, and Phi-4 are evaluated using DNA-style prompts (Description, Narrative, Aspects). Agreement with human labels is measured using Cohen's κ, while RBO scores assess query difficulty and system discrimination.

## Key Results
- Domain-specific frameworks (M1-M3) significantly outperform baseline M0 (κ = 0.153-0.389) with M3 achieving κ up to 0.639
- SKIM document representation outperforms HEAD by 101-140% in agreement across both GPT and Llama models
- LLMs reliably identify difficult queries (RBO 0.71-0.76) but show moderate discrimination between retrieval systems (RBO 0.52-0.56)
- GPT-3.5 achieves best overall performance (κ = 0.650) compared to open-source alternatives (max κ = 0.374)

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific evaluation frameworks improve LLM-judge agreement by decomposing relevance into explicit, domain-grounded dimensions (e.g., grade-level appropriateness, source credibility, curriculum alignment), constraining the LLM's judgment space to criteria that experts actually use. The core assumption is that the dimensions capture substantial variance in human relevance judgments and the LLM can reliably score each dimension when explicitly instructed. Evidence shows M1-M3 frameworks consistently outperform M0 baseline across all configurations. Break condition: if the domain is highly heterogeneous or task contexts shift rapidly, a fixed dimension set may fail to capture emerging relevance criteria.

### Mechanism 2
Sampled document representations (SKIM) outperform head-only representations because teachers evaluate resources partially rather than holistically, and relevance often resides in specific sections. Sampling multiple segments across a longer document increases the probability that task-relevant content is included in the LLM's context window. The core assumption is that relevant signals are distributed throughout educational resources, not concentrated in the first ~5,000 characters. Evidence shows SKIM configuration performs better with κ increases of 140% for GPT and 101% for Llama. Break condition: if resources are extremely long with relevant content in later sections, or if sampled segments miss critical figures/tables.

### Mechanism 3
LLM-generated labels reliably identify difficult queries but provide only moderate discrimination between retrieval systems because query difficulty is largely driven by whether any system retrieves relevant results in top positions—a coarse signal that LLMs can detect via presence/absence of relevant content. System discrimination requires finer-grained relevance gradations across overlapping result sets, which is more sensitive to prompt design and dimension weighting. Evidence shows strong RBO for query difficulty (0.71-0.89) but moderate RBO for system rankings (0.52-0.56). Break condition: if LLMs systematically over- or under-score certain dimensions, system rankings will diverge from human rankings.

## Foundational Learning

- **Cohen's κ (inter-rater agreement coefficient)**: The paper uses κ to quantify agreement between LLM and human judgments, accounting for chance agreement unlike raw accuracy. Understanding this is essential to interpret claims like "κ = 0.639 indicates strong agreement." Quick check: If an LLM and human both label 90% of documents as relevant by always predicting "relevant," what would κ be?

- **Rank-Biased Overlap (RBO)**: RBO measures similarity between ranked lists with top-weighted emphasis, used to compare LLM vs. human rankings of query difficulty and system performance. Understanding RBO is crucial for interpreting system evaluation results. Quick check: Why would RBO with φ = 0.9 emphasize the worst 10 queries differently than φ = 0.7 emphasizes the top 3-4 systems?

- **Multi-dimensional relevance decomposition**: The core intervention is breaking relevance into explicit aspects (e.g., affective match, situational match), mirroring rubric-based human evaluation and critical to prompt engineering for domain-specific tasks. Quick check: If you add a new dimension "Adaptability" to a 5-dimension framework, what trade-offs might arise in LLM scoring consistency?

## Architecture Onboarding

- **Component map**: Document Preprocessor -> Prompt Assembler -> LLM Judge Module -> Agreement Evaluator
- **Critical path**: Load dataset -> Preprocess documents (HEAD/SKIM) -> Assemble prompts per framework -> Call LLM for each document-framework combination -> Compute κ and RBO metrics
- **Design tradeoffs**: Framework complexity vs. LLM coherence (M1 provides nuance but lower explained variance than M2); Document length vs. context window (SKIM covers more content but increases cost); Proprietary vs. open-source LLMs (GPT achieves κ up to 0.65, open-source max 0.374); Image vs. text input (image modality consistently underperforms)
- **Failure signatures**: κ drops below 0.3 (likely using M0 baseline or image modality); RBO for system rankings near 0 (M1 framework producing erratic scores); High variance across paraphrased prompts (prompt sensitivity issue)
- **First 3 experiments**: 1) Replicate M3(10) vs. M0 comparison on held-out subset to validate κ improvement; 2) Ablate SKIM vs. HEAD representations for single framework to quantify sampling contribution; 3) Test hybrid framework combining M2 groupings with 2-3 participant dimensions to explore system discrimination improvement

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal combination of relevance dimensions for LLM-based educational resource evaluation? The authors state they did not perform exhaustive evaluation to identify optimal dimension combinations. This remains unresolved because only three frameworks were tested with no systematic exploration of which dimensions should be combined. Resolution would require factorial experiments testing multiple dimension combinations.

### Open Question 2
Why do vision-capable LLMs perform significantly worse with image inputs compared to text inputs for relevance assessment? Image modality performed poorly across all vision models (κ = 0.117-0.245) while text-based SKIM inputs achieved κ up to 0.639. This remains unresolved because the paper reports the gap but doesn't investigate whether the issue lies in document rendering, visual parsing, or fundamental limitations. Resolution would require error analysis comparing LLM explanations for image vs. text inputs.

### Open Question 3
Why do domain-specific frameworks excel at identifying difficult queries but show only moderate discrimination between retrieval systems? The authors note this disparity suggests domain-specific frameworks may better capture nuanced factors making queries challenging but may not translate as effectively to discriminating between systems. This remains unresolved because the paper documents the gap but doesn't explain whether different relevance aspects matter for different evaluation tasks. Resolution would require analysis of which dimensions drive query difficulty vs. system discrimination.

## Limitations
- Single dataset of 401 judgments from one teacher user study limits generalizability to other educational contexts
- Performance gaps between proprietary (GPT) and open-source models (Llama, Phi) suggest findings may not transfer well to cost-sensitive settings
- Evaluation focuses on agreement with human judges rather than actual impact on retrieval system development or user outcomes

## Confidence

- **High Confidence**: Domain-specific frameworks significantly outperform baseline for document-level relevance assessment (κ improvements from 0.15-0.39 to 0.65)
- **Medium Confidence**: LLMs reliably identify difficult queries but show only moderate discrimination between retrieval systems
- **Low Confidence**: The specific optimal number of dimensions (5 vs 10) for participant-derived frameworks shows only marginal differences

## Next Checks
1. Replicate findings on an independent educational resource dataset with different subject domains (e.g., mathematics vs. humanities) to test framework generalizability
2. Conduct ablation studies varying dimension count systematically (3, 5, 8, 12) within the same framework to identify optimal complexity
3. Test prompt sensitivity by varying narrative descriptions while keeping aspect frameworks constant, measuring variance in κ scores to quantify robustness