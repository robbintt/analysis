---
ver: rpa2
title: 'LLMPerf: GPU Performance Modeling meets Large Language Models'
arxiv_id: '2503.11244'
source_url: https://arxiv.org/abs/2503.11244
tags:
- kernel
- execution
- performance
- input
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMPerf, a pioneering approach that leverages
  large language models (LLMs) to predict GPU kernel execution times from static OpenCL
  source code without requiring runtime information. The authors develop a large-scale
  OpenCL performance dataset through automated techniques including memory analysis-based
  input generation and execution configuration sampling, resulting in over 400K diverse
  kernel launch configurations.
---

# LLMPerf: GPU Performance Modeling meets Large Language Models

## Quick Facts
- arXiv ID: 2503.11244
- Source URL: https://arxiv.org/abs/2503.11244
- Reference count: 25
- Primary result: LLMPerf-2B achieves 24.25% MAPE on 400K validation set for OpenCL kernel execution time prediction from static source code

## Executive Summary
LLMPerf introduces a novel approach to GPU kernel performance modeling by leveraging large language models to predict execution times directly from static OpenCL source code, without requiring runtime profiling or hardware-specific analytical models. The authors develop a large-scale OpenCL performance dataset through automated techniques including memory analysis-based input generation and execution configuration sampling, resulting in over 400K diverse kernel launch configurations. By adapting the CodeGen LLM architecture with a regression head and fine-tuning on carefully constructed prompts, LLMPerf demonstrates that LLMs can effectively learn GPU performance patterns from static code analysis, achieving promising accuracy while opening new avenues for performance modeling that eliminates the need for dynamic profiling.

## Method Summary
LLMPerf fine-tunes the CodeGen LLM (350M or 2B parameters) on a large dataset of OpenCL kernels by replacing the language prediction head with an MLP regression head that predicts log-transformed execution times. The approach uses Clang AST-based memory analysis to infer valid input array sizes from kernel memory access patterns, samples execution configurations across different utilization regimes, and constructs prompts containing kernel source code, input specifications, and execution parameters. The model is trained using MSE loss on log₂(execution_time) with AdamW optimizer, learning to map static code features to performance predictions without runtime information.

## Key Results
- LLMPerf-2B achieves 24.25% MAPE on a 400K-sample validation set
- Performance degrades to 46.1% MAPE on publicly available OpenCL benchmarks (SHOC/Rodinia)
- Smaller model (350M) shows severe overfitting: 1.86% train MAPE vs 43.77% validation MAPE
- Kernels with data-dependent performance (spmv_csr_scalar_kernel) show highest error at 56.76% MAPE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fine-tuned on structured prompts can estimate GPU kernel execution time from static source code alone.
- Mechanism: CodeGen models pre-trained on code corpora capture semantic patterns (loops, memory access, parallelism). Fine-tuning with prompts containing kernel source, input specifications (array sizes, scalar values), and execution parameters (global/local workgroup sizes) teaches the model to map these static features to execution time distributions. The model learns correlations between code structure and performance without runtime profiling.
- Core assumption: Performance-relevant patterns in OpenCL kernels (memory access patterns, work-item parallelism, synchronization) are learnable from source code representations, and the training distribution covers the target inference distribution.
- Evidence anchors:
  - [abstract] "LLMPerf...predict OpenCL kernel execution times based solely on static source code information...achieves a mean absolute percentage error (MAPE) of 24.25% on a 400K-sample validation set"
  - [section III-A] "our model does not use any runtime information. It functions without any requirement of the OpenCL or GPU execution environment"
  - [corpus] Related work on GPU kernel tuning (arXiv:2601.12698) demonstrates that semantic code understanding enables optimization, supporting the premise that code structure encodes performance-relevant information.
- Break condition: If kernels exhibit performance behaviors dominated by runtime-determined factors (e.g., input-dependent branch divergence, dynamic load balancing), static-only prediction will degrade significantly.

### Mechanism 2
- Claim: Compiler-based memory analysis enables automated inference of valid input array sizes from kernel memory access patterns.
- Mechanism: Using Clang AST, the framework instruments memory accesses and records access locations under two sample global work sizes. By identifying that array indices are typically affine functions of global/local IDs, the system solves for coefficients (c, d) in `size = c × gsize + d`, enabling generation of diverse, valid launch configurations without manual specification.
- Core assumption: OpenCL kernels exhibit regular memory access patterns where array sizes correlate with execution configuration parameters via affine relationships.
- Evidence anchors:
  - [section II-B-2] "most input array sizes can be inferred by statically analyzing memory access patterns, as GPU programs tend to have regular memory accesses to exploit parallelism"
  - [section II-B-2] Four patterns identified: data partitioning, offset and stride, boundary check, complex forms—all with "array index...an affine function in the global ID, workgroup ID and local ID"
  - [corpus] No direct corpus evidence for this specific memory-analysis technique; it appears novel to this work.
- Break condition: Kernels with irregular memory access (e.g., sparse computations, graph algorithms with data-dependent access) will yield invalid inferred sizes.

### Mechanism 3
- Claim: Predicting log-transformed execution time via a regression head stabilizes training and reduces sensitivity to large execution time outliers.
- Mechanism: The language modeling head is replaced with an MLP regression head that takes concatenated hidden states and projects to a scalar. Training uses MSE loss on `log₂(execution_time)`, compressing the dynamic range and preventing large execution times from dominating gradients.
- Core assumption: Execution time prediction can be treated as a regression task rather than sequence generation, and log-transformation sufficiently normalizes the target distribution.
- Evidence anchors:
  - [section III-B] "We chose to predict the logarithm of execution time to reduce sensitivity to large execution times and stabilize the training process"
  - [section III-B] "remove the language prediction head from CodeGen and introduce a new regression head to allow direct execution time prediction"
  - [corpus] Related work on program analysis (arXiv:2509.22337) uses GPU acceleration for belief propagation but does not address regression head design for LLMs—this architecture choice is specific to LLMPerf.
- Break condition: If execution time distribution is multimodal or has heavy tails not captured by log-transformation, MSE loss on log-values may still yield poor calibration.

## Foundational Learning

- Concept: **OpenCL execution model (work-items, workgroups, NDRange)**
  - Why needed here: The prompt explicitly includes global and local workgroup sizes; understanding how these map to GPU parallelism is essential for interpreting why these features matter for prediction.
  - Quick check question: Given a global size of 1024 and local size of 64, how many workgroups are launched?

- Concept: **Memory access patterns in GPU kernels (coalescing, strided access, boundary checks)**
  - Why needed here: The memory-analysis technique assumes specific patterns (data partitioning, offset/stride, boundary check) to infer array sizes; recognizing these in kernel code enables understanding of the input generation logic.
  - Quick check question: In a kernel where each work-item processes 4 consecutive elements starting at `id * 4`, what memory access pattern is this?

- Concept: **LLM fine-tuning vs. in-context learning**
  - Why needed here: LLMPerf uses fine-tuning with a regression head rather than prompting a frozen model; understanding this distinction clarifies why the model requires substantial training data (~400K samples).
  - Quick check question: What is the difference between adapting an LLM via fine-tuning versus via few-shot prompting?

## Architecture Onboarding

- Component map:
  Source Code Corpus -> Input Argument Generator -> Execution Setting Selector -> Prompt Builder -> LLMPerf Model -> Training Pipeline

- Critical path:
  1. Kernel source → Clang AST instrumentation → memory access pattern extraction
  2. Sample execution configs → run instrumented kernel → infer array size coefficients
  3. Build prompt with (code, inputs, gsize, lsize) → execute on GPU → record time
  4. Fine-tune CodeGen with regression head on (prompt, log_time) pairs
  5. Inference: prompt only → model predicts execution time directly

- Design tradeoffs:
  - **Static-only vs. runtime features**: Eliminates profiling overhead but sacrifices accuracy on input-dependent behaviors (46.1% MAPE on real benchmarks vs. 24.25% on generated validation)
  - **1D kernels only**: Simplifies learning scope but excludes ~30% of corpus kernels (2D/3D)
  - **Array sizes vs. element values in prompts**: Reduces context length but ignores data-dependent divergence; authors note this hurts kernels like `spmv_csr_scalar_kernel`
  - **Model size (350M vs. 2B)**: 2B shows better generalization (24.25% vs. 43.77% val MAPE on 400K dataset) but requires more compute

- Failure signatures:
  - **High MAPE on kernels without input size = global size correlation**: Scan and Reduction kernels (63–66% MAPE) where global size stays fixed while input size varies
  - **Overfitting in smaller models**: LLMPerf-350M-400K shows 1.86% train MAPE but 43.77% val MAPE
  - **Poor performance on data-dependent kernels**: `spmv_csr_scalar_kernel` (56.76% MAPE) where performance depends on sparse matrix structure

- First 3 experiments:
  1. **Reproduce the 400K dataset generation**: Run the memory-analysis pipeline on 10–20 kernels from the corpus, verify that inferred array sizes produce valid executions, and confirm the IQR rebalancing improves execution time distribution spread.
  2. **Ablate the prompt format**: Train LLMPerf-2B on the 200K dataset with prompts omitting (a) input specifications, (b) local workgroup size, to quantify each component's contribution to accuracy.
  3. **Evaluate on a held-out kernel family**: Select 5 kernels from SHOC/Rodinia not in training, generate execution times across varying gsizes, and compare model predictions against actual measurements to assess generalization bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can increasing the proportion of varied input size samples (decoupled from global size) in training data improve prediction accuracy for kernels where input size and global work size differ?
- Basis in paper: [explicit] The authors state "we believe increasing the varied input size samples using memory analysis-based input argument generation could potentially overcome this limitation, which we leave for future work" regarding poor performance on Scan and Reduction kernels where global size remains fixed while data size changes.
- Why unresolved: The training set contains only ~25K varied input size samples versus ~365K with input size-global size correlation, creating severe class imbalance.
- What evidence would resolve it: Retraining LLMPerf with a balanced dataset and measuring MAPE reduction on kernels like Scan-bottom_scan (63.0% MAPE) and Reduction-reduce (65.85% MAPE).

### Open Question 2
- Question: How can LLM-based performance models effectively represent and predict execution times for multi-dimensional (2D and 3D) GPU kernels?
- Basis in paper: [inferred] The paper explicitly constrains evaluation to 1D kernels only (page 2), despite 2D (25%) and 3D (5%) kernels comprising 30% of the corpus, stating this "simplif[ies] the learning scope."
- Why unresolved: Multi-dimensional kernels introduce additional complexity in execution space representation, workgroup configuration, and memory access patterns that current prompt designs do not address.
- What evidence would resolve it: Developing prompt formats for NDRange configurations and evaluating prediction accuracy on held-out 2D/3D kernels from the corpus.

### Open Question 3
- Question: Can LLM-based performance models generalize across different GPU architectures without architecture-specific retraining?
- Basis in paper: [inferred] All experiments use a single GPU (NVIDIA Tesla V100S) and explicitly fix hardware factors: "we only consider the OpenCL programs and keep other factors fixed" (page 2), with no cross-hardware evaluation.
- Why unresolved: GPU performance depends heavily on architecture-specific parameters (SM count, memory bandwidth, cache hierarchy, scheduling policies) that vary across vendors and generations.
- What evidence would resolve it: Evaluating LLMPerf on the same kernel suite across different GPUs (AMD, Intel, other NVIDIA generations) and reporting per-architecture MAPE.

### Open Question 4
- Question: Can incorporating representative information about array element values into prompts improve accuracy for kernels with data-dependent performance behaviors?
- Basis in paper: [explicit] The authors acknowledge excluding array values because "array element values can impact performance, especially in cases of memory or execution divergences" (page 5), and note the model struggles with kernels "where performance depends on array element values" like spmv_ellpackr_kernel (page 7-8).
- Why unresolved: Current prompts only include array sizes to avoid context length limits, but this omission violates assumptions for data-dependent kernels, particularly those with branch divergence or irregular memory access.
- What evidence would resolve it: Creating prompts with value distribution summaries (e.g., sparsity ratios, value ranges) and measuring MAPE improvement on data-dependent kernels versus baseline.

## Limitations

- **Generalization gap**: 46.1% MAPE on real benchmarks versus 24.25% on validation set reveals significant performance degradation on unseen kernels
- **Limited kernel dimensionality**: 1D kernel constraint excludes ~30% of corpus kernels, missing complex access patterns
- **Hardware specificity**: Only evaluated on NVIDIA Tesla V100S, with no evidence for cross-GPU or cross-vendor generalization

## Confidence

- **High confidence** in fundamental claim: Supported by 24.25% MAPE on large validation set and sound methodology
- **Medium confidence** in benchmark generalization: 46.1% MAPE on SHOC/Rodinia and poor performance on specific kernel families (scan, reduction) indicate limitations
- **Low confidence** in cross-GPU portability: No evidence provided for performance on different GPU architectures or manufacturers

## Next Checks

1. **Cross-architecture validation**: Evaluate LLMPerf-2B on a different GPU architecture (e.g., AMD or Intel GPUs) using the same OpenCL kernels to assess hardware portability and quantify any architecture-specific performance pattern dependencies.

2. **Memory access pattern stress test**: Systematically generate kernels with irregular memory access patterns (sparse matrices, graph algorithms, random access) and measure model performance degradation compared to regular access patterns to establish the boundaries of the memory analysis technique.

3. **Input size decoupling experiment**: Create a controlled dataset where input array sizes are explicitly decoupled from global work sizes across a wide range, then evaluate whether the model can learn the underlying performance relationships or if it has simply memorized the common correlation observed in training data.