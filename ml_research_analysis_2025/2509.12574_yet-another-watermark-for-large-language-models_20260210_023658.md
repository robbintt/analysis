---
ver: rpa2
title: Yet Another Watermark for Large Language Models
arxiv_id: '2509.12574'
source_url: https://arxiv.org/abs/2509.12574
tags:
- watermark
- text
- watermarking
- llms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new watermarking framework for large language\
  \ models (LLMs) that embeds watermarks by manipulating the internal parameters of\
  \ the model, specifically in the output layer, to amplify logits of designated tokens.\
  \ Unlike existing methods that adjust token sampling predictions or rely on post-processing\u2014\
  often reducing semantic quality\u2014this approach tightly couples the watermark\
  \ with the LLM\u2019s intrinsic parameters, achieving better balance between robustness\
  \ and imperceptibility."
---

# Yet Another Watermark for Large Language Models

## Quick Facts
- arXiv ID: 2509.12574
- Source URL: https://arxiv.org/abs/2509.12574
- Reference count: 24
- Primary result: Introduces parameter-space watermarking for LLMs achieving PPL ~3.93 vs 3.82 baseline while maintaining strong detection (z-score ~6.49)

## Executive Summary
This paper presents a novel watermarking framework for large language models that embeds watermarks by directly manipulating the output layer parameters, specifically scaling weights for designated tokens. Unlike existing methods that adjust token sampling predictions or rely on post-processing, this approach tightly couples the watermark with the LLM's intrinsic parameters, achieving better balance between robustness and imperceptibility. The method enables black-box detection without requiring access to the LLM, making it practical for real-world deployment. Experiments using LLaMA3-8B show the approach preserves text quality while achieving strong detection performance.

## Method Summary
The method modifies the output layer weights of a pre-trained LLM by applying scaling factors to rows corresponding to designated tokens. A secret key governs token selection through a pseudo-random function that partitions the vocabulary into marked and unmarked subsets. The selected tokens' weights are scaled upward (α↑ ≥ 1) while others can be scaled downward (α↓ ≤ 1). This parameter-space intervention requires no retraining and achieves detection through statistical analysis of token frequencies. The watermark can be verified without model access by comparing observed token frequencies to expected null-hypothesis distributions using z-score analysis.

## Key Results
- Watermarked text achieves perplexity of ~3.93 compared to 3.82 baseline, demonstrating minimal quality degradation
- Detection z-score reaches ~6.49, well above the 2.33 threshold for 99% confidence
- Method is robust to common text perturbations including masking, deletion, and insertion
- Outperforms related methods in maintaining text fluency while ensuring reliable detection

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Space Logit Amplification
The method scales output-layer weights (rather than adding bias terms) to create a watermark signal more tightly coupled with the model's text generation dynamics. By applying scaling factors directly to weight matrix rows, the watermark becomes entangled with how semantic representations map to tokens. This proportional modulation produces smoother probability distributions compared to rigid bias additions used in related work.

### Mechanism 2: Secret-Key-Governed Token Selection
A pseudo-random function `f(key)` determines which vocabulary tokens are marked at each generation step, enabling black-box verification without model access. The token selection rule partitions the vocabulary into marked and unmarked subsets, where the fraction selected controls watermark density. Statistical frequency analysis of key-selected tokens creates detectable deviations from baseline distributions.

### Mechanism 3: Localized Parameter Intervention
Modifying only the output layer (not Transformer backbone) achieves computational efficiency and cross-architecture portability without retraining. The intervention is restricted to modest parameter scales relative to the full model, requiring only direct scaling of existing parameters. This locality ensures the modification's impact is contained and predictable.

## Foundational Learning

- **Concept: Transformer Output Layer and Logit Space** - Why needed: The method operates entirely on how hidden states become token probabilities. Quick check: Given hidden state `h` and weight matrix `W`, what happens to token probabilities if you double one row of `W` while keeping others fixed?

- **Concept: Statistical Hypothesis Testing (z-score)** - Why needed: Detection relies on comparing observed token frequencies to null-hypothesis expectations. Quick check: If watermark produces z-score = 1.5, can you claim 99% confidence in detection? What about z-score = 6.49?

- **Concept: Perplexity as Text Quality Metric** - Why needed: The paper's main claim is preserving text quality (PPL ~3.93 vs 3.82). Quick check: If PPL increases from 3.82 to 5.08, what does this indicate about the model's predictive quality?

## Architecture Onboarding

- **Component map:** `f(key)` -> Token selector (determines Gi per step) -> `W, b` -> Output layer parameters (modified via scaling factors) -> `α↑, α↓, β↑, β↓` -> Scaling hyperparameters (control embedding strength) -> `γ` -> Token selection fraction (controls watermark density) -> Statistical detector -> z-score calculator using token frequency analysis

- **Critical path:** 1. Initialize: Load pre-trained LLM, freeze all parameters except output layer access 2. Key setup: Define secret key and γ value 3. Parameter modification: Scale W[idx(g),:] by α↑ for selected tokens, by α↓ for others 4. Generation: Run autoregressive decoding with modified parameters 5. Detection: Count Gi token occurrences, compute z-score, compare to threshold

- **Design tradeoffs:** Higher α↑ yields stronger detection but potentially higher PPL; α↑ > 1.3 shows diminishing returns; γ too low yields weak signal while γ too high dilutes distinctiveness; weight scaling couples more tightly with model dynamics than bias scaling

- **Failure signatures:** Low z-score (< 2.33) suggests α↑ may be too low, γ suboptimal, or text heavily perturbed; high PPL (> 5.0) indicates α↑ excessive or incompatible with model architecture; detection fails after edits suggests attack ratio exceeds robustness threshold

- **First 3 experiments:** 1. Reproduce α sweep: Vary α from 1.0 to 1.3, plot z-score and PPL, target z-score > 2.33 at α ≈ 1.05, PPL < 4.5 2. Reproduce γ sweep: Fix α = 1.1, vary γ from 0.1 to 0.9, target optimal γ near 0.5 where z-score peaks 3. Robustness stress test: Apply masking, deletion, insertion at 10%, 20%, 30% attack ratios, target z-score remains > 2.33 at moderate perturbation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the watermark resilient to semantic-preserving paraphrase attacks performed by non-watermarked LLMs?
- Basis: Robustness evaluation limited to primitive text perturbations, not paraphrasing
- Why unresolved: Paraphrasing alters token sequence structure while preserving meaning, potentially disrupting statistical frequency of designated tokens
- What evidence would resolve it: Detection z-scores and PPL for watermarked text after paraphrasing model processing

### Open Question 2
- Question: Does joint tuning of amplification (α↑) and suppression (α↓, β↓) scaling factors yield superior trade-off compared to simplified boundary case?
- Basis: Experiments focused on boundary case with α↓ = β↑ = β↓ = 1, leaving full parameter space unexplored
- Why unresolved: Unclear if subtle suppression of non-designated tokens alongside amplification could achieve higher z-scores with lower perplexity impacts
- What evidence would resolve it: Ablation study varying all four scaling parameters to map resulting Pareto frontier

### Open Question 3
- Question: Does the watermark persist after model undergoes further fine-tuning or PEFT such as LoRA?
- Basis: Method modifies output layer weights directly; paper doesn't evaluate robustness to model modifications
- Why unresolved: Gradient updates from fine-tuning could overwrite or distort the modulation, effectively removing the watermark
- What evidence would resolve it: Detection statistics measured before and after applying standard fine-tuning or LoRA adaptation

### Open Question 4
- Question: How can advanced steganographic strategies be adapted to this parameter-modulation framework?
- Basis: Conclusion states future work focuses on extending advanced strategies to LLMs
- Why unresolved: Current framework relies on simple frequency bias, whereas advanced strategies may require differentiable and structured manipulations beyond scalar weight scaling
- What evidence would resolve it: Theoretical mapping or implementation embedding semantic-aware or acrostic steganography into output layer

## Limitations
- Token selection algorithm details are unspecified, critical for reproducing detection performance
- Generation hyperparameters (temperature, top-p) are not specified, affecting token frequency distributions
- Architecture portability claims lack validation beyond LLaMA3-8B, theoretical gap in cross-model behavior
- No rigorous theoretical analysis of why weight scaling produces reliable frequency deviations across diverse contexts

## Confidence

**High Confidence**: Core mechanism of parameter-space watermarking through output layer weight scaling is technically sound and novel; text quality preservation (PPL 3.93 vs 3.82) is credible given localized intervention

**Medium Confidence**: Detection performance claims (z-score ~6.49) are plausible but require exact reproduction of token selection algorithm; robustness claims supported by Fig. 3 but limited to specific attack types

**Low Confidence**: "Any generative model" compatibility lacks sufficient validation; optimal γ = 0.5 finding may be model-specific and requires broader testing

## Next Checks
1. **Token Selection Algorithm Verification** - Implement and test multiple plausible pseudo-random selection schemes to determine which produces detection performance matching reported z-score > 2.33, compare detection rates across different keys and γ values

2. **Cross-Model Validation** - Apply watermarking method to at least two additional LLM architectures (e.g., OPT-1.3B and Mistral-7B) and measure detection performance and PPL preservation to test claimed architecture portability

3. **Generation Strategy Sensitivity Analysis** - Systematically vary temperature (0.0, 0.7, 1.5) and top-p (0.9, 0.95, 1.0) during generation while keeping watermark parameters fixed, measure how sampling strategy affects detection z-score and text quality