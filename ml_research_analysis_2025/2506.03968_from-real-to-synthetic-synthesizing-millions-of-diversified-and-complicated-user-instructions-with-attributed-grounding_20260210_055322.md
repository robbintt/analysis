---
ver: rpa2
title: 'From Real to Synthetic: Synthesizing Millions of Diversified and Complicated
  User Instructions with Attributed Grounding'
arxiv_id: '2506.03968'
source_url: https://arxiv.org/abs/2506.03968
tags:
- instructions
- questions
- data
- instruction
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for synthesizing large-scale,
  high-quality instruction data for language model alignment. The core idea is "attributed
  grounding," which attributes real user instructions to real-world documents, users,
  and motivations, then uses this to synthesize new grounded instructions from web
  documents.
---

# From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding

## Quick Facts
- arXiv ID: 2506.03968
- Source URL: https://arxiv.org/abs/2506.03968
- Reference count: 40
- Authors: Chiwei Zhu; Benfeng Xu; Xiaorui Wang; Zhendong Mao
- 1M high-quality synthetic instructions synthesized via attributed grounding, achieving SOTA on Alpaca Eval 2.0 (19.15% win rate) and Arena Hard (15.4% win rate)

## Executive Summary
This paper introduces a novel framework for synthesizing large-scale, high-quality instruction data for language model alignment. The core idea is "attributed grounding," which attributes real user instructions to real-world documents, users, and motivations, then uses this to synthesize new grounded instructions from web documents. This approach produces more diverse and complex instructions than existing methods by grounding synthetic data in real-world contexts. The authors create a dataset of 1 million instructions (SynthQuestions) and demonstrate that models trained on it achieve state-of-the-art performance on alignment benchmarks compared to other open-source datasets. The results show continual improvement with more web corpora and effectiveness across different model sizes.

## Method Summary
The framework works in two stages: top-down attribution and bottom-up synthesis. First, 29K high-quality real instructions are attributed to documents, users, and motivations using concept extraction and web search. Then, web documents from FineWeb, PILE, and MathPILE are used to generate situations and instructions through LLM prompting with demonstrations from the attributed real instructions. The synthesized instructions are filtered using a 7-dimensional complexity scoring system based on Arena Hard criteria, keeping only those scoring ≥3. The final 1M dataset is topic-balanced using BERTopic clustering. Models are fine-tuned using SFT with the synthesized data, then optionally preference-optimized.

## Key Results
- Alpaca Eval 2.0: 19.15% win rate (best open-source model)
- Arena Hard: 15.4% win rate (best open-source model)
- GSM8K accuracy: 58.30% (strong reasoning performance)
- t-SNE visualization shows SynthQuestions occupies the most extensive semantic area among synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
Grounding synthetic instructions to real-world contexts (documents, users, motivations) produces higher-quality alignment data than ungrounded generation methods. The attributed grounding framework reverses the natural instruction formation process—first attributing real instructions to their grounding factors (top-down), then synthesizing new instructions by constructing realistic situations from web documents (bottom-up). This ensures generated instructions reflect situated use cases rather than abstract task patterns.

### Mechanism 2
Leveraging diverse web-scale corpora as the synthesis seed enables broader topic coverage and complexity than seed-task augmentation or Wikipedia-entity approaches. The framework uses FineWeb (general web), PILE, and MathPILE as document sources, then prompts LLMs to generate user/motivation pairs and instructions from these documents using demonstrations from REAL QUESTIONS. This bottom-up synthesis harvests natural distribution diversity from pretraining-scale web text.

### Mechanism 3
Multi-dimensional complexity filtering (7 Arena Hard criteria) produces instruction distributions concentrated toward challenging tasks that better elicit model capabilities. Each instruction is scored on specificity, domain knowledge, complexity, problem-solving, creativity, technical accuracy, and real-world application. Only instructions scoring ≥3 (heuristic threshold) are retained; final 1M dataset is selected via topic-balanced sampling.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) / Instruction Tuning**: The entire framework produces SFT training data; understanding how instruction-response pairs shape model behavior is essential for evaluating data quality.
  - Why needed here: The framework produces SFT training data; understanding how instruction-response pairs shape model behavior is essential for evaluating data quality.
  - Quick check question: Can you explain why diverse, complex instructions are theorized to improve generalization over narrow, simple instruction sets?

- **In-Context Learning (ICL) with Demonstrations**: The synthesis pipeline uses ICL—REAL QUESTIONS samples serve as demonstrations to guide LLM behavior during instruction generation.
  - Why needed here: The synthesis pipeline uses ICL—REAL QUESTIONS samples serve as demonstrations to guide LLM behavior during instruction generation.
  - Quick check question: How does demonstration selection (random vs. semantic-based) potentially affect the diversity of generated outputs?

- **Preference Optimization (DPO/PPO)**: The paper shows SYNTH QUESTIONS + DPO achieves 33.81% Alpaca Eval win rate (outperforming the 70B generator model), suggesting SFT data quality determines preference optimization ceiling.
  - Why needed here: The paper shows SYNTH QUESTIONS + DPO achieves 33.81% Alpaca Eval win rate (outperforming the 70B generator model), suggesting SFT data quality determines preference optimization ceiling.
  - Quick check question: Why might high-quality SFT data be a prerequisite for effective preference learning?

## Architecture Onboarding

- **Component map**:
```
[Data Collection] → [Cleaning/Dedup] → [Quality Filtering] → REAL QUESTIONS (29K)
                                                              ↓
[Web Search for Documents] ← [Attribution: extract concepts, recall docs]
                                                              ↓
[LLM: Generate User/Motivation] → Attributed RQα = {(i, d, u, m)}
                                                              ↓
[Web Corpora: FineWeb + PILE + MathPILE] ← [Synthesis Seed]
                                                              ↓
[LLM: Generate Situation] → [LLM: Generate Instruction] → [Score & Filter] → [Topic Sampling] → SYNTH QUESTIONS (1M)
```

- **Critical path**:
1. Seed quality determines synthesis quality: REAL QUESTIONS filtering (29K from 690K) directly shapes demonstration quality for ICL-driven generation.
2. Attribution ↔ Synthesis bidirectional consistency: The (document, user, motivation) → instruction mapping learned during attribution must be reversible during synthesis.
3. Filtering threshold calibration: Score ≥3 heuristic balances quality vs. retention; verify against downstream task performance.

- **Design tradeoffs**:
  - Scale vs. filtering strictness: 1M final dataset from larger unfiltered pool; stricter thresholds reduce scale but may improve per-sample quality
  - Document source diversity vs. coherence: Mixing FineWeb + PILE + MathPILE improves reasoning coverage but may introduce distribution discontinuity
  - LLM generator choice: LLaMA-3-70B-Instruct for attribution vs. LLaMA-3-8B-Instruct for synthesis—cost/quality tradeoff not fully analyzed

- **Failure signatures**:
  - Low Vendi Score / narrow t-SNE clustering → document sources too homogeneous or synthesis collapsing to local modes
  - High filter rejection rate (>80%) → prompts misaligned with complexity criteria or document-instruction mapping broken
  - Benchmark gains without reasoning improvement → instructions may be stylistically complex but not cognitively demanding

- **First 3 experiments**:
1. **Attribution ablation**: Generate 10K instructions with vs. without grounding (direct instruction generation using REAL QUESTIONS demonstrations only); compare Alpaca Eval 2.0 and GSM8K to quantify grounding contribution.
2. **Document source impact**: Synthesize 50K instructions from FineWeb-only vs. FineWeb+PILE+MathPILE; evaluate on reasoning benchmarks (MATH, GSM8K) to validate domain-specific document necessity.
3. **Filtering threshold sweep**: Generate full dataset at score thresholds {2, 3, 4, 5}; plot scale vs. Alpaca Eval performance to identify optimal quality/quantity tradeoff point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model performance plateau or continue to improve when scaling the SynthQuestions dataset beyond 1 million instructions?
- Basis in paper: The "Limitation and Potential Risks" section states, "we do not test data scale larger than 1M," despite the scaling curve showing potential for further improvement.
- Why unresolved: The authors constrained the dataset size to 1M for the reported experiments.
- What evidence would resolve it: Benchmark results (e.g., Alpaca Eval 2.0) for models trained on 2M, 5M, and 10M instruction subsets.

### Open Question 2
- Question: What is the optimal distribution of web corpora (e.g., general web vs. math/code) for maximizing reasoning capabilities without degrading general alignment?
- Basis in paper: The "Limitation and Potential Risks" section notes that "a more thorough study about the optimal selection and distribution of web corpora used for synthesizing can be conducted."
- Why unresolved: The current mixture relied on heuristic additions of MathPILE and PILE to FineWeb, rather than a systematic optimization.
- What evidence would resolve it: Ablation studies varying the ratio of domain-specific documents (code/math) and measuring the impact on GSM8K/MATH versus general alignment tasks.

### Open Question 3
- Question: Does grounding synthetic instructions in web documents increase the risk of model hallucination compared to other synthesis methods?
- Basis in paper: The "Limitation and Potential Risks" section warns that "the dataset has not been assessed in terms of hallucination, which may lead language models to output false or unfaithful contents."
- Why unresolved: The evaluation focused on win rates and reasoning accuracy, but not on factuality or faithfulness metrics.
- What evidence would resolve it: Evaluation using factuality benchmarks (e.g., FactScore or TruthfulQA) comparing SynthQuestions-trained models against baselines.

## Limitations

- **Unknown optimal scale**: The authors didn't test data scale larger than 1M, leaving open whether performance plateaus or continues improving with more data.
- **Untested cross-architecture generalization**: All evaluations use LLaMA-3-8B; effectiveness for very small or very large models isn't established.
- **Unassessed hallucination risk**: The dataset hasn't been evaluated for factuality or faithfulness, which may lead to model hallucinations.

## Confidence

- **Attribution quality**: Medium - The mechanism is sound but lacks direct evaluation of attribution accuracy itself
- **Web-scale corpus benefits**: Medium - Ablation shows positive impact but doesn't explore optimal document mix or individual source contributions
- **Cross-model generalization**: Low - Limited evaluation scope doesn't test architectural boundaries

## Next Checks

1. **Attribution fidelity study**: Manually evaluate 100 randomly sampled attribution cases (real instruction → extracted concepts → retrieved document → attributed user/motivation) to measure precision and recall of the concept extraction and document retrieval components.

2. **Cross-dataset generalization test**: Apply the full SYNTH QUESTIONS + DPO pipeline to a different base model family (e.g., Mistral, Qwen) and evaluate on the same benchmarks.

3. **Safety risk mapping**: Perform detailed error analysis on the 4.32% flagged safety cases to determine if specific document sources or synthesis patterns are disproportionately generating risky content.