---
ver: rpa2
title: How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation
  under the One-Time-Pad-Based Framework
arxiv_id: '2507.19219'
source_url: https://arxiv.org/abs/2507.19219
tags:
- benchmarks
- evaluation
- llms
- private
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the overestimation problem in large language
  model (LLM) evaluation caused by data contamination and biased overtraining. To
  tackle this, the authors propose ArxivRoll, a dynamic evaluation framework inspired
  by one-time pad encryption.
---

# How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework

## Quick Facts
- arXiv ID: 2507.19219
- Source URL: https://arxiv.org/abs/2507.19219
- Reference count: 15
- Models exhibit significant overestimation, with some achieving over 100% in contamination scores

## Executive Summary
This paper addresses the overestimation problem in LLM evaluation caused by data contamination and biased overtraining. The authors propose ArxivRoll, a dynamic evaluation framework inspired by one-time pad encryption that generates private test cases from recent arXiv articles and quantifies overestimation through Rugged Scores comparing public and private benchmark performance. Extensive experiments show that ArxivRoll produces high-quality, stable benchmarks and effectively quantifies overestimation in current LLMs, revealing that popular models exhibit significant overestimation with some achieving over 100% contamination scores.

## Method Summary
The framework constructs benchmarks every six months using recent arXiv preprints, ensuring models cannot have memorized test cases. SCP (Sequencing, Cloze, and Prediction) automatically generates three types of multiple-choice tasks from extracted text fragments. ArxivRollBench contains these private benchmarks used for one-time evaluations, after which they expire. Rugged Scores quantify overestimation by comparing model performance on public and private benchmarks, with RSI measuring contamination and RSII measuring biased overtraining. Evaluation uses LM Evaluation Harness with greedy decoding and exact matching scoring.

## Key Results
- ArxivRollBench (Sequencing) achieves Spearman correlation of 0.76 with ChatbotArena rankings
- Phi-1 has Absolute RSI of 1.21 (>100% overestimation), Qwen2.5-72B has 1.41
- SCP benchmarks show high stability with standard deviation <1 point across 32 random seeds

## Why This Works (Mechanism)

### Mechanism 1: Temporal Containment via One-Time-Pad Analogy
The framework draws fresh test material from arXiv preprints within a 6-month window, ensuring models cannot have memorized specific test cases. Benchmarks expire after use, preventing future models from treating them as training data. This mirrors one-time pad cryptography where each key is used exactly once.

### Mechanism 2: SCP Tasks Require Coherence Modeling Over Pattern Matching
The three task formats—Sequencing, Cloze, Prediction—assess discourse-level understanding rather than surface-level token prediction. All three are formatted as multiple-choice (4 options) to ensure objective, automated scoring.

### Mechanism 3: Rugged Score Quantifies Public-Private Performance Gap
The ratio of performance difference between public and private benchmarks estimates contamination/overtraining magnitude. RSI compares accuracy on matched domain pairs, while RSII measures variance across private benchmarks to detect biased overtraining.

## Foundational Learning

- **Data Contamination in LLM Evaluation**: Models may have seen test data during training, artificially inflating scores. Understanding this is crucial for grasping why "private" benchmarks are needed.
  - Quick check: If a model trained on the entire internet until 2023 is tested on MMLU (published 2021), what type of contamination risk exists?

- **One-Time Pad Cryptography**: The paper's core analogy—using a benchmark once, then expiring it—draws from OTP's property that each encryption key is used exactly once. This explains why benchmarks are released post-evaluation.
  - Quick check: In OTP, why is reusing a key catastrophic for security? How does this map to reusing a benchmark?

- **Discourse Coherence and Cloze Testing**: SCP tasks test whether models understand how sentences relate, not just whether they predict tokens. Understanding cloze tests clarifies why masking sentences is a valid comprehension probe.
  - Quick check: Why might a model score well on next-token prediction but poorly on sentence ordering?

## Architecture Onboarding

- **Component map**: arXiv papers (8 domains) → Text fragment extraction → SCP Generator → ArxivRollBench (private) → LM Evaluation Harness → Rugged Score computation → Leaderboard + expiration

- **Critical path**: 
  1. Paper collection window: 6-month windows of recent arXiv papers
  2. Fragment quality filtering: Text must be >80 words, low formula density
  3. Distractor generation: TF-IDF retrieves similar sentences from same paper as wrong options

- **Design tradeoffs**: 
  - Automation vs. quality: SCP is fully automated but includes manual review for quality control
  - Temporal freshness vs. domain balance: CS/Math/Stat have many more papers than Econ/Finance
  - Objective scoring vs. task richness: Multiple-choice enables exact match but limits assessment to recognition

- **Failure signatures**:
  - RSI > 1.0: Model performs substantially better on public than private benchmarks
  - High RSII with low NRSII: Large absolute variance but normalized variance low
  - Near-random SCP scores (~25%): Model fails at discourse-level tasks
  - Correlation with ChatbotArena drops: SCP may have drifted in quality

- **First 3 experiments**:
  1. Run 32-seed stability experiment on a different model to confirm SCP variance remains <1% accuracy
  2. Take a model with known training data, deliberately add ArxivRollBench2024b-CS to its fine-tuning set, then re-evaluate
  3. Evaluate a model fine-tuned heavily on math on all 8 domains and check if RSII shows high variance

## Open Questions the Paper Calls Out

### Open Question 1
Does the Rugged Score strictly isolate data contamination and biased overtraining, or does it conflate these factors with natural performance gaps caused by domain shifts or difficulty variances between public and private benchmarks?

### Open Question 2
Does the exclusion of text fragments containing heavy mathematical formulas and tables during the SCP generation process limit the benchmark's ability to assess complex symbolic reasoning capabilities?

### Open Question 3
Does the reliance on ArXiv preprints introduce a domain bias that prevents the framework from evaluating "comprehensive" capabilities in non-academic or creative domains?

## Limitations
- Temporal contamination window uncertainty: 6-month cutoff may not guarantee no contamination
- SCP task construct validity: Correlation with ChatbotArena doesn't establish identical underlying constructs
- Rugged Score interpretation ambiguity: Depends on private benchmarks being perfectly uncontaminated and representative

## Confidence
- High Confidence: ArxivRoll produces stable, reproducible benchmarks (32-seed experiment shows <1% accuracy variance)
- Medium Confidence: ArxivRoll effectively quantifies overestimation (supported by empirical RSI results but depends on untested assumptions)
- Low Confidence: ArxivRoll establishes new standard for realistic LLM capability assessment (requires longer-term validation)

## Next Checks
1. Benchmark equivalence validation: Compare uncontaminated model performance on ArxivRollBench vs public benchmarks to confirm RSI ≈ 1.0
2. Contamination sensitivity test: Fine-tune a base model on ArxivRollBench2024b papers and verify proportional RSI increase
3. Long-term correlation stability: Track ArxivRollBench correlations with ChatbotArena across multiple 6-month releases