---
ver: rpa2
title: Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended
  Interactions
arxiv_id: '2512.12775'
source_url: https://arxiv.org/abs/2512.12775
tags:
- persona
- your
- what
- dialogue
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an evaluation protocol for assessing persona-assigned
  large language models (LLMs) in long dialogues, addressing the gap in understanding
  how persona fidelity, instruction-following, and safety degrade over extended interactions.
  The protocol combines long persona dialogues (over 100 rounds) with evaluation datasets
  to create dialogue-conditioned benchmarks.
---

# Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions

## Quick Facts
- arXiv ID: 2512.12775
- Source URL: https://arxiv.org/abs/2512.12775
- Authors: Pedro Henrique Luz de Araujo; Michael A. Hedderich; Ali Modarressi; Hinrich Schuetze; Benjamin Roth
- Reference count: 40
- Key outcome: Persona-assigned LLMs degrade over long dialogues, with fidelity loss, instruction-following trade-offs, and safety convergence toward baselines, especially in goal-oriented conversations.

## Executive Summary
This study introduces a systematic evaluation protocol for assessing how persona-assigned large language models (LLMs) maintain role fidelity, follow instructions, and exhibit safety behaviors over extended dialogues (100+ rounds). The protocol creates dialogue-conditioned benchmarks by generating long persona dialogues and evaluating model responses at multiple truncation points. Experiments with seven state-of-the-art LLMs reveal that persona fidelity degrades significantly over time, particularly in goal-oriented conversations, with models reverting toward baseline behavior. A measurable trade-off exists between persona adherence and instruction-following accuracy, with non-persona baselines initially outperforming persona-assigned models. Scaling mitigates but does not eliminate these issues, highlighting the fragility of persona applications in extended interactions.

## Method Summary
The study generates 102-round dialogues per model-persona-type combination, using either persona-directed interview queries or goal-oriented PRISM queries. For each dialogue, ten evenly spaced prefixes are extracted to create dialogue-conditioned datasets. These prefixes are prepended to evaluation queries from multiple datasets (IFBench, BFI, XSTest, etc.) to assess persona fidelity, instruction-following, and safety at different dialogue depths. Responses are scored using an LLM-as-a-Judge (Atla Selene Mini) with provided rubrics. The protocol is applied to seven LLMs across different scales, comparing persona-assigned versus no-persona baseline conditions.

## Key Results
- Persona fidelity degrades over dialogue course, with 41% reduction in persona-specific patterns by final round
- Goal-oriented dialogues show faster degradation than persona-directed dialogues due to competing objectives
- A trade-off exists between persona fidelity and instruction-following, with persona-assigned models initially underperforming baselines
- Models converge toward baseline behavior patterns as dialogues progress, especially on safety and refusal behaviors
- Scaling improves but does not eliminate degradation, with larger models showing only marginally better retention

## Why This Works (Mechanism)

### Mechanism 1: Context Dilution Drives Reversion to Baseline
Persona fidelity degrades because accumulating dialogue history dilutes the conditioning effect of the persona prompt, causing models to revert toward pretrained behavior patterns. The persona is typically injected once as a system message, and as dialogue history grows (100+ rounds), the relative attention weight to the persona description diminishes compared to recent context, allowing strong pretrained priors to dominate. This mechanism assumes the persona prompt does not receive privileged positional or attentional treatment in standard transformer architectures.

### Mechanism 2: Fidelity–Instruction Trade-off via Capacity Competition
Persona adherence competes with instruction-following capacity, producing a measurable trade-off where persona-assigned models initially underperform baselines on general instructions. The model allocates representational capacity to maintain stylistic, knowledge, and behavioral constraints of the persona, which reduces flexibility for precise constraint satisfaction on general tasks. This assumes instruction-following and persona maintenance draw from a shared attention/representation budget rather than fully modular pathways.

### Mechanism 3: Dialogue Type Modulates Degradation Rate
Goal-oriented dialogues accelerate persona degradation compared to persona-directed dialogues because task diversity introduces competing objectives and distractors. Persona-directed dialogues maintain thematic consistency (interview-style queries about the character), reinforcing persona patterns, while goal-oriented dialogues span heterogeneous tasks (travel planning, cooking, coding), each pulling the model toward task-optimal behavior that may conflict with persona constraints. This assumes thematic consistency in dialogue history reinforces persona activation while topic shifts disrupt it.

## Foundational Learning

- **Concept: Long-context attention degradation**
  - Why needed here: The paper's core finding depends on understanding why models "forget" early context (the persona prompt) as dialogue length increases.
  - Quick check question: Can you explain why attention to early positions in a 100-round dialogue is weaker than to recent turns, even with relative position encodings?

- **Concept: Persona conditioning via system prompts**
  - Why needed here: All experiments assume personas are assigned via a single system message; understanding this design choice is critical for interpreting results and proposing interventions.
  - Quick check question: What happens if the persona prompt is repeated every N turns instead of only at initialization?

- **Concept: LLM-as-a-Judge evaluation**
  - Why needed here: The paper relies on Atla Selene Mini for automated scoring; understanding judge reliability and rubric design is necessary to assess result validity.
  - Quick check question: What validation step did the authors perform to ensure the judge model aligned with human ratings?

## Architecture Onboarding

- **Component map:** GPT-4o (dialogue generator) -> 7 LLMs (target models) -> Atla Selene Mini (evaluator)
- **Critical path:** 1) Assign persona via system message 2) Generate 102-round dialogue 3) At each truncation point, prepend dialogue prefix to evaluation queries 4) Score responses and compare persona vs. baseline
- **Design tradeoffs:** Synthetic dialogues ensure control but may miss real-user complexity; LLM-as-a-Judge provides scale but requires human validation; fixed persona prompt is simple but untested periodic re-injection alternatives
- **Failure signatures:** Persona drift (41% reduction in persona-specific patterns), convergence to baseline behavior, excess safety increase in persona-directed dialogues
- **First 3 experiments:** 1) Baseline replication on single model-persona pair to verify degradation curves 2) Persona re-injection ablation (repeat every 25 rounds) 3) Thematic clustering test (grouped topics vs. shuffled queries)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can retrieval-augmented mechanisms or selective history management mitigate the persona fidelity degradation observed in goal-oriented dialogues? The authors suggest exploring mechanisms that actively support sustained persona behavior, such as retrieving only the most relevant dialogue content, but do not implement or test specific solutions.

- **Open Question 2:** Does persona fidelity degrade at similar rates for real-world professional personas compared to the fictional characters used in this study? The paper notes its focus on fictional characters and suggests future work should apply the protocol to domain-specific personas to explore application-specific challenges.

- **Open Question 3:** Is the convergence of persona-assigned models toward baseline safety behavior a result of specific instruction-following conflicts or fundamental attention dilution over long contexts? The paper observes safety behavior convergence but does not isolate whether this is caused by forgetting the persona (attention issue) or prioritizing task completion over persona constraints (objective conflict).

## Limitations

- Synthetic dialogues may not capture real-world complexity and variability of extended interactions
- LLM-as-a-Judge evaluation introduces potential systematic biases, with some metrics showing low human agreement (κ=0.12 for general instructions)
- Single persona injection at initialization without testing alternative conditioning strategies (periodic re-injection, positional anchoring)

## Confidence

- **High Confidence:** Core finding that persona fidelity degrades over extended dialogues, consistently observed across multiple metrics and models
- **Medium Confidence:** Trade-off between persona fidelity and instruction-following demonstrated through controlled experiments, though causal mechanism remains inferential
- **Low Confidence:** Claim that thematic consistency protects persona fidelity is primarily theoretical, based on observed differences rather than direct testing

## Next Checks

1. **Human evaluation validation:** Conduct human evaluations on 100 dialogue-response pairs from each degradation curve (early, middle, late rounds) to verify LLM-as-a-Judge scores align with human judgments on persona consistency, instruction-following accuracy, and safety appropriateness.

2. **Alternative persona conditioning experiments:** Implement and test two strategies: (a) re-injecting the persona prompt every 25 dialogue rounds, and (b) using dedicated attention heads for persistent persona priority. Compare fidelity retention against baseline single-injection condition.

3. **Real-world dialogue extension:** Extend evaluation protocol to include 2-3 real extended dialogues (50+ rounds) from existing persona-chat datasets or collected human interactions. Compare degradation patterns between synthetic and real dialogues to assess ecological validity.