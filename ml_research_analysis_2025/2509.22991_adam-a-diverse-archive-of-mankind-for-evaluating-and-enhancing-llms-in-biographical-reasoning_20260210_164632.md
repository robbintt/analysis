---
ver: rpa2
title: 'ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical
  Reasoning'
arxiv_id: '2509.22991'
source_url: https://arxiv.org/abs/2509.22991
tags:
- biographical
- reasoning
- retrieval
- multimodal
- individuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ADAM, the first framework to systematically\
  \ evaluate multimodal LLMs in biographical reasoning. ADAM includes AdamDB, a large-scale\
  \ multilingual dataset of over 4 million individuals; AdamBench, a cognitively structured\
  \ benchmark based on Bloom\u2019s taxonomy; and AdamRAG, a retrieval-augmented system\
  \ to mitigate hallucinations."
---

# ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning

## Quick Facts
- arXiv ID: 2509.22991
- Source URL: https://arxiv.org/abs/2509.22991
- Reference count: 11
- Primary result: ADAM introduces the first framework to systematically evaluate multimodal LLMs in biographical reasoning, showing that retrieval augmentation substantially reduces hallucinations and improves accuracy across cognitive levels.

## Executive Summary
ADAM introduces a comprehensive framework for evaluating and enhancing multimodal large language models (MLLMs) in biographical reasoning. The framework comprises three components: AdamDB (a 4+ million record multilingual dataset), AdamBench (a cognitively structured benchmark using Bloom's taxonomy), and AdamRAG (a retrieval-augmented system). ADAM addresses critical gaps in evaluating MLLMs beyond generic benchmarks by focusing on biographical reasoning across six cognitive levels, multiple languages, and multimodal inputs. Experiments demonstrate that retrieval substantially improves factual accuracy, particularly for lesser-known individuals, while multimodal input offers smaller gains than retrieval.

## Method Summary
ADAM builds on WikiDBS and Wikidata to create AdamDB through NER filtering for PERSON entities, merging via name mappings, deduplication using Wikidata Q-Ids, and popularity scoring via Wikipedia page views. AdamBench samples 1,650 individuals using popularity-stratified k-means clustering and generates Bloom-taxonomy questions in English and native languages. AdamRAG implements retrieval using LaBSE embeddings for text and face embeddings for images, with multi-stage filtering by nationality and birth date. Evaluation uses accuracy metrics stratified by cognitive level, popularity, language, and modality.

## Key Results
- Retrieval augmentation substantially improves open-source model accuracy (e.g., Qwen2.5-7b from sub-40% to well above 70% on mid/high-popularity individuals)
- Popularity strongly mediates accuracy, with closed-source models performing best overall
- Multimodal input (face images) provides modest gains compared to retrieval
- Lower-order Bloom levels (Remembering, Understanding) show highest accuracies; higher-order levels (Evaluating, Creating) expose model weaknesses
- Cross-lingual performance degrades significantly, with non-English questions performing worse than English

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Grounded Generation Reduces Hallucination for Long-Tail Entities
Retrieving structured biographical records before generation substantially improves factual accuracy, particularly for lesser-known individuals. AdamRAG retrieves context from AdamDB (4M+ structured records) and injects it into the prompt, anchoring generation in verified data rather than relying solely on parametric knowledge. Core assumption: Parametric knowledge is sparse or noisy for low-popularity entities; external retrieval compensates for this gap. Evidence anchors: [abstract] "AdamRAG substantially improves open-source models and modestly benefits closed-source ones"; [section 4.3] "Qwen2.5-7b improves from sub-40% to well above 70% on mid- and high-popularity individuals" with retrieval.

### Mechanism 2: Popularity-Weighted Retrieval Prioritizes Canonical Sources
Weighting retrieval by Wikipedia engagement metrics improves accuracy by surfacing more reliable, well-documented records. AdamRAG incorporates popularity-weighted search using page view metrics, helping disambiguate common names and surface the most referenced individual. Core assumption: Higher Wikipedia engagement correlates with more complete and accurate biographical records. Evidence anchors: [abstract] "Popularity strongly mediates accuracy"; [section 3.2] "To quantify popularity, annual page views of each subject's English Wikipedia entry (2024) are recorded."

### Mechanism 3: Cognitive Stratification via Bloom's Taxonomy Reveals Model Gaps
Structuring evaluation across Bloom's taxonomy exposes systematic weaknesses in higher-order reasoning versus factual recall. AdamBench generates questions at six cognitive levels (Remembering through Creating), enabling fine-grained diagnosis of where models fail. Core assumption: Bloom's taxonomy validly transfers from educational psychology to LLM evaluation. Evidence anchors: [abstract] "AdamBench provides cognitively structured evaluations based on Bloom's taxonomy, spanning six reasoning levels"; [section 4.1] "Lower-order levels such as Remembering and Understanding exhibit the highest accuracies... higher-order levels such as Evaluating and Creating expose more pronounced weaknesses."

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: AdamRAG is the core intervention for hallucination reduction; understanding retrieval-then-generation pipelines is essential.
  - Quick check question: Can you explain how retrieved context is merged with the user query before LLM generation?

- **Concept: Bloom's Taxonomy**
  - Why needed here: AdamBench structures all evaluation around this cognitive hierarchy; misinterpretation leads to flawed benchmarking.
  - Quick check question: Can you distinguish "Understanding" from "Applying" in the context of biographical questions?

- **Concept: Cross-Lingual Entity Linking**
  - Why needed here: AdamDB integrates nearly 600 languages; entity resolution across scripts and aliases is non-trivial.
  - Quick check question: How would you link "Горбачёв" (Russian) to "Gorbachev" (English) in a shared embedding space?

## Architecture Onboarding

- **Component map**: WikiDBS -> NER filtering -> person-centric records -> Wikidata alignment -> multilingual name extraction -> popularity scoring -> clustering -> benchmark question generation; Query -> exact match/LaBSE semantic retrieval -> nationality/birth-date filters -> top candidate selection -> context injection -> LLM generation

- **Critical path**: 
  1. Data pipeline: WikiDBS → NER filtering → person-centric records → Wikidata alignment → multilingual name extraction → popularity scoring → clustering → benchmark question generation
  2. Retrieval path: Query → exact match/LaBSE semantic retrieval → nationality/birth-date filters → top candidate selection → context injection → LLM generation
  3. Evaluation path: AdamBench questions → lm-eval-harness or Khayyam Challenge → accuracy reporting stratified by cognitive level, popularity, language, and modality

- **Design tradeoffs**:
  - Coverage vs. quality: Automatic pipeline scales to 4M records but may include noise from NER errors or unverified Wikipedia content
  - Retrieval vs. parametric knowledge: RAG improves accuracy but adds latency and dependency on external database freshness
  - Multimodality vs. consistency: Face image input provides modest gains but can introduce noise if not properly aligned with textual context

- **Failure signatures**:
  - Low accuracy on low-popularity entities even with RAG → AdamDB lacks coverage for that individual
  - Cross-lingual mismatch → LaBSE embeddings fail to link multilingual aliases; consider adding transliteration or alias tables
  - Image retrieval returns wrong person → Face embeddings insufficiently discriminative; need stricter nationality/temporal filters or face verification

- **First 3 experiments**:
  1. Baseline probe: Evaluate a mid-sized open-source model (e.g., Qwen2.5-7b) on AdamBench without RAG to establish parametric knowledge gaps across Bloom levels
  2. RAG ablation: Add AdamRAG and measure accuracy gains stratified by popularity tier; identify where retrieval helps most
  3. Cross-lingual stress test: Compare English vs. native-language question accuracy with and without RAG to quantify linguistic grounding effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal fusion strategies be refined to provide consistent, substantial performance improvements for biographical reasoning?
- Basis in paper: [explicit] The conclusion states future work should "refine multimodal fusion strategies," noting that face images yielded smaller, inconsistent gains compared to retrieval.
- Why unresolved: Current results show visual inputs offer limited value, sometimes introducing noise, rather than robustly augmenting textual reasoning.
- What evidence would resolve it: Demonstrating architectures where face images significantly boost accuracy across diverse popularity tiers and cognitive levels.

### Open Question 2
- Question: Can fairness-aware evaluation protocols and retrieval augmentation fully eliminate the popularity bias observed in biographical reasoning?
- Basis in paper: [explicit] The authors note "fairness concerns must be addressed" and that retrieval "alleviates, but does not fully eliminate, this disparity" for lesser-known individuals.
- Why unresolved: Models consistently underperform on low-popularity individuals compared to famous ones, indicating a reliance on pretraining exposure over factual grounding.
- What evidence would resolve it: A system achieving statistically equivalent accuracy for low and high-popularity individuals on AdamBench.

### Open Question 3
- Question: What specific techniques are required to improve model performance on higher-order cognitive tasks (Evaluating, Creating) where current RAG benefits are limited?
- Basis in paper: [inferred] Results show RAG gains are largest on lower-order reasoning; higher-order tasks remain challenging with "persistent abstraction gaps" even in state-of-the-art models.
- Why unresolved: Retrieval effectively aids fact-checking (Remembering) but fails to bridge the gap for complex synthesis or evaluation of biographical impacts.
- What evidence would resolve it: New methods showing significantly improved scores specifically in the "Evaluating" and "Creating" tiers of Bloom's taxonomy in AdamBench.

## Limitations

- Entity disambiguation across multilingual records may introduce noise despite Wikidata alignment
- Question generation quality depends heavily on LLM prompting, which is not fully specified
- Popularity-weighted retrieval assumes Wikipedia traffic correlates with biographical accuracy
- Cross-lingual question quality varies significantly by language, with non-English performance degrading

## Confidence

- **High confidence**: Retrieval improves accuracy for lesser-known individuals; popularity strongly affects performance; multimodal input provides modest gains
- **Medium confidence**: Bloom's taxonomy effectively stratifies cognitive difficulty; RAG benefits are consistent across open/closed models
- **Low confidence**: Exact magnitude of improvement varies by implementation details; cross-lingual robustness claims need more validation

## Next Checks

1. **Replication stress test**: Evaluate model performance on ADAM without RAG, then with RAG, stratified by popularity tiers to verify the claimed improvement curves
2. **Cross-lingual consistency audit**: Manually sample 50 native-language questions to assess semantic equivalence with English versions and verify translation quality
3. **Hallucination measurement**: Compare factuality scores (e.g., using FactScore or similar) between RAG and non-RAG conditions on a held-out test set