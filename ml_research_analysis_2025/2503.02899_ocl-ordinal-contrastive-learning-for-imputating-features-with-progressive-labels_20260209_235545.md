---
ver: rpa2
title: 'OCL: Ordinal Contrastive Learning for Imputating Features with Progressive
  Labels'
arxiv_id: '2503.02899'
source_url: https://arxiv.org/abs/2503.02899
tags:
- imputation
- learning
- disease
- imaging
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ordinal Contrastive Learning (OCL) for imputing
  missing neuroimaging features in Alzheimer's disease research. The authors address
  the challenge of incomplete multi-modal imaging data by proposing a framework that
  maps data into a modality-agnostic embedding space aligned with disease progression.
---

# OCL: Ordinal Contrastive Learning for Imputating Features with Progressive Labels

## Quick Facts
- **arXiv ID**: 2503.02899
- **Source URL**: https://arxiv.org/abs/2503.02899
- **Reference count**: 30
- **Primary result**: OCL framework improves imputation of missing neuroimaging features, boosting downstream AD classification accuracy from 67.3% to 82.9% on ADNI dataset.

## Executive Summary
This paper introduces Ordinal Contrastive Learning (OCL) for imputing missing neuroimaging features in Alzheimer's disease research. The authors address the challenge of incomplete multi-modal imaging data by proposing a framework that maps data into a modality-agnostic embedding space aligned with disease progression. Their method uses domain adversarial training to remove modality-specific information, maximizes coherence between modalities from the same subject, and introduces OCL to align samples based on disease severity. Experiments on the ADNI dataset show their approach improves statistical sensitivity in group comparisons, detecting more significant regions of interest. The method also enhances downstream classification performance, achieving 82.9% accuracy compared to 67.3% without imputation, outperforming multiple baseline methods including MICE, MissForest, and GAIN.

## Method Summary
The OCL framework consists of an encoder that maps ROI features from different modalities into a shared embedding space, a domain adversarial component that removes modality-specific information, and a conditional decoder that can reconstruct features for any modality. The method employs three key loss functions: domain adversarial loss to create modality-agnostic embeddings, modality coherence loss to align embeddings from the same subject, and ordinal contrastive loss to structure the embedding space according to disease progression. At inference, the encoder generates embeddings from available modalities, which the decoder then uses to impute missing modalities based on conditioning vectors.

## Key Results
- Imputation with OCL improves downstream AD classification accuracy from 67.3% to 82.9%
- OCL detects 3× more significant ROIs in group comparisons than baselines
- Outperforms multiple baseline methods including MICE, MissForest, and GAIN
- Creates continuous embedding trajectories that better reflect disease progression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring the embedding space to respect the ordinal progression of disease severity (CN → EMCI → LMCI → AD) creates a more continuous and informative representation for data imputation than standard classification.
- Mechanism: Standard Supervised Contrastive Learning (SCL) repels all negative samples equally. OCL modifies the loss function (Equation 3) by introducing an adaptive temperature $\tau_{i,n}$ inversely proportional to the label distance $d(y_i, y_n)$. This forces embeddings of distinct classes (e.g., CN vs. AD) further apart than adjacent classes (e.g., CN vs. EMCI), creating a trajectory in the latent space that mirrors disease pathology.
- Core assumption: The biological progression of Alzheimer's Disease maps linearly to the diagnostic labels, and intermediate stages share features that can be interpolated.
- Evidence anchors:
  - [abstract] "...ordinal contrastive loss, which aligns samples in the embedding space according to the progression of AD."
  - [section 2.1] "Therefore, we make $\tau_{i,n}$ dependent on $y_{i,\cdot}$ and $y_{n,\cdot}$ as $\tau / d(i, n)$ to penalize greater label distance."
  - [corpus] Related work (arXiv:2503.10440) supports the utility of learning disease states from ordinal labels, though primarily in noisy label contexts.
- Break condition: If the disease progression is non-linear or if diagnostic labels are highly noisy (subjects fluctuating between stages), the rigid ordinal spacing could force embeddings into incorrect geometric configurations, degrading imputation.

### Mechanism 2
- Claim: Decoupling modality-specific information from disease progression information allows the encoder to generalize across missing input types.
- Mechanism: A Gradient Reversal Layer (GRL) is inserted between the encoder and a modality classifier ($C_{DC}$). During backpropagation, the encoder is penalized if it retains features that allow the classifier to identify the input modality (MRI vs. PET), effectively forcing the embedding to retain only "modality-agnostic" disease features.
- Core assumption: A shared anatomical/pathological reality exists across imaging modalities that can be captured in a single vector, distinct from the imaging modality's "style."
- Evidence anchors:
  - [abstract] "...domain adversarial training to remove modality-specific information..."
  - [section 2.1] "...E is trained to maximize $L_{DA}$, which leads to the discarding of modality-specific information..."
  - [corpus] "Modality-Agnostic Style Transfer" (arXiv:2503.02898) validates the general concept of disentangling content/style for imputation.
- Break condition: If the modalities measure fundamentally unrelated pathological processes (low correlation), stripping modality info may remove the signal entirely, resulting in a generic, blurry embedding.

### Mechanism 3
- Claim: A unified decoder can reconstruct any target modality from a shared embedding if conditioned on the target modality type, enabling one-to-many translation.
- Mechanism: The decoder takes the modality-agnostic embedding $z$ concatenated with a one-hot modality condition vector $c_t$. It is trained to reconstruct the input $x_{k,t}$ from $z$. Because $z$ is modality-agnostic (via Mechanism 2), at inference time, $z$ derived from Modality A can be fed to the decoder with condition $c_B$ to generate Modality B.
- Core assumption: The encoder captures sufficient "personalized" disease information to reconstruct specific quantitative measures (ROIs) for a different modality solely from the condition.
- Evidence anchors:
  - [abstract] "...A decoder to reconstruct the original measures conditioned on their imaging modalities."
  - [section 2.2] "Thus, the decoder D is tasked with estimating values of the target modality from a given embedding under a condition."
  - [corpus] Weak explicit corpus validation for this specific decoder architecture; relies on general principles of conditional generation.
- Break condition: If the condition vector is too weak to specify the complex output distribution, or if the embedding compresses information too aggressively (information bottleneck), the decoder will output average-looking subjects rather than personalized imputations.

## Foundational Learning

**Supervised Contrastive Learning (SCL)**
- Why needed here: OCL is a modification of SCL. You must understand how SCL creates clusters of same-class samples and separates different-class samples using InfoNCE loss to grasp how OCL modifies these "push/pull" forces based on ordinality.
- Quick check question: How does the gradient signal differ in SCL when comparing two samples of the same class vs. different classes?

**Domain Adversarial Training (DANN)**
- Why needed here: This is the "modality remover." Understanding the saddle point problem (minimizing classification loss while maximizing domain loss) is critical for debugging why the encoder might be failing to merge modalities.
- Quick check question: What happens to the encoder gradients if the domain discriminator becomes too strong or too weak?

**Conditional Variational Autoencoders (cVAE) / Conditioning**
- Why needed here: The decoder is not a generic generator; it is conditioned. Understanding how concatenating a one-hot vector acts as a "switch" for the decoder is necessary.
- Quick check question: In this architecture, does the condition vector control the *style* of the output or the *content*?

## Architecture Onboarding
- **Component map**: Input ROI features -> Encoder (2-layer MLP) -> Embedding $z$ -> Domain Critic + Projection Head -> Decoder (2-layer MLP with condition)
- **Critical path**: Input → Encoder → (Adversarial Loss + Ordinal Contrastive Loss + Modality Coherence Loss). The encoder must learn to align modalities *before* the decoder can effectively learn to translate them.
- **Design tradeoffs**: The paper uses a simple MLP architecture (2 layers) on ROI-level features rather than voxels. This reduces computational cost but discards spatial texture information, limiting the method to region-based summaries.
- **Failure signatures**:
  - **Mode Collapse**: If $L_{MC}$ (coherence) fails, embeddings from different modalities of the same subject may map to distinct clusters, causing the decoder to output noise when cross-translating.
  - **Over-regularization**: If domain adversarial loss is too strong, the encoder might produce a zero-vector (constant) to satisfy the "modality-agnostic" constraint, killing all information.
- **First 3 experiments**:
  1. **t-SNE Visualization**: Train E with cross-entropy vs. SCL vs. OCL. Plot embeddings colored by label. Verify that OCL creates a "stream" or line (CN → AD) rather than distinct blobs.
  2. **Modality Ablation**: Train with $L_{DA}$ ON vs. OFF. Measure the classification accuracy of a separate modality classifier on the resulting embeddings to confirm modality information is actually being removed.
  3. **Reconstruction Consistency**: Input a subject's MRI, encode it, decode to PET. Then take that *generated* PET, encode it, and decode back to MRI. Check the cycle consistency error (L2 distance between original and cycled MRI).

## Open Questions the Paper Calls Out
None

## Limitations
- **Limited population diversity**: Validated on ADNI dataset, primarily Caucasian participants from North America, limiting generalizability to diverse populations.
- **Label quality assumption**: Assumes ordinal labels accurately reflect true disease progression, but clinical diagnoses can be noisy with patients transitioning between stages.
- **ROI-level abstraction**: Operating on region-of-interest features rather than raw imaging data discards spatial information and texture patterns that may be clinically relevant.

## Confidence
- **High confidence**: The core mechanism of OCL modifying temperature based on label distance (Mechanism 1)
- **Medium confidence**: The effectiveness of domain adversarial training for modality-agnostic embedding (Mechanism 2)
- **Medium confidence**: The conditional decoder's ability to perform cross-modal imputation (Mechanism 3)

## Next Checks
1. **Out-of-distribution testing**: Evaluate the imputation performance on an independent cohort with different demographic characteristics (age, ethnicity, geographic location) to assess generalizability.

2. **Label noise robustness**: Systematically introduce label noise (e.g., randomly mislabeling 10-30% of samples) and measure degradation in imputation quality and downstream classification accuracy.

3. **Spatial information ablation**: Compare performance using ROI-level features versus voxel-level features to quantify the impact of spatial information loss on both imputation accuracy and downstream clinical utility.