---
ver: rpa2
title: A Theorem-Proving-Based Evaluation of Neural Semantic Parsing
arxiv_id: '2510.11225'
source_url: https://arxiv.org/abs/2510.11225
tags:
- semantic
- logical
- language
- parsing
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates neural semantic parsers using automated theorem
  proving alongside graph-matching, revealing that high graph-matching scores do not
  guarantee logical equivalence. Target normalization via prenex form improves well-formedness
  and logical adequacy.
---

# A Theorem-Proving-Based Evaluation of Neural Semantic Parsing

## Quick Facts
- **arXiv ID:** 2510.11225
- **Source URL:** https://arxiv.org/abs/2510.11225
- **Reference count:** 15
- **Primary result:** High graph-matching scores do not guarantee logical equivalence; theorem-proving reveals systematic gaps in neural semantic parser adequacy.

## Executive Summary
This paper evaluates neural semantic parsers by comparing graph-matching metrics (Dmatch) with automated theorem proving (ATP) to test logical equivalence between predicted and gold logical forms. The authors find that parsers performing well on graph-matching often fail to produce logically equivalent formulas, with dominant errors in variable binding and predicate naming. Target normalization via prenex form improves well-formedness and logical adequacy. Error analysis reveals performance degradation with formula complexity and syntactic phenomena like coordination, prepositional phrases, and passive voice.

## Method Summary
The study uses the SICK dataset (2,392 train, 2,580 test pairs) where sentences are mapped to first-order logic event-semantics formulas via ccg2lambda and depccg. Two target formats are compared: raw ccg2lambda outputs and prenex-normalized formulas. Models include T5-Small/Base (fine-tuned with SFT) and GPT-4o/4.1/5 (zero-shot ICL). Evaluation uses both graph-matching (Dmatch F1 via Counter on DRS) and theorem proving (bidirectional entailment with Vampire). Well-formedness is measured by Non-WFF Ratio.

## Key Results
- T5-Base achieves Dmatch F1 of 0.873 (raw) but Prover Accuracy of only 0.634; prenex normalization improves Prover Accuracy to 0.689 while reducing Non-WFF Ratio from 0.031 to 0.007
- High Dmatch scores do not guarantee logical equivalence; systematic gaps exist between surface overlap and semantic correctness
- Performance degrades with increasing formula complexity (quantifier/conjunction/negation count) and with coordination, prepositional phrases, and passive voice
- Dominant failure modes are quantifier-count mismatches (48.9%) and predicate-name errors (32.9%)

## Why This Works (Mechanism)

### Mechanism 1: Prenex Normalization Reduces Target-Side Incidental Variability
- **Claim:** Converting first-order logic formulas to prenex normal form (quantifiers at prefix with standardized variable naming) is associated with higher Prover Accuracy and lower non-well-formed output rates across both SFT and ICL settings.
- **Mechanism:** Normalization reduces spurious surface variation (quantifier position, variable indices, underscore prefixes) that confounds sequence models during training. By presenting a deterministic canonical form, the model learns a more consistent mapping from natural language to logical structure rather than memorizing incidental notation patterns.
- **Core assumption:** The performance gains stem from reduced learning complexity rather than from prenex form being inherently more aligned with model architecture.
- **Evidence anchors:**
  - [abstract] "Normalization reduces incidental target variability, improves well-formedness, and strengthens logical adequacy."
  - [Section 4.1] T5-Base Prover Accuracy improves from 0.634 (raw) to 0.689 (prenex); Non-WFF Ratio drops from 0.031 to 0.007.
  - [corpus] Neighbor papers on semantic parsing (e.g., ReCOGS) similarly note that minor surface differences in logically equivalent forms substantially affect evaluation, though they do not employ theorem provers.
- **Break condition:** If target formulas require non-prenex representations to preserve scopal ambiguity or if downstream reasoning tasks depend on quantifier ordering being faithful to surface word order, prenex normalization could obscure semantically relevant distinctions.

### Mechanism 2: Theorem-Proving Evaluation Exposes Logical Adequacy Gaps Hidden by Graph-Matching
- **Claim:** High graph-matching (Dmatch) scores do not guarantee logical equivalence; theorem-prover bidirectional entailment testing reveals systematic misalignment between surface overlap and semantic correctness.
- **Mechanism:** Dmatch computes clause-level F-scores over Discourse Representation Structures after alignment, which rewards structural similarity even when predictions are not logically equivalent (e.g., over-specification, argument-role swaps). Automated theorem proving (Vampire) tests whether gold entails prediction and prediction entails gold, capturing equivalence under variable renaming, conjunct permutation, and richer logical transformations.
- **Core assumption:** The prover's notion of equivalence (first-order logic with standard axioms) is the appropriate ground truth for semantic parsing intended to support downstream inference.
- **Evidence anchors:**
  - [abstract] "models performing well on graph-matching often fail to produce logically equivalent formulas"
  - [Section 4.1] T5-Base achieves Dmatch F1 of 0.873 (raw) but Prover Accuracy of only 0.634; the gap persists with prenex normalization (0.880 vs. 0.689).
  - [Section 3.5] Provides concrete examples: pairs with identical Dmatch scores (0.5) can reflect either proper entailment or complete semantic role mismatch.
  - [corpus] No neighbor papers in this corpus directly employ theorem provers for semantic parser evaluation; the gap this paper identifies remains under-explored in related work.
- **Break condition:** If the theorem prover times out or fails on formulas that are nonetheless logically correct, or if axioms are insufficient for the domain, prover-based accuracy underestimates true logical adequacy.

### Mechanism 3: Semantic Complexity and Syntactic Phenomena Degrade Parser Performance via Binding and Naming Errors
- **Claim:** Parser performance (Prover Accuracy) declines with increasing formula complexity (quantifier/conjunction/negation count) and in the presence of coordination, prepositional phrases, and passive voice; dominant failure modes are quantifier-count mismatches and predicate-name errors.
- **Mechanism:** Complex formulas require precise variable binding across more constituents; coordination introduces scope ambiguity and multiple event structures; PPs demand correct attachment and relational predicate selection; passive voice requires argument-structure inversion. Sequence-to-sequence models, lacking explicit symbolic variable management, produce well-formed but semantically incorrect outputs—most commonly by introducing wrong numbers of quantified variables or mislabeling predicates.
- **Core assumption:** These error patterns reflect fundamental limitations in how current neural architectures handle compositional semantic structure, not merely insufficient training data.
- **Evidence anchors:**
  - [abstract] "Error analysis shows performance degrades with increasing formula complexity and with coordination, prepositional phrases, and passive voice; the dominant failures involve variable binding and indexing, and predicate naming."
  - [Section 5.1, Figure 1] Prover Accuracy decreases across six complexity bins for both T5-Base and GPT-5.
  - [Section 5.3, Figure 3] Quantifier-count mismatch accounts for 48.9% of prover-failed predictions; predicate-name errors 32.9%.
  - [corpus] Related work on compositional generalization (COGS, ReCOGS, SLOG) documents similar brittleness to syntactic variation, though with exact-match or graph-based metrics rather than theorem proving.
- **Break condition:** If training data were enriched with targeted examples for each phenomenon, the observed degradation might reduce without architectural changes. Error patterns may also shift with much larger models not evaluated here.

## Foundational Learning

- **Concept: First-Order Logic (FOL) and Prenex Normal Form**
  - **Why needed here:** The paper evaluates parsers by converting sentences to FOL event-semantics representations; prenex normalization is a key intervention. Understanding quantifier scope, variable binding, and canonical forms is necessary to interpret results and error types.
  - **Quick check question:** Given `∃x.(P(x) ∧ ∃y.Q(y))`, can you rewrite it in prenex form and explain why the transformation is equivalence-preserving?

- **Concept: Automated Theorem Proving (ATP) and Bidirectional Entailment**
  - **Why needed here:** The core evaluation method uses a theorem prover (Vampire) to test whether gold and predicted formulas mutually entail each other. Understanding ATP basics helps interpret what "Prover Accuracy" measures and its limitations.
  - **Quick check question:** If formula A entails B and B entails A, what is their logical relationship? What does it mean if the prover returns "unknown" or times out?

- **Concept: Sequence-to-Sequence Models for Semantic Parsing**
  - **Why needed here:** The paper fine-tunes T5 models and uses GPT ICL; both treat logical form generation as text-to-text. Understanding seq2seq limitations (variable management, compositional generalization) clarifies why normalization helps and where errors concentrate.
  - **Quick check question:** Why might a seq2seq model produce a well-formed formula with the wrong number of quantified variables, and how does this differ from a symbolic parser's failure modes?

## Architecture Onboarding

- **Component map:** Natural language sentences -> T5/GPT models -> FOL event-semantics formulas -> DRS conversion -> Dmatch evaluation OR Theorem prover (Vampire) -> Prover Accuracy evaluation

- **Critical path:**
  1. Dataset preparation: Filter SICK pairs where prover judgment matches gold label; pair sentences with ccg2lambda-derived formulas
  2. Target normalization: Apply prenex transformation (quantifier fronting, variable reindexing, predicate cleaning)
  3. Training/inference: SFT (T5) or few-shot prompting (GPT)
  4. Dual evaluation: Compute Dmatch F1 and Prover Accuracy; analyze error breakdown by complexity and syntactic category

- **Design tradeoffs:**
  - Raw vs. prenex targets: Raw preserves original structure but introduces incidental variability; prenex improves learnability but may obscure scopal distinctions
  - SFT vs. ICL: SFT yields higher Prover Accuracy and exact match; ICL provides strong well-formedness with less engineering but lower semantic alignment
  - Evaluation choice: Dmatch is fast and interpretable but insensitive to logical equivalence; ATP is precise but computationally heavier and dependent on axiom coverage

- **Failure signatures:**
  - **High Dmatch, low Prover Accuracy:** Model captures surface structure but misrepresents logical content (e.g., wrong quantifier count, argument-role swap)
  - **Non-WFF outputs:** Model fails syntactic validity (unbalanced parentheses, malformed quantifiers)—more frequent with raw targets and smaller models
  - **Complexity-scaled degradation:** Accuracy drops predictably with formula complexity; if not observed, check data filtering or evaluation pipeline

- **First 3 experiments:**
  1. **Reproduce the SFT gap:** Train T5-Base on raw vs. prenex targets; verify that Prover Accuracy improves (target ~0.63 → ~0.69) while Non-WFF Ratio decreases. This confirms the normalization mechanism in your setup.
  2. **Stratified error analysis:** On the best checkpoint, select all well-formed but prover-failed predictions; classify errors (quantifier count, predicate name, argument role, missing subformula) using automated labeling or manual review. Verify that quantifier-count and predicate-name errors dominate (~80% combined).
  3. **Phenomenon ablation:** Evaluate Prover Accuracy on subsets annotated for coordination, PPs, and passive voice; confirm that coordination causes the largest drop. Then test whether adding 50-100 targeted training examples per phenomenon reduces the gap, informing data-augmentation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would a hybrid pipeline combining ICL as a candidate generator with a fine-tuned parser or prover-guided reranker narrow the gap between well-formedness and logical adequacy?
- **Basis in paper:** [explicit] Section 5.4: "use ICL as a high–well-formedness candidate generator, paired with a component optimized for semantic adequacy (e.g., a fine-tuned parser or a prover-guided reranker/repair module). Such a hybrid pipeline can narrow the remaining gap without sacrificing the strengths of either approach."
- **Why unresolved:** The authors propose this direction but do not implement or evaluate any hybrid system.
- **What evidence would resolve it:** Experiments combining GPT-based generation with T5-based reranking or Vampire-guided repair, measuring Prover Accuracy vs. Non-WFF Ratio tradeoffs.

### Open Question 2
- **Question:** Can training objectives that incorporate logic-aware signals (e.g., prover feedback or consistency constraints) reduce the dominant quantifier-count and predicate-naming errors?
- **Basis in paper:** [explicit] Section 5.3: "Addressing them requires strategies that tie natural language understanding more tightly to the intended semantic framework (here, event semantics) during prediction."
- **Why unresolved:** Current SFT uses standard sequence-to-sequence loss without explicit logical supervision.
- **What evidence would resolve it:** Compare models trained with auxiliary losses based on prover verification or structural consistency against vanilla SFT on error type distributions.

### Open Question 3
- **Question:** Does theorem-proving-based evaluation (Prover Accuracy) correlate with end-to-end performance on downstream natural language inference tasks?
- **Basis in paper:** [explicit] Section 7 (Limitations): "we do not evaluate end-to-end natural language inference, one of the principal downstream tasks. Instead, we focus on bidirectional entailment between predicted and gold logical forms."
- **Why unresolved:** The study evaluates formula-to-formula equivalence but not whether improved Prover Accuracy translates to better NLI outcomes.
- **What evidence would resolve it:** Run full NLI pipeline (sentence → predicted formula → prover judgment) and correlate Prover Accuracy with gold NLI label accuracy.

## Limitations

- The evaluation pipeline depends on theorem prover behavior and DRS conversion rules, which may introduce brittleness
- Results are based solely on the SICK dataset, limiting generalizability to other semantic parsing domains
- Target normalization may obscure scopal ambiguity preservation needed for certain downstream tasks

## Confidence

- **High:** The finding that Dmatch and Prover Accuracy diverge systematically, exposing logical adequacy gaps in graph-based metrics
- **Medium:** The mechanism by which prenex normalization improves performance (incidental variability reduction vs. alignment with prover)
- **Low:** The extent to which the observed error patterns are inherent to neural seq2seq architectures versus addressable with more data or better prompts

## Next Checks

1. **Reproduce with Alternative Provers and DRS Tools:** Swap Vampire for a different theorem prover (e.g., E-prover) and use a different DRS conversion library to verify that the Dmatch-Prover Accuracy gap persists and is not pipeline-dependent

2. **Stratified Ablation on Target Normalization:** Train a model on a hybrid target set (prenex for simple formulas, raw for complex ones) and measure whether gains in logical adequacy are maintained while preserving scopal distinctions

3. **Cross-Dataset Generalization:** Evaluate the best T5-Base checkpoint (prenex targets) on a semantically diverse semantic parsing dataset (e.g., Spider or WebQuestionsSP) to assess robustness beyond SICK