---
ver: rpa2
title: Multi-output Classification for Compound Fault Diagnosis in Motor under Partially
  Labeled Target Domain
arxiv_id: '2503.13534'
source_url: https://arxiv.org/abs/2503.13534
tags:
- domain
- fault
- classification
- adaptation
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-output classification (MOC) framework
  for domain adaptation in compound fault diagnosis, addressing the challenges of
  partially labeled target domain data and coexisting faults in rotating machinery.
  Unlike conventional multi-class classification (MCC) methods, MOC independently
  classifies the severity of each fault using task-specific layers (TSLs), reducing
  inter-class interference and improving feature alignment.
---

# Multi-output Classification for Compound Fault Diagnosis in Motor under Partially Labeled Target Domain

## Quick Facts
- arXiv ID: 2503.13534
- Source URL: https://arxiv.org/abs/2503.13534
- Reference count: 1
- Multi-output classification framework achieves 0.874 average macro F1 score across six domain adaptation cases, outperforming conventional multi-class classification methods.

## Executive Summary
This paper introduces a multi-output classification (MOC) framework for domain adaptation in compound fault diagnosis, addressing challenges of partially labeled target domain data and coexisting faults in rotating machinery. The approach independently classifies each fault type using task-specific layers, reducing inter-class interference compared to conventional multi-class classification. The method integrates multi-kernel maximum mean discrepancy loss and entropy minimization loss to enhance feature transferability, while frequency layer normalization effectively handles stationary vibration signals by preserving mechanical frequency characteristics.

## Method Summary
The proposed MOC framework decomposes compound fault classification into separate tasks for each fault type (IRF, ORF, misalignment, unbalance) using task-specific layers. It employs frequency layer normalization to preserve spectral features in vibration signals, and combines MKMMD loss for domain alignment with entropy minimization loss for semi-supervised learning. The system is trained with 100 epochs on source domain data followed by 100 epochs fine-tuning on target domain with partial labels, using STFT-transformed vibration signals sampled at 25.6 kHz.

## Key Results
- MOC achieves 0.874 average macro F1 score compared to 0.848 for MCC baseline across six domain adaptation cases
- Frequency Layer Normalization outperforms BN and LN, achieving 0.874 average macro F1 score
- The approach demonstrates superior performance under varying operating conditions and limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
Independent fault-specific classification reduces inter-class interference in compound fault scenarios by decomposing high-cardinality problems into multiple lower-cardinality binary/multi-class problems using task-specific layers.

### Mechanism 2
Frequency-dimension normalization preserves domain-invariant spectral features in vibration signals by calculating mean and standard deviation along frequency dimension only, maintaining relative magnitude relationships across frequency bins.

### Mechanism 3
Combining distribution alignment (MKMMD) with entropy minimization (EM) improves transfer under partial labeling by aligning source and target feature distributions while encouraging confident predictions on unlabeled target data.

## Foundational Learning

**Multi-task Learning (MTL) with Task-Specific Heads**
- Why needed here: MOC is structurally an MTL problem where each fault type is a task
- Quick check question: Can you explain why shared backbone + separate heads might outperform a single large-output classifier when tasks share low-level features but diverge at higher levels?

**Unsupervised Domain Adaptation (UDA)**
- Why needed here: The paper addresses domain shift under partially labeled target data
- Quick check question: What is the difference between domain-invariant representation learning and domain-specific fine-tuning, and when would you choose each?

**Normalization Layers in Deep Networks**
- Why needed here: FLN is introduced as an alternative to BN and LN
- Quick check question: If you have a 2D spectrogram input (time x frequency), which dimensions would BN, LN, and FLN normalize over, and what inductive biases does each impose?

## Architecture Onboarding

**Component map:**
Input: STFT of vibration signals (time x frequency) -> Backbone: Convolutional feature extractor (shared across faults) -> Normalization: FLN after each conv block (frequency-dimension stats) -> Task-Specific Layers: One TSL per fault type, each containing h (feature alignment head with MKMMD loss) and g (classification head with CCE + EM loss) -> Output: Per-fault severity level predictions

**Critical path:**
1. Pre-train entire network on labeled source domain (100 epochs)
2. Fine-tune on target domain with MKMMD + EM enabled (100 epochs)
3. Select checkpoint with lowest validation loss
4. Inference: concatenate per-fault predictions for compound diagnosis

**Design tradeoffs:**
- FLN vs BN: FLN preserves frequency structure but may be sensitive to batch size; BN stabilizes training but mixes frequency information
- MOC vs MCC: MOC reduces inter-class interference but increases parameters and requires explicit fault decomposition; MCC is simpler but combinatorially explosive for compound faults
- Partial labeling ratio: 10% labeled target data is assumed; lower ratios may require stronger EM or pseudo-labeling

**Failure signatures:**
- Macro F1 drops significantly on specific domain pairs (e.g., A→C, B→A) → check RPM/torque distribution overlap
- FLN underperforms LN → verify STFT parameters; time-varying RPM may violate stationarity assumption
- Per-fault F1 imbalance → check severity level distribution; TSLs may need class weighting

**First 3 experiments:**
1. Replicate A→B and A→C cases with MOC vs MCC, confirming macro F1 gap (expected: larger gap for larger domain shift)
2. Ablate FLN by substituting BN and LN; plot per-domain macro F1 to identify which operating conditions benefit most
3. Vary target label ratio (5%, 10%, 20%) to characterize EM contribution; plot macro F1 vs label ratio for MOC with and without EM loss

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced deep learning architectures (e.g., attention mechanisms or transformers) further enhance the performance of the Multi-Output Classification (MOC) framework?
- Basis in paper: The conclusion states, "Future work may explore advanced model architectures to further enhance MOC performance."
- Why unresolved: The current study utilizes a TCDCN-based architecture and focuses on the classification strategy and normalization, leaving the potential of newer architectural paradigms untested.
- What evidence would resolve it: Comparative experiments replacing the current convolutional base with state-of-the-art architectures while retaining the MOC and FLN components.

### Open Question 2
How does Frequency Layer Normalization (FLN) perform on non-stationary vibration signals where frequency components are not strictly dominant or stationary?
- Basis in paper: The paper states FLN is "well-suited for the dominance of frequency components... in stationary vibration signals" (Section 2.2).
- Why unresolved: The method is optimized for signals with clear frequency dominance; its efficacy on transient or highly noisy non-stationary data remains unverified.
- What evidence would resolve it: Evaluation of the FLN method on datasets containing significant non-stationary faults (e.g., instantaneous speed changes) compared to standard normalization techniques.

### Open Question 3
How robust is the MOC framework when the ratio of labeled data in the target domain is significantly reduced below the tested 10% threshold?
- Basis in paper: The experimental setup fixed the target domain as "10% of labeled data and 90% of unlabeled data" (Section 2.3).
- Why unresolved: Real-world scenarios may suffer from extreme label scarcity (e.g., <1%); linearity of performance degradation with label scarcity is not established.
- What evidence would resolve it: A sensitivity analysis measuring macro F1 score across varying percentages (e.g., 1%, 5%, 10%) of available labeled target data.

## Limitations
- FLN's effectiveness depends on stationarity assumption of vibration signals, which may break down under variable RPM
- The partial labeling assumption (10%) is specific and may not generalize to scenarios with less labeled data
- Compound fault decomposition via MOC assumes fault independence, which may not hold in all mechanical systems

## Confidence
- MOC architecture & performance claims: **High** (supported by direct textual evidence and comparable methods in corpus)
- FLN superiority claims: **Medium** (evidence is present but FLN is novel; no ablation studies provided)
- MKMMD + EM combination: **Medium** (well-established principles, but specific contribution to this dataset is not quantified)

## Next Checks
1. Conduct ablation studies: MOC vs MCC + BN vs FLN on each domain pair to quantify individual contributions
2. Test FLN under non-stationary conditions (e.g., variable RPM) to verify stationarity assumption
3. Evaluate MOC with varying target label ratios (5%, 20%) to determine EM loss contribution boundary