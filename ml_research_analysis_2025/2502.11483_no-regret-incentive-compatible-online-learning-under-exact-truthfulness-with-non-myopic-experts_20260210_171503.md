---
ver: rpa2
title: No-regret incentive-compatible online learning under exact truthfulness with
  non-myopic experts
arxiv_id: '2502.11483'
source_url: https://arxiv.org/abs/2502.11483
tags:
- expert
- round
- setting
- online
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of online forecasting with strategic
  experts who aim to maximize their future selection probability. The goal is to design
  mechanisms that achieve low belief regret while being truthful for non-myopic experts.
---

# No-regret incentive-compatible online learning under exact truthfulness with non-myopic experts

## Quick Facts
- arXiv ID: 2502.11483
- Source URL: https://arxiv.org/abs/2502.11483
- Reference count: 40
- Primary result: First no-regret algorithms (Õ(√TN) and Õ(T^{2/3}N^{1/3})) for truthful online forecasting with non-myopic experts

## Executive Summary
This paper addresses online forecasting with strategic non-myopic experts who aim to maximize their future selection probability. The authors propose two algorithms, FPL-ELF for full-information and FPL-ELF-ε for bandit settings, achieving low belief regret while maintaining exact truthfulness. The key innovation is using Follow-the-Perturbed-Leader with loss-dependent noise ("sad lotteries") where experts are rewarded for bad performance to ensure stability. The algorithms achieve expected regret bounds of Õ(√TN) for full-information and Õ(T^{2/3}N^{1/3}) for bandit settings, which are the first no-regret results for truthful mechanisms in these settings.

## Method Summary
The core approach uses Follow-the-Perturbed-Leader (FPL) with a novel perturbation scheme based on "sad lotteries." Each round, a candidate expert C_t is randomly selected, and if they're not the current leader, they receive a "sad point" with probability 1/2 + ℓ_{j,t}/4. This creates a random walk that ensures truthful reporting is optimal for non-myopic experts. The full-information version FPL-ELF maintains this process continuously, while FPL-ELF-ε adds ε-exploration (ε=(N/T)^{1/3}) for the bandit setting. The analysis relies on new Poisson binomial tail bounds to control the size of the "lead pack" and prove expected regret guarantees.

## Key Results
- FPL-ELF achieves expected regret Õ(√TN) in full-information setting
- FPL-ELF-ε achieves expected regret Õ(T^{2/3}N^{1/3}) in bandit setting
- Both algorithms are exactly truthful for non-myopic experts under belief independence
- First no-regret results for truthful mechanisms in these settings
- Analysis introduces new Poisson binomial tail bounds for controlling leader stability

## Why This Works (Mechanism)
The mechanism works by creating a random walk where experts are rewarded for poor performance (sad lotteries), ensuring that truthful reporting maximizes future selection probability. When an expert is selected as candidate but not leader, they receive a "sad point" with probability 1/2 + ℓ_{j,t}/4. This makes it disadvantageous to misreport - overreporting leads to higher loss and more sad points, while underreporting leads to lower selection probability. The perturbation ensures that even if an expert is currently selected, they cannot guarantee future selection without truthful reporting.

## Foundational Learning

**Poisson binomial random variables**: Sum of independent Bernoulli trials with different probabilities. Needed for analyzing the distribution of "sad points" and proving concentration bounds. Quick check: Verify that Σ P(success_i) ∈ [0,1] and that variance can be computed as Σ p_i(1-p_i).

**Strictly proper loss functions**: Loss functions where reporting true belief minimizes expected loss. Required to ensure that truthful reporting is incentive-compatible. Quick check: For Brier score ℓ(p,o)=(p-o)², verify that E[ℓ(b,o)] is minimized when b equals true belief.

**Belief regret**: The difference between cumulative loss of algorithm's predictions and the best expert's predictions. Central metric for evaluating forecasting performance. Quick check: Compute belief regret for simple example with N=2 experts and T=3 rounds.

## Architecture Onboarding

**Component map**: Candidates (C_t) → Sad lottery points (W_{j,t}) → Leader selection (I_t) → Output prediction (ŷ_t)

**Critical path**: Each round: draw candidate C_t → award sad points → compute leader argmin_j Σ_s W_{j,s} → output prediction based on leader's belief

**Design tradeoffs**: Using "sad lotteries" (rewarding bad performance) instead of "happy lotteries" trades intuitive appeal for stability and truthfulness. The random candidate selection ensures no expert can game the system by manipulating when they're selected.

**Failure signatures**: 
- Leader changes too frequently → check that sad point probability is correctly set to 1/2 + ℓ_{j,t}/4
- Regret doesn't scale as expected → verify initial noise W_{j,0} range is 1/4N scale
- Bandit version achieves wrong rate → ensure ε-exploration probability is exactly (N/T)^{1/3}

**First experiments**:
1. Verify that with all experts truthful, leader remains stable when not selected as candidate
2. Test that misreporting leads to lower selection probability in simple two-expert case
3. Compare empirical regret against theoretical Õ bounds for small N,T values

## Open Questions the Paper Calls Out

**Open Question 1**: Is Ω(√TN) a tight lower bound on belief regret for truthful mechanisms in full-information setting? The authors conjecture this is fundamental but haven't proven it, noting that alternative constructions allowing multiple lottery winners were shown to violate incentive compatibility.

**Open Question 2**: Can bandit regret bound improve from Õ(T^{2/3}) to standard Õ(√T)? The current exploration-separated approach limits performance to T^{2/3}, and it's unclear if this is avoidable for truthful mechanisms.

**Open Question 3**: Can logarithmic factor in regret bound be removed? The log T factor arises from Poisson binomial tail bound analysis, and while believed to be an artifact, removing it is nontrivial.

## Limitations
- Exact constants in Poisson binomial tail bounds are not specified, making precise numerical verification difficult
- Incentive compatibility relies on belief independence assumption that may not hold with correlated information
- The mechanisms are complex and require careful implementation of the sad lottery process

## Confidence

**High confidence**: General algorithmic framework and regret rate structures (Õ(√TN) and Õ(T^{2/3}N^{1/3})) are theoretically sound

**Medium confidence**: Exact constants and tie-breaking procedures are implementable but may require tuning

**Low confidence**: Behavior under realistic non-myopic expert strategies beyond idealized truthful reporting assumption

## Next Checks

1. Verify Poisson binomial concentration bounds empirically by simulating sad lottery process and measuring selection probabilities against theoretical predictions

2. Test incentive compatibility under simple non-myopic strategies where experts can slightly misreport beliefs to gain selection advantage

3. Compare empirical regret scaling across multiple T, N combinations to confirm theoretical Õ bounds, particularly focusing on transition between full-information and bandit settings