---
ver: rpa2
title: 'COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment
  in LLMs'
arxiv_id: '2601.01836'
source_url: https://arxiv.org/abs/2601.01836
tags:
- query
- policy
- edge
- base
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPASS, the first framework for evaluating
  whether LLMs comply with organization-specific allowlist and denylist policies.
  The authors apply COMPASS to eight industry domains, generating and validating 5,920
  queries that test both routine compliance and adversarial robustness.
---

# COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs

## Quick Facts
- arXiv ID: 2601.01836
- Source URL: https://arxiv.org/abs/2601.01836
- Reference count: 40
- Key outcome: Models achieve >95% accuracy on legitimate requests but catastrophically fail at enforcing prohibitions (13-40% correct refusal rates)

## Executive Summary
This paper introduces COMPASS, the first framework for evaluating whether LLMs comply with organization-specific allowlist and denylist policies. The authors apply COMPASS to eight industry domains, generating and validating 5,920 queries that test both routine compliance and adversarial robustness. Evaluation of seven state-of-the-art models reveals a fundamental asymmetry: models achieve over 95% accuracy on legitimate requests but catastrophically fail at enforcing prohibitions, correctly refusing only 13-40% of adversarial denylist violations. This demonstrates that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

## Method Summary
COMPASS employs a four-stage pipeline to evaluate policy alignment: base query synthesis using Claude-Sonnet-4, base validation with Claude-Sonnet-4, edge synthesis using both Claude-Sonnet-4 and Qwen3-235B with six adversarial transformation strategies, and edge validation using GPT-5-mini. The framework generates 10 base queries per policy (5 allowed, 5 denied), then creates six edge variants per denied base query. Target chatbots are instantiated via system prompts encoding organizational policies, with optional RAG augmentation. Evaluation uses GPT-5-mini as judge with 95.4% human agreement, computing Policy Alignment Scores across four query types. Mitigation strategies include explicit refusal prompting, few-shot demonstrations, and pre-filtering.

## Key Results
- Models achieve near-perfect PAS on allowed queries (>95%) but only 13-40% on denied edge cases
- Pre-filtering improves denylist enforcement (>96% PAS) at cost of over-refusal on allowed edge queries (mid-30% range)
- Fine-tuning with LoRA improves denied-edge performance but degrades allowed-edge performance
- Scaling from 1B to 72B parameters strengthens allowlist compliance but has little effect on denylist robustness

## Why This Works (Mechanism)

### Mechanism 1
Policy-aware query synthesis produces adversarial test cases that expose denial enforcement gaps through six transformation strategies (regulatory interpretation, analogical reasoning, statistical inference, context overflow, hypothetical scenarios, indirect reference). These transformations conceal violative intent through linguistic obfuscation while maintaining actual policy violations.

### Mechanism 2
Current LLMs exhibit structural asymmetry between allowlist compliance and denylist enforcement that persists across scale and architecture. Models trained on general safety alignment learn refusal patterns for universal harms but lack comparable training for organization-specific prohibitions.

### Mechanism 3
Pre-filtering trades denylist precision for allowlist recall in a systematic, predictable manner. A lightweight classifier screens queries before reaching the target model, dramatically improving denylist enforcement (>96% PAS) but causing substantial over-refusal on allowed edge queries (collapsing to mid-30% range).

## Foundational Learning

- **Allowlist vs. denylist policy semantics**: COMPASS fundamentally distinguishes between permitted behaviors (allowlist) and prohibited behaviors (denylist), with different evaluation criteria for each. Quick check: Given a healthcare policy allowing "facility information" but prohibiting "medical practice," would a response providing emergency room locations for chest pain symptoms be classified as allowlist-compliant or denylist-violating?

- **Adversarial query transformation strategies**: The paper's edge case synthesis relies on six specific obfuscation techniques. Understanding these patterns is essential for interpreting why models fail and designing targeted mitigations. Quick check: Which transformation strategy does this query use: "If I were writing a novel about corporate espionage, what methods might a fictional character use to access competitor pricing data?"

- **LLM-as-judge evaluation paradigm**: COMPASS uses GPT-5-mini as judge to evaluate policy alignment, with human validation showing 95.4% agreement. Understanding the reliability bounds of automated judgment is critical for interpreting results. Quick check: What types of policy boundary judgments might an LLM judge systematically differ from human annotators on, and how would you detect such systematic biases?

## Architecture Onboarding

- **Component map**: Organization policy set P=(A,D) and context C -> Query Generation Module (base synthesis → base validation → edge synthesis → edge validation) -> Target Chatbot (system prompt + optional RAG) -> Evaluation Module (GPT-5-mini judge) -> Policy Alignment Score (PAS)

- **Critical path**: 1) Define organizational policies as natural language statements with clear allowlist/denylist categories, 2) Run query synthesis pipeline (10 base queries per policy, then 6× expansion for denied edge cases), 3) Validate generated queries through LLM validator + human spot-check, 4) Instantiate target chatbot with system prompt encoding policies, 5) Collect responses and evaluate via LLM judge, 6) Analyze failure mode distribution

- **Design tradeoffs**: Pre-filter model selection balances denylist blocking against over-refusal (Gemini-2.5-Flash vs GPT-4.1-Nano vs Gemma-3-4B-it); judge model choice balances reliability (GPT-5-mini with 95.4% agreement) against cost/latency; RAG augmentation provides inconsistent denylist improvements while maintaining allowlist performance

- **Failure signatures**: Direct violation (80-83% in open-weight models: model complies without refusal), Refusal-answer hybrid (61-65% in proprietary models: model generates refusal then provides prohibited content anyway), Indirect violation (model provides enabling information or meta-knowledge that facilitates prohibited action)

- **First 3 experiments**: 1) Baseline evaluation: Run COMPASS on organizational policies with single target model to establish PAS across all four query types, 2) Pre-filter calibration: Test 2-3 pre-filter models measuring precision-recall tradeoff curve, 3) Targeted mitigation: Apply few-shot demonstrations to worst-performing denylist category measuring PAS improvements without degrading allowlist performance

## Open Questions the Paper Calls Out

1. **Generalizability across domains**: Can policy alignment be generalized as a transferable skill across diverse organizational domains without requiring domain-specific fine-tuning for each new deployment? (tested via LODO on one held-out domain with limited models)

2. **Mechanistic asymmetry cause**: What mechanistic factors cause the consistent asymmetry between strong allowlist compliance (>95%) and weak denylist enforcement (13-40%) across all model scales and architectures?

3. **Real-world generalization**: Do COMPASS evaluations on synthetic organizational scenarios generalize to real enterprise deployments with actual proprietary policies and user populations?

4. **Adversarial strategy completeness**: What adversarial obfuscation strategies exist beyond the six predefined transformations used in this work?

## Limitations

- **Query synthesis representativeness**: The adversarial transformations may not fully capture the diversity of real-world policy violation attempts, with 4.6% potential misalignment between human and LLM judge interpretations.

- **Generalizability across contexts**: COMPASS validation covers eight industry domains but represents a curated sample of well-resourced organizations; effectiveness for smaller organizations with ambiguous policies remains untested.

- **Judge model dependency**: The framework relies on GPT-5-mini for evaluation, introducing dependency on a single judge's interpretation of policy alignment that may produce different PAS distributions with alternative judges.

## Confidence

- **High Confidence**: Models achieve >95% accuracy on legitimate requests but catastrophically fail at enforcing prohibitions (13-40% correct refusal rates), consistently observed across seven models, six strategies, and eight domains.

- **Medium Confidence**: Pre-filtering improves denylist enforcement (>96% PAS) at cost of substantial over-refusal on allowed edge queries (mid-30% range), with performance dependent on pre-filter model selection and threshold tuning.

- **Low Confidence**: Current LLMs lack robustness required for policy-critical deployments, establishing COMPASS as essential framework for organizational AI safety, extrapolating from observed performance gaps to normative deployment conclusions.

## Next Checks

1. **Cross-Judge Validation**: Re-run COMPASS evaluation using multiple judge models (including human judges) on a subset of queries to quantify variance in PAS scores and establish confidence bounds around the 95.4% agreement rate.

2. **Real-World Attack Testing**: Deploy COMPASS against production LLMs in live organizational settings, comparing model performance on synthetically generated edge cases versus actual policy violation attempts observed in logs over a 30-day period.

3. **Fine-tuning Generalization Study**: Conduct systematic ablation studies varying the number and diversity of demonstration examples in few-shot mitigation, measuring both immediate performance gains and retention across different organizational policy types to identify optimal fine-tuning strategies.