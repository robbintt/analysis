---
ver: rpa2
title: Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks
  for Deterministic Games
arxiv_id: '2510.15974'
source_url: https://arxiv.org/abs/2510.15974
tags:
- should
- state
- reasoning
- answer
- does
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We evaluated whether providing a Tower of Hanoi agent with environment
  interaction delays reasoning performance collapse. Models were tested in both single-pass
  and agentic frameworks with tool-based move execution.
---

# Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games

## Quick Facts
- **arXiv ID**: 2510.15974
- **Source URL**: https://arxiv.org/abs/2510.15974
- **Reference count**: 40
- **Primary result**: Environment access does not prevent reasoning collapse; agentic performance degrades faster than single-pass baselines, with models exhibiting deterministic looping and policy divergence from both optimal and random strategies.

## Executive Summary
This paper investigates whether providing LLMs with environment interaction in an agentic framework improves reasoning performance on deterministic games like Tower of Hanoi. Contrary to expectations, the study finds that environment access accelerates reasoning collapse compared to single-pass baselines. Models exhibit deterministic mode-following behavior, diverging from both optimal and random policies, and show looping behavior that prevents escape from failure states. The findings suggest that apparent reasoning in LLMs is fixed pattern execution rather than emergent problem-solving.

## Method Summary
The authors evaluate LLMs in both single-pass and agentic frameworks on Tower of Hanoi puzzles of varying complexity (3-8 disks). In the agentic framework, models receive ground-truth state feedback after each move and can use tool-based execution. Performance is measured through success rates, loop detection, and Jensen-Shannon divergence analysis comparing model policies against optimal and random baselines. The study tests whether environment access mitigates the reasoning cliff phenomenon observed in prior work.

## Key Results
- Agentic performance degrades at lower complexity than single-pass baselines
- Jensen-Shannon divergence analysis shows increasing divergence from both optimal and random policies
- Models exhibit looping behavior and fail to adapt to novel states
- Environment access exacerbates rather than mitigates underlying reasoning limitations

## Why This Works (Mechanism)

### Mechanism 1: State-Sparse Agentic Degradation
- **Claim**: Agentic frameworks with environment access accelerate reasoning collapse compared to single-pass baselines.
- **Mechanism**: Stepwise interaction forces reasoning based on sparse intermediate states rather than complete solution retrieval, exposing brittle state-transition priors where models default to suboptimal patterns.
- **Core assumption**: Single-pass performance relies partially on complete solution retrieval, which the agentic framework disables.
- **Evidence anchors**: Abstract states agentic performance degrades at lower complexity; Section 5 discusses how stepwise execution exposes reasoning limitations; related work arXiv:2506.18957 refers to this as the "Agentic Gap."

### Mechanism 2: Deterministic Mode-Following (JSD Divergence)
- **Claim**: LLMs exhibit deterministic policy collapse, diverging from both optimal and random policies as complexity increases.
- **Mechanism**: The model policy clusters probability mass on specific learned patterns rather than exploring uniformly, leading to confident execution of suboptimal patterns.
- **Core assumption**: Increasing JSD implies the model is locking onto a specific suboptimal policy distribution.
- **Evidence anchors**: Abstract mentions JSD divergence from optimal and random policies; Section 4.3 analyzes JSD increases with complexity; foundational paper arXiv:2506.06941 establishes the reasoning cliff phenomenon.

### Mechanism 3: History-Insensitive Looping
- **Claim**: Models fail to integrate interaction history into future decisions, causing repetition of identical error sequences.
- **Mechanism**: When returning to previously visited states, conditional probability effectively collapses to depend only on the immediate state, ignoring the loop implied by history.
- **Core assumption**: Self-attention over history context fails to override the strong prior of immediate state representation.
- **Evidence anchors**: Abstract notes looping behavior despite complete history access; Section 5 discusses failure to adapt despite history availability; inference based primarily on paper text.

## Foundational Learning

- **Concept**: Tower of Hanoi State Space & Optimality
  - **Why needed here**: Understanding optimal play requires $2^n - 1$ moves to diagnose why models fail and what "optimal policy" means in JSD analysis.
  - **Quick check question**: Does the model's failure stem from violating rules (invalid moves) or inefficiency (sub-optimal path lengths)?

- **Concept**: Jensen-Shannon Divergence (JSD)
  - **Why needed here**: Primary metric proving models are neither optimal nor exploring randomly; essential for interpreting "mode collapse."
  - **Quick check question**: If JSD(P || Q) = 0, what does that imply about policy P relative to Q?

- **Concept**: LLM-Parameterized Policy $\hat{\pi}(s, a)$
  - **Why needed here**: Treating LLM as stochastic policy approximator rather than text generator is key to bridging NLP outputs with RL concepts.
  - **Quick check question**: How is the joint distribution $\hat{\pi}_{LLM}(s, a)$ approximated from trajectory datasets?

## Architecture Onboarding

- **Component map**: Agent Interface (System/User Prompts) -> Environment (deterministic validator) -> Analysis Module (logs trajectories)
- **Critical path**: Prompt engineering -> LLM tool selection -> Environment state update -> History appending
- **Design tradeoffs**: Single-pass tests retrieval/memorization; Agentic tests incremental reasoning. This paper proves Agentic is strictly harder for current models on deterministic tasks.
- **Failure signatures**: Looping (repeating identical transitions), Mode Collapse (JSD vs. Random increasing), Complexity Threshold (success rate dropping at specific disk counts)
- **First 3 experiments**:
  1. Reproduce Baseline Collapse: Run single-pass generation for n=3 to 8 disks to establish reasoning cliff
  2. Loop Detection: Execute agentic framework and log "Unique length k transitions" to verify sequence repetition
  3. Policy Divergence Analysis: Calculate $\hat{\pi}_{LLM}$ for n=6 and compute JSD against optimal solver

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the observed performance collapse generalize to stochastic environments or non-puzzle reasoning tasks?
- **Basis**: Limitations section states uncertainty about generalization to other task classes, noting exclusive focus on deterministic Tower of Hanoi.
- **Why unresolved**: Study only verifies failure in deterministic, fully observable domains; does not test probabilistic environments.
- **What evidence would resolve it**: Replicating agentic framework evaluation on stochastic games or open-ended planning tasks.

### Open Question 2
- **Question**: Can decoding strategies or self-consistency mechanisms mitigate the mode-collapse behavior?
- **Basis**: Authors explicitly note they did not report ablations over temperature, self-consistency, majority vote, or beam search.
- **Why unresolved**: Observed looping may be artifact of greedy sampling rather than fundamental reasoning deficit.
- **What evidence would resolve it**: Experiments comparing standard agentic setup against variations using majority voting or higher temperatures.

### Open Question 3
- **Question**: Does state history representation in context window contribute to failure to escape looping?
- **Basis**: Paper notes models fail to adapt "despite models having access to their complete interaction history," suggesting format may be ineffective.
- **Why unresolved**: Unclear if model cannot retrieve relevant error signals from long context or is constitutionally incapable of updating policy.
- **What evidence would resolve it**: Testing variations where state history is summarized rather than raw logs.

## Limitations
- Analysis focuses on a single deterministic game, limiting generalizability
- Does not explore alternative prompting strategies or model architectures
- Looping behavior mechanism lacks direct experimental validation

## Confidence
- **Jensen-Shannon divergence claims**: High confidence (strong quantitative evidence)
- **Looping mechanism**: Medium confidence (inferred from behavior patterns, limited direct evidence)
- **Agentic Gap connection**: Low confidence (mentioned but not deeply explored)

## Next Checks
1. **Controlled loop injection experiment**: Force agent into known loop state and test whether explicit warning messages in history context prevent repetition
2. **Multi-step lookahead probe**: Modify agent to evaluate 2-3 step consequences before executing moves, testing whether shallow planning improves success rates
3. **Policy distribution comparison**: For models showing collapse, systematically compare action distributions against optimal solvers and random policies across all encountered states