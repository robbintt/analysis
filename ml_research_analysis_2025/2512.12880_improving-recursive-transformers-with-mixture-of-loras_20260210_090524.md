---
ver: rpa2
title: Improving Recursive Transformers with Mixture of LoRAs
arxiv_id: '2512.12880'
source_url: https://arxiv.org/abs/2512.12880
tags:
- arxiv
- recursive
- experts
- shared
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Mixture of LoRAs (MoL), a conditional-computation
  mechanism that integrates low-rank adaptation (LoRA) experts into a shared feed-forward
  network of a recursive transformer. This design enables token-conditional weight-space
  modulation within the shared architecture, addressing the expressivity collapse
  caused by aggressive parameter sharing in models like ALBERT.
---

# Improving Recursive Transformers with Mixture of LoRAs

## Quick Facts
- **arXiv ID:** 2512.12880
- **Source URL:** https://arxiv.org/abs/2512.12880
- **Reference count:** 13
- **Key outcome:** ModernALBERT (50M-120M params) achieves state-of-the-art performance among compact models, outperforming both prior baselines and larger fully-parameterized models on GLUE, SQuAD-v2, and BEIR.

## Executive Summary
The authors propose Mixture of LoRAs (MoL), a conditional-computation mechanism that integrates low-rank adaptation (LoRA) experts into a shared feed-forward network (FFN) of a recursive transformer. This design enables token-conditional weight-space modulation within the shared architecture, addressing the expressivity collapse caused by aggressive parameter sharing in models like ALBERT. To validate the approach, they develop ModernALBERT, a compact recursive transformer incorporating rotary embeddings, GeGLU, and FlashAttention, and pretrain it on 30B tokens with knowledge distillation. ModernALBERT achieves state-of-the-art performance among compact models, outperforming both prior baselines and larger fully-parameterized models on GLUE, SQuAD-v2, and BEIR. Additionally, the authors introduce an expert-merging procedure that compresses MoL into a single adapter at inference, preserving accuracy while reducing latency and memory use.

## Method Summary
The authors introduce Mixture of LoRAs (MoL), which integrates LoRA experts into the shared FFN of a recursive transformer to enable token-conditional weight-space modulation. A router network assigns routing probabilities to each LoRA expert based on the input token representation, selecting the top-2 experts. The low-rank updates from these experts are applied to the shared FFN weights, and the final output is a renormalized weighted sum of the expert-modified FFN outputs. ModernALBERT, a compact recursive transformer with MoL, is pretrained on 30B tokens with knowledge distillation from ModernBERT. The authors also propose an expert-merging procedure using EMA to compress MoL into a single adapter at inference, preserving accuracy while reducing latency and memory use.

## Key Results
- ModernALBERT (50M-120M parameters) achieves state-of-the-art performance among compact models on GLUE, SQuAD-v2, and BEIR.
- MoL with 8 experts and top-2 routing achieves GLUE average 77.24, outperforming MoA at 76.87 with similar parameter counts.
- EMA expert merging preserves accuracy during inference, achieving RTE 86.28 vs. Vanilla at 84.83, nearly matching No Merging baseline (85.6).

## Why This Works (Mechanism)

### Mechanism 1
Token-conditional weight-space modulation within the shared FFN may restore expressivity lost under aggressive parameter sharing. A router network assigns routing probabilities to each LoRA expert based on the input token representation. The top-2 experts are selected, and their low-rank updates are applied to the shared FFN weights. The final output is a renormalized weighted sum of the expert-modified FFN outputs. The router learns to activate syntactically or semantically coherent expert specializations during pretraining.

### Mechanism 2
Injecting LoRA experts directly into the shared FFN weight space outperforms appending adapters post-FFN. In MoL, low-rank updates modify both the down-projection and up-projection matrices inside the FFN computation. This internal modulation differs from Mixture-of-Adapters (MoA), which routes among adapters placed after the FFN output. Modulating internal representations enables deeper integration of conditional computation than output-space adaptation.

### Mechanism 3
Dynamic expert routing during training can be compressed into a static merged adapter at inference with minimal accuracy loss. Experts are merged into a single adapter using EMA based on batch-level router activation statistics. During fine-tuning, the merged adapter is trained directly while routing is disabled. The aggregate expert behavior captured by router statistics transfers to downstream tasks even when dynamic routing is removed.

## Foundational Learning

- **Parameter sharing in recursive transformers (ALBERT-style)**: Why needed here: MoL is designed to address the expressivity collapse that occurs when the same attention and FFN weights are reused across all layers. Understanding this trade-off is essential to grasp what MoL restores. Quick check: Given a 12-layer transformer with hidden dimension $d$, how many FFN parameters does a fully parameterized model have vs. a fully shared recursive model?

- **Low-Rank Adaptation (LoRA)**: Why needed here: MoL uses LoRA modules as experts. You must understand that LoRA adds trainable low-rank matrices $A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{r \times d}$ with $r \ll d$, updating weights as $W' = W + \frac{\alpha}{r} AB$. Quick check: If $d = 1024$ and $r = 8$, how many parameters does a LoRA module add to a $1024 \times 4096$ weight matrix?

- **Mixture-of-Experts routing (Top-K sparse routing)**: Why needed here: MoL inherits the MoE routing paradigm. A router computes softmax logits over experts, selects top-K, and renormalizes. The paper uses top-2 routing. Quick check: For 8 experts with routing probabilities $[0.4, 0.3, 0.15, 0.1, 0.03, 0.01, 0.007, 0.003]$, what is the renormalized weight of the top-2 experts?

## Architecture Onboarding

- **Component map**: Input -> Shared transformer backbone (MHA + FFN with Pre-Norm, RoPE, GeGLU, FlashAttention) -> MoL layer (shared FFN + 8 LoRA experts + router) -> Output
- **Critical path**: Implement shared transformer backbone with recursive weight tying across groups. Add router network: `router_logits = Linear(hidden_dim, num_experts)(hidden_states)`. Implement top-K routing with load-balancing auxiliary loss (if needed). For each selected expert, compute effective FFN weights with LoRA updates. Apply gated activation (GeGLU) and compute weighted expert outputs. During fine-tuning, optionally merge experts via EMA and disable routing.
- **Design tradeoffs**: Expert count vs. rank: More experts increase routing flexibility; higher rank increases per-expert capacity. MoL placement density: Placing MoL at every layer maximizes expressivity but increases routing overhead. Merging strategy: Uniform initialization is simpler but EMA preserves more accuracy.
- **Failure signatures**: Router collapse: >80% of tokens route to single expert → check routing entropy, add load-balancing loss. Degraded GLUE average vs. MoA: If MoL underperforms MoA, verify LoRA injection is inside FFN (not post-FFN) and activation is GeGLU. Merged model accuracy drop >2 points: EMA decay may be too high; router statistics may not generalize to downstream task distribution.
- **First 3 experiments**: Ablation: MoL vs. MoA vs. Static LoRA on GLUE with controlled parameter counts to verify internal injection benefit. Router entropy analysis: Log per-expert selection frequencies across pretraining; diagnose load imbalance if entropy < 1.5 bits for 8 experts. Expert merging sanity check: Train with MoL, merge via EMA, fine-tune on MNLI and RTE; compare against unmerged baseline to quantify compression loss.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Mixture of LoRAs (MoL) framework effectively scale to large autoregressive language models? The conclusion suggests applying parameter sharing with MoL to large autoregressive models to reduce memory footprint while preserving capacity. This is unresolved because the current study exclusively evaluates bidirectional encoder architectures on NLU and retrieval tasks. Pretraining a large generative transformer with MoL and comparing perplexity and generation quality against dense baselines would resolve it.

### Open Question 2
Does the MoL mechanism transfer effectively to multimodal architectures? The conclusion identifies extending the framework to multimodal settings as a promising direction for studying conditional representations across diverse inputs. This is unresolved because the paper limits validation to text-only benchmarks. Integrating MoL into a vision-language model and evaluating performance on multimodal benchmarks against static adapter baselines would resolve it.

### Open Question 3
Is the success of ModernALBERT contingent upon distillation-based initialization? The authors state that initialization from a fully parameterized teacher is "critical," leaving the efficacy of MoL in isolation unclear. It is unknown if MoL provides the same expressivity gains over static recursive transformers when trained from scratch without a teacher signal. Ablation studies training MoL and recursive baselines from random initialization with identical data budgets would resolve it.

## Limitations

- **Missing hyperparameters**: The paper does not specify the LoRA rank (r) and scaling factor (α) values used for the main experiments, which are critical for reproducing the reported performance.
- **Router architecture details**: The exact implementation of the router network (e.g., hidden dimensions, temperature, use of load-balancing auxiliary loss) is not provided, affecting both performance and potential router collapse modes.
- **Distillation procedure**: The exact formulation of the distillation loss and the precise parameter mapping procedure from ModernBERT to ModernALBERT are not detailed, leaving room for initialization mismatch.

## Confidence

- **High Confidence**: The core claim that token-conditional weight-space modulation via MoL restores expressivity in parameter-shared recursive transformers is well-supported by the experimental results, the logical mechanism of addressing expressivity collapse, and the demonstrated compression capability via expert merging.
- **Medium Confidence**: The claim that internal LoRA injection (vs. post-FFN adapters) is the key differentiator for MoL's performance is supported by the GLUE comparison table but lacks strong direct corpus evidence and clear architectural ablation.
- **Low Confidence**: The specific benefits of the EMA merging strategy for expert compression are demonstrated empirically but lack supporting corpus evidence or theoretical justification for why router statistics transfer well to downstream tasks.

## Next Checks

1. **Ablation Study Replication**: Replicate the MoL vs. MoA vs. Static LoRA comparison on GLUE (Table 5) with controlled parameter counts to directly validate the claim that internal injection of LoRA experts into the FFN is the key differentiator for performance.

2. **Router Behavior Analysis**: Log and analyze the per-expert activation frequencies and entropy during pre-training to diagnose potential router collapse (e.g., if entropy < 1.5 bits for 8 experts) and validate the mechanism of conditional computation.

3. **Expert Merging Transfer Check**: Perform the EMA expert merging procedure on a fine-tuned MoL model (e.g., on MNLI and RTE) and compare the accuracy drop against the unmerged baseline to quantify the practical benefits and limitations of the compression method.