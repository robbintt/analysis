---
ver: rpa2
title: Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations
  and Ambient Dimensions
arxiv_id: '2601.21873'
source_url: https://arxiv.org/abs/2601.21873
tags:
- source
- estimation
- target
- transfer
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies transfer learning for structured matrix estimation
  when ambient dimensions and latent representations grow simultaneously. The key
  problem is how to reuse previously learned structure when moving from a source task
  with fixed representation to a target task with expanded ambient space and representation.
---

# Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions

## Quick Facts
- arXiv ID: 2601.21873
- Source URL: https://arxiv.org/abs/2601.21873
- Reference count: 40
- Primary result: Transfer learning estimator achieves strictly improved rates over single-task estimation when rank and sparsity increments are small under growing ambient dimensions

## Executive Summary
This paper develops a transfer learning framework for estimating structured matrices (low-rank plus sparse decomposition) when ambient dimensions and latent representations grow simultaneously across tasks. The key innovation is an anchored alternating projection estimator that embeds the source subspace into the target space and estimates only low-dimensional innovations and sparse edits relative to the transferred structure. Theoretical guarantees show that the transfer estimator achieves improved error rates compared to single-task estimation when the rank and sparsity increments are small, with error bounds decomposing into target noise, representation growth, and source estimation terms. Empirical results demonstrate consistent transfer gains in structured covariance estimation under enlarged dimensions.

## Method Summary
The method addresses transfer learning for low-rank plus sparse matrix estimation when moving from a source task with fixed representation to a target task with expanded ambient space and representation. The anchored alternating projection estimator embeds the source subspace into the target space via zero-padding and estimates only low-dimensional innovations and sparse edits relative to the transferred structure. The algorithm iterates between projecting onto sparse edit sets (relative to the embedded source sparse estimate) and anchored low-rank projections (onto the orthogonal complement of the embedded source subspaces). Theoretical guarantees show that the transfer estimator achieves strictly improved rates compared to single-task estimation when rank and sparsity increments are small, with error bounds decomposing into target noise, representation growth, and source estimation terms.

## Key Results
- Transfer estimator achieves improved error rates compared to single-task estimation when rank increment δr,2 and sparse edit budget δs,2 are small
- Error bounds decompose into target noise, representation growth, and source estimation terms, with transfer gains vanishing when increments are large
- Empirical validation shows consistent transfer gains in structured covariance estimation under enlarged dimensions, particularly in low-sample regimes
- Method outperforms both non-transfer structured estimation and single-task PCA across various dimension growth scenarios

## Why This Works (Mechanism)

### Mechanism 1: Anchored Subspace Reuse
If the source subspace is estimated accurately and constitutes a significant portion of the target subspace, embedding it directly into the target space reduces the effective estimation complexity from the full target rank (r₂) to the rank increment (δr,₂). The algorithm treats the embedded source factors as fixed "anchor" columns and projects the target data onto their orthogonal complement, forcing the SVD step to discover only missing innovation directions.

### Mechanism 2: Sparse Edit Isolation
If the target sparse structure deviates only locally from the source, projecting onto a constrained "edit budget" (δs,₂) relative to the anchor prevents overfitting to noise in the high-dimensional target space. Instead of solving for a full sparse matrix, the algorithm solves for residual edits via the operator P_edit, minimizing the search space from O(p₂q₂) to a small subset of entries.

### Mechanism 3: Orthogonal Error Decomposition
If the optimization converges, the total error can be strictly decomposed into source anchor error, target noise, and representation growth error, preventing entanglement of errors across tasks. The analysis aligns the approximation point with the ground truth to separate subspace misalignment from innovation estimation errors.

## Foundational Learning

- **Low-Rank Plus Sparse Decomposition (RPCA)**: The entire framework relies on the assumption that the matrix Θ is the sum of a low-rank latent structure L and a sparse corruption S. Without this, "transferring" a subspace and "editing" sparse entries is meaningless. Quick check: Can you explain why a sparse matrix of all ones cannot be separated from a low-rank matrix of all ones (identifiability failure)?

- **Spectral Shifting and Projections**: The "Anchored Low-Rank Projection" requires projecting data onto the orthogonal complement of the anchor subspace (I - P_eU). Understanding how to manipulate row/column spaces via projection matrices is essential for implementing the algorithm. Quick check: If you have a basis U ∈ R^{p×r}, how do you project a vector y onto the space orthogonal to the span of U?

- **Matrix Embedding Operators**: The paper defines an operator B(·) to map a p₁×q₁ matrix into a p₂×q₂ space by zero-padding. This is the physical mechanism of "representation growth." Quick check: Does zero-padding a matrix change its spectral norm? Does it change its rank?

## Architecture Onboarding

- **Component map**: Inputs (Source Estimates, Target Data) -> Preprocessing (Embedding Operator B(·)) -> Estimator (TransferAltProj) -> Outputs (Target Low-Rank and Sparse)
- **Critical path**: 1) Estimate source structure using standard RPCA, 2) Embed source factors into target dimensions (padding), 3) Initialize S₀ using embedded sparse estimate, 4) Iterate: Compute residual → Project to Sparse Edits → Compute residual → Project to Anchored Low-Rank, 5) Check convergence
- **Design tradeoffs**: Setting increment size too low ignores genuine new features (underfitting); setting it too high wastes the transfer benefit. Calculating precise source anchors is computationally expensive but vital.
- **Failure signatures**: Negative transfer (error higher than single-task PCA) when source anchor is misaligned; stagnation when innovation space is empty; divergence when incoherence assumption violated.
- **First 3 experiments**: 1) Dimensional stress test with growing target dimensions, 2) Ablation on anchor noise by varying source data quality, 3) Sparse edit budget tuning to assess sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
How do estimation errors propagate and accumulate in a sequential chain of M > 2 tasks with growing representations? The paper specializes to M=2 for clarity but states extensions to multiple sequential tasks follow naturally, implying the M>2 case is not formally analyzed. The error decomposition includes a "source estimation error" term; in a chain, the target error becomes the source error for the next task, and accumulation of subspace misalignment across many steps is not quantified.

### Open Question 2
Can the rank increment δr,₂ and sparse edit budget δs,₂ be selected adaptively without prior knowledge of the target structure? The theoretical guarantees rely on knowing the true increment sizes; in practice, overestimating or underestimating these budgets could lead to suboptimal rates or convergence failures.

### Open Question 3
Does the theoretical framework remain valid if the embedding operator B is approximate rather than a strict zero-padding identity? The bounds depend on the orthogonality of innovation subspaces to the embedded anchor; if the embedding is perturbed, this orthogonality is violated, potentially breaking the incoherence assumptions.

## Limitations
- Theoretical guarantees assume deterministic noise bounds and incoherence conditions that may be violated in practice
- The edit budget hyperparameters must be set based on prior knowledge of representation growth, with limited guidance on practical selection strategies
- Deterministic analysis may be conservative compared to stochastic bounds that could capture typical-case behavior more accurately

## Confidence

- **High confidence**: Algorithmic framework and implementation details are clearly specified and reproducible; synthetic experiment design is rigorous
- **Medium confidence**: Theoretical error decomposition and transfer gain conditions are mathematically sound but rely on idealized assumptions; Markov transition application is promising but lacks empirical validation
- **Low confidence**: Real-world applicability for high-dimensional covariance estimation under growing dimensions, given stringent incoherence and deterministic noise requirements

## Next Checks

1. Test transfer performance when source and target subspaces have partial overlap (e.g., 70% shared basis vectors) to assess robustness to imperfect source estimation
2. Implement cross-validation strategies for selecting δr,₂ and δs,₂ when ground truth representation growth is unknown in practice
3. Evaluate the method on real high-dimensional datasets (e.g., neuroimaging or genomics) where ambient dimensions naturally grow across tasks, comparing against non-transfer baselines