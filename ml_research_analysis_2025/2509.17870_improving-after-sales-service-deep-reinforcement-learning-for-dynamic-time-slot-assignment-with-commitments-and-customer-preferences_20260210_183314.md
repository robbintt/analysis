---
ver: rpa2
title: 'Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time
  Slot Assignment with Commitments and Customer Preferences'
arxiv_id: '2509.17870'
source_url: https://arxiv.org/abs/2509.17870
tags:
- time
- customer
- customers
- each
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the dynamic time slot assignment problem
  with commitments and customer preferences (DTSAP-CCP) in after-sales service, where
  an OEM must assign maintenance time slots to customers in real-time while balancing
  customer preferences and operational efficiency. The authors propose two approaches:
  1) an attention-based deep reinforcement learning with rollout execution (ADRL-RE)
  that combines a trained attention-based neural network with online trajectory simulation,
  and 2) a scenario-based planning approach (SBP) that samples scenarios to guide
  time slot assignment.'
---

# Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences

## Quick Facts
- arXiv ID: 2509.17870
- Source URL: https://arxiv.org/abs/2509.17870
- Reference count: 37
- Primary result: ADRL-RE achieves lowest total cost and highest customer satisfaction across dynamic time slot assignment with commitments and preferences

## Executive Summary
This paper addresses the dynamic time slot assignment problem with commitments and customer preferences (DTSAP-CCP) in after-sales service, where an OEM must assign maintenance time slots to customers in real-time while balancing customer preferences and operational efficiency. The authors propose two approaches: ADRL-RE that combines a trained attention-based neural network with online trajectory simulation, and SBP that samples scenarios to guide time slot assignment. Computational experiments demonstrate that ADRL-RE outperforms rule-based and rollout-based benchmarks, achieving the lowest total cost and highest customer satisfaction across various systems.

## Method Summary
The method employs a Modified Attention Model (MAM) as a neural heuristic solver to enable rapid route planning solutions, making rollouts computationally feasible. ADRL-RE uses a policy gradient approach with a greedy rollout baseline, where the attention-based encoder-decoder architecture processes customer features to generate action probabilities. During inference, promising actions are evaluated through rollout execution by sampling future customer scenarios and solving complete trajectories with MAM. The SBP approach samples multiple scenarios and uses OR-Tools to solve multi-period VRPSTW problems, selecting time slots via majority voting.

## Key Results
- ADRL-RE outperforms rule-based and rollout-based benchmarks, achieving the lowest total cost and highest customer satisfaction
- Rollout framework significantly enhances solution quality, with 52.29% improvement in system S4
- ADRL-RE demonstrates stable performance with low variability across different system configurations

## Why This Works (Mechanism)

### Mechanism 1
Neural heuristic solvers can accelerate training and rollout execution in combinatorial optimization settings where traditional solvers are too slow. The Modified Attention Model (MAM) provides millisecond-level VRPSTW solutions by replacing iterative heuristics with learned construction policies, enabling batch processing and GPU parallelization during training.

### Mechanism 2
Rollout execution improves end-to-end learned policies by explicitly simulating future trajectories rather than relying solely on value approximations. The ADRL model generates probability distributions over actions; the rollout framework evaluates promising actions by sampling future customer scenarios, solving complete trajectories with MAM, and selecting the action with lowest expected cost.

### Mechanism 3
Pre-Norm attention layers stabilize training in Transformer-style architectures for sequential combinatorial decisions. Normalization before (rather than after) attention and feed-forward sub-layers reduces gradient magnitude variance across layers, addressing training instability common in deep attention networks.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation with post-decision states**
  - Why needed here: The paper splits state transitions into deterministic post-decision states and stochastic exogenous information arrivals. Understanding this decomposition is essential for implementing the training loop and rollout correctly.
  - Quick check question: Can you explain why the paper separates the transition into S_k → Ŝ_k → S_{k+1} rather than modeling a single transition?

- **Concept: REINFORCE algorithm with greedy rollout baseline**
  - Why needed here: Training uses policy gradient with a baseline network to reduce variance. The baseline is a greedy version of the same policy; understanding this is required to implement the loss computation correctly.
  - Quick check question: Why does the paper use a separate baseline network rather than a simple running average of returns?

- **Concept: Vehicle Routing Problem with Soft Time Windows (VRPSTW)**
  - Why needed here: The RP subproblem must balance travel costs, waiting time, and delay penalties. Understanding the soft time window formulation (constraints 6-7 in the MILP) is necessary to interpret MAM outputs and debug routing failures.
  - Quick check question: What happens when β (delay penalty) is set too low relative to travel costs?

## Architecture Onboarding

- **Component map:**
  - ADRL Encoder: Pre-Norm attention layers → customer features → node embeddings H_L and graph embedding g
  - ADRL Decoder: GRU-based attention layer → TSA embedding Φ, node embeddings, graph embedding → action probabilities
  - MAM: Separate encoder + dual decoders (vehicle selection, customer selection) for VRPSTW construction
  - Rollout Framework: Samples m future customer sequences → evaluates each promising action → selects lowest mean cost action

- **Critical path:** Data generation → MAM pre-training → ADRL training (using MAM for routing) → Online ADRL-RE execution (using MAM for rollouts, OR-Tools for final routing)

- **Design tradeoffs:**
  - MAM speed vs. solution quality: MAM enables training/rollout but produces inferior routes compared to OR-Tools
  - Rollout depth vs. decision latency: More rollout samples improve decisions but increase per-epoch time (~0.74s for 10 samples, ~2s for SBP with 30 scenarios)
  - Promising actions vs. exhaustive search: ADRL-RE only rolls out actions with non-zero probability, sacrificing potential edge cases for speed

- **Failure signatures:**
  - Training divergence: Check if gradient norm clipping (5.0) is active; verify learning rate decay is applied
  - Low SAR with high travel costs: Model may over-prioritize customer preferences; rebalance α (assignment penalty) vs. travel coefficient
  - Inconsistent input lengths during training: Ensure depot-feature padding is applied for batch processing

- **First 3 experiments:**
  1. Replicate S1 system training with default hyperparameters; verify learning curve convergence matches Figure 3(a) within 20% of reported iteration counts.
  2. Ablate rollout framework: Compare EE-ADRL (end-to-end) vs. ADRL-RE on held-out S1 instances; confirm cost gap exceeds 35% as reported.
  3. Sensitivity analysis on β: Run S2 with β ∈ {1, 3, 5} holding α=2 constant; characterize the tradeoff between delay penalty and satisfied assignment ratio.

## Open Questions the Paper Calls Out

### Open Question 1
Can transfer learning techniques effectively adapt the ADRL-RE model to handle varying spatial and temporal customer distributions without retraining? The conclusion suggests exploring transfer learning to improve the model's ability to generalize across different systems with varying demand patterns.

### Open Question 2
How robust is the ADRL-RE approach when deployed in environments with non-stationary customer behavior or sudden external disruptions? The conclusion states that the current approach assumes stable environments and lacks adaptive learning strategies for unpredictable changes.

### Open Question 3
Can the training efficiency of the ADRL model be improved through alternative reinforcement learning algorithms or architectural optimizations? The conclusion identifies the relatively high training time as a limitation and suggests rigorous hyperparameter tuning or new algorithms.

### Open Question 4
How does the deterministic travel time assumption impact the performance of the rollout execution in the presence of stochastic traffic delays? The problem definition defines travel duration as a deterministic function, whereas real-world logistics typically involve uncertainty.

## Limitations
- Computational burden of rollout execution may restrict scalability to larger systems or longer time horizons
- Pre-Norm attention architecture lacks theoretical guarantees for combinatorial optimization settings
- Assumption that future customer arrivals follow training distribution may not hold in practice

## Confidence
- **High Confidence**: Core mechanism of using neural heuristics to accelerate training/rollout is well-supported by experimental results
- **Medium Confidence**: Architectural choices (Pre-Norm layers, MAM design) are plausible but not rigorously validated against alternatives
- **Low Confidence**: Generalizability of results beyond the 2×2 coordinate space and sensitivity to hyperparameter choices remain underexplored

## Next Checks
1. Evaluate ADRL-RE performance across different spatial distributions (e.g., clustered vs. uniform customer locations) to assess generalizability
2. Systematically remove the rollout component and measure degradation in solution quality to quantify the exact contribution of the rollout framework
3. Test the approach on larger instances (e.g., 50+ customers, 5+ vehicles) and measure both solution quality and computation time to identify practical limits of the neural heuristic approach