---
ver: rpa2
title: Active Model Selection for Large Language Models
arxiv_id: '2510.09418'
source_url: https://arxiv.org/abs/2510.09418
tags:
- best
- language
- arxiv
- selection
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLM SELECTOR addresses active model selection for large language\
  \ models, efficiently identifying the best model under limited annotation budgets.\
  \ Unlike prior evaluation approaches that rely on fully annotated datasets, LLM\
  \ SELECTOR adaptively selects the most informative queries to annotate using a two-parameter\
  \ model that measures information gain as Shannon\u2019s mutual information between\
  \ the unknown best model and annotations."
---

# Active Model Selection for Large Language Models

## Quick Facts
- **arXiv ID:** 2510.09418
- **Source URL:** https://arxiv.org/abs/2510.09418
- **Reference count:** 22
- **Key outcome:** LLM SELECTOR reduces annotation costs by up to 59.62% while maintaining competitive identification probability across six benchmarks with 151 LLMs.

## Executive Summary
LLM SELECTOR introduces an active model selection framework that efficiently identifies the best large language model under limited annotation budgets. Unlike prior evaluation approaches that rely on fully annotated datasets, LLM SELECTOR adaptively selects the most informative queries to annotate using a two-parameter model that measures information gain as Shannon's mutual information between the unknown best model and annotations. The framework leverages judge-based oracle annotations, comparing model responses pairwise against a baseline to reduce annotation costs. Experiments show LLM SELECTOR consistently selects best or near-best models with high confidence while significantly reducing annotation costs compared to existing methods.

## Method Summary
The method employs sequential information maximization to select queries one at a time based on their expected information gain about the unknown best model. It uses a two-parameter model (ε_loss, ε_draw) to describe the behavior of the unknown best language model relative to a baseline, enabling tractable posterior updates without requiring model-specific parameters. Weak judges using k-gram language models estimate expected entropy before committing oracle budget, allowing the algorithm to prioritize queries that most reduce uncertainty about model rankings. The framework maintains posterior beliefs about each model being the best and updates them after each oracle annotation using Bayes rule.

## Key Results
- Reduces annotation costs by up to 59.62% compared to existing methods
- Maintains competitive identification probability across six benchmarks
- Consistently selects best or near-best models with high confidence
- Fully model-agnostic requiring no access to internal parameters

## Why This Works (Mechanism)

### Mechanism 1
Sequential query selection based on mutual information reduces annotation cost while maintaining model identification accuracy. At each step, the algorithm selects the query that minimizes expected conditional entropy of the unknown best model given current annotations, greedily maximizing mutual information between the best model and annotations. This focuses resources on queries that most reduce uncertainty about model rankings. The greedy information-maximization strategy approximates the optimal batch solution well enough for practical use.

### Mechanism 2
The two-parameter model (ε_loss, ε_draw) enables tractable posterior updates without requiring model-specific parameters. The annotation model assumes that for the true best model, the probability of losing/drawing against baseline follows P(loss) = ε_loss, P(draw) = ε_draw, P(win) = 1 - ε_loss - ε_draw. After each oracle annotation, posterior beliefs are updated via Bayes rule, allowing efficient belief propagation without accessing internal model parameters.

### Mechanism 3
Weak judges (k-gram models) provide sufficient signal for query prioritization without oracle annotations. For each unannotated query, z weak judges using k-gram language models compute average sequence likelihoods for each model response, selecting the response with higher likelihood. Expected entropy is computed by averaging over all weak judges, enabling entropy estimation before committing oracle budget.

## Foundational Learning

- **Concept: Mutual Information and Entropy**
  - Why needed here: The entire selection criterion depends on understanding I(F; A) = H(F) - H(F|A)—how much annotations reduce uncertainty about the best model.
  - Quick check question: Given two queries, if one produces identical responses from all candidate models while the other shows clear differences, which has higher mutual information with F?

- **Concept: Bayesian Posterior Updates**
  - Why needed here: The algorithm maintains P(F=f_j|A_t) beliefs that update after each annotation via Bayes rule. Understanding how likelihoods combine with priors is essential.
  - Quick check question: After observing that model A beats the baseline on query q, how does P(F=A|observation) change relative to models that lost to the baseline?

- **Concept: Active Learning and Pool-Based Sampling**
  - Why needed here: The problem is a variant of pool-based active learning where queries are selected from a fixed unlabeled pool to optimize a downstream objective (model selection).
  - Quick check question: Why might uncertainty sampling fail compared to information-theoretic selection in this context?

## Architecture Onboarding

- **Component map:** Candidate Model Pool M -> Query Pool Q -> Baseline Model f̄ -> Weak Judge Ensemble -> Oracle Judge -> Posterior Belief State -> Parameter Store

- **Critical path:** 
  1. Pre-computation: Generate all candidate model responses f_j(q_i) for all queries
  2. Parameter tuning: Run grid search over ε_loss, ε_draw using weak judges as noisy oracle
  3. Initialization: Set uniform prior P(F=f_j|A_0) = 1/m
  4. Iterative selection (repeat b times): Compute expected entropy using weak judges, select query with minimum expected entropy, query oracle, update posterior
  5. Return: Model with highest win rate on annotated queries

- **Design tradeoffs:**
  - Baseline selection: Strong baseline provides more informative comparisons but may not exist in all pools
  - Number of weak judges z: Paper uses z=10 (k=1 to 10); additional judges provide diminishing returns
  - Budget allocation: Higher b increases identification probability but annotation cost
  - Oracle choice: LLM-as-judge vs. human annotators tradeoff between cost and reliability

- **Failure signatures:**
  - Flat identification curves: Poor weak judge correlation with oracle
  - Baseline dominance: All models lose to baseline (win rates < 50%)
  - Posterior collapse: One model dominates early (P → 1 after few annotations)
  - High variance across realizations: Query pool lacks discriminative examples

- **First 3 experiments:**
  1. Sanity check: On MT-Bench with 6 models, 80 queries, run with full budget b=n and verify 100% identification probability
  2. Parameter sensitivity: Fix budget b at 20% of pool size, vary ε_loss and ε_draw, plot identification probability heatmap
  3. Ablation against baselines: On Arena-Hard, compare vs. Random, Bradley-Terry, Confidence, and Uncertainty baselines across budgets

## Open Questions the Paper Calls Out

- **Question 1:** How robust is LLM SELECTOR when the designated baseline model is not a top-performing model? The experiments consistently use high-performing baselines, but the impact of a weak or median baseline on selection accuracy is not analyzed.

- **Question 2:** Can the k-gram "weak judge" approximation effectively guide query selection for tasks requiring complex semantic reasoning? The method relies on n-gram statistics, which may fail to estimate informativeness in reasoning-heavy tasks where syntax diverges from semantic validity.

- **Question 3:** Is it possible to generalize the two-parameter model to remove the need for dataset-specific tuning? The necessity of pre-selection grid search adds overhead, and it's unexplored whether universal parameters could maintain efficiency across diverse domains.

## Limitations

- Parameter tuning sensitivity: Grid search for ε_loss and ε_draw depends heavily on weak judge quality, which is not validated across all benchmarks
- Baseline selection impact: Method's effectiveness depends critically on baseline selection, but sensitivity analysis is limited
- Generalization beyond pairwise: Framework assumes pairwise comparisons against a single baseline, which may not generalize to multi-way ranking scenarios

## Confidence

- **High confidence:** Mutual information framework and sequential selection strategy are well-established theoretically
- **Medium confidence:** Two-parameter annotation model and weak judge approach are reasonable but lack direct validation
- **Low confidence:** Assumption that k-gram weak judges provide sufficient signal across all domains is not thoroughly validated

## Next Checks

1. **Weak judge validation:** On MT-Bench, measure agreement rates between weak judge decisions and oracle judgments; compare identification probability curves using oracle vs. weak judge entropy estimates

2. **Baseline sensitivity analysis:** Systematically vary baseline model strength on Arena-Hard; measure identification probability and annotation efficiency to quantify baseline impact

3. **Domain generalization test:** Apply to code generation benchmark (e.g., HumanEval); measure whether method maintains annotation efficiency compared to text benchmarks and verify weak judge performance degradation