---
ver: rpa2
title: 'Splitformer: An improved early-exit architecture for automatic speech recognition
  on edge devices'
arxiv_id: '2506.18035'
source_url: https://arxiv.org/abs/2506.18035
tags:
- layers
- layer
- exit
- conformer
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of dynamic computational load
  adjustment in neural models for on-device speech recognition, where resources are
  limited and variable. The authors propose Splitformer, an improved early-exit architecture
  inspired by Zipformer, which introduces parallel layers that process downsampled
  versions of the input alongside standard layers.
---

# Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices

## Quick Facts
- arXiv ID: 2506.18035
- Source URL: https://arxiv.org/abs/2506.18035
- Reference count: 22
- Primary result: Splitformer achieves 4.8% WER on LibriSpeech test-clean at exit 12, outperforming conformer-based early-exit baseline (5.1%) while maintaining comparable inference times

## Executive Summary
This paper introduces Splitformer, an early-exit architecture for on-device automatic speech recognition that processes downsampled versions of the input in parallel with standard conformer layers. The design enables the model to exploit variable frame rate analysis, improving ASR performance at lower computational costs without affecting inference time. Experiments on LibriSpeech and TEDLIUM benchmarks show that Splitformer significantly outperforms both a conformer-based early-exit baseline and low-layer exits of large pre-trained models like Wav2Vec2 and WavLM, while maintaining comparable inference times.

## Method Summary
Splitformer builds on conformer-based early-exit ASR architectures by introducing parallel downsampling layers that process inputs at half the frame rate (50Hz → 25Hz) alongside standard conformer layers. The model uses 14 conformer layers grouped into 7 blocks with exit decoders placed every 2 layers. Joint CTC loss is optimized across all exits, with parallel downsampling branches added at the first and last exits. The architecture uses 36.7M parameters and trains with Adam optimizer, warmup equal to dataloader size, and dropout before residual summation.

## Key Results
- Splitformer achieves 4.8% WER on LibriSpeech test-clean at exit 12 vs 5.1% for baseline
- At exit 2, Splitformer achieves 28.2% WER vs 31.4% for baseline on test-clean
- Performance improvements are consistent across LibriSpeech and TEDLIUM benchmarks
- Inference times remain comparable despite 16% parameter increase
- Splitformer outperforms low-layer exits of Wav2Vec2 and WavLM on both datasets

## Why This Works (Mechanism)

### Mechanism 1
Parallel downsampling layers improve early-exit ASR performance, particularly at lower exits, without significantly increasing inference time. The parallel branch processes input at half the frame rate (50Hz → 25Hz), enabling broader acoustic context capture at early layers. This compensates for the limited receptive field in shallow exits by providing multi-resolution temporal analysis.

### Mechanism 2
Joint optimization of all exit losses improves performance across all exit depths compared to single-exit training. Training with the joint CTC loss forces intermediate layers to learn representations useful for decoding, rather than only optimizing final-layer representations. Gradient signals from all exits propagate backward, improving feature quality at every depth.

### Mechanism 3
Higher computational cost at early exits does not translate to longer total inference time due to CTC decoding efficiency gains. Later exits produce more confident predictions with higher blank-token probabilities (>0.95), which are pruned before CTC trellis expansion. This decoding speedup offsets the additional encoder computation at deeper layers.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**: All exit branches use CTC loss; understanding blank tokens, alignment paths, and decoding behavior is essential for interpreting performance and timing results. Quick check: Can you explain why CTC produces many blank tokens and how pruning them affects decoding speed?

- **Conformer Architecture**: Splitformer builds on conformer blocks; understanding convolution-augmented attention, positional encoding, and residual connections is prerequisite. Quick check: What components does a conformer block combine, and why is this hybrid design beneficial for speech?

- **Early-Exit Networks**: The entire paper assumes familiarity with exit branches, resource-aware vs result-aware inference modes, and the motivation for dynamic computation. Quick check: What is the difference between resource-aware and result-aware early-exit strategies?

## Architecture Onboarding

- **Component map**: Audio → MFCC extraction → Conv1D downsample → First 2 conformer layers → split: (a) direct path, (b) parallel downsample path → Both paths sum → Exit 1 decoder → Middle blocks process at 50Hz through layers 3-10 → Final 2 layers → parallel branch re-joins → Exit 6 decoder → Each exit produces CTC logits for decoding

- **Critical path**: 1) Audio → MFCC extraction → Conv1D downsample 2) First 2 conformer layers → split: (a) direct path, (b) parallel downsample path 3) Both paths sum → Exit 1 decoder 4) Middle blocks process at 50Hz through layers 3-10 5) Final 2 layers → parallel branch re-joins → Exit 6 decoder 6) Each exit produces CTC logits for decoding

- **Design tradeoffs**: Parameter increase: Splitformer has 36.7M params vs 31.5M baseline (~16% more); Parallel branches only at first/last exits (not all layers) — empirically best complexity/performance balance; Downsampling factor fixed at 2x; No data augmentation used (unlike Zipformer baseline), making comparisons conservative

- **Failure signatures**: Early exits still show high WER (>25% on test-other at exit 2) — expected for very shallow processing; If inference time increases unexpectedly at early exits, check CTC decoder blank-pruning threshold; If performance degrades at higher exits, verify residual connections correctly sum parallel branch outputs

- **First 3 experiments**: 1) Reproduce baseline comparison: Train EE-baseline and Splitformer on LibriSpeech 100h subset; verify WER gap at exit 2 (target: ~28% vs ~31%) 2) Ablate parallel branch: Remove parallel downsampling from first exit only; measure WER impact to isolate contribution 3) Profile exit timing: Measure encoder vs decoder time separately for each exit; confirm blank-token pruning hypothesis by varying threshold (0.90, 0.95, 0.99)

## Open Questions the Paper Calls Out

1. **Result-aware evaluation**: How does the Splitformer architecture perform in a "result-aware" setting using confidence-based metrics for dynamic exit selection? The authors only focused on fixed-exit "resource-aware" scenarios and did not address automatic exit selection according to reliability measures.

2. **Low-parameter effectiveness**: Does the Splitformer's parallel downsampling approach remain effective when applied to models with significantly fewer parameters? The current experiments use 31.5M+ parameters, and it's unknown if relative performance gains persist with heavily constrained model capacity.

3. **Transfer to SLU**: Can the Splitformer architecture transfer its efficiency gains to Spoken Language Understanding (SLU) tasks? The paper validates the architecture solely on ASR, but SLU tasks require distinct semantic representations which may necessitate different optimal exit depths or downsampling strategies.

## Limitations

- Downsampling trade-off uncertainty: The choice of 2× downsampling at only first and last exits appears empirically motivated but not theoretically grounded, and may not generalize to different languages or domains.
- Decoder dependency caveat: The claimed inference time improvements rely on CTC-specific blank-token pruning behavior and may not transfer to attention-based or hybrid decoders.
- Parameter efficiency trade-off: Splitformer increases parameters by ~16% while achieving better early-exit performance, but the paper doesn't explore whether this increase is necessary or if similar gains could be achieved with fewer parameters.

## Confidence

- **High confidence (4/5)**: The core claim that Splitformer outperforms EE-baseline across most exits is well-supported by experimental results on standard benchmarks.
- **Medium confidence (3/5)**: The mechanism explanation for why parallel downsampling improves performance is plausible but not rigorously validated.
- **Low confidence (2/5)**: The inference time analysis depends on CTC-specific behavior that may not generalize to other decoding architectures.

## Next Checks

1. **Ablation of downsampling factor**: Systematically vary the downsampling factor (1×, 2×, 4×) and measure WER impact at each exit to validate whether 2× is optimal.

2. **Cross-decoder validation**: Replicate the main experiments using an attention-based decoder instead of CTC to determine whether timing advantages are specific to CTC's blank-token pruning behavior.

3. **Weight sharing analysis**: Train a variant where parallel downsampling branches share weights with the main conformer layers to determine whether the parameter increase is necessary for observed improvements.