---
ver: rpa2
title: 'Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for
  Vital Signs like Blood Pressure'
arxiv_id: '2510.10366'
source_url: https://arxiv.org/abs/2510.10366
tags:
- vision
- tasks
- arxiv
- blood
- vfms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision4PPG transforms one-dimensional PPG signals into image-like
  two-dimensional representations (e.g., Short-Time Fourier Transform) to leverage
  pre-trained Vision Foundation Models for physiological tasks. Using models like
  DINOv3 and SIGLIP-2, the approach achieves state-of-the-art performance in blood
  pressure estimation, outperforming both time-series and PPG-specific foundation
  models.
---

# Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure

## Quick Facts
- **arXiv ID:** 2510.10366
- **Source URL:** https://arxiv.org/abs/2510.10366
- **Reference count:** 40
- **Primary result:** Vision4PPG achieves state-of-the-art blood pressure estimation by transforming PPG signals into 2D images and leveraging pre-trained Vision Foundation Models with parameter-efficient fine-tuning.

## Executive Summary
Vision4PPG introduces a novel approach to non-invasive vital signs estimation by transforming one-dimensional photoplethysmography (PPG) signals into image-like two-dimensional representations such as Short-Time Fourier Transforms (STFT), phase images, and recurrence plots. This transformation enables the use of pre-trained Vision Foundation Models (VFMs) like DINOv3 and SIGLIP-2 for physiological regression tasks including blood pressure, heart rate, respiration rate, SpO2, and blood lab measurements. The method demonstrates superior performance compared to both traditional time-series models and PPG-specific foundation models, achieving state-of-the-art results across multiple datasets while maintaining computational efficiency through parameter-efficient fine-tuning techniques.

## Method Summary
The method transforms 30-second, 40Hz PPG signals into 2D image representations through signal processing techniques including STFT (log-power and phase) and recurrence plots. These image-like features are then normalized and fed into frozen Vision Foundation Models (DINOv3-ViT-B/16 or SIGLIP-2-ViT-B/14) with Low-Rank Adaptation (LoRA) applied to attention matrices. A mask-aware soft attention pooling layer extracts features, which are then processed by a simple regression head (LayerNorm → GELU → Linear) to predict physiological values. The approach leverages the general visual feature extraction capabilities of VFMs pre-trained on massive image datasets, repurposing them for signal analysis through this image transformation pipeline.

## Key Results
- Vision4PPG VFMs win 9/14 blood pressure tasks against PPG-specific (PPG-GPT) and Time-Series (MOMENT) models
- Recurrence plots reduce Systolic BP MAE from 16.42 to 13.22 on PPG-BP dataset
- VFMs show strong generalization across six additional vital signs and blood lab measurement tasks
- Parameter-efficient fine-tuning enables computational efficiency while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1
Transforming 1D PPG signals into 2D image representations (spectrograms/recurrence plots) allows Vision Foundation Models to extract physiological features better than raw 1D time-series processing. The transformation maps temporal dynamics into spatial structures (e.g., frequency over time in STFT, state space trajectories in recurrence plots). VFMs pre-trained on natural images utilize spatial attention mechanisms to recognize these textures and patterns as "visual" features, repurposing their hierarchical feature extraction for signal analysis. Performance degrades if the 2D representation introduces artifacts that VFMs overfit to or if the resolution is insufficient to capture critical high-frequency physiological details.

### Mechanism 2
Large-scale pre-training on diverse web-scale image datasets confers an "emergent" generalization capability for physiological pattern recognition, outperforming domain-specific models. VFMs learn robust, general-purpose visual representations from billions of images. When applied to PPG images, these models utilize this general feature space to map signal variations to physiological states without requiring domain-specific pre-training, effectively treating the spectrogram as a foreign "language" they can visually parse. The mechanism fails if test data distribution shifts significantly from transform statistics seen during fine-tuning, or if "emergent" features are actually spurious correlations specific to the image pre-training domain.

### Mechanism 3
Parameter-Efficient Fine-Tuning (PEFT) allows massive VFMs to adapt to regression tasks on small physiological datasets without catastrophic forgetting or overfitting. By freezing the VFM backbone and injecting trainable low-rank decomposition matrices (LoRA) into the attention layers, the model retains its general visual recognition power while learning a lightweight adapter for the specific PPG-to-vital-sign mapping. If the LoRA rank is too low to capture the complexity of the physiological mapping, or if the regression head is too simple to translate the VFM's high-dimensional features into precise scalar estimates, the mechanism fails.

## Foundational Learning

- **Concept: Short-Time Fourier Transform (STFT) & Recurrence Plots**
  - **Why needed here:** You cannot implement the data pipeline without understanding how to convert raw PPG waves into the 2D tensors (Log-power spectrograms, Phase plots, Recurrence matrices) that the Vision Transformers require as input.
  - **Quick check question:** If you feed a raw 1D PPG wave directly into a ViT-B/16 model without patching or 2D conversion, will it work? (Answer: No, the architecture expects spatial patches).

- **Concept: Vision Transformers (ViT) & Patching**
  - **Why needed here:** The core architecture (DINOv3, SIGLIP-2) relies on splitting images into fixed-size patches (14x14 or 16x16). Understanding this is critical for resizing input spectrograms correctly to avoid padding errors or information loss.
  - **Quick check question:** Why does the paper resize STFT features to multiples of 14 or 16 before feeding them to the model? (Answer: To match the patch size of the specific ViT architecture and generate integer token counts).

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** This is the fine-tuning strategy. You need to understand that you are *not* updating the full weight matrix but rather decomposing weight updates into low-rank matrices to save memory and compute.
  - **Quick check question:** In the context of this paper, does "fine-tuning" imply retraining all 86M parameters of DINOv3? (Answer: No, only the LoRA adapters and regression head are trained).

## Architecture Onboarding

- **Component map:** 1D PPG Signal (30s, 40Hz) -> Signal Transformation (STFT/Recurrence) -> Z-score Normalization -> VFM-Specific Normalization -> Frozen ViT Backbone + LoRA Adapters -> Mask-Aware Attention Pooling -> GELU -> Linear Regression Head

- **Critical path:** The **Normalization Layer** is a critical divergence point. DINOv3 requires ImageNet-specific normalization (mean=[0.485, 0.456, 0.406]) while SIGLIP-2 uses standard 0.5 centering. Mixing these up will misalign the input distribution relative to the pre-trained weights, likely degrading performance immediately.

- **Design tradeoffs:**
  - **STFT vs. Recurrence:** STFT captures frequency (good for HR, Respiration); Recurrence captures dynamics/morphology (good for SBP in specific datasets like PPG-BP).
  - **DINOv3 vs. SIGLIP-2:** DINOv3 uses registers and patch-size 16; SIGLIP-2 uses patch-size 14 and has no register tokens. Choice impacts resolution handling and compute cost.
  - **LoRA rank:** The paper chooses a conservative r=8. Increasing this might improve accuracy on complex tasks (Lab estimation) at the cost of efficiency.

- **Failure signatures:**
  - **High MAE on Lab values (e.g., Lactate):** Indicated in Table 2 where VFMs slightly lag or tie baselines. Suggests the visual features may lack the specific temporal resolution or domain knowledge for chemical biomarkers.
  - **Overfitting on small datasets:** If validation loss diverges while training loss drops, the LoRA rank may be too high or the regression head too complex for the data size.
  - **Shape Mismatch Errors:** Forgetting to pad the 2D representation to a multiple of patch size (14 or 16) will cause the patch embedding layer to crash.

- **First 3 experiments:**
  1. **Sanity Check (DINOv3 vs. SIGLIP-2 on STFT):** Implement the full pipeline using only STFT (log-power) for Heart Rate estimation on a single dataset (e.g., DALIA) to verify the normalization and patching logic works for both architectures.
  2. **Transform Ablation (BP Estimation):** Run BP estimation on the PPG-BP dataset comparing STFT vs. Recurrence plots to replicate the finding that Recurrence plots significantly reduce Systolic BP error (MAE 13.22 vs 16.42).
  3. **Generalization Test (Zero-shot):** Take the fine-tuned model from Experiment 2 and test it on the "Aurora-Oscillometric" dataset without further training to assess how well the "emergent" features handle dataset shifts.

## Open Questions the Paper Calls Out

### Open Question 1
Does a learned feature fusion or pooling architecture combining STFT, phase, and recurrence plots yield consistently superior performance across all datasets compared to single representation methods? The authors note in Section 6 that Table 3 shows "scattered improvements" and states: "We defer such investigation of feature pooling and/or learned fusion to future work." Different 2D representations currently excel on different datasets (e.g., recurrence plots perform best on PPG-BP SBP, while STFT wins on Aurora-O), making it unclear if a unified multi-representation model can capture these complementary strengths effectively. Comparative results showing that a fused model achieves lower Mean Absolute Error (MAE) on the majority of the seven blood pressure datasets compared to the best-performing single-representation baselines would resolve this.

### Open Question 2
How does the performance of Vision4PPG scale with the size of the Vision Foundation Model (VFM) backbone? The conclusion explicitly states: "In the future, we will report the scalability of Vision4PPG with bigger VFMs." The study only evaluates the base variants (approx. 86M parameters) of DINOv3 and SIGLIP-2. It remains unknown if increasing model capacity (e.g., to Large or Giant variants) follows standard scaling laws for physiological signal regression or if it leads to overfitting on limited medical data. Benchmarking blood pressure estimation error rates using larger VFM architectures (e.g., ViT-L or ViT-G) would establish a relationship between parameter count and physiological estimation accuracy.

### Open Question 3
Can Vision Foundation Models trained on non-natural images (e.g., satellite or remote sensing data) provide effective or superior feature extraction for PPG analysis? The conclusion proposes to "explore the generalizability of domain-specific vision models, such as those for satellite images." Current success relies on VFMs pre-trained on massive datasets of natural images (Instagram/WebLI). It is unclear if the low-level textural and geometric features learned from domain-specific imagery transfer effectively to "imagified" physiological signals. A comparative analysis fine-tuning satellite-image VFMs against natural-image VFMs (like DINOv3) on the Vision4PPG pipeline using identical PPG datasets would resolve this.

## Limitations
- **Data source ambiguity:** The paper reports performance on specific test sets but does not clarify whether training data was drawn from the same datasets or separate sources, limiting reproducibility.
- **Transform hyperparameter gaps:** Critical implementation details like window size, hop length, and Gaussian bandwidth for recurrence plots are unspecified, potentially impacting feature quality.
- **Optimization protocol opacity:** Missing details about learning rate schedules, batch sizes, optimizer choice, and training duration create uncertainty about result optimality.

## Confidence
- **High Confidence:** The general architectural approach (transforming PPG to 2D images, using VFMs with PEFT) is technically sound and well-supported by the methodology description.
- **Medium Confidence:** The claim that VFMs outperform both time-series and PPG-specific models is supported by presented results, but lack of specified training procedures tempers certainty about generalizability.
- **Low Confidence:** The "emergent capability" narrative suggesting VFMs can generalize to tasks like blood lab measurements with minimal adaptation is the most speculative claim, particularly given mixed performance on lab values.

## Next Checks
1. **Training Data Verification:** Obtain and document the complete training dataset composition, including whether models were trained on data from the same distribution as test sets or truly held-out data, to assess the validity of reported generalization claims.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary STFT parameters (window size, hop length) and LoRA rank to determine sensitivity and identify optimal configurations, particularly for tasks where VFMs showed weaker performance (blood lab measurements).

3. **Zero-Shot Transfer Testing:** Evaluate pre-trained VFMs on PPG data without any fine-tuning to distinguish between genuine emergent capabilities versus adapter-based learning, particularly for the blood lab measurement tasks where the visual-to-chemical mapping is least intuitive.