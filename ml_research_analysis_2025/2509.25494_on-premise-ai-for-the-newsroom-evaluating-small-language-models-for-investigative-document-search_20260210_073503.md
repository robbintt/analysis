---
ver: rpa2
title: 'On-Premise AI for the Newsroom: Evaluating Small Language Models for Investigative
  Document Search'
arxiv_id: '2509.25494'
source_url: https://arxiv.org/abs/2509.25494
tags:
- document
- search
- corpus
- system
- investigative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates small, locally-deployable language models
  for investigative document search in newsrooms, addressing concerns about hallucination
  risks and data privacy. A five-stage pipeline combining corpus summarization, search
  planning, parallel thread execution, quality evaluation, and synthesis uses transparent
  citation chains to maintain editorial control.
---

# On-Premise AI for the Newsroom: Evaluating Small Language Models for Investigative Document Search

## Quick Facts
- arXiv ID: 2509.25494
- Source URL: https://arxiv.org/abs/2509.25494
- Authors: Nick Hagar; Nicholas Diakopoulos; Jeremy Gilbert
- Reference count: 31
- Primary result: Small, locally-deployable language models can effectively assist investigative journalism while preserving verification standards and data security

## Executive Summary
This study evaluates small language models for investigative document search in newsrooms, addressing critical concerns about hallucination risks and data privacy. The research introduces a five-stage pipeline that combines corpus summarization, search planning, parallel thread execution, quality evaluation, and synthesis, all while maintaining complete auditability through explicit citation chains. Testing Gemma 3 12B, Qwen 3 14B, and GPT-OSS 20B on two document corpora, the system achieved high citation validity and ran effectively on standard desktop hardware with 24 GB memory. Qwen 3 14B showed the most consistent performance, while GPT-OSS 20B's results varied dramatically based on training data overlap with corpus content. The findings demonstrate that small models can provide effective AI assistance for investigative journalism while preserving verification standards and data security.

## Method Summary
The research employs a five-stage RAG pipeline for investigative document search, using locally-deployable LLMs on standard desktop hardware. The method involves processing two corpora—64 documents on AI environmental impact and 292 Trump Scotland documents—through stages of corpus synopsis, search planning, parallel thread execution, quality evaluation, and synthesis. Three 4-bit quantized models (Gemma 3 12B, Qwen 3 14B, GPT-OSS 20B) run on MacBook Air M3 with 24GB unified memory via LM Studio. Documents are chunked, embedded, and indexed with citation keys generated via chunk hashing. The system runs 5-7 parallel search threads per corpus, with typical runtime of 2-3 hours per complete analysis.

## Key Results
- Qwen 3 14B delivered the most consistent performance across both specialized and general-topic corpora
- GPT-OSS 20B showed dramatic performance variation based on training data overlap, with hallucination severity index ranging from 1.50 to 22.50 across different corpora
- The system achieved high citation validity and ran effectively on standard desktop hardware (24 GB memory) without requiring cloud infrastructure

## Why This Works (Mechanism)

### Mechanism 1
Explicit citation chains with document chunk hashing enable transparent verification and reduce reliance on model parametric memory. Each retrieved chunk receives a hash-based citation key that resolves to the exact source passage, creating an auditable trail from output to source. Models reliably copy and apply citation keys when generating claims, though Gemma 3 12B showed 10% invalid citation rate indicating this can degrade.

### Mechanism 2
Training data overlap with corpus content causes models to conflate retrieved evidence with parametric knowledge, increasing hallucination risk. When corpus topics are well-represented in pre-training data, models may reference "known" facts not actually retrieved, producing confident but ungrounded claims. This effect diminishes for niche or specialized corpora, as evidenced by GPT-OSS 20B's HSI dropping from 22.50 to 1.50 on the specialized Trump Scotland corpus.

### Mechanism 3
Multi-stage pipeline architectures without correction mechanisms propagate errors from thread-level reports into final synthesis. Parallel search threads generate reports; a synthesis stage merges them without validation or filtering of thread-level hallucinations, meaning errors persist and accumulate in the final output. The evaluation stage attempts quality filtering but only scores relevance/coverage, not factual accuracy.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed here—the entire system is a RAG pipeline combining semantic search with LLM synthesis. Quick check: Can you explain why RAG aims to reduce hallucinations compared to standalone LLMs?

- **Model Quantization**: Why needed here—all tested models use 4-bit quantization to fit in 24GB memory. Understanding precision-memory tradeoffs is essential for replicating hardware constraints. Quick check: What is the expected accuracy impact of reducing model weights from FP16 to 4-bit quantization?

- **Semantic Search / Vector Embeddings**: Why needed here—documents are chunked, embedded, and indexed for retrieval. The system's effectiveness depends on embedding quality and chunking strategy. Quick check: How does semantic search differ from keyword search in matching queries to documents?

## Architecture Onboarding

- **Component map**: Corpus Synopsis → Search Planning → Execution → Evaluation → Synthesis
- **Critical path**: Stage 1 → Stage 2 → Stage 3 → Stage 4 → Stage 5. Failure at Stage 4 blocks synthesis. High token counts at Stage 1 can exceed memory (Gemma 3 12B failed on Trump Scotland synopsis).
- **Design tradeoffs**: Small vs. large models fit in 24GB but show higher hallucination variance than cloud models; parallel threads improve coverage but increase synthesis complexity; transparent citations may constrain generation fluency.
- **Failure signatures**: Memory overflow (Gemma 3 12B could not process Trump Scotland synopsis); hallucination cascade (GPT-OSS 20B produced HSI of 22.50 on familiar-topic corpus); synthesis introducing errors (Qwen 3 14B introduced 2 severe hallucinations during synthesis); plan non-adherence (Gemma 3 12B completed only 79% of planned sub-objectives).
- **First 3 experiments**: 
  1. Baseline validation: Run Qwen 3 14B on small test corpus and manually verify citation validity and claim support rate against ground truth.
  2. Training overlap stress test: Compare model performance on well-represented vs. niche domains to quantify contamination effect observed with GPT-OSS.
  3. Error propagation measurement: Inject known false claims into thread reports and measure whether synthesis stage propagates, modifies, or filters them.

## Open Questions the Paper Calls Out

### Open Question 1
How does training data overlap with corpus content systematically affect hallucination rates in small language models used for document search? The interaction between model architecture, training data overlap, and document diversity requires further investigation to optimize system performance across different investigative contexts. GPT-OSS 20B showed dramatically different hallucination severity across corpora, with authors hypothesizing training data overlap as the cause but not confirming the mechanism.

### Open Question 2
Can correction mechanisms be integrated into multi-stage synthesis pipelines to prevent error propagation from individual thread reports? The system lacks any correction mechanism, meaning hallucinated or incorrect information from individual thread reports propagates unchanged into the final output. This error persistence represents a fundamental challenge for multi-stage agentic systems.

### Open Question 3
How can editorial newsworthiness assessments be systematically integrated into AI-assisted document search evaluation? The evaluation prioritized functional accuracy and citation validity over editorial judgment. Future work should incorporate newsworthiness assessments.

## Limitations
- The system lacks any correction mechanism, allowing hallucinated information from individual search threads to propagate unchanged into final synthesis
- Performance variance across models and corpora suggests training data contamination is a significant but poorly characterized risk, with no standardized metrics for measuring this effect
- The evaluation methodology's reliance on manual annotation for metrics like hallucination severity introduces reproducibility challenges and lacks standardized hallucination metrics for document-grounded generation

## Confidence
- **High Confidence**: Transparent citation chains enable verification and reduce parametric memory reliance
- **High Confidence**: Error propagation through multi-stage synthesis is a real failure mode
- **Medium Confidence**: Training data overlap causes corpus-specific hallucination spikes
- **Medium Confidence**: Qwen 3 14B delivers most consistent performance

## Next Checks
1. Hallucination Mechanism Isolation: Run controlled experiments where identical queries are posed to models using both retrieved corpus content and no-retrieval conditions. Compare claim validity rates to quantify how much hallucination stems from parametric knowledge vs. retrieval errors.

2. Citation Chain Robustness Testing: Systematically corrupt citation keys in thread reports and measure whether synthesis stage catches or propagates these errors. This tests the audit trail's reliability under adversarial conditions.

3. Correction Mechanism Validation: Implement a simple fact-checking stage between thread generation and synthesis that re-queries the corpus for each claim's citation. Measure reduction in propagated hallucinations versus baseline to establish whether error correction is technically feasible within memory constraints.