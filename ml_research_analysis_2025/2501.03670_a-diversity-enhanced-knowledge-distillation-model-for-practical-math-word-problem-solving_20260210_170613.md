---
ver: rpa2
title: A Diversity-Enhanced Knowledge Distillation Model for Practical Math Word Problem
  Solving
arxiv_id: '2501.03670'
source_url: https://arxiv.org/abs/2501.03670
tags:
- divkd
- diversity
- knowledge
- equations
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a diversity-enhanced knowledge distillation
  (DivKD) model for math word problem (MWP) solving. The method introduces an adaptive
  diversity knowledge distillation (AdaKD) approach that selectively transfers high-quality
  knowledge from a teacher model to a student model, and incorporates a conditional
  variational autoencoder (CVAE) to capture the diversity distribution of solution
  equations.
---

# A Diversity-Enhanced Knowledge Distillation Model for Practical Math Word Problem Solving

## Quick Facts
- **arXiv ID:** 2501.03670
- **Source URL:** https://arxiv.org/abs/2501.03670
- **Reference count:** 15
- **Primary result:** Proposes DivKD method achieving up to 4.3% accuracy improvement on MathQA dataset for MWP solving

## Executive Summary
This paper addresses the challenge of generating diverse solution equations in Math Word Problem (MWP) solving by proposing a Diversity-Enhanced Knowledge Distillation (DivKD) model. The method combines an adaptive diversity knowledge distillation approach with a conditional variational autoencoder (CVAE) to enable a student model to learn from a teacher model's diverse, correct solution pathways. Unlike traditional methods that focus on generating a single ground-truth equation, DivKD captures the distribution of valid solutions through answer-matching and latent variable sampling, achieving state-of-the-art performance while maintaining computational efficiency.

## Method Summary
DivKD is a teacher-student framework where a pre-trained encoder-decoder (teacher) transfers knowledge to an enhanced student model. The key innovation is AdaKD, which uses beam search on the teacher to generate multiple candidate equations, then filters them based on answer correctness rather than expression matching. The student model incorporates a CVAE that learns to sample from a latent diversity distribution, allowing generation of varied valid equations for the same problem. The training combines three losses: CVAE reconstruction with KL regularization, adaptive hard knowledge distillation using filtered correct equations, and adaptive soft knowledge distillation using weighted teacher distributions.

## Key Results
- Achieves up to 4.3% accuracy improvement on MathQA dataset compared to strong baselines
- Demonstrates superior performance on four benchmark datasets (Math23K, MAWPS, MathQA, SVAMP)
- Maintains computational efficiency by using a single decoder with latent sampling versus multiple decoders
- Shows significant gains in generating diverse correct equations per problem

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Filtering teacher outputs by calculating answer correctness (rather than expression matching) allows the student to learn valid, diverse solution pathways that were absent from the original ground truth.
- **Mechanism:** The Adaptive Diversity Knowledge Distillation (AdaKD) performs beam search on the teacher to generate top-K equations. Instead of merely matching these to the single ground-truth expression, it evaluates them by computing their numerical result. Equations that yield the correct answer are assigned high weights ($\omega_x$) and used as adaptive hard labels, while noisy teacher predictions are down-weighted or ignored.
- **Core assumption:** The teacher model possesses the capacity to generate diverse, mathematically valid equations within its beam search results, even if they differ syntactically from the single ground-truth annotation.
- **Evidence anchors:**
  - [Section 4.2]: "We can assess the quality of the teacher's soft labels based on the outcomes of the beam search... if an equation's result matches the ground-truth value, it is considered a correct equation."
  - [Figure 1]: Visualizes how teacher beam search produces diverse equations (e.g., $6.0 \times (8.0 + 2.0)$ vs $6.0 \times 8.0 + 6.0 \times 2.0$) that are mathematically equivalent but structurally different.
- **Break condition:** If the teacher model's beam search consistently fails to generate any equations (correct or incorrect) beyond the ground truth, the adaptive hard label set $D_{kd}$ will be empty, reducing the mechanism to standard KD.

### Mechanism 2
- **Claim:** Injecting a latent variable $z$ into the decoder allows the student model to capture the distribution of diverse solutions rather than memorizing a single deterministic mapping.
- **Mechanism:** The Diversity Prior-Enhanced Student model integrates a Conditional Variational Autoencoder (CVAE). During training, it learns a posterior distribution $q(z|x, y)$ conditioned on both the problem and the equation. During inference, it samples $z$ from a prior distribution $p(z|x)$ (Gaussian parameters derived from the problem text), effectively allowing the decoder to generate varied valid equations for the same input.
- **Core assumption:** The distribution of valid solution equations for a given problem can be approximated by a Gaussian distribution in a latent space.
- **Evidence anchors:**
  - [Section 4.1.2]: "A latent variable $z$ is introduced to capture the latent distribution over possible solution equations... generate solution equation $y$ from the conditional generative distribution $p(y|x, z)$."
  - [Section 5.5]: Shows quantitative analysis where the DivKD student produces a significantly higher number of correct equations per problem compared to the baseline.
- **Break condition:** If the KL divergence term in the CVAE loss collapses to zero, the latent variable $z$ becomes deterministic, causing the model to lose its generative diversity.

### Mechanism 3
- **Claim:** Modulating the decoder with a latent diversity vector ($h + h_z$) is computationally cheaper and more efficient than running multiple decoders in parallel.
- **Mechanism:** Previous approaches (like TSN-MD) used multiple decoders to generate diverse equations, increasing inference latency linearly with the number of decoders. DivKD modifies the input to a single Tree Decoder by summing the problem representation $h$ and the latent sample representation $h_z$. This achieves diversity via sampling rather than architectural duplication.
- **Core assumption:** The addition of $h_z$ is sufficient to perturb the decoder's trajectory significantly without requiring a full secondary decoding pipeline.
- **Evidence anchors:**
  - [Section 5.4]: "TSN-MD requires significantly more time for inference... In contrast, 'GTS+DivKD' utilizes only one decoder... enhancing performance without compromising efficiency."
  - [Figure 3]: Plots showing testing time for DivKD is nearly identical to the base GTS model, whereas TSN-MD is significantly slower.
- **Break condition:** If the sampling process from the prior network becomes a bottleneck (e.g., requiring thousands of samples to find one correct answer), the efficiency gain over multi-decoder approaches diminishes.

## Foundational Learning

- **Concept:** **Knowledge Distillation (KD)**
  - **Why needed here:** The proposed DivKD is fundamentally a teacher-student framework. Understanding how "soft labels" (probability distributions over operators/numbers) transfer generalization capability is required to grasp the AdaKD loss function.
  - **Quick check question:** How does temperature ($\tau$) in softmax affect the "softness" of the teacher's labels, and why might this help the student model?

- **Concept:** **Variational Autoencoders (VAE) & CVAE**
  - **Why needed here:** The student model is a CVAE. You must understand the reparameterization trick ($h_z = \mu + \sigma \odot \epsilon$) and the role of KL divergence to debug the training stability of the diversity prior.
  - **Quick check question:** During training, do we sample $z$ from the prior $p(z|x)$ or the posterior $q(z|x,y)$? Why does this differ from the testing phase?

- **Concept:** **Tree-Structured Decoding (Seq2Tree)**
  - **Why needed here:** The base models (GTS, Graph2Tree) predict expression trees (prefix/preorder traversal) rather than linear sequences to ensure mathematical validity.
  - **Quick check question:** In a tree decoder, how does the model predict the left child versus the right child of an operator node?

## Architecture Onboarding

- **Component map:** Teacher (Frozen) -> AdaKD Filter -> Student Encoder -> Diversity Prior Network -> Student Decoder
- **Critical path:**
  1.  **Teacher Pre-processing:** Run teacher beam search on the dataset to identify correct alternative equations and compute weights $\omega_x$ (Eq. 13).
  2.  **Forward Pass:** Student encodes problem $\to h$. CVAE samples $z$ (using posterior during train, prior during test) $\to h_z$.
  3.  **Loss Computation:** Calculate $\mathcal{L}_{CVAE}$ (Reconstruction + KL), $\mathcal{L}_{AdaHardKD}$ (XE on filtered equations), and $\mathcal{L}_{AdaSoftKD}$ (Weighted KL on distributions).

- **Design tradeoffs:**
  - **Answer Accuracy vs. Expression Accuracy:** The paper deliberately uses *answer accuracy* (calculation result matches ground truth) to filter teacher equations. This allows learning structurally different valid equations (e.g., $a \times (b+c)$ vs $a \times b + a \times c$) but risks learning mathematically equivalent equations that might be computationally inefficient or non-canonical forms.
  - **Beam Size ($K$):** A larger $K$ increases the chance of finding diverse valid equations for AdaHardKD but increases the preprocessing time for the teacher.

- **Failure signatures:**
  - **Posterior Collapse:** The KL divergence term drops to near zero early in training. The model ignores the latent variable $z$, resulting in deterministic outputs (low diversity).
  - **Teacher Noise Propagation:** If the attenuation factor $\lambda$ or weight $\omega_x$ is set too high for a low-quality teacher, the student learns incorrect equations.

- **First 3 experiments:**
  1.  **Sanity Check - Teacher Diversity:** Before training the student, analyze the teacher's beam search output. Does the teacher actually generate valid alternative equations in the top-K list? If not, AdaHardKD has no data to work with.
  2.  **Ablation - Loss Components:** Train three student variants: (A) CVAE only (no KD), (B) AdaHardKD only, (C) AdaSoftKD only. Compare against the full DivKD to isolate which component drives the 4.3% accuracy gain on MathQA.
  3.  **Latency Benchmark:** Profile the inference time of `Ro-Graph2Tree-Z + DivKD` vs. `TSN-MD`. Verify the claim that adding the CVAE does not significantly increase latency compared to adding a second decoder.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DivKD model perform when applied to datasets requiring high-complexity mathematical reasoning, such as GSM8K or MATH?
- Basis in paper: [explicit] Section 6 (Conclusion and Future Work) explicitly identifies evaluating the robustness of the method on complex datasets, specifically listing CM17K, GSM8K, and MATH, as a future research direction.
- Why unresolved: The current study evaluates the model on benchmarks (e.g., Math23K, SVAMP) that primarily involve simpler arithmetic, whereas the listed future datasets involve more diverse and difficult reasoning chains.
- What evidence would resolve it: Reporting answer accuracy metrics and convergence speeds for the DivKD model when trained and tested on the GSM8K and MATH benchmarks.

### Open Question 2
- Question: Can the DivKD framework effectively distill knowledge from Large Language Models (LLMs) into efficient student models suitable for mobile educational applications?
- Basis in paper: [explicit] Section 6 states that a primary direction for future research is compressing large-scale models (like LLaMA or GPT-3.5) into smaller versions to facilitate integration into personalized mobile educational platforms.
- Why unresolved: The current experiments focus on distilling knowledge from standard encoder-decoder models (e.g., Graph2Tree) or PLMs like RoBERTa, rather than the massive, high-performing LLMs mentioned in the discussion.
- What evidence would resolve it: Experiments utilizing a large model (e.g., LLaMA-7B) as the teacher and a lightweight CVAE-student, measuring the balance between parameter reduction and accuracy retention.

### Open Question 3
- Question: Does the reliance on answer-matching rather than structural validity for filtering teacher equations risk reinforcing logically incorrect reasoning paths?
- Basis in paper: [inferred] The AdaHardKD method identifies "correct" equations for the student by checking if the calculated value equals the ground truth, rather than verifying the expression structure (Section 4.2, Footnote 1).
- Why unresolved: An equation might generate the correct answer through a mathematically coincidental or semantically flawed path; learning these "lucky" equations could degrade the student's logical reasoning generalization.
- What evidence would resolve it: A qualitative analysis of the equations selected by AdaHardKD to verify if they represent diverse but structurally valid reasoning steps, or if they include "spurious" correlations that yield correct numbers via invalid logic.

## Limitations

- **Answer Accuracy vs. Expression Validity:** The method's reliance on answer-matching rather than structural validity may reinforce logically incorrect reasoning paths that happen to yield correct numerical results.
- **CVAE Diversity Distribution Assumptions:** The paper assumes Gaussian distributions can adequately model the latent space of valid equations without empirical validation of this assumption.
- **Beam Search Dependency:** The effectiveness of AdaKD heavily depends on the teacher model's ability to generate diverse valid equations during beam search, which is not quantified in the paper.

## Confidence

- **High Confidence:** The computational efficiency claims comparing DivKD to TSN-MD are well-supported by the provided timing analysis in Figure 3 and the architectural explanation of using a single decoder versus multiple decoders.
- **Medium Confidence:** The answer accuracy improvements on benchmark datasets (up to 4.3% on MathQA) are supported by Table 3, though the paper lacks detailed statistical significance testing across multiple runs.
- **Low Confidence:** The claim that the CVAE effectively captures "the distribution of diverse solutions" is weakly supported. While Section 5.5 mentions higher diversity in generated equations, there is no quantitative analysis of the diversity distribution quality or comparison to alternative diversity modeling approaches.

## Next Checks

1. **Teacher Diversity Baseline:** Before implementing DivKD, analyze the teacher model's beam search output on the training data. Measure what percentage of problems have correct alternative equations within the top-K results. If this rate is below 30%, the diversity enhancement mechanism has insufficient data to work with.

2. **KL Divergence Monitoring:** During CVAE training, track the KL divergence term in real-time. If it collapses to near zero within the first 10 epochs, implement KL annealing with a weight schedule increasing from 0.0 to 1.0 over the first 30 epochs to prevent posterior collapse.

3. **Alternative Equation Quality Analysis:** For a sample of problems where the student generates multiple correct answers, analyze whether the alternative equations are computationally equivalent or if some are clearly suboptimal (e.g., requiring more operations, introducing unnecessary complexity). This validates whether diversity enhancement maintains solution quality.