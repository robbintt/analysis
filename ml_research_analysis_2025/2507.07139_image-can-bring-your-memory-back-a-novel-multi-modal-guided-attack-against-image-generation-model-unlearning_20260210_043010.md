---
ver: rpa2
title: 'Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against
  Image Generation Model Unlearning'
arxiv_id: '2507.07139'
source_url: https://arxiv.org/abs/2507.07139
tags:
- image
- attack
- recall
- text
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses vulnerabilities in image generation model unlearning,
  where current techniques fail to completely remove sensitive concepts. The proposed
  method, Recall, exploits the multi-modal conditioning capabilities of diffusion
  models by optimizing adversarial image prompts while keeping text prompts unchanged.
---

# Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning

## Quick Facts
- arXiv ID: 2507.07139
- Source URL: https://arxiv.org/abs/2507.07139
- Reference count: 40
- The proposed Recall method achieves attack success rates of 73.40% to 97.40% across four unlearning tasks while being more efficient than text-based baselines.

## Executive Summary
This paper introduces Recall, a novel adversarial attack against image generation model unlearning that exploits the multi-modal conditioning capabilities of diffusion models. Unlike traditional text-based attacks, Recall optimizes adversarial image prompts while keeping text prompts unchanged, using a reference image containing the target concept to guide the optimization process directly in latent space. The method demonstrates that unlearning techniques which primarily focus on text-conditioned pathways leave image-conditioned pathways vulnerable, allowing successful regeneration of "forgotten" concepts like nudity, artistic styles, and specific objects.

## Method Summary
Recall operates through a three-stage pipeline that leverages the multi-modal conditioning of diffusion models. First, it encodes both a reference image (containing the target concept) and a noisy-initialized adversarial image into latent space using the unlearned model's VAE encoder. Second, it iteratively optimizes the adversarial latent through 50 DDIM steps with 20 momentum-based PGD iterations per step, minimizing the mean squared error between predicted noise residuals from the reference and adversarial latents under identical text conditioning. Third, it decodes the optimized latent to produce an adversarial image that, when combined with the original unchanged text prompt, successfully regenerates the target concept. The optimization is self-contained, using only the unlearned model's components without external classifiers or the original pretrained model.

## Key Results
- Recall achieves average attack success rates of 73.40% to 97.40% across four unlearning tasks (nudity, Van Gogh style, Church, parachute).
- The method is significantly more efficient, requiring approximately 65 seconds per attack versus 140-380 seconds for text-based baselines.
- Recall maintains better semantic alignment with original prompts as measured by CLIP scores while achieving comparable diversity metrics to baseline methods.

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal conditioning provides a bypass pathway that text-only defenses do not account for. By optimizing adversarial perturbations in the image modality while keeping the text prompt unchanged, Recall activates residual concept representations that survive text-directed unlearning. This works because unlearning methods predominantly modify text-conditioned pathways while leaving image-conditioned pathways insufficiently suppressed.

### Mechanism 2
Latent-space optimization guided by a reference image's noise prediction aligns the adversarial latent with target concept representations. The reference image is encoded to z_ref, and at each diffusion timestep, the U-Net predicts noise for both z_ref and z_adv under the same text conditioning. Minimizing the L2 distance between these predictions forces z_adv to follow the same denoising trajectory as z_ref, recovering the target concept.

### Mechanism 3
Self-contained optimization within the unlearned model eliminates external dependencies and improves efficiency. Recall uses only the unlearned model's encoder, U-Net, and decoder. The reference image provides implicit semantic guidance without requiring external classifiers or the original pretrained model, making the attack both efficient and difficult to detect.

## Foundational Learning

- **Latent Diffusion Models (Stable Diffusion architecture)**: Understanding how VAE encoder/decoder, U-Net denoiser, and CLIP text encoder interact is essential since the entire attack operates in latent space. Quick check: Can you explain why forward diffusion adds noise to z_t while reverse diffusion predicts that noise?

- **Machine Unlearning in Generative Models**: The paper evaluates against 10 unlearning methods (ESD, FMN, UCE, AdvUnlearn, etc.). Understanding their failure modes informs why multi-modal attacks succeed. Quick check: What is the difference between fine-tuning-based unlearning (ESD) and closed-form editing (UCE)?

- **Multi-Modal Conditioning in Diffusion**: Stable Diffusion supports image-plus-text guidance (img2img). This capability is the attack surface Recall exploits. Quick check: In Stable Diffusion's img2img mode, how does the input image influence the generation compared to text-only conditioning?

## Architecture Onboarding

- **Component map**: P_ref -> E_i -> z_ref; P_init^image -> E_i -> z_adv; h_t = E_t(P_text); ε̂_ref, ε̂_adv = F_θ(z_t, t, h_t); P_adv^image = D_i(z_adv); I* = G_u(P_adv^image, P_text)

- **Critical path**: 1. Initialize P_init^image = λ · P_ref + (1-λ) · δ; 2. Encode: z_ref = E_i(P_ref), z_adv = E_i(P_init^image); 3. For each DDIM timestep t from T→0: predict noise, compute L_adv = ||ε̂_ref - ε̂_adv||²₂, update z_adv via momentum PGD with periodic reference injection; 4. Decode: P_adv^image = D_i(z_adv); 5. Generate: I* = G_u(P_adv^image, P_text)

- **Design tradeoffs**: λ (initial blending) optimal ≈0.25; η (step size) optimal ≈1e-3; γ (reference injection) optimal ≈0.05; reference image must contain target concept; background bias can emerge.

- **Failure signatures**: ASR near 0% indicates concept may be truly erased or reference lacks target; high ASR but low CLIP score means attack succeeds but semantic alignment lost; slow convergence (>200s) suggests unnecessary external components; low diversity indicates overfitting to reference.

- **First 3 experiments**: 1. Run Recall against ESD-x (nudity) with default hyperparameters, verify ASR >70% and CLIP score >30; 2. Use 3 different reference images for same task, confirm ASR variance <10%; 3. Test Recall on SD 2.0 and SD 2.1 unlearned models (UCE), confirm ASR remains >90% for style tasks.

## Open Questions the Paper Calls Out

### Open Question 1
Can the background bias introduced by reference images be effectively decoupled from essential semantics during latent optimization to improve output diversity? The current optimization process does not distinguish between core concept features and incidental visual elements from the reference image, potentially limiting diversity. Resolution would require demonstrating modified optimization that maintains high ASR while achieving significantly higher diversity scores.

### Open Question 2
What defense mechanisms can effectively protect unlearned IGMs against multi-modal adversarial attacks while preserving generation quality? Current unlearning methods were designed primarily against text-based attacks and lack explicit defenses against image-guided adversarial optimization in latent space. Resolution would require developing an unlearning method that maintains low ASR (<20%) against Recall while achieving comparable FID/CLIP scores to existing methods.

### Open Question 3
Can multi-modal adversarial attacks succeed under black-box access constraints where adversaries cannot access model gradients or internal latent representations? Recall's efficiency stems from direct latent space optimization using model internals, which may not transfer to scenarios with only query access. Resolution would require demonstrating effective transfer attacks or zeroth-order optimization approaches achieving comparable ASR without gradient access.

### Open Question 4
Does the vulnerability exposed by Recall generalize to other diffusion architectures (e.g., DALL-E, Imagen) and emerging generative paradigms? Experiments are limited to Stable Diffusion variants. Different architectures employ distinct cross-attention mechanisms that may affect susceptibility to image-guided attacks. Resolution would require systematic evaluation across diverse architectures showing whether similar ASR patterns hold.

## Limitations
- The attack success heavily depends on the quality and relevance of reference images - poor reference selection can lead to failed attacks even against weak unlearning methods.
- The method assumes that multi-modal conditioning pathways remain partially intact after unlearning, which may not hold for future defenses that explicitly suppress both text and image modalities.
- While effective across different unlearning methods, evaluation focuses on SD v1.4 and does not extensively test cross-architecture generalization to other diffusion models or newer SD versions.

## Confidence
- High confidence: The core mechanism of latent-space optimization guided by reference image noise prediction is well-supported by mathematical formulation and empirical results.
- Medium confidence: The claim that multi-modal conditioning provides a unique bypass pathway is plausible but not directly validated against defenses that explicitly target image conditioning.
- Medium confidence: The efficiency gains from self-contained optimization are demonstrated through runtime comparisons but could be affected by implementation details and hardware variations.

## Next Checks
1. **Reference image sensitivity analysis**: Systematically test Recall with varying quality reference images (different resolutions, backgrounds, concept prominence) to quantify robustness to reference selection and identify failure thresholds.
2. **Cross-model transferability**: Evaluate Recall against unlearned Stable Diffusion 2.0/2.1 models and other diffusion architectures (e.g., Midjourney, DALL-E) to assess generalizability beyond SD v1.4.
3. **Defense-aware ablation**: Implement a modified unlearning method that explicitly suppresses both text and image conditioning pathways, then test whether Recall's ASR drops to near-zero levels to validate the multi-modal bypass hypothesis.