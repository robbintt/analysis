---
ver: rpa2
title: Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware
  Data Selection
arxiv_id: '2507.02378'
source_url: https://arxiv.org/abs/2507.02378
tags:
- data
- code
- performance
- selection
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficient training for code-focused
  large language models by improving data quality rather than relying solely on large-scale
  datasets. The authors introduce a parametric model-based data selection approach
  that optimizes feature distribution consistency and diversity in the selected subset,
  avoiding the inefficiencies of discrete selection methods.
---

# Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection

## Quick Facts
- **arXiv ID:** 2507.02378
- **Source URL:** https://arxiv.org/abs/2507.02378
- **Reference count:** 12
- **Primary result:** Achieves 69.5% on HumanEval and 77.2% on MBPP using only 10K samples, outperforming training on the full 92K dataset by 2.4% and 2.3% respectively.

## Executive Summary
This paper addresses the challenge of efficient training for code-focused large language models by improving data quality rather than relying on massive datasets. The authors introduce a parametric model-based data selection approach that optimizes feature distribution consistency and diversity in the selected subset. By encoding instruction texts into feature space and iteratively refining a parametric model, the method selects representative and diverse samples for fine-tuning. Experiments demonstrate that using only 10K samples, the approach achieves strong performance on HumanEval and MBPP benchmarks while requiring significantly less sampling time than baseline methods.

## Method Summary
The method employs a parametric model for code data selection to improve both training efficiency and model performance. It encodes instruction texts into a feature space using a pre-trained encoder, then initializes a parametric model with `m` continuous vectors (where `m` is the target subset size). The optimization process iteratively refines these parameters to ensure distribution consistency with the full dataset while enforcing diversity through a repulsion term. After optimization, the method selects samples whose features are closest to the final parameter vectors. This approach avoids the inefficiencies of discrete selection methods by framing data selection as a continuous optimization problem over a small set of parameters.

## Key Results
- Achieves 69.5% Pass@1 on HumanEval and 77.2% on MBPP using only 10K samples
- Outperforms training on the full 92K dataset by 2.4% and 2.3% respectively
- Requires only 13.5 minutes for selection compared to 19.9 hours for DQ and 7.2 hours for DEITA
- Demonstrates superior performance and efficiency compared to several baseline sampling strategies

## Why This Works (Mechanism)

### Mechanism 1: Distribution Matching via Learnable Parametric Proxies
The method approximates the feature distribution of a massive dataset using a small set of learnable parameters, enabling efficient selection of a representative subset. By optimizing these parameters to occupy the densest regions of the original dataset's feature distribution, the approach ensures that selected samples capture the core semantic content of the full dataset.

### Mechanism 2: Diversity Enforcement via Parameter Repulsion
A repulsion term between parameters prevents selection redundancy by forcing the parameters to spread out across distinct regions of the feature space. This ensures the final selected samples are diverse and cover different aspects of the task distribution, improving model generalization.

### Mechanism 3: Continuous Optimization for Efficiency
Framing data selection as a continuous optimization problem over a small set of parameters is computationally cheaper than discrete methods requiring per-sample inference or pairwise distance calculations. The primary computational cost is a parallelizable similarity matrix calculation between data points and parameters.

## Foundational Learning

- **Feature Space & Cosine Similarity**: Understanding that the entire selection logic operates in a vector space where cosine similarity measures semantic closeness is fundamental. Why needed: The method's success depends on meaningful feature representations. Quick check: If two code instructions have high cosine similarity in feature space, what does that imply about their semantic meaning?

- **Parametric Optimization (Gradient Descent)**: The core innovation replaces discrete selection with a learnable parametric model. Why needed: Readers must grasp how random vectors can be iteratively updated to "move" towards data clusters and "away" from each other. Quick check: Describe what the gradient of the distribution matching loss would encourage a specific parameter to do relative to a nearby data feature.

- **Instruction Tuning & Data Quality**: The paper challenges the "more data is better" paradigm. Why needed: Understanding that the goal is to identify a high-quality core subset that teaches the model more effectively than a larger, noisier dataset is crucial. Quick check: Why does model performance peak at 10K samples and then degrade when using the full 92K dataset?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Parametric Model -> Optimization Loop -> Final Selector
- **Critical path**: The quality of the Feature Extractor is the most critical dependency. If the encoder fails to capture code-specific semantics, the entire geometric optimization is undermined.
- **Design tradeoffs**: 
  - Encoder Generality vs. Specificity: The chosen encoder is a general text model; a code-specific encoder might yield better features but could be larger or slower.
  - Efficiency vs. Granularity: The method is highly efficient because it only optimizes `m` parameters but doesn't provide individual sample quality scores.
  - Hyperparameter Sensitivity: The balance factor between distribution matching and diversity is set to 1 by default and may require tuning.
- **Failure signatures**:
  - Mode Collapse: All parameters converge to the single most dense cluster, resulting in a redundant subset.
  - Outlier Selection: Parameters are pushed into sparse regions, selecting a subset of bizarre or edge-case instructions that harm model training.
  - Semantic Misalignment: If the feature encoder doesn't understand code, semantically distinct instructions might appear similar, causing the model to select only one type.
- **First 3 experiments**:
  1. Ablation on Regularization: Run a sweep on the diversity regularization parameter to visualize the trade-off between distribution consistency and diversity.
  2. Encoder Benchmarking: Substitute the default encoder with a code-specific model and measure the change in downstream HumanEval performance.
  3. Scale Test: Apply the method to a significantly larger dataset to verify that efficiency and optimal subset size hold at scale.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the method generalize to low-resource programming languages where training data is scarce? The authors acknowledge that current experimental validation is limited to Python-specific evaluation benchmarks.
- **Open Question 2**: To what extent does excluding code content during feature extraction limit the method's ability to select diverse and correct samples? The current approach only uses instruction text, potentially missing opportunities to filter out semantically similar code.
- **Open Question 3**: Would integrating an executable sandbox environment to verify functional correctness prior to selection improve model reliability? The authors acknowledge their method does not directly validate the functional correctness of the code.

## Limitations
- The current experimental validation is limited to Python-specific evaluation benchmarks, with no testing on other programming languages.
- The method does not directly validate the functional correctness of the code, potentially allowing flawed samples into the training set.
- The approach relies on a general-purpose text encoder for code instructions, which may not capture code-specific semantics as effectively as domain-specific models.

## Confidence
- **High Confidence**: Experimental results showing strong performance on HumanEval and MBPP with small sample sizes are well-supported and align with the paper's claims.
- **Medium Confidence**: The mechanism of distribution matching via parametric proxies is clearly defined mathematically, but its practical superiority over simpler methods is not fully explored.
- **Low Confidence**: The paper lacks detailed analysis of what specific types of instructions are being selected and how they differ from unselected samples.

## Next Checks
1. Replace the general text encoder with a code-specific embedding model and measure the impact on downstream HumanEval performance to validate feature space quality.
2. Conduct an ablation study on the diversity regularization weight and temperature parameter to determine their impact on the trade-off between distribution consistency and diversity.
3. Apply the method to a significantly larger dataset (>500K samples) to verify whether efficiency and optimal subset size scale linearly with data size.