---
ver: rpa2
title: Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning
arxiv_id: '2506.14251'
source_url: https://arxiv.org/abs/2506.14251
tags:
- fairness
- local
- personalized
- global
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the trade-off among convergence, privacy, and
  fairness in personalized federated learning (PFL), focusing on DP-Ditto, a non-trivial
  extension of Ditto with differential privacy (DP). The authors analyze how DP affects
  convergence and fairness in PFL, which has not been investigated in the literature.
---

# Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning

## Quick Facts
- arXiv ID: 2506.14251
- Source URL: https://arxiv.org/abs/2506.14251
- Reference count: 40
- Primary result: DP-Ditto achieves >32.71% fairness and >9.66% accuracy improvement over DP-perturbed PFL baselines

## Executive Summary
This paper investigates the fundamental trade-offs between convergence, privacy, and fairness in personalized federated learning (PFL) using DP-Ditto, a non-trivial extension of the Ditto framework with differential privacy. The authors derive convergence upper bounds under DP noise, optimize the number of global aggregations given a privacy budget, and analyze how privacy affects fairness in personalized models. Their theoretical analysis reveals that optimal DP-Ditto parameters can simultaneously improve both convergence and fairness metrics. Extensive experiments on MNIST, FMNIST, and CIFAR10 demonstrate that DP-Ditto outperforms DP-perturbed versions of state-of-the-art PFL models (FedAMP, pFedMe, APPLE, FedALA) by significant margins in both accuracy and fairness metrics.

## Method Summary
The authors extend Ditto with differential privacy by adding calibrated Gaussian noise to clipped local model updates during global aggregation. Clients train local models that are aggregated with noise to protect privacy, then use these noisy global models to train personalized models through a regularization term. The key innovation is deriving analytical convergence bounds under DP and optimizing hyperparameters (number of aggregations T* and personalization coefficient λ*) to balance the three competing objectives. The method uses Gaussian mechanism with (ε, δ)-DP, where noise scales with the privacy budget and number of aggregations. Experiments validate the theoretical analysis across multiple datasets and compare against DP-perturbed baselines using standard PFL algorithms.

## Key Results
- Derived convergence upper bound for personalized models under DP-Ditto showing bounded degradation from noise
- Proved existence of optimal aggregation count T* that minimizes convergence-fairness trade-off
- Demonstrated monotonic relationship between privacy noise and optimal personalization coefficient λ*
- Achieved >32.71% fairness improvement and >9.66% accuracy improvement over DP-pFedMe, DP-FedAMP, DP-APPLE, and DP-FedALA baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Calibrated Gaussian noise on local updates provides (ε, δ)-DP while maintaining bounded convergence degradation
- **Mechanism:** Clients clip gradients to sensitivity C, add noise N(0, σ²_u) where σ_u scales with √T. Personalized models ϖ_n depend on noisy global model ω̃ but regularization λ∥ϖ_n - ω*∥² limits deviation
- **Core assumption:** Loss functions are µ-strongly convex and L-smooth with bounded gradient variance
- **Evidence anchors:** Section III-C shows noise scaling formula; Lemma 1 derives convergence bound; related DP-FL work confirms noise mechanisms
- **Break condition:** Misestimated sensitivity ∆s or T exceeding optimal T* causes privacy degradation or noise domination

### Mechanism 2
- **Claim:** Optimal T* minimizes convergence upper bound h(T, λ) and can be found via one-dimensional search
- **Mechanism:** Convergence bound contains competing terms: exponential decay ε^T_L (beneficial) and linear DP noise growth φ_L·T (harmful). Unique minimum exists within bounded interval
- **Core assumption:** ε_L ≠ 1, H_5 > 0, and convexity conditions hold
- **Evidence anchors:** Section IV-B proves T* exists; Figure 2 shows clear minimum; Wei et al. (2023) derives similar T-optimization
- **Break condition:** Privacy budget ϵ → 0 forces T* → 0; perfect IID data may require larger T*

### Mechanism 3
- **Claim:** Optimal λ* decreases monotonically as DP noise variance σ²_z increases
- **Mechanism:** Fairness measure R(λ) depends on α_0(λ) = bλ/((2-λ)ρ + bλ). As σ²_z increases, gradient steepens, forcing optimal α*_0 toward zero, thus decreasing λ*
- **Core assumption:** Clipping threshold C < √d/(2N·S_1), linear regression model with isotropic feature covariance
- **Evidence anchors:** Section V-B Corollary 1 proves λ* decreases; Figure 7a shows empirical relationship; Li et al. (Ditto, 2021) establishes λ fairness trade-off
- **Break condition:** Multiple local optima if C ≥ √d/(2N·S_1); non-linear models require empirical grid search

## Foundational Learning

- **Concept: Differential Privacy (Gaussian Mechanism)**
  - **Why needed here:** DP provides formal privacy guarantees; understanding (ε, δ)-DP, sensitivity ∆s, and noise calibration is essential to grasp why DP-Ditto's noise scaling works
  - **Quick check question:** Given ∆s = 0.1, T = 100, ε = 10, δ = 0.01, calculate σ_u. (Answer: σ_u ≈ 0.043)

- **Concept: Strong Convexity and Smoothness**
  - **Why needed here:** Convergence bounds rely on µ-strong convexity and L-smoothness to guarantee exponential decay terms ε_L < 1
  - **Quick check question:** If Fn is µ-strongly convex with µ = 0.1, what does F(ω) - F(ω*) ≤ (1/2µ)∥∇F(ω)∥² imply about convergence rate?

- **Concept: Multi-Task Learning Regularization**
  - **Why needed here:** The personalization objective uses λ-regularization toward global model; this is a proximal/MOREAU envelope structure common in MTL
  - **Quick check question:** When λ = 0, what does the personalized model reduce to? When λ = 2?

## Architecture Onboarding

- **Component map:**
Server aggregates noisy local models → ω̃^t = (1/N)Σ û^t_n → broadcasts global model
Client n (parallel across N clients):
Local FL model: u^t_n ← u^t_n - η_G·∇F_n(u^t_n)
Clipping: û^t_n ← clip(u^t_n, C)
DP noise: û^t_n ← û^t_n + z^t_n [upload to server]
Personalized model: ϖ^t_n ← ϖ^t_n - η_L·[(1-λ/2)∇F_n(ϖ^t_n) + λ(ϖ^t_n - ω̃^t)]

- **Critical path:** Coupling between FL global model quality and PL personalized model performance. Excessive DP noise in ω̃^t degrades ϖ^t_n unless λ is reduced.

- **Design tradeoffs:**
  - Higher ε (weaker privacy): Lower σ²_z, better convergence, reduced privacy protection
  - Higher λ: More global model reliance, better IID generalization, more sensitive to DP noise
  - Higher T: More communication rounds, better initial convergence, accumulated DP noise eventually dominates

- **Failure signatures:**
  - Divergence after round t_crit: Training loss plateaus then increases → T exceeded optimal T*, reduce aggregation rounds
  - Fairness collapse (high variance): λ too high for given ε → decrease λ per Corollary 1
  - Accuracy floor at ~50%: C too small, clipping destroys gradient information → increase clipping threshold

- **First 3 experiments:**
  1. **Baseline convergence test:** Run DP-Ditto on MNIST with N=20, ε ∈ {1, 10, 100, ∞}, T=100, λ=0.1. Plot training loss vs. T. Verify: (a) convergence bound holds, (b) T* decreases as ε decreases
  2. **λ sensitivity analysis:** Fix ε=10, T=30, vary λ ∈ {0, 0.1, 0.5, 1.0, 2.0}. Plot accuracy and fairness (variance) vs. aggregation round t. Confirm: (a) optimal λ* exists, (b) accuracy peaks then declines for high λ
  3. **Benchmark comparison:** Compare DP-Ditto (with optimal λ*, T*) vs. DP-pFedMe, DP-FedAMP, DP-APPLE, DP-FedALA on CIFAR10 with CNN. Metrics: accuracy, fairness (variance of client losses). Expected: DP-Ditto achieves >9% accuracy and >32% fairness improvement over baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance fairness of personalized federated learning (PFL) be theoretically analyzed for nonlinear models, rather than just linear regression?
- Basis in paper: [explicit] The conclusion states, "In the future, we will analyze fairness for nonlinear PFL models in imperfect wireless environments."
- Why unresolved: The current fairness analysis (Theorem 2) is restricted to a class of personalized Bayesian linear regression models where an analytical solution for the optimal weighting coefficient is derivable
- What evidence would resolve it: A theoretical framework or bound defining fairness for non-convex deep neural networks without relying on closed-form linear solutions

### Open Question 2
- Question: How do imperfect wireless channel conditions (e.g., fading, noise, packet loss) impact the joint optimization of convergence, privacy, and fairness in PFL?
- Basis in paper: [explicit] The conclusion explicitly identifies the need to "analyze fairness for nonlinear PFL models in imperfect wireless environments"
- Why unresolved: The current system model assumes ideal transmission of model parameters, effectively ignoring channel distortion apart from the intentional differential privacy noise
- What evidence would resolve it: A convergence and fairness analysis that incorporates channel fading models or simulations evaluating model performance under varying Signal-to-Noise Ratios (SNR)

### Open Question 3
- Question: Can the convergence guarantees of DP-Ditto be extended to general non-convex loss functions?
- Basis in paper: [inferred] Theorem 1 and Lemma 1 rely on Assumption 1, which requires local loss functions to be µ-strongly convex and L-smooth
- Why unresolved: Real-world deep learning models used in the experiments (DNN, CNN) are non-convex, meaning the theoretical upper bounds derived in the paper do not strictly apply to the empirical results presented
- What evidence would resolve it: Deriving convergence bounds using relaxed assumptions, such as the Polyak-Lojasiewicz (PL) condition, or providing convergence analysis for non-convex objectives

## Limitations

- Theoretical convergence bounds rely on strong convexity and smoothness assumptions that may not hold for deep neural networks on CIFAR10
- Analytical fairness-λ relationship depends on linear regression assumptions (isotropic feature covariance) that don't extend to CNN architectures
- Privacy parameter configurations (particularly δ=0.01) lack contextualization against standard DP practices for PFL applications

## Confidence

- **High Confidence:** DP-Ditto algorithm implementation and basic convergence bound derivation (Lemma 1, Theorem 1)
- **Medium Confidence:** Optimal T* and λ* analytical formulas under simplifying assumptions
- **Medium Confidence:** Fairness-λ monotonic relationship (Corollary 1) given linear model assumptions
- **Low Confidence:** Extension of analytical results to deep CNN models on CIFAR10 without empirical validation of assumptions

## Next Checks

1. **Convergence validation:** Verify empirical convergence curve on MNIST matches theoretical bound h(T,λ) for multiple privacy budgets ε∈{1,10,100}
2. **Assumption testing:** Check gradient smoothness and strong convexity metrics for CNN models on CIFAR10 to validate Theorem 1 applicability
3. **Privacy parameter sensitivity:** Test DP-Ditto performance across wider range of δ values (10⁻³ to 10⁻⁵) to assess practical privacy-utility tradeoffs