---
ver: rpa2
title: 'MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with
  Adaptive Feature Modulation'
arxiv_id: '2512.19438'
source_url: https://arxiv.org/abs/2512.19438
tags:
- image
- watermark
- distortions
- feature
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel deep image watermarking framework,
  MT-Mark, which addresses the fundamental architectural limitation in existing methods:
  the weak coupling between embedding and extraction components. Instead of sequential,
  loss-only optimization, MT-Mark introduces a Collaborative Interaction Mechanism
  (CIM) that enables bidirectional, feature-level communication between the embedder
  and extractor, allowing them to act as mutual teachers.'
---

# MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation

## Quick Facts
- arXiv ID: 2512.19438
- Source URL: https://arxiv.org/abs/2512.19438
- Reference count: 34
- Key outcome: Achieves 99.36% extraction accuracy with only 2 training distortions, outperforming SOTA by 4.47%

## Executive Summary
This paper introduces MT-Mark, a deep image watermarking framework that fundamentally rethinks the architecture of watermarking systems. Unlike traditional methods that optimize embedding and extraction components sequentially with loss functions alone, MT-Mark establishes bidirectional, feature-level communication between these components through its Collaborative Interaction Mechanism (CIM). This enables a mutual-teacher training paradigm where both components can guide each other's learning process. The framework incorporates Adaptive Feature Modulation Modules (AFMMs) that provide content-aware feature regulation, allowing the system to embed watermarks in stable features while suppressing host interference during extraction. Remarkably, MT-Mark achieves state-of-the-art performance even when trained with only two representative distortions, demonstrating strong generalization capabilities.

## Method Summary
MT-Mark consists of a U-Net embedder and a multi-scale extractor connected through CIM. The embedder uses 8 AFMMs strategically placed on skip connections and decoder paths, while the extractor employs 12 AFMMs across three scales. AFMMs decouple modulation into strength and shape branches for content-aware adjustment. CIM extracts modulation states from AFMMs, applies EMA smoothing, projects them cross-end through learned matrices, and fuses them with gating for channel-wise modulation. Training uses only Affine transforms and Gaussian noise, with a loss combining BCE, MSE, and LPIPS objectives. The framework is trained end-to-end for 75 epochs with a batch size of 16 on COCO dataset.

## Key Results
- Achieves 99.36% combined accuracy on 15 distortions compared to 91.49% without CIM
- Maintains PSNR of 38.96dB while achieving superior extraction accuracy
- Demonstrates strong generalization, outperforming SOTA methods by at least 4.47% across all tested distortion categories
- Shows minimal performance degradation when trained with only 2 distortions versus exhaustive distortion sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional feature-level communication between embedder and extractor enables coordinated optimization that surpasses loss-only coupling.
- Mechanism: CIM extracts modulation states from AFMMs on both sides, smooths them via EMA, projects them cross-end through learned matrices, and fuses them with gating for channel-wise adjustments.
- Core assumption: Modulation states from AFMMs encode meaningful information about embedding/extraction behavior that can guide the counterpart component.
- Evidence anchors: Abstract states CIM enables "mutual-teacher training paradigm"; Section 3.2 provides equations for state extraction, EMA, projection, and fusion; ablation shows 91.49% vs 99.36% accuracy with/without CIM.

### Mechanism 2
- Claim: Decoupling modulation into separate strength and shape branches enables content-aware, distortion-resilient feature adjustment.
- Mechanism: AFMM normalizes features, processes them through parallel strength (sigmoid-bounded) and shape (conv-based) branches, multiplies them element-wise, and adds as residual.
- Core assumption: Images have region-varying sensitivity to perturbation, and separating "what to modify" from "how strongly" improves tradeoffs.
- Evidence anchors: Abstract mentions "content-aware feature regulation by decoupling modulation structure and strength"; Section 3.3 provides equations for strength, shape, coupling, and residual; no direct validation in related work.

### Mechanism 3
- Claim: Robustness to unseen distortions emerges from coordinated representation learning rather than exhaustive distortion simulation.
- Mechanism: Training uses only Affine transforms and Gaussian noise; CIM-guided AFMMs learn to embed in stable features and extract from distortion-resilient representations.
- Core assumption: Two representative distortions span sufficient space for learning generalizable robustness when combined with collaborative architecture.
- Evidence anchors: Abstract states performance with "only two representative distortions"; Section 4.3 Tab. 5 shows 99.07% with Affine+Gaussian vs 72.19% without distortions; InvZW uses noise-adversarial training but different approach.

## Foundational Learning

- **Skip-connection injection in U-Net architectures**
  - Why needed here: AFMMs modulate features on skip connections and decoder paths; understanding residual learning helps diagnose whether modulation adds or overwrites information.
  - Quick check question: Can you explain why adding M(F) as a residual (Eq. 12) preserves base representation better than direct replacement?

- **Exponential Moving Average (EMA) for temporal smoothing**
  - Why needed here: CIM uses EMA to smooth instantaneous modulation states; understanding β's role (Eq. 3) is critical for tuning stability vs responsiveness.
  - Quick check question: If β=0.99, how many timesteps before a state change is 50% reflected in the smoothed value?

- **Content-aware attention vs. modulation**
  - Why needed here: AFMM's strength/shape decoupling differs from standard attention; distinguishing them prevents misapplication of attention-tuning techniques.
  - Quick check question: How does multiplicative coupling (c ⊙ t) differ from additive attention (softmax(QK^T)V)?

## Architecture Onboarding

- **Component map**: Embedder U-Net (8 AFMMs) -> CIM state extraction/EMA/projection/fusion -> Extractor multi-scale (12 AFMMs) -> MLP fusion -> Linear projection

- **Critical path**: Embedder AFMM produces modulation map → CIM extracts GAP-compressed state → EMA smooths → cross-end projection provides guidance to extractor AFMM → extractor AFMM fuses self-state with guidance → coordinated feature adjustment on both ends.

- **Design tradeoffs**:
  - More AFMMs → finer-grained modulation but higher memory/compute; paper uses 8 embedder + 12 extractor
  - Higher β (EMA) → more stable states but slower adaptation to distribution shifts
  - Training with more distortions → marginal improvement (98.91% vs 99.07%) with substantial engineering cost

- **Failure signatures**:
  - CIM collapse: Gating α → 0.5 uniformly indicates self-state and cross-end guidance are indistinguishable
  - AFMM saturation: Strength coefficients all near 0 → no modulation; all near 1 → no content-awareness
  - Extractor overfitting: Perfect accuracy on seen distortions, <60% on unseen (especially if single-distortion ablation outperforms)
  - Perceptual degradation: LPIPS > 0.02 or visible artifacts in difference maps indicate embedding too aggressively

- **First 3 experiments**:
  1. Ablate CIM while retaining AFMMs: Compare PSNR and combined accuracy against full model (Tab. 4 shows 33.24 dB vs 38.96 dB, 91.49% vs 99.36%).
  2. Vary distortion training set: Train with (a) none, (b) Affine only, (c) Gaussian only, (d) both, (e) all distortions. Plot combined accuracy (Tab. 5 provides reference points).
  3. Probe modulation state informativeness: Visualize t-SNE of smoothed states s̃_e and s̃_x across different image content and distortion types.

## Open Questions the Paper Calls Out

- **Question**: What are the theoretical limits of generalization when training with only two representative distortions, and can a principled framework be developed for optimal distortion selection?
  - Basis in paper: The paper states "MT-Mark achieves this performance even when trained with only two representative distortions" and notes that Affine was chosen because it "serves as a comprehensive proxy for the entire class of geometric attacks" while Gaussian noise "represents a fundamental, unstructured corruption." However, no theoretical justification is provided for why these two specifically are sufficient or optimal.
  - Why unresolved: The selection appears empirical rather than theoretically grounded, and the paper does not explore whether other minimal distortion combinations would perform equally well or what failure modes might exist.
  - What evidence would resolve it: Systematic experiments varying the choice and number of training distortions, combined with theoretical analysis of distortion coverage in feature space; identification of specific distortion types where the two-distortion approach fails.

- **Question**: How does MT-Mark perform against adversarial attacks specifically designed to remove watermarks, rather than natural distortions?
  - Basis in paper: The evaluation focuses entirely on geometric and signal processing distortions (Tab. 2-3), but does not include adversarial perturbations crafted to maximize extraction error while minimizing perceptual change—a known threat model for watermarking systems.
  - Why unresolved: The collaborative architecture may introduce new vulnerabilities or unexpected robustness properties under adversarial optimization that differ from natural distortion patterns.
  - What evidence would resolve it: Experiments using gradient-based adversarial attacks (e.g., PGD) targeting the extraction network, comparing MT-Mark's adversarial robustness against baseline methods.

- **Question**: What is the computational overhead of the Collaborative Interaction Mechanism during training, and how does it scale with model size or watermark capacity?
  - Basis in paper: The CIM introduces bidirectional feature-level communication with EMA smoothing, cross-end mapping, and fusion operations, but the paper provides no analysis of training time, memory consumption, or scalability compared to standard pipelines.
  - Why unresolved: The mutual-teacher paradigm requires maintaining and exchanging modulation states across training steps, which may impose significant overhead not present in sequential architectures.
  - What evidence would resolve it: Detailed profiling of training time, GPU memory usage, and convergence curves comparing MT-Mark with and without CIM; experiments varying network depth or watermark bit length.

## Limitations

- Several critical architectural parameters are unspecified (EMA coefficient β, MLP' architecture, normalization type, kernel sizes), which may significantly affect performance and reproducibility.
- The claim of strong generalization from only two distortions lacks theoretical justification and has not been validated against domain-specific attacks fundamentally different from geometric/unstructured noise.
- The mechanism's effectiveness depends on the assumption that modulation states encode meaningful behavior information, which remains empirically unverified through state visualization.

## Confidence

- **High confidence**: Performance claims on tested datasets (Tab. 1, 2) showing MT-Mark's consistent superiority in both quality and robustness metrics across multiple benchmarks. The ablation results (Tab. 4, 5) demonstrating the necessity of CIM and effectiveness of minimal-distortion training are directly measured.
- **Medium confidence**: The mechanism descriptions for AFMM and CIM are architecturally coherent and differentiable, but their functional benefits depend on unspecified parameters. The generalization claim is supported by controlled ablation but lacks validation on fundamentally different attack classes.
- **Low confidence**: The assumption that two representative distortions span sufficient space for robust generalization without exhaustive simulation is theoretically plausible but empirically limited to tested distortion families.

## Next Checks

1. **CIM State Informativeness**: Visualize t-SNE embeddings of smoothed modulation states across varying content types and distortion conditions. Cluster analysis will reveal whether CIM learns content-aware coordination (states group by image content) or fails to encode meaningful information (random clustering).

2. **Generalization Stress Test**: Train with only Affine+Gaussian noise, then test on domain-specific attacks (e.g., adversarial patches, semantic manipulations, compression-specific artifacts). Compare against training with more diverse distortions to identify generalization boundaries.

3. **Component Ablation under Realistic Conditions**: Replace CIM with fixed random projections or remove AFMMs entirely while keeping the U-Net backbone. Test on the full 15-distortion benchmark to isolate whether performance gains come from coordination versus increased model capacity.