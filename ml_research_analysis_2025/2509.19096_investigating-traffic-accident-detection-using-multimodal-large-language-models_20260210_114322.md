---
ver: rpa2
title: Investigating Traffic Accident Detection Using Multimodal Large Language Models
arxiv_id: '2509.19096'
source_url: https://arxiv.org/abs/2509.19096
tags:
- accident
- detection
- traffic
- performance
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the zero-shot capabilities of multimodal\
  \ large language models (MLLMs) for traffic accident detection using infrastructure\
  \ camera images. The authors evaluate four MLLMs\u2014Gemini 1.5, Gemini 2.0, Gemma\
  \ 3, and Pixtral\u2014on the simulated DeepAccident dataset from CARLA."
---

# Investigating Traffic Accident Detection Using Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2509.19096
- Source URL: https://arxiv.org/abs/2509.19096
- Reference count: 31
- Primary result: Pixtral achieved highest F1-score (71%) and recall (83%) for traffic accident detection from simulated camera images

## Executive Summary
This paper evaluates multimodal large language models (MLLMs) for traffic accident detection using infrastructure camera images. The authors test four MLLMs—Gemini 1.5, Gemini 2.0, Gemma 3, and Pixtral—on a simulated dataset from CARLA, integrating advanced visual analytics including YOLO for object detection, Deep SORT for tracking, and Segment Anything for segmentation to enhance prompts. Results show Pixtral as the top performer with 71% F1-score and 83% recall, while enhanced prompts generally degraded performance despite improving precision for Gemini models. The study demonstrates the potential of combining MLLMs with visual analytics for automated traffic monitoring, though results highlight challenges in prompt optimization and real-world generalization.

## Method Summary
The authors evaluate four MLLMs using a simulated DeepAccident dataset from CARLA, incorporating advanced visual analytics to enhance prompts and improve explainability. YOLO detects vehicles, Deep SORT tracks multiple objects across frames, and Segment Anything performs segmentation. These visual analytics results are integrated into prompts to provide context for the MLLMs. The study examines zero-shot capabilities without fine-tuning and compares performance across models with and without enhanced prompts. The methodology emphasizes the combination of MLLMs with visual analytics tools to create a comprehensive traffic monitoring system.

## Key Results
- Pixtral achieved the highest performance with 71% F1-score and 83% recall
- Enhanced prompts improved Gemini models' precision but significantly reduced F1-score and recall
- Gemma 3 showed the most balanced performance with minimal metric fluctuation across conditions
- Enhanced prompts did not universally improve performance, potentially introducing noise

## Why This Works (Mechanism)
MLLMs can process visual information and understand complex scenarios when provided with appropriate context. The integration of visual analytics tools (YOLO, Deep SORT, Segment Anything) enhances prompts by providing structured information about objects, their relationships, and scene dynamics. This combination leverages MLLMs' reasoning capabilities while compensating for their limitations in detailed visual analysis. The enhanced prompts create a framework where MLLMs can reason about accident likelihood based on detected objects, their trajectories, and spatial relationships, though the effectiveness depends on optimal prompt construction to avoid introducing noise.

## Foundational Learning
- Multimodal Large Language Models: AI systems that process both text and visual inputs, needed to understand traffic scenes from camera images; quick check: can process and reason about images with textual descriptions
- Zero-shot Learning: Ability to perform tasks without task-specific training, needed to evaluate MLLMs' general capabilities; quick check: performance without fine-tuning on the specific dataset
- Object Detection and Tracking: YOLO for detection and Deep SORT for tracking, needed to identify and monitor vehicles in traffic scenes; quick check: can accurately locate and track vehicles across frames
- Prompt Engineering: Technique to improve model responses by providing context, needed to guide MLLMs toward accurate accident detection; quick check: enhanced prompts improve precision without degrading recall
- Segmentation: Process of dividing images into meaningful regions, needed to identify specific areas and objects of interest; quick check: can accurately separate vehicles from background
- Visual Analytics Integration: Combining multiple computer vision tools, needed to create comprehensive scene understanding; quick check: all tools work together seamlessly in the pipeline

## Architecture Onboarding

**Component Map:**
Image -> YOLO (Object Detection) -> Deep SORT (Multi-Object Tracking) -> Segment Anything (Segmentation) -> Enhanced Prompt Construction -> MLLM (Gemini 1.5, Gemini 2.0, Gemma 3, Pixtral) -> Accident Detection Output

**Critical Path:**
Image acquisition → Object detection → Multi-object tracking → Scene segmentation → Prompt enhancement → MLLM inference → Accident classification

**Design Tradeoffs:**
- Zero-shot approach avoids fine-tuning costs but may limit performance compared to specialized models
- Enhanced prompts add context but risk introducing noise that degrades overall accuracy
- Simulation-based dataset enables controlled evaluation but may not reflect real-world complexity
- Single-image analysis simplifies processing but misses temporal dynamics crucial for accident detection

**Failure Signatures:**
- Enhanced prompts causing performance degradation indicate suboptimal prompt construction
- Large gaps between simulation and real-world performance suggest dataset limitations
- Inconsistent results across models point to varying MLLM capabilities in visual reasoning
- High precision but low recall suggests models are conservative in accident classification

**First 3 Experiments:**
1. Test each MLLM on real-world traffic camera data with ground truth accident labels
2. Perform ablation studies on enhanced prompt components to identify optimal prompt structure
3. Evaluate video sequence processing to incorporate temporal information for accident detection

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation-only dataset may not capture real-world traffic complexity and variability
- Small dataset size (1,120 images) and single simulation environment limit model robustness
- Enhanced prompt strategy introduces noise that degrades performance, suggesting suboptimal implementation
- Single-frame analysis misses temporal dynamics essential for accurate accident detection

## Confidence
- **High** confidence in reported model performance metrics on the tested dataset, given clear methodology and reproducible results
- **Medium** confidence in conclusions about enhanced prompts' effectiveness due to observed performance degradation and unexplained variability
- **Low** confidence in real-world applicability without additional validation on actual traffic camera data, given the simulation-based approach

## Next Checks
1. Evaluate all four MLLMs on real-world traffic camera datasets with ground truth accident labels to assess performance transfer from simulation to reality
2. Implement and test the enhanced prompt strategy with optimized noise reduction techniques, including ablation studies on individual visual analytics components
3. Extend the analysis to video sequences rather than single frames to capture temporal dynamics essential for accurate accident detection