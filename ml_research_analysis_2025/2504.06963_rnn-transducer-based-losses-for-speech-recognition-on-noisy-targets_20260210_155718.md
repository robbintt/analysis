---
ver: rpa2
title: RNN-Transducer-based Losses for Speech Recognition on Noisy Targets
arxiv_id: '2504.06963'
source_url: https://arxiv.org/abs/2504.06963
tags:
- loss
- rnn-t
- data
- training
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces three novel loss functions for RNN-Transducer
  models to improve speech recognition performance when training with noisy transcripts.
  The Star-Transducer loss addresses deletion errors by allowing the model to skip
  frames during alignment, restoring over 90% of performance compared to clean data.
---

# RNN-Transducer-based Losses for Speech Recognition on Noisy Targets

## Quick Facts
- **arXiv ID:** 2504.06963
- **Source URL:** https://arxiv.org/abs/2504.06963
- **Reference count:** 40
- **Primary result:** Introduces Star-Transducer, Bypass-Transducer, and Target-Robust Transducer losses that restore over 70% of RNN-T performance on noisy transcripts by allowing the model to skip frames or tokens during alignment.

## Executive Summary
This paper addresses the challenge of training RNN-Transducer models on imperfect transcripts containing deletions, insertions, or substitutions. The authors propose three novel loss functions that modify the RNN-T alignment lattice to tolerate noisy labels. The Star-Transducer loss handles deletion errors by allowing frame skipping, the Bypass-Transducer loss addresses insertion errors through token skipping with probability-weighted penalties, and the combined Target-Robust Transducer integrates both mechanisms for arbitrary error types. Experiments on LibriSpeech show these losses can restore 70-90% of clean-data performance when trained on transcripts with up to 50% synthetic noise.

## Method Summary
The method involves training RNN-Transducer ASR models on LibriSpeech with synthetically corrupted transcripts using robust loss functions implemented as weighted finite state transducers (WFSTs) in the k2 library. The Fast Conformer encoder with 8x subsampling projects audio to features, which are combined with LSTM prediction network outputs in a joint network. The standard RNN-T loss is replaced with one of three novel variants: Star-Transducer (adds skip-frame arcs with fixed penalty), Bypass-Transducer (adds skip-token arcs with sumexcl probability weighting), or Target-Robust Transducer (combines both). Bypass-T and TRT require a penalty scheduling strategy starting at -20.0 and decaying by 0.9x per epoch from epoch 3. The model is trained with AdamW optimizer using cosine annealing for 60-100 epochs with global batch size 2048.

## Key Results
- Star-Transducer loss restores over 90% of performance on deletion errors compared to clean data
- Bypass-Transducer loss recovers more than 60% of quality on insertion errors
- Target-Robust Transducer loss improves performance on arbitrary error types, restoring over 70% of quality
- On 50% synthetic arbitrary corruption, TRT reduces WER from ~80% (standard RNN-T) to ~11% (test-other)

## Why This Works (Mechanism)

### Mechanism 1: Alignment Relaxation via Skip-Frames (Star-Transducer)
The Star-Transducer loss mitigates deletion errors by allowing the model to ignore specific audio frames during alignment. Standard RNN-T requires every audio frame to align to either a blank or a token, but Star-T introduces a "skip frame" (`<sf>`) transition parallel to the blank transition. This creates a path where the model can emit a "null" token for audio segments that likely contain untranscribed speech, preventing the gradient from being penalized for failing to predict missing text. The core assumption is that the encoder can distinguish between silence and untranscribed speech, and the alignment penalty for skipping a frame can be tuned without collapsing the lattice.

### Mechanism 2: Probability-Weighted Token Skipping (Bypass-Transducer)
The Bypass-Transducer loss addresses insertion errors by allowing the alignment to skip ground-truth tokens using a dynamic, probability-weighted penalty. This loss adds a "skip token" (`<st>`) arc parallel to the emission of the ground-truth token. Unlike Star-T, a static penalty fails here; the model requires a signal to distinguish a "hard" skip from a "soft" one. The paper calculates the weight for this arc as the sum of log-probabilities of all non-blank, non-target outputs (`sumexcl`). If the model believes the correct output is *not* the transcript token, the penalty for skipping the token is low, encouraging the alignment to bypass the error. The core assumption is that the Joint Network's output distribution provides a reliable proxy for "incorrectness" of the ground-truth token at a given step.

### Mechanism 3: Unified Lattice Topology (Target-Robust Transducer)
The Target-Robust Transducer loss combines both skip-frame and skip-token mechanisms to restore performance on arbitrary errors. Substitutions are effectively a deletion (missing audio for a word) and an insertion (extra audio for another word). TRT merges the Star-T and Bypass-T lattices, allowing the forward-backward algorithm to find high-probability paths that simultaneously skip "bad" frames and "bad" tokens. The core assumption is that the hyperparameters for frame-skipping and token-skipping can be scheduled such that they do not interfere destructively during early training.

## Foundational Learning

- **Concept: RNN-Transducer (RNN-T) Forward-Backward Algorithm**
  - **Why needed here:** The proposed losses modify the *lattice* over which the RNN-T loss is calculated. You must understand that RNN-T sums probabilities over all possible alignment paths (T x U grid) rather than a single path.
  - **Quick check question:** How does adding a parallel arc (like a skip-connection) in the RNN-T lattice affect the gradient of the loss function?

- **Concept: Weighted Finite State Transducers (WFSTs) in `k2`**
  - **Why needed here:** The implementation relies on the `k2` library to construct differentiable graphs. The paper maps audio frames and text tokens to a graph structure (Grid-Transducer).
  - **Quick check question:** In the `k2` framework, how do you assign a weight to a specific arc derived from a neural network logit versus a fixed scalar penalty?

- **Concept: Weak Supervision / Noisy Labels**
  - **Why needed here:** The core problem is training on imperfect data. Understanding the difference between deletion (under-transcription) and insertion (over-transcription) noise is critical for selecting the correct loss variant.
  - **Quick check question:** Why is a deletion error generally more damaging to standard RNN-T training than an insertion error of the same magnitude?

## Architecture Onboarding

- **Component map:** Encoder (Fast Conformer) -> Prediction Network (LSTM) -> Joint Network -> Loss Layer (GraphTargetRobustTransducerLoss)
- **Critical path:**
  1. Calculate Joint Network outputs
  2. Build the WFST lattice (using `k2`), injecting `<sf>` and `<st>` arcs for TRT
  3. Populate arcs with logits (Joint output) and penalties (hyperparameters)
  4. Run `get_tot_scores` (forward pass) on the graph to compute the negative log-likelihood

- **Design tradeoffs:**
  - Star-T vs. TRT: Star-T is simpler and handles the most disruptive error (deletions) very well. TRT is more robust but requires tuning two penalty schedules
  - Penalty Scheduling: Bypass-T requires a penalty schedule (start high/strict, decay to low/permissive) to converge. Star-T works well with a fixed small penalty

- **Failure signatures:**
  - Mode Collapse (Empty Output): Skip penalties are too low; the model learns that "skipping" is always cheaper than emitting a token
  - No Recovery (High WER): Skip penalties are too high (effectively infinite); the loss behaves exactly like standard RNN-T, failing to ignore noisy labels
  - Divergence: Using Bypass-T with "constant" or "mean" probability modes instead of "sumexcl" leads to unstable gradients

- **First 3 experiments:**
  1. Baseline Deletion Test: Train standard RNN-T on 50% synthetic deletion noise. Observe WER degradation (expect ~80% WER as per Table 2)
  2. Star-T Validation: Switch to `GraphStarTransducerLoss` with `skip_weight=-0.5` on the same data. Verify WER drops to ~11% (Table 3)
  3. TRT Stress Test: Train `GraphTargetRobustTransducerLoss` on mixed arbitrary errors (50% corruption). Confirm `sumexcl` mode prevents collapse and restores >70% quality (Table 6)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Target-Robust Transducer maintain its efficiency and performance recovery when applied to encoder architectures with lower subsampling rates (e.g., 4x subsampling)?
- Basis in paper: [explicit] The Conclusion states, "In further work, we plan to investigate other models (e.g., Conformer with 4x subsampling)."
- Why unresolved: The experiments were restricted to the Fast Conformer (8x subsampling), and different temporal resolutions could affect the alignment dynamics of the "skip frame" mechanism.
- What evidence would resolve it: Benchmarking the proposed losses on a standard Conformer encoder (4x subsampling) on LibriSpeech to compare WER recovery rates against the Fast Conformer results.

### Open Question 2
- Question: Can the Star-Transducer loss effectively recover punctuation when training on a mixture of datasets with and without punctuation labels?
- Basis in paper: [explicit] The Conclusion suggests that missing punctuation in datasets like LibriSpeech can be viewed as "deletions" and proposes the Star-Transducer for this specific application.
- Why unresolved: The paper only evaluated word-level deletions; it did not test the model's ability to learn punctuation prediction when the label is absent in the ground truth.
- What evidence would resolve it: An experiment training a model on a mix of punctuated and unpunctuated data using Star-Transducer, followed by an evaluation of punctuation prediction accuracy (e.g., F1 score).

### Open Question 3
- Question: How robust are the proposed losses when applied to naturally occurring transcription errors rather than synthetic noise?
- Basis in paper: [inferred] The methodology relies entirely on artificially mutating clean LibriSpeech transcripts (Section 3.3), which may not capture the complex distribution of real-world human transcription errors.
- Why unresolved: Synthetic deletions are random, whereas real errors often follow linguistic patterns; the paper acknowledges the goal is industrial application but validates only on synthetic data.
- What evidence would resolve it: Evaluating the Target-Robust Transducer on a dataset with naturally imperfect labels, such as the unfiltered LibriVox data mentioned in the literature review.

## Limitations
- Implementation dependency on k2 library creates significant adoption barrier as the mechanism is tightly coupled to WFST construction
- Hyperparameter sensitivity for Bypass-T penalty scheduling requires specific decay strategy without theoretical justification
- Limited cross-dataset generalization as all experiments use synthetic noise on LibriSpeech rather than real-world noisy transcripts

## Confidence
- **High Confidence:** Star-Transducer effectively recovers deletion errors (>90% performance restoration); Bypass-T with sumexcl outperforms alternatives for insertions; TRT provides unified framework for arbitrary errors
- **Medium Confidence:** Specific penalty scheduling for Bypass-T (-20.0 start, 0.9x decay) is optimal; sumexcl weight assignment is best heuristic
- **Low Confidence:** Losses will provide similar benefits on real-world noisy transcripts as synthetic noise; sumexcl mechanism is theoretically correct way to assign skip-token weights

## Next Checks
1. Apply Target-Robust Transducer loss to model trained on real-world noisy transcripts (YouTube captions, classroom recordings) and measure WER improvement versus standard RNN-T
2. Systematically vary Bypass-T penalty scheduling parameters (starting penalty, decay rate, starting epoch) on development set to identify sensitivity and optimal configuration
3. Perform detailed WER breakdown by error type (deletion, insertion, substitution) for each loss variant on test set with 50% arbitrary corruption to reveal effectiveness patterns