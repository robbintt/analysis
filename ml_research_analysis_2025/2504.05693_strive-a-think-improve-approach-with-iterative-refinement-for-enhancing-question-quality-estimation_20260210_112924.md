---
ver: rpa2
title: 'STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing
  Question Quality Estimation'
arxiv_id: '2504.05693'
source_url: https://arxiv.org/abs/2504.05693
tags:
- question
- human
- evaluation
- baseline
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically assessing question
  quality in educational contexts, which is time-consuming and often subjective when
  done manually. The authors propose STRIVE, a novel methodology that leverages large
  language models (LLMs) to iteratively refine and evaluate question quality across
  multiple dimensions such as grammaticality, relevance, appropriateness, complexity,
  and novelty.
---

# STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation

## Quick Facts
- arXiv ID: 2504.05693
- Source URL: https://arxiv.org/abs/2504.05693
- Authors: Aniket Deroy; Subhankar Maity
- Reference count: 35
- Primary result: LLM-based iterative refinement significantly improves automated question quality assessment correlation with human judgments across five educational metrics

## Executive Summary
This paper addresses the challenge of automatically assessing question quality in educational contexts, which is time-consuming and often subjective when done manually. The authors propose STRIVE, a novel methodology that leverages large language models (LLMs) to iteratively refine and evaluate question quality across multiple dimensions such as grammaticality, relevance, appropriateness, complexity, and novelty. STRIVE employs two Think & Improve modules that generate and select strengths and weaknesses for each question, iteratively refining the evaluation until convergence is achieved.

Experimental results on two datasets (EduProbe and SciQ) demonstrate that STRIVE significantly improves correlation with human judgments compared to baseline methods. Error analysis reveals notable improvements in relevance and appropriateness scores, with percentage matches between LLM-generated scores and human experts increasing from 45-60% to 60-71% for these metrics. The approach shows that automated question quality assessment can effectively approximate human evaluation, supporting educators in creating better teaching materials and enhancing educational practices.

## Method Summary
STRIVE employs a two-module iterative refinement system where each module generates 10 strength/weakness analysis pairs at different temperature values, then uses an LLM-as-judge to select the best pair and compute metric scores. The modules exchange feedback iteratively until their scores converge (identical for two consecutive iterations). The methodology is evaluated on EduProbe and SciQ datasets using Pearson correlation with human expert judgments across five metrics: grammaticality, relevance, appropriateness, complexity, and novelty. A baseline approach using single LLM calls is compared against the full STRIVE implementation.

## Key Results
- STRIVE achieves 0.47-0.65 Pearson correlation with human judgments across EduProbe and SciQ datasets
- Exact match percentage with human experts increases from 45-60% (baseline) to 60-71% for relevance and appropriateness metrics
- Iterative refinement process improves evaluation quality through structured reasoning and cross-module consensus
- Error analysis shows significant improvements in relevance and appropriateness scores while maintaining grammaticality performance

## Why This Works (Mechanism)

### Mechanism 1: Multi-Sample Evaluation Diversity Through Temperature Variation
Generating multiple strength/weakness analyses at varied temperatures creates a more robust evaluation space than single-pass scoring. The system samples 10 pairs per question using different temperature settings, then uses an LLM-as-judge to select the best evaluation pair and corresponding metric scores. This creates an implicit ensemble effect where diverse reasoning paths are explored before commitment.

### Mechanism 2: Cross-Module Consensus as Quality Signal
Requiring two independent Think & Improve modules to reach identical scores reduces evaluation variance and improves alignment with human judgment. TM1 generates initial evaluation; TM2 receives this as feedback and generates refined evaluations; TM1 then responds to TM2's output. This ping-pong continues until scores from both modules are identical for two consecutive iterations.

### Mechanism 3: Structured Reasoning Via Explicit Strength/Weakness Articulation
Forcing the system to explicitly articulate strengths and weaknesses before scoring improves evaluation reasoning quality compared to direct scoring. Rather than asking LLMs to output scores directly, STRIVE requires generation of textual strength/weakness analyses first, then uses these as context for scoring. This creates a chain-of-thought-like structure where justification precedes rating.

## Foundational Learning

- **Concept: LLM temperature sampling**
  - Why needed here: STRIVE generates multiple evaluations at different temperatures. Understanding that higher temperatures increase output diversity (but potentially noise) is essential for tuning this parameter.
  - Quick check question: If you observe near-identical outputs across 10 samples, which parameter should you adjust and in which direction?

- **Concept: Convergence criteria in iterative systems**
  - Why needed here: The algorithm terminates when TM1 and TM2 scores match for two consecutive iterations. Understanding convergence behavior (including non-convergence risks) is critical for debugging and practical deployment.
  - Quick check question: What happens if TM1 and TM2 never reach identical scores—should you add a maximum iteration cap, and how would this affect result reliability?

- **Concept: LLM-as-judge evaluation paradigm**
  - Why needed here: STRIVE uses an LLM to select the "best" evaluation from candidates. This paradigm has known failure modes (length bias, self-preference) that may affect system reliability.
  - Quick check question: If the judge LLM consistently selects longer evaluations regardless of quality, what bias might be present, and how could you detect it?

## Architecture Onboarding

- **Component map:** Input: <Context, Question> pair + Metric definitions → TM1: Generate 10 <strength, weakness> pairs → Select best → Score → (feedback loop) → TM2: Receive TM1 output → Generate 10 variations → Select best → Score → (feedback loop) → TM1: Receive TM2 output → Generate 10 variations → Select best → Score → Convergence check → Output: Final scores + selected strength/weakness pairs

- **Critical path:** The convergence loop is the computational bottleneck. Each iteration requires 20 LLM calls (10 each for strength/weakness variations) per module, plus judge selections. Plan for rate limiting and cost management.

- **Design tradeoffs:**
  - Exact match convergence vs. tolerance threshold: Paper uses strict identical-score matching. A tolerance threshold could reduce iterations but may introduce instability.
  - Temperature range: Paper does not specify temperature values used. Too low = insufficient diversity; too high = incoherent evaluations. Start with [0.3, 0.5, 0.7, 0.9] and validate.
  - Model selection for TM1 vs. TM2: Paper uses same model type for both modules. Using different models could provide diversity but complicates deployment.

- **Failure signatures:**
  - Infinite loop: Modules never converge. Detect via iteration counter; add max_iterations=10 with early termination logging.
  - Trivial convergence: Modules agree immediately without genuine reasoning. Check initial score variance across samples.
  - Metric drift: Scores oscillate rather than converge toward human alignment. Monitor per-iteration score trajectories.
  - Cost explosion: Each question requires 40+ LLM calls minimum. Implement caching and budget caps.

- **First 3 experiments:**
  1. Baseline replication: Implement Algorithm 1 (baseline approach) and verify your GPT-4 scores approximate Table 1 values (e.g., EduProbe Gram ~4.71). This validates your prompt construction.
  2. Single-module ablation: Run TM1 alone (no TM2 feedback loop) and compare correlation scores to full STRIVE. This isolates the contribution of the cross-module iteration mechanism.
  3. Convergence diagnostics: Log per-iteration scores for 100 questions. Analyze: (a) average iterations to convergence, (b) failure-to-converge rate, (c) score trajectory patterns. This reveals whether convergence is genuine or artifact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based methods effectively replace human evaluations entirely for educational question quality assessment?
- Basis in paper: The introduction explicitly asks: "Whether a novel method comprising of LLMs can replace human evaluations in terms of assessing educational question quality?"
- Why unresolved: While STRIVE improved correlation, the error analysis shows that exact matches with human experts still range between 60-74%, leaving a significant gap where human oversight is still necessary.
- What evidence would resolve it: Achieving consistently high exact match rates (e.g., >90%) with human experts across all metrics on larger, more diverse datasets.

### Open Question 2
- Question: Does the iterative refinement process always converge, or does it risk oscillating indefinitely for ambiguous questions?
- Basis in paper: Algorithm 2 specifies termination only when scores are "identical for two consecutive iterations," but it does not define a maximum iteration cap or a stopping rule for non-convergence.
- Why unresolved: Stochastic LLM outputs might fluctuate or contradict each other in successive iterations without reaching a stable state, potentially causing infinite loops or increased costs.
- What evidence would resolve it: An analysis of convergence rates across the datasets, specifically reporting the distribution of iteration counts and identifying any instances of failure to converge.

### Open Question 3
- Question: Is the STRIVE approach computationally efficient enough for practical, large-scale deployment?
- Basis in paper: The methodology requires generating 10 variations of strengths/weaknesses per iteration and utilizes two separate "Think and Improve" modules running iteratively.
- Why unresolved: The paper focuses on accuracy metrics but does not report on the latency, inference time, or API costs associated with running multiple LLMs iteratively compared to the baseline.
- What evidence would resolve it: A comparative analysis of computational overhead (time and cost per question) between the baseline and STRIVE approaches.

## Limitations

- Exact prompt templates for baseline, pair generation, and judge/selection phases are referenced but not provided, making faithful reproduction challenging.
- Temperature values used for generating 10 variations per module are unspecified, though the mechanism depends critically on temperature variation producing meaningful diversity rather than noise.
- Convergence criterion requires exact integer matching across all five metrics for two consecutive iterations, which may be overly strict and could cause non-convergence in edge cases.
- The paper does not analyze the quality of generated strength/weakness justifications or examine whether convergence reflects genuine improvement versus agreement on incorrect assessments.

## Confidence

**High confidence**: The core methodology (iterative refinement between two Think & Improve modules) and experimental setup (EduProbe and SciQ datasets, correlation metrics) are clearly described and reproducible. The claim that STRIVE improves correlation with human judgments compared to baseline is well-supported by the experimental results.

**Medium confidence**: The mechanism by which temperature variation produces meaningful diversity in strength/weakness analyses is assumed rather than empirically validated. The paper claims this creates a "robust evaluation space" but does not provide evidence that temperature variation actually improves evaluation quality versus simply adding noise.

**Low confidence**: The convergence mechanism's relationship to evaluation quality is asserted but not independently validated. The paper assumes that score convergence between modules indicates improved evaluation, but does not test whether this convergence correlates with human preferences or whether modules might converge on incorrect assessments.

## Next Checks

1. **Prompt template validation**: Implement the baseline approach using the metric definitions provided and verify your scores approximate the reported baseline values (e.g., EduProbe Gram ~4.71, Rel ~3.64) before attempting the full STRIVE implementation.

2. **Temperature variation analysis**: Run a controlled experiment comparing STRIVE with temperature variation against a version using fixed temperature (e.g., 0.7). Measure whether temperature variation actually improves correlation with human judgments or merely adds computational overhead.

3. **Convergence quality audit**: For questions where modules converge rapidly versus slowly, compare the resulting scores against human judgments. If rapid convergence consistently produces lower quality scores, this suggests the convergence mechanism may be capturing agreement on incorrect assessments rather than genuine improvement.