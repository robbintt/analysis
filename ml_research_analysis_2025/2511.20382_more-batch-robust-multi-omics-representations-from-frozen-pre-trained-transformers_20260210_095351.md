---
ver: rpa2
title: 'MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers'
arxiv_id: '2511.20382'
source_url: https://arxiv.org/abs/2511.20382
tags:
- more
- cell
- single-cell
- across
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoRE (Multi-Omics Representation Embedding) introduces a parameter-efficient
  framework that repurposes frozen pre-trained transformer backbones for robust multi-omics
  integration. By attaching lightweight, modality-specific adapters and a task-adaptive
  fusion layer, MoRE optimizes masked modeling with supervised contrastive and batch-invariant
  alignment losses, producing embeddings that generalize across unseen cell types
  and platforms.
---

# MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers

## Quick Facts
- **arXiv ID:** 2511.20382
- **Source URL:** https://arxiv.org/abs/2511.20382
- **Reference count:** 0
- **Primary result:** Parameter-efficient multi-omics integration using frozen transformers with adapters achieves competitive batch robustness while reducing trainable parameters.

## Executive Summary
MoRE (Multi-Omics Representation Embedding) introduces a parameter-efficient framework that repurposes frozen pre-trained transformer backbones for robust multi-omics integration. By attaching lightweight, modality-specific adapters and a task-adaptive fusion layer, MoRE optimizes masked modeling with supervised contrastive and batch-invariant alignment losses, producing embeddings that generalize across unseen cell types and platforms. Benchmarking against scGPT, scVI, Scrublet, and Harmony demonstrates that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters.

## Method Summary
MoRE leverages frozen pre-trained transformer encoders with modality-specific adapters and task-adaptive fusion layers to integrate heterogeneous omics data. The framework employs iterative batch refinement to progressively align representations while preserving biological semantics. Training is parameter-efficient, with only adapters and fusion layers being optimized while the backbone remains frozen. The multi-objective loss combines cross-entropy, supervised contrastive, modality alignment, and variance reduction terms to optimize both classification accuracy and batch robustness.

## Key Results
- Achieves competitive batch robustness and biological conservation against established baselines (scGPT, scVI, Scrublet, Harmony)
- Demonstrates superior integration fidelity and rare population detection capabilities
- Shows cross-modality transfer performance with minimal retraining requirements
- Reduces trainable parameters significantly compared to full fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Freezing the pre-trained transformer backbone while training only lightweight adapters preserves generalizable biological semantics while enabling batch-specific adaptation.
- **Mechanism:** The frozen backbone acts as a regularizer that encodes transferable representations from large-scale pre-training. Modality-specific adapters (small projection layers) learn to map omics inputs into the backbone's semantic space without modifying attention weights.
- **Core assumption:** Pre-trained transformer weights already encode biologically meaningful patterns that transfer across cell types, tissues, and platforms.
- **Evidence anchors:** [abstract] "MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone"
- **Break condition:** If the pre-trained backbone lacks coverage of the target modality's feature space, adapter projections may fail to bridge the semantic gap.

### Mechanism 2
- **Claim:** Task-adaptive element-wise fusion with learned attention weights enables dynamic modality prioritization and suppresses noisy or missing signals.
- **Mechanism:** Each modality embedding is multiplied element-wise by learned weights (Equation 7: z_f = Σ w_G ⊙ z_G). This allows the model to attend to informative dimensions per modality per task.
- **Core assumption:** Different downstream tasks benefit from different modality weightings, and element-wise gating is sufficient to capture cross-modality interactions.
- **Evidence anchors:** [section 4.2] "By learning modality importance per task and per feature dimension, this mechanism enables the model to dynamically prioritize the most informative modalities"
- **Break condition:** If modalities require complex cross-attention, element-wise fusion may underperform compared to bilinear or transformer cross-attention.

### Mechanism 3
- **Claim:** Iterative residual refinement with learned batch embeddings progressively removes batch-specific variance while preserving semantic content.
- **Mechanism:** Each refinement step subtracts a learned batch embedding and applies a feedforward network (Equation 8: z_refined = z_prev - b_batch + Refine).
- **Core assumption:** Batch effects are additive or projectable to a learnable embedding space.
- **Evidence anchors:** [section 4.3] "This module progressively aligns the representations while maintaining semantic content, acting as a denoising and harmonization mechanism"
- **Break condition:** If batch effects are entangled with biological signal, subtracting batch embeddings may remove relevant biological variance.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** MoRE's core design relies on freezing backbone weights and training only adapters. Understanding adapter bottlenecks and gradient flow through frozen layers is essential.
  - **Quick check question:** Can you explain why freezing the backbone reduces catastrophic forgetting and how gradients reach the adapters?

- **Contrastive Learning with Supervision**
  - **Why needed here:** MoRE's multi-objective loss includes supervised contrastive loss. Understanding positive/negative pair construction is needed to tune λ weights.
  - **Quick check question:** How does supervised contrastive loss differ from self-supervised contrastive loss, and what happens if positive pairs span batch labels?

- **Multi-Omics Data Modalities**
  - **Why needed here:** MoRE handles heterogeneous inputs (scRNA-seq, ATAC-seq). Understanding sparsity patterns and tokenization schemes per modality is prerequisite for data preprocessing.
  - **Quick check question:** What normalization and tokenization steps are required to convert a scRNA-seq count matrix into transformer-compatible input tokens?

## Architecture Onboarding

- **Component map:** Input layer -> Frozen transformer backbone -> Modality-specific adapters -> Task-adaptive fusion layer -> Iterative batch refinement -> Loss head
- **Critical path:** 1. Verify backbone weights are frozen (no gradient computation) 2. Initialize adapter weights 3. Forward pass: tokenize → backbone → adapters → fusion → refinement → loss 4. Backward pass: gradients flow only through adapters, fusion, and refinement
- **Design tradeoffs:**
  - Fewer trainable parameters vs. expressivity: Adapters reduce overfitting risk but may limit adaptation to novel modalities
  - Iterative refinement depth vs. compute: More iterations improve batch correction but increase latency
  - Loss weight tuning: λ balance is empirical; no principled scheduling proposed
- **Failure signatures:**
  - Embeddings collapse to single cluster → contrastive loss weight too low or temperature too high
  - Batch labels still separable in latent space → refinement iterations insufficient or batch embedding underfit
  - Rare cell types absorbed into major populations → variance reduction loss over-weighted
- **First 3 experiments:**
  1. **Sanity check:** Train only the fusion layer on a single modality (scRNA-seq) with known batch labels; verify batch mixing via UMAP and silhouette scores.
  2. **Ablation:** Disable iterative refinement (set refinement steps = 0) and compare integration fidelity metrics against full MoRE.
  3. **Modality transfer:** Train adapters on paired RNA+ATAC data, then evaluate zero-shot embeddings on held-out ATAC-only samples.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can MoRE maintain its integration fidelity and batch robustness when applied to high-dimensional ATAC-seq or spatial transcriptomics data?
  - **Basis in paper:** [explicit] Primary evaluation focused on scRNA-seq, and "the full potential of MoRE in integrating high-dimensional ATAC-seq or spatial transcriptomics data requires further extensive validation."
  - **Why unresolved:** The unique sparsity and structure of chromatin accessibility (ATAC) and spatial coordinates present distinct integration challenges not fully tested by the transcriptomic benchmarks.
  - **What evidence would resolve it:** Benchmarking results on ATAC-seq and spatial datasets showing MoRE's performance in peak-gene alignment and spatial neighborhood preservation.

- **Open Question 2:** Can knowledge distillation effectively compress the MoRE architecture to reduce inference latency without degrading the quality of the multi-omics embeddings?
  - **Basis in paper:** [explicit] The authors note that "inference speed is still bounded by the transformer's depth" and identify "knowledge distillation techniques to compress the adapted model" as a specific direction for future work.
  - **Why unresolved:** While freezing the backbone reduces training costs, the computational overhead during inference remains high.
  - **What evidence would resolve it:** A study comparing the runtime and embedding quality of a distilled MoRE variant against the full-size model.

- **Open Question 3:** How can the adapter architecture be extended to explicitly model hierarchical biological relationships, such as cell lineage trees, rather than treating cell types as flat categories?
  - **Basis in paper:** [explicit] The authors outline a plan to "extend the adapter architecture to explicitly model hierarchical biological relationships" to move closer to a "true general-purpose foundation model."
  - **Why unresolved:** The current implementation relies on standard contrastive losses which may optimize for linear separability of discrete classes.
  - **What evidence would resolve it:** Demonstrations of improved performance on trajectory inference tasks and hierarchical classification metrics.

## Limitations
- Pre-trained transformer backbone specification and tokenization protocols are underspecified, preventing full independent verification
- Evaluation relies heavily on a single benchmark dataset (GSE153935) with primary focus on RNA-seq modalities
- Claims about rare population detection and cross-modality transfer lack systematic sensitivity analysis
- Computational overhead remains high due to transformer depth despite parameter efficiency gains

## Confidence

**High Confidence:** The parameter-efficient fine-tuning approach using frozen backbones with lightweight adapters is technically sound and well-established in NLP.

**Medium Confidence:** The reported performance improvements over baselines are plausible given the architectural advantages, but limited dataset diversity reduces certainty about generalizability.

**Low Confidence:** Claims about superior rare population detection and cross-modality transfer are based on single-dataset evaluations without systematic sensitivity analysis.

## Next Checks

1. **Ablation Study:** Systematically disable each component (fusion layer, iterative refinement, individual loss terms) to quantify their individual contributions to integration fidelity and batch correction performance.

2. **Cross-Dataset Generalization:** Evaluate MoRE on multiple independent multi-omics datasets with varying batch effect characteristics to assess robustness beyond the primary GSE153935 benchmark.

3. **Hyperparameter Sensitivity:** Conduct grid searches or Bayesian optimization over λ weights and refinement iteration counts to identify optimal configurations and characterize performance stability across parameter ranges.