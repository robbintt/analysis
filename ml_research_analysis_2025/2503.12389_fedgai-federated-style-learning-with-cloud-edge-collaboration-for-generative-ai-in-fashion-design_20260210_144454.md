---
ver: rpa2
title: 'FedGAI: Federated Style Learning with Cloud-Edge Collaboration for Generative
  AI in Fashion Design'
arxiv_id: '2503.12389'
source_url: https://arxiv.org/abs/2503.12389
tags:
- designers
- fedgai
- design
- sketches
- style
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedGAI, a federated learning-based generative
  AI system for fashion design that enables collaborative sketch generation while
  preserving intellectual property. The system addresses the challenge of designers'
  reluctance to share sketches by allowing style exchange without data disclosure
  through cloud-edge collaboration.
---

# FedGAI: Federated Style Learning with Cloud-Edge Collaboration for Generative AI in Fashion Design

## Quick Facts
- **arXiv ID**: 2503.12389
- **Source URL**: https://arxiv.org/abs/2503.12389
- **Reference count**: 40
- **Primary result**: FedGAI achieves comparable sketch quality to human designs while preserving IP through federated learning, with FID scores 9.966-248.089 across designers

## Executive Summary
FedGAI introduces a federated learning framework for collaborative sketch generation in fashion design, addressing designers' reluctance to share proprietary sketches. The system enables style fusion through cloud-edge collaboration where designers train locally and share only model parameters, not raw data. Using a lightweight GAN architecture with MRFM modules and CLIP loss, FedGAI achieves high-quality sketch generation while maintaining computational efficiency suitable for edge devices. The federated approach aggregates only Discriminator parameters while excluding BatchNorm layers, protecting intellectual property while enabling collaborative style learning.

## Method Summary
FedGAI employs a federated learning framework where fashion designers train lightweight GANs locally on their sketch-image pairs without sharing raw data. The system uses VGG-16 features extracted before max-pooling, combined with a lightweight GAN containing MRFM modules and AdaIN_SD. Training uses a composite loss function combining Gram, adversarial, and CLIP losses. Only Discriminator parameters are uploaded to the central server for aggregation via modified FedAvg, with BatchNorm parameters excluded. The process iterates through 11 communication rounds with 11 local epochs each. For edge deployment, GAN compression via depthwise separable convolutions reduces computational load while maintaining generation quality.

## Key Results
- Achieved FID scores ranging from 9.966 to 248.089 across different designer styles, demonstrating consistent sketch generation quality
- Successfully fused styles from multiple designers while maintaining individual style characteristics and protecting IP
- Outperformed baseline methods in both sketch generation quality and style fusion stability on the custom dataset of ~1000 paired images from 8 designers

## Why This Works (Mechanism)
The system works by leveraging federated learning to aggregate knowledge across designers without exposing proprietary data. By sharing only Discriminator parameters (excluding BatchNorm layers), the framework preserves individual design styles while enabling collaborative learning. The lightweight MRFM architecture reduces computational requirements for edge deployment, while the combined Gram, adversarial, and CLIP loss functions ensure both style preservation and generation quality. The FedDecorr regularization stabilizes training across heterogeneous client data distributions.

## Foundational Learning
- **Federated Learning with GANs**: Trains generative models across distributed clients without sharing raw data - needed to enable collaborative style learning while protecting IP
- **Lightweight GAN Architecture**: Uses MRFM modules with depthwise separable convolutions for efficiency - needed to meet computational constraints on edge devices
- **Style Transfer via CLIP Loss**: Incorporates semantic understanding through CLIP embedding - needed to maintain design consistency across fused styles
- **FedAvg with Selective Parameter Aggregation**: Uploads only Discriminator weights while excluding BatchNorm - needed to preserve individual style characteristics during fusion
- **Multi-Resolution Feature Fusion**: Combines features at different VGG levels through MRFM - needed to capture both coarse and fine design details

## Architecture Onboarding
**Component Map**: Sketch-Image Pairs -> Local Client Training -> Upload Discriminator -> Server Aggregation -> Download Updated Discriminator -> Continue Training
**Critical Path**: Local GAN training → Parameter upload → Server aggregation → Parameter download → Style fusion
**Design Tradeoffs**: Privacy (parameter-only sharing) vs. performance (full model sharing); efficiency (lightweight architecture) vs. quality (full GAN); stability (FedDecorr) vs. speed (simpler aggregation)
**Failure Signatures**: Mode collapse when BatchNorm is aggregated; poor fusion when one client dominates; artifacts when MRFM dimensions mismatch
**First Experiments**: 1) Train single client GAN with full loss to verify baseline generation quality; 2) Test parameter aggregation with synthetic clients; 3) Validate MRFM efficiency gains through ablation study

## Open Questions the Paper Calls Out
- **Additional Design Elements**: Can integrate more comprehensive fashion design elements beyond basic style features to support complex design tasks
- **Edge Device Performance**: How system performs on actual resource-constrained edge devices compared to server-grade GPUs used in evaluation
- **Technical Barrier**: Can simplify system to lower technical barrier for designers lacking deep learning expertise

## Limitations
- Critical hyperparameters (learning rate, batch size, FedDecorr coefficient) omitted from paper
- Custom dataset of designer sketches not publicly available for reproduction
- MRFM architecture dimensions underspecified, requiring approximation
- Evaluation performed on server-grade GPUs rather than actual edge devices

## Confidence
- **High confidence**: Federated learning framework and overall system architecture clearly described
- **Medium confidence**: GAN architecture and loss formulation reproducible with reasonable approximations
- **Low confidence**: Quantitative results may not be directly reproducible without exact hyperparameters and custom dataset

## Next Checks
1. Implement sensitivity analysis on FedDecorr coefficient β to determine optimal value for stable federated training across heterogeneous clients
2. Conduct ablation studies comparing full GAN vs. lightweight MRFM architectures to verify claimed efficiency improvements on edge devices
3. Test system with alternative datasets (DeepFashion, edge-detected sketches) to assess generalizability beyond custom designer dataset