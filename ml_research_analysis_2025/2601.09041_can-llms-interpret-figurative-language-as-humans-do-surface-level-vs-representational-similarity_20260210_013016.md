---
ver: rpa2
title: 'Can LLMs interpret figurative language as humans do?: surface-level vs representational
  similarity'
arxiv_id: '2601.09041'
source_url: https://arxiv.org/abs/2601.09041
tags:
- sensible
- non-sensible
- human
- sentences
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared human and LLM interpretations of figurative
  language across six categories (idiomatic, sarcastic, emotional, funny, slang, conventional)
  using 240 sentences rated on 40 interpretive dimensions. Both humans and four LLMs
  (GPT-4, Gemma-2-9B, Llama-3.2, Mistral-7B) provided 10-point Likert ratings.
---

# Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity

## Quick Facts
- **arXiv ID**: 2601.09041
- **Source URL**: https://arxiv.org/abs/2601.09041
- **Reference count**: 31
- **Key outcome**: GPT-4 most closely approximated human representational patterns for figurative language, but all models showed systematic divergence from human semantic organization, especially for non-compositional expressions like idioms and slang.

## Executive Summary
This study compared human and LLM interpretations of figurative language across six categories (idiomatic, sarcastic, emotional, funny, slang, conventional) using 240 sentences rated on 40 interpretive dimensions. Both humans and four LLMs (GPT-4, Gemma-2-9B, Llama-3.2, Mistral-7B) provided 10-point Likert ratings. Surface-level alignment was measured via Pearson correlation; representational alignment via RSA on pairwise Euclidean distances. GPT-4 most closely approximated human representational patterns, especially for conventional and emotional sentences. All models struggled with context-dependent categories like sarcasm, slang, and idiomacy, with Llama showing the weakest alignment. Representational similarity remained below human–human consistency, indicating that current LLMs do not fully capture human-like semantic organization, particularly for pragmatically rich expressions.

## Method Summary
The study collected ratings from 211 human participants and four LLMs (GPT-4, Gemma-2-9B, Llama-3.2-3B, Mistral-7B) on 240 sentences (120 sensible, 120 nonsensical) across six categories. Each sentence was rated on 40 interpretive questions using a 1-10 Likert scale. Surface-level similarity was assessed via Pearson correlation between human and model ratings per category. Representational similarity analysis (RSA) was performed by computing pairwise Euclidean distances across 40-dimensional rating vectors to build dissimilarity matrices, then correlating the upper triangles of human and model matrices per category.

## Key Results
- GPT-4 showed the highest representational alignment with humans (r = .50–.85), while Llama consistently showed the weakest (r = .10–.35)
- All models achieved moderate to high surface-level similarity with humans (r = .60–.90) but diverged at representational level
- Models struggled most with context-dependent categories like sarcasm, slang, and idiomacy
- Human-human representational consistency was r = .75–.85, setting an upper bound no model reached

## Why This Works (Mechanism)

### Mechanism 1: Representational Divergence Under Surface Agreement
- Claim: Models can produce human-like rating outputs without sharing human-like internal semantic organization.
- Mechanism: Pearson correlation on raw ratings captures output-level agreement, but RSA compares pairwise distance relationships across all sentence pairs. A model that assigns similar scores through surface pattern matching will show high SLS but different relational structure (lower RSA), because semantic similarity is dimension-dependent.
- Core assumption: Human interpretation involves stable relational structures that persist across rating dimensions.
- Evidence anchors:
  - [abstract] "while LLMs aligned with humans at the surface level (r = .60–.90), they diverged significantly at the representational level"
  - [discussion] "Despite moderate to high correlations in Surface-level similarity, Representational Similarity Analysis revealed systematic divergence"
  - [corpus] Neighbor work on "Geometry matters" explicitly warns RSA conclusions can be misleading without considering representational geometry.
- Break condition: If models develop identical distance matrices to humans, RSA would converge to 1.0; current results show r = .10–.85 depending on model and category.

### Mechanism 2: Non-Compositional Language Processing Gap
- Claim: Figurative expressions requiring meaning beyond literal composition show systematically weaker human-model alignment.
- Mechanism: Idioms and slang require non-compositional reinterpretation—meaning cannot be derived from surface form. Models trained on next-token prediction privilege literal form patterns, lacking pragmatic grounding or shared cultural context humans use for such expressions.
- Core assumption: Pragmatic inference relies on contextual and social knowledge not fully captured in training corpora.
- Evidence anchors:
  - [results] "Llama demonstrates consistently low representational alignment with humans (r = .10–.35), particularly for figurative categories like idiomacy, sarcasm, and Gen Z slang"
  - [discussion] "The weaker RSA alignment observed for idioms suggests that model representations privilege literal form meaning over pragmatically inferred meaning"
  - [corpus] "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language" examines similar cultural/pragmatic gaps.
- Break condition: If models received explicit context-sensitive supervision for pragmatic phenomena, representational alignment might improve for these categories.

### Mechanism 3: Scale-Dependent Representational Capacity
- Claim: Larger parameter models exhibit closer representational alignment with humans, but do not converge to human-human reliability.
- Mechanism: GPT-4 (undisclosed but presumed largest) showed strongest RSA (r = .50–.85), while smaller models like Llama-3.2-3B showed weakest (r = .10–.35). Larger capacity may enable more nuanced relational encoding, but next-token prediction objective does not enforce human-like semantic organization.
- Core assumption: Scale improves capacity to approximate patterns but does not guarantee shared representational structure.
- Evidence anchors:
  - [results] "GPT-4 most closely approximates human representational patterns" while "Llama consistently showed the weakest alignment"
  - [discussion] "improved alignment does not imply convergence toward human-like semantic organization"
  - [corpus] Weak direct evidence on scale-RSA relationship; neighbor papers focus on figurative language gaps without scale comparisons.
- Break condition: Human-human RSA baseline (r = .75–.85) represents upper bound; no tested model reached this ceiling.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**
  - Why needed here: RSA extracts relational structure from ratings, enabling comparison of how meaning is organized—not just what outputs are produced.
  - Quick check question: If two systems give identical ratings for all items but organize those items differently in semantic space, will RSA detect this?

- **Compositional vs. Non-Compositional Meaning**
  - Why needed here: The paper's core finding hinges on why conventional/emotional language (more compositional) shows better alignment than idioms/slang (non-compositional).
  - Quick check question: Why would "bite the bullet" be harder for a model to represent than "the dog is outside"?

- **Pragmatics and Context-Dependence**
  - Why needed here: Sarcasm, humor, and slang require speaker intent, social context, and cultural knowledge—all identified as weak points for current LLMs.
  - Quick check question: What information would a model need to reliably distinguish "I love your shirt, for now" (sarcastic) from "I love your shirt" (genuine)?

## Architecture Onboarding

- **Component map**: 240 sentences × 40 questions → 9,600 sentence-question pairs → human and model ratings → 40D rating vectors → pairwise Euclidean distances → 240×240 dissimilarity matrices → upper triangle correlation

- **Critical path**: Quality of rating vectors determines reliability of distance matrices; low within-group consistency propagates to unstable RSA correlations.

- **Design tradeoffs**:
  - Likert scale captures nuanced judgment but reduces complex reasoning to single values
  - Context-free sentences enable controlled comparison but remove cues critical for pragmatic phenomena
  - Zero-shot prompting tests intrinsic capability but may underestimate performance under few-shot conditions

- **Failure signatures**:
  - High SLS (r > .70) with low RSA (r < .40) → surface pattern matching without shared structure
  - Large SLS-RSA gap specifically for idioms/slang → non-compositional processing deficit
  - High variance across prompt conditions → unstable representations

- **First 3 experiments**:
  1. Replicate with context-embedded sentences (discourse context before target) to test whether context narrows RSA gap for pragmatically rich categories.
  2. Compare RSA across different model layers (if accessible) to identify where representational alignment emerges or diverges.
  3. Test few-shot prompting with exemplar human ratings to assess whether in-context learning improves representational—not just surface—alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does providing rich discourse context (e.g., speaker intent, situational knowledge) close the representational gap between humans and LLMs for context-dependent expressions?
- Basis in paper: [explicit] The authors note in the Limitations that sentences were presented without contextual cues, which are critical for pragmatic phenomena like sarcasm, and suggest in the Discussion that missing context may not fully explain representational divergence.
- Why unresolved: The study isolated sentences to ensure comparability between humans and models, leaving the specific impact of contextual grounding on internal representational structure untested.
- What evidence would resolve it: A follow-up study comparing RSA scores for the same figurative sentences presented in isolation versus those embedded within rich, informative narrative contexts.

### Open Question 2
- Question: Can specific training regimes that prioritize context-sensitive supervision induce stable pragmatic representations in LLMs that mirror human semantic organization?
- Basis in paper: [explicit] The Conclusion states there is a need for "training regimes that support context-sensitive and socially grounded semantic representations" to overcome the limitations of next-token prediction.
- Why unresolved: The study demonstrates that current training methods result in representational divergence, but it does not test if alternative training objectives can successfully restructure the internal semantic space.
- What evidence would resolve it: Fine-tuning models on datasets specifically designed for pragmatic inference (e.g., containing speaker intent labels) and observing a significant increase in RSA correlation with human baselines.

### Open Question 3
- Question: Is the weak representational alignment observed in smaller models like Llama primarily a function of parameter scale, or do architectural differences play a larger role?
- Basis in paper: [inferred] The Results show a clear hierarchy where GPT-4 aligns best and Llama aligns worst, but the Discussion notes limitations in encoding context-sensitive cues without attributing the failure solely to scale.
- Why unresolved: The study compared distinct models (GPT-4, Gemma, Mistral, Llama) with varying architectures and training data, making it impossible to isolate parameter count as the sole cause of low representational similarity.
- What evidence would resolve it: A controlled ablation study evaluating different sizes of the same model family (e.g., Llama-3.1 8B vs. 70B) on the same RSA task to determine if scaling improves representational alignment.

## Limitations

- **Human data reliability**: The specific aggregation method for human ratings and inter-rater agreement metrics are not fully detailed, making it difficult to assess the stability of the human baseline.
- **Model output parsing**: The study does not specify how numeric ratings were extracted from LLM responses, particularly when justifications or additional text were included, which could introduce parsing bias or noise.
- **Representational analysis sensitivity**: RSA results are sensitive to the number of dimensions and rating scale granularity; with 40 interpretive dimensions, the method may conflate systematic judgment patterns with noise, especially for subtle pragmatic distinctions.

## Confidence

- **High confidence**: Surface-level alignment results (Pearson r = .60-.90) are robust and directly observable from rating data.
- **Medium confidence**: Representational alignment findings (RSA r = .10-.85) are methodologically sound but depend heavily on the stability of human rating aggregation and the choice of distance metric.
- **Medium confidence**: Claims about non-compositional language processing gaps are supported but could be influenced by model training data exposure to figurative expressions.

## Next Checks

1. Replicate RSA analysis with human-human split-half reliability computed from within-study participant subsamples to verify the r = .75-.85 baseline.
2. Test alternative distance metrics (cosine, correlation distance) in RSA to ensure geometric findings are not artifacts of Euclidean distance choice.
3. Conduct few-shot prompting with exemplar human ratings for each category to assess whether in-context learning can improve representational (not just surface) alignment, especially for idioms and slang.