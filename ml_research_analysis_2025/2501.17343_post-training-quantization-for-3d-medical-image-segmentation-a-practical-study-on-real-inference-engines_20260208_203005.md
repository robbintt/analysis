---
ver: rpa2
title: 'Post-Training Quantization for 3D Medical Image Segmentation: A Practical
  Study on Real Inference Engines'
arxiv_id: '2501.17343'
source_url: https://arxiv.org/abs/2501.17343
tags:
- quantization
- medical
- int8
- segmentation
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a practical post-training quantization (PTQ)\
  \ framework for 3D medical image segmentation models that achieves real 8-bit integer\
  \ (INT8) quantization on modern GPUs. Unlike previous methods relying on \"fake\
  \ quantization\" that only simulate low-precision computations without actual efficiency\
  \ gains, this approach converts pre-trained models to INT8 using NVIDIA TensorRT,\
  \ resulting in tangible reductions in model size (2.42\xD7 to 3.85\xD7) and inference\
  \ latency (2.05\xD7 to 2.66\xD7) across seven state-of-the-art models tested on\
  \ three datasets."
---

# Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines

## Quick Facts
- arXiv ID: 2501.17343
- Source URL: https://arxiv.org/abs/2501.17343
- Authors: Chongyu Qu; Ritchie Zhao; Ye Yu; Bin Liu; Tianyuan Yao; Junchao Zhu; Bennett A. Landman; Yucheng Tang; Yuankai Huo
- Reference count: 40
- Primary result: Achieved real 8-bit integer (INT8) quantization on modern GPUs for 3D medical image segmentation models, reducing model size by 2.42× to 3.85× and inference latency by 2.05× to 2.66× while maintaining segmentation accuracy.

## Executive Summary
This paper introduces a practical post-training quantization (PTQ) framework that achieves real INT8 quantization for 3D medical image segmentation models using NVIDIA TensorRT. Unlike previous methods that rely on "fake quantization" simulations without actual efficiency gains, this approach converts pre-trained models to INT8 using calibration statistics from unlabeled data, resulting in tangible reductions in model size and inference latency across seven state-of-the-art models tested on three datasets. The framework preserves segmentation accuracy, maintaining comparable Dice scores to full-precision models, enabling efficient deployment of complex medical imaging models in resource-constrained clinical environments.

## Method Summary
The framework employs a two-step process: First, pretrained PyTorch models are exported to ONNX format and converted to fake quantized models using TensorRT's ModelOpt with calibration datasets to insert QuantizeLinear and DequantizeLinear nodes. Second, the fake quantized ONNX models are converted to real INT8 TensorRT engines. The calibration process determines scale factors and zero-points from a small set of unlabeled samples, enabling quantization without retraining. Models are trained with Adam optimizer (lr=1e-4, weight decay=1e-5), input size 96×96×96, DiceCE loss, on RTX 4090 GPUs.

## Key Results
- Model size reduction: 2.42× to 3.85× across seven models
- Inference latency reduction: 2.05× to 2.66× on RTX 4090
- Maintained mDSC within 0.01 of FP32 baseline
- Seven state-of-the-art models tested on three datasets (BTCV, Whole Brain, TotalSegmentator V2)

## Why This Works (Mechanism)

### Mechanism 1
Calibration statistics from unlabeled data determine optimal quantization parameters without retraining. A small calibration dataset passes through the FP32 network, collecting min/max activations per layer to compute scale factor and zero-point for affine quantization mapping. Core assumption: calibration dataset distribution approximates inference-time data distribution. Evidence anchors: [abstract] calibration using unlabeled samples; [section 3.1] equations defining quantization formula; [corpus] weak direct evidence on calibration sufficiency in medical imaging.

### Mechanism 2
TensorRT converts simulated quantization nodes into actual INT8 kernel executions with measurable hardware gains. QuantizeLinear and DequantizeLinear nodes in ONNX serve as annotations that TensorRT detects, replacing corresponding operations with optimized INT8 kernels and fusing ReLU into preceding layers. Core assumption: TensorRT has INT8 kernel support for all critical operations. Evidence anchors: [abstract] conversion to real quantization via TensorRT engine; [section 3.2] TensorRT detection and replacement of quantization nodes; [corpus] FlexQ paper on INT6/INT8 tradeoffs.

### Mechanism 3
Medical segmentation models exhibit sufficient activation/weight distribution smoothness for INT8 to preserve accuracy. Observed Dice scores remain nearly unchanged (e.g., U-Net: 0.822 → 0.822, VISTA3D: 0.893 → 0.891), suggesting quantization error is small relative to task tolerance. Core assumption: segmentation tasks have higher tolerance to per-pixel perturbation than classification. Evidence anchors: [section 4.3, Table 1] mDSC values nearly identical post-quantization; [section 5] nearly unchanged mDSC challenges accuracy degradation concerns.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed: Framework explicitly avoids QAT's retraining cost; explains why only calibration data is required
  - Quick check: If INT8 quantization failed and you needed accuracy recovery, would you add more calibration samples or switch to QAT with labeled data?

- **Concept: Fake vs. Real Quantization**
  - Why needed: Paper's central contribution is demonstrating fake quantization provides no hardware speedup
  - Quick check: A model runs INT8 inference in PyTorch with simulated quantization nodes—will it reduce GPU memory usage during execution?

- **Concept: Calibration Dataset Design**
  - Why needed: Quantization quality depends on calibration data representing inference distributions
  - Quick check: Should calibration data include full range of anatomical variations (pathologies, imaging protocols) expected at deployment?

## Architecture Onboarding

- **Component map:**
  PyTorch FP32 Model -> ONNX Export -> ModelOpt (inserts Q/DQ nodes via calibration) -> Fake Quantized ONNX -> TensorRT Engine Builder -> INT8 TensorRT Engine

- **Critical path:**
  1. Export trained model to ONNX with correct opset version
  2. Prepare 100-500 representative unlabeled calibration samples
  3. Run ModelOpt calibration to generate scale/zero-point per tensor
  4. Build TensorRT engine with INT8 precision flag
  5. Deploy engine; verify mDSC against FP32 baseline

- **Design tradeoffs:**
  - Calibration set size: Larger → better coverage but longer calibration; paper uses dataset-specific subsets (20-200 samples)
  - Precision granularity: Per-tensor (faster) vs. per-channel (potentially better accuracy)—paper's configuration unspecified
  - Input/output precision: Paper retains FP32 I/O; full INT8 pipeline would add quantization overhead at boundaries

- **Failure signatures:**
  - Accuracy drop > 1-2% mDSC: Likely miscalibration, out-of-range activations, or unsupported layer precision fallback
  - TensorRT build errors: Dynamic shapes, custom ops, or control flow not representable in static graph
  - No latency improvement: Engine may be falling back to FP32 kernels for unsupported operations
  - Memory not reduced: Non-quantizable layers (e.g., certain normalizations) dominating footprint

- **First 3 experiments:**
  1. Reproduce U-Net quantization on BTCV subset (smallest model, fastest validation cycle); confirm model size drops ~3.5× and mDSC unchanged
  2. Ablate calibration set size (50, 100, 200 samples) to find minimum viable calibration for a transformer-based model (e.g., SwinUNETR)
  3. Stress-test with out-of-distribution data (different scanner protocol) to characterize robustness margins and identify failure modes before clinical deployment

## Open Questions the Paper Calls Out

- **Open Question 1:** Can INT4 (4-bit) quantization be applied to 3D medical segmentation models without significant accuracy degradation?
  - Basis: Authors state focusing on INT4 is a promising direction but achieving it without compromising accuracy presents substantial challenges
  - Why unresolved: Reduced precision at INT4 introduces considerable quantization errors
  - What evidence would resolve: Study demonstrating INT4 quantization on same model architectures with maintained Dice scores within clinical thresholds

- **Open Question 2:** How can the PTQ framework be adapted to support models with dynamic architectures, such as those with variable input sizes or conditional operations?
  - Basis: Authors acknowledge TensorRT may not fully support models with dynamic blocks or layers requiring runtime flexibility
  - Why unresolved: TensorRT's optimization pipeline assumes static computational graphs
  - What evidence would resolve: Successful quantization of a representative dynamic architecture using enhanced TensorRT compatibility or alternative framework

- **Open Question 3:** To what extent does calibration dataset selection (size, distribution, representativeness) affect quantization accuracy in medical imaging?
  - Basis: Paper uses "unlabeled calibration dataset" but does not systematically analyze how calibration data characteristics affect quantization quality
  - Why unresolved: Relationship between calibration data properties and quantization quality remains unexplored
  - What evidence would resolve: Ablation study varying calibration set sizes and sampling strategies across datasets, reporting resulting mDSC variance

## Limitations

- Framework has not been tested on models with dynamic control flow or unsupported layer types that may fail TensorRT conversion
- Calibration dataset design is underspecified in terms of optimal size and sampling strategy across diverse anatomical variations
- Focus on GPU-based TensorRT engines limits applicability to edge devices and other inference platforms

## Confidence

- **High confidence**: Claim that TensorRT converts fake quantization into real INT8 execution with measurable hardware gains (2.05×-2.66× latency reduction, 2.42×-3.85× model size reduction) is well-supported by direct experimental evidence
- **Medium confidence**: Assertion that medical segmentation models exhibit sufficient activation/weight distribution smoothness for INT8 to preserve accuracy (mDSC nearly unchanged) is supported by experimental results but lacks broader validation
- **Low confidence**: Mechanism that calibration statistics from unlabeled data determine optimal quantization parameters without retraining is theoretically sound but provides limited empirical evidence on calibration dataset design

## Next Checks

1. **Calibration robustness testing**: Evaluate model accuracy degradation when inference data distribution differs from calibration data, using datasets with varying scanner protocols or anatomical variations

2. **Platform portability assessment**: Test the quantized models on alternative inference engines (OpenVINO, ONNX Runtime) and edge devices to verify hardware independence of the quantization gains

3. **Clinical boundary validation**: Conduct radiologist review of segmentation outputs to verify that subthreshold quantization noise remains clinically acceptable across all anatomical structures and pathology types