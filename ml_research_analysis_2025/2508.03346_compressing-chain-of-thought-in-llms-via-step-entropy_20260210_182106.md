---
ver: rpa2
title: Compressing Chain-of-Thought in LLMs via Step Entropy
arxiv_id: '2508.03346'
source_url: https://arxiv.org/abs/2508.03346
tags:
- reasoning
- steps
- step
- tokens
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Chain-of-Thought (CoT) compression
  framework using step entropy to identify and remove redundant reasoning steps in
  Large Language Models (LLMs). The core idea is that steps with low entropy are highly
  predictable and thus less informative, making them safe to prune without compromising
  accuracy.
---

# Compressing Chain-of-Thought in LLMs via Step Entropy

## Quick Facts
- arXiv ID: 2508.03346
- Source URL: https://arxiv.org/abs/2508.03346
- Reference count: 8
- Key outcome: Novel CoT compression framework using step entropy identifies and removes up to 80% redundant reasoning steps while maintaining or improving accuracy across multiple models and benchmarks.

## Executive Summary
This paper introduces a novel Chain-of-Thought (CoT) compression framework using step entropy to identify and remove redundant reasoning steps in Large Language Models (LLMs). The core idea is that steps with low entropy are highly predictable and thus less informative, making them safe to prune without compromising accuracy. The method involves calculating step entropy for each reasoning step, pruning up to 80% of the lowest-entropy steps, and replacing them with [SKIP] tokens. Experiments across multiple models (DeepSeek-R1-7B, 14B, Qwen3-8B, QwQ-32B) and benchmarks (GSM8k, Math500, AIME 2024/2025, MMLU) show substantial token reductions (16-57%) while maintaining or improving accuracy. A two-stage training strategy combining Supervised Fine-Tuning and Group Relative Policy Optimization enables models to autonomously generate compressed reasoning trajectories. The approach provides significant computational efficiency gains for practical LLM deployment while preserving interpretability and verifiability of explicit reasoning chains.

## Method Summary
The approach uses step entropy to identify and prune low-entropy reasoning steps in CoT. First, full CoT trajectories are generated and segmented by `\n\n` delimiters. Step entropy is computed by aggregating token-level Shannon entropy across all tokens in each step. Steps are ranked by ascending entropy and the lowest 80% are replaced with [SKIP] tokens to create compressed training pairs. A two-stage training pipeline follows: (1) SFT on 70k compressed CoT pairs to initialize compressed reasoning behavior, and (2) GRPO with composite reward (correctness + skip ratio + penalties) to optimize the balance between accuracy and compression. The method achieves 16-57% token reduction while maintaining or improving accuracy across mathematical and general reasoning benchmarks.

## Key Results
- Up to 80% of low-entropy reasoning steps can be pruned with only minor accuracy degradation (74% accuracy at 80% pruning vs. immediate degradation for high-entropy pruning)
- Token reduction of 16-57% across GSM8k, Math500, AIME 2024/2025, and MMLU benchmarks while maintaining or improving accuracy
- Step-level pruning outperforms token-level pruning, preserving accuracy to 40% token reduction vs. immediate decline after 20% for token-level
- Two-stage training (SFT + GRPO) with multi-component rewards is essential; removing constraints causes catastrophic accuracy degradation (88.17%→51.00% on Math500)

## Why This Works (Mechanism)

### Mechanism 1: Step Entropy Quantifies Informational Contribution
- **Claim**: Low-entropy reasoning steps have bounded conditional mutual information with the final answer, making them safe to prune.
- **Mechanism**: Shannon entropy is computed per token, then aggregated into step entropy H(S_i|S_<i). Theoretical analysis shows I(S_j; A|S̄_j) ≤ H(S_j|S_<j)—meaning low-entropy steps contribute minimally to answer A. Pruning these removes redundancy without breaking causal reasoning chains.
- **Core assumption**: Model uncertainty (entropy) during generation faithfully reflects informational importance, not miscalibration or noise.
- **Evidence anchors**:
  - [abstract]: "steps with low entropy are indeed highly redundant...an astonishing 80% of low-entropy intermediate steps can be pruned with minor degradation"
  - [section]: Figure 1 demonstrates low-entropy pruning maintains 74% accuracy at 80% mask ratio, while high-entropy pruning immediately degrades performance
  - [corpus]: "Can Aha Moments Be Fake?" proposes True Thinking Score to quantify causal contribution of steps, suggesting entropy-based identification is part of broader effort to measure step importance
- **Break condition**: If critical reasoning happens with high confidence (low entropy), or if entropy reflects artifacts rather than importance, pruning will silently remove necessary steps.

### Mechanism 2: Step-Level Pruning Preserves Semantic Coherence
- **Claim**: Removing complete reasoning steps maintains accuracy better than token-level pruning at equivalent compression rates.
- **Mechanism**: Steps are semantic units bounded by `\n\n` delimiters. Pruning entire steps removes complete thoughts while preserving syntactic integrity of remaining steps. Token-level pruning can fragment critical steps, making them incomprehensible.
- **Core assumption**: Step boundaries correctly segment coherent reasoning units; low-entropy tokens are not concentrated within high-entropy steps.
- **Evidence anchors**:
  - [section]: Figure 4 shows token-level pruning causes immediate accuracy decline after 20% masking; step-level maintains baseline to 40% token reduction
  - [section]: "removing an entire low-entropy step preserves the structure of the remaining, more important steps"
  - [corpus]: No direct corpus comparison found for step vs. token pruning granularity
- **Break condition**: Poorly segmented steps (mixed entropy content) will cause either over-pruning (removing useful information) or under-pruning (preserving redundancy).

### Mechanism 3: Multi-Component Reward Prevents Degenerate Compression
- **Claim**: Two-stage training (SFT → GRPO) requires all four reward components to balance accuracy and compression; removing constraints causes collapse.
- **Mechanism**: SFT initializes compressed reasoning behavior. GRPO then optimizes R_total = R_correctness + R_skip_ratio + R_skip_num + R_response_length. Without constraints, naive skip optimization generates excessive low-quality [SKIP] tokens.
- **Core assumption**: Thresholds (κ_high=0.8, κ_low=0.5, τ_skip_num=100, τ_length=3500) correctly balance trade-offs for target domain.
- **Evidence anchors**:
  - [section]: Table 4 shows R_correctness + R_skip_ratio alone causes catastrophic degradation (Math500: 88.17%→51.00% accuracy, +10.2% tokens)
  - [section]: Full reward achieves 43.3% token reduction on GSM8k while maintaining accuracy
  - [corpus]: "CODI" and related latent compression papers don't address explicit multi-component reward design for skip tokens
- **Break condition**: Mis-calibrated rewards lead to over-compression (accuracy loss) or under-compression (efficiency loss); domain-specific tuning required.

## Foundational Learning

- **Concept: Shannon Entropy for Token Distributions**
  - **Why needed here**: Step entropy aggregates token-level entropy. Must understand how H(t|c) = -Σp(w|c)log₂p(w|c) reflects model uncertainty.
  - **Quick check question**: Given P("the")=0.8, P("a")=0.2 at a token position, compute entropy. What does H→0 indicate?

- **Concept: Conditional Mutual Information**
  - **Why needed here**: Theoretical justification relies on I(S_j; A|S̄_j) ≤ H(S_j|S_<j) to bound step importance.
  - **Quick check question**: If H(S_j|S_<j)=0.3 bits, what's the maximum possible I(S_j; A|other steps)?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: Stage 2 uses GRPO, which compares K completions per prompt without a separate value function.
  - **Quick check question**: How does GRPO's group-based advantage estimation differ from PPO's value function approach?

## Architecture Onboarding

### Component Map:
```
Problem → [Generate Full CoT] → Extract steps (\n\n delimited)
        → [Compute Step Entropy] → Rank by entropy
        → [Prune κ=0.8 lowest] → Replace with [SKIP]
        → [Compressed Prompt] → Generate final answer

Training Pipeline:
Stage 1 (SFT): 70k (problem, compressed-CoT) pairs → Cross-entropy
Stage 2 (GRPO): Sample K=14 completions → Compute R_total → Policy update
```

### Critical Path:
1. **Step segmentation** (regex on `\n\n`): Missegmentation corrupts all downstream entropy calculations
2. **Entropy extraction during generation**: Requires full vocab distribution p(w|context), not just greedy tokens
3. **Reward threshold tuning**: κ_high, κ_low, τ_skip_num, τ_length must be calibrated per domain

### Design Tradeoffs:
- **Pruning ratio κ**: Paper finds 0.8 safe for math; may need reduction for domains with denser reasoning
- **SFT data filtering**: Removing >4096 token sequences reduces coverage of hard problems
- **GRPO sample count K**: Higher K improves gradient estimates but increases memory

### Failure Signatures:
- **Accuracy drops sharply with compression** → Check if high-entropy steps incorrectly ranked low (entropy computation error)
- **Token count increases post-training** → Degenerate [SKIP] over-generation; increase R_skip_num penalty
- **No compression learned** → Verify [SKIP] token in vocabulary; check reward shaping

### First 3 Experiments:
1. **Validate pruning threshold**: Apply κ=0.8 to 100 held-out samples from target domain; confirm accuracy maintained before training
2. **Ablate reward components**: Train GRPO with R_correctness + R_skip_ratio only; observe failure mode per Table 4
3. **Step vs. token pruning comparison**: Implement both at 30% token reduction; confirm step-level accuracy advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the optimal low-entropy pruning thresholds for non-mathematical reasoning domains, and do they differ systematically across task types?
- **Basis in paper**: [explicit] "The method's effectiveness is limited to mathematical reasoning tasks, necessitating validation for new application domains with potentially different optimal compression ratios."
- **Why unresolved**: The paper validates the 80% threshold only on mathematical benchmarks (GSM8k, Math500, AIME) and limited MMLU subsets. Preliminary MMLU results suggest domain-specific differences—History tolerated 90% compression while College Medicine showed more sensitivity—but systematic characterization across diverse reasoning domains remains unexplored.
- **What evidence would resolve it**: Experiments across diverse reasoning domains (legal, medical, scientific, commonsense, multi-hop) with threshold sensitivity analysis to identify domain-specific optimal pruning ratios and whether they correlate with measurable task properties.

### Open Question 2
- **Question**: Can adaptive or dynamic pruning thresholds be developed that adjust based on task difficulty, model confidence, or reasoning structure?
- **Basis in paper**: [explicit] "One possible way to address this challenge is to develop adaptive thresholds for task-aware and model-aware compression strategies."
- **Why unresolved**: The current method uses a fixed 80% threshold determined empirically. While effective, this static approach cannot adapt to varying difficulty levels—AIME problems may benefit from different compression rates than GSM8k problems. The paper shows that AIME benchmarks achieve higher compression ratios (36-55%) compared to GSM8k (0.6-1.9%), suggesting difficulty-dependent optimization potential.
- **What evidence would resolve it**: Development and evaluation of dynamic threshold mechanisms that adjust pruning ratios based on input characteristics, initial entropy distributions, or intermediate confidence signals, demonstrating improved accuracy-efficiency trade-offs compared to fixed thresholds.

### Open Question 3
- **Question**: Does step entropy scale predictably with model size, and can this relationship be formalized to predict optimal compression for new architectures?
- **Basis in paper**: [inferred] The extended experiments show QwQ-32B achieves 55.1% token reduction on AIME 2024 versus 36.3% for DeepSeek-R1-7B, with the authors noting "larger models generate proportionally more redundant reasoning steps." However, the theoretical explanation for this scaling behavior remains unexplored.
- **Why unresolved**: The paper demonstrates that step entropy captures "fundamental properties of reasoning redundancy" across architectures but doesn't explain why larger models produce more low-entropy steps or whether this relationship is predictable. The cross-architecture consistency suggests a systematic pattern, but no formal characterization exists.
- **What evidence would resolve it**: Systematic experiments across a controlled scaling series of models (e.g., 1B, 3B, 7B, 14B, 32B, 70B) within the same model family, analyzing how step entropy distributions shift with scale and whether compression efficiency follows predictable scaling laws.

## Limitations
- Theoretical bounds are probabilistic rather than deterministic, providing only upper bounds on information contribution
- Domain specificity of entropy patterns may limit generalizability to non-mathematical reasoning tasks
- Potential for hidden semantic dependencies where critical information is distributed across multiple low-entropy steps

## Confidence

**High Confidence**: The empirical demonstration that step entropy correlates with informational redundancy (Figure 1 showing 74% accuracy at 80% pruning vs. immediate degradation for high-entropy pruning) is well-supported. The multi-component reward structure's necessity (Table 4 showing catastrophic degradation without constraints) is clearly demonstrated.

**Medium Confidence**: The theoretical framework connecting entropy to informational contribution via conditional mutual information is plausible but relies on assumptions about model behavior that aren't fully validated. The generalizability across diverse reasoning domains is supported by results on MMLU but not comprehensively tested.

**Low Confidence**: The specific threshold values (κ=0.8, κ_low=0.5, κ_high=0.8, τ_skip=100, τ_length=3500) are presented as effective but lack systematic sensitivity analysis. The claim that step-level pruning preserves "semantic coherence" better than token-level pruning is supported by Figure 4 but could be influenced by specific dataset characteristics.

## Next Checks

1. **Cross-domain entropy pattern validation**: Apply the entropy calculation and pruning pipeline to non-mathematical reasoning tasks (legal reasoning, scientific analysis, commonsense reasoning) to verify that the 80% pruning threshold maintains accuracy across domains with different reasoning structures.

2. **Step boundary sensitivity analysis**: Systematically vary step segmentation rules (different delimiter patterns, hierarchical step structures) to quantify how segmentation quality affects pruning effectiveness and accuracy preservation.

3. **Dynamic threshold calibration**: Implement an adaptive thresholding mechanism that adjusts κ based on domain-specific entropy distributions and problem difficulty, then validate whether this improves compression rates while maintaining accuracy across diverse reasoning tasks.