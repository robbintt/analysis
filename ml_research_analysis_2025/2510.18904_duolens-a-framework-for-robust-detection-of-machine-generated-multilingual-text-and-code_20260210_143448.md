---
ver: rpa2
title: 'DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual
  Text and Code'
arxiv_id: '2510.18904'
source_url: https://arxiv.org/abs/2510.18904
tags:
- code
- language
- samples
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes DuoLens, a dual-encoder framework for detecting\
  \ machine-generated multilingual text and source code. It uses fine-tuned encoder-only\
  \ Small Language Models (SLMs) \u2014 specifically RoBERTa and CodeBERTa \u2014\
  \ trained on specialized, balanced datasets spanning seven programming languages\
  \ and eight natural languages."
---

# DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code

## Quick Facts
- arXiv ID: 2510.18904
- Source URL: https://arxiv.org/abs/2510.18904
- Reference count: 40
- DuoLens achieves AUROC 0.97–0.99 and macro-F1 0.89–0.94, outperforming LLM baselines while reducing latency by 8–12× and VRAM by 3–5×.

## Executive Summary
DuoLens introduces a dual-encoder framework for detecting machine-generated content across multilingual text and source code. By fine-tuning encoder-only Small Language Models (SLMs) such as RoBERTa and CodeBERTa on specialized, balanced datasets, DuoLens achieves high accuracy (AUROC 0.97–0.99, macro-F1 0.89–0.94) while significantly reducing computational costs compared to LLM baselines. The framework maintains robustness under cross-model and adversarial conditions, retaining over 92% of clean AUROC performance. It offers an efficient, scalable solution for binary classification of AI-generated content in both natural and programming languages.

## Method Summary
DuoLens employs a dual-encoder architecture, fine-tuning CodeBERT and CodeBERTa for code detection, and XLM-RoBERTa-base/large and mBERT for multilingual text detection. The framework fuses representations from paired encoders via a learned gate and linear classifier. Training uses class-balanced binary cross-entropy on curated datasets spanning seven programming languages and eight natural languages, with 512-token maximum sequence length and chunking. Model checkpoints are selected based on dev set performance, and temperature scaling is applied for calibration.

## Key Results
- Achieves AUROC 0.97–0.99 and macro-F1 0.89–0.94 on multilingual text and code detection.
- Outperforms LLM baselines with 8–12× lower latency and 3–5× reduced VRAM usage.
- Maintains >92% clean AUROC under cross-model and adversarial conditions.

## Why This Works (Mechanism)
DuoLens leverages the efficiency and specialization of encoder-only SLMs for binary classification, avoiding the computational overhead of LLMs. By fusing complementary encoders (CodeBERT + CodeBERTa for code, XLM-R variants for text), the framework captures diverse linguistic and syntactic features. Fine-tuning on balanced, multilingual datasets ensures robust generalization across languages and modalities. The learned gate fusion mechanism adaptively combines encoder outputs, enhancing detection accuracy while maintaining low resource consumption.

## Foundational Learning
- **Fine-tuning encoder-only SLMs**: Needed for efficient adaptation to binary classification tasks; check by confirming model convergence on dev set.
- **Balanced dataset curation**: Ensures fair evaluation across languages and classes; check by verifying label distributions and per-language performance.
- **Learned gate fusion**: Enables adaptive combination of dual encoder outputs; check by comparing fusion vs single-encoder baselines.
- **Temperature scaling**: Calibrates model confidence scores; check by evaluating reliability diagrams on validation data.
- **Class-balanced BCE**: Prevents bias toward majority classes; check by monitoring per-class precision/recall.
- **Adversarial robustness**: Tests model resilience to out-of-distribution samples; check by evaluating cross-generator performance.

## Architecture Onboarding

**Component Map**
Text Encoder (XLM-R/mBERT) + Code Encoder (CodeBERT/CodeBERTa) -> Learned Gate Fusion -> Linear Classifier -> Binary Output

**Critical Path**
Dual encoders extract features → learned gate fuses representations → linear classifier produces detection score

**Design Tradeoffs**
- SLM efficiency vs. LLM expressiveness: SLMs offer speed and lower resource use but lack generative capabilities.
- Dual-encoder fusion vs. single encoder: Fusion improves robustness but adds complexity.
- Balanced vs. imbalanced datasets: Balancing improves fairness but may require data augmentation.

**Failure Signatures**
- Sharp accuracy drop on low-resource languages: Indicates insufficient training data or poor cross-lingual transfer.
- Overfitting to specific generators: Suggests need for more diverse training data.
- Class imbalance despite balancing: Points to preprocessing or label noise issues.

**First Experiments**
1. Fine-tune CodeBERT and CodeBERTa on code datasets; evaluate per-language AUROC.
2. Fine-tune XLM-R variants on multilingual text; assess cross-language generalization.
3. Implement and test learned gate fusion; compare against single-encoder baselines.

## Open Questions the Paper Calls Out
- Can integrating an LLM with DuoLens enable sentence-level detection and explanatory outputs without sacrificing the efficiency gains from SLMs?
- Does balancing the multilingual text dataset improve low-resource language performance without degrading overall accuracy?
- How well does DuoLens generalize to machine-generated content from closed-source generators beyond GPT-4o (e.g., Claude, Gemini)?

## Limitations
- Dataset curation details and preprocessing steps are not fully specified, limiting reproducibility.
- Fusion gate architecture is only briefly described, leaving ambiguity about its implementation.
- Claims about cross-model robustness and adversarial resilience lack granularity in evaluation protocols.

## Confidence
- **High Confidence**: Dual-encoder architecture and fusion mechanism are clearly described and reproducible; latency and VRAM improvements are plausible.
- **Medium Confidence**: AUROC and macro-F1 metrics are well-defined, but generalization depends on undisclosed dataset details.
- **Low Confidence**: Adversarial robustness claims lack specificity in generation methods and evaluation protocols.

## Next Checks
1. Obtain and preprocess the multilingual text and code datasets as specified, ensuring balanced class distributions and consistent tokenization (512 tokens, chunking). Validate dataset splits and label integrity.
2. Replicate training with varied learning rates (1e-5 to 5e-5), batch sizes (16 to 64), and epochs (3 to 5) to assess stability of reported AUROC/Macro-F1 scores. Document convergence patterns and early stopping criteria.
3. Generate adversarial samples using diverse language models not included in training data. Evaluate DuoLens performance under these conditions, comparing against baselines to confirm ≥92% clean AUROC retention.