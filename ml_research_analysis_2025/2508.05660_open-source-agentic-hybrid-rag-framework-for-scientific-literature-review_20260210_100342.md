---
ver: rpa2
title: Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review
arxiv_id: '2508.05660'
source_url: https://arxiv.org/abs/2508.05660
tags:
- retrieval
- literature
- arxiv
- agentic
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an open-source agentic hybrid RAG framework
  for scientific literature review that dynamically selects between graph-based (GraphRAG)
  and vector-based (VectorRAG) retrieval methods for each query. The framework integrates
  a Neo4j knowledge graph with citation relationships and a FAISS vector store using
  all-MiniLM-L6-v2 embeddings, orchestrated by a Llama-3.3-70B agent that adapts retrieval
  strategies based on query type.
---

# Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review

## Quick Facts
- **arXiv ID**: 2508.05660
- **Source URL**: https://arxiv.org/abs/2508.05660
- **Reference count**: 40
- **Primary result**: Introduces an open-source agentic hybrid RAG framework that dynamically selects between graph-based and vector-based retrieval methods for scientific literature review

## Executive Summary
This paper presents an open-source agentic hybrid RAG framework designed to automate scientific literature review by dynamically selecting between GraphRAG and VectorRAG retrieval methods based on query type. The framework integrates a Neo4j knowledge graph with citation relationships and a FAISS vector store using all-MiniLM-L6-v2 embeddings, orchestrated by a Llama-3.3-70B agent that adapts retrieval strategies. Instruction tuning with Direct Preference Optimization (DPO) refines the response generator, while bootstrapped evaluation provides uncertainty quantification. On synthetic benchmarks mimicking real-world queries, the framework achieved substantial performance gains in context precision, recall, faithfulness, and answer relevance.

## Method Summary
The framework implements a hybrid retrieval system that dynamically routes queries to either GraphRAG (Neo4j-based) or VectorRAG (FAISS-based) depending on query characteristics. A Llama-3.3-70B agent classifies each query using few-shot prompting and invokes appropriate retrieval tools. The system ingests bibliometric data and full-text PDFs, filters using TF-IDF and cosine similarity, and stores content in both a knowledge graph and vector store. Mistral-7B-Instruct-v0.3 serves as the response generator, fine-tuned with DPO using 15 human preference pairs. Evaluation uses bootstrapped resampling on synthetic benchmarks to measure context precision, recall, faithfulness, and answer relevance.

## Key Results
- Instruction-Tuned Agent with DPO achieved 0.63 improvement in VS Context Recall
- Overall Context Precision improved by 0.56
- VS Faithfulness increased by 0.24
- Both VS Precision and KG Answer Relevance improved by 0.12

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing between GraphRAG and VectorRAG based on query type improves retrieval relevance over static pipelines.
- Mechanism: A Llama-3.3-70B agent classifies each query and invokes either Cypher-based graph traversal or hybrid sparse-dense vector retrieval with reranking. Few-shot prompting (10 examples total) guides classification.
- Core assumption: Query intent maps cleanly to one retrieval modality; hybrid queries can be decomposed or approximated by single-mode selection.
- Evidence anchors: [abstract] "A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking)." [section] Page 4: "the agent's decision-making and promotes consistent performance across different query types."
- Break condition: Queries requiring both structured relationships AND deep semantic understanding simultaneously may force suboptimal single-path selection.

### Mechanism 2
- Claim: Separating bibliometric metadata (graph) from full-text content (vectors) leverages complementary retrieval strengths.
- Mechanism: Neo4j stores citation networks, author relationships, and keyword connections. FAISS stores embedded text chunks (2024 chars, 50 overlap) using all-MiniLM-L6-v2. TF-IDF keywords filtered by cosine similarity threshold (Q3) populate both systems.
- Core assumption: Structured queries (collaboration patterns, timelines) benefit from graph traversal; conceptual queries benefit from dense semantic retrieval.
- Evidence anchors: [abstract] "integrates a Neo4j knowledge graph built from bibliometric data and a FAISS vector store containing embedded full-text PDFs" [section] Page 4-5: GraphRAG "excels at structured, metadata-driven queries" while VectorRAG "captures nuanced, full-text information"
- Break condition: When key information exists only in figures/tables (not chunked into vectors) or when citation networks are sparse/incomplete.

### Mechanism 3
- Claim: Direct Preference Optimization with minimal human annotation (15 pairs) improves faithfulness and context utilization.
- Mechanism: DPO explicitly trains the Mistral-7B generator to prefer context-grounded responses over hallucinated content, aligning outputs with human judgment without full RLHF infrastructure.
- Core assumption: Small preference datasets can shift generator behavior meaningfully when domain and task are well-scoped.
- Evidence anchors: [abstract] "Instruction tuning and Direct Preference Optimization (DPO) refine domain-specific generation" [section] Page 7: "DPO explicitly teaches the response generator to prefer answers grounded in the retrieved context"
- Break condition: Domain shift or novel query types may require additional preference annotation; overfitting to small preference sets possible.

## Foundational Learning

- Concept: **Knowledge Graph Querying (Cypher)**
  - Why needed here: GraphRAG requires translating natural language to Cypher queries; understanding node-edge schemas is prerequisite for debugging retrieval failures.
  - Quick check question: Can you write a Cypher query to find all papers co-authored by two specific authors?

- Concept: **Hybrid Retrieval (Sparse + Dense)**
  - Why needed here: VectorRAG combines BM25 (lexical matching) with L2-distance semantic search; understanding when each dominates guides routing logic.
  - Quick check question: Explain why BM25 might outperform dense retrieval on queries with rare technical terms.

- Concept: **RAG Faithfulness Metrics**
  - Why needed here: The paper optimizes for context precision, recall, and faithfulness; these differ from standard generation quality metrics.
  - Quick check question: How does Context Recall differ from Answer Relevance in RAG evaluation?

## Architecture Onboarding

- Component map:
  - Data ingestion (PubMed/ArXiv/Google Scholar APIs) → deduplication → TF-IDF keyword extraction → cosine similarity filtering → Neo4j graph storage (publications, authors, keywords, citations) + FAISS vector storage (embedded text chunks)
  - Query → Llama-3.3-70B agent classification → Tool invocation (Cypher query OR ensemble retrieval + rerank) → Context assembly → Mistral-7B generation → Response

- Critical path: Query → Agent classification → Tool invocation (Cypher OR ensemble retrieval + rerank) → Context assembly → Mistral generation → Response

- Design tradeoffs:
  - GraphRAG: Better for author/timeline/citation queries; weaker on full-text semantics
  - VectorRAG: Better for conceptual/novel queries; weaker on multi-hop relational reasoning
  - Single-agent routing (current) vs. multi-agent decomposition (future): simpler but may miss hybrid queries

- Failure signatures:
  - Low KG Precision: Cypher translation errors; check few-shot examples and schema alignment
  - Low VS Faithfulness: Reranking failure or chunking quality issues; inspect retrieved chunks
  - Routing errors: Query falls between training examples; expand few-shot coverage

- First 3 experiments:
  1. **Routing accuracy audit**: Manually classify 50 queries; compare against agent decisions to quantify misclassification rate.
  2. **Retrieval ablation**: Disable GraphRAG, then VectorRAG, on benchmark set to measure per-modality contribution.
  3. **DPO scaling test**: Train with 15 vs. 50 vs. 100 preference pairs to validate whether the paper's minimal-annotation claim generalizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) optimize the agent's policy for dynamically selecting between GraphRAG and VectorRAG more effectively than the current instruction-tuned approach?
- Basis in paper: [explicit] The Discussion section states, "RL (e.g., RLHF or reward-model fine-tuning) could learn an optimal mix of GraphRAG and VectorRAG based on end-task rewards."
- Why unresolved: The current framework relies on a Llama-3.3-70B agent with few-shot prompting to orchestrate retrieval. It is undetermined if an RL-based policy would yield superior routing decisions or adaptability compared to the current heuristic-based agent.
- What evidence would resolve it: A comparative study measuring retrieval accuracy and latency where the routing agent is trained via RLHF against the current instruction-tuned baseline.

### Open Question 2
- Question: Does fine-tuning a specialized model for Cypher generation significantly reduce translation errors and improve multi-hop recall compared to few-shot prompting?
- Basis in paper: [explicit] The Discussion notes, "GraphRAG currently uses few-shot prompting to generate Cypher, which can misinterpret complex queries. Fine-tuning a model... should reduce translation errors."
- Why unresolved: While the authors identify misinterpretation of complex queries as a limitation causing slight decreases in KG Precision (-0.04), they have not tested if fine-tuning specifically on (Natural Language Query, Cypher) pairs resolves this.
- What evidence would resolve it: Evaluation of the GraphRAG module's precision and recall using a fine-tuned translator compared to the current few-shot Llama-3.3-70B implementation.

### Open Question 3
- Question: How does the framework perform on multi-modal scientific inquiries involving figures and tables, which are currently excluded from the evaluation?
- Basis in paper: [explicit] The Limitations section states, "Our synthetic benchmark... may not capture the full complexity of real scientific inquiries, such as multi-modal reasoning over figures and tables."
- Why unresolved: The current system embeds full-text PDFs into a vector store but lacks explicit mechanisms for parsing charts or figures. Consequently, its ability to answer queries dependent on visual data remains unverified.
- What evidence would resolve it: Benchmarking the system on a dataset where ground-truth answers require extracting information from images or tables within the ingested papers.

### Open Question 4
- Question: Does scaling the Direct Preference Optimization (DPO) dataset beyond 15 pairs reverse the observed degradation in Knowledge Graph Faithfulness and Precision?
- Basis in paper: [inferred] The paper reports using only "15 high-quality, human-annotated preference pairs" for DPO, while the Results section shows the Fine-Tuned Agentic model caused slight decreases in KG Faithfulness (-0.03) and KG Precision (-0.04).
- Why unresolved: It is unclear if the performance drop in KG metrics is a fundamental limitation of the agentic approach or a result of the DPO dataset being too small or too biased toward VectorRAG-style responses.
- What evidence would resolve it: Experiments varying the size and composition of the DPO training data to analyze the correlation between dataset scale and the recovery of KG-specific performance metrics.

## Limitations

- The paper lacks direct comparison against static hybrid baselines to validate claims about dynamic routing superiority
- Benchmark construction relies on incomplete specifications for synthetic question generation, limiting reproducibility
- Claims about minimal DPO annotation requirements lack comparative validation across different domains or query distributions

## Confidence

- **High confidence**: Technical architecture description (Neo4j schema, FAISS embedding parameters, retrieval pipeline) is detailed and verifiable
- **Medium confidence**: Performance improvements reported are statistically validated but may reflect synthetic benchmark artifacts
- **Low confidence**: Claims about minimal DPO annotation requirements and routing superiority over alternatives lack direct comparative validation

## Next Checks

1. Conduct a routing accuracy audit by manually classifying 50 queries to quantify misclassification rates against the agent's tool selection decisions
2. Perform retrieval ablation by disabling GraphRAG and VectorRAG separately on the benchmark set to measure individual modality contributions and identify query types where routing fails
3. Test DPO scaling by training with 15, 50, and 100 preference pairs to validate whether the paper's minimal-annotation claim generalizes across different domains