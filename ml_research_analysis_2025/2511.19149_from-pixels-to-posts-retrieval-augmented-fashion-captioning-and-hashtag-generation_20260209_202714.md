---
ver: rpa2
title: 'From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation'
arxiv_id: '2511.19149'
source_url: https://arxiv.org/abs/2511.19149
tags:
- fashion
- captioning
- image
- generation
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a retrieval-augmented pipeline for automatic
  fashion caption and hashtag generation, integrating multi-garment detection, attribute
  reasoning, and LLM prompting. The system detects apparel items using YOLO, extracts
  dominant colors via k-means clustering, and retrieves fabric and gender attributes
  using CLIP-FAISS similarity search.
---

# From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation

## Quick Facts
- arXiv ID: 2511.19149
- Source URL: https://arxiv.org/abs/2511.19149
- Authors: Moazzam Umer Gondal; Hamad Ul Qudous; Daniya Siddiqui; Asma Ahmad Farhan
- Reference count: 31
- Primary result: Retrieval-augmented pipeline achieves mean attribute coverage of 0.80 and full coverage at 50% threshold for hashtag generation, with better factual grounding than supervised baseline.

## Executive Summary
This paper presents a retrieval-augmented pipeline for automatic fashion caption and hashtag generation, integrating multi-garment detection, attribute reasoning, and LLM prompting. The system detects apparel items using YOLO, extracts dominant colors via k-means clustering, and retrieves fabric and gender attributes using CLIP-FAISS similarity search. These structured evidence packs guide an LLM to generate visually grounded captions and contextually rich hashtags. A fine-tuned BLIP model serves as a supervised baseline. Experimental results show YOLO achieves mAP@0.5 of 0.71 for nine garment categories, while the RAG-LLM pipeline achieves mean attribute coverage of 0.80 and full coverage at the 50% threshold in hashtag generation. The retrieval-augmented approach demonstrates better factual grounding, reduced hallucination, and improved generalizability compared to BLIP, making it a promising solution for automated, visually grounded fashion content creation.

## Method Summary
The system uses YOLOv11s for multi-garment detection, k-means clustering for dominant color extraction from detected crops, and CLIP-FAISS retrieval for fabric and gender attributes. The modular pipeline constructs an evidence pack containing detection results, color information, and retrieved attributes, which conditions an LLM (Llama 3 via Groq) for caption and hashtag generation. A BLIP baseline is fine-tuned end-to-end for comparison. The retrieval-augmented approach trades lexical fidelity for better factual grounding and generalization to unseen categories.

## Key Results
- YOLO achieves mAP@0.5 of 0.71 for nine garment categories on fashion dataset
- RAG-LLM pipeline achieves mean attribute coverage of 0.80 and full coverage at 50% threshold for hashtag generation
- BLIP baseline shows higher BLEU/METEOR scores but RAG-LLM demonstrates better factual grounding and reduced hallucination
- RAG-LLM captions maintain factual accuracy while achieving greater linguistic diversity compared to originals

## Why This Works (Mechanism)

### Mechanism 1: Modular Perception-Decoupling via Structured Evidence Packs
Separating visual perception from language generation through explicit structured intermediates reduces hallucination and improves attribute grounding compared to end-to-end architectures. YOLO detects garment instances → k-means extracts localized colors from crops (not full images) → CLIP-FAISS retrieves similar catalog items → all outputs aggregate into evidence pack E={D,A,R} → LLM generates from pack rather than raw pixels. The retrieval step acts as a stability prior by voting over neighbors rather than single-image inference. Core assumption: Catalog metadata is accurate and representative; detection errors do not cascade catastrophically.

### Mechanism 2: Similarity-Weighted Attribute Voting Over Retrieval Neighbors
Aggregating attributes from top-K visually similar catalog examples reduces single-image inference variance and stabilizes predictions for fine-grained attributes like fabric. CLIP embeds query image → FAISS retrieves K=20 nearest neighbors via cosine similarity → each neighbor contributes weighted vote for its attribute labels → exponential weighting (temperature τ) emphasizes high-similarity neighbors → confidence computed as normalized score. Low-confidence predictions marked "unknown." Core assumption: Visual similarity in CLIP space correlates with shared attributes; exponential weighting appropriately balances near vs. distant neighbors.

### Mechanism 3: Retrieval-Conditioned LLM Generation Trading Lexical Fidelity for Grounding
Conditioning LLM on structured evidence packs yields lower n-gram overlap but higher factual grounding and generalization to unseen categories compared to supervised fine-tuning. Evidence pack serialized to prompt template → LLM generates caption + hashtags with temperature ~0.7 → retrieval examples provide stylistic cues without template repetition. BLIP baseline fine-tuned end-to-end memorizes training syntax but generalizes poorly. Core assumption: Prompt engineering can effectively constrain LLM to use provided attributes; CLIP similarity meaningfully captures visual-semantic alignment.

## Foundational Learning

- Concept: Mean Average Precision (mAP) and IoU thresholds
  - Why needed here: YOLO detector evaluated at mAP@0.5=0.71; understanding this metric is essential for diagnosing detection quality and its downstream impact on captioning.
  - Quick check question: If mAP@0.5 is high but mAP@0.5:0.95 drops sharply, what does this indicate about localization precision?

- Concept: CLIP vision-language embeddings and cosine similarity
  - Why needed here: Retrieval module depends on CLIP ViT-B/32 embeddings; similarity search enables attribute propagation from catalog to query.
  - Quick check question: Why must CLIP embeddings be L2-normalized before computing cosine similarity via inner product in FAISS?

- Concept: Prompt engineering with structured context injection
  - Why needed here: LLM generation quality depends on evidence pack E being serialized effectively into prompt templates with appropriate instructions.
  - Quick check question: What failure mode occurs if the prompt does not explicitly instruct the LLM to use all provided attributes?

## Architecture Onboarding

- Component map:
Input Image → [YOLOv11s Detector] → Bounding boxes + class labels
                    ↓
            [Crop per detection] → [K-means color extraction] → Primary/secondary colors
                    ↓
[Full image] → [CLIP encoder] → [FAISS index query] → Top-K neighbors + metadata
                    ↓
            [Weighted attribute voting] → Fabric, gender predictions
                    ↓
            [Evidence pack assembly] → {D: detections/colors, A: attributes, R: retrieved examples}
                    ↓
            [LLM prompt templates] → Caption + 15-18 hashtags

- Critical path: Detection quality → Color extraction accuracy → Retrieval relevance → Evidence pack completeness → LLM output grounding. Errors compound downstream.

- Design tradeoffs:
  - BLIP vs RAG-LLM: BLIP gives higher BLEU/METEOR (lexical fidelity to training corpus) but requires retraining for new categories. RAG-LLM generalizes zero-shot but with lower n-gram overlap.
  - K=20 neighbors: Higher K smooths noise but may dilute signal; temperature τ controls weighting sharpness.
  - Detection threshold θ_conf=0.35: Lower threshold increases recall but introduces false positives polluting downstream text.

- Failure signatures:
  - Low attribute coverage (<0.5): Check if detection missing garments, catalog lacks relevant items, or voting threshold too aggressive.
  - Hallucinated attributes in captions: Verify evidence pack is being injected into prompt correctly; check LLM temperature.
  - Repetitive hashtags (low Distinct-n): Prompt may lack diversity instructions; retrieved examples may be homogeneous.
  - Detection misses on occluded garments: YOLO trained primarily on visible instances; may need augmentation or higher resolution.

- First 3 experiments:
  1. Ablate retrieval by setting K=0 (no neighbors): Compare attribute coverage and caption grounding to quantify retrieval contribution.
  2. Vary detection confidence threshold (0.25, 0.35, 0.45): Measure precision-recall tradeoff and downstream impact on caption completeness.
  3. Compare prompt templates (with/without explicit attribute usage instructions): Evaluate whether LLM uses provided evidence or generates from priors.

## Open Questions the Paper Calls Out

### Open Question 1
Can jointly fine-tuning the CLIP encoder and captioning model in a unified retrieval-generation loop improve semantic cohesion compared to the current modular pipeline?
Basis in paper: Authors state in Section 6: "the CLIP encoder and the captioning model can be fine-tuned together in a single retrieval-generation loop, which could enhance semantic cohesion and decrease reliance on fixed metadata." Why unresolved: Current architecture uses CLIP and LLM modules separately without gradient flow between retrieval and generation components. What evidence would resolve it: Comparative experiment showing improved CLIP similarity scores or reduced hallucination rates from end-to-end joint training versus the modular baseline.

### Open Question 2
Does the RAG-LLM pipeline produce captions and hashtags that yield higher user engagement metrics (click-through rates, likes, shares) in live social media deployments compared to the BLIP baseline?
Basis in paper: Section 6 states: "user engagement metrics (like click-through or like rates) would offer a viable authentication of caption and hashtag efficiency in social settings." Why unresolved: All evaluation in the paper uses offline metrics (BLEU, METEOR, attribute coverage) rather than real-world user behavior signals. What evidence would resolve it: A/B test deploying both systems on live fashion social accounts, measuring statistically significant differences in engagement over sufficient sample sizes.

### Open Question 3
How robust is the retrieval-augmented pipeline to noise, bias, or sparsity in the catalog metadata used for retrieval?
Basis in paper: Section 6 notes: "The quality of the indexed catalog and its representativeness are very important in the retrieval stage; noise and biased metadata may be transferred to the generated captions." Why unresolved: The paper assumes a curated catalog with clean metadata but does not systematically evaluate failure modes under degraded retrieval conditions. What evidence would resolve it: Controlled experiments injecting label noise or removing metadata subsets, measuring resulting changes in attribute coverage and hallucination rates.

## Limitations
- Pipeline depends heavily on catalog metadata quality and visual diversity - missing fabric types or garment styles limit weighted voting effectiveness
- Detection errors cascade downstream; YOLO's mAP@0.5 of 0.71 means occlusions or unusual poses may cause missed garments
- k-means color extraction may capture background colors if garments are small or complex, introducing noise
- Reliance on Llama 3 via Groq API introduces cost and reproducibility constraints; exact prompt templates remain unspecified

## Confidence
- High confidence: YOLO detection performance metrics (mAP@0.5=0.71) and the retrieval-augmented generation framework architecture
- Medium confidence: Attribute coverage results (0.80 mean, 1.0@0.5) and the claim that RAG-LLM generalizes better than BLIP
- Low confidence: The exact contribution of each module to final output quality without ablation studies, and the robustness of the system to domain shifts in fashion styles

## Next Checks
1. **Catalog Bias Analysis**: Systematically evaluate how retrieval quality degrades when catalog images are missing certain fabric types, colors, or garment categories. This would quantify the limits of the weighted voting mechanism.

2. **Cross-Domain Generalization Test**: Apply the trained pipeline to fashion images from a different cultural context (e.g., Western vs South Asian styles) to assess whether the CLIP-FAISS retrieval remains effective when visual priors shift.

3. **Evidence Pack Ablation Study**: Compare attribute coverage and caption factual accuracy when: (a) using only detection output, (b) adding color extraction, (c) adding retrieval-based attributes. This would isolate each module's contribution to grounding.