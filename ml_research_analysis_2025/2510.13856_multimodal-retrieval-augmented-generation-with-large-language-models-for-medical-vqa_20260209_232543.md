---
ver: rpa2
title: Multimodal Retrieval-Augmented Generation with Large Language Models for Medical
  VQA
arxiv_id: '2510.13856'
source_url: https://arxiv.org/abs/2510.13856
tags:
- visual
- images
- wound
- medical
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a lightweight retrieval-augmented generation
  approach for medical visual question answering, specifically targeting wound-care
  queries. Using a general-domain instruction-tuned LLM (LLaMA-4 Scout 17B), the system
  incorporates multimodal exemplars from in-domain data to improve grounding and reasoning.
---

# Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA

## Quick Facts
- arXiv ID: 2510.13856
- Source URL: https://arxiv.org/abs/2510.13856
- Authors: A H M Rezaul Karim; Ozlem Uzuner
- Reference count: 19
- Ranked 3rd among 19 teams in MEDIQA-WV 2025 shared task with average score of 41.37%

## Executive Summary
This study presents a lightweight retrieval-augmented generation approach for medical visual question answering, specifically targeting wound-care queries. The system uses a general-domain instruction-tuned LLM (LLaMA-4 Scout 17B) enhanced with multimodal exemplars from in-domain data to improve grounding and reasoning. A multimodal RAG pipeline retrieves top-2 relevant text and image exemplars at inference time, which are then included in the prompt to enhance response quality. The approach achieved third place in the MEDIQA-WV 2025 shared task, demonstrating that lightweight RAG with general-purpose LLMs offers an effective solution for multimodal clinical NLP tasks.

## Method Summary
The approach employs a multimodal retrieval-augmented generation pipeline for medical visual question answering. The system uses LLaMA-4 Scout 17B as the base LLM, which is not fine-tuned but enhanced through prompt engineering with retrieved exemplars. At inference time, a multimodal RAG pipeline retrieves the top-2 most relevant text and image exemplars from in-domain data. These exemplars are incorporated into the prompt to improve the model's grounding and reasoning capabilities for wound-care related queries. The method specifically targets the MEDIQA-WV 2025 shared task dataset and does not employ any domain-specific fine-tuning of the underlying language model.

## Key Results
- Achieved 3rd place ranking among 19 teams in MEDIQA-WV 2025 shared task with average score of 41.37%
- Retrieval-augmented prompting significantly outperformed zero-shot and few-shot baselines
- Multimodal retrieval improved clinical specificity and schema adherence compared to unimodal approaches

## Why This Works (Mechanism)
The approach leverages retrieval-augmented generation to provide context-specific exemplars that help ground the general-purpose LLM in medical domain knowledge. By retrieving relevant text and image examples from in-domain data, the system compensates for the LLM's lack of specialized medical training. The multimodal retrieval ensures both visual and textual context are available, which is critical for VQA tasks where understanding requires integrating information from both modalities. The lightweight nature of the approach makes it reproducible and efficient, avoiding the computational overhead of fine-tuning while still achieving competitive performance.

## Foundational Learning

1. **Retrieval-augmented generation (RAG)**: Why needed - provides relevant context to LLMs without fine-tuning; Quick check - verify retrieval relevance and integration into prompts

2. **Multimodal embeddings**: Why needed - enables retrieval across both text and image modalities; Quick check - ensure embeddings capture semantic similarity within and across modalities

3. **Visual question answering (VQA) schema**: Why needed - standardizes expected outputs for medical wound-care queries; Quick check - validate adherence to schema requirements

4. **Instruction-tuned LLMs**: Why needed - provides baseline reasoning capabilities that can be enhanced with retrieval; Quick check - verify model follows instructions consistently

5. **In-domain exemplar selection**: Why needed - ensures retrieved examples are clinically relevant and representative; Quick check - assess exemplar quality and relevance to query types

6. **Zero-shot vs. few-shot vs. retrieval-augmented baselines**: Why needed - establishes performance improvements from retrieval; Quick check - compare output quality across different prompting strategies

## Architecture Onboarding

Component Map: Query -> Multimodal Retriever -> Exemplars -> Prompt Construction -> LLM (LLaMA-4 Scout 17B) -> Response

Critical Path: Input query and image are processed by retriever to obtain top-2 text and image exemplars, which are formatted into a prompt that includes the exemplars, query, and image. This prompt is sent to the LLM, which generates the final response following the MEDIQA-WV schema.

Design Tradeoffs: The approach trades model specialization for prompt engineering and retrieval efficiency. By avoiding fine-tuning, the system maintains flexibility and reduces computational costs, but relies heavily on the quality of retrieved exemplars and prompt construction. The choice of top-2 exemplars represents a balance between providing sufficient context and avoiding prompt overload.

Failure Signatures: Poor retrieval relevance leading to incorrect or irrelevant context, insufficient exemplar diversity causing biased responses, and prompt engineering issues where exemplars are not effectively integrated into the reasoning process. The general-purpose LLM may also struggle with highly specialized medical terminology or rare conditions not well-represented in the exemplar set.

First Experiments:
1. Test retrieval relevance by manually evaluating top-2 exemplars across diverse query types
2. Compare performance with varying numbers of retrieved exemplars (1, 2, 5) to optimize context quantity
3. Evaluate response quality with and without multimodal exemplars to quantify the contribution of visual context

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single shared task competition dataset, limiting generalizability to real-world clinical settings
- No domain-specific fine-tuning raises questions about robustness for complex or rare medical cases
- Lack of detailed failure mode analysis limits understanding of where the approach may underperform
- Optimal retrieval parameters (e.g., number of exemplars) not extensively explored

## Confidence

| Claim Cluster | Confidence Level |
|---|---|
| Retrieval-augmented prompting improves VQA performance | High |
| Multimodal retrieval adds clinical specificity and schema adherence | Medium |
| Lightweight RAG with general-purpose LLMs is effective for multimodal clinical NLP | Medium |

## Next Checks

1. Conduct external validation on diverse, independent medical imaging datasets to assess generalizability beyond the MEDIQA-WV 2025 benchmark

2. Perform ablation studies to determine optimal number and type of retrieved exemplars, and compare retrieval strategies (e.g., semantic similarity vs. clinical relevance)

3. Analyze failure cases and error types to identify specific limitations of the retrieval-augmented approach in handling complex or rare medical scenarios