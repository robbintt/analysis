---
ver: rpa2
title: Partial Convolution Meets Visual Attention
arxiv_id: '2503.03148'
source_url: https://arxiv.org/abs/2503.03148
tags:
- attention
- partial
- vision
- hybrid
- patnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of depthwise convolution (DWConv)
  in efficient CNNs, which suffers from low throughput due to frequent memory access,
  and the accuracy compromise of partial convolution (PConv) introduced by FasterNet
  due to underutilized channels. The authors propose a novel Partial visual ATtention
  mechanism (PAT) that combines PConv with visual attention to enhance feature representation
  while maintaining efficiency.
---

# Partial Convolution Meets Visual Attention

## Quick Facts
- arXiv ID: 2503.03148
- Source URL: https://arxiv.org/abs/2503.03148
- Reference count: 40
- Primary result: PATNet achieves 1.3% higher top-1 accuracy and 25% higher GPU throughput than FasterNet on ImageNet-1K

## Executive Summary
This paper addresses the limitations of depthwise convolution (DWConv) in efficient CNNs, which suffers from low throughput due to frequent memory access, and the accuracy compromise of partial convolution (PConv) introduced by FasterNet due to underutilized channels. The authors propose a novel Partial visual ATtention mechanism (PAT) that combines PConv with visual attention to enhance feature representation while maintaining efficiency. PAT introduces three types of blocks: Partial Channel-Attention (PAT_ch), Partial Spatial-Attention (PAT_sp), and Partial Self-Attention (PAT_sf), each integrating attention mechanisms into different aspects of the network. The proposed PATNet architecture, built upon these blocks, achieves superior top-1 accuracy and inference speed compared to FasterNet on ImageNet-1K classification, with 1.3% higher accuracy and 25% higher GPU throughput. PATNet also excels in object detection and segmentation on the COCO dataset, demonstrating its effectiveness across various vision tasks.

## Method Summary
PATNet introduces a Partial visual ATtention (PAT) mechanism that combines Partial Convolution (PConv) with visual attention modules. The architecture processes only a subset of channels with convolution (ratio $r_p = 1/4$) while applying attention to the remaining channels. Three PAT blocks are defined: PAT_ch uses enhanced Gaussian channel attention (mean + std), PAT_sp uses spatial attention within MLP layers (fusable for efficiency), and PAT_sf uses partial self-attention with relative position encoding in the final stage. The network employs a 4-stage design with depths 2-2-6-4 and progressively increasing channel widths. Training uses AdamW optimizer with 300 epochs, large batch sizes, and strong augmentation including Mixup and Cutmix.

## Key Results
- PATNet-T2 achieves 80.2% top-1 accuracy on ImageNet-1K, 1.3% higher than FasterNet baselines
- GPU throughput reaches 4761 FPS compared to FasterNet's 3976 FPS (25% improvement)
- On COCO detection, PATNet-L2 achieves 50.8% mAP with 2.2× faster inference than FasterNet-L2
- FLOPs reduction of 40% while maintaining or improving accuracy across all model variants

## Why This Works (Mechanism)

### Mechanism 1: Partial Channel Processing with Redundancy Exploitation
- **Claim**: Processing only a subset of channels with convolution while applying attention to the remainder improves efficiency without sacrificing accuracy.
- **Mechanism**: PConv operates on ~25% of channels (ratio $r_p = 1/4$), reducing FLOPs and memory access. The untouched channels receive attention-based global information rather than identity mapping, enabling cross-channel interaction without full convolutional cost.
- **Core assumption**: Feature map channels contain significant redundancy, so not all channels require dense spatial processing.
- **Evidence anchors**:
  - [abstract] "Our exploration indicates that the partial attention mechanism can completely replace the full attention mechanism and reduce model parameters and FLOPs."
  - [section] "PConv leverages redundancy within feature maps to selectively apply Conv to a subset of input channels, leaving the remaining channels untouched." (Page 2)
  - [corpus] "Partial Channel Network: Compute Fewer, Perform Better" validates channel redundancy exploitation as a general efficiency strategy.
- **Break condition**: If channel redundancy assumption fails (e.g., in very narrow networks < 32 channels), partial processing may underperform full convolution.

### Mechanism 2: Enhanced Gaussian Channel Attention for Statistical Representation
- **Claim**: Using both mean and standard deviation in channel attention captures richer statistical information than mean-only approaches like SENet.
- **Mechanism**: PAT_ch computes $\mu$ and $\sigma$ per channel, concatenates them, applies Conv1×2 to generate attention weights via Hard-Sigmoid. This infuses global distribution information into untouched channels.
- **Core assumption**: Feature maps approximately follow normal distribution during training, so Gaussian statistics (mean + variance) fully characterize channel importance.
- **Evidence anchors**:
  - [section] "Considering that the feature maps obey an approximately normal distribution during training, we fully utilize the Gaussian statistical to express the channel-wise representation information." (Page 5)
  - [table] Table 10 shows PAT achieves 80.2% vs. SENet's 79.8% and SRM's 79.6% top-1 accuracy with similar FLOPs.
  - [corpus] Weak direct evidence—no corpus papers validate Gaussian assumptions in attention mechanisms.
- **Break condition**: If feature distributions deviate significantly from Gaussian (e.g., after ReLU in early layers), variance-based weighting may not provide additional signal.

### Mechanism 3: Stage-Specific Attention Allocation with Late Self-Attention
- **Claim**: Placing computationally expensive self-attention only in the final stage balances global receptive field expansion with quadratic complexity costs.
- **Mechanism**: Stages 1-3 use PAT_ch (local) + PAT_sp (channel mixing); Stage 4 replaces PAT_ch with PAT_sf (global self-attention). Self-attention's $O(n^2)$ complexity becomes manageable only at h/32 × w/32 resolution.
- **Core assumption**: Global context becomes most critical at later stages where features are semantically rich and spatially compressed.
- **Evidence anchors**:
  - [section] "we restrict the use of PAT_sf to the last stage to achieve a superior speed-accuracy trade-off." (Page 6)
  - [table] Table 4 shows adding PAT_sf boosts accuracy from 78.9% → 80.2% (+1.3%), while earlier-stage modifications show smaller gains.
  - [corpus] Weak direct evidence—no comparative stage allocation studies in neighbors.
- **Break condition**: If early-stage global context is critical for specific tasks (e.g., scene-level reasoning), this allocation may be suboptimal.

## Foundational Learning

- **Concept: Channel Redundancy in Feature Maps**
  - **Why needed here**: Understanding that feature channels contain redundant information is fundamental to grasping why processing 25% of channels works.
  - **Quick check question**: Why can PATNet process only $C_p = r_p \times C$ channels without significant accuracy loss?

- **Concept: FLOPs vs. Throughput vs. Latency**
  - **Why needed here**: The paper optimizes for actual hardware speed (FPS), not just theoretical FLOPs; DWConv has low FLOPs but poor throughput due to memory access patterns.
  - **Quick check question**: Why does DWConv achieve 4017 FPS while PAT_ch achieves 4761 FPS despite DWConv having lower FLOPs (1.28G vs 1.03G)?

- **Concept: Attention Fusion and Inference Optimization**
  - **Why needed here**: PAT_sp's Conv1×1 can fuse with MLP's second Conv1×1 during inference, eliminating overhead.
  - **Quick check question**: How does fusing PAT_sp with MLP layers maintain attention benefits without inference cost?

## Architecture Onboarding

- **Component map**:
  - Embedding: Conv4×4 (stride 4) → BN
  - Merging: Conv2×2 (stride 2) → BN (between stages)
  - PAT_ch: Split → PConv3×3 on $C_p$ + Gaussian-SE on $C-C_p$ → Concat → BN
  - PAT_sp: Split → Conv1×1 + Hard-Sigmoid spatial attention → Concat → BN (fusable with MLP)
  - PAT_sf: Split → Self-attention with RPE on $C-C_p$ → Concat → LN
  - MLP: Conv1×1 → BN → Acti → Conv1×1 (PAT_sp fuses into second Conv1×1)

- **Critical path**:
  1. Set partial ratio $r_p = 0.25$; split channels into $C_p$ and $C-C_p$
  2. Implement Gaussian-SE: compute mean and std → concat → Conv1×2 → Hard-Sigmoid
  3. Implement PAT_sp with fusion capability: the Conv1×1 must be structured for re-parameterization
  4. Configure stage depths as 2-2-6-4 (last stage = 2× first two stages)
  5. Use GELU for tiny variants, ReLU for larger variants

- **Design tradeoffs**:
  - **Partial ratio**: Lower $r_p$ = faster but potentially less accurate; paper uses 1/4 as default
  - **Stage 4 depth**: Increasing from 2 to 4 blocks yields +1.4% accuracy (Table 4, rows 5-6)
  - **Width reduction**: PATNet uses narrower channels than FasterNet for speed; compensates with attention and depth

- **Failure signatures**:
  - If GPU throughput doesn't improve over baseline, verify PAT_sp-MLP fusion is correctly implemented
  - If accuracy degrades significantly, check split-concatenate channel ordering
  - If self-attention OOM in Stage 4, ensure input resolution is h/32 × w/32

- **First 3 experiments**:
  1. Reproduce Table 3 ablation: compare partial vs. full attention on PATNet-T2 to validate PAT efficiency
  2. Reproduce Table 5: compare PAT_ch vs. Conv vs. DWConv with matched throughput to verify superiority
  3. Reproduce Table 4 stage depth experiment (2-2-6-4 vs. 2-2-8-2) to confirm last-stage importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the partial visual attention mechanism be effectively adapted to improve efficiency in Natural Language Processing (NLP) or Large Language Models (LLMs)?
- Basis in paper: [explicit] The conclusion states, "The idea of partial attention still has great potential in the natural language processing (NLP) or large language model (LLM) domains."
- Why unresolved: The current work evaluates the mechanism exclusively on computer vision tasks (ImageNet-1K classification and COCO detection/segmentation), leaving its applicability to sequential data and language tasks unverified.
- What evidence would resolve it: Implementation of partial attention within standard Transformer architectures (e.g., BERT, GPT) and evaluation on NLP benchmarks to verify if FLOPs and latency reduction occurs without compromising perplexity or accuracy.

### Open Question 2
- Question: How does the PAT mechanism perform when combined with visual attention modules other than the three specific types (channel, spatial, self-attention) presented?
- Basis in paper: [explicit] Section 3.2 states, "Our PAT is not limited to the above three combinations, it can be efficiently combined with more visual attention modules."
- Why unresolved: While the authors claim versatility, the experimental section only provides ablations and results for PAT_ch, PAT_sp, and PAT_sf.
- What evidence would resolve it: Ablation studies integrating the partial convolution framework with alternative attention mechanisms (e.g., flash attention, linear attention, or large-kernel attention) to demonstrate generalizable efficiency gains.

### Open Question 3
- Question: Is the assumption that feature maps obey an approximately normal distribution valid across all network layers, and how does deviation affect the Gaussian channel attention?
- Basis in paper: [inferred] Section 3.2 justifies the Gaussian channel attention by noting, "Considering that the feature maps obey an approximately normal distribution... we fully utilize the Gaussian statistical to express the channel-wise representation information."
- Why unresolved: The paper does not provide empirical visualization or statistical testing of feature map distributions within the trained PATNet; if deeper layers produce multi-modal or sparse features, the Gaussian approximation may lead to information loss.
- What evidence would resolve it: Statistical analysis of intermediate feature maps in trained models and comparative performance analysis against a non-parametric channel attention baseline.

## Limitations

- **Gaussian distribution assumption**: The enhanced channel attention relies on feature maps following approximately normal distributions, which lacks strong empirical validation and may not hold for all network layers or activation functions.
- **Stage allocation strategy**: The optimal placement of self-attention in Stage 4 is claimed to provide superior speed-accuracy trade-offs, but lacks rigorous ablation studies across different stage configurations and task types.
- **RPE implementation details**: The Relative Position Encoding used in PAT_sf is not fully specified in the paper, potentially affecting reproducibility and limiting understanding of how position information is integrated into the self-attention mechanism.

## Confidence

- **High**: Top-1 accuracy improvements (80.2% vs FasterNet baselines), GPU throughput measurements, and COCO detection results are well-documented and reproducible.
- **Medium**: FLOPs vs. actual throughput comparisons are sound, but the memory access optimization benefits depend on specific hardware implementation details.
- **Low**: The Gaussian distribution assumption for channel attention and the optimal stage allocation strategy lack robust theoretical or empirical justification in the literature.

## Next Checks

1. **Gaussian Assumption Validation**: Test PAT_ch performance when feature distributions deviate from normality (e.g., after ReLU activations) to verify variance-based attention provides meaningful signal beyond mean-only approaches.
2. **Stage Allocation Sensitivity**: Systematically evaluate PAT_sf placement across different stages (not just Stage 4) to quantify the claimed optimal stage-specific attention allocation.
3. **RPE Implementation Verification**: Implement PAT_sf with explicit RPE equations matching the cited [47] paper to ensure the self-attention mechanism is correctly integrated and its complexity benefits are realized.