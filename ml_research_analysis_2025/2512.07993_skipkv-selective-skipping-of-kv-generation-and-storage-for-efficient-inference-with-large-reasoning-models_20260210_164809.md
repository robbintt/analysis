---
ver: rpa2
title: 'SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference
  with Large Reasoning Models'
arxiv_id: '2512.07993'
source_url: https://arxiv.org/abs/2512.07993
tags:
- skipkv
- reasoning
- cache
- generation
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large reasoning models (LRMs) suffer from high KV cache memory
  overhead and prolonged generation due to verbose chain-of-thought (CoT) reasoning.
  SkipKV introduces a training-free KV cache compression framework that selectively
  skips redundant KV storage and generation at the sentence level.
---

# SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models

## Quick Facts
- arXiv ID: 2512.07993
- Source URL: https://arxiv.org/abs/2512.07993
- Reference count: 40
- Large reasoning models (LRMs) suffer from high KV cache memory overhead and prolonged generation due to verbose chain-of-thought (CoT) reasoning. SkipKV introduces a training-free KV cache compression framework that selectively skips redundant KV storage and generation at the sentence level.

## Executive Summary
SkipKV is a training-free KV cache compression framework designed for Large Reasoning Models (LRMs) that addresses the dual challenges of high memory overhead and prolonged generation time caused by verbose chain-of-thought (CoT) reasoning. The method introduces sentence-level redundancy scoring to identify and remove highly similar sentences while maintaining semantic coherence, combined with adaptive steering to suppress unnecessary thoughts and batch grouping to reduce padding overhead. Evaluated on DeepSeek-R1 variants across four reasoning benchmarks, SkipKV achieves up to 26.7% higher accuracy, generates 1.6× fewer tokens, and improves throughput by up to 1.7× compared to state-of-the-art methods, while reducing KV cache memory by 2×.

## Method Summary
SkipKV is a training-free KV cache compression approach that operates through three complementary mechanisms: (1) Sentence-level KV eviction using pairwise cosine similarity of hidden states to identify and remove redundant sentences while preserving semantic coherence, (2) Adaptive steering vector injection to suppress non-execution thoughts and encourage concise generation, and (3) Batch grouping by prefill length to reduce padding waste and maximize effective KV budget in multi-batch settings. The method performs compression every 128 decoding steps using a fixed KV budget, with similarity threshold τ∈[0.95,0.99] and attention-redundancy trade-off σ=0.1.

## Key Results
- Up to 26.7% higher accuracy on AIME-24 compared to state-of-the-art methods
- Generates 1.6× fewer tokens while maintaining or improving accuracy
- Improves throughput by up to 1.7× and reduces KV cache memory by 2×

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Level Redundancy Scoring
Aggregating tokens into sentence-level semantic units for eviction scoring preserves reasoning coherence better than token-level eviction. The method computes pairwise sentence similarity using last-layer hidden state averages per sentence, flagging sentences exceeding similarity threshold τ≥0.95 for eviction as complete units. This combined with token importance and redundancy scores creates a cumulative eviction score prioritizing semantically redundant sentences.

### Mechanism 2: Adaptive Steering for KV Generation Suppression
Dynamically adjusting a steering vector during inference can proactively suppress generation of non-essential thoughts. A steering vector V, computed as the difference between average hidden states of execution and non-execution thoughts, is added to hidden states at a specific decoder layer. The steering strength αt adapts based on a running count of non-execution thoughts No generated so far (αt = α0 + γ·No), increasing pressure to be concise as redundancy accumulates.

### Mechanism 3: Batch Grouping for Effective KV Budget Restoration
Grouping inference requests by prefill length minimizes intra-batch padding, thereby maximizing effective KV budget available for valid tokens in fixed-memory multi-batch settings. Instead of random batching, requests are sorted by prefill token count and grouped into batches of size bs, reducing padding token count that would otherwise consume valuable KV cache space.

## Foundational Learning

- Concept: KV Cache in Autoregressive Models
  - Why needed here: The entire paper is predicated on memory and throughput bottlenecks caused by linear growth of KV cache during long CoT reasoning. Understanding that cache stores past attention states to avoid recomputation is essential.
  - Quick check question: Explain why larger KV cache directly increases memory usage and slows down decoding phase of an LLM.

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: SkipKV targets LRMs using verbose CoT. Mechanism design (sentence-level scoring, "execution" vs. "non-execution" thoughts) is based on specific structure of CoT outputs, which differs from standard text completion.
  - Quick check question: Describe key characteristic of CoT outputs from models like DeepSeek-R1 that makes them distinct from simpler question-answering responses.

- Concept: Semantic Similarity (Cosine Similarity)
  - Why needed here: Core eviction decision relies on computing pairwise sentence similarity using cosine similarity between sentence embeddings. Grasping this metric is necessary to understand how redundancy is identified.
  - Quick check question: Given two sentence vectors, what does cosine similarity score close to 1.0 indicate?

## Architecture Onboarding

- Component map: Sentence Tracker -> Scoring Engine -> Eviction Controller -> Adaptive Steering Module -> Batch Grouper (Inference-time)

- Critical path: The decoding loop. On each step, tokens are generated, sentence tracker is updated, and steering is applied. Every ∆t steps, critical path involves: computing all scores, evicting tokens from KV cache, and updating cache-space mapping for all surviving sentences. Latency added to these eviction steps is critical.

- Design tradeoffs:
  1. Eviction Granularity (Sentence vs. Token): Sentence-level eviction preserves semantic coherence but may evict more/fewer tokens than optimal token-level policy. Token-level can cause fragmentation.
  2. Steering Strength (Fixed vs. Adaptive): Adaptive steering offers dynamic control but is more complex and relies on proxy (non-execution thought count). Fixed steering is simpler but may be too weak/aggressive for all cases.
  3. Batch Formation (Random vs. Grouped): Grouping improves effective KV budget but adds sorting and potential queueing latency. Random batching is simpler but less memory-efficient.

- Failure signatures:
  1. Accuracy collapse at low KV budgets: If eviction budget is too low or threshold τ is too aggressive, model may lose critical context.
  2. Runaway generation length: If steering strength is too low or eviction policy removes context needed for termination, model may generate endlessly.
  3. Incoherent outputs: If steering vector is too strong or applied to wrong layer, outputs can become nonsensical or hallucinated.

- First 3 experiments:
  1. Validate sentence-level eviction vs. token-level baseline (H2O, R-KV): Reproduce accuracy and token length comparison on single benchmark (e.g., MATH-500) with fixed batch size and KV budget.
  2. Ablate the adaptive steering module: Run SkipKV with and without adaptive steering component (fixed α vs. adaptive αt) to isolate its contribution to token reduction and verify it doesn't degrade accuracy.
  3. Test batch grouping impact: Measure effective KV budget and resulting accuracy with and without batch grouping in multi-batch setting (e.g., bs=10) to validate multi-batch performance claims.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Sentence-level coherence assumption may not hold for all reasoning patterns and lacks extensive empirical validation beyond accuracy metrics
- Adaptive steering effectiveness is not directly compared to fixed-strength approaches and relies on potentially oversimplified binary classification of thoughts
- Multi-batch benefits are heavily dependent on variance in prefill lengths and provide no benefit for single-stream inference

## Confidence
- High Confidence: Basic premise that KV cache compression is necessary for efficient LRM inference is well-established. Sentence-level eviction mechanism's core logic is clearly specified and reproducible.
- Medium Confidence: Claimed improvements in accuracy, token reduction, and throughput are supported by experiments on four benchmarks, though comparison to R-KV may be confounded by baseline differences.
- Low Confidence: Claim that SkipKV "maintains semantic coherence" while achieving 2× KV cache reduction is primarily supported by pass@1 accuracy metrics without qualitative analysis of reasoning quality or coherence preservation.

## Next Checks
1. **Ablation study on steering component**: Run SkipKV with three variants: (a) no steering, (b) fixed-strength steering (SEAL-style), and (c) adaptive steering. Compare accuracy, token count, and KV usage across all four benchmarks to isolate the adaptive component's contribution.

2. **Cross-architecture validation**: Apply SkipKV to a non-DeepSeek LRM (e.g., Claude-3-Sonnet or OpenAI o1-mini if accessible) on the same benchmarks. This would test the generality of the sentence-level eviction and adaptive steering mechanisms beyond the DeepSeek architecture.

3. **Quality analysis of compressed reasoning**: For a subset of MATH-500 samples, compare the full vs. SkipKV-compressed chain-of-thought outputs using: (a) human evaluation of reasoning coherence, (b) analysis of whether critical intermediate steps are preserved, and (c) detection of any hallucinated reasoning steps introduced by aggressive steering.