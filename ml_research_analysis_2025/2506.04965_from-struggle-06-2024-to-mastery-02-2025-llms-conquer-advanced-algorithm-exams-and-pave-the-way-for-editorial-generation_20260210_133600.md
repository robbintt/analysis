---
ver: rpa2
title: From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm
  Exams and Pave the Way for Editorial Generation
arxiv_id: '2506.04965'
source_url: https://arxiv.org/abs/2506.04965
tags:
- llms
- exam
- graph
- advanced
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) now achieve performance comparable
  to top students on advanced university algorithm exams. Testing 18 state-of-the-art
  models on both Romanian and English versions of a complex 11-problem algorithms
  exam revealed that recent models (released January-February 2025) consistently score
  in the top 5-15%, while older models fall below 40%.
---

# From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation

## Quick Facts
- **arXiv ID:** 2506.04965
- **Source URL:** https://arxiv.org/abs/2506.04965
- **Reference count:** 23
- **Primary result:** Recent LLMs (Jan-Feb 2025) achieve top 5-15% scores on advanced algorithms exams, while older models fall below 40%

## Executive Summary
This study demonstrates a remarkable evolution in large language models' ability to tackle advanced university-level algorithm exams. Testing 18 state-of-the-art models on an 11-problem algorithms exam revealed that recent models released in January-February 2025 consistently achieve performance comparable to top students, while older models struggle significantly. The research identifies specific weaknesses in visual graph analysis tasks while highlighting strengths in theoretical proofs and string algorithms. The findings suggest LLMs are now ready for advanced educational applications, including automated grading and editorial generation.

## Method Summary
The study evaluated 18 LLMs on an 11-problem university algorithms exam covering graphs, BFS, Eulerian paths, critical nodes, Floyd-Warshall, Prim's, Ford-Fulkerson/max-flow, Hamiltonian graphs, LCS, and optimal pathfinding. Models were tested using one-shot evaluation (entire exam in single interaction) with default settings and neutral prompts. The exam dataset, available in both Romanian and English versions, was graded by a course instructor. Consistency was measured through 5 independent runs per model, and collaborative solving strategies (RunAvg, RunAvgAll) were tested to explore ensemble approaches.

## Key Results
- o3-mini achieved 86.6/100 (98th percentile), outperforming all tested models
- GPT-4 Legacy scored only 29.0/100 (4th percentile), highlighting temporal performance improvements
- Recent models (Jan-Feb 2025) consistently score in top 5-15%, while older models fall below 40%
- Visual graph analysis tasks remain the primary weakness, with LLMs struggling to interpret graph structures
- Newer models show minimal performance difference between Romanian and English versions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporal improvements in LLM reasoning and context handling correlate directly with performance thresholds required for advanced STEM exams.
- **Mechanism:** Recent model releases (Jan–Feb 2025) utilize architectures better suited for multi-step algorithmic challenges and maintaining coherence over long contexts (11-problem exams), whereas older models (>6 months) fail to sustain logic across the full exam session.
- **Core assumption:** The performance jump is primarily driven by architectural advances in reasoning and context window management rather than mere data memorization.
- **Evidence anchors:** The older models struggle not only with intricate algorithmic problems but also with processing the extensive context of the exam. Recent models demonstrate robust reasoning skills on complex, multi-step algorithmic challenges.
- **Break condition:** If exam format shifts to adversarial or out-of-distribution problems not represented in recent training sets, this temporal correlation may invert.

### Mechanism 2
- **Claim:** Solution diversity through cross-model collaboration ("RunAvgAll") elevates lower-performing models closer to the performance frontier.
- **Mechanism:** Weaker models (e.g., Sonnet 3.5, DeepSeekR1) improve their scores when exposed to higher-quality solutions from superior models (e.g., o3-mini), allowing them to correct local errors via in-context learning.
- **Core assumption:** The improvement stems from the "teacher" model providing valid reasoning traces that the "student" model can verify and incorporate.
- **Evidence anchors:** RunAvgAll strategy resulted in a substantial increase for Gemini 2.0 Flash, Sonnet 3.5, and DeepSeekR1, benefiting from exposure to the better solutions of o3-mini. Smart Cubing suggests parallel solving strategies are effective in search spaces, analogous to this LLM ensemble.
- **Break condition:** If the "teacher" model provides a plausible but hallucinated solution, this mechanism may propagate errors rather than correct them.

### Mechanism 3
- **Claim:** Consistency in output (low Standard Deviation) acts as a reliability filter for educational deployment, distinct from raw capability.
- **Mechanism:** Models like o3-mini stabilize their reasoning paths across multiple runs, reducing the "black swan" volatility seen in models like Sonnet 3.5, making them viable for grading/tutoring.
- **Core assumption:** Low variance implies a stable internal representation of the algorithmic concepts.
- **Evidence anchors:** o3-mini and Gemini 2.0 Flash exhibit significantly higher consistency compared to Sonnet 3.5, which shows substantial variability. Consistency is measured by the standard deviation (SD), with a lower SD indicating higher consistency.
- **Break condition:** High consistency does not guarantee correctness; a model can be consistently wrong.

## Foundational Learning

- **Concept:** One-Shot Long-Context Processing
  - **Why needed here:** The study evaluated LLMs on an 11-problem exam in a single interaction, rather than problem-by-problem, to test context retention.
  - **Quick check question:** How does the "one-shot" evaluation method differ from standard benchmarking, and why does it challenge older models?

- **Concept:** Bipartite & Eulerian Graph Theory
  - **Why needed here:** These specific visual/structural problems (identifying maximal bipartite subgraphs or Eulerian paths) were the primary failure points for LLMs in the study.
  - **Quick check question:** Why do LLMs struggle to determine valid bipartitions or Eulerian paths when the graph is presented visually or as an adjacency list?

- **Concept:** Percentile Ranking vs. Absolute Scoring
  - **Why needed here:** Performance is contextualized against human students (e.g., "98nd percentile"), which provides a more meaningful metric than raw accuracy in educational settings.
  - **Quick check question:** What does it imply when an LLM scores in the 4th percentile (GPT-4 Legacy) versus the 98th percentile (o3-mini) regarding its utility as a teaching assistant?

## Architecture Onboarding

- **Component map:** User Input -> Backend (FastAPI) -> Prompt Augmentation (2000 chars) -> LLM API (Gemini/Mistral) -> Frontend (React + BlueprintJS)
- **Critical path:** User inputs problem statement + (optional) current ideas/solution sketches → Backend retrieves auth token from local storage (security check) → System augments user input with the 2000-character prompt engineering layer → Request is routed to selected LLM API (Gemini/Mistral) → LLM returns editorial/solution → Frontend renders result
- **Design tradeoffs:**
  - **Cost vs. Capability:** The platform restricts usage to Gemini 2.0 Flash and Mistral Large (skipping o3-mini) likely due to API pricing constraints, despite o3-mini having higher performance
  - **Security vs. Accessibility:** Uses a pre-shared "auth token" in browser storage rather than a robust authentication system, limiting public deployment
- **Failure signatures:**
  - **Visual Graph Tasks:** LLMs consistently fail to interpret graph structures from text descriptions or images (Problems 1 & 3), leading to invalid algorithmic steps
  - **Token Stalls:** "Thinking" models (e.g., DeepSeekR1) may stall if max_output_tokens is insufficient for the reasoning trace
- **First 3 experiments:**
  1. **Baseline consistency check:** Run the same 11-problem exam 5 times on a single model (e.g., Gemini Flash) to calculate the Standard Deviation of scores
  2. **RunAvgAll Ensemble:** Feed the output of a high-performing model (o3-mini) into a lower-performing model (Sonnet 3.5) to measure the delta in grade improvement
  3. **Visual vs. Text ablation:** Present the graph problems (Problems 1-6) as pure adjacency matrices vs. image descriptions to isolate the visual processing bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multimodal LLM capabilities achieve seamless interpretation of visual and textual data in graph-based algorithmic tasks, and what architectural improvements are required?
- **Basis in paper:** Future Work section states: "Future research should address these limitations, exploring multimodal capabilities for seamless interpretation of visual and textual data to unlock LLMs' full potential for interactive and effective learning experiences."
- **Why unresolved:** Current LLMs fail on visual graph tasks (Problems 1, 3, 8) where students succeed, indicating fundamental limitations in processing graphical information without explicit textual representation
- **What evidence would resolve it:** Comparative evaluation of multimodal LLMs on visual graph analysis tasks showing performance parity with human students, along with architectural analysis of successful approaches

### Open Question 2
- **Question:** What is the causal impact of LLM-generated editorial feedback on student learning outcomes in advanced algorithms courses?
- **Basis in paper:** Future Work states: "conducting controlled studies to measure the impact of such editorial feedback on student learning outcomes in advanced algorithms courses will be essential for validating these techniques."
- **Why unresolved:** The paper demonstrates editorial generation capability but lacks empirical validation of educational effectiveness
- **What evidence would resolve it:** Randomized controlled trial comparing learning gains between students receiving LLM-generated editorials versus traditional feedback, measured through exam performance improvement

### Open Question 3
- **Question:** Can collaborative LLM ensembles systematically outperform individual models on algorithmic tasks where single models consistently struggle?
- **Basis in paper:** Section 6 states: "Another promising research direction emerging from this work is the use of collaborative LLMs to tackle tasks where individual models consistently struggle, as discussed in subsection 3.4."
- **Why unresolved:** RunAvgAll experiments showed mixed results—some models improved while o3-mini decreased; systematic ensemble strategies remain unexplored
- **What evidence would resolve it:** Evaluation of diverse ensemble methods across multiple exam iterations, identifying which collaboration strategies yield consistent improvements for all participant models

## Limitations

- **Visual Graph Processing:** LLMs consistently fail to interpret graph structures from text descriptions or images, representing a fundamental limitation for algorithmic problem-solving
- **Single-Instructor Grading:** The study relies on one instructor's evaluation for grading, introducing potential subjectivity in scoring complex algorithmic solutions
- **Domain Specificity:** The exam's exclusive focus on graph theory and string algorithms limits conclusions about LLM capabilities in other algorithmic domains

## Confidence

**High Confidence:** The temporal correlation between model release dates and performance, the consistency findings, and the language capability improvements are well-supported by the data with clear statistical backing.

**Medium Confidence:** The mechanism explaining why recent models excel (architectural improvements in reasoning and context handling) is plausible but not definitively proven, as the study doesn't directly measure these internal capabilities.

**Low Confidence:** The assertion that LLMs are "ready for educational applications" including automated grading is premature given the significant struggles with visual graph analysis and the high variability seen in some models.

## Next Checks

1. **Prompt Sensitivity Analysis:** Systematically vary the prompt wording (neutral, encouraging, step-by-step) across 3-5 different formulations and measure performance variance to quantify the impact of prompt engineering on the reported scores.

2. **Multi-Instructor Grading Validation:** Have 3-5 different domain experts independently grade the same set of LLM responses to calculate inter-rater reliability scores and establish confidence intervals around the reported percentile rankings.

3. **Cross-Domain Algorithmic Testing:** Evaluate the same model cohort on a complementary exam focused on dynamic programming, computational geometry, or number theory problems to determine whether the observed weaknesses are specific to graph theory or indicative of broader algorithmic reasoning limitations.