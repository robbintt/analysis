---
ver: rpa2
title: 'Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and
  Interpretability through Attention Maps'
arxiv_id: '2502.15120'
source_url: https://arxiv.org/abs/2502.15120
tags:
- attention
- reasoning
- language
- dataset
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a critical parameter threshold (~1.6 billion)
  beyond which reasoning performance in decoder-only transformer-based language models
  significantly improves for tasks such as commonsense reasoning and deductive reasoning.
  Specifically, models above this threshold achieve better success rates in chain-of-thought
  (CoT) prompting for deductive reasoning tasks, particularly those requiring longer
  reasoning chains.
---

# Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps

## Quick Facts
- arXiv ID: 2502.15120
- Source URL: https://arxiv.org/abs/2502.15120
- Authors: Yen-Che Hsiao; Abhishek Dutta
- Reference count: 40
- Primary result: Identified ~1.6B parameter threshold for reasoning emergence in decoder-only transformers

## Executive Summary
This study identifies a critical parameter threshold (~1.6 billion) beyond which reasoning performance in decoder-only transformer-based language models significantly improves for tasks such as commonsense reasoning and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, particularly those requiring longer reasoning chains. To address limitations in sub-threshold models, fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without exemplars in the prompt for tasks with shorter reasoning chains. Analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes.

## Method Summary
The study evaluates decoder-only transformers across six model families (GPT-2, SmolLM2, OpenELM, TinyLlama, Stable LM 2, Gemma 2) with parameter counts ranging from 117M to 9B. Chain-of-thought prompting is used for commonsense reasoning (CSQA) and deductive reasoning (PrOntoQA-OOD) tasks. Sub-threshold models (<1.6B parameters) are fine-tuned on 1,800 exemplars using Hugging Face Trainer API with Adam optimizer (lr=2e-5) for 100 epochs. Attention maps are extracted using float16 precision and normalized per Kang & Shin (2023) for interpretability analysis. Performance is measured via string parsing accuracy for extracted answers.

## Key Results
- A critical parameter threshold (~1.6B) marks where reasoning performance significantly improves for commonsense and deductive reasoning tasks
- Fine-tuning sub-threshold models on deductive reasoning exemplars enables accurate CoT generation without exemplars in the prompt for simpler inference rules
- Models generating correct CoTs exhibit higher attention scores on subsequent correct tokens and grammatically appropriate constituents

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Reasoning Emergence
- Claim: Decoder-only transformers exhibit a critical parameter threshold (~1.6B) beyond which chain-of-thought reasoning capability emerges more reliably.
- Mechanism: Increased parameter count appears to enable more sophisticated attention allocation patterns that support multi-step logical inference. Larger models demonstrate capacity to attend to semantically relevant tokens across longer reasoning chains.
- Core assumption: The threshold reflects underlying computational capacity for compositional reasoning, not merely memorization of training patterns.
- Evidence anchors:
  - [abstract] "We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning."
  - [section 3.1] "A noticeable accuracy increase is observed between gpt2-xl (1.542B parameters) with an accuracy of 19.57% and stablelm-2-1.6B with 43.00%"
  - [corpus] Weak external validation—related work on CoT reasoning focuses on prompting strategies rather than scale thresholds specifically at 1.6B.
- Break condition: OpenELM models show inconsistent performance relative to threshold, suggesting architecture and training data quality also mediate the effect.

### Mechanism 2: Task-Specific Fine-Tuning for Shorter Reasoning Chains
- Claim: Fine-tuning sub-threshold models on deductive reasoning exemplars substantially improves performance on simpler inference rules, even without in-context exemplars at inference time.
- Mechanism: Supervised fine-tuning on structured logical patterns internalizes reasoning schemas that sub-threshold models cannot reliably acquire through in-context learning alone.
- Core assumption: The improvement reflects genuine reasoning capability rather than surface-level pattern matching with the training distribution.
- Evidence anchors:
  - [abstract] "Fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains."
  - [section 3.3] Fine-tuned models "exhibit substantial improvements in implication elimination, conjunction introduction, conjunction elimination, and disjunction introduction, all achieving accuracy above 90%"
  - [corpus] Related work on CoT distillation (e.g., "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning") supports the viability of transferring reasoning patterns through supervised training, though specific threshold interactions remain underexplored.
- Break condition: Longer reasoning chains (proof by contradiction, disjunction elimination) show inconsistent improvement—some models even degrade, suggesting capacity limits remain.

### Mechanism 3: Attention Pattern Alignment with Correct Token Sequences
- Claim: Models capable of correct CoT generation allocate higher attention scores to subsequent correct tokens and grammatically appropriate constituents compared to failing models.
- Mechanism: Successful reasoning correlates with attention patterns that prioritize semantically and syntactically relevant context over surface-level distractors.
- Core assumption: Attention patterns are causally related to reasoning success rather than merely correlational epiphenomena.
- Evidence anchors:
  - [abstract] "Models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech"
  - [section 3.4] "Gemma2-9B-IT model correctly generates the gold CoT... assigns higher attention scores to tokens such as 'loud,' 'liquid,' and most importantly, 'transparent'" whereas "gpt2... assigns high attention to irrelevant tokens such as 'Every' and 'is'"
  - [corpus] Limited external validation—related interpretability work focuses on causal mechanisms in CoT rather than attention score distributions specifically.
- Break condition: Causality not established; attention differences may reflect rather than cause reasoning success.

## Foundational Learning

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: The entire experimental framework depends on CoT prompting methodology for eliciting reasoning.
  - Quick check question: Can you explain how CoT prompting differs from standard few-shot prompting in terms of input structure and expected model behavior?

- Concept: **Decoder-Only Transformer Architecture**
  - Why needed here: The threshold findings are specific to decoder-only architectures; different architectural families may exhibit different scaling properties.
  - Quick check question: What distinguishes decoder-only from encoder-decoder attention patterns, and why might this matter for autoregressive reasoning?

- Concept: **Attention Map Interpretation**
  - Why needed here: The interpretability claims require understanding how to extract and normalize attention weights for analysis.
  - Quick check question: Given an attention matrix A ∈ R^(n×n), how would you compute the normalized token-level scores described in the paper?

## Architecture Onboarding

- Component map:
  - Model families tested: GPT-2 (117M-1.5B), SmolLM2 (135M-1.7B), OpenELM (270M-3B), TinyLlama (1.1B), Stable LM 2 (1.6B), Gemma 2 (2B, 9B)
  - Key architectural variations: GQA, RoPE, SwiGLU, RMSNorm vs LayerNorm, context lengths (1024-8192)
  - Attention extraction: Load models in float16 for weight extraction; apply Kang & Shin (2023) normalization for visualization

- Critical path:
  1. Model selection: Prioritize models ≥1.6B parameters for reasoning tasks; sub-threshold models require fine-tuning consideration
  2. Context length planning: GPT-2's 1024-token limit constrains CoT exemplar count; allocate accordingly
  3. Fine-tuning setup: Target models ≤355M parameters for single-GPU feasibility; 100 epochs with validation-loss-based early stopping

- Design tradeoffs:
  - Scale vs. efficiency: Larger models achieve better reasoning but incur higher inference costs
  - Fine-tuning vs. ICL: Fine-tuning improves sub-threshold models but requires task-specific data and may not generalize to longer reasoning chains
  - Attention precision: float16 enables attention extraction but differs from inference precision (bfloat16 for Gemma)

- Failure signatures:
  - Sub-threshold models: High unparseable response rates (>200 on CSQA), inability to complete longer proofs (proof by contradiction consistently at 0%)
  - Attention misalignment: Failing models show diffuse attention to function words rather than content tokens
  - Fine-tuning instability: Some models (SmolLM2-135M) show degraded performance post-fine-tuning on complex rules

- First 3 experiments:
  1. **Threshold validation**: Replicate CSQA accuracy curve across model scales within a single architecture family (e.g., GPT-2 variants) to confirm ~1.6B transition point
  2. **Fine-tuning transfer test**: Fine-tune a sub-threshold model on PrOntoQA-OOD, then evaluate on held-out deductive rules to assess generalization vs. memorization
  3. **Attention perturbation**: Systematically mask high-attention tokens in successful model outputs to test whether attention alignment is causal for reasoning success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training data characteristics cause the OpenELM family to deviate from the observed ~1.6 billion parameter reasoning threshold?
- Basis in paper: [explicit] The authors note that a significant performance jump occurs beyond 1.6 billion parameters "except for models from the OpenELM family," suggesting they do not follow the scaling trends of GPT-2, Gemma, or SmolLM.
- Why unresolved: While the paper identifies OpenELM as an outlier, it does not isolate the cause, leaving open whether the anomaly stems from OpenELM's layer-wise non-uniform scaling, training dataset composition, or efficient inference design.
- What evidence would resolve it: A comparative ablation study isolating the effects of architecture (uneven layer widths) versus training data size on OpenELM's reasoning performance relative to standard transformers.

### Open Question 2
- Question: Can sub-threshold models be trained to perform complex reasoning tasks (e.g., proof by contradiction) that currently fail under standard fine-tuning?
- Basis in paper: [inferred] The authors demonstrate that fine-tuning sub-threshold models succeeds on simpler deductive rules but explicitly state that tasks requiring longer reasoning chains "remain challenging for fine-tuned models with limited parameters."
- Why unresolved: It is unclear if the failure on complex tasks is a fundamental capacity limit of small models or a limitation of the specific supervised fine-tuning approach used in the study.
- What evidence would resolve it: Experiments applying reinforcement learning or curriculum learning to sub-threshold models to test if alternative training objectives can unlock complex reasoning capabilities.

### Open Question 3
- Question: Is the higher attention on subsequent correct tokens the cause of successful reasoning, or is it merely a byproduct of model confidence?
- Basis in paper: [inferred] The paper observes that models capable of correct Chain-of-Thought reasoning "exhibit higher token-level attention scores on subsequent correct tokens," proposing that attention allocation plays a crucial role.
- Why unresolved: The paper establishes a correlation between attention patterns and reasoning success but does not perform causal intervention to determine if modifying attention mechanisms in smaller models improves their reasoning.
- What evidence would resolve it: Causal tracing or attention patching experiments that force sub-threshold models to attend to "correct" tokens to see if it artificially induces reasoning capabilities.

## Limitations

- Threshold specificity: The 1.6B parameter threshold appears architecture-dependent and may not generalize across different model families
- Causality in attention patterns: Correlation between attention scores and reasoning success does not establish causal direction
- Fine-tuning generalization: Improvements on simpler reasoning chains may reflect pattern matching rather than genuine logical reasoning transfer

## Confidence

**High Confidence**:
- Scale-dependent accuracy improvements exist in CSQA tasks, with clear performance jumps around 1.6B parameters for certain model families
- Fine-tuning consistently improves sub-threshold models on simple deductive rules
- Attention patterns differ systematically between successful and failing models

**Medium Confidence**:
- The 1.6B threshold represents a generalizable critical point for reasoning emergence across all decoder-only architectures
- Fine-tuning enables zero-shot reasoning capability for shorter chains without exemplars
- Attention score distributions causally influence reasoning success

**Low Confidence**:
- The threshold applies uniformly across different architectural choices (GQA, RoPE, SwiGLU variations)
- Fine-tuning generalizes to unseen reasoning patterns beyond the training distribution
- Attention differences represent learned reasoning strategies rather than training artifacts

## Next Checks

1. **Architecture Transfer Test**: Evaluate the same parameter range (1.1B-1.7B) across multiple distinct architectures (GPT-2, Llama, Gemma variants) to determine whether the 1.6B threshold is architecture-specific or represents a more general scaling phenomenon.

2. **Attention Intervention Experiment**: Systematically perturb attention weights in successful models during inference by masking high-attention tokens, then measure degradation in reasoning accuracy to establish whether attention alignment is necessary for correct outputs.

3. **Fine-tuning Transfer Evaluation**: Design fine-tuning datasets that deliberately exclude certain deductive rules, then test whether models can generalize to these held-out rules without exemplars, distinguishing genuine reasoning transfer from memorization.