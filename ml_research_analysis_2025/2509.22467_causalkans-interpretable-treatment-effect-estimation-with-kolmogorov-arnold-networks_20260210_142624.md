---
ver: rpa2
title: 'CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold
  networks'
arxiv_id: '2509.22467'
source_url: https://arxiv.org/abs/2509.22467
tags:
- page
- cited
- treatment
- cate
- causalkans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CausalKANs transform neural estimators of heterogeneous treatment
  effects into interpretable Kolmogorov-Arnold Networks, enabling post-training pruning
  and symbolic substitution to produce closed-form, auditable CATE formulas while
  preserving predictive accuracy. Experiments on benchmark datasets (IHDP, ACIC, NSLM,
  NEWS, TCGA) show causalKANs match or surpass neural baselines on PEHE/ATE, with
  even simple KAN variants achieving competitive performance.
---

# CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks

## Quick Facts
- arXiv ID: 2509.22467
- Source URL: https://arxiv.org/abs/2509.22467
- Reference count: 0
- CausalKANs achieve PEHE/ATE matching or exceeding neural baselines while producing interpretable closed-form CATE formulas.

## Executive Summary
CausalKANs transform neural estimators of heterogeneous treatment effects into interpretable Kolmogorov-Arnold Networks by replacing MLP backbones with spline-based KAN layers, then applying pruning and symbolic substitution to produce closed-form, auditable CATE formulas. Experiments on benchmark datasets (IHDP, ACIC, NSLM, NEWS, TCGA) show causalKANs match or surpass neural baselines on PEHE/ATE while enabling controllable accuracy-interpretability trade-offs. The approach offers domain-agnostic, transparent estimators suitable for high-stakes settings where interpretability is essential, though training is more computationally demanding than standard NNs.

## Method Summary
CausalKANs replace MLP backbones in standard causal neural networks (S-Learner, T-Learner, TARNet, DragonNet) with KAN blocks that parameterize one-dimensional edge functions using B-splines. The training pipeline combines factual MSE loss with edge activity regularization (L1 penalties on edge outputs) and spline coefficient penalties, optimized via Adam with early stopping. Post-training, edges below a threshold are pruned to expose sparse structure, then auto-symbolic substitution fits simple analytic atoms (polynomials, trigonometric functions) to remaining splines if the fit error and validation loss budget permit. This yields executable expressions for CATE while preserving predictive accuracy.

## Key Results
- CausalKANs match or exceed neural baselines (MLPs, TARNet, DragonNet) on PEHE/ATE across IHDP, ACIC, NSLM, NEWS, and TCGA datasets
- Symbolic substitution successfully recovers interpretable formulas, with T-KAAM variants capturing ground truth better than raw spline models
- Simple KAN variants (S-KAAM, shallow T-KAAN) achieve competitive performance while producing accessible visualizations like radar plots
- Even with pruning, CausalKANs maintain predictive accuracy while dramatically reducing model complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing MLP backbones with KANs exposes compositional structure by shifting learnable parameters from fixed node activations to flexible edge functions.
- **Mechanism:** KANs parameterize one-dimensional splines on edges rather than weight matrices with fixed nonlinearities, composing univariate splines via summation to approximate complex response surfaces while retaining inspectability of each input's transformation path.
- **Core assumption:** True CATE function is smooth and approximately decomposable into low-dimensional additive or multiplicative interactions (sparsity).
- **Evidence anchors:** [abstract] mentions interpretable closed-form formulas; [section 3.2] describes KAN parameterization with learnable univariate functions on edges.
- **Break condition:** If true data generating process is highly non-smooth or contains discontinuous jumps, KAN may require excessive parameters, losing interpretability advantage.

### Mechanism 2
- **Claim:** Pruning low-activity edges isolates relevant subset of effect modifiers, reducing noise and preparing network for symbolic interpretation.
- **Mechanism:** Pipeline employs L1 regularization on edge activity during training, then post-training removes edges with importance scores below threshold, effectively performing feature selection and sparsification within network topology.
- **Core assumption:** Irrelevant covariates contribute minimally to potential outcomes, and their removal doesn't significantly bias estimated CATE.
- **Evidence anchors:** [abstract] mentions pruning yields interpretable formulas; [section 4.1] describes edge removal with threshold.
- **Break condition:** If treatment effect depends on weak but numerous interactions, aggressive pruning may drop critical weak signals, degrading PEHE.

### Mechanism 3
- **Claim:** Symbolic substitution maps learned splines to closed-form algebraic expressions, transforming model from numerical approximator to auditable formula.
- **Mechanism:** After pruning, grid search fits simple analytic atoms to remaining spline curves via regression; if fit error exceeds threshold and validation loss remains within budget, spline is replaced by analytic function.
- **Core assumption:** Learned spline shapes approximate simple mathematical primitives found in symbol library rather than complex, non-symbolizable noise.
- **Evidence anchors:** [abstract] mentions auto-symbolic substitution produces executable expressions; [section 5.3] shows symbolic T-KAAM captures true function better.
- **Break condition:** If validation loss budget is too tight or data is highly stochastic, system reverts to raw spline representation, failing to produce closed-form formula.

## Foundational Learning

- **Concept: Neyman-Rubin Potential Outcomes**
  - **Why needed here:** CATE is defined as τ(x) = E[Y(1)|x] - E[Y(0)|x]; understanding only one potential outcome is observed per unit is necessary to grasp why architectures use shared representations to infer counterfactuals.
  - **Quick check question:** Why does CATE require predicting two potential outcomes μ₁(x) and μ₀(x) rather than predicting difference directly?

- **Concept: B-Splines**
  - **Why needed here:** CausalKANs rely on B-splines to parameterize learnable functions on network edges; understanding trade-off between grid size and smoothness is critical for debugging overfitting.
  - **Quick check question:** In KAN edge, what happens to function expressiveness if spline grid size is set too low?

- **Concept: Regularization for Sparsity**
  - **Why needed here:** Interpretability depends entirely on network being sparse enough to inspect; grasping how L1 penalties force model to ignore irrelevant inputs is key to running pipeline.
  - **Quick check question:** How does edge activity penalty (L1) directly facilitate pruning step later in pipeline?

## Architecture Onboarding

- **Component map:** Input X -> KAN Blocks (Spline Layers) -> Heads (μ₀(x), μ₁(x), e(x)) -> Post-Processor (Pruning Module -> Symbolic Solver) -> Output: Closed-form formula for τ̂(x)

- **Critical path:**
  1. Architecture Swap: Replace MLP layers in standard causal neural network with KAN layers (Spline-based)
  2. Train & Regularize: Optimize factual loss + Edge Entropy/L1 penalties to encourage sparse connectivity
  3. Simplify: Apply pruning (remove low-magnitude edges) -> Symbolic substitution (fit polynomials/trig functions)
  4. Extract: Compose symbolic edge functions to write final CATE equation

- **Design tradeoffs:**
  - Grid Size vs. Speed: Higher spline grid size increases resolution but drastically increases training time (8x slower than MLP)
  - Interpretability Budget (Λsymb): Tight budget preserves accuracy but may prevent finding clean symbolic formula; loose budget yields cleaner math but lower PEHE
  - Depth vs. Accessibility: Shallow KANs offer easy radar plots; deep KANs offer better accuracy but complex, hard-to-visualize formulas

- **Failure signatures:**
  - Performance Gap: CausalKAN PEHE significantly exceeds MLP baselines (e.g., TCGA dataset); Mitigation: Increase grid size or revert to MLP
  - Spiky Splines: Learned edge functions exhibit high-frequency oscillations (overfitting); Mitigation: Increase spline coefficient regularization (λc)
  - Symbolic Fallback: Pipeline returns "Symbolic fit failed" for most edges; Mitigation: Relax ΓR2 threshold or expand atom dictionary

- **First 3 experiments:**
  1. Sanity Check (IHDP): Run T-KAN on linear IHDP setting A; verify symbolic recovery yields linear expression matching ground truth
  2. Pruning Sensitivity: On non-linear IHDP setting B, vary pruning threshold Γ; plot "Number of Active Edges" vs. "PEHE Error" to find optimal sparsity knee
  3. Architecture Comparison: Compare shallow S-KAAM against deep T-KAN on ACIC data; check if added deep model complexity justifies training cost with improved PEHE

## Open Questions the Paper Calls Out

- **Open Question 1:** How do causalKANs compare against post-hoc explainability methods (e.g., SHAP, LIME) when evaluated using task-grounded user studies?
  - Basis in paper: Authors state future work should assess interpretability with "task-grounded and user-study metrics... and contrasted with post-hoc explanations such as LIME and SHAP."
  - Why unresolved: Current evaluation relies on semi-synthetic metrics rather than human-subject experiments comparing utility of intrinsic symbolic formulas versus extrinsic post-hoc explanations.
  - What evidence would resolve it: Results from human-subject experiments measuring decision-making speed, accuracy, and trust when users are provided causalKAN formulas versus SHAP/LIME explanations for same black-box models.

- **Open Question 2:** Can causalKAN framework be effectively adapted to high-dimensional, non-tabular data such as images, graphs, or genomic sequences?
  - Basis in paper: Authors identify adapting causalKANs to non-tabular modalities as "open challenge" and leave "multimodal CATE benchmarking" to future work.
  - Why unresolved: Current implementation restricted to tabular covariates; unclear if symbolic simplification pipeline scales effectively to high-dimensional unstructured data.
  - What evidence would resolve it: Successful application of convolutional or graph-based KAN variants on datasets like MNIST or genomic benchmarks, yielding closed-form CATE expressions without performance degradation.

- **Open Question 3:** Do causalKANs maintain accuracy and interpretability on real-world interventional data?
  - Basis in paper: Authors acknowledge evaluation relies mainly on semi-synthetic datasets and state "further assessment on field data is needed."
  - Why unresolved: "Ground truth" for CATE unknown in real-world observational data, making it difficult to verify if pruned/symbolic formulas remain faithful to true causal mechanism outside synthetic setups.
  - What evidence would resolve it: Validation on datasets with experimental benchmarks (e.g., randomized controlled trials) where estimated CATEs from causalKANs can be compared against known experimental effects.

## Limitations
- Interpretability gains come at substantial computational cost (8x slower training vs. MLPs), making large-scale applications impractical without hardware acceleration.
- Symbolic substitution success heavily depends on hyperparameter tuning; reported R2 ≥ 0.98 threshold may be too stringent for noisy real-world data.
- Pruning mechanism assumes sparsity in effect modifiers, which may not hold for complex biological or social systems with distributed effects.

## Confidence
- **High Confidence:** Core architectural claims (KAN replacing MLP backbones) well-supported by ablation studies showing comparable PEHE/ATE to baselines.
- **Medium Confidence:** Pruning and symbolic substitution pipeline works as described on benchmark datasets, but generalization to highly non-linear or discontinuous treatment effects remains untested.
- **Low Confidence:** Claim that CausalKANs are "domain-agnostic" for high-stakes settings is aspirational; paper only validates on semi-synthetic datasets without clinical or policy deployment evidence.

## Next Checks
1. **Real-World Deployment:** Test CausalKAN on healthcare dataset with known treatment effect modifiers (e.g., clinical trial data) to verify symbolic formulas align with medical expertise.
2. **Adversarial Stress Test:** Introduce synthetic discontinuities or high-frequency interactions into IHDP-like data to measure when spline-based KANs fail vs. adaptive activation functions.
3. **Efficiency Benchmarking:** Profile training and inference time on GPUs vs. CPUs across all benchmark datasets to quantify practical feasibility of CausalKANs at scale.