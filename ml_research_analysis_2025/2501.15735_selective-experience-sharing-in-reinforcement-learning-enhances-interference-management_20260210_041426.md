---
ver: rpa2
title: Selective Experience Sharing in Reinforcement Learning Enhances Interference
  Management
arxiv_id: '2501.15735'
source_url: https://arxiv.org/abs/2501.15735
tags:
- experiences
- interference
- agents
- agent
- share
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses inter-cell interference in multi-cell cellular
  networks using multi-agent reinforcement learning. The proposed SMART approach selectively
  shares experiences between base station agents based on measured inter-cell interference
  power, enabling fully decentralized training while minimizing communication overhead.
---

# Selective Experience Sharing in Reinforcement Learning Enhances Interference Management

## Quick Facts
- arXiv ID: 2501.15735
- Source URL: https://arxiv.org/abs/2501.15735
- Reference count: 20
- Primary result: Achieves 98% of sum-rate performance of full experience sharing while reducing communication overhead by 75% in multi-cell cellular networks

## Executive Summary
This paper introduces SMART, a selective experience sharing mechanism for multi-agent reinforcement learning in multi-cell cellular networks. The approach addresses inter-cell interference by enabling base station agents to selectively share experiences based on measured interference power, achieving near-optimal performance while dramatically reducing communication overhead. Each agent uses a DQN-based algorithm to jointly optimize beamforming and power control for spectral efficiency maximization. The method demonstrates that intelligent filtering of experience sharing can maintain high performance while significantly reducing network communication requirements.

## Method Summary
The SMART algorithm employs DQN agents at each base station to jointly optimize beamforming and power control through binary action encoding. Agents share experiences selectively when inter-cell interference exceeds a threshold of -110 dBm, reducing communication overhead by 75% while maintaining 98% of full-sharing performance. The approach uses local replay buffers where shared experiences are inserted for training, allowing agents to learn coordination implicitly through filtered information exchange rather than explicit signaling.

## Key Results
- Achieves 98% of sum-rate performance compared to algorithms sharing all experiences
- Reduces experience sharing by 75% (only 25% of experiences shared)
- 30% of users achieve SINR > 20 dB
- Outperforms existing multi-agent RL baselines (Share Nothing, CRDU)
- Matches state-of-the-art approaches (CTDE, Share All) with significantly less communication

## Why This Works (Mechanism)

### Mechanism 1
Selective experience sharing based on measured inter-cell interference power achieves comparable learning to full sharing while reducing communication overhead by ~75%. Each BS agent computes inter-cell interference power and only shares experiences when this exceeds a threshold, filtering out low-value experiences where interference is negligible relative to noise floor. The core assumption is that interference events exceeding the noise floor contain the signal information necessary for learning coordination.

### Mechanism 2
Joint beamforming and power control can be learned concurrently through a compact binary action representation without explicit coordination signaling. Actions are encoded as binary vectors where bit pairs control each UE for power adjustment and beamforming codebook index stepping. The DQN learns to map local state to these joint actions that maximize reward, assuming the state representation captures sufficient information about interference coupling between cells.

### Mechanism 3
Shared experiences inserted into local replay buffers enable agents to approximate the value functions they would learn with full observability. When an agent receives shared experience from another, it inserts this into its local replay buffer, allowing gradient updates that account for inter-cell effects during training. The assumption is that the distribution of shared experiences is representative enough of inter-cell interference dynamics to provide useful gradient signals.

## Foundational Learning

- **Concept:** Deep Q-Network (DQN) with experience replay
  - **Why needed here:** SMART uses DQN as the base learner; understanding replay buffers, target networks, and temporal-difference learning is essential to modify the architecture.
  - **Quick check question:** Can you explain why DQN uses a separate target network and why experiences are stored in a buffer rather than used immediately?

- **Concept:** Multi-agent RL paradigms (CTDE vs. decentralized training)
  - **Why needed here:** The paper positions SMART against CTDE and CRDU baselines; you must understand what each paradigm assumes about communication and centralization.
  - **Quick check question:** What is the difference between centralized training with decentralized execution (CTDE) and fully decentralized training in terms of what information must be communicated?

- **Concept:** Cellular interference modeling (SINR, inter-cell vs. intra-cell)
  - **Why needed here:** The selection criterion for experience sharing is derived from SINR measurements; without this, you cannot reason about threshold tuning.
  - **Quick check question:** Given the SINR formula in Equation (2), how would you isolate inter-cell interference from total measured interference?

## Architecture Onboarding

- **Component map:** UE reports SINR → BS computes I^Inter → Agent selects action, observes reward → Store local experience; conditionally share if I^Inter > -110 dBm → Receive shared experiences → Insert into local buffer → Sample minibatch → Compute loss → Gradient descent update

- **Critical path:** 1. UE reports SINR → BS computes I^Inter via Equations (6)-(9) 2. Agent selects action, observes reward and next state 3. Store local experience; conditionally share if I^Inter > I_min 4. Receive shared experiences → insert into local buffer 5. Sample minibatch → compute loss (Equation 15) → gradient descent update

- **Design tradeoffs:**
  - **Threshold I_min:** Lower = more sharing, higher communication but potentially faster convergence; higher = less sharing, risk of missing informative edge cases
  - **Buffer size:** Must accommodate both local and shared experiences; too small loses rare high-interference events
  - **Sharing granularity:** Per-UE vs. per-cell; current design shares per-UE experiences when any UE exceeds threshold

- **Failure signatures:**
  - **Sum-rate plateaus below baseline:** Likely threshold too high, blocking informative experiences; check sharing frequency
  - **Training instability (oscillating loss):** Possible cause is off-policy drift from stale shared experiences; investigate buffer turnover rate
  - **SINR CDF shifts left over time:** Agents may be converging to greedy policies; verify reward penalty ℜ is applied correctly

- **First 3 experiments:**
  1. **Ablation on threshold:** Sweep I_min from -120 dBm to -90 dBm; plot sum-rate vs. % experiences shared to find operating point where performance degrades
  2. **Scalability test:** Increase L from 2 to 4 to 7 cells; measure whether sharing percentage and convergence time scale with network density
  3. **Latency sensitivity:** Introduce artificial delay in experience sharing; measure performance degradation to bound acceptable backhaul latency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but implies several through its discussion of future work and limitations.

## Limitations
- Performance claims rely heavily on a specific 2-cell network topology, making generalization to larger, heterogeneous networks uncertain
- The choice of interference threshold (-110 dBm) appears heuristic without sensitivity analysis across different noise floors or cell densities
- The state representation assumes UE coordinates and previous actions capture sufficient spatial-temporal information, which may break down under high mobility scenarios
- The paper does not address potential staleness of shared experiences in time-varying channels or quantify communication delay impacts

## Confidence
- **High confidence:** The core mechanism of selective experience sharing based on inter-cell interference thresholds is well-defined and performance claims are supported by experimental results within the specific 2-cell configuration
- **Medium confidence:** The claim that joint beamforming and power control can be learned concurrently through binary action encoding is reasonable given the controlled experimental setup
- **Low confidence:** The assumption that shared experiences inserted into local replay buffers provide representative gradient signals for all agents requires further validation

## Next Checks
1. **Threshold sensitivity analysis:** Systematically sweep the sharing threshold I_min from -120 dBm to -90 dBm across multiple random network realizations and measure the tradeoff between communication overhead reduction and sum-rate performance degradation.

2. **Scalability testing:** Extend experiments from 2-cell to 4-cell and 7-cell networks while maintaining the same sharing percentage, measuring convergence time, sum-rate performance, and communication overhead.

3. **Latency impact quantification:** Introduce controlled artificial delays (0ms, 5ms, 10ms, 20ms) in experience sharing and measure the corresponding degradation in sum-rate and SINR CDF to establish acceptable backhaul latency bounds.