---
ver: rpa2
title: 'Symbol grounding in computational systems: A paradox of intentions'
arxiv_id: '2505.00002'
source_url: https://arxiv.org/abs/2505.00002
tags:
- symbols
- system
- computational
- mind
- computing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a paradox showing that computationalism cannot
  explain symbol grounding. If the mind is a digital computer, it must either compute
  over meaningful symbols (implying semantic nativism) or over meaningless symbols
  (making symbol grounding impossible).
---

# Symbol grounding in computational systems: A paradox of intentions

## Quick Facts
- arXiv ID: 2505.00002
- Source URL: https://arxiv.org/abs/2505.00002
- Reference count: 0
- Primary result: Computationalism cannot explain symbol grounding without either presupposing meaningful symbols (semantic nativism) or requiring non-computational processes

## Executive Summary
This paper presents a paradox showing that computationalism cannot resolve the symbol grounding problem. If the mind is a digital computer, it must either compute over meaningful symbols (impolving semantic nativism) or over meaningless symbols (making symbol grounding impossible). The author argues that purely syntactic computation is possible, but cannot acquire meaning without intentional cognitive processes. This leaves computationalism in a dilemma: it either presupposes innate meaning or cannot explain how meaning arises. The paper concludes that the mind is likely a hybrid system with some computational and some non-computational modules, and that meaning acquisition requires non-computational processes.

## Method Summary
The paper employs conceptual analysis using Searle's Chinese Room, Putnam/Kripke's causal theory of reference, and logical argumentation to establish a paradox for computationalism. It examines two versions of computationalism (LOCO and syntactic) and shows both fail to explain symbol grounding. The analysis distinguishes between physical, syntactical, and symbolic levels of description in computation, and argues that intentionality is necessary for grounding symbols in the world.

## Key Results
- Computationalism faces a paradox: LOCO presupposes meaningful symbols (semantic nativism), while syntactic computation cannot ground symbols without intentionality
- Purely syntactic computation is possible at the mechanical level without system-level understanding of rules
- Symbol grounding requires intentional states (desires, attention, beliefs) that syntactic systems cannot possess
- The mind is likely a hybrid system with some computational and some non-computational modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language of Thought computationalism (LOCO) presupposes meaningful symbols, making semantic nativism unavoidable
- Mechan: If cognition operates over meaningful symbols, those symbols must exist prior to cognitive processing. The system cannot acquire meaning through a language of thought that itself requires meaning to function
- Core assumption: Thinking is computation over language-like representations that already possess semantic content
- Evidence anchors:
  - [abstract] "If it is computing over meaningful symbols its functioning presupposes the existence of meaningful symbols in the system, i.e. it implies semantic nativism"
  - [section 2.2] "The acquisition of these meaningful symbols can thus not be the work of a language of thought"
  - [corpus] Weak direct corpus support; related work on symbol grounding limits (arXiv:2509.20409) addresses formal constraints but not nativism specifically
- Break condition: If symbols could acquire meaning through non-LOT processes before LOT operates, the paradox dissolves

### Mechanism 2
- Claim: Purely syntactic computation is technically possible and does not require system-level understanding of rules or programs
- Mechan: Computers operate at three levels: physical (causal state transitions), syntactical (token manipulation per algorithms), and symbolic (observer-attributed meaning). The syntactical level is mechanically sufficient; "following rules" is an observer-relative description, not a requirement for operation
- Core assumption: Token recognition (distinguishing 0 from 1) does not require semantic interpretation by the system
- Evidence anchors:
  - [section 3.1] "The manipulation follows the algorithms and only concerns these tokens as tokens, not their physical realization or their interpretation; it is 'purely syntactical'"
  - [section 3.2] "The computer does not literally follow a rule. Being in a particular state, given a particular input, it will perform a series of steps"
  - [corpus] "The Outputs of Large Language Models are Meaningless" (arXiv:2509.22206) argues LLM outputs lack meaning due to missing intentions—consistent with syntactic-only operation
- Break condition: If token individuation necessarily requires semantic primitives (per Kuczynski's critique mentioned in section 3.1), purely syntactic computation is incoherent

### Mechanism 3
- Claim: Symbol grounding requires intentional states (desires, attention, beliefs) that syntactic systems cannot possess
- Mechan: Causal connections between symbols and referents are underdetermined—any token stands in infinitely many causal relations. Resolving this requires an "explanatory cause" selected by the agent's intention to refer. Without desires or intentions, no symbol can be grounded
- Core assumption: Reference determination is not purely causal; it requires agent-directed selection among causal chains
- Evidence anchors:
  - [section 3.3] "A successful story of the causal relation between my tokens of 'gold' and gold has to involve my desire to refer to that particular metal"
  - [section 3.3] "Could there be a theory of language acquisition...that assumes a language can be learned by a system that has no cognitive processes? I propose...it cannot be done"
  - [corpus] "A Unified Formal Theory on the Logical Limits of Symbol Grounding" (arXiv:2509.20409) distinguishes internal meaning (axiom-derivable) from external grounding (reference)—supports the reference problem but from a formal angle
- Break condition: If causal-theoretic reference fixing could work without intentional selection (e.g., via reliable covariance plus teleofunction), grounding might proceed syntactically

## Foundational Learning

- **Intentionality** (the directedness of mental states at objects/properties)
  - Why needed here: The entire paradox hinges on whether computational systems can have intentional states. Without this concept, the argument that syntactic systems lack grounding-relevant capacities is opaque
  - Quick check question: Can you explain why "desiring gold" differs causally from a thermostat "detecting" temperature?

- **Semantic Nativism** (meanings are innate, not learned)
  - Why needed here: This is the undesirable conclusion both horns of the paradox produce. Understanding why nativism is costly clarifies why the paradox matters
  - Quick check question: If a newborn has innate meaningful symbols, what would that imply about the evolution of meaning?

- **Levels of Description** (physical, syntactic, symbolic)
  - Why needed here: The paper's argument that syntactic computation is possible requires distinguishing what the system does from how we describe it
  - Quick check question: Is "XOR gate" a physical description, syntactic description, or symbolic description—and why does it fit multiple levels?

## Architecture Onboarding

- Component map: Physical -> Syntactic -> Symbolic (observer layer) -> (Hypothetical) Grounding interface
- Critical path:
  1. Identify whether your system assumes meaningful symbols at initialization (LOCO path) or meaningless tokens (syntactic path)
  2. If syntactic: verify no intentional states (desires, attention, beliefs) are invoked in grounding procedures
  3. If grounding is required: determine what non-computational process supplies intentionality
- Design tradeoffs:
  - LOCO vs. syntactic: LOCO gives you semantics but requires nativism; syntactic avoids nativism but cannot explain grounding
  - Hybrid systems: Paper suggests mind may be modular—some computational, some non-computational. This trades theoretical purity for explanatory coverage
  - Assumption: Hybrid architectures require specifying which modules are non-computational and how they interact with computational ones
- Failure signatures:
  - Claiming a purely syntactic system "learns meanings" without specifying the intentional mechanism
  - Using observer-relative descriptions ("the system understands X") as if they were system-internal states
  - Assuming causal covariance alone suffices for reference (disjunction problem)
- First 3 experiments:
  1. Nativism audit: List all symbols your system treats as meaningful at initialization. If non-empty, you're committed to semantic nativism—document the justification
  2. Intentionality inventory: Enumerate every process that selects among causal chains (attention, interest, goal-directedness). If empty, your system cannot ground symbols per this paper's argument
  3. Hybrid boundary test: If you claim hybrid architecture, specify which modules are non-computational. Test: can these modules be fully simulated on a Turing machine? If yes, they're computational and the paradox applies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a theory of language acquisition be developed that explains how a system with no intentional cognitive processes can learn a language?
- Basis in paper: [explicit] In Section 3.3, the author poses this as a challenge, asking if such a theory is possible given that successful reference usually requires intentional states to select the relevant "explanatory cause" from complex causal chains
- Why unresolved: Current theories rely on intentional states (desire, attention) to disambiguate reference, but a purely syntactic system ex hypothesis lacks these states
- What evidence would resolve it: A formal model or demonstration of a system acquiring semantics through purely syntactic operations and causal inputs, without utilizing pre-existing intentional architecture

### Open Question 2
- Question: Is there a viable version of computationalism that avoids the grounding paradox by operating without symbols or utilizing forms of information processing distinct from classical computation?
- Basis in paper: [explicit] Section 4.1 explicitly asks "whether there is another version of computationalism that could save the day," suggesting possibilities like "computation without symbols" or non-computational information processing
- Why unresolved: The paper establishes a dilemma for classical computationalism (syntactic vs. semantic), but does not disprove non-classical architectures (e.g., dynamical systems) that might process information without manipulating symbols
- What evidence would resolve it: A cognitive architecture that processes information in a way that is neither syntactic symbol manipulation nor reliant on innate semantics, yet still grounds meaning

### Open Question 3
- Question: By what specific mechanisms would non-computational modules (such as "desire" or dynamic systems) confer meaning onto symbols in a hybrid cognitive system?
- Basis in paper: [inferred] The author concludes that the mind is likely a "hybrid and modular" system where meaning acquisition requires "non-computational processes" (p. 11), but does not detail the mechanism for this interaction
- Why unresolved: Identifying the mind as hybrid shifts the problem rather than solving it; the interaction between the "magical bit" (non-computational intention) and the computational module remains undefined
- What evidence would resolve it: A theoretical model or empirical data showing how a non-computational property (like biological intention or desire) causally interacts with computational states to generate semantic content

## Limitations

- The argument relies on philosophical intuitions about intentionality that lack empirical operationalization
- The distinction between computational and non-computational processes is proposed but not rigorously defined
- The paper does not address whether alternative frameworks like embodied cognition or dynamic systems theory could escape the paradox

## Confidence

- High Confidence: The logical structure of the paradox itself (showing that computationalism faces a dilemma between semantic nativism and inability to ground symbols)
- Medium Confidence: The claim that purely syntactic computation is possible without system-level rule-following
- Medium Confidence: The assertion that intentional states are necessary for symbol grounding
- Low Confidence: The conclusion that hybrid systems are the best explanation, due to limited engagement with alternative frameworks

## Next Checks

1. **Formalize the paradox**: Translate the argument into formal logic to verify validity and identify exactly where the premises rely on contested philosophical assumptions
2. **Operationalize intentionality**: Develop operational criteria for distinguishing intentional from non-intentional processes that could be tested empirically in AI systems
3. **Test hybrid boundaries**: For existing hybrid AI architectures, specify which modules are claimed to be non-computational and attempt to simulate them computationally—if successful, the paradox may still apply