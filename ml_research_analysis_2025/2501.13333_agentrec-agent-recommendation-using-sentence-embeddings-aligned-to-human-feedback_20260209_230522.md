---
ver: rpa2
title: 'AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human
  Feedback'
arxiv_id: '2501.13333'
source_url: https://arxiv.org/abs/2501.13333
tags:
- agent
- sentence
- embeddings
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting the most appropriate
  LLM agent from a pool of specialized agents for a given task described in natural
  language. The authors propose AgentRec, a method that extends the Sentence-BERT
  (SBERT) encoder model to recommend agents based on sentence embeddings aligned with
  human feedback.
---

# AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback

## Quick Facts
- arXiv ID: 2501.13333
- Source URL: https://arxiv.org/abs/2501.13333
- Authors: Joshua Park; Yongfeng Zhang
- Reference count: 31
- Primary result: 92.2% top-1 accuracy for recommending the correct LLM agent from 8 specialized agents

## Executive Summary
This paper presents AgentRec, a method for recommending the most appropriate LLM agent from a pool of specialized agents based on natural language prompts. The approach extends SBERT with triplet loss fine-tuning and RLHF alignment to create agent-specific embedding clusters. Using a synthetic dataset of 10,000 prompts, the system achieves 92.2% top-1 accuracy while maintaining sub-300ms inference latency. The architecture is designed to be computationally efficient, adaptive to new agents, interpretable, and controllable through human feedback.

## Method Summary
AgentRec generates single-sentence prompts for 8 specialized agents using Llama-3.1-8B-Instruct, then fine-tunes SBERT with BatchAllTripletLoss to create separable embedding clusters per agent. The system uses generalized p-means (p=200) over cosine similarities to score candidate agents, achieving 92.2% top-1 accuracy. A RaR module standardizes variable-length prompts to single sentences, and RLHF aligns embeddings to human preferences, correcting edge cases like health/fitness classification. The approach requires 6,000 prompts for SFT, 2,000 for reward modeling, and 2,000 for testing, with inference taking under 300ms including RaR preprocessing.

## Key Results
- 92.2% top-1 accuracy on test data using logarithmic generalized p-means (p=200)
- Classification latency under 300ms including RaR preprocessing, or ~50ms without
- RLHF alignment improves edge-case handling, correcting health/fitness confusion
- System remains adaptive to new agents without retraining existing embedding space

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering via Triplet Loss Fine-Tuning
- **Claim:** Minimizing intra-class embedding distance creates separable agent clusters in embedding space.
- **Mechanism:** The SBERT encoder is fine-tuned using BatchAllTripletLoss to generate (anchor, positive, negative) triplets from single-sentence prompts. This forces embeddings of prompts belonging to the same agent to cluster together while pushing different agents' embeddings apart. Classification then reduces to nearest-neighbor search via cosine similarity.
- **Core assumption:** Agent-appropriate prompts share semantic structure that can be captured in 768-dimensional embedding space and remains separable across domains.
- **Evidence anchors:**
  - [section 2]: "The base encoder fine-tuning dataset was re-organized... through the sentence_transformers Python library's BatchAllTripletLoss function."
  - [section 3]: "The machine learning objective is to produce sentence embeddings where embeddings for a given agent generate a clean, separable cluster."
  - [corpus]: Weak direct evidence; neighbor papers discuss embedding semantics but not triplet-based agent clustering specifically.
- **Break condition:** If prompt semantic structure does not correlate with agent expertise (e.g., ambiguous or multi-domain prompts), clustering degrades. Figure 2 shows noisy overlap without fine-tuning.

### Mechanism 2: Generalized P-Means Score Function Amplifies High-Confidence Matches
- **Claim:** Using generalized p-means (p=200) as the aggregation function over cosine similarities significantly outperforms naive max or arithmetic mean.
- **Mechanism:** Rather than using the single highest cosine similarity (32.55% accuracy) or arithmetic mean (90.05%), the p-means with high p amplifies extreme similarity values while suppressing moderate ones. The logarithmic formulation (Sq = ln[(1/n) Σ cos(θ)^p]^(1/p)) numerically accentuates embeddings the model is confident about (±1 range extremes).
- **Core assumption:** Correct agent recommendations are associated with at least one high-confidence similarity match in the corpus, not distributed consensus.
- **Evidence anchors:**
  - [section 3.1]: "With a value of p=200—which accentuates the extreme cosine similarity scores to a high degree—we were able to produce a top-1 accuracy rating of 92.2%."
  - [section 3.1]: "Accurate agent recommendation relies heavily on cosine similarity scores which the recommendation system can confidently assess as similar (+1) or dissimilar (-1)."
  - [corpus]: No direct corroboration; p-means for retrieval scoring is not discussed in neighbor papers.
- **Break condition:** If the correct agent's corpus lacks any high-similarity embedding for a given prompt (e.g., out-of-distribution queries), the score function cannot recover via consensus.

### Mechanism 3: RLHF Aligns Embedding Space to Human Preference
- **Claim:** Supervised fine-tuning alone produces high accuracy but fails on nuanced edge cases; RLHF corrects this.
- **Mechanism:** After initial SFT (which achieved 93.55% but misclassified "how do I eat well?" as fitness instead of health), a reward model trained on human feedback adjusts the policy to align embeddings with human expectations about domain boundaries.
- **Core assumption:** Human annotators consistently agree on which agent should handle edge-case prompts, and this signal is learnable.
- **Evidence anchors:**
  - [section 3.2]: "even with traditional fine-tuning, it is irresponsible to claim that a model trained naively on the dataset... is aligned to human values."
  - [section 3.3]: "Using a learning rate of 1×10⁻⁴, a fine-tuned SBERT encoder had a top-1 test accuracy of 93.55%. However, it was unable to accurately recommend cases such as 'how do I eat well?' appropriately to the health agent."
  - [corpus]: Neighbor paper "MGFRec" discusses reinforcement-based recommendation with feedback, but not in embedding alignment context.
- **Break condition:** If reward model data is small or biased (N=2000 split), alignment may overfit to annotated edge cases without generalizing.

## Foundational Learning

- **Concept: Sentence-BERT (SBERT) Siamese Networks**
  - **Why needed here:** AgentRec extends SBERT; understanding how siamese networks produce comparable embeddings is prerequisite to grasping the triplet loss and cosine similarity pipeline.
  - **Quick check question:** Can you explain why SBERT uses a siamese architecture rather than a standard classifier head for sentence similarity?

- **Concept: Triplet Loss and Metric Learning**
  - **Why needed here:** The core training mechanism uses BatchAllTripletLoss to shape the embedding space. Without this, the clustering mechanism is opaque.
  - **Quick check question:** In a triplet (anchor, positive, negative), what is the optimization objective, and what happens if the negative is too similar to the anchor?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** The paper claims RLHF is necessary for alignment beyond raw accuracy. Understanding reward modeling and policy optimization clarifies why SFT alone failed on edge cases.
  - **Quick check question:** What is the role of the reward model in RLHF, and how does it differ from the supervised loss?

## Architecture Onboarding

- **Component map:** User prompt -> RaR rephrasing -> SBERT encoding -> cosine similarity vs. all corpora -> p-means aggregation -> top-K selection
- **Critical path:**
  1. User prompt → RaR rephrasing → SBERT encoding → cosine similarity vs. all corpora → p-means aggregation → top-K selection
  2. Training path: Synthetic data generation → triplet sampling → SFT → reward model training → RLHF policy update
- **Design tradeoffs:**
  - **Single-sentence limitation:** SBERT is designed for single sentences; long prompts lose information unless RaR succeeds. *Mitigation:* RaR module, but adds ~250ms latency.
  - **Cached corpora vs. dynamic updates:** Caching enables 50ms inference but requires re-computation if encoder weights change or new agents are added.
  - **High p-value in p-means:** Maximizes accuracy but is sensitive to outliers; may fail if no high-confidence match exists.
  - **Synthetic data dependency:** Real-world distribution may differ; deduplication may under-represent common edge cases.
- **Failure signatures:**
  - **Misclassified health/fitness overlap:** Prompts like "how do I eat well?" incorrectly routed to fitness agent (pre-RLHF).
  - **Context-sensitive word disambiguation failure:** Pre-fine-tuning, "calculus in teeth" routed to math agent instead of health.
  - **Out-of-domain prompts:** No mechanism for "none of the above"; system always returns top-K even if all scores are low.
- **First 3 experiments:**
  1. **Baseline comparison:** Run all-mpnet-base-v2 without fine-tuning on the test split; measure top-1 accuracy and visualize embeddings (expect ~32% based on max-cosine baseline).
  2. **Ablate p-means:** Replace p=200 with arithmetic mean (p=1) and geometric mean; compare top-1 accuracy to isolate scoring contribution.
  3. **RLHF vs. SFT-only edge-case probe:** Create a held-out set of ambiguous prompts (health/fitness, therapy/health); compare accuracy with and without RLHF alignment to quantify edge-case improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the classification accuracy of AgentRec generalize to real-world, multi-sentence user prompts that differ statistically from the synthetic single-sentence dataset?
- **Basis in paper:** [explicit] Section 3.3 states that a "lack of real-world data and statistics on how often certain prompts are actually asked... prevent us from assessing the real-world impact of our methodology."
- **Why unresolved:** The training and testing data were entirely synthetic and reduced to single sentences, which may not capture the noise, complexity, or frequency distribution of actual user behavior.
- **What evidence would resolve it:** Benchmarks from a deployed multi-agent system comparing synthetic test accuracy against performance on a dataset of real human-composed queries.

### Open Question 2
- **Question:** How does the retrieval latency and classification accuracy scale as the number of available agents grows from the tested 8 to hundreds or thousands?
- **Basis in paper:** [inferred] The authors test on only 8 agents despite claiming the method is "adaptive to new classes," and the nearest-neighbor search complexity generally increases with the number of classes/corpora.
- **Why unresolved:** The paper provides efficiency metrics (<300ms) only for a specific case of 8 agents; it is unknown if the generalized p-means scoring remains robust or computationally efficient with denser agent pools.
- **What evidence would resolve it:** A study plotting retrieval latency and Top-1 accuracy curves against increasing agent counts (e.g., 10, 50, 100, 500 agents).

### Open Question 3
- **Question:** Is the reliance on the "Rephrase and Respond" (RaR) technique for variable-length prompts a computational bottleneck for the proposed architecture?
- **Basis in paper:** [explicit] Section 3 notes that SBERT was designed for single sentences, necessitating the use of RaR to handle "variable length prompts," adding a separate inference step.
- **Why unresolved:** While the AgentRec classification step is fast (<300ms), the authors do not isolate the latency added by the RaR pre-processing step, which could dominate the total inference time.
- **What evidence would resolve it:** A breakdown of end-to-end latency specifically isolating the time taken by the RaR module versus the AgentRec classifier.

## Limitations

- **Synthetic data dependency:** The entire evaluation relies on synthetically generated prompts, making real-world generalization uncertain.
- **No out-of-domain handling:** The system always returns top-K agents even when no agent is appropriate for a given query.
- **Missing RLHF details:** Critical hyperparameters and implementation details for the RLHF alignment step are not specified.

## Confidence

- **High confidence:** The technical feasibility of fine-tuning SBERT with triplet loss and using cosine similarity for classification is well-established (multiple similar works exist).
- **Medium confidence:** The claim that p=200 in generalized p-means meaningfully improves accuracy over alternatives requires validation—the improvement could be dataset-specific.
- **Low confidence:** The RLHF component's contribution to edge-case handling is poorly specified and cannot be independently verified.

## Next Checks

1. **Triplet loss ablation:** Train identical models with mean squared error vs. triplet loss; compare top-1 accuracy and visualize embeddings to verify clustering is the source of performance gain.
2. **P-means sensitivity analysis:** Systematically vary p from 1 to 200 in increments; plot accuracy to determine if 92.2% is a local optimum or arbitrary choice.
3. **Out-of-domain detection:** Create a benchmark of prompts clearly outside all 8 agent domains; measure false positive rate and implement a confidence threshold to reject uncertain classifications.