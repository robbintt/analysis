---
ver: rpa2
title: Practical quantum federated learning and its experimental demonstration
arxiv_id: '2501.12709'
source_url: https://arxiv.org/abs/2501.12709
tags:
- quantum
- uni00000013
- learning
- uni00000003
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces QuNetQFL, a practical quantum federated learning
  framework that leverages quantum networks for secure, scalable machine learning.
  The method uses quantum key distribution to generate secure keys, which serve as
  one-time pads to mask local model updates, ensuring information-theoretic security
  without relying on costly encryption.
---

# Practical quantum federated learning and its experimental demonstration

## Quick Facts
- arXiv ID: 2501.12709
- Source URL: https://arxiv.org/abs/2501.12709
- Reference count: 0
- Primary result: Introduces QuNetQFL, a quantum-enhanced federated learning framework using QKD for secure aggregation, demonstrating 32.8 kbps key rates and 75% communication cost reduction with 8-bit quantization.

## Executive Summary
This work presents QuNetQFL, a practical quantum federated learning framework that leverages quantum key distribution (QKD) to enable secure, scalable machine learning over quantum networks. The method uses pairwise quantum-generated keys to mask local model updates, providing information-theoretic security without costly encryption. Experiments on a four-client quantum network demonstrate key rates exceeding 32.8 kbps, with the framework scaling to 200 clients while reducing communication costs by 75% through quantization. Performance evaluations show enhanced classification accuracy for both quantum and classical datasets, with notable improvements in entanglement and non-stabilizer quantum data classification.

## Method Summary
QuNetQFL integrates QKD with federated learning by using quantum-generated keys as one-time pads to mask quantized model updates. The framework employs Hardware-Efficient Ansatzes (HEAs) for local quantum neural networks, with clients performing amplitude embedding of data and exchanging quantum keys via a Sagnac interferometer network. Updates are quantized to q-bit integers (q=8,16,32), masked with pairwise keys, and securely aggregated by a server where random masks cancel out. The system uses 6-qubit/4-layer HEAs for quantum datasets and 4-qubit/3-layer circuits for MNIST, with Adam optimizer and MSE loss.

## Key Results
- Experimental demonstration on 4-client quantum network achieving >32.8 kbps key rates
- 75% reduction in communication costs through 8-bit quantization while maintaining ~97% MNIST accuracy
- Improved classification accuracy for quantum data: entanglement (88.5% → 91.5%) and non-stabilizerness
- Scalability validation through simulation with 200 clients and LeNet5 architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pairwise masking via quantum-generated keys allows a server to aggregate model updates without decrypting individual contributions, providing information-theoretic security.
- **Mechanism:** The system utilizes Quantum Key Distribution (QKD) to establish pairwise secret keys ($QK_{i,j}$) between clients. Clients mask their quantized model updates ($Q(\Delta \theta)$) with a sum of these keys. When the server sums all masked updates, the random masks cancel out ($\sum m_i = 0$), revealing the aggregated update while hiding individual components.
- **Core assumption:** The threat model is "honest-but-curious"; the server follows the protocol but may try to infer data. It excludes extreme collusion where all-but-one client conspire with the server.
- **Evidence anchors:**
  - [abstract] "utilizing distributed quantum secret keys to protect local model updates... enabling secure aggregation with information-theoretic security."
  - [Page 3, Eq. 2-4] Demonstrates the algebraic cancellation of masks ($m_i$) during the modular summation.
  - [corpus] Related work in "QFed" supports the necessity of parameter compactness and privacy in quantum-classical settings, though this paper uniquely implements it via QKD.

### Mechanism 2
- **Claim:** Quantization reduces the volume of quantum keys required for secure masking, making the system scalable within the limited bit-rate constraints of current QKD networks.
- **Mechanism:** Local updates are quantized from 32-bit floats to $q$-bit integers (e.g., $q=8, 16$). Since the quantum keys must match the update size to act as one-time pads, reducing the bit-width linearly reduces the required secret key rate.
- **Core assumption:** The quantization noise introduced ($\mathcal{O}(1/2^q)$) does not destroy the convergence properties of the machine learning model.
- **Evidence anchors:**
  - [Page 7, Table II] Shows a 75% reduction in communication costs with 8-bit quantization while maintaining accuracy $\approx 0.97$ on MNIST.
  - [Page 3, Methods] Describes $Q_q(\cdot)$ and $D_q(\cdot)$ functions mapping real updates to integers.
  - [Supplementary, Theorem S1] Provides the convergence bound including the quantization error term $E_q$.

### Mechanism 3
- **Claim:** Integrating Quantum Neural Networks (QNNs) as local models enhances the classification capability for quantum-native data structures (like entanglement) compared to purely classical processing.
- **Mechanism:** Clients use variational quantum circuits (Hardware-Efficient Ansatzes) to process quantum states. The federated averaging aggregates the parameters of these quantum circuits, allowing distributed learning of quantum features without centralizing the quantum states.
- **Core assumption:** The quantum datasets possess structures (entanglement/magic) that are hard to classify classically but tractable for QNNs.
- **Evidence anchors:**
  - [Page 5, Fig 3] Shows accuracy improvements in entanglement (88.5% → 91.5%) and non-stabilizerness classification when adding clients.
  - [Page 9, Methods] Details the "Two quantum datasets" used (NTangled, Stabilizer Rényi entropy).

## Foundational Learning

- **Concept:** One-Time Pad (OTP) Encryption
  - **Why needed here:** The security of the aggregation relies on masking updates with random keys of equal length. Without understanding OTP, one might mistake this for computational encryption (like AES), which is not "information-theoretically secure."
  - **Quick check question:** If an attacker intercepts a masked update $y = x + k \pmod N$, and they know $y$ but not $k$, why can't they deduce anything about $x$?

- **Concept:** Federated Averaging (FedAvg)
  - **Why needed here:** This is the base protocol. You must understand that the server only ever sees the *average* of the updates, not the raw data or gradients, to understand what the masks are protecting.
  - **Quick check question:** In standard FedAvg, does the server need to decrypt individual updates to compute the average? (Answer: No, summation is linear).

- **Concept:** Measurement-Device-Independent QKD (MDI-QKD)
  - **Why needed here:** The experimental setup uses MDI-QKD to generate the keys. This protocol removes detector side-channels, which is critical for the security claims.
  - **Quick check question:** In MDI-QKD, who performs the measurement—Alice/Bob (clients) or Eve (the untrusted node)?

## Architecture Onboarding

- **Component map:** Clients (QNN/NN + QKD hardware) -> Quantum Network Layer (Sagnac loop + detection node) -> Aggregation Server -> Classical Network (masked updates + global parameters)
- **Critical path:** 1. Key Gen: Clients run 4-phase MDI-QKD → generate $QK_{i,j}$ 2. Local Train: Clients compute update $\Delta \theta_i$ and quantize to integers 3. Masking: Apply OTP mask using generated keys 4. Aggregation: Server sums masked vectors; masks cancel; server de-quantizes result
- **Design tradeoffs:** Quantization Bit-width ($q$): Higher $q$ (e.g., 32) improves model precision but drastically increases key generation time (linear scaling). Lower $q$ (e.g., 8) saves keys/rate but risks accuracy. Network Topology: Fully connected graph for key sharing vs. Star topology. This paper uses a scalable Sagnac loop for the physical layer but logically requires pairwise keys.
- **Failure signatures:** Key Starvation: Training pauses if the QKD rate (kbps) falls below the model update size * number of rounds. Modular Overflow: If server summation exceeds $2^q$ before de-quantization, weights wrap around (though algorithm handles this via mod logic, implementation bugs here break learning). Non-IID Divergence: High data heterogeneity causes standard FL divergence, which the convergence analysis (Theorem S1) predicts but does not fully solve.
- **First 3 experiments:** 1. Sanity Check (Masking): Run the aggregation with 2 clients. Verify that $y_1 + y_2$ equals the unmasked sum $\Delta \theta_1 + \Delta \theta_2$ locally before server deployment. 2. Rate Limiting: Profile the QKD key generation rate vs. the model size. Determine the maximum model size (parameters) trainable in real-time with the generated key rate. 3. Quantization Sweep: Train a classical LeNet model on MNIST with $q=32, 16, 8, 4$ to find the "knee" of the accuracy curve (identifying the acceptable compression threshold).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of blind quantum computing or quantum homomorphic encryption into the QuNetQFL framework successfully reduce communication complexity without compromising the efficiency or security guarantees established by the current QKD-based masking approach?
- Basis in paper: [explicit] The authors state in the Discussion: "Future work will focus on reducing communication complexity by integrating these advanced quantum algorithms... balancing efficiency with practical implementation."
- Why unresolved: The current framework relies on QKD for one-time pads, which involves distinct communication overhead compared to the proposed blind computing or homomorphic encryption methods; the trade-offs of switching these underlying mechanisms remain unexplored.
- What evidence would resolve it: A comparative analysis of communication rounds and bandwidth usage between the current QKD-masking implementation and a modified version utilizing quantum homomorphic encryption for secure aggregation.

### Open Question 2
- Question: How can the security protocol be modified to withstand the "extreme collusion" scenario where the server conspires with all but one client to isolate the remaining honest client's data?
- Basis in paper: [explicit] The Security Analysis section explicitly excludes this case: "Our scheme excludes this extreme collusion... If the server colludes with all but one of the clients... exposing the honest client's data."
- Why unresolved: The current masking mechanism relies on pairwise keys where the sum of masks is zero; if all other masks are known by the coalition (server + $K-1$ clients), the remaining client's update is mathematically exposed.
- What evidence would resolve it: A proposed modification to the secure aggregation protocol (potentially involving differential privacy or a different cryptographic structure) that provably protects individual updates even when only one participant is honest, verified through cryptographic proofs or simulation of the specific attack vector.

### Open Question 3
- Question: Does the linear reduction in communication cost via quantization hold experimentally when using true quantum secret keys (rather than pseudo-random numbers) in networks exceeding the demonstrated four-client topology?
- Basis in paper: [explicit] The paper notes regarding the 200-client simulation: "Due to limitations in our ability to implement such scale quantum networks, we used pseudo-random numbers as masks instead of true quantum secret keys."
- Why unresolved: The interaction between aggressive quantization (8-bit to 16-bit) and the generation rate of true quantum keys in a large-scale, fully connected quantum network has not been empirically validated, only simulated.
- What evidence would resolve it: An experimental demonstration or rigorous simulation incorporating real-world key generation constraints (such as photon loss and finite-key effects) for 50+ clients to verify if the key generation rates can sustain the training latency.

## Limitations

- **Experimental Scope:** The quantum network demonstration involves only 4 clients with relatively simple 4-6 qubit QNNs, raising questions about scalability to the claimed 200-client regime and deeper/more complex quantum circuits.
- **Security Model:** The information-theoretic security relies on the "honest-but-curious" threat model, which does not protect against mass collusion attacks where all-but-one client conspire with the server.
- **Noise Resilience:** The framework's performance under realistic NISQ noise conditions (gate errors, decoherence, readout errors) is not extensively validated, which is critical for real-world deployment.

## Confidence

- **High Confidence:** The core mechanism of using QKD-generated keys for OTP-based masking in federated learning (Mechanism 1) is well-supported by the algebra and aligns with established principles of secure aggregation.
- **Medium Confidence:** The scalability claims (Mechanism 2) and the quantum advantage for quantum data classification (Mechanism 3) are supported by the experimental results but require further validation under more diverse and challenging conditions.
- **Low Confidence:** The framework's robustness to NISQ noise and its security under more sophisticated adversarial models (beyond honest-but-curious) are not sufficiently addressed.

## Next Checks

1. **Scalability Stress Test:** Implement and benchmark the framework with a larger number of clients (e.g., 20-50) and more complex QNN architectures (e.g., >6 qubits, >4 layers) to validate the claimed scalability and identify potential bottlenecks.
2. **NISQ Noise Injection:** Introduce realistic noise models (depolarizing channels, gate errors, measurement errors) into the QNN training and secure aggregation process to quantify the impact on model accuracy and convergence.
3. **Security Under Collusion:** Design and simulate a mass-collusion attack scenario (e.g., N-1 clients colluding with the server) to assess the vulnerability of the system and explore potential mitigation strategies (e.g., threshold cryptography, verifiable secret sharing).