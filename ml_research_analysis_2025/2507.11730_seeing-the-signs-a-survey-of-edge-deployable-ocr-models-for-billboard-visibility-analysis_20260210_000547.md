---
ver: rpa2
title: 'Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility
  Analysis'
arxiv_id: '2507.11730'
source_url: https://arxiv.org/abs/2507.11730
tags:
- text
- recognition
- accuracy
- vlms
- conditions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks OCR models for billboard visibility analysis,
  comparing traditional CNN-based OCR (PaddleOCRv4) with Vision-Language Models (VLMs)
  across two datasets with synthetic weather augmentations. Results show that while
  VLMs like Qwen 2.5 VL 3B excel at holistic scene reasoning, lightweight CNN pipelines
  achieve competitive accuracy for cropped text with far less computational cost.
---

# Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis

## Quick Facts
- **arXiv ID:** 2507.11730
- **Source URL:** https://arxiv.org/abs/2507.11730
- **Reference count:** 30
- **Primary result:** CNN-based OCR (PaddleOCRv4) matches or exceeds VLMs on cropped text accuracy with far lower computational cost, while Qwen 2.5 VL 3B excels at holistic scene reasoning under adverse conditions.

## Executive Summary
This paper benchmarks OCR models for billboard visibility analysis, comparing traditional CNN-based OCR (PaddleOCRv4) with Vision-Language Models (VLMs) across two datasets with synthetic weather augmentations. Results show that while VLMs like Qwen 2.5 VL 3B excel at holistic scene reasoning, lightweight CNN pipelines achieve competitive accuracy for cropped text with far less computational cost. Qwen 2.5 VL 3B was the most robust VLM under adverse conditions, but PaddleOCRv4 matched or outperformed VLMs on cropped text regions. The study emphasizes trade-offs between accuracy and edge deployment feasibility, releasing an augmented benchmark for future research.

## Method Summary
The study conducts an inference-only benchmark comparing CNN-based OCR (PaddleOCRv4) and Vision-Language Models on two datasets (ICDAR 2015, SVT) with synthetic weather augmentations (rain, fog, combined). Models are evaluated in two modes: cropped text recognition (using ground-truth bounding boxes) and full-scene recognition (no prior detection). Accuracy is measured using exact string match ignoring punctuation. No training is performed; all models use default inference settings. Weather augmentation parameters include fog blending coefficients (0.5, 0.7, 0.9) and rain drop counts (800, 1600, 2400).

## Key Results
- PaddleOCRv4 achieved competitive accuracy on cropped text across all weather conditions with only ~15M parameters
- Qwen 2.5 VL 3B demonstrated highest overall robustness for full-scene recognition but required 200x more parameters than PaddleOCRv4
- Fog degradation caused more severe accuracy drops than rain for all models, particularly on the SVT dataset
- VLMs showed better robustness to adverse conditions than CNNs for full-scene analysis, but CNNs excelled when text was pre-cropped

## Why This Works (Mechanism)

### Mechanism 1: Contextual Grounding in Vision-Language Models
Vision-Language Models mitigate detection errors in full-scene analysis by leveraging global semantic context rather than relying solely on local pixel features. VLMs process the entire image and associated text prompts jointly, using spatial attention mechanisms to infer text presence and content based on surrounding visual cues. This allows them to "read" text even when segmentation boundaries are ambiguous due to noise. Accuracy degrades if visual noise obscures the semantic structure of the scene.

### Mechanism 2: Feature Resolution in CNN-based Pipelines
Lightweight CNN architectures maintain superior efficiency and competitive accuracy on cropped text by optimizing specifically for character-level feature extraction. The CRNN architecture uses a lightweight backbone for spatial feature extraction followed by sequence modeling (CTC loss). When text is pre-cropped, the model utilizes its full capacity on high-resolution character strokes rather than wasting compute on background segmentation. Performance collapses if input crop deviates significantly from training distribution.

### Mechanism 3: Differential Degradation from Weather Augmentation
Synthetic fog degrades OCR performance more severely than rain because it directly reduces global contrast (feature confusion), whereas rain introduces localized occlusion (feature loss). Fog reduces signal-to-noise ratio across the entire feature map, confusing both VLM context reasoning and CNN stroke extraction. Rain creates high-frequency noise that may trigger false positives but preserves background context. The mechanism assumes static images; real-world sensor glare is not modeled.

## Foundational Learning

- **Concept: Scene Text Recognition vs. Document OCR**
  - Why needed: Differentiates between recognizing text in controlled environments (documents) and "in the wild" (billboards with stylized fonts and weather)
  - Quick check: Does the model expect a uniform background and straight text lines, or can it handle perspective distortion and clutter?

- **Concept: Holistic vs. Modular Pipelines**
  - Why needed: Compares end-to-end VLMs against modular CNN pipelines (detection separated from recognition)
  - Quick check: In PaddleOCRv4 experiments, is the model evaluated on its ability to *find* the text or just *read* the provided crop?

- **Concept: Exact Match Metrics**
  - Why needed: Evaluation protocol ignores punctuation and checks for exact string matching, which is unforgiving for VLMs
  - Quick check: If a VLM outputs "Open 24/7" when ground truth is "Open 24 hours," is this counted as partial success or total failure?

## Architecture Onboarding

- **Component map:** Input Images → Augmentation Engine (Rain/Fog/Combined) → Model Interface (Crop-mode vs Full-scene prompt) → Model Layer (VLM or CNN) → String Normalizer (remove punctuation) → Exact Match Scorer
- **Critical path:** The augmentation logic is the critical variable. Fog severity (alpha blending coefficient) directly dictates the "floor" of model performance.
- **Design tradeoffs:**
  - Latency vs. Robustness: Qwen 2.5 VL 3B offers best full-scene robustness but requires ~200x more parameters than PaddleOCRv4
  - Detection Risk vs. Recognition Accuracy: Using PaddleOCRv4 on crops yields high accuracy but assumes perfect upstream detector
- **Failure signatures:**
  - Heavy Fog: Accuracy drops to near zero for all models (e.g., SVT accuracy < 15% in R+F-H conditions)
  - Hallucination: VLMs may invent text that fits scene context or fail on stylized fonts
  - Corpus Alert: Related work suggests VLMs may look at right pixels but fail to output correct text
- **First 3 experiments:**
  1. **Sanity Check (Clean Data):** Run PaddleOCRv4 and Qwen 2.5 VL on original (non-augmented) SVT test set to establish performance gap
  2. **Sensitivity Analysis (Fog vs. Rain):** Isolate "Heavy Fog" and "Heavy Rain" augmentations; plot accuracy delta for PaddleOCRv4 vs small VLM
  3. **Pareto Verification:** Reproduce Pareto curve (Fig 3) using parameter count vs. accuracy to determine edge-deployment trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses exclusively on inference-time performance without training, limiting generalizability to real-world deployment
- Synthetic weather augmentations may not capture full complexity of real environmental degradation (e.g., specular reflections from wet surfaces)
- Strict exact-match evaluation criterion may underrepresent VLM capabilities for semantically reasonable interpretations
- Assumes perfect text detection for cropped-text evaluation, not representative of practical edge deployment

## Confidence
- **High Confidence:** PaddleOCRv4 achieves competitive accuracy for cropped text with significantly lower computational cost; VLMs struggle with stylized fonts compared to CNNs
- **Medium Confidence:** Qwen 2.5 VL 3B provides best robustness-to-efficiency trade-off for full-scene analysis, but depends on implementation details not fully specified
- **Low Confidence:** Mechanism explanation for why fog degrades performance more severely than rain (feature confusion vs. feature loss) is plausible but not directly validated

## Next Checks
1. **Prompt Sensitivity Analysis:** Systematically vary VLM prompts for full-scene recognition and measure impact on accuracy across different weather conditions
2. **Real-World Degradation Comparison:** Apply same OCR models to images captured in actual adverse weather conditions and compare performance degradation patterns to synthetic results
3. **End-to-End Pipeline Evaluation:** Implement complete OCR pipeline where PaddleOCRv4's detection module crops text regions, followed by recognition, and compare integrated accuracy to idealized cropped-text results