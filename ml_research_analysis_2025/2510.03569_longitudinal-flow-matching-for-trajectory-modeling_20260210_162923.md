---
ver: rpa2
title: Longitudinal Flow Matching for Trajectory Modeling
arxiv_id: '2510.03569'
source_url: https://arxiv.org/abs/2510.03569
tags:
- flow
- trajectory
- matching
- trajectories
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Interpolative Multi-Marginal Flow Matching
  (IMMFM), a method for learning continuous stochastic dynamics from sparsely and
  irregularly sampled trajectories. IMMFM uses a piecewise-quadratic interpolation
  path for smooth target vector fields and jointly learns drift and a data-driven
  diffusion coefficient, supported by a theoretical condition for stable learning.
---

# Longitudinal Flow Matching for Trajectory Modeling

## Quick Facts
- arXiv ID: 2510.03569
- Source URL: https://arxiv.org/abs/2510.03569
- Reference count: 40
- Primary result: IMMFM improved Dice score from 0.920 to 0.947 and reduced Hausdorff Distance from 6.50 to 2.88 pixels compared to baseline models on ADNI data.

## Executive Summary
This paper introduces Interpolative Multi-Marginal Flow Matching (IMMFM), a method for learning continuous stochastic dynamics from sparsely and irregularly sampled trajectories. IMMFM uses a piecewise-quadratic interpolation path for smooth target vector fields and jointly learns drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sampling, and produces subject-specific trajectories. Experiments on synthetic and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in forecasting accuracy and downstream tasks.

## Method Summary
IMMFM is a generative model that learns continuous stochastic dynamics from sparse, irregularly sampled high-dimensional trajectories by combining conditional flow matching with multi-marginal optimal transport. The method constructs smooth conditional paths using piecewise-quadratic velocity blending between trajectory segments, then jointly learns drift and data-driven diffusion coefficients through a stable training objective. It decomposes the high-dimensional multi-marginal path problem into pairwise optimal transport plans, avoiding expensive spline fitting in pixel space. The model operates on latent representations from a frozen UNet autoencoder and uses a U-ViT regressor with separate heads for drift, score, and diffusion predictions.

## Key Results
- Improved Dice score from 0.920 to 0.947 on ADNI data
- Reduced Hausdorff Distance from 6.50 to 2.88 pixels on ADNI data
- Outperformed baseline models in forecasting accuracy and downstream tasks on both synthetic and real-world longitudinal neuroimaging datasets

## Why This Works (Mechanism)

### Mechanism 1: Piecewise-Quadratic Velocity Blending
Using a piecewise-quadratic conditional path allows the model to learn smooth trajectory dynamics between sparse, irregular observations, avoiding the "kinks" caused by standard linear interpolation. Instead of connecting two points with a straight line, the model constructs a conditional path where the velocity is a time-dependent blend of the current segment's velocity and the next segment's velocity. This assumes the underlying system follows smooth acceleration rather than instantaneous jumps.

### Mechanism 2: Uncertainty-Calibrated Diffusion
Learning a data-driven diffusion coefficient stabilized training and improved performance on noisy data by matching the model's stochasticity to its predictive uncertainty. The model jointly learns the drift and diffusion, training the diffusion network to predict the squared error of the velocity forecast. This forces the model to inject more noise in high-variance regions and less in deterministic regions, effectively regularizing the generative process.

### Mechanism 3: Decoupled Multi-Marginal Optimal Transport
Decomposing the high-dimensional multi-marginal path problem into a sequence of pairwise Optimal Transport plans enables the model to handle sparse, irregularly sampled data without expensive spline fitting in pixel space. Rather than fitting a complex spline through all time points, the method solves simpler pairwise OT problems between consecutive time points, then reconstructs the full joint distribution by multiplying these pairwise couplings.

## Foundational Learning

- **Concept: Probability Flow ODE**
  - Why needed: The paper trains a stochastic model by leveraging its deterministic counterpart, allowing simulation-free training without solving the differential equation during the forward pass.
  - Quick check: Can you explain why learning the score function allows a deterministic ODE to have the same marginal distributions as a stochastic SDE?

- **Concept: Conditional Flow Matching (CFM)**
  - Why needed: The core training objective is not to match an unknown true vector field, but a conditional vector field constructed via Gaussian paths. CFM makes the loss tractable by analytically deriving the target velocity for these constructed paths.
  - Quick check: How does the conditional probability path differ from the marginal path, and why is the former easier to train?

- **Concept: Optimal Transport (OT)**
  - Why needed: Used to determine the "most probable" pairing of images across time points when sampling is sparse and irregular. It provides the "ground truth" couplings required to construct the paths.
  - Quick check: In this paper, is the Optimal Transport solution learned end-to-end, or is it a pre-processing step?

## Architecture Onboarding

- **Component map:** Input latent vectors from UNet Autoencoder + Time + Previous Frame -> U-ViT Regressor -> 3 Heads (Drift, Score, Diffusion) -> Output velocity and uncertainty predictions
- **Critical path:** Register images spatially → Compress to latent vectors via Autoencoder → Compute pairwise OT plans offline → Train U-ViT regressor on frozen latents → Generate trajectories via Euler/Euler-Maruyama integration
- **Design tradeoffs:** Quadratic vs Linear Path (smoothness vs simplicity), Learned vs Fixed Diffusion (adaptability vs stability)
- **Failure signatures:** Velocity Kinks (disjointed trajectories), Mode Collapse (drift to mean), Uncertainty Drift (unbounded diffusion)
- **First 3 experiments:** 1) Reproduce S-shape Gaussian experiment to verify quadratic velocity blending, 2) Train SU-IMMFM vs S-IMMFM on noisy data to confirm uncertainty loss benefit, 3) Visualize latent space trajectories for Starmen to check intermediate frame coherence

## Open Questions the Paper Calls Out

- **Open Question 1:** Does replacing standard autoencoders with temporally aware autoencoders improve latent space continuity for longitudinal trajectory modeling?
- **Open Question 2:** Can incorporating biophysical constraints or PINNs into IMMFM improve generalization in data-scarce settings?
- **Open Question 3:** Can IMMFM be extended to support causal counterfactual reasoning for simulating patient trajectories under hypothetical interventions?

## Limitations
- Optimal hyperparameters (β, σ₀) for uncertainty loss are not explicitly tuned or reported
- Preprocessing step involving "augmented empirical distributions" for MMOT is underspecified
- No analysis of model behavior on non-smooth or discontinuous trajectory data

## Confidence
- Mechanism 1 (Quadratic Velocity Blending): High - Well-supported by visual and quantitative comparisons
- Mechanism 2 (Learned Diffusion): Medium - Ablation shows benefit, but proxy assumption not rigorously validated
- Mechanism 3 (Decoupled MMOT): Medium - Mathematically sound but lacks explicit validation of long-range dependency handling

## Next Checks
1. Test IMMFM on synthetic data with sharp state transitions to evaluate quadratic smoothing limits
2. Perform ablation study comparing learned vs fixed diffusion on ADNI data with varying noise levels
3. Visualize and quantify impact of pairwise vs global OT decomposition on trajectory coherence in multi-time-point sequences