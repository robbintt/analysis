---
ver: rpa2
title: 'InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information
  Gain-based Reranking and Filtering'
arxiv_id: '2509.12765'
source_url: https://arxiv.org/abs/2509.12765
tags:
- document
- documents
- infogain-rag
- reranker
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoGain-RAG introduces Document Information Gain (DIG), a metric
  that quantifies the contribution of retrieved documents to answer generation by
  measuring changes in LLM confidence scores with and without the document. It trains
  a reranker using a multi-task learning strategy combining binary classification
  and pairwise ranking to filter irrelevant content and prioritize valuable documents.
---

# InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering

## Quick Facts
- arXiv ID: 2509.12765
- Source URL: https://arxiv.org/abs/2509.12765
- Reference count: 19
- On NaturalQA, achieves 17.9%, 4.5%, and 12.5% improvements in exact match accuracy over naive RAG, self-reflective RAG, and modern ranking-based RAG respectively, with an average 15.3% improvement across all datasets.

## Executive Summary
InfoGain-RAG introduces Document Information Gain (DIG) as a novel metric to quantify how much retrieved documents contribute to answer generation quality in RAG systems. Unlike traditional semantic similarity measures, DIG measures the change in LLM confidence for the correct answer with and without a document. The method trains a reranker using multi-task learning (classification + ranking) on DIG-derived labels, then filters and prioritizes documents before generation. On four benchmarks including NaturalQA, TriviaQA, PopQA, and FM2, it achieves substantial improvements over existing RAG approaches.

## Method Summary
InfoGain-RAG computes Document Information Gain (DIG) by measuring the change in LLM confidence for the correct answer when retrieved documents are included versus excluded. The DIG metric uses sliding window smoothing and token importance weighting to calculate p_φ(y|x,d) − p_φ(y|x). Documents are categorized as helpful (DIG>0), neutral (DIG≈0), or harmful (DIG<0). A RoBERTa-large reranker is trained using multi-task learning combining binary classification (CE loss) and pairwise ranking (margin loss) on DIG-labeled data. During inference, the reranker filters and prioritizes documents (threshold=0.2, minimum 2 docs) before LLM generation.

## Key Results
- Achieves 17.9%, 4.5%, and 12.5% improvements in exact match accuracy on NaturalQA over naive RAG, self-reflective RAG, and modern ranking-based RAG respectively
- Average 15.3% improvement across all four tested datasets (NaturalQA, TriviaQA, PopQA, FM2)
- Multi-task training (CE + margin loss) outperforms either objective alone (Qwen2.5-72B: 76.8% vs 73.0%/71.4%)
- 335M parameter reranker outperforms larger GTE-7B baseline despite computational constraints

## Why This Works (Mechanism)

### Mechanism 1: Document Information Gain (DIG) as Generation-Relevance Proxy
DIG measures p_φ(y|x,d) − p_φ(y|x) using token probabilities with sliding window smoothing and importance weighting for first k tokens. Documents categorized as helpful (DIG>0), neutral (DIG≈0), or harmful (DIG<0). Core assumption: confidence shifts reflect genuine information contribution rather than surface-level relevance.

### Mechanism 2: Multi-Task Reranker Training with Classification + Ranking Objectives
Jointly trains reranker on binary relevance classification (CE loss) and relative ranking (margin loss). Combined as L_total = βL_CE + (1−β)L_Margin. Core assumption: absolute relevance judgments and relative ordering preferences are complementary and jointly learnable.

### Mechanism 3: Query-Categorized DIG Data Collection
Separates queries into model-proficient (high p_φ(y|x)) and model-challenging (low p_φ(y|x)) categories to yield richer DIG training signals. Training data: 70K high-positive, 150K negative, 1.2M neutral, sampled into 88K unified dataset.

## Foundational Learning

- **Cross-Encoder Reranking**: InfoGain-RAG adds RoBERTa-based cross-encoder after retrieval to predict DIG-derived relevance. Why needed: cross-encoders yield higher accuracy than bi-encoders. Quick check: Explain why cross-encoders typically yield higher accuracy than bi-encoders, and what the latency tradeoff is?

- **Sliding Window Probability Smoothing**: DIG depends on stable answer-confidence estimates; smoothing mitigates length bias and token-level noise. Why needed: DIG calculation requires stable confidence estimates. Quick check: Given token probabilities [0.1, 0.9, 0.8, 0.3] and window size W=3, how would you compute smoothed p_smooth(t2)?

- **Multi-Task Learning (Classification + Ranking)**: Reranker jointly learns binary relevance and pairwise ordering under single loss. Why needed: Combining objectives yields better document selection. Quick check: If β=0.8 in L_total, what fraction of gradient signal comes from classification vs ranking at each step?

## Architecture Onboarding

- **Component map**: Retriever (Contriever) → DIG Data Collection (offline) → Reranker Training (offline) → Inference: Retriever → Reranker (score + filter) → LLM (single call)

- **Critical path**: 
  1. Sample queries from domain corpus (train-test overlap removed)
  2. Retrieve top-100 docs; compute DIG using target LLM (or proxy)
  3. Threshold DIG (b1=0.5, b2=−0.2); sample balanced CE pairs + margin groups
  4. Train reranker with β≈0.7−0.8, lr=5e-6
  5. At inference: retrieve → rerank top-100 → filter (threshold=0.2, keep ≥2 docs) → LLM generation

- **Design tradeoffs**:
  - Reranker size (335M vs 7B): Smaller model = lower latency but may miss fine-grained relevance
  - DIG thresholds (b1/b2): Higher b1 = stricter positive; lower b2 = stricter negative
  - Inference filter threshold (0.2): Lower = more documents retained; too low may include noise

- **Failure signatures**:
  - DIG ≈ 0 for most docs → query categorization may be misaligned or retriever is poor
  - Reranker overfilters (output <2 docs) → threshold too high or training data imbalanced
  - Performance degrades on new domain → DIG data collected from mismatched domain/LLM

- **First 3 experiments**:
  1. DIG sanity check: Compute DIG distribution for 500 queries on your LLM; verify three categories emerge and correlate with manual relevance judgments
  2. Ablate multi-task: Train reranker with CE-only, margin-only, and multi-task (β=0.75) on held-out validation set; compare EM accuracy
  3. Threshold sensitivity: Sweep inference filter threshold (0.1–0.5) and b1/b2 (0.3–0.7/−0.1–−0.4); plot accuracy vs retained-doc count to find stability region

## Open Questions the Paper Calls Out

### Open Question 1
Can the DIG metric be modified to distinguish between documents that increase generation confidence and those that introduce convincing but factually incorrect information (hallucinations)? The DIG metric measures confidence delta (p(y|x,d) − p(y|x)), so a document containing plausible misinformation might increase model confidence in a wrong answer, resulting in a false positive "gain." Evidence needed: evaluation on adversarial datasets containing plausible but factually false documents.

### Open Question 2
Is the specific confidence estimation method (sliding window smoothing and token importance weighting) robust when transferred to non-text modalities like code or vision? The DIG calculation relies heavily on token probability manipulation to mitigate length bias. It is unclear if visual tokens or code tokens exhibit similar probability distributions. Evidence needed: experimental results applying framework to multimodal LLM or code generation model.

### Open Question 3
What is the performance-latency trade-off when scaling the reranker architecture beyond 335M parameters to match size of proprietary baselines like GTE-7B? While the 335M model outperforms the 7B baseline, it is unknown if DIG objective saturates at smaller model sizes. Evidence needed: training and benchmarking reranker using larger backbones (e.g., 7B parameters) on same DIG dataset.

## Limitations

- DIG metric cannot distinguish factual inaccuracies in retrieved documents, which may require an extra module to address this issue
- Computational constraints limit reranker to 335M parameters rather than larger models (7B+) which could offer better performance
- The DIG metric has only been tested on text modalities, though it is theoretically extensible to other modalities such as visual or code data

## Confidence

- **High Confidence**: DIG computation methodology (token probability smoothing, importance weighting, threshold-based categorization) is clearly specified and reproducible
- **Medium Confidence**: Reported performance improvements are credible given ablation studies, but absolute EM scores and comparison with more recent methods would strengthen confidence
- **Low Confidence**: Generalizability of DIG across different LLM architectures and domains is not thoroughly validated

## Next Checks

1. **Domain Transfer Validation**: Evaluate InfoGain-RAG on a domain significantly different from Wikipedia (e.g., scientific literature or medical records) to test whether DIG-based reranking generalizes beyond reported datasets.

2. **LLM Architecture Sensitivity**: Test reranker trained with DIG from Qwen2.5-7B when applied to generation with different LLM families (e.g., GPT-4, Claude, Llama) to quantify performance degradation and identify architecture-specific patterns.

3. **Threshold Robustness Analysis**: Systematically sweep inference filter threshold (0.1–0.5) and DIG categorization thresholds (b1=0.3–0.7, b2=−0.1 to −0.4) across multiple datasets to identify stable operating regions and quantify performance variance.