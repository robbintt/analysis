---
ver: rpa2
title: 'Shapley Values: Paired-Sampling Approximations'
arxiv_id: '2508.12947'
source_url: https://arxiv.org/abs/2508.12947
tags:
- value
- function
- shapley
- paired-sampling
- kernelshap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper analyzes Shapley value approximations via KernelSHAP
  and PermutationSHAP sampling methods, providing asymptotic normality results for
  both approaches. Key findings include: (1) both paired-sampling methods provide
  exact Shapley values when the value function contains only second-order interactions;
  (2) the paired-sampling PermutationSHAP possesses an additive recovery property
  allowing exact identification of additive components, while KernelSHAP does not;
  (3) paired-sampling reduces variance compared to non-paired sampling.'
---

# Shapley Values: Paired-Sampling Approximations

## Quick Facts
- **arXiv ID**: 2508.12947
- **Source URL**: https://arxiv.org/abs/2508.12947
- **Reference count**: 31
- **Primary result**: Paired-sampling approximations for Shapley values (KernelSHAP and PermutationSHAP) provide variance reduction and exactness for second-order interactions, with PermutationSHAP uniquely enabling additive recovery for feature clusters

## Executive Summary
This paper analyzes two paired-sampling methods for approximating Shapley values: KernelSHAP and PermutationSHAP. Both methods exploit symmetry by simultaneously sampling coalitions and their complements, achieving variance reduction compared to independent sampling. The theoretical analysis proves asymptotic normality for both approaches and identifies conditions where exact recovery occurs. Crucially, the paper demonstrates that PermutationSHAP possesses an additive recovery property that allows exact identification of aggregate contributions from disjoint feature subsets, while KernelSHAP does not. This property is particularly valuable for identifying interaction clusters among feature components in machine learning applications.

## Method Summary
The paper compares two paired-sampling approaches for estimating Shapley values: Sampling KernelSHAP and Paired-Sampling PermutationSHAP. KernelSHAP solves a weighted least squares problem using coalitions and their complements as paired samples, while PermutationSHAP estimates Shapley values by averaging marginal contributions over permutations and their reverses. Both methods leverage the symmetry of value functions to reduce variance. The analysis establishes asymptotic normality through M-estimation theory, deriving asymptotic covariance matrices for both approaches. Exactness is proven when value functions contain only second-order interactions, and the additive recovery property is demonstrated for PermutationSHAP under disjoint interaction assumptions.

## Key Results
- Both paired-sampling methods provide exact Shapley values when the value function contains only second-order interactions (bilinear forms)
- Paired-sampling reduces asymptotic variance compared to non-paired sampling methods
- Paired-Sampling PermutationSHAP possesses an additive recovery property allowing exact identification of aggregate contributions for disjoint feature subsets
- KernelSHAP does not possess the additive recovery property and may produce biased results for additive partitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Paired-sampling reduces the asymptotic variance of Shapley value estimators compared to independent sampling.
- **Mechanism:** By simultaneously sampling a coalition $Z$ and its complement $Z^c = \mathbf{1}-Z$, the estimator exploits the symmetry of the value function. This correlation effectively cancels error components, resulting in a positive semi-definite difference between the non-paired and paired asymptotic covariance matrices ($T - T_2 \ge 0$).
- **Core assumption:** The value function $\nu$ is not purely linear and sampling is i.i.d.
- **Evidence anchors:**
  - [abstract] "paired-sampling reduces variance compared to non-paired sampling."
  - [section 2.3] Proposition 2.3 proves $T - T_2 \ge 0$.
  - [corpus] General context: Efficient estimation is a core focus of Shapley literature.

### Mechanism 2
- **Claim:** Both Paired-Sampling KernelSHAP and PermutationSHAP yield exact Shapley values if the value function contains interactions of maximal order 2 (bilinear forms).
- **Mechanism:** When $\nu(Z) = Z^\top A Z$ (bilinear), the combination of $\nu(Z)$ and $\nu(\mathbf{1}-Z)$ collapses into a linear form solvable via regression or direct calculation. Specifically, $\nu(Z) + \nu(\mathbf{1}) - \nu(\mathbf{1}-Z) = 2\beta^\top Z$, reducing the problem to a linear regression with exact solution properties.
- **Core assumption:** The underlying model/value function strictly adheres to the bilinear structure.
- **Evidence anchors:**
  - [abstract] "...paired-sampling approaches provide exact results in case of interactions being of maximal order two."
  - [section 2.4] Corollary 2.5 proves exactness for KernelSHAP given full rank design matrix.
  - [section 3] Proposition 3.1 proves exactness for PermutationSHAP requires only $n=1$.

### Mechanism 3
- **Claim:** Paired-PermutationSHAP possesses an "additive recovery property" allowing exact identification of aggregate contributions for disjoint feature subsets, whereas KernelSHAP does not.
- **Mechanism:** If features partition into disjoint subsets with no interactions between them, summing the marginal contributions of features within a subset across a single permutation creates a telescoping sum. This sum equals the exact contribution of that subset to the total payoff, independent of the specific permutation used.
- **Core assumption:** Features can be partitioned into subsets where interactions across subsets are strictly zero (additive decomposition).
- **Evidence anchors:**
  - [abstract] "...paired-sampling PermutationSHAP possesses the additive recovery property, whereas its kernel counterpart does not."
  - [section 4] Proposition 4.4 proves $\sum_{j \in A_k} \hat{\phi}_j^{(1)} = \sum_{j \in A_k} \phi_j$ for any permutation.

## Foundational Learning

- **Concept: Shapley Axioms (Efficiency, Symmetry, Dummy, Linearity)**
  - **Why needed here:** The paper relies on these axioms to define the "ground truth" value $\phi$ that the sampling methods approximate. Understanding the *Efficiency* axiom is particularly critical for grasping the Additive Recovery proof.
  - **Quick check question:** Can you explain why the Dummy Player axiom implies that features outside a disjoint interaction group should contribute zero to that group's internal Shapley calculation?

- **Concept: Value Functions (Conditional vs. Interventional)**
  - **Why needed here:** The paper is model-agnostic regarding the *choice* of $\nu$ but mathematically rigorous about its *structure*. You must distinguish between the value function's definition (e.g., how missing features are masked) and the sampling approximation of it.
  - **Quick check question:** Does the paper's proof of asymptotic normality depend on the value function being interventional, or does it apply to any fixed function $\nu$?

- **Concept: M-estimation and Z-estimation**
  - **Why needed here:** Section 2 frames KernelSHAP as an M-estimation problem (minimizing objective) converted to a Z-estimation problem (solving score equations). This framework is necessary to understand the derivation of the asymptotic covariance matrix $T = J^{-1}IJ^{-1}$.
  - **Quick check question:** In the context of Sampling KernelSHAP, what does the matrix $J$ represent regarding the curvature of the objective function?

## Architecture Onboarding

- **Component map:** Sampler -> Evaluator -> Solver -> Estimator
  - Sampler: Generates binary coalitions $Z$ (Kernel) or permutations $\pi$ (Permutation) and their complements $Z^c / \rho(\pi)$
  - Evaluator: Computes $\nu(Z)$ for the sampled coalitions (This is the computational bottleneck)
  - Solver:
    - Kernel: Solves weighted linear regression $(Z^\top Z)^{-1}Z^\top Y$
    - Permutation: Averages marginal differences $\nu(S \cup \{j\}) - \nu(S)$

- **Critical path:** The evaluation of the value function $\nu$ is the primary cost driver. While KernelSHAP uses fewer evaluations per sample (2 vs. $2q$ for Permutation), PermutationSHAP often converges with lower variance per sample.

- **Design tradeoffs:**
  - **PermutationSHAP:** Lower variance per sample; guarantees Additive Recovery; higher compute per sample ($\propto q$)
  - **KernelSHAP:** Higher variance per sample; fails Additive Recovery; lower compute per sample (constant $\propto 2$)
  - **Decision:** Use Paired-PermutationSHAP for cluster identification or when $q$ is moderate; consider Paired-KernelSHAP only if $q$ is very large and evaluation cost is strictly prohibitive

- **Failure signatures:**
  - **Singular Matrix:** In KernelSHAP, if the design matrix $Z$ lacks full rank $q-1$ (too few samples or collinear samples), the solution is undefined
  - **Bias in Clusters:** If using KernelSHAP on a problem with distinct additive clusters, the aggregate credit allocated to those clusters will be biased

- **First 3 experiments:**
  1. **Bilinear Validation:** Create a value function $\nu(Z) = Z^\top A Z$. Verify that both Paired-Sampling methods return exact Shapley values with $n=1$ (Permutation) or minimal samples (Kernel)
  2. **Variance Decomposition:** Reproduce Figure 1/2. Plot the empirical standard deviation against the theoretical $\sqrt{T_{jj}/n}$ to verify the asymptotic normality holds for your specific model complexity
  3. **Additive Stress Test:** Construct a function with two independent additive blocks (e.g., $\nu(Z) = f(Z_A) + g(Z_B)$). Show that Paired-PermutationSHAP recovers the block sums exactly, while KernelSHAP produces biased block sums

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Sampling KernelSHAP estimator be modified to guarantee a unique, unbiased solution for small sample sizes where the design matrix currently suffers from rank deficiency?
- **Basis in paper:** [explicit] Remark 2.1 explicitly denies the possibility of unbiasedness for the standard estimator because the design matrix may lack full rank with positive probability.
- **Why unresolved:** The authors conclude that because the solution is not unique in rank-deficient cases, the question of unbiasedness "needs to be denied" for the standard formulation, leaving the potential for a regularized solution unexplored.
- **What evidence would resolve it:** A modified estimator (e.g., using regularization) that maintains full rank properties and demonstrates consistent unbiasedness across stochastic simulations with small sample sizes $n$.

### Open Question 2
- **Question:** How can the block-diagonal structure of the asymptotic covariance matrix $\Sigma$ be robustly identified to automatically cluster additive feature components in high-dimensional settings?
- **Basis in paper:** [explicit] Remarks 4.5 notes that practical application faces difficulties because the covariance matrix must be estimated from noisy samples and components must be reordered to reveal the block-diagonal structure.
- **Why unresolved:** While Proposition 4.4 proves the additive recovery property exists, the paper states that "typically the components of the feature $Z$ will have an arbitrary order," requiring a method to manually or algorithmically identify these blocks.
- **What evidence would resolve it:** An algorithm capable of permuting rows and columns of the estimated covariance matrix to reveal additive clusters without prior knowledge of the partition $\{A_k\}$.

### Open Question 3
- **Question:** Under what specific conditions of computational budget and feature dimensionality does Paired-Sampling KernelSHAP strictly outperform PermutationSHAP in terms of estimation error per unit of time?
- **Basis in paper:** [inferred] Example 3.3 shows that while PermutationSHAP has lower variance per sample, KernelSHAP requires fewer value function evaluations ($2$ vs $2q$), suggesting a trade-off based on model complexity.
- **Why unresolved:** The paper concludes that "preference may change to the kernel version" depending on evaluation costs but does not provide a theoretical threshold or heuristic for choosing between the two methods in practice.
- **What evidence would resolve it:** A theoretical analysis or comprehensive benchmark defining the "break-even" point where the lower sample complexity of PermutationSHAP is outweighed by the higher computational cost of evaluating the value function.

## Limitations
- Exact recovery results are limited to bilinear functions (second-order interactions), which rarely occur in practical applications
- Theoretical framework assumes i.i.d. sampling and sufficient sample sizes, with no finite-sample performance guarantees
- Computational cost analysis assumes equal evaluation costs for coalitions and their complements, which may not hold in practice
- No empirical comparison against state-of-the-art approximation methods beyond the two paired-sampling approaches

## Confidence
- **High confidence**: Variance reduction claims - well-proven with Proposition 2.3 and empirical validation
- **Medium confidence**: Exactness for bilinear functions - theoretically sound but narrow applicability
- **Medium confidence**: Additive recovery property - proven but requires strict additivity assumptions

## Next Checks
1. Test the additive recovery property on real-world datasets with known feature groupings to verify practical utility beyond synthetic examples
2. Conduct a comprehensive runtime comparison measuring actual evaluation costs across different coalition sizes and model complexities
3. Validate asymptotic normality assumptions on non-linear value functions with varying interaction structures to identify breakdown conditions