---
ver: rpa2
title: How Well Can Preference Optimization Generalize Under Noisy Feedback?
arxiv_id: '2510.01458'
source_url: https://arxiv.org/abs/2510.01458
tags:
- learning
- preference
- noise
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the generalization of preference optimization
  under noisy human feedback, a critical issue for aligning large language models
  with human values. The authors provide the first theoretical framework for understanding
  how noise in preference labels impacts learning, considering both mislabeling and
  uncertainty models that reflect real-world data issues.
---

# How Well Can Preference Optimization Generalize Under Noisy Feedback?

## Quick Facts
- arXiv ID: 2510.01458
- Source URL: https://arxiv.org/abs/2510.01458
- Reference count: 40
- This paper provides the first theoretical framework for understanding how noise in preference labels impacts generalization in large language model alignment.

## Executive Summary
This paper analyzes the generalization of preference optimization under noisy human feedback, a critical issue for aligning large language models with human values. The authors provide the first theoretical framework for understanding how noise in preference labels impacts learning, considering both mislabeling and uncertainty models that reflect real-world data issues. Their key contribution is a generalization bound showing that under well-separated data distributions, model performance can remain near-optimal even with moderate noise levels, but requires significantly more samples as noise increases. The analysis applies to a broad family of preference optimization losses including DPO, IPO, and SLiC. Empirical validation on both controlled synthetic data and real Anthropic datasets confirms the theoretical predictions, demonstrating that data separability is crucial for robustness to noise. The work provides practical guidance for practitioners: examining dataset properties before applying preference optimization can inform whether standard or noise-aware methods are needed.

## Method Summary
The paper analyzes preference optimization under noisy feedback using both theoretical and empirical approaches. The theoretical framework models preference embeddings as von Mises-Fisher distributions on a hypersphere, deriving generalization bounds based on data concentration (γ) and angular separation (2ϕ) between preferred and non-preferred classes. The empirical validation uses Llama-3.1-8B fine-tuned with DPO, IPO, and SLiC losses on Anthropic Evaluations datasets, injecting synthetic noise at rates 0 to 0.5. The experiments measure test accuracy (0-1 loss based on reward margin sign) across different noise levels and dataset characteristics, averaging over 10 runs. The training uses 2 epochs with AdamW optimizer, learning rate 10^-5.5, batch size 100, and DeepSpeed ZeRO-3 on 4× A100 80GB GPUs.

## Key Results
- Population risk remains near zero up to a noise threshold that depends on data concentration and sample size
- Beyond this threshold, risk grows approximately linearly toward 0.5 as noise approaches 0.5
- Well-separated, highly concentrated data distributions are significantly more robust to noise than poorly separated or low-concentration data

## Why This Works (Mechanism)

### Mechanism 1: Population Risk Remains Near Zero Up to a Noise Threshold
The proof analyzes boundary dynamics during finite-step optimization. By modeling preference embeddings as von Mises-Fisher (vMF) distributions on a hypersphere with concentration parameter κ, the analysis tracks how the decision boundary shifts under gradient updates. When the normalized concentration γ = 2κ/d is sufficiently large, positive and negative samples cluster tightly around distinct mean directions (μ+, μ−), allowing clean signals to dominate noisy label flips during early training. The boundary remains close to the geometric separation plane. Theorem 3.3 bounds R(P) ≤ c·exp(−dγ²/(5(2+γ))) for ϵ < 1/2 − O((2+γ)/γ · √(log N/N)).

### Mechanism 2: Linear Risk Growth Beyond Threshold Due to Inflection at ϵ = 0.5
The analysis shows E[R(P)]|_{ϵ=1/2} = 1/2 and d²E[R(P)]/dϵ²|_{ϵ=1/2} = 0, implying zero curvature at the midpoint. Combined with the near-zero region below threshold, this forces approximately linear degradation between threshold and ϵ = 0.5. Equation (7) establishes the second derivative condition; Figure 4 curves show the inflection pattern empirically.

### Mechanism 3: Robustness Scales with Concentration and Sample Size
Concentration results on vMF distributions bound the deviation of sample means from μ+, μ−. With more samples and tighter clusters, the effective signal from correct labels overwhelms noise contributions during gradient aggregation, preserving correct boundary direction. Key takeaway #2: sample complexity scales as Õ(1/γ²(1−2ϵ)²); Figure 4 shows curves across different γ and N.

## Foundational Learning

- **Concept**: Von Mises-Fisher (vMF) distribution
  - Why needed here: The paper models preference embeddings as vMF distributions on the unit hypersphere to derive concentration-dependent generalization bounds. Understanding parameters (mean direction μ, concentration κ, normalized γ = 2κ/d) is essential to interpret the theory.
  - Quick check question: If κ increases, does the distribution become more or less concentrated around the mean direction?

- **Concept**: Population risk vs. empirical risk in preference learning
  - Why needed here: Theoretical results bound population risk (expected 0-1 loss on new preference pairs) based on noise rate and distributional properties, not just training loss—critical for real-world deployment.
  - Quick check question: If a model achieves near-zero training loss but ϵ = 0.3, what does Theorem 3.3 predict about population risk when γ is low?

- **Concept**: Reward margin and decision boundary dynamics
  - Why needed here: GPO learns an implicit reward model r_θ(x, y_w, y_l) = β(log π_θ(y_w|x)/π_ref(y_w|x) − log π_θ(y_l|x)/π_ref(y_l|x)). The analysis tracks how this margin evolves during gradient flow.
  - Quick check question: If the reward margin is positive for a test sample, how does the 0-1 loss classify it?

## Architecture Onboarding

- **Component map**: Preference Data -> (x, y_w, y_l) triplets -> GPO Loss Module -> Boundary Tracker -> Risk Estimator
- **Critical path**:
  1. Extract last-layer embeddings after RMSNorm for preference pairs
  2. Estimate separation (cosine distance between mean embeddings of preferred vs. non-preferred classes) and concentration (average cosine similarity to class mean)
  3. Apply GPO training for finite epochs (early stopping to stay within theoretical regime)
  4. Evaluate on clean test set to measure population risk
- **Design tradeoffs**:
  - Higher β increases reward margin sensitivity but may amplify noise effects
  - Longer training improves empirical loss but risks overfitting to noisy labels (theory assumes finite steps)
  - Larger N improves robustness but increases annotation cost; diminishing returns once ϵ threshold is met
- **Failure signatures**:
  - Test accuracy drops sharply near ϵ ≈ 0.25–0.35 for low-γ data (see Figure 5 Behavior 1)
  - Embeddings show low separation (< 0.3 cosine) between preferred/rejected classes
  - Model predictions flip randomly on held-out pairs even with moderate training loss
- **First 3 experiments**:
  1. **Diagnostic: Measure your data's γ and ϕ.** On a sample of preference pairs, compute last-layer embedding norms (check uniformity), average cosine similarity to class mean (estimate concentration), and angular separation between class means. Compare to Table 1 values.
  2. **Controlled noise sweep.** Inject synthetic ϵ-mislabeling at rates 0.05, 0.15, 0.25, 0.35 and plot test accuracy vs. noise for DPO/IPO/SLiC. Verify inflection near ϵ ≈ 0.5 and compare curves to Figure 4.
  3. **Model comparison.** Run the same noise sweep on base vs. instruct-tuned variants (e.g., Llama-3.1-8B vs. Llama-3.1-8B-Instruct). Measure separation differences and correlate with robustness curves (see Figure 6 and Table 2).

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the generalization guarantees for preference optimization extend to online reinforcement learning settings where feedback is integrated dynamically? (Basis: Limitations section; reliance on offline methods)
- **Open Question 2**: How do noise-robust DPO objectives (e.g., Dr. DPO, cDPO, rDPO) compare in generalization performance to standard GPO objectives under noisy feedback? (Basis: Limitations section; need to extend to noise-aware objectives)
- **Open Question 3**: How do generalization guarantees change under adversarial or non-i.i.d. noise contamination models? (Basis: Remarks section; current analysis assumes i.i.d. label flipping)

## Limitations

- Theoretical framework assumes symmetric noise (random label flips) and relies on strong distributional assumptions (vMF concentration, well-separated means)
- Analysis focuses on specific GPO losses (DPO, IPO, SLiC) and assumes finite-step training to avoid overfitting
- Empirical validation is limited to a single model architecture (Llama-3.1-8B) and specific preference optimization tasks

## Confidence

- **High confidence**: Population risk remaining near zero up to noise threshold (Mechanism 1) and linear growth beyond threshold (Mechanism 2)
- **Medium confidence**: Robustness scaling with concentration and sample size (Mechanism 3)
- **Medium confidence**: Practical guidance about examining dataset properties before applying preference optimization

## Next Checks

1. **Cross-model validation**: Test the theoretical predictions on different LLM architectures (e.g., GPT variants, Mistral) to verify if the vMF concentration assumptions and generalization bounds hold across diverse embedding spaces and model sizes.

2. **Structured noise evaluation**: Evaluate performance under instance-dependent noise patterns (e.g., content-aware noise where certain topics are more likely to be mislabeled) to test whether the linear risk growth beyond threshold generalizes beyond the symmetric noise model.

3. **Training duration impact**: Systematically vary training duration beyond the finite-step regime to measure the transition from the theoretical regime to potential overfitting behavior, particularly for high-noise scenarios where the risk approaches 0.5.