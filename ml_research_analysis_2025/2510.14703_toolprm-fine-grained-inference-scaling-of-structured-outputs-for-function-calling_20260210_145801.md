---
ver: rpa2
title: 'ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function
  Calling'
arxiv_id: '2510.14703'
source_url: https://arxiv.org/abs/2510.14703
tags:
- function
- arxiv
- calling
- inference
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ToolPRM introduces a fine-grained process reward model for structured
  function calling in LLM agents, decomposing function calls into interpretable intermediate
  steps (e.g., selecting function names, parameter names, and values) and training
  on a novel step-level annotated dataset. By integrating this model with fine-grained
  beam search, the framework improves function calling accuracy significantly compared
  to coarse-grained baselines and existing inference scaling strategies.
---

# ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling

## Quick Facts
- arXiv ID: 2510.14703
- Source URL: https://arxiv.org/abs/2510.14703
- Reference count: 40
- Introduces fine-grained process reward model (ToolPRM) for structured function calling, improving accuracy over coarse-grained baselines.

## Executive Summary
ToolPRM introduces a fine-grained process reward model for structured function calling in LLM agents, decomposing function calls into interpretable intermediate steps (e.g., selecting function names, parameter names, and values) and training on a novel step-level annotated dataset. By integrating this model with fine-grained beam search, the framework improves function calling accuracy significantly compared to coarse-grained baselines and existing inference scaling strategies. Experiments on BFCL and ToolAlpaca benchmarks show consistent performance gains, especially for smaller models. The work also reveals a key principle for structured inference scaling: "explore more but retain less," emphasizing efficient resource allocation in structured output generation.

## Method Summary
ToolPRM decomposes function calling into fine-grained intermediate steps (function name selection, parameter name selection, parameter value filling) and trains a process reward model on a step-level annotated dataset derived from xlam-function-calling-60k and xlam-irrelevance-7.5k. The model backbone is Hammer2.1-3b, trained for 5 epochs with Adam, batch size 1024, learning rate 1e-3, and weight decay 1e-5. Inference uses fine-grained beam search with temperature 0.8, exploring more candidates (high M) but retaining fewer beams (low N) to balance exploration and pruning, guided by ToolPRM's step-level reward signals.

## Key Results
- ToolPRM significantly outperforms baselines in function calling accuracy on BFCL and ToolAlpaca benchmarks.
- Fine-grained beam search with "explore more, retain less" principle consistently improves performance, especially for smaller models.
- The method demonstrates better efficiency in structured output generation by pruning invalid partial outputs early.

## Why This Works (Mechanism)
The method works by decomposing the structured function calling task into interpretable intermediate steps, allowing the reward model to provide granular feedback at each step. This fine-grained supervision enables more precise pruning during inference, avoiding invalid partial outputs and focusing computational resources on promising candidates. The "explore more, retain less" principle leverages the unrecoverable nature of structured outputs (like JSON) by aggressively exploring the search space while pruning aggressively to maintain efficiency.

## Foundational Learning
- **Function Masking**: Randomly replaces function names and parameter names in training data to prevent memorization and force contextual understanding. Needed to ensure the model learns robust logic rather than relying on specific function signatures. Quick check: verify masked functions are syntactically valid and diverse.
- **Step-Level Annotation**: Labels each intermediate step (function name, param name, param value) as correct (+) or incorrect (-) during data generation. Needed to provide granular reward signals for fine-grained process modeling. Quick check: ensure label distribution is balanced and matches ground truth.
- **Fine-Grained Beam Search**: Expands M candidates per step but retains only top-N beams, guided by step-level rewards. Needed to efficiently explore structured output space while pruning invalid paths early. Quick check: confirm "explore more, retain less" improves accuracy vs. other M-N configurations.

## Architecture Onboarding
- **Component Map**: Input Query → Masked Candidates → Partial Function Call → ToolPRM Scoring → Beam Expansion → Output Function Call
- **Critical Path**: Query → Function Masking → Rollout Generation (Hammer2.1) → Step Annotation → ToolPRM Training → Fine-Grained Beam Search Inference
- **Design Tradeoffs**: Fine-grained steps increase annotation complexity but enable precise pruning; aggressive beam pruning improves efficiency but risks missing valid outputs if rewards are noisy.
- **Failure Signatures**: Low step/trajectory accuracy indicates poor reward modeling or annotation errors; invalid JSON outputs suggest misalignment between state transitions and token boundaries.
- **First Experiments**:
  1. Validate annotation and masking process on a small subset.
  2. Test "explore more, retain less" scaling principle against baselines.
  3. Stress-test ToolPRM on diverse, edge-case function calls.

## Open Questions the Paper Calls Out
- Can adaptive strategies dynamically calibrate the "explore more, retain less" trade-off based on input complexity or model confidence?
- Does the "explore more, retain less" principle generalize to other structured generation tasks, such as SQL generation or code synthesis?
- How does the reliance on function-masking during data collection affect the reward model's generalization to standard, non-masked function definitions?

## Limitations
- Function masking scheme and state transition boundaries are underspecified, making exact reproduction difficult.
- Performance gains are measured on curated benchmarks and may not reflect real-world function calling diversity.
- The "explore more, retain less" principle is validated within a narrow experimental scope and may not generalize to all structured output tasks.

## Confidence
- **High Confidence**: Overall methodology of fine-grained process reward modeling is sound and well-motivated.
- **Medium Confidence**: Implementation details for data preparation, training, and inference are clear but contain minor ambiguities.
- **Low Confidence**: Generalisability of scaling principle and robustness to real-world diversity are not fully established.

## Next Checks
1. Validate annotation and masking process on a small subset of data.
2. Test "explore more, retain less" scaling principle against baselines.
3. Stress-test ToolPRM on diverse, edge-case function calls.