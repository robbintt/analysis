---
ver: rpa2
title: 'ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search'
arxiv_id: '2507.17096'
source_url: https://arxiv.org/abs/2507.17096
tags:
- zorms-lfd
- control
- problems
- loss
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ZORMS-LfD, a method for learning optimal control
  problems from expert demonstrations using zeroth-order random matrix search. Unlike
  existing gradient-based methods that require smooth loss functions, ZORMS-LfD uses
  random matrix oracles to handle nonsmooth learning landscapes and constraints in
  both continuous and discrete time.
---

# ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search

## Quick Facts
- arXiv ID: 2507.17096
- Source URL: https://arxiv.org/abs/2507.17096
- Authors: Olivia Dry; Timothy L. Molloy; Wanxin Jin; Iman Shames
- Reference count: 33
- Primary result: ZORMS-LfD learns optimal control parameters from demonstrations using zeroth-order random matrix search, achieving over 80% reduction in compute time while matching or surpassing state-of-the-art methods on both unconstrained and constrained problems.

## Executive Summary
This paper introduces ZORMS-LfD, a novel method for Learning from Demonstrations (LfD) that uses zeroth-order random matrix search to learn optimal control parameters. Unlike gradient-based approaches that require smooth loss functions, ZORMS-LfD handles nonsmooth learning landscapes and constraints through random matrix oracles. The method iteratively updates parameters by solving forward optimal control problems with perturbed parameters and computing smoothed loss gradients via random matrix sampling.

The approach demonstrates significant computational advantages, achieving over 80% reduction in compute time compared to gradient-based methods while maintaining or improving loss performance. ZORMS-LfD shows robustness to noisy measurements and scales effectively to large parameter spaces, making it particularly valuable for constrained problems where specialized methods do not exist. The method's complexity bounds provide guidance for hyperparameter selection and convergence guarantees.

## Method Summary
ZORMS-LfD solves inverse optimal control problems by learning parameters θ that make optimal control solutions match expert demonstrations. The method uses a zeroth-order random matrix oracle that approximates potentially nonsmooth loss gradients without requiring differentiability. At each iteration, symmetric random matrices from the Gaussian Orthogonal Ensemble (GOE) are sampled, and two forward optimal control problems are solved: one at the current parameters and one at perturbed parameters. The oracle computes a smoothed gradient estimate from these two loss values, and parameters are updated via projected gradient descent. The method handles both continuous and discrete time problems with constraints, returning the parameter set with minimum loss across all iterations.

## Key Results
- Achieved over 80% reduction in compute time compared to CPDP on unconstrained continuous-time problems while maintaining comparable loss performance
- Outperformed Nelder-Mead on constrained problems where no specialized method exists
- Demonstrated robustness to noisy measurements with up to 30% noise levels
- Successfully scaled to human motion learning tasks with 21-dimensional parameter spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The zeroth-order random matrix oracle approximates potentially nonsmooth loss gradients without requiring differentiability of costs, constraints, or dynamics.
- Mechanism: At each iteration, sample a symmetric random matrix $MU_k$ from the Gaussian Orthogonal Ensemble (GOE), then compute $O_\mu(\hat{\theta}_k, MU_k) = \mu^{-1}(L(\hat{\theta}_k + \mu MU_k) - L(\hat{\theta}_k))MU_k$. This constructs a smoothed estimate of directional derivatives in random directions, with $\mu$ controlling smoothing degree.
- Core assumption: The loss function $L$ is Lipschitz continuous (but potentially nondifferentiable) with constant $\lambda(L)$.
- Evidence anchors:
  - [abstract]: "uses random matrix oracles to handle nonsmooth learning landscapes and constraints"
  - [Section V-A]: "The oracle (5) can be interpreted as constructing a smoothed estimate of the loss function L by (iteratively) examining its directional derivatives in the random directions"
  - [corpus]: Related work on randomized smoothing for nonsmooth optimization supports this approach (Maass et al., 2022; Suh et al., 2022).
- Break condition: Loss function is not Lipschitz continuous, or $\mu$ is set too large relative to $\lambda(L)$ causing excessive smoothing bias.

### Mechanism 2
- Claim: Parallel forward optimal control solves enable faster iteration than gradient-based methods despite requiring two solves per oracle evaluation.
- Mechanism: Each oracle evaluation requires solving (1) with $\theta = \hat{\theta}_k$ and (2) with $\theta = \hat{\theta}_k + \mu MU_k$. These are independent problems enabling embarrassingly parallel computation, whereas gradient methods like CPDP require integrating coupled ODEs sequentially.
- Core assumption: Forward optimal control problems can be solved efficiently.
- Evidence anchors:
  - [Section V-A]: "these optimal control problems are independent, enabling their solution in a (embarrassingly) parallel manner"
  - [Section VII-A]: "each iteration of ZORMS-LfD is over 80% faster than CPDP, despite CPDP involving the solution of one optimal control problem... whilst ZORMS-LfD involves the solution of two"
  - [corpus]: Weak direct evidence; related papers focus on convergence properties rather than parallelization benefits.
- Break condition: Forward optimal control solver is slow or unstable; parallelization infrastructure unavailable.

### Mechanism 3
- Claim: Block-diagonal matrix parameterization with GOE sampling provides tighter complexity bounds than vector or full-matrix alternatives.
- Mechanism: By exploiting block-diagonal structure of $\theta = \text{blkdiag}(\theta_1, \ldots, \theta_\rho)$, the fourth moment $m_4$ used in complexity bounds depends on both total dimension and block structure. This yields less conservative bounds than treating parameters as vectors or full matrices.
- Core assumption: Parameters can be represented as symmetric matrices (always possible via diagonal construction from vectors).
- Evidence anchors:
  - [Section V-B, Figure 1c]: "our bound is less conservative than those in [24] and [26], which follows from exploiting the block diagonal structure"
  - [Propositions 1-2]: Complexity bounds explicitly depend on $m_4$ computed from block structure
  - [corpus]: Related zeroth-order optimization work (Nesterov & Spokoiny, 2015) uses vector parameterization with more conservative bounds.
- Break condition: Parameter space has no natural block structure; blocks have highly disparate scales requiring normalization.

## Foundational Learning

- Concept: **Inverse Optimal Control (IOC) / Inverse Reinforcement Learning**
  - Why needed here: ZORMS-LfD solves the bilevel problem of finding parameters $\theta$ such that optimal control solutions match demonstrations. Understanding this two-level structure is essential.
  - Quick check question: Given a demonstration trajectory, what are we trying to infer about the underlying optimization problem?

- Concept: **Zeroth-Order Optimization / Derivative-Free Optimization**
  - Why needed here: The method uses only function evaluations (loss values), not gradients. This is the key distinction from DiffMPC, PDP, and other first-order methods.
  - Quick check question: Why might a zeroth-order method outperform a gradient-based method even when gradients exist?

- Concept: **Gaussian Orthogonal Ensemble (GOE)**
  - Why needed here: Random matrices drawn from GOE define the search directions. Understanding their distribution properties (independent entries with specific variances) is needed to implement the oracle correctly.
  - Quick check question: What are the variance parameters for diagonal vs. off-diagonal entries in a GOE matrix?

## Architecture Onboarding

- Component map:
  Oracle module -> Forward OCP solver -> Loss evaluator -> Parameter update -> Projected onto feasible set

- Critical path:
  1. Initialize $\hat{\theta}_0 \in \Theta$
  2. Generate $MU_k$ from GOE blocks
  3. Solve two forward OCPs (parallel): $\theta = \hat{\theta}_k$ and $\theta = \hat{\theta}_k + \mu MU_k$
  4. Compute oracle $O_\mu$ using both loss values
  5. Project update: $\hat{\theta}_{k+1} = P_\Theta[\hat{\theta}_k - \alpha_k O_\mu]$
  6. After $N$ iterations, return $\hat{\theta}$ with minimum loss

- Design tradeoffs:
  - $\mu$ (precision): Smaller $\mu$ reduces smoothing bias but increases sensitivity to numerical errors; paper suggests $\mu \leq \epsilon / (\lambda(L)\sqrt{4m_2})$
  - $N$ (iterations): Scales as $O(m_4 \lambda^2(L) \bar{r}^2 \epsilon^{-2})$ for convex case; larger parameter spaces require more iterations
  - Parallel vs. sequential: Parallel OCP solves reduce wall-clock time but require 2× solver instances

- Failure signatures:
  - Loss oscillates without decreasing: $\alpha_k$ too large; reduce step size
  - Convergence to poor local minimum: Initial $\hat{\theta}_0$ far from $\theta^*$; try multiple initializations
  - Gradient estimate has high variance: GOE sampling may need multiple samples per iteration (not in base algorithm)
  - Constraints violated in learned OCP: Check projection $P_\Theta$ correctly enforces parameter constraints

- First 3 experiments:
  1. **Unconstrained Cartpole (discrete-time)**: Start with 4D state, 1D control, $p=7$ parameters. Compare against PDP baseline using same $\alpha$ and $\hat{\theta}_0$. Target: match Table III loss ~0.016 within 90 iterations.
  2. **Constrained Quadrotor (continuous-time)**: 13D state, 4D control, $p=11$ parameters with path constraints. Compare against Nelder-Mead. Test noise robustness with $\nu \in \{0, 0.1, 0.2, 0.3\}$ per Figure 5.
  3. **Scaling test**: Implement on human motion task (21D parameter space). Verify loss decreases within 10 iterations per Figure 6. Check that block-diagonal GOE sampling correctly handles $\rho$ separate matrix blocks.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- The paper does not specify complete benchmark problem formulations (dynamics, costs, constraints only referenced to external papers), requiring significant external knowledge for implementation
- Hyperparameter tuning procedures and exact values for $\mu$, $\alpha_k$, and initializations are unspecified across different problem types
- The claimed computational speedup may not generalize to all forward optimal control solver implementations or problem scales

## Confidence
- **High confidence**: The zeroth-order random matrix oracle mechanism for nonsmooth optimization, the theoretical complexity bounds for convex/strongly convex cases, and the parallelization advantage over gradient-based methods
- **Medium confidence**: The empirical performance claims (loss matching/surpassing baselines, compute time reduction) given the implementation-dependent nature of forward OCP solving and hyperparameter sensitivity
- **Low confidence**: The exact hyperparameter selection guidance from complexity bounds and the noise robustness claims without detailed sensitivity analysis

## Next Checks
1. **Hyperparameter sensitivity**: Systematically vary $\mu$ and $\alpha_k$ on the Cartpole problem to identify optimal ranges and quantify sensitivity to initial parameter choices
2. **Solver dependency**: Implement ZORMS-LfD with two different forward OCP solvers (e.g., direct collocation vs. shooting method) to assess robustness to solver choice and quantify the parallel computation speedup
3. **Scalability verification**: Test the block-diagonal GOE sampling and complexity bounds on parameter spaces with varying block structures (e.g., increasing $\rho$ while keeping total $p$ fixed) to validate the theoretical scaling predictions