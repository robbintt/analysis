---
ver: rpa2
title: 'Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark
  for Vulnerability Detection'
arxiv_id: '2508.04448'
source_url: https://arxiv.org/abs/2508.04448
tags:
- code
- static
- large
- language
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic benchmark comparing three static
  code analysis tools (SonarQube, CodeQL, SnykCode) with three large language models
  (GPT-4.1, Mistral Large, DeepSeek V3) for vulnerability detection in C projects.
  The study evaluates detection accuracy (precision, recall, F1-score), analysis latency,
  and developer effort using a curated dataset of ten real-world projects with 63
  embedded vulnerabilities.
---

# Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection

## Quick Facts
- arXiv ID: 2508.04448
- Source URL: https://arxiv.org/abs/2508.04448
- Authors: Damian Gnieciak; Tomasz Szandala
- Reference count: 40
- Three static code analysis tools were systematically benchmarked against three large language models for vulnerability detection in C# projects.

## Executive Summary
This paper presents a systematic benchmark comparing static code analysis tools (SonarQube, CodeQL, SnykCode) with large language models (GPT-4.1, Mistral Large, DeepSeek V3) for vulnerability detection in C# projects. The study evaluates detection accuracy, analysis latency, and developer effort using a curated dataset of ten real-world projects with 63 embedded vulnerabilities. Large language models significantly outperformed static tools in recall and F1-scores, achieving 0.797, 0.753, and 0.750 respectively versus 0.260, 0.386, and 0.546 for the static tools. However, LLMs generated more false positives and mislocated issues at line/column granularity due to tokenization artifacts. The results suggest that while LLMs excel at broad, context-aware vulnerability detection, their noisier output and imprecise localization limit standalone use in safety-critical scenarios. The study recommends a hybrid pipeline using LLMs for early triage and static tools for high-assurance verification, providing an open benchmark for reproducible future research.

## Method Summary
The study compares three static analysis tools (SonarQube, CodeQL, SnykCode) with three large language models (GPT-4.1, Mistral Large, DeepSeek V3) using a curated dataset of 10 real-world C# projects containing 63 known vulnerabilities. LLMs were accessed via GitHub Models API using a custom C# tool (`ProjectAnalyzer`) with a specific zero-shot prompt, while static tools were run with default configurations. Results were harmonized into SARIF format for consistent metric calculation, with evaluation based on precision, recall, F1-score, false positive rate, and execution latency.

## Key Results
- LLMs achieved significantly higher recall (0.797, 0.753, 0.750) compared to static tools (0.260, 0.386, 0.546)
- GPT-4.1 achieved the highest F1-score (0.797) among all tools tested
- All LLMs mislocated vulnerabilities at line/column granularity due to tokenization artifacts
- LLMs generated more false positives, particularly regarding outdated dependency claims

## Why This Works (Mechanism)

### Mechanism 1: Contextual Semantic Reasoning vs. Rule Matching
LLMs achieve higher recall than static tools by inferring vulnerability intent across broad code contexts, whereas static tools are constrained by predefined rules and data-flow patterns. LLMs treat code as natural language, allowing them to recognize semantic vulnerability patterns that may not trigger a specific hardcoded rule in a static analyzer.

### Mechanism 2: Tokenization-Induced Spatial Displacement
LLMs consistently fail to report precise line and column numbers because the BPE (Byte Pair Encoding) tokenization process destroys the character-level mapping required for precise SARIF coordinates. Transformers operate on tokens, not raw characters, resulting in mislocated issues.

### Mechanism 3: Generative Hallucination in Dependency Analysis
LLMs generate significant false positives by hallucinating version constraints rather than verifying them against a deterministic database. The model generates the concept of a warning because such statements are common in its training distribution, even if the specific library version in the prompt is actually current.

## Foundational Learning

- **Concept: Precision-Recall Trade-off**
  - Why needed here: You cannot evaluate these tools on "accuracy" alone. The paper shows LLMs have high Recall but lower Precision, while Static tools have high Precision but low Recall.
  - Quick check question: If a tool reports 10 vulnerabilities and 9 are false positives, is it useless? (Answer: It depends on the cost of missing the 1 real vulnerability versus the cost of reviewing 10 items).

- **Concept: SARIF (Static Analysis Results Interchange Format)**
  - Why needed here: This is the "lingua franca" the authors use to compare disparate tools. Understanding the JSON structure is necessary to diagnose why LLMs failed the localization test.
  - Quick check question: Which field in the SARIF schema did the LLMs fail to populate correctly? (Answer: The `region` object, specifically `startLine`/`startColumn`).

- **Concept: Context Windows & Tokenization**
  - Why needed here: The study attributes the success of GPT-4.1 partially to its massive 1M token context, allowing it to "see" whole projects. Conversely, tokenization is identified as the root cause for the localization failure.
  - Quick check question: Why does a larger context window help in vulnerability detection? (Answer: It allows the model to trace data flow across multiple files, whereas smaller windows require chunking which breaks the context).

## Architecture Onboarding

- **Component map:** C# Source Projects -> ProjectAnalyzer (custom tool) -> Prompt Construction -> LLMs/ST static analysis -> JSON Output -> SARIF Harmonization -> Metric Calculation

- **Critical path:**
  1. Prompt Construction: Merging multi-file C# code into a single prompt payload
  2. Execution: Sending payload to LLM vs running binary scan for SAST
  3. Harmonization: Converting LLM text responses into structured SARIF JSON for metric calculation

- **Design tradeoffs:**
  - Cost vs. Privacy: DeepSeek V3 is cheapest but has privacy risks; GPT-4.1 is expensive but offers better data controls
  - Latency vs. Depth: CodeQL provides deep analysis but takes 3-4x longer than LLMs

- **Failure signatures:**
  - Localization Drift: Vulnerability exists in function `Foo` (Line 50), but LLM reports Line 60
  - Package Hallucination: LLM flags `Microsoft.Data.Sqlite 9.0.5` as outdated when it is actually the latest version
  - Alert Fatigue: High volume of false positives from LLMs may lead developers to ignore results

- **First 3 experiments:**
  1. Localization Test: Run `ProjectAnalyzer` on a single file with an obvious SQLi. Compare the `startLine` in the LLM's SARIF output against the actual line number to measure drift.
  2. Hallucination Check: Provide the LLM with a `csproj` file containing up-to-date packages. Verify if the model still generates warnings for "outdated dependencies."
  3. Hybrid Pipeline Simulation: Run a small project through SnykCode (high precision) and GPT-4.1 (high recall). Manually intersect the results to identify "High Confidence" vulnerabilities vs "Triage Required."

## Open Questions the Paper Calls Out

- How can a hybrid system effectively combine large language models for high-recall triage with static analysis tools for high-precision verification?
  - Basis: The authors explicitly state in the "Future Works" section that a "direction could be an attempt to create a hybrid solution that uses both classical tools and language models."
  - Why unresolved: The study benchmarks the tools independently and only theoretically recommends a pipeline without implementing or testing the interaction.

- To what extent can prompt engineering strategies reduce the rate of false positives and hallucinations in LLM-based vulnerability detection?
  - Basis: The authors list "prompt engineering, i.e. manipulating the content of queries" as a specific aspect "worth exploring" in the "Future Works" section.
  - Why unresolved: The experiment utilized a fixed system prompt and did not investigate how varying the prompt might optimize the precision-recall trade-off.

- Can tokenization artifacts be corrected to enable LLMs to report vulnerability locations with the precise line-and-column granularity required by standard formats like SARIF?
  - Basis: The discussion notes that "all language models mislocate issues at line-or-column granularity due to tokenisation artefacts," which limits their standalone utility.
  - Why unresolved: The paper identifies the root cause and symptom but does not propose or test a technical solution for precise code localization.

## Limitations
- The study relies on a relatively small dataset (10 projects, 63 vulnerabilities) that may not generalize to larger codebases or other languages
- Static tool performance may be sensitive to rule configurations not fully documented beyond "default settings"
- The localization failure in LLMs, while attributed to tokenization artifacts, could also reflect prompt engineering limitations that weren't exhaustively explored

## Confidence

- **High Confidence:** LLM superiority in recall and F1-scores over static tools - this is directly measured and statistically clear from the reported metrics
- **Medium Confidence:** Tokenization as the primary cause of localization errors - while the mechanism is plausible and documented, alternative explanations weren't ruled out
- **Low Confidence:** Generalization to safety-critical scenarios - the study's hybrid recommendation assumes static tools provide "high-assurance verification," but this wasn't empirically validated

## Next Checks
1. **Cross-Language Validation:** Replicate the benchmark with Python/JavaScript projects to test whether LLM advantages hold across programming paradigms beyond C#.
2. **Localization Error Quantification:** Measure the distribution of localization drift (in lines) across all LLM findings to determine if errors are systematic or random.
3. **Hybrid Pipeline Effectiveness:** Implement the proposed LLM-first/static-verification pipeline on a separate test set and measure whether the hybrid approach achieves both high recall AND high precision.