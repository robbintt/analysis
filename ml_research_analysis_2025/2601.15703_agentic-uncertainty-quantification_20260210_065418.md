---
ver: rpa2
title: Agentic Uncertainty Quantification
arxiv_id: '2601.15703'
source_url: https://arxiv.org/abs/2601.15703
tags:
- confidence
- system
- agent
- reflection
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dual-process framework for uncertainty-aware
  agents that dynamically balances fast execution with reflective correction. The
  method addresses the "spiral of hallucination" problem by transforming verbalized
  uncertainty into active control signals.
---

# Agentic Uncertainty Quantification

## Quick Facts
- **arXiv ID:** 2601.15703
- **Source URL:** https://arxiv.org/abs/2601.15703
- **Reference count:** 40
- **Primary result:** Dual-process framework achieves 74.3% success rate on ALFWorld (+10.7% over baseline) and 42.9% on WebShop (+13.6%)

## Executive Summary
This paper introduces a dual-process framework for uncertainty-aware agents that dynamically balances fast execution with reflective correction. The method addresses the "spiral of hallucination" problem by transforming verbalized uncertainty into active control signals. System 1 (Uncertainty-Aware Memory) propagates confidence and semantic explanations to constrain future decisions, while System 2 (Uncertainty-Aware Reflection) uses these explanations as rational cues to trigger targeted corrections only when needed. Extensive experiments across ALFWorld, WebShop, and DeepResearch benchmarks demonstrate superior performance while maintaining better calibration and trajectory-level discrimination.

## Method Summary
AUQ implements a dual-process cognitive architecture where System 1 performs fast, uncertainty-aware decision-making by preserving verbalized confidence and explanations in context, while System 2 provides targeted reflection when confidence drops below threshold. The framework uses the minimum confidence across trajectory history as a reliability signal, enabling the agent to identify and correct errors before they propagate. Reflection is triggered only when necessary, using the specific semantic explanation of uncertainty as a rational cue for efficient, targeted correction rather than generic self-correction.

## Key Results
- Achieves 74.3% success rate on ALFWorld (+10.7% over baseline)
- Achieves 42.9% success rate on WebShop (+13.6% over baseline)
- Demonstrates better calibration with lower Brier scores and improved AUC-ROC across multiple benchmarks
- Shows robust gains across multiple model families including GPT-5.1, Gemini-2.5-Pro, and open-source LLMs

## Why This Works (Mechanism)

### Mechanism 1: Soft Cognitive Constraint via Attention
Retaining verbalized uncertainty in the context window implicitly regularizes the model's policy, shifting it from exploitation to exploration when risk is detected. System 1 appends confidence and explanation to history, and the Transformer's self-attention mechanism attends to these tokens in subsequent steps. If the explanation describes a knowledge gap, attention weights suppress high-commitment actions and upweight information-gathering actions. Core assumption: models possess sufficient latent capability to condition generation on explicit uncertainty tokens.

### Mechanism 2: Targeted Inverse Optimization
Using the specific semantic explanation of uncertainty as a prompt constraint significantly improves reflection efficiency compared to generic self-correction. When confidence drops below threshold, System 2 injects the specific explanation into the reflection prompt, transforming reflection from open-ended search into targeted inverse problem: finding an action that resolves the specific gap identified. Core assumption: verbalized explanation accurately identifies the root cause of uncertainty rather than a symptom or hallucination.

### Mechanism 3: Weakest-Link Propagation
Trajectory reliability is best estimated by the minimum confidence across history (Process Reliability), treating the plan as a chain that breaks at its weakest link. Standard calibration averages confidence, but AUQ posits that for long-horizon tasks, a single error propagates irreversibly. Therefore, the system evaluates trajectory using minimum confidence, enforcing strict safety margins—if any step is uncertain, the entire trajectory is flagged for intervention.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The agent operates in a POMDP where true state is latent, and only has belief state derived from history. Understanding this explains why "blind execution" fails—errors in belief state compound over time.
  - **Quick check question:** Why can't the agent simply "observe" the environment to know its true state, and how does this necessitate "Uncertainty-Aware Memory"?

- **Concept: Verbalized Uncertainty vs. Logit Probability**
  - **Why needed here:** AUQ relies on generating natural language confidence scores rather than accessing internal token logits. This distinction is vital because logit probabilities measure fluency, not factual accuracy, whereas verbalized uncertainty is a metacognitive act.
  - **Quick check question:** Why might a model have high logit probability (statistical confidence) for a statement that is factually false?

- **Concept: Inverse Problems**
  - **Why needed here:** The paper frames correction phase as "Inverse Problem"—inferring optimal action given desired outcome (reliability). This distinction guides design of reflection operator to be diagnostic rather than random.
  - **Quick check question:** How does an "Inverse Problem" approach to error correction differ from simply re-generating an answer with higher temperature?

## Architecture Onboarding

- **Component map:**
  - LLM generates action, confidence, explanation
  - Switch function checks confidence threshold
  - System 1 (UAM) stores tuple in context
  - System 2 (UAR) triggers targeted reflection if needed
  - Memory module stores explicit (confidence, explanation) pairs

- **Critical path:**
  1. Agent observes and constructs prompt from history
  2. LLM generates action, confidence, explanation
  3. Branch: if confidence ≥ threshold, execute action
  4. If confidence < threshold, trigger System 2 reflection
  5. Generate parallel reflections conditioned on explanation
  6. Select action with highest consistency-weighted score
  7. Execute and append tuple to memory

- **Design tradeoffs:**
  - Latency vs. Reliability: System 2 introduces inference latency but prevents long hallucination spirals
  - Context vs. Forgetting: Storing tuples consumes context window, mitigated by Adaptive Expansion
  - Threshold τ ≈ 0.9 is empirically derived Pareto optimal point

- **Failure signatures:**
  - Delusional Confirmation: Reflection boosts confidence but action is wrong
  - Regressive Over-Correction: System 2 overrides correct but low-confidence action
  - Looping: Vague explanations fail to converge in reflection

- **First 3 experiments:**
  1. Threshold Sensitivity Sweep: Vary τ from 0.8 to 0.95, plot Success Rate vs. Average API Calls
  2. Ablation on "Rational Cues": Compare generic reflection vs. explanation injection
  3. Memory Stress Test: Force small history window, compare UAM-Only vs. Full AUQ

## Open Questions the Paper Calls Out
None

## Limitations
- Weakest-Link Propagation Reliability: Assumes strictly sequential task dependencies, may be overly conservative for tasks with error-recovery steps
- Delusional Confirmation Risk: Reflection may optimize for non-existent constraints when explanations contain hallucinated facts
- Small Model Scalability: Attention-based soft constraint may not function in models <7B parameters, turning signal into noise

## Confidence
- **High Confidence:** Dual-process framework design, performance improvements across benchmarks, calibration results
- **Medium Confidence:** Targeted reflection efficiency, mechanism for converting verbalized uncertainty into control signals
- **Low Confidence:** Performance on small models (<7B parameters), scalability of attention-based soft constraint mechanism

## Next Checks
1. **Non-Sequential Task Adaptation:** Design experimental suite with error-recovery steps and parallelizable subtasks; compare weakest-link against alternative trajectory reliability metrics
2. **Small Model Scalability Test:** Implement AUQ on 1B-7B parameter models; test whether attention-based soft constraint still functions with verbalized uncertainty tokens
3. **Delusional Confirmation Characterization:** Create diagnostic benchmark with hallucinated explanations; measure confidence inflation frequency and test alternative reflection strategies for detection and mitigation