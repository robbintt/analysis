---
ver: rpa2
title: Exact Reformulation and Optimization for Direct Metric Optimization in Binary
  Imbalanced Classification
arxiv_id: '2507.15240'
source_url: https://arxiv.org/abs/2507.15240
tags:
- optimization
- feasible
- methods
- precision
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing precision and
  recall metrics in binary imbalanced classification (IC) problems. The key innovation
  is an exact reformulation of indicator functions that avoids the approximation errors
  inherent in existing methods.
---

# Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification

## Quick Facts
- **arXiv ID**: 2507.15240
- **Source URL**: https://arxiv.org/abs/2507.15240
- **Reference count**: 40
- **Primary result**: Exact reformulation enables gradient-based optimization of precision/recall metrics without approximation errors, outperforming smooth surrogates across multiple benchmark datasets.

## Executive Summary
This paper addresses the fundamental challenge of optimizing precision and recall metrics in binary imbalanced classification problems. The key innovation is an exact reformulation of indicator functions that avoids the approximation errors inherent in existing methods. This reformulation allows gradient-based optimization of constrained problems like fix precision optimize recall (FPOR), fix recall optimize precision (FROP), and optimize Fβ-score (OFBS). The method uses an exact penalty approach to solve the reformulated problems and is shown to outperform state-of-the-art methods across multiple benchmark datasets spanning image, text, and structured data. Experiments demonstrate consistent and often substantial improvements in both optimization performance and test-time F1 scores compared to methods based on smooth approximations.

## Method Summary
The method introduces an auxiliary variable s and a continuous piecewise linear function H_t(a,s) = s + [s + a - 1 - t]_+ - [s + a - t]_+ that is mathematically equivalent to the indicator function s - 1{a > t} = 0 under non-singularity conditions. This allows gradients to propagate through what was previously a discrete step function. The constrained optimization problems are relaxed by replacing equality constraints with inequalities (s ≤ 1{a > t} for positive samples, s ≥ 1{a > t} for negative samples), and solved using an ℓ₁ exact penalty method that guarantees constraint satisfaction with finite penalty parameters. The framework is applied to three specific formulations: Fix Precision Optimize Recall, Fix Recall Optimize Precision, and Optimize Fβ-Score, using Projected ADAM optimization with dynamic regularization to prevent singular points.

## Key Results
- Outperforms state-of-the-art methods across four benchmark datasets (Wilt, Fire, Eyepacs, ADE-Corpus-V2) with consistent F1 score improvements
- Exact penalty method guarantees constraint satisfaction with finite penalty parameters, unlike quadratic penalties requiring infinite parameters
- Reformulation enables gradient-based optimization of discrete metrics without approximation errors inherent in smooth surrogates
- Dynamic logit regularization effectively prevents singular points where gradients vanish
- Minor constraint violation (≤ 0.05) at test time attributed to finite-sample effects

## Why This Works (Mechanism)

### Mechanism 1: Exact Indicator Function Reformulation
The proposed method enables gradient-based optimization of discrete metrics without approximation errors by introducing an auxiliary variable s and a continuous piecewise linear function H_t(a,s) that is mathematically equivalent to the indicator function under non-singularity conditions. This allows gradients to propagate through what was previously a discrete step function.

### Mechanism 2: Inequality Relaxation Equivalence
Relaxing equality constraints to inequalities expands the feasible set interior, preventing numerical instability while preserving the exact global optimum. The objective functions being monotonic with respect to s ensures optimal solutions are pushed to the boundary, effectively satisfying the original equality constraint.

### Mechanism 3: ℓ₁ Exact Penalty Method
An ℓ₁ exact penalty method guarantees constraint satisfaction with finite penalty parameters, unlike quadratic penalties which require infinite parameters. The ℓ₁ nature creates a "kink" at the constraint boundary, allowing the optimizer to find feasible stationary points without needing to "heat up" the penalty to infinity.

## Foundational Learning

- **Concept**: **Clarke Subdifferentials / Non-smooth Optimization**
  - **Why needed here**: The reformulated function H_t is piecewise linear, not smooth. Standard gradients do not exist everywhere, requiring understanding of subgradient methods.
  - **Quick check question**: Can you explain why a standard gradient descent update fails on a step function, but a subgradient method works on a ReLU-like function?

- **Concept**: **Constrained Optimization & KKT Conditions**
  - **Why needed here**: The paper solves problems like "Fix Precision, Optimize Recall." Understanding Lagrangians and penalty methods is required to grasp why the algorithm increases λ over time.
  - **Quick check question**: In a standard quadratic penalty method, why must the penalty coefficient go to infinity to guarantee feasibility?

- **Concept**: **Binary Classification Metrics (Precision/Recall/Fβ)**
  - **Why needed here**: The entire paper is a "Direct Metric Optimization" (DMO) framework. You need to distinguish between optimizing a loss (Cross Entropy) vs. optimizing the metric directly.
  - **Quick check question**: Why is high accuracy misleading in an imbalanced dataset where the negative class dominates 99% of the data?

## Architecture Onboarding

- **Component map**: Backbone (f_θ) -> Auxiliary Variable (s) -> Threshold (t) -> Reformulation Layer (H_t) -> Solver (ADAM + Penalty + Projection)

- **Critical path**: Initialize s -> Forward pass f_θ -> Calculate H_t violation -> Update θ, s, t via Penalty Objective -> Check Feasibility -> Increase λ

- **Design tradeoffs**:
  - **Exactness vs. Scalability**: The method introduces N auxiliary variables and 2N constraints, creating linear memory scaling with dataset size, potentially challenging for massive datasets compared to standard ERM.
  - **L1 Penalty vs. Augmented Lagrangian**: The paper chooses L1 penalty (Exact Penalty) to guarantee feasibility with finite parameters, trading ease of stochastic extension for deterministic feasibility guarantees.

- **Failure signatures**:
  - **Singular Collapse**: If f_θ(x_i) collapses to the threshold t for many i, gradients vanish. (Mitigation: Logit Regularization in Eq 3.26).
  - **Oscillating Feasibility**: If λ is increased too fast, the optimizer may jump between feasible regions, destabilizing training.

- **First 3 experiments**:
  1. **Sanity Check (Toy Data)**: Replicate Figure 3 to verify the optimizer finds the correct "kink" in the piecewise linear surface using exact reformulation vs. sigmoid approximation.
  2. **Ablation on Regularization**: Disable the logit regularizer ψ(θ,s) and measure frequency of |f_θ(x) - t| < ε. Confirm without it, the model gets stuck on singular points.
  3. **Benchmark FPOR**: Run Fix-Precision-Optimize-Recall task on Wilt dataset. Compare "Feasibility Rate" between this ERO method and TFCO.

## Open Questions the Paper Calls Out

### Open Question 1: Metric Generalization
Can the Exact Reformulation and Optimization (ERO) framework be extended to handle non-decomposable ranking metrics like Average Precision (AP) or Normalized Discounted Cumulative Gain (NDCG), and can it be adapted for multiclass or multilabel classification? The theoretical guarantees rely on specific monotonicity properties that may not hold for ranking metrics requiring new reformulations.

### Open Question 2: Stochastic Optimization Scalability
Can stochastic optimization methods be developed to solve the ERO subproblems effectively, allowing the framework to scale to large datasets where the number of constraints scales with dataset size? The current deterministic exact penalty method cannot scale to large-scale datasets due to computational requirements of full-batch constraint evaluation.

### Open Question 3: Test-Time Feasibility Guarantees
How can the ERO framework provide rigorous guarantees for test-time feasibility despite finite-sample approximation errors inherent in empirical risk minimization? The current framework lacks a mechanism to calibrate training constraints to guarantee population-level precision/recall under distribution shifts.

## Limitations
- **Scalability concerns**: Linear memory scaling with dataset size due to N auxiliary variables and 2N constraints may limit applicability to large-scale datasets
- **Regularization sensitivity**: Dynamic logit regularization is critical but implementation details and hyperparameter sensitivity remain unclear
- **Dataset preprocessing specifics**: Exact preprocessing pipelines not fully detailed, potentially affecting reproducibility

## Confidence
- **High confidence**: Mathematical equivalence between indicator functions and H_t reformulation, monotonicity-based relaxation equivalence, and exact penalty method's feasibility guarantees
- **Medium confidence**: Empirical superiority over state-of-the-art methods across all four datasets, though exact implementation details may affect reproducibility
- **Low confidence**: Performance on truly massive datasets (>1M samples) where linear scaling of auxiliary variables may become prohibitive

## Next Checks
1. **Toy data verification**: Replicate Figure 3 to verify exact reformulation correctly navigates piecewise linear surface, comparing convergence behavior against sigmoid approximations

2. **Regularization ablation**: Systematically disable the logit regularizer and measure frequency of |f_θ(x) - t| < ε. Quantify how often the model gets stuck on singular points without regularization

3. **Constraint satisfaction monitoring**: During FPOR optimization on Wilt dataset, track "Feasibility Rate" and compare against TFCO baselines to verify constraint satisfaction performance