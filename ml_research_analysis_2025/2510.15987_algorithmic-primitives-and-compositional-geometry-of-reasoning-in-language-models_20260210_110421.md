---
ver: rpa2
title: Algorithmic Primitives and Compositional Geometry of Reasoning in Language
  Models
arxiv_id: '2510.15987'
source_url: https://arxiv.org/abs/2510.15987
tags:
- reasoning
- primitive
- primitives
- cluster
- algorithmic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for identifying and steering
  algorithmic primitives underlying LLM reasoning by clustering neural activations
  and linking them to reasoning traces. Primitive vectors are extracted using function
  vector methods and injected into residual streams to induce specific algorithmic
  behaviors.
---

# Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models

## Quick Facts
- arXiv ID: 2510.15987
- Source URL: https://arxiv.org/abs/2510.15987
- Reference count: 40
- This paper introduces a framework for identifying and steering algorithmic primitives underlying LLM reasoning by clustering neural activations and linking them to reasoning traces.

## Executive Summary
This work introduces a framework for identifying and steering algorithmic primitives underlying LLM reasoning by clustering neural activations and linking them to reasoning traces. Primitive vectors are extracted using function vector methods and injected into residual streams to induce specific algorithmic behaviors. The approach is validated across four reasoning tasks (TSP, 3SAT, AIME, graph navigation) using Phi-4, Phi-4-Reasoning, and Llama-3-8B models. Results show that primitives compose geometrically through vector arithmetic, transfer across tasks, and differ between base and reasoning-finetuned models.

## Method Summary
The methodology clusters residual stream activations (layer 17) from reasoning traces using k-means, then maps clusters to interpretable reasoning primitives through manual inspection. Primitive vectors are extracted by averaging attention head activations over tokens assigned to each cluster. These vectors are injected into residual streams at various layers during generation to induce specific algorithmic behaviors. Cross-task and cross-model evaluations test primitive transferability and compositionality through vector arithmetic.

## Key Results
- Primitive vectors extracted from clustered activations can induce specific algorithmic behaviors when injected into residual streams
- Vector arithmetic (addition/subtraction) successfully combines primitive behaviors to achieve composite task performance
- Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives compared to Phi-4-Base
- Injecting corresponding primitive vectors into Phi-4-Base induces behavioral hallmarks of the finetuned model

## Why This Works (Mechanism)

### Mechanism 1: Clustering-Based Primitive Identification
- **Claim:** Distinct algorithmic primitives correspond to separable clusters in activation space.
- **Mechanism:** K-means clustering on layer 17 residuals produces clusters that map to interpretable reasoning functions (e.g., "nearest neighbor," "verification").
- **Core assumption:** Algorithmic operations have consistent neural signatures at single layers.
- **Evidence anchors:** Abstract states "clustering neural activations and labeling their matched reasoning traces"; Section 4.2 describes k=50 clusters on layer 17 with manual mapping.

### Mechanism 2: Primitive Vector Extraction via Attention Head Averaging
- **Claim:** Averaging attention head activations yields a causal direction for the primitive.
- **Mechanism:** Mean activation across top-k attention heads over cluster tokens creates primitive vector.
- **Core assumption:** Extracted vector captures causal direction, not just correlation.
- **Evidence anchors:** Abstract mentions "primitive vectors can be combined through addition, subtraction, and scalar operations"; Section 4.2 adapts Todd et al. (2024) function vector method.

### Mechanism 3: Compositional Arithmetic in Activation Space
- **Claim:** Primitive vectors can be composed additively to induce combined behaviors.
- **Mechanism:** Linear combination of primitive vectors approximates composite behavior.
- **Core assumption:** Activation space supports linear superposition of functional primitives.
- **Evidence anchors:** Abstract mentions "cross-task and cross-model evaluations show both shared and task-specific primitives"; Section 5 demonstrates composition on Llama-3-8B.

## Foundational Learning

- **Concept: Residual Stream Interventions**
  - Why needed: All primitive vector injections operate on the residual stream.
  - Quick check: Can you explain why adding a vector to h_ℓ affects subsequent layer outputs?

- **Concept: Function Vectors (Todd et al., 2024)**
  - Why needed: Primitive extraction directly adapts function vector methodology.
  - Quick check: How does a function vector differ from a prompt embedding?

- **Concept: K-Means Clustering in High Dimensions**
  - Why needed: Primitive identification relies on clustering activations.
  - Quick check: What does cluster purity mean, and how would you assess it for this task?

## Architecture Onboarding

- **Component map:** Trace Collector -> Clustering Module -> Cluster Interpreter -> Vector Extractor -> Injector
- **Critical path:** Trace collection → clustering → interpretation → vector extraction → injection validation
- **Design tradeoffs:**
  - Layer choice: Layer 17 worked for Phi-4; other models may require different layers
  - Cluster count (k): Too few clusters merge distinct primitives; too many yield fragmentation
  - Head count: Top 35 heads balances signal and noise; reducing may lose information
- **Failure signatures:**
  - Injection produces no behavioral change → wrong layer or insufficient magnitude
  - Injection causes incoherent output → noisy cluster or excessive α
  - Cross-task transfer fails → primitives may be task-specific
- **First 3 experiments:**
  1. Replicate TSP primitive extraction on Phi-4-base; validate "nearest neighbor" cluster effect
  2. Test additive composition: extract "get first node" and "get last node" vectors; verify each, then inject sum
  3. Cross-model transfer: inject Phi-4-Reasoning verification vector into Llama-3-8B; assess verification count increase

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the compositional geometry of reasoning require non-linear operations on manifolds rather than linear vector arithmetic?
- **Basis in paper:** Authors state they "focus on linear compositions... leaving more complex interactions and potentially non-linear combinations of several vectors on manifolds for future work."
- **Why unresolved:** While linear addition works, complex reasoning may rely on curved manifolds that simple arithmetic cannot capture.
- **What evidence would resolve it:** Demonstration that non-linear manifold transformations predict or induce complex reasoning behaviors more accurately than linear vector addition.

### Open Question 2
- **Question:** Can algorithmic primitives serve as effective optimization targets for direct finetuning of LLMs?
- **Basis in paper:** Authors identify "algorithmic training and finetuning of LLMs, with algorithmic objectives" as a key future direction.
- **Why unresolved:** Paper demonstrates inference-time steering but doesn't test whether primitives as loss functions yield stable improvements.
- **What evidence would resolve it:** Experiments showing models finetuned to align with specific primitive vectors exhibit improved generalization on out-of-distribution reasoning tasks.

### Open Question 3
- **Question:** Are algorithmic primitives universal representations shared across architectures, or specific to a model's training history?
- **Basis in paper:** Authors call for "universal and task-specific primitive libraries" and note different models may require different modes of composition.
- **Why unresolved:** Cross-task transfer observed but cross-model comparisons suggest primitives might not be fully universal.
- **What evidence would resolve it:** Identification of primitives in one model that can be mathematically mapped to and successfully steer a different model architecture without performance loss.

## Limitations

- Manual cluster interpretation is subjective and may not generalize across annotators or datasets
- Layer-specific primitive extraction (layer 17) raises questions about generalizability across model architectures
- Cross-task transfer results are promising but limited in scope without systematic exploration of transfer boundaries

## Confidence

**High Confidence (8/10):**
- Primitive vectors can induce specific algorithmic behaviors when injected into residual streams
- Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives
- Vector arithmetic can combine primitive behaviors to achieve composite task performance

**Medium Confidence (6/10):**
- Primitive identification through clustering reliably captures algorithmic operations
- Cross-task transfer of primitives generalizes beyond the specific tasks tested
- Geometric compositionality of primitive vectors extends to all reasoning tasks

**Low Confidence (4/10):**
- Manual cluster interpretation provides definitive mapping to reasoning primitives
- Layer 17 is universally optimal for primitive extraction across model families
- Linear vector composition captures all relevant primitive interactions

## Next Checks

1. **Cluster Stability Analysis**: Apply clustering methodology to 10 different subsets of TSP problems and measure cluster consistency using adjusted mutual information scores to validate robustness.

2. **Layer Transferability Test**: Extract primitive vectors from multiple layers (10, 15, 17, 20) for the same reasoning traces and compare behavioral effects when injected at layer 17 to determine optimal layer choice.

3. **Cross-Annotator Validation**: Have three independent researchers label the same set of clusters from TSP traces and compute inter-annotator agreement (Cohen's kappa) to quantify subjectivity of manual interpretation.