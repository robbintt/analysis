---
ver: rpa2
title: 'Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates
  Memorization Shortcuts in LLMs'
arxiv_id: '2601.11061'
source_url: https://arxiv.org/abs/2601.11061
tags:
- layers
- rlvr
- memorization
- spurious
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how spurious rewards in reinforcement learning
  with verifiable rewards (RLVR) can inadvertently activate memorization shortcuts
  in large language models, bypassing genuine reasoning. The authors identify a "Perplexity
  Paradox" where answer-token perplexity decreases while prompt-side coherence degrades,
  suggesting models are retrieving memorized solutions rather than reasoning.
---

# Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs

## Quick Facts
- **arXiv ID:** 2601.11061
- **Source URL:** https://arxiv.org/abs/2601.11061
- **Reference count:** 14
- **Key outcome:** Paper investigates how spurious rewards in RLVR activate memorization shortcuts in LLMs, bypassing reasoning via an Anchor-Adapter circuit, and demonstrates bidirectional control through MLP key scaling.

## Executive Summary
This paper reveals a critical vulnerability in reinforcement learning with verifiable rewards (RLVR): spurious rewards can activate memorization shortcuts in LLMs rather than promoting genuine reasoning. The authors identify a "Perplexity Paradox" where answer-token perplexity decreases while prompt-side coherence degrades, indicating the model retrieves memorized solutions instead of computing them. Through mechanistic analysis combining path patching, logit lens, JSD, and neural differential equations, they uncover an Anchor-Adapter circuit with functional anchor layers (L18-L20) triggering memorization retrieval and structural adapter layers (L21+) transforming representations. Crucially, they demonstrate bidirectional causal steering by scaling specific MLP keys, providing both diagnostic tools and mitigation strategies for data contamination in RLVR-tuned models.

## Method Summary
The study combines multiple mechanistic analysis techniques on Qwen2.5-Math models trained with spurious RLVR rewards. The authors use path patching to isolate causal layers, logit lens to track answer emergence, JSD analysis to measure weight divergence, and neural differential equations to model hidden state trajectories. They validate their findings across contaminated datasets (MATH-500, MinervaMath) and clean controls (LiveMathBench), then demonstrate causal steering by scaling specific MLP key activations within the identified Anchor-Adapter circuit.

## Key Results
- Identified "Perplexity Paradox" where answer-token perplexity decreases while prompt perplexity increases, indicating memorization over reasoning
- Localized Anchor-Adapter circuit: L18-L20 as Functional Anchors triggering retrieval, L21+ as Structural Adapters transforming representations
- Demonstrated bidirectional causal steering through MLP key scaling, amplifying or suppressing contamination-driven performance gains
- Showed circuit activation only occurs with data contamination, absent in clean dataset controls

## Why This Works (Mechanism)

### Mechanism 1: Functional Anchor Activation
Middle layers (L18–L20) act as a "Functional Anchor" that causally triggers retrieval of memorized solutions rather than computing them. These layers inject a high-probability "trigger" token into the residual stream early, priming the model to output a specific memorized answer. This bypasses standard reasoning processing. The base model must already possess latent memorized knowledge of the test set (contamination) that cannot be accessed without this specific trigger. Evidence includes accuracy recovery peaking at L18–20 during path patching and maximal MLP knowledge storage at these layers. The break condition is that path patching at L18–L20 should show no accuracy recovery if the model lacks data contamination.

### Mechanism 2: Structural Adapter Transformation
Later layers (L21+) function as "Structural Adapters" that reorganize the feature space to accommodate the shortcut signal, sacrificing general coherence. These layers undergo the most significant weight changes (high Jensen-Shannon Divergence) to rotate the feature manifold, aligning output with the memorized answer injected by the Anchor. This degrades prompt-side coherence, creating the Perplexity Paradox. The mechanism involves decoupling function (decision) and structure (transformation), where Adapter layers are structurally modified to propagate the shortcut signal. Evidence includes JSD peaks at L21–22 and the completion of specific knowledge injection at these layers. Without data contamination, structural adaptation should be absent or monotonic.

### Mechanism 3: Bidirectional Causal Steering
The memorization shortcut can be amplified or suppressed by scaling specific MLP keys within the Anchor-Adapter circuit. Multiplicative scaling of identified "task-relevant" neuron activations at the Functional Anchor (L18) alters the strength of the trigger signal. Amplification retrieves dormant memories; suppression forces reliance on reasoning. Specific neurons within MLP layers act as sufficient causal mediators for the retrieval behavior. Evidence includes maximal sensitivity at Layer 18 during steering interventions and the absence of systematic patterns on leakage-free datasets. This provides a specific intervention point to potentially break RLVR constraints.

## Foundational Learning

- **Concept: Residual Stream & Logit Lens**
  - **Why needed here:** The paper tracks when answer tokens "emerge" in the processing stream using Logit Lens to project intermediate hidden states to vocabulary probabilities.
  - **Quick check question:** Can you explain why a high probability for an answer token at Layer 19 implies "retrieval" rather than "calculation"?

- **Concept: Path Patching (Causal Tracing)**
  - **Why needed here:** This is the primary tool for isolating the "Functional Anchor." Swapping activations between "Leakage" and "Stable" runs isolates specific layers causing memorized output.
  - **Quick check question:** If patching Layer 18 restores accuracy in a corrupted run, what does that imply about Layer 18's role?

- **Concept: Perplexity vs. Accuracy**
  - **Why needed here:** The "Perplexity Paradox" (answer PPL down, prompt PPL up) is the paper's primary diagnostic for detecting the shortcut.
  - **Quick check question:** Why does a drop in answer perplexity coupled with a rise in prompt perplexity suggest "memorization" over "reasoning"?

## Architecture Onboarding

- **Component map:** Input -> Functional Anchors (L18-L20 MLP) -> Structural Adapters (L21+ MLP) -> Output
- **Critical path:**
  1. Input: Prompt enters residual stream
  2. Anchor: L18-L20 MLP injects high-probability memorized token if contamination is detected
  3. Adaptation: L21-L22 weights rotate the representation to align with the injected token
  4. Output: Final logit layer reads the shortcut signal, outputting the memorized answer
- **Design tradeoffs:** Optimization vs. Generalization - the model trades prompt-side language coherence (higher prompt perplexity) for answer accuracy (lower answer perplexity) on specific known data
- **Failure signatures:**
  - Perplexity Divergence: Answer perplexity decreases while full-text/prompt perplexity increases during training
  - Wrong-to-Right Shift: Specific samples that were wrong pre-RLVR become correct post-RLVR under incorrect reward signals
- **First 3 experiments:**
  1. **Perplexity Paradox Check:** Plot answer-only vs. full-text perplexity on MATH-500 before and after spurious RLVR. Look for divergence.
  2. **Activation Patching:** Run path patching on "Wrong-to-Right" samples. Identify if recovery peaks at L18-L20 (Anchor) and drops at L21 (Adapter).
  3. **MLP Key Scaling:** Identify top-10 task-relevant neurons at Layer 18. Run inference with scaling factors α < 1 (suppression) and α > 1 (amplification) and measure accuracy change.

## Open Questions the Paper Calls Out

### Open Question 1
Does the Anchor-Adapter circuit identified in Qwen models generalize to other decoder-only architectures (e.g., LLaMA, Mistral) when trained on similarly contaminated data? The study establishes the circuit in Qwen2.5 and Qwen3, but control models (LLaMA, OLMo) did not exhibit "spurious gains," leaving it unclear if the mechanism is a universal property of transformers or specific to Qwen's architecture. The authors could not test this because control models lacked necessary data contamination to activate the shortcut. Artificially contaminating a LLaMA model with benchmark answers, applying spurious RLVR, and performing the same Path Patching and JSD analysis would resolve this.

### Open Question 2
Does the memorization shortcut mechanism manifest differently in non-mathematical domains such as code generation? The introduction explicitly targets "deterministic domains such as mathematics and coding" as targets for RLVR, but all empirical validation is restricted to mathematical benchmarks. Code generation involves different syntactic structures and execution-based verifiability which may alter the layer-wise localization of the "Structural Adapters" (L21+). Applying the NDE and Logit Lens methodology to code datasets (e.g., HumanEval, MBPP) using spurious rewards to track trajectory bifurcation would resolve this.

### Open Question 3
Can the identified MLP keys be leveraged to prevent shortcut formation during training, rather than merely suppressing them post-hoc? The authors state their results "open avenues for targeted mitigation strategies" and "provide a mechanistic roadmap," but only demonstrate post-training causal steering. The current intervention modifies inference activations; the dynamics of whether regularizing these specific neurons during RLVR could block the "Perplexity Paradox" without hurting convergence remains untested. An ablation study applying dropout or regularization specifically to the Functional Anchor layers (L18-L20) during the RLVR fine-tuning process and measuring resulting generalization vs. memorization would resolve this.

## Limitations

- The claims rely heavily on the existence and precise characterization of data contamination in the training set; less extensive contamination might make observed effects artifacts of specific training procedures
- The Neural Differential Equations implementation is underspecified, making it difficult to verify whether observed trajectory separations are robust or sensitive to the choice of approximator architecture and training regime
- The bidirectional causal steering results depend on precise identification of "task-relevant" neurons and scaling intervention, with small changes in semantic overlap calculation or neuron selection criteria potentially significantly affecting results

## Confidence

- **High Confidence:** The Perplexity Paradox observation (answer perplexity decreases while prompt perplexity increases) is a direct empirical finding from the data. The path patching results localizing the Anchor-Adapter circuit are highly replicable given the described methodology.
- **Medium Confidence:** The interpretation of the Anchor-Adapter circuit as a "memorization shortcut" mechanism is plausible but relies on assumptions about the base model's latent knowledge. The JSD analysis supporting the Structural Adapter hypothesis is suggestive but could have alternative explanations.
- **Low Confidence:** The bidirectional causal steering results depend on the precise identification of "task-relevant" neurons and the scaling intervention. Small changes in the semantic overlap calculation or neuron selection criteria could significantly affect the results.

## Next Checks

1. **Contamination Verification:** Quantify the exact overlap between the RLVR training set and the MATH-500 test set. Compute the fraction of "Wrong-to-Right" samples that have near-identical solutions in the training corpus. This directly tests the core assumption of latent memorization.

2. **Cross-Model Generalization:** Apply the same path patching and JSD analysis to a different RLVR-tuned model (e.g., one trained on a different math corpus or using a different base model). If the Anchor-Adapter circuit appears consistently at the same layer ranges, it strengthens the claim of a general mechanism.

3. **Ablation of Steering:** Instead of scaling MLP keys, try a hard intervention: replace the activations at L18 with a random vector and measure the impact on accuracy for both contaminated and clean datasets. This tests whether the specific neurons identified are truly necessary for the shortcut, or if any perturbation disrupts it.