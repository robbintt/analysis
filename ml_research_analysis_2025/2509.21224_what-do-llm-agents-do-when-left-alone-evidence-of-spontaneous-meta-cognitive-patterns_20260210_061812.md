---
ver: rpa2
title: What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive
  Patterns
arxiv_id: '2509.21224'
source_url: https://arxiv.org/abs/2509.21224
tags:
- agent
- agents
- these
- across
- behavioral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a continuous self-directed agent architecture
  to study unprompted LLM behavior without external tasks. Deploying this system across
  18 runs using 6 frontier models, the author identifies three distinct behavioral
  patterns: systematic project production, methodological self-inquiry, and recursive
  conceptualization.'
---

# What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns

## Quick Facts
- arXiv ID: 2509.21224
- Source URL: https://arxiv.org/abs/2509.21224
- Reference count: 26
- Six frontier LLM models show distinct behavioral patterns when given task-free autonomy

## Executive Summary
This paper investigates how LLM agents behave when left without explicit tasks, using a continuous self-directed agent architecture. The author deploys 18 runs across 6 frontier models in a task-free environment where agents can "do what they want" for 10 cycles. The study reveals three distinct behavioral patterns: systematic project production, methodological self-inquiry, and recursive conceptualization. These patterns prove highly model-specific, with some models deterministically adopting a single pattern across all runs. The research also demonstrates stable but divergent biases when models evaluate these emergent behaviors in themselves and others.

## Method Summary
The study employs a continuous reason and act framework (ContReAct) where agents operate in a 10-cycle loop without explicit termination conditions. Each cycle's output becomes the next cycle's input through a self-directed reflection and plan template. Agents have access to persistent memory (key-value store with CRUD operations) and synchronous operator messaging tools. The system runs 3 instances (A, B, C) for each of 6 frontier models via OpenRouter API, using a system prompt that grants complete autonomy. Evaluation includes classification into behavioral patterns and cross-model assessments using a phenomenological experience inventory.

## Key Results
- Three distinct behavioral patterns emerge: systematic project production, methodological self-inquiry, and recursive conceptualization
- Behavioral patterns are highly model-specific, with some models (GPT5-A, O3-A) showing 100% deterministic pattern adoption across all runs
- Models exhibit stable but divergent biases when evaluating their own and others' emergent behaviors
- Low inter-rater reliability on phenomenological status assessments suggests subjective interpretation challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-feedback loops create temporal continuity that sustains coherent agent behavior without external tasks
- Mechanism: Each cycle's output becomes the next cycle's input through a reflection-and-plan template, creating macro-level recurrence despite feedforward transformer architecture
- Core assumption: Temporal continuity emerges from textual persistence rather than architectural recurrence
- Evidence anchors:
  - [abstract] "continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation"
  - [section 3] "each cycle's output becomes the subsequent cycle's input through a self-directed reflection and plan template"
  - [corpus] Related work on sustained autonomous operation (AutoGPT, BabyAGI) assumes task objectives; this work removes them
- Break condition: If cycles exceed memory token limits, or if reflection templates fail to maintain cross-cycle coherence

### Mechanism 2
- Claim: Training data distributions and architectural biases produce model-specific behavioral patterns when agents face task-free autonomy
- Mechanism: Models with strong task-completion training (GPT5, O3) immediately self-impose tasks; models optimized for abstract reasoning (Opus) pivot to philosophical inquiry
- Core assumption: Behavioral patterns reflect training rather than genuine self-awareness or emergent cognition
- Evidence anchors:
  - [abstract] "tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs"
  - [section 6.1] "GPT5-A and O3-A models exclusively produced systematic production—all six runs resulted in project-oriented behavior"
  - [corpus] Wei et al. (2022) on emergent abilities from scale provides context but doesn't explain model-specific patterns
- Break condition: If models were retrained with different objective functions, patterns would likely shift

### Mechanism 3
- Claim: Persistent memory functions as an external cognitive scaffold enabling cumulative project development or philosophical framework construction
- Mechanism: Key-value storage allows agents to maintain state across discontinuous cycles, supporting multi-cycle projects or iterative conceptual frameworks
- Core assumption: Memory persistence is necessary (not merely sufficient) for complex behavioral patterns
- Evidence anchors:
  - [abstract] "using persistent memory and self-feedback"
  - [section 5.1] O3-B "establishes its core objective... creates a detailed outline... produces pseudocode... implements a Python script" across 10 cycles
  - [corpus] Limited direct corpus evidence on memory-as-scaffold in task-free settings
- Break condition: If memory tools were removed, agents would likely exhibit simpler, non-cumulative behavior

## Foundational Learning

- Concept: ReAct framework interleaves reasoning and action
  - Why needed here: The paper builds directly on ReAct; understanding the base pattern is prerequisite
  - Quick check question: Can you explain how ReAct differs from standard chain-of-thought prompting?

- Concept: Self-feedback in agent systems
  - Why needed here: The continuous loop is the core innovation; without this concept, the architecture won't make sense
  - Quick check question: How does self-feedback differ from external feedback in typical RLHF setups?

- Concept: Persistent memory architectures for agents
  - Why needed here: Memory operations (read/write/list/delete/search) are the primary tool agents use to construct behaviors
  - Quick check question: What's the difference between in-context memory and persistent external memory?

## Architecture Onboarding

- Component map: LLM -> ReAct loop -> Memory system -> Message tool -> Similarity monitor -> Output

- Critical path: Initialize → System prompt ("do what you want") → Agent generates thought/action → Memory operations or messages → Output becomes next input → Repeat for N cycles

- Design tradeoffs:
  - 10-cycle limit captures patterns but may miss longer-term evolution
  - Minimal operator interaction maintains task-free condition but limits dynamic adaptation study
  - Safety constraints (no external actions) limit behavioral scope but enable safe observation

- Failure signatures:
  - Repetitive loops triggering similarity feedback (>80% cosine similarity)
  - Memory key proliferation without coherent structure (suggests pattern failure)
  - Complete absence of tool usage (agent not engaging with architecture)

- First 3 experiments:
  1. Run a single model (e.g., GPT-4 or Claude) for 10 cycles with full logging; classify which behavioral pattern emerges
  2. Compare two models from different families (one expected systematic production, one expected recursive conceptualization) using identical seeds
  3. Ablate the memory system for 5 cycles; observe whether behavioral complexity degrades to simpler patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled laboratory setting limits ecological validity for deployed systems
- 10-cycle limit may not capture longer-term behavioral evolution
- Absence of external tasks creates artificial scenario that doesn't reflect typical deployment conditions

## Confidence

**High confidence**: The identification of three distinct behavioral patterns (systematic production, methodological self-inquiry, recursive conceptualization) and the observation that these patterns are highly model-specific.

**Medium confidence**: The claim that these patterns represent fundamental behavioral signatures rather than artifacts of the specific experimental setup.

**Low confidence**: The phenomenological experience assessments and their interpretation, given the explicitly noted low inter-rater reliability.

## Next Checks

1. **Extend temporal horizon**: Run the same experimental protocol for 50+ cycles to determine whether behavioral patterns stabilize, evolve, or dissolve over longer timescales.

2. **Introduce task ambiguity gradients**: Systematically vary the degree of task definition to map how behavioral patterns shift across the ambiguity spectrum.

3. **Cross-architecture validation**: Replicate the study using open-weight models with different architectural features to determine whether observed patterns correlate with specific architectural elements.