---
ver: rpa2
title: 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning'
arxiv_id: '2510.01459'
source_url: https://arxiv.org/abs/2510.01459
tags:
- training
- lspo
- arxiv
- sampling
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LSPO, a length-aware dynamic sampling method
  for reinforcement learning with verifiable rewards (RLVR) in LLM reasoning. Motivated
  by the observation that incorrect responses are often longer, LSPO filters training
  prompts based on average response length, retaining only the shortest and longest
  responses after initial accuracy-based filtering.
---

# LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning

## Quick Facts
- **arXiv ID**: 2510.01459
- **Source URL**: https://arxiv.org/abs/2510.01459
- **Reference count**: 12
- **Primary result**: LSPO improves RLVR performance on mathematical reasoning by filtering responses based on length, achieving consistent gains across multiple base models and algorithms.

## Executive Summary
This paper introduces LSPO, a dynamic sampling method that leverages the observation that incorrect LLM responses are often longer than correct ones. The method filters training prompts based on average response length after initial accuracy-based filtering, retaining only the shortest and longest responses. Evaluated across multiple base models and RLVR algorithms on challenging math benchmarks, LSPO consistently improves both efficiency and final model effectiveness compared to baseline RLVR approaches.

## Method Summary
LSPO implements length-aware dynamic sampling for reinforcement learning with verifiable rewards in LLM reasoning. The method operates by first running initial accuracy-based filtering on responses, then computing average response lengths for each prompt, and finally retaining only the shortest and longest responses while discarding intermediate-length ones. This filtering occurs dynamically during training, allowing the model to focus on the most informative responses based on the length-correctness correlation observed in mathematical reasoning tasks.

## Key Results
- LSPO improves avg@32 accuracy from 37.9% to 38.6% on Qwen-2.5-Math-7B with DAPO across benchmarks
- The method consistently outperforms baseline RLVR algorithms across multiple base models (Qwen-2.5-Math-7B, Qwen3-4B-Base, Llama-3.2-4B-Instruct) and algorithms (GRPO, DAPO, GSPO)
- LSPO reaches target performance faster and with better final results despite increased rollout time due to additional filtering

## Why This Works (Mechanism)
LSPO exploits the empirical correlation between response length and correctness in mathematical reasoning. Incorrect responses tend to be longer due to verbose incorrect reasoning paths, self-correction attempts, or over-explanation of wrong approaches. By filtering out intermediate-length responses and focusing on the extremes, the method creates a more informative training signal that helps the model distinguish between correct and incorrect reasoning patterns more effectively.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Training method where model actions are evaluated against ground truth or verifiable criteria; needed because mathematical reasoning requires precise correctness evaluation rather than human preference
- **Dynamic Sampling**: Adaptive selection of training data during RL training; needed to focus on the most informative samples and improve sample efficiency
- **Length-Correctness Correlation**: Empirical observation that incorrect responses are often longer; needed as the foundation for LSPO's filtering strategy
- **Multi-Algorithm Evaluation**: Testing across GRPO, DAPO, and GSPO; needed to demonstrate LSPO's generalizability across different RLVR approaches
- **Benchmark Diversity**: Using AIME25, Olympiad, and Minerva-Math; needed to validate effectiveness across different mathematical reasoning difficulties

## Architecture Onboarding

**Component Map**: Base Model -> RLVR Algorithm -> LSPO Filtering -> Updated Policy

**Critical Path**: Forward pass → Accuracy evaluation → Length calculation → Dynamic filtering → Policy update

**Design Tradeoffs**: 
- Computational overhead from additional filtering vs. improved sample efficiency
- Risk of discarding potentially useful intermediate responses vs. focusing on most informative extremes
- Generalizability across domains vs. optimization for mathematical reasoning

**Failure Signatures**:
- Performance degradation if length-correctness correlation weakens
- Increased training time without corresponding accuracy gains
- Overfitting to specific length patterns rather than reasoning quality

**First Experiments**:
1. Verify length-correctness correlation on target domain before applying LSPO
2. Test baseline RLVR performance without filtering as comparison
3. Evaluate impact of different length threshold parameters on performance

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The length-correctness correlation may not generalize beyond mathematical reasoning domains
- Computational overhead scales linearly with samples retained due to multiple LLM inference passes
- Optimal length thresholds appear dataset-dependent without clear transfer guidelines

## Confidence
**High confidence**: Core empirical finding that length-aware filtering improves RLVR performance on mathematical reasoning benchmarks
**Medium confidence**: Claim that LSPO improves RLVR efficiency by reaching target performance faster
**Low confidence**: Generalizability to non-mathematical reasoning tasks and theoretical basis for length-correctness correlation

## Next Checks
1. Test LSPO on non-mathematical reasoning domains (commonsense reasoning, code generation, logical inference) to evaluate domain transferability
2. Conduct controlled experiments varying length thresholds and filtering criteria to determine optimal parameters across different task types
3. Compare LSPO against alternative dynamic sampling methods (accuracy-based, uncertainty-based, diversity-based filtering) on the same benchmarks