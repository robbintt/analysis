---
ver: rpa2
title: 'CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?'
arxiv_id: '2503.21717'
source_url: https://arxiv.org/abs/2503.21717
tags:
- weakness
- claim
- claims
- target
- weaknesses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAIM CHECK introduces a new dataset and benchmark for assessing
  how well LLMs can produce claim-grounded critiques of scientific papers. It consists
  of NeurIPS 2023/2024 submissions, reviews, and expert annotations linking reviewer-identified
  weaknesses to specific claims in the papers.
---

# CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?

## Quick Facts
- arXiv ID: 2503.21717
- Source URL: https://arxiv.org/abs/2503.21717
- Reference count: 40
- Key outcome: Introduces CLAIMCHECK benchmark to evaluate how well LLMs can produce claim-grounded critiques of scientific papers, finding current models struggle with claim association, specificity improvement, and verification without human references.

## Executive Summary
CLAIMCHECK introduces a new benchmark for evaluating how well LLMs can produce claim-grounded critiques of scientific papers. The dataset consists of NeurIPS 2023/2024 submissions, reviews, and expert annotations linking reviewer-identified weaknesses to specific claims in the papers. Three tasks are evaluated: Claim Association (grounding weaknesses to claims), Weakness Labeling and Editing (predicting fine-grained labels and improving specificity), and Claim Verification (generating weaknesses from scratch). Experiments with leading LLMs show that while models can predict weakness types reasonably well, they struggle to accurately associate weaknesses with claims, improve specificity, and verify claims without human-written references, indicating that current LLMs still fall short of expert performance in producing grounded peer reviews.

## Method Summary
The authors construct CLAIMCHECK by collecting rejected NeurIPS 2023/2024 papers from OpenReview, filtering for claim-related reviews using keyword matching, and annotating weaknesses with expert human annotators. The annotation process involves three stages: identifying weakness spans, associating them with specific paper claims, and labeling weakness types. The benchmark evaluates LLMs on three tasks: Claim Association (linking weaknesses to claims with F1_edit and F1_exact metrics), Weakness Labeling and Editing (predicting fine-grained labels and improving specificity with Krippendorff's α), and Claim Verification (generating weaknesses from scratch using GPT-4o as judge). The dataset contains 41 papers, 60 reviews, and 168 weaknesses with expert annotations.

## Key Results
- LLMs achieve F1_edit scores of 23.1-34.8 on Claim Association task, substantially below human performance
- Models predict weakness type labels with moderate-to-high agreement (α scores of 94.9 for Novelty, 70.6 for Insufficient evidence)
- Generated weaknesses differ from expert critiques in >80% of cases and tend to be overly generic
- Even with reasoning-enhanced models (o1, o3-mini), performance remains well below human expert levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can predict fine-grained weakness type labels for reviewer-identified weaknesses with moderate-to-high agreement, particularly for novelty and insufficient evidence categories.
- Mechanism: The paper shows that reasoning-enhanced models (o1, o3-mini) leverage lexical cues (e.g., "novel(ty)", "convincing") and explicit citations to classify weakness types. Label prediction benefits from structured taxonomies and multi-label annotation schemes that align with common reviewer language patterns.
- Core assumption: Weakness types are partially recoverable from surface-level linguistic signals without deep scientific reasoning about the claim's validity.
- Evidence anchors:
  - [abstract] "cutting-edge LLMs, while capable of predicting weakness labels in (2), continue to underperform relative to human experts on all other tasks"
  - [section §4.2, Table 3] "o1 performs the best on all the remaining labels" with α scores of 94.9 for Novelty and 70.6 for Insufficient evidence
  - [corpus] Weak corpus signals—neighbor papers focus on LLM review generation broadly but not specifically on weakness label prediction mechanisms
- Break condition: When weaknesses require deep scientific reasoning beyond lexical patterns (e.g., distinguishing contradictory from insufficient evidence), model agreement drops substantially (αContradictory = 25.1 for o1).

### Mechanism 2
- Claim: LLMs struggle to accurately associate reviewer-identified weaknesses with the specific paper claims they dispute, with F1 scores substantially below human performance.
- Mechanism: The task requires identifying direct target claims (claims whose truth is disputed) versus indirect targets or merely relevant claims. Models receive candidate claims extracted from papers and must select up to three targets per weakness.
- Core assumption: Accurate claim association requires understanding the dialectic relationship between criticism and assertion, not just semantic similarity.
- Evidence anchors:
  - [abstract] "they struggle to accurately associate weaknesses with claims"
  - [section §4.1, Figure 3] Human annotators achieved F1,edit = 45.8 and F1,exact = 28.5 on pilot data; best LLM (o1) achieved only 34.8 F1,edit and 23.1 F1,exact on main data
  - [corpus] Related work (ARIES, ArgSciChat) addresses review-rebuttal alignment but not claim-weakness grounding specifically
- Break condition: When weaknesses are broad, speculative, or require inference across multiple paper sections, models fail to identify appropriate target claims.

### Mechanism 3
- Claim: LLMs cannot reliably generate specific, claim-grounded weaknesses de novo (without human-written references), producing critiques that differ from expert reviewers in >80% of cases.
- Mechanism: The Claim Verification task requires models to (1) extract evidence from paper text/figures/tables, (2) describe a weakness of the focal claim, and (3) provide weakness labels—all without seeing reviewer-written weaknesses.
- Core assumption: Generating grounded scientific critique requires understanding both the claim's technical content and the broader methodological standards of the field.
- Evidence anchors:
  - [abstract] "verifying a paper's claims with grounded reasoning...remains challenging"
  - [section §4.3, Table 6] "LLM-generated weaknesses across all evaluated models...overwhelmingly tend to be judged different from those identified by the reviewers (> 80% of cases)"
  - [section §4.3] "model-written weaknesses tend to be overly generic in their diagnoses (e.g. 'there is a lack of precise evidence linking GSNR to controlling the generalization gap as claimed')"
  - [corpus] Neighbor paper "Can AI Validate Science?" (FMR=0.50) addresses claim-evidence reasoning but in a different evaluation paradigm
- Break condition: When claims require domain-specific methodological knowledge or compositional reasoning across tables/figures, models produce generic or irrelevant critiques.

## Foundational Learning

- Concept: **Direct vs. Indirect Target Claims**
  - Why needed here: The paper explicitly distinguishes claims directly disputed by a weakness from claims merely affected through intermediary claims. This distinction is critical for accurate claim association.
  - Quick check question: If a weakness disputes experimental results in Table 2, and Claim A asserts those results while Claim B generalizes from them, which claim(s) should be annotated as direct targets?

- Concept: **Multi-label Weakness Taxonomy**
  - Why needed here: CLAIMCHECK uses overlapping weakness types (insufficient evidence, contradictory evidence, novelty, clarity, related work, other) rather than mutually exclusive categories, reflecting real-world review complexity.
  - Quick check question: A weakness states "The method's claimed novelty is unclear and lacks comparison to [prior work X]." Which weakness types apply?

- Concept: **Groundedness as Evaluation Criterion**
  - Why needed here: The paper evaluates critiques not just on plausibility but on whether they are grounded in specific paper claims—a core quality criterion for peer review that distinguishes LLM output from expert critique.
  - Quick check question: What distinguishes a "grounded" weakness from a generic criticism like "the experiments seem incomplete"?

## Architecture Onboarding

- Component map:
  OpenReview API -> keyword filtering -> NLP topic classification (GPT-4o) -> PDF parsing (PaperMage) -> claim extraction (GPT-4o) -> manual table/figure cropping -> three-stage annotation (Weakness Identification -> Claim Association + Weakness Labeling) -> LLM evaluation (CA, WLE, CV tasks)

- Critical path:
  1. Preprocessing: Filter reviews by claim-related keywords -> classify papers as NLP-related -> extract and clean text
  2. Claim extraction: GPT-4o extracts candidate claims from full paper text (excluding methods/setup descriptions)
  3. Annotation: Experts identify weaknesses -> ground to claims -> assign labels
  4. Evaluation: Compare LLM outputs to expert annotations using appropriate metrics per task

- Design tradeoffs:
  - **Dataset size vs. quality**: 60 reviews / 41 papers with rich expert annotations vs. larger automated datasets; authors explicitly note CLAIMCHECK is "intended purely as an evaluation benchmark" not for fine-tuning
  - **Rejected papers only**: Ensures version alignment (reviewed version = public version) but excludes accepted work
  - **NLP-focused**: Aligns with annotator expertise but limits domain generalization
  - **GPT-4o for preprocessing**: Creates potential annotation bottlenecks if claim extraction fails (annotators can manually add missing claims)

- Failure signatures:
  - **Claim Association**: Low F1,exact (<25%) indicates models select wrong claims even when correct ones are in candidate set
  - **Weakness Editing**: Models strip useful anchors (line numbers, quotes) or change tone without improving specificity (Table 4)
  - **Claim Verification**: Generic critiques that "deny the paper comments on the claim at all" or provide boilerplate insufficient-evidence complaints

- First 3 experiments:
  1. **Baseline CA task**: Provide GPT-4o with paper text, extracted claims, and single weakness; measure F1,edit and F1,exact against expert annotations. Expect F1,edit ~30-35% based on paper results.
  2. **Ablate candidate claim set**: Test whether poor CA performance stems from claim extraction quality by providing models with gold-standard (human-verified) claim sets vs. GPT-4o-extracted claims.
  3. **Probe specificity improvement**: For WLE task, conduct human evaluation on whether model-edited weaknesses improve specificity/groundedness; expect ties as majority judgment based on Table 5 (72.5-77.5% ties for GPT-4o/Gemini).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning (SFT) on scaled-up data improve LLM performance on claim-grounded review tasks?
- Basis in paper: [explicit] The authors state in the Limitations section that CLAIMCHECK is "likely not large enough for meaningful supervised fine-tuning," restricting its use to evaluation only.
- Why unresolved: The current dataset size (168 weaknesses, 41 papers) is insufficient to train models to learn the complex mapping between reviews and specific paper claims.
- What evidence would resolve it: Training performance curves and success metrics on a significantly expanded version of the dataset (e.g., 10x size).

### Open Question 2
- Question: How can LLMs be better equipped to identify "omission" weaknesses that do not target specific explicit claims?
- Basis in paper: [explicit] The paper notes that it focuses only on claim-related weaknesses, acknowledging that "weaknesses that identify important experiments or related work that were omitted are also valuable" and arguably harder to identify.
- Why unresolved: The current annotation schema and task design explicitly exclude weaknesses that are not grounded in a specific paper assertion, leaving this capability untested.
- What evidence would resolve it: An expansion of the CLAIMCHECK schema to include "missing elements" and a corresponding benchmark evaluating LLMs on this new task.

### Open Question 3
- Question: Does retrieval-augmented generation (RAG) or access to broader scientific corpora improve performance on the Claim Verification (CV) task?
- Basis in paper: [inferred] The results show models perform poorly on verifying claims "from scratch," often providing "overly generic" diagnoses or failing to verify claims without human-written references.
- Why unresolved: The current setup limits evidence to the reviewed paper and explicitly cited related works; models may lack the external knowledge required to verify novelty or soundness independently.
- What evidence would resolve it: A comparison of zero-shot CV performance against models equipped with search tools or connected to comprehensive scientific citation indices.

## Limitations

- The dataset size (41 papers, 60 reviews) is small and may not capture the full complexity of scientific review discourse
- Reliance on GPT-4o for both preprocessing and annotation introduces potential biases in claim extraction and classification
- The NLP domain restriction limits generalizability to other scientific fields with different review conventions
- Only claim-related weaknesses are included, excluding valuable critiques about omitted experiments or related work

## Confidence

- **High confidence** in the experimental methodology and evaluation metrics for the three benchmark tasks
- **Medium confidence** in the generalizability of findings to non-NLP domains and larger-scale review systems
- **Medium confidence** in the claim that current LLMs fall short of expert performance, given the specialized nature of the benchmark

## Next Checks

1. **Domain Transfer Test**: Evaluate CLAIMCHECK benchmark tasks on papers from non-NLP domains (e.g., biology, physics) to assess whether current model limitations persist across scientific fields.

2. **Scaling Analysis**: Test whether increasing model scale (e.g., GPT-4o vs. GPT-5 class models) significantly improves performance on the most challenging tasks (Claim Association and Claim Verification), or if architectural improvements are needed.

3. **Annotation Pipeline Validation**: Re-run claim extraction and classification steps with multiple annotators and different GPT-4o variants to quantify the variability introduced by the preprocessing pipeline.