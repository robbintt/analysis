---
ver: rpa2
title: Learning Robot Safety from Sparse Human Feedback using Conformal Prediction
arxiv_id: '2501.04823'
source_url: https://arxiv.org/abs/2501.04823
tags:
- policy
- conformal
- safety
- unsafe
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning robot safety from sparse
  human feedback using conformal prediction. The approach collects human-labeled unsafe
  trajectories and uses nearest neighbor classification combined with conformal prediction
  to identify a region of states likely to be unsafe.
---

# Learning Robot Safety from Sparse Human Feedback using Conformal Prediction

## Quick Facts
- arXiv ID: 2501.04823
- Source URL: https://arxiv.org/abs/2501.04823
- Reference count: 40
- Primary result: Sample-efficient method for learning robot safety from sparse human feedback using conformal prediction with guaranteed miss rate

## Executive Summary
This paper presents a method for learning robot safety from sparse human feedback using conformal prediction. The approach collects human-labeled unsafe trajectories and uses nearest neighbor classification combined with conformal prediction to identify regions likely to be unsafe. The key innovation is applying full conformal prediction in closed form without withholding data, enabling efficient calibration even with limited unsafe samples. Experiments demonstrate the approach on quadcopter navigation tasks, showing improved safety when modifying policies to avoid the learned unsafe region.

## Method Summary
The method uses conformal prediction to learn a "Suspected Unsafe Sublevel (SUS) region" C(ε) from sparse human binary feedback (safe/unsafe trajectory labels). Given P labeled trajectories, error states E are extracted from unsafe trajectories. The conformal threshold r is computed in closed form using intra-dataset nearest-neighbor distances αᵢ (Eq. 8 or 17), where k = ⌈(N+1)(1-ε)⌉ and r = α₍ₖ₎. The SUS region C(ε) = {x | s(D; x) ≤ r} provides guaranteed coverage Pr(x ∈ C(ε)) ≥ 1-ε. For policy modification, MPC plans avoiding C(ε), switching to a backup policy tracking historical safe trajectories upon alert.

## Key Results
- Conformal prediction threshold computable in closed form without data splitting, avoiding sample inefficiency
- SUS region geometry (union of balls or polyhedra) enables tractable runtime monitoring and MPC constraint enforcement
- Backup policy approach achieves ~6% collision rate vs 36% for direct constraint method, preserving calibration
- Method works with high-dimensional data through representation learning while maintaining guarantees

## Why This Works (Mechanism)

### Mechanism 1: Closed-Form Conformal Calibration Without Data Splitting
The authors prove that nearest neighbor conformal prediction can be computed without holding out calibration data. Using the score function s(D; x) = min_{x'∈D} d(x', x), the threshold r = α(k) can be precomputed from intra-data nearest neighbor distances. This avoids the standard split conformal requirement of reserving calibration data. The guarantee holds under exchangeability—unsafe states must be drawn from the same distribution for calibration and deployment.

### Mechanism 2: SUS Region Geometry Enables Runtime Monitoring and Constraint Enforcement
The SUS region C(ε) has tractable geometry (union of balls or polyhedra) enabling both fast membership queries and integration into optimization-based control. With Euclidean distance, C(ε) becomes a union of balls. With the unsafe-safe extension using squared Euclidean distance, C(ε) becomes a union of polyhedra with linear constraints. This geometric structure allows O(log N) queries via KD-trees and linear constraint encoding in MPC.

### Mechanism 3: Backup Policy Avoids Distribution Shift That Would Violate Calibration
Directly constraining the original policy to avoid C(ε) causes distribution shift toward unsafe set boundaries, paradoxically increasing collision probability. By switching to a backup policy upon alert, states under the original policy remain exchangeable with training data, preserving the miss rate guarantee. The backup policy tracks historical safe trajectories while explicitly avoiding C(ε) via polyhedral constraints.

## Foundational Learning

- **Concept: Conformal Prediction and Exchangeability**
  - Why needed: The entire guarantee framework depends on understanding that conformal prediction provides distribution-free coverage bounds under exchangeability
  - Quick check: Given exchangeable scores s₁, ..., s_N, s_{N+1}, what is Pr(s_{N+1} ≤ s_{(k)})? (Answer: k/(N+1))

- **Concept: Nearest Neighbor Classification and Distance Metrics**
  - Why needed: The score function is fundamentally nearest neighbor distance; the unsafe-safe extension requires understanding how subtracting nearest safe neighbor distance implements an approximate likelihood ratio test
  - Quick check: Why does the unsafe-safe score produce polyhedra rather than balls as the decision boundary? (Answer: Squared Euclidean distance difference yields linear constraints.)

- **Concept: Model Predictive Control with State Constraints**
  - Why needed: Policy modification requires encoding C(ε) as linear constraints in the SCP/QP formulation
  - Quick check: How does the backup policy π_B differ from the nominal π in Eq. 29? (Answer: π_B tracks historical safe states x_t^B instead of goal x_g, and adds half-space constraints from C(ε).)

## Architecture Onboarding

- **Component map:**
Human Labeler → [Trajectory Dataset D = {τ₁, ..., τ_P}] → [Extract Error States E = {e₁, ..., e_N}] → [Compute αᵢ values via Eq. 8 or 17] → [Threshold r = α(k)] → [SUS Region C(ε) = ∪ᵢ Cᵢ] → [Warning System] and [Policy Modification]

- **Critical path:**
1. Collect N unsafe trajectories (sparse human labeling in simulation)
2. Compute intra-data nearest neighbor distances α₁, ..., α_N
3. Select user-specified ε (miss rate) and compute r = α(k) where k = ⌈(N+1)(1-ε)⌉
4. At runtime, check if s(x) ≤ r for each observed state (warning) or planned state (backup trigger)

- **Design tradeoffs:**
- Smaller ε: Lower miss rate but more false alarms (larger C(ε), more frequent backup switching)
- More unsafe samples N: Tighter coverage bounds, but requires more human labeling effort
- Unsafe-only vs. unsafe-safe: Unsafe-safe reduces false alarms by incorporating safe data, but requires polyhedral geometry
- Representation learning: Required for high-dimensional observations, but adds hyperparameters and potential drift

- **Failure signatures:**
- High false alarm rate → C(ε) bleeding into safe region; try unsafe-safe extension or better distance metric
- Missed unsafe events → Exchangeability violated (policy changed, environment changed); recalibrate
- Backup policy fails to recover → Insufficient backup trajectory coverage near alert regions
- Direct constraint approach has high error → Distribution shift; must use backup switching approach

- **First 3 experiments:**
1. Validate conformal coverage: Generate synthetic 2D data with known unsafe distribution, fit C(ε), verify Pr[x_{N+1} ∈ C(ε)] ≥ 1-ε empirically over 1000 repetitions
2. Warning system ablation: Implement unsafe-only, safe-only, and unsafe-safe variants on simple navigation task; compare ROC curves
3. Policy modification comparison: Implement both direct constraint MPC and backup MPC; measure collision rates and confirm backup MPC achieves ≈εβ while direct constraint fails

## Open Questions the Paper Calls Out

- **Dynamic environments:** How can the conformal prediction framework be extended to dynamic environments where the unsafe set is time-varying?
- **Constrained RL integration:** How can the calibrated p-value be effectively utilized as a safety penalty in constrained Reinforcement Learning?
- **Sample weighting for recalibration:** Can the SUS region be re-calibrated for new policies or environmental conditions via sample weighting without collecting new data?

## Limitations

- Core theoretical guarantee depends critically on exchangeability assumption between training and deployment data
- Backup policy approach requires sufficient coverage of safe backup trajectories near alert regions
- Method assumes sparse but non-zero human feedback; with truly minimal labeling, coverage guarantees weaken
- Sim-to-real transfer for quadcopter experiments remains partially demonstrated

## Confidence

- **High Confidence:** Closed-form conformal prediction derivation, geometric structure of SUS regions, core failure mode of direct policy constraint
- **Medium Confidence:** Practical effectiveness of backup policy approach in real-world scenarios, scalability to high-dimensional data
- **Low Confidence:** Robustness to representation learning failures when training data differs from deployment conditions

## Next Checks

1. **Exchangeability Stress Test:** Systematically vary the policy and measure empirical coverage degradation; plot miss rate vs. policy modification strength
2. **Backup Trajectory Coverage Analysis:** Quantify spatial density of backup trajectories near alert regions; measure recovery success rate as function of backup coverage density
3. **Representation Robustness:** Compare performance across multiple representation learning approaches (PCA, KPCA, autoencoder, pre-trained vision models) on the same task