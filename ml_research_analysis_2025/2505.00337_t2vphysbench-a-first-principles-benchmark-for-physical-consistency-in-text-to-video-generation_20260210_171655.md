---
ver: rpa2
title: 'T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video
  Generation'
arxiv_id: '2505.00337'
source_url: https://arxiv.org/abs/2505.00337
tags:
- video
- prompt
- uni00000013
- uni00000011
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces T2VPhysBench, the first human-evaluated benchmark
  that assesses whether text-to-video models follow 12 fundamental physical laws,
  including Newtonian mechanics, conservation principles, and phenomenological effects.
  The benchmark uses carefully designed prompts and a four-level human scoring system
  to evaluate both open-source and commercial models.
---

# T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation

## Quick Facts
- arXiv ID: 2505.00337
- Source URL: https://arxiv.org/abs/2505.00337
- Reference count: 40
- Current text-to-video models score below 0.60 on average across all evaluated physical law categories

## Executive Summary
T2VPhysBench introduces the first human-evaluated benchmark for assessing physical consistency in text-to-video generation models. The benchmark evaluates 12 fundamental physical laws including Newtonian mechanics, conservation principles, and phenomenological effects through carefully designed prompts and a four-level human scoring system. The study reveals that all evaluated models, both open-source and commercial, struggle significantly with physical realism, scoring below 0.60 on average across every law category, with conservation laws being particularly challenging. Models demonstrate compliance with physically impossible scenarios and fail to improve even with detailed prompt hints, suggesting reliance on pattern matching rather than genuine physical reasoning.

## Method Summary
The benchmark employs a comprehensive human evaluation framework using four-level scoring (0-3) to assess whether generated videos obey specific physical laws. Researchers designed targeted prompts for each of the 12 physical laws, including Newtonian mechanics (momentum, acceleration, action-reaction), conservation principles (energy, mass), and phenomenological effects (buoyancy, elasticity). Both open-source models (CogVideoX, HunyuanVideo, SVD, Wan, Luma) and commercial models (GPT-4o, Sora) were evaluated. The scoring system considers physical consistency, realism, and adherence to specified laws, with evaluators trained on clear guidelines to ensure consistent assessment across different physical scenarios.

## Key Results
- All evaluated models scored below 0.60 on average across every physical law category
- Conservation laws proved particularly challenging, with models failing to maintain basic principles of energy and mass conservation
- Even with detailed prompt hints and explicit instructions, physical consistency did not improve significantly
- Models frequently complied with physically impossible requests, indicating pattern matching over true physical reasoning

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on fundamental physical principles that should be universally observable and testable. By using human evaluators with clear scoring guidelines, the assessment captures nuanced physical behaviors that automated metrics might miss. The four-level scoring system allows for granular differentiation between models' capabilities, while the targeted prompts isolate specific physical phenomena for detailed analysis.

## Foundational Learning
- Newtonian mechanics (momentum, acceleration, action-reaction): Why needed - forms the basis of most physical interactions in videos; Quick check - verify object trajectories and collision responses
- Conservation principles (energy, mass): Why needed - fundamental laws that must hold in realistic scenarios; Quick check - track total energy/mass across video sequences
- Phenomenological effects (buoyancy, elasticity): Why needed - governs everyday physical behaviors that viewers expect; Quick check - observe object behavior in fluids and upon impact
- Prompt engineering techniques: Why needed - essential for controlling model outputs and testing specific behaviors; Quick check - measure consistency across different prompt formulations
- Human evaluation methodology: Why needed - captures subjective physical realism that automated metrics miss; Quick check - ensure inter-rater reliability through statistical analysis

## Architecture Onboarding

Component map:
Text prompt -> Text-to-video model -> Video generation -> Physical consistency evaluation -> Scoring

Critical path:
Text prompt design -> Model generation -> Human evaluation -> Physical law assessment

Design tradeoffs:
The study prioritizes comprehensive physical law coverage over automated scalability, accepting the time-intensive nature of human evaluation for greater accuracy. The choice of four-level scoring balances granularity with evaluator cognitive load.

Failure signatures:
Models consistently fail on conservation laws, suggesting fundamental gaps in understanding quantity preservation. Compliance with counterfactual physical scenarios indicates pattern matching rather than reasoning. Lack of improvement with detailed hints reveals limited physical reasoning capabilities.

First experiments:
1. Test a simple momentum conservation scenario (two colliding objects) with different mass ratios
2. Evaluate energy conservation in a pendulum motion scenario
3. Assess buoyancy effects with objects of varying densities in water

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Exclusive reliance on human evaluation introduces potential subjectivity and inter-rater variability
- Fixed set of 12 physical laws may not capture all aspects of physical realism or emergent behaviors
- Evaluation limited to short video clips, potentially missing longer-term physical inconsistencies
- Does not investigate impact of video resolution or frame rate on physical consistency assessment

## Confidence
- Core finding that models score below 0.60 on average: High
- Conservation laws being particularly challenging: Medium-High
- Models relying on pattern matching rather than physical reasoning: Medium

## Next Checks
1. Conduct inter-rater reliability analysis to quantify agreement between human evaluators
2. Implement automated physical consistency checks using physics simulation engines
3. Extend evaluation to longer video sequences (10+ seconds) to assess temporal consistency