---
ver: rpa2
title: Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive
  and Independent Effects
arxiv_id: '2509.21923'
source_url: https://arxiv.org/abs/2509.21923
tags:
- macms
- additive
- part
- multiplicative
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multiplicative-Additive Constrained Models (MACMs) address the
  limitations of GAMs and CESR by jointly modeling both independent and higher-order
  interaction effects while maintaining interpretability. The method decouples multiplicative
  and additive components to expand the hypothesis space and avoid parameter entanglement,
  enabling visualization of both parts' shape functions.
---

# Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects

## Quick Facts
- arXiv ID: 2509.21923
- Source URL: https://arxiv.org/abs/2509.21923
- Reference count: 40
- Primary result: Neural network-based MACMs significantly outperform CESR, polynomial-based GAMs, and state-of-the-art neural network-based GAMs on CA Housing, Stroke Prediction, and Water Quality datasets.

## Executive Summary
Multiplicative-Additive Constrained Models (MACMs) address limitations in Generalized Additive Models (GAMs) and CESR by jointly modeling both independent and higher-order interaction effects while maintaining interpretability. The method decouples multiplicative and additive components to expand the hypothesis space and avoid parameter entanglement, enabling visualization of both parts' shape functions. Experimental results demonstrate that neural network-based MACMs significantly outperform CESR, polynomial-based GAMs, and state-of-the-art neural network-based GAMs (NAMs, NBMs, ProtoNAMs) on three benchmark datasets, achieving lower RMSE for regression tasks and higher AUC for classification. MACMs achieve performance between their multiplicative part alone and unconstrained DNNs, confirming the effectiveness of combining multiplicative and additive components for improved accuracy while preserving interpretability.

## Method Summary
MACMs combine a multiplicative part (product of univariate shape functions) with an additive part (sum of univariate shape functions) to jointly model interactive and independent effects. The model takes k features, min-max normalized to [-1, 1], and computes output as k·∏fmi(xi) + ∑fai(xi), where each fmi and fai is a neural network (10 hidden layers, 20 neurons each, ReLU). For regression, scaling factor k=10 with learning rate 0.0005 (exponential decay 0.99 per 100 epochs); for classification, k=1000 with learning rate 0.00005 (decay 0.995 per 10 epochs). The shape functions are normalized post-training for interpretable dynamic contribution visualization.

## Key Results
- Neural network-based MACMs significantly outperform CESR, polynomial-based GAMs, and state-of-the-art neural network-based GAMs on CA Housing, Stroke Prediction, and Water Quality datasets
- MACMs achieve RMSE values between standalone multiplicative part and unconstrained DNNs for regression tasks
- MACMs achieve higher AUC scores than all compared models for binary classification tasks
- Performance improvements demonstrate effectiveness of combining multiplicative and additive components while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1: Coefficient Decoupling and Hypothesis Space Expansion
- **Claim:** Adding an additive component to CESR disentangles coefficients of independent and interaction terms, expanding the hypothesis space to represent functions unreachable by either component alone.
- **Mechanism:** CESR's single weight parameter contributes simultaneously to independent and interaction terms, constraining representational capacity. MACMs introduce separate coefficients for additive and multiplicative parts, allowing independent adjustment. The combined hypothesis space $H_m + H_a$ satisfies $dim(H_m + H_a) > max\{dim(H_m), dim(H_a)\}$.
- **Core assumption:** The target function is better approximated by a combination of multiplicative and additive components than by either alone, given this decoupling.
- **Evidence anchors:** Abstract states MACMs "effectively broaden the hypothesis space" by augmenting CESR with an additive part. Section 3.1 explains the additive part "decouples and corrects coefficients."
- **Break condition:** If underlying data relationships are purely multiplicative or purely additive, the decoupling mechanism is less critical, and simpler models might suffice with lower computational cost.

### Mechanism 2: Neural Network Shape Functions for Universal Approximation
- **Claim:** Replacing polynomial shape functions with neural networks provides universal approximation capabilities, overcoming inherent polynomial limitations.
- **Mechanism:** Polynomials have limited representational capacity and cannot approximate all functions with arbitrary precision. Neural networks with nonlinear activations are universal function approximators. Parameterizing shape functions as neural networks grants flexibility to learn complex, non-polynomial mappings from data.
- **Core assumption:** True feature shape functions are complex and non-polynomial, requiring neural network expressive power for accurate modeling.
- **Evidence anchors:** Abstract states neural network-based MACMs "significantly outperform... polynomial-based GAMs." Section 3.2 explains polynomials "cannot approximate arbitrary functions with arbitrary precision."
- **Break condition:** If true feature relationships are simple (e.g., linear or low-order polynomial), neural network complexity becomes unnecessary and could lead to overfitting.

### Mechanism 3: Dynamic Feature Contribution for Interpretability
- **Claim:** The multiplicative part creates a dynamic scaling factor for each feature's contribution that, when visualized alongside static additive contribution, provides semi-global interpretability.
- **Mechanism:** A feature's total contribution is $\alpha U_{mi}(x_i) + U_{ai}(x_i)$, where $\alpha = C_m \prod_{j \neq i} U_{mj}(x_j)$ depends on all other features. Plotting curves across sampled $\alpha$ values reveals how the feature's shape function is dynamically scaled.
- **Core assumption:** This form of interpretability is useful to domain experts for understanding complex, interacting effects.
- **Evidence anchors:** Abstract states shape functions "can be both be naturally visualized, thereby assisting users in interpreting how features participate in the decision-making process." Section 3.2.1 explains the contribution is "dynamic."
- **Break condition:** If feature count is very large, the dynamic scaling factor becomes difficult to attribute to specific feature interactions, reducing practical interpretability.

## Foundational Learning

- **Concept: Generalized Additive Models (GAMs)**
  - **Why needed here:** GAMs are the baseline interpretable model MACMs improve upon. Understanding that GAMs model output as a sum of univariate shape functions ($y = \beta + \sum f_i(x_i)$) is essential to see how MACMs add a multiplicative component.
  - **Quick check question:** How does a GAM model the relationship between a feature and the target, and what is its primary limitation regarding interactions?

- **Concept: The Multiplicative Model (CESR)**
  - **Why needed here:** CESR is the multiplicative counterpart to GAMs that MACMs build upon. Understanding its form ($y = C \prod U_i(x_i)$) and its key weakness—entangled coefficients—is the core motivation for the paper's proposed solution.
  - **Quick check question:** In the CESR formula, how does the coefficient for an independent feature term also affect the interaction terms? Why is this a problem?

- **Concept: Hypothesis Space**
  - **Why needed here:** The paper frames its contribution as expanding the "hypothesis space." Grasping this concept is key to understanding how adding an additive part decouples coefficients and allows the model to represent a broader set of functions.
  - **Quick check question:** What does it mean for the hypothesis space of MACMs to be the sum of the multiplicative and additive hypothesis spaces ($H_m + H_a$), and why is this considered an expansion?

## Architecture Onboarding

- **Component map:** Inputs (k features, min-max normalized to [-1, 1]) -> Multiplicative Part (k subnetworks, k·∏fmi(xi)) + Additive Part (k subnetworks, ∑fai(xi)) -> Total Output (k_scale·∏fmi(xi) + ∑fai(xi)) -> Shape Function Normalization (for visualization)

- **Critical path:**
  1. **Feature Normalization (CRITICAL):** Inputs must be min-max scaled to [-1, 1] before training to prevent gradient explosion in the multiplicative part.
  2. **Shape Function Definition:** Define architecture for fmi and fai (e.g., FC layers, activations).
  3. **Loss Calculation & Backpropagation:** Compute loss (RMSE or cross-entropy) and update all network parameters.
  4. **Shape Function Normalization (Post-Training):** Transform shape functions for interpretable dynamic curves. Extract constants from fmi to get Umi and Cm; remove bias from fai to get Uai and Ca.

- **Design tradeoffs:**
  - **Interpretability vs. Accuracy:** MACMs are not as transparent as simple GAMs (which have static shape functions) but offer better accuracy. Interpretability is "semi-global."
  - **Architecture Depth:** The paper uses 10 hidden layers for shape functions, increasing capacity but raising training cost and overfitting risk. Simpler architectures may be preferable for smaller datasets.
  - **Scaling Factor k:** Critical hyperparameter. Too small → multiplicative part vanishes; too large → multiplicative dominates. Paper uses large value (1000) for classification.

- **Failure signatures:**
  - **Gradient Vanishing in Multiplicative Part:** If model fails to learn (especially regression), scaling factor k may be too small. Product of many values < 1 creates extremely small gradients.
  - **Dominance of One Part:** Flat-line visualizations for one component indicate suppressed learning. Adjust scaling factor k or check initialization.
  - **Numerical Instability:** Large input values (if not normalized) will cause product explosion in multiplicative part, leading to NaN or Inf.

- **First 3 experiments:**
  1. **Reproduce Baseline Comparison:** Re-implement MACMs(NNs) and benchmark against a standard GAM (like NAM) on CA Housing dataset. This validates core implementation and confirms performance improvement over purely additive models.
  2. **Ablation on Scaling Factor k:** Systematically vary scaling factor k (e.g., 1, 10, 20, 100, 1000) on held-out validation set. Critical for finding stable value that balances multiplicative and additive contributions.
  3. **Visualization Study:** Train model on dataset with known simple interactions (e.g., y = x1 + x1*x2). Generate dynamic shape function plots. Verify they correctly reveal the known interaction (non-parallel lines in dynamic plot for x1). This tests interpretability claims directly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an automatic adjustment mechanism for the scaling factor k be designed to prevent imbalance between the multiplicative and additive parts?
- **Basis in paper:** The authors note that without careful adjustment, the multiplicative component may suffer from "insufficient learning," and they list "design an automatic adjustment mechanism" as future work.
- **Why unresolved:** The current implementation relies on manual tuning, and the multiplicative component is sensitive to the number of features.
- **What evidence would resolve it:** A proposed algorithm that dynamically sets k based on dataset characteristics, demonstrating stable convergence without manual intervention.

### Open Question 2
- **Question:** Does overlaying raw data samples onto dynamic shape function plots enhance the user's ability to identify prominent feature-output patterns?
- **Basis in paper:** The paper suggests "overlaying raw data samples on dynamic shape function plots could help reveal which patterns... are more prominent" as a future direction.
- **Why unresolved:** Current visualizations offer "semi-global interpretability," but the utility of adding data density overlays remains untested.
- **What evidence would resolve it:** User studies or qualitative analysis confirming that data overlays reduce ambiguity in interpreting dynamic contributions.

### Open Question 3
- **Question:** Can integrating shape function designs from models like NBMs or ProtoNAMs into the MACM framework further improve predictive accuracy?
- **Basis in paper:** The authors state they plan to enhance MACMs by "incorporating shape function designs from NBMs and ProtoNAMs."
- **Why unresolved:** The current paper instantiates shape functions primarily using standard fully connected layers, leaving these specific architectural improvements for future research.
- **What evidence would resolve it:** Experimental results showing MACMs built with ProtoNAM sub-networks outperforming the standard neural network-based MACMs reported in Table 2.

## Limitations

- **Limited dataset scope:** Performance claims are based on three specific datasets and may not generalize across different domains or data distributions.
- **Computational complexity:** MACMs require separate neural networks for each feature's multiplicative and additive components, significantly increasing parameter count and training cost compared to simpler alternatives.
- **Interpretability degradation:** Dynamic visualization approach may become less meaningful with very large feature counts, as the product-based scaling factor becomes difficult to attribute to specific feature interactions.

## Confidence

- **High Confidence:** The mechanism of coefficient decoupling between multiplicative and additive components is well-established theoretically and clearly explained. The mathematical formulation and architectural design are sound.
- **Medium Confidence:** The performance claims are supported by experimental results on three datasets, but the sample size is limited and statistical significance testing is absent. The superiority over baselines appears genuine but may not generalize universally.
- **Medium Confidence:** The interpretability mechanism through dynamic visualization is innovative and theoretically sound, but practical validation with domain experts is limited. The paper doesn't demonstrate whether users can actually extract meaningful insights from these visualizations in real-world scenarios.

## Next Checks

1. **Statistical Validation:** Perform paired t-tests or bootstrap confidence intervals on the performance metrics across all compared models to establish statistical significance of MACMs' improvements over baselines.

2. **Domain Expert Validation:** Conduct a user study with domain experts to evaluate whether the dynamic shape function visualizations actually help in understanding feature interactions and making decisions, beyond the technical performance metrics.

3. **High-Dimensional Scalability Test:** Evaluate MACMs on datasets with significantly higher feature counts (e.g., 50+ features) to assess whether the interpretability mechanism breaks down and whether computational costs become prohibitive compared to simpler alternatives.