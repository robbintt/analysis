---
ver: rpa2
title: From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum
  Hardware
arxiv_id: '2509.07614'
source_url: https://arxiv.org/abs/2509.07614
tags:
- quantum
- policy
- learning
- data
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates the integration of quantum machine learning
  (QML) and quantum policy evaluation (QPE) to enable reinforcement learning on quantum
  hardware. The authors address the challenge of implementing QPE, which offers quadratic
  quantum advantage over classical Monte Carlo methods, by first learning quantum
  environment parameters from classical data using QML on IBM quantum hardware.
---

# From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum Hardware

## Quick Facts
- arXiv ID: 2509.07614
- Source URL: https://arxiv.org/abs/2509.07614
- Reference count: 23
- Primary result: Demonstrates integration of QML and QPE for RL on quantum hardware, achieving 0.015 parameter deviation on IBM quantum hardware for two-armed bandit problem

## Executive Summary
This paper presents a hybrid approach combining quantum machine learning (QML) and quantum policy evaluation (QPE) to enable reinforcement learning on quantum hardware. The authors tackle the challenge of implementing QPE, which theoretically offers quadratic quantum advantage over classical Monte Carlo methods, by first learning quantum environment parameters from classical data using QML on IBM quantum hardware. The learned parameters are then applied in QPE on IonQ's trapped-ion quantum computer, demonstrating the feasibility of quantum advantage in reinforcement learning despite current hardware limitations.

## Method Summary
The authors implement a two-stage approach: first, they use QML on IBM quantum hardware to learn rotation angles in a variational quantum circuit from classical data for a two-armed bandit problem. Gradient-free optimization with noise mitigation successfully learned parameters with 0.015 deviation accuracy. These learned parameters are then applied in QPE on IonQ's forte-1 trapped-ion quantum computer. The QPE implementation uses 5- and 6-qubit circuits to evaluate policies with different action probabilities. The approach addresses the practical challenge of applying QPE by learning the environment parameters classically before quantum evaluation.

## Key Results
- QML on IBM hardware achieved parameter deviations of 0.015 for both arms with 70%/20% and 0%/50% winning probabilities
- QPE on IonQ's forte-1 showed deterministic policies remained robust despite noise, while mixed action probability policies degraded significantly
- Demonstrated successful integration of QML-QPE pipeline, though current hardware limitations prevent realizing theoretical quadratic quantum advantage

## Why This Works (Mechanism)
The approach works by decoupling the parameter learning from the policy evaluation phase. QML learns the quantum environment parameters (rotation angles) from classical data in a noise-tolerant manner using gradient-free optimization. These learned parameters are then fixed during QPE, reducing the quantum circuit depth required during policy evaluation. This separation allows each stage to optimize for its specific challenges: QML handles parameter estimation under noise, while QPE focuses on policy evaluation using the learned parameters.

## Foundational Learning
- Variational Quantum Circuits: Adjustable quantum circuits used for parameter learning; needed for encoding environment parameters, quick check is verifying circuit expressivity for target distributions
- Quantum Policy Evaluation: Quantum algorithm for estimating policy value using amplitude estimation; needed for achieving quadratic speedup over classical methods, quick check is measuring estimation variance reduction
- Noise Mitigation Techniques: Methods to reduce impact of quantum hardware noise; needed for reliable parameter learning, quick check is comparing performance with/without mitigation
- Gradient-Free Optimization: Optimization methods that don't require gradient computation; needed for QML training on noisy quantum hardware, quick check is convergence rate comparison with gradient-based methods
- Amplitude Estimation: Quantum algorithm for estimating solution probabilities; needed for QPE's quadratic advantage, quick check is verifying confidence interval width
- Trapped-Ion vs Superconducting Qubits: Different quantum hardware platforms with distinct noise characteristics; needed for understanding hardware-specific limitations, quick check is comparing gate fidelities

## Architecture Onboarding
- Component Map: Classical Data -> QML (IBM) -> Parameter Learning -> QPE (IonQ) -> Policy Value Estimation
- Critical Path: Data Collection → QML Training → Parameter Extraction → QPE Execution → Value Output
- Design Tradeoffs: Hardware-specific noise characteristics vs algorithm universality; parameter learning accuracy vs circuit depth; classical preprocessing vs quantum evaluation
- Failure Signatures: High variance in parameter estimates indicates QML convergence issues; poor policy value correlation suggests QPE noise dominance; systematic bias suggests hardware calibration problems
- First Experiments: 1) Test QML parameter learning accuracy on synthetic data with known parameters, 2) Validate QPE on classically-simulated environments with learned parameters, 3) Measure quantum advantage scaling by comparing QPE vs classical Monte Carlo as problem size increases

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Current quantum hardware noise severely impacts performance for policies with mixed action probabilities
- Two-armed bandit benchmark represents a highly simplified problem domain that may not scale effectively
- Quadratic quantum advantage assumes ideal hardware conditions not achievable with current NISQ devices
- Performance gap between theoretical expectations and experimental results remains substantial

## Confidence
- QML parameter learning accuracy: High - Specific measurable deviations of 0.015 with clear experimental validation
- QPE implementation on IonQ: Medium - Implemented but performance degradation under noise not fully characterized
- Scalability to complex RL problems: Low - Based solely on two-armed bandit results with insufficient evidence for generalization

## Next Checks
1. Implement the same QML-QPE pipeline on a more complex multi-armed bandit problem (4-6 arms) with varying reward distributions to test scalability limits
2. Perform systematic error mitigation analysis by comparing QPE performance across different noise levels and gate fidelities on the same quantum hardware
3. Validate the approach on a classical-quantum hybrid benchmark where partial environment parameters are learned classically while QPE handles quantum components, measuring any potential quantum advantage gains