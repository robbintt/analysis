---
ver: rpa2
title: A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and
  Inference-time Scaling Law
arxiv_id: '2505.02665'
source_url: https://arxiv.org/abs/2505.02665
tags:
- reasoning
- arxiv
- learning
- reward
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of reasoning large
  language models (LLMs) designed to mimic "slow thinking," inspired by Kahneman''s
  dual-process theory. It categorizes key technologies into three main areas: test-time
  scaling, which dynamically adjusts computational resources based on task complexity
  using search, sampling, and verification mechanisms; reinforcement learning, which
  refines decision-making through policy networks, reward models, and self-evolution
  strategies; and slow-thinking frameworks, which structure problem-solving using
  techniques like long chain-of-thought and hierarchical reasoning.'
---

# A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law

## Quick Facts
- **arXiv ID:** 2505.02665
- **Source URL:** https://arxiv.org/abs/2505.02665
- **Reference count:** 40
- **Primary result:** Comprehensive survey categorizing reasoning LLM technologies into test-time scaling, reinforcement learning, and slow-thinking frameworks, identifying key challenges and future directions.

## Executive Summary
This survey provides a comprehensive overview of reasoning large language models (LLMs) designed to mimic "slow thinking," inspired by Kahneman's dual-process theory. It categorizes key technologies into three main areas: test-time scaling, which dynamically adjusts computational resources based on task complexity using search, sampling, and verification mechanisms; reinforcement learning, which refines decision-making through policy networks, reward models, and self-evolution strategies; and slow-thinking frameworks, which structure problem-solving using techniques like long chain-of-thought and hierarchical reasoning. The review synthesizes over 100 studies, charting the evolution of these models and identifying critical challenges such as balancing fast and slow thinking, designing robust reward mechanisms, and extending capabilities to multi-modal reasoning.

## Method Summary
The survey employs a systematic literature review methodology, synthesizing over 100 research papers on reasoning LLMs. It organizes findings into three technical categories: test-time scaling mechanisms (search, sampling, verification), reinforcement learning approaches (policy optimization, reward modeling, self-evolution), and slow-thinking architectural frameworks (long CoT, hierarchical reasoning). The analysis traces the development of these technologies from early Chain-of-Thought prompting through recent advances in Monte Carlo Tree Search and Group Relative Policy Optimization. The survey also identifies future research directions and critical challenges facing the field, including the balance between fast and slow thinking modes and the need for multi-modal reasoning capabilities.

## Key Results
- Categorization of reasoning LLM technologies into three main areas: test-time scaling, reinforcement learning, and slow-thinking frameworks
- Identification of critical challenges including reward hacking, balancing fast/slow thinking, and generalization to out-of-distribution tasks
- Synthesis of over 100 studies charting the evolution from early CoT prompting to advanced MCTS and self-evolution techniques
- Emphasis on the importance of advancing reasoning LLMs for real-world applications from scientific discovery to decision support systems

## Why This Works (Mechanism)

### Mechanism 1: Test-Time Scaling via Search
- **Claim:** Reasoning accuracy may improve if computational resources are dynamically scaled during inference to explore multiple solution paths.
- **Mechanism:** The model utilizes search algorithms (e.g., Monte Carlo Tree Search, Beam Search) and sampling strategies (e.g., Best-of-N) to generate multiple reasoning trajectories. A verifier or scoring function then selects the optimal path, effectively trading compute for reasoning depth.
- **Core assumption:** The base model possesses sufficient knowledge to generate the correct reasoning path within the search space if guided or filtered effectively.
- **Evidence anchors:** [abstract] Mentions "test-time scaling, which dynamically adjusts computational resources based on task complexity using search, sampling, and verification mechanisms." [section] Section 4.1 details Beam Search and MCTS, noting they "systematically explore reasoning paths." [corpus] "Rethinking External Slow-Thinking" explores the probability of correct reasoning under these external mechanisms.
- **Break condition:** Fails if the search space is too vast for the compute budget, or if the verifier/reward model is unreliable, leading to high latency without accuracy gains.

### Mechanism 2: Reinforcement Learning with Reward Models
- **Claim:** Decision-making precision can be refined by optimizing a policy network against specific reward signals that evaluate reasoning steps or outcomes.
- **Mechanism:** Algorithms like PPO (Proximal Policy Optimization) or GRPO (Group Relative Policy Optimization) update the model's weights to maximize expected rewards. These rewards are provided by Outcome Reward Models (ORMs) or Process Reward Models (PRMs), which act as synthetic supervisors.
- **Core assumption:** The reward model accurately proxies human judgment or logical correctness, and the reward landscape is learnable without severe local optima.
- **Evidence anchors:** [abstract] States RL "refines decision-making through policy networks, reward models, and self-evolution strategies." [section] Section 5.2 explains that PRMs evaluate intermediate steps while ORMs evaluate final outputs to guide optimization. [corpus] Evidence is weak or missing regarding the specific convergence rates of these RL algorithms in this specific survey context.
- **Break condition:** "Reward hacking," where the model learns to generate high-reward outputs that are syntactically correct but semantically flawed.

### Mechanism 3: Self-Evolution and Reflection
- **Claim:** Models can bootstrap their own reasoning capabilities by iteratively generating, critiquing, and learning from their outputs without continuous human labeling.
- **Mechanism:** A "Generate-Filter-Learn" cycle is established. The model generates reasoning chains (Self-Training), critiques them (Self-Evaluation), and distills the high-quality correct traces back into the training set.
- **Core assumption:** The model's initial capability is sufficient to generate a non-zero percentage of correct solutions to serve as positive reinforcement signals.
- **Evidence anchors:** [section] Section 5.3 describes Self-Evolution as utilizing "intrinsic capabilities... to progressively improve its performance." [section] Section 6.1.4 discusses "Reflection and Backtracking" where models "monitor internal reasoning processes, detect errors, and dynamically adjust." [corpus] "VisuoThink" supports the integration of self-verification in multimodal contexts.
- **Break condition:** "Snowball errors" where early mistakes in the reasoning chain compound, leading the self-correction mechanism astray.

## Foundational Learning

- **Concept: Kahnemanâ€™s Dual-Process Theory (System 1 vs. System 2)**
  - **Why needed here:** The entire architectural philosophy of "Slow Thinking" relies on distinguishing between fast, intuitive pattern matching (System 1) and slow, deliberate calculation (System 2).
  - **Quick check question:** Can you explain why standard LLM autoregression is often considered "System 1" while test-time scaling mimics "System 2"?

- **Concept: Reward Modeling (ORM vs. PRM)**
  - **Why needed here:** Understanding the difference between Outcome Reward Models (final answer) and Process Reward Models (step-by-step) is critical for designing the feedback loop in RL.
  - **Quick check question:** Why might an Outcome Reward Model lead to "shortcut solutions" compared to a Process Reward Model?

- **Concept: Search Algorithms (MCTS/Beam Search)**
  - **Why needed here:** "Thinking" in this context is often implemented as searching a tree of possible thoughts.
  - **Quick check question:** How does Beam Search differ from Greedy Search in the context of exploring reasoning paths?

## Architecture Onboarding

- **Component map:** Input -> Base Reasoning LLM -> Controller (Search Strategy + Reward Model) -> Trainer (RL Algorithm + Self-Evolution Loop)
- **Critical path:**
  1. **Cold Start:** SFT on high-quality reasoning data (Long CoT)
  2. **RL Alignment:** Train using PPO/GRPO with Process/Outcome Rewards
  3. **Inference:** Activate Test-Time Scaling (Search/Sampling) to utilize compute for depth
- **Design tradeoffs:**
  - **PRM vs. ORM:** PRMs offer better supervision but are expensive to label/train; ORMs are cheap but provide sparse, potentially misleading signals
  - **Search vs. Sample:** Search (MCTS) is thorough but computationally heavy; Sampling (Best-of-N) is parallelizable but may lack depth
- **Failure signatures:**
  - **Reward Hacking:** The model generates outputs that maximize the reward function but fail logic checks (e.g., empty reasoning chains getting high scores)
  - **Context Overflow:** Long CoT exceeds context window limits during Test-Time Scaling
  - **Snowball Errors:** Accumulation of small errors in early reasoning steps that make final verification impossible
- **First 3 experiments:**
  1. **Implement Best-of-N Sampling:** Run a base model $N$ times on a math dataset, using a simple rule-based verifier (e.g., Python execution) to select the best answer
  2. **Train an Outcome Reward Model (ORM):** Fine-tune a smaller model to predict the correctness of a final answer given a question and reasoning chain
  3. **Supervised Fine-Tuning (SFT) on Distilled CoT:** Take a high-capability model (teacher), generate reasoning chains, and fine-tune a smaller student model on this "Slow Thinking" data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement learning reward models be designed to prevent "reward hacking" while ensuring signals align with genuine reasoning quality rather than superficial patterns?
- **Basis in paper:** [explicit] The authors explicitly state in Section 7 that RL methods "often suffer from training instability and reward hacking," and note the difficulty in designing robust rewards that do not incentivize "shortcut solutions."
- **Why unresolved:** Current Outcome Reward Models (ORMs) often provide sparse signals that fail to distinguish between correct answers derived from flawed logic versus sound reasoning, leading to instability.
- **What evidence would resolve it:** The development of a reward mechanism that maintains high performance on out-of-distribution tasks while showing resistance to adversarial attacks or optimization loopholes that artificially inflate scores.

### Open Question 2
- **Question:** What mechanisms enable a single model to dynamically switch between "fast" (System 1) and "slow" (System 2) thinking based on real-time task complexity?
- **Basis in paper:** [explicit] Section 7 identifies "The Balance between Fast and Slow Thinking" as a critical challenge, calling for "hybrid architectures that dynamically switch between fast and slow thinking based on task requirements."
- **Why unresolved:** While "Hybrid Thinking" (Section 6.3) frameworks exist, the paper notes that current LLMs predominantly operate in a "fast-thinking" mode by default and struggle to integrate the two modes effectively without external guidance.
- **What evidence would resolve it:** A unified framework that autonomously selects the appropriate thinking mode (e.g., simple retrieval vs. complex tree search) and demonstrates optimal trade-offs between latency/compute and accuracy across varied datasets.

### Open Question 3
- **Question:** How can reasoning models improve generalization to real-world scenarios without overfitting to specific mathematical or coding benchmarks (e.g., GSM8K, MATH)?
- **Basis in paper:** [explicit] Section 7 highlights the challenge of "Generalization vs. Over-Optimization," noting that high performance on benchmarks may mask struggles with "unfamiliar tasks or domains" and that ensuring "cross-domain robustness" is essential.
- **Why unresolved:** The survey notes that models often excel on specific datasets used for fine-tuning or reinforcement learning but may fail when faced with the diversity of real-world problem-solving.
- **What evidence would resolve it:** Demonstrated proficiency on out-of-distribution (OOD) tasks or novel domains that were not present in the training or fine-tuning data, showing that the model has learned transferable reasoning strategies rather than dataset-specific heuristics.

### Open Question 4
- **Question:** How can slow-thinking capabilities be effectively extended to multi-modal reasoning (e.g., vision and audio) while maintaining consistency across modalities?
- **Basis in paper:** [explicit] Section 7 lists "Multi-modal Reasoning Large Language Model" as a future direction, citing "challenges include aligning representations across modalities [and] ensuring consistency in reasoning."
- **Why unresolved:** The paper notes that current LLMs are primarily text-based, limiting their ability to handle complex tasks requiring the simultaneous interpretation of text, images, and diagrams.
- **What evidence would resolve it:** A slow-thinking model that successfully applies long Chain-of-Thought (CoT) and verification mechanisms to multi-modal inputs (e.g., scientific charts or medical scans) with performance comparable to text-only reasoning.

## Limitations

- The analysis is primarily theoretical, aggregating mechanisms from multiple papers without direct empirical validation of the combined systems
- Specific performance claims and effectiveness of individual mechanisms remain dependent on unpublished implementation details and hyperparameter choices
- Claims about specific performance improvements, compute-efficiency tradeoffs, and self-evolution capabilities often lack the quantitative evidence needed to establish general principles

## Confidence

- **High Confidence:** The categorization framework distinguishing test-time scaling, RL training, and slow-thinking architectures is well-supported by the corpus and represents a clear structural understanding of the field
- **Medium Confidence:** The described mechanisms (MCTS, Beam Search, PRM/ORM training, GRPO) are technically sound based on their original papers, but their specific effectiveness when combined into reasoning LLMs varies significantly across implementations
- **Low Confidence:** Claims about specific performance improvements, compute-efficiency tradeoffs, and self-evolution capabilities often lack the quantitative evidence needed to establish general principles, as they depend heavily on model size, dataset quality, and evaluation protocols

## Next Checks

1. **Implement a Minimal GRPO System:** Reproduce the Group Relative Policy Optimization training pipeline on a small reasoning dataset (e.g., GSM8K) to empirically verify reward stability and learning dynamics described in the survey
2. **Benchmark Search vs. Sampling:** Conduct controlled experiments comparing MCTS, Beam Search, and Best-of-N sampling on identical reasoning tasks with the same base model and verifier to quantify the claimed tradeoffs in depth versus efficiency
3. **Analyze Reward Hacking Vulnerabilities:** Systematically test a trained reasoning LLM against adversarial inputs designed to trigger shortcut solutions or reward maximization without logical correctness, measuring the prevalence and severity of this failure mode