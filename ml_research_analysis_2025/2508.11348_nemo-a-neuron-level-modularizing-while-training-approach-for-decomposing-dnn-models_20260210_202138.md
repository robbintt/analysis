---
ver: rpa2
title: 'NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing
  DNN Models'
arxiv_id: '2508.11348'
source_url: https://arxiv.org/abs/2508.11348
tags:
- nemo
- training
- class
- loss
- modular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NeMo, a novel modularizing-while-training (MwT)
  approach for decomposing deep neural network (DNN) models at the neuron level. Unlike
  existing methods that focus on convolutional kernels in CNNs or special structural
  components in Transformers, NeMo operates at the fundamental neuron level, making
  it applicable to various DNN architectures including Transformers.
---

# NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models

## Quick Facts
- **arXiv ID:** 2508.11348
- **Source URL:** https://arxiv.org/abs/2508.11348
- **Reference count:** 40
- **Primary result:** NeMo achieves 1.72% average improvement in module classification accuracy and 58.10% reduction in module size compared to state-of-the-art MwT method

## Executive Summary
This paper proposes NeMo, a novel modularizing-while-training (MwT) approach that operates at the neuron level to decompose deep neural networks. Unlike existing methods focused on convolutional kernels or structural components, NeMo's neuron-level approach enables modularization across diverse architectures including Transformers and CNNs. The method introduces a contrastive learning-based composite loss function that simultaneously optimizes task performance, cohesion, and coupling. Comprehensive experiments on ViT, DeiT, and CNN models demonstrate significant improvements in both accuracy and module efficiency.

## Method Summary
NeMo introduces a neuron-level modularization framework by attaching mask generator modules to linear layers in DNNs. During training, these generators output continuous masks that are applied to layer outputs, while a contrastive loss function optimizes for class-specific neuron activation patterns. The approach differs from previous methods by operating at the fundamental neuron level rather than kernel or structural component levels. After training, binary masks are generated for specific classes and used to physically prune neurons from the model, followed by fine-tuning with a new classification head to restore performance.

## Key Results
- Achieves 1.72% average improvement in module classification accuracy over state-of-the-art MwT method
- Reduces module size by 58.10% compared to baseline approach
- Demonstrates effectiveness across two Transformer models (ViT, DeiT) and four CNN models
- Particularly effective for large-scale Transformer-based models while maintaining strong CNN performance

## Why This Works (Mechanism)

### Mechanism 1
Operating at the neuron level allows the modularization framework to generalize across different DNN architectures, including Transformers and CNNs, where previous methods were architecture-specific. By defining the fundamental unit of modularization as the "neuron" (which maps to an output channel in a linear layer or a convolutional kernel in a CNN layer), the approach creates a universal target for masking and pruning. This abstraction bypasses the need for specialized structural components like attention heads.

### Mechanism 2
Contrastive learning-based loss functions provide a more stable and effective way to simultaneously optimize for task performance, inter-class separation (low coupling), and intra-class consistency (high cohesion) compared to simple summation of losses. The method defines cohesion as the similarity of neuron masks for samples of the same class and coupling as the similarity for samples of different classes, then uses a contrastive loss to maximize their ratio.

### Mechanism 3
Structured modularization by physically removing neurons (via their corresponding weights) from the model yields efficient, on-demand sub-modules with minimal accuracy loss after fine-tuning. After training with the neuron identifier, a binary mask for a target functionality is generated and used to prune entire rows from weight matrices in linear layers or entire kernels in convolutional layers.

## Foundational Learning

- **Concept: Contrastive Learning**
  - **Why needed here:** Understanding contrastive learning is critical because NeMo's core novelty is its use of a contrastive loss function to optimize the cohesion and coupling of neuron masks.
  - **Quick check question:** Can you explain how the contrastive loss function in NeMo differs from simply adding a cohesion loss term and a coupling loss term together?

- **Concept: Neuron Masks and Structured Pruning**
  - **Why needed here:** The paper's method relies on generating a continuous mask for each neuron and then using that mask to perform structured pruning.
  - **Quick check question:** How does removing a "neuron" in a linear layer correspond to an operation on the layer's weight matrix?

- **Concept: Cohesion and Coupling**
  - **Why needed here:** These are the primary metrics used to quantify the success of modularization. Cohesion measures how consistently the same neurons are used for a single class, while coupling measures the overlap between neurons used for different classes.
  - **Quick check question:** For a well-modularized model, would you expect high or low cohesion and high or low coupling? Why?

## Architecture Onboarding

- **Component map:** DNN Model -> Neuron Identifier (Mask Generators G) -> Modular Training Loop -> Modularizer -> Sub-model

- **Critical path:**
  1. Attach Neuron Identifier: Integrate the G mask generators with the model's layers
  2. Modular Training: Train the combined model, optimizing for both classification accuracy and modular contrastive loss
  3. Generate Masks: Run a forward pass with the training data to aggregate neuron activations per class and generate binary masks
  4. Decompose: Physically prune the model's weight matrices based on the generated masks for the target functionality
  5. Fine-tune: Train the new sub-model with a new classification head on the target task's data

- **Design tradeoffs:**
  - Granularity: Neuron-level is a good balance between weight-level (too fine) and structural-level (too coarse)
  - Hyperparameters: Introduces α (weighting factor for contrastive loss) and τ (temperature), with α's effect being predictable but crucial for balancing accuracy and NRR
  - Overhead: Modular training introduces 30-70% computational overhead, justified only for extensively reused models

- **Failure signatures:**
  - High accuracy loss post-modular training: α may be too high, forcing too much separation
  - Sub-model fails to fine-tune: Pruning may be too aggressive or base model lacks capacity
  - High coupling/low cohesion: Temperature τ might be incorrectly set or model too small

- **First 3 experiments:**
  1. Ablation on Loss Function: Compare NeMo's contrastive loss against MwT's linear summation loss on VGG16 with CIFAR10
  2. Granularity Analysis: Compare NeMo (neuron-level) against attention-head-level modularization on Vision Transformer
  3. On-Demand Reuse Scaling: Test sub-models for different class counts (2, 5, 10 classes) measuring FLOPs reduction vs fine-tuning accuracy

## Open Questions the Paper Calls Out

- **Question 1:** Can NeMo be adapted for generative models (e.g., GPTs) without incurring prohibitive computational overhead during modular training?
  - **Basis:** Section 5.3.1 states applying NeMo to generative models is "highly challenging" due to token-level identification overhead
  - **What would resolve:** A modified NeMo framework applied to a generative transformer achieving modular decomposition with acceptable training time complexity

- **Question 2:** How can NeMo maintain modularization effectiveness when scaling to datasets with thousands of classes, such as ImageNet?
  - **Basis:** Section 5.3.3 notes performance degradation as class count increases on ImageNet (1000 classes)
  - **What would resolve:** Experimental results on ImageNet-1K showing modified NeMo maintains high cohesion and low coupling without significant accuracy loss

- **Question 3:** Can the modularization approach be refined to achieve low coupling in tasks inherently requiring high feature entanglement, such as object detection?
  - **Basis:** Section 5.1 shows NeMo applied to DETR resulted in high coupling (0.78)
  - **What would resolve:** A specialized adaptation of the loss function successfully reducing coupling in a detection model to levels comparable with classification tasks

## Limitations
- Experimental validation limited to two small image datasets (CIFAR10, SVHN) and four relatively small models
- Reported improvements may not scale linearly to larger, more complex architectures or diverse tasks
- Mask Generator architecture is underspecified, creating uncertainty about reproducibility
- Computational overhead of 30-70% during modular training may be prohibitive for some applications

## Confidence

- **High:** The general concept of neuron-level modularization and the problem formulation are well-established
- **Medium:** The contrastive loss formulation and its advantages over simple summation are theoretically plausible but require more rigorous ablation studies
- **Medium:** The reported quantitative improvements are based on specific experimental conditions and model scales

## Next Checks

1. **Ablation Study on Loss Components:** Systematically remove the contrastive component and test with only CE loss plus simple cohesion/coupling additions to isolate the benefit of the exponential temperature scaling

2. **Scaling Validation:** Test NeMo on a larger-scale Transformer model (e.g., ViT-Base) on a more complex dataset (e.g., ImageNet) to verify the claimed improvements hold at scale

3. **Granularity vs. Performance Analysis:** Compare NeMo's neuron-level modularization against a coarse-grained attention-head-level modularization on the same ViT model to quantify the trade-off between module size and functionality preservation