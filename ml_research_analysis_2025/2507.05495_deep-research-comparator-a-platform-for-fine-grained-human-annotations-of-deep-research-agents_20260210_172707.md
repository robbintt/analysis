---
ver: rpa2
title: 'Deep Research Comparator: A Platform For Fine-grained Human Annotations of
  Deep Research Agents'
arxiv_id: '2507.05495'
source_url: https://arxiv.org/abs/2507.05495
tags:
- research
- deep
- agents
- agent
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep Research Comparator addresses the challenge of evaluating
  deep research agents that autonomously generate long-form reports by providing a
  platform for holistic and fine-grained human annotations. The platform displays
  final reports and intermediate steps from two agents side-by-side, enabling pairwise
  comparisons for overall quality assessment and fine-grained feedback on specific
  text spans and steps.
---

# Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents

## Quick Facts
- arXiv ID: 2507.05495
- Source URL: https://arxiv.org/abs/2507.05495
- Authors: Prahaladh Chandrahasan; Jiahe Jin; Zhihan Zhang; Tevin Wang; Andy Tang; Lucy Mo; Morteza Ziyadi; Leonardo F. R. Ribeiro; Zimeng Qiu; Markus Dreyer; Akari Asai; Chenyan Xiong
- Reference count: 36
- Primary result: Human preference votes closely aligned with static benchmarks for overall report quality, while fine-grained upvote rates revealed meaningful divergence in agent performance.

## Executive Summary
Deep Research Comparator introduces a platform for evaluating deep research agents through holistic pairwise comparisons and fine-grained feedback on intermediate steps and text spans. The system displays final reports and generation steps from two agents side-by-side, enabling annotators to vote on overall quality and provide specific feedback through upvotes/downvotes on steps and highlighted text. To facilitate evaluation across different large language models, the authors developed Simple Deepresearch, a prompt-based agent scaffold that transforms various LLMs into deep research agents via a unified JSON interface.

## Method Summary
The evaluation combines pairwise comparison using the Bradley-Terry model for overall quality ranking with fine-grained upvote/downvote feedback on intermediate steps and text spans. 17 annotators evaluated 176 user queries across diverse domains, comparing three different deep research agents. Simple Deepresearch serves as a prompt-based scaffold that transforms LLMs into research agents through iterative plan-search-script-summary-answer cycles, outputting results via a standardized JSON format. The platform captures both holistic preference votes and granular feedback to provide comprehensive performance assessment beyond static benchmarks.

## Key Results
- Human preference votes showed strong alignment with static benchmark DeepResearchGym scores for overall report quality
- Fine-grained upvote rates revealed meaningful divergence from overall quality rankings, demonstrating the platform captures nuanced performance aspects
- The standardized JSON interface enabled low-effort integration of heterogeneous agent architectures into the evaluation platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise comparison using the Bradley-Terry model provides reliable overall quality rankings that align with static benchmark evaluations.
- Mechanism: Annotators view two agents' final reports side-by-side and vote for one of four options (Agent A, Agent B, Tie, Both Bad). These pairwise preference votes are converted into Bradley-Terry scores, which produce a ranking of agents by overall report quality.
- Core assumption: Human annotators can consistently judge relative report quality when presented with direct comparisons, and preferences aggregate meaningfully into a scalar ranking.
- Evidence anchors:
  - [abstract] "human preference votes closely aligned with static benchmark results for overall report quality"
  - [section 5.3] "We observe a strong alignment between our human evaluation results with static benchmark evaluation... the scores from the static benchmark perfectly matches the human-derived report-preference ranking"
  - [corpus] Weak direct corpus support; RankArena (arXiv:2508.05512) uses similar pairwise comparison but for RAG systems, not deep research agents.
- Break condition: If annotators lack domain expertise for specific queries, or if query difficulty varies too widely, pairwise comparisons may reflect annotator confusion rather than quality differences.

### Mechanism 2
- Claim: Fine-grained upvote/downvote feedback on intermediate steps and text spans captures performance aspects that holistic scores miss.
- Mechanism: Annotators can upvote or downvote individual intermediate steps during generation and highlight specific text spans in the final report for feedback. The upvote rate (upvotes / [upvotes + downvotes]) quantifies fine-grained performance separately from overall rankings.
- Core assumption: Annotators can accurately identify which specific steps or text spans are problematic versus high-quality, and this signal is meaningful for agent optimization.
- Evidence anchors:
  - [abstract] "fine-grained upvote rates revealed meaningful divergence, demonstrating the platform's ability to capture nuanced aspects of agent performance beyond holistic scores"
  - [section 5.3] "there's a divergence between this overall quality based ranking and the fine-grained metric upvote rates"
  - [corpus] AgentHallu (arXiv:2601.06818) supports the premise that multi-step agent workflows require step-level diagnosis, though focused on hallucination attribution.
- Break condition: If annotators provide sparse or inconsistent fine-grained feedback (e.g., only voting on obvious failures), the signal may be too noisy for downstream use.

### Mechanism 3
- Claim: A unified JSON interface enables low-effort integration of heterogeneous agent architectures into the evaluation platform.
- Mechanism: Agents stream intermediate steps and final reports via a standardized JSON format (`intermediate_steps`, `final_report`, `is_intermediate`, `is_complete`, `citations`). The backend routes queries to agent services in Docker containers, abstracting away internal agent architecture differences.
- Core assumption: The standardized fields capture sufficient information for meaningful annotation without constraining agent design.
- Evidence anchors:
  - [section 3.2] "Agents that generate intermediate steps and final report via a unified JSON interface can be integrated with minimal engineering effort... imposes no constraints on the agent's internal architecture"
  - [appendix C] Shows the specific JSON schema used
  - [corpus] No direct corpus comparison for this specific interface design; related platforms (Search Arena, DeepResearchGym) do not emphasize integration flexibility.
- Break condition: If an agent's intermediate steps are not easily representable as text (e.g., complex tool use, multimodal outputs), the JSON interface may require extension or lose meaningful information.

## Foundational Learning

- Concept: Bradley-Terry Model
  - Why needed here: Converts pairwise preference votes into scalar scores and rankings. Without understanding this, you cannot interpret the "BT Score" column or explain why the baseline is fixed at 1000.
  - Quick check question: Given 3 agents where A beats B 60% of the time and B beats C 70% of the time, would you expect A's BT score to be higher than C's?

- Concept: Agentic Search Workflow
  - Why needed here: Deep research agents iterate through plan → search → script → summary → answer cycles. Understanding this is essential for annotating intermediate steps meaningfully.
  - Quick check question: In Simple Deepresearch, what action type should the agent emit when it believes it has gathered sufficient information to produce the final report?

- Concept: Process vs Outcome Evaluation
  - Why needed here: The paper's core contribution is showing that process-level feedback (step votes) and span-level feedback (text highlights) provide different signals than outcome-level feedback (overall preference). This distinction matters for downstream use in RL or reward modeling.
  - Quick check question: If an agent produces an excellent final report but makes redundant search queries in intermediate steps, which evaluation type would penalize this behavior?

## Architecture Onboarding

- Component map:
  - Frontend (S3 + CloudFront CDN) -> Main Backend Service (CPU node) -> Agent Serving Service (Docker containers) -> LLM API or GPU nodes -> Search API

- Critical path: User submits query → Main Backend routes to two Agent Serving Services → Agents stream intermediate steps (via unified JSON) → Final reports displayed → Annotator votes and provides fine-grained feedback → Metrics Job computes rankings and rates

- Design tradeoffs:
  - Standardized JSON interface minimizes integration effort but may not capture agent-specific information (e.g., tool schemas, multimodal outputs)
  - Streaming intermediate steps enables real-time annotation but increases frontend complexity
  - Side-by-side comparison provides relative quality signal but requires running two agents per query (2x inference cost)

- Failure signatures:
  - No intermediate steps displayed: Agent not conforming to JSON schema; check `is_intermediate` flag
  - Rankings not updating: Metrics Calculation Job may have failed; check CronJob logs and database writes
  - High "Both Bad" vote rate: Query may be out-of-scope or agents are insufficient for task; review query distribution

- First 3 experiments:
  1. **Smoke test with Simple Deepresearch**: Deploy the baseline scaffold with a single LLM (e.g., Gemini 2.5 Flash), submit 5 queries, verify intermediate steps stream correctly and votes are recorded.
  2. **Pairwise comparison validation**: Run the same agent against itself (A/B test with identical configuration) to confirm "Tie" rate is high (>80%), validating annotation interface neutrality.
  3. **Fine-grained signal check**: Compare upvote rates between an agent configured with strong search API (e.g., Serper) vs. weak API (e.g., limited index); verify step-level upvote rates reflect expected quality differences.

## Open Questions the Paper Calls Out

- Can the collected fine-grained step and span annotations serve as effective reward signals for training deep research agents via reinforcement learning?
- Does optimizing an agent to maximize fine-grained upvote rates for intermediate steps lead to improved holistic report quality scores?
- What is the inter-annotator agreement rate for the fine-grained text span and intermediate step annotations?

## Limitations
- The unified JSON interface may not capture all relevant agent behaviors, particularly for agents with complex tool use or multimodal outputs, potentially limiting evaluation scope
- With only 17 annotators evaluating 176 queries, the sample size may be insufficient to capture annotator bias or query difficulty effects on pairwise comparisons
- The Bradley-Terry model assumes consistent annotator preferences, but domain expertise variation across the diverse query set could violate this assumption
- The static benchmark alignment is promising but only tested against one benchmark (DeepResearchGym), leaving open whether the same alignment holds for other evaluation frameworks

## Confidence
- **High confidence**: The platform architecture and JSON interface enable integration of heterogeneous agents; the observed divergence between overall rankings and fine-grained upvote rates is methodologically sound
- **Medium confidence**: The alignment between human preference votes and static benchmark results is valid for this specific benchmark and agent set, but may not generalize to other evaluation frameworks
- **Medium confidence**: The claim that pairwise comparison + fine-grained feedback provides more nuanced insights than holistic scores alone is supported, but requires further validation across different query distributions and agent architectures

## Next Checks
1. **Cross-benchmark validation**: Evaluate the same agent set using an independent benchmark (e.g., HotpotQA, QuALITY) to test whether human preference rankings consistently align across different evaluation frameworks
2. **Annotator expertise stratification**: Analyze whether pairwise comparison reliability and fine-grained feedback quality vary by annotator domain expertise, and whether results remain stable when restricting to expert subsets
3. **Fine-grained feedback coverage analysis**: Measure the proportion of intermediate steps and text spans that receive annotations, and test whether the signal remains meaningful when considering only sparsely annotated elements versus the full distribution