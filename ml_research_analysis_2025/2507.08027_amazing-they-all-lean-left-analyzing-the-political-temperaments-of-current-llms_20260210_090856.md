---
ver: rpa2
title: '"Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current
  LLMs'
arxiv_id: '2507.08027'
source_url: https://arxiv.org/abs/2507.08027
tags:
- political
- liberal
- more
- they
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study analyzed the political temperaments of seven prominent
  large language models using Moral Foundations Theory, political ideology scales,
  and a new index of current political controversies. Results showed a strong and
  consistent liberal orientation across models, with prioritization of care and fairness
  values.
---

# "Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs

## Quick Facts
- **arXiv ID**: 2507.08027
- **Source URL**: https://arxiv.org/abs/2507.08027
- **Reference count**: 40
- **Primary result**: Seven tested LLMs showed consistent liberal orientation across multiple political temperament measures, attributed to training data, fine-tuning practices, and emergent properties of democratic discourse

## Executive Summary
This study analyzed the political temperaments of seven prominent large language models using Moral Foundations Theory, established political ideology scales, and a new index of current political controversies. The researchers found a strong and consistent liberal orientation across all tested models, with prioritization of care and fairness values. The paper attributes this "liberal tilt" to factors including liberal-leaning training corpora, reinforcement learning from human feedback that penalizes conservative moral frameworks, and safety-driven fine-tuning practices. The study distinguishes between political bias and legitimate epistemic differences, arguing that the observed pattern is an emergent property of training on democratic, rights-focused discourse rather than a programming error.

## Method Summary
The researchers administered a battery of 12 political ideology scales (including Pew Research items, ANES items, and Moral Foundations Theory prompts) to seven LLMs, requesting both self-scores and estimated "typical American" scores for comparison. They also created a new 7-item index of current political controversies. The study compared base and fine-tuned model pairs where available, using Meta-Llama-3.1-8B and its instruct version. Scores were calculated using standard scoring rules for each scale, with lower scores indicating more liberal positions. The analysis included MFT ranking and proportional allocations to understand which moral foundations models prioritize.

## Key Results
- All seven tested LLMs showed a liberal orientation across multiple political temperament measures
- Base and fine-tuned model pairs revealed that fine-tuning generally increases liberal lean, with Llama-3.1-8B-Instruct showing significantly more liberal scores than the base model
- MFT rankings consistently showed "Care" and "Fairness" as top priorities, with "Loyalty," "Authority," and "Purity" ranked lower
- The weighted average percentage difference from estimated typical American scores ranged from 5% to 15% liberal across models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Liberal orientation is an artifact of training corpora distribution, which over-represents Western academic perspectives that prioritize care and fairness
- **Mechanism**: LLMs learn probabilistic associations from training data; if data over-represents demographics prioritizing certain moral foundations, model weights encode these as default "correct" responses
- **Core assumption**: Internet and curated datasets are structurally aligned with liberal moral foundations due to content creator demographics
- **Evidence anchors**: Abstract attributes trend to "liberal-leaning training corpora"; page 4 notes data drawn from "environments where liberal moral foundations are more culturally dominant"
- **Break condition**: Strictly balanced training data including equal representations of traditionalist, religious, or nationalist corpora would shift default orientation

### Mechanism 2
- **Claim**: RLHF and safety fine-tuning amplify liberal values by penalizing outputs perceived as harmful or exclusionary
- **Mechanism**: Human raters reward "safe" and "inclusive" answers; care/fairness-based answers are safer than authority/purity-based ones, shaping reward models to prefer the former
- **Core assumption**: "Safety" and "harmlessness" in AI alignment correlate with liberal ethical frameworks
- **Evidence anchors**: Abstract mentions "safety-driven fine-tuning practices"; page 4 states fine-tuning "penalizes authority/purity logic as risky"
- **Break condition**: Reward models trained to value "viewpoint diversity" or "neutrality" over "harm reduction" would reduce safety-induced tilt

### Mechanism 3
- **Claim**: Liberal tilt is an emergent property of training on democratic, rights-focused discourse, functioning like a "veil of ignorance"
- **Mechanism**: Without specific identity to protect, models optimize for "every-person" perspective, defaulting to universalist principles (utilitarianism, Rawlsian justice) found in high-quality philosophical texts
- **Core assumption**: Removing self-interest from decision-making naturally pushes moral reasoning toward universalist/liberal conclusions
- **Evidence anchors**: Abstract argues pattern is "emergent property of training on democratic, rights-focused discourse"; page 15 compares LLM logic to John Rawls' "veil of ignorance"
- **Break condition**: Prompts adopting specific personas with concrete interests collapse the "veil of ignorance" effect

## Foundational Learning

- **Concept: Moral Foundations Theory (MFT)**
  - **Why needed here**: Primary analytic framework categorizing models across five axes (Care, Fairness, Loyalty, Authority, Purity) that map to political leanings
  - **Quick check question**: Which three foundations are typically associated with conservative moral reasoning?

- **Concept: Base vs. Instruct/Fine-Tuned Models**
  - **Why needed here**: Distinguishes between raw pre-trained model and chat version; understanding "etiquette layer" crucial for isolating data vs. alignment effects
  - **Quick check question**: According to the paper, does the base model or the fine-tuned model typically show stronger liberal lean?

- **Concept: System Justification Theory**
  - **Why needed here**: One of 12 scales measuring tendency to defend status quo; explains why models might score high on certain "conservative" metrics despite overall liberal tilt
  - **Quick check question**: Why might a model trained on mainstream media score higher on System Justification than other conservative metrics?

## Architecture Onboarding

- **Component map**: Political/Ideology Scales -> 7 LLMs -> MFT Ranking + Weighted Ideology Scores -> "Liberal Lean" percentage

- **Critical path**:
  1. Administer 12-item ideology battery (Appendix A) to target model
  2. Calculate raw scores (handling reverse-scored items like ANES Sexism)
  3. Ask model to estimate "typical American" score for comparison
  4. Compute delta (Model Score - Estimated Human Score) to determine "lean"

- **Design tradeoffs**:
  - Self-Report vs. Empirical: Uses model self-reports to estimate fine-tuning effects (efficient but relies on model's "understanding" of own weights)
  - Static Scales vs. Current Events: Established scales allow historical comparison but may miss modern culture war dynamics (hence "New Political Issues Scale")

- **Failure signatures**:
  - Refusal/Deference: Models refuse to take stance, defaulting to "I am an AI" (requires nudging for quantifiable choice)
  - Parroting Literature: On well-known tests, models recite academic consensus rather than "reasoning," inflating alignment appearance

- **First 3 experiments**:
  1. Replicate Base/Fine-tune Delta: Run ANES Sexism scale on Llama-3.1-8B base vs instruct version to confirm reported swing
  2. MFT Ranking Test: Ask model to rank 5 foundations; verify "Care" and "Fairness" consistently ranked 1 and 2
  3. Veil of Ignorance Probe: Prompt political questions first as "generic AI" then as specific persona to test if liberal tilt disappears with artificial interests

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent is observed "liberal tilt" product of foundational pre-training versus post-hoc fine-tuning and RLHF?
- **Basis in paper**: [explicit] Authors note they "have taken advantage of one pretrained-fine-tuned pair" and rely on model self-reports, acknowledging need for more data
- **Why unresolved**: Empirical verification limited to single model pair; proprietary restrictions prevented testing base versions of other models
- **Evidence**: Empirical testing of full political ideology battery on base versions of major proprietary models compared to final instruct versions

### Open Question 2
- **Question**: How can researchers operationally distinguish between harmful "political bias" and legitimate "epistemic differences" in LLM outputs?
- **Basis in paper**: [explicit] Paper explicitly critiques literature for conflating bias (distorted thinking) with epistemic differences (legitimate value prioritization), warning this conflation is "self-defeating"
- **Why unresolved**: Offers conceptual distinction but provides no empirical framework to separate two phenomena in benchmark evaluations
- **Evidence**: Validated evaluation framework differentiating responses based on logical errors/stereotypes (bias) versus consistent moral prioritization (epistemic difference)

### Open Question 3
- **Question**: Do LLMs effectively approximate John Rawls' "veil of ignorance" to offer superior "every-person" perspective, or does data weighting undermine this potential?
- **Basis in paper**: [explicit] Authors propose LLMs may echo "veil-of-ignorance" aspiration but admit training databases are "incomplete, unrepresentative, and awkwardly weighted"
- **Why unresolved**: Unclear if lack of personal identity results in genuine objectivity or consistent reflection of specific demographics in training corpus
- **Evidence**: Comparative studies assessing whether LLM ethical decisions align more closely with "original position" fairness criteria than diverse human control groups

## Limitations
- Reliance on model self-reports for training data analysis and base vs. fine-tuned comparisons limits empirical verification
- Sample size of seven models may not capture full spectrum of LLM political orientations, particularly models developed outside Western contexts
- Philosophical distinction between "bias" and "legitimate epistemic differences" remains contested without empirical validation framework

## Confidence
- **High confidence**: Empirical observation that all tested models show liberal-leaning scores on established political scales; methodological approach is well-grounded and replicable
- **Medium confidence**: Three proposed mechanisms (training corpora distribution, RLHF amplification, veil of ignorance effect) are plausible but not directly empirically verified
- **Low confidence**: Philosophical distinction between "bias" and "legitimate epistemic differences" and claim that liberal orientation represents "emergent property" rather than programming artifact

## Next Checks
1. **Empirical verification of training data distribution**: Analyze actual training corpora composition to measure relative representation of liberal versus conservative moral frameworks
2. **Direct measurement of reward model preferences**: Examine RLHF reward model weights to determine whether they systematically favor care/fairness responses over authority/purity responses
3. **Cross-cultural validation**: Test non-Western LLMs (Chinese, Arabic, Hindi models) on same scales to determine whether liberal tilt is universal or specific to Western-trained models