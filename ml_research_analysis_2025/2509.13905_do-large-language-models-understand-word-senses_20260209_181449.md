---
ver: rpa2
title: Do Large Language Models Understand Word Senses?
arxiv_id: '2509.13905'
source_url: https://arxiv.org/abs/2509.13905
tags:
- word
- definition
- llms
- table
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) such as GPT-4o and DeepSeek-V3 achieve
  Word Sense Disambiguation (WSD) performance comparable to specialized systems, with
  accuracy up to 83% on standard benchmarks. When allowed to generate definitions
  or explanations freely, their accuracy improves further, reaching up to 98% in unconstrained
  settings.
---

# Do Large Language Models Understand Word Senses?

## Quick Facts
- **arXiv ID:** 2509.13905
- **Source URL:** https://arxiv.org/abs/2509.13905
- **Reference count:** 40
- **Primary result:** LLMs achieve WSD accuracy up to 83% on standard benchmarks, improving to 98% in unconstrained generative settings.

## Executive Summary
This paper systematically evaluates whether large language models (LLMs) truly understand word senses by testing them on Word Sense Disambiguation (WSD) tasks. The study compares LLMs like GPT-4o and DeepSeek-V3 against specialized WSD systems across multiple benchmarks and task formats. Results show LLMs achieve competitive performance (~83% accuracy) with standard WSD, but dramatically outperform when allowed to generate definitions freely (up to 98%). LLMs also demonstrate greater robustness across diverse and challenging datasets compared to specialized systems, though they still fall short of human-level performance on infrequent or subtle sense distinctions.

## Method Summary
The paper evaluates LLMs on WSD using definition selection and three generative tasks (definition, explanation, example). Models are tested in zero-shot, one-shot, and three-shot settings using 20 prompt variations. Answer extraction uses lexical overlap between model outputs and WordNet definitions. Benchmarks include Maru et al. 2022 (refined Raganato), 42D (cross-domain), hardEN (challenging), and a private WikiPortal WSD dataset. Human evaluation is used for generative tasks with 4-category schemes.

## Key Results
- LLMs achieve up to 83% accuracy on standard WSD benchmarks, comparable to specialized systems
- Performance improves to 98% when models generate definitions freely without predefined sense inventories
- LLMs show greater robustness across domain shifts and challenging datasets than specialized WSD systems
- Smaller models exhibit strong positional bias, with performance dropping ~16 points when definitions are shuffled

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs demonstrate superior semantic understanding in unconstrained generative tasks compared to constrained classification tasks.
- Mechanism: When asked to freely explain or define word meanings without choosing from predefined lists, LLMs achieve 98% accuracy versus ~83% on standard WSD. This suggests their internal word sense representations are richer and more continuous than discrete inventories allow, and generation tasks better align with their fundamental training objectives.
- Core assumption: Higher performance in free-form tasks indicates deeper semantic understanding rather than simply easier evaluation criteria.
- Evidence anchors: LLM free-form explanation accuracy reaches 98% (abstract); removing constraints allows more accurate expression of contextual meaning up to 98% (section 4).
- Break condition: If constrained WSD performance consistently matched or exceeded generative task performance.

### Mechanism 2
- Claim: LLMs exhibit greater cross-domain robustness than specialized WSD systems through broader knowledge acquisition.
- Mechanism: Larger instruction-tuned LLMs leverage diverse pre-training data to maintain performance across domain shifts. While specialized systems excel on "easy" cases (88.5% on non-hardEN items), they score near 0% on hardEN, whereas LLMs maintain ~40% on hardEN and higher on diverse domains (42D).
- Core assumption: Performance gaps on challenging datasets reflect genuine generalization differences.
- Evidence anchors: LLMs show greater robustness across diverse and challenging datasets (abstract); GPT-4o demonstrates more balanced behavior across all cases versus specialized systems failing on hardest instances (section 3.2).
- Break condition: If specialized WSD systems consistently outperformed LLMs across all domain-shift and difficulty benchmarks.

### Mechanism 3
- Claim: LLM performance on WSD tasks is significantly influenced by prompt design and exhibits positional bias.
- Mechanism: Accuracy varies by ~5 percentage points across different prompt templates, and shuffling definition order causes performance drops (especially in 1B parameter models). The paper tested 20 prompt variations and found question-based prompts performed best overall.
- Core assumption: Performance variations reflect underlying reasoning limitations rather than evaluation methodology issues.
- Evidence anchors: Performance varies across 20 prompt templates (section 3.2); Llama-3.2-1B shows 16-point drop with shuffled definitions (table 2).
- Break condition: If all prompt variations and shuffling experiments produced statistically identical results.

## Foundational Learning

- Concept: **Word Sense Disambiguation (WSD)**
  - Why needed here: WSD is the fundamental task being evaluated—determining which meaning of a polysemous word is activated in context. Understanding this task is prerequisite to interpreting all experimental results.
  - Quick check question: In "The bank approved my loan application," what is the correct sense of "bank": (a) financial institution or (b) sloping land beside a river?

- Concept: **Sense Inventory**
  - Why needed here: The paper examines limitations of predefined sense inventories (like WordNet) and shows that removing this constraint improves LLM performance. Understanding what sense inventories are—and why they might limit evaluation—is essential for grasping the core contribution about generative vs. classification evaluation.
  - Quick check question: If a model generates a perfect explanation of a word's meaning that doesn't match any definition in WordNet, would it score correctly on traditional WSD benchmarks?

- Concept: **Instruction Tuning**
  - Why needed here: All evaluated models are instruction-tuned to follow natural language commands. This is crucial because WSD is framed as an instruction ("Choose the correct dictionary definition...") and performance depends on the model's ability to parse and comply with such directives.
  - Quick check question: Why might a base pre-trained model struggle with a prompt like "Provide as output only the correct dictionary definition"?

## Architecture Onboarding

- Component map: Input Processor -> LLM Engine -> Answer Extraction Module -> Evaluation Layer
- Critical path: 1. Select prompt template (Prompt 2 recommended) 2. Format WSD instance: sentence + target word + candidate definitions 3. Execute inference on instruction-tuned LLM 4. Extract answer via lexical overlap scoring 5. Validate against gold definitions and compute F1
- Design tradeoffs:
  - Constrained vs. Unconstrained Evaluation: WSD enables automated scoring but may underestimate semantic understanding; free-form generation reveals richer capabilities but requires human evaluation
  - Lexical Overlap vs. Perplexity Extraction: Paper chose lexical overlap for cross-model compatibility; validation tests showed minimal difference between methods
  - Prompt Selection: 20 templates tested; selected Prompt 2 (question-based) for best mean performance across model sizes
- Failure signatures:
  - Positional Bias: Model preferentially selecting early options (especially in 1B parameter models with ~16-point drop on shuffled definitions)
  - Overgeneralization Errors: Selecting overly broad senses (e.g., "human being" instead of "adult male")
  - Metonymy Confusion: Conflating related senses (e.g., "government" vs. "people of a country")
  - Prompt Sensitivity: >5-point variance across prompt phrasings indicating unstable reasoning
  - Generation Hallucination: Producing plausible but incorrect definitions/examples in unconstrained tasks
- First 3 experiments:
  1. Baseline Validation: Reproduce GPT-4o WSD performance (~83% F1) on Maru et al. 2022 benchmark using Prompt 2
  2. Positional Bias Test: Run 50-instance subset with standard vs. shuffled definition ordering on Llama-3.2-1B
  3. Generative vs. Classification Comparison: For 20 instances where WSD fails, manually compare model's free-form explanation quality against its incorrect classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on word sense understanding tasks in languages other than English, where annotated WSD data and sense inventories are sparse?
- Basis in paper: [explicit] All experiments conducted exclusively in English; authors leave multilingual evaluation to future work due to limited manually annotated data and incomplete sense inventories in other languages.
- Why unresolved: Multilingual WSD requires different sense inventories (often incomplete) and manually annotated benchmarks, which are scarce outside English.
- What evidence would resolve it: Systematic evaluation of LLMs on multilingual WSD benchmarks and generative sense explanation tasks across diverse languages.

### Open Question 2
- Question: Can adaptive or task-specific prompting techniques reduce the observed sensitivity of LLMs to prompt phrasing in WSD tasks?
- Basis in paper: [explicit] Performance varied across 20 prompt templates with ~5-point gaps between best and worst prompts; authors suggest future work could explore adaptive and task-specific prompting techniques.
- Why unresolved: Paper demonstrates prompt sensitivity but does not investigate whether techniques like prompt optimization or chain-of-thought could stabilize performance.
- What evidence would resolve it: Comparative study evaluating LLM WSD performance using optimized prompts versus fixed templates, measuring variance reduction.

### Open Question 3
- Question: What mechanisms or training approaches could help LLMs better handle infrequent and long-tail word senses where they currently underperform?
- Basis in paper: [inferred] LLMs still fall short of human-level performance particularly with infrequent or subtle sense distinctions; achieve only 45.6 F1 on hardEN dataset versus 83.2 on standard benchmarks.
- Why unresolved: Paper identifies this weakness but does not propose or test interventions for rare sense handling.
- What evidence would resolve it: Targeted experiments with sense-frequency-aware training, data augmentation for rare senses, or retrieval-augmented approaches evaluated on hardEN and long-tail sense subsets.

## Limitations
- Data contamination concerns: Limited contamination check performed on only one withheld dataset, broader contamination landscape across multiple benchmarks unexplored
- Human evaluation reliability: Generative task evaluation relies on human raters without inter-rater reliability statistics or detailed annotation guidelines
- Cross-linguistic generalizability: All experiments focus on English language WSD, leaving uncertain whether observed advantages generalize to other languages

## Confidence
- **High confidence:** LLMs achieve competitive WSD performance (~83% accuracy) compared to specialized systems, well-supported by systematic evaluation
- **Medium confidence:** Unconstrained generative tasks (98% accuracy) reveal superior semantic understanding, though alternative explanations cannot be ruled out
- **Low confidence:** LLMs demonstrate superior cross-domain robustness compared to specialized systems, based on limited challenging dataset comparisons

## Next Checks
1. **Contamination assessment protocol:** Develop and apply systematic contamination detection methodology across all benchmark datasets, including statistical analysis of potential overlap with public LLM training corpora
2. **Prompt robustness replication:** Conduct multi-institution replication study testing 20 prompt variations across different LLM families to quantify stability and generalizability of observed prompt sensitivity effects
3. **Fine-grained semantic analysis:** For instances where LLMs fail on classification but succeed on generative tasks, perform detailed semantic analysis comparing free-form explanation against gold sense to determine whether failures reflect genuine semantic gaps or discrete inventory limitations