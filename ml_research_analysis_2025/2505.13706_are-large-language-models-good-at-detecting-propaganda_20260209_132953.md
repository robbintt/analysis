---
ver: rpa2
title: Are Large Language Models Good at Detecting Propaganda?
arxiv_id: '2505.13706'
source_url: https://arxiv.org/abs/2505.13706
tags:
- technique
- propaganda
- llms
- techniques
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Propaganda detection in news articles was evaluated using Large
  Language Models (GPT-3.5, GPT-4, Claude 3 Opus) under zero-shot, one-shot, chain-of-thought,
  generated knowledge, and self-consistency prompting. Models were tested on six propaganda
  techniques from the PTC dataset.
---

# Are Large Language Models Good at Detecting Propaganda?

## Quick Facts
- arXiv ID: 2505.13706
- Source URL: https://arxiv.org/abs/2505.13706
- Authors: Julia Jose; Rachel Greenstadt
- Reference count: 16
- Key outcome: GPT-4 achieves 0.16 macro-F1 on propaganda detection, outperformed by transformer baselines (RoBERTa-CRF 0.67 macro-F1)

## Executive Summary
This paper evaluates Large Language Models (GPT-3.5, GPT-4, Claude 3 Opus) on phrase-level propaganda technique detection using the PTC dataset. The models are tested across six propaganda techniques using various prompting strategies including zero-shot, one-shot, chain-of-thought, generated knowledge, and self-consistency approaches. Despite testing multiple prompting strategies, the LLMs significantly underperform compared to established transformer-based baselines, with GPT-4 achieving only 0.16 macro-F1 compared to RoBERTa-CRF's 0.67 macro-F1. The study finds that while LLMs can outperform some transformer baselines on specific techniques like name-calling and flag-waving, they struggle particularly with nuanced techniques such as loaded language and doubt detection, often showing high false positive rates.

## Method Summary
The study evaluates three LLMs (GPT-3.5, GPT-4, Claude 3 Opus) on phrase-level propaganda detection using the PTC dataset with six target techniques. Models are tested under five prompting strategies: zero-shot, one-shot, chain-of-thought, generated knowledge, and self-consistency. The evaluation uses span-level F1, precision, recall, and macro-F1 metrics. Results are compared against two transformer baselines: MGN (Multi-Granularity Network) and RoBERTa-CRF. The study also includes fine-tuning experiments with GPT-3.5 using specific hyperparameters (epochs=3, learning rate multiplier=2, batch size=1).

## Key Results
- GPT-4 achieves highest macro-F1 of 0.16 among LLMs, significantly below RoBERTa-CRF baseline (0.67 macro-F1)
- GPT-3.5 and GPT-4 outperform MGN baseline on name-calling, appeal to fear, and flag-waving techniques
- All LLMs show high false positive rates, particularly on loaded language and doubt detection techniques

## Why This Works (Mechanism)
The study demonstrates that while LLMs can perform basic propaganda detection, they struggle with the nuanced linguistic patterns required for accurate identification of specific propaganda techniques. The high false positive rates, particularly on loaded language and doubt detection, suggest that LLMs may be overfitting to surface-level linguistic cues rather than understanding deeper contextual and semantic aspects of propaganda. The consistent underperformance across multiple prompting strategies indicates fundamental limitations in how LLMs process and categorize propaganda techniques compared to specialized transformer architectures trained specifically for this task.

## Foundational Learning
- **Propaganda Technique Detection**: Identifying specific rhetorical strategies in text that manipulate audience perception; needed for understanding what models are being evaluated on; quick check: can identify examples of name-calling vs. loaded language
- **Phrase-level Annotation**: Marking exact text spans corresponding to propaganda techniques; needed for precise evaluation metrics; quick check: understand difference between phrase-level vs. sentence-level annotation
- **Macro-F1 Calculation**: Averaging F1 scores across multiple classes/classes; needed for fair comparison across techniques with imbalanced distributions; quick check: can compute macro-F1 from per-class precision/recall
- **Chain-of-Thought Prompting**: Guiding models through intermediate reasoning steps; needed to potentially improve complex detection tasks; quick check: can construct CoT prompts for classification tasks
- **Self-Consistency**: Sampling multiple outputs and using majority voting; needed to reduce variance in LLM predictions; quick check: understand voting mechanisms for ensemble predictions
- **Transformer-based Sequence Labeling**: Using architectures like RoBERTa-CRF for token-level classification; needed as strong baseline comparison; quick check: understand CRF layer in sequence labeling

## Architecture Onboarding

**Component Map**: PTC Dataset -> Preprocessing/Filtering -> LLM Inference (5 prompting strategies) -> Span-level Evaluation -> Baseline Comparison

**Critical Path**: Data preparation and filtering → Prompt template application → LLM inference with temperature control → Span matching and F1 computation → Statistical comparison with baselines

**Design Tradeoffs**: The study uses zero/one-shot learning to test LLM generalization without extensive fine-tuning, but this limits performance compared to fully fine-tuned models. The choice of 6 specific propaganda techniques provides focused analysis but may not capture the full complexity of propaganda detection.

**Failure Signatures**: High false positive rates, especially on loaded language and doubt detection, indicate models may be overfitting to superficial linguistic patterns. Consistent underperformance across all prompting strategies suggests fundamental architectural limitations rather than prompt engineering issues.

**First Experiments**: 1) Run zero-shot inference on 100 samples to establish baseline performance, 2) Implement chain-of-thought prompting on same samples to test reasoning improvements, 3) Apply self-consistency with 5 samples per instance to evaluate variance reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- Substantial performance gap between LLMs and transformer baselines (0.16 vs 0.67 macro-F1) indicates fundamental limitations
- High false positive rates, especially on loaded language and doubt detection, suggest systematic weaknesses
- Lack of detailed experimental configuration (dataset splits, self-consistency parameters, fine-tuning details) creates reproducibility barriers

## Confidence

**High Confidence**: The relative ranking of models and the observation that LLMs struggle particularly with loaded language and doubt detection are well-supported by the presented results.

**Medium Confidence**: The specific macro-F1 scores and per-technique performance metrics, while presented with apparent precision, are difficult to fully verify without the exact dataset splits and evaluation parameters used.

**Low Confidence**: Claims about LLMs detecting propaganda instances missed by human annotators require external validation and could reflect annotation inconsistencies rather than genuine model capability.

## Next Checks

1. **Dataset Split Verification**: Obtain and verify the exact train/test split used from the PTC dataset, including the number of samples per technique in the test set, to enable exact reproduction of the reported macro-F1 scores.

2. **Self-Consistency Parameter Replication**: Implement and test the self-consistency approach with documented sampling parameters (number of samples, voting mechanism) to verify whether this technique genuinely provides any performance benefit over simpler prompting strategies.

3. **Manual Error Analysis**: Conduct a small-scale manual review (e.g., 50-100 samples) of LLM predictions, particularly focusing on high false positive cases in loaded language and doubt detection, to determine whether these represent true model errors or potential annotation inconsistencies in the PTC dataset.