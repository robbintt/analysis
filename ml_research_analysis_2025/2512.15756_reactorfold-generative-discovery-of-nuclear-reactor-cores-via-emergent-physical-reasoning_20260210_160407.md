---
ver: rpa2
title: 'ReactorFold: Generative discovery of nuclear reactor cores via emergent physical
  reasoning'
arxiv_id: '2512.15756'
source_url: https://arxiv.org/abs/2512.15756
tags:
- design
- reactor
- nuclear
- optimization
- inventory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ReactorFold, a generative framework that\
  \ reformulates fuel-assembly design as a sequence modeling task for language models.\
  \ Using Monte Carlo simulations, parameter-efficient fine-tuning, and Direct Preference\
  \ Optimization (DPO), the model learns the latent structure of a 17\xD717 PWR assembly\
  \ and generates candidate layouts in a single forward pass."
---

# ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning

## Quick Facts
- arXiv ID: 2512.15756
- Source URL: https://arxiv.org/abs/2512.15756
- Authors: Yoonpyo Lee
- Reference count: 40
- Key outcome: Generative framework that discovers novel reactor core designs by reformulating assembly layout as a sequence modeling task for language models

## Executive Summary
ReactorFold introduces a novel approach to nuclear reactor core design by treating fuel-assembly layout as a sequence modeling problem for language models. The framework uses Monte Carlo simulations to generate training data, applies parameter-efficient fine-tuning with LoRA, and employs Direct Preference Optimization (DPO) to learn the latent structure of a 17×17 PWR assembly. Notably, the model autonomously adjusts gadolinium inventory beyond its training constraints and discovers high-performing asymmetric configurations that deviate from conventional symmetric loading heuristics. Within 1,000 high-fidelity simulations, ReactorFold achieves a six-fold improvement in fitness over a genetic algorithm baseline.

## Method Summary
The method reformulates 17×17 PWR assembly design as a sequence modeling task, rasterizing the 2D lattice into 289 tokens representing fuel, Gd rods, and guide tubes. The framework employs a curriculum learning approach: large-scale low-fidelity Monte Carlo simulations (100K samples) train a Gemma 3 270M model to learn coarse geometric syntax, followed by LoRA fine-tuning (rank 32) on 10K high-fidelity samples to refine parameter correlations. Online DPO then navigates the latent space toward safe configurations by sampling pairs of layouts, evaluating them with OpenMC, and updating the policy to favor higher-fitness designs. The fitness function combines power peaking constraints (Fq, FΔH) with criticality (keff) and a penalty for deviations from the target 1.05.

## Key Results
- Six-fold improvement in fitness over genetic algorithm baseline within 1,000 high-fidelity simulations
- Autonomous adjustment of Gd inventory beyond the 16-rod constraint used during training
- Discovery of high-performing asymmetric configurations that deviate from conventional symmetric loading heuristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention captures long-range neutronic coupling across the 2D lattice.
- Mechanism: By flattening the 17×17 grid into a 289-token sequence, the model's self-attention layers can compute pairwise dependencies between spatially distant pins, which mirrors how neutron diffusion links a Gd rod's position to global $k_{\text{eff}}$ and power peaking.
- Core assumption: Flattened sequence order preserves sufficient spatial context for the model to infer long-range interactions.
- Evidence anchors: [abstract] "reformulates fuel-assembly design as a sequence modeling problem for language models"; [section: Discussion] "self-attention computes pairwise relationships across the entire token sequence"; [corpus] Weak/absent direct corpus support.
- Break condition: If attention maps show no correlation between Gd positions and distant fuel-pin tokens during inference, the long-range coupling hypothesis is likely false.

### Mechanism 2
- Claim: DPO induces an implicit reward model that directly favors lower power peaking within the target $k_{\text{eff}}$ band.
- Mechanism: Online DPO generates candidate pairs, evaluates them with OpenMC, and updates the policy to increase the likelihood of the higher-fidelity-preferred layout without a separate RL value function.
- Core assumption: The fitness function sufficiently approximates true safety constraints and generalizes to out-of-distribution layouts.
- Evidence anchors: [abstract] "using ... Direct Preference Optimization (DPO), the model learns the latent structure"; [section: Methods 4.6] "optimizes the policy directly on preference data"; [corpus] Weak/no direct corpus evidence.
- Break condition: If DPO samples collapse to low-diversity layouts or overfit to the penalty weight $\lambda=100$, the policy may exploit the reward model artifact rather than real physics.

### Mechanism 3
- Claim: Curriculum learning transfers coarse physical grammar into precise alignment.
- Mechanism: Large-scale low-fidelity simulations teach global geometric syntax; LoRA on high-fidelity data refines parameter correlations; DPO then navigates the resulting latent space toward safe configurations.
- Core assumption: Relative rankings of configurations are consistent between low- and high-fidelity simulations.
- Evidence anchors: [section: Results 2.1] "low-fidelity corpus... preserve the relative ranking and coarse structure"; [section: Methods 4.5] describes full fine-tuning, then LoRA, then DPO; [corpus] Weak/absent direct support.
- Break condition: If validation loss on high-fidelity data does not decrease after LoRA, or if DPO fails to improve fitness over the LoRA checkpoint, the curriculum may not be transferring useful representations.

## Foundational Learning

- Concept: Pressurized Water Reactor (PWR) assembly geometry and neutronic parameters
  - Why needed here: ReactorFold operates on a 17×17 lattice with specific pin types and targets ($k_{\text{eff}}$, $F_q$, $F_{\Delta H}$).
  - Quick check question: Given a layout with more Gd rods, would you expect $k_{\text{eff}}$ to increase or decrease, and why?

- Concept: Tokenization and sequence modeling for discrete structures
  - Why needed here: The framework's core step is rasterizing the 2D lattice into a 1D token sequence for the LM.
  - Quick check question: How does the newline token after every 17 positions help the model infer row structure?

- Concept: Direct Preference Optimization (DPO) vs. RL fine-tuning
  - Why needed here: ReactorFold uses DPO instead of PPO-based RLHF, directly optimizing on pairwise preferences from the OpenMC oracle.
  - Quick check question: What advantage does DPO offer by not requiring a separate reward model during training?

## Architecture Onboarding

- Component map: OpenMC simulator -> Gemma 3 270M LM -> Full fine-tuning (100K samples) -> LoRA adapter (rank 32) -> Online DPO (500 steps) -> Active token correction

- Critical path: 1) Generate and serialize low-fidelity Monte Carlo dataset; 2) FFT to acquire lattice grammar; 3) Generate high-fidelity dataset; run LoRA fine-tuning; 4) Initialize DPO with target prompt; 5) For each DPO step, sample two layouts, simulate with OpenMC, compute fitness, update policy; 6) Enforce guide-tube token overwriting

- Design tradeoffs: Smaller 270M model vs. larger backbone (lower VRAM but limited representation); LoRA rank 32 (balances adaptation and efficiency); Temperature T=1.0 (encourages exploration but may produce infeasible layouts)

- Failure signatures: DPO loss diverging (check learning rate and beta); generated layouts violate guide-tube constraints (verify correction hook); fitness stagnates early (inspect if Gd inventory remains fixed)

- First 3 experiments: 1) Ablate low-fidelity pretraining: train LoRA directly on high-fidelity data; 2) Vary LoRA rank (16, 32, 64) and measure impact on $F_q$ and $F_{\Delta H}$; 3) Replace DPO with PPO-based RL loop using same fitness function and simulation budget

## Open Questions the Paper Calls Out

- Can the sequence-modeling approach be effectively scaled to three-dimensional full-core design with coupled multi-physics feedback?
- Does optimization based on Beginning-of-Life parameters ensure safety and performance throughout the full depletion cycle?
- Does the model's ability to autonomously adjust Gd inventory reflect genuine causal physical reasoning or statistical interpolation?

## Limitations
- The mechanism by which the model learns to adjust Gd inventory autonomously remains underspecified
- Self-attention long-range coupling mechanism lacks direct validation through attention pattern analysis
- DPO implementation ambiguity due to "simplified" loss formulation that differs from standard approaches

## Confidence

- High Confidence: The sequence modeling approach works for generating feasible layouts; the curriculum learning framework is technically sound; the six-fold fitness improvement over GA is measurable and reproducible
- Medium Confidence: The mechanism of long-range physical reasoning through self-attention; the autonomy of Gd inventory adjustment; the relative importance of each curriculum stage
- Low Confidence: The exact DPO loss implementation; whether the model truly discovers novel physics or exploits reward function artifacts; the generality of findings beyond the 17×17 PWR geometry

## Next Checks

1. **Ablation of Gd Inventory Learning:** Train a variant where the model is explicitly constrained to generate exactly 16 Gd rods throughout DPO training. Compare the final fitness and layout diversity to the original model.

2. **Attention Pattern Analysis:** Extract and visualize the self-attention weights for Gd rod tokens versus fuel rod tokens across multiple successful layouts. Compute correlation coefficients between attention strength and physical distance to test whether the model learns the expected neutronic coupling patterns.

3. **Cross-Geometry Transfer Test:** Apply the trained ReactorFold model to a different reactor geometry (e.g., 16×16 or 18×18 lattice with modified guide tube patterns). Measure performance degradation and compare to training a model from scratch on the new geometry.