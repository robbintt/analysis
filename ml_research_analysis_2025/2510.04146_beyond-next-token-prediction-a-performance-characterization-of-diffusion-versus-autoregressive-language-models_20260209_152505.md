---
ver: rpa2
title: 'Beyond Next-Token Prediction: A Performance Characterization of Diffusion
  versus Autoregressive Language Models'
arxiv_id: '2510.04146'
source_url: https://arxiv.org/abs/2510.04146
tags:
- arxiv
- dlms
- diffusion
- block-wise
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive performance characterization
  of Diffusion Language Models (DLMs) compared to Autoregressive Language Models (ARMs).
  The authors analyze the computational characteristics of both architectures using
  theoretical analysis and empirical profiling, focusing on arithmetic intensity,
  roofline modeling, and inference throughput under various conditions.
---

# Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models

## Quick Facts
- **arXiv ID**: 2510.04146
- **Source URL**: https://arxiv.org/abs/2510.04146
- **Reference count**: 40
- **Primary result**: Comprehensive performance characterization showing DLMs can achieve higher arithmetic intensity than ARMs through parallelism but fail to scale with long contexts due to full-sequence recomputation.

## Executive Summary
This paper presents a comprehensive performance characterization comparing Diffusion Language Models (DLMs) and Autoregressive Language Models (ARMs) during inference. The authors analyze computational characteristics through theoretical analysis and empirical profiling, focusing on arithmetic intensity, roofline modeling, and inference throughput under various conditions. They find that while DLMs can achieve higher arithmetic intensity by leveraging parallelism across token positions, they fail to scale effectively with longer contexts due to their need to re-process the full sequence at each sampling step. The study explores block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts. The authors also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch.

## Method Summary
The authors conduct empirical profiling of LLaMA-3-8B-Instruct (ARM baseline) and LLaDA-8B-Instruct (DLM baseline) using FP16 inference on NVIDIA RTX A6000 (48GB) for single-batch and A100 (80GB) for batched experiments. They measure runtime latency, throughput, and arithmetic intensity across various conditions: prompt lengths Lp ∈ {128, 512, 1024, 2048}, generation lengths Lg varying from ~32 to 2048 tokens, batch sizes B from 1 to 16, and block sizes G ∈ {8, 16, 32, 64, 128} for block-wise DLM decoding. The block-wise approach implements approximate KV caching following Fast-dLLM methodology, allowing positions within each active block to update in parallel while maintaining autoregressive structure across blocks.

## Key Results
- DLMs achieve higher arithmetic intensity than ARMs by exploiting parallelism across token positions, but this advantage diminishes as batch size increases
- Block-wise decoding with KV caching reduces DLM latency by roughly 2× to 3× compared to naive DLM decoding while maintaining higher arithmetic intensity
- ARMs provide superior throughput scalability in batched serving scenarios, scaling to larger batch sizes before hitting compute saturation
- Reducing sampling steps K is critical for DLMs to achieve lower latency, though naive reduction causes severe accuracy degradation (GSM8K accuracy drops from 73.6% to 6.1% when K reduced from 128 to 4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DLMs achieve higher arithmetic intensity than ARMs during decoding by exploiting parallelism across token positions.
- Mechanism: DLMs update all token positions in parallel at each refinement step, enabling larger matrix multiplications that better utilize compute units. This contrasts with ARMs, which generate tokens sequentially, resulting in small per-step operations that become memory-bandwidth-bound once context length exceeds hidden dimension.
- Core assumption: Hardware has sufficient parallel compute capacity to exploit the increased operational parallelism; memory bandwidth is the limiting factor for ARM decode.
- Evidence anchors:
  - [abstract] "DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions"
  - [Section 3.2, Table 1] Shows ARM decode AI drops to O(1) when Lp ≫ d, while naive DLM maintains AI ≈ O(L)
  - [corpus] dKV-Cache paper confirms DLMs constrained by "slow inference" and bidirectional attention precluding KV cache

### Mechanism 2
- Claim: Block-wise decoding with approximate KV caching reduces DLM latency by 2-3× while maintaining higher arithmetic intensity than ARMs.
- Mechanism: Divides the sequence into blocks of size G. Decoding proceeds autoregressively across blocks (ARM-like) while positions within each active block update in parallel (DLM-like). This enables approximate KV caching of the prompt and non-active blocks, avoiding full-sequence recomputation at each step.
- Core assumption: Approximate KV caching introduces acceptable accuracy degradation; block-level autoregressive structure aligns with task requirements.
- Evidence anchors:
  - [abstract] "block-wise decoding with KV caching can reduce latency by roughly 2× to 3× compared to naive DLM decoding"
  - [Section 3.4, Figure 3] Empirical profiling shows latency reduction and AI becoming invariant to generation length, depending only on block size G
  - [corpus] Fast-dLLM and dKV-Cache papers corroborate block-wise approaches as key DLM optimization strategy

### Mechanism 3
- Claim: Reducing the number of sampling steps K is the critical path for DLMs to achieve lower latency than ARMs.
- Mechanism: Current open-source DLMs use K = Lg (steps equal generation length), meaning only ~1 token finalized per step on average. Reducing K through multi-token finalization, confidence-based early stopping, or distillation directly decreases total FLOPs and iterations.
- Core assumption: Advanced techniques (autoregressive guidance, distillation) can maintain quality with fewer steps; naive step reduction causes severe accuracy drops.
- Evidence anchors:
  - [Section 4] "reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs"
  - [Appendix C, Table 3] Reducing K from 128 to 4 causes GSM8K accuracy to drop from 73.6% to 6.1%, demonstrating quality-quantity tradeoff
  - [corpus] Efficient-DLM paper explores AR-to-DLM conversion and speed optimization; closed-source systems (Mercury, Gemini Diffusion) claim 10× speedups

## Foundational Learning

- Concept: **Arithmetic Intensity (AI) and Roofline Model**
  - Why needed here: The paper's core analysis framework compares ARM and DLM efficiency through AI (FLOPs per byte moved) and roofline placement. Understanding compute-bound vs. memory-bound behavior is essential for interpreting why DLMs show advantages in some scenarios but not others.
  - Quick check question: Given a kernel with 1000 FLOPs and 100 bytes transferred, what is the arithmetic intensity? If the hardware ridge point is 50 FLOPs/byte, is this kernel compute-bound or memory-bound?

- Concept: **KV Caching in Autoregressive Models**
  - Why needed here: The paper positions block-wise DLM decoding as enabling "approximate KV caching" analogous to ARMs. Understanding standard KV caching—storing Key and Value vectors to avoid recomputing past token activations—is prerequisite to grasping why naive DLMs cannot use it (bidirectional attention) and how block-wise decoding approximates it.
  - Quick check question: Why does KV caching increase memory footprint while reducing computation in autoregressive decoding?

- Concept: **Diffusion Process Basics (Iterative Denoising)**
  - Why needed here: DLMs generate text by starting from noise and applying K refinement steps. The number of steps K directly controls the latency-quality tradeoff. Understanding that each step updates all positions in parallel (unlike sequential ARM generation) is fundamental to the paper's performance characterization.
  - Quick check question: In a diffusion model with 10 sampling steps, how does the output change if you reduce to 2 steps without modifying the model? What quality degradation might occur?

## Architecture Onboarding

- Component map:
  - ARM Inference: Prefill phase (compute-bound, processes full prompt) → Decode phase (memory-bound, sequential token generation with KV cache retrieval)
  - Naive DLM: K refinement steps, each recomputing full sequence with bidirectional attention, no KV caching
  - Block-wise DLM: Blocks of size G processed sequentially; within each block, K/G diffusion steps with parallel token updates; approximate KV cache for prompt and completed blocks

- Critical path:
  1. Profile workload on roofline model (measure AI, identify compute-bound vs. memory-bound regime)
  2. If DLM: implement block-wise decoding with KV caching (reduces AI, enables long-context scaling)
  3. Reduce sampling steps K via multi-token finalization or distillation (critical for latency competitiveness)
  4. For batched serving: prefer ARMs; DLMs plateau at smaller batch sizes due to compute saturation

- Design tradeoffs:
  - Block size G: Smaller blocks → lower AI, better memory efficiency, more ARM-like behavior; larger blocks → higher parallelism, higher AI. Paper tests G ∈ {32, 64, 128}.
  - Sampling steps K: Fewer steps → lower latency but potential accuracy loss. Naive reduction (K=4) causes catastrophic quality drop; requires advanced techniques.
  - Batch size vs. prompt length: Longer prompts reduce throughput gains from batching for both architectures; DLMs hit OOM earlier.

- Failure signatures:
  - Naive DLM with long context: Latency scales linearly with total sequence length; each step recomputes full bidirectional attention
  - Block-wise DLM at large batch + long prompt: OOM errors at smaller batch sizes than ARM (peak memory scales with B × L)
  - Aggressive step reduction without quality techniques: GSM8K accuracy drops from ~73% to <10% when K reduced from 128 to 4

- First 3 experiments:
  1. Roofline profiling: Measure AI for ARM prefill, ARM decode, and DLM variants across sequence lengths. Plot on roofline model to identify compute-bound vs. memory-bound regimes. Expected: ARM decode on bandwidth roof, DLM shifts right with longer sequences.
  2. Block-wise decoding ablation: Compare naive DLM vs. block-wise DLM (G=32, 64, 128) on latency and AI. Measure end-to-end runtime for fixed prompt (Lp=1k) across generation lengths. Expected: 2-3× latency reduction, AI invariant to Lg.
  3. Batched throughput scaling: Measure tokens/s vs. batch size for ARM and block-wise DLM at short (Lp=128) and long (Lp=2k) prompts. Identify OOM boundaries and throughput plateaus. Expected: ARMs scale to larger B; DLMs plateau earlier and OOM sooner with long prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can open-source Diffusion Language Models (DLMs) reduce the number of sampling steps (e.g., via distillation or guided decoding) to match Autoregressive Model (ARM) latency without suffering severe quality degradation?
- Basis in paper: [explicit] The authors state that "reducing the number of sampling steps is key" to closing the latency gap, noting that naive step reduction results in "severe accuracy drops" (Table 3).
- Why unresolved: Current open-source DLMs typically require steps scaling with generation length (K=L_g), whereas closed-source models claim efficiency gains that open-source models have not yet replicated.
- What evidence would resolve it: A method that achieves few-step (K ≪ L_g) or one-step generation while maintaining benchmark parity with standard ARM baselines.

### Open Question 2
- Question: Can DLMs effectively surpass ARMs in throughput specifically in small-batch serving scenarios by leveraging step-reduction techniques?
- Basis in paper: [explicit] The paper hypothesizes that "reducing the number of sampling steps is particularly promising in small-batch scenarios, where a DLM's throughput could surpass an ARM's."
- Why unresolved: While the paper demonstrates throughput gains from mechanical step reduction, it notes that achieving this potential requires advanced techniques to prevent the associated accuracy collapse.
- What evidence would resolve it: Profiling results showing a step-optimized DLM providing higher tokens/second than an ARM at batch sizes ≤ 8 without quality loss.

### Open Question 3
- Question: What specific performance gains can be achieved by applying orthogonal system-level optimizations, such as low-precision execution and sparse attention, to block-wise DLMs?
- Basis in paper: [explicit] The authors identify "compute-focused methods like low-precision execution as well as sparse attention methods" as opportunities for "additional speedups."
- Why unresolved: The paper characterizes block-wise decoding but leaves the integration of these specific system-level optimizations for future work.
- What evidence would resolve it: Roofline and latency benchmarks of block-wise DLMs implemented with FP8/int8 quantization or sparse attention kernels.

## Limitations

- The performance characterization focuses primarily on 8B-parameter models, which may not scale predictably to larger model sizes where different computational bottlenecks emerge.
- The study assumes FP16 precision inference, but emerging hardware support for FP8 or higher precisions could alter the arithmetic intensity and roofline positioning of both architectures.
- The block-wise decoding approach with approximate KV caching is validated empirically but lacks rigorous theoretical analysis of the accuracy degradation introduced by the approximation.

## Confidence

**High Confidence**: The roofline analysis showing that ARM decode becomes memory-bandwidth-bound when context length exceeds hidden dimension (Lp ≫ d), while DLMs maintain higher arithmetic intensity through parallel token position updates.

**Medium Confidence**: The 2-3× latency reduction claim for block-wise DLM decoding with KV caching. While the empirical measurements are provided, the exact speedup depends heavily on implementation details of the approximate caching mechanism.

**Medium Confidence**: The assertion that ARMs provide superior throughput scalability in batched serving scenarios. The profiling shows ARMs scale to larger batch sizes before hitting compute saturation, but this depends on specific hardware characteristics.

**Low Confidence**: The critical importance of reducing sampling steps K for DLM latency competitiveness. While the paper demonstrates that naive step reduction causes catastrophic accuracy loss, it does not conclusively prove that quality-preserving step reduction techniques can bridge the latency gap with ARMs.

## Next Checks

1. **Scale-Up Validation**: Profile ARM and DLM architectures at multiple model scales (7B, 34B, 70B parameters) to verify that the computational characteristics identified at 8B parameters hold across the model size spectrum.

2. **Step Reduction Quality-Quantity Tradeoff**: Implement and evaluate advanced sampling step reduction techniques including autoregressive guidance, confidence-based early stopping, and distillation approaches. Measure the quality degradation curve as K decreases from full generation length to minimal viable steps.

3. **Long-Context Generalization**: Extend the performance characterization to extremely long contexts (Lp > 8k tokens) to test whether the block-wise DLM approach with approximate KV caching maintains its advantages for tasks requiring long-range reasoning.