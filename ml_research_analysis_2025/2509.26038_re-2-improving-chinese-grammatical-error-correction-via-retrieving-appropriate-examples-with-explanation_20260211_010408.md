---
ver: rpa2
title: 'RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate
  Examples with Explanation'
arxiv_id: '2509.26038'
source_url: https://arxiv.org/abs/2509.26038
tags:
- error
- grammatical
- examples
- explanation
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Chinese grammatical error
  correction (CGEC) by proposing a method that retrieves reference examples based
  on grammatical error explanations rather than text similarity. The core idea is
  to use an explainer model to generate error explanations for input sentences, then
  retrieve similar examples from a constructed grammatical error explanation dataset
  based on these explanations.
---

# RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation

## Quick Facts
- arXiv ID: 2509.26038
- Source URL: https://arxiv.org/abs/2509.26038
- Reference count: 40
- Primary result: Improves Chinese grammatical error correction by retrieving examples based on grammatical error explanations rather than text similarity, achieving F0.5 scores of 59.76 on FCGEC and 51.66 on NaCGEC datasets

## Executive Summary
This paper addresses the challenge of Chinese grammatical error correction (CGEC) by proposing a method that retrieves reference examples based on grammatical error explanations rather than text similarity. The core idea is to use an explainer model to generate error explanations for input sentences, then retrieve similar examples from a constructed grammatical error explanation dataset based on these explanations. The retrieved examples are provided to large language models to improve their CGEC performance. Experiments on two CGEC datasets show that this approach significantly outperforms baselines, achieving the best results.

## Method Summary
The RE$^2$ method consists of three main components: an explainer model that generates grammatical error explanations, a retrieval system that finds relevant examples based on these explanations, and a corrector model that performs the actual error correction. The explainer is fine-tuned on a constructed dataset (FCGEE) containing sentences with grammatical error explanations. During inference, the explainer generates an explanation for the input sentence, which is then used to retrieve similar examples from the training set using TF-IDF cosine similarity on n-grams (2-3). The top-k retrieved examples (k=3) are provided to the corrector model, which is fine-tuned on a mixed dataset containing both examples-with-retrieval and zero-shot formats. This approach leverages grammatical error explanations rather than text similarity to find more relevant correction examples.

## Key Results
- F0.5 score improves from 57.92 to 59.76 on the FCGEC dataset
- F0.5 score improves from 49.65 to 51.66 on the NaCGEC dataset
- Outperforms all baseline methods on both evaluation datasets
- Ablation study confirms the importance of the retrieval component

## Why This Works (Mechanism)
The approach works because grammatical error explanations capture the semantic intent of corrections more effectively than surface-level text similarity. When the explainer identifies specific error types (such as word usage, word order, or redundant words), the retrieval system can find examples that address the same grammatical issues, even if the surface text differs significantly. This semantic alignment between the input's error type and the retrieved examples' corrections leads to more relevant and accurate corrections.

## Foundational Learning
- **Chinese Grammatical Error Correction (CGEC)**: Task of identifying and correcting grammatical errors in Chinese text
  - Why needed: Core problem being addressed
  - Quick check: Can you distinguish CGEC from general text correction tasks?

- **Error Explanation Generation**: Using models to describe grammatical errors in natural language
  - Why needed: Enables semantic-based retrieval rather than text-based retrieval
  - Quick check: Can you explain how error explanations improve retrieval quality?

- **TF-IDF Cosine Similarity**: Text similarity metric using weighted term frequency
  - Why needed: Method for measuring similarity between error explanations
  - Quick check: Can you calculate TF-IDF similarity between two short texts?

## Architecture Onboarding

**Component Map**: Input Sentence -> Explainer -> Error Explanation -> TF-IDF Retrieval -> Retrieved Examples -> Corrector -> Corrected Output

**Critical Path**: The pipeline's critical path is Input Sentence → Explainer → Retrieval → Corrector. Any degradation in explainer quality or retrieval relevance directly impacts final correction accuracy.

**Design Tradeoffs**: The method trades computational overhead (running explainer + retrieval for each input) for improved correction accuracy. The 50/50 training split balances exposure to retrieved examples with generalization capability.

**Failure Signatures**: Poor explainer quality manifests as irrelevant retrieved examples and degraded performance. Over-reliance on retrieved examples without zero-shot training data leads to overfitting and poor generalization.

**First Experiments**: 1) Evaluate explainer ROUGE-L against held-out set to establish quality baseline. 2) Manually verify if top-3 retrieved examples share error type with input sentences. 3) Test inference pipeline with θ=0.6 threshold and measure retrieval relevance.

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies heavily on the quality of generated error explanations - if the explainer hallucinates errors or produces weak explanations (ROUGE-L below 0.6), the retrieval mechanism will fail
- The approach is dataset-dependent and may not generalize well to datasets with different error distributions
- The computational overhead of running the explainer for each input and retrieving examples may impact practical deployment

## Confidence

**High**: The core methodology is sound and logically coherent; key components (explainer, retrieval, SFT) are clearly specified; the approach is novel in using grammatical error explanations rather than text similarity for retrieval.

**Medium**: Experimental results on two datasets show clear improvements; F0.5 score improvements are statistically significant; the ablation study demonstrates the importance of the retrieval component.

**Low**: Exact reproduction requires unavailable seed data ("exam dataset with rough explanation"); performance may degrade significantly without access to this bootstrap dataset; hyperparameter sensitivity (θ threshold, split ratios) not thoroughly explored.

## Next Checks
1. **Explainer Quality Validation**: Evaluate the generated explainer model on a held-out set using ROUGE-L scores and manually verify if retrieved examples share the same error type as input sentences, establishing a baseline quality threshold.

2. **Threshold Sensitivity Analysis**: Systematically vary the TF-IDF cosine similarity threshold (θ) from 0.4 to 0.8 in increments of 0.1 to determine its impact on retrieval quality and final CGEC performance.

3. **Bootstrap Dataset Impact Study**: Compare CGEC performance using three different explainer training strategies: (a) with synthetic GPT-4o generated data only, (b) with a small subset of manually verified explanations, and (c) the ideal case with the full exam dataset, to quantify the bootstrap data's contribution.