---
ver: rpa2
title: 'AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness'
arxiv_id: '2511.09316'
source_url: https://arxiv.org/abs/2511.09316
tags:
- adaptdel
- certified
- deletion
- input
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaptDel improves certified robustness for sequence classification
  by adapting deletion rates based on input properties. Unlike fixed-rate methods,
  it dynamically adjusts smoothing strength, achieving up to 30 orders of magnitude
  larger certified regions.
---

# AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness

## Quick Facts
- arXiv ID: 2511.09316
- Source URL: https://arxiv.org/abs/2511.09316
- Reference count: 40
- AdaptDel achieves up to 30 orders of magnitude larger certified regions for sequence classification

## Executive Summary
AdaptDel introduces an adaptive deletion-rate randomized smoothing method that dynamically adjusts smoothing strength based on input properties to improve certified robustness for sequence classification. Unlike fixed-rate approaches, AdaptDel optimizes deletion rates to achieve substantially larger certified regions while maintaining clean accuracy. The method extends randomized smoothing theory to variable-rate deletion, enabling efficient edit distance certification. Empirical results demonstrate significant improvements over state-of-the-art methods, particularly for longer sequences where robustness is most critical.

## Method Summary
AdaptDel extends randomized smoothing to variable deletion rates, allowing the smoothing parameter to adapt based on input properties rather than using a fixed deletion rate. The theoretical framework derives certified bounds for edit distance perturbations under variable deletion rates, showing that adaptive rates can achieve substantially larger certified regions than fixed-rate methods. AdaptDel+ further optimizes deletion rates through empirical calibration, using clean data to select rates that maximize certified radii. The method is evaluated on four NLP tasks including sentiment analysis and natural language inference, demonstrating improved certified accuracy while maintaining minimal clean accuracy loss.

## Key Results
- Achieves up to 30 orders of magnitude larger certified regions compared to fixed-rate methods
- Outperforms state-of-the-art methods in certified accuracy across four NLP tasks
- Shows particular effectiveness for longer sequences where robustness gains are most pronounced
- Maintains minimal clean accuracy loss while improving certified robustness

## Why This Works (Mechanism)
AdaptDel works by dynamically adjusting the deletion rate during the smoothing process based on input characteristics, rather than using a uniform deletion rate for all inputs. This adaptive approach allows the method to apply stronger smoothing to inputs where robustness is most needed while maintaining lower smoothing for inputs where high clean accuracy is critical. The theoretical extension enables certification under variable deletion rates, which creates larger certified regions by optimizing the trade-off between smoothing strength and accuracy. The empirical calibration in AdaptDel+ further refines this process by selecting deletion rates that empirically maximize certified radii on clean data.

## Foundational Learning
- Randomized smoothing fundamentals: Why needed - provides the theoretical foundation for provable robustness guarantees; Quick check - verify understanding of how random noise addition creates certified bounds
- Edit distance certification: Why needed - enables robustness guarantees for text under character-level perturbations; Quick check - confirm ability to calculate certified radii under edit distance metrics
- Variable-rate smoothing theory: Why needed - extends traditional fixed-rate approaches to adaptive methods; Quick check - understand the mathematical extension from fixed to variable rates
- Empirical rate calibration: Why needed - bridges theoretical bounds with practical performance optimization; Quick check - verify the process of selecting optimal deletion rates from clean data

## Architecture Onboarding

Component map: Input sequence -> Deletion rate selection -> Randomized deletion smoothing -> Prediction aggregation -> Certified radius calculation

Critical path: Input → Adaptive deletion rate selection → Smoothed prediction → Certified bound computation

Design tradeoffs: Adaptive rates vs computational overhead, certified radius size vs clean accuracy, theoretical guarantees vs empirical calibration needs

Failure signatures: Poor rate selection leading to reduced clean accuracy, theoretical bounds not tight enough for practical use, computational costs exceeding benefits

First experiments to run:
1. Compare certified radii on a simple sequence classification task using fixed vs adaptive deletion rates
2. Measure clean accuracy degradation as deletion rate varies across different input lengths
3. Benchmark computational overhead of adaptive rate selection against baseline fixed-rate methods

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes i.i.d. character deletions which may not capture all real-world adversarial perturbations
- Evaluation focused on relatively clean datasets, leaving uncertainty about performance on noisy real-world text
- Computational overhead of adaptive rate selection could become significant at scale
- Performance on non-character-level tasks (e.g., word-piece tokenization) remains unexplored

## Confidence

High confidence in theoretical extension of randomized smoothing to adaptive deletion rates
Medium confidence in empirical gains given strong baselines but limited dataset diversity
Medium confidence in scalability claims pending larger-scale deployment tests

## Next Checks

1. Test AdaptDel on noisy, real-world text datasets with spelling errors and informal language to assess robustness beyond controlled benchmarks
2. Evaluate computational overhead and latency on large-scale text classification tasks with varying sequence lengths
3. Benchmark AdaptDel against adaptive defense methods in computer vision to establish cross-domain generalizability of the adaptive smoothing framework