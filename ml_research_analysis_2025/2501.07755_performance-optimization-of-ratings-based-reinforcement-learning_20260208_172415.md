---
ver: rpa2
title: Performance Optimization of Ratings-Based Reinforcement Learning
arxiv_id: '2501.07755'
source_url: https://arxiv.org/abs/2501.07755
tags:
- reward
- rbrl
- learning
- performance
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates optimization methods to improve the performance
  of rating-based reinforcement learning (RbRL). RbRL learns reward functions from
  human ratings, which are then used to train policies via standard RL algorithms.
---

# Performance Optimization of Ratings-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.07755
- Source URL: https://arxiv.org/abs/2501.07755
- Reference count: 3
- This paper investigates optimization methods to improve the performance of rating-based reinforcement learning (RbRL), achieving over 100% performance gains in some cases.

## Executive Summary
This paper investigates optimization methods to improve the performance of rating-based reinforcement learning (RbRL). RbRL learns reward functions from human ratings, which are then used to train policies via standard RL algorithms. The authors explore eight optimization techniques, including reward boundary selection, confidence index tuning, novel class probability functions, and traditional ML methods like dropout, activation functions, and learning rate adjustments. Experiments on environments like Walker, Quadruped, and Cheetah demonstrate that optimized RbRL achieves better and more consistent performance across different numbers of rating classes. Key improvements include over 100% performance gains in some cases and enhanced stability. The study provides guidelines for selecting hyperparameters to maximize RbRL effectiveness.

## Method Summary
The method involves training a reward predictor network using human ratings of trajectory segments, then using these predicted rewards to train a PPO policy. The optimized configuration uses 2 hidden layers with ArcTan activation, 5% dropout, AdamW optimizer with learning rate 0.0005, and confidence index k=1. The reward predictor minimizes cross-entropy loss between predicted class probabilities and human ratings, using either boundary-based (Eq. 3) or distance-to-mean (Eq. 5) probability functions. Training begins with entropy-based exploration rewards for 32,000 timesteps before switching to predicted rewards.

## Key Results
- Over 100% performance gains in Cheetah environment when using ArcTan activation instead of Tanh
- Eq. (5) distance-to-mean probability function shows better stability across different reward boundaries compared to Eq. (3)
- k=1 confidence index produces best performance in most environments, with extreme values degrading results
- 5% dropout and AdamW optimizer with learning rate 0.0005 provide optimal configuration

## Why This Works (Mechanism)

### Mechanism 1: Center-Based Class Probability Function
A probability function measuring deviation from class midpoint (Eq. 5) yields more stable performance across varying reward boundaries than boundary-distance functions (Eq. 3). This reduces sensitivity to boundary placement and handles cases where segments cluster near class edges.

### Mechanism 2: Confidence Index Calibration
Setting confidence index k ≈ 1 produces best performance in most environments; extreme values degrade results. The confidence index controls the sharpness of the softmax-like probability distribution, with values too low producing flat distributions and values too high producing peaked distributions.

### Mechanism 3: ArcTan Activation for Reward Prediction
ArcTan activation provides superior consistency and performance across rating class counts compared to Tanh, Sigmoid, and Lecun Tanh. It provides smoother gradient flow in the reward predictor network, potentially avoiding saturation issues while maintaining bounded outputs suitable for reward scaling.

## Foundational Learning

- **Cross-Entropy Loss for Ordinal Classification**
  - Why needed here: RbRL frames reward learning as classification where human ratings are labels; understanding how cross-entropy penalizes mismatch between predicted probabilities Q_σ(i) and actual ratings μ_σ(i) is essential.
  - Quick check question: Can you explain why minimizing cross-entropy between predicted class probabilities and one-hot encoded human ratings produces a useful reward function?

- **Reward Shaping and Normalization**
  - Why needed here: The paper normalizes cumulative rewards across batches and uses boundaries (R̄ᵢ) to define rating classes; understanding this scaling is critical for interpreting Eq. (3) and (5).
  - Quick check question: Why must cumulative rewards be normalized before computing class probabilities?

- **Preference-based vs. Rating-based RLHF**
  - Why needed here: RbRL differs from PbRL by using individual ratings rather than pairwise preferences; this affects data collection, loss formulation, and hyperparameter sensitivity.
  - Quick check question: What are the trade-offs between requiring pairwise comparisons (PbRL) versus single-sample ratings (RbRL) for human annotators?

## Architecture Onboarding

- **Component map:** Environment -> Reward Predictor -> PPO Agent -> Updated Policy -> New Data Collection
- **Critical path:** 1. Collect 32K timesteps with entropy-based exploration reward 2. Extract segments and obtain human (or synthetic) ratings 3. Train reward predictor by minimizing cross-entropy loss 4. Switch to predicted rewards for PPO policy updates 5. Iterate: periodically re-rate segments from updated policy
- **Design tradeoffs:** More rating classes (higher n) → finer reward granularity but increased class imbalance and variance; Higher confidence index k → sharper probability distributions but potential gradient issues; More hidden layers → higher capacity but overfitting risk (3 layers competitive but 2 preferred); Dropout → better generalization but can harm performance if set incorrectly (5% worked best)
- **Failure signatures:** High variance across runs with same hyperparameters → check dropout rate and optimizer (AdamW more stable than Adam for higher n); Performance degrades as n increases → class imbalance issue; consider boundary rebalancing; Inconsistent performance across reward boundaries → switch from Eq. (3) to Eq. (5); Training instability or collapse → check learning rate (0.0005 recommended), confidence index (k≈1)
- **First 3 experiments:** 1. Baseline replication: Run original RbRL (Eq. 3, Adam, Tanh, k=1, no dropout) on Walker with n=2,4,6 ratings to establish performance benchmarks; 2. Ablation sweep: Test ArcTan vs. Tanh activation with AdamW optimizer on Cheetah n=2; expect ~100% improvement based on Figure 7; 3. Stability validation: Compare Eq. (3) vs. Eq. (5) across reward boundaries (15, 20, 25, 30) in Cheetah n=6; expect Eq. (5) to show lower variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods be developed to mitigate the performance harm caused by mislabeled segments in the training data?
- Basis in paper: The authors ask, "Is there a method which can be used to mitigate performance harm done by mislabeled segments?"
- Why unresolved: The current study relies on synthetic ratings derived from true environment rewards, which assumes perfectly accurate labels, unlike noisy real-world human feedback.
- What evidence would resolve it: A study demonstrating robust policy performance when a defined percentage of training labels are intentionally incorrect or noisy.

### Open Question 2
- Question: Is it possible to combine Preference-based RL (PbRL) and Rating-based RL (RbRL) into a single schema to improve consistency?
- Basis in paper: The authors ask, "Can we leverage both Preference-based Reinforcement Learning (PbRL) and RbRL in the same learning schema to improve consistency in performance?"
- Why unresolved: This paper optimizes RbRL in isolation, and it is unknown if the absolute ratings of RbRL are compatible with the comparative approach of PbRL.
- What evidence would resolve it: Implementation of a hybrid algorithm that outperforms standalone PbRL or RbRL on standard benchmarks.

### Open Question 3
- Question: Do the optimization techniques identified in this work transfer successfully from robotics tasks to language tasks?
- Basis in paper: The authors ask, "Do these optimization techniques transfer from traditional robotics tasks to language tasks?"
- Why unresolved: The experiments were limited to simulated locomotion environments (Walker, Quadruped, Cheetah), which have different data structures than language domains.
- What evidence would resolve it: Application of the optimized hyperparameters (e.g., ArcTan activation, specific learning rates) to a text-based RL environment with comparable success.

### Open Question 4
- Question: Do these optimization guidelines hold true for human users with different backgrounds and rating styles?
- Basis in paper: The authors ask to "Conduct human subject tests to validate these methods for users with different backgrounds."
- Why unresolved: The paper notes that "synthetic humans" were used to simplify the study; thus, the variability of actual human cognition was not captured.
- What evidence would resolve it: Empirical results from human-in-the-loop experiments confirming that the optimized settings reduce variance across different human raters.

## Limitations
- Results rely on synthetic ratings rather than real human data, limiting external validity
- Experiments limited to three locomotion environments, restricting generalizability
- Several critical implementation details unspecified (hidden layer width, exact PPO hyperparameters)

## Confidence
- Medium confidence due to strong empirical evidence for specific hyperparameter choices but missing critical details
- Results show consistent improvements across multiple environments
- Reliance on synthetic ratings rather than real human data introduces uncertainty about external validity

## Next Checks
1. Implement the minimum viable reproduction with Walker environment using synthetic ratings to verify the >100% performance gains reported for ArcTan activation.
2. Conduct a hyperparameter sensitivity analysis around k=1 to confirm the claimed optimal confidence index and identify failure modes.
3. Test the reward boundary selection recommendations (Eq. 5 vs. Eq. 3) across a broader range of boundary values to quantify stability improvements.