---
ver: rpa2
title: Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph
  Learning
arxiv_id: '2505.17875'
source_url: https://arxiv.org/abs/2505.17875
tags:
- feature
- label
- selection
- learning
- sgmfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a consistent sparse graph learning method
  for semi-supervised multi-label feature selection (SGMFS). The method addresses
  two main challenges in semi-supervised multi-label scenarios: (1) difficulty in
  evaluating label correlations without enough labeled samples, and (2) suboptimal
  similarity graph structures derived from original feature spaces.'
---

# Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning

## Quick Facts
- **arXiv ID:** 2505.17875
- **Source URL:** https://arxiv.org/abs/2505.17875
- **Reference count:** 40
- **Key outcome:** SGMFS outperforms state-of-the-art methods on seven multi-label benchmarks, showing improved stability and robustness especially with limited labeled data.

## Executive Summary
This paper introduces SGMFS, a semi-supervised multi-label feature selection method that addresses key challenges in learning label correlations and constructing reliable similarity graphs when labeled data is scarce. The method learns a low-dimensional independent label subspace and performs sparse reconstruction in both label space and subspace to maintain consistency. Through extensive experiments on seven multi-label benchmarks, SGMFS demonstrates superior performance compared to existing state-of-the-art approaches, particularly when limited labeled samples are available.

## Method Summary
SGMFS is an alternating minimization framework that jointly learns feature-to-label mapping, a shared label subspace, and a sparse similarity graph. The method alternates between updating the projection matrix, subspace basis, feature weights, bias, soft labels, and graph structure. Key innovations include learning label correlations via an orthogonal subspace and adaptively constructing a sparse graph that maintains consistency between label space and subspace. The optimization combines semi-supervised learning, label correlation learning, and adaptive sparse graph learning with convergence guarantees.

## Key Results
- SGMFS consistently outperforms seven state-of-the-art methods across seven multi-label benchmarks
- Performance improvements are most significant when labeled data is limited (10-35% labeled)
- Ablation studies confirm the importance of space consistency and label correlation learning
- The method shows good stability across different label proportions and subspace dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning a shared label subspace captures multi-label correlations even when labeled samples are sparse.
- **Mechanism:** The method maps features via weight matrix W into a label space, then projects into a low-dimensional subspace where each dimension is independent (orthogonal). This decomposes correlated labels into uncorrelated latent factors. The constraint $Q^TQ = I$ enforces orthogonality, implicitly encoding label dependencies into the subspace structure rather than explicit pairwise correlations.
- **Core assumption:** Label correlations can be factorized into independent latent dimensions; the subspace dimension $l_{sd}$ is sufficient to capture the correlation structure.
- **Evidence anchors:** [abstract]: "learns a low-dimensional and independent label subspace... which can compatibly cross multiple labels and effectively achieve the label correlations"; [Section 3.3, Eq. 9]: Subspace learning objective with orthogonality constraint; [corpus]: Related work (SCFS, CSFS) also uses subspace learning but without the joint sparse graph mechanism; limited direct validation of why orthogonality helps correlation capture.
- **Break condition:** If label correlations are highly non-linear or hierarchical, a linear orthogonal subspace may fail to capture them; if $l_{sd}$ is set too low, information is lost; if too high, the subspace becomes redundant.

### Mechanism 2
- **Claim:** Adaptively learning a sparse graph jointly in label space and subspace yields more reliable soft labels than fixed kNN graphs from original features.
- **Mechanism:** Instead of pre-computing a similarity graph from raw features (which may be noisy or irrelevant), the method optimizes graph weights M via l1-regularized reconstruction. Samples are reconstructed from their neighbors in both the predicted label space F and the shared subspace Q simultaneously (Eq. 10: $\|MF - F\|_F^2 + \|MQ - Q\|_F^2$). The l1-norm induces sparsity, automatically selecting the most relevant neighbors.
- **Core assumption:** The manifold structure in label/subspace is more informative for semi-supervised propagation than the structure in raw feature space; sparsity corresponds to selecting true neighbors.
- **Evidence anchors:** [abstract]: "instead of constructing a fixed similarity graph... SGMFS thoroughly explores the intrinsic structure... by performing sparse reconstruction of samples in both the label space and the learned subspace simultaneously"; [Section 3.2, Eq. 6]: Adaptive sparse graph objective; [corpus]: GPMFS and Graph Random Walk methods similarly use adaptive graphs but differ in alignment strategy; consensus that adaptive graphs outperform fixed kNN is emerging but not universally proven.
- **Break condition:** If the initial features are so noisy that even label-space reconstruction fails, the graph may still be unreliable; sparsity parameter γ must be tuned—too sparse disconnects the graph, too dense adds noise.

### Mechanism 3
- **Claim:** Enforcing consistency between label space and subspace during graph learning stabilizes soft label propagation and improves feature selection.
- **Mechanism:** The unified objective (Eq. 11) couples three components: (1) regression from features to labels, (2) subspace learning, and (3) sparse graph learning. By minimizing reconstruction error in both spaces simultaneously, the learned graph M must support consistent neighbor relationships across representations. This regularizes the soft label matrix F, preventing drift during propagation.
- **Core assumption:** Consistency across representations implies correctness; the alternating optimization converges to a mutually compatible solution.
- **Evidence anchors:** [abstract]: "the similarity graph can be adaptively learned to maintain the consistency between label space and the learned subspace, which can promote propagating proper soft labels"; [Section 3.4, Eq. 11]: Full objective combining all terms; [Section 5.6 Ablation Study]: SGMFS\sc (without space consistency) and SGMFS\lc (without label correlation) both underperform full SGMFS; [corpus]: Limited direct evidence; related methods (SFSS, SCFS) use similar consistency terms but vary in formulation.
- **Break condition:** If the label space and subspace have inherently different geometries (e.g., one discrete, one continuous), enforcing consistency may force compromises that degrade both.

## Foundational Learning

- **Concept: Semi-supervised label propagation**
  - **Why needed here:** SGMFS propagates labels from a small labeled set to unlabeled data via graph-based diffusion. Understanding LP basics (graph construction, transition matrices, convergence) is essential to grasp how M and F interact.
  - **Quick check question:** Can you explain why the graph structure critically affects which labels get propagated to unlabeled nodes?

- **Concept: Sparse representation and l1-regularization**
  - **Why needed here:** The graph M is learned via l1-penalized reconstruction, which induces sparsity. You need to understand why l1 yields sparse solutions and how this relates to automatic neighbor selection.
  - **Quick check question:** Why does l1-regularization tend to produce sparse weights while l2 does not?

- **Concept: Multi-label correlation modeling**
  - **Why needed here:** The core innovation is capturing label dependencies via a shared subspace. Understanding why label correlation matters for feature selection (shared vs. label-specific features) clarifies the motivation.
  - **Quick check question:** In multi-label data, why might ignoring label correlations cause feature selection to discard important features?

## Architecture Onboarding

- **Component map:** X (features) -> W (feature weights) -> F (soft labels) -> M (sparse graph) -> Q (subspace basis)
- **Critical path:** The eigendecomposition for Q and the matrix inversion for W are the computational bottlenecks. Sparse graph update is O(n³) but uses efficient element-wise updates.
- **Design tradeoffs:**
  - **Subspace dimension $l_{sd}$**: Paper suggests $c/2$ (half the label count). Smaller = more compression but may lose correlation nuance; larger = more expressive but risk overfitting.
  - **Sparsity parameter γ**: Controls graph density. Paper sets γ=1; tuning range {10⁻³, ..., 10³} used in experiments.
  - **Label proportion**: Method is designed for 10-35% labeled data. With very few labels (<5%), graph initialization may be too unreliable.
- **Failure signatures:**
  - **Non-convergence:** If M becomes disconnected or F diverges, check γ and initialization. Algorithm is proven convergent but sensitive to extreme parameters.
  - **Degenerate Q:** If eigendecomposition yields near-identical eigenvectors, the subspace may be too small or data too isotropic.
  - **All-zero Fu:** If soft labels for unlabeled data stay at zero, the graph may be too sparse or β too small.
- **First 3 experiments:**
  1. **Reproduce on Emotions dataset** (smallest, 6 labels, 72 features): Run with 20% labeled data, γ=1, $l_{sd}$=3. Verify Hamming Loss and Ranking Loss match paper trends (~15-25 iterations to converge).
  2. **Ablate space consistency**: Set β=0 (disable graph consistency term). Compare to full SGMFS on same dataset to confirm performance drop as shown in Table 5.
  3. **Vary label proportion**: Test with 10%, 20%, 35% labeled data on Scene or Yeast. Expect larger performance gaps vs. baselines at lower label rates, per Section 5.5 findings.

## Open Questions the Paper Calls Out

- **Can the SGMFS framework be extended to automatically determine the optimal proportion of features to select without relying on manual thresholding?**
  - **Basis in paper:** [explicit] The conclusion states future research will focus on "developing semi-supervised methods capable of automatically determining optimal feature selection proportions."
  - **Why unresolved:** The current method ranks features by weight norm but requires the user to manually select a percentage (e.g., 2% to 30%) as shown in the experiments.
  - **What evidence would resolve it:** An enhanced algorithm that dynamically identifies a cut-off point in the feature weight distribution or a theoretical criterion for the number of features $k$.

- **How can the proposed method be adapted to effectively handle multi-view scenarios where data is drawn from heterogeneous feature sources?**
  - **Basis in paper:** [explicit] The conclusion lists "addressing multi-view scenarios" as a specific focus for future research.
  - **Why unresolved:** The current model assumes a single feature matrix $X$, and the graph learning process does not account for distinct manifold structures that might exist across different views of the same data.
  - **What evidence would resolve it:** A modified SGMFS architecture with view-specific graph constraints or a fusion strategy for multi-view sparse graph learning.

- **Can the dimension of the shared label subspace ($lsd$) be determined adaptively based on data structure rather than being set as a hyperparameter?**
  - **Basis in paper:** [inferred] Section 5.8 (Sensitivity Analysis) notes that $lsd$ influences performance and "the mezzo value is propitious," while Section 5.2 states it is fixed to $c/2$ during experiments.
  - **Why unresolved:** The dimension significantly impacts label correlation learning, yet the paper relies on empirical tuning rather than an adaptive determination mechanism.
  - **What evidence would resolve it:** Integration of a variance-explained criterion or an optimization penalty (e.g., rank constraints) to learn $lsd$ directly from the data.

## Limitations
- Initialization details for W and the initial graph M1 are underspecified, creating reproducibility gaps
- Computational complexity may limit scalability to large-scale high-dimensional datasets
- Method is only validated on seven multi-label datasets; effectiveness on highly imbalanced labels or multi-label streams is untested

## Confidence
- **High:** The core alternating optimization framework is sound and converges as proven. The integration of subspace learning, label propagation, and sparse graph learning is technically valid.
- **Medium:** The empirical superiority over baselines is well-demonstrated, but some claims (e.g., exact mechanisms of label correlation capture) lack direct ablation or theoretical grounding.
- **Low:** Claims about robustness to very sparse labels (<5%) or extremely high-dimensional features are speculative; the paper only tests 10-35% label rates and moderate feature counts.

## Next Checks
1. **Ablation of subspace dimension $l_{sd}$:** Test SGMFS with $l_{sd} = c/4$, $c/2$, $c$ on a medium dataset (e.g., Scene) to empirically validate the $c/2$ heuristic and check for overfitting or underfitting.
2. **Stress test on low-label regime:** Evaluate SGMFS with only 5% labeled data on Yeast to assess its limits and compare degradation against baselines.
3. **Runtime and scalability profiling:** Measure per-iteration runtime and memory usage on a large dataset (e.g., Enron) to identify bottlenecks and confirm scalability assumptions.