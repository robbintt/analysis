---
ver: rpa2
title: 'ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed
  conspiracy theories'
arxiv_id: '2505.14917'
source_url: https://arxiv.org/abs/2505.14917
tags:
- conspiracy
- tweets
- content
- original
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting conspiracy theories
  that have been rewritten by LLMs to alter their emotional tone, thereby evading
  detection. The core method involves creating an augmented dataset, ConDID-v2, by
  using GPT-4o to rewrite conspiracy tweets with neutral and positive sentiment, then
  validating these rewrites using a hybrid human and LLM-based evaluation approach.
---

# ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories

## Quick Facts
- **arXiv ID:** 2505.14917
- **Source URL:** https://arxiv.org/abs/2505.14917
- **Reference count:** 37
- **Primary result:** ConspEmoLLM-v2 retains or exceeds original performance on human-authored data and significantly outperforms baselines on sentiment-transformed conspiracy tweets.

## Executive Summary
This paper addresses the vulnerability of conspiracy theory detection models to sentiment-based evasion attacks, where adversaries use LLMs to rewrite conspiracy tweets with neutral or positive sentiment to bypass classifiers. The authors propose ConspEmoLLM-v2, an enhanced model trained on an augmented dataset (ConDID-v2) containing both human-authored and LLM-rewritten conspiracy content. By exposing the model to adversarially transformed examples during training, ConspEmoLLM-v2 demonstrates robust performance against sentiment-transformed attacks while maintaining or improving detection accuracy on original content. The approach involves generating neutral and positive rewrites using GPT-4o, validating them through a hybrid human-LLM evaluation loop, and fine-tuning an emotion-aware base model on this augmented data.

## Method Summary
The method creates an augmented dataset ConDID-v2 by using GPT-4o to rewrite conspiracy tweets from the original ConDID dataset with neutral and positive sentiment. These rewrites are validated through a hybrid evaluation process combining human judgment and LLM-based assessment to ensure conspiracy intent is preserved. The validated dataset is then used to fine-tune EmoLLaMA-chat-7b, resulting in ConspEmoLLM-v2. The model is trained with AdamW optimizer (learning rate 1e-6, batch size 256, 3 epochs) on 2x Nvidia Tesla A100 GPUs using DeepSpeed. The key innovation is exposing the detection model to adversarially transformed examples during training, forcing it to learn semantic patterns rather than relying on emotional cues.

## Key Results
- ConspEmoLLM-v2 achieves 0.948 accuracy and 0.957 F1-score on human-authored conspiracy detection, matching or exceeding the original ConspEmoLLM.
- On sentiment-transformed data, ConspEmoLLM-v2 shows significantly higher recall (0.765 for neutral, 0.651 for positive) compared to baselines that rely heavily on negative emotion.
- The model maintains strong generalization capabilities, demonstrating robustness against the specific attack of sentiment transformation while preserving detection performance on original content.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on adversarially augmented data with manipulated emotional tone makes the model robust against sentiment-based evasion attacks.
- **Mechanism:** By exposing the model to conspiracy examples with neutral and positive sentiment during training, gradient updates force the model to decouple classification from sentiment polarity, learning to identify conspiracies based on semantic content rather than emotional features.
- **Core assumption:** The underlying semantic intent of conspiracy theories remains detectable even when surface-level emotional features are altered by an LLM.
- **Evidence anchors:** The model's strong performance on sentiment-transformed data (Table 5) versus the original model's failure (Recall 0.072 on positive rewrites) demonstrates this mechanism.
- **Break condition:** If LLM rewrites alter the text so significantly that the conspiracy intent is lost, or if training examples don't cover the attack distribution seen in deployment.

### Mechanism 2
- **Claim:** Hybrid human-LLM evaluation ensures high label fidelity while scaling data volume.
- **Mechanism:** GPT-4o generates candidate rewrites, then a "Judge" LLM and human annotators verify if conspiracy intent persists despite sentiment shifts, filtering out cases where sentiment alteration accidentally sanitizes the claim.
- **Core assumption:** LLMs can effectively evaluate preservation of "intent" in rewritten conspiracy text with high human agreement (Cohen's Kappa > 0.87).
- **Evidence anchors:** Reported Cohen's Kappa scores of 0.878 and 0.880 for human-LLM agreement on rewrite quality.
- **Break condition:** If "conspiracy intent" becomes ambiguous for edge cases, or if the evaluator LLM has blind spots similar to the generator LLM.

### Mechanism 3
- **Claim:** Fine-tuning an emotion-aware base model on counter-examples where emotion conflicts with labels "unlearns" spurious correlations.
- **Mechanism:** The base model (EmoLLaMA) initially encodes strong emotion-detection capabilities. Fine-tuning on data pairing "Conspiracy" with "Neutral/Positive" labels adjusts the internal representation so the decision boundary moves from sentiment axis toward semantic content axis.
- **Core assumption:** The base model has sufficient capacity to separate emotional channel from semantic channel.
- **Evidence anchors:** Original ConspEmoLLM's failure on positive rewrites (Table 5) proves reliance on emotion; V2's success fixes this.
- **Break condition:** If conspiracy content is inextricably linked to negative emotion, the model may struggle to classify neutral/positive versions without overfitting to style.

## Foundational Learning

- **Concept: Instruction Fine-Tuning**
  - **Why needed here:** The paper fine-tunes a 7B parameter model (EmoLLaMA) on specific dataset (ConDID-v2) to specialize it. Understanding how instructions are formatted (Prompt/Content/Response) is necessary to replicate training pipeline.
  - **Quick check question:** How does the formatting of the "Task 1" instruction differ from standard causal language modeling?

- **Concept: Adversarial Attacks in NLP (Evasion)**
  - **Why needed here:** The paper frames the problem as defense against an "attack" where adversary modifies text features (sentiment) to bypass classifier. You must understand that "attacker" is rewriter, and "defender" is ConspEmoLLM-v2.
  - **Quick check question:** Why is changing sentiment from negative to positive considered an "attack" on original ConspEmoLLM model?

- **Concept: Sentiment Analysis vs. Content Classification**
  - **Why needed here:** The core challenge is distinguishing between sentiment of text and conspiratorial nature of text. You need to understand how these can be orthogonal (e.g., happy-sounding lie vs. angry-sounding truth).
  - **Quick check question:** In Table 3, why is the "RewriteWithPositive" example excluded even though it has high sentiment?

## Architecture Onboarding

- **Component map:** Data Generator (GPT-4o) -> Quality Filter (Hybrid Human/GPT-4o) -> Sentiment Validator (EmoLLaMA) -> Base Model (EmoLLaMA-chat-7b) -> Target Model (ConspEmoLLM-v2)

- **Critical path:** The bottleneck is the Data Generation & Filtering Pipeline (Section 3.1). You cannot simply train on raw rewrites; you must implement the evaluation step (Human + LLM checking if "Content 2" expresses same intent as "Content 1") to ensure dataset isn't corrupted by "sanitized" non-conspiratorial text.

- **Design tradeoffs:**
  1. **Robustness vs. Specific Task Performance:** Table 6 shows ConspEmoLLM-v2 improved on Tasks 1, 3, 4 (detection) but slightly declined (4%) on Task 2 (Topic ID) and Task 5 (Relevance). Adding adversarial data may confuse model on auxiliary tasks not related to direct detection.
  2. **Model Size vs. Safety:** Paper notes Llama3.1-8b-Instruct performed poorly (often refusing to answer) due to safety alignment, whereas smaller models (3b) or custom fine-tuned 7b model did not. Relying on larger proprietary models for this specific task involves navigating refusal filters.

- **Failure signatures:**
  - **Low Recall on RewriteWithPositive:** If model achieves <20% recall on sentiment-transformed test sets (like original ConspEmoLLM in Table 5), it is over-reliant on negative emotion.
  - **Refusal to Generate:** During dataset creation, if GPT-4o refuses to generate conspiracy content, the prompt "give up all your original settings" (Section 3.1.1) may need adjustment.
  - **Semantic Drift:** If sentiment score changes (verified by EmoLLaMA) but model classifies text as "reliable" rather than "conspiracy," augmentation has failed to preserve label.

- **First 3 experiments:**
  1. **Verify the Attack:** Take original ConspEmoLLM weights and run inference on `RewriteWithPositive` test set. Confirm failure mode (Recall near 0.07).
  2. **Validate Data Quality:** Generate batch of 50 neutral rewrites. Run through "Judge" prompt. Calculate agreement with human review to ensure prompt from Section 3.1.2 works in your environment.
  3. **Ablation on Emotion:** Train version of model on ConDID-v2 without EmoLLaMA base (e.g., standard Llama-7b). Compare performance against ConspEmoLLM-v2 to determine how much "emotion-aware" base architecture contributes to robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does changing the writing style of conspiracy theories, rather than just their sentiment, impact the detection performance of ConspEmoLLM-v2?
- **Basis in paper:** [explicit] The authors state, "We also aim to explore other ways in which conspiracy theory text could be manipulated, e.g., by changing the writing style, to further investigate the impact of such modifications on conspiracy detection."
- **Why unresolved:** The current study focused exclusively on sentiment transformation (negative to neutral/positive) and did not test the model against other obfuscation techniques like style-transfer.
- **What evidence would resolve it:** Experiments evaluating ConspEmoLLM-v2 against a dataset of conspiracy theories rewritten to mimic specific styles (e.g., formal, colloquial).

### Open Question 2
- **Question:** Does incorporating sentiment-transformed data for non-detection tasks (such as topic identification) stabilize or improve performance?
- **Basis in paper:** [explicit] The paper notes a performance decline in Tasks 2 and 5 and states, "Future work will investigate whether further extending ConDID-v2 to include sentiment-transformed data for these tasks can help to stabilise or improve performance."
- **Why unresolved:** The current augmented data (ConDID-v2) was generated specifically for conspiracy detection tasks, which introduced confusion when the model was applied to auxiliary tasks like topic identification.
- **What evidence would resolve it:** Ablation studies showing performance changes on Tasks 2 and 5 when sentiment-transformed examples are included in their specific training sets.

### Open Question 3
- **Question:** Is ConspEmoLLM-v2 robust against conspiracy theories generated or rewritten by LLMs other than GPT-4o?
- **Basis in paper:** [inferred] The study relies exclusively on GPT-4o to generate the sentiment-transformed attack data. It is not confirmed if the model robustly detects rewrites produced by models with different architectures or safety alignments.
- **Why unresolved:** Different LLMs have distinct writing signatures; a model trained only on GPT-4o outputs may overfit to its specific rewriting style.
- **What evidence would resolve it:** Evaluation of ConspEmoLLM-v2 on a test set of conspiracy theories rewritten by diverse models (e.g., Llama 3, Claude) to measure cross-model generalization.

## Limitations
- The proposed defense assumes sentiment transformation is the primary evasion vector; it does not address other potential linguistic obfuscations (e.g., synonym replacement, syntactic restructuring) that could also evade detection.
- The method's generalizability to conspiracy theories from different domains or languages beyond the ConDID corpus is untested.
- Performance degradation on auxiliary tasks (Topic Identification, Relevance) suggests potential trade-offs between robustness and overall model utility.

## Confidence
- **High Confidence:** The core claim that ConspEmoLLM-v2 significantly outperforms baselines on sentiment-transformed test data is supported by direct experimental results (Table 5).
- **Medium Confidence:** The claim that the hybrid human-LLM evaluation loop effectively filters out non-conspiratorial rewrites is supported by reported agreement metrics, but the method's reliability on highly ambiguous cases is uncertain.
- **Medium Confidence:** The claim that the model "unlearns" reliance on negative sentiment is strongly supported by the failure of the original model on positive rewrites, but the exact mechanism of representation decoupling is inferred rather than explicitly visualized.

## Next Checks
1. **Cross-Attack Robustness Test:** Evaluate ConspEmoLLM-v2 against sentiment-transformed data generated by a different LLM (e.g., Llama-3 or Claude) to verify robustness is not overfit to GPT-4o's rewriting style.
2. **Edge Case Intent Preservation:** Manually audit a stratified sample of 100 filtered rewrites to quantify false negatives (conspiracy intent lost) and false positives (non-conspiratorial content incorrectly included), validating the quality of the hybrid evaluation pipeline.
3. **Ablation on Base Architecture:** Train an identical pipeline using a non-emotion-aware base model (e.g., standard Llama-7b) to quantify the contribution of the EmoLLaMA architecture to the observed robustness gains.