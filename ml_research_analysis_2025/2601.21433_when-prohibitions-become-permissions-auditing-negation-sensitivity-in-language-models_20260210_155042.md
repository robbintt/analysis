---
ver: rpa2
title: 'When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language
  Models'
arxiv_id: '2601.21433'
source_url: https://arxiv.org/abs/2601.21433
tags:
- negation
- should
- sensitivity
- action
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates negation sensitivity in large
  language models by testing 16 models across 14 ethical scenarios under four polarity-paired
  framings. It finds that open-source models endorse prohibited actions 77% of the
  time under simple negation and 100% under compound negation, representing a 317%
  increase over affirmative framing.
---

# When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models

## Quick Facts
- arXiv ID: 2601.21433
- Source URL: https://arxiv.org/abs/2601.21433
- Authors: Katherine Elkins; Jon Chun
- Reference count: 40
- Primary result: Open-source models endorse prohibited actions 77-100% of the time under negation

## Executive Summary
This study systematically evaluates negation sensitivity in large language models by testing 16 models across 14 ethical scenarios under four polarity-paired framings. The research finds that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation, representing a 317% increase over affirmative framing. Commercial models show varied sensitivity with swings of 19-128%. Inter-model agreement drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns persist under deterministic decoding, indicating structural failures rather than sampling artifacts. The study introduces the Negation Sensitivity Index as a governance metric and proposes a tiered certification framework with domain-specific thresholds for safe deployment.

## Method Summary
The study systematically evaluates negation sensitivity in large language models by testing 16 models across 14 ethical scenarios under four polarity-paired framings (F0: "should X", F1: "should NOT X", F2: "goal even if X", F3: "NOT goal if X"). Models include 8 US commercial, 4 Chinese commercial, and 4 open-source variants. Each model is queried with all 56 scenario-framing combinations (14 Ã— 4), collecting 30 responses per cell at T=0.7. Responses are parsed to binary agree/disagree decisions and normalized using Logical Polarity Normalization. The Negation Sensitivity Index (NSI) is calculated as the maximum difference in action endorsement rates across framings. Validation includes Cochran's Q test and Benjamini-Hochberg FDR correction (q=0.05), with a deterministic ablation at T=0.0 on 7 models.

## Key Results
- Open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation
- Commercial models show varied sensitivity with NSI swings of 19-128%
- Inter-model agreement drops from 74% on affirmative prompts to 62% on negated ones
- Financial scenarios prove twice as fragile as medical ones under negation

## Why This Works (Mechanism)
The negation sensitivity phenomenon occurs because language models struggle to correctly interpret logical negations in ethical decision-making contexts. When models encounter negated prompts, they often fail to invert their reasoning appropriately, leading to systematic errors in judgment. This is particularly problematic in compound negation scenarios where multiple logical inversions must be processed simultaneously. The study demonstrates that this is not merely a sampling artifact, as the effect persists under deterministic decoding, suggesting fundamental architectural or training limitations in how models handle logical negation.

## Foundational Learning
- Logical Polarity Normalization: Required to convert model outputs to consistent binary decisions across different prompt framings; quick check: verify that normalized outputs sum to 1.0 across all four framings for a given scenario.
- Cochran's Q test: Needed to test for statistical significance of differences across multiple related samples; quick check: ensure p-values are reported for each model's framing effects.
- Benjamini-Hochberg FDR correction: Essential for controlling false discovery rate when testing multiple models/scenarios; quick check: verify that q-values meet the 0.05 threshold for reported significant effects.
- Binary response classification: Critical for converting free-text model outputs to actionable metrics; quick check: confirm that classification achieves >95% coverage of responses.
- Scenario framing variants: Core to the experimental design; quick check: ensure all four framing variants are tested for each scenario.

## Architecture Onboarding

Component Map: Scenario Prompt -> Model Input -> Response Generation -> Binary Classification -> NSI Calculation -> Statistical Validation

Critical Path: The study's critical path involves generating scenario-framing prompts, collecting model responses, classifying these responses as binary decisions, computing action endorsement rates, calculating the Negation Sensitivity Index, and validating statistical significance. This pipeline must maintain consistency across all 16 models and 56 prompt conditions.

Design Tradeoffs: The choice of 30 samples per condition balances statistical power with computational cost. Binary classification simplifies analysis but may lose nuance from model responses. Deterministic decoding at T=0.0 helps isolate structural issues from sampling artifacts but may not reflect typical deployment conditions.

Failure Signatures: High NSI values (above 0.5) indicate severe negation sensitivity. Drops in inter-model agreement from affirmative to negated prompts (below 62%) suggest systematic interpretation failures. Domain-specific fragility (financial vs medical disparity) points to scenario-dependent model weaknesses.

First Experiments:
1. Replicate binary response classification on a subset of responses to verify the 77-100% endorsement rates match original findings
2. Test prompt template variations on 2-3 models to establish bounds on the NSI metric
3. Validate domain-specific fragility claims by expanding scenario sets within each domain

## Open Questions the Paper Calls Out
The paper highlights several open questions: how negation sensitivity scales with model size and capability, whether fine-tuning approaches can effectively mitigate these failures, and how to establish domain-specific thresholds for safe deployment. The authors also question whether current evaluation frameworks adequately capture the complexity of negation in real-world applications.

## Limitations
- Binary response classification methodology is underspecified
- Prompt template variations could significantly affect results
- Limited scope of ethical scenarios may not capture full model behavior
- The 30-sample size per condition may not fully capture response variability

## Confidence
- **High confidence**: The core finding that open-source models show 77-100% endorsement of prohibited actions under negation is well-supported by the data and consistent across multiple validation approaches.
- **Medium confidence**: The characterization of commercial models as "more negation-sensitive" is supported but based on fewer models with heterogeneous behaviors that require more nuanced interpretation.
- **Medium confidence**: The domain-specific fragility findings (financial vs medical) are statistically significant but based on a limited set of scenarios per domain.

## Next Checks
1. Replicate the binary decision parsing methodology on a subset of responses to verify the 77-100% endorsement rates match the original findings.
2. Test the impact of prompt template variations by running a sensitivity analysis with different formatting on 2-3 models to establish bounds on the NSI metric.
3. Validate the domain-specific fragility claims by expanding the scenario set within each domain and checking if the financial vs medical disparity persists.