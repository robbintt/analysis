---
ver: rpa2
title: Can human clinical rationales improve the performance and explainability of
  clinical text classification models?
arxiv_id: '2507.21302'
source_url: https://arxiv.org/abs/2507.21302
tags:
- rationales
- reports
- rationale
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether human-based clinical rationales
  could improve the performance and explainability of transformer-based models for
  clinical text classification. It explored the use of 99,125 rationales, which are
  human-annotated highlights from pathology reports, as additional training data alongside
  128,649 electronic pathology reports to classify primary cancer sites.
---

# Can human clinical rationales improve the performance and explainability of clinical text classification models?

## Quick Facts
- arXiv ID: 2507.21302
- Source URL: https://arxiv.org/abs/2507.21302
- Reference count: 40
- The study found that rationales improve explainability by priming models to focus on rationale-like features, but additional documents are more effective for optimizing accuracy

## Executive Summary
This study investigates whether human-annotated clinical rationales can improve the performance and explainability of transformer-based models for classifying primary cancer sites from pathology reports. Using 99,125 rationales alongside 128,649 electronic pathology reports, the researchers evaluated Clinical Longformer models trained with and without rationales. The results demonstrate that while rationales enhance explainability by directing model attention to clinically relevant text features, they are less effective than simply adding more labeled documents for improving classification accuracy. The study also reveals inconsistent behavior in low-resource scenarios, where rationales sometimes degraded performance for specific cancer classes.

## Method Summary
The researchers fine-tuned Clinical Longformer (CLF) models on SEER pathology reports to classify 69 primary cancer sites. They compared three training approaches: reports only, reports plus rationales (treated as independent supplementary samples), and reports plus additional labeled reports. Performance was measured using accuracy and macro-F1 scores, while explainability was quantified by computing the overlap between model attention weights and human-annotated rationale tokens. The study also tested automatic sufficiency filtering to pre-select high-quality rationales and evaluated a CLF variant with Deformable Phrase-Level Attention (DPLA) architecture.

## Key Results
- Rationales significantly improved model explainability by priming models to attend to rationale-like features
- Models trained on reports plus rationales achieved comparable performance to models trained on reports plus additional documents in high-resource scenarios
- In low-resource settings, rationales sometimes degraded performance, particularly for esophagus cancer classification
- Sufficiency-based rationale pre-selection yielded inconsistent results across different experimental conditions

## Why This Works (Mechanism)
Rationales improve explainability by providing explicit supervision on which text features are clinically relevant for classification decisions. When models are trained on rationales, they learn to prioritize the same features that human experts identified as important, creating more interpretable attention patterns. However, rationales may not capture all predictive features the model learns from full documents, leading to performance trade-offs. The discrepancy between human-plausible rationales and model-faithful explanations suggests that models may discover complex, distributed representations that cannot be fully captured by extractive highlights.

## Foundational Learning

- **Concept: Extractive Rationales vs. Full Documents**
  - Why needed here: The core intervention uses human-highlighted text snippets rather than full reports. Understanding this distinction is crucial for interpreting results.
  - Quick check question: In this paper, what is a "rationale" and how does it differ from a "complement"?

- **Concept: Model Explainability via Attention**
  - Why needed here: Explainability is measured by comparing model attention weights to human rationale tokens. This requires understanding attention as a proxy for feature importance.
  - Quick check question: What metric does the paper use to quantify "explainability," and what model component's output does it analyze?

- **Concept: Faithfulness vs. Plausibility in Explanations**
  - Why needed here: The paper argues rationales may be plausible to humans but not faithful to the model's decision process, explaining performance limitations.
  - Quick check question: Why does the paper argue that rationales, while plausible to humans, might not be "faithful" to the model's learning process?

## Architecture Onboarding

- **Component map:**
  - SEER pathology reports with labels -> Clinical Longformer (CLF-BS or CLF-DPLA) -> Classification head -> Accuracy/Macro-F1 scores
  - SEER pathology reports with rationales -> CLF with DPLA -> Attention weights -> Rationale coverage metric

- **Critical path:**
  1. Data Preparation: Obtain labeled pathology reports and human-annotated rationales, split into Train/Val/Test sets
  2. Model Training: Fine-tune Clinical Longformer on various regimens (reports only, reports + rationales, reports + additional reports)
  3. Performance Evaluation: Compute Accuracy and Macro-F1 on held-out test set
  4. Explainability Analysis: Compare model attention scores to rationale tokens using coverage metric

- **Design tradeoffs:**
  - Data Augmentation with Rationales vs. More Documents: Rationales are concise but may miss features; more documents are noisy but provide fuller data distribution
  - Attention as Explanation: Convenient built-in proxy vs. potentially misleading feature importance indicator
  - Rationale Pre-selection (Sufficiency): Reducing training size/variability vs. increasing data quality

- **Failure signatures:**
  - Low-Resource Regime Performance Drop: Rationales introducing conflicting signals the model cannot reconcile
  - High Attention on Complement: Model learning predictive features exist beyond human explanation
  - Inconsistent Sufficiency Filtering: Automatic metric failing as proxy for rationale quality

- **First 3 experiments:**
  1. Baseline and Control: Train CLF-BS and CLF-DPLA on "reports only" and "reports + additional reports"
  2. Rationale Augmentation: Train both models on "reports + all rationales" to test core rationale effectiveness claim
  3. Low-Resource Simulation: Incrementally add small numbers of rationales for subset of classes, monitoring performance changes

## Open Questions the Paper Calls Out

- Can an automatic metric be identified that effectively measures clinical rationale quality and consistently improves model performance when used for pre-selection?
- What is the optimal ratio of rationales to full reports necessary for effective model training, and how does this learning curve behave?
- Can Large Language Models generate synthetic or abstractive rationales that outperform human-based extractive rationales in clinical text classification tasks?

## Limitations
- Results are based on a single cancer classification task and may not generalize to other clinical NLP applications
- The study uses attention scores as explainability proxies, which may not accurately reflect model decision-making processes
- CLF-DPLA architecture improvements may confound the observed effects of rationale training

## Confidence

- **Performance vs. Explainability Tradeoff:** High - Consistently observed across multiple experimental conditions
- **Rationales as Independent Samples:** High - Explicitly validated against multi-task learning approaches
- **Low-Resource Regime Inconsistency:** Medium - Results show mixed patterns requiring further investigation
- **Sufficiency Filtering Unpredictability:** Medium - Clear inconsistency but limited exploration of alternative metrics

## Next Checks

1. **Cross-Domain Replication:** Apply the methodology to a different clinical NLP task (e.g., radiology report classification) to test whether rationale effectiveness correlates with task characteristics

2. **Rationale Quality Impact Study:** Systematically vary rationale quality by introducing controlled noise or incompleteness, then measure effects on both performance and explainability

3. **Architectural Ablation Test:** Conduct experiments isolating the effect of DPLA architecture by comparing rationale impact on CLF-BS versus CLF-DPLA