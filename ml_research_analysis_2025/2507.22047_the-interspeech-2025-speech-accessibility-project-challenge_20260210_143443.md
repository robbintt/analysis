---
ver: rpa2
title: The Interspeech 2025 Speech Accessibility Project Challenge
arxiv_id: '2507.22047'
source_url: https://arxiv.org/abs/2507.22047
tags:
- speech
- semscore
- data
- baseline
- challenge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Interspeech 2025 Speech Accessibility Project Challenge addressed
  the performance gap in ASR systems for individuals with speech disabilities by leveraging
  over 400 hours of speaker-independent SAP data from 500+ participants with diverse
  speech disorders. Hosted on EvalAI with remote evaluation, the challenge assessed
  submissions using Word Error Rate (WER) and Semantic Score (SemScore).
---

# The Interspeech 2025 Speech Accessibility Project Challenge

## Quick Facts
- **arXiv ID:** 2507.22047
- **Source URL:** https://arxiv.org/abs/2507.22047
- **Reference count:** 0
- **Primary result:** 12 out of 22 teams surpassed whisper-large-v2 baseline on WER; top team achieved 8.11% WER (54.49% relative improvement).

## Executive Summary
The Interspeech 2025 Speech Accessibility Project Challenge addressed the performance gap in ASR systems for individuals with speech disabilities by leveraging over 400 hours of speaker-independent SAP data from 500+ participants with diverse speech disorders. Hosted on EvalAI with remote evaluation, the challenge assessed submissions using Word Error Rate (WER) and Semantic Score (SemScore). Among 22 valid teams, 12 surpassed the whisper-large-v2 baseline on WER, and 17 on SemScore. The top team achieved a WER of 8.11% and a SemScore of 88.44%, setting new benchmarks for impaired speech recognition. These results demonstrate the effectiveness of fine-tuning speech foundation models on large-scale, diverse impaired speech data and validate the importance of inclusive ASR research.

## Method Summary
The challenge used the SAP-240430 dataset (~415 hours, 524 speakers) with audio at 16kHz and normalized transcripts. Participants fine-tuned foundation models (Whisper-large-v3 or Parakeet-tdt-1.1B) using LoRA/AdaLoRA on the training split. Techniques included audio segmentation, model merging, and LLM-based error correction. Evaluation used a custom WER formula (capped at 100%, choosing better of two references) and SemScore (weighted combination of NLI, BERT, and Soundex scores). Teams submitted transcriptions to EvalAI for remote scoring on Test1 and Test2 sets.

## Key Results
- 12 teams surpassed the whisper-large-v2 baseline on WER; top team achieved 8.11% WER (54.49% relative improvement).
- 17 teams surpassed the baseline on SemScore; top team achieved 88.44% SemScore.
- Performance gap between typical and impaired speech ASR was significantly reduced, demonstrating fine-tuning effectiveness.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Fine-Tuning of Speech Foundation Models
Fine-tuning large-scale speech foundation models on diverse, impaired speech data significantly reduces WER compared to off-the-shelf baselines. Pre-trained models like Whisper or Parakeet possess general acoustic knowledge but lack exposure to the specific acoustic anomalies of dysarthric speech (e.g., flat pitch, breathiness, stuttering). Fine-tuning re-calibrates model weights to map these atypical acoustic features to their corresponding phonemes, a process the paper explicitly validates.

### Mechanism 2: Semantic Alignment over Lexical Accuracy
Optimizing for semantic fidelity (SemScore) rather than strict lexical matching enables systems to better serve end-users by capturing intent, even when exact words are missed. Dysarthric speech often contains disfluencies or phonetic distortions. SemScore uses a composite of NLI, BERT embeddings, and phonetic distance to assess if the meaning is preserved. This mechanism allows the system to "pass" an utterance that conveys the correct intent despite lexical errors, aligning ASR output with human perception of intelligibility.

### Mechanism 3: Audio Segmentation and Post-Hoc Correction
Pre-processing long audio into shorter segments and post-processing outputs with LLMs improves robustness against dysarthric variability and model hallucination. Dysarthric speech often contains irregular pauses and prosody. Segmentation (via forced alignment or VAD) prevents the model from misinterpreting silence or stuttering as distinct tokens. Post-processing (LLMs) corrects residual semantic or grammatical errors.

## Foundational Learning

- **Concept: Dysarthria and Atypical Speech Patterns**
  - **Why needed here:** To understand why standard ASR fails. You must grasp that conditions like Parkinson's (hypokinetic) or ALS cause "flat pitch" or "breathy voicing," which confuses models trained on typical speech prosody.
  - **Quick check question:** Why might a standard ASR model interpret a prolonged pause in Parkinsonian speech as the end of a sentence rather than a hesitation?

- **Concept: Transfer Learning in Speech (Foundation Models)**
  - **Why needed here:** The winning solutions do not train from scratch. You need to understand how models like Whisper (trained on 680k hours of weak supervision) provide a "base" that is then specialized (fine-tuned) for the 400 hours of impaired speech.
  - **Quick check question:** What is the difference between "weakly supervised pre-training" (used by Whisper) and the supervised fine-tuning applied in this challenge?

- **Concept: Evaluation Metrics (WER vs. SemScore)**
  - **Why needed here:** To interpret the results correctly. You must understand that a low WER isn't the only goal; SemScore measures if the message got through, which is critical for accessibility.
  - **Quick check question:** If a user says "I need a doctor" and the ASR outputs "I need doctor," does WER or SemScore better capture the usability of this transcript?

## Architecture Onboarding

- **Component map:** Audio Input (SAP-240430 Dataset @ 16kHz) -> Audio Segmentation (VAD/Forced Alignment) -> Foundation Model Encoder (Whisper-large-v3/Parakeet-tdt-1.1B) -> Fine-tuning Layer (LoRA/AdaLoRA) -> LLM Post-processing (Optional) -> WER/SemScore Evaluation
- **Critical path:**
  1. Data Prep: Apply text normalization (remove brackets, handle disfluencies).
  2. Segmentation: (Optional but recommended) Split audio based on VAD or forced alignment to reduce hallucination.
  3. Fine-Tuning: Adapt the foundation model on the ~290 hours of training data (dominantly Parkinson's).
  4. Inference: Run inference on "unshared" test sets.
  5. Scoring: Submit to EvalAI for remote calculation of WER and SemScore.
- **Design tradeoffs:**
  - Generality vs. Specificity: The dataset is 73% Parkinson's. A model fine-tuned here may excel at hypokinetic dysarthria but struggle with ALS (spastic/flaccid) despite the reported gains.
  - Speed vs. Accuracy: Whisper-large-v3 is slower but accurate; "Turbo" models or smaller Parakeet models trade off slight accuracy for speed.
  - Disfluency Handling: You must choose whether to train the model to transcribe stutters (repandandums) or ignore them to maximize semantic flow.
- **Failure signatures:**
  - Hallucination: The model generates text during long silences or stutter events (common in Whisper). Fix: Implement WhisperX preprocessing or silence thresholds.
  - Omission: Low-energy speech (common in hypokinetic dysarthria) is skipped entirely.
  - Semantic Drift: Post-ASR LLMs might change the meaning to make the sentence grammatically "standard," losing the user's intent.
- **First 3 experiments:**
  1. Baseline Validation: Run `whisper-large-v2` (zero-shot) on the Test1 unshared set to reproduce the ~15-17% WER baseline.
  2. LoRA Fine-Tuning: Fine-tune `whisper-large-v3` using Low-Rank Adaptation (LoRA) on the SAP-240430 Train split to minimize compute cost while measuring performance delta.
  3. SemScore Correlation Check: Calculate WER and SemScore on your Dev set outputs to verify if your model improvements are "real" (semantic preservation) or just lexical overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the distinct within- and across-group performance disparities among etiology-based or severity-based populations when fine-tuning ASR models on the SAP dataset?
- **Basis in paper:** The conclusion states that future research "should further explore within- and across-group similarities and differences, e.g., among etiology-based or impairment severity-based populations."
- **Why unresolved:** The challenge results focused on aggregate benchmarks (WER/SemScore) and limited etiology-specific analysis (PD vs. ALS) due to the dominance of Parkinson's Disease data.
- **What evidence would resolve it:** A detailed ablation study reporting performance metrics disaggregated by specific etiology (e.g., Down Syndrome, Cerebral Palsy) and impairment severity levels.

### Open Question 2
- **Question:** Is the higher relative Word Error Rate (WER) reduction observed for ALS speakers compared to PD speakers attributable to lower acoustic variability in the ALS test cohort?
- **Basis in paper:** The authors hypothesize that the larger relative improvement for ALS (26.9%) versus PD (41.1%) is "possibly due to lower variability among ALS speakers," but do not verify this.
- **Why unresolved:** The paper reports standard deviations for WER but does not quantify acoustic variability (e.g., pitch range, speaking rate) or its correlation with model adaptation success.
- **What evidence would resolve it:** Acoustic analysis of the Test2 ALS and PD subsets to correlate speaker-level variability with the magnitude of fine-tuning improvements.

### Open Question 3
- **Question:** How can ASR models be optimized to preserve disfluencies and self-corrections without incurring penalties in semantic fidelity scores?
- **Basis in paper:** The paper notes that only 3 out of 29 top systems preferred transcribing disfluencies, and the evaluation method dynamically selects the reference type that optimizes the metric, potentially disincentivizing disfluency preservation.
- **Why unresolved:** It is unclear if the omission of disfluencies in top submissions is a failure of the model architecture or a result of "gaming" the evaluation metric which favors the cleaner transcript.
- **What evidence would resolve it:** Evaluation using a dedicated metric for disfluency detection accuracy alongside standard WER to assess true transcription capability.

## Limitations

- The SAP-240430 dataset is heavily imbalanced (73.4% Parkinson's Disease), making it unclear how well top models generalize to less-represented conditions.
- The SemScore metric lacks external validation against human perceptual studies to confirm it accurately reflects real-world usability.
- Fine-tuning improvements may not transfer well across etiologies due to dataset composition and acoustic variability differences.

## Confidence

- **High Confidence:** The effectiveness of fine-tuning foundation models on the SAP dataset for improving WER (supported by explicit results showing 12 teams surpassing the baseline with the top team achieving 8.11% WER, representing a 54.49% relative improvement).
- **Medium Confidence:** The superiority of semantic preservation (SemScore) over strict lexical accuracy for accessibility applications (supported by the mechanism description and example, but lacking direct human perception validation).
- **Low Confidence:** Generalization claims across all speech disorders given the dataset composition (73.4% Parkinson's Disease) and the SemScore formula's proxy validity without perceptual studies.

## Next Checks

1. **External Generalization Test:** Evaluate the top-performing models on an independent dataset with a more balanced distribution of speech disorders to verify cross-etiology performance claims.

2. **Human Perception Study:** Conduct a small-scale study where individuals with speech disabilities rate the understandability of ASR outputs, comparing systems optimized for WER versus SemScore to validate the semantic metric's practical relevance.

3. **Ablation Study on Dataset Composition:** Train models on subsets of the SAP-240430 data (e.g., only Parkinson's vs. only ALS) and test on each other's data to quantify the impact of the dataset's class imbalance on cross-disorder performance.