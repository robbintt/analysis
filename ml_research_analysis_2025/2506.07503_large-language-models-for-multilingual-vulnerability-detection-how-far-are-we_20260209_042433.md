---
ver: rpa2
title: 'Large Language Models for Multilingual Vulnerability Detection: How Far Are
  We?'
arxiv_id: '2506.07503'
source_url: https://arxiv.org/abs/2506.07503
tags:
- vulnerability
- llms
- detection
- multilingual
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for multilingual
  vulnerability detection across seven programming languages. The authors assess PLMs
  and LLMs using a dataset of 30,987 real-world vulnerability-fixing patches, examining
  both function-level and line-level detection capabilities.
---

# Large Language Models for Multilingual Vulnerability Detection: How Far Are We?

## Quick Facts
- arXiv ID: 2506.07503
- Source URL: https://arxiv.org/abs/2506.07503
- Reference count: 40
- Primary result: GPT-4o with instruction tuning and few-shot prompting achieves 71.96% accuracy at function-level and 66.41% F1-score at line-level for multilingual vulnerability detection

## Executive Summary
This study evaluates large language models (LLMs) for multilingual vulnerability detection across seven programming languages. The authors assess PLMs and LLMs using a dataset of 30,987 real-world vulnerability-fixing patches, examining both function-level and line-level detection capabilities. GPT-4o, enhanced through instruction tuning and few-shot prompting, significantly outperforms other models, achieving 71.96% accuracy at function-level and 66.41% F1-score at line-level. The LLM-based approach demonstrates superior capability in detecting unique multilingual vulnerabilities, particularly excelling in identifying the most dangerous and high-severity vulnerabilities. These results underscore the promising potential of adopting LLMs for multilingual vulnerability detection at both granularity levels, revealing their complementary strengths and substantial improvements over traditional PLM approaches.

## Method Summary
The study evaluates multilingual vulnerability detection using the REEF dataset containing 30,987 vulnerability-fixing patches across seven programming languages. Functions are extracted using Tree-sitter and labeled using DIFFLIB for line-level analysis. The evaluation compares traditional PLMs (CodeBERT, CodeT5, CodeT5P) against LLMs (GPT-4o, DeepSeek-Coder) using multiple strategies: zero-shot prompting, few-shot prompting with BM25 retrieval, instruction tuning, and the combination of instruction tuning plus few-shot prompting. Line-level formatting requires explicit "Line N:" prefixes to help LLMs locate issues. Models are fine-tuned using LoRA, with GPT-4o showing superior performance through the instruction tuning and few-shot prompting approach.

## Key Results
- GPT-4o with instruction tuning and few-shot prompting achieves 71.96% accuracy at function-level and 66.41% F1-score at line-level
- The LLM-based approach significantly outperforms traditional PLMs, showing 19.20% and 85.94% improvements in average accuracy
- Instruction tuning is critical for improving LLM effectiveness, while few-shot prompting provides stable predictions through in-context examples
- LLMs demonstrate superior capability in detecting unique multilingual vulnerabilities, especially high-severity vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Based Task Alignment
The model shifts from general code comprehension to supervised classification mode through instruction tuning, learning the specific mapping between code patterns and vulnerability labels. This alignment layer acts as task adaptation rather than knowledge injection, requiring sufficient pre-trained semantic knowledge to map instructions to code features.

### Mechanism 2: Contextual Calibration via Few-Shot Examples
BM25 retrieves semantically similar vulnerability examples, providing a reference distribution for the model to compare against. This dynamic calibration stabilizes predictions and handles class imbalance better than prompting alone, though performance degrades if retrieved examples are misleading.

### Mechanism 3: Semantic Generalization in Multilingual Contexts
LLMs outperform PLMs in multilingual settings by relying less on language-specific structural features and more on generalized semantic patterns learned from vast multilingual corpora. This shared semantic space allows transfer of vulnerability patterns across syntax boundaries, though generalization may fail for low-resource languages.

## Foundational Learning

- **False Positive Rate vs. False Negative Rate Trade-off**: In security, missing a vulnerability (high FNR) is dangerous while flagging safe code (high FPR) wastes developer time. Quick check: Does the model prioritize catching every bug or ensuring flagged bugs are real?

- **Instruction Tuning vs. Fine-Tuning**: The study distinguishes between training the whole model (fine-tuning for PLMs) and training LLMs on instruction-response pairs (instruction tuning). Quick check: Are we feeding raw code or structured prompts to the optimizer?

- **Cross-Project/Context Retrieval (BM25)**: The few-shot strategy relies on selecting demonstration examples from the training set using BM25 retrieval. Quick check: How does the choice of retrieval algorithm impact the quality of "shots" provided to the LLM?

## Architecture Onboarding

- **Component map**: REEF Dataset -> Parser (Tree-sitter) -> Function Extractor -> Labeling (DiffLib) -> Strategy Selector -> Retriever (BM25) -> Model (LLM/PLM)

- **Critical path**: Correctly mapping patches to before/after functions using edit distance, then properly formatting prompts to prevent verbose LLM outputs from breaking automated evaluation

- **Design tradeoffs**: PLMs are cheaper but suffer from high FPR (>74%), while LLMs require expensive API calls but achieve significantly lower FPR (~23%). Increasing model size doesn't necessarily improve performance due to overfitting risks

- **Failure signatures**: "Random guessing" behavior shows high accuracy with low F1-score, while line-level hallucination occurs when LLMs predict non-existent line numbers. High FPR (>0.5) indicates the model is likely guessing the majority class

- **First 3 experiments**: 1) Run CodeT5P on multilingual set to establish baseline (Expected: High Recall, Low Precision), 2) Test GPT-4o with zero-shot prompting to confirm random guessing limitation, 3) Fine-tune GPT-4o with instruction tuning then test using BM25-retrieved examples (Expected: F1-score > 0.65 at line level)

## Open Questions the Paper Calls Out

- Can fine-tuning reasoning-capable LLMs via reinforcement learning significantly enhance their performance beyond standard instruction tuning? The study evaluated reasoning LLMs only under zero-shot conditions, where they showed minimal improvement.

- Do learning strategies such as multi-task learning or meta-learning effectively reduce the performance gap between high-resource (e.g., JavaScript) and low-resource (e.g., C#) programming languages? The current study focused on fine-tuning and prompting strategies but didn't evaluate architectural training changes.

- Can unified intermediate representations for code enable graph-based models to match or exceed LLM multilingual capabilities? The paper excluded graph-based approaches due to practical challenges but suggests exploring different intermediate representations as future work.

## Limitations

- The evaluation relies on a single dataset (REEF), which may introduce bias in performance measurements
- Comparative analysis focuses primarily on GPT-4o versus other models without extensive ablation studies on instruction tuning datasets
- Cost implications of using instruction-tuned LLMs at scale are not discussed

## Confidence

- **High confidence**: Superiority of GPT-4o over traditional PLMs for multilingual vulnerability detection (well-supported by metrics)
- **Medium confidence**: Specific mechanisms of instruction tuning and few-shot prompting effectiveness (mechanism described but not extensively validated through ablation)
- **Low confidence**: Generalizability of results to real-world deployment scenarios and different vulnerability types not well-represented in REEF

## Next Checks

1. Test the best-performing approach (IT + Few-shot GPT-4o) on an independent vulnerability dataset to verify generalization beyond REEF

2. Measure operational costs of instruction tuning and few-shot inference at scale, comparing against accuracy improvements over traditional PLMs

3. Systematically evaluate model performance on each language individually to identify which languages benefit most from the multilingual approach and which may require specialized models