---
ver: rpa2
title: 'Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention'
arxiv_id: '2511.10268'
source_url: https://arxiv.org/abs/2511.10268
tags:
- object
- counterfactual
- spurious
- image
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Causal-HalBench, a causal analysis framework
  for evaluating object hallucination in Large Vision-Language Models (LVLMs). The
  core idea is to formalize spurious correlations using a Structural Causal Model
  (SCM) and introduce Visual Content Intervention (VCI) to quantify their impact.
---

# Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention

## Quick Facts
- **arXiv ID:** 2511.10268
- **Source URL:** https://arxiv.org/abs/2511.10268
- **Reference count:** 9
- **Primary result:** Causal-HalBench reveals mainstream LVLMs are susceptible to object hallucinations due to spurious correlations from co-occurrence bias, with metrics showing varying model performance across distinct causal mechanisms.

## Executive Summary
This paper introduces Causal-HalBench, a causal analysis framework designed to evaluate object hallucination in Large Vision-Language Models (LVLMs) by quantifying spurious correlations arising from co-occurrence bias. The authors formalize the problem using a Structural Causal Model (SCM) and introduce Visual Content Intervention (VCI) to systematically break confounding pathways. Through an automated pipeline generating over 10,000 counterfactual samples, the benchmark evaluates models using three causality-based metrics: Contextual object Accuracy Change (CAC), Absent object Accuracy Change (AAC), and Counterfactual object Hallucination Rate (CHR). Experiments reveal that mainstream LVLMs exhibit high susceptibility to spurious correlations, with performance varying significantly across metrics, highlighting the need for comprehensive evaluation beyond traditional accuracy measures.

## Method Summary
Causal-HalBench constructs a benchmark through a three-stage automated pipeline: Intervention Objects Selection (using Gemini to identify low co-occurrence replacement objects), Counterfactual Description Generation (via Gemini), and Counterfactual Inpainting (using SAM for masking and FLUX-controlnet for generation). The framework creates counterfactual images by replacing high co-occurrence objects with low co-occurrence alternatives, disrupting the confounding pathway that causes spurious correlations. Three causality-based metrics quantify different aspects of model behavior: CAC measures accuracy drops on contextual objects when context changes, AAC measures accuracy improvements on absent objects when co-occurrence cues are removed, and CHR measures direct hallucination rates on counterfactual objects. The benchmark includes 757 original MSCOCO validation images, 1,387 counterfactual images, and 9,709 binary QA pairs.

## Key Results
- LLaVA-1.5-7B achieved the lowest CAC at 1.8%, indicating minimal reliance on contextual spurious correlations.
- Kimi-VL-A3B showed the highest AAC at 10.2%, suggesting strong dependence on co-occurrence cues for absent object predictions.
- Qwen2.5-VL-7B demonstrated highest CHR at 8.8%, revealing significant hallucination rates on counterfactual objects despite moderate performance on other metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object hallucination in LVLMs arises from spurious correlations formed during training via a confounding pathway.
- Mechanism: The Structural Causal Model (SCM) positions Co-occurrence Bias (C) as a confounder that creates a backdoor path: X ← C → Y. The path C → X reflects that training images inherently contain biased co-occurrence patterns; the path C → Y reflects that the model uses these learned priors as a cognitive shortcut, bypassing direct visual analysis (X → Y).
- Core assumption: The proposed SCM accurately represents the causal structure of LVLM object recognition, and co-occurrence bias operates as described rather than through other pathways.
- Evidence anchors:
  - [abstract]: "We propose this primarily stems from spurious correlations arising when models strongly associate highly co-occurring objects during training"
  - [section 3.1]: "This confounding effect creates a backdoor path: X ← C → Y. Spurious correlations form through this path."
  - [corpus]: PAS paper confirms that "in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output," supporting shortcut mechanisms.
- Break condition: If models do not rely on co-occurrence shortcuts, or if the confounding structure differs from the proposed SCM.

### Mechanism 2
- Claim: Visual Content Intervention (VCI) isolates the causal effect of spurious correlations by breaking the confounding pathway.
- Mechanism: VCI constructs counterfactual images by replacing high co-occurrence objects with low co-occurrence objects, disrupting the C → X link. By comparing model outputs before and after intervention, the Average Causal Effect (ACE) quantifies how much predictions depend on spurious versus visual information.
- Core assumption: Inpainting-based counterfactuals adequately approximate the theoretical do(X=x_cf) intervention without introducing confounding artifacts.
- Evidence anchors:
  - [section 3.2]: "VCI systematically constructs a counterfactual image (x_cf) from an original image by introducing counterfactual visual content. This strategic modification of X disrupts the influence of C inherent in the original image."
  - [section 4.1]: "We acknowledge that using an inpainting model is an approximation of the theoretical do(X=x_cf) intervention."
  - [corpus]: Causal-LLaVA applies "causal disentanglement for mitigating hallucination," supporting intervention-based approaches.
- Break condition: If inpainting artifacts introduce systematic biases, or if counterfactual images fail to meaningfully alter the co-occurrence structure perceived by the model.

### Mechanism 3
- Claim: Spurious correlations manifest in distinct behavioral patterns requiring multiple causality-based metrics.
- Mechanism: Three metrics capture different effects: (1) CAC measures accuracy drop on contextual objects when context changes; (2) AAC measures accuracy improvement on absent objects when co-occurrence cues are removed; (3) CHR measures direct hallucination rate on counterfactual objects, reflecting visual perception quality.
- Core assumption: These three metrics capture distinct causal mechanisms rather than measurement noise or overlapping phenomena.
- Evidence anchors:
  - [section 5.1]: "The influence of spurious correlations on models is multifaceted... Kimi-VL-A3B demonstrates exceptional performance in CHR, it exhibits the poorest performance in AAC."
  - [section 4.3]: Formal definitions connect CAC to ACE, AAC to ACE on absent questions, and CHR to Direct Causal Strength.
  - [corpus]: Context-Aware Object Similarities paper evaluates "object hallucination based on context-aware object similarities," confirming contextual influence on hallucination.
- Break condition: If metrics are highly correlated or primarily reflect model capacity rather than distinct causal pathways.

## Foundational Learning

- Concept: **Structural Causal Models (SCMs) and Confounders**
  - Why needed here: The entire framework is built on SCM notation with C as confounder; understanding backdoor paths is essential to grasp why VCI works.
  - Quick check question: Explain why a confounder C that affects both X and Y creates spurious correlation between them.

- Concept: **Causal Intervention and do-Calculus**
  - Why needed here: VCI is designed to implement the do(X) operator; distinguishing intervention from observation is critical.
  - Quick check question: What does do(X=x) mean operationally, and how does it differ from simply conditioning on X=x?

- Concept: **Counterfactual Sample Generation**
  - Why needed here: The benchmark relies on synthetic counterfactual images to test model behavior under intervention.
  - Quick check question: Why can't we estimate causal effects using only naturally occurring images from the training distribution?

## Architecture Onboarding

- Component map: Original image -> Intervention Objects Selection -> Counterfactual Description Generation -> Counterfactual Inpainting -> QA pair construction -> Metric computation
- Critical path: Original image → (target, contextual, counterfactual) object triplet selection → description generation → inpainting → QA pair construction → metric computation across image pairs
- Design tradeoffs:
  - Inpainting vs. real counterfactual collection: Scalability vs. approximation error; paper acknowledges inpainting is an approximation
  - Automated vs. manual filtering: Uses Gemini for selection/generation plus manual quality filtering; balances scale with quality
  - CLIP-based quality validation: Assumes CLIP score correlates with semantic correctness of inpainted regions
- Failure signatures:
  - High CAC with low CHR: Model relies on context but has good visual perception
  - Low CAC with high CHR: Model ignores context but has poor visual grounding
  - Inconsistent CLIP scores (synthetic images score lower on target but not higher on counterfactual): indicates failed intervention
- First 3 experiments:
  1. Reproduce CAC/AAC/CHR on 2–3 models from Table 2 to validate metric computation pipeline.
  2. Ablate inpainting quality by substituting FLUX with a simpler inpainting model; measure metric sensitivity.
  3. Analyze per-object-class metric breakdown to identify which co-occurrence patterns drive highest CAC values.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do visual artifacts introduced by the counterfactual inpainting process confound the measured causal effects?
- **Basis in paper:** [explicit] The authors acknowledge in Section 4.1 that using an inpainting model is an approximation and that "potential visual artifacts... could act as interfering factors."
- **Why unresolved:** While the authors use CLIP scores and manual filtering to ensure quality, they do not quantitatively decouple the model's reaction to visual artifacts from its reaction to the broken spurious correlation.
- **What evidence would resolve it:** A comparative study evaluating model performance on automated counterfactuals versus "gold-standard" human-curated counterfactuals to measure the "artifact gap."

### Open Question 2
- **Question:** Can specific training interventions or data curation strategies effectively mitigate the spurious correlations identified by Causal-HalBench?
- **Basis in paper:** [inferred] The Conclusion identifies the detection of these correlations as a "key area for improving model faithfulness," and the analysis notes that newer models may paradoxically be more susceptible.
- **Why unresolved:** The paper focuses entirely on the evaluation and diagnosis of the problem; it does not propose or test specific debiasing algorithms or training modifications.
- **What evidence would resolve it:** Experiments applying causal debiasing techniques (e.g., counterfactual data augmentation) to LVLMs and measuring the resulting reduction in CAC and AAC scores.

### Open Question 3
- **Question:** Does the scale of training data inherently increase susceptibility to co-occurrence biases in LVLMs?
- **Basis in paper:** [inferred] In Section 5.1, the authors hypothesize that the high susceptibility in recent models like Qwen2.5-VL may be "attributed to the increased scale of training data."
- **Why unresolved:** This observation is based on a comparison of existing mainstream models and is post-hoc reasoning rather than a controlled experimental variable.
- **What evidence would resolve it:** A controlled ablation study training identical model architectures on datasets of varying sizes and co-occurrence frequencies to observe the trajectory of Causal-HalBench metrics.

## Limitations

- The framework relies on inpainting as an approximation of the theoretical do(X) intervention, which may introduce visual artifacts that confound causal effect measurements.
- The causal assumptions are based on a simple SCM structure that may not capture all pathways through which spurious correlations affect model behavior.
- The benchmark is constructed using automated Gemini-based counterfactual generation and MSCOCO images, which may not fully represent all co-occurrence patterns present in LVLM training data.

## Confidence

- **High Confidence:** The core framework design (SCM formulation, VCI methodology, and metric definitions) is internally consistent and methodologically sound. The benchmark construction pipeline is detailed and reproducible.
- **Medium Confidence:** The experimental results showing varying CAC/AAC/CHR values across models are likely reliable, though absolute values may be sensitive to inpainting quality and question selection. The qualitative insights about model-specific weaknesses appear valid.
- **Low Confidence:** The causal interpretation of metrics (e.g., claiming high CAC directly proves reliance on spurious correlations) requires stronger validation. The assumption that CLIP scores adequately validate inpainted regions may not hold for all object types.

## Next Checks

1. **Ablation on Inpainting Quality:** Substitute FLUX-controlnet with a simpler inpainting model and measure how CAC/AAC/CHR values change across the same LVLM models. This would quantify sensitivity to inpainting approximation error.

2. **Alternative Causal Structure Testing:** Design experiments where models are evaluated on images with manipulated co-occurrence patterns that break the assumed confounding structure (e.g., images with high co-occurrence objects but visual cues contradicting the correlation). This would test whether models follow the predicted backdoor path.

3. **Metric Correlation Analysis:** Compute pairwise correlations between CAC, AAC, and CHR across all object instances in the benchmark. If metrics are highly correlated (>0.7), this would suggest they capture overlapping phenomena rather than distinct causal mechanisms.