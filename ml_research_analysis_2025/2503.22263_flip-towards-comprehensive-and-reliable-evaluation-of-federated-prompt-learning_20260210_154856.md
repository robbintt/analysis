---
ver: rpa2
title: 'FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning'
arxiv_id: '2503.22263'
source_url: https://arxiv.org/abs/2503.22263
tags:
- learning
- prompt
- federated
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLIP, a comprehensive framework for evaluating
  federated prompt learning algorithms. The framework assesses 8 state-of-the-art
  methods across 4 federated protocols, 12 datasets, and 6 evaluation scenarios.
---

# FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning

## Quick Facts
- **arXiv ID:** 2503.22263
- **Source URL:** https://arxiv.org/abs/2503.22263
- **Reference count:** 40
- **Primary result:** Comprehensive evaluation framework for federated prompt learning methods across 12 datasets, 4 protocols, and 6 scenarios

## Executive Summary
FLIP introduces a comprehensive framework for evaluating federated prompt learning algorithms on vision-language models. The framework systematically assesses 8 state-of-the-art methods across 4 federated protocols, 12 diverse datasets, and 6 evaluation scenarios including global learning, personalized learning, and few-shot generalization. Key findings demonstrate that federated prompt learning maintains strong generalization in both in-distribution and out-of-distribution settings while consuming minimal computational resources. The framework reveals that regularization-based methods consistently improve global performance, distribution alignment enables effective personalization, and multiple-prompt approaches excel in few-shot scenarios. All code and datasets are open-sourced to facilitate further research in this emerging field.

## Method Summary
FLIP evaluates federated prompt learning by adapting CLIP models through soft prompt tuning across decentralized clients. The framework uses FedAvg as the baseline aggregation protocol, with 10 clients in full participation and 100 clients with 10% participation for partial scenarios. Training uses SGD with learning rate 0.002, momentum 0.9, cosine LR decay, batch size 16, and 50 global rounds with 1 local epoch. Non-IID data partitions follow Dirichlet distribution with α=0.1. The framework assesses both global accuracy (αg) and personalized accuracy (αp) across 12 datasets spanning fine-grained classification, cross-domain adaptation, and few-shot learning tasks. Method-specific components include regularization losses, optimal transport alignment, and meta-network conditional prompt generation.

## Key Results
- Regularization-based methods (f-SRC, f-KgCoOp) consistently improve global performance by mitigating client drift in non-IID settings
- FedOTP excels in personalized learning through distribution alignment, outperforming simple averaging approaches
- Multiple-prompt methods (FedOTP, f-ProDA, f-SRC) demonstrate superior few-shot generalization by reducing sample selection bias
- Communication cost scales linearly with prompt parameter count, creating a fundamental trade-off between accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Regularization Reduces Client Drift in Global Learning
- **Claim:** Regularization-based methods mitigate negative effects of non-IID data heterogeneity on the global model.
- **Mechanism:** Regularization enforces consistency between learned prompts and generic pretrained knowledge, constraining local updates and reducing client drift during aggregation.
- **Core assumption:** Local client drift is a primary degrader of global performance, and aligning local updates to a static reference acts as a functional anchor.
- **Evidence anchors:** Abstract states "regularization-based methods (f-SRC, f-KgCoOp) consistently improve global performance."
- **Break condition:** If local datasets are extremely homogeneous, the "drift" regularization prevents might not exist, potentially limiting gains or causing underfitting.

### Mechanism 2: Distribution Alignment Facilitates Personalization
- **Claim:** Aligning representations between global and local prompts allows better adaptation to specific client distributions than simple averaging.
- **Mechanism:** Methods like FedOTP use optimal transport to align local feature distributions with global ones, balancing shared knowledge and client-specific customization.
- **Core assumption:** Personalization requires preserving the geometric structure of local data manifolds, which weight averaging might distort.
- **Evidence anchors:** Abstract states "FedOTP excels in personalized learning through distribution alignment."
- **Break condition:** If divergence between client distributions is minimal (highly IID), computational overhead of distribution alignment may not yield proportional accuracy benefits.

### Mechanism 3: Multi-Prompt Ensembling Reduces Few-Shot Bias
- **Claim:** Utilizing multiple prompt sets or distributions improves few-shot generalization by reducing sample selection bias.
- **Mechanism:** In data-scarce regimes, single prompts may overfit to specific samples. Ensembling knowledge from multiple prompt vectors creates more robust decision boundaries.
- **Core assumption:** A diverse set of prompts captures broader semantic manifold than a single static prompt, compensating for limited training data.
- **Evidence anchors:** Abstract states "multiple-prompt methods (FedOTP, f-ProDA, f-SRC) show superior few-shot generalization."
- **Break condition:** As shot count increases (e.g., >16 shots), variance reduction from ensembling may diminish compared to increased communication cost.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) & CLIP**
  - **Why needed here:** Entire framework adapts pretrained VLMs (specifically CLIP) rather than training from scratch. Understanding joint embedding space of image and text encoders is prerequisite to understanding why only prompts are tuned.
  - **Quick check question:** How does the CLIP loss function align image and text embeddings, and what part of the architecture remains frozen during prompt tuning?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** FedAvg serves as baseline aggregation protocol for most methods evaluated. Understanding how it handles non-IID data is critical to diagnosing why regularization helps.
  - **Quick check question:** In FedAvg, how are local model updates aggregated on the server, and what theoretical assumption does it make about client data distributions?

- **Concept: Soft Prompts (Continuous Prompt Learning)**
  - **Why needed here:** This is the parameter-efficient fine-tuning technique being evaluated. Unlike discrete text prompts, these are learnable continuous vectors.
  - **Quick check question:** What are the specific tensor dimensions of a "soft prompt" in the transformer architecture, and how are they concatenated with the class embedding?

## Architecture Onboarding

- **Component map:** Frozen CLIP backbone (ResNet-50 image encoder + Transformer text encoder) -> Trainable soft prompt vectors -> Federated server (aggregates prompts) -> Federated clients (hold private non-IID datasets)

- **Critical path:**
  1. Initialize learnable prompt vectors (random or hand-craft anchored)
  2. Client step: Forward pass image/text pairs → Calculate contrastive loss → Backprop only to prompt vectors
  3. Server step: Aggregate prompt vectors (e.g., weighted average) → Distribute updated global prompt
  4. Evaluation: Zero-shot inference using aggregated prompt

- **Design tradeoffs:**
  - Communication vs. Accuracy: Increasing prompt length or number of prompts improves accuracy (to a point) but linearly increases communication overhead
  - Global vs. Personal: Aggregating to single global prompt reduces variance but may fail on specific local distributions; personalized methods improve local fit but require transmitting extra parameters

- **Failure signatures:**
  - Feature Aggregation Collapse: Methods aggregating image features can degrade under severe non-IID data because biased local features skew prompt generation
  - Over-regularization: If regularization is too strong, it may force model to mimic zero-shot CLIP too closely, negating benefits of local training data

- **First 3 experiments:**
  1. Baseline Establishment: Run PromptFL (CoOp + FedAvg) on Caltech-101 (IID vs Non-IID) to establish communication cost and accuracy baseline
  2. Regularization Check: Implement f-SRC on same Non-IID split to measure accuracy gain from regularizing against frozen CLIP features
  3. Scalability Stress Test: Run FedOTP vs PromptFL on 100 clients with 10% participation to observe if distribution alignment mitigates "catastrophic forgetting" in partial participation scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the "intrinsic dilemma" between optimizing for global consensus and local personalization be resolved without compromising either performance metric?
- **Basis in paper:** Authors state in Personalized Learning results that "achieving improvements on global and personalized performance could compromise each other and requires further exploration."
- **Why unresolved:** While FedOTP balances these goals via distribution alignment, results show regularization methods successful for global tasks are less prominent in personalized settings, indicating fundamental tension.
- **What evidence would resolve it:** A novel algorithm achieving state-of-the-art results on both global (αg) and personalized (αp) metrics simultaneously, breaking current trade-off.

### Open Question 2
- **Question:** How can conditional prompt learning modules be redesigned to withstand feature aggregation bias caused by non-IID data?
- **Basis in paper:** Paper hypothesizes f-CoCoOp's inferior performance is caused by its meta-network being "susceptible to data heterogeneity raised by non-i.i.d. data partitions," creating risks for direct porting.
- **Why unresolved:** Current conditional modules aggregate image features that may be biased towards local client distributions, causing generated prompts to deviate from real class semantics when data is heterogeneous.
- **What evidence would resolve it:** An image-conditional prompt learning method consistently outperforming static baselines in non-IID settings by implementing invariant feature aggregation.

### Open Question 3
- **Question:** How can federated prompt learning enhance robustness against severe domain shifts, such as adversarial examples or sketches, where current methods fail?
- **Basis in paper:** In Appendix C.3, authors note that accuracy on ImageNet-A and ImageNet-S is low, "necessitates future research endeavors for improving generalization ability and robustness" of these methods.
- **Why unresolved:** Study finds that even with federated tuning, performance drops significantly on these domains, suggesting that simply tuning prompts on standard data is insufficient for severe distributional shifts.
- **What evidence would resolve it:** A method significantly increasing accuracy on ImageNet-Sketch and ImageNet-Adversarial (e.g., >10% gain) without requiring full model fine-tuning or excessive communication overhead.

## Limitations
- Method-specific hyperparameter sensitivity is not thoroughly analyzed, with critical parameters like regularization weights and OT coefficients remaining dataset-dependent
- Evaluation scope is limited to CLIP-based VLMs, restricting generalizability to other foundation models and modalities
- Implementation fidelity barriers exist due to incomplete method implementations in the open-sourced framework

## Confidence
- **High Confidence:** Global accuracy trends showing regularization-based methods consistently outperforming baselines across non-IID scenarios
- **Medium Confidence:** Claims about personalized learning performance (FedOTP superiority) and few-shot generalization benefits are supported but limited to specific dataset subsets
- **Low Confidence:** Framework's applicability to extremely large-scale federated settings (1000+ clients, high data heterogeneity) remains untested

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary regularization weights (f-SRC), OT coefficients (FedOTP), and aggregation temperatures across all 12 datasets to identify stable configurations and failure modes
2. **Cross-model generalization test:** Evaluate FLIP methods using ViT-B/16 CLIP backbone and alternative VLMs (e.g., OpenCLIP) to assess framework robustness beyond default ResNet-50 configuration
3. **Extreme-scale federated scenario:** Deploy FLIP methods on 100+ clients with 1% participation rates and Dirichlet α=0.01 to stress-test communication efficiency and convergence stability under realistic production constraints