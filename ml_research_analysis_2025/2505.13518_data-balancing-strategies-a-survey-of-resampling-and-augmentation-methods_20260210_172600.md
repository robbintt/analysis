---
ver: rpa2
title: 'Data Balancing Strategies: A Survey of Resampling and Augmentation Methods'
arxiv_id: '2505.13518'
source_url: https://arxiv.org/abs/2505.13518
tags:
- class
- data
- samples
- minority
- majority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews resampling and augmentation
  methods for addressing imbalanced data in machine learning. It categorizes techniques
  into synthetic oversampling (e.g., SMOTE variants, ADASYN), adaptive methods (e.g.,
  MWMOTE, AMDO), generative models (e.g., GANs, VAEs), ensemble strategies (e.g.,
  RUSBoost, SMOTEBoost), and undersampling approaches (e.g., NearMiss, Tomek Links).
---

# Data Balancing Strategies: A Survey of Resampling and Augmentation Methods

## Quick Facts
- arXiv ID: 2505.13518
- Source URL: https://arxiv.org/abs/2505.13518
- Reference count: 23
- This survey comprehensively reviews resampling and augmentation methods for addressing imbalanced data in machine learning.

## Executive Summary
This paper provides a comprehensive survey of resampling and augmentation techniques for handling imbalanced data in machine learning. The authors systematically categorize methods into synthetic oversampling, adaptive methods, generative models, ensemble strategies, and undersampling approaches. Through extensive literature review, they evaluate these techniques using standard metrics like accuracy, ROC AUC, and F1-score across benchmark datasets. The survey reveals that while no universal solution exists, hybrid approaches combining multiple techniques often demonstrate superior performance compared to single-method implementations.

## Method Summary
The survey methodology involves systematic literature review across major machine learning venues, categorizing existing data balancing techniques into coherent frameworks. The authors examine 23 key references covering synthetic oversampling methods like SMOTE variants and ADASYN, adaptive approaches such as MWMOTE and AMDO, generative models including GANs and VAEs, ensemble strategies like RUSBoost and SMOTEBoost, and undersampling techniques including NearMiss and Tomek Links. Each method is evaluated based on its theoretical foundation, implementation characteristics, and empirical performance across various benchmark datasets with different imbalance ratios.

## Key Results
- Synthetic oversampling methods (SMOTE variants, ADASYN) generate synthetic minority class samples to balance class distributions
- Hybrid approaches combining multiple techniques typically outperform single-method implementations
- Effectiveness of balancing strategies varies significantly based on dataset characteristics including size, feature types, and noise levels

## Why This Works (Mechanism)
Data balancing methods address the fundamental challenge that standard machine learning algorithms tend to favor majority classes when training data is imbalanced. By modifying the training data distribution through oversampling minority classes, undersampling majority classes, or generating synthetic examples, these techniques help classifiers learn more balanced decision boundaries. The mechanism involves either directly altering the training set composition or modifying the learning algorithm's loss function to account for class imbalance, thereby improving minority class recognition without sacrificing majority class performance.

## Foundational Learning
- Imbalanced learning concepts - Understanding class distribution skew and its impact on model performance; why needed for grasping the problem context; quick check: calculate class imbalance ratio in sample dataset
- Synthetic data generation - Methods for creating realistic synthetic examples; why needed for understanding oversampling techniques; quick check: implement basic SMOTE algorithm
- Ensemble learning principles - Combining multiple models for improved performance; why needed for understanding boosting-based balancing methods; quick check: implement AdaBoost on balanced dataset
- Performance metrics for imbalanced data - Precision, recall, F1-score, ROC AUC; why needed for proper evaluation; quick check: compute confusion matrix and derived metrics

## Architecture Onboarding

**Component Map:** Data Preprocessing -> Balancing Method Selection -> Model Training -> Performance Evaluation

**Critical Path:** Feature extraction and preprocessing must complete before any balancing method can be applied, which must complete before model training, which must complete before evaluation

**Design Tradeoffs:** Synthetic oversampling increases training data size but risks overfitting; undersampling reduces computational cost but loses information; generative models produce diverse samples but require careful hyperparameter tuning

**Failure Signatures:** Overfitting to synthetic samples manifests as poor generalization; aggressive undersampling leads to information loss and degraded overall accuracy; improper parameter settings in balancing algorithms result in ineffective or counterproductive transformations

**First Experiments:**
1. Compare SMOTE vs random oversampling on a binary imbalanced dataset using logistic regression
2. Evaluate Tomek Links cleaning before and after SMOTE application on multi-class imbalanced data
3. Test ensemble method (RUSBoost) versus single classifier on highly imbalanced dataset with varying imbalance ratios

## Open Questions the Paper Calls Out
- How can adaptive, data-aware resampling methods be developed that automatically adjust to dataset characteristics?
- What are the optimal ways to integrate balancing techniques with end-to-end deep learning pipelines?
- How can balancing strategies be made more robust to high-dimensional and noisy real-world data streams?

## Limitations
- Benchmark datasets may not fully capture real-world complexities such as high-dimensional features or evolving data distributions
- Effectiveness assessments lack unified experimental validation across diverse domains
- Limited exploration of performance when balancing methods are tightly coupled with deep learning architectures

## Confidence

**High:** Categorization of methods into synthetic oversampling, adaptive methods, generative models, ensemble strategies, and undersampling approaches

**Medium:** Effectiveness of hybrid approaches over single techniques, as this is based on literature trends rather than unified experiments

**Low:** Generalizability of findings to high-dimensional or noisy real-world datasets, due to reliance on benchmark datasets

## Next Checks

1. Conduct experiments comparing hybrid and single techniques across diverse real-world datasets with varying noise levels and feature dimensions
2. Evaluate the performance of surveyed methods when integrated with deep learning architectures in end-to-end learning pipelines
3. Test the robustness of resampling and augmentation methods on imbalanced datasets with class imbalance ratios beyond typical benchmark ranges