---
ver: rpa2
title: 'f-INE: A Hypothesis Testing Framework for Estimating Influence under Training
  Randomness'
arxiv_id: '2510.10510'
source_url: https://arxiv.org/abs/2510.10510
tags:
- influence
- training
- data
- f-ine
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of influence estimation methods
  under training randomness, where the same example may appear critical in one run
  and irrelevant in the next. The authors introduce f-influence, a new influence estimation
  framework grounded in hypothesis testing that explicitly accounts for training randomness.
---

# f-INE: A Hypothesis Testing Framework for Estimating Influence under Training Randomness

## Quick Facts
- arXiv ID: 2510.10510
- Source URL: https://arxiv.org/abs/2510.10510
- Reference count: 40
- Primary result: Introduces f-INE framework for stable influence estimation under training randomness, successfully applied to detect poisoned samples in Llama-3.1-8B

## Executive Summary
This paper addresses a fundamental challenge in influence estimation: instability under training randomness. Traditional influence estimation methods can yield drastically different results across training runs, making it difficult to reliably identify which training examples are truly influential. The authors introduce f-influence, a new framework grounded in hypothesis testing that explicitly accounts for this randomness, establishing desirable properties like compositionality and asymptotic normality. They also develop f-INE, an efficient algorithm that computes f-influence in a single training run.

## Method Summary
The authors develop f-influence as a hypothesis testing framework that models the influence of individual training examples while accounting for training randomness. Unlike previous methods that provide point estimates of influence, f-influence provides statistical confidence measures. The f-INE algorithm efficiently computes these estimates by leveraging the statistical properties of the training process. The framework is designed to be compositionally stable - meaning influence estimates can be combined reliably - and exhibits asymptotic normality, making statistical inference possible. The method was validated on Llama-3.1-8B to detect poisoned samples in instruction tuning data.

## Key Results
- Successfully scales influence estimation to large models (Llama-3.1-8B) while maintaining computational efficiency
- Demonstrates ability to reliably detect poisoned samples that steer model opinions in instruction tuning
- Shows f-influence provides stable estimates across training runs where traditional methods fail

## Why This Works (Mechanism)
The core mechanism relies on reframing influence estimation as a hypothesis testing problem rather than a point estimation problem. By explicitly modeling the randomness inherent in stochastic optimization, f-influence provides statistical confidence intervals around influence estimates rather than single values. This allows for more robust decision-making about which examples are truly influential. The framework leverages asymptotic normality to enable efficient computation through f-INE, making it practical for large-scale models.

## Foundational Learning
- Influence functions: Used to approximate how training points affect model predictions
  - Why needed: Traditional method for understanding training data impact
  - Quick check: Can estimate test loss change for removing a point
- Training randomness: Variability in optimization due to stochastic gradient descent
  - Why needed: Explains why influence estimates vary across runs
  - Quick check: Same model trained twice yields different influences
- Hypothesis testing framework: Statistical approach to inference
  - Why needed: Provides principled way to handle uncertainty
  - Quick check: Generates p-values and confidence intervals
- Compositionality: Property allowing combination of estimates
  - Why needed: Enables scalable analysis across datasets
  - Quick check: Influence of groups = sum of individual influences
- Asymptotic normality: Statistical property of estimators
  - Why needed: Enables efficient computation via f-INE
  - Quick check: Distribution approaches normal as sample size increases

## Architecture Onboarding
Component map: Data points -> f-influence computation -> Statistical inference -> Influence ranking

Critical path: Training run → f-INE computation → Statistical testing → Influence identification

Design tradeoffs: Single-run efficiency vs. multi-run averaging; statistical rigor vs. computational cost

Failure signatures: Non-normal influence distributions; high variance across runs; poor detection of known poisoned samples

First experiments:
1. Compare f-INE influence estimates across multiple random seeds
2. Validate detection of synthetically inserted poisoned samples
3. Test scalability by applying to increasingly large model architectures

## Open Questions the Paper Calls Out
The paper acknowledges that while f-INE provides a more stable framework for influence estimation, questions remain about optimal hyperparameter settings for the hypothesis testing framework and how to best interpret statistical significance in the context of influence estimation. The authors also note that extending the framework to handle more complex training scenarios (like federated learning) remains an open direction.

## Limitations
- The method requires careful tuning of statistical parameters
- Computational overhead compared to simpler influence estimation methods
- May not fully capture complex interactions in highly non-convex loss landscapes
- Limited validation to specific types of poisoning attacks

## Confidence
High: Theoretical properties established (compositionality, asymptotic normality)
Medium: Practical effectiveness demonstrated on Llama-3.1-8B
Low: Generalizability to all training scenarios and model types

## Next Checks
1. Test f-INE across multiple training runs with different random seeds to verify its stability claims empirically
2. Apply f-INE to detect poisoned samples in diverse datasets beyond the instruction tuning scenario to assess generalizability
3. Compare f-INE's influence estimates against ground truth influence (when available through controlled experiments) to validate accuracy claims