---
ver: rpa2
title: 'FIRE: Faithful Interpretable Recommendation Explanations'
arxiv_id: '2508.05225'
source_url: https://arxiv.org/abs/2508.05225
tags:
- user
- explanations
- explanation
- features
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating faithful and interpretable
  explanations in recommender systems, where many existing methods rely on generating
  explanations that mimic user reviews but fail to reflect the true decision logic
  of the recommendation model. To overcome this, the authors propose FIRE, a lightweight
  framework that combines SHAP-based feature attribution with structured, prompt-driven
  language generation.
---

# FIRE: Faithful Interpretable Recommendation Explanations

## Quick Facts
- arXiv ID: 2508.05225
- Source URL: https://arxiv.org/abs/2508.05225
- Reference count: 40
- Primary result: Achieves F1 scores up to 0.65 while significantly outperforming baselines in explanation quality metrics

## Executive Summary
This paper addresses the problem of generating faithful and interpretable explanations in recommender systems, where many existing methods rely on generating explanations that mimic user reviews but fail to reflect the true decision logic of the recommendation model. To overcome this, the authors propose FIRE, a lightweight framework that combines SHAP-based feature attribution with structured, prompt-driven language generation. FIRE identifies the most influential user and item features from the model's decision-making process and generates explanations that explicitly link these features to the predicted sentiment. Experiments on TripAdvisor, Amazon, and Yelp datasets show that FIRE achieves competitive recommendation accuracy and significantly outperforms baselines in explanation quality, including better alignment with model predictions, stronger structural coherence, and near-human diversity.

## Method Summary
FIRE is a lightweight framework that addresses the gap between explanation faithfulness and interpretability in recommender systems. The method combines SHAP-based feature attribution with structured, prompt-driven language generation. First, it uses SHAP to identify the most influential user and item features that contribute to a recommendation decision. Then, it generates explanations through a structured prompt template that explicitly links these features to the predicted sentiment. This approach avoids the common pitfall of generating explanations that merely mimic user reviews, instead focusing on explaining the actual decision logic of the recommendation model. The framework is designed to be model-agnostic and can be applied to various recommendation scenarios where feature importance can be meaningfully interpreted.

## Key Results
- Achieves F1 scores up to 0.65 on recommendation accuracy
- Significantly outperforms baselines in explanation alignment scores (0.56 vs 0.31)
- Demonstrates near-human diversity in generated explanations (USR scores up to 0.99)
- Shows superior structural coherence compared to review-mimicry approaches

## Why This Works (Mechanism)
FIRE works by bridging the gap between model interpretability and explanation faithfulness through a two-stage process. The SHAP-based feature attribution provides a mathematically grounded way to identify which user and item features most strongly influence a recommendation decision, ensuring that explanations are faithful to the model's actual reasoning. The structured prompt generation then transforms these feature attributions into coherent natural language explanations that maintain the logical connections between features and predictions. This approach avoids the common pitfall of generating explanations that sound plausible but don't reflect the model's true decision process, which is particularly problematic in review-mimicry methods.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: Why needed - Provides mathematically grounded feature attribution that ensures explanations reflect the model's true decision logic. Quick check - Verify SHAP values sum to the model's prediction margin for each instance.
- **Feature attribution in recommender systems**: Why needed - Enables identification of which user/item features drive recommendations, crucial for faithful explanations. Quick check - Confirm that top features identified by SHAP align with domain knowledge.
- **Prompt-driven language generation**: Why needed - Structures explanations to explicitly link features to predictions while maintaining natural language coherence. Quick check - Ensure generated explanations follow the template structure while varying content appropriately.
- **Explanation faithfulness vs. plausibility**: Why needed - Distinguishes between explanations that sound good versus those that actually reflect model reasoning. Quick check - Compare explanation metrics that measure faithfulness (alignment with predictions) versus plausibility (human-likeness).
- **Structural coherence in generated text**: Why needed - Ensures explanations are logically organized and easy to understand. Quick check - Evaluate coherence scores and verify logical flow between feature descriptions and sentiment predictions.
- **Recommendation accuracy metrics (F1, etc.)**: Why needed - Validates that explanation generation doesn't compromise core recommendation performance. Quick check - Compare recommendation performance between FIRE and baseline models.

## Architecture Onboarding

Component map: User/Item Features -> SHAP Attribution -> Feature Selection -> Prompt Template -> Explanation Generation -> Output

Critical path: The most critical path runs from SHAP attribution through feature selection to prompt generation. This is where the faithfulness of explanations is determined - if SHAP attribution fails to identify truly influential features, or if the prompt template fails to properly structure these features into coherent explanations, the entire framework's effectiveness is compromised.

Design tradeoffs: The authors chose a lightweight approach combining existing SHAP methods with structured prompt generation rather than developing a more complex end-to-end model. This tradeoff prioritizes interpretability and faithfulness over potentially higher performance that might come from more sophisticated generation techniques. The structured template approach sacrifices some natural language fluency for guaranteed alignment between features and explanations.

Failure signatures: Common failure modes include: (1) SHAP attribution identifying noisy or irrelevant features, leading to misleading explanations; (2) prompt templates producing repetitive or overly structured explanations that lack diversity; (3) feature importance scores that don't generalize well across different user/item types; (4) explanations that become too technical or feature-focused, losing interpretability for end users.

First experiments:
1. Verify SHAP attribution correctly identifies known influential features on a small, interpretable dataset
2. Test prompt template generation with hand-crafted feature sets to validate explanation structure
3. Compare recommendation accuracy with and without FIRE explanation generation to ensure no performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology lacks human evaluation studies beyond automated metrics, raising questions about real-world effectiveness and user comprehension
- Claims of "near-human diversity" (USR scores up to 0.99) are concerning without direct human comparison studies
- Approach's generalizability to diverse recommendation domains and non-textual features remains uncertain
- Reliance on structured prompt templates may not translate well to all recommendation scenarios

## Confidence

| Claim | Confidence |
|-------|------------|
| Technical implementation of SHAP-based feature attribution and prompt generation framework | High |
| Claims about improved structural coherence and alignment scores compared to baselines | Medium |
| Claims about explanation quality being "near-human" and comprehensive real-world effectiveness | Low |

## Next Checks

1. Conduct comprehensive user studies comparing FIRE explanations against human-written explanations and other baseline methods, measuring comprehension, trust, and decision-making impact

2. Test FIRE's performance across diverse recommendation domains beyond the three benchmark datasets, particularly for non-textual features and cold-start scenarios

3. Evaluate FIRE's robustness to adversarial inputs and distribution shifts to assess the faithfulness of explanations under real-world conditions