---
ver: rpa2
title: Comparing energy consumption and accuracy in text classification inference
arxiv_id: '2508.14170'
source_url: https://arxiv.org/abs/2508.14170
tags:
- energy
- consumption
- https
- inference
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares energy consumption and accuracy in text classification
  inference across different model architectures and hardware configurations. The
  authors evaluated traditional machine learning models (linear models, XGBoost) and
  large language models (LLMs) on a text classification task, measuring both accuracy
  and energy usage during inference.
---

# Comparing energy consumption and accuracy in text classification inference

## Quick Facts
- **arXiv ID:** 2508.14170
- **Source URL:** https://arxiv.org/abs/2508.14170
- **Reference count:** 14
- **Primary result:** Energy-efficient models achieved best accuracy while larger LLMs consumed significantly more energy with lower accuracy

## Executive Summary
This study provides a comprehensive comparison of energy consumption and accuracy in text classification inference across different model architectures and hardware configurations. The authors evaluated traditional machine learning models (linear models, XGBoost) alongside large language models (LLMs) on a text classification task, measuring both accuracy metrics and energy usage during inference. Their findings reveal that the best-performing model was also energy-efficient, while larger LLMs consumed significantly more energy with lower classification accuracy. Energy consumption varied by six orders of magnitude, influenced by model type, size, and hardware choices. A strong correlation was observed between inference energy consumption and runtime, suggesting that execution time can serve as a practical proxy for energy usage when direct measurement is not feasible.

## Method Summary
The authors conducted controlled experiments comparing traditional machine learning models and large language models on text classification tasks. They measured both accuracy metrics and energy consumption during inference across different hardware configurations. The study systematically evaluated how model architecture, size, and hardware choices affect both performance and energy efficiency. Energy consumption was measured directly, and runtime was recorded to establish correlation patterns between execution time and energy usage.

## Key Results
- Best-performing model also achieved highest energy efficiency
- Larger LLMs consumed significantly more energy while delivering lower classification accuracy
- Energy consumption varied by six orders of magnitude across different configurations
- Strong correlation found between inference energy consumption and runtime

## Why This Works (Mechanism)
The energy efficiency of traditional machine learning models likely stems from their simpler computational architectures and smaller parameter counts compared to LLMs. Linear models and tree-based methods like XGBoost require fewer floating-point operations and less memory bandwidth during inference. The strong correlation between runtime and energy consumption suggests that models with lower computational complexity naturally consume less energy, as energy usage is fundamentally tied to the number and type of operations performed.

## Foundational Learning
This study reinforces the principle that model size and complexity do not necessarily correlate with better performance in all tasks. For text classification specifically, simpler models can achieve both superior accuracy and energy efficiency compared to larger language models. The findings support the broader machine learning community's growing recognition that smaller, task-specific models are often more appropriate than general-purpose LLMs for many applications.

## Architecture Onboarding
The research demonstrates that when deploying text classification systems, practitioners should consider the trade-off between model complexity and energy efficiency. Organizations can optimize their deployments by selecting appropriate model architectures based on task requirements rather than defaulting to the largest available models. The strong runtime-energy correlation provides a practical guideline: monitoring inference latency can serve as a proxy for energy consumption, enabling easier optimization without requiring specialized energy measurement tools.

## Open Questions the Paper Calls Out
None provided

## Limitations
- Limited to single text classification task, reducing generalizability across different NLP applications
- Hardware configuration testing was limited to specific combinations, potentially missing optimization opportunities
- Only inference phase energy consumption measured, not accounting for model training energy costs
- Runtime as proxy for energy consumption may vary across different deployment scenarios

## Confidence
- Major claim clusters rated as Medium-High due to controlled experimental setup
- Single task focus introduces uncertainty about broader applicability
- Limited hardware configurations constrain generalizability
- Inference-only measurements omit full lifecycle energy costs

## Next Checks
1. Replicate experiments across multiple text classification tasks with varying dataset sizes, class distributions, and complexity to assess generalizability
2. Expand hardware configuration testing to include additional CPU architectures, GPU types, and potential accelerator devices to identify broader optimization patterns
3. Conduct longitudinal measurements to evaluate how energy consumption and accuracy trade-offs evolve with model updates, hardware improvements, and changing inference patterns