---
ver: rpa2
title: 'FEval-TTC: Fair Evaluation Protocol for Test-Time Compute'
arxiv_id: '2511.01203'
source_url: https://arxiv.org/abs/2511.01203
tags:
- cost
- feval-ttc
- reasoning
- methods
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEval-TTC, a fair evaluation protocol for
  test-time compute methods in large language models. The core challenge addressed
  is the inconsistency in LLM performance and API costs over time, which can invalidate
  prior research conclusions.
---

# FEval-TTC: Fair Evaluation Protocol for Test-Time Compute

## Quick Facts
- **arXiv ID**: 2511.01203
- **Source URL**: https://arxiv.org/abs/2511.01203
- **Reference count**: 12
- **Primary result**: Pre-recorded LLM responses and unified cost model enable reproducible, cost-efficient evaluation of test-time compute methods without live API calls

## Executive Summary
FEval-TTC addresses the challenge of inconsistent LLM performance and API costs over time, which can invalidate prior research conclusions about test-time compute methods. The protocol provides pre-recorded LLM responses and a unified cost model, enabling researchers to evaluate methods like Self-Consistency, Best-of-N, Mixture of Thoughts, and ModelSwitch without live API calls. By standardizing few-shot prompting and answer extraction across 16 datasets covering commonsense, arithmetic, and mathematical reasoning tasks, FEval-TTC significantly reduces evaluation time and cost while maintaining reproducibility. The open-source package supports multiple LLM families including LLaMA, QWEN, Deepseek, Mistral, and GPT models.

## Method Summary
FEval-TTC implements a fair evaluation protocol for test-time compute methods by pre-recording 40 LLM responses per question (3 for expensive models like o3-mini) using standardized few-shot CoT prompts. The protocol stores these responses with metadata including tokens, extracted answers, and dollar costs in a unified cost model: DollarCost = 10⁻⁶(Ci × Token(INP) + Co × Token(OUT)). Researchers access cached responses instead of making live API calls, enabling reproducible comparisons across different TTC methods and LLM families. The package provides a Dataset module with standardized prompts and answer formats, and an LLM module with pre-recorded responses for multiple model families including LLaMA, QWEN, Deepseek, Mistral, and GPT models.

## Key Results
- Reduces evaluation time from hours to seconds by eliminating live API calls
- Enables reproducible comparisons through frozen cost models and pre-recorded responses
- Supports multiple LLM families and 16 datasets across commonsense, arithmetic, and mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Pre-recorded Responses
- Claim: Pre-recorded LLM responses eliminate evaluation variability from API fluctuations and latency
- Mechanism: Each question is pre-queried 40 times per model using standardized few-shot CoT prompts. Responses are cached with metadata (tokens, extracted answers, dollar cost). Researchers retrieve cached responses instead of making live API calls.
- Core assumption: The quality and distribution of pre-recorded CoTs sufficiently represent what live queries would produce
- Evidence anchors: [abstract]: "providing pre-recorded LLM responses... enabling researchers to evaluate test-time compute methods without live API calls"; [section 2.2]: "Each model is queried 40 times using a few-shot CoT prompt with standard few-shot examples"
- Break condition: If a new TTC method requires prompting strategies not captured in pre-recorded responses (e.g., different few-shot examples, multi-turn interactions), cached data becomes insufficient

### Mechanism 2: Unified Cost Model
- Claim: A unified cost model enables fair cross-method comparison independent of API pricing changes
- Mechanism: Dollar cost computed via fixed formula: `DollarCost = 10⁻⁶(Ci × Token(INP) + Co × Token(OUT))` where Ci/Co are fixed USD-per-million-token rates. Costs are frozen at a specific date (June 2, 2025) to prevent temporal drift
- Core assumption: Linear token-based pricing adequately represents true inference costs for comparison purposes
- Evidence anchors: [abstract]: "unified cost model ensures fair comparisons by estimating token and dollar costs per query"; [section 2.3]: "We adopt this simplified cost model to enable fair comparisons... independent of external factors such as the query date"
- Break condition: If methods have fundamentally different token efficiency profiles not captured by linear pricing (e.g., parallel vs sequential inference, KV-cache reuse), comparisons may mislead

### Mechanism 3: Standardized Prompting and Extraction
- Claim: Standardized prompting and answer extraction reduce experimenter variance across datasets
- Mechanism: Dataset module provides system prompts per dataset; LLM module stores extracted answers in standardized format. Answer extraction failures are marked as `None` rather than handled inconsistently
- Core assumption: Standardized extraction doesn't systematically disadvantage certain model families or reasoning styles
- Evidence anchors: [abstract]: "standardized few-shot prompting and answer extraction across 16 datasets"; [section 2.1]: "the answer format was standardized across the package"
- Break condition: If answer formats are ambiguous or models produce valid answers in unexpected formats, standardization could undercount correct responses

## Foundational Learning

- **Test-Time Compute (TTC)**: Why needed here: FEval-TTC evaluates TTC methods (Self-Consistency, Best-of-N, etc.) that allocate extra computation during inference rather than training. Quick check question: Can you explain why Self-Consistency requires 20 CoT samples while Best-of-N might need different evaluation?

- **Chain-of-Thought (CoT) Prompting**: Why needed here: All pre-recorded responses use CoT format; understanding how CoT affects token counts and answer extraction is essential. Quick check question: What happens to evaluation if a model produces a valid final answer but fails to show CoT reasoning?

- **API Cost Structures**: Why needed here: The unified cost model assumes input/output token pricing differential; understanding this helps interpret cost-accuracy tradeoffs. Quick check question: Why does equation (1) separate Ci and Co instead of using a single token price?

## Architecture Onboarding

- **Component map**:
  ```
  FEval-TTC Package
  ├── Dataset Module (questions, answers, system_prompt)
  │   └── List[DatasetEntry] with standardized answer format
  └── LLM Module (pre-recorded responses)
      ├── LLMConfig (name, temperature, max_tokens)
      └── List[LLMResponse]
          ├── LLMRequest (few-shot prompt, token count, dollar_cost)
          └── List[CoT] (raw_text, extracted answer, metadata)
  ```

- **Critical path**: `load(DatasetType, [LLMType])` → iterate dataset entries → call `llm(question_id, N=samples)` → access `.cots[i].answer` and `.cots[i].dollar_cost` → apply TTC algorithm logic to cached responses

- **Design tradeoffs**:
  - Pre-recording enables speed (hours → seconds) but limits flexibility in prompting
  - 40 samples per question balances coverage vs storage; may be insufficient for some TTC methods
  - Fixed cost model ensures reproducibility but may diverge from actual API costs over time

- **Failure signatures**:
  - `answer` field is `None` → extraction failed; check raw_text manually
  - Fewer than N CoTs returned → model hit max_tokens or API limit during pre-recording
  - Cost estimates seem wrong → verify you're using the frozen cost table (Appendix A), not current API prices

- **First 3 experiments**:
  1. Reproduce Self-Consistency accuracy curve on GSM8K with Qwen-32B (vary N from 1-40) to validate package installation and understand cost-accuracy tradeoff
  2. Compare Best-of-N vs Self-Consistency on AQuA dataset using Mixtral-8x22B to verify your TTC algorithm implementation matches Table 1 results
  3. Add a new dataset by following Dataset module schema, generate pre-recorded responses with your target model, and verify answer extraction works on your data format before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the fixed upper bound of 40 pre-recorded responses per question limit the accurate evaluation of compute-optimal scaling strategies that may theoretically require significantly larger sample sizes?
- **Basis in paper**: [inferred] Section 2.2 states that each model is queried only 40 times (and only 3 times for the expensive o3-mini), creating a hard cap on the available search space for any TTC method
- **Why unresolved**: Recent research on test-time compute scaling suggests performance can improve with thousands of samples; the protocol's fixed cache may truncate the evaluation of such scaling laws
- **What evidence would resolve it**: A comparison of performance saturation curves between FEval-TTC (capped at N=40) and live evaluations with N>100 to determine if method rankings change at higher compute levels

### Open Question 2
- **Question**: Can the protocol effectively evaluate TTC methods that rely on dynamic prompt refinement or multi-turn agentic interactions, given its reliance on static, standardized few-shot prompts?
- **Basis in paper**: [inferred] The Dataset module (Section 2.1) enforces a standardized system prompt, and the LLM module (Section 2.2) caches responses based on these fixed inputs
- **Why unresolved**: Methods like Tree of Thoughts or debate approaches often generate new, dependent prompts on the fly, which may not match the pre-recorded responses available in the static cache
- **What evidence would resolve it**: Demonstrating the successful integration of a multi-turn TTC algorithm (e.g., a debate agent) within the current FEval-TTC framework without requiring live API calls for intermediate steps

### Open Question 3
- **Question**: How sensitive are the relative cost-efficiency rankings of different TTC methods to the unified pricing coefficients ($C_i$ and $C_o$) fixed by the protocol?
- **Basis in paper**: [inferred] Appendix A acknowledges that the cost model assumes fixed prices (as of June 2025) to ensure consistency, ignoring real-world volatility and provider-specific discount structures
- **Why unresolved**: A method that is cost-efficient under the paper's specific pricing table might rank poorly if input token costs rise or output token costs drop relative to the fixed assumptions
- **What evidence would resolve it**: A sensitivity analysis showing the correlation of method rankings when $C_i$ and $C_o$ parameters are perturbed to simulate different market pricing scenarios

## Limitations

- The protocol's fixed 40-sample cap may not support TTC methods requiring larger sample sizes for optimal performance
- The simplified linear token-based cost model may not capture true cost dynamics for parallel vs sequential execution patterns
- Pre-recorded responses limit flexibility for TTC methods requiring dynamic prompt refinement or multi-turn interactions

## Confidence

**High Confidence Claims:**
- The protocol reduces evaluation time from hours to seconds by eliminating live API calls
- The framework is open-source and supports multiple LLM families and datasets
- The package enables reproducible comparisons through frozen cost models and pre-recorded responses

**Medium Confidence Claims:**
- Pre-recorded responses sufficiently represent live API behavior for TTC evaluation
- Standardized prompting and extraction don't systematically bias results across model families
- The simplified cost model enables fair cross-method comparisons

**Low Confidence Claims:**
- The specific threshold of 40 samples per question is optimal for all TTC methods
- The cost model accurately captures all relevant cost factors for TTC method comparison
- The framework captures all relevant edge cases in answer extraction across diverse datasets

## Next Checks

**Validation Check 1: Prompt and Extraction Audit** - Review the repository's prompt templates and answer extraction code to verify that prompts are truly standardized across datasets, extraction logic handles all valid answer formats consistently, and the implementation doesn't systematically disadvantage certain model architectures or reasoning styles.

**Validation Check 2: Sample Sufficiency Analysis** - For each TTC method supported (Self-Consistency, Best-of-N, Mixture of Thoughts, ModelSwitch), analyze whether 40 samples provide sufficient diversity and quality for accurate evaluation, checking if certain datasets or model families require different sample counts for reliable results.

**Validation Check 3: Cost Model Validation** - Compare the unified cost model's predictions against actual API costs for a subset of methods and models, evaluating whether the linear token-based model captures important cost differences between TTC methods, particularly for parallel versus sequential execution patterns.