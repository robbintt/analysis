---
ver: rpa2
title: 'HatePrototypes: Interpretable and Transferable Representations for Implicit
  and Explicit Hate Speech Detection'
arxiv_id: '2511.06391'
source_url: https://arxiv.org/abs/2511.06391
tags:
- hate
- prototypes
- speech
- linguistics
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HatePrototypes, a parameter-free approach for
  classifying implicit and explicit hate speech using class prototypes derived from
  fine-tuned language models. The method constructs prototype vectors from as few
  as 50 examples per class and uses them for cross-task transfer between implicit
  and explicit hate detection benchmarks, achieving performance close to fine-tuned
  baselines without any additional training.
---

# HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection

## Quick Facts
- arXiv ID: 2511.06391
- Source URL: https://arxiv.org/abs/2511.06391
- Authors: Irina Proskurina; Marc-Antoine Carpentier; Julien Velcin
- Reference count: 0
- Primary result: Parameter-free prototype method achieves near-baseline performance on hate speech detection with cross-domain transfer

## Executive Summary
This paper introduces HatePrototypes, a parameter-free approach for classifying implicit and explicit hate speech using class prototypes derived from fine-tuned language models. The method constructs prototype vectors from as few as 50 examples per class and uses them for cross-task transfer between implicit and explicit hate detection benchmarks, achieving performance close to fine-tuned baselines without any additional training. The prototypes are also applied to guide early exiting in language models, enabling efficient inference with minimal accuracy loss. Experiments show that prototype-based classification significantly enhances the performance of guardrail models on out-of-domain hate speech data. The authors release code, prototype resources, and evaluation scripts to support further research on efficient and transferable hate speech detection.

## Method Summary
HatePrototypes builds class prototypes by averaging L2-normalized hidden states from fine-tuned BERT or OPT models. For each class, prototypes are constructed using 50-500 examples, extracting either the [CLS] token (encoders) or last non-padding token (decoders) at each transformer layer. Classification uses cosine similarity between input embeddings and class prototypes. Early exiting monitors the margin between top-2 similarities and exits when it exceeds threshold Î´. The method enables cross-domain transfer by reusing prototypes trained on one benchmark to classify another, and can adapt frozen guardrail models by replacing their output layer with prototype-based classification.

## Key Results
- Prototype transfer achieves +20 to +28 F1 points when classifying between explicit and implicit hate domains
- Early exiting enables efficient inference with minimal accuracy loss, particularly for explicit hate detected in early layers
- Guardrail models (e.g., LLaMA-Guard-1B) gain +18 F1 on implicit hate tasks when using prototypes instead of default output layers
- As few as 50 examples per class are sufficient to construct effective prototypes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-domain transfer in hate speech detection can be achieved without parameter updates by reusing the geometric centroids (prototypes) of class representations from fine-tuned models.
- **Mechanism:** The method computes the $\ell_2$-normalized mean hidden state (centroid) for "hate" and "non-hate" classes using a source dataset. During inference on a target dataset, it classifies inputs based on the maximum cosine similarity between the input's hidden state and these pre-computed centroids. This suggests that the semantic boundaries for hate speech, once learned, occupy similar regions in the embedding space across different explicit and implicit benchmarks.
- **Core assumption:** The model's hidden space is linearly separable or organizes class concepts into distinct clusters that remain stable across different data distributions (domains).
- **Evidence anchors:**
  - [abstract]: "prototypes... built from as few as 50 examples per class, enable cross-task transfer... with interchangeable prototypes across benchmarks."
  - [section 5.1]: Table 2 shows BERT/OPT models fine-tuned on one domain (e.g., HateXplain) seeing massive F1 gains (+20 to +28 points) when classifying other domains (e.g., OLID/SBIC) using prototypes, matching or exceeding the original fine-tuned baseline.
  - [corpus]: Related work (*Transfer Learning via Lexical Relatedness*) supports the feasibility of transfer but implies domain gaps exist; *Lost in Moderation* highlights the specific failure modes in commercial systems that this geometric approach aims to bypass.
- **Break condition:** The mechanism likely fails if the source and target domains have fundamentally conflicting definitions of "hate" (e.g., one domain marks sarcasm as non-hate while another marks it as hate), causing the centroids to drift or overlap significantly.

### Mechanism 2
- **Claim:** Implicit hate requires deeper semantic processing than explicit hate, and this difference can be quantified and exploited for early exiting using a prototype margin criterion.
- **Mechanism:** The system monitors the "margin" (gap between the similarity to the top prototype and the second-best) at each transformer layer. Explicit hate, often relying on surface features (slurs), achieves high confidence (large margin) in early layers (e.g., 9th layer). Implicit hate, requiring context, often requires deeper layers (e.g., 11th-12th) to stabilize the margin above a threshold $\delta$.
- **Core assumption:** Transformer layers process linguistic features hierarchically, with surface lexical cues appearing early and complex semantic/pragmatic inferences emerging late.
- **Evidence anchors:**
  - [abstract]: "While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing."
  - [section 6.1]: Figure 4 and Section 6 results show SBIC (implicit) examples exiting significantly later than HateXplain (explicit) examples for comparable F1-scores.
  - [corpus]: *ImpliHateVid* and *Causality Guided Representation Learning* validate the difficulty of implicit hate but do not specifically address the layer-wise processing depth mechanism described here.
- **Break condition:** If an implicit hate example uses "surface-level" coded language that happens to appear in early layers but is ambiguous, the model might exit early with high confidence but wrong classification (high confidence error).

### Mechanism 3
- **Claim:** Large safety-guard models (e.g., Llama-Guard) can be specialized for specific hate speech benchmarks via prototype-based classification without modifying the model weights.
- **Mechanism:** Instead of relying on the guard model's default output layer, this method uses the guard model purely as a feature extractor. It constructs class prototypes from the guard model's embeddings on a target benchmark and classifies via nearest-prototype distance. This bypasses the generic "safety" classification head in favor of a data-specific geometric interpretation.
- **Core assumption:** The guard model's internal representations contain sufficient signal to distinguish specific hate nuances (e.g., implicit bias) even if its default output head suppresses or over-generalizes them.
- **Evidence anchors:**
  - [section 5.4]: Table 3 shows LLaMA-Guard-1B gaining +18 F1 on SBIC (implicit) when using prototypes compared to its baseline, suggesting the model "knew" the features but the standard head failed to utilize them effectively.
  - [corpus]: *SoftHateBench* and *Lost in Moderation* highlight that commercial models struggle with nuance; this mechanism offers a potential lightweight patch for those specific failures.
- **Break condition:** This fails if the guard model has been "aligned" such that it suppresses internal representations of implicit hate to avoid false positives, effectively removing the features needed to form distinct prototypes.

## Foundational Learning

- **Concept: Prototypical Networks & Centroid Classification**
  - **Why needed here:** The core method relies on calculating the average vector (centroid) for a class and using Euclidean/Cosine distance for classification, rather than a traditional feed-forward neural network head. Understanding that "distance in embedding space $\approx$ semantic similarity" is critical.
  - **Quick check question:** If you compute the centroid of "hate" and "non-hate" and they are 0.9 cosine similar, would you expect the classifier to work well on ambiguous implicit hate?

- **Concept: Layer-wise Semantics in Transformers**
  - **Why needed here:** The early exiting mechanism depends on the intuition that different layers process different levels of abstraction. You must understand that Layer 1-4 captures syntax/lexicon, while Layer 8-12 captures high-level intent/social meaning to set effective exit thresholds.
  - **Quick check question:** Why might exiting at layer 6 correctly identify a racial slur (explicit) but miss a demeaning "dog whistle" (implicit) that requires understanding historical context?

- **Concept: Implicit vs. Explicit Hate Speech Definitions**
  - **Why needed here:** The paper benchmarks on OLID/HateXplain (Explicit) vs. IHC/SBIC (Implicit). The success of the method is evaluated based on how well it bridges these distinct definitions. Without knowing that "implicit" includes "white grievance" or "stereotypes," the error analysis in Section 5.1 is unreadable.
  - **Quick check question:** A sentence like "They are taking our jobs" contains no slurs. Is it explicit or implicit hate, and which dataset in the paper primarily focuses on this type?

## Architecture Onboarding

- **Component map:** Encoder/Decoder (Frozen or FT) -> Prototype Store -> Similarity Gate -> Exit Controller
- **Critical path:**
  1. **Data Prep:** Select 50-500 examples per class. Ensure they are high-quality (noise in prototypes propagates to bias).
  2. **Extraction:** Run data through model. Extract `last_hidden_state` for decoders (last token) or `pooler_output`/`[CLS]` for encoders at *every* layer.
  3. **Averaging:** Calculate mean vector per class per layer. Normalize.
  4. **Inference:** For new text, compute similarity at layer 1. Check margin. If low, proceed to layer 2, etc.

- **Design tradeoffs:**
  - **Margin $\delta$:** High $\delta$ = high accuracy but low speedup (deep exits). Low $\delta$ = high speedup but risk of "early confident errors."
  - **Prototype sample size:** $<50$ samples leads to unstable centroids; $>500$ yields diminishing returns (see Figure 3).
  - **Domain Mismatch:** Using explicit-hate prototypes (OLID) for implicit-hate tasks (SBIC) works surprisingly well (Table 2), but the reverse (Implicit $\to$ Explicit) may be less effective.

- **Failure signatures:**
  - **"Stuck at Layer 1":** If $\delta$ is too low or prototypes are poorly separated, the model exits immediately with random guesses.
  - **"Never Exits":** If $\delta$ is too high or implicit hate is very subtle, the model computes all layers, negating efficiency gains.
  - **"False Positives on Counter-Speech":** Neutral discussions of protected groups often overlap vector space with hate speech; prototypes may not distinguish "discussing hate" from "promoting hate."

- **First 3 experiments:**
  1. **Baseline Validation:** Replicate the "50 samples per class" result on a held-out test set. Verify that cosine similarity to the prototype actually correlates with the ground truth label.
  2. **Layer-wise Ablation:** Plot accuracy vs. layer depth for Implicit vs. Explicit hate. Confirm that Implicit hate accuracy peaks later (e.g., layer 10-12) than Explicit (e.g., layer 6-8).
  3. **Cross-Domain Stress Test:** Train prototypes on explicit slurs (OLID) and test on implicit sarcasm (IHC). Measure the drop in F1 to quantify the "semantic gap" the paper claims to bridge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does early exiting introduce performance disparities across different protected groups, and can group-specific prototypes mitigate such biases?
- **Basis in paper:** [explicit] The authors acknowledge they "do not examine the potential effects of early exiting on biases in texts targeting different minorities" but suggest future work could construct "group-specific prototypes."
- **Why unresolved:** The current experiments report aggregate macro-F1 scores but do not perform a disaggregated analysis across specific demographic targets (e.g., race vs. religion).
- **What evidence would resolve it:** A comparative analysis of F1-scores across demographic subgroups using generic versus group-specific prototypes.

### Open Question 2
- **Question:** Can layer-wise calibration of the similarity gap threshold ($\delta$) significantly reduce the performance degradation caused by early exiting?
- **Basis in paper:** [explicit] The limitations section states that the exiting threshold "can be further calibrated at the layer-wise level to significantly reduce the performance gap" currently observed in the static implementation.
- **Why unresolved:** The current study utilizes a fixed margin threshold across all layers, which may force premature exits on deeper, more complex implicit hate examples.
- **What evidence would resolve it:** Experiments comparing the performance of a static $\delta$ against a dynamic, layer-dependent threshold schedule.

### Open Question 3
- **Question:** Can low prototype similarity scores be effectively used to identify ambiguous or underrepresented implicit hate instances for human review?
- **Basis in paper:** [explicit] The authors propose that analyzing "examples with low prototype similarity" could help "detect ambiguous or underrepresented cases of hate" to guide dataset development.
- **Why unresolved:** While conceptually proposed, the paper does not provide experimental validation of the correlation between low similarity scores and annotation ambiguity or uncertainty.
- **What evidence would resolve it:** A human annotation study measuring the inter-annotator agreement or uncertainty specifically for samples located far from class prototypes.

## Limitations
- Prototype transferability assumes consistent geometric representations of "hate" across domains, which may break for domain-specific definitions or cultural contexts
- Early exiting performance heavily depends on threshold $\delta$ requiring per-benchmark calibration and may not generalize across datasets
- Guardrail adaptation assumes frozen models retain sufficient semantic discrimination in internal representations, which may fail for safety-aligned models that suppress hate-related features

## Confidence

- **Prototype transfer claims (High):** Strong empirical support from Table 2 showing consistent gains across multiple domain pairs
- **Early exiting claims (Medium):** Validated on specific benchmarks but threshold sensitivity and domain dependence noted
- **Guardrail adaptation claims (Medium):** Promising results but based on single model family (LLaMA-Guard)

## Next Checks

1. Test prototype transferability on a truly out-of-domain dataset (e.g., non-English hate speech) to verify geometric consistency breaks down as expected
2. Conduct ablation studies varying prototype sample sizes below 50 and above 500 to quantify diminishing returns and instability thresholds
3. Evaluate false positive rates on counter-speech and academic discussions of hate groups to measure prototype overlap between "discussing hate" and "promoting hate"