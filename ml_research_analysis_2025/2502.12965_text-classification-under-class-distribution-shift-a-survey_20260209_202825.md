---
ver: rpa2
title: 'Text Classification Under Class Distribution Shift: A Survey'
arxiv_id: '2502.12965'
source_url: https://arxiv.org/abs/2502.12965
tags:
- learning
- text
- data
- classification
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews methods for handling class
  distribution shift in text classification, focusing on three key paradigms: learning
  with background class, zero-shot learning, and open-set learning. The paper identifies
  the challenges posed by emerging topics in text data and proposes future research
  directions, particularly emphasizing continual learning approaches.'
---

# Text Classification Under Class Distribution Shift: A Survey

## Quick Facts
- arXiv ID: 2502.12965
- Source URL: https://arxiv.org/abs/2502.12965
- Authors: Adriana Valentina Costache; Silviu Florin Gheorghe; Eduard Gabriel Poesina; Paul Irofti; Radu Tudor Ionescu
- Reference count: 37
- Key outcome: This survey systematically reviews methods for handling class distribution shift in text classification, focusing on three key paradigms: learning with background class, zero-shot learning, and open-set learning. The paper identifies the challenges posed by emerging topics in text data and proposes future research directions, particularly emphasizing continual learning approaches.

## Executive Summary
This survey systematically reviews methods for handling class distribution shift in text classification, focusing on three key paradigms: learning with background class, zero-shot learning, and open-set learning. The paper identifies the challenges posed by emerging topics in text data and proposes future research directions, particularly emphasizing continual learning approaches. While current methods show promise, they face limitations in scalability, adaptability, and real-world applicability. The authors highlight the need for unified frameworks that can simultaneously detect, classify, and adapt to new classes, suggesting continual learning as a promising direction. The survey provides valuable insights into existing methodologies and outlines practical recommendations for advancing the field of open-set text classification.

## Method Summary
The paper reviews three main paradigms for handling class distribution shift in text classification. Learning with background class adds an explicit "Universum" class to training data, using regularization techniques like Entropic Open-Set Loss and Objectosphere Loss to separate known and unknown classes. Zero-shot learning transfers knowledge through semantic representations (word embeddings, attribute vectors) that bridge seen and unseen classes, often using entailment-based approaches. Open-set learning and discovery requires a multi-stage pipeline: outlier detection to identify unknown samples, topic mining to discover new classes, and zero-shot classification to assign labels. The paper proposes a joint optimization framework combining these approaches with a four-class taxonomy (Known Known, Known Unknown, Unknown Known, Unknown Unknown) and suggests continual learning for model adaptation.

## Key Results
- Current methods face limitations in scalability, adaptability, and real-world applicability
- The field lacks a unified framework for simultaneously discovering, categorizing, and adapting to emerging classes
- Continual learning is identified as a promising direction for future research
- Standard LLM prompting performs poorly on OOD detection, with smaller customized models outperforming prompting
- The four-class taxonomy (KKC, KUC, UKC, UUC) provides a useful organizing framework for understanding distribution shift scenarios

## Why This Works (Mechanism)

### Mechanism 1: Background Class Regularization for Decision Boundary Refinement
Training with explicit background/universum samples may improve out-of-distribution detection by creating clearer separation between known and unknown samples in feature space. The approach adds regularization terms (Entropic Open-Set Loss, Objectosphere Loss) that maximize entropy for Known Unknown Classes (KUCs) while minimizing entropy for Known Known Classes (KKCs). Energy-based models complement this by assigning different energy levels to in-distribution vs out-of-distribution samples. Assumption: KUC samples are representative of the open-world distribution—stated as a key limitation in the paper. Break condition: When background samples fail to capture real-world diversity, UUCs (Unknown Unknown Classes) will likely be misclassified as one of the KKCs.

### Mechanism 2: Semantic Transfer via Auxiliary Embedding Space
Zero-shot classification appears to work by transferring knowledge through semantic representations (word embeddings, attribute vectors) that bridge seen and unseen classes. Maps text features to a semantic space A via f:X→A, then maps semantic representations to labels via g:A→Y. Entailment-based approaches reformulate classification as determining whether text x entails "This text is about <class>" for each candidate label. Assumption: The semantic space provides sufficient structure to distinguish between Unknown Known Classes (UKCs) without explicit examples. Break condition: Zero-shot models are sensitive to label wording; cannot handle UUCs since samples must map to supplied classes; may struggle with fine-grained distinctions between UKCs.

### Mechanism 3: Multi-Stage Open-Set Discovery Pipeline
Open-set learning and discovery likely requires sequential processing: outlier detection → topic mining → zero-shot classification, potentially iterated via continual learning. Open-set classifier splits corpus into KKCs and Cuu, topic mining identifies new relevant topics in open space, zero-shot classifier assigns samples to newly discovered topics. Outlier detection methods (LOF, ABOF) compute outlierness factors to identify semantic shift versus background shift. Assumption: Outlier detection can distinguish semantic shift (new classes) from background shift (class-agnostic changes) without access to outliers during training. Break condition: Threshold selection is difficult without knowing the Cuu ratio in test data; defining boundaries around KKCs without UUC examples is inherently challenging; multiple optimization stages required.

## Foundational Learning

- **Concept: Independent and Identically Distributed (IID) Assumption**
  - Why needed here: The fundamental ML assumption violated by class distribution shift—text classification inherently violates this as new topics emerge over time (e.g., COVID-19 appearing December 2019).
  - Quick check question: Can you explain why topic classification inherently violates the IID assumption, unlike image recognition of fixed object categories?

- **Concept: Four-Class Taxonomy (KKC, KUC, UKC, UUC)**
  - Why needed here: The paper's organizing framework—Known Known (labeled training data), Known Unknown (background class samples), Unknown Known (zero-shot targets), Unknown Unknown (truly novel classes at inference).
  - Quick check question: If you have unlabeled documents that you know don't belong to any training class, which category are they?

- **Concept: Semantic Shift vs Background Shift**
  - Why needed here: Critical distinction for outlier detection—semantic shift means new classes emerged; background shift means class-agnostic changes (formality, stance). Outlier detectors should only flag semantic shift.
  - Quick check question: If your sentiment classifier encounters reviews about a newly released product category, is that semantic shift or background shift?

## Architecture Onboarding

- **Component map:**
  Input Text → [Feature Encoder] → Embeddings → [Outlier Detection Module] → LOF/ABOF/Energy-based → Cuu flag → [Closed-Set Classifier] → KKC prediction (if KKC) / [Topic Mining/Clustering] → New class candidates → [Zero-Shot Classifier] → Assign to discovered topics → [Continual Learning Update] → Model adaptation with replay

- **Critical path:**
  1. Train closed-set classifier on KKCs with optional background regularization (α > 0 in joint objective)
  2. At inference: classify → compute outlierness factor → if below threshold, route to discovery pipeline
  3. Cluster outliers → generate/prompty semantic labels → classify with zero-shot model
  4. Periodically retrain with discovered classes using replay buffer to prevent catastrophic forgetting

- **Design tradeoffs:**
  - Universum vs Open-set: Universum requires auxiliary KUC data but easier optimization; open-set needs no OOD data but threshold selection is difficult
  - LLM vs small models: Paper reports LLaMA2-7B performs poorly on OOD detection—smaller customized models can outperform prompting
  - Detection-only vs Discovery: Detection flags unknowns; discovery attempts to classify them (harder, multiple stages)

- **Failure signatures:**
  - High false positive outlier rate → background shift being misidentified as semantic shift
  - Cluster incoherence → discovered topics not semantically consistent with existing classes
  - Performance degradation on KKCs → catastrophic forgetting after adaptation
  - Threshold drift → open-space boundaries poorly calibrated as distribution evolves

- **First 3 experiments:**
  1. **Baseline validation:** Reproduce zero-shot entailment approach (Yin et al. 2019) on AG News with label-partially-unseen setup (6k dev, 10k test per label) to establish semantic transfer benchmark.
  2. **Outlier detection comparison:** Compare LOF vs ABOF vs energy-based methods on DBPedia open-set benchmark with KKCs-to-UUCs ratios of 25%, 50%, 75%, measuring both detection accuracy and false positive rates.
  3. **Discovery pipeline integration:** Implement the three-stage discovery pipeline (Figure 3) on Yahoo! Answers, holding out 2-4 classes as UUCs, measuring cluster quality (NMI, ARI) and semantic consistency with existing class taxonomy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a unified framework be developed to simultaneously detect, classify, and adapt to emerging classes in open-set text classification?
- Basis in paper: [explicit] The authors state, "The field still lacks a unified framework capable of simultaneously discovering, categorizing, and adapting to emerging classes," specifically within NLP.
- Why unresolved: Current methods rely on disjoint stages or fixed zero-shot assumptions, failing to handle the full "learning and discovery" cycle for Unknown Unknown Classes (UUCs).
- What evidence would resolve it: An architecture combining outlier detection, topic mining, and zero-shot classification that dynamically labels UUCs.

### Open Question 2
- Question: Can Large Language Models (LLMs) utilize interactive reasoning to effectively refine and label discovered topics?
- Basis in paper: [explicit] The paper suggests, "Using a reasoning model to interactively refine a list of possible topics is a path worth exploring" to ensure semantic consistency.
- Why unresolved: Standard LLM prompting performs poorly on OOD detection, and methods to overcome memory limits for topic discovery via reasoning are not established.
- What evidence would resolve it: Studies demonstrating that interactive reasoning agents generate semantically consistent labels for new clusters better than standard prompting.

### Open Question 3
- Question: What new datasets are needed to evaluate realistic class distribution shifts and emergent topics?
- Basis in paper: [explicit] The authors conclude, "Collecting new resources that cover realistic open-set setups is an important avenue," noting current benchmarks are outdated.
- Why unresolved: Existing datasets are static snapshots over a decade old and do not simulate the dynamic, temporal nature of real-world topic emergence.
- What evidence would resolve it: New benchmarks featuring temporal evolution and emergent classes that test a model's ability to adapt to UUCs.

## Limitations

- The four-class taxonomy (KKC, KUC, UKC, UUC) may oversimplify real-world scenarios where class boundaries are fuzzy and samples can belong to multiple categories simultaneously
- Claims about continual learning being the "promising direction" remain somewhat speculative without sufficient empirical validation in the survey
- The emphasis on open-set learning requiring "no OOD data" overlooks practical scenarios where limited background samples might actually improve performance
- The paper doesn't provide concrete failure rates or performance degradation metrics from deployed systems to support claims about real-world applicability limitations

## Confidence

- **High Confidence:** The three-paradigm framework (background class, zero-shot, open-set) accurately captures the landscape of class distribution shift approaches. The identification of key challenges (scalability, adaptability, real-world applicability) is well-supported by the surveyed literature.
- **Medium Confidence:** Claims about continual learning being a promising direction are reasonable but under-supported by empirical evidence in the survey itself. The characterization of zero-shot learning limitations (sensitivity to label wording, inability to handle UUCs) is accurate but may be somewhat pessimistic given recent LLM advances.
- **Low Confidence:** The assertion that current methods face "limitations in real-world applicability" lacks specific empirical backing in the survey. The paper doesn't provide concrete failure rates or performance degradation metrics from deployed systems.

## Next Checks

1. **Threshold Sensitivity Analysis:** Implement the discovery pipeline on a held-out dataset and systematically vary outlier detection thresholds to measure the precision-recall tradeoff, quantifying how background shift samples affect false positive rates.

2. **Cross-Domain Transferability Test:** Train open-set classifiers on one text domain (e.g., news articles) and evaluate performance on a different domain (e.g., social media posts) to assess the scalability and adaptability limitations identified in the survey.

3. **Semantic Consistency Validation:** For discovered classes in the pipeline, conduct human evaluation of cluster coherence and semantic alignment with existing classes, measuring agreement rates to quantify the quality of the discovery process.