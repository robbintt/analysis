---
ver: rpa2
title: Improving In-Context Learning with Reasoning Distillation
arxiv_id: '2504.10647'
source_url: https://arxiv.org/abs/2504.10647
tags:
- rule
- reasoning
- hypothesis
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving in-context learning
  (ICL) for language models on tasks requiring inductive reasoning. It proposes ReDis,
  a reasoning distillation method that uses a teacher model to generate and evaluate
  candidate hypotheses explaining input-output relationships in few-shot demonstrations.
---

# Improving In-Context Learning with Reasoning Distillation

## Quick Facts
- arXiv ID: 2504.10647
- Source URL: https://arxiv.org/abs/2504.10647
- Reference count: 40
- The method achieves 23.2%, 2.8%, and 66.6% relative improvements on 1D-ARC, ACRE, and MiniSCAN, respectively, and surpasses GPT-4o on three out of four tasks.

## Executive Summary
This paper addresses the challenge of improving in-context learning (ICL) for language models on tasks requiring inductive reasoning. The authors propose ReDis, a reasoning distillation method that uses a teacher model to generate and evaluate candidate hypotheses explaining input-output relationships in few-shot demonstrations. By combining data augmentation, supervised fine-tuning, and preference alignment, ReDis teaches the student model both rule generation and rule-following. Experiments show that ReDis outperforms few-shot prompting baselines and even surpasses the teacher model GPT-4o on three out of four tasks, while providing inference-time efficiency gains.

## Method Summary
ReDis is a three-stage method that distills inductive reasoning ability from a teacher model (GPT-4o) into smaller open-weight models. First, the teacher generates 50 candidate hypotheses per task and evaluates their fitness by checking how many few-shot demonstrations each hypothesis satisfies. High-scoring hypotheses are filtered for supervised fine-tuning (SFT) on both rule generation and rule-following tasks. Second, SFT is performed using LoRA with rank 8, training on both rule-generation and rule-following datasets. Third, preference alignment (ORPO) is applied to the rule-generation component using pairs of chosen and rejected hypotheses. At inference, the student generates multiple hypotheses, applies the best-scoring rule to inputs, and selects the highest-scoring output.

## Key Results
- ReDis achieves 23.2%, 2.8%, and 66.6% relative improvements on 1D-ARC, ACRE, and MiniSCAN, respectively.
- ReDis surpasses GPT-4o on three out of four tasks.
- ReDis is 87%, 53%, and 25% more token-efficient than GPT-4o on MiniSCAN, 1D-ARC, and ACRE, respectively.

## Why This Works (Mechanism)

### Mechanism 1: Noisy Fitness Estimation Enables Quality Filtering Without Ground Truth Rules
The method uses fitness scores to filter hypotheses by checking how many few-shot examples each hypothesis satisfies. Longer hypotheses tend to score lower (Pearson r = -0.20 to -0.31), naturally filtering overly complex rules. This works because the teacher's rule-following ability is sufficiently reliable that high-scoring hypotheses are more likely correct, even if not perfect.

### Mechanism 2: Decomposed Training on Rule Generation and Rule-Following Transfers Inductive Reasoning
The method jointly fine-tunes on both reasoning sub-tasks (generating rules from examples, applying rules to inputs) rather than input-output imitation. Ablation shows that rule-following training is most critical (77% relative accuracy drop on 1D-ARC when using base model for rule-following), with rule-generation providing second-largest gain.

### Mechanism 3: Preference Alignment (ORPO) Concentrates Hypothesis Quality in Top-K Positions
ORPO alignment shifts probability mass toward higher-quality hypotheses, enabling smaller search spaces at inference. ORPO outperforms SFT-only across all hypothesis sizes, with larger gaps at smaller K. The method combines SFT loss with odds-ratio loss, jointly optimizing generation quality without requiring a separate reference model.

## Foundational Learning

- **In-Context Learning (ICL)**: Why needed here: ReDis targets ICL scenarios where models must generalize from few-shot demonstrations without weight updates. Quick check: Can you explain why standard ICL fails on tasks requiring symbolic rule induction rather than semantic pattern matching?

- **Knowledge Distillation (Black-Box)**: Why needed here: Teacher is GPT-4o (API-only); no access to logits or representations. Method relies on output-based distillation. Quick check: What constraints does black-box distillation impose compared to white-box methods using intermediate activations?

- **Preference Alignment (DPO/ORPO/KTO)**: Why needed here: Post-SFT alignment improves hypothesis ranking. Understanding the differences is critical for method selection. Quick check: Why might ORPO outperform DPO when the student-teacher capability gap is large?

## Architecture Onboarding

- **Component map**: Data Augmentation -> SFT (LoRA rank=8) -> Preference Alignment (ORPO) -> Inference (K hypotheses, select best)
- **Critical path**: Quality of fitness estimation → SFT data quality → baseline capability; Preference pair construction → alignment effectiveness → inference efficiency; RF training quality → hypothesis selection accuracy → final output
- **Design tradeoffs**: Larger K improves accuracy but increases cost; 50% fitness threshold balances retention vs. quality; ORPO > DPO/KTO here but requires careful hyperparameter tuning
- **Failure signatures**: Low accuracy with high hypothesis count indicates RF model not properly trained; alignment degrading performance suggests preference pairs have incorrect ordering; incoherent rules indicate SFT data quality issues
- **First 3 experiments**: 1) Validate fitness estimation reliability by computing correlation with ground-truth rule correctness; 2) Ablate RG vs. RF training impact by training separate checkpoints; 3) Sweep hypothesis size K to verify alignment concentrates quality at low K

## Open Questions the Paper Calls Out

### Open Question 1
How can inductive reasoning methods handle tasks with ambiguous rules where few-shot demonstrations are insufficient to identify a single underlying rule? The paper states this as an interesting direction for future research since the current method assumes a single correct hypothesis can be identified from demonstrations.

### Open Question 2
How does the accuracy of noisy fitness estimation affect the quality of distilled models, and can ground-truth rule evaluation further improve performance? The method depends on the teacher model both generating and evaluating hypotheses, creating potential error propagation that was not isolated or ablated.

### Open Question 3
Why does ORPO outperform DPO and KTO specifically for rule generation alignment, and does this generalize to other inductive reasoning settings? The paper reports ORPO achieves best performance but only hypothesizes that the SFT loss component contributes to additional improvement without thorough investigation.

## Limitations
- The method's performance on more complex reasoning tasks beyond the evaluated benchmarks remains unknown.
- The reliability of the noisy fitness estimation mechanism for filtering high-quality hypotheses is not fully validated.
- ORPO alignment component lacks independent validation beyond citing Hong et al. (2024).

## Confidence
- **High Confidence**: The decomposition of reasoning into rule-generation and rule-following sub-tasks is effective; inference-time efficiency gains are well-demonstrated.
- **Medium Confidence**: The overall effectiveness of ReDis in improving ICL performance, though some results show smaller margins.
- **Low Confidence**: The reliability of the noisy fitness estimation mechanism and the specific superiority of ORPO over other preference alignment methods.

## Next Checks
1. Validate fitness estimation reliability by computing correlation between fitness scores and ground-truth rule correctness on a held-out set with known rules.
2. Ablate RG vs. RF training impact by training separate checkpoints to measure isolated contribution.
3. Cross-task generalization test by evaluating ReDis on a held-out reasoning task not used in training.