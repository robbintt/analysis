---
ver: rpa2
title: 'VoxServe: Streaming-Centric Serving System for Speech Language Models'
arxiv_id: '2602.00269'
source_url: https://arxiv.org/abs/2602.00269
tags:
- serving
- streaming
- audio
- inference
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VoxServe, a unified serving system designed
  to efficiently deploy modern Speech Language Models (SpeechLMs) for streaming applications.
  SpeechLMs combine an LLM backbone with audio-specific modules like detokenizers,
  creating multi-stage inference pipelines with diverse architectures and performance
  requirements.
---

# VoxServe: Streaming-Centric Serving System for Speech Language Models

## Quick Facts
- arXiv ID: 2602.00269
- Source URL: https://arxiv.org/abs/2602.00269
- Reference count: 25
- Key outcome: Achieves 10-20× higher throughput than existing implementations at comparable latency while maintaining high streaming viability for SpeechLM serving

## Executive Summary
VoxServe is a unified serving system designed to efficiently deploy modern Speech Language Models (SpeechLMs) for streaming applications. SpeechLMs combine LLM backbones with audio-specific modules like detokenizers, creating multi-stage inference pipelines with diverse architectures and performance requirements. Existing serving systems struggle to support this architectural diversity and optimize for streaming-specific metrics like Time-To-First-Audio (TTFA) and streaming viability. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, enabling support for diverse SpeechLM architectures within a single framework while implementing streaming-aware scheduling and asynchronous inference pipelines to improve end-to-end efficiency.

## Method Summary
VoxServe introduces a unified model-execution abstraction with four composable stages (Preprocess, LLM Forward, Sampling, Postprocess) that expose standardized tensor contracts regardless of model-specific internals. The system implements streaming-aware priority scheduling that treats streaming viability as a binary constraint, prioritizing requests within 1 second of deadline violation while batching non-urgent requests for throughput. An asynchronous inference pipeline overlaps GPU computation with CPU-side control flow, allowing LLM forward and detokenizer forward to execute as independent GPU tasks with explicit state dependencies. The architecture uses CUDA graphs for LLM forward and postprocess stages to reduce kernel-launch overhead, and maintains per-request KV caches for autoregressive models and stateful detokenizers.

## Key Results
- Achieves 10-20× higher throughput than existing implementations at comparable latency
- Reduces TTFA by approximately 15% at 4.0 requests/second using asynchronous pipelining
- Maintains high streaming viability across diverse SpeechLM architectures including CosyVoice, SpeechT5, and others

## Why This Works (Mechanism)

### Mechanism 1: Unified Model-Execution Abstraction
Decoupling model architecture from system optimizations enables consistent performance gains across heterogeneous SpeechLM families without per-model reimplementation. A model interface exposes four composable stages with standardized tensor contracts, allowing worker-level optimizations (batching, CUDA graphs, cache management) to operate against a stable interface regardless of model-specific internals. This breaks if models require non-composable inference that violates stage boundaries.

### Mechanism 2: Streaming-Aware Priority Scheduling
Treating streaming viability as a binary constraint rather than latency minimization allows schedulers to reclaim slack for throughput without perceptual quality loss. Requests in startup phase are prioritized for TTFA minimization, while steady-state requests receive soft deadlines based on chunk playback duration and accumulated lag, with only deadline-risking requests elevated. This fails if request load exceeds system capacity to meet deadlines even with optimal scheduling.

### Mechanism 3: Asynchronous Pipeline Execution
Overlapping GPU computation with CPU-side control flow reduces end-to-end latency compared to synchronous stage-by-stage execution. LLM forward and detokenizer forward execute as independent GPU tasks with explicit state dependencies, while CPU handles sampling, request state updates, and scheduling decisions for the next iteration. This provides diminishing returns if GPU utilization is already near-saturation from compute alone.

## Foundational Learning

- **KV Cache Management for Autoregressive Models**: Required because both LLM backbone and stateful detokenizers need per-request cache to avoid recomputing attention over prior tokens across streaming chunks. Quick check: Given a batch of 128 requests with 256 cached tokens each, how would you allocate GPU memory for a 12-layer LLM with hidden dimension 4096?

- **CUDA Graphs and Static Shapes**: Used to reduce kernel-launch overhead but requires fixed tensor shapes at model interface boundaries despite dynamic batch sizes. Quick check: Why would variable-length sequences in a batch prevent CUDA graph capture, and what padding/segmentation strategy could work around this?

- **Neural Audio Codecs and Multi-Codebook Tokenization**: SpeechLMs generate discrete audio tokens via models like DAC (9 codebooks) or SNAC (hierarchical granularity); understanding codebook structure is essential for correct detokenizer invocation. Quick check: A model generates 86 tokens/second across 9 codebooks—how many token values are produced per 50ms chunk?

## Architecture Onboarding

- **Component map**: Interface Process -> Scheduler -> Worker -> Model
- **Critical path**: Request received → Preprocess (CPU, may include audio encoder) → LLM Prefill (GPU, TTFA-critical) → LLM Decode iterations (GPU) → Detokenizer Postprocess (GPU, streaming-viability-critical) → Chunk delivered
- **Design tradeoffs**: Smaller chunk sizes reduce TTFA but increase detokenizer invocations and overhead; higher batching improves throughput but increases per-request latency variance; CUDA graphs reduce overhead but limit dynamic shape flexibility
- **Failure signatures**: Rising p99 TTFA with flat p90 indicates head-of-line blocking from batching policy; streaming viability dropping below 100% with low GPU utilization suggests scheduling policy misconfiguration; detokenizer cache mismatch causing audio artifacts indicates incorrect state management
- **First 3 experiments**: 1) Reproduce CosyVoice baseline vs. VoxServe TTFA curve at 1-4 req/s; 2) Ablate async pipeline by forcing synchronous execution; 3) Profile GPU utilization and kernel timeline with Nsight to confirm LLM/detokenizer overlap

## Open Questions the Paper Calls Out

### Open Question 1
How should the chunk size (detokenizer invocation interval) be automatically optimized for different SpeechLM architectures and request patterns? The paper treats chunk size as an operator-specified parameter but acknowledges it affects TTFA and streaming viability; no automated selection mechanism is proposed.

### Open Question 2
Is the 1-second deadline-risk threshold for scheduler prioritization optimal across diverse deployment scenarios? This threshold is presented as a fixed heuristic without justification or sensitivity analysis.

### Open Question 3
Can VoxServe's abstraction efficiently support emerging SpeechLM architectures with fundamentally different tokenization or generation paradigms? The model interface accommodates current diversity but may require extension for future architectures.

## Limitations
- Assumes SpeechLM inference decomposes cleanly into four composable stages, which may not hold for future architectures with tighter modality coupling
- Relies on accurate prediction of chunk generation times and client-side buffering capabilities for scheduling policy
- Assumes CPU-side overhead is non-trivial relative to GPU computation for asynchronous pipelining benefits

## Confidence
- **High confidence**: Claims about architectural improvements (10-20× throughput at comparable latency) are well-supported by systematic evaluation across seven SpeechLM models
- **Medium confidence**: Claims about streaming viability as binary metric enabling scheduling slack reclamation rely on unstated assumptions about client-side buffering
- **Medium confidence**: Claims about asynchronous pipeline reducing TTFA by 15% are supported by measurements but lack independent component ablation

## Next Checks
1. **Ablation Study of Individual Optimizations**: Measure TTFA and throughput impact of disabling CUDA graphs, async pipelining, and unified model interface separately across multiple SpeechLM architectures

2. **Client-Side Buffer Impact Analysis**: Evaluate streaming viability under varying client buffer sizes (50ms, 100ms, 200ms) and network conditions to validate scheduling policy assumptions

3. **Architectural Abstraction Stress Test**: Implement a non-composable SpeechLM variant that violates stage boundary assumptions and measure performance penalty of forcing through VoxServe's abstraction layer