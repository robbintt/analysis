---
ver: rpa2
title: 'Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language
  Model via Reinforcement Learning'
arxiv_id: '2506.19469'
source_url: https://arxiv.org/abs/2506.19469
tags:
- reasoning
- surgical
- question
- visual
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surgery-R1, the first Reasoning Multimodal
  Large Language Model (MLLM) for Surgical-VQLA. The authors address the limitations
  of existing Surgical-VQLA models that lack deep reasoning capabilities and interpretability.
---

# Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.19469
- **Source URL**: https://arxiv.org/abs/2506.19469
- **Reference count**: 40
- **Primary result**: Surgery-R1 outperforms existing state-of-the-art models on EndoVis-18 and EndoVis-17 datasets, achieving 0.7356 accuracy, 0.4576 F-score, and 0.8721 mIoU on EndoVis-18, and 0.5672 accuracy, 0.4422 F-score, and 0.8422 mIoU on EndoVis-17.

## Executive Summary
This paper introduces Surgery-R1, the first Reasoning Multimodal Large Language Model for Surgical-VQLA, addressing limitations of existing models that lack deep reasoning capabilities and interpretability. The authors build a 54k dataset with paired data for Visual-QA, Grounding-QA, and Chain-of-Thought reasoning, then propose a two-stage fine-tuning mechanism using supervised fine-tuning and reinforcement fine-tuning with Group Relative Policy Optimization (GRPO). The model achieves state-of-the-art performance on EndoVis-18 and EndoVis-17 datasets while addressing spatial hallucinations through a multimodal coherence reward system.

## Method Summary
Surgery-R1 uses a two-stage fine-tuning approach on Qwen2.5-VL-3B-Instruct with LoRA adapters. Stage 1 involves supervised fine-tuning (SFT) on 80% of the Surgery-R1-54k dataset for 2 epochs with learning rate 1e-6. Stage 2 applies reinforcement fine-tuning (RFT) with GRPO using a rule-based reward system including Visual Grounding (IoU-based), Linguistic Answer (exact match), and Multimodal Coherence (quadrant alignment) rewards on the remaining 20% of data. The model is trained on 8× RTX 4090 GPUs with batch size 1/GPU and bfloat16 precision.

## Key Results
- Achieves 0.7356 accuracy, 0.4576 F-score, and 0.8721 mIoU on EndoVis-18
- Achieves 0.5672 accuracy, 0.4422 F-score, and 0.8422 mIoU on EndoVis-17
- Outperforms existing state-of-the-art models on both datasets
- Successfully mitigates spatial hallucinations through multimodal coherence rewards

## Why This Works (Mechanism)

### Mechanism 1: Structured Chain-of-Thought for Surgical Scene Decomposition
Decomposing surgical questions into sequential reasoning stages (planning → principle analysis → visual analysis → comparison → contact analysis → conclusion) creates intermediate verification points that improve answer accuracy and localization. This structured decomposition forces the model through systematic reasoning steps that can be validated before final output.

### Mechanism 2: Group Relative Policy Optimization (GRPO) for Multi-Objective Reward Alignment
GRPO uses group-based relative advantage estimation with rule-based rewards to enable stable optimization without a separate critic model. For each question, the method samples multiple outputs, computes rewards, then calculates relative advantages using normalized group statistics, updating the policy to maximize high-reward output probability while maintaining KL divergence from the reference model.

### Mechanism 3: Multimodal Coherence Reward for Spatial Hallucination Mitigation
The MC reward penalizes mismatches between textual spatial descriptions and predicted bounding box locations by dividing images into quadrants and comparing the quadrant of the bounding box center to the spatial term used in reasoning. This creates explicit supervision for text-visual spatial consistency, reducing positional hallucinations.

## Foundational Learning

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed: Standard RL requires a learned critic model, which is computationally expensive and unstable. GRPO uses group-based advantage estimation instead.
  - Quick check: Can you explain why GRPO avoids training a separate value function while PPO requires one?

- **Concept**: Visual Grounding in Multimodal Models
  - Why needed: Surgical-VQLA requires not just answering questions but localizing answers with bounding boxes. Standard MLLMs often hallucinate visual content.
  - Quick check: How does IoU-based reward differ from classification-based reward for localization tasks?

- **Concept**: Positional/Spatial Hallucinations in VLMs
  - Why needed: The paper identifies a specific failure mode where text descriptions of location don't match visual grounding outputs.
  - Quick check: Why might an SFT-trained model produce correct bounding boxes but incorrect spatial language?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-3B-Instruct (frozen) -> LoRA adapters -> SFT (Stage 1) -> RFT with GRPO (Stage 2) -> Reward System (VG, LA, MC)

- **Critical path**: Data preparation → CoT generation via qwen2.5-vl-72b-instruct API → Surgeon review → SFT training (2 epochs, lr=1e-6) → RFT with GRPO (1 epoch, 4 samples per question, temp=0.7) → Final Surgery-R1 model

- **Design tradeoffs**: 3B parameter base model prioritizes deployment feasibility over maximum performance; quadrant-based MC reward uses simpler implementation but coarser supervision; rule-based rewards provide interpretability and stability but may miss nuanced correctness.

- **Failure signatures**: SFT-only models produce correct bounding boxes with wrong spatial text (positional hallucination); missing MC reward results in high LA/VG but inconsistent reasoning; insufficient CoT diversity leads to memorized reasoning templates rather than generalizable patterns.

- **First 3 experiments**: 1) Baseline comparison: Run zero-shot Qwen2.5-VL-3B on EndoVis-18-VQLA to establish baseline (expected Acc ~0.03, mIoU ~0.31); 2) SFT-only ablation: Train Stage 1 only, visualize reasoning outputs to identify positional hallucination patterns; 3) Reward component ablation: Remove MC reward and compare quadrant alignment accuracy to validate coherence mechanism.

## Open Questions the Paper Calls Out

- **Question 1**: How would Surgery-R1 perform on diverse surgical procedures beyond robotic surgery datasets (EndoVis-18/17), such as laparoscopic or open surgery scenarios?
  - Basis: Only evaluated on EndoVis-18-VQLA and EndoVis-17-VQLA datasets from robotic surgeries
  - Unresolved: Generalization breadth to other surgical modalities remains unknown

- **Question 2**: Can the rule-based reward system (VG, LA, MC rewards) be improved or replaced with learned reward models to better capture nuanced clinical reasoning quality?
  - Basis: Uses fixed thresholds (IoU threshold τ, binary matching) that may not capture full complexity of surgical reasoning
  - Unresolved: Rule-based rewards may limit optimization potential compared to learned approaches

- **Question 3**: Would using larger base models (e.g., 7B or 72B parameters instead of 3B) provide significant improvements in surgical reasoning capabilities?
  - Basis: Uses 3B base model without ablation studies exploring model scaling effects
  - Unresolved: Relationship between model scale and surgical reasoning performance remains unexplored

- **Question 4**: How does the model perform on temporally extended surgical video sequences requiring integration of information across multiple frames?
  - Basis: Processes individual frames rather than video sequences, though surgical procedures are inherently temporal
  - Unresolved: Temporal reasoning for tracking instrument states or anatomical changes across time is not addressed

## Limitations

- **Limited GRPO Hyperparameter Specification**: Critical hyperparameters (KL penalty coefficient β, IoU threshold τ) are not specified, preventing precise replication of optimization behavior.

- **Single Validation Dataset**: Results are only reported on two EndoVis-VQLA datasets, limiting generalizability claims without testing on other surgical specialties or video datasets.

- **Rule-Based Reward Limitations**: The three rule-based rewards may not capture all clinically relevant aspects of surgical reasoning, and the binary quadrant alignment may be too coarse-grained for boundary cases.

## Confidence

- **High Confidence**: SFT+RFT pipeline improves performance over SFT-only baselines; CoT structure provides systematic reasoning decomposition; MC reward effectively reduces spatial hallucinations.

- **Medium Confidence**: GRPO is superior to standard RL approaches (limited direct comparison evidence); 54k dataset size is sufficient (not empirically validated against alternatives); reward function weights and thresholds are optimal (not systematically explored).

- **Low Confidence**: Performance on surgical datasets beyond EndoVis-17/18 (no external validation beyond these two datasets); long-term stability of RFT-optimized models compared to SFT-only (only single-epoch RFT reported); clinical relevance of improvements (all metrics are technical, no clinical validation).

## Next Checks

1. **GRPO Hyperparameter Sensitivity**: Systematically vary the KL penalty coefficient β (0.01, 0.1, 1.0) and IoU threshold τ (0.5, 0.7, 0.9) to identify optimal values and ensure reported performance is robust to these critical parameters.

2. **Spatial Reward Granularity Analysis**: Replace the binary MC reward with a continuous spatial distance metric (e.g., normalized Euclidean distance between predicted and actual instrument centers) and compare quadrant alignment accuracy to determine if finer-grained supervision improves spatial consistency.

3. **External Dataset Validation**: Evaluate Surgery-R1 on at least one additional surgical video dataset from a different source or surgical specialty (e.g., Cholec80, M2CAI) to assess generalizability beyond the EndoVis datasets used in training and testing.