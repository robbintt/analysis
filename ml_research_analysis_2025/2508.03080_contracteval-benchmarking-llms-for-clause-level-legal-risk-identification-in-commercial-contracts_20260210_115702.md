---
ver: rpa2
title: 'ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification
  in Commercial Contracts'
arxiv_id: '2508.03080'
source_url: https://arxiv.org/abs/2508.03080
tags:
- contract
- legal
- open-source
- qwen3
- clause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContractEval, the first benchmark for evaluating
  both proprietary and open-source LLMs on clause-level legal risk identification
  in commercial contracts. Using the CUAD dataset, the study assesses 19 LLMs across
  41 legal categories, measuring correctness (F1/F2 scores), output effectiveness
  (Jaccard similarity), and "laziness" (false "no related clause" rates).
---

# ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts

## Quick Facts
- **arXiv ID:** 2508.03080
- **Source URL:** https://arxiv.org/abs/2508.03080
- **Reference count:** 30
- **Primary result:** Proprietary models (GPT-4.1, Claude, Gemini) outperform open-source models on clause-level legal risk identification, with open-source models requiring targeted fine-tuning to address correctness, effectiveness, and category imbalance.

## Executive Summary
This paper introduces ContractEval, the first benchmark for evaluating both proprietary and open-source LLMs on clause-level legal risk identification in commercial contracts. Using the CUAD dataset, the study assesses 19 LLMs across 41 legal categories, measuring correctness (F1/F2 scores), output effectiveness (Jaccard similarity), and "laziness" (false "no related clause" rates). Proprietary models like GPT 4.1 outperform open-source ones, though top open-source models narrow the gap in specific dimensions. Key findings include: (1) correctness and effectiveness favor proprietary models, (2) model size yields diminishing returns, (3) thinking mode improves conciseness but hurts accuracy, (4) quantization trades efficiency for performance, and (5) performance varies significantly across legal categories. Open-source models require targeted fine-tuning to address correctness, effectiveness, and category imbalance for practical deployment in high-stakes legal settings.

## Method Summary
The study evaluates 19 LLMs (4 proprietary, 15 open-source) on the CUAD test set (4,128 samples from 102 contracts across 41 legal categories). Models perform zero-shot clause extraction using a standardized prompt instructing them to return exact sentences or "No related clause." Evaluation uses custom metrics: F1/F2 scores (correctness) based on span coverage definitions, Jaccard similarity (effectiveness) on positive cases, and false "no related clause" rate (laziness). Models are tested in standard, thinking, and quantized (AWQ, FP8) modes. The analysis examines performance differences across model families, sizes (4B-14B parameters), and legal categories, with particular attention to category imbalance (30% positive clauses) and rare clause types.

## Key Results
- Proprietary models (GPT-4.1, Claude, Gemini) significantly outperform open-source models in both correctness (F1 gap ~16% for best open-source vs. GPT-4.1) and output effectiveness
- Thinking mode improves output conciseness (higher Jaccard similarity) but reduces correctness by encouraging over-elaboration on straightforward extraction tasks
- Model size shows diminishing returns: 8B parameters optimal for open-source models, with 14B model underperforming due to training-data mismatch
- Quantization (AWQ, FP8) provides 2-4× GPU savings but reduces accuracy by 5-15%, with FP8 particularly problematic
- Performance varies dramatically across legal categories, with rare categories (e.g., "Joint IP Ownership") showing near-zero scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proprietary models outperform open-source models in clause-level legal risk identification due to superior training scale and data quality.
- Mechanism: Proprietary models (GPT-4.1, Claude, Gemini) leverage larger pre-training corpora with better instruction tuning, enabling more accurate span extraction. Open-source models, while competitive in specific dimensions, lack domain-specific fine-tuning for legal text, leading to lower correctness (F1 gap of ~16% for best open-source vs. GPT-4.1) and effectiveness.
- Core assumption: Performance differences stem primarily from training data and scale, not architectural superiority. This assumes similar evaluation conditions across model families.
- Evidence anchors:
  - [abstract]: "Proprietary models outperform open-source models in both correctness and output effectiveness"
  - [section 4.1]: "Qwen3 8B in 'thinking' mode achieves an F1 score of 0.540... still about 16% lower than GPT 4.1"
  - [corpus]: LawGPT paper confirms open-source models face "significant limitations in legal reasoning tasks" with similar proprietary vs. open-source performance gaps
- Break condition: If open-source models receive targeted legal-domain fine-tuning, the performance gap should narrow. Conversely, if proprietary models' advantage is primarily inference-time compute, smaller fine-tuned open-source models may never close the gap.

### Mechanism 2
- Claim: "Thinking" mode improves output conciseness (higher Jaccard similarity) but reduces correctness (lower F1) by encouraging over-elaboration on straightforward extraction tasks.
- Mechanism: Thinking mode triggers step-by-step reasoning chains. For span-extraction tasks requiring precise substring identification, this reasoning causes models to second-guess simple answers, include peripheral context, or misinterpret extraction boundaries—increasing verbosity while decreasing exact-match accuracy.
- Core assumption: The trade-off is task-specific. Thinking benefits complex multi-step reasoning but harms high-precision extraction where the answer exists verbatim in the context.
- Evidence anchors:
  - [abstract]: "Reasoning ('thinking') mode improves output effectiveness but reduces correctness, likely due to over-complicating simpler tasks"
  - [section 4.5]: "thinking mode often improves output effectiveness... but simultaneously reduces correctness, with notable drops in F1 scores across most models"
  - [corpus]: No direct corpus evidence on thinking-mode trade-offs in legal tasks; this mechanism is underexplored in related work
- Break condition: If thinking mode is adapted with legal-specific prompts or constrained to avoid over-analysis, the trade-off may shift. Tasks requiring interpretation (e.g., "Is this clause high-risk?") rather than extraction may benefit from thinking mode.

### Mechanism 3
- Claim: Open-source model performance scales with size up to an optimal point (8B parameters), after which diminishing or negative returns emerge due to training-data mismatch and category imbalance.
- Mechanism: Larger models have greater capacity but require proportionally more domain-specific training data. The CUAD dataset's 30/70 positive/negative clause imbalance and rare categories (e.g., "Joint IP Ownership") under-utilize larger model capacity. The 8B model balances capacity and generalization; the 14B model overfits or fails to leverage its scale without corresponding legal-domain fine-tuning.
- Core assumption: Diminishing returns are not inherent to model architecture but reflect insufficient domain-specific pre-training/fine-tuning for legal span extraction.
- Evidence anchors:
  - [section 4.4]: "the 8B model achieves the highest F1 scores... This performance surpasses both the smaller Qwen3-4B and the larger Qwen3 14B model"
  - [section 4.6]: "both models show near-zero scores in more nuanced or rare categories like 'Uncapped Liability', 'Joint IP Ownership'"
  - [corpus]: Related work on legal LLMs (LawGPT, SaulLM-7B) emphasizes domain-specific training; no direct corpus evidence on size-diminishing returns in legal tasks
- Break condition: If the 14B model receives extensive legal-domain fine-tuning with balanced category sampling, its performance should surpass the 8B model. The current result reflects under-training, not a fundamental ceiling.

## Foundational Learning

- Concept: Span-level extraction vs. document-level retrieval
  - Why needed here: Contract review requires extracting exact text substrings (clauses) from contracts, not just retrieving relevant documents. This distinction affects model evaluation (exact-match metrics) and failure modes (over-selection, false negatives).
  - Quick check question: Can you explain why F1/Jaccard metrics are appropriate for span extraction but not for document retrieval tasks?

- Concept: Category imbalance in legal datasets
  - Why needed here: CUAD has 30% positive clauses (relevant clause present) and 70% negative. Rare categories like "Joint IP Ownership" have few training examples, causing models to underperform or default to "no related clause."
  - Quick check question: How would you adjust evaluation or training to handle a dataset where 70% of examples are negative cases?

- Concept: Thinking mode (chain-of-thought) in LLMs
  - Why needed here: The paper shows thinking mode has a trade-off: better conciseness, worse accuracy. Understanding when to enable/disable it is critical for practical deployment.
  - Quick check question: For a task requiring verbatim clause extraction, would you enable thinking mode? Why or why not?

## Architecture Onboarding

- Component map:
  Input Layer (Contract text + legal question) -> Model Backbone (19 LLM variants) -> Inference Modes (Standard/Thinking/Quantized) -> Evaluation Module (F1/F2/Jaccard/laziness) -> Output (Extracted clause or "no related clause")

- Critical path:
  1. Load contract + question pair from CUAD test set (4,128 examples)
  2. Format prompt: system instructions + context (full contract) + question
  3. Run inference (select model, mode: thinking/non-thinking, precision)
  4. Parse output: exact span vs. "no related clause"
  5. Compare against ground-truth annotations; compute F1, F2, Jaccard, false negative rate
  6. Aggregate results across 41 categories; analyze category-level performance

- Design tradeoffs:
  - **Model selection**: Proprietary (higher accuracy, data privacy concerns, $2–8/million tokens) vs. open-source (lower accuracy, local deployment, no per-token cost)
  - **Precision vs. speed**: Full-precision (bfloat16) vs. quantized (FP8, AWQ) – 2–4× GPU savings with ~5–15% performance drop
  - **Thinking mode**: Enable for complex interpretation tasks (conciseness priority); disable for straightforward extraction (accuracy priority)

- Failure signatures:
  - **High false "no related clause" rate**: Model avoids answering; indicates low confidence or "laziness" (e.g., Qwen3 8B AWQ at 30.6%)
  - **Low Jaccard similarity with high F1**: Model correctly identifies clauses but includes excessive irrelevant text
  - **Near-zero F1 in specific categories**: Rare clause types (e.g., "Joint IP Ownership") – indicates training data gap
  - **Thinking mode performance collapse**: F1 drops below 0.1 (e.g., DeepSeek R1 Distill Qwen 7B) – model over-reasons on simple extraction

- First 3 experiments:
  1. **Baseline comparison**: Run GPT-4.1 mini vs. Qwen3 8B (non-thinking) on 10 random contracts; measure F1 and Jaccard gap. Expected: GPT-4.1 mini leads by ~15–20% F1.
  2. **Thinking mode ablation**: Test Qwen3 8B in thinking vs. non-thinking mode on 5 contracts with mixed complexity (simple "Effective Date" vs. complex "IP Ownership"). Expected: Thinking mode improves Jaccard by ~5–10% but drops F1 by ~5–8%.
  3. **Category imbalance stress test**: Evaluate best open-source model (Gemma 3 12B) on the 5 rarest categories (lowest F1 in paper); measure false negative rate. Expected: False negative rate exceeds 40% for rare categories without fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation metrics for legal NLP be better aligned with the subjective standards and expectations of senior legal professionals?
- Basis in paper: [explicit] The discussion section explicitly asks: "First, how can evaluation methods for contract review and legal tasks be better aligned with the standards and expectations of senior legal professionals?"
- Why unresolved: The paper relies on automated metrics like F1 and Jaccard similarity, which function as proxies for correctness and conciseness but may not capture the nuance of legal sufficiency or risk severity as judged by human experts.
- What evidence would resolve it: A study correlating automated benchmark scores with blind reviews by senior lawyers to develop a new "legal alignment" scoring rubric.

### Open Question 2
- Question: How can the reliability and consistency of LLMs be improved in legal workflows given their high prompt sensitivity?
- Basis in paper: [explicit] The discussion section explicitly asks: "Second, given the high prompt sensitivity of LLMs, how can we improve their reliability and consistency in real-world workflows?"
- Why unresolved: The paper establishes that models perform at the level of junior assistants but notes that production deployment requires stability that current prompt-response behaviors may not guarantee.
- What evidence would resolve it: Research into prompt engineering stability or architectural modifications that reduce output variance across semantically equivalent legal prompts.

### Open Question 3
- Question: Can the trade-off between "thinking" (reasoning) mode and correctness be mitigated in clause extraction tasks?
- Basis in paper: [inferred] Section 4.5 shows that reasoning mode improves output effectiveness (conciseness) but consistently reduces correctness (F1 score), likely due to over-complicating simple extraction tasks.
- Why unresolved: The paper identifies this trade-off as a "mismatch" but does not propose a method to achieve the benefits of reasoning (conciseness) without the cost of accuracy loss.
- What evidence would resolve it: Experiments with hybrid models or specialized prompting strategies that trigger reasoning only for complex clauses while using direct extraction for simple ones.

### Open Question 4
- Question: What specific targeted fine-tuning strategies are most effective for correcting the performance imbalance across legal clause categories?
- Basis in paper: [inferred] Section 4.6 reveals that performance drops significantly for less common or complex clauses (e.g., Joint IP Ownership), and the conclusion suggests targeted fine-tuning is required to address this imbalance.
- Why unresolved: While the paper identifies the imbalance (high performance on "Governing Law", low on "Uncapped Liability"), it stops short of testing the suggested fine-tuning remedies.
- What evidence would resolve it: A follow-up study comparing standard fine-tuning vs. few-shot or curriculum-based fine-tuning specifically on the underrepresented clause categories identified in the benchmark.

## Limitations
- Results based on zero-shot evaluation may not generalize to all legal domains or contract types
- Performance gaps between proprietary and open-source models could reflect inference-time compute differences rather than inherent quality
- Thinking mode trade-offs lack detailed error analysis of why over-elaboration occurs
- Category imbalance and rare clause types remain significant challenges without proposed mitigation strategies

## Confidence
- **High Confidence:** Proprietary models outperform open-source models in correctness and effectiveness metrics; category imbalance affects performance across all models; quantization provides efficiency gains at measurable accuracy costs
- **Medium Confidence:** Thinking mode improves conciseness but reduces accuracy; model size shows diminishing returns beyond 8B parameters; the 30/70 positive/negative clause imbalance significantly impacts evaluation
- **Low Confidence:** Specific mechanisms behind thinking mode trade-offs; whether performance gaps reflect fundamental architectural differences or simply training data disparities; generalizability to non-CUAD legal domains

## Next Checks
1. **Fine-tuning experiment:** Take the top-performing open-source model (Gemma 3 12B) and fine-tune it on a balanced subset of CUAD with equal positive/negative samples and oversampled rare categories. Compare post-fine-tuning F1 scores against the zero-shot baseline.
2. **Thinking mode error analysis:** Manually examine 20 failed extractions where thinking mode was enabled. Categorize failure modes (over-elaboration, boundary errors, hallucination) and compare against 20 non-thinking failures to quantify the specific degradation mechanism.
3. **Cross-domain validation:** Evaluate the best-performing models on a different legal dataset (e.g., contract review tasks from LawGPT's evaluation set or real-world legal briefs). Measure performance drop/gain to assess domain generalization.