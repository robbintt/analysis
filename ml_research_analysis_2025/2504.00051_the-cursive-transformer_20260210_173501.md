---
ver: rpa2
title: The Cursive Transformer
arxiv_id: '2504.00051'
source_url: https://arxiv.org/abs/2504.00051
tags:
- stroke
- cursive
- data
- handwriting
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Cursive Transformer, a GPT-based model
  for generating realistic cursive handwriting from ASCII input. The authors introduce
  a novel tokenization scheme that converts pen stroke offsets into polar coordinates,
  discretizes them into bins, and uses two tokens per stroke (angle and radius/pen-state).
---

# The Cursive Transformer

## Quick Facts
- **arXiv ID:** 2504.00051
- **Source URL:** https://arxiv.org/abs/2504.00051
- **Reference count:** 16
- **Primary result:** Custom tokenizer converts pen strokes to polar coordinates, enabling standard GPT to generate realistic cursive handwriting from ASCII input without specialized architectures

## Executive Summary
This paper presents the Cursive Transformer, a GPT-based model that generates realistic cursive handwriting from ASCII input. The key innovation is a novel tokenization scheme that converts pen stroke offsets into polar coordinates, discretizes them into bins, and uses two tokens per stroke (angle and radius/pen-state). This allows training a standard GPT model without specialized architectures like mixture density networks. Using just 3,500 handwritten words and simple data augmentations, the model achieves high-quality cursive generation competitive with more complex RNN-based approaches.

## Method Summary
The model tokenizes pen stroke data by converting Cartesian offsets to polar coordinates, then discretizing angle and radius into bins. Each stroke becomes two tokens: angle first, then radius plus pen-state. The 523-token vocabulary enables standard GPT-2 training with cross-attention layers that condition on ASCII characters. The model uses 5 decoder blocks with 4 heads each, trained on 745k four-word samples with cross-entropy loss. Data augmentation includes random horizontal shear, scaling, and downsampling to improve generalization from the limited training corpus.

## Key Results
- Standard GPT-2 with custom tokenizer generates realistic cursive handwriting without mixture density networks
- Cross-attention layers learn character-to-stroke alignment without explicit positional alignment mechanisms
- Two-token per stroke scheme (angle then radius+pen-state) improves prediction accuracy over joint encoding
- High-quality results achieved from only 3,500 handwritten words with data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting pen stroke offsets to polar coordinates and discretizing into bins allows a standard GPT to capture multimodal stroke distributions without mixture density networks.
- **Mechanism:** Cartesian offsets (Δx, Δy) → polar coordinates (θ, r) → binning → token indices. Cross-entropy loss over discrete bins implicitly captures skew and multimodal distributions that would otherwise require specialized output layers.
- **Core assumption:** Binning resolution (J=220 angle bins, K=150 radius bins) preserves sufficient information for legible handwriting.
- **Evidence anchors:** [abstract] "converts pen stroke offsets to polar coordinates, discretizes them into bins... without using any specialized architectures" [section 2] "the frequency of θ bins was fairly evenly distributed, and the range of r values that needed binning was relatively easy to reason about"

### Mechanism 2
- **Claim:** Using two tokens per stroke (angle first, then radius+pen-state) improves prediction accuracy by allowing the model to condition radius prediction on already-sampled angle.
- **Mechanism:** Autoregressive decomposition: P(θ, r, p) = P(θ) × P(r, p | θ). The model "points" first, then "shoots" with context of the chosen direction.
- **Core assumption:** Angle and magnitude are sufficiently decoupled that sequential prediction is easier than joint prediction.
- **Evidence anchors:** [section 2] "the stroke angle information in the first token was enough to 'point' the model and then... it was able to 'shoot' with greater accuracy" [section 1] "multimodal 2D Gaussian distributions... are captured implicitly by the fact that our model is trained to predict a multinomial distribution"

### Mechanism 3
- **Claim:** Cross-attention between ASCII tokens and stroke tokens learns character-to-stroke alignment without explicit positional alignment mechanisms.
- **Mechanism:** Standard cross-attention layers allow stroke tokens to query ASCII context. Diagonal attention patterns emerge showing strokes attend primarily to their corresponding character.
- **Core assumption:** The model can learn the many-to-one mapping (25-40 strokes per character) from data alone.
- **Evidence anchors:** [section 3] "diagonal attention structure in the cross-attention maps... indicates that the model learned the correspondence between ASCII characters and their appropriate stroke sequences in an entirely data-driven manner" [section 3] "some heads (head 1) focused almost entirely on the current character while other heads (head 3) focused more on the preceding and succeeding characters"

## Foundational Learning

- **Concept:** Polar coordinates (r, θ) vs. Cartesian (x, y)
  - **Why needed here:** The tokenizer converts all stroke offsets to polar form before binning. Understanding why direction and magnitude are decoupled is essential for debugging tokenization issues.
  - **Quick check question:** Given a stroke offset of (Δx=3, Δy=4), what are the approximate polar coordinates?

- **Concept:** Cross-attention in encoder-decoder transformers
  - **Why needed here:** The model conditions on ASCII via cross-attention at every layer. Distinguishing self-attention (stroke-to-stroke) from cross-attention (stroke-to-ASCII) is critical for architecture modifications.
  - **Quick check question:** In cross-attention, which sequence provides the queries and which provides keys/values?

- **Concept:** Discretization of continuous variables for sequence modeling
  - **Why needed here:** The core contribution is replacing continuous regression with classification over bins. Understanding the tradeoffs (quantization error vs. distributional flexibility) informs extension to other modalities.
  - **Quick check question:** What distribution does an MLP with MSE loss implicitly assume for outputs, and why might this fail for pen strokes?

## Architecture Onboarding

- **Component map:** ASCII string + (optional) style seed → ASCII tokenizer → character-level tokens → Stroke history → polar binned tokens (θ token, r+p token pairs) → GPT-2 decoder with cross-attention at each of 5 blocks (4 heads, 64-dim embeddings) → Output logits → sample next stroke token pair → Detokenize: bin index → (θ, r, p) → (Δx, Δy, p) → cumulative sum → (x, y, p)

- **Critical path:** The bin boundaries for θ (linear) and r (mixed linear/geometric with transition at 0.2) directly affect output quality. Incorrect bin reconstruction during decoding produces garbled strokes.

- **Design tradeoffs:** Vocabulary size J + 2K + 3 = 523 tokens vs. J × K × 2 ≈ 66,000 for joint encoding; Data collection: dotting i's immediately after stem vs. end-of-word (reduces long-range dependencies but changes natural writing patterns); Downsampling 55-75% of points: forces model to learn robust stroke representations but may lose fine detail

- **Failure signatures:** Overlapping/disconnected letters → cross-attention not aligning properly; check learning rate schedule; Jagged strokes → bin resolution insufficient or downsampling too aggressive; Missing characters → WORD token handling issues or context length exceeded; Inconsistent slant → horizontal shear augmentation too extreme

- **First 3 experiments:** 1) Validate tokenization roundtrip: Take raw stroke data, tokenize, detokenize, and overlay original vs. reconstructed strokes. Quantify reconstruction error. 2) Ablate two-token scheme: Compare dual-token (θ then r+p) vs. single-token (joint θ×r×p with larger vocabulary) on same training data. Monitor loss curves and sample quality. 3) Visualize early vs. late cross-attention: Replicate Figure 4 analysis. Confirm diagonal patterns emerge by layer 3-4. If not, increase training steps or check ASCII token positioning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the tokenizer-based approach handle natural cursive writing where dots and crosses are added at the end of words rather than immediately after each character stem?
- **Basis in paper:** [explicit] The authors state they "resolved to change our data collection method" because standard writing "introduces long-range dependencies which are exceedingly difficult to model" — they bypassed rather than solved this problem.
- **Why unresolved:** The model was trained on modified data; it is unknown whether the approach can capture true long-range stroke dependencies without specialized architectural components.
- **What evidence would resolve it:** Train on naturally-collected cursive data and measure whether generated samples correctly place delayed dots and crosses.

### Open Question 2
- **Question:** Can the polar coordinate tokenization scheme scale effectively to 3D motion data for robotics applications?
- **Basis in paper:** [explicit] "Our polar coordinate tokenization scheme...could be generalized to spherical coordinates" and used "to train transformers to generate robotic joint articulations."
- **Why unresolved:** The authors propose this as an extension but provide no experiments; binning strategies may face curse-of-dimensionality issues in higher dimensions.
- **What evidence would resolve it:** Apply spherical coordinate binning to a 3D trajectory dataset and compare generation quality against continuous-output baselines.

### Open Question 3
- **Question:** Does binning continuous variables and training with cross-entropy consistently outperform direct regression across diverse continuous control domains?
- **Basis in paper:** [explicit] "It is possible that most, if not all problems that use continuous variables can be modeled at least as well via binning and tokenization."
- **Why unresolved:** This is a broad claim supported by only one domain; the tradeoffs may differ across tasks with different distributional properties.
- **What evidence would resolve it:** Systematic benchmarking on multiple continuous tasks (e.g., time series, motion capture, control) comparing binned tokenization against mixture density networks and RMSE baselines.

## Limitations

- **Data scale and generalization:** Model trained on only 3,500 handwritten words raises questions about true generalization despite augmentation strategy
- **Evaluation methodology:** No quantitative metrics beyond subjective visual assessment; comparison to Graves (2014) relies on visual inspection without objective measures
- **Technical debt:** Several implementation details underspecified (exact r-binning boundaries, shear augmentation parameters, downsampling algorithm), making faithful reproduction challenging

## Confidence

**High Confidence** (well-supported by evidence):
- Polar coordinate discretization enables standard GPT training without mixture density networks
- Cross-attention learns character-to-stroke alignment without explicit positional mechanisms
- Two-token per stroke scheme improves prediction accuracy

**Medium Confidence** (reasonable but under-supported):
- Model generates "high-quality" cursive from limited data (3,500 words)
- Results are competitive with more complex RNN-based approaches
- Discretization preserves sufficient information for legible handwriting

**Low Confidence** (claims exceed evidence):
- Approach generalizes to diverse handwriting styles beyond training corpus
- Method represents fundamental advance in handling continuous multimodal data
- Model would scale effectively to longer sequences or different writing systems

## Next Checks

1. **Quantify reconstruction error:** Take raw stroke data, tokenize, detokenize, and measure Euclidean distance between original and reconstructed stroke sequences. Report mean and maximum errors across test set.

2. **Test cross-attention alignment:** Extract cross-attention weights from trained model. For each stroke token, compute attention distribution over ASCII characters. Calculate percentage of strokes attending to corresponding character versus neighboring characters.

3. **Evaluate ablation of two-token scheme:** Train identical models with single-token joint encoding versus dual-token scheme using identical training data and hyperparameters. Compare final cross-entropy loss, sample quality, and training stability.