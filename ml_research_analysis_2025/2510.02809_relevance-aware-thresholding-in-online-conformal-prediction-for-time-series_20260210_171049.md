---
ver: rpa2
title: Relevance-Aware Thresholding in Online Conformal Prediction for Time Series
arxiv_id: '2510.02809'
source_url: https://arxiv.org/abs/2510.02809
tags:
- prediction
- interval
- coverage
- methods
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining both coverage
  validity and prediction interval tightness in online conformal prediction for time
  series, where distribution shifts are common. The authors propose replacing the
  binary (inside/outside) evaluation of prediction intervals with a family of relevance-aware
  functions that quantify the distance between the ground truth and interval bounds.
---

# Relevance-Aware Thresholding in Online Conformal Prediction for Time Series

## Quick Facts
- arXiv ID: 2510.02809
- Source URL: https://arxiv.org/abs/2510.02809
- Reference count: 40
- Primary result: Relevance-aware thresholding achieves comparable or better coverage while reducing average and median interval widths compared to original binary methods

## Executive Summary
This paper addresses the challenge of maintaining both coverage validity and prediction interval tightness in online conformal prediction for time series, where distribution shifts are common. The authors propose replacing the binary (inside/outside) evaluation of prediction intervals with a family of relevance-aware functions that quantify the distance between the ground truth and interval bounds. These functions, customizable via parameters ω and v, dynamically adjust threshold updates based on relevance feedback, preventing abrupt changes and producing tighter intervals. Experiments on real-world datasets (stock prices, temperature) show that the modified PID and ECI methods achieve comparable or better coverage while reducing average and median interval widths compared to original methods. The approach improves efficiency without sacrificing validity, advancing trustworthy AI for sensitive applications.

## Method Summary
The paper modifies online conformal prediction by introducing relevance-aware thresholding functions that replace binary inside/outside indicators. The relevance function $f_{\omega,v,\mu_t}(s_t - q_t)$ outputs continuous values in [0,1] based on the distance between ground truth and interval bounds, using scale-normalization via a rolling-window average $\mu_t$. This function is integrated into existing PID and ECI update rules, creating modified versions that respond more gradually to errors. The method uses symmetric intervals $[\hat{Y}_t \pm q_t]$ with a regressor (AR or Theta) retrained on sliding 365-point windows, and evaluates performance through coverage validity and interval width metrics.

## Key Results
- Modified PID and ECI methods achieve comparable or better coverage (target 90%) compared to original binary methods
- Average and median prediction interval widths are reduced by the proposed relevance-aware approaches
- The relevance functions prevent abrupt threshold changes that cause interval width spikes in binary methods
- Scale-normalization via $\mu_t$ enables consistent behavior across datasets with different scales

## Why This Works (Mechanism)

### Mechanism 1
Replacing binary coverage indicators with graduated relevance functions yields tighter prediction intervals while maintaining coverage validity. The proposed functions output continuous values in [0,1] based on distance from interval bounds, triggering larger threshold increases when ground truth falls outside and allowing threshold reduction when inside. This graduated response prevents abrupt threshold changes that treat all errors equally regardless of magnitude.

### Mechanism 2
Scale-normalization via rolling-window average $\mu_t$ enables cross-dataset transfer of relevance function behavior without retuning. The term $\mu_t$ computes mean absolute distance over a window, adapting the sigmoid sensitivity: large historical errors stretch the response, small errors compress it. This prevents the same absolute error from being penalized differently across datasets with different scales.

### Mechanism 3
Long-run coverage guarantees are preserved when relevance functions satisfy boundedness and the cumulative relevance error dominates cumulative binary error after finite time. The proof structure shows that replacing only the proportional term leaves the integral saturation term unchanged, which is sufficient for the original guarantee. Additional conditions require the dominance of relevance error sum over binary error sum after some finite time.

## Foundational Learning

- **Conformal Prediction (CP) fundamentals**: Why needed - The paper builds on Split CP and its calibration-based quantile construction. Quick check - Given calibration scores $\{|Y_i - \hat{Y}_i|\}_{i=1}^m$ and miscoverage $\alpha=0.1$, what quantile defines the prediction interval?
- **Online learning and regret bounds**: Why needed - OCP methods are framed as online algorithms with long-run coverage analogous to regret. Quick check - Why does a larger learning rate $\eta$ in OGD-style updates produce more volatile threshold trajectories?
- **Control theory basics (PID controllers)**: Why needed - The PID-based OCP method explicitly maps to proportional-integral-derivative control. Quick check - In the update rule, which term would cause persistent under-coverage to gradually increase $q_{t+1}$ even after individual errors stop?

## Architecture Onboarding

- **Component map**: Regressor module -> Score computer -> Relevance function -> Threshold updater -> Interval constructor
- **Critical path**: 1) Regressor outputs $\hat{Y}_t$ using most recent 365 points, 2) Construct $\hat{C}_t$ using current threshold $q_t$, 3) Observe $Y_t$, compute $s_t$, 4) Update $\mu_t$ from windowed history, 5) Compute $f_{\omega,v,\mu_t}(s_t - q_t)$, 6) Update $q_{t+1}$ via chosen rule, 7) Shift window, repeat
- **Design tradeoffs**: $v$ (sensitivity) - small values produce gentler changes but risk slow adaptation; $T_w$ (window size) - large values provide stable $\mu_t$ but sluggish response to shifts; $l$ (number of sigmoids) - $l=1$ is simplest while $l>1$ enables plateau shapes for domain-specific error tolerance
- **Failure signatures**: Coverage collapse - check if $\eta$ violates Theorem 3 bounds; exploding intervals - likely $\mu_t \approx 0$ causing division instability; stagnant thresholds - $f_{\omega,v,\mu_t}$ clustering at $\alpha$
- **First 3 experiments**: 1) Baseline reproduction - run original PID and ECI on Amazon stock with AR regressor, verify coverage ≈0.90 and record widths, 2) Single-parameter sweep - fix $\omega=1$, sweep $v \in \{1, 2, 4, 8, 16\}$ with modified PID, plot coverage vs. median width, 3) Distribution shift robustness - introduce synthetic changepoint at $t=1500$ (mean shift +20%), compare original vs. modified PID recovery speed

## Open Questions the Paper Calls Out

### Open Question 1
How can the hyper-parameters $\omega$ and $v$ be systematically optimized to minimize prediction interval sizes? The authors state in the Conclusion that "one needs to study and optimize the hyper-parameters of the proposed function to reduce, even more, the prediction interval sizes." This remains unresolved as the current work relies on empirical selection rather than a formal optimization strategy.

### Open Question 2
How can domain expertise be formally translated into the specific parameterization of the relevance functions? The Conclusion suggests it would be interesting to "understand the links between the parametrization of our functions and DMs' [Decision Makers] expertise." There is currently no methodology for mapping a user's domain knowledge or risk preferences to specific values of $\omega$ and $v$.

### Open Question 3
How sensitive are the results to the definition of the scale-independence parameter $\mu_t$, and are there superior alternatives? The Conclusion notes that the choice of $\mu_t$ to ensure scale independence "is not unique" and suggests "future researches may be conducted about the impact that this parameter has." The paper implements a specific mean-based definition but does not analyze how this choice affects robustness compared to other scaling methods.

## Limitations

- Coverage validity guarantees depend on the dominance assumption in Theorem 2, which could be strong and may not hold if relevance functions cluster near $\alpha$
- Scale-normalization via $\mu_t$ could lag during sharp distribution shifts, causing inappropriate sensitivity until the window flushes outdated data
- The saturation function parameters for PID and optimal learning rates for ECI are not fully specified, limiting exact reproduction

## Confidence

- **High confidence**: The mechanism by which graduated relevance functions prevent abrupt threshold changes and produce tighter intervals is well-supported by experimental results
- **Medium confidence**: Scale-normalization via rolling-window average enables cross-dataset transfer, but the lag during sharp shifts and choice of window size $T_w$ introduce uncertainty
- **Medium confidence**: Long-run coverage guarantees are preserved under the stated conditions, but the dominance assumption in Theorem 2 could be strong

## Next Checks

1. **Coverage sensitivity analysis**: Systematically test different learning rates $\eta$ and relevance function parameters ($\omega$, $v$) to identify regions where coverage validity breaks down, especially near the boundaries of Theorem 3's assumptions

2. **Distribution shift stress test**: Design experiments with synthetic abrupt and gradual distribution shifts to measure how quickly modified methods restore coverage compared to baselines, and whether $\mu_t$ lag causes inappropriate scaling

3. **Asymmetric interval extension**: Modify the framework to handle asymmetric prediction intervals and evaluate whether the relevance signal remains aligned with actual prediction quality, identifying any misalignment issues