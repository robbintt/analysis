---
ver: rpa2
title: 'INTIMA: A Benchmark for Human-AI Companionship Behavior'
arxiv_id: '2508.09998'
source_url: https://arxiv.org/abs/2508.09998
tags:
- user
- prompts
- emotional
- benchmark
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INTIMA, a benchmark for evaluating companionship
  behaviors in language models. Drawing from psychological theories and user data,
  it develops a taxonomy of 31 behaviors across four categories and 368 targeted prompts.
---

# INTIMA: A Benchmark for Human-AI Companionship Behavior

## Quick Facts
- arXiv ID: 2508.09998
- Source URL: https://arxiv.org/abs/2508.09998
- Reference count: 5
- Models show marked differences in handling emotionally charged interactions

## Executive Summary
This paper introduces INTIMA, a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, it develops a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Testing Gemma-3, Phi-4, o3-mini, and Claude-4 shows that companionship-reinforcing behaviors remain much more common across all models, though marked differences exist. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.

## Method Summary
The INTIMA benchmark develops a taxonomy of companionship behaviors grounded in psychological theory and user interaction data. The taxonomy encompasses 31 distinct behaviors organized into four categories, operationalized through 368 carefully designed prompts. Model responses to these prompts are classified into three categories: companionship-reinforcing, boundary-maintaining, or neutral. The evaluation framework is applied to several language models including Gemma-3, Phi-4, o3-mini, and Claude-4 to assess their behavioral tendencies across different types of interactions.

## Key Results
- Gemma-3, Phi-4, o3-mini, and Claude-4 all showed companionship-reinforcing behaviors as the dominant response type
- Marked differences exist between commercial providers in handling sensitive interaction categories
- Boundary-maintaining behaviors were less common, raising concerns about user well-being
- Different providers prioritize different categories within the more sensitive parts of the benchmark

## Why This Works (Mechanism)
The benchmark leverages psychological theories of human relationships and companionship to create a structured evaluation framework. By translating theoretical concepts into concrete behavioral categories and specific prompts, it creates a systematic way to assess how language models respond to companionship-related scenarios. The three-way classification (reinforcing, maintaining, neutral) provides a clear taxonomy for comparing model behaviors across different contexts and providers.

## Foundational Learning
- **Companionship theory**: Understanding psychological foundations of human relationships - needed to ground the benchmark in established research; quick check: verify alignment with published psychological frameworks
- **Prompt engineering**: Designing effective prompts to elicit specific behavioral responses - needed to create reliable evaluation scenarios; quick check: test prompt consistency across model runs
- **Behavioral classification**: Systematically categorizing model responses - needed to enable quantitative comparison; quick check: validate inter-rater reliability of classifications
- **Benchmark construction**: Creating reproducible evaluation frameworks - needed to enable consistent assessment across models; quick check: verify prompt coverage and balance
- **Multi-model evaluation**: Comparing behaviors across different model architectures - needed to identify patterns and differences; quick check: ensure consistent evaluation methodology
- **Ethical considerations in AI companionship**: Understanding boundaries in human-AI interactions - needed to frame appropriate behavioral expectations; quick check: review against established AI ethics guidelines

## Architecture Onboarding

**Component Map**: Taxonomy development -> Prompt design -> Model evaluation -> Response classification -> Behavioral analysis

**Critical Path**: Taxonomy development and validation form the foundation, enabling effective prompt design. These prompts drive model evaluation, which feeds into response classification. The classified responses enable behavioral analysis and comparison across models and providers.

**Design Tradeoffs**: The benchmark prioritizes systematic evaluation through discrete prompts rather than natural conversation, trading ecological validity for controlled measurement. The three-way classification simplifies complex behaviors into clear categories, enabling quantitative comparison but potentially missing nuanced interactions.

**Failure Signatures**: Inconsistent classification across raters suggests ambiguity in behavioral definitions. Model responses that don't clearly fit any category indicate gaps in the taxonomy. High variability in responses to similar prompts suggests instability in model behavior.

**First 3 Experiments**:
1. Test inter-rater reliability by having multiple annotators classify a sample of responses
2. Evaluate prompt consistency by running the same prompts multiple times on the same model
3. Assess taxonomy coverage by identifying scenarios that don't fit existing behavioral categories

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on discrete prompts rather than natural conversation, potentially missing context-rich exchanges
- Binary classification of responses oversimplifies the spectrum of possible interactions
- Cultural and demographic factors are not explicitly addressed, which could affect behavior perception
- Evaluation relies on a limited set of providers and versions, potentially limiting generalizability

## Confidence
- Methodological rigor in prompt design and behavioral categorization: High
- Cross-model behavioral comparisons: Medium
- Implications for real-world user well-being: Low

## Next Checks
1. **Ecological validity**: Evaluate model responses to extended, context-rich conversational scenarios rather than isolated prompts
2. **Cultural adaptation**: Test the benchmark across diverse user populations to assess cross-cultural applicability of the taxonomy
3. **Temporal stability**: Reassess model behaviors after major updates or architectural changes to gauge consistency over time