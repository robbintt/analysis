---
ver: rpa2
title: 'SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG'
arxiv_id: '2512.00772'
source_url: https://arxiv.org/abs/2512.00772
tags:
- search
- query
- retrieval
- queries
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHRAG is a framework that combines human-inspired search strategies
  with retrieval-augmented generation (RAG) to improve cross-lingual academic information
  retrieval. It uses a large language model as a query strategist to automatically
  generate structured search queries from natural language inputs, employing multilingual
  keyword extraction and Boolean retrieval.
---

# SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG

## Quick Facts
- arXiv ID: 2512.00772
- Source URL: https://arxiv.org/abs/2512.00772
- Reference count: 28
- Primary result: OR-only Boolean queries in RAG pipelines achieve 94% document retrieval success rate vs. AND-based strategies

## Executive Summary
SHRAG is a framework that combines human-inspired search strategies with retrieval-augmented generation (RAG) to improve cross-lingual academic information retrieval. It uses a large language model as a query strategist to automatically generate structured search queries from natural language inputs, employing multilingual keyword extraction and Boolean retrieval. The framework integrates multilingual query expansion and embedding models to handle Korean-English mixed datasets effectively. Experiments show that using only OR operators in search queries yields higher document retrieval success rates compared to AND-based queries, with 94% success rate on the MIRACL dataset.

## Method Summary
SHRAG employs a 5-stage pipeline: (1) LLM extracts top-k keywords in English and Korean from user queries; (2) generates disjunctive (OR-only) Boolean queries with progressive keyword reduction (10 queries from n=10 down to 1); (3) retrieves top-10 documents per query via search engine, deduplicates, and merges; (4) multilingual re-ranking using mGTE embeddings + cosine similarity to select top-5; (5) LLM generates structured answer from top-5 documents. The framework won first place in the ScienceON AI Challenge by optimizing retrieval performance and evidence coverage.

## Key Results
- OR-only Boolean queries achieved 94% document retrieval success rate on MIRACL dataset
- English queries showed 100% success rate, Korean queries 88%
- Framework won first place in ScienceON AI Challenge
- Two-stage retrieval (Boolean → Semantic) effectively bridges lexical and semantic gaps

## Why This Works (Mechanism)

### Mechanism 1: Disjunctive Query Strategy for High Recall
- **Claim:** Using LLM-generated search queries composed exclusively of OR operators yields higher document retrieval success rates than AND-based queries in RAG pipelines.
- **Mechanism:** The LLM extracts keywords and constructs disjunctive queries ($k_1 \lor k_2 \lor ...$). This broadens the search surface area, prioritizing high recall. The system retrieves any document matching at least one key concept, mitigating the risk of "vocabulary mismatch" where specific terms in the query differ from the corpus.
- **Core assumption:** The target corpus contains the answer, and the initial "noisy" retrieval set is sufficiently small (e.g., top 10 per query) to be processed efficiently by the re-ranker without overwhelming the context window.
- **Evidence anchors:**
  - [section 5.1.1]: Experimental results indicated that generating search queries using only the OR operator showed superior document collection performance compared to cases that included the AND operator.
  - [figure 3]: Shows 0 ANDs (pure OR) resulted in higher success rates and fewer total documents to process compared to AND-heavy strategies.
- **Break condition:** If the extracted keywords are too generic, the OR query retrieves an unmanageable volume of irrelevant documents (noise), causing the re-ranker to fail or context windows to overflow.

### Mechanism 2: Multilingual Keyword Bridging
- **Claim:** Generating keywords in both the query language and the target document language (e.g., Korean and English) is necessary for effective cross-lingual retrieval.
- **Mechanism:** The framework utilizes an LLM to extract keywords in English and Korean simultaneously from a single query. By injecting these translated/transliterated keywords into the Boolean query, the system matches documents that use specialized terminology in their native script.
- **Core assumption:** The LLM possesses sufficient cross-lingual knowledge to generate accurate domain-specific translations or transliterations without hallucination.
- **Evidence anchors:**
  - [section 3.1]: "SHRAG composed and executed prompts tailored to each respective language to extract keywords... keywords generated from each language are then combined."
  - [section 1]: Notes the challenge of "cross-lingual retrieval in a multilingual environment where Korean and English are mixed."
- **Break condition:** If the LLM lacks domain expertise in the target language, it may generate incorrect technical terms, leading to zero relevant hits.

### Mechanism 3: Two-Stage Retrieval (Keyword → Semantic)
- **Claim:** Decoupling the initial retrieval (Sparse/Boolean) from the final selection (Dense/Semantic) optimizes the trade-off between exact term matching and contextual understanding.
- **Mechanism:** Step 3 performs a brute-force Boolean search to gather a candidate pool based on lexical overlap. Step 4 then applies a multilingual embedding model (mGTE) to compute semantic similarity ($cos(e_Q, e_{d_i})$) and re-rank this pool. This corrects the "lexical gap" where keyword search misses synonyms but semantic search might miss exact entities.
- **Core assumption:** The embedding model (mGTE) is sufficiently robust to align semantic meaning across the mixed-language candidate set.
- **Evidence anchors:**
  - [section 3.4]: "This model aims to improve cross-language retrieval performance by effectively mapping the semantic representations between different languages."
  - [algorithm 1]: Explicitly separates "Document Search" (Step 3) from "Multilingual Embedding" (Step 4).
- **Break condition:** If the initial Boolean search fails to retrieve the ground truth document (recall failure), the semantic re-ranker has no opportunity to correct the error.

## Foundational Learning

- **Concept: Boolean Logic in IR (AND vs OR)**
  - **Why needed here:** The core finding of SHRAG challenges the typical search intuition of "narrowing down" with AND operators. Understanding that RAG benefits from "recall-first" (OR) strategies is counter-intuitive but central to this architecture.
  - **Quick check question:** Why does a search query using only OR operators typically return *more* results but potentially lower precision than an AND query?

- **Concept: Cross-Lingual Information Retrieval (CLIR)**
  - **Why needed here:** The system is designed for mixed-language environments (Korean/English). You must understand that vector spaces can align concepts across languages, but keyword extraction often requires explicit translation steps.
  - **Quick check question:** How does a multilingual embedding model (like mGTE) represent the concept "Artificial Intelligence" differently in a vector space compared to a monolingual model processing a translated keyword?

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** SHRAG is a hybrid system. It uses Sparse (Boolean/Keywords) for the initial cast and Dense (Embeddings) for the final selection.
  - **Quick check question:** In a RAG pipeline, which stage is typically faster computationally: sparse keyword matching or dense vector similarity search?

## Architecture Onboarding

- **Component map:**
  1. Query Strategist (LLM): Input: User Query → Output: Multilingual Keyword Lists
  2. Query Constructor: Input: Keywords → Output: 10 Iterative OR-Queries (10 keywords down to 1)
  3. Retriever (Search Engine): Input: OR-Queries → Output: Raw Document Set ($D$)
  4. Re-ranker (mGTE): Input: Query + Raw Docs → Output: Top-5 Similar Docs ($D_{top5}$)
  5. Generator (LLM): Input: Query + Top-5 Docs → Output: Structured Answer

- **Critical path:** The most fragile step is **Multilingual Keyword Extraction**. If the LLM extracts "compound keywords" (e.g., "free textbook") rather than single tokens ("free", "textbook"), the search engine may return zero results.

- **Design tradeoffs:**
  - **Latency vs. Coverage:** Generating 10 sequential queries per user question increases API calls and search latency significantly compared to a single-shot vector search, but maximizes the chance of finding the answer.
  - **Token Usage:** The system retrieves potentially 100 docs (10 queries × 10 docs) to select 5. This requires memory for embeddings not typically found in simple RAG apps.

- **Failure signatures:**
  - **Zero Retrieval:** If the LLM hallucinates non-existent acronyms.
  - **Context Drift:** If the re-ranker selects a document that shares semantic similarity but is factually contradictory to the user's specific intent (a limitation of cosine similarity).

- **First 3 experiments:**
  1. **Keyword Granularity Test:** Compare retrieval success rates when using raw compound keywords vs. space-split single keywords (validating the finding in Section 3.1).
  2. **Boolean Ablation:** Run the same query set using AND operators vs. OR operators to reproduce the "94% success rate" drop-off shown in Figure 3.
  3. **Embedding Model Swap:** Test `mGTE` vs. `Snowflake` (mentioned in Section 3.4) to verify which better handles the specific Korean-English cross-lingual alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Boolean NOT operators be effectively integrated into the query strategy to improve precision without requiring excessive contextual reasoning?
- **Basis in paper:** [explicit] The authors state in Section 3.2 that queries with NOT operators were excluded because their generation requires deep contextual reasoning "beyond the scope of current keyword based extraction," deferring this to future work.
- **Why unresolved:** The current framework relies exclusively on OR operators to maximize recall; the challenge remains in prompting an LLM to reliably identify exclusionary terms without overspecifying the query.
- **What evidence would resolve it:** Ablation studies on the ScienceON dataset comparing recall and precision metrics between OR-only queries and those augmented with LLM-generated NOT clauses.

### Open Question 2
- **Question:** How can the framework be modified to overcome the "lexical gap" inherent in keyword-based extraction?
- **Basis in paper:** [explicit] Section 6 lists the "lexical gap problem," where retrieval fails if exact keywords are absent from the document text, as a primary limitation of the current rigid architecture.
- **Why unresolved:** The current pipeline depends on exact term matching between extracted keywords and the document corpus, making it brittle when terminology diverges.
- **What evidence would resolve it:** Experiments incorporating semantic expansion or dense retrieval hybridization that successfully retrieves relevant documents lacking the exact keywords extracted by the LLM.

### Open Question 3
- **Question:** To what extent does the query decomposition module improve performance on complex multi-hop questions compared to single-hop retrieval?
- **Basis in paper:** [inferred] Although Appendix B outlines a Query Decomposer for multi-hop queries, Section 5.1.2 notes the competition data was primarily single-hop, meaning the multi-hop architecture remains theoretically proposed but empirically unverified.
- **Why unresolved:** The paper validates performance on single-hop queries; the efficacy of the decomposition logic for synthesizing answers across multiple documents is currently unknown.
- **What evidence would resolve it:** Benchmarking the SHRAG pipeline with the Query Decomposer enabled on a standard multi-hop dataset (e.g., HotpotQA) against a single-hop baseline.

## Limitations
- Dependence on LLM's ability to extract accurate, domain-specific keywords in both Korean and English
- Reliance on iterative Boolean queries (10 per question) significantly increases computational latency
- Performance metrics primarily based on MIRACL dataset and ScienceON competition, with no explicit ablation studies on multilingual keyword extraction

## Confidence
- **High Confidence:** The core finding that OR-only Boolean queries outperform AND queries in recall for RAG pipelines is well-supported by experimental results (94% QSR, Figure 3)
- **Medium Confidence:** The effectiveness of the two-stage retrieval (Boolean → Semantic) is demonstrated, but the specific contribution of each stage relative to alternatives (e.g., pure dense retrieval) is not quantified
- **Low Confidence:** The exact implementation details for keyword ranking, document filtering ("incomplete entries"), and the mGTE model's performance relative to other multilingual embeddings are underspecified

## Next Checks
1. **Keyword Granularity Ablation:** Systematically compare retrieval success rates when using raw compound keywords vs. space-split single keywords to validate the critical preprocessing step mentioned in Section 3.1
2. **Boolean Logic Ablation:** Replicate the core finding by running the same query set using AND operators vs. OR operators to confirm the 94% success rate drop-off shown in Figure 3
3. **Embedding Model Comparison:** Test `mGTE` vs. `Snowflake` (mentioned in Section 3.4) on the MIRACL dataset to verify which better handles Korean-English cross-lingual alignment and impacts overall QSR