---
ver: rpa2
title: A Survey on Vision-Language-Action Models for Autonomous Driving
arxiv_id: '2506.24044'
source_url: https://arxiv.org/abs/2506.24044
tags:
- arxiv
- driving
- autonomous
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first comprehensive overview of Vision-Language-Action
  (VLA) models for autonomous driving (VLA4AD), consolidating over 20 representative
  models and tracing their evolution from explanatory language overlays to reasoning-centric
  agents. The work formalizes the architectural building blocks of VLA4AD systems,
  covering multimodal inputs (visual, sensor, and language), core modules (vision
  encoders, language processors, action decoders), and output formats (low-level actions
  to trajectory planning).
---

# A Survey on Vision-Language-Action Models for Autonomous Driving

## Quick Facts
- arXiv ID: 2506.24044
- Source URL: https://arxiv.org/abs/2506.24044
- Reference count: 40
- One-line primary result: First comprehensive survey of Vision-Language-Action (VLA) models for autonomous driving (VLA4AD), covering 20+ models, 4 evolutionary stages, and 5 future research directions.

## Executive Summary
This survey provides the first comprehensive overview of Vision-Language-Action (VLA) models for autonomous driving (VLA4AD), consolidating over 20 representative models and tracing their evolution from explanatory language overlays to reasoning-centric agents. The work formalizes the architectural building blocks of VLA4AD systems, covering multimodal inputs (visual, sensor, and language), core modules (vision encoders, language processors, action decoders), and output formats (low-level actions to trajectory planning). It also reviews key datasets and benchmarks, such as nuScenes, BDD-X, Bench2Drive, and Impromptu VLA, which jointly assess driving safety, accuracy, and explanation quality. The survey identifies four evolutionary stages: Explanatory Language Models, Modular VLA4AD, End-to-End VLA4AD, and Reasoning-Augmented VLA4AD, highlighting how each stage progressively closes the loop between perception, language understanding, and control. Training paradigms include imitation learning, reinforcement learning, and multi-stage curricula, while evaluation protocols measure closed-loop driving, open-loop prediction, language competence, and robustness. Open challenges include real-time efficiency, formal verification, multimodal alignment, and multi-agent social complexity. Future directions point toward foundation-scale driving models, neuro-symbolic safety kernels, fleet-scale continual learning, standardized traffic language, and cross-modal social intelligence. The survey aims to inspire transparent, instruction-following, and socially aligned autonomous vehicles powered by integrated vision, language, and action.

## Method Summary
The survey synthesizes existing literature on VLA4AD models through systematic review of over 40 papers. It organizes models into four evolutionary stages and identifies common architectural patterns across vision encoders (e.g., CLIP), language backbones (e.g., LLaMA), and action decoders (autoregressive or diffusion-based). The methodology involves extracting training paradigms (imitation learning, reinforcement learning, multi-stage curricula), evaluation metrics (closed-loop driving, open-loop prediction, language competence), and identifying open challenges through thematic analysis of the surveyed literature.

## Key Results
- Consolidates over 20 representative VLA4AD models into a unified taxonomy
- Identifies four evolutionary stages from explanatory language overlays to reasoning-centric agents
- Highlights key datasets (nuScenes, BDD-X, Bench2Drive, Impromptu VLA) and benchmarks
- Outlines five major open challenges including real-time efficiency and formal verification
- Proposes five future research directions including foundation-scale models and neuro-symbolic safety kernels

## Why This Works (Mechanism)

### Mechanism 1: Semantic Grounding via Multimodal Alignment
- **Claim:** VLA models appear to improve generalization to long-tail scenarios by anchoring visual features to high-level semantic concepts via language, rather than relying solely on statistical patterns in driving logs.
- **Mechanism:** A pretrained Vision Encoder (e.g., CLIP) projects raw pixels into a latent space aligned with text embeddings from an LLM. This allows the model to "recognize" abstract concepts (e.g., an "ambulance" requires "yielding") even if the specific visual configuration is rare in the training data, by leveraging internet-scale linguistic priors.
- **Core assumption:** The semantic knowledge from pretraining (internet data) transfers effectively to the temporal-spatial domain of driving without losing fidelity.
- **Evidence anchors:**
  - [Section 2.4]: "By leveraging foundation models pretrained on internet-scale visual and linguistic data... VLA models demonstrate strong generalization."
  - [Section 3.2]: "Multi-scale fusion using language-derived keys improves grounding at fine spatial levels."
  - [Corpus]: The related paper "Reasoning-VLA" supports this, claiming improved generalization to novel scenarios via integrated reasoning.
- **Break condition:** If the visual encoder fails to extract distinct features (e.g., due to severe occlusion or weather), the language grounding will likely hallucinate context, leading to confident but incorrect planning.

### Mechanism 2: Reasoning-Augmented Action Generation
- **Claim:** Injecting explicit Chain-of-Thought (CoT) reasoning steps before action output appears to enhance safety verification and interpretability compared to direct "image-to-control" mapping.
- **Mechanism:** The model functions in two phases. First, the LLM generates a textual rationale describing the scene and intent (e.g., "Pedestrian crossing, I must stop"). Second, this rationale conditions the action decoder (e.g., a diffusion head) to generate a trajectory consistent with that logic. This decomposes the complex mapping into verifiable sub-steps.
- **Core assumption:** The textual rationale accurately reflects the latent decision process and is not merely a post-hoc rationalization (a known issue in LLMs).
- **Evidence anchors:**
  - [Section 4.4]: "Impromptu VLA... aligns CoT with action... learns to verbalise its decision path before acting, delivering state-of-the-art zero-shot negotiation."
  - [Section 4]: "Evolution... from explanatory language overlays to reasoning-centric agents."
  - [Corpus]: "IRL-VLA" suggests standard imitation learning captures dataset biases; CoT may help mitigate this by enforcing explicit logic.
- **Break condition:** If the reasoning step introduces computational latency exceeding the control loop budget (e.g., >100ms), the vehicle may become reactive rather than predictive, negating safety benefits.

### Mechanism 3: Hybrid Policy Optimization (IL + RL)
- **Claim:** VLA systems likely achieve robust performance by using Imitation Learning (IL) to match expert distribution and Reinforcement Learning (RL) to handle rare safety constraints and collision avoidance.
- **Mechanism:** The model is warm-started using IL on large datasets (nuScenes, BDD-X) to learn general driving smoothness. It is then fine-tuned using RL (e.g., PPO) in simulation (CARLA), where a reward function penalizes infractions that are rare in logs but critical for safety (e.g., hitting a jaywalking pedestrian).
- **Core assumption:** The simulation environment (e.g., CARLA/Bench2Drive) is realistic enough that the learned safety behaviors transfer to the real world without "sim-to-real" gaps.
- **Evidence anchors:**
  - [Section 6.1]: "RL is usually layered on top of an IL warm-start... optimised with PPO or DQN style updates for route progress, collision avoidance."
  - [Table 1]: Shows models like "MindDrive" and "WAM-Diff" explicitly utilizing online RL.
  - [Corpus]: "MindDrive" confirms that IL suffers from causal confusion and proposes RL as a solution.
- **Break condition:** If the reward function is poorly specified, the agent may learn "reward hacking" behaviors (e.g., driving in circles to avoid collisions) rather than useful driving.

## Foundational Learning

- **Concept: Bird's-Eye-View (BEV) Perception**
  - **Why needed here:** The survey emphasizes that modern VLA inputs are evolving from single front-facing cameras to "multi-view" and "BEV maps" (Section 3.1). Understanding how 2D images are projected into a top-down 3D space is crucial for grasping how the model reasons about distances and spatial relationships.
  - **Quick check question:** Can you explain why projecting multiple camera views into a unified BEV feature map helps in resolving occlusions for trajectory planning?

- **Concept: Action Tokenization vs. Diffusion**
  - **Why needed here:** Section 3.2 describes Action Decoders as either "Autoregressive tokenizers" or "Diffusion heads." Without understanding these, one cannot evaluate the tradeoff between discrete (tokenized) and continuous (diffusion) control representations.
  - **Quick check question:** How does a diffusion-based planner handle multi-modal predictions (e.g., turning left vs. going straight) differently than a regression-based planner?

- **Concept: Multimodal Pretraining (VLM)**
  - **Why needed here:** The core capability of VLA4AD comes from "pretrained decoders" and "foundation models" (Section 3.2). You need to understand how contrastive learning (like CLIP) binds text and images to see why these models can follow instructions like "turn left at the church."
  - **Quick check question:** What is the "alignment" problem in VLMs, and why does freezing the LLM backbone during fine-tuning (using LoRA) help preserve linguistic fluency?

## Architecture Onboarding

- **Component map:** Multi-view Cameras + LiDAR (optional) + Text Prompt (Goal) -> Vision Encoder (ViT/CLIP) -> BEV Projection -> LLM/VLM Backbone (LLaMA/Vicuna) -> Generates Plan/Rationale -> Action Decoder (Diffusion/MLP) -> Waypoints -> PID Controller

- **Critical path:** The bottleneck is typically the Vision-to-Language Adapter and the LLM Inference. The model must process high-resolution images, project them to tokens, and run an LLM forward pass within the control frequency (ideally 10-30 Hz).

- **Design tradeoffs:**
  - **Unified vs. Modular:** End-to-end VLA (Section 4.3) offers better integration but is harder to debug than Modular VLA (Section 4.2) where the "language planner" is distinct from the controller.
  - **Latency vs. Reasoning:** Heavy Chain-of-Thought (CoT) improves decision quality but risks violating real-time constraints; token reduction (TS-VLM) is often required (Section 4.1).

- **Failure signatures:**
  - **Hallucination:** The LLM describes a hazard (e.g., "red light") that does not exist visually, causing a panic stop.
  - **Language-Motor Misalignment:** The model produces the correct text rationale ("I should turn left") but outputs a trajectory turning right.
  - **Reactive Oscillation:** In closed-loop testing, the car wobbles or leaves the lane because the LLM's discrete token updates cause jerky control signals.

- **First 3 experiments:**
  1. **Inference Latency Baseline:** Measure the forward pass time of a baseline VLA (e.g., DriveGPT-4 or LLaVA-AD) on a single GPU. Determine if it meets the 10Hz minimum for stable control without token reduction.
  2. **Closed-Loop Instruction Following:** Deploy a VLA in a CARLA simulator. Give it a conflicting instruction (e.g., "Go straight" at a red light). Evaluate if the "Safety Layer" (Section 7, Neuro-symbolic) correctly overrides the language command.
  3. **Modality Ablation:** Run the VLA on a corner-case dataset (e.g., Impromptu VLA) with the language prompt removed vs. included. Quantify the improvement in collision rate attributable purely to the language grounding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can VLA models be optimized to achieve the necessary $\geq 30$ Hz inference frequency on automotive-grade hardware without compromising reasoning capability?
- **Basis in paper:** [explicit] The authors state that running a vision transformer plus LLM at "≥30 Hz on automotive hardware is non-trivial" and cite token-reduction and MoE sparsity as current but incomplete solutions.
- **Why unresolved:** Current end-to-end VLA stacks often exceed hundreds of GFLOPs per frame, relying on distillation or event-triggered reasoning which may miss rapidly evolving hazards.
- **What evidence would resolve it:** A VLA model demonstrating state-of-the-art driving performance and reasoning accuracy while operating at 30 Hz on standard embedded automotive chips (e.g., Orin).

### Open Question 2
- **Question:** What is the effective architecture for a principled, temporally consistent fusion of heterogeneous modalities (LiDAR, Radar, HD-maps) beyond the current camera-centric approaches?
- **Basis in paper:** [explicit] The survey notes that "A principled, temporally consistent fusion of heterogeneous modalities is still missing," as current work is largely camera-centric with only partial fusion of other sensors.
- **Why unresolved:** Integrating sparse point clouds and temporal state history with language tokens remains computationally expensive and lacks a unified representation standard.
- **What evidence would resolve it:** A model architecture that robustly weights LiDAR and map data relative to vision/language inputs in adverse conditions (e.g., fog, glare) with quantifiable improvements in temporal consistency.

### Open Question 3
- **Question:** How can neural VLA outputs be bridged with symbolic safety kernels to provide formal verification for language-conditioned policies?
- **Basis in paper:** [explicit] The paper highlights that "formal verification and 'socially-compliant' driving policies remain largely unexplored" and suggests future neuro-symbolic hybrids where a VLA outputs a structured plan for a symbolic verifier.
- **Why unresolved:** The probabilistic nature of LLMs conflicts with the deterministic requirements of formal verification logic, creating a "semantic gap" between generated text/action and certifiable safety.
- **What evidence would resolve it:** A framework where a VLA generates a Chain-of-Thought plan that is mathematically proven to satisfy specific traffic rules before execution.

### Open Question 4
- **Question:** How should driving performance rewards be mathematically balanced with language fidelity losses in Reinforcement Learning paradigms?
- **Basis in paper:** [explicit] The authors identify that "A key open question is how to blend driving rewards with language fidelity," noting that current work often freezes the LLM to avoid unsafe actions, leaving joint gradients unexplored.
- **Why unresolved:** Optimizing for control safety often degrades the model's ability to follow nuanced instructions, while optimizing for instruction following can lead to hallucinations or unsafe control signals.
- **What evidence would resolve it:** A training loss function that simultaneously minimizes collision rates and maximizes command-following accuracy without requiring the freezing of model weights.

## Limitations
- Architectural synthesis relies on aggregating findings across heterogeneous models without providing implementation-level details (e.g., exact loss hyperparameters, token vocabularies, or training schedules)
- Evaluation of "reasoning" capabilities is largely qualitative, based on textual rationale quality rather than formal verification of causal correctness
- Reported generalization benefits from language grounding are inferred from ablation studies on benchmark datasets, but underlying distributional overlap between internet-scale pretraining and rare driving scenarios remains unquantified

## Confidence
- **High Confidence**: The evolutionary stages of VLA4AD (Explanatory → Modular → End-to-End → Reasoning-Augmented) are well-supported by the surveyed literature and represent a consensus view of architectural progression
- **Medium Confidence**: The claim that language grounding improves generalization to long-tail scenarios is plausible but depends on the strength of semantic alignment between pretraining and driving domains; evidence is indirect (benchmark performance) rather than causal
- **Medium Confidence**: The hybrid IL+RL training paradigm is standard and effective, but the survey does not provide empirical comparisons showing RL specifically improves safety beyond IL on rare events
- **Low Confidence**: The assertion that explicit Chain-of-Thought reasoning enhances safety verification is based on qualitative examples; the risk of post-hoc rationalization by the LLM is not addressed with quantitative measures

## Next Checks
1. **Grounding Fidelity Test**: Conduct a controlled experiment where a VLA4AD model is evaluated on a curated dataset containing both real and synthetic visual artifacts (e.g., hallucinated stop signs). Measure the frequency and severity of textual hallucinations that lead to unsafe actions, and compare against a baseline without language grounding.

2. **Reasoning-to-Action Causal Trace**: Implement a formal validation pipeline that checks whether the LLM's Chain-of-Thought rationale causally leads to the executed action. This could involve: (a) masking parts of the reasoning text and measuring degradation in action quality, or (b) using a symbolic verifier to check if the textual logic implies the geometric trajectory.

3. **Sim-to-Real Transfer of Safety Behaviors**: Design an RL fine-tuning experiment in CARLA where the reward function explicitly penalizes rare but critical safety violations (e.g., hitting a pedestrian at night). After training, deploy the model in a real-world or high-fidelity simulator with novel rare-event scenarios to measure if the learned safety behaviors transfer, or if they overfit to the simulation's limitations.