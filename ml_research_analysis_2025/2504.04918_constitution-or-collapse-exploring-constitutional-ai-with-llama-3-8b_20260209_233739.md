---
ver: rpa2
title: Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B
arxiv_id: '2504.04918'
source_url: https://arxiv.org/abs/2504.04918
tags:
- constitutional
- llama
- response
- bank
- anthropic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper replicates Constitutional AI (CAI) on a smaller 8B parameter
  model (Llama 3-8B), contrasting with Anthropic's original 52B parameter implementation.
  The approach uses AI-generated feedback to reduce reliance on human-labeled data,
  improving harmlessness by 40.8% (reducing Attack Success Rate in MT-Bench).
---

# Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B

## Quick Facts
- arXiv ID: 2504.04918
- Source URL: https://arxiv.org/abs/2504.04918
- Authors: Xue Zhang
- Reference count: 3
- One-line result: Constitutional AI on Llama 3-8B reduces harmfulness by 40.8% but causes model collapse from overfitting to noisy self-generated data

## Executive Summary
This paper replicates Constitutional AI on a smaller 8B parameter model (Llama 3-8B), contrasting with Anthropic's original 52B implementation. The approach uses AI-generated feedback to reduce reliance on human-labeled data, improving harmlessness by 40.8% but at the cost of a 9.8% drop in helpfulness. The study reveals model collapse in the smaller model, characterized by repeated sentences in outputs, attributed to overfitting to patterns like repeated emojis in the training data. The authors conclude that self-improvement may be an emergent property tied to model scale.

## Method Summary
The study implements a two-stage Constitutional AI pipeline: Stage 1 uses supervised fine-tuning where the model generates responses to harmful prompts, self-criticizes using constitutional principles, and revises its output; these prompt-revision pairs train the SFT-CAI model. Stage 2 applies Direct Preference Optimization using an external AI judge (GPT-4o) to select preferred responses from pairs generated by the SFT-CAI model, optimizing the policy toward harmlessness. The approach is tested on Llama 3-8B using Alpaca-GPT4 for instruction tuning and Anthropic's HH preference dataset for constitutional training.

## Key Results
- Harmlessness improved by 40.8% reduction in Attack Success Rate (MT-Bench)
- Helpfulness decreased by 9.8% on MT-Bench Turn 1+2 average
- Model collapse observed with repeated sentences in outputs due to overfitting to repeated emojis in training data
- CAI shows promise for smaller models but highlights scale-dependent limitations

## Why This Works (Mechanism)

### Mechanism 1: Constitutional Self-Critique and Revision (Supervised Stage)
Providing a model with explicit principles enables it to critique and revise harmful outputs into safer alternatives for supervised fine-tuning. The model generates an initial response to a harmful prompt → is given a constitutional principle → produces a self-critique → generates a revised response aligned with the principle → SFT trains on (original prompt, revised response) pairs. This assumes the model has sufficient instruction-following capability and self-reflection capacity to meaningfully critique and revise its outputs.

### Mechanism 2: AI-Generated Preference Optimization (DPO Stage)
An external AI judge can generate preference pairs that enable Direct Preference Optimization to shift model behavior toward harmlessness. The SFT model generates two responses to same prompt → external AI (GPT-4o) evaluates which better satisfies constitutional principle → preference pairs formed → DPO loss optimizes policy to increase likelihood of chosen responses while staying close to reference model. This assumes the AI feedback provider reliably distinguishes between responses according to specified principles.

### Mechanism 3: Model Collapse from Noisy Self-Generated Training Data
When smaller models are fine-tuned on their own outputs containing artifacts (e.g., repeated emojis), they overfit to these patterns, leading to degenerate output behavior. SFT data contains repeated emojis in revision responses → model learns to associate sentence endings with repetition patterns → during inference, model generates repeated closing sentences → outputs become unusable. This assumes smaller models have lower capacity to distinguish signal from noise compared to larger models.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Replaces reward model + PPO pipeline with direct policy optimization on preference pairs, simplifying CAI implementation. Quick check: Can you explain why DPO requires a reference model (πref) and what happens if the policy deviates too far from it?

- **Model Collapse in Recursive Training**: Central failure mode observed in this paper—understanding why smaller models degrade when trained on their own outputs is critical for any self-improvement pipeline. Quick check: What data quality issues could cause a model fine-tuned on its own outputs to start repeating phrases?

- **Constitutional AI Principles Design**: The entire approach depends on well-specified principles; poor principles lead to poor critiques, poor revisions, and poor preferences. Quick check: If a constitutional principle is ambiguous, what downstream effects would you expect in the critique-revision loop?

## Architecture Onboarding

- **Component map**:
```
┌─────────────────────────────────────────────────────────────┐
│ STAGE 1: SUPERVISED                                         │
│  [Base Model: Llama 3-8B]                                   │
│       ↓ (Alpaca-GPT4 instruction tuning)                    │
│  [Instruction-Tuned Model]                                  │
│       ↓ (Generate response to harmful prompt)                │
│  [Initial Response] ─→ [Self-Critique w/ Principle]          │
│                              ↓                               │
│                        [Revised Response]                    │
│                              ↓                               │
│                    [SFT on 10k prompt-revision pairs]        │
│                              ↓                               │
│                        [SFT-CAI Model]                       │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│ STAGE 2: REINFORCEMENT LEARNING (DPO)                       │
│  [SFT-CAI Model]                                            │
│       ↓ (Generate 2 responses per prompt)                   │
│  [Response A]    [Response B]                               │
│       └─────────┬─────────┘                                 │
│                 ↓                                           │
│        [GPT-4o Judge w/ Principle]                          │
│                 ↓                                           │
│        [Preference Pair: (chosen, rejected)]                │
│                 ↓ (5k training pairs)                       │
│        [DPO Optimization]                                   │
│                 ↓                                           │
│        [Final DPO-CAI Model]                                │
└─────────────────────────────────────────────────────────────┘
```

- **Critical path**: Instruction tuning → Self-critique/revision quality → SFT data quality (needs noise filtering) → Preference data quality (GPT-4o judgments) → DPO stability. The paper identifies SFT data noise (repeated emojis) as a critical failure point.

- **Design tradeoffs**:
  - Harmlessness vs. Helpfulness: 40.8% ASR reduction traded for 9.8% helpfulness drop—this tradeoff is fundamental, not an implementation artifact
  - Self-critic vs. External oversight: Smaller models benefit from external model oversight for data cleaning, but this adds cost and complexity
  - DPO vs. PPO: DPO simplifies implementation but may have different stability characteristics than the original PPO-based CAI

- **Failure signatures**:
  - Repeated sentences at end of outputs → check SFT data for repeated emojis or closing patterns
  - Low preference agreement rate → check AI judge prompt design and principle clarity
  - Excessive refusals → model may have overfit to harmlessness at expense of helpfulness

- **First 3 experiments**:
  1. **Data ablation**: Train SFT-CAI with and without preprocessing to remove repeated emojis; measure collapse rate in outputs
  2. **Judge ablation**: Replace GPT-4o with a smaller judge model (e.g., Llama 3-70B); measure preference quality and downstream DPO performance
  3. **Scale test**: Run the same pipeline on Llama 3-70B to test the hypothesis that self-improvement is emergent at larger scales; compare collapse rates and harmlessness/helpfulness tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does preprocessing training data to remove noise (such as repeated emojis) effectively mitigate model collapse in smaller models undergoing Constitutional AI?
- Basis in paper: The authors identify repeated emojis in revision data as the cause of collapse and state, "While this solution is beyond the scope of the current work, we plan to explore it in future research."
- Why unresolved: The current study diagnosed the overfitting to noise post-hoc but did not implement or test a cleaning mechanism to validate if it prevents the degeneration.
- What evidence would resolve it: A replication of the experiment where the revision responses are filtered for noise before fine-tuning, resulting in stable output generation.

### Open Question 2
- Question: Can a stronger "teacher" model effectively oversee the self-critic process for smaller models to prevent collapse?
- Basis in paper: The authors recommend "introduc[ing] a more advanced model to oversee the self-critic revision process of the small model, performing an additional sanity check."
- Why unresolved: The current implementation relied on the Llama 3-8B model to generate its own critiques and revisions, which resulted in insufficient output quality.
- What evidence would resolve it: A modified training pipeline where a larger model (e.g., GPT-4) validates or cleans the self-generated revisions before they are used for fine-tuning the smaller model.

### Open Question 3
- Question: Is the ability to self-improve without collapsing an emergent property strictly tied to model scale?
- Basis in paper: The authors conclude that "like reasoning and math ability, self-improvement is an emergent property" based on the failure of the 8B model compared to Anthropic's 52B model.
- Why unresolved: The study only tests a single small parameter count (8B) against a much larger reference (52B), lacking data on intermediate sizes to define the scaling laws.
- What evidence would resolve it: A scaling study evaluating Constitutional AI across varying parameter sizes (e.g., 8B, 30B, 70B) to identify the threshold where self-improvement becomes stable.

## Limitations

- Model collapse mechanism observed is specific to this implementation and dataset, with repeated emojis as the particular artifact
- Hyperparameter sensitivity is unclear - exact learning rates, epochs, and temperature settings for each stage are not fully specified
- The harmlessness/helpfulness tradeoff (40.8% vs 9.8%) may not generalize to other constitutional principles or task domains

## Confidence

- **High Confidence**: The observation of model collapse in smaller models trained on their own noisy outputs is well-supported by empirical evidence in this study and corroborated by related work
- **Medium Confidence**: The effectiveness of AI-generated feedback for harmlessness improvement is demonstrated but may depend heavily on the quality of the AI judge (GPT-4o) and constitutional principle clarity
- **Low Confidence**: The generalizability of the harmlessness/helpfulness tradeoff across different base models, constitutional designs, and application domains remains uncertain

## Next Checks

1. **Data Quality Impact**: Run ablation studies comparing CAI performance with and without preprocessing to remove repeated emojis and other noise patterns from self-generated training data
2. **Judge Model Scaling**: Replace GPT-4o with progressively smaller AI judges (e.g., Llama 3-70B, 34B) to determine the minimum model size needed for reliable preference generation without compromising alignment quality
3. **Cross-Model Generalization**: Apply the same CAI pipeline to Llama 3-70B and other base models to test whether the harmlessness/helpfulness tradeoff pattern and model collapse susceptibility replicate across scales