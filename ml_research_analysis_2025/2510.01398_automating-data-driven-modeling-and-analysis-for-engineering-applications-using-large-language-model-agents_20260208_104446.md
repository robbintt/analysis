---
ver: rpa2
title: Automating Data-Driven Modeling and Analysis for Engineering Applications using
  Large Language Model Agents
arxiv_id: '2510.01398'
source_url: https://arxiv.org/abs/2510.01398
tags:
- agent
- agents
- data
- engineering
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an automated pipeline using Large Language
  Model (LLM) agents to perform end-to-end data-driven modeling and analysis for engineering
  applications, focusing on regression tasks. Two frameworks are evaluated: a multi-agent
  system with specialized roles and a single ReAct (Reasoning and Acting)-based agent.'
---

# Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents

## Quick Facts
- arXiv ID: 2510.01398
- Source URL: https://arxiv.org/abs/2510.01398
- Reference count: 40
- Primary result: LLM agents automate end-to-end data-driven modeling with accuracy matching human-developed deep ensembles

## Executive Summary
This paper introduces an automated pipeline using Large Language Model (LLM) agents to perform end-to-end data-driven modeling and analysis for engineering applications, focusing on regression tasks. Two frameworks are evaluated: a multi-agent system with specialized roles and a single ReAct (Reasoning and Acting)-based agent. Both autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). The approach is validated using a critical heat flux (CHF) prediction benchmark with approximately 25,000 experimental data points. Results show that LLM-agent-developed models surpass traditional CHF lookup tables and achieve predictive accuracy and UQ comparable to state-of-the-art human-developed Bayesian optimized deep neural networks, demonstrating the potential of LLM-based agents to automate complex engineering modeling tasks with minimal human intervention.

## Method Summary
The methodology employs LLM agents to automate the complete workflow of data-driven engineering modeling. Two agent architectures are implemented: a multi-agent system with specialized Supervisor, Coding, and Tuning agents, and a single ReAct agent using reasoning-action loops. Both frameworks use deep ensemble models with negative log-likelihood loss to output mean and variance predictions, enabling uncertainty quantification through aleatory and epistemic decomposition. The system is validated on a critical heat flux prediction task using an OECD/NEA benchmark dataset of ~24,579 experimental points. Hyperparameter optimization is performed via Bayesian optimization with search spaces covering learning rate, batch size, network architecture, and activation functions.

## Key Results
- Multi-agent framework reduces computational overhead by 68% (11,287 vs 35,311 average tokens) while achieving 7/10 error-free completions versus 6/10 for ReAct agent
- LLM-agent-developed deep ensembles achieve RMSE of 228.9-234.4, matching or exceeding human-expert Bayesian optimized models (RMSE 228.9)
- Deep ensemble uncertainty quantification provides interpretable aleatory and epistemic components, with both types showing comparable magnitudes in the CHF prediction task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving reasoning with tool execution enables dynamic error recovery in single-agent systems.
- Mechanism: The ReAct paradigm structures the agent's behavior into iterative Thought→Action→Observation cycles. When an action fails, the error becomes an Observation, which informs the next Thought, allowing the agent to diagnose and patch its own generated code.
- Core assumption: The LLM can correctly interpret error messages and generate syntactically valid fixes without domain-specific training.
- Evidence anchors:
  - [abstract]: "single-agent system based on the Reasoning and Acting (ReAct) paradigm"
  - [Section 2.3]: "This step synthesizes a concise summary of the outcome...informing the subsequent 'Thought' step. A key feature...is its capacity for self-correction."
  - [corpus]: Limited direct corpus evidence for ReAct in engineering; "Automating modeling in mechanics" paper (FMR=0.54) demonstrates LLMs designing physics-constrained networks but uses different agent architecture.
- Break condition: Errors requiring domain knowledge absent from LLM pretraining; ambiguous error messages that prevent accurate diagnosis.

### Mechanism 2
- Claim: Supervisor-centric multi-agent architecture reduces computational overhead and improves reliability through role specialization.
- Mechanism: A central Supervisor Agent manages state, orchestrates task delegation, and routes all information through a hub-and-spoke topology. Specialized Coding, Tuning, and Execution agents operate on narrowly scoped tasks with dynamically generated prompts containing precise file paths.
- Core assumption: The modeling workflow decomposes cleanly into sequential phases with well-defined handoff points.
- Evidence anchors:
  - [Section 2.2]: "all task delegations, results, and error logs are routed through the Supervisor. This design prevents information silos."
  - [Table 6]: Multi-agent uses 11,287 average tokens vs. 35,311 for ReAct (68% reduction); achieves 7/10 error-free completions vs. 6/10.
  - [corpus]: "AI Agents in Engineering Design: A Multi-Agent Framework" (FMR=0.53) similarly proposes specialized design agents for automotive engineering.
- Break condition: Subtasks with complex interdependencies requiring iterative cross-phase refinement; supervisor becoming bottleneck in highly parallel workflows.

### Mechanism 3
- Claim: Deep ensembles decompose predictive uncertainty into interpretable aleatory and epistemic components suitable for risk-informed engineering decisions.
- Mechanism: M independently trained networks each output both mean μ_m(x) and variance σ²_m(x). Law of total variance separates aleatory uncertainty (average of individual variances) from epistemic uncertainty (variance of means across ensemble).
- Core assumption: Target variable is conditionally Gaussian; random weight initialization produces sufficient diversity for meaningful epistemic uncertainty.
- Evidence anchors:
  - [Section 2.1, Eq. 6]: Explicit decomposition showing aleatory and epistemic terms.
  - [Figure 6 description]: "epistemic uncertainty is observed to be of a comparable magnitude to the aleatory uncertainty...both data noise and data sparsity contributes."
  - [corpus]: "Bayesian-optimised, feature-augmented deep ensemble" (referenced in paper) provides precedent for ensemble CHF prediction.
- Break condition: Multimodal target distributions; ensemble collapse where members converge to near-identical solutions.

## Foundational Learning

- Concept: **ReAct prompting paradigm**
  - Why needed here: Single-agent system's self-correction capability depends on understanding how Thoughts, Actions, and Observations form a reasoning loop.
  - Quick check question: Why does explicitly generating a "Thought" before an "Action" improve error recovery compared to direct code generation?

- Concept: **Deep ensemble uncertainty quantification**
  - Why needed here: Paper's engineering contribution requires interpreting both prediction accuracy and uncertainty bounds for safety-critical applications.
  - Quick check question: If all ensemble members produce nearly identical predictions but with high variance, what type of uncertainty dominates?

- Concept: **Bayesian hyperparameter optimization**
  - Why needed here: Human-expert baseline uses BO; understanding this gap contextualizes agent performance (RMSE 228.9 vs. 230.2–234.4).
  - Quick check question: What role does the acquisition function play in balancing exploration of new hyperparameter regions vs. exploitation of known good configurations?

## Architecture Onboarding

- Component map:
  - Supervisor Agent (O3) -> Multi-Agent System -> OpenAI Agents SDK -> Sandboxed Python environment -> PyTorch
  - Supervisor Agent (O3) -> ReAct Agent (GPT-4.1) -> OpenAI Agents SDK -> Sandboxed Python environment -> PyTorch

- Critical path:
  Data ingestion → Model generation → Training script → Training execution → Evaluation script → Evaluation execution → Report. Self-correction loop (generate→execute→tune→re-execute) activates on any execution failure.

- Design tradeoffs:
  - **Multi-agent vs. ReAct**: Multi-agent = lower tokens, higher reliability; ReAct = more adaptive for unforeseen errors.
  - **Specialized tools vs. general interpreter**: Constrained toolset (generate_model, execute_python_script) trades flexibility for predictability.
  - **Hub-and-spoke vs. peer-to-peer**: Supervisor-centric design prevents silos but creates orchestration bottleneck.

- Failure signatures:
  - **FileNotFoundError**: Incorrect relative paths in generated scripts; resolution via ProjectContext absolute paths.
  - **ValueError in plotting**: 2D arrays passed to matplotlib functions expecting 1D; resolution via `.squeeze()`.
  - **Cascading errors**: ReAct showed 1/10 trials with ≥2 errors, indicating susceptibility without specialized tuning agent.

- First 3 experiments:
  1. **Sanity check on toy dataset**: Run multi-agent pipeline on sklearn California Housing (20k samples, 8 features) to verify end-to-end workflow before domain deployment.
  2. **Token efficiency replication**: Execute both architectures on identical CHF subset (5k samples), logging token usage per phase to reproduce 68% efficiency gap.
  3. **Controlled error injection**: Introduce deliberate bugs (wrong path, shape mismatch) into generated scripts to verify self-correction loop triggers correctly and measure recovery iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Retrieval-Augmented Generation (RAG) effectively enforce physical constraints and conservation laws within LLM-generated engineering models?
- Basis in paper: [explicit] The authors state that agents currently lack domain-specific knowledge to implement physical constraints and identify RAG as a specific future approach to integrate vectorized knowledge databases.
- Why unresolved: The current study relies on data-driven learning without embedded physics, risking predictions that violate fundamental engineering principles.
- What evidence would resolve it: A demonstration of an RAG-augmented agent autonomously rejecting model architectures or predictions that violate specified thermodynamic conservation laws.

### Open Question 2
- Question: Does the relative efficiency of the multi-agent system versus the ReAct-agent persist when applied to non-regression engineering tasks?
- Basis in paper: [inferred] The methodology and validation are restricted to regression tasks (CHF prediction), leaving the generalizability of the observed token-efficiency and robustness trade-offs unknown.
- Why unresolved: Different tasks (e.g., classification, time-series forecasting) may require distinct reasoning strategies that favor the flexible ReAct loop over the structured multi-agent pipeline.
- What evidence would resolve it: Benchmarking both frameworks on a standardized engineering classification or transient analysis task to compare token consumption and error recovery rates.

### Open Question 3
- Question: Can open-source, locally hosted LLMs achieve comparable reliability in self-correction and error recovery to the proprietary models used in this study?
- Basis in paper: [inferred] While the authors claim the framework applies to any LLM, the validation relies exclusively on high capability proprietary models (GPT-4.1, O3).
- Why unresolved: The self-correction mechanisms (debugging loops) depend heavily on the LLM's reasoning capacity, which may degrade significantly in smaller, open-source models.
- What evidence would resolve it: Replicating the automated CHF modeling workflow using a leading open-source model (e.g., Llama 3) and measuring the "completed without error" rate.

## Limitations
- The specific agent prompts are not provided, preventing exact replication of the methodology
- The ReAct agent's self-correction reliability is unverified on more complex failure modes beyond the 6/10 error-free rate observed
- Claims of "comparable" uncertainty quantification to human-expert baselines require careful interpretation as the paper shows both uncertainty types are present but doesn't establish equivalence in all risk scenarios

## Confidence

- **High**: Automated pipeline successfully performs data preprocessing, model development, training, and evaluation
- **Medium**: Agent-developed models achieve comparable accuracy to human-expert baselines (RMSE 228.9-234.4 vs 228.9)
- **Medium**: Deep ensemble uncertainty decomposition provides interpretable aleatory and epistemic components
- **Low**: ReAct agent's self-correction reliably handles all domain-specific error types

## Next Checks

1. Implement controlled error injection tests to measure ReAct agent's recovery iterations across different failure types
2. Benchmark computational efficiency on larger datasets to verify token reduction scales proportionally
3. Conduct ablation studies on ensemble size (M) to determine minimum configuration achieving reliable uncertainty quantification