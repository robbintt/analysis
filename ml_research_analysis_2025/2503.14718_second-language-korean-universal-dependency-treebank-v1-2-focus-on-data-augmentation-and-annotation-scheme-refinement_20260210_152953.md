---
ver: rpa2
title: 'Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation
  and annotation scheme refinement'
arxiv_id: '2503.14718'
source_url: https://arxiv.org/abs/2503.14718
tags:
- korean
- language
- annotation
- data
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study expanded an L2-Korean Universal Dependencies treebank\
  \ from 7,530 to 12,984 sentences and revised annotation guidelines to better align\
  \ with UD standards. Three Korean language models\u2014Stanza, spaCy, and Trankit\u2014\
  were fine-tuned on the enhanced dataset and evaluated on both in-domain and out-of-domain\
  \ L2-Korean data."
---

# Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement

## Quick Facts
- arXiv ID: 2503.14718
- Source URL: https://arxiv.org/abs/2503.14718
- Reference count: 8
- Expanded L2-Korean UD treebank from 7,530 to 12,984 sentences with annotation refinement

## Executive Summary
This study significantly expanded and refined the L2-Korean Universal Dependencies treebank, increasing sentence count from 7,530 to 12,984 while revising annotation guidelines to better align with UD standards. Three Korean language models—Stanza, spaCy, and Trankit—were fine-tuned on this enhanced dataset and evaluated on both in-domain and out-of-domain L2-Korean data. Trankit achieved the highest performance across multiple metrics, demonstrating the effectiveness of transformer-based architectures and the importance of L2-specific training data for morphosyntactic analysis.

## Method Summary
The authors expanded the L2-Korean UD treebank v1.2 to 12,984 sentences (from 7,530) and refined annotation schemes to strictly follow UD's left-to-right head assignment rules. They fine-tuned three models—Stanza (biLSTM), spaCy (tok2vec), and Trankit (transformer/XLM-RoBERTa)—using default hyperparameters on the training set (9,649 sentences), evaluating on in-domain (1,205 sentences) and out-of-domain (922 sentences from KoLLA dataset) test sets. Performance was measured using F1 scores for XPOS, LEMMA, UAS, and LAS metrics.

## Key Results
- Trankit achieved highest performance: 91.81 F1 (XPOS), 92.28 UAS, 89.13 LAS on in-domain data
- Stanza excelled at lemmatization: 95.64 F1 (in-domain) and 91.01 F1 (out-of-domain)
- Fine-tuning significantly improved performance across all models and metrics compared to baseline
- Out-of-domain performance showed substantial drops, highlighting domain adaptation challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning general-purpose L1-based language models on L2-specific data significantly improves morphosyntactic analysis performance.
- Mechanism: Pre-trained language models encode generalized linguistic patterns from predominantly L1 data. When fine-tuned on L2-specific treebanks, these models adapt their weights to the unique error patterns, simplified structures, and non-target-like usage characteristic of learner language. The transformer-based architecture in Trankit (XLM-RoBERTa) captures long-range dependencies and complex syntactic structures more effectively than the BiLSTM architecture in Stanza.
- Core assumption: L2 learner language exhibits systematic patterns that differ from L1 production but are learnable given sufficient labeled examples.
- Evidence anchors:
  - [abstract] "The results show that fine-tuning significantly improves their performance across various metrics, thus highlighting the importance of using well-tailored L2 datasets for fine-tuning first-language-based, general-purpose language models for the morphosyntactic analysis of L2 data."
  - [section 4.4, Table 2] Fine-tuned Trankit achieves 89.13 LAS on in-domain data compared to baseline 60.69 LAS, and 85.45 LAS on out-of-domain data compared to 58.53 LAS.
  - [corpus] Related paper "UD-KSL Treebank v1.3" extends this work, suggesting continued success with the approach.

### Mechanism 2
- Claim: Strict adherence to the Universal Dependencies left-to-right head assignment rule enhances cross-linguistic consistency and model performance on dependency parsing.
- Mechanism: The UD framework enforces consistent left-to-right head structure for coordination (`conj`), flat structures (`flat`), and other relations. By revising the annotation scheme to strictly follow this rule, the model learns a more uniform syntactic representation, reducing the learning burden for the parser by eliminating language-specific right-headed exceptions.
- Core assumption: A more consistent annotation scheme, even if it contradicts some language-specific linguistic intuitions, leads to better model generalization by reducing rule ambiguity.
- Evidence anchors:
  - [section 3.1.1] "The UD framework enforces a strict left-to-right rule for coordination to ensure consistency and cross-linguistic applicability in morphosyntactic annotations... we revised the previous approach to strictly follow the left-to-right head structure."
  - [section 5] "We expanded the L2-Korean UD treebank with refined annotation schemes to improve model performance after fine-tuning."

### Mechanism 3
- Claim: Character-based seq2seq models augmented with dictionary-based resources achieve superior lemmatization performance on L2 data.
- Mechanism: Stanza's superior lemmatization (95.64 F1 in-domain, 91.01 out-of-domain) over Trankit (88.84, 86.90) stems from its dictionary-based lemmatizer alongside the character-based seq2seq model. This hybrid approach leverages both learned morphological patterns and explicit lexical knowledge. Stanza's pre-training on L1 UD-Korean GSD data provides a stronger foundation for vocabulary-dependent lemmatization.
- Core assumption: L2 learner errors often involve incorrect morphology on known lexical items, which a dictionary-based approach can correct by mapping non-standard forms to canonical lemmas.
- Evidence anchors:
  - [section 4.4, Table 2] Stanza achieves best LEMMA scores in both in-domain (95.64) and out-of-domain (91.01) tests.
  - [section 5] "Stanza's superior lemmatization performance compared to Trankit can be attributed to two primary factors. First, Stanza includes a dictionary-based lemmatizer... Second, Stanza uniquely leverages a model that was pretrained on L1 data (UD-Korean GSD)."

## Foundational Learning

- Concept: **Universal Dependencies (UD) Framework**
  - Why needed here: The entire treebank and evaluation are built upon UD standards. Understanding annotation schemes (UPOS, XPOS, DEPREL tags like `nsubj`, `obj`, `conj`, `flat`) is essential for interpreting results and model outputs.
  - Quick check question: What key change did authors make to align with UD standards, and how does it affect head assignment in a phrase like "apples and bananas"?

- Concept: **Dependency Parsing Metrics (UAS & LAS)**
  - Why needed here: These are primary evaluation metrics. Unlabeled Attachment Score (UAS) measures correctness of head-dependent relationships; Labeled Attachment Score (LAS) also requires correct dependency labels.
  - Quick check question: If a model correctly identifies "banana" depends on "apple" in "apples and bananas" but labels the relationship `conj` instead of `cc`, how would this affect UAS and LAS?

- Concept: **Fine-tuning vs. From-Scratch Training**
  - Why needed here: The study's core contribution is fine-tuning existing models on L2 data rather than training new ones. This explains how pre-trained weights from L1 data are adapted to the L2 domain.
  - Quick check question: Why might fine-tuning a model pre-trained on L1 data be more effective for L2 analysis than training a new model solely on a smaller L2 dataset?

## Architecture Onboarding

- Component map:
  Input: Raw L2-Korean sentences (agglutinative, morphologically rich)
  -> Tokenizer: Splits sentences into tokens and morphemes (e.g., `ce-nun` → `ce` + `nun`)
  -> Taggers/Parsers: Stanza (BiLSTM + dictionary lemmatizer), spaCy (tok2vec embeddings), Trankit (Transformer/XLM-RoBERTa - best overall)
  -> Output: CoNLL-U format with LEMMA, XPOS, UPOS, HEAD, DEPREL

- Critical path:
  1. Obtain L2-Korean UD treebank v1.2; split into train/dev/test sets
  2. Select Trankit for parsing accuracy or Stanza for lemmatization
  3. Fine-tune using toolkit API (e.g., Trankit's `TPipeline` class) on L2-Korean training data
  4. Evaluate on held-out in-domain and out-of-domain test sets

- Design tradeoffs:
  - **Transformer (Trankit) vs. BiLSTM (Stanza)**: Transformers offer better parsing accuracy (LAS 89.13 vs 80.36) but require more compute. BiLSTMs with dictionaries excel at lemmatization.
  - **Left-to-right vs. Right-headed Annotation**: Paper adopts UD-standard left-to-right for cross-linguistic compatibility, potentially sacrificing linguistic granularity for Korean's right-headed structures.
  - **In-domain vs. Out-of-domain**: Models fine-tuned on specific learner corpora perform best on similar data; out-of-domain results show performance drops.

- Failure signatures:
  - Sharp out-of-domain performance drop → overfitting to training genre/proficiency level
  - Low LAS but high UAS → correct syntactic heads but struggling with dependency labels (check annotation ambiguities)
  - Poor lemmatization on novel words → dictionary-based component failing

- First 3 experiments:
  1. Reproduce baseline vs. fine-tuned comparison using Stanza-Korean (GSD) on L2-Korean test set
  2. Ablation study: train separate models on old right-headed vs. new left-to-right annotation schemes to isolate impact
  3. Cross-domain robustness check: fine-tune on provided training data, evaluate on different L2-Korean dataset (e.g., spoken transcripts) to measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining L2-Korean data from diverse genres and learner backgrounds significantly improve the generalizability of dependency parsers compared to single-source datasets?
- Basis in paper: [explicit] The authors state that future treebanks should adopt a strategy involving "combining L2-Korean data drawn from various genres or diverse learner backgrounds" to harness the potential of transformer-based architectures.
- Why unresolved: The current v1.2 treebank is expanded primarily from a specific subset of learner data (Park and Lee, 2016), limiting the ability to test the impact of cross-genre or cross-background diversity on model robustness.
- What evidence would resolve it: Constructing a new treebank with stratified genre and L1-background metadata, followed by a comparative evaluation of model performance on varied out-of-domain test sets.

### Open Question 2
- Question: Can refining the alignment between universal UPOS tags and language-specific XPOS tags effectively close the performance gap in lemmatization between dictionary-based and transformer-based models?
- Basis in paper: [explicit] The authors suggest "refining the match between universal UPOS tags and language-specific XPOS tags" as a method to boost effectiveness for lemmatization within the seq2seq framework used by Trankit.
- Why unresolved: Trankit (transformer-based) underperformed Stanza (dictionary-based) on the LEMMA metric, and it is currently unconfirmed whether tag misalignment is the primary cause or if the dictionary approach is inherently superior for L2 Korean.
- What evidence would resolve it: An ablation study re-training the transformer model with revised UPOS-XPOS mapping rules to observe statistical changes in LEMMA F1 scores.

### Open Question 3
- Question: Does strict adherence to the UD left-to-right coordination rule negatively affect the parsing accuracy of inherently right-headed Korean syntactic structures?
- Basis in paper: [inferred] The authors note they revised the annotation scheme to follow UD standards (left-to-right) despite Korean having a "right-headed structure" in complex clauses, citing Gerdes and Kahane (2016) regarding the lack of justification for the left-headed choice.
- Why unresolved: The study prioritized alignment with the UD framework over language-specific syntactic theories; therefore, the cost (if any) in parsing accuracy or linguistic fidelity resulting from this structural imposition remains unmeasured.
- What evidence would resolve it: A comparative evaluation of parsers trained on parallel treebanks—one following strict UD left-headedness and one utilizing a Korean-specific right-headed annotation scheme.

## Limitations

- Limited baseline comparisons: Only compared against Stanza-Korean (GSD) baseline without fine-tuning, omitting comparisons with other UD Korean treebanks or contemporary language models.
- Hyperparameter opacity: States models were fine-tuned using "default hyperparameter settings" but does not specify learning rates, batch sizes, number of epochs, or optimizer configurations.
- Annotation scheme impact quantification: Does not provide quantitative evidence isolating the impact of specific annotation changes from the effect of increased dataset size and fine-tuning.

## Confidence

- **High confidence**: The core finding that fine-tuning general-purpose L1-based language models on L2-specific data improves morphosyntactic analysis performance (Mechanism 1). The reported performance improvements are substantial and consistent across multiple metrics.
- **Medium confidence**: The attribution of Trankit's superior parsing performance to its transformer architecture versus Stanza's BiLSTM (Mechanism 1). While plausible, the study does not perform direct architectural ablation studies.
- **Medium confidence**: The claim that strict adherence to UD's left-to-right head assignment rule enhances cross-linguistic consistency and model performance (Mechanism 2). The paper asserts this but provides no quantitative comparison with alternative annotation schemes.
- **Medium confidence**: The explanation for Stanza's superior lemmatization performance due to its dictionary-based component and L1 pre-training (Mechanism 3). The mechanism is logical but not empirically isolated in the study.

## Next Checks

1. **Reproduce with specified hyperparameters**: Contact the authors or use toolkit documentation to identify exact hyperparameter values used for fine-tuning (learning rate, batch size, epochs, optimizer). Reproduce the full experimental pipeline to verify whether reported scores are achievable.

2. **Isolate annotation scheme impact**: Train separate models on the old right-headed annotation scheme versus the new left-to-right scheme (both using the same expanded dataset size). Compare performance to quantify the specific contribution of annotation refinement versus dataset expansion.

3. **Test out-of-domain generalization**: Fine-tune on the provided training data and evaluate on a completely different L2-Korean dataset (e.g., spoken learner transcripts or data from different proficiency levels) to assess how well improvements generalize beyond the specific test domain.