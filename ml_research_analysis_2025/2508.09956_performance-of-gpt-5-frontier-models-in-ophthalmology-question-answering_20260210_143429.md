---
ver: rpa2
title: Performance of GPT-5 Frontier Models in Ophthalmology Question Answering
arxiv_id: '2508.09956'
source_url: https://arxiv.org/abs/2508.09956
tags:
- gpt-5
- accuracy
- reasoning
- configurations
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated OpenAI\u2019s GPT-5 series on 260 ophthalmology\
  \ multiple-choice questions from the American Academy of Ophthalmology BCSC dataset.\
  \ Twelve GPT-5 configurations (three model tiers \xD7 four reasoning effort settings)\
  \ were tested alongside o1-high, o3-high, and GPT-4o."
---

# Performance of GPT-5 Frontier Models in Ophthalmology Question Answering

## Quick Facts
- **arXiv ID**: 2508.09956
- **Source URL**: https://arxiv.org/abs/2508.09956
- **Reference count**: 34
- **Primary result**: GPT-5-high achieved 96.5% accuracy on 260 ophthalmology MCQs, outperforming all other tested configurations including o3-high and GPT-4o.

## Executive Summary
This study evaluated OpenAI's GPT-5 series on 260 ophthalmology multiple-choice questions from the American Academy of Ophthalmology BCSC dataset. Twelve GPT-5 configurations (three model tiers × four reasoning effort settings) were tested alongside o1-high, o3-high, and GPT-4o. GPT-5-high achieved the highest accuracy (96.5%; 95% CI, 94.2–98.5%), outperforming all GPT-5-nano variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high (95.8%). GPT-5-high ranked first in both accuracy (1.66× stronger than o3-high) and rationale quality (1.11× stronger) via an LLM-as-a-judge autograder. Cost-accuracy analysis identified multiple GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the best low-cost, high-performance balance. These results benchmark GPT-5 as near-perfect in ophthalmology question answering, introduce an autograder framework for scalable evaluation, and guide optimal model selection for high-accuracy or budget-conscious applications.

## Method Summary
The study used zero-shot prompting with structured JSON output to evaluate GPT-5, GPT-5-mini, and GPT-5-nano models across four reasoning effort settings (minimal, low, medium, high) on 260 closed-access ophthalmology MCQs from the AAO BCSC dataset. Each model generated answers (A-D) with one-sentence rationales. Accuracy was measured against ground truth, while rationale quality was assessed using an LLM-as-a-judge framework (o4-mini) that performed reference-anchored pairwise comparisons with randomized A/B assignment. Bradley-Terry modeling aggregated pairwise wins into global skill rankings, and cost-accuracy Pareto analysis used token usage to determine optimal configurations.

## Key Results
- GPT-5-high achieved 96.5% accuracy (95% CI, 94.2–98.5%), significantly outperforming all GPT-5-nano variants (P < .001) and GPT-4o (P < .001)
- GPT-5-high ranked first in both accuracy (1.66× stronger than o3-high) and rationale quality (1.11× stronger) via LLM-as-a-judge autograder
- Cost-accuracy analysis identified multiple GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the best low-cost, high-performance balance

## Why This Works (Mechanism)

### Mechanism 1
Increased reasoning effort improves accuracy on complex medical MCQs through internal "thinking" computation. The reasoning_effort parameter controls generation of reasoning tokens used for processing and planning before final output. Higher settings allocate more compute to chain-of-thought style deliberation, helping with multi-step clinical reasoning. This pattern was observed across model families in related ophthalmology work (arxiv 2501.13949).

### Mechanism 2
Model tier sets a ceiling on achievable accuracy; reasoning effort modulates performance within that ceiling. Larger models (GPT-5 > GPT-5-mini > GPT-5-nano) encode more parametric knowledge and reasoning patterns. At fixed reasoning effort, larger tiers consistently outperform smaller ones, demonstrating tier dominance over effort at extremes.

### Mechanism 3
Reference-anchored pairwise LLM-as-a-judge evaluation provides a scalable proxy for rationale quality assessment. An autograder (o4-mini) receives the BCSC reference explanation plus two anonymized rationales, extracts salient facts, and selects the better-aligned response. Bradley-Terry modeling aggregates pairwise wins into a global skill ranking, with position bias mitigated through randomization.

## Foundational Learning

- **Bradley-Terry modeling**: Converts pairwise win/loss data into continuous skill estimates, enabling quantitative comparison beyond raw accuracy. Quick check: If Model A beats Model B in 60% of discordant questions and Model B beats Model C in 70%, what does Bradley-Terry predict about A vs. C?

- **Pareto frontier (cost-accuracy trade-off)**: Identifies configurations where no other option is simultaneously cheaper and more accurate—critical for production deployment decisions. Quick check: If Configuration X costs $0.01/question at 90% accuracy and Configuration Y costs $0.005 at 89% accuracy, which is Pareto-efficient?

- **Reasoning tokens (internal compute)**: Distinguishes "thinking" computation from output generation; explains why higher effort increases latency and cost without longer visible outputs. Quick check: Why might increasing reasoning tokens help on a multi-step diagnosis question but not a simple recall question?

## Architecture Onboarding

- **Component map**: Query input → OpenAI Responses API (model tier + reasoning_effort parameter) → thinking variant if effort > minimal → output parser → accuracy evaluator and autograder → Bradley-Terry aggregation → cost calculator

- **Critical path**: Reasoning effort configuration → token generation → accuracy. The minimal setting was excluded because it produced no reasoning tokens (likely routed to non-thinking variant), breaking the mechanism entirely.

- **Design tradeoffs**: GPT-5-high: maximum accuracy (96.5%), highest cost ($0.012/question), longest latency; GPT-5-mini-low: Pareto-optimal balance (92.7% accuracy, $0.000/question effectively), fast; GPT-5-nano-low: cheapest but accuracy drops to 77.3%—unsuitable for clinical use; o1-high: higher cost than GPT-5-medium with lower accuracy—non-Pareto.

- **Failure signatures**: Minimal reasoning effort: no reasoning tokens generated, likely indicates routing to non-thinking model variant; Nano tier at any effort: significant accuracy gap (~15-20 percentage points) from full model; Confidence interval overlap: GPT-5-high vs o3-high (0.965 vs 0.958) not statistically significant (P = 0.87).

- **First 3 experiments**: 1) Replicate on held-out BCSC questions (different 260-item sample) to confirm reasoning effort scaling is not dataset-specific; 2) Test on free-response clinical cases (not MCQ) to evaluate whether rationale quality rankings generalize beyond single-sentence justifications; 3) Ablate the judge model (swap o4-mini for GPT-4o or human annotators on a subset) to measure sensitivity of rationale rankings to judge selection.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the best-performing GPT-5 configurations perform on complex, free-response, and multimodal (image-based) ophthalmology tasks? The current study was restricted to text-only, multiple-choice questions, leaving performance on open-ended clinical dialogues and image analysis untested.

- **Open Question 2**: Is the LLM-as-a-judge autograder framework generalizable to the evaluation of longer-form clinical rationales? The current study limited model outputs to single-sentence justifications to mitigate verbosity bias; it is unknown if the judge model can objectively assess detailed, paragraph-length clinical reasoning.

- **Open Question 3**: Do high accuracy scores on multiple-choice benchmarks translate to improved performance in real-world clinical decision-making? Selecting a correct answer from a list differs cognitively from diagnosing a patient in a dynamic clinical environment where information is unstructured.

- **Open Question 4**: To what degree does training data contamination influence the high accuracy scores observed in frontier models on the BCSC dataset? While the authors took steps to use API data that is not used for training, contamination from other researchers' use or public replication of the BCSC questions remains a potential confounder for the "near-perfect" scores.

## Limitations

- **Closed-access dataset**: The study relies on the American Academy of Ophthalmology's BCSC dataset, which is not publicly available, limiting reproducibility and independent validation.
- **Single-question format**: The multiple-choice format does not fully replicate real-world clinical decision-making, where information is unstructured and dynamic.
- **Single-sentence rationale constraint**: Rationale quality assessment was limited to one-sentence justifications, potentially missing depth of reasoning quality that longer explanations would reveal.

## Confidence

- **High confidence**: GPT-5-high achieving 96.5% accuracy on ophthalmology MCQs, and GPT-5-high ranking first in both accuracy and rationale quality via autograder.
- **Medium confidence**: Cost-accuracy Pareto frontier identification and the mechanism that reasoning effort improves accuracy through internal "thinking" computation.
- **Low confidence**: Generalization of findings to non-MCQ clinical reasoning tasks.

## Next Checks

1. **Held-out dataset replication**: Apply the exact evaluation protocol to a different 260-item sample from the BCSC dataset (if accessible) or to an equivalent ophthalmology MCQ benchmark to verify that reasoning effort scaling patterns are not dataset-specific.

2. **Autograder sensitivity analysis**: Systematically replace the o4-mini judge with alternative models (GPT-4o, GPT-4o-mini, or human annotators on a subset) and measure how rationale rankings change. This will quantify the robustness of the autograder framework to judge selection.

3. **Free-response clinical case evaluation**: Test the same model configurations on open-ended clinical cases rather than MCQs, using the same autograder framework but allowing longer, multi-sentence rationales. This will determine whether the rationale quality rankings generalize beyond the single-sentence constraint and structured question format.