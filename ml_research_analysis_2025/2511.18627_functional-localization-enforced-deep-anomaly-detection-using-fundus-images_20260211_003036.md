---
ver: rpa2
title: Functional Localization Enforced Deep Anomaly Detection Using Fundus Images
arxiv_id: '2511.18627'
source_url: https://arxiv.org/abs/2511.18627
tags:
- fundus
- anomaly
- images
- augmentation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based framework for explainable
  retinal disease detection using fundus images. The authors fine-tune a Vision Transformer
  (ViT) with various augmentation strategies across multiple public datasets and their
  own AEyeDB dataset.
---

# Functional Localization Enforced Deep Anomaly Detection Using Fundus Images

## Quick Facts
- arXiv ID: 2511.18627
- Source URL: https://arxiv.org/abs/2511.18627
- Reference count: 40
- Primary result: ViT-B/16 with geometric augmentation achieved 0.843 accuracy on multi-class retinal disease classification

## Executive Summary
This paper proposes a transformer-based framework for explainable retinal disease detection using fundus images. The authors fine-tune a Vision Transformer (ViT) with various augmentation strategies across multiple public datasets and their own AEyeDB dataset. They incorporate a constrained attention mask into the ViT to promote inherent spatial interpretability, and extend the GANomaly model for anomaly detection with reconstruction-based explainability. The best ViT performance achieved an accuracy of 0.843 with geometric augmentation, outperforming prior CNN ensembles. The anomaly detector reached an AUC of 0.76. The constrained attention mechanism provided stable, physiologically meaningful localization maps. Probabilistic calibration via GUESS enabled threshold-independent decision support.

## Method Summary
The method combines a ViT-B/16 classifier with a U-Net-based attention mask for interpretability and a GANomaly model for anomaly detection. The ViT is fine-tuned with geometric and color augmentations, while the attention mask is trained jointly with a composite loss function. The GANomaly model is trained exclusively on healthy images to detect unseen pathologies through reconstruction error. Probabilistic calibration is applied to convert anomaly scores into interpretable probabilities. The approach was validated across multiple public fundus datasets (FIVES, Mendeley, Papila, Messidor, JSIEC) and an in-house AEyeDB dataset.

## Key Results
- ViT-B/16 with geometric augmentation achieved 0.843 accuracy, outperforming prior CNN ensembles
- Anomaly detector achieved 0.76 AUC, providing interpretable reconstruction-based explanations
- Constrained attention mechanism provided stable, physiologically meaningful localization maps
- Probabilistic calibration via GUESS enabled threshold-independent clinical decision support

## Why This Works (Mechanism)

### Mechanism 1: Augmentation-Induced Domain Generalization
Geometric and color augmentations stabilize Vision Transformer (ViT) performance across heterogeneous fundus datasets, whereas structural enhancement (Laplacian) may degrade it. Geometric transformations force the model to learn invariance to camera positioning and lighting conditions, reducing overfitting to dataset-specific acquisition artifacts.

### Mechanism 2: Jointly Optimized Attention Masking for Intrinsic Interpretability
Integrating a U-Net-based masking network with the classifier enforces spatial focus on diagnostically relevant regions without requiring post-hoc explanation methods. A composite loss function simultaneously minimizes classification error and mask complexity, forcing the network to identify the minimal set of pixels required for the diagnosis.

### Mechanism 3: Reconstruction Error as a Generalizable Anomaly Surrogate
Training a generative model (GANomaly) exclusively on healthy fundus images allows detection of unseen pathologies by quantifying reconstruction fidelity. The model learns a manifold of "healthy" retinal representations, and inputs containing pathologies deviate from this manifold, resulting in high latent reconstruction errors that serve as the anomaly score.

## Foundational Learning

- **Vision Transformers (ViT) vs. CNNs**: Understanding patch-based processing and self-attention is required to understand why geometric augmentations are critical and how the attention mask integrates. Quick check: How does the ViT handle spatial relationships differently than a CNN, and why does this necessitate the specific data augmentations mentioned?

- **Generative Adversarial Networks (GANs) for Anomaly Detection**: The anomaly detection module uses GANomaly (Encoder-Decoder-Encoder). You must understand why the model reconstructs the image but compares the latent vector to detect anomalies. Quick check: In the GANomaly architecture, does the discriminator compare the image pixels or the latent features to determine if an input is "normal"?

- **Probabilistic Calibration (GUESS)**: Raw anomaly scores are unitless and hard to interpret clinically. The paper uses GUESS to convert these scores into probabilities. Quick check: Why is "threshold-independent" evaluation (using AUC) preferred over fixed accuracy thresholds when calibrating a model for clinical decision support?

## Architecture Onboarding

- **Component map**: Input Fundus Image (224x224) -> Augmentation (Geometric + Color Jitter) -> Classifier Branch (ViT-B/16 + U-Net Mask Generator) -> Anomaly Branch (GANomaly) -> Output (Classification Label + Calibrated Abnormality Probability + Reconstruction Error Map)

- **Critical path**: 1. Preprocess images (exclude low quality) 2. Train/Fine-tune ViT with geometric augmentations (Highest priority) 3. Attach and train U-Net mask generator (Secondary, for interpretability) 4. Train GANomaly on healthy only subset (Parallel workflow)

- **Design tradeoffs**: Interpretability vs. Accuracy (U-Net mask offers "inherent spatial interpretability" but resulted in a slight accuracy drop), Stability vs. Diversity (KL regularization introduced noisy training dynamics), Enhancement vs. Artifact (Laplacian enhancement emphasized vasculature but introduced non-physiological intensity distributions)

- **Failure signatures**: Glaucoma Confusion (High rate of misclassifying Glaucoma as Normal), Training Instability (KL-regularized GANomaly showed visibly noisier and less stable loss trajectories), Overfitting (Perfect accuracy on AEyeDB suggests potential overfitting to specific acquisition protocol)

- **First 3 experiments**: 1. Baseline Validation: Reproduce Table 2 by fine-tuning ViT-B/16 with geometric augmentation on the combined datasets to verify the 0.843 accuracy benchmark 2. Ablation on Localization: Train the U-Net mask with varying Î» values to observe the trade-off between mask sparsity and classification accuracy 3. Generalization Test: Train GANomaly on the healthy subset of FIVES/Mendeley and evaluate the AUC specifically on the unseen JSIEC dataset to confirm the 0.76 AUC generalization claim

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing the size and diversity of the healthy training cohort improve the calibration stability of probabilistic anomaly detection in fundus imaging? The paper notes that limited healthy sample size biased probabilistic outputs toward higher abnormality probabilities, affecting reliability.

### Open Question 2
Can domain adaptation or self-supervised pretraining effectively mitigate the dataset-specific biases and overfitting observed in high-quality, curated datasets like AEyeDB? The paper established that acquisition consistency substantially impacts robustness but did not implement specific algorithms to harmonize feature distributions.

### Open Question 3
Do hybrid architectures that jointly optimize discriminative transformer classifiers and generative anomaly detectors offer superior performance compared to using them as separate, complementary pipelines? The current study evaluated the ViT classifier and the GANomaly detector as distinct components used in parallel.

### Open Question 4
Can specific preprocessing or architectural modifications improve the detection of early-stage glaucoma, which remains the most misclassified pathology despite transformer-based modeling? The paper identifies glaucoma as the most frequently misclassified disease, with early glaucomatous changes remaining difficult to capture.

## Limitations

- Generalizability concerns due to high accuracy on in-house AEyeDB dataset potentially indicating overfitting to specific acquisition protocols
- Uncertainty about exact composition of "healthy" training samples across all datasets for GANomaly training
- Lack of quantitative metrics to validate the clinical utility of the generated attention masks

## Confidence

- **High Confidence**: Core mechanism of using geometric augmentations to improve ViT robustness across heterogeneous datasets is well-supported by ablation results
- **Medium Confidence**: Claim that joint classification-attention loss provides interpretable masks is supported by design but lacks quantitative validation
- **Low Confidence**: Assertion of "surpassing prior methods" lacks direct comparison on identical datasets

## Next Checks

1. Quantify the quality of the attention masks by measuring their overlap with ground-truth lesion segmentation masks or conducting a clinician study to assess clinical relevance
2. Train the GANomaly anomaly detector on a limited subset of healthy images from a single source and evaluate its AUC on all other datasets to test true cross-dataset generalization
3. Analyze the confusion matrix of the anomaly detector to determine which specific pathologies are most consistently detected vs. missed, and report per-class anomaly detection rates