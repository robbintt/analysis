---
ver: rpa2
title: 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal
  Feedback'
arxiv_id: '2502.00988'
source_url: https://arxiv.org/abs/2502.00988
tags:
- data
- feedback
- code
- agent
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PlotGen is a multi-agent LLM framework that automates scientific\
  \ data visualization by integrating code generation with multimodal feedback. It\
  \ decomposes user requests via a Query Planning Agent, generates executable Python\
  \ code through a Code Generation Agent, and refines visualizations using three feedback\
  \ agents\u2014Numeric, Lexical, and Visual\u2014that leverage multimodal LLMs to\
  \ iteratively correct data accuracy, textual labels, and visual aesthetics."
---

# PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback

## Quick Facts
- arXiv ID: 2502.00988
- Source URL: https://arxiv.org/abs/2502.00988
- Authors: Kanika Goswami; Puneet Mathur; Ryan Rossi; Franck Dernoncourt
- Reference count: 30
- Primary result: 4-6% performance improvement over baselines on MatPlotBench dataset

## Executive Summary
PlotGen is a multi-agent LLM framework that automates scientific data visualization by integrating code generation with multimodal feedback. It decomposes user requests via a Query Planning Agent, generates executable Python code through a Code Generation Agent, and refines visualizations using three feedback agents—Numeric, Lexical, and Visual—that leverage multimodal LLMs to iteratively correct data accuracy, textual labels, and visual aesthetics. Evaluated on MatPlotBench, PlotGen achieves significant performance improvements over strong baselines, with a 10-12% overall enhancement in visualization quality across various LLM settings. User studies show reduced debugging time and improved trust in LLM-generated visualizations, particularly benefiting novice users.

## Method Summary
PlotGen uses a five-agent architecture where a Query Planning Agent decomposes natural language requests into structured pseudocode steps, a Code Generation Agent produces executable Python with self-debugging capabilities, and three specialized feedback agents (Numeric, Lexical, Visual) sequentially refine the output using GPT-4V to perceive and correct errors. The framework operates through iterative loops where each agent can request regeneration from the Code Agent until convergence or maximum iterations are reached, with temperature=0 for deterministic code generation.

## Key Results
- Achieves 4-6% performance improvement over strong baselines on MatPlotBench
- Demonstrates 10-12% overall enhancement in visualization quality across various LLM settings
- Ablation study shows 5-15% performance drops when removing individual feedback agents

## Why This Works (Mechanism)

### Mechanism 1
Iterative multimodal feedback improves visualization code generation by enabling self-correction across orthogonal error dimensions. Three specialized feedback agents (Numeric, Lexical, Visual) use GPT-4V to perceive intermediate plot outputs—de-rendering data, reading labels, observing aesthetics—and generate textual corrections fed back to the Code Generation Agent for refinement. Core assumption: Multimodal LLMs can reliably diagnose visualization errors that text-only code LLMs cannot detect from code alone.

### Mechanism 2
Decomposing user requests into structured execution steps via chain-of-thought reduces code generation ambiguity. Query Planning Agent transforms natural language requests into explicit pseudocode steps specifying function calls, parameters, data formats, and visual characteristics before code generation. Core assumption: Visualization requests contain implicit requirements that benefit from explicit decomposition before code synthesis.

### Mechanism 3
Specialized feedback dimensions prevent error type interference and enable targeted corrections. Sequential execution of Numeric → Lexical → Visual feedback agents, each focused on one error category, with iteration limits per agent. Core assumption: Visualization errors naturally cluster into data accuracy, text labeling, and visual aesthetics, and addressing them separately improves correction precision.

## Foundational Learning

- **Multimodal LLM perception capabilities (Vision-Language Models)**: Understanding how GPT-4V "sees" plots, extracts data via de-rendering, and diagnoses visual errors is essential for debugging feedback agent behavior. Quick check: Can you explain why a vision-language model might correctly read axis labels but fail to detect subtle data trend mismatches in a scatter plot?

- **Chain-of-Thought (CoT) prompting**: The Query Planning Agent relies on CoT to decompose requests; understanding CoT failure modes helps diagnose planning errors. Quick check: What types of user requests would likely fail to benefit from CoT decomposition due to inherent ambiguity?

- **Self-reflection and iterative refinement in LLMs**: The entire PlotGen framework depends on LLM self-correction loops; understanding convergence conditions and iteration limits is critical. Quick check: How would you detect when a feedback loop is oscillating between two error states rather than converging?

## Architecture Onboarding

- **Component map**: User request + raw data → Query Planning Agent → execution steps → Code Generation Agent → Python code → execute → draft figure → Numeric Feedback Agent → Lexical Feedback Agent → Visual Feedback Agent → final output

- **Critical path**: 1) User request + raw data → Query Planning Agent → execution steps 2) Execution steps → Code Generation Agent → Python code → execute → draft figure 3) Draft figure → Numeric Feedback Agent → (if issues) feedback → Code Agent regenerates 4) Refined figure → Lexical Feedback Agent → (if issues) feedback → Code Agent regenerates 5) Refined figure → Visual Feedback Agent → (if issues) feedback → Code Agent regenerates 6) Iterate until all agents pass or max iterations reached → final output

- **Design tradeoffs**: Sequential vs parallel feedback agents (current: sequential; parallel could speed up but may cause conflicting feedback), iteration limits per agent (prevents infinite loops but may exit before convergence), code LLM backbone selection (GPT-4 performs best; smaller open-source models trade quality for cost), temperature=0 for deterministic code generation vs potential diversity benefits

- **Failure signatures**: Non-convergence (same error repeatedly corrected without resolution), conflicting feedback (agents suggest incompatible fixes), hallucinated corrections (feedback for non-existent issues), execution errors (syntactically invalid code)

- **First 3 experiments**: 1) Baseline comparison: Run Direct Decoding, Zero-Shot CoT, and MatPlotAgent on MatPlotBench subset (20 queries) to reproduce the 4-6% improvement claim 2) Ablation study: Disable each feedback agent one at a time on 30 queries to verify 5-15% performance drops 3) Backbone swap test: Replace GPT-4V with weaker VLM (e.g., LLaVA) for feedback agents to assess multimodal perception quality sensitivity

## Open Questions the Paper Calls Out
- Future work will extend PlotGen beyond traditional table data visualization to explore its application in real-time environments such as interactive dashboards, Virtual Reality simulations and visual arts.

## Limitations
- Evaluation relies entirely on LLM-as-a-judge (GPT-4V) scoring, introducing potential bias and subjectivity
- Performance with open-source code LLMs suggests robustness but doesn't explore smaller, more cost-effective models
- Claim that novice users particularly benefit lacks direct empirical validation through user studies

## Confidence

**High Confidence**: The sequential multi-agent architecture with specialized feedback agents is technically sound and the reported improvements over baselines are likely reproducible.

**Medium Confidence**: The 4-6% performance improvement and 10-12% overall quality enhancement are reasonable but LLM-as-a-judge methodology introduces uncertainty about absolute magnitude.

**Low Confidence**: The claim about novice user benefits lacks direct empirical validation in the paper.

## Next Checks
1. **Human Evaluation Validation**: Conduct a blind comparison study where domain experts rate visualizations generated by PlotGen versus baseline approaches using the same MatPlotBench queries.

2. **Error Type Analysis**: Systematically categorize failure modes across the 100 MatPlotBench queries to determine whether sequential feedback processing actually prevents error interference.

3. **Cost-Performance Tradeoff Analysis**: Benchmark PlotGen with progressively smaller code LLMs and weaker VLMs to establish the minimum viable configuration that maintains acceptable performance.