---
ver: rpa2
title: What Does It Take to Build a Performant Selective Classifier?
arxiv_id: '2510.20242'
source_url: https://arxiv.org/abs/2510.20242
tags:
- selective
- error
- learning
- conference
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a decomposition of the selective-classification
  gap into five measurable error terms: Bayes noise, approximation error, ranking
  error, statistical noise, and implementation/shift-induced slack. The authors argue
  that monotone post-hoc calibration methods, such as temperature scaling, have limited
  impact on reducing this gap because they rarely alter the underlying score ranking.'
---

# What Does It Take to Build a Performant Selective Classifier?

## Quick Facts
- **arXiv ID**: 2510.20242
- **Source URL**: https://arxiv.org/abs/2510.20242
- **Reference count**: 40
- **Primary result**: Introduces a five-term decomposition of the selective-classification gap and shows monotone calibration has limited impact while richer, feature-aware scoring methods significantly improve selective performance.

## Executive Summary
This paper presents the first finite-sample decomposition of the selective-classification gap into five measurable error terms: Bayes noise, approximation error, ranking error, statistical noise, and implementation/shift-induced slack. The authors demonstrate that post-hoc calibration methods like temperature scaling rarely reduce this gap because they preserve the underlying score ranking. Through experiments on synthetic and real-world datasets, they show that only non-monotone, feature-aware scoring methods (Deep Ensembles, Self-Adaptive Training, learned correctness heads) meaningfully improve selective performance by reordering predictions. The decomposition provides an actionable error budget and practical guidelines for building selective classifiers that more closely approximate ideal oracle behavior.

## Method Summary
The paper introduces a five-term decomposition of the selective-classification gap and validates it through experiments on CIFAR-100, StanfordCars, and CIFAR-10N. The method involves training base classifiers (SimpleCNN, ResNet-18, WideResNet-50) using standard SGD optimization, then evaluating various selective methods including Maximum Softmax Probability (MSP), Temperature Scaling (TEMP), Self-Adaptive Training (SAT), and Deep Ensembles (DE). The primary metric is Excess-AURC, which measures the area between the model's accuracy-coverage curve and the oracle bound. The approach systematically compares monotone vs. non-monotone scoring methods to isolate the impact of ranking improvements on selective performance.

## Key Results
- Temperature scaling improves calibration (ECE) but fails to reduce the selective gap (E-AURC remains nearly unchanged)
- Non-monotone methods (Deep Ensembles, SAT) significantly reduce E-AURC by reordering predictions
- The five-term decomposition provides actionable error budgets for diagnosing selective performance limitations
- Ranking error (ε_rank) is the primary addressable term after Bayes and approximation errors are fixed

## Why This Works (Mechanism)

### Mechanism 1: Five-Term Error Budget Decomposition
The selective-classification gap admits a finite-sample decomposition into five additive error terms (Equation 1). The gap Δ(c) = acc(a_full, c) − acc_c(h,g) is bounded by algebraically separating contributions from irreducible label noise, model capacity limits, ranking misalignment, finite-sample estimation, and optimization/shift artifacts. Each term is measurable in principle, enabling targeted diagnosis. The decomposition is a sufficient upper bound but not proven tight or unique, assuming the acceptance region A_c is defined by the current scoring function and that Bayes-optimal behavior is well-defined.

### Mechanism 2: Monotone Calibration Cannot Fix Ranking
Post-hoc calibration via temperature scaling or isotonic regression has limited ability to reduce the selective-classification gap because monotone transformations preserve the induced total order over examples. Since the gap is driven by ε_rank (misalignment between score order and true correctness order), calibration that does not reorder cannot substantially reduce the gap. Temperature scaling can produce small re-rankings via softmax non-linearity, but this effect is sparse and limited to finely-tuned logit patterns.

### Mechanism 3: Non-Monotone, Feature-Aware Scoring Reduces Gap via Reordering
Methods that use richer representations (Deep Ensembles, Self-Adaptive Training, learned correctness heads) can meaningfully reduce the selective-classification gap by producing confidence scores that are not monotone transformations of the base softmax. Ensembles aggregate predictions from diverse trajectories; SAT relabels uncertain samples during training; auxiliary heads learn from hidden representations. All three can reorder the acceptance set A_c, directly lowering ε_rank.

## Foundational Learning

- **Concept: Selective Classification and the Oracle Bound**
  - Why needed here: The entire paper frames performance relative to a "perfect-ordering oracle" that accepts correct examples first; without this baseline, the gap Δ(c) has no reference point.
  - Quick check question: Given a classifier with 85% full-coverage accuracy, what is the oracle's selective accuracy at 70% coverage?

- **Concept: Calibration vs. Ranking**
  - Why needed here: The paper's central claim is that calibration (matching predicted probabilities to empirical frequencies) is distinct from ranking (ordering examples by correctness likelihood); conflating them leads to incorrect conclusions about temperature scaling.
  - Quick check question: If a model's scores are perfectly calibrated (ECE=0), does it necessarily have optimal ranking for selective classification?

- **Concept: Multicalibration and Loss Prediction**
  - Why needed here: Appendix E connects ε_rank to the ability to predict one's own loss; this provides an alternative diagnostic and theoretical grounding for ranking error.
  - Quick check question: If no auxiliary predictor can beat the model's self-entropy estimate, what does this imply about ε_rank?

## Architecture Onboarding

- **Component map**: Base classifier h -> Selection score g(x, h) -> Threshold τ_c -> Acceptance region A_c -> Gap estimator Δ̂(c)
- **Critical path**: 
  1. Train base classifier to convergence; measure a_full on validation set
  2. Compute selection scores g(x, h) using chosen method (start with MSP as baseline)
  3. Sweep thresholds τ to produce accuracy-coverage curve; compute E-AURC vs. oracle
  4. If gap is large, diagnose: estimate ε_Bayes via repeated labels or noisy subsets; estimate ε_approx by comparing architectures; estimate ε_rank via LP advantage (Appendix E.5)
  5. If ε_rank dominates, switch to non-monotone scoring (DE, SAT, or feature-aware head)

- **Design tradeoffs**:
  - MSP: trivial to implement, often hard-to-beat baseline; cannot reduce ε_rank
  - TEMP: improves ECE (useful for downstream calibration); negligible gap reduction
  - SAT: requires training modification; modest gap improvement; may interact with ε_approx
  - DE: strongest gap reduction; 5× training cost; requires storage of multiple checkpoints
  - Feature-aware head: requires architecture modification and auxiliary training data; potentially high ROI if ε_rank is large

- **Failure signatures**:
  - TEMP applied but E-AURC unchanged → confirms ranking is the bottleneck, not calibration
  - DE shows no improvement over MSP → check ensemble diversity (initializations, data order)
  - SAT worsens full-coverage accuracy → relabeling may be too aggressive; reduce momentum
  - Gap persists despite addressing ε_rank → investigate ε_Bayes (intrinsic label noise) or ε_misc (distribution shift)

- **First 3 experiments**:
  1. Establish baseline: Train ResNet-18 on CIFAR-100; compute MSP-based accuracy-coverage curve and E-AURC; compare to oracle bound. This quantifies the initial gap and identifies which coverage levels are most problematic.
  2. Isolate calibration vs. ranking: Apply temperature scaling; report both ECE and E-AURC. A substantial ECE drop with no E-AURC change confirms the paper's central claim and rules out calibration as a solution path.
  3. Test non-monotone scoring: Train a 5-model Deep Ensemble or SAT variant; recompute E-AURC. Compare LP advantage (Appendix E.5) between MSP and DE/SAT. A reduction in LP advantage correlated with gap reduction validates the ranking mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the gap decomposition be extended to large-scale foundation models and open-ended language generation tasks?
- **Basis in paper**: [explicit] "extending these insights to large-scale foundation models would be an important direction... extending it to (i) settings where out-of-distribution inputs must be rejected and (ii) open-ended language generation constitutes a promising agenda for future work" (Section 5).
- **Why unresolved**: The paper's preliminary LLM experiments (Appendix F.2) note that "uncertainty for generative models remains ill-defined" and "prompting artefacts add variance" that can obscure the effects the decomposition aims to isolate. Only classification-style prompts were tested.
- **What evidence would resolve it**: Established abstention semantics and evaluation protocols for generative outputs, plus a comprehensive study showing how the five error terms distribute across diverse LLM tasks.

### Open Question 2
- **Question**: How should the oracle bound and gap decomposition be generalized to asymmetric or class-dependent cost functions?
- **Basis in paper**: [explicit] The paper states that "our oracle bound and gap are defined for 0–1 loss, adapting to asymmetric or class-dependent cost functions—often required in high-stakes decision-making—will require generalizing both the bound and its decomposition" (Section 5).
- **Why unresolved**: The current Definition 3 (perfect ordering upper bound) assumes uniform cost of errors, which does not reflect scenarios where false negatives have different consequences than false positives.
- **What evidence would resolve it**: A theoretical extension of Theorem 1 to general loss functions, validated empirically on domains like medical diagnosis or fraud detection with known asymmetric costs.

### Open Question 3
- **Question**: How can unique attribution of gap improvements be achieved when interventions simultaneously affect multiple error terms?
- **Basis in paper**: [inferred] The paper acknowledges that "error budgets can interact—for example, increasing capacity often improves both approximation and ranking—which makes unique attribution challenging" and that "many training-time calibration schemes... simultaneously affect ranking and full-coverage accuracy, confounding the separation of budgets" (Section 5).
- **Why unresolved**: The decomposition is mathematically clean, but practical interventions (e.g., SAT, mixup, model scaling) do not respect term boundaries.
- **What evidence would resolve it**: Development of attribution methods or controlled experimental protocols that isolate individual term contributions, perhaps via interventions designed to target one term while holding others constant.

## Limitations

- The five-term decomposition is a sufficient bound, not proven tight or unique; interactions between terms may occur in practice, especially under label noise or covariate shift.
- Empirical results are limited to image classification benchmarks (CIFAR variants, StanfordCars); generalization to other modalities or structured prediction tasks remains untested.
- The paper focuses on classification tasks and does not address the challenges of extending selective classification to generative or open-ended language models.

## Confidence

- **High confidence**: The empirical finding that temperature scaling improves ECE without reducing the selective gap; this is directly observable from Table 1 and Figure 1.
- **Medium confidence**: The formal decomposition of the gap into five additive terms; the bound holds mathematically, but the tightness and practical separability of terms depend on data and model specifics.
- **Low confidence**: The claim that non-monotone methods "significantly" outperform monotone ones in all selective scenarios; results show strong gains on CNNs, but weaker or inconsistent gains on ResNets/WideResNets.

## Next Checks

1. **Cross-dataset robustness**: Reproduce the decomposition and E-AURC trends on a non-image task (e.g., tabular or text classification) to confirm mechanism generality.
2. **Effect of training set size**: Vary n and measure whether ε_rank remains the dominant term as sample size decreases; this tests whether ranking is always addressable or if statistical noise overtakes it in small-data regimes.
3. **Interaction with distribution shift**: Evaluate all methods under covariate shift (e.g., CIFAR-10-C) to test whether the decomposition remains additive or if ε_misc terms become dominant.