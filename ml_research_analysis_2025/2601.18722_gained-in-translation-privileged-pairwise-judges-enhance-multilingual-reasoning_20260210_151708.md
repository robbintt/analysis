---
ver: rpa2
title: 'Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning'
arxiv_id: '2601.18722'
source_url: https://arxiv.org/abs/2601.18722
tags:
- language
- lang
- qwen2
- judge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SP3F (Self-Play with Privileged Pairwise Feedback) is a two-stage
  framework for improving multilingual reasoning without any data in the target language(s).
  First, supervised fine-tuning (SFT) is performed on translated versions of English
  reference responses to raise base model correctness.
---

# Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning

## Quick Facts
- **arXiv ID:** 2601.18722
- **Source URL:** https://arxiv.org/abs/2601.18722
- **Reference count:** 40
- **Primary result:** SP3F-7B achieves 61.91% accuracy on MGSM vs 55.87% for Qwen2.5-7B-Instruct, using 1/8th the training data

## Executive Summary
This paper introduces SP3F (Self-Play with Privileged Pairwise Feedback), a two-stage framework that improves multilingual reasoning without requiring any target language training data. The method first performs supervised fine-tuning on translated English reference responses, then applies reinforcement learning with feedback from a pairwise judge that receives the English reference as privileged information. This allows the judge to evaluate how closely model responses align with the reference, even when no response is completely correct. The approach is evaluated across 18 languages on four tasks including math, reading comprehension, and general knowledge, consistently outperforming Qwen2.5-7B-Instruct while requiring significantly less training data.

## Method Summary
SP3F is a two-stage framework: (1) Supervised fine-tuning on translated versions of English reference responses to improve base model correctness, and (2) Reinforcement learning with feedback from a pairwise judge that receives the English reference response as privileged information. The judge evaluates how closely model responses align with the reference by comparing pairs of responses, even when neither is completely correct. This approach enables reasoning improvements in target languages without requiring reasoning data in those languages, addressing the challenge of sparse rewards in multilingual reasoning tasks.

## Key Results
- SP3F-7B achieves 61.91% accuracy on MGSM (math) vs 55.87% for Qwen2.5-7B-Instruct
- On MT Math100: 72.50% vs 66.36% accuracy
- On Belebele (reading comprehension): 67.54% vs 56.79% accuracy
- Requires only 1/8 of the training data used by Qwen2.5-7B-Instruct
- Better generalization to unseen languages (58.3% vs 39.9% on Belebele)

## Why This Works (Mechanism)
The core insight is that a pairwise judge with access to English reference responses can provide dense, informative feedback about reasoning quality even when responses are imperfect. By comparing responses rather than scoring them individually, the judge can identify which reasoning chains are closer to the reference, enabling learning from partial successes. The self-play framework converts these pairwise comparisons into win-rate rewards that are more informative than binary correctness signals. This approach addresses the cold-start problem in reasoning tasks where sparse rewards make initial learning difficult, and it maintains language fidelity by incorporating a threshold-based language reward rather than a scalar score that could be gamed.

## Foundational Learning
- **Concept: Reinforcement Learning Verifiable Rewards (RLVR)**
  - Why needed here: This is the baseline RL method SP3F builds upon. RLVR uses sparse, outcome-based rewards (e.g., binary check if final answer is correct) to train a model. You must understand it to see why SP3F introduces a dense feedback signal from a judge.
  - Quick check question: Why is a reward signal based only on final answer correctness considered "sparse," and how does this create a cold-start problem for reasoning models?

- **Concept: LLM-as-a-Judge / Pairwise Preference**
  - Why needed here: The core of the method is using a second LLM (the judge) to provide feedback. It does not score a single response; it compares two. You must understand this paradigm to grasp how the privileged information and self-play aggregation fit together.
  - Quick check question: What are two common failure modes of LLM judges that the paper explicitly mentions and attempts to mitigate with its method?

- **Concept: Self-Play in RL**
  - Why needed here: The paper uses a "self-play" framework where a model's different outputs compete against each other. The win rate from these competitions becomes the reward. This is a non-standard RL setup compared to fitting a reward model.
  - Quick check question: How does converting a series of pairwise judgments into a single win-rate score for each response solve the problem of fitting a reward model on inconsistent, intransitive preferences?

## Architecture Onboarding
- **Component map:** Data Translator (GPT-5-Nano) -> SFT Engine (Qwen2.5-7B) -> RL Self-Play Loop (policy, sampler, Privileged Judge, Reward Aggregator)
- **Critical path:** The Privileged Judge within the RL Loop. Its latency and cost dominate the RL step, as it must evaluate N*(N-1)/2 pairs per prompt per training batch. Its reliability directly determines the quality of the learning signal.
- **Design tradeoffs:** Judge Power vs. Cost (stronger judge gives better signal but is more expensive), Batch Size vs. Compute (larger N leads to more reliable win-rate but quadratically increases API calls), In-domain vs. Out-of-domain Performance (judge feedback improves in-domain math but has mixed results on out-of-domain tasks)
- **Failure signatures:** Mode Collapse (short, simple responses), Judge Intransitivity (cyclic preferences), Language Fidelity Drop (reasoning in English instead of target language)
- **First 3 experiments:** 1) Sanity Check - SFT Only: Train on translated SFT data only, establish baseline; 2) Privileged vs. Non-Privileged Ablation: Compare RL with/without privileged info on subset of languages; 3) Intransitivity Analysis: Generate N=8 responses, calculate percentage of cycles in judge preferences

## Open Questions the Paper Calls Out
- How can SP3F be adapted to improve generalization to out-of-domain tasks while maintaining strong in-domain performance? (The authors note mixed results on out-of-domain non-math tasks, suggesting need for better regularization techniques)
- Can privileged pairwise judges be effectively applied to other domains like code generation or symbolic logic where reference solutions are available? (The paper suggests this as an interesting direction beyond multilingual reasoning)
- Is the reliance on a high-capability proprietary model (GPT-4o-mini) as the judge a strict necessity, or can the judge capability be distilled into a smaller, open-weight model? (The method relies on GPT-4o-mini but doesn't test whether smaller models could serve as effective judges)

## Limitations
- Heavy reliance on mathematical problems with verifiable answers, limiting generalizability to broader reasoning tasks
- Performance improvements may be partly attributable to having 8x more training data rather than methodology alone
- Evaluation relies on LLM judges (gpt-4o-mini) for measuring language fidelity, introducing potential bias that compounds with training judge
- Unknown translation quality from unavailable GPT-5-Nano model, which could create performance ceiling independent of SP3F methodology

## Confidence
- **High Confidence:** SP3F improves performance on tested tasks (MGSM, MT Math100, Belebele, Global MMLU Lite); privileged judges reduce intransitivity and improve judge consistency
- **Medium Confidence:** SP3F generalizes better to unseen languages (limited evidence from one unseen language); achieves results with 1/8th the data (confounds methodology with data quantity)
- **Low Confidence:** SP3F represents a general solution for multilingual reasoning without target language data (overstated given narrow task set and heavy math emphasis)

## Next Checks
1. **Translation Quality Validation:** Conduct human evaluation of translated training data to establish baseline translation quality, then measure correlation between translation quality and downstream model performance across 18 languages
2. **Ablation on Data Efficiency:** Run controlled experiment comparing SP3F against SFT-only and RLVR-only approaches using matched training data sizes (not 1/8th) to isolate contribution of privileged pairwise feedback
3. **Cross-Domain Generalization Test:** Evaluate SP3F-7B on diverse non-mathematical multilingual reasoning tasks (commonsense reasoning, logical inference) lacking easily verifiable answers to test transfer beyond mathematical domain