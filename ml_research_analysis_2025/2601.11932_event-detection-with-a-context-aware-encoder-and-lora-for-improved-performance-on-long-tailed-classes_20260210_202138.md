---
ver: rpa2
title: Event Detection with a Context-Aware Encoder and LoRA for Improved Performance
  on Long-Tailed Classes
arxiv_id: '2601.11932'
source_url: https://arxiv.org/abs/2601.11932
tags:
- event
- llama
- performance
- lora
- macro-f1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates two key limitations in event detection:
  the unidirectional attention of decoder-only LLMs and the overreliance on Micro-F1
  metrics that mask poor performance on long-tailed event types. To address these,
  the authors propose incorporating sentence-level context into decoder-only models
  using FiLM, BiLSTM, or gated fusion methods, and apply LoRA for efficient fine-tuning.'
---

# Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes

## Quick Facts
- arXiv ID: 2601.11932
- Source URL: https://arxiv.org/abs/2601.11932
- Authors: Abdullah Al Monsur; Nitesh Vamshi Bommisetty; Gene Louis Kim
- Reference count: 10
- Primary result: Context-aware encoder and LoRA regularization significantly improve long-tailed event detection performance

## Executive Summary
This paper addresses two key limitations in event detection: the unidirectional attention of decoder-only LLMs and the overreliance on Micro-F1 metrics that mask poor performance on long-tailed event types. The authors propose incorporating sentence-level context into decoder-only models using FiLM, BiLSTM, or gated fusion methods, and apply LoRA for efficient fine-tuning. Experiments on the MA VEN and RAMS datasets show that sentence context significantly improves Macro-F1 scores for models like Llama and Qwen, with FiLM performing best overall. LoRA not only reduces computational cost but also acts as a regularizer, enhancing generalization to rare event types and improving both Micro- and Macro-F1 scores.

## Method Summary
The method uses decoder-only LLMs (Llama 3.2, Qwen2) with context injection via FiLM modulation and LoRA-based fine-tuning. Context is added by mean-pooling sentence representations to generate γ (scaling) and β (shifting) vectors that modulate token representations. LoRA with rank 8 is applied to all linear layers for parameter-efficient adaptation. The classification head uses dropout, layer norm, and GELU activation before softmax over event types. Training uses AdamW with learning rates of 5×10^-6 for Llama/Qwen and 5×10^-5 for BERT baselines.

## Key Results
- FiLM context injection improves Macro-F1 by 1-2 points over context-agnostic baselines
- LoRA acts as regularizer, improving long-tail generalization beyond computational efficiency gains
- Both Llama and Qwen show consistent improvements with context-aware architectures and LoRA
- Proper tokenization (leading space for Llama) critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting sentence-level context via FiLM modulation compensates for unidirectional attention limitations in decoder-only LLMs.
- Mechanism: Mean pooling creates global sentence embedding s. Two linear layers generate γ (scaling) and β (shifting) vectors from s. Token representations H are modulated via H_FiLM = γ⊙H + β, giving each token access to sentence semantics without architectural changes.
- Core assumption: Event type classification for rare classes requires distinguishing subtle contextual cues that decoder-only models cannot access token-by-token due to causal masking.

### Mechanism 2
- Claim: LoRA acts as an implicit regularizer that prevents overfitting to majority event types during fine-tuning.
- Mechanism: LoRA constrains weight updates to low-rank decompositions (W' = W + BA), limiting effective capacity for adaptation. This bottleneck prevents the model from memorizing frequent class patterns at the expense of rare class generalization.
- Core assumption: Full fine-tuning of large decoder models leads to overfitting on high-frequency event types because parameter count vastly exceeds rare-class supervision.

### Mechanism 3
- Claim: Macro-F1 reveals long-tail performance failures that Micro-F1 conceals.
- Mechanism: Micro-F1 aggregates TP/FP/FN across all instances, giving majority classes disproportionate influence. Macro-F1 computes per-class F1 then averages, weighting rare and frequent classes equally—exposing tail performance.
- Core assumption: A robust event detection system should perform acceptably across all event types, not just the most frequent ones.

## Foundational Learning

- **Feature-wise Linear Modulation (FiLM)**
  - Why needed here: Core technique for injecting conditioning signals (sentence context) into neural representations without modifying base architecture.
  - Quick check question: Given a context vector c and feature map H, how would you compute modulated features using FiLM?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: Enables parameter-efficient fine-tuning while providing regularization benefits critical for long-tailed generalization.
  - Quick check question: If a weight matrix W is 4096×4096 and LoRA rank r=8, how many trainable parameters does LoRA add?

- **Micro vs. Macro Averaging**
  - Why needed here: Essential for evaluating models on imbalanced datasets; incorrect metric selection can hide systematic failures.
  - Quick check question: For a 3-class problem with per-class F1 scores [0.9, 0.5, 0.3] and class frequencies [1000, 100, 10], which metric would a model optimizing for Micro-F1 prioritize?

## Architecture Onboarding

- **Component map**: Raw tokens -> LLM forward pass -> final layer hidden states -> context modulation (γ,β from mean-pooled sentence) -> linear classifier -> per-token event type prediction

- **Critical path**: Raw tokens → LLM forward pass → final layer hidden states → context modulation (γ,β from mean-pooled sentence) → linear classifier → per-token event type prediction

- **Design tradeoffs**:
  - FiLM: Best Macro-F1 but adds ~2D parameters for γ/β generators
  - BiSE-BiLSTM: Competitive but introduces sequential processing overhead
  - ConcatPool: Simplest but lower performance than FiLM
  - LoRA rank >8: No additional gains observed; stick to rank 4–8

- **Failure signatures**:
  - LLM2Vec showed no meaningful gains—bidirectional conversion alone insufficient without task-specific adaptation
  - Naive upsampling decreased both Micro- and Macro-F1—class balancing must be architectural, not data-level
  - Missing leading spaces in Llama tokenization dropped Macro-F1 from 57.23% to 39.08%

- **First 3 experiments**:
  1. Reproduce BaseTE baseline with and without LoRA on MA VEN to confirm LoRA regularization effect (target: Macro-F1 gap ≥2 points)
  2. Compare FiLM vs ConcatPool on Llama-1B to verify context injection hierarchy (FiLM should lead by ~1–2 Macro-F1 points)
  3. Profile Macro-F1 by event frequency quartile to confirm long-tail improvement pattern (expect largest gains in bottom quartile)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed context-aware architectures (specifically FiLM) vary relative to input token length and sentence complexity?
- Basis in paper: The authors state in the Limitations section that an analysis including a "breakdown by context length... would give us a better idea of the function of the models."
- Why unresolved: The current results report aggregate Macro-F1 scores, masking whether the sentence-context improvements are consistent across short triggers versus long, complex dependencies.
- What evidence would resolve it: A fine-grained evaluation reporting Macro-F1 scores binned by sentence word count or dependency tree depth.

### Open Question 2
- Question: Does the observed improvement in long-tailed class performance stem specifically from LoRA's low-rank bottleneck acting as a regularizer, or simply from the reduction in trainable parameters?
- Basis in paper: The authors hypothesize that LoRA acts as a "potent regularizer" that prevents overfitting, but they do not isolate the low-rank constraint as the specific cause.
- Why unresolved: While LoRA outperformed full finetuning, the paper did not compare LoRA against other regularization techniques (e.g., high dropout) or parameter-efficient methods (e.g., adapters) to confirm the mechanism.
- What evidence would resolve it: An ablation study comparing LoRA against full finetuning with varying dropout rates and other parameter-efficient fine-tuning (PEFT) baselines.

### Open Question 3
- Question: What specific semantic factors cause the "U-shaped pattern" where some high-frequency event types underperform compared to mid-frequency types?
- Basis in paper: Appendix J notes a "U-shaped pattern" in performance and suggests that "other factors may influence class performance," such as inter-class relatedness, but does not analyze these specific failure cases.
- Why unresolved: The paper primarily attributes difficulty to the long-tail distribution, but the U-shape indicates that frequency is not the sole predictor of error; semantic overlap may be a confounding variable.
- What evidence would resolve it: A confusion matrix analysis of the most frequent event types to identify if semantic similarity with other frequent classes drives the performance drop.

## Limitations
- Architectural details for non-FiLM context injection methods remain underspecified
- Evaluation lacks per-frequency-quartile breakdowns to verify long-tail improvements
- Hyperparameter sensitivity (LoRA rank, learning rate) untested despite central role
- Decoder-to-encoder conversion approach mentioned but not thoroughly evaluated

## Confidence
- **High Confidence (≥ 0.8)**: Context-aware modulation via FiLM improves event detection performance; LoRA provides computational efficiency and acts as a regularizer; Macro-F1 is more informative than Micro-F1 for long-tailed evaluation
- **Medium Confidence (0.5-0.8)**: FiLM is superior to alternative context injection methods; specific hyperparameter choices are optimal; improvements generalize across decoder-only LLM architectures
- **Low Confidence (< 0.5)**: Decoder-to-encoder conversion without modifications provides no benefit; approach maintains advantage on substantially different datasets

## Next Checks
1. **Per-frequency-quartile performance analysis**: Report Macro-F1 separately for event types in quartiles based on training frequency to confirm long-tail concentration of improvements.

2. **Hyperparameter sensitivity sweep**: Systematically vary LoRA rank (2, 4, 8, 16) and learning rate (1e-6 to 1e-4) for both Llama and Qwen models to determine robustness.

3. **Architecture ablation with balanced sampling**: Implement balanced sampling approach in addition to architectural solutions to isolate whether gains come from architectural innovation versus implicit data rebalancing.