---
ver: rpa2
title: Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment
  and Confusion-Aware Calibrated Margin
arxiv_id: '2505.02056'
source_url: https://arxiv.org/abs/2505.02056
tags:
- concept
- classes
- margin
- pseudolabels
- confusion-aware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of imbalanced pseudolabels in
  vision-language model adaptation. The authors identify two key causes: concept mismatch,
  where text features fail to align with visual concepts, and concept confusion, where
  similar classes are difficult to distinguish.'
---

# Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin

## Quick Facts
- **arXiv ID:** 2505.02056
- **Source URL:** https://arxiv.org/abs/2505.02056
- **Reference count:** 32
- **Primary result:** 6.29% relative gain over state-of-the-art methods on imbalanced pseudolabeling tasks.

## Executive Summary
This paper addresses the challenge of imbalanced pseudolabels in vision-language model adaptation by identifying two key issues: concept mismatch and concept confusion. The authors propose a framework that combines concept alignment (using iterative clustering and LLM-generated descriptions) with a confusion-aware calibrated margin mechanism to improve pseudolabel quality and balance. The method demonstrates significant improvements across six datasets and three learning paradigms, achieving faster training and better pseudolabel balance compared to iterative methods.

## Method Summary
The method employs a three-stage approach: (1) Concept Alignment detects mismatched classes via iterative K-means clustering and generates enhanced descriptions using an LLM to correct semantic gaps between text and visual features; (2) Confusion-Aware Calibrated Margin constructs a dynamic margin matrix based on inter-class similarity and prediction tendency to encourage more distinguishable predictions; (3) Dual-Adapter Decoupling separates learning into Main Adapter (trained on high-quality aligned pseudolabels) and Pseudo Adapter (trained on dynamic pseudolabels) to prevent error accumulation. The framework is evaluated on six benchmarks across unsupervised, semi-supervised, and transductive zero-shot learning paradigms.

## Key Results
- Achieves 6.29% relative gain over state-of-the-art methods
- Improves pseudolabel balance across all tested datasets
- Demonstrates faster training compared to iterative pseudolabel refinement methods
- Shows consistent performance across six diverse datasets (RESISC45, DTD, EuroSAT, FGVC-Aircraft, CUB, Flowers102)
- Effective across three learning paradigms (unsupervised, semi-supervised, transductive zero-shot)

## Why This Works (Mechanism)

### Mechanism 1: Concept Alignment via Iterative Cluster-Text Matching
The system identifies "concept mismatch" where text embeddings (class names) are distant from visual clusters. It uses iterative K-Means clustering to isolate mismatched classes, then employs an LLM to generate descriptive sentences that align text features with visual clusters. The core assumption is that visual features are intrinsically clusterable and the failure is purely a semantic gap. If visual features are widely dispersed or heavily overlapped, clustering fails and LLM augmentation cannot fix alignment.

### Mechanism 2: Confusion-Aware Calibrated Margin (Logit Adjustment)
This mechanism addresses "concept confusion" by adding a dynamic margin matrix to the cross-entropy loss. The matrix uses inter-class similarity and model prediction tendency to force separation in logit space, effectively lowering decision boundaries for rare but distinct classes while raising them for confusingly similar pairs. The core assumption is that prediction frequency is a reliable proxy for class imbalance. If unlabeled data is severely out-of-distribution, the margin will be calculated on the wrong distribution.

### Mechanism 3: Dual-Adapter Decoupling for Error Isolation
To prevent error accumulation from self-training, the method deploys separate Main and Pseudo adapters. The Main Adapter trains exclusively on high-confidence aligned pseudolabels to generate labels, while the Pseudo Adapter trains on these generated labels. This prevents noisy gradients from degrading the label generator. The core assumption is that the initial concept alignment produces sufficiently clean data. If the initial static set contains systematic errors, the Main Adapter will reinforce these errors across the entire dataset.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - Why needed: The entire method relies on CLIP latent space geometry where zero-shot classification measures cosine similarity between image and text embeddings.
  - Quick check: If text encoder output for "dog" is orthogonal to image encoder output for a dog photo, what happens to zero-shot probability?

- **Concept: Pseudolabeling & Confirmation Bias**
  - Why needed: The paper addresses degradation from training on self-generated guesses. Understanding how high-confidence wrong labels reinforce bad decision boundaries is crucial.
  - Quick check: Why might filtering for only highest confidence predictions fail to solve class imbalance?

- **Concept: Logit Adjustment / Margin Learning**
  - Why needed: The Calibrated Margin is a logit adjustment implementation. You need to know that adding values to logits before softmax changes probabilities.
  - Quick check: If you add a positive constant to a specific class's logit, does its probability increase or decrease relative to others?

## Architecture Onboarding

- **Component map:** CLIP Encoder (Frozen) -> Clustering Module -> LLM (External) -> Main Adapter (DPL) -> Pseudo Adapter (DUL) -> Text Adapter
- **Critical path:** The initialization phase (Concept Alignment) is the "make or break" step. If Algorithm 1 fails to identify mismatched classes or LLM generates irrelevant descriptions, the Main Adapter will be corrupted immediately.
- **Design tradeoffs:** Static vs. Dynamic Labels (trades data volume for stability), Computation vs. Precision (iterative clustering is expensive but necessary).
- **Failure signatures:** Collapse to Mean (if margin too high), Stagnant Accuracy (if mismatch detection threshold too strict).
- **First 3 experiments:**
  1. Verify Detection: Run Algorithm 1 on RESISC45 with known hard classes. Check if detected mismatched classes align with lowest zero-shot accuracy classes.
  2. Margin Sensitivity: Ablate margin scale m (as in Figure 8) to find stability window. Observe if accuracy drops sharply when m=0.
  3. Adapter Ablation: Train with shared vs. independent adapters (Table 4). Verify shared adapters cause accuracy drop, confirming error isolation hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the concept alignment module robust to hallucinations or non-visual descriptions generated by the LLM for highly abstract or fine-grained classes?
- Basis: The method relies on ChatGPT 4o-mini to generate "enhanced textual descriptions" using a specific prompt, but LLMs can struggle with specific visual attributes of obscure classes.
- Why unresolved: The paper does not analyze failure cases of LLM generation nor compare different LLMs or prompt templates.
- What evidence would resolve it: Ablation study using different LLMs or manual evaluation of generated descriptions for classes where the method failed.

### Open Question 2
- Question: Does the iterative mismatch detection algorithm remain effective when the majority of classes suffer from severe concept mismatch?
- Basis: Algorithm 1 assumes only a small fraction of classes are mismatched (threshold t=C/10), but may fail if most classes are misaligned.
- Why unresolved: The algorithm might incorrectly identify noise as mismatched classes in extreme domain shift scenarios.
- What evidence would resolve it: Experiments on adversarial datasets or domains with extreme domain shift where zero-shot CLIP accuracy is universally low.

### Open Question 3
- Question: Can the calibrated margin hyperparameter be determined dynamically in a purely unsupervised manner?
- Basis: Section 4.4 shows performance varies with m but doesn't propose a heuristic for unsupervised selection.
- Why unresolved: Determining optimal m requires validation set performance unavailable in real-world UL scenarios.
- What evidence would resolve it: A method or metric based on unlabeled data statistics that correlates with optimal m.

## Limitations

- The method's performance heavily depends on the quality of LLM-generated descriptions, which may vary across different domains and class granularities.
- The calibrated margin mechanism assumes prediction tendency is a reliable proxy for class imbalance, which may not hold under severe domain shift or when unlabeled data is out-of-distribution.
- The iterative clustering approach for concept alignment is computationally expensive and may not scale well to extremely large datasets or fine-grained classification tasks.

## Confidence

- **High Confidence:** Dual-adapter architecture effectively isolates error propagation from noisy pseudolabels (supported by ablation studies in Table 4).
- **Medium Confidence:** Concept alignment mechanism successfully identifies and corrects mismatched classes (though LLM description quality remains unverified).
- **Low Confidence:** Calibrated margin mechanism's universal applicability across all datasets and learning paradigms is not fully established, particularly under extreme class imbalance or domain shift.

## Next Checks

1. **LLM Description Quality Audit:** Manually inspect LLM-generated descriptions for detected mismatched classes to verify relevance and visual discriminative power. Compare against baseline descriptions.
2. **Margin Sensitivity Robustness Test:** Conduct experiments with varying margin scales (m=9, 10, 11, 13, 14) on a subset of classes to determine stability window and identify potential overfitting or under-correction points.
3. **Cross-Domain Generalization Test:** Evaluate complete framework on a dataset significantly different from training distribution (e.g., domain-shifted versions of existing datasets) to assess calibrated margin's effectiveness under distribution shift.