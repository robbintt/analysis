---
ver: rpa2
title: Unsupervised Domain Adaptation with an Unobservable Source Subpopulation
arxiv_id: '2509.20587'
source_url: https://arxiv.org/abs/2509.20587
tags:
- domain
- source
- target
- probability
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles unsupervised domain adaptation in the presence
  of an unobservable subpopulation within the source domain. Specifically, it addresses
  the challenge of adapting prediction models when one combination of label and background
  is entirely absent from the source data, but present in the target.
---

# Unsupervised Domain Adaptation with an Unobservable Source Subpopulation

## Quick Facts
- arXiv ID: 2509.20587
- Source URL: https://arxiv.org/abs/2509.20587
- Reference count: 40
- One-line primary result: A method for adapting prediction models when a subpopulation (Y=1, A=1) is entirely absent from labeled source data but present in target data, using KL-divergence-based proportion estimation and conditional invariance assumptions.

## Executive Summary
This paper addresses unsupervised domain adaptation (UDA) under a critical missingness: one subpopulation (Y=1, A=1) is completely absent from the labeled source domain but present in the unlabeled target domain. The authors propose a method that estimates the target subpopulation proportions through a KL-divergence-based distribution matching objective, avoiding complex feature density modeling. Theoretical guarantees include consistency of the estimator and an upper bound on prediction error. Experiments on synthetic and Waterbirds datasets demonstrate that the method can recover accurate predictions for the unobserved group, outperforming naive benchmarks.

## Method Summary
The method operates in three steps: (1) Train source models (ξ(x), ξ₀(x), τᵣ(x)) and a domain classifier κ(x) on combined source/target data for A=1 using logistic regression; (2) Estimate the target subpopulation proportion β̂₁₀ via KL-divergence minimization over target A=0 data using the learned ξ₀(x); (3) Apply closed-form formulas from Proposition 1 to correct source predictions and derive target-specific models η₁(x), η₀(x), η(x). For Waterbirds, ResNet-18 features (512-dim) are extracted from ImageNet-pretrained models.

## Key Results
- The KL-divergence-based method accurately estimates target subpopulation proportions without explicit density modeling.
- Theoretical guarantees include estimator consistency and an upper bound on prediction error.
- Experiments show the method outperforms naive benchmarks, particularly in recovering performance on the unobserved subpopulation (Y=1, A=1).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-divergence-based distribution matching estimates target proportions without modeling complex feature densities.
- Mechanism: The target distribution is a mixture of source conditional distributions weighted by unknown proportions. Lemma 2 reformulates this as a constrained KL-divergence minimization, reducing to maximizing an empirical expectation using the source classifier ξ₀(x) and observable target statistics.
- Core assumption: An anchor set exists where the unobservable subpopulation's feature density is zero; feature distributions p(X|Y, A) are sufficiently distinct.
- Evidence anchors:
  - [abstract] "We propose a Kullback-Leibler divergence-based method for estimating these proportions, avoiding the need to model complex feature distributions directly."
  - [section] Lemma 2 provides the reformulated objective: argmax E[log(ξ₀(X)β₁₀/b₁ + {1-ξ₀(X)}(ϱ-β₁₀)/(1-b₁))].
- Break condition: The anchor set assumption is violated (unobservable subpopulation has full feature support) or feature distributions p(X|Y=y, A) are identical.

### Mechanism 2
- Claim: Target prediction models can be derived in closed form once target proportions are known.
- Mechanism: Using Bayes' rule and the conditional invariance assumption, target posteriors are expressed as functions of source posteriors, the domain classifier κ(x), and target proportions (β parameters). Proposition 1 gives explicit formulas, decoupling adaptation into proportion estimation and model correction.
- Core assumption: p(X|Y, A, R=1) = p(X|Y, A, R=0) (conditional invariance across domains).
- Evidence anchors:
  - [abstract] "...we show that the prediction in the target domain can still be recovered. Specifically, we rigorously derive both background-specific and overall prediction models..."
  - [section] Proposition 1 states: η₁(x) = 1 - (β₀₁(1-π))/(α₀₁π) · κ(x)/(1-κ(x)).
- Break condition: The conditional invariance assumption fails due to feature distribution shifts for the same (Y, A) across domains.

### Mechanism 3
- Claim: Theoretical guarantees ensure reliability under sample size and tail conditions.
- Mechanism: Theorem 1 uses concentration inequalities to bound proportion estimation error as a function of source/target sample sizes. This error propagates to a generalization bound (Proposition 2) via Rademacher complexity.
- Core assumption: Sub-Gaussian tails for feature distributions, bounded loss function, and identifiability conditions.
- Evidence anchors:
  - [abstract] "Theoretical guarantees include consistency of the estimator and an upper bound on the prediction error."
  - [section] Theorem 1 gives: ||β̂ - β||₁ ≤ c χₙ √log(1/δ) where χₙ → 0 as sample sizes grow.
- Break condition: Insufficient sample sizes, heavy-tailed feature distributions, or unbounded loss function.

## Foundational Learning

- **Concept: Conditional Invariance / Label Shift**
  - Why needed here: This core assumption (Eq. 3) allows the problem to be posed. Without it, source data provides no information about target conditional distributions.
  - Quick check question: Given two hospitals (R=1, R=0), if a patient has disease Y and demographic A, would their symptom distribution X be the same in both hospitals? If not, this assumption fails.

- **Concept: Mixture Proportion Estimation / PU Learning**
  - Why needed here: Estimating β is essentially a mixture proportion estimation problem. For A=1 background, it reduces to PU learning where the "positive" class (Y=1, A=1) is unobserved in source.
  - Quick check question: You have samples from a mixture of two unknown distributions, and samples from only one component. Can you estimate the mixture weight? Under what condition is this identifiable?

- **Concept: Generalization Bounds via Rademacher Complexity**
  - Why needed here: Proposition 2 uses Rademacher complexity to bound classifier error trained with reweighted source data, connecting proportion estimate quality to final prediction risk.
  - Quick check question: How does hypothesis class complexity (e.g., deep neural network vs. linear model) affect the generalization bound in Proposition 2?

## Architecture Onboarding

- **Component map**: Source Models (ξ(x), ξ₀(x), τ₁(x)) → Domain Classifier (κ(x)) → KL Optimizer (β̂₁₀) → Target Prediction Corrector (η₁(x), η₀(x), η(x))
- **Critical path**: The entire method's success hinges on accurate estimation of β̂ via the KL optimizer (Component 3). Errors here propagate through all target predictions.
- **Design tradeoffs**:
  1. **Divergence choice**: KL divergence is used for analytical convenience, avoiding density estimation. Alternatives (L2, MMD) could be more robust but may lose closed-form gradients.
  2. **Anchor set assumption**: This is a strong theoretical requirement. In practice, its violation is hard to diagnose. The method's robustness to near-violation is not theoretically characterized.
  3. **Model capacity for ξ₀(x)**: A more complex model may capture finer details for better distribution matching, but risks overfitting on the source subpopulation.
- **Failure signatures**:
  1. **Proportion estimates at boundary**: If β̂ hits 0 or 1, check identifiability (p₁₀(x) ≈ p₀₀(x)) or data quality.
  2. **High variance in β̂ across runs**: Indicates the objective in Lemma 2 is flat or target sample size n₀·₀ is too small.
  3. **Degraded performance on unobserved group (Y=1,A=1) despite good overall accuracy**: Suggests κ(x) or the anchor set assumption is problematic.
- **First 3 experiments**:
  1. **Synthetic validation**: Replicate the Gaussian setup from Section 6. Verify that β̂ converges to true β as n increases, and compare η₁(x) vs. naive ξ₁(x) on the held-out group.
  2. **Ablation on anchor set**: On synthetic data, gradually violate the anchor set assumption (overlap p₁₁(x) and p₀₁(x)) and measure the degradation in β̂ accuracy and downstream prediction error.
  3. **Real-data benchmark**: Apply to Waterbirds as in Section 7. Compare against the naive benchmark. Critically, report performance *separately* for the four (Y,A) subgroups in the target, not just overall accuracy, to verify recovery on the unobserved group.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the identification strategy and estimation efficiency be maintained when extending the framework to multi-class labels ($n_y > 2$) or multi-level environments ($n_a > 2$)?
- Basis in paper: [explicit] The Discussion notes that while tools generalize to larger spaces, identification requires $(n_y - 2)n_a + 1$ anchor set assumptions, significantly increasing complexity compared to the binary case.
- Why unresolved: The current theoretical guarantees and experiments are strictly limited to binary $Y$ and $A$.
- What evidence would resolve it: Derivation of identification conditions and empirical validation on datasets with more than two classes or backgrounds.

### Open Question 2
- Question: What is the optimal choice for the moment function $m(x)$ in the alternative moment-matching approach for estimating $\beta$?
- Basis in paper: [explicit] Appendix S.1 states, "A further research question of interest is to identify the optimal choice of this moment function... by borrowing the semiparametric techniques."
- Why unresolved: The primary paper utilizes KL-divergence; the moment-matching alternative is presented but the optimal selection of $m(x)$ is not derived.
- What evidence would resolve it: Derivation of the efficient influence function or semiparametric optimality criteria for the moment function.

### Open Question 3
- Question: Is the proposed method robust to violations of the conditional invariance assumption ($p(X|Y,A)$ remains constant across domains)?
- Basis in paper: [inferred] The derivation of target prediction models in Proposition 1 relies entirely on the assumption $p(X|Y,A,R=1) = p(X|Y,A,R=0)$, which may be fragile in practice.
- Why unresolved: The paper provides no theoretical bounds or empirical analysis for scenarios where this conditional distribution shifts.
- What evidence would resolve it: Theoretical analysis of error bounds under approximate invariance or empirical stress tests with perturbed conditional distributions.

## Limitations

- The anchor set assumption (zero feature density for the unobservable subpopulation) is theoretically convenient but likely violated in real-world data, and the method's robustness to near-violations is not characterized.
- The sub-Gaussian tail assumption and requirement for sufficient sample sizes are critical for theoretical guarantees but not empirically validated across diverse datasets.
- The method's performance depends heavily on accurate estimation of the domain classifier κ(x), which may be challenging when the unobservable subpopulation has distinctive features.

## Confidence

- **High**: The theoretical framework for proportion estimation via KL divergence (Lemma 2) is mathematically sound given the stated assumptions.
- **Medium**: The closed-form expressions for target predictions (Proposition 1) are correctly derived but rely heavily on the conditional invariance assumption.
- **Low**: Empirical results demonstrate effectiveness but the synthetic data is idealized and the real-data experiment lacks rigorous subgroup performance analysis.

## Next Checks

1. **Anchor Set Violation Analysis**: Systematically relax the anchor set assumption in synthetic experiments by introducing controlled overlap between p₁₁(x) and p₀₁(x). Measure how β̂ accuracy and downstream prediction error degrade as the assumption is increasingly violated.

2. **Heavy Tail Robustness**: Replace the sub-Gaussian feature distributions with heavy-tailed alternatives (e.g., Cauchy or t-distributions) in synthetic experiments. Assess whether the consistency guarantees and error bounds from Theorem 1 still hold or degrade predictably.

3. **Subgroup Performance Dissection**: For the Waterbirds experiment, compute and report accuracy and F1 scores separately for all four (Y, A) target subgroups, not just overall metrics. This will definitively show whether the method recovers performance on the specifically unobserved group (Y=1, A=1) as claimed.