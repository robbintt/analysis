---
ver: rpa2
title: Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?
arxiv_id: '2506.12119'
source_url: https://arxiv.org/abs/2506.12119
tags:
- arxiv
- dense
- data
- optimal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses whether Mixture-of-Experts (MoE) language\
  \ models can outperform dense models when all resources\u2014total parameters, training\
  \ compute, and data\u2014are held strictly equal. To investigate this, the authors\
  \ propose a three-step methodology: first, they optimize MoE architecture by selecting\
  \ the best arrangement of MoE and dense layers, gate normalization, and parameter\
  \ allocation; second, they analyze the effect of activation rate (the proportion\
  \ of activated parameters per token) under fixed compute and parameter constraints;\
  \ third, they introduce a data reuse strategy to equalize the data budget across\
  \ MoE and dense models."
---

# Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?

## Quick Facts
- **arXiv ID:** 2506.12119
- **Source URL:** https://arxiv.org/abs/2506.12119
- **Reference count:** 40
- **Primary result:** MoE models with optimal activation rate ~20% outperform dense models under strictly equal parameters, compute, and data.

## Executive Summary
This paper investigates whether Mixture-of-Experts (MoE) language models can surpass dense models when all resources—total parameters, training compute, and data—are held strictly equal. Through a systematic three-step methodology involving architectural optimization, activation rate analysis, and data reuse strategy, the authors demonstrate that MoE models can indeed achieve superior performance. The key finding is an optimal activation rate of approximately 20% that balances capacity and computational efficiency, enabling MoE to outperform dense models across multiple scales (2B, 3B, and 7B parameters) and tasks (upstream and downstream benchmarks).

## Method Summary
The authors propose a three-step methodology: first, they optimize MoE architecture by selecting the best arrangement of MoE and dense layers, gate normalization, and parameter allocation; second, they analyze the effect of activation rate under fixed compute and parameter constraints; third, they introduce a data reuse strategy to equalize the data budget across MoE and dense models. Experiments were conducted on models of 2B, 3B, and 7B parameters, training nearly 200 2B-scale and over 50 7B-scale models on 50 trillion tokens total. The optimal architecture uses a 1dense+SE configuration with shared experts and no gate normalization, achieving best performance at activation rate ~20%.

## Key Results
- MoE models with activation rate ~20% consistently outperform dense models under equal total parameters, compute, and data across 2B-7B scales
- The optimal activation rate of ~20% is independent of model size, remaining consistent across tested scales
- Data reuse strategy successfully mitigates MoE's additional data requirements with minimal performance degradation
- 1dense+SE architecture (1 dense layer followed by MoE layers with shared experts) provides the best performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoE models with activation rate ra ≈ 20% outperform dense models under equal total parameters (N), compute (C), and data (D).
- Mechanism: The activation rate ra determines the balance between capacity and compute per token. At ra ≈ 20%, the model maintains sufficient expert specialization while preserving computational efficiency. Too low ra (< 10%) limits knowledge storage; too high ra (> 50%) dilutes expert specialization.
- Core assumption: Expert specialization improves when each expert handles a focused subset of patterns, which emerges at optimal activation rates.
- Evidence anchors:
  - [abstract]: "MoE models with this optimal activation rate surpass dense models in both upstream and downstream tasks under equal resources."
  - [Section 5.1, Figure 1b]: "When plotting the results from a fixed training compute perspective (C = C0 = 9.1e20)... the optimal AR point is approximately r∗∗a ≈ 20%."
  - [Section 5.3]: "r∗∗a remains consistent for both 2B and 7B models at approximately 20%, suggesting that r∗∗a is independent of model size."
  - [corpus]: Weak direct support. Related work (Abnar et al., 2025) finds optimal sparsity scales with model size—contradicting this paper's conclusion—though that work used undertrained models.

### Mechanism 2
- Claim: Data reuse (multi-epoch training on smaller datasets) mitigates MoE's additional data requirements with minimal performance degradation.
- Mechanism: MoE routers benefit from repeated exposure to the same data, enabling more refined routing decisions across epochs. This counters the degradation typically seen in dense models from data repetition.
- Core assumption: Router learning continues across epochs even when expert parameters saturate.
- Evidence anchors:
  - [abstract]: "Data reuse successfully mitigates MoE's additional data needs, making the performance gains robust."
  - [Section 6]: "Surprisingly, the loose scheme (green dashed line) often outperforms training with unique dataset."
  - [Section 6, Discussion]: "We hypothesize that MoE routers benefit from additional epochs, potentially leading to more refined routing decisions."
  - [corpus]: Muennighoff et al. [2023] (cited in paper) found negligible loss for up to 4 epochs in multi-epoch training.

### Mechanism 3
- Claim: Optimized MoE architecture (1dense+SE configuration with K>1, no gate normalization) maximizes performance under resource constraints.
- Mechanism: A single dense layer before MoE layers stabilizes training. Shared experts capture common patterns across tokens, while routed experts specialize. The ratio Dse/(Dse + KDe) has minimal impact, but K values should avoid extremes (K=1 or large K).
- Core assumption: Dense layer provides gradient stability early in training; shared experts reduce redundancy across routed experts.
- Evidence anchors:
  - [Section 4]: "1dense+SE performs the best, possibly because the dense layer contributes to more stable training."
  - [Section 4, Table 5]: "The ratio of shared expert size to total expert size Dse/(Dse + KDe) has minimal impact on model performance."
  - [Section 4, Table 7]: "Larger K generally hurt performance, except in the first group where K = 1."

## Foundational Learning

- Concept: **Activation Rate (ra = Na/N)**
  - Why needed here: The ratio of activated parameters to total parameters is the primary lever for trading off compute per token vs. model capacity. Understanding this is essential for interpreting the ~20% optimal value.
  - Quick check question: If a model has N=7B total parameters and ra=20%, how many parameters are activated per forward pass?

- Concept: **Training Compute Budget (C ≈ 3 × M × D)**
  - Why needed here: The paper enforces strict compute parity between MoE and dense models. MoE's lower per-token compute (lower ra) means it requires more tokens D to match total compute C.
  - Quick check question: Given equal C, why does an MoE model with ra=20% need ~5× more tokens than a dense model?

- Concept: **Expert Specialization**
  - Why needed here: The hypothesized mechanism for MoE superiority is that experts learn to specialize in specific patterns. This connects activation rate to model quality.
  - Quick check question: What happens to expert specialization when ra approaches 100% (dense) vs. when ra is very low (< 5%)?

## Architecture Onboarding

- Component map:
  - Dense layers (Ld) -> MoE layers (Le) -> Router/gate -> Top-K selection -> Activated experts (K out of E total)

- Critical path:
  1. Choose total parameters N and compute budget C
  2. Set architecture: 1dense layer + MoE layers with shared experts (Dse = K×De)
  3. Select shape hyperparameters: ζ ≈ 88, µ ≈ 22, α = 2.77
  4. Set activation rate ra ≈ 20% to determine K, E, De
  5. Calculate required tokens D = C / (3 × M) where M ≈ 2×ra×N
  6. If D exceeds unique data, apply data reuse (multi-epoch training)

- Design tradeoffs:
  - Lower ra → less compute per token, needs more data to match C
  - Higher ra → more compute per token, approaches dense model behavior
  - More experts E with smaller De (fine granularity) vs. fewer larger experts: paper favors moderate E with K×De constant
  - Strict data reuse (exact epoch count for compute parity) vs. loose reuse (fixed 2 epochs): loose often performs better

- Failure signatures:
  - ra < 10%: Insufficient activated capacity, model underperforms dense baseline
  - ra > 50%: Weak expert specialization, diminished returns
  - K = 1 (Switch-style): Can work but is an edge case; K ≥ 2 generally preferred
  - Gate normalization with K = 1: Zero gradient issue
  - Large K values: Performance degradation observed
  - Data reuse with many epochs (>5): Knowledge benchmarks degrade; reasoning remains stable

- First 3 experiments:
  1. **Replicate optimal ra at small scale**: Train N ≈ 500M MoE models with ra ∈ {10%, 20%, 35%, 50%} under fixed C. Verify ra ≈ 20% minimizes validation BPC.
  2. **Ablate shared experts**: Compare 1dense+SE vs. 1dense (no SE) vs. full-MoE configurations at fixed N. Measure both BPC and training stability (loss variance).
  3. **Test data reuse limits**: For an ra=20% MoE model, compare training on unique data D vs. repeated data (2, 4, 8 epochs on D/2, D/4, D/8). Measure where performance degradation becomes significant across knowledge vs. reasoning benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- The optimal activation rate of ~20% is demonstrated only for 2B-7B parameter models; scalability to larger models remains uncertain
- The data reuse strategy's effectiveness depends on dataset quality and may not generalize to all data distributions
- The space of MoE architectures was not exhaustively explored, potentially missing other optimal configurations

## Confidence
- **High Confidence:** The experimental methodology and the 20% optimal activation rate finding are well-supported and reproducible
- **Medium Confidence:** The claim that data reuse fully mitigates MoE's additional data requirements, as the underlying mechanism remains speculative
- **Medium Confidence:** The architectural superiority of 1dense+SE configuration, given the limited exploration of alternative architectures

## Next Checks
1. **Scale Boundary Test:** Train 15B-30B parameter MoE models with ra ∈ {15%, 20%, 25%, 35%} under fixed compute. Verify whether the 20% optimal rate persists or shifts at larger scales.

2. **Data Quality Dependency:** Repeat the data reuse experiments using datasets of varying quality (high-quality curated data vs. lower-quality web-scraped data). Measure the point at which multi-epoch training causes performance degradation in MoE versus dense models.

3. **Router Learning Mechanism:** Implement gradient visualization to track router weight updates across training epochs. Compare expert utilization patterns and routing stability between single-epoch and multi-epoch training to empirically validate whether routers continue to refine decisions across epochs.