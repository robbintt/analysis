---
ver: rpa2
title: 'ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression'
arxiv_id: '2501.07045'
source_url: https://arxiv.org/abs/2501.07045
tags:
- regression
- learning
- contrastive
- deep
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACCon, an angle-compensated contrastive regularizer
  for deep regression that addresses the challenge of capturing relationships among
  continuous labels in feature space. The method introduces angle compensation to
  refine distance-weighted negative cosine similarity in contrastive learning, projecting
  representations onto a semi-hypersphere that preserves label relationships.
---

# ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression

## Quick Facts
- **arXiv ID:** 2501.07045
- **Source URL:** https://arxiv.org/abs/2501.07045
- **Reference count:** 40
- **Key result:** Proposes angle-compensated contrastive regularizer that improves deep regression performance, achieving up to 9.27% improvement in MAE on AgeDB dataset

## Executive Summary
This paper addresses the challenge of capturing relationships among continuous labels in deep regression tasks. The proposed ACCon method introduces angle compensation to refine distance-weighted negative cosine similarity in contrastive learning, projecting representations onto a semi-hypersphere that preserves label relationships. The approach is theoretically justified and demonstrates plug-and-play compatibility with existing contrastive learning methods. Extensive experiments on three datasets show significant improvements over state-of-the-art methods, particularly excelling in imbalanced regression tasks and data-limited scenarios.

## Method Summary
ACCon modifies the standard supervised contrastive loss by introducing an angle-compensated similarity term for negative pairs. Instead of directly weighting cosine similarity, it calculates a target angle for negatives based on normalized label distance, then derives a compensated angle to enforce this geometry. The method is designed as a regularizer that can be combined with standard regression losses. The total loss function is a weighted sum of regression loss and the angle-compensated contrastive loss. The approach projects input onto a semi-hypersphere, preserving relationships among labels while maintaining theoretical guarantees about feature locations.

## Key Results
- Achieves up to 9.27% improvement in MAE and 4.63% improvement in G-means on AgeDB dataset
- Demonstrates plug-and-play compatibility with existing contrastive learning methods
- Excels particularly in imbalanced regression tasks and data-limited scenarios
- Shows robust performance across various hyperparameter settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing a linear negative correlation between label distances and representation similarities on a semi-hypersphere improves distance-aware representation learning for regression.
- **Mechanism:** ACCon introduces an "angle compensation" term (φ) to modify the cosine similarity between anchor and negative pairs. Instead of directly weighting cosine similarity, it calculates a target angle (θ̂) for negatives based on normalized label distance, then derives a compensated angle (θ̃) to enforce this geometry.
- **Core assumption:** The paper hypothesizes a linear negative correlation between label distances and representation similarities in an ideal feature space.
- **Break condition:** This mechanism assumes continuous labels with meaningful distance metrics. Performance degrades if the assumption of linear correlation does not hold for the dataset.

### Mechanism 2
- **Claim:** Modifying the negative sample similarity term in the contrastive loss function captures fine-grained relationships among continuous labels.
- **Mechanism:** Standard supervised contrastive loss treats all negatives equally, which is suboptimal for regression where samples with similar labels should be more similar in representation. ACCon modifies the denominator by replacing standard negative cosine similarity with angle-compensated similarity.
- **Core assumption:** The mechanism assumes existing representation is L2-normalized and anchor/negative representations are not yet perfectly aligned with target angles.
- **Break condition:** The mechanism relies on labeled dataset to compute angle compensation during training. Cannot be applied in purely self-supervised manner.

### Mechanism 3
- **Claim:** Joint optimization of regression loss and angle-compensated contrastive loss leads to better feature representations that improve downstream regression performance.
- **Mechanism:** ACCon is designed as a regularizer. The total loss function is a weighted sum of standard regression loss and ACCon loss. The regression loss provides primary signal for prediction accuracy while ACCon loss shapes feature space to be more semantically meaningful.
- **Core assumption:** Improving geometry of feature space as defined by ACCon is complementary to and assists primary regression objective.
- **Break condition:** The weighting coefficient γ between the two losses is a critical hyperparameter. If too high, model may over-regularize at cost of prediction accuracy.

## Foundational Learning

- **Concept:** Supervised Contrastive Learning (SupCon)
  - **Why needed here:** ACCon is a modification of SupCon loss. Understanding standard contrastive framework is essential to grasp how ACCon modifies it for regression.
  - **Quick check question:** Can you explain how SupCon differs from standard cross-entropy or triplet loss, and what positive and negative pairs represent?

- **Concept:** Cosine Similarity and Angular Distance
  - **Why needed here:** Core innovation is manipulating angles on semi-hypersphere. Understanding relationship between dot product, cosine similarity, and angular distance is critical for implementing angle compensation term.
  - **Quick check question:** If two feature vectors have cosine similarity of 0.5, what is the angle between them in radians and degrees?

- **Concept:** Representation Learning for Regression
  - **Why needed here:** Problem ACCon solves is about preserving ordinal and distance information in feature space for continuous targets. Understanding this goal contextualizes why standard regression losses alone might be insufficient.
  - **Quick check question:** Why might a model trained with pure MAE fail to capture semantic relationship that label of 50 is more similar to 51 than to 90?

## Architecture Onboarding

- **Component map:** Data Augmentation -> Feature Encoder -> Projection Layer -> Predictor
- **Critical path:**
  1. Batch of (input, label) pairs drawn
  2. Two augmentations created for each input
  3. Encoder and projection layer produce batch of normalized representations
  4. For each anchor: identify positive/negative pairs, compute ideal angle θ̂, compute angle-compensated similarity cos(θ̃)
  5. Compute L_ACCon using modified negative similarities
  6. Compute L_reg from predictor's output
  7. Total loss L = L_reg + γ * L_ACCon is backpropagated

- **Design tradeoffs:**
  - **Label Binning (M bins):** Finer bins capture more precise similarity but may lead to very few positive samples per anchor in sparse data regions
  - **Projection Dimension (d_l):** Larger dimension preserves more information but increases computation
  - **Weighting Coefficient (γ):** Balances primary regression task with representation shaping
  - **Smoothing Term (ε):** Prevents division by zero in angle compensation calculation

- **Failure signatures:**
  - **Loss NaN:** Likely caused by numerical instability in sqrt(1 - (z_i*z_m^T)^2) term. Ensure smoothing term is added correctly
  - **No Improvement over Baseline:** Check if label normalization is computed correctly, verify positive pairs are being sampled, assumption of linear correlation may be too strong
  - **Regression Performance Degrades:** Weight γ might be too high, causing model to prioritize feature geometry over prediction accuracy

- **First 3 experiments:**
  1. **Baseline Reproduction:** Implement architecture with L_reg only to establish baseline performance metric on chosen dataset
  2. **Ablation on Positive/Negative Definition:** Implement ACCon with single, simple binning strategy. Verify angle-compensated term is being computed correctly
  3. **Full Model Tuning:** Train full model with L = L_reg + γ * L_ACCon. Perform small hyperparameter search on validation set for key parameters

## Open Questions the Paper Calls Out

- **Open Question 1:** Can ACCon be effectively adapted for dense pixelwise regression tasks (e.g., depth estimation) given current high computational resource requirements?
  - **Basis in paper:** Appendix D lists application to dense pixelwise regression as primary limitation due to high computational resource requirements
  - **Why unresolved:** Current implementation is resource-intensive, making it difficult to apply to tasks requiring precise, pixel-level predictions
  - **What evidence would resolve it:** Modified framework applied to dense prediction benchmarks that demonstrates competitive performance with manageable computational overhead

- **Open Question 2:** Can angle-compensated mechanism be extended to unsupervised or semi-supervised regression settings where labeled training data is scarce or unavailable?
  - **Basis in paper:** Appendix D states method currently requires labeled training data which may not be universally available
  - **Why unresolved:** Core mechanism relies on label distances to calculate angle compensation, creating dependency on ground-truth annotations
  - **What evidence would resolve it:** Extension utilizing pseudo-labels or self-supervisory signals that achieves comparable regularization without ground-truth labels

- **Open Question 3:** Does suboptimal performance on STS-B-DIR dataset stem primarily from limited data volume or from constraints specific to semantic text augmentation?
  - **Basis in paper:** Page 6 notes method did not outperform baselines on STS-B-DIR and states "We hypothesize that this exception may be attributed to limited amount of data and constraints on data augmentation techniques"
  - **Why unresolved:** Authors identified failure but did not isolate whether data scarcity or nature of NLP augmentation was bottleneck
  - **What evidence would resolve it:** Ablation studies scaling dataset size and employing varied NLP augmentation strategies to observe performance impact

## Limitations
- Strong assumption of linear correlation between label distances and representation similarities may not hold for all datasets
- Requires labeled data and cannot be applied in purely self-supervised settings
- Performance depends critically on hyperparameter γ that controls balance between regression accuracy and feature space regularization

## Confidence

- **High Confidence:** Mechanism of angle compensation is mathematically sound and directly modifies contrastive loss as claimed. Experimental results showing improved MAE on AgeDB and STS-B are robust and reproducible
- **Medium Confidence:** Theoretical justification that ACCon enforces features on semi-hypersphere according to label distances is provided but proof relies on assumption of linear correlation
- **Low Confidence:** Claim about "data-limited scenarios" is mentioned but not extensively validated with very small datasets in main experiments

## Next Checks

1. **Robustness to Non-Linear Relationships:** Test ACCon on synthetic regression dataset where relationship between labels is clearly non-linear (e.g., circular or clustered) to verify mechanism breaks down when linear correlation assumption fails

2. **Label Binning Sensitivity:** Systematically vary number of bins (M) used to define positive pairs and measure impact on performance, particularly in sparse data regions, to quantify tradeoff between precision and sample availability

3. **Comparison with Self-Supervised Baselines:** Implement self-supervised contrastive method (like SimCLR) and compare its performance with ACCon on labeled regression task to isolate benefit of supervised angle compensation versus contrastive structure itself