---
ver: rpa2
title: 'MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models'
arxiv_id: '2505.11613'
source_url: https://arxiv.org/abs/2505.11613
tags:
- clinical
- arxiv
- medguide
- score
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The MedGUIDE benchmark evaluates whether large language models
  can follow structured clinical decision trees from NCCN guidelines. It uses 55 decision
  trees across 17 cancer types, generates 16K MCQs from synthetic clinical vignettes,
  and filters to 7,747 high-quality samples using expert-labeled reward models and
  LLM-as-a-judge ensembles.
---

# MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models

## Quick Facts
- arXiv ID: 2505.11613
- Source URL: https://arxiv.org/abs/2505.11613
- Reference count: 40
- Primary result: Medical LLMs underperform general models on structured clinical decision trees; guideline-in-context prompting improves accuracy by up to 102%

## Executive Summary
MedGUIDE evaluates whether large language models can follow structured clinical decision trees from NCCN guidelines using MCQs derived from synthetic patient vignettes. The benchmark uses 55 decision trees across 17 cancer types, generates 16K MCQs, and filters to 7,747 high-quality samples via expert-labeled reward models and LLM-as-a-judge ensembles. Across 25 models tested, even domain-specific medical LLMs often underperform, suggesting limitations in following guideline-based decision logic. Including the guideline in context improves performance significantly (e.g., 86% accuracy gain for Meditron-7B), while continued pretraining on guidelines yields smaller gains.

## Method Summary
The benchmark constructs MCQs by converting NCCN decision trees to JSON, enumerating root-to-leaf paths, and generating clinical vignettes using multiple LLMs. Quality filtering applies a two-stage process: (1) a 5-head reward model trained on 500 expert-annotated samples scores clinical plausibility and utility; (2) an LLM-as-a-judge ensemble rates clarity and safety. Samples passing both filters (min >2, avg >3) are retained. Models are evaluated on 7,747 filtered MCQs using accuracy and weighted accuracy (difficulty-adjusted).

## Key Results
- Medical LLMs (Meditron, Medalpaca, ClinicalCamel) underperform general models despite domain training
- In-context guideline provision yields 86-102% accuracy gains for Meditron models
- Continued pretraining on guidelines provides smaller improvements than in-context grounding
- Even top models struggle with structured clinical reasoning beyond general medical knowledge

## Why This Works (Mechanism)

### Mechanism 1: In-Context Guideline Grounding
Providing structured clinical guidelines directly in the prompt substantially improves LLM decision-making accuracy. The decision tree (in JSON form) is included in the context, allowing the model to parse and traverse the tree step-by-step rather than relying on parametric memory. The paper shows Meditron-7B improves by 86% and Meditron-70B by 102% in weighted accuracy when the guideline is provided. Core assumption: Models can reliably parse structured JSON trees and follow conditional branching logic when explicitly grounded.

### Mechanism 2: Dual-Stage Quality Filtering via Reward Models and LLM-as-Judge
Combining expert-labeled reward models with LLM-as-judge ensembles yields a high-quality MCQ benchmark. Six medical experts annotate 500 MCQs on 5 clinical criteria (plausibility, utility, decision path quality, alignment, answer accuracy). A 5-head reward model (Qwen2.5-7B-Instruct backbone) is trained and scores the remaining 15,500 samples. Separately, 4 LLMs rate all 16K samples on 5 general criteria (clarity, consistency, safety, professionalism, option distinctiveness). Samples passing both filters (min >2, avg >3) yield 7,747 final samples.

### Mechanism 3: Synthetic Vignette Generation from Decision Paths
Enumerating all root-to-leaf paths from clinical decision trees and generating corresponding patient vignettes creates diverse, guideline-aligned MCQs. NCCN PDFs are converted to JSON, GPT-4o enumerates all valid paths (manually reviewed), and multiple LLMs (GPT-4o, Claude, DeepSeek, Llama, Qwen, Mistral) generate diverse clinical profiles. Each vignette ends with a question about the next step; options are all leaf nodes from the same tree.

## Foundational Learning

- Concept: Decision Trees as Clinical Guidelines
  - Why needed here: NCCN guidelines encode diagnostic/treatment pathways as branching logic; understanding node types (condition, action, terminal) is prerequisite to parsing JSON trees and generating path-aligned vignettes.
  - Quick check question: Given a tree node "Early relapse (<6 mo) after ATRA" with two children (therapy options), what determines which branch a patient follows?

- Concept: Reward Model Training (Multi-Head Regression)
  - Why needed here: The 5-head reward model predicts expert ratings across clinical dimensions; understanding MSE loss, backbone selection, and validation metrics is needed to interpret the filtering pipeline.
  - Quick check question: If the reward model consistently over-predicts "Clinical Plausibility" by 1 point on a 5-point scale, what happens to sample retention under the min >2 rule?

- Concept: In-Context Learning vs. Continued Pretraining
  - Why needed here: Method I provides guidelines at inference; Method II fine-tunes on guidelines. Understanding why the former yields larger gains (models don't need to memorize but can parse) explains the performance gap.
  - Quick check question: Why might continued pretraining on 55 trees yield smaller gains than providing the relevant tree at inference time?

## Architecture Onboarding

- Component map: NCCN PDFs -> JSON trees -> path enumeration -> clinical vignette generation -> MCQ creation -> quality filtering (reward model + LLM-as-judge) -> benchmark evaluation
- Critical path: Guideline selection -> JSON conversion -> path enumeration -> vignette generation -> quality filtering -> benchmark evaluation. The filtering stage is the bottleneck (expert annotation + reward model training).
- Design tradeoffs: Synthetic vignettes enable scale (16K -> 7.7K) but may lack clinical nuance of real patient data; multi-LLM generation increases diversity but introduces variable plausibility (hence strict filtering); weighted accuracy accounts for option count but assumes difficulty scales with branch factor.
- Failure signatures: Medical LLMs underperform due to older backbones (Llama2) and weaker instruction-following, not lack of domain knowledge (Section 4.3); continued pretraining yields marginal gains except Meditron-7B (low baseline, more headroom); without guideline context, models hallucinate plausible-but-guideline-inconsistent rationales (case study, Appendix I.1).
- First 3 experiments: 1) Baseline evaluation: Run all 25 models on MedGUIDE at temperature 0; report accuracy and weighted accuracy. 2) Method I (guideline-in-context): For a subset (e.g., Meditron-7B, Llama-3.1-8B, GPT-4.1), prepend the relevant JSON tree to the prompt. Measure relative gain. 3) Correlation analysis: Evaluate the same models on IFEval and MMLU-Professional Medicine; compute correlations with MedGUIDE.

## Open Questions the Paper Calls Out

- Can reinforcement learning from process-based clinical feedback significantly improve LLM adherence to structured decision trees compared to the modest gains observed from continued pretraining? The paper tested two improvement methods but did not explore RL-based approaches that might better align model reasoning with stepwise clinical logic.

- Why do domain-specific medical LLMs (Meditron, Medalpaca, ClinicalCamel) consistently underperform general-purpose models on guideline-following tasks despite their specialized training? The paper identifies the phenomenon and offers hypotheses but does not isolate whether the cause is training data composition, backbone architecture, or lack of structured reasoning in medical corpora.

- How well does performance on MedGUIDE transfer to real clinical settings with authentic patient cases, as opposed to synthetic vignettes? The benchmark uses synthetic vignettes generated by multiple LLMs from decision tree paths. While quality filtering was applied via reward models and expert annotation, it is unknown whether the observed model failures reflect oncology-specific complexity or a broader inability to follow structured clinical protocols.

## Limitations

- Synthetic vignettes may not capture the complexity and nuance of real clinical cases, potentially limiting external validity
- The filtering pipeline depends heavily on expert annotation quality, but the 500 annotated samples are not publicly available
- The benchmark uses static JSON guidelines rather than dynamic patient history, which may overestimate real-world applicability

## Confidence

- High Confidence: The 86% accuracy gain from in-context guideline provision and the underperformance of medical LLMs are well-supported by direct experimental results
- Medium Confidence: The claim that continued pretraining yields smaller gains than in-context grounding is supported, but the mechanism is inferred rather than directly tested
- Medium Confidence: The correlation analysis shows MedGUIDE aligns more with MMLU than IFEval, but statistical significance and practical implications are not fully explored

## Next Checks

1. Evaluate MedGUIDE performance on a small set of real patient cases from EHR data to assess whether synthetic vignette performance generalizes to real-world clinical decision-making

2. Reconstruct the reward model using publicly available clinical QA datasets to test whether the filtering criteria (min >2, avg >3) are overly conservative or permissive

3. Modify the benchmark to include patient history in the prompt and measure how much performance degrades without full context, simulating real clinical scenarios