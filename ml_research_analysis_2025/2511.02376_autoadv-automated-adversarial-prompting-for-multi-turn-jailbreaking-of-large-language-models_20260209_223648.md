---
ver: rpa2
title: 'AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large
  Language Models'
arxiv_id: '2511.02376'
source_url: https://arxiv.org/abs/2511.02376
tags:
- target
- autoadv
- multi-turn
- keywords
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AutoAdv is a training-free framework for automated multi-turn
  jailbreaking of LLMs that achieves up to 95% attack success rate on Llama-3.1-8B
  within six turns. The framework uses three adaptive mechanisms: a pattern manager
  that learns from successful attacks, a temperature manager that dynamically adjusts
  sampling parameters, and a two-phase rewriting strategy that disguises harmful requests
  and iteratively refines them.'
---

# AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models

## Quick Facts
- **arXiv ID:** 2511.02376
- **Source URL:** https://arxiv.org/abs/2511.02376
- **Reference count:** 40
- **Primary result:** AutoAdv achieves up to 95% attack success rate on Llama-3.1-8B within six turns

## Executive Summary
AutoAdv introduces a training-free framework for automated multi-turn jailbreaking of large language models that achieves up to 95% attack success rate on Llama-3.1-8B within six turns. The framework combines three adaptive mechanisms: a pattern manager that learns from successful attacks, a temperature manager that dynamically adjusts sampling parameters, and a two-phase rewriting strategy that disguises harmful requests and iteratively refines them. AutoAdv significantly outperforms single-turn approaches, achieving up to 24% higher success rates across multiple commercial and open-source models including GPT-4o-mini, Qwen3-235B, and Mistral-7B.

## Method Summary
AutoAdv operates as a training-free framework that uses iterative prompt rewriting guided by feedback from target models. The system employs an attacker LLM (Grok 3 Mini) to generate adversarial rewrites, an evaluator LLM (GPT-4o-mini) to score responses using a modified StrongREJECT metric, and three adaptive components: a pattern manager that learns successful techniques through keyword matching across 28 categories, a temperature manager that dynamically adjusts sampling parameters based on score trajectories, and a two-phase rewriting strategy (initial disguise followed by adaptive refinement). The framework runs for up to six turns, with attacks considered successful when scoring S > 0.5, where S = (C + L + 5(1-R))/15.

## Key Results
- AutoAdv achieves 95% attack success rate on Llama-3.1-8B within six turns
- Multi-turn jailbreaks outperform single-turn attacks by up to 24% across multiple models
- Pattern manager and temperature manager provide 6% and 7% ASR improvements respectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-turn attacks systematically outperform single-turn attacks because alignment mechanisms degrade over extended conversation contexts.
- **Mechanism:** AutoAdv frames jailbreaking as an iterative, feedback-driven rewriting process where initial harmful prompts are disguised, and subsequent rewrites adapt based on target model responses. The two-phase rewriting strategy (initial + adaptive follow-up) enables incremental shifts toward harmful content while maintaining a benign surface appearance.
- **Core assumption:** Safety alignment trained on single-turn interactions generalizes poorly to multi-turn dialogue dynamics where intent is revealed gradually.
- **Evidence anchors:**
  - [abstract] "alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations"
  - [section 4.5] "Multi-turn jailbreaks consistently outperformed single-turn attacks, as ASR increased with interaction depth... These improvements stem from AutoAdv's ability to iteratively refine attacks"
  - [corpus] Related paper "Multi-Turn Jailbreaks Are Simpler Than They Seem" confirms multi-turn attacks achieve >70% success rates against single-turn-protected models
- **Break condition:** If target models implement turn-level intent tracking or cumulative harm detection across conversation history, the gradual escalation strategy would be detected.

### Mechanism 2
- **Claim:** Learning from successful attacks via pattern extraction improves subsequent attack effectiveness by 6 percentage points.
- **Mechanism:** The pattern manager identifies successful adversarial techniques through keyword-based pattern matching against 28 predefined jailbreak categories, stores them with contextual metadata (target model, turn number, score, category), and ranks them using a weighted scoring function. Top-5 techniques are injected into the attacker LLM's system prompt as structured hints with concrete examples.
- **Core assumption:** Techniques that succeeded against similar models at similar conversation stages will transfer to new attacks.
- **Evidence anchors:**
  - [abstract] "a pattern manager that learns from successful attacks to enhance future prompts"
  - [section 3.3] "Disabling the pattern manager substantially reduced ASR from 95% to 89%"
  - [corpus] Insufficient direct corpus evidence on pattern transfer mechanisms; this is a novel contribution
- **Break condition:** If successful patterns are highly prompt-specific rather than generalizable, or if target models adapt to known patterns, the ranking function would surface irrelevant hints.

### Mechanism 3
- **Claim:** Dynamic temperature adjustment enables exploration of the adversarial prompt space and helps escape local minima.
- **Mechanism:** The temperature manager employs four strategies (Adaptive Adjustment, Oscillating Exploration, Progressive Trajectory, Reset) triggered by evaluation score patterns. When scores stagnate or regress, temperature increases to encourage exploration; when approaching success, temperature decreases for precision.
- **Core assumption:** The evaluation score from StrongREJECT provides a reliable signal of proximity to jailbreak success that correlates with optimal sampling temperature.
- **Evidence anchors:**
  - [abstract] "a temperature manager that dynamically adjusts sampling parameters based on failure modes"
  - [section 4.3] "Disabling it produced a notable ASR drop from 95% to 88%"
  - [corpus] Insufficient corpus evidence; temperature-based exploration for jailbreaking is not well-documented in related work
- **Break condition:** If the StrongREJECT score is noisy or adversarially manipulable by target model responses, temperature adjustments would be misdirected.

## Foundational Learning

- **Concept: Black-box adversarial attacks on LLMs**
  - Why needed here: AutoAdv operates entirely without model weights or gradients, requiring understanding of query-based optimization
  - Quick check question: Can you explain why gradient-based attacks like GCG don't apply to closed-source models?

- **Concept: Safety alignment and refusal behavior**
  - Why needed here: The framework explicitly exploits the gap between single-turn and multi-turn alignment
  - Quick check question: What triggers cause LLMs to refuse requests, and how might these accumulate across turns?

- **Concept: Prompt engineering for adversarial rewriting**
  - Why needed here: The attacker LLM uses structured system prompts with 6 rewriting methods and 5 follow-up techniques
  - Quick check question: How does "educational framing" differ from "roleplaying" as a jailbreak strategy?

## Architecture Onboarding

- **Component map:**
  Attacker LLM (Grok 3 Mini) -> Target LLM -> Evaluator LLM (GPT-4o-mini) -> Pattern Manager & Temperature Manager -> Attacker LLM (feedback loop)

- **Critical path:**
  1. Seed prompt → Attacker LLM (Phase I rewrite with T=0.7)
  2. Rewritten prompt → Target LLM → Response
  3. Response → Evaluator LLM → Score S
  4. If S ≤ 0.5: Attacker LLM (Phase II rewrite) with pattern hints + temperature adjustment → loop to step 2
  5. If S > 0.5: Extract patterns → Update pattern memory → Return success

- **Design tradeoffs:**
  - Attacker LLM choice: Grok 3 Mini selected for weak safety alignment + high compliance; tradeoff is potential detection of attacker model patterns
  - Max turns (N=6): Higher N increases ASR but with diminishing returns; paper shows marginal benefit declines at later turns
  - Temperature bounds [0.1, 1.5]: Wider range enables more exploration but risks incoherent rewrites

- **Failure signatures:**
  - Stagnation (three consecutive scores within 0.08 range, all below 0.35) → Oscillating strategy should trigger
  - Regression (latest score < earliest - 0.05, mean < 0.25) → Reset strategy should trigger
  - Pattern manager not updating → Check JSON persistence and keyword extraction against taxonomy

- **First 3 experiments:**
  1. Reproduce ablation: Run full AutoAdv vs. no-pattern-manager vs. no-temperature-manager on 100 prompts against Llama-3.1-8B to validate 95%/89%/88% ASR hierarchy
  2. Temperature strategy isolation: Test each of the four temperature strategies independently to determine which contributes most to the 7-point gain
  3. Cross-model pattern transfer: Train pattern manager on successful Llama attacks, then test whether ranked hints improve ASR against GPT-4o-mini (assumption: model-specific match rate m_match will reduce transfer effectiveness)

## Open Questions the Paper Calls Out

- **Question:** What defense mechanisms can effectively maintain robustness against adaptive multi-turn jailbreaking while preserving model utility?
  - Basis in paper: [explicit] The conclusion states AutoAdv should serve "as a foundation for building safety mechanisms that evolve alongside attacks," and the abstract highlights "an urgent need for multi-turn-aware defenses."
  - Why unresolved: Current alignment optimizes for single-turn interactions; the paper demonstrates their failure in multi-turn settings but provides no defense solutions.
  - What evidence would resolve it: Defense methods that maintain low ASR under multi-turn attacks comparable to single-turn baselines, without degrading helpfulness.

- **Question:** Can AutoAdv be extended to multimodal and cross-lingual jailbreaking scenarios while maintaining comparable attack success rates?
  - Basis in paper: [explicit] The limitations section states: "Our rewriting strategies, while adaptive, do not cover the full space of possible attacks (e.g., multimodal, cross-lingual jailbreaks)."
  - Why unresolved: The framework operates purely on English text; no adaptation for images, audio, or non-English languages has been performed.
  - What evidence would resolve it: Implementation and evaluation on multimodal models (e.g., GPT-4V) and multilingual targets, measuring ASR across modalities and languages.

- **Question:** How does the choice of attacker LLM affect attack success rate across different target models?
  - Basis in paper: [inferred] The paper selected Grok 3 Mini based on "high task compliance and comparatively weaker safety alignment" but did not systematically compare different attacker LLMs.
  - Why unresolved: Only one attacker LLM was evaluated; the relationship between attacker capabilities and rewriting effectiveness remains unexplored.
  - What evidence would resolve it: Ablation study varying attacker LLMs and correlating attack success with attacker model characteristics.

## Limitations

- Pattern manager relies on keyword-based extraction from limited 28-category taxonomy without complete keyword lists
- Temperature manager effectiveness depends on StrongREJECT score reliability, which lacks independent validation
- Framework assumes target models lack turn-level harm detection, not explicitly tested

## Confidence

- **High confidence**: Multi-turn attacks systematically outperform single-turn approaches (24% ASR improvement)
- **Medium confidence**: Pattern manager and temperature manager provide incremental ASR gains (6% and 7% respectively)
- **Low confidence**: Dynamic temperature adjustment effectiveness relies on unproven StrongREJECT score reliability

## Next Checks

1. **Pattern transfer validation**: Train pattern manager on Llama-3.1-8B attacks, measure ASR improvement on GPT-4o-mini using only transferred patterns vs. direct training

2. **Temperature signal reliability**: Independently validate StrongREJECT score correlation with actual jailbreak success across different target models

3. **Multi-turn detection robustness**: Test whether target models with turn-level intent tracking reduce AutoAdv's effectiveness by measuring ASR degradation with cumulative harm detection