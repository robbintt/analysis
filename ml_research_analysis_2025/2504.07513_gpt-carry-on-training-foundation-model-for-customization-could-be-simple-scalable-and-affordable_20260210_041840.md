---
ver: rpa2
title: 'GPT Carry-On: Training Foundation Model for Customization Could Be Simple,
  Scalable and Affordable'
arxiv_id: '2504.07513'
source_url: https://arxiv.org/abs/2504.07513
tags:
- training
- carry-on
- layer
- layers
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPT Carry-On, a framework for customizing
  large language models (LLMs) using lightweight training on existing inference infrastructure.
  The key idea is to train additional transformer layers on top of the final-layer
  embedding of a pretrained LLM, rather than fine-tuning the base model itself.
---

# GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable

## Quick Facts
- **arXiv ID**: 2504.07513
- **Source URL**: https://arxiv.org/abs/2504.07513
- **Reference count**: 8
- **Primary result**: Introduces a lightweight framework for customizing LLMs using minimal GPU memory while preserving base model intelligence

## Executive Summary
GPT Carry-On presents a novel approach to customizing large language models by training lightweight transformer layers on top of pretrained model embeddings, rather than fine-tuning the entire base model. This architecture enables customization using existing inference infrastructure, with training requirements as low as 1GB GPU memory for 100M parameters. The framework supports mixing specialized models across domains and uses shallow layer shortcuts for complementary information flow. Experimental results demonstrate faster convergence during pretraining and significant improvements on math tasks with minimal parameter overhead.

## Method Summary
The framework works by adding transformer layers on top of the final embedding layer of a pretrained LLM, with most computation offloaded to inference nodes optimized for forward passes. Training occurs only on the lightweight carry-on modules, which can be as small as 1MB of parameters. The approach supports mixing multiple domain-specific LLMs and uses shallow layer shortcuts to capture complementary information. This design enables customization without modifying the base model, preserving its general intelligence while allowing specialized adaptation for specific tasks.

## Key Results
- Training a 100M parameter carry-on layer requires less than 1GB GPU memory on a 30B parameter base model
- 1000 chain-of-thought samples and 1MB of carry-on parameters achieved significant improvements on math questions
- Faster loss convergence observed when continuing pretraining on Qwen and DeepSeek models

## Why This Works (Mechanism)
The framework leverages the separation of concerns between inference and training workloads. By offloading the heavy forward-pass computations to inference-optimized infrastructure while only training lightweight modules, it achieves both computational efficiency and customization capability. The shallow layer shortcuts provide complementary information flow that enhances the model's ability to integrate knowledge from multiple specialized sources.

## Foundational Learning
- **Transformer layer stacking**: Understanding how additional transformer layers can be appended to existing model architectures without disrupting base functionality. Quick check: Verify layer compatibility and dimension matching.
- **Inference offloading**: Recognizing that inference infrastructure can be repurposed for training with minimal modifications. Quick check: Confirm that forward pass computations remain unchanged.
- **Parameter efficiency**: Learning that significant performance gains can be achieved with minimal parameter additions. Quick check: Measure parameter-to-performance ratio.
- **Cross-domain model mixing**: Understanding how different specialized models can be combined effectively. Quick check: Validate information flow through shortcuts.

## Architecture Onboarding

### Component Map
Inference Nodes -> Base LLM -> Carry-On Layers -> Output
Training Nodes -> Carry-On Layers (parameter updates only)

### Critical Path
Forward pass through base LLM → Carry-on layer computation → Output generation → Parameter updates (training nodes only)

### Design Tradeoffs
- **Memory vs. performance**: Minimal carry-on parameters reduce memory requirements but may limit maximum achievable performance
- **Base model preservation vs. customization**: Keeping the base model frozen preserves general intelligence but may limit deep integration of specialized knowledge
- **Inference offloading vs. training complexity**: Offloading computations simplifies training but requires compatible infrastructure

### Failure Signatures
- Poor performance on tasks requiring deep integration with base model knowledge
- Instability when mixing models with significantly different architectures
- Suboptimal results when carry-on layers are too small relative to task complexity

### First 3 Experiments
1. Test basic carry-on layer addition on a pretrained model with synthetic data
2. Evaluate performance on simple task-specific fine-tuning
3. Measure memory usage and training speed compared to full fine-tuning

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Uncertain generalization capabilities beyond math and coding tasks
- Unclear effectiveness on smaller models or with limited pretraining data
- Potential limitations in capturing complementary information across all use cases
- Need for validation across broader model families and training regimes

## Confidence
- **High**: Technical feasibility and memory efficiency claims (1GB GPU memory for 100M parameters)
- **Medium**: Performance improvements on math questions with limited samples (1000 chain-of-thought samples)
- **Low**: Broader claims about cost-effectiveness and scalability across diverse customization scenarios

## Next Checks
1. Test the approach on a wider range of model families and sizes, including smaller models (<10B parameters), to verify the scalability claims
2. Evaluate performance across diverse tasks (beyond math and coding) to assess generalization capabilities
3. Conduct ablation studies on the shallow layer shortcuts to quantify their contribution to performance gains and validate the complementary information hypothesis