---
ver: rpa2
title: Toward a Flexible Framework for Linear Representation Hypothesis Using Maximum
  Likelihood Estimation
arxiv_id: '2502.16385'
source_url: https://arxiv.org/abs/2502.16385
tags:
- representation
- linear
- concept
- activation
- directions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Toward a Flexible Framework for Linear Representation Hypothesis Using Maximum Likelihood Estimation

## Quick Facts
- **arXiv ID:** 2502.16385
- **Source URL:** https://arxiv.org/abs/2502.16385
- **Reference count:** 40
- **Key outcome:** Introduces SAND algorithm using Maximum Likelihood Estimation on von Mises-Fisher distributions to derive concept directions from activation differences, eliminating dependency on unembedding representations.

## Executive Summary
This paper proposes a flexible framework for estimating concept directions in large language models by leveraging Maximum Likelihood Estimation (MLE) on activation differences modeled as samples from a von Mises-Fisher (vMF) distribution. Unlike prior methods that rely on token-level counterfactuals, SAND uses prompt-based contrasts to derive steering vectors, enabling representation of abstract concepts like "truthfulness." The framework introduces two variants: SAND-e (Identity normalization) and SAND-w (Whitening normalization), which yield similar results for LLaMA-2 due to its flat embedding spectrum.

## Method Summary
The SAND algorithm estimates concept directions by treating activation differences between contrasting prompts as samples from a vMF distribution. The MLE estimator for the mean direction simplifies to the normalized sum of these difference vectors. The method uses two geometric mappings: Identity (SAND-e) and Whitening (SAND-w). Activation differences are computed between positive and negative prompt pairs at specific layers, then normalized and summed to produce the final concept vector. This approach eliminates dependency on unembedding matrices and enables representation of abstract concepts beyond concrete token pairs.

## Key Results
- SAND-e and SAND-w variants produce highly similar concept directions for LLaMA-2 models.
- The embedding matrix spectrum is flat, explaining why simple normalization (SAND-e) approximates whitened normalization (SAND-w).
- The framework successfully derives concept vectors for both concrete concepts (gender) and abstract concepts (truthfulness).
- Linear Artificial Tomography using SAND vectors achieves competitive monitoring and steering performance.

## Why This Works (Mechanism)

### Mechanism 1: Maximum Likelihood Estimation on Directional Data
The method models activation differences as directional samples from a von Mises-Fisher distribution on a unit sphere. The MLE estimator for the mean direction simplifies to the normalized sum of difference vectors, justifying the SAND algorithm. This works when activation differences are unimodal and cluster around a true concept direction.

### Mechanism 2: Decoupling from Unembedding Constraints
By using prompt-based activation differences instead of token-level counterfactuals, the framework can capture abstract concepts lacking specific token mappings. This enables representation of context-dependent concepts like "truthfulness" that cannot be expressed through simple token pairs.

### Mechanism 3: Invariance to Geometry via Spectral Flatness
For models with flat embedding spectra (like LLaMA-2), the choice between Identity and Whitening normalization yields similar results. The tightly clustered singular values of the embedding matrix mean Euclidean normalization approximates the theoretically superior whitened normalization.

## Foundational Learning

- **von Mises-Fisher (vMF) Distribution:** Statistical distribution on a hypersphere used to model concept directions as probability distributions. Needed because it handles noise in directional data better than simple vector addition. *Quick check:* If you have 5 vectors pointing in slightly different directions, how does assuming a vMF distribution help you find the "average" direction compared to simple vector addition?

- **Linear Representation Hypothesis (LRH):** Assumption that concepts are linearly represented as directions in activation space. Needed as the fundamental basis for steering models via vector addition. *Quick check:* Does LRH imply that a concept is stored in a single neuron or a direction across many neurons?

- **Causal Separability (Park et al. 2024):** Prior method requiring concepts to be manipulable independently via token swaps. Needed as a contrast to explain why SAND is more flexible. *Quick check:* Why is the pair ("king", "queen") considered "ambiguous" or problematic under strict causal separability definitions?

## Architecture Onboarding

- **Component map:** Stimulus Set -> Feature Extractor (LLM) -> Normalizer (C matrix) -> Aggregator (SAND Algorithm) -> Output (Concept Vector)

- **Critical path:** The selection of the Stimulus Set and the specific Layer from which activations are extracted. Middle-to-final layers often show strongest alignment.

- **Design tradeoffs:**
  - **SAND-e vs. SAND-w:** SAND-e is computationally cheaper but assumes isotropy; SAND-w accounts for covariance but is heavier. SAND-e is sufficient for LLaMA-2 due to spectral flatness.
  - **Prompt vs. Token:** Prompts allow abstract concepts but may introduce noise; tokens are precise but limited to concrete concepts.

- **Failure signatures:**
  - **Vector Collapse:** If positive and negative prompts are not semantically opposed, difference vectors sum to near-zero.
  - **Wrong Layer:** Extracting from early layers (syntax-heavy) or extreme final layers (output-logits-heavy) may yield weak concept directions.
  - **Spectral Mismatch:** Using SAND-e on a model with highly anisotropic embeddings without checking the spectrum of C.

- **First 3 experiments:**
  1. **Sanity Check (Gender):** Replicate "male â†’ female" steering on LLaMA-2-7B using simple word pairs to verify the pipeline matches Figure 4 results.
  2. **Ablation (Geometry):** Compare SAND-e vs. SAND-w on TruthfulQA to confirm if the singular values of the embedding matrix C are indeed flat (Figure 2 replication).
  3. **Intervention Strength:** Sweep the multiplier coefficient for a "Truthfulness" vector to find the "Goldilocks" zone where answers become truthful without degrading fluency.

## Open Questions the Paper Calls Out

None explicitly called out in the provided content.

## Limitations
- The framework assumes activation differences follow a von Mises-Fisher distribution, which may not hold for all concepts or model architectures.
- The method currently only handles binary concepts, with no extension proposed for hierarchical or multi-class concepts.
- Performance may degrade on models with highly anisotropic embeddings where the flat spectrum assumption fails.

## Confidence
- **SAND algorithm derivation:** High - The MLE derivation from vMF distribution is mathematically sound.
- **Empirical results on LLaMA-2:** Medium - Results are consistent but limited to one architecture family.
- **Generalizability to other models:** Low - The flat spectrum assumption may not hold for other architectures.

## Next Checks
1. Verify the von Mises-Fisher distribution assumption by performing goodness-of-fit tests on activation differences for various concepts.
2. Replicate the spectral analysis (Figures 2 and 3) on a diverse set of open-source models to test the flat spectrum assumption.
3. Implement a non-linear version of the $\Psi$ mapping and compare steering/monitoring performance against linear SAND variants.