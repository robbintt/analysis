---
ver: rpa2
title: 'PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer'
arxiv_id: '2511.19472'
source_url: https://arxiv.org/abs/2511.19472
tags:
- prefix
- mask
- prefixgpt
- design
- coordinate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PrefixGPT is a GPT-style model for directly generating optimized
  prefix adders from scratch, reformulating the problem as sequence generation with
  a legality mask ensuring validity by construction. The model uses a two-head decoder-only
  Transformer to predict 2D coordinates, pre-trained on random valid adders and fine-tuned
  via RL for optimized area-delay product.
---

# PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer

## Quick Facts
- arXiv ID: 2511.19472
- Source URL: https://arxiv.org/abs/2511.19472
- Authors: Ruogu Ding; Xin Ning; Ulf Schlichtmann; Weikang Qian
- Reference count: 40
- One-line primary result: New optimal 16-bit prefix adder design with 7.7% improved Area-Delay Product (ADP) and up to 79.1% lower average ADP versus state-of-the-art methods

## Executive Summary
PrefixGPT introduces a GPT-style transformer model that directly generates optimized prefix adder topologies from scratch, reformulating circuit design as sequence generation with a legality mask ensuring validity by construction. The system uses a two-head decoder-only Transformer to predict 2D coordinates, pre-trained on random valid adders and fine-tuned via RL for optimized area-delay product. Experiments demonstrate the model discovers a new optimal design with 7.7% improved ADP while significantly outperforming state-of-the-art methods across different initializations and bit-widths.

## Method Summary
PrefixGPT treats prefix adder design as sequence generation, where the model predicts 2D coordinates representing node connections in the adder topology. The architecture features a custom decoder-only Transformer with RoPE embeddings and a two-head output for row and column predictions. A dynamic legality mask is applied during generation to guarantee structural validity. The system employs a two-stage training approach: pre-training on 1 million random valid adders to learn the structural "grammar," followed by RL fine-tuning using GRPO to optimize area-delay product. The fine-tuning includes KL regularization against a frozen reference model and best-design retrieval to prevent mode collapse.

## Key Results
- Discovers new optimal 16-bit prefix adder design with 7.7% improved ADP versus state-of-the-art
- Achieves up to 79.1% lower average ADP across different bit-widths compared to existing methods
- Maintains robust performance across random and manual initializations, avoiding initialization bias

## Why This Works (Mechanism)

### Mechanism 1: Constrained Decoding via Legality Masking
The model achieves 100% validity by hard-masking the output distribution at every generation step using a binary mask based on current partial sequence and prefix graph topology rules. This mask is applied to Transformer logits before softmax sampling, zeroing out invalid coordinate choices.

### Mechanism 2: Pre-training for "Grammar" Acquisition
Pre-training on random valid adders allows the model to internalize structural "grammar" of circuit topology, providing strong prior that accelerates RL convergence. The pre-trained distribution covers enough state space to provide useful initialization for optimized designs.

### Mechanism 3: Generative Exploration vs. Refinement
Generating designs from scratch avoids initialization bias inherent in refinement-based methods. While methods like MCTS or Q-learning perform local searches around seed designs, PrefixGPT samples from learned distribution of all valid adders, enabling jumps to distant topological regions.

## Foundational Learning

**Concept: Prefix Graph Topology**
Why needed: Understanding nodes ℓ_{j:i}, input/output rules, and "merge rule" is required to understand why legality mask exists.
Quick check: Given a node at row 5, column 5, what determines valid candidates for its Less Significant Parent (LSP)?

**Concept: Autoregressive Generation with RoPE**
Why needed: Model predicts next 2D coordinate based on previous ones. RoPE captures spatial relationship between coordinates better than standard 1D embeddings.
Quick check: Why does paper use RoPE instead of standard absolute positional encodings for coordinate sequence?

**Concept: Group Relative Policy Optimization (GRPO)**
Why needed: This is the fine-tuning engine. Unlike standard PPO, GRPO computes advantages relative to group mean, removing need for value function.
Quick check: In fine-tuning phase, why is "reference model" kept frozen alongside trainable "policy model"?

## Architecture Onboarding

**Component map:**
Input: Sequence of 2D coordinates L_{1:k} → Spatial Embedding: Separate learned embeddings for Row/Col → RoPE application → Concatenation → Backbone: 4-layer Shared Decoder → Heads: Row Head (1 Decoder layer + Linear) and Column Head (2 Decoder layers + Linear) → Constraint: Legality Mask → Training Loop: Pre-train (Self-supervised) → Fine-tune (GRPO + Best-Design Retrieval)

**Critical path:**
The dependency of Column Head on Row Head. Column coordinate depends on row context. Parallelizing both heads independently violates design logic.

**Design tradeoffs:**
MSP Restriction: Model restricts More Significant Parent (MSP) to immediately preceding node, reducing action space complexity but theoretically limiting design space. Batch Size vs. VRAM: GRPO requires large batch sizes (G=512) for stable advantage estimation, creating primary memory bottleneck.

**Failure signatures:**
High Std Dev in Reward: Indicates KL divergence (β) is too low or "Best-Design Retrieval" buffer is corrupt, causing model to forget high-quality patterns. Invalid Designs (with Mask ON): Should not happen; indicates mask implementation is buggy or bit-width n exceeds training context.

**First 3 experiments:**
1. Overfit Test: Train only on Brent-Kung adder sequence. Verify if model can memorize exact coordinate sequence (Sanity Check).
2. Mask Stress Test: Generate 10k random sequences with Mask disabled. Check "legal rate" to verify model learns validity from data.
3. Ablation Run: Run 16-bit optimization without RoPE embeddings. Expect performance drop to confirm importance of relative spatial encoding.

## Open Questions the Paper Calls Out
None

## Limitations
- Internal Transformer architecture details (attention head count, FFN dimensions) not fully specified, requiring assumptions in reproduction
- Synthesis methodology details for ABC tool parameters incomplete, affecting reproducibility of area/delay measurements
- MSP restriction (using immediately preceding node as MSP) reduces design space but impact on optimality not quantified

## Confidence

**High confidence:** Legality mask ensuring 100% valid designs by construction, pre-training efficiency improvements (31.6% reward increase), 7.7% ADP improvement on 16-bit adder

**Medium confidence:** Claims about avoiding initialization bias and superiority over refinement methods, as these rely on relative comparisons with unspecified competitor implementations

**Low confidence:** Claims about model's ability to "develop foundational understanding" rather than pattern memorization, as this is subjective

## Next Checks

1. **Ablation on MSP restriction:** Remove MSP restriction to allow any existing node as MSP and measure impact on discovered optimal designs and exploration efficiency

2. **Cross-technology validation:** Implement same optimization framework using different synthesis tool (e.g., Yosys) to verify ADP improvements are tool-independent

3. **Generalization test:** Apply pre-trained model (without RL fine-tuning) to different adder architecture (e.g., carry-select) to assess transfer learning capabilities and rule generalization