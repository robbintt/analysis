---
ver: rpa2
title: 'Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short
  Answer Grading'
arxiv_id: '2508.04063'
source_url: https://arxiv.org/abs/2508.04063
tags:
- data
- examples
- answer
- llama-3-8b
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates fine-tuning approaches for automated short
  answer grading (ASAG) using structured JSON outputs, comparing few-shot prompting
  and supervised fine-tuning methods. Experiments used GPT-4o-mini (closed model)
  and Llama3.1-8B-Instruct (open-weight) with limited labeled data (~148 examples)
  and consumer-grade GPU resources.
---

# Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading

## Quick Facts
- arXiv ID: 2508.04063
- Source URL: https://arxiv.org/abs/2508.04063
- Reference count: 23
- Primary result: GPT-4o-mini fine-tuning on ~150 examples improved F1 score from 0.68 to 0.73, while Llama3.1-8B-Instruct required 1,000 synthetic examples to reach F1 of 0.653

## Executive Summary
This paper evaluates fine-tuning approaches for automated short answer grading (ASAG) using structured JSON outputs, comparing few-shot prompting and supervised fine-tuning methods. Experiments used GPT-4o-mini (closed model) and Llama3.1-8B-Instruct (open-weight) with limited labeled data (~148 examples) and consumer-grade GPU resources. Results showed GPT-4o-mini fine-tuning significantly improved F1 score from 0.68 to 0.73, particularly in domain-specific tasks. Llama3.1-8B-Instruct baseline performance was poor (F1 0.165), but adding synthetic training data generated via Gemini 1.5 Flash improved F1 to 0.653. The study demonstrates that fine-tuning closed models on small datasets enhances ASAG performance, while open-weight models require substantial synthetic data augmentation to become competitive.

## Method Summary
The study used 148 labeled training examples from OpenTutor project (42 lessons) with multi-concept ASAG requiring structured JSON outputs. Three held-out "gold" lessons (220 total responses) served as evaluation. GPT-4o-mini was fine-tuned via OpenAI API, while Llama3.1-8B-Instruct was fine-tuned using QLoRA (4-bit quantization) on a single NVIDIA A40 GPU. Synthetic data generation used Gemini 1.5 Flash to create 1,000 examples from sampled real examples. Models were evaluated across N-shot contexts (0-40) yielding 17,820 test prompts. QLoRA training used 6 epochs (9 epochs showed overfitting) with 90:10 train-validation split for synthetic data.

## Key Results
- GPT-4o-mini fine-tuning improved F1 score from 0.68 to 0.73 on ASAG task
- Llama3.1-8B-Instruct baseline performance was poor (F1 0.165) with repetitive outputs
- Adding 1,000 synthetic examples improved Llama F1 from 0.408 to 0.653
- Fine-tuned models showed better N-shot context utilization, with F1 increasing at a higher rate as N-shots increased

## Why This Works (Mechanism)

### Mechanism 1: Sample-Efficient Alignment in Closed Models
Fine-tuning closed models on limited data (~150 examples) improves structured ASAG performance by internalizing the grading schema, shifting from reliance on prior knowledge to reliance on the provided few-shot rubric. This reduces the "cognitive load" required to parse instructions at inference time. The closed-model fine-tuning infrastructure utilizes proprietary optimizations that allow robust generalization from small sample sizes without catastrophic forgetting.

### Mechanism 2: Synthetic Data Scaling for Open Weights
Open-weight models require synthetic data augmentation to overcome sample inefficiency and instability when fine-tuning on small datasets. The initial dataset (N=148) was insufficient to adjust the 8B parameters via QLoRA without causing instability (e.g., repetitive outputs). Adding 1,000 synthetic examples expanded the training manifold, regularizing the model and exposing it to varied linguistic expressions of target concepts.

### Mechanism 3: N-Shot Context Utilization
Fine-tuning models across varied domains with varied N-shot counts teaches the model to utilize context examples more effectively as the context window grows. By training on subsets of data with different N-shot contexts, the model learns a meta-learning signal: how to extract grading criteria from the prompt context rather than relying solely on parametric knowledge.

## Foundational Learning

- **QLoRA (Quantized Low-Rank Adaptation)**: Understanding how an 8B model fits on a single consumer GPU (A40) and why it might underperform compared to full-parameter fine-tuning. Quick check: Why would a 4-bit quantized model struggle to learn a complex new JSON schema with only 100 examples compared to a full-precision closed model?

- **ASAG (Automated Short Answer Grading)**: The target task involves multi-label classification (concept verification) rather than simple text generation, requiring specific output constraints. Quick check: How does the requirement for structured JSON output change the loss function compared to standard text generation?

- **Synthetic Data Generation (SDG)**: Crucial for bridging the performance gap between small open-weight models and large closed models. Quick check: What are the risks of "model collapse" if synthetic data is used recursively without human-in-the-loop validation?

## Architecture Onboarding

- **Component map**: OpenTutor Dialogue Data (Human graded) -> Augmentor (GPT-4o adds confidence/justification) -> Synthetic Loop: Gemini 1.5 Flash generates 1k new JSON examples -> Merged with 148 real examples -> Training: Path A: OpenAI API (Closed fine-tuning), Path B: QLoRA on Llama3.1-8B (Open fine-tuning) -> Inference: N-shot prompt construction (N=0 to 40) -> Model -> JSON Parser -> Score

- **Critical path**: The generation and cleaning of the synthetic dataset is the bottleneck. If the synthetic JSON is invalid or semantically weak, the open-weight model fails (F1 < 0.2).

- **Design tradeoffs**: Closed (GPT-4o-mini): High performance (F1 0.73), minimal data needed, but vendor lock-in and opaque internal logic. Open (Llama 3.1): Requires significant synthetic data generation effort, lower final performance (F1 0.65), but full control and offline capability.

- **Failure signatures**: Degenerate Repetition: Llama 3.1 baseline often "failed to generate stopping points" or repeated text. Class Collapse: Baseline models predicting only "False" (Precision 0.49, Recall 0.10). JSON Invalidity: Gemini 2.5 Flash (Thinking model) failed to create valid JSONs; structured output constraints must be carefully selected.

- **First 3 experiments**: 
  1. Baseline N-Shot: Establish baseline for GPT-4o-mini and Llama3.1-8B using 0 to 30-shot prompting on 3 "gold" lessons without fine-tuning.
  2. Closed Fine-Tuning: Fine-tune GPT-4o-mini on 148 labeled examples and measure delta in F1 score and "slope" of improvement as N-shot increases.
  3. Synthetic Rescue: Train Llama3.1-8B using QLoRA on 148 examples (likely failure), then retry with 1,000 synthetic examples injected to verify performance jump.

## Open Questions the Paper Calls Out

- **Does augmenting training data with synthetic examples improve fine-tuning performance for closed models like GPT-4o-mini to the same extent it does for open-weight models?** The study only evaluated impact of synthetic data on Llama3.1-8B model; closed-model experiments relied solely on ~148 real labeled examples.

- **Can an agentic process, where a model generates free-form text before parsing it into JSON, better preserve reasoning capabilities than direct structured output constraints?** Current study utilized direct structured (JSON) outputs and did not test agentic or multi-step generation pipelines.

- **Is "cold start" supervised fine-tuning followed by Group Relative Policy Optimization (GRPO) more sample-efficient than synthetic data augmentation strategies?** Paper evaluated QLoRA and standard SFT but did not implement or test reinforcement learning techniques like GRPO.

- **What is the optimal volume of synthetic data required to maximize performance in open-weight models before hitting diminishing returns?** Results rely on single, arbitrarily chosen volume of synthetic data (1,000 examples), leaving relationship between data volume and performance gains unknown.

## Limitations

- Synthetic data volume was chosen somewhat arbitrarily without testing several different amounts
- Unable to fine-tune GPT-4o-mini with synthetic data to see how it would affect performance
- Constraining LLMs to structured outputs can have deleterious effect on model reasoning

## Confidence

- **GPT-4o-mini fine-tuning performance**: High
- **Llama3.1-8B synthetic data improvement**: Medium
- **N-shot context utilization findings**: Medium
- **Generalizability to other domains**: Low

## Next Checks

- Validate QLoRA hyperparameters (rank, alpha, learning rate) to reproduce Llama3.1-8B results
- Test synthetic data augmentation on GPT-4o-mini to compare with real-data-only fine-tuning
- Implement agentic generation pipeline to compare with direct structured output approach