---
ver: rpa2
title: 'Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal
  Language Models'
arxiv_id: '2507.16572'
source_url: https://arxiv.org/abs/2507.16572
tags:
- language
- vision
- grasp
- physics
- plausible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multimodal large language models (MLLMs) on
  intuitive physics tasks using the GRASP and IntPhys 2 datasets. Despite recent advances,
  even state-of-the-art models like Qwen 2.5 VL, InternVL 2.5, LLaVA-OneVision, and
  Gemini 2.0 Flash Thinking struggle to reliably distinguish physically plausible
  from implausible scenarios, achieving at most 54% accuracy.
---

# Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models

## Quick Facts
- arXiv ID: 2507.16572
- Source URL: https://arxiv.org/abs/2507.16572
- Reference count: 27
- State-of-the-art MLLMs achieve at most 54% accuracy on intuitive physics tasks

## Executive Summary
This paper evaluates multimodal large language models (MLLMs) on intuitive physics tasks using the GRASP and IntPhys 2 datasets. Despite recent advances, even state-of-the-art models like Qwen 2.5 VL, InternVL 2.5, LLaVA-OneVision, and Gemini 2.0 Flash Thinking struggle to reliably distinguish physically plausible from implausible scenarios. To understand the underlying limitations, the authors probe model embeddings at key processing stages, extracting representations before and after vision-language projection layers. Their analysis reveals a critical vision-language misalignment: while vision encoders successfully capture physical plausibility cues, the language model often fails to utilize this information effectively.

## Method Summary
The authors evaluate nine MLLMs on intuitive physics classification tasks using GRASP (Level 1: 4 scenes × 128 videos; Level 2: 16 scenes × 256 videos) and IntPhys 2 datasets. Videos are 10-second clips sampled at 1 FPS. They extract embeddings at three key stages: vision encoder output, projection layer output, and language model final layer. Linear probes (1-hidden-layer MLP, 512 units, ReLU) are trained on these embeddings to predict physical plausibility. t-SNE visualizations are used to analyze clustering of physical concepts. Three evaluation methods are tested: free generation, greedy decoding, and chain-of-thought prompting.

## Key Results
- All evaluated MLLMs achieve at most 54% accuracy on intuitive physics tasks, barely above random chance
- Vision embeddings from models like InternVL 2.5 (78B) achieve 85% accuracy in linear probing, significantly above random baseline
- t-SNE visualizations show vision embeddings naturally cluster by physical concepts, but this structure deteriorates after passing through the language model
- The projection layer generally reduces performance, though InternVL 2.5 is a notable exception

## Why This Works (Mechanism)

### Mechanism 1: Vision Encoder Information Capture
- **Claim**: The vision encoder in MLLMs successfully captures physical plausibility cues necessary for distinguishing plausible from implausible scenarios, even when the full model fails at the task.
- **Mechanism**: The Vision Transformer encodes raw pixel data into embedding representations that preserve task-relevant physical information, extractable via linear probes.
- **Core assumption**: Linear probe accuracy implies the information is present and structured within the representation.
- **Evidence anchors**: Vision embeddings achieve 85% accuracy on probing tasks; paper shows vision component encodes physical plausibility cues.

### Mechanism 2: Vision-Language Projection Bottleneck
- **Claim**: The primary bottleneck for intuitive physics reasoning in MLLMs is the ineffective integration of visual and linguistic information, occurring during or after the projection layer.
- **Mechanism**: The projection layer aligns visual embeddings with language model space, often degrading physics-relevant information that linear probes can detect.
- **Core assumption**: Performance drop from vision to language embeddings indicates failure in transferring relevant signal.
- **Evidence anchors**: All nine models perform better when probing vision embeddings; paper identifies this as the primary limitation.

### Mechanism 3: Embedding Structure Dilution via Language Model
- **Claim**: The structured representation of physical concepts present in vision embeddings is progressively diluted as the signal passes through the language model.
- **Mechanism**: t-SNE visualizations reveal that vision embeddings form distinct clusters for different physical scenes, but this semantic separability is lost in language model embeddings.
- **Core assumption**: Separability of embeddings in t-SNE space is a reliable proxy for the model's internal organization of physical concepts.
- **Evidence anchors**: t-SNE shows vision embeddings naturally cluster by physical concepts but lose this structure after passing through the language model.

## Foundational Learning

- **Concept: Linear Probing**
  - **Why needed here**: This is the paper's core diagnostic method to determine if information is present in a frozen representation.
  - **Quick check question**: If you train a simple logistic regression classifier on a model's intermediate activations, can it predict the target variable (e.g., "physically plausible") better than random guessing?

- **Concept: Vision-Language Misalignment**
  - **Why needed here**: The paper identifies this as the root cause of failure in MLLMs for intuitive physics tasks.
  - **Quick check question**: Does the model perform well on visual-only and language-only tasks, but fail on tasks that require integrating information from both modalities?

- **Concept: t-SNE Visualization for Feature Structure**
  - **Why needed here**: The paper uses t-SNE to visually demonstrate the loss of physical concept structure in embeddings.
  - **Quick check question**: In a 2D t-SNE plot of embeddings, do data points with the same label (e.g., all "gravity" videos) group together, or are they randomly interspersed with other labels?

## Architecture Onboarding

- **Component map**: Input Video -> Vision Encoder (ViT) -> [Embeddings Before Projection] -> Projection Layer (MLP/Q-Former) -> [Embeddings After Projection] -> LLM Backbone -> [Final Layer Embeddings] -> Output Token
- **Critical path**: The Vision-to-Language Projection Layer is identified as the critical bottleneck where physics-relevant information is lost or degraded.
- **Design tradeoffs**: Engineers must balance a connector's capacity to align features against the risk of it diluting fine-grained physical cues that standard language embeddings may not emphasize.
- **Failure signatures**:
  - High vision probe accuracy, low language probe accuracy indicates vision-language alignment failure
  - t-SNE dispersion showing loss of cluster structure in language embeddings compared to vision embeddings
  - Hallucination/Omission in Descriptions where models miss or fabricate key physical events
- **First 3 experiments**:
  1. Extract and save embeddings from a chosen MLLM at three stages for the GRASP dataset: (1) output of the last ViT block, (2) output of the projection layer, (3) output of the LLM's final layer. Train and evaluate linear classifiers on each to predict physical plausibility.
  2. Generate t-SNE plots of the extracted embeddings from step 1, color-coding points by physical concept (e.g., gravity, collision). Assess if distinct clustering present in vision embeddings is lost in language embeddings.
  3. Run the probing experiment on language model embeddings using both "simple prompt" and "detailed prompt" provided in the paper's appendix. Compare if explicit physical definitions in the prompt improve the probe's accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: At which specific layers within the vision encoders and language decoders does the degradation of physics-relevant information occur?
- **Basis in paper**: The authors conclude that "future research could conduct a layer-by-layer analysis of both vision encoders and language decoders to further investigate these bottlenecks."
- **Why unresolved**: This study extracted embeddings only from the last vision block, the end of the vision encoder, and the final language layer, leaving intermediate processing stages unexplored.
- **What evidence would resolve it**: A layer-wise probing analysis tracking the linear separability of physical concepts as data passes through every transformer block.

### Open Question 2
- **Question**: How is physical plausibility information specifically lost or misrepresented during the vision-language alignment process?
- **Basis in paper**: The Limitations section states that "how this information is lost or misrepresented during alignment requires further investigation."
- **Why unresolved**: The current analysis identifies that the projection layer and language model fail to utilize vision features, but does not isolate the specific mathematical or structural reasons for this failure.
- **What evidence would resolve it**: Mechanistic interpretability studies analyzing attention patterns and weight distributions in the projection layers during physics tasks.

### Open Question 3
- **Question**: Do proprietary, closed-source models (e.g., GPT-4o) fail intuitive physics tasks due to the same vision-language misalignment found in open-source models?
- **Basis in paper**: The authors note their analysis was restricted to open-source models because accessing features of proprietary models is impossible.
- **Why unresolved**: Proprietary models like Gemini 2.0 Flash Thinking were evaluated on performance but could not be probed internally, so the root cause of their near-chance accuracy remains unknown.
- **What evidence would resolve it**: Internal access to hidden states of proprietary models or the development of "white-box" variants for diagnostic purposes.

### Open Question 4
- **Question**: Why does the vision-language projection layer improve performance for InternVL 2.5 while degrading it for almost all other model families?
- **Basis in paper**: Insight 5 notes that the projection layer "generally reduces performance... however, notable exceptions include InternVL 2.5."
- **Why unresolved**: The paper observes this divergent behavior but does not offer an explanation for why InternVL's alignment mechanism preserves physics information better than others.
- **What evidence would resolve it**: A comparative structural analysis of the connector modules and alignment training objectives used by InternVL versus LLaVA and Qwen.

## Limitations
- The analysis focuses on a specific architecture pattern (vision encoder → projection → LLM) and may not generalize to other multimodal architectures.
- The interpretation that vision embeddings contain structured physical concepts is plausible but not definitively proven.
- The paper doesn't establish whether the degradation occurs specifically in the projection layer versus earlier or later stages.

## Confidence

- **High Confidence**: The core finding that MLLMs struggle with intuitive physics tasks (all models < 54% accuracy) is robust and directly measured.
- **Medium Confidence**: The claim about vision-language misalignment as the primary bottleneck is supported by the probing results and t-SNE analysis.
- **Low Confidence**: The specific mechanism of information loss through the projection layer is inferred from the probing results but not directly measured.

## Next Checks

1. **Probe Architecture Sensitivity**: Repeat the linear probing experiments with multiple probe architectures (e.g., different hidden layer sizes, attention-based probes) to verify that the vision vs language embedding performance gap is not an artifact of the specific MLP architecture chosen.

2. **Cross-Dataset Generalization**: Test the same probing methodology on additional physics understanding datasets beyond GRASP and IntPhys 2 to determine if the vision-language misalignment pattern holds across different types of physical reasoning tasks.

3. **Ablation of Projection Layer**: For models that allow it, disable or bypass the vision-language projection layer entirely and measure the impact on both end-task performance and probe accuracy. This would directly test whether the projection layer is the specific bottleneck identified in the analysis.