---
ver: rpa2
title: Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering
arxiv_id: '2511.07274'
source_url: https://arxiv.org/abs/2511.07274
tags:
- clustering
- candidate
- color
- multi-dproxy
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-DProxy introduces a multi-modal dynamic proxy learning framework
  that addresses the limitations of static semantic rigidity and inflexible feature
  fusion in existing multiple clustering methods. By combining gated cross-modal fusion,
  dynamic candidate management, and dual-constraint proxy optimization, it generates
  personalized clusterings aligned with user interests.
---

# Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering

## Quick Facts
- arXiv ID: 2511.07274
- Source URL: https://arxiv.org/abs/2511.07274
- Reference count: 19
- Key outcome: Introduces multi-modal dynamic proxy learning framework achieving state-of-the-art performance across seven benchmarks, with perfect NMI/RI on Fruit dataset

## Executive Summary
Multi-DProxy addresses limitations in existing multiple clustering methods through a novel framework that combines gated cross-modal fusion, dynamic candidate management, and dual-constraint proxy optimization. The method learns personalized clusterings aligned with user interests by leveraging learnable textual proxies refined through clustering feedback, while visual features dynamically gate textual representations to prioritize discriminative attributes. Experimental results demonstrate significant improvements over baselines across seven publicly available datasets.

## Method Summary
Multi-DProxy learns personalized clusterings from visual data guided by user-specified interests through a three-stage process. First, it initializes learnable proxies from a placeholder token and generates LLM-produced candidate words for the user concept. Second, it applies gated cross-modal fusion using bidirectional attention with sigmoid-gated residuals to synthesize discriminative joint representations. Third, it optimizes proxies via dual constraints (semantic consistency and cluster discrimination) while dynamically managing candidates through iterative clustering feedback. The final clustering is obtained by applying K-means to the fused representations.

## Key Results
- Achieves perfect NMI and RI scores on the Fruit dataset for color clustering
- Demonstrates state-of-the-art performance across seven benchmark datasets
- Shows significant improvements over baseline methods including CLIP_GPT and M3C
- Proves proxy stability under dynamic updates through theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1: Gated Cross-Modal Fusion
- **Claim:** Bidirectional attention with sigmoid-gated residuals enables visual features to modulate textual representations proportionally to their discriminative power
- **Core assumption:** Visual and textual modalities contain complementary discriminative information that fixed fusion cannot capture optimally
- **Evidence anchors:** Abstract states "gated cross-modal fusion that synthesizes discriminative joint representations by adaptively modeling feature interactions"; Theorem 1 proves visual features gate text proportionally to discriminative power
- **Break condition:** If modalities are highly uninformative or contradictory, gated fusion may amplify noise rather than signal

### Mechanism 2: Dual-Constraint Proxy Optimization
- **Claim:** Enforcing both semantic consistency and cluster discrimination yields proxies that are both meaningful and separable
- **Core assumption:** LLM-generated candidates provide semantically meaningful initializations aligned with user intent
- **Evidence anchors:** Abstract mentions "dual-constraint proxy optimization where user interest constraints enforce semantic consistency with domain concepts while concept constraints employ hard example mining"; Equations 5-7 formalize the dual constraints
- **Break condition:** If LLM generates irrelevant or noisy candidates misaligned with dataset concepts, the weighted combination may inherit semantic drift

### Mechanism 3: Dynamic Candidate Management
- **Claim:** Iteratively pruning candidates based on cluster alignment scores filters out dataset-irrelevant LLM suggestions while retaining semantically valid options
- **Core assumption:** Cluster centroids from learned proxies reflect ground-truth semantic structure, making alignment scores a valid filtering criterion
- **Evidence anchors:** Abstract mentions "dynamic candidate management that refines textual proxies through iterative clustering feedback"; Remark 1 formalizes the 2^β M → M reduction
- **Break condition:** If early clustering is unstable or misaligned, pruning may incorrectly discard valid candidates before proxies converge

## Foundational Learning

- **Concept: CLIP Vision-Language Alignment**
  - Why needed here: Multi-DProxy builds on frozen CLIP encoders to extract aligned visual and textual embeddings. Understanding CLIP's contrastive pre-training explains why proxy-based guidance works
  - Quick check question: Can you explain why CLIP embeddings enable zero-shot classification via cosine similarity?

- **Concept: Multi-Head Attention**
  - Why needed here: Gated cross-modal fusion uses bidirectional MultiHead(·) for V→T and T→V interactions. Understanding Q/K/V formulation is essential for grasping Theorem 1's gradient analysis
  - Quick check question: Given Q, K, V matrices, compute the attention output for a single head

- **Concept: Proxy-Based Classification**
  - Why needed here: Proxies serve as learnable class representatives, similar to prototypical networks. The dual constraints are conceptually proxy classification losses
  - Quick check question: How does a proxy-based classifier differ from a standard linear classifier?

## Architecture Onboarding

- **Component map:**
  ```
  Input Images → CLIP Vision Encoder → V ∈ R^(D×d)
  User Interest u → GPT-4 → Candidate Words C → CLIP Text Encoder → C ∈ R^(|C|×d)
  Base Proxies W initialized from placeholder "*"
  
  Gated Cross-Modal Fusion (L layers):
    V^l, T^l ← BidirectionalAttention + GatedRes
    F ← λT^L + (1-λ)V^L (adaptive fusion)
  
  Dual-Constraint Optimization:
    w_i ← attention-weighted combination of C
    L = L_align + α(t)L_u + β(t)L_c
  
  Every R epochs: Dynamic Candidate Management
    K-means on W → prune C by centroid alignment
  
  Output: Fused features F → K-means for final clusters
  ```

- **Critical path:** Base proxy initialization → Cross-modal fusion → Proxy update via dual constraints → Candidate refinement (every R epochs). The fused representations F determine clustering quality

- **Design tradeoffs:**
  - Larger R (update interval): More stable candidates but slower adaptation to dataset-specific concepts
  - Larger β (initial candidate multiplier): Broader semantic coverage but more pruning iterations needed
  - Temperature τ_α: Lower values sharpen attention, risking mode collapse; higher values smooth distribution

- **Failure signatures:**
  - NMI near 0: Check CLIP encoder loading; candidates may be misaligned with visual features
  - Cluster collapse (all samples → one cluster): L_c weight β(t) may be too low; increase concept discrimination
  - Erratic performance across runs: Candidate initialization varies; fix random seed or validate GPT-4 outputs
  - Slow convergence: Gate weights σ(·) may saturate; check initialization of W_g matrices

- **First 3 experiments:**
  1. **Sanity check:** Run on Fruit dataset (105 samples) with default hyperparameters (R=200, τ_α=0.2, σ=0.2). Expect NMI ≥ 0.99 for color clustering
  2. **Ablation validation:** Disable dynamic candidate management (w/o-Dynamic) on Fruit360. Expect ~5-10% NMI drop
  3. **Zero-shot comparison:** Compare Multi-DProxy vs. CLIP_GPT baseline on Card dataset. Verify improvement in Order clustering (NMI: 0.53 vs 0.35)

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown architecture details: Number of cross-modal fusion layers L and hidden/FFN dimensions are unspecified
- Training parameters unclear: Batch size B and exact CLIP variant (ViT-B/16 vs. others) not explicitly stated
- Implementation specifics missing: GPT-4 prompt template for candidate generation is not provided

## Confidence
- **High:** Dual-constraint proxy optimization mechanism, gated cross-modal fusion core concept, and theoretical gradient analysis
- **Medium:** Dynamic candidate management effectiveness and claim that visual features consistently enhance cluster discrimination
- **Low:** Generalization to datasets with weak visual-textual alignment and robustness to noisy LLM candidates for abstract concepts

## Next Checks
1. **Ablation study:** Disable dynamic candidate management on Fruit360 to verify the ~5-10% NMI drop predicted in Figure 2
2. **Robustness test:** Run Multi-DProxy on CIFAR-10 for texture clustering to check if gated fusion amplifies noise with low inter-modal agreement
3. **Hyperparameter sensitivity:** Vary fusion temperature τ across {0.01, 0.1, 1.0} on Stanford Cars to confirm λ distribution stability