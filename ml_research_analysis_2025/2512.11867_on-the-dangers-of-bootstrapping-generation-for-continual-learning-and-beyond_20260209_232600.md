---
ver: rpa2
title: On the Dangers of Bootstrapping Generation for Continual Learning and Beyond
arxiv_id: '2512.11867'
source_url: https://arxiv.org/abs/2512.11867
tags:
- data
- synthetic
- training
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the statistical and empirical consequences
  of repeatedly training generative models on synthetic data, a process relevant to
  generative experience replay in continual learning. Theoretical analysis demonstrates
  that replacing real data with synthetic samples introduces bias and variance into
  maximum likelihood estimation, weakening statistical reliability.
---

# On the Dangers of Bootstrapping Generation for Continual Learning and Beyond

## Quick Facts
- arXiv ID: 2512.11867
- Source URL: https://arxiv.org/abs/2512.11867
- Reference count: 40
- Primary result: Repeated training on synthetic data leads to statistical bias, variance, and distributional drift, degrading both generative fidelity and downstream classifier performance.

## Executive Summary
This work investigates the statistical and empirical consequences of repeatedly training generative models on synthetic data, a process relevant to generative experience replay in continual learning. Theoretical analysis demonstrates that replacing real data with synthetic samples introduces bias and variance into maximum likelihood estimation, weakening statistical reliability. Empirical experiments on GANs and diffusion models show that iterative training on generated data leads to distributional drift and downstream performance degradation. Metrics such as FID, conditional FID, and OTDD quantify increasing divergence between synthetic and real data over time. Analysis of state-of-the-art GER methods (GAN Memory and DDGR) reveals persistent latent space misalignment between real and synthetic domains. Classification performance on synthetic data collapses toward random chance within few training steps. These findings highlight fundamental limitations of synthetic data bootstrapping and raise critical concerns about its use in continual learning and large-scale model training.

## Method Summary
The study investigates distributional drift when generative models are repeatedly trained on synthetic data, as occurs in generative experience replay for continual learning. The experimental pipeline consists of: (1) training a generator on real data D; (2) generating a synthetic dataset D̂ of equal size to D; (3) retraining the next-stage generator on D̂ only; (4) repeating this loop for T stages; and (5) evaluating FID, conditional FID, OTDD, and classifier accuracy on synthetic vs. real data at each stage. Experiments span a 2D GMM toy task (WGAN), Flower102 (64×64, DDPM/StyleGAN v2), and CIFAR-100 (5-task split). Metrics include FID/CFID (per-class Fréchet distance), OTDD (optimal transport dataset distance), and top-1 classification accuracy on ResNet classifiers trained on synthetic data and validated on real data.

## Key Results
- FID, conditional FID, and OTDD all increase monotonically with retraining stages, indicating growing distributional divergence between synthetic and real data.
- Classifier accuracy trained on synthetic data drops sharply toward random chance within a few retraining steps, demonstrating severe degradation in representational fidelity.
- GAN Memory and DDGR, two state-of-the-art GER methods, fail to prevent latent space misalignment, as quantified by high OTDD and visualized via UMAP projections.
- The theoretical analysis shows that synthetic-data bootstrapping increases bias and variance in maximum likelihood estimation, weakening statistical reliability.

## Why This Works (Mechanism)
The mechanism underlying the observed degradation is the statistical bias and variance introduced when training on synthetic data rather than real data. In maximum likelihood estimation, synthetic data inherits estimation errors from the previous model, which compound over successive training stages. This leads to a drift in the learned data distribution, causing both generative models and downstream classifiers to lose fidelity. The bias arises from the mismatch between the synthetic data distribution and the true data distribution, while variance increases due to the instability of estimating parameters from non-i.i.d. synthetic samples.

## Foundational Learning
- **Maximum Likelihood Estimation (MLE)**: The standard framework for training generative models by maximizing the probability of observed data; needed to understand the statistical degradation when real data is replaced by synthetic samples. Quick check: confirm that the synthetic data distribution converges to the real distribution under infinite real data and perfect model capacity.
- **Fréchet Inception Distance (FID)**: A metric for evaluating generative model quality by comparing feature distributions of real and synthetic images; needed to quantify distributional drift. Quick check: ensure consistent batch statistics and InceptionV3 feature layers across runs.
- **Optimal Transport Dataset Distance (OTDD)**: A distance metric that measures the minimal transport cost between two datasets in feature space; needed to detect latent space misalignment. Quick check: verify OTDD implementation uses the same feature extractor and normalization as classification tasks.
- **Generative Experience Replay (GER)**: A continual learning strategy where generative models replay past experiences via synthetic data; needed to contextualize the problem of distributional drift. Quick check: confirm GER baselines (GAN Memory, DDGR) are implemented as described in their respective papers.
- **Mode Collapse**: A failure mode in GANs where the generator produces limited varieties of outputs; needed to diagnose early-stage training failures. Quick check: visualize generated samples and KDEs to ensure coverage of all classes/modes.

## Architecture Onboarding
- **Component Map**: Real Data → Generator → Synthetic Data → Retrained Generator → (repeat) → Drift
- **Critical Path**: Real Data → Generator → Synthetic Data → (Metrics: FID/CFID/OTDD/Accuracy)
- **Design Tradeoffs**: Using synthetic data for replay saves memory but introduces bias/variance; using real data avoids drift but is impractical for continual learning.
- **Failure Signatures**: Mode collapse in early retraining, FID instability, large synthetic→real accuracy gap, high OTDD indicating latent misalignment.
- **First Experiments**:
  1. Reproduce WGAN–GMM toy experiment: train WGAN on 5 Gaussians, generate 5k synthetic samples, retrain for 10 iterations, plot KDEs and FID.
  2. Run DDPM/StyleGAN v2 on Flower102 (64×64): train for 150 epochs/steps, generate synthetic data, compute FID, CFID, OTDD, and classifier accuracy per task.
  3. Validate OTDD implementation: compute OTDD between real and synthetic Flower102 using same feature extractor as classifier.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can architectural modifications or regularization techniques be developed to enforce latent space alignment in Generative Experience Replay (GER) to prevent the observed domain separation?
  - Basis in paper: [explicit] The authors conclude that state-of-the-art GER methods "fail to prevent latent space separation" and note that these "fundamental issues... must be resolved" to maintain performance.
  - Why unresolved: The paper quantifies the misalignment (using OTDD and UMAP projections) but does not propose mechanisms to correct the geometric divergence between real and synthetic embeddings.
  - What evidence would resolve it: A modified GER method that maintains a low, stable Optimal Transport Dataset Distance (OTDD) and consistent classifier accuracy over successive tasks.

- **Open Question 2**: Does the theoretical bias-variance trade-off identified in Maximum Likelihood Estimation apply equivalently to large language models (LLMs) trained on synthetic text?
  - Basis in paper: [inferred] The paper draws a connection to the "GPT-4 release cycle" and "model collapse" in LLMs, but restricts its empirical validation (FID, classification accuracy) to image-based GANs and diffusion models on Flower102 and CIFAR.
  - Why unresolved: While the theoretical analysis of MLE applies generally, the dynamics of distributional drift may manifest differently in sequential text data compared to the image synthesis tasks tested.
  - What evidence would resolve it: Empirical measurements of distributional drift and classifier performance on an LLM trained recursively on its own generated text, mirroring the experimental setup used for the image models.

- **Open Question 3**: Can a theoretical bound be established for the ratio of real-to-synthetic data required to prevent the amplification of variance in the MLE objective?
  - Basis in paper: [inferred] The analysis proves that replacing real data with synthetic samples increases variance and bias, but the paper does not quantify a "safe" threshold of synthetic data contamination.
  - Why unresolved: The paper demonstrates that degradation is inevitable under the tested conditions, leaving open the question of whether partial mixing ratios could statistically stabilize the training process.
  - What evidence would resolve it: A theoretical derivation or empirical curve showing a mixing ratio that mitigates the growth of variance described in Eq. 4 while retaining the memory benefits of replay.

## Limitations
- The theoretical bias-variance analysis is asymptotic and does not directly account for finite-sample GAN/DDPM dynamics such as mode collapse or model capacity constraints.
- Key experimental details (random seeds, OTDD implementation, UMAP hyperparameters, classifier training schedules) are not specified, limiting reproducibility of exact metric trajectories.
- The study focuses on image-based generative models; the applicability of results to other modalities (e.g., text, audio) remains an open question.

## Confidence
- **Empirical observation of drift**: High — consistent degradation across multiple generative architectures, datasets, and metrics.
- **Specific metric trajectories (FID/CFID/OTDD values)**: Medium — dependent on unreported seeds and implementation details.
- **Theoretical bias-variance decomposition**: Low to Medium — asymptotic analysis not directly tied to practical finite-sample dynamics.

## Next Checks
1. Run the WGAN–GMM toy experiment with fixed random seed; record FID at each retraining step to verify monotonic increase and mode collapse.
2. Replicate the DDPM/StyleGAN v2 Flower102 loop; compare FID and classification accuracy trajectories under both default and regularized training (e.g., gradient penalty or spectral normalization) to test if drift is mitigated.
3. Implement OTDD using the same feature extractor as the classification task (e.g., pretrained ResNet50) and measure per-task distances to confirm that distributional drift is captured independently of FID.