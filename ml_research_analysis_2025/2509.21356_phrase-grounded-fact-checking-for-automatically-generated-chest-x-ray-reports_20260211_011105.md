---
ver: rpa2
title: Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports
arxiv_id: '2509.21356'
source_url: https://arxiv.org/abs/2509.21356
tags:
- reports
- chest
- report
- real
- finding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a phrase-grounded fact-checking model (FC model)
  to detect errors in findings and their indicated locations in automatically generated
  chest X-ray radiology reports. The method involves generating a large synthetic
  dataset of over 27 million real and fake findings-location pairs by perturbing ground
  truth reports, and training a multi-label cross-modal contrastive regression network
  to discriminate and anatomically ground these pairs.
---

# Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports

## Quick Facts
- arXiv ID: 2509.21356
- Source URL: https://arxiv.org/abs/2509.21356
- Reference count: 34
- The FC model achieves a concordance correlation coefficient of 0.997 with ground truth-based verification for error detection in automated reports.

## Executive Summary
This paper introduces a phrase-grounded fact-checking model (FC model) designed to detect errors in findings and their indicated locations within automatically generated chest X-ray radiology reports. The approach leverages a large synthetic dataset of over 27 million real and fake findings-location pairs created by perturbing ground truth reports, which is then used to train a multi-label cross-modal contrastive regression network. The FC model predicts both the veracity and anatomical location of findings in automated reports, demonstrating high concordance with ground truth verification and potential utility as a surrogate for ground truth during clinical inference in radiology workflows.

## Method Summary
The proposed methodology involves generating a massive synthetic dataset by perturbing ground truth chest X-ray reports to create both real and fake findings-location pairs. This dataset is used to train a multi-label cross-modal contrastive regression network that learns to discriminate between accurate and erroneous report findings while simultaneously grounding them to specific anatomical locations. The model operates by taking the automated report and corresponding X-ray image as input, then predicting whether each finding is correct and, if incorrect, identifying the anatomical location of the error. The training leverages contrastive learning to align the visual and textual modalities, enabling the model to understand the relationship between radiological findings and their visual manifestations in the X-ray images.

## Key Results
- FC model achieves concordance correlation coefficient of 0.997 with ground truth-based verification for error detection
- Successfully discriminates between real and fake findings in automated reports
- Anatomically grounds findings to specific locations in X-ray images

## Why This Works (Mechanism)
The model works by leveraging the strong correlation between textual radiological findings and their visual representations in chest X-ray images. By training on a large synthetic dataset that captures both correct and incorrect findings across various anatomical locations, the contrastive regression network learns to identify inconsistencies between what is reported and what is visible in the image. The multi-label architecture allows simultaneous prediction of veracity and location, while the cross-modal approach ensures that both visual and textual evidence are considered in the verification process.

## Foundational Learning
- **Cross-modal contrastive learning**: Why needed - to align visual and textual representations for accurate verification; Quick check - verify that positive pairs (correct findings with corresponding image regions) are closer in embedding space than negative pairs
- **Synthetic data generation for error simulation**: Why needed - to create sufficient training examples for rare error types; Quick check - validate that generated errors span realistic clinical scenarios
- **Multi-label regression for simultaneous prediction**: Why needed - to predict both veracity and location in a unified framework; Quick check - ensure that regression targets are properly normalized and scaled
- **Anatomical grounding in medical imaging**: Why needed - to provide clinically actionable feedback on report errors; Quick check - verify that predicted locations align with standard anatomical landmarks
- **Large-scale dataset creation for rare events**: Why needed - to capture sufficient examples of different error types; Quick check - analyze class balance and coverage of error types
- **Phrase-level verification**: Why needed - to provide granular feedback rather than document-level assessment; Quick check - ensure that phrase boundaries are correctly identified and processed

## Architecture Onboarding

**Component Map**: X-ray Image -> Visual Encoder -> Cross-modal Fusion -> Multi-label Regression -> Veracity and Location Predictions

**Critical Path**: Input image and report text are encoded separately, then fused through cross-attention mechanisms. The fused representation is passed through regression layers to predict veracity scores and location coordinates for each finding phrase.

**Design Tradeoffs**: The model trades computational complexity for accuracy by using separate encoders for image and text, rather than a single unified encoder. This allows specialized processing of each modality but increases model size and inference time.

**Failure Signatures**: The model may struggle with rare findings that were underrepresented in the synthetic training data, or with complex findings that require integration of information across multiple image regions. Performance may degrade when reports contain ambiguous or colloquial language not well-represented in the training corpus.

**3 First Experiments**:
1. Evaluate model performance on a held-out validation set with known ground truth to establish baseline accuracy
2. Test the model's ability to generalize to reports generated by different report generation systems
3. Assess the impact of synthetic data perturbation strategies on final model performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Synthetic data generation may not capture the full complexity of real-world clinical errors
- Model performance on actual erroneous reports from state-of-the-art generators remains to be validated
- Generalizability across different report generation systems and clinical contexts is unclear

## Confidence

**High Confidence**: The technical methodology for phrase-grounded fact-checking using multi-label cross-modal contrastive regression is well-defined and implemented.

**Medium Confidence**: The synthetic data generation approach is sound, but its representativeness of real-world errors is uncertain.

**Low Confidence**: The model's performance on real-world erroneous reports and its generalizability across different clinical contexts and report generation systems.

## Next Checks

1. **Real-world error evaluation**: Test the FC model on a dataset of manually annotated real-world erroneous radiology reports to assess performance beyond synthetic data.

2. **Cross-system generalizability**: Evaluate the model's performance across multiple report generation systems and different clinical institutions to assess robustness.

3. **Clinical workflow integration**: Conduct a pilot study to assess the model's utility and accuracy when integrated into actual radiology reporting workflows, including measuring its impact on radiologist efficiency and report quality.