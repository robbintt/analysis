---
ver: rpa2
title: 'SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations
  Archive'
arxiv_id: '2507.04395'
source_url: https://arxiv.org/abs/2507.04395
tags:
- resolutions
- education
- open-ended
- religion
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpiritRAG is a Question Answering system designed to address the
  challenge of retrieving religion and spirituality-related knowledge from large,
  noisy United Nations resolution archives. It uses Retrieval-Augmented Generation
  with a modular architecture that includes document retrieval via embedding-based
  search and answer generation using Qwen3 models.
---

# SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive

## Quick Facts
- arXiv ID: 2507.04395
- Source URL: https://arxiv.org/abs/2507.04395
- Reference count: 40
- Primary result: RAG system for religion/spirituality queries in UN resolutions with human evaluation showing average relevance/accuracy scores above 4.0

## Executive Summary
SpiritRAG is a Question Answering system specifically designed to retrieve and generate responses about religion and spirituality topics from United Nations resolution archives. The system addresses the challenge of extracting relevant knowledge from large, noisy document collections containing religious and spiritual content. It employs a modular Retrieval-Augmented Generation architecture that combines embedding-based document retrieval with answer generation using Qwen3 models. The system supports seven languages and provides interactive querying through a chat-based interface, enabling users to access original source documents in real-time.

## Method Summary
SpiritRAG implements a RAG architecture with two main components: document retrieval and answer generation. The system uses embedding-based search to retrieve relevant UN resolution documents from a corpus of approximately 7,500 documents, then generates answers using Qwen3 models. The architecture is designed to be lightweight and deployable on modest GPU resources, making it practical for real-world applications. The system processes queries through its modular pipeline and provides access to original source documents for verification purposes.

## Key Results
- Human evaluation by domain experts on 100 questions showed average relevance and accuracy scores above 4.0 (on a 5-point scale) for both document retrieval and answer generation
- System demonstrated strong performance in health + R/S and education domains
- Successfully implemented multilingual support across seven languages
- Lightweight architecture deployable on modest GPU resources with real-time document access

## Why This Works (Mechanism)
The system leverages the complementary strengths of retrieval and generation: embedding-based search effectively handles the large document corpus to find relevant passages, while Qwen3 models provide contextually appropriate answers that can synthesize information from multiple sources. The modular design allows for targeted improvements and maintenance of individual components without disrupting the entire system.

## Foundational Learning
- **Embedding-based document retrieval**: Why needed - to efficiently search through large document collections; Quick check - measure recall@k for retrieval accuracy
- **RAG architecture**: Why needed - combines benefits of retrieval precision with generation flexibility; Quick check - compare against pure retrieval or pure generation baselines
- **Multilingual support**: Why needed - UN documents exist in multiple languages requiring cross-lingual capabilities; Quick check - evaluate retrieval/generation quality across all seven supported languages
- **Human evaluation methodology**: Why needed - quantitative metrics insufficient for subjective religious/spiritual content assessment; Quick check - ensure inter-rater reliability and clear scoring rubrics
- **Modular design patterns**: Why needed - enables independent optimization and maintenance of components; Quick check - verify component interfaces remain stable during updates

## Architecture Onboarding

**Component map**: Query -> Embedding-based retrieval -> Document selection -> Qwen3 answer generation -> Response + source links

**Critical path**: The end-to-end pipeline from user query through retrieval to answer generation represents the critical path, with each component's performance directly affecting the final output quality.

**Design tradeoffs**: Lightweight deployment (modest GPU requirements) versus potential performance limitations compared to more computationally intensive alternatives. Modular architecture enables customization but introduces potential error propagation between components.

**Failure signatures**: Poor retrieval leading to irrelevant documents passed to generation; generation errors when documents contain ambiguous or conflicting religious/spiritual information; multilingual inconsistencies across supported languages.

**3 first experiments**:
1. Test retrieval accuracy on a sample of known relevant documents using precision@5 and recall@k metrics
2. Evaluate generation quality by comparing model outputs against ground truth answers for simple queries
3. Assess multilingual consistency by running identical queries across all seven supported languages

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Small sample size (100 questions evaluated by four experts) limits generalizability of results
- Limited testing of complex theological interpretations and out-of-domain queries
- Potential for error propagation between retrieval and generation components
- Uncertainty about cross-linguistic consistency in multilingual performance

## Confidence

High confidence: The system successfully implements a RAG architecture with modular components for document retrieval and answer generation using Qwen3 models on a corpus of UN resolution documents.

Medium confidence: The system demonstrates good performance on human-evaluated questions in health and education domains, with average relevance and accuracy scores above 4.0.

Low confidence: The system's generalizability to other domains, languages, and complex theological queries; the robustness of multilingual performance; and the long-term sustainability of the deployment architecture.

## Next Checks
1. Conduct a larger-scale human evaluation with diverse religious and spiritual queries across all seven supported languages to assess cross-linguistic consistency
2. Perform systematic error analysis comparing SpiritRAG performance against state-of-the-art RAG systems on identical test sets
3. Evaluate system robustness through adversarial testing with intentionally ambiguous or context-dependent religious terminology queries