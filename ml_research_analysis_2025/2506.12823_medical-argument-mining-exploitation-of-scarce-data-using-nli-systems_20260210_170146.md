---
ver: rpa2
title: 'Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems'
arxiv_id: '2506.12823'
source_url: https://arxiv.org/abs/2506.12823
tags:
- relation
- clinical
- entities
- attack
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a two-step argument mining approach for clinical
  texts, addressing data scarcity challenges. The method uses token classification
  to identify argumentative entities (premises and claims) and a Natural Language
  Inference (NLI)-based system for relation extraction.
---

# Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems

## Quick Facts
- arXiv ID: 2506.12823
- Source URL: https://arxiv.org/abs/2506.12823
- Authors: Maitane Urruela; Sergio Martín; Iker De la Iglesia; Ander Barrena
- Reference count: 14
- Key outcome: Two-step approach achieves Micro-F1 of 0.7978 on entity recognition and outperforms standard text classification in low-data scenarios using NLI transfer

## Executive Summary
This study presents a two-step argument mining approach for clinical texts, addressing data scarcity challenges. The method uses token classification to identify argumentative entities (premises and claims) and a Natural Language Inference (NLI)-based system for relation extraction. The approach leverages cross-task transfer learning by fine-tuning NLI models on medical argumentation tasks, enabling effective performance with limited data. Experimental results show the token classification model achieves a Micro-F1 of 0.7978 on entity recognition, while the NLI-based relation extraction outperforms standard text classification methods, especially in low-data scenarios. The study highlights the value of incorporating pre-existing argumentative knowledge from NLI models to improve medical argument mining performance. These findings lay the foundation for developing tools that provide evidence-based justifications for machine-generated clinical conclusions, aiding healthcare professionals in diagnosis and treatment decisions.

## Method Summary
The approach consists of two sequential components: (1) token classification for argumentative entity recognition using a bilingual medical language model fine-tuned on casiMedicos dataset with IOB2 tagging, and (2) NLI-based relation extraction that classifies support/attack relations between entities and Major Claims. The entity recognition module identifies premises and claims at the token level, while the relation extraction module reformulates relation classification as verbalized entailment queries using Ask2Transformers. The NLI model is fine-tuned on medical argumentation tasks, leveraging cross-task transfer from general NLI pre-training. The system uses threshold-controlled classification with multiple verbalizations to determine relation types, enabling zero-shot adaptation to medical contexts.

## Key Results
- Token classification achieves Micro-F1 of 0.7978 on test set for argumentative entity recognition
- NLI-based relation extraction outperforms standard text classification methods, especially with limited data
- Q&A sections achieve high performance (0.9755 F1) while explanation sections show lower performance (0.6478 F1)
- Best relation extraction performance (0.3628 dev, 0.3255 test F1) achieved with V4 neutral strategy and threshold of 0.9830

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Transfer from NLI Pre-training to Argument Relation Classification
Pre-trained NLI models encode transferable argumentative knowledge that improves relation classification in data-scarce medical domains. NLI models learn to recognize entailment and contradiction patterns during pre-training, which map analogously to argumentative support (entailment) and attack (contradiction) relations. Fine-tuning on limited medical data adapts this general knowledge without requiring large annotated corpora. The semantic relationship between premise-hypothesis pairs in general NLI tasks is sufficiently analogous to support/attack relations in clinical argumentation. Experimental evidence shows NLI model consistently outperforms non-fine-tuned version with best improvement reaching 0.232 points. This transfer may degrade if clinical argumentative relations require domain-specific medical reasoning that general entailment patterns cannot capture.

### Mechanism 2: Token Classification with Domain-Specific Pre-training for Entity Recognition
Medical-domain pre-trained language models improve argumentative entity boundary detection over general-purpose models. A bilingual medical LM pre-trained on clinical texts encodes domain-specific vocabulary and discourse patterns. Fine-tuning for token classification (IOB2 format) leverages this knowledge to identify premise and claim boundaries, even in technical clinical narratives. Argumentative entities in clinical texts share lexical and syntactic patterns with medical entities the model encountered during pre-training. Experimental evidence shows token classification achieves Micro-F1 of 0.7978 on test, with performance varying sharply by section type. This approach may fail if argumentative structures are fundamentally pragmatic or discourse-level rather than token-adjacent.

### Mechanism 3: Verbalization-Based Zero-Shot Relation Classification via Entailment Scoring
Reformulating relation classification as verbalized entailment queries enables threshold-controlled, flexible classification of support/attack relations. Ask2Transformers generates multiple NLI queries (e.g., "X supports Y," "X attacks Y") using candidate verbalizations. The fine-tuned NLI model scores entailment probability for each; the highest-scoring verbalization above a threshold determines output. This supports zero-shot adaptation. The correct relation type consistently produces the highest entailment probability when verbalized appropriately. Experimental evidence shows verbalizations "attack," "confirm," and "corroborate" proved most effective, with V1 model achieving best F1 (0.3628 dev, 0.3255 test) with threshold 0.9830. Classification becomes unstable if multiple verbalizations produce similar entailment scores or if optimal thresholds vary significantly across document types.

## Foundational Learning

- Concept: **Natural Language Inference (NLI)**
  - Why needed here: The entire relation extraction approach depends on understanding NLI as determining whether a hypothesis is entailed by, contradicts, or is neutral with respect to a premise.
  - Quick check question: Given premise "The patient has fever and cough" and hypothesis "The patient has a respiratory infection," what is the likely NLI label? (Answer: Entailment—supported but not logically certain)

- Concept: **IOB2 Tagging Scheme**
  - Why needed here: Entity recognition uses IOB2 annotation for token classification; understanding B-/I- prefixes and O tags is essential for interpreting model output.
  - Quick check question: For "This [supports the diagnosis]," if "supports the diagnosis" is a claim, what are the IOB2 tags? (Answer: O, B-CLAIM, I-CLAIM, I-CLAIM)

- Concept: **Cross-Task Transfer Learning**
  - Why needed here: The paper's core contribution is transferring knowledge from NLI pre-training to argument mining; understanding what transfers and why is critical.
  - Quick check question: Why might NLI pre-training help argument relation classification more than random initialization? (Answer: NLI models already encode semantic relationship patterns that align with support/attack structures)

## Architecture Onboarding

- Component map:
  1. **Entity Recognition Module**: Domain-specific medical LM → Token classification head → IOB2-tagged premises and claims
  2. **Relation Extraction Module**: NLI-pretrained model (fine-tuned) → Ask2Transformers verbalization engine → Threshold-based classification (support/attack/no-relation)
  3. **Data Pipeline**: casiMedicos dataset → IOB2 annotations (entities) + CoNLL-format relations → Four neutral-case variants (V1–V4)

- Critical path:
  1. Extract argumentative entities (premises, claims) via token classification
  2. Pair each entity with Major Claim to form premise-hypothesis pairs
  3. Generate verbalizations for each relation type
  4. Score entailment probabilities via fine-tuned NLI model
  5. Select highest-scoring verbalization; apply threshold to classify or output "no-relation"

- Design tradeoffs:
  - **V1 vs. V4 neutral definition**: V1 (all non-annotated = neutral) achieves higher peak performance but is less stable; V4 (only isolated entities = neutral) is more stable but lower-performing.
  - **Verbalization selection**: Simple verbalizations ("support"/"attack") outperformed expanded synonym sets in most configurations.
  - **Pipeline separation**: Components evaluated independently (gold entities for relation extraction); real deployment requires error propagation analysis.

- Failure signatures:
  - **Boundary detection errors**: Conjunctions ("but," "and") cause entity merging; subordinating conjunctions ("although," "because") cause splitting.
  - **Positional bias**: Early-paragraph entities detected more accurately than later ones.
  - **Threshold sensitivity**: V1 requires ~0.98 threshold; V4 requires ~0.35—misconfiguration causes over/under-classification.
  - **Structure dependency**: Q&A sections (0.9755 F1) far outperform free-text explanations (0.6478).

- First 3 experiments:
  1. **Reproduce entity recognition baseline**: Fine-tune medical LM on casiMedicos IOB2; verify ~0.80 Micro-F1. Ablate by training on Q&A vs. explanation sections only.
  2. **Compare NLI transfer vs. text classification**: Train with 5%, 15%, 20%, 100% data using both NLI fine-tuning and RoBERTa text classification; confirm NLI advantage in low-data regimes.
  3. **Verbalization sweep**: Test all attack/support verbalizations on development set; confirm optimal set ("attack" + "confirm"/"corroborate").

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance degrade in an end-to-end pipeline where the relation extraction step relies on predicted entities rather than gold-standard annotations?
- Basis in paper: [Explicit] The authors note in the conclusion that "it would also be necessary to integrate both parts of the pipeline... since they were tested separately in this study."
- Why unresolved: The study deliberately isolated components to avoid error propagation, so the compounding error rate of the full system in a real-world scenario remains unmeasured.
- What evidence would resolve it: An evaluation of the combined pipeline on the test set, using the noisy output of the entity recognition model as input for the relation extraction model.

### Open Question 2
- Question: Can the NLI-based approach maintain its stability and performance advantages when applied to non-exam clinical texts, such as Electronic Health Records?
- Basis in paper: [Explicit] The conclusion states that "leveraging other clinical texts with different structures and arguments would also be beneficial... to measure the performance of this methodology across different texts."
- Why unresolved: The casiMedicos dataset consists of exam-like texts with a specific academic tone, which may exhibit different argumentative patterns than those found in standard clinical practice or hospital notes.
- What evidence would resolve it: Benchmarking the fine-tuned models on external datasets comprising diverse, non-exam clinical documentation to assess generalization capabilities.

### Open Question 3
- Question: To what extent can integrating domain-specific clinical reasoning patterns improve boundary detection in less structured, free-flowing explanatory discourse?
- Basis in paper: [Explicit] The discussion highlights that the model struggled with "less structured explanatory texts" and suggests "integrating domain-specific knowledge of clinical reasoning patterns could further improve performance."
- Why unresolved: The current token classification model suffers from boundary detection issues (e.g., merging or splitting entities at conjunctions) in natural discourse where structure is absent.
- What evidence would resolve it: Experiments incorporating clinical knowledge graphs or reasoning features into the model, specifically measuring F1-score improvements on the unstructured explanation sections of the dataset.

## Limitations
- Entity recognition performance varies significantly across document types, with Q&A sections (0.9755 F1) far outperforming explanation sections (0.6478 F1)
- Relation extraction system's heavy dependence on verbalization selection and threshold tuning raises deployment robustness concerns
- Experimental validation only tests 5%, 15%, 20%, and 100% data splits, leaving truly scarce regimes (1-4%) unexplored

## Confidence
- **High confidence**: Cross-task transfer from NLI improves relation classification performance over standard text classification, particularly in low-data scenarios
- **Medium confidence**: Domain-specific medical LM improves entity boundary detection over general-purpose models
- **Medium confidence**: Verbalization-based zero-shot relation classification enables flexible, threshold-controlled classification

## Next Checks
1. **Error Propagation Analysis**: Evaluate the complete pipeline end-to-end using automatically detected entities rather than gold annotations to measure real-world performance degradation
2. **Intermediate Data Scarcity Testing**: Validate model performance on 1-4% data splits to confirm claimed advantages in truly scarce data regimes beyond the tested 5% minimum
3. **Cross-Document Type Generalization**: Test the entity recognition model on clinical text formats not represented in the casiMedicos dataset (e.g., patient narratives, discharge summaries) to assess generalizability beyond Q&A and explanation formats