---
ver: rpa2
title: Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the
  Wisconsin Card Sorting Test
arxiv_id: '2505.22112'
source_url: https://arxiv.org/abs/2505.22112
tags:
- cognitive
- vllms
- performance
- wcst
- flexibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated cognitive flexibility in visual large language
  models (VLLMs) using the Wisconsin Card Sorting Test (WCST), a gold-standard measure
  of set-shifting ability. Three state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and
  Claude-3.5 Sonnet) were tested under four conditions varying input modality (visual/textual)
  and prompting strategy (straight-to-answer/chain-of-thought).
---

# Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test

## Quick Facts
- arXiv ID: 2505.22112
- Source URL: https://arxiv.org/abs/2505.22112
- Reference count: 40
- VLLMs achieve human-level or superior cognitive flexibility on WCST under optimal prompting conditions

## Executive Summary
This study evaluates cognitive flexibility in visual large language models (VLLMs) using the Wisconsin Card Sorting Test (WCST), a gold-standard measure of set-shifting ability. Three state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) were tested under four conditions varying input modality (visual/textual) and prompting strategy (straight-to-answer/chain-of-thought). Results showed that VLLMs achieved human-level or superior cognitive flexibility under chain-of-thought prompting with textual inputs, but performance was highly dependent on both input modality and prompting strategy. Under optimal conditions, Claude-3.5 Sonnet achieved perfect WCST performance, surpassing human baseline. The study also demonstrated that VLLMs could simulate cognitive impairment patterns through role-playing, suggesting similar cognitive architecture to human prefrontal cortex function.

## Method Summary
The study employed the WCST-64 card sorting test with rule-switching after 10 consecutive correct matches, using an ALIEN Task variant for memorization control. A 2×2 factorial design varied input modality (Visual/Textual) and prompting strategy (Straight-to-Answer/Chain-of-Thought), with 10 repetitions per condition across three VLLMs. Thirty human participants (age 20-35) provided baseline data. Performance was measured using six metrics: Categories Completed (CC), Perseverative Errors (PE), Non-Perseverative Errors (NPE), Trials to First Category (TFC), Conceptual Level Responses (CLR), and Failure to Maintain Set (FMS). The study used API calls to GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet, with exact prompt templates and sampling parameters not fully specified in the main text.

## Key Results
- VLLMs achieved human-level cognitive flexibility under chain-of-thought prompting with textual inputs
- Claude-3.5 Sonnet achieved perfect WCST performance (CC=1.0), surpassing human baseline (μ=0.95)
- Performance was highly dependent on input modality and prompting strategy, with straight-to-answer conditions yielding near-chance performance
- VLLMs successfully simulated cognitive impairment patterns through role-playing, suggesting similar cognitive architecture to human prefrontal cortex

## Why This Works (Mechanism)
The study does not explicitly detail the underlying computational mechanisms that enable VLLMs to perform set-shifting tasks. The authors call for future research to elucidate these mechanisms, as the current study demonstrates performance but does not investigate internal representational or algorithmic processes that support cognitive flexibility.

## Foundational Learning
- **WCST paradigm**: Why needed - gold-standard measure of cognitive flexibility and set-shifting ability; Quick check - understanding of rule-switching after 10 consecutive correct matches
- **Chain-of-thought prompting**: Why needed - enables step-by-step reasoning that significantly improves VLLM performance; Quick check - ability to parse and follow multi-step reasoning instructions
- **Cognitive flexibility metrics**: Why needed - standardized measures to quantify set-shifting ability and compare across models and humans; Quick check - understanding of Categories Completed, Perseverative Errors, and other WCST metrics

## Architecture Onboarding
- **Component map**: VLLM model -> Input processor (visual/textual) -> Prompting strategy (STA/CoT) -> Response parser -> Performance metrics
- **Critical path**: Input modality → Prompting strategy → Chain-of-thought reasoning → Card selection → Feedback processing → Rule inference
- **Design tradeoffs**: Visual inputs require image recognition capability but may introduce perceptual noise; textual inputs rely on accurate description parsing but may miss visual nuances
- **Failure signatures**: Near-chance performance in straight-to-answer conditions (CC≈0.02-0.10); GPT-4o misidentifying card counts (5→6 cards); perseverative errors under rule switches
- **3 first experiments**: 1) Test CoT vs STA performance with controlled inputs; 2) Evaluate visual vs textual input modality effects; 3) Simulate cognitive impairment through role-playing and validate against human clinical data

## Open Questions the Paper Calls Out
- What underlying computational mechanisms enable VLLMs to perform set-shifting tasks like the WCST? The authors state future research should focus on elucidating these mechanisms, as this study demonstrated performance but did not investigate internal processes.
- Do VLLMs' simulated cognitive impairments reflect authentic computational analogues of human deficits or stereotyped associations from training data? The authors acknowledge simulations may reflect stereotypical representations rather than authentic mechanisms.
- Can VLLMs' cognitive flexibility demonstrated on the WCST generalize to other domains of executive function and more ecologically valid scenarios? The authors call for investigating generalizability beyond this specific task paradigm.

## Limitations
- Performance highly contingent on specific prompting strategies and input modalities, with near-chance performance in straight-to-answer conditions
- Use of proprietary models raises concerns about reproducibility and ability to isolate architectural contributions
- Simulation of cognitive impairment patterns lacks validation against actual clinical populations
- Does not address potential biases in WCST-64 task design or generalizability to non-standardized cognitive tasks

## Confidence
- **High confidence**: VLLMs can achieve human-level cognitive flexibility under optimal conditions (CoT + textual input), well-supported by data with Claude-3.5 Sonnet achieving perfect performance
- **Medium confidence**: VLLMs can simulate cognitive impairment patterns is plausible but requires further validation against clinical populations
- **Low confidence**: Assertion that VLLMs have "approached human-level cognitive flexibility" is overstated given performance dependency on prompting and input modality

## Next Checks
1. Generalize to alternative cognitive tasks: Test VLLMs on other measures of cognitive flexibility (e.g., Stroop task, Dimensional Change Card Sort) to assess task-specific vs. broad generalizability
2. Validate impairment simulation: Compare VLLMs' simulated impairment patterns to clinical populations using standardized neuropsychological assessments across different prompting strategies
3. Explore architectural contributions: Conduct ablation studies or use open-source models to isolate role of specific architectural features (attention mechanisms, multimodal integration) in enabling cognitive flexibility