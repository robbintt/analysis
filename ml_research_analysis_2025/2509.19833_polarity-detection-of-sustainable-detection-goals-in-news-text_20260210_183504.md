---
ver: rpa2
title: Polarity Detection of Sustainable Detection Goals in News Text
arxiv_id: '2509.19833'
source_url: https://arxiv.org/abs/2509.19833
tags:
- text
- data
- which
- polarity
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the novel task of SDG polarity detection,
  which aims to classify text as indicating positive, negative, or neutral progress
  toward specific UN Sustainable Development Goals. The authors propose SDG-POD, a
  benchmark dataset combining manually and synthetically generated data.
---

# Polarity Detection of Sustainable Detection Goals in News Text

## Quick Facts
- arXiv ID: 2509.19833
- Source URL: https://arxiv.org/abs/2509.19833
- Reference count: 40
- Primary result: QWQ-32B achieves 61.6% F1 after fine-tuning on synthetic data for 3-class SDG polarity classification

## Executive Summary
This paper introduces SDG-POD, a benchmark for detecting polarity (positive, neutral, negative) of text regarding progress toward specific UN Sustainable Development Goals. The authors propose a synthetic data generation approach using five LLMs with majority voting to create training labels, then fine-tune models on this data. They evaluate six state-of-the-art LLMs in zero-shot and fine-tuned settings, demonstrating that fine-tuning on synthetic data significantly improves performance, with QWQ-32B achieving the best results at 61.6% F1. The task remains challenging due to the abstract nature of SDGs and the need to reason about goal-specific impact rather than general sentiment.

## Method Summary
The method combines a manually annotated test set (576 texts) with synthetic training data (5,824 texts) generated by five diverse LLMs (Llama-3.1-8B, Mixtral-8x7B, Phi-3-mini, Gemma-1.1-7b, Qwen2.5-7B) using majority voting with tiered quality labels. Models are fine-tuned for 5 epochs using a specific JSON-based prompt format. The synthetic annotation process uses weighted consensus where agreement levels determine label quality (Platinum/Gold/Silver/Bronze tiers), with special heuristics for resolving 2-2 splits. Evaluation uses standard F1 metrics plus cost-weighted variants that penalize positive-negative confusion more heavily.

## Key Results
- Fine-tuning on synthetic data improves performance across all evaluated models
- QWQ-32B achieves best overall F1 at 61.6% after fine-tuning
- Cost-weighted evaluation reveals larger performance gaps between zero-shot and fine-tuned models
- Models show particularly strong performance on concrete SDGs like SDG-9, SDG-12, and SDG-15
- Multi-LLM consensus improves annotation quality over single-model approaches

## Why This Works (Mechanism)

### Mechanism 1
Multi-LLM majority voting produces more reliable synthetic annotations than single-model generation. Five diverse LLMs independently classify each text, with a weighted consensus protocol (Platinum/Gold/Silver/Bronze tiers based on agreement count) filtering noisy labels through redundancy. This works because label errors from individual models are partially uncorrelated, allowing agreement to surface higher-quality annotations. Evidence shows 774 Platinum (5/5 agreement), 2338 Gold (4/5), 2429 Silver (3/5), and 283 Bronze (2/5) tiers.

### Mechanism 2
Fine-tuning on synthetic data reduces critical polarity reversals more than standard metrics suggest. The specialization helps particularly on ambiguous cases where surface sentiment contradicts actual SDG impact. Evidence shows when penalizing positive-negative confusion 10x, macro F1 gap between zero-shot and fine-tuned grows from ~2pp to ~10pp (Phi4: 40.7→49.9, QWQ: 40.2→50.6). The synthetic annotations, despite noise, capture sufficient signal about the decision boundary for fine-tuning to exploit.

### Mechanism 3
SDG polarity detection is structurally distinct from sentiment analysis, requiring goal-conditioned reasoning. Polarity depends on the relationship between described events and a specific SDG target—not text tone. The paper provides examples like "With enthusiasm and hope, we celebrate...while acknowledging progress...has fallen behind" which has positive sentiment but negative SDG-13 polarity. This decoupling of emotional framing from actual impact assessment is what makes the task challenging for current models.

## Foundational Learning

- **Zero-shot vs. Fine-tuned Transfer**: Understanding when synthetic data helps requires distinguishing what models already know (zero-shot) from what they learn through parameter updates (fine-tuning). Quick check: Can you explain why the same model might outperform in zero-shot but lose to a larger model after both are fine-tuned?

- **Annotation Agreement Tiers (Platinum/Gold/Silver/Bronze)**: The quality filtering mechanism depends on understanding how consensus reliability varies with agreement levels. Quick check: What heuristic would you apply when only 2 of 5 models agree on a label?

- **Cost-Weighted Evaluation Metrics**: Standard F1 obscures whether models make "expensive" errors (positive↔negative confusion) versus "cheap" ones (neutral confusion). Quick check: If confusing positive for negative is 10x more costly than confusing positive for neutral, how does that change which model you'd deploy?

## Architecture Onboarding

- **Component map**: OSDG dataset (40K+ texts) → sample 6,400 → split 5,824 train / 576 test → 5 LLMs with identical prompts → JSON outputs → tiered majority voting → synthetic labels → fine-tuning pipeline → evaluation

- **Critical path**: Prompt design determines annotation quality across all 5 models; consensus heuristics (Bronze tie-breaking) directly impact ~5% of training data; fine-tuning stability (larger models like QWQ-32B may require different epoch counts)

- **Design tradeoffs**: Synthetic vs. Human Annotation (faster/cheaper but introduces label noise; paper shows net benefit for fine-tuning despite noise); Model Size vs. Inference Cost (QWQ-32B best after fine-tuning, but Phi-4 leads zero-shot); Agreement Threshold (lower thresholds increase data quantity but reduce quality; paper uses all tiers)

- **Failure signatures**: High zero-shot accuracy but low fine-tuned gains → synthetic labels may conflict with model's pre-existing knowledge; large gap between standard F1 and weighted F1 → model is making critical polarity reversals; low inter-annotator agreement on specific SDGs → those categories may need targeted data augmentation

- **First 3 experiments**: Ablate synthetic data tiers (train separate models using only Platinum, Platinum+Gold, etc. to quantify quality vs. quantity tradeoff); Cross-SDG analysis (identify which SDGs show largest zero-shot→fine-tuned gains; hypothesis: more concrete goals may be easier than abstract ones); Ensemble inference (combine predictions from multiple fine-tuned models using same consensus protocol as annotation—test whether inference-time voting provides additional gains beyond training-time data enrichment)

## Open Questions the Paper Calls Out

- **Cross-lingual Generalization**: Can SDG polarity detection models trained on English text generalize effectively to other languages, or does performance degrade significantly without language-specific training data? The SDG-POD benchmark contains only English texts; no cross-lingual experiments were conducted.

- **Synthetic Label Reliability**: What is the alignment between LLM-generated synthetic polarity labels and human expert judgments, particularly for low-agreement ("Bronze") cases where only 2 of 5 models agreed? Synthetic labels were not independently validated against human judgments; heuristics for resolving low-agreement cases were arbitrary.

- **SDG-Specific Performance Factors**: Why do models achieve significantly better performance on specific SDGs (e.g., SDG-9, SDG-12, SDG-15) compared to others, and can these factors be leveraged to improve performance uniformly? The paper notes the variation but provides no analysis of whether it stems from SDG abstractness, training data distribution, or concept specificity.

## Limitations
- Synthetic annotation quality is uncertain without ground truth comparison for training set
- Abstract nature of SDGs creates inherent ambiguity that may limit performance ceiling
- 10x cost weighting for positive-negative confusion is arbitrary without domain validation
- Limited analysis of why certain SDGs are easier than others
- No real-world deployment evaluation beyond benchmark testing

## Confidence
- **High Confidence**: Multi-LLM majority voting improves synthetic annotation quality over single-model generation (supported by agreement tier distributions and performance gains from fine-tuning)
- **Medium Confidence**: Synthetic data enrichment effectively improves SDG polarity detection (supported by F1 improvements, but lacks ablation on synthetic tier quality)
- **Low Confidence**: The 10x cost weighting for polarity reversals accurately reflects real-world impact (appears reasonable but lacks domain validation)

## Next Checks
1. **Synthetic Label Quality Audit**: Sample 100 training instances from each agreement tier and manually verify annotations to quantify remaining noise levels
2. **Cost Function Sensitivity**: Test multiple weighting schemes (5x, 10x, 20x) for polarity confusion to determine optimal trade-offs between precision and recall
3. **Cross-SDG Performance Analysis**: Break down zero-shot vs. fine-tuned gains by individual SDG to identify which goals are most amenable to synthetic fine-tuning and which require alternative approaches