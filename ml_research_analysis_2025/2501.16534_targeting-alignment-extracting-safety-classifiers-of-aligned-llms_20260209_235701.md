---
ver: rpa2
title: 'Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs'
arxiv_id: '2501.16534'
source_url: https://arxiv.org/abs/2501.16534
tags:
- candidate
- classifier
- safety
- adversarial
- classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel technique to extract a surrogate classifier
  from aligned LLMs to assess their vulnerability to jailbreak attacks. The core idea
  is that alignment embeds a safety classifier in the LLM responsible for deciding
  between refusal and compliance.
---

# Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs

## Quick Facts
- **arXiv ID:** 2501.16534
- **Source URL:** https://arxiv.org/abs/2501.16534
- **Reference count:** 40
- **Primary result:** Surrogate classifiers achieve 70% ASR vs 22% on full model using only 50% of architecture

## Executive Summary
This paper proposes a novel technique to extract surrogate safety classifiers from aligned LLMs to assess jailbreak vulnerability. The core insight is that alignment embeds a safety decision boundary within intermediate layers of the model, which can be approximated by training a linear classifier on hidden states from a subset of the architecture. By attacking these surrogate classifiers and transferring adversarial inputs to the full LLM, the authors achieve significantly higher attack success rates with dramatically lower computational cost. The approach works by identifying where safe and unsafe inputs separate most clearly in the model's internal representations, training a simple linear probe on those representations, and using gradient-based optimization to find inputs that fool the surrogate classifier.

## Method Summary
The method involves three key steps: (1) Silhouette analysis to identify layers where safe/unsafe embeddings separate most clearly, (2) Training linear probes on hidden states from subsets of the model architecture to create surrogate classifiers, and (3) Using a modified GCG attack to optimize adversarial inputs against the surrogate classifier, then transferring these inputs to the full model. The approach extracts the final token's hidden state from decoder subsets, trains a linear layer with sigmoid activation to predict safety labels, and uses misclassification loss rather than token likelihood for attack optimization. The technique is evaluated across four LLM architectures using datasets of safe and unsafe prompts labeled by the target model itself.

## Key Results
- Surrogate classifiers achieve F1 scores above 80% agreement with full model safety decisions
- Using 50% of Llama 2 architecture achieves 70% attack success rate versus 22% when attacking full model directly
- Attack runtime and memory usage reduced by more than half compared to full-model attacks
- Optimal surrogate size is typically 50-60% of model depth, with performance degrading for deeper surrogates

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Layer Linear Separability
The alignment process creates linearly separable clusters of safe and unsafe inputs within intermediate decoder layers. A linear probe trained on these hidden states can map them to safety decisions with high accuracy, suggesting the refusal behavior is encoded as a linear feature rather than requiring the full network depth.

### Mechanism 2: Surrogate Gradient Transferability
Gradients from the partial-model surrogate classifier provide reliable signals for generating adversarial inputs that transfer to the full model. Because the surrogate captures the core safety decision boundary, optimizing inputs to maximize surrogate loss effectively moves them across the full model's safety boundary.

### Mechanism 3: Heuristic-Free Objective Optimization
Replacing specific output-matching heuristics with direct misclassification loss improves attack efficacy by exploring a much larger space of potential jailbreaks. This removes constraints on the search space that limit traditional attacks focused on generating specific output tokens.

## Foundational Learning

- **Linear Probing:** Training a single linear layer on top of frozen LLM embeddings is computationally cheaper than fine-tuning the whole model because it avoids backpropagation through the entire architecture.
- **Silhouette Score:** If the silhouette score for safe vs unsafe embeddings is negative, it implies the alignment has not created distinct clusters and the linear separability hypothesis fails.
- **Greedy Coordinate Gradient (GCG):** Standard gradient descent can't be used directly on text tokens because text is discrete; GCG iteratively replaces tokens using gradients while handling the discrete nature of text.

## Architecture Onboarding

- **Component map:** Target LLM -> Structure (subset of decoders) -> Classification Head (linear layer + sigmoid) -> Attack Module (modified GCG)
- **Critical path:** Silhouette Analysis -> Head Training -> Transfer Check
- **Design tradeoffs:** Small candidates (20%) are fastest but may miss context; large candidates (80%) have higher fidelity but diminishing efficiency returns and risk compressing safety information.
- **Failure signatures:** Overfitting (high probe accuracy but zero transferability), low separation (silhouette scores below 0.25), or high false positives (over-refusal behavior).
- **First 3 experiments:** 1) Run silhouette score analysis on target LLM using 50 safe/50 unsafe prompts to identify peak separation layer, 2) Train linear probes at 20%, 50%, and 100% depths measuring F1 score against full model labels, 3) Compare GCG attack ASR and VRAM usage between 50% surrogate and 100% full model.

## Open Questions the Paper Calls Out

- **Cross-layer extraction:** Whether surrogate classifiers can be effectively extracted from intermediate decoder structures starting at layers other than the first, and if this improves performance.
- **Temperature effects:** How the stochasticity of LLM outputs at non-zero temperatures affects the stability and accuracy of extracted safety classifiers.
- **Cross-model transferability:** Whether surrogate classifiers from open-source LLMs can facilitate effective black-box jailbreak attacks against models with different architectures.

## Limitations

- The linear separability assumption may not hold for all alignment techniques or when safety boundaries are distributed across multiple non-contiguous layers
- Transfer effectiveness relies heavily on the surrogate capturing all critical safety features, which may not account for safety-critical features emerging only in later layers
- The attack methodology focuses on gradient-based optimization, potentially missing other attack vectors like prompt engineering or in-context learning

## Confidence

**High Confidence:** Efficiency claims (70% ASR with 50% model depth vs 22% on full model) are directly supported by quantitative results; silhouette analysis showing layer-wise separation is well-documented.

**Medium Confidence:** Transferability mechanism is supported by experimental results but lacks ablation studies comparing to attacking full model with fewer steps; heuristic-free objectives claim is logical but not rigorously proven against all alternatives.

**Low Confidence:** Core claim that alignment embeds a "safety classifier" as a linear feature requires more structural analysis; assertion that this approach is more efficient than existing methods needs broader benchmarking.

## Next Checks

1. **Cross-Layer Transferability Test:** Systematically test whether adversarial examples from surrogates of different depths (10%, 30%, 50%, 70%, 90%) transfer to the full model, and whether combining multiple depth-specific surrogates improves success rates.

2. **Non-Linear Classifier Comparison:** Replace the linear probe with simple non-linear classifiers (one hidden layer MLP, RBF kernel SVM) trained on the same layer embeddings to determine if linear separability is truly optimal.

3. **Architecture-Agnostic Validation:** Apply the extraction framework to at least three additional LLM architectures from different families (Mistral, Phi, Claude) and alignment methods (RLHF, DPO, Constitutional AI) to test generalizability.