---
ver: rpa2
title: Can Instructed Retrieval Models Really Support Exploration?
arxiv_id: '2601.10936'
source_url: https://arxiv.org/abs/2601.10936
tags:
- retrieval
- instruction
- instructed
- retrievers
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates instruction-following retrieval models for
  seed-guided exploratory search, a common yet under-supported search paradigm. The
  authors use CSFCube, an expert-annotated test collection for aspect-conditional
  retrieval, to assess both ranking relevance and instruction-following performance
  across multiple instructed retrievers including fine-tuned LLMs (GritLM-7B), general-purpose
  LLMs with Pairwise Ranking Prompting (gpt-4o), and baseline models.
---

# Can Instructed Retrieval Models Really Support Exploration?

## Quick Facts
- arXiv ID: 2601.10936
- Source URL: https://arxiv.org/abs/2601.10936
- Reference count: 38
- Primary result: Instructed retrievers significantly improve ranking relevance but show near-zero sensitivity to instruction variations, failing to effectively support nuanced exploratory search

## Executive Summary
This paper evaluates instruction-following retrieval models for seed-guided exploratory search using CSFCube, an expert-annotated test collection for aspect-conditional retrieval. The authors assess both ranking relevance and instruction-following performance across multiple instructed retrievers including fine-tuned LLMs (GritLM-7B), general-purpose LLMs with Pairwise Ranking Prompting (gpt-4o), and baseline models. While the best instructed retrievers significantly improve ranking relevance over instruction-agnostic approaches, their instruction-following performance does not mirror these gains. Models show inconsistent or near-zero sensitivity to instruction variations, suggesting current instructed retrievers may not effectively support long-running, nuanced exploratory sessions despite strong relevance rankings.

## Method Summary
The paper evaluates instructed retrievers as re-rankers on top-200 documents from first-stage retriever SciNCL using the CSFCube test collection (50 queries, 800k scientific documents). Models are evaluated on NDCG@20 for ranking relevance and p-MRR for instruction following across three aspects: Background, Method, and Result. The study compares dense retrievers (Specter2, SciNCL), multi-vector models (otAspire), fine-tuned instructed retrievers (GritLM-7B), and LLMs with Pairwise Ranking Prompting (gpt-3.5-turbo, gpt-4o, Mistral-7B, Llama-3-8B, Llama-3-70B).

## Key Results
- GritLM-7B and gpt-4o_prp achieve highest NDCG@20 (~42-43) but show p-MRR near zero or negative, indicating instruction-agnostic behavior
- Llama-3-70B shows opposite pattern with lower NDCG but higher p-MRR, suggesting instruction sensitivity
- Method aspect consistently underperforms other aspects (NDCG@20 ~24-30 vs ~45-53) across all models
- Aspect subset strategy improves p-MRR (+5.89) but degrades NDCG (38.21 vs 42.38) due to lost query context

## Why This Works (Mechanism)

### Mechanism 1
- Pairwise Ranking Prompting with LLMs achieves competitive ranking relevance by computing win-rates through pairwise comparisons using heapsort for O(n log n) efficiency
- Core assumption: LLMs reliably judge relative relevance between document pairs better than absolute scoring
- Evidence: PRP strongly outperforms other prompting methods, but external validation studies are limited
- Break condition: Similar documents differing only on instructed aspect may result in random ordering

### Mechanism 2
- Instruction following operates independently of ranking relevance - models achieve high NDCG while showing near-zero instruction sensitivity
- Core assumption: Fine-tuning teaches models to condition on instructions, not just improve overall retrieval quality
- Evidence: p-MRR scores near zero indicate instruction-agnostic behavior despite strong relevance metrics
- Break condition: Generic instructions default to topical similarity, failing aspect-specific steering without explicit guidance

### Mechanism 3
- Aspect-specific sentence selection improves instruction following but degrades ranking relevance
- Core assumption: Aspect sentences contain sufficient signal without surrounding context
- Evidence: p-MRR improves from +2.02 to +5.89, but NDCG@20 drops from 42.38 to 38.21
- Break condition: Sparse or ambiguous aspect sentences reduce discriminative power

## Foundational Learning

- **p-MRR (paired Mean Reciprocal Rank)**: Quantifies instruction following by measuring ranking changes when instructions change for same query. Positive values indicate correct steering; zero indicates instruction-agnostic behavior. *Quick check*: If a model ranks same documents identically for "find papers with similar methods" and "find papers with similar results," what is its p-MRR?

- **Seed-guided exploratory search**: Use case where users start from seed document and explore corpus based on specific aspects, requiring aspect-conditional relevance rather than topical similarity. *Quick check*: How does seed-guided exploration differ from query-by-example retrieval?

- **Two-stage retrieval architecture**: Large instructed retrievers operate as re-rankers over candidates from smaller first-stage models. Understanding this pipeline is essential for interpreting results and computational costs. *Quick check*: Why can't GritLM-7B be used directly on 800k documents, and what does this imply for latency in interactive exploration?

## Architecture Onboarding

- **Component map**: First-stage ranker (SciNCL) -> Re-rankers (GritLM-7B OR LLM+PRP) -> Evaluation (NDCG@20, p-MRR) -> Test collection (CSFCube)
- **Critical path**: Load CSFCube queries with aspect annotations -> Run SciNCL for top-200 candidates -> Apply re-ranker with aspect-specific instruction -> Compute NDCG@20 against expert qrels and p-MRR across aspect pairs
- **Design tradeoffs**: Relevance vs. instruction following (GritLM-7B/gpt-4o maximize NDCG but show weak p-MRR); Generic vs. aspect-specific instructions (generic maintains relevance but loses steering); Cost vs. performance (gpt-4o_prp matches GritLM-7B on NDCG but expensive)
- **Failure signatures**: p-MRR near 0 (model ignores instructions entirely); Negative p-MRR (counter-intuitive responses); Large NDCG gap between aspects (Background well-handled, Method fails)
- **First 3 experiments**: 1) Baseline comparison: SciNCL â†’ GritLM-7B pipeline on CSFCube, report NDCG@20 per aspect and aggregate p-MRR; 2) Instruction sensitivity test: Compare Base vs. Generic vs. Definition vs. Aspect subset instructions, plot NDCG vs. p-MRR tradeoff; 3) Aspect difficulty analysis: Stratify queries by aspect type, identify where Method retrieval fails, test whether PRP with explicit reasoning steps improves performance

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond scientific document retrieval is uncertain due to CS/engineering domain-specificity and limited query count (50 queries)
- Computational cost of PRP with large LLMs raises practical deployment concerns despite performance benefits
- Instruction templates and PRP implementation details are not fully specified, creating reproducibility barriers

## Confidence
- Ranking relevance improvements with instructed retrievers: High
- Disconnect between relevance and instruction-following: Medium
- Aspect-specific sentence strategy effectiveness: Medium

## Next Checks
1. Test instructed retrievers on non-scientific corpus (news/web documents) to assess domain generalization of relevance-instruction following disconnect
2. Implement ablation studies varying instruction specificity and phrasing to determine minimum viable instruction structure for effective aspect steering
3. Compare PRP implementation variations (different prompting strategies, comparison ordering) to isolate whether ranking gains stem from PRP methodology versus underlying LLM capabilities