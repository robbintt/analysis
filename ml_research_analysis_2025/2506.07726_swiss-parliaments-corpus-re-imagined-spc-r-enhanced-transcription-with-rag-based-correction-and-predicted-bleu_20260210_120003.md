---
ver: rpa2
title: 'Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with
  RAG-based Correction and Predicted BLEU'
arxiv_id: '2506.07726'
source_url: https://arxiv.org/abs/2506.07726
tags:
- transcription
- whisper
- swiss
- bleu
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SPCR, a long-form release of the Swiss Parliaments
  Corpus featuring 801 hours of Swiss German parliamentary debate audio aligned with
  official protocols. The authors employ a multi-stage pipeline: initial transcription
  using Whisper Large-v3 with high-compute settings, followed by GPT-4o-based correction
  leveraging RAG from official summaries to fix named entity errors, and evaluation
  via GPT-4o-mini.'
---

# Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU

## Quick Facts
- **arXiv ID**: 2506.07726
- **Source URL**: https://arxiv.org/abs/2506.07726
- **Reference count**: 5
- **Primary result**: 6-point BLEU improvement over original SPC via RAG-based correction and Predicted BLEU filtering

## Executive Summary
This paper introduces SPC_R, a long-form Swiss German speech corpus with 801 hours of parliamentary debate audio and Standard German transcriptions. The authors employ a multi-stage pipeline: initial transcription using Whisper Large-v3 with high-compute settings, followed by GPT-4o-based correction leveraging RAG from official summaries to fix named entity errors, and evaluation via GPT-4o-mini. Quality is ensured through Predicted BLEU scores (derived from Whisper's confidence) and LLM-based filtering, resulting in 751 high-quality hours. The approach achieves a 6-point BLEU improvement over the original SPC dataset, demonstrating that combining robust ASR, LLM correction, and data-driven filtering effectively enhances low-resource domain-specific speech corpora. The dataset is publicly available under CC BY 4.0 license.

## Method Summary
The authors build SPC_R by first transcribing 801 hours of Swiss German parliamentary debate audio using Whisper Large-v3 with high-compute settings (beam_size=10, best_of=10, batch_size=8, float16). They compute Predicted BLEU scores using Whisper's avg_log_prob and a linear regression model, filtering out segments with Predicted BLEU below 65. For remaining segments, they build a RAG index from official session protocols using text-embedding-3-large embeddings and 600-character chunks with 450 overlap. GPT-4o performs correction on retrieved chunks, followed by GPT-4o-mini evaluation. Segments are filtered based on semantic completeness (collapsed judgment tokens 2/3). The pipeline improves WER from 15.7% to 11.1% and named entity accuracy from 72.2% to 100%.

## Key Results
- 6-point BLEU improvement over original SPC dataset
- WER reduction from 15.7% to 11.1%
- Named entity accuracy improvement from 72.2% to 100%
- 751 high-quality hours retained after filtering from 801 total hours

## Why This Works (Mechanism)
The pipeline works by combining robust ASR transcription with domain-specific correction. Whisper Large-v3 provides strong baseline transcription, while RAG-based correction addresses the specific challenge of named entity errors in Swiss German parliamentary debates. Predicted BLEU filtering ensures only high-confidence segments are processed further, reducing computational costs. The multi-stage approach allows for targeted error correction where it matters most - domain-specific terminology and named entities that standard ASR struggles with in low-resource languages.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Why needed - to provide context for correcting domain-specific terminology; Quick check - verify retrieval accuracy exceeds 94%
- **Predicted BLEU**: Why needed - to filter low-quality segments before expensive LLM processing; Quick check - validate linear regression coefficients on held-out data
- **WhisperX**: Why needed - optimized Whisper inference with confidence scoring; Quick check - confirm avg_log_prob extraction works correctly
- **Judgment tokens**: Why needed - to filter semantically incomplete segments; Quick check - ensure collapsed token distribution favors 2/3 (complete segments)
- **Chunk overlap strategy**: Why needed - to ensure full coverage of named entities across chunk boundaries; Quick check - verify no entity is split across chunks
- **Temperature control in LLMs**: Why needed - to minimize over-correction while maintaining accuracy; Quick check - confirm temperature=0.1 produces minimal edits

## Architecture Onboarding

**Component Map**: Audio -> WhisperX Transcription -> Predicted BLEU Calculation -> RAG Index -> GPT-4o Correction -> GPT-4o-mini Evaluation -> Quality Filtering

**Critical Path**: The most compute-intensive steps are Whisper Large-v3 transcription and GPT-4o correction. The pipeline must maintain segment boundaries through all stages to enable accurate evaluation and filtering.

**Design Tradeoffs**: High compute settings for Whisper provide better transcription quality but increase processing time. Using RAG for correction targets specific error types but requires building and maintaining an index. Predicted BLEU filtering reduces costs but may exclude some correctable segments.

**Failure Signatures**: Named entity errors in Whisper output (e.g., "Alba Rutschi" vs "Alberucci") indicate RAG retrieval problems. Over-correction by LLMs (changing conjugations) suggests temperature settings are too high. Low judgment token scores (0/1) indicate semantic incompleteness that requires segment boundary adjustment.

**First Experiments**:
1. Run WhisperX on a small audio sample and verify avg_log_prob extraction works correctly
2. Build RAG index from official summaries and test retrieval accuracy on known named entities
3. Process one segment through the full pipeline (Whisper -> RAG -> GPT-4o -> GPT-4o-mini) to verify component integration

## Open Questions the Paper Calls Out
1. **Semantic evaluation metrics**: Can metrics like Paraboth better capture named entity fidelity compared to standard BLEU scores? The authors suggest exploring this as BLEU doesn't weight named entity errors correctly.
2. **Dialectical robustness**: Does incorporating data from non-Bernese parliamentary debates improve Swiss German ASR performance? The current corpus is restricted to Bernese dialect.
3. **Open-source alternatives**: Do open-source transcription models offer viable cost-performance alternatives to Whisper Large-v3 within this pipeline? The methodology relies entirely on proprietary OpenAI models.

## Limitations
- The 10 manually-transcribed Swiss German datasets used to fit the BLEU-confidence regression are NDA-restricted and not publicly available
- The pipeline relies entirely on proprietary OpenAI models (Whisper Large-v3 and GPT-4o), limiting accessibility
- No downstream task evaluation is provided to demonstrate practical impact beyond BLEU/WER improvements

## Confidence
- **BLEU improvement claim**: High (supported by WER reduction 15.7% → 11.1% and NE accuracy 72.2% → 100%)
- **RAG-based correction pipeline**: Medium (retrieval accuracy reported as 94%+ but correction impact not quantified)
- **Predicted BLEU filtering**: Low (linear regression model trained on NDA-restricted data)

## Next Checks
1. Test the BLEU-confidence regression on a held-out subset of Swiss German audio to verify generalizability of slope=1.59, intercept=–0.68 coefficients
2. Sample 50 corrected segments to manually assess false correction rates and semantic preservation after GPT-4o processing
3. Evaluate corpus utility by running a downstream task (e.g., information extraction) on original vs. SPC_R transcriptions to measure practical impact beyond BLEU/WER metrics