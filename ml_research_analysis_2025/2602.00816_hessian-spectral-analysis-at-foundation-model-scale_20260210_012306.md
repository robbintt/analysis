---
ver: rpa2
title: Hessian Spectral Analysis at Foundation Model Scale
arxiv_id: '2602.00816'
source_url: https://arxiv.org/abs/2602.00816
tags:
- hessian
- lanczos
- vector
- curvature
- fsdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable framework for computing exact Hessian
  vector products under Fully Sharded Data Parallelism (FSDP), enabling second-order
  analysis at foundation model scale. The core method uses shard-local finite-difference
  Hessian vector products that operate directly on FSDP-sharded parameters without
  requiring full parameter gathers.
---

# Hessian Spectral Analysis at Foundation Model Scale

## Quick Facts
- arXiv ID: 2602.00816
- Source URL: https://arxiv.org/abs/2602.00816
- Reference count: 28
- Key outcome: Scalable framework for exact Hessian vector products under FSDP enabling second-order analysis at foundation model scale

## Executive Summary
This paper presents a scalable framework for computing exact Hessian vector products under Fully Sharded Data Parallelism (FSDP), enabling second-order analysis at foundation model scale. The core method uses shard-local finite-difference Hessian vector products that operate directly on FSDP-sharded parameters without requiring full parameter gathers. By avoiding expensive all-gathers and aligning with existing FSDP communication patterns, the approach makes Hessian analysis practical at scale.

The method is validated through stochastic Lanczos quadrature on language models ranging from hundreds of millions to over 100 billion parameters, producing the first Hessian spectral density estimates at this scale. Empirical results demonstrate near-linear scaling across nodes with only modest constant-factor overhead relative to first-order gradient evaluation.

## Method Summary
The paper introduces a shard-local finite-difference Hessian vector product (HVP) primitive compatible with FSDP. This method computes Hv≈∇θL(θ+εv)−∇θL(θ−εv) / 2ε by perturbing parameters in-place on each rank's local DTensor shard. The approach leverages existing FSDP backward pass communication patterns, adding only O(P_loc) local AXPY operations and at most one scalar all-reduce. This avoids expensive full-parameter gathers while maintaining exact second-order information within theoretical error bounds.

## Key Results
- Shard-local finite-difference HVPs enable exact second-order analysis under FSDP without expensive full-parameter gathers
- Widely used block-diagonal curvature approximations fail catastrophically, exhibiting order-one relative error even in mid-scale transformers
- Global curvature statistics stabilize with scale - larger models exhibit more robust Hessian spectral structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shard-local finite-difference Hessian vector products (HvPs) enable exact second-order analysis under FSDP without expensive full-parameter gathers.
- **Mechanism:** The method computes Hv≈∇θL(θ+εv)−∇θL(θ−εv) / 2ε by perturbing parameters in-place on each rank's local DTensor shard. This leverages the existing FSDP backward pass communication pattern, adding only O(P_loc) local AXPY operations and at most one scalar all-reduce.
- **Core assumption:** The central finite-difference truncation error (O(ε²)) and floating-point roundoff error (O(ε_mach/ε)) can be balanced to yield an acceptable total approximation error.
- **Evidence anchors:**
  - [abstract] "Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism... avoiding expensive all-gathers."
  - [section 3.1] Theorem 3.1 quantifies error bounds, showing the optimal step size ε★ and the resulting O(ε_mach^{2/3}) error scaling.
  - [corpus] Weak direct support; the core FSDP-native mechanism is a primary contribution.
- **Break condition:** If the Hessian's third directional derivative ∥∇θ(D³ᵥL(θ))∥ is extremely large or numerical precision is insufficient (e.g., fp16), the finite-difference error overwhelms the true curvature signal.

### Mechanism 2
- **Claim:** Widely used block-diagonal Hessian approximations fail catastrophically even in mid-scale transformers, while full Hessian access reveals strong cross-layer curvature coupling.
- **Mechanism:** The authors quantify this failure by comparing the full Hessian-vector product (Hv) to a block-diagonal proxy (H_block*v). They measure the relative error and cosine similarity between these vectors across transformer layers.
- **Core assumption:** The random probe vectors `v` and sampled datasets adequately represent the typical failure modes of block-diagonal approximations.
- **Evidence anchors:**
  - [abstract] "...direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error..."
  - [section 6, Table 7] Shows a mean relative error of 1.214 and near-zero cosine similarity for a 1.3B parameter code-generation model.
  - [corpus] Weak direct support for this specific finding.
- **Break condition:** The measured errors are an artifact of the finite-difference approximation noise rather than a true property of the loss landscape. The paper's use of theoretically grounded error bounds (Theorem 3.1) attempts to mitigate this.

### Mechanism 3
- **Claim:** Global Hessian spectral structure exhibits emergent stability at foundation model scale.
- **Mechanism:** Using the scalable Lanczos algorithm (enabled by the shard-local HvP), the paper estimates spectral densities for models from 160M to 100B+ parameters. It finds that the spectral estimates for larger models are consistent across a wider range of finite-difference step sizes (ε) and data subsampling rates.
- **Core assumption:** The Lanczos algorithm remains stable in finite precision and with finite-difference matrix-vector products.
- **Evidence anchors:**
  - [section 7.1] "What this exposes is that larger models trained on the same dataset are more stable in metrics of global curvature compared to their smaller counterparts."
  - [section 7.1, Figure 8] Shows consistent spectral density for a 1.4B parameter model across a wide range of ε values.
  - [corpus] The related paper "Local properties of neural networks through the lens of layer-wise Hessians" focuses on a complementary scale.
- **Break condition:** Numerical instabilities in the Lanczos process, such as loss of orthogonality, create spurious "ghost" eigenvalues that corrupt the spectral density estimate. The paper notes using limited iterations for the largest model to manage this.

## Foundational Learning

- **Concept: Hessian-Vector Products (HVPs)**
  - **Why needed here:** HVPs are the fundamental primitive for all second-order analysis in the paper (spectral estimation, influence functions). Understanding how they can be computed efficiently without forming the full Hessian matrix is key.
  - **Quick check question:** How does the Pearlmutter trick compute an HVP using only gradient operations?

- **Concept: Stochastic Lanczos Quadrature (SLQ)**
  - **Why needed here:** SLQ is the core algorithm used to estimate spectral density from HVPs. It connects the abstract idea of eigenvalue distribution to a practical, scalable computation.
  - **Quick check question:** How does SLQ approximate a function of a matrix's eigenvalues using only matrix-vector products?

- **Concept: Fully Sharded Data Parallelism (FSDP)**
  - **Why needed here:** The entire contribution is built to work within an FSDP training environment. The "shard-local" nature of the method is what makes it scalable by avoiding communication bottlenecks.
  - **Quick check question:** What are the communication patterns (all-gather, reduce-scatter) in FSDP's forward and backward passes, and why is avoiding a full parameter gather crucial at 100B+ scale?

## Architecture Onboarding

- **Component map:** Shard-Local HVP Primitive -> Distributed Lanczos Wrapper -> Spectral Density Estimator
- **Critical path:**
  1. Implement the perturbation logic (θ ← θ ± εv) directly on FSDP's DTensor shards
  2. Integrate with the autograd engine to perform two backward passes per HVP
  3. Ensure all vector operations (AXPY, scaling) are performed shard-locally
  4. Implement the Lanczos recurrence, managing orthogonality and numerical precision

- **Design tradeoffs:**
  - **Finite Difference vs. Pearlmutter:** The paper uses finite-difference HvPs, which are conceptually simpler to integrate with FSDP and allow for gradient checkpointing reuse, but introduce approximation error. A Pearlmutter-based HVP (double-backprop) would be exact but potentially more complex to implement with sharding.
  - **Precision vs. Memory:** Storing Lanczos vectors in bf16 vs fp32. The paper finds bf16 sufficient for vectors, saving memory, but fp32 is required for scalar Lanczos coefficients to avoid numerical instability.
  - **Re-orthogonalization:** Full re-orthogonalization in Lanczos improves eigenvalue accuracy but increases memory and communication. The paper suggests a sliding window or no re-orthogonalization for the largest models.

- **Failure signatures:**
  1. **Naive Implementation:** An HVP that triggers a full parameter all-gather, leading to out-of-memory errors or extreme slowdowns.
  2. **Numerical Instability:** Spectral estimates that look like random noise, likely from an inappropriate finite-difference step size (ε) or accumulation of floating-point errors in low precision.
  3. **Breakdown of Approximation:** When applying the method to a new architecture, if the finite-difference error dominates (e.g., due to very high curvature), the spectral estimates will be unreliable.

- **First 3 experiments:**
  1. **Validate HVP Primitive:** On a small model (e.g., 100M params) that fits on a single GPU, compare the shard-local finite-difference HVP against a baseline (e.g., exact Pearlmutter HVP) to validate correctness and quantify approximation error for different ε values.
  2. **Scaling Benchmarks:** Measure the wall-clock time and peak memory of the HVP primitive on a multi-node FSDP cluster (e.g., 8, 16, 32 GPUs). Confirm near-linear scaling and modest overhead relative to a standard gradient pass, as reported in Table 3 & 4.
  3. **Spectral Density Reproduction:** Reproduce a spectral density plot for a known open-source model (e.g., a smaller GPT-2 variant) and check for expected characteristics (e.g., power-law tails, concentration near zero). Verify stability across different Lanczos iterations and probe vectors.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical error bounds for finite-difference approximation rely on assumptions about the Hessian's third directional derivative that may not hold uniformly across all architectures
- The Lanczos-based spectral estimation introduces numerical challenges including potential loss of orthogonality and "ghost" eigenvalues
- The relationship between spectral properties and actual optimization behavior remains largely correlational rather than causal

## Confidence

**High Confidence Claims:**
- The shard-local finite-difference HVP implementation is correct and achieves claimed scalability benefits under FSDP
- Block-diagonal Hessian approximations fail catastrophically with order-one relative error in tested transformer models

**Medium Confidence Claims:**
- Global Hessian spectral structure exhibits emergent stability at foundation model scale
- The overhead of Hessian analysis is "modest" relative to first-order gradients

**Low Confidence Claims:**
- The specific value of spectral density features for individual models
- Generalizability of findings to non-transformer architectures or different training regimes

## Next Checks

1. **Architecture Transfer Test:** Apply the shard-local HVP method to a non-transformer architecture (e.g., a convolutional vision model or a state-space model) and compare the observed block-diagonal approximation failure and spectral stability patterns against the transformer results.

2. **Precision Sensitivity Analysis:** Systematically vary the finite-difference step size ε and compute precision (fp32, bf16, fp16) to quantify their impact on HVP accuracy and Lanczos spectral estimates. Measure actual error in HVP computation against exact methods on smaller models where full Hessian computation is feasible.

3. **Optimization Impact Study:** Conduct controlled experiments using the computed Hessian information for second-order optimization (e.g., natural gradient descent or K-FAC variants) on mid-scale models. Measure actual training speed-up and final performance improvements compared to first-order methods.