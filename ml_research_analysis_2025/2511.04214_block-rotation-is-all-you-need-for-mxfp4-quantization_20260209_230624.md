---
ver: rpa2
title: Block Rotation is All You Need for MXFP4 Quantization
arxiv_id: '2511.04214'
source_url: https://arxiv.org/abs/2511.04214
tags:
- rotation
- quantization
- mxfp4
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a comprehensive benchmark for post-training
  quantization (PTQ) under the MXFP4 format and reveals fundamental incompatibility
  between rotation-based methods and MXFP4. Through systematic evaluation, it shows
  that while compensation-based methods like GPTQ perform well under MXFP4, rotation-based
  methods suffer severe performance degradation.
---

# Block Rotation is All You Need for MXFP4 Quantization

## Quick Facts
- arXiv ID: 2511.04214
- Source URL: https://arxiv.org/abs/2511.04214
- Authors: Yuantian Shao; Peisong Wang; Yuanteng Chen; Chang Xu; Zhihui Wei; Jian Cheng
- Reference count: 15
- Primary result: Block rotation quantization (BRQ) substantially improves post-training quantization accuracy under MXFP4 format by aligning rotation block size with MXFP4's quantization blocks.

## Executive Summary
This paper addresses a fundamental incompatibility between rotation-based quantization methods and the MXFP4 format, establishing a comprehensive benchmark for post-training quantization under this emerging low-precision standard. Through systematic evaluation, the authors reveal that while compensation-based methods like GPTQ perform well under MXFP4, rotation-based methods suffer severe performance degradation due to a mismatch between MXFP4's PoT (power-of-two) block scaling mechanism and global rotation's redistribution of outlier energy across all channels. To resolve this, they propose Block Rotation Quantization (BRQ), which applies rotation transformations independently within each MXFP4 block, substantially improving quantization accuracy across diverse LLMs and outperforming both naive rotation methods and even optimized global rotation approaches.

## Method Summary
The authors propose Block Rotation Quantization (BRQ) that adapts rotation-based methods to MXFP4 by applying rotation transformations within the same block size as MXFP4's quantization blocks (32 channels). BRQ constructs block-diagonal Hadamard rotation matrices and applies them before quantization, optionally integrating with GPTQ for further optimization. The method can be enhanced with gradient-based optimization of rotation matrices (BRQSpin). By confining rotation effects to individual blocks, BRQ prevents the amplification of small values in regular blocks that causes quantization collapse under MXFP4's PoT scaling.

## Key Results
- BRQ substantially improves perplexity across diverse LLMs under MXFP4, outperforming both naive rotation methods and optimized global rotation approaches
- GPTQ consistently delivers strong performance under MXFP4 while rotation-based methods suffer severe incompatibility
- The optimal rotation dimension matches MXFP4 block size (32), with performance degrading when deviating from this size
- BRQ reduces online computation from O(N²) to O(N×32) for down-projection layers while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: The MXFP4-Global Rotation Incompatibility
Global rotation amplifies small values in regular blocks, and MXFP4's PoT scaling cannot accurately reconstruct these newly-amplified values, causing quantization collapse. Global rotation redistributes outlier energy across ALL channels → Small-value regular blocks get amplified → MXFP4's PoT block scaling has coarse granularity at large magnitudes → Increased quantization error in regular blocks → Since regular blocks vastly outnumber outlier blocks, total error dominates.

### Mechanism 2: Block-Wise Rotation Alignment with MXFP4 Structure
Restricting rotation to the same block size as MXFP4's quantization blocks prevents cross-block error propagation while maintaining outlier suppression. Partition activations into blocks matching MXFP4 block size (32) → Apply independent orthogonal transform within each block → Outliers redistributed locally without affecting other blocks' scales → Each block's rotation confined, preventing scale amplification in regular blocks.

### Mechanism 3: Compensation-Based Methods Avoid Rotation Incompatibility
GPTQ performs well under MXFP4 because it avoids rotation entirely, bypassing the redistribution problem. Compensation methods adjust quantized weights via Hessian-based optimization → No rotation applied → No outlier energy redistribution → MXFP4's native block scaling handles outliers effectively.

## Foundational Learning

- **MXFP4 Format (E2M1 + E8M0 block scaling)**: Essential for understanding why PoT scaling creates coarse granularity at large magnitudes—the root cause of incompatibility. Quick check: What is the MXFP4 block size and what format is the scaling factor?

- **Rotation Transformations Preserve L2 Norm**: Core to understanding why global rotation redistributes rather than reduces outlier energy. Quick check: Does rotation change the L2 norm? If not, where does the outlier energy go?

- **PoT Scaling vs. FP16 Scaling**: PoT's discrete exponent steps create larger rounding errors at high magnitudes than FP16's continuous scaling. Quick check: Why does PoT scaling cause larger quantization errors for large values?

## Architecture Onboarding

- **Component map**: Input activations → Partition into 32-channel blocks → Block-diagonal rotation R_block = diag(R_1...R_B) → MXFP4 quantize (E2M1 + E8M0) → Output

- **Critical path**:
  1. Set rotation block size to 32 (matching MXFP4 quantization block)
  2. Construct block-diagonal Hadamard rotation matrix
  3. Apply rotation before quantization; optionally integrate with GPTQ
  4. For further gains, optimize rotation matrices via gradient descent (BRQSpin)

- **Design tradeoffs**:
  - Random vs. optimized rotation: Random is fast; optimized adds training cost but may improve accuracy marginally
  - Rotation dimension: Must equal 32; larger causes cross-block contamination, smaller reduces outlier diffusion
  - Online computation: O(N²) → O(N×32) reduction for down-projection layers

- **Failure signatures**:
  - Global rotation + MXFP4: Perplexity explodes (e.g., LLaMA-2 7B: 7.08→13.09)
  - Block size mismatch: Performance degrades if rotation block ≠ 32
  - Rotation-only (no GPTQ): Suboptimal; always pair with compensation

- **First 3 experiments**:
  1. Reproduce rotation dimension sweep (Figure 8) on LLaMA-3.2 1B to verify block size=32 is optimal.
  2. Compare global vs. block-wise rotation perplexity to quantify incompatibility magnitude.
  3. Measure prefill latency overhead (Table 4) to confirm O(N×32) computational savings.

## Open Questions the Paper Calls Out
None

## Limitations

- **MXFP4 Format Specificity**: The proposed solution is tightly coupled to MXFP4's specific block size (32) and PoT scaling characteristics, raising questions about generalizability to other low-precision formats.
- **Outlier Distribution Assumptions**: The analysis assumes specific outlier distribution patterns in LLM activations that may vary across architectures and tasks.
- **GPTQ Integration Trade-offs**: The interaction between rotation-based methods and compensation-based quantization is complex, with unclear additive effects.

## Confidence

- **Rotation-Redistribution Mechanism**: High - well-supported by empirical evidence and theoretical analysis
- **Block Rotation Optimality**: High - strongly supported by systematic dimension sweep experiments
- **GPTQ Performance Under MXFP4**: High - robustly demonstrated across multiple model sizes and benchmarks
- **MXFP4 Format Specificity**: Medium - tightly coupled to specific format characteristics
- **Outlier Distribution Assumptions**: Medium - based on observed patterns that may not hold universally

## Next Checks

1. **Cross-Format Generalization Test**: Evaluate BRQ performance under FP8 and other emerging quantization formats to determine whether the block-size alignment principle extends beyond MXFP4's specific PoT scaling.

2. **Architecture-Agnostic Outlier Analysis**: Conduct a comprehensive study of outlier distributions across diverse LLM architectures (Transformers, Mamba, etc.) and tasks to validate the assumption about regular-block numerical superiority.

3. **Dynamic Rotation Adaptation**: Implement an adaptive rotation scheme that dynamically adjusts block size or rotation patterns based on activation statistics, testing whether this can further improve BRQ performance or handle cases where the fixed 32-block assumption breaks down.