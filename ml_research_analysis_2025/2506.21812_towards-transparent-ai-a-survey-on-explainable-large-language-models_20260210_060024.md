---
ver: rpa2
title: 'Towards Transparent AI: A Survey on Explainable Large Language Models'
arxiv_id: '2506.21812'
source_url: https://arxiv.org/abs/2506.21812
tags:
- methods
- language
- llms
- probing
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews XAI methods for LLMs, introducing
  a taxonomy based on transformer architectures (encoder-only, decoder-only, encoder-decoder).
  It categorizes techniques like feature attribution, probing, attention-based, and
  self-explanation methods, highlighting their unique interpretability challenges
  and applications.
---

# Towards Transparent AI: A Survey on Explainable Large Language Models

## Quick Facts
- arXiv ID: 2506.21812
- Source URL: https://arxiv.org/abs/2506.21812
- Reference count: 40
- Systematic survey and taxonomy of XAI methods for LLMs across three transformer architectures

## Executive Summary
This survey provides a comprehensive review of Explainable Artificial Intelligence (XAI) methods for Large Language Models (LLMs), introducing a novel taxonomy based on transformer architectures: encoder-only, decoder-only, and encoder-decoder. The authors systematically categorize techniques including feature attribution, probing, attention-based, and self-explanation methods, highlighting the unique interpretability challenges each faces. The work identifies critical gaps in current XAI approaches, particularly around scalability, faithfulness, and ethical considerations, while proposing future research directions to enhance model transparency and trustworthiness.

## Method Summary
The paper conducts a systematic literature review of 40+ papers on XAI techniques for LLMs, categorizing methods based on three transformer architectures and four methodological approaches. The survey synthesizes existing work without implementing new experiments, instead providing a comprehensive framework for understanding how different XAI techniques apply to specific LLM architectures. The categorization considers access levels (internal/external), scope (local/global), and integration methods (post-hoc/prompt-time), creating a structured taxonomy that maps existing research to architectural and methodological dimensions.

## Key Results
- Introduces a novel taxonomy organizing XAI methods by transformer architecture (encoder-only, decoder-only, encoder-decoder)
- Identifies critical challenges in LLM interpretability including scalability, faithfulness, efficiency, and ethical considerations
- Highlights the limitations of attention-based explanations and the need for more robust validation frameworks
- Outlines four key future directions: explanation quality improvement, shortcut learning mitigation, temporal analysis, and ethical framework development

## Why This Works (Mechanism)
The survey's systematic approach works by creating a structured framework that maps existing XAI research to specific LLM architectures and methodological categories. By organizing techniques based on architectural constraints (encoder-only vs decoder-only vs encoder-decoder), the taxonomy reveals why certain explanation methods work better for specific model types. The classification by access level and scope provides clarity on when and how different techniques can be applied, while the identification of common failure modes across architectures helps researchers understand the fundamental limitations of current approaches.

## Foundational Learning
- **Transformer architectures**: Understanding encoder-only, decoder-only, and encoder-decoder differences is crucial because XAI methods must adapt to each architecture's unique properties - why needed: different architectures have distinct information flow patterns that affect explanation methods; quick check: identify which architecture your target model uses.
- **Feature attribution methods**: Techniques like LIME and SHAP quantify feature importance but face scalability challenges with LLMs - why needed: these are foundational for understanding input-output relationships; quick check: verify computational feasibility on your model size.
- **Attention-based explanations**: While intuitive, attention weights may not reflect true decision-making processes - why needed: commonly misused as explanations despite known limitations; quick check: cross-validate with feature attribution results.
- **Probing methods**: Extract linguistic properties from intermediate representations to understand model behavior - why needed: provides insight into how models encode semantic information; quick check: test on multiple linguistic phenomena.
- **Self-explanation generation**: Encoder-decoder models can generate explanations, but may reflect learned patterns rather than causal reasoning - why needed: represents a promising but potentially misleading approach; quick check: verify explanations track with actual reasoning paths.

## Architecture Onboarding

**Component map**: Input text → Tokenizer → Model layers (attention/FFN) → Output logits → Explanation method → Interpretable output

**Critical path**: Text input → Model processing → Intermediate representations → Explanation extraction → Human-interpretable visualization

**Design tradeoffs**: 
- Feature attribution offers scalability but may lack faithfulness
- Attention-based methods are efficient but potentially misleading
- Probing provides linguistic insight but requires task-specific training
- Self-explanation is intuitive but may not reflect true reasoning

**Failure signatures**: 
- Misleading attention explanations when used in isolation
- Computational infeasibility of SHAP on large models
- Explanations that don't track with model behavior changes
- Shortcut learning biases in generated explanations

**3 first experiments**:
1. Implement LIME on BERT-base for sentiment classification to validate feature attribution approach
2. Apply BertViz to visualize attention patterns in GPT-2 for text generation
3. Test TextGenSHAP on T5-small for text-to-text tasks to evaluate encoder-decoder explanations

## Open Questions the Paper Calls Out
**Open Question 1**: How can algorithms be designed to faithfully reflect LLM decision-making processes in the absence of ground truth explanations? The paper explicitly lists this as a key open question, noting that benchmark datasets for global explanations do not exist, making it impossible to validate explanation faithfulness, particularly for emergent abilities that arise without explicit training.

**Open Question 2**: What specific model architectures and data characteristics drive emergent skills in large language models? Section 6 explicitly asks this question, noting that emergent abilities appear spontaneously as models scale, yet the precise structural or data-driven triggers for these skills remain unidentified.

**Open Question 3**: How do prediction rationales differ between fine-tuning and prompting paradigms, and can out-of-distribution (OOD) robustness be linked to these variations? Section 6 poses questions regarding how rationales differ between paradigms and if OOD robustness can be linked to reasoning variations.

**Open Question 4**: Do self-explanations generated by encoder-decoder models reflect causal reasoning processes or merely learned label patterns? Section 5.3.4 discusses self-explanation methods and notes uncertainty about whether explanations reflect causal reasoning or just label patterns from training.

## Limitations
- The survey relies on qualitative synthesis rather than quantitative benchmarking across methods
- Lacks concrete frameworks for implementing ethical considerations in XAI
- Does not provide specific evaluation datasets or hyperparameters for reviewed methods
- Future directions remain aspirational without empirical validation or implementation roadmaps

## Confidence
- **High**: Taxonomic framework for categorizing XAI methods across transformer architectures
- **Medium**: Characterization of methodological challenges (scalability, faithfulness, efficiency)
- **Low**: Proposed future directions and ethical considerations without implementation roadmaps

## Next Checks
1. Implement a comparative study using standardized metrics (faithfulness scores, computational overhead) across at least one method from each architecture category to validate claimed efficiency and scalability challenges
2. Conduct ablation studies to test whether attention-based explanations indeed yield misleading results when used in isolation, as claimed in Section 5.1.4
3. Develop and test a framework for temporal analysis of LLM explanations to address the identified gap in understanding how explanations evolve with model updates or fine-tuning