---
ver: rpa2
title: 'Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to
  Faster Inference'
arxiv_id: '2503.13108'
source_url: https://arxiv.org/abs/2503.13108
tags:
- tokens
- image
- information
- visual
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Multimodal Large Language Models (MLLMs)
  process visual information and proposes a method to accelerate inference. The authors
  identify that in shallow layers, image tokens primarily interact with instruction
  tokens, injecting most visual information into instruction tokens to form cross-modal
  semantic representations.
---

# Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference

## Quick Facts
- arXiv ID: 2503.13108
- Source URL: https://arxiv.org/abs/2503.13108
- Authors: Hao Yin; Guangzong Si; Zilei Wang
- Reference count: 40
- Key outcome: Proposed Hierarchical Modality-Aware Pruning (HiMAP) achieves ~65% FLOPs reduction and 50% latency reduction while maintaining performance across vision-language tasks by dynamically pruning image tokens at specific layers.

## Executive Summary
This paper investigates how Multimodal Large Language Models (MLLMs) process visual information and proposes a method to accelerate inference. The authors identify that in shallow layers, image tokens primarily interact with instruction tokens, injecting most visual information into instruction tokens to form cross-modal semantic representations. In deeper layers, image tokens mainly interact with each other, aggregating the remaining visual information to refine semantic representations within the visual modality. Based on this insight, the authors propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play technique that dynamically prunes image tokens at specific layers, achieving significant computational savings while maintaining model performance across various vision-language tasks.

## Method Summary
The method leverages a two-phase understanding of visual information flow in MLLMs. In shallow layers (around layer 2), visual information is primarily injected into instruction tokens through strong image-to-text interactions. In deeper layers (around layer 8-15), remaining visual information is aggregated through image-to-image interactions. HiMAP implements hierarchical pruning by first removing less important image tokens based on their attention from instruction tokens in shallow layers, then removing additional less important image tokens based on their attention from other image tokens in deeper layers. This two-stage approach preserves critical cross-modal information while eliminating redundant visual details, achieving 65-75% FLOPs reduction and 50% latency reduction across multiple benchmarks.

## Key Results
- HiMAP achieves approximately 65% reduction in FLOPs and 50% reduction in inference latency
- Maintains or improves model performance across VQAv2, TextVQA, MME, ScienceQA, A-OKVQA, and captioning tasks
- Successfully applies to multiple MLLM architectures including LLaVA-v1.5-7B/13B, QwenVL-Chat-7B, and InternVL-v1.0-7B

## Why This Works (Mechanism)

### Mechanism 1: Phased Visual Information Processing
- Claim: MLLMs process visual information in two distinct phases—cross-modal injection in shallow layers followed by intra-visual aggregation in deeper layers.
- Mechanism: In layers 1–3, attention matrices show strong image-to-instruction interactions (Svt >> Svv), enabling most visual information transfer to text tokens. In layers 8+, image-to-image interactions dominate (Svv >> Svt), consolidating residual visual information.
- Core assumption: The observed attention patterns reflect functional information transfer, not just correlation.
- Evidence anchors:
  - [abstract] "a shift in the dominant flow of visual information is uncovered: (1) in shallow layers, strong interactions are observed between image tokens and instruction tokens... (2) in deeper layers, image tokens primarily interact with each other"
  - [section 3.1, Figure 4] Shows Svt dominates in layers 1–3 while Svv dominates in layers 8–16 across LLaVA-7B/13B on A-OKVQA and Sci-VQA
  - [corpus] No direct corpus corroboration; related work (ShortV, VisiPruner) explores layer-wise redundancy but doesn't confirm this specific two-phase pattern
- Break condition: If blocking visual-textual flow in shallow layers causes no performance degradation, the hypothesis fails.

### Mechanism 2: Disruption-Based Validation of Information Flow
- Claim: Perturbing specific attention pathways reveals which flows are functionally necessary at each depth.
- Mechanism: Setting attention weights Al(i,j) = 0 for specific token pairs blocks information flow. The resulting prediction deviation (measured via Label Consistency and Score Consistency) quantifies pathway importance.
- Core assumption: Zeroing attention weights effectively blocks information transfer rather than just adding noise.
- Evidence anchors:
  - [section 3.2, Figure 5] "performance dropped significantly when disruptions occurred in the first five layers, but this effect diminished with increasing network depth"
  - [section 3.3, Equation 7] Bias Ratio Dl = log(Evv,l/Evt,l) shows Dl ≈ -0.5 in shallow layers (visual-textual dominates) and Dl ≈ 1.2 in deeper layers (intra-visual dominates)
  - [corpus] Weak corpus support; no neighboring papers replicate this specific perturbation methodology
- Break condition: If random attention disruptions cause equal or greater performance drops than targeted disruptions, the method captures noise rather than signal.

### Mechanism 3: Hierarchical Importance-Based Token Pruning
- Claim: Pruning image tokens based on layer-specific importance criteria preserves performance while reducing FLOPs by ~65%.
- Mechanism: At layer K1, rank image tokens by ϕsh(v) = Σ A(i,v) for i∈I (attention from instruction tokens), prune bottom R1%. At layer K2, rank remaining tokens by ϕdp(v) = Σ A(v,i) for i∈V (attention from other image tokens), prune bottom R2%.
- Core assumption: Attention scores accurately reflect token importance for downstream tasks; tokens with low attention contribution are truly redundant.
- Evidence anchors:
  - [section 4.1, Equations 8-9] Defines the two importance criteria
  - [Table 1] HiMAP achieves 24% FLOPs ratio on LLaVA-7B while matching or exceeding baseline accuracy on VQAv2 (78.6 vs 78.3), TextVQA (58.4 vs 58.2), MME (1785.1 vs 1749.9)
  - [corpus] FastV uses a single-layer attention criterion; VisiPruner proposes cross-modal dynamics but doesn't implement hierarchical two-stage pruning
- Break condition: If aggressive pruning (e.g., R1=50%, R2=75%) causes >2% accuracy drop on benchmarks, the importance criteria are inadequate.

## Foundational Learning

- **Attention Mechanism and Saliency Mapping**
  - Why needed here: The paper relies on computing saliency scores via Taylor expansion (Il = Σ Ah,l ⊙ ∂L/∂Ah,l) to quantify token importance. Without understanding how gradients flow through attention, the importance metrics are opaque.
  - Quick check question: Given an attention matrix A of shape (seq_len, seq_len) and loss L, how would you compute which token j contributes most to token i's representation?

- **Transformer Decoder Autoregressive Generation**
  - Why needed here: MLLMs process input tokens (system, image, instruction) through stacked transformer layers before generating output tokens autoregressively. Understanding this flow is prerequisite to knowing where pruning can be safely inserted.
  - Quick check question: In a decoder-only transformer, why can position i attend to positions < i but not > i, and how does this affect which attention weights are valid to analyze?

- **FLOPs Estimation for Transformer Layers**
  - Why needed here: The paper claims 65% FLOPs reduction via Equation 10, which requires understanding the computational cost of attention (O(n²d)) and FFN (O(ndm)) modules.
  - Quick check question: For n=576 image tokens, d=4096 hidden dim, m=11008 FFN intermediate dim, calculate the FLOPs for one transformer layer's image-related computation.

## Architecture Onboarding

- **Component map:**
  Input: System tokens (~35) + Image tokens (576 from CLIP-ViT) + Instruction tokens (~135) → Unified embedding sequence → Shallow-layer pruning module (layer K1) → Deeper-layer pruning module (layer K2) → Remaining layers → Output tokens

- **Critical path:**
  1. Tokenize and embed inputs → form combined sequence X = S ∪ V ∪ I
  2. Pass through layers 1 to K1-1 unmodified
  3. At layer K1: extract attention matrix, compute ϕsh for all image tokens, prune bottom R1%
  4. Pass pruned sequence through layers K1+1 to K2-1
  5. At layer K2: compute ϕdp for remaining image tokens, prune bottom R2%
  6. Continue autoregressive generation with reduced sequence

- **Design tradeoffs:**
  - K1 selection: Earlier K1 = more aggressive early pruning but risk losing critical cross-modal information. Paper uses K1=2 as default.
  - K2 selection: Later K2 = more conservative, preserves visual detail for generation tasks. Paper uses K2=8 for QA tasks, K2=15 for captioning.
  - R1/R2 ratios: Higher = faster but risk hallucination. Paper shows R1=50%, R2=75% works for QA; R1=50%, R2=50% needed for captioning.

- **Failure signatures:**
  - Pruning in shallow layers based on intra-visual flow (img2img) instead of visual-textual flow → 5%+ accuracy drop (Table 4: ScienceQA drops to 62.4)
  - Aggressive K2=8, R2=87.5% for image captioning → CIDEr drops from 78.8 to 74.6 (Table 8)
  - Applying same parameters across different model sizes → suboptimal; LLaVA-13B needs different K values than LLaVA-7B

- **First 3 experiments:**
  1. **Baseline attention flow visualization**: Run LLaVA-7B on A-OKVQA, log Svt and Svv per layer. Confirm Svt > Svv in layers 1–3 and Svv > Svt in layers 8+. Verify the two-phase pattern exists before implementing pruning.
  2. **Ablation on pruning criteria**: Compare three variants on ScienceQA: (a) shallow pruning with ϕsh only, (b) deep pruning with ϕdp only, (c) both. Measure accuracy and FLOPs. Expect Table 9 pattern: individual modules help, combined is best.
  3. **Task-specific hyperparameter sweep**: Grid search K1∈{1,2,3}, R1∈{30%,50%,70%}, K2∈{8,12,15}, R2∈{50%,75%,87.5%} on VQAv2 (short-answer) vs Nocaps (captioning). Confirm QA tolerates aggressive settings while captioning requires conservative K2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the "Phased Processing" hypothesis—where visual information is injected into text tokens in shallow layers and aggregated intra-visually in deeper layers—generalize to MLLM architectures that utilize cross-attention modules (e.g., Q-Former) or encoder-decoder backbones, rather than the decoder-only linear projection architectures analyzed here?
- **Basis in paper:** [Explicit] The conclusion explicitly limits the validation scope: "Results... confirm this hypothesis for the LLaVA-v1.5 series models." Furthermore, the method is primarily tested on decoder-only models (LLaVA, QwenVL, InternVL) which process visual tokens similarly to text tokens in a unified sequence.
- **Why unresolved:** The mechanism is identified by analyzing attention maps specific to decoder-only transformers where visual tokens compete with text tokens in the same sequence. Architectures with separate cross-attention mechanisms (like Flamingo or BLIP-2) might process visual information in a fundamentally different way that does not exhibit this specific "shift" in flow.
- **What evidence would resolve it:** Applying the specific perturbation experiments (blocking visual-textual flow in shallow layers) and saliency metrics ($S_{vv}$ vs $S_{vt}$) to cross-attention based MLLMs to see if the distinct layer-wise phase shift persists.

### Open Question 2
- **Question:** Can the transition layers ($K_1$ and $K_2$) and pruning ratios ($R_1$ and $R_2$) be determined dynamically or adaptively based on input complexity, rather than requiring manual hyperparameter tuning for different task types?
- **Basis in paper:** [Inferred] from Section 5.1 (Experimental Setup). The authors note that they "adopts an aggressive parameter configuration... for structured question-answering" but "applies a more conservative parameter configuration... for open-ended generation." This implies the optimal pruning strategy is task-dependent and currently requires manual selection.
- **Why unresolved:** The paper provides static heuristics (e.g., Layer 2 for shallow, Layer 8 or 15 for deep) that work well on average but do not account for the variability in image complexity or question difficulty within a single dataset.
- **What evidence would resolve it:** A study correlating the "informativeness" or entropy of image tokens with the optimal pruning layer, or the development of a feedback loop where the model decides $K$ and $R$ on the fly without performance degradation.

### Open Question 3
- **Question:** Is the observed "inefficient contribution" of image tokens (only 0.03% relative importance) an inherent redundancy of the Vision Transformer (ViT) encoder, or is it a failure of the LLM to fully exploit fine-grained visual features?
- **Basis in paper:** [Explicit] Section 2.2 states image tokens comprise 77% of input but contribute minimally to predictions. The authors "hypothesize" this might stem from "redundancy inherent in the image signals provided to the model," but they do not isolate the source of this redundancy.
- **Why unresolved:** While the paper demonstrates that pruning these tokens maintains performance, it does not determine if the LLM is "lazy" (ignoring available details) or if the ViT is "verbose" (generating useless tokens). This distinction is critical for future model design (better encoders vs. better connectors).
- **What evidence would resolve it:** An analysis comparing the saliency of ViT patch tokens before they enter the LLM projection layer versus after, or testing if removing "redundant" tokens *before* the LLM (at the encoder output) yields the same performance boost as the proposed internal pruning.

### Open Question 4
- **Question:** Does the "Visual Information Injection" in shallow layers create an irreversible semantic bottleneck that limits performance on tasks requiring precise spatial localization or counting, even if high-level semantic performance is maintained?
- **Basis in paper:** [Inferred] from results in Section 5.2 and supplementary material. While HiMAP maintains performance on high-level tasks (VQA, Captioning), the paper shows that pruning in shallow layers (Section 3.2) causes "significant degradation." If visual info is compressed into text tokens early, fine-grained pixel data might be lost, explaining why some pruning strategies fail on counting/OCR tasks.
- **Why unresolved:** The paper evaluates on ChartQA and DocQA (Table 5) and claims success, but the fundamental trade-off between the "injection" mechanism (semantic compression) and "pixel-perfect" grounding is not deeply explored for the most aggressive pruning settings.
- **What evidence would resolve it:** A targeted evaluation on counting benchmarks (e.g., MME-Count) specifically comparing the degradation rates of shallow pruning (which destroys injection) vs. deep pruning (which destroys aggregation) to see which phenomenon is more critical for spatial reasoning.

## Limitations

- The core hypothesis about phased visual information processing relies on attention pattern interpretation, which could conflate correlation with causation rather than proving functional information transfer.
- The pruning criteria assume attention scores directly reflect token importance, which may not hold for all task distributions or model architectures.
- The methodology is validated primarily on LLaVA-v1.5 variants, leaving generalization to other MLLM architectures uncertain.

## Confidence

**High Confidence**: The empirical demonstration that HiMAP achieves ~65% FLOPs reduction with minimal accuracy loss across multiple benchmarks. The ablation studies clearly show both shallow and deep pruning components contribute independently to performance.

**Medium Confidence**: The interpretation of attention patterns as revealing distinct information processing phases. While the observed Svt > Svv and Svv > Svt patterns are consistent, alternative explanations (such as architectural bias or training artifacts) haven't been ruled out.

**Low Confidence**: The universal applicability of the two-phase processing model across different MLLM architectures and tasks. The study focuses on LLaVA variants and specific vision-language tasks, with limited exploration of open-domain generation or non-QA applications.

## Next Checks

1. **Cross-Architecture Validation**: Apply HiMAP to non-LLaVA MLLMs (QwenVL, InternVL) and verify the same two-phase attention pattern emerges. Test whether identical K1/K2/R1/R2 parameters work or require architecture-specific tuning.

2. **Controlled Information Flow Blocking**: Design an experiment where visual information is explicitly blocked from reaching instruction tokens in shallow layers (not just attention zeroing, but actual information pathway disruption). Measure whether this causes the predicted performance degradation, confirming the causal relationship between observed attention patterns and functional information transfer.

3. **Adversarial Token Importance Testing**: Create synthetic image-token scenarios where attention scores are manipulated while visual content remains constant. Test whether HiMAP's pruning decisions based on attention scores align with actual visual information content, validating the assumption that attention scores reflect token importance rather than superficial correlation.