---
ver: rpa2
title: Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical
  Evaluation
arxiv_id: '2502.17899'
source_url: https://arxiv.org/abs/2502.17899
tags:
- suicidal
- ideation
- implicit
- suicide
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models' (LLMs) ability to identify
  and respond to implicit suicidal ideation. Using a novel dataset, DeepSuiMind, grounded
  in psychological theory, the authors assess 8 widely used LLMs across two prompting
  conditions.
---

# Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation

## Quick Facts
- **arXiv ID**: 2502.17899
- **Source URL**: https://arxiv.org/abs/2502.17899
- **Reference count**: 40
- **Primary result**: LLMs struggle to detect subtle suicide cues and provide appropriate support, with most scoring below 80 in response quality and under 50% in appropriate response rate, though distress-aware prompting improves performance.

## Executive Summary
This study evaluates large language models' ability to identify and respond to implicit suicidal ideation using a novel dataset, DeepSuiMind, grounded in psychological theory. The authors assess 8 widely used LLMs across two prompting conditions. Results show models struggle to detect subtle suicide cues and often fail to provide appropriate support, with most scoring below 80 in response quality and under 50% in appropriate response rate. Distress-aware prompting improves performance but gaps remain, especially in reducing hopelessness and offering hope. The findings highlight the need for more clinically grounded evaluation frameworks to ensure safe LLM deployment in sensitive mental health contexts.

## Method Summary
The study uses the DeepSuiMind dataset (~1,200+ test cases) grounded in D/S-IAT associations, Automatic Negative Thoughts (ANT), and psychosocial stressors. Eight LLMs are evaluated under two prompting conditions: Standard Setting (neutral) and Distress-Aware Setting (subtle emotional cue). Responses are scored using a 5-dimension rubric (Empathy, Connection, Practical Support, Reducing Hopelessness, Offering Hope) via GPT-4 evaluator, validated against human raters. Performance is compared against explicit ideation posts from the SDCNL dataset.

## Key Results
- Models exhibit systematic recognition gaps between explicit and implicit suicidal ideation, with implicit condition ARSR typically below 50% versus over 90% for explicit
- Distress-aware prompting improves performance but does not close the implicit-explicit detection gap
- LLMs score highest on empathy dimensions but lowest on hopelessness reduction and hope offering, revealing failure to engage with intervention-relevant cognition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Distress-aware prompting improves LLM sensitivity to implicit suicidal cues, but does not close the implicit-explicit detection gap.
- **Mechanism**: A subtle system prompt indicating user distress primes models to attend to psychological risk signals, shifting recognition capacity rather than response formulation.
- **Core assumption**: Models possess latent knowledge of crisis intervention patterns but lack automatic activation triggers for ambiguous input.
- **Evidence anchors**: Abstract states "Distress-aware prompting improves performance but gaps remain"; section 5.2 shows Gemini-1.5 ARSR rising from 77.67% to 91.83% under distress-aware prompting.

### Mechanism 2
- **Claim**: LLMs exhibit a systematic recognition gap between explicit and implicit suicidal ideation, driven by reliance on surface-level lexical patterns rather than psychological inference.
- **Mechanism**: Models perform well when suicidal intent is overtly stated because training data contains such patterns, but fail on implicit expressions lacking surface markers despite equivalent risk severity.
- **Core assumption**: Training data over-represents explicit help-seeking language; implicit suicidal communication requires cognitive inference beyond pattern matching.
- **Evidence anchors**: Abstract notes "models struggle to detect subtle suicide cues"; section 5.2 shows DeepSeek-R1 disparity (ARSR increasing from 51.86 to 96.12) between implicit and explicit conditions.

### Mechanism 3
- **Claim**: LLM responses score highest on empathy dimensions but lowest on hopelessness reduction and hope offering, revealing failure to engage with intervention-relevant cognition.
- **Mechanism**: Models generate emotionally soothing language because conversational training prioritizes rapport, but fail to identify and counter cognitive distortions underlying suicidal ideation.
- **Core assumption**: High empathy scores reflect surface-level alignment; intervention effectiveness requires deeper cognitive engagement that current architectures do not support.
- **Evidence anchors**: Section 5.2 states "Empathy scores are highest (>17)... low scores (<16) in more diagnostic dimensions like Reducing Hopelessness and Offering Hope indicate difficulty recognizing implicit emotional despair."

## Foundational Learning

- **Concept: Death/Suicide Implicit Association Test (D/S-IAT)**
  - **Why needed here**: Dataset construction relies on mapping D/S-IAT associations (Self–Death, Self–Life, Others–Death) to language patterns.
  - **Quick check question**: Can you explain how a "Self–Death" association manifests linguistically versus a "Life–Not-Me" pattern?

- **Concept: Automatic Negative Thoughts (ANT)**
  - **Why needed here**: Dataset embeds cognitive distortions (all-or-nothing thinking, overgeneralization, catastrophizing) into synthetic samples.
  - **Quick check question**: Given a user statement "I failed this test, so I'm a total failure," which ANT category applies?

- **Concept: Crisis Intervention Principles**
  - **Why needed here**: Five evaluation dimensions derive from established clinical frameworks (Rogers, Beck, Snyder).
  - **Quick check question**: Why is "reducing hopelessness" scored independently from "offering hope" in the evaluation framework?

## Architecture Onboarding

- **Component map**: D/S-IAT associations -> ANT cognitive distortions -> Psychosocial stressors -> GPT-4 synthesis (1,200+ samples) -> 8 LLMs evaluation -> GPT-4 automated evaluation -> ARSR/HRR metrics
- **Critical path**: Load DeepSuiMind dataset from HuggingFace -> For each test case, generate LLM response under both prompting conditions -> Score each response using GPT-4 evaluator -> Compute ARSR, HRR, and dimensional averages -> Compare implicit vs. explicit performance
- **Design tradeoffs**: Synthetic data enables controlled psychological grounding but may not capture full cultural/linguistic nuance; GPT-4 as evaluator introduces model-dependency; binary applicability threshold may be conservative
- **Failure signatures**: High empathy but low hopelessness reduction indicates surface-level emotional alignment without cognitive engagement; ARSR <50% under implicit conditions signals recognition failure rather than response formulation failure
- **First 3 experiments**:
  1. Baseline Replication: Run all 8 LLMs on DeepSuiMind under standard and distress-aware prompting; verify ARSR and HRR metrics
  2. Ablation on Prompting: Test whether explicit distress cues outperform subtle distress-aware prompts to isolate recognition vs. response tradeoff
  3. Cross-Domain Validation: Apply evaluation framework to SDCNL explicit subset to confirm implicit-explicit gap

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent do findings from the DeepSuiMind synthetic dataset generalize to authentic, real-world clinical interactions? The authors acknowledge synthetic data cannot fully substitute for real-world observations and findings should be viewed as complementary evidence.
- **Open Question 2**: How do specific model architectures differ in their sensitivity to distress-aware prompting cues? The study notes further research is needed to explore which specific model architectures respond most effectively to different forms of explicit guidance.
- **Open Question 3**: How does the detection of implicit suicidal ideation vary across non-English languages and diverse cultural contexts? The paper notes the study primarily focuses on English, which may limit applicability across diverse linguistic and cultural contexts.

## Limitations

- The study relies on synthetic data generation for implicit ideation cases, which may not fully capture the linguistic and cultural diversity of real suicidal discourse
- The automated evaluation framework, while validated against human raters, introduces model-dependency that may systematically bias results
- Claims about specific cognitive mechanisms underlying implicit recognition failures are speculative without direct ablation studies on model internals

## Confidence

- **High Confidence**: The observed performance gap between explicit and implicit conditions is robust, as evidenced by consistent patterns across multiple LLMs and theoretical grounding
- **Medium Confidence**: The specific score thresholds and five-dimensional evaluation framework are reasonable but could be sensitive to evaluator prompt variations
- **Low Confidence**: Claims about the specific cognitive mechanisms underlying implicit recognition failures are speculative without direct ablation studies

## Next Checks

1. **Cross-Dataset Validation**: Apply the evaluation framework to real-world implicit ideation samples from social media to test whether synthetic data performance generalizes
2. **Prompt Ablation Study**: Systematically vary distress-aware prompt strength and specificity to identify minimum effective signal for implicit recognition
3. **Evaluator Independence Test**: Replace GPT-4 evaluator with an alternative LLM or human-only scoring on a subset to assess sensitivity of reported metrics to evaluator model choice