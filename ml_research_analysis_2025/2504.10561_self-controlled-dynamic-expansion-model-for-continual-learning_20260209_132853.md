---
ver: rpa2
title: Self-Controlled Dynamic Expansion Model for Continual Learning
arxiv_id: '2504.10561'
source_url: https://arxiv.org/abs/2504.10561
tags:
- learning
- continual
- task
- feature
- backbone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Self-Controlled Dynamic Expansion Model
  (SCDEM) for continual learning across multiple data domains. The method addresses
  catastrophic forgetting and limited plasticity in existing models by orchestrating
  multiple pre-trained ViT backbones as a shared module and dynamically constructing
  new experts for each incoming task.
---

# Self-Controlled Dynamic Expansion Model for Continual Learning

## Quick Facts
- arXiv ID: 2504.10561
- Source URL: https://arxiv.org/abs/2504.10561
- Authors: Runqing Wu; Kaihui Huang; Hanyi Zhang; Fei Ye
- Reference count: 40
- Primary result: 44.42% relative error reduction vs. DER+++refresh, 17.25% vs. StarPrompt

## Executive Summary
This paper introduces a Self-Controlled Dynamic Expansion Model (SCDEM) for continual learning across multiple data domains. The method addresses catastrophic forgetting and limited plasticity in existing models by orchestrating multiple pre-trained ViT backbones as a shared module and dynamically constructing new experts for each incoming task. SCDEM employs three key mechanisms: a Collaborative Optimization Mechanism (COM) that uses KL divergence to maintain prediction consistency across historical and current experts, a Feature Distribution Consistency (FDC) approach that aligns semantic similarity between representations via optimal transport distance, and a Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) that autonomously determines regularization intensity per layer to avoid over-regularization. Extensive experiments across dual, triple, and quadruple domain settings demonstrate that SCDEM significantly outperforms state-of-the-art methods while reducing computational overhead.

## Method Summary
SCDEM uses multiple pre-trained ViT backbones with only the last L=3 layers trainable. For each new task, the method creates a new expert consisting of an adaptive module and classifier. The training process involves three key losses: (1) standard cross-entropy classification, (2) Collaborative Optimization Mechanism using KL divergence between predictions from active and frozen backbones, and (3) Feature Distribution Consistency using Wasserstein distance to align semantic representations. After training, the backbones are frozen and new experts are added for subsequent tasks. The Dynamic Layer-Wise Feature Attention Mechanism automatically adjusts regularization intensity per layer to prevent over-regularization while maintaining plasticity.

## Key Results
- Achieves up to 44.42% relative error reduction compared to DER+++refresh across multiple domain settings
- Outperforms StarPrompt by 17.25% while using 51.08% fewer parameters and 47.29% less GPU memory
- Shows consistent improvement from dual to triple backbone configurations, with diminishing returns beyond three backbones
- Demonstrates strong performance across class-incremental and task-incremental learning scenarios

## Why This Works (Mechanism)
SCDEM addresses the fundamental tension in continual learning between preventing catastrophic forgetting and maintaining plasticity for new tasks. The method achieves this by: (1) using pre-trained backbones as stable feature extractors that are frozen after each task, (2) adding new experts for each task rather than modifying existing ones, (3) using KL divergence to ensure consistency between new and historical predictions, and (4) employing optimal transport to align semantic distributions across tasks. The dynamic attention mechanism ensures that regularization is applied appropriately at different layers, preventing both under-regularization (forgetting) and over-regularization (loss of plasticity).

## Foundational Learning
- **ViT backbones and pretraining**: Understanding Vision Transformer architecture and pretraining schemes (ImageNet-21k vs 1k) is crucial for replicating the feature extraction foundation
- **Optimal transport and Wasserstein distance**: Required for implementing the FDC mechanism that aligns semantic distributions across tasks
- **KL divergence for knowledge distillation**: Core to the COM mechanism that maintains consistency between historical and current predictions
- **Continual learning evaluation metrics**: Average accuracy across tasks and Last accuracy on final task are standard metrics for assessing forgetting and plasticity
- **Multi-expert systems**: Understanding how to dynamically add and manage multiple experts for sequential tasks

## Architecture Onboarding

**Component Map**
Pre-trained ViTs (frozen except last L layers) -> Shared Feature Extractor -> Dynamic Layer-Wise Feature Attention -> Fused Representation -> Multiple Task Experts (one per task)

**Critical Path**
Input -> Backbone feature extraction -> Attention-weighted fusion -> Expert classification -> COM loss (KL divergence) + FDC loss (Wasserstein) -> Total loss optimization

**Design Tradeoffs**
- **Backbone count vs. efficiency**: More backbones improve performance but increase parameters and memory usage
- **Trainable layers (L) vs. plasticity**: More trainable layers increase plasticity but risk forgetting and computational cost
- **Regularization intensity vs. adaptation**: Higher regularization prevents forgetting but may limit learning new tasks
- **Expert capacity vs. scalability**: More complex experts improve performance but increase memory and computation per task

**Failure Signatures**
- **Over-regularization**: Poor performance on new tasks, high training loss, low learning rate required
- **Catastrophic forgetting**: Degradation on previous tasks when learning new ones, high COM loss
- **Expert selection failure**: High entropy in expert selection, incorrect task identification
- **Distribution misalignment**: High FDC loss, poor semantic consistency across tasks

**3 First Experiments**
1. Verify backbone freezing mechanism works correctly by checking parameter updates during training
2. Test KL divergence computation between predictions from active vs. frozen backbones
3. Validate Wasserstein distance calculation for feature distribution alignment using synthetic data

## Open Questions the Paper Calls Out
- **Open Question 1**: How does increasing the number of pre-trained backbones beyond three affect the trade-off between performance saturation and computational efficiency?
- **Open Question 2**: How sensitive is the model's performance to the hyperparameter selection of the number of trainable layers (L) within the backbones?
- **Open Question 3**: Is the proposed entropy-based expert selection mechanism robust for Class-Incremental Learning scenarios with high task similarity or blurry boundaries?

## Limitations
- **Unknown hyperparameters**: Critical values for learning rates, batch sizes, loss weights, and ViT configuration are not specified
- **Implementation complexity**: Requires managing multiple frozen backbones and dynamically adding experts, increasing code complexity
- **Computational overhead**: While more efficient than some baselines, the method still requires multiple pre-trained backbones
- **Task identifier dependency**: Current implementation requires task identifiers, limiting applicability to scenarios with ambiguous task boundaries

## Confidence
- **High confidence**: General architectural framework and three proposed mechanisms are clearly described
- **Medium confidence**: Theoretical formulations of losses appear sound but practical implementation details are missing
- **Low confidence**: Absolute performance claims cannot be independently verified without code and hyperparameters

## Next Checks
1. Request and test with the exact ViT-Base model specification (pretrained weights source and configuration)
2. Verify the practical implementation of the Wasserstein distance calculation with specified regularization parameters
3. Confirm the frozen backbone snapshotting mechanism preserves model states correctly between tasks