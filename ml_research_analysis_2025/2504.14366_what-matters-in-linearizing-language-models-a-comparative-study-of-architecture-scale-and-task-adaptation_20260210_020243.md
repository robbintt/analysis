---
ver: rpa2
title: What Matters in Linearizing Language Models? A Comparative Study of Architecture,
  Scale, and Task Adaptation
arxiv_id: '2504.14366'
source_url: https://arxiv.org/abs/2504.14366
tags:
- gated
- deltanet
- distillation
- teacher
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic empirical comparison of subquadratic
  language models trained via knowledge distillation from Transformer teachers. By
  comparing seven representative architectures under a unified training setup, the
  study isolates how architectural design choices affect distillation performance,
  scaling behavior, and downstream adaptation.
---

# What Matters in Linearizing Language Models? A Comparative Study of Architecture, Scale, and Task Adaptation

## Quick Facts
- arXiv ID: 2504.14366
- Source URL: https://arxiv.org/abs/2504.14366
- Authors: Patrick Haller; Jonas Golde; Alan Akbik
- Reference count: 7
- Key outcome: This paper presents a systematic empirical comparison of subquadratic language models trained via knowledge distillation from Transformer teachers. By comparing seven representative architectures under a unified training setup, the study isolates how architectural design choices affect distillation performance, scaling behavior, and downstream adaptation. The results show that performance hierarchies remain stable across parameter scales from 140M to 1.7B, with gated state-update models outperforming ungated alternatives. A key finding is that error-correcting update rules, such as those in Gated DeltaNet, exhibit superior scaling and long-context retrieval compared to purely additive models. While all models improve with more training data, architectural differences persist, suggesting that the structure of the token mixer is the primary determinant of success. Instruction tuning amplifies these differences rather than compensating for weaker designs. Overall, the findings highlight that selective memory and controlled forgetting are critical for effective linearization, more so than simply increasing the distillation token budget.

## Executive Summary
This study systematically compares seven subquadratic language model architectures distilled from Transformer teachers, establishing a clear performance hierarchy that persists across scales from 140M to 1.7B parameters. The research identifies gated delta-rule models (particularly Gated DeltaNet) as superior performers due to their error-correcting state updates and input-dependent gating mechanisms that prevent state saturation. The authors demonstrate that architectural differences are fundamental rather than optimization artifacts, as performance gaps established early in training persist even with 10B training tokens. A key finding is that instruction tuning amplifies rather than compensates for architectural limitations, confirming that state resolution capacity is the primary bottleneck for subquadratic models.

## Method Summary
The study employs a three-stage knowledge distillation pipeline to convert pretrained Transformer teachers into subquadratic student models. Stage 1 aligns attention outputs via MSE loss (80M tokens), Stage 2 aligns hidden states via L2 loss (160M tokens), and Stage 3 performs end-to-end distillation combining cross-entropy with KL divergence. Seven token mixers are compared: Linear Attention, Gated Linear Attention, RetNet, xLSTM, DeltaNet, Gated DeltaNet, and Kimi. All models are trained on the FineWeb dataset (3B tokens standard, 10B for scaling) and evaluated on zero-shot benchmarks, needle-in-a-haystack retrieval tasks, and instruction-following capabilities. Students are initialized by copying teacher weights, with new parameters trained from scratch.

## Key Results
- Performance hierarchy remains stable across 140M to 1.7B parameter scales, with gated models consistently outperforming ungated alternatives
- Gated DeltaNet achieves 81.8% accuracy at 2k tokens on S-NIAH-1 versus xLSTM's 25.2% and Linear Attention's 0%
- At 1.7B parameters, gated models (Gated DeltaNet 94.2%, xLSTM 91.6%) substantially outperform ungated Linear Attention (70.9%)
- Performance gaps are established early and persist through asymptotic maturity at 10B tokens
- Instruction tuning amplifies architectural differences rather than compensating for weaker designs

## Why This Works (Mechanism)

### Mechanism 1: Error-Correcting State Updates Enable Scalable Retrieval
- **Claim:** Delta-rule update mechanisms allow subquadratic models to maintain higher state resolution at scale compared to purely additive approaches.
- **Mechanism:** The delta-rule applies content-aware partial overwrites to state components aligned with current input, inducing "last-write-wins" behavior where new evidence replaces older, related information. When combined with gating (α_t decay), this enables both selective retention and controlled forgetting within a fixed-size recurrent state.
- **Core assumption:** The explicit error-correction term (I − β_t k_t k_t^⊤) provides a learned mechanism for state cleanup that additive accumulation cannot replicate.
- **Evidence anchors:**
  - [abstract] "error-correcting update rules, such as those in Gated DeltaNet, exhibit superior scaling and long-context retrieval compared to purely additive models"
  - [Table 3] Gated DeltaNet achieves 81.8% accuracy at 2k tokens on S-NIAH-1 versus xLSTM's 25.2% and Linear Attention's 0%
  - [corpus] Related work LIGER combines gating with sliding-window attention, suggesting gating as a convergent design principle
- **Break condition:** If the key-query alignment mechanism fails to match related content, the overwrite may remove wrong state entries, degrading rather than improving retrieval.

### Mechanism 2: Input-Dependent Gating Prevents Irreversible State Saturation
- **Claim:** Input-dependent gating provides fine-grained memory control that input-independent decay or ungated accumulation cannot achieve.
- **Mechanism:** Gating modulates both the retention of prior state (via forget gates or decay) and the incorporation of new information (via input gates). Channel-wise and head-wise gating granularities allow different state components to be forgotten at different rates, preventing uniform decay that loses important distant information.
- **Core assumption:** The model can learn when to retain versus forget based on input content, rather than relying on fixed temporal decay.
- **Evidence anchors:**
  - [Section 2.1] "Gated Linear Attention refines this idea by applying channel-wise decay, allowing different components of the state to be forgotten at different rates"
  - [Table 2] At 1.7B parameters, gated models (Gated DeltaNet 94.2%, xLSTM 91.6%) substantially outperform ungated Linear Attention (70.9%)
  - [Section 4.2.2] "additive models suffer from irreversible state saturation"
  - [corpus] No direct corpus comparison of gating granularities; this remains an open question
- **Break condition:** If gate inputs fail to capture task-relevant features, gating becomes noise and may underperform simple decay.

### Mechanism 3: Architectural Inductive Biases Constrain Distillation Efficiency
- **Claim:** Performance hierarchies are determined early in distillation and persist regardless of additional training tokens.
- **Mechanism:** The token mixer's inherent state resolution capacity—its ability to compress history into a fixed-size state without information loss—establishes a fundamental ceiling. Distillation can transfer knowledge up to this ceiling but cannot overcome it.
- **Core assumption:** The 10B token experiments reach or approach asymptotic performance for each architecture.
- **Evidence anchors:**
  - [Section 4.2.1] "performance gaps are established early and persist through asymptotic maturity at 10B tokens"
  - [Section 4.2.1] Gated DeltaNet and xLSTM "performance largely plateauing after 6–7B tokens" while weaker models "saturate earlier and at a significantly lower level"
  - [abstract] "architectural inductive biases remain the primary constraint that cannot be overcome by simply scaling training compute"
  - [corpus] Neighbor papers focus on hybrid attention rather than direct architecture comparison; limited external validation
- **Break condition:** If hybrid architectures combine multiple mixers, the standalone architectural limits may not apply.

## Foundational Learning

- **Concept: Linear Attention and State Space Models**
  - **Why needed here:** The paper compares architectures that all replace O(L²) attention with recursive state updates. Understanding how attention can be reformulated as key-value outer products stored in a state matrix is prerequisite.
  - **Quick check question:** Can you explain why linear attention has O(L) complexity but loses token-level selectivity compared to softmax attention?

- **Concept: Knowledge Distillation Objectives**
  - **Why needed here:** The three-stage pipeline uses attention-output alignment, hidden-state alignment, and end-to-end KL divergence. Understanding why staged optimization improves stability over end-to-end training is essential.
  - **Quick check question:** Why might aligning attention outputs provide a denser supervision signal than aligning attention maps?

- **Concept: Recurrent State Saturation**
  - **Why needed here:** The central finding is that additive models suffer from state saturation while gated delta-rule models maintain precision. Understanding fixed-size state bottlenecks is critical.
  - **Quick check question:** As sequence length increases, what happens to a fixed-size state that only accumulates information without explicit forgetting?

## Architecture Onboarding

- **Component map:** Teacher Transformer → Stage 1: Attention-output alignment → Student Token Mixer → Stage 2: Hidden-state alignment → Student Layers → Stage 3: End-to-end KD → Final Student

- **Critical path:**
  1. Initialize student by copying teacher feedforward weights
  2. Stage 1 (80M tokens): Align attention outputs via MSE
  3. Stage 2 (160M tokens): Align hidden states via L2 loss
  4. Stage 3 (remainder): End-to-end KD with CE + KL objectives
  5. For instruction following: Apply post-training SFT (1B tokens) on instruction data

- **Design tradeoffs:**
  | Architecture | Strength | Weakness |
  |--------------|----------|----------|
  | Linear Attention | Simplest implementation | State saturation, no forgetting mechanism |
  | GLA/RetNet | Channel-wise decay control | Fixed decay rate limits adaptivity |
  | xLSTM | Input-dependent gating | Still additive at core; saturation at 2k+ tokens |
  | Gated DeltaNet | Error-correcting + gating | Most complex; highest long-context performance |

- **Failure signatures:**
  - **Near-zero retrieval at 1k+ tokens:** Indicates state saturation (typical of Linear Attention, RetNet)
  - **Sharp accuracy drop between 1k and 2k tokens:** Indicates limited state resolution despite gating (typical of xLSTM, DeltaNet)
  - **Instruction tuning fails to improve reasoning:** Suggests insufficient state capacity for multi-step inference

- **First 3 experiments:**
  1. **Baseline distillation at 360M scale:** Train all 7 architectures on 3B tokens with 3-stage pipeline. Evaluate on zero-shot benchmarks to establish performance hierarchy.
  2. **State resolution probe:** Run NIAH tasks at 512, 1k, 2k, 4k token contexts on best and worst performers. Quantify the saturation point for each architecture.
  3. **Instruction tuning comparison:** Apply identical SFT to top-2 and bottom-2 architectures. Measure whether performance gaps amplify (confirming architectural constraint) or narrow (suggesting compensation is possible).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the architectural performance hierarchies observed in standalone mixers predict the efficacy of hybrid architectures that combine multiple token mixers?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they "do not investigate hybrid architectures" and cite prior work suggesting hybrid performance cannot be predicted from standalone components.
- **Why unresolved:** This study isolated single-mixer architectural effects to establish a baseline; exploring the combinatorial design space of hybrid models was beyond its scope.
- **What evidence would resolve it:** A comparison study applying the unified distillation pipeline to hybrid models (e.g., layers mixing attention and linear recurrences) to see if Gated DeltaNet components still dominate.

### Open Question 2
- **Question:** Do the advantages of error-correcting update rules persist when linearizing teachers significantly larger than 1.7B parameters?
- **Basis in paper:** [inferred] The authors identify a "crossover" at 1.7B where Gated DeltaNet surpasses xLSTM, suggesting shifting scaling exponents, but stop short of testing larger models. They also explicitly note the restriction to the SmolLM2 family as a limitation.
- **Why unresolved:** It is unclear if the "superior scaling exponents" of error-correcting models continue linearly or saturate compared to gated alternatives as model capacity increases further.
- **What evidence would resolve it:** Extending the distillation experiments to teacher models in the 7B to 70B parameter range and evaluating the recovery rates of the different student architectures.

### Open Question 3
- **Question:** What specific internal gating dynamics allow error-correcting models to mitigate state saturation, and can these mechanisms be visualized during long-context tasks?
- **Basis in paper:** [explicit] The Limitations section notes the lack of "detailed mechanistic or qualitative analysis of internal dynamics, such as gating behavior," despite identifying state saturation as a key failure mode for additive models.
- **Why unresolved:** The study relies on behavioral metrics (benchmark accuracy) rather than intrinsics analysis, leaving the precise algorithmic reason for Gated DeltaNet's robustness unexplored.
- **What evidence would resolve it:** Projections or visualizations of the recurrent state matrices during Needle-In-A-Haystack tasks to observe how different update rules manage information over thousands of timesteps.

### Open Question 4
- **Question:** Can the "state resolution" bottleneck in subquadratic models be overcome through architectural innovation alone, or is it a fundamental limit of recurrent compression?
- **Basis in paper:** [inferred] The authors conclude that "state resolution is a more fundamental bottleneck than the distillation budget" and that architectural differences persist even at asymptotic maturity (10B tokens).
- **Why unresolved:** While the paper proves some architectures are better than others, it leaves open whether *any* subquadratic mixer can fully match the precision of a Transformer's explicit KV-cache for long-range retrieval.
- **What evidence would resolve it:** The development of new state-update mechanisms that achieve near-100% accuracy on NIAH tasks at 4k+ context lengths without increasing state size to quadratic complexity.

## Limitations

- The uniform training pipeline may not represent optimal configurations for each architecture, potentially underutilizing architectures with different learning dynamics
- The focus on transformer-to-subquadratic distillation may not generalize to other teacher architectures or alternative training objectives
- The NIAH task, while novel, has not been validated against established retrieval datasets

## Confidence

**High Confidence Claims:**
- The stable performance hierarchy across scales (140M to 1.7B) is well-supported by systematic comparisons across multiple benchmarks and training regimes
- The superiority of error-correcting update rules (Gated DeltaNet) over purely additive approaches is demonstrated through consistent improvements in both zero-shot performance and long-context retrieval
- Architectural differences persist and amplify after instruction tuning, suggesting fundamental constraints rather than optimization issues

**Medium Confidence Claims:**
- The assertion that 10B tokens represent asymptotic training for all architectures is reasonable but not definitively proven, as some models show continued improvement beyond 6B tokens
- The interpretation that state saturation specifically causes additive models' poor long-context performance is supported but could also reflect other architectural limitations
- The claim that instruction tuning amplifies rather than compensates for architectural differences is well-supported but based on a limited set of instruction tasks

**Low Confidence Claims:**
- The generalizability of findings to larger scales (beyond 1.7B parameters) remains untested
- The specific contribution of each architectural component (gating granularity, decay rates, delta-rule terms) is not systematically isolated through ablation studies
- The comparison with alternative linearization approaches (hybrid attention, block-wise methods) is limited to two neighbor papers without direct experimental comparison

## Next Checks

1. **Scaling Validation to 7B+ Parameters**: Train the top-performing architectures (Gated DeltaNet, xLSTM) at 7B+ parameters on the full 10B token curriculum. Measure whether performance continues to scale linearly or if architectural advantages plateau. This would test the fundamental claim about architectural constraints persisting at larger scales.

2. **Ablation of Gating Mechanisms**: Implement controlled ablations of gating components in Gated DeltaNet and xLSTM (remove channel-wise decay, input-dependent gates, or delta-rule terms individually). Compare performance on NIAH retrieval and zero-shot benchmarks to isolate which mechanisms contribute most to long-context success versus general performance.

3. **Cross-Teacher Generalization**: Repeat the distillation pipeline using alternative teacher architectures (e.g., LLaMA, Mistral) rather than SmolLM2. Evaluate whether the same architectural hierarchy emerges and whether gated delta-rule models maintain their advantage across different teacher representations and initialization strategies.