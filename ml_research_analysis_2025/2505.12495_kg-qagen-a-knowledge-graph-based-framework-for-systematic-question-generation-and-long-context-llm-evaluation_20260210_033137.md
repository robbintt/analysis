---
ver: rpa2
title: 'KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation
  and Long-Context LLM Evaluation'
arxiv_id: '2505.12495'
source_url: https://arxiv.org/abs/2505.12495
tags:
- person
- what
- name
- role
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KG-MULQA, a knowledge-graph-based framework
  for generating systematic question-answer pairs to evaluate long-context large language
  models. It leverages structured RDF representations of annotated financial credit
  agreements to extract QA pairs across three complexity dimensions: multi-hop reasoning,
  set operations, and answer plurality.'
---

# KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation

## Quick Facts
- **arXiv ID:** 2505.12495
- **Source URL:** https://arxiv.org/abs/2505.12495
- **Reference count:** 40
- **Primary result:** Constructs KG-MULQA, a dataset of 20,139 QA pairs from financial documents to systematically evaluate long-context LLM reasoning across complexity levels.

## Executive Summary
This paper introduces KG-MULQA, a knowledge-graph-based framework for generating systematic question-answer pairs to evaluate long-context large language models. It leverages structured RDF representations of annotated financial credit agreements to extract QA pairs across three complexity dimensions: multi-hop reasoning, set operations, and answer plurality. Using this approach, the authors construct a dataset of 20,139 QA pairs, the largest among existing long-context benchmarks. Evaluation of 16 LLMs shows performance degrades with increasing complexity, especially on set-based reasoning and implicit relation extraction. Human evaluation confirms that even top models struggle with tasks requiring deeper document understanding, highlighting KG-MULQA's effectiveness in revealing model limitations in long-context comprehension.

## Method Summary
The method constructs a knowledge graph from 170 SEC EDGAR credit agreements (7 entity types, 6 relations) annotated by human experts. SPARQL-based templates systematically generate QA pairs across 5 complexity levels, controlling for retrieval depth (hops), set operations, and answer cardinality. The resulting 20,139 QA pairs are evaluated using F1 scores and LLM-as-a-Judge (Gemini-2.0-Pro) for semantic correctness. Documents are chunked to ≤128K tokens for model compatibility, with two-stage prompting for multi-chunk answers. The framework evaluates 16 LLMs across proprietary and open-weight models, revealing systematic degradation in performance with increasing complexity.

## Key Results
- KG-MULQA contains 20,139 QA pairs, the largest long-context benchmark dataset.
- Performance drops significantly with complexity: 15% → 77% "Not Found" responses from Easy to Hard questions.
- Top models achieve only 30-50% F1 on Hard questions, with set operation failures as a primary error mode.
- Full-document contexts outperform oracle snippet-only settings for hard questions, suggesting models leverage implicit contextual cues.

## Why This Works (Mechanism)

### Mechanism 1: Programmatic Complexity Scaling via Graph Templates
- **Claim:** Structuring unstructured documents into Knowledge Graphs (KG) allows for the deterministic generation of question-answer pairs with precisely controlled difficulty, mitigating the variance found in synthetic LLM-generated benchmarks.
- **Mechanism:** The system parses credit agreements into an RDF graph (entities and relations). It then applies SPARQL-based templates that systematically inject complexity along three axes: Retrieval Depth (Hops), Set Logic (Set Operations), and Answer Cardinality (Plurality). Complexity is quantified as $L = H + P + \#SO$.
- **Core assumption:** The annotation schema (7 labels, 6 relations) captures sufficient semantic richness to generate non-trivial reasoning paths without creating logical fallacies in the graph structure.
- **Evidence anchors:**
  - [Section 2.4]: "We then systematically expand these templates along three dimensions... increasing retrieval depth via intermediate nodes... introducing comparisons across entities using set logic."
  - [Section 2.5]: "Question instances are constructed by dynamically substituting entities into these templates using SPARQL queries... ensuring precise and scalable QA pair creation."
  - [Corpus]: Related work supports the utility of structured representations for complex tasks, noting KGs help where "standard RAG approaches... struggle with high-level conceptual understanding" (MegaRAG).
- **Break condition:** If the source documents lack sufficient relational density, the graph becomes disconnected, and templates yield sparse or "Not Applicable" results, failing to generate higher-level questions.

### Mechanism 2: Stress-Testing Retrieval vs. Reasoning Separation
- **Claim:** The framework isolates "Lost in the Middle" retrieval failures from logical reasoning deficits by evaluating performance across distinct complexity levels (Easy vs. Hard).
- **Mechanism:** By holding the document content constant and only varying the query's graph distance (hops) and logic (set operations), the framework distinguishes between a model's inability to locate information (retrieval) and its inability to combine information (reasoning).
- **Core assumption:** The chunking strategy (128k token limits) does not sever the relational links required for multi-hop reasoning.
- **Evidence anchors:**
  - [Section 4.1]: "Not Found Responses... Increases from 15.00% at Level 1 to 77.39%... reflecting increasing retrieval difficulty."
  - [Section 4.2]: "Despite the presence of answers in the document, models often return 'Not Found'... aligning with the 'lost-in-the-middle' phenomenon."
  - [Corpus]: External evaluation of long-context models corroborates these limits, noting that efficacy in long-context scenarios often remains "underexplored" despite optimization (Systematic Evaluation of Optimization Techniques).
- **Break condition:** If a model uses a naive retrieval strategy (e.g., keyword matching) that misses implicit relations, performance collapses regardless of reasoning capability.

### Mechanism 3: Semantic Alignment via LLM-as-a-Judge
- **Claim:** Relying solely on F1 scores penalizes valid semantic variations in set-based answers; an LLM-based evaluator provides a necessary alignment with human judgment for complex answers.
- **Mechanism:** The system uses a two-stage evaluation: F1 for token overlap and a "Judge" LLM (Gemini-2.0-Pro) for semantic scoring (1-5 scale). This captures scenarios where a model answers correctly but uses different phrasing or ordering.
- **Core assumption:** The Judge LLM is sufficiently capable to parse logical "set operations" in the ground truth without hallucinating correctness.
- **Evidence anchors:**
  - [Section D.3]: "LLM-as-a-Judge and Avg LLM-Human Rating (r = 0.947) show the highest correlations."
  - [Section 3.3]: "Qwen2's high judge score but low F1 reflects... verbose, loosely correct text that seems semantically reasonable."
- **Break condition:** If the Judge LLM exhibits bias toward specific answer styles (e.g., favoring verbose outputs over concise ones), the correlation with human judgment decouples.

## Foundational Learning

- **Concept: SPARQL and RDF Triples**
  - **Why needed here:** The core extraction engine is not a neural network but a symbolic query engine (SPARQL) running over RDF triples. Understanding subject-predicate-object structures is required to debug why certain questions are generated and others are not.
  - **Quick check question:** Can you write a query to select all `Person` entities connected to an `Organization` via an `Employee` relation?

- **Concept: Set Theory in NLP (Intersection/Difference)**
  - **Why needed here:** The paper identifies "Set Operation Failures" as a primary error category. Understanding $A \cap B$ (intersection) and $A \setminus B$ (difference) is crucial to interpreting the "Hard" level questions where models fail.
  - **Quick check question:** If the question is "Who is VP but *not* Treasurer?", should the model return people who are both, or only those who hold the first title exclusively?

- **Concept: The "Lost in the Middle" Phenomenon**
  - **Why needed here:** This cognitive bias in LLMs explains why retrieval accuracy drops for information located in the middle of long contexts, a key finding in the paper's error analysis.
  - **Quick check question:** If a relevant entity is located at the 50% mark of a 100k token context, is a standard LLM more or less likely to retrieve it compared to an entity at the 5% mark?

## Architecture Onboarding

- **Component map:** Data Layer (SEC EDGAR Credit Agreements) -> Annotation Layer (Label Studio) -> Graph Layer (RDF/Turtle) -> Extraction Engine (SPARQL Templates) -> Evaluation Pipeline (vLLM/LangChain)
- **Critical path:** The **Annotation Schema (Appendix A)**. If annotators miss "implicit" relations (e.g., inferring a person's org from a signature block), the graph fragments, and the pipeline generates "Not Found" answers for valid questions.
- **Design tradeoffs:**
  - **Chunking vs. Full Context:** The pipeline chunks documents to <128k tokens for compatibility. This trades off context integrity for model compatibility, potentially breaking cross-chunk reasoning (Section D.5).
  - **Automation vs. Noise:** The system automates QA generation to ensure scale (20k pairs), but risks introducing "noisy text" (URL characters, duplicates) which requires post-processing (Section B.1).
- **Failure signatures:**
  - **High "Not Found" Rate:** Indicates retrieval failure or chunking issues, not necessarily reasoning failure.
  - **Low F1 / High Judge Score:** Indicates verbose/paraphrased answers (Model is "right" conceptually but fails exact match).
  - **Set Operation Errors:** Model returning intersection ($A \cap B$) when difference ($A \setminus B$) was asked.
- **First 3 experiments:**
  1. **Schema Validation:** Run the annotation-to-SPARQL pipeline on a single document to verify that "Implicit Relations" (Section 4.2) are captured in the graph.
  2. **Baseline Retrieval:** Evaluate a standard model (e.g., GPT-4o) on "Easy" questions only to establish a retrieval baseline before testing reasoning.
  3. **Metric Correlation:** Compare F1 scores vs. LLM-Judge scores on a sample of "Hard" questions to quantify the "verbosity penalty" discussed in Section 3.3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be systematically improved for set-based reasoning operations (intersections, exclusions, comparisons across entities)?
- **Basis in paper:** [explicit] The paper identifies "Set Operation Failures" as a major error category, noting "Questions involving comparisons, intersections, or exclusions across entities consistently yield low F1 scores. The model fails to isolate shared or distinct roles when multiple organizations or individuals are involved."
- **Why unresolved:** While the paper documents the failure mode, it does not propose or evaluate specific interventions (architectural, prompting, or fine-tuning) that could address this systematic weakness.
- **What evidence would resolve it:** A follow-up study comparing targeted approaches (e.g., explicit set-operation prompting strategies, chain-of-thought decomposition of set logic, or fine-tuning on synthetic set-operation tasks) against baseline performance on the Hard subset of KG-MULQA-D.

### Open Question 2
- **Question:** Why do full-document contexts outperform oracle (gold-snippet-only) settings for hard multi-hop questions, and which implicit contextual cues are models leveraging?
- **Basis in paper:** [explicit] The paper reports that "for hard questions, the Full Document consistently outperforms the Oracle setting" and hypothesizes that "isolating the 'correct' snippet removes necessary contextual cues that models need for reasoning." The specific cues remain unidentified.
- **Why unresolved:** The authors attribute this to implicit information gaps but do not conduct controlled experiments to isolate which contextual features (document structure, formatting cues, surrounding text) contribute to improved performance.
- **What evidence would resolve it:** Ablation studies systematically removing or masking different types of contextual information (e.g., signature blocks, section headings, document formatting) from full documents to measure impact on hard-question accuracy.

### Open Question 3
- **Question:** How can retrieval-augmented generation methods be adapted for densely cross-referenced documents where relevant information is distributed across distant spans?
- **Basis in paper:** [inferred] The paper finds that both standard RAG and dynamic RAG "perform drastically worse" than full-document settings across all difficulty levels, concluding this "highlights the incompatibility of standard retrieval techniques with dense, cross-referenced financial documents." No alternative retrieval strategies are proposed or tested.
- **Why unresolved:** The paper identifies the problem but does not explore whether hierarchical retrieval, multi-hop retrieval chains, or graph-enhanced retrievers could bridge this gap.
- **What evidence would resolve it:** Evaluation of retrieval methods explicitly designed for multi-hop reasoning (e.g., iterative retrieval, graph-based passage linking, or query decomposition) on the KG-MULQA-D benchmark, comparing against the reported poor RAG baseline.

## Limitations
- The framework's generalizability beyond financial documents remains untested; performance on domains with different relational structures (e.g., biomedical) is unknown.
- The 128k token chunking strategy may artificially fragment relational paths, underestimating true reasoning capabilities.
- Human evaluation sample size (21 questions) is small relative to the full dataset complexity.

## Confidence
- **High:** The framework successfully generates systematically complex QA pairs (20k+ examples) with controlled difficulty scaling; retrieval failure patterns align with "lost-in-the-middle" literature.
- **Medium:** Semantic evaluation via LLM-as-a-Judge correlates strongly with human judgment (r=0.947), but potential bias toward verbose outputs exists.
- **Low:** Generalization to non-financial domains and resilience to annotation schema variations require further validation.

## Next Checks
1. **Cross-Domain Transfer:** Apply KG-MULQA templates to a non-financial corpus (e.g., legal contracts) and measure annotation-to-QA generation fidelity.
2. **Chunking Impact:** Compare model performance on full-document vs. chunked contexts for multi-hop questions to quantify fragmentation effects.
3. **Schema Robustness:** Introduce synthetic noise into the KG (e.g., missing implicit relations) and test if QA generation gracefully degrades or produces logical inconsistencies.