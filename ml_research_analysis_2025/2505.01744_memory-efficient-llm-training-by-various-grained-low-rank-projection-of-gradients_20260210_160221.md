---
ver: rpa2
title: Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients
arxiv_id: '2505.01744'
source_url: https://arxiv.org/abs/2505.01744
tags:
- gradient
- projection
- memory
- training
- vlorp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of fine-tuning large
  language models by introducing a new framework called VLoRP that allows for adjustable
  projection granularity in low-rank gradient projection methods. The key insight
  is that by reshaping the gradient matrix before projection, one can control the
  projection granularity alongside the rank parameter, leading to better performance
  under fixed memory budgets.
---

# Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients

## Quick Facts
- arXiv ID: 2505.01744
- Source URL: https://arxiv.org/abs/2505.01744
- Reference count: 40
- One-line primary result: VLoRP with ProjFactor achieves 70% memory reduction vs Adam while maintaining or improving task accuracy on commonsense reasoning, MMLU, and GSM8K

## Executive Summary
This paper introduces VLoRP (Various-Grained Low-Rank Projection of Gradients), a memory-efficient training framework for large language models that provides controllable trade-offs between memory usage and gradient estimation quality. By reshaping the gradient matrix before low-rank projection, VLoRP allows adjustment of projection granularity alongside rank, leading to better performance under fixed memory budgets. The authors also propose ProjFactor, a memory-efficient optimizer that maintains Adam-like adaptation while reducing memory consumption. Empirically, VLoRP with ProjFactor achieves superior or comparable results to state-of-the-art methods while reducing memory consumption by up to 70% compared to Adam.

## Method Summary
VLoRP extends low-rank gradient projection by introducing a granularity factor c that reshapes the gradient matrix G ∈ R^(n×m) into G̃ ∈ R^(nc×m/c) before projection. The method projects G̃ using a random matrix P, stores only the low-rank representation G̃_s, and projects back when needed. ProjFactor is a memory-efficient optimizer that stores first moments in the low-dimensional subspace while factorizing second moments using rank-1 decomposition. This maintains Adam-like adaptation with reduced memory usage. The framework is theoretically grounded with O(1/T) convergence guarantees for VLoRP with SGD and convergence analysis for ProjFactor through Hamiltonian descent.

## Key Results
- VLoRP with ProjFactor achieves 70% memory reduction compared to Adam on LLaMA2-7B fine-tuning
- On Commonsense Reasoning, VLoRP achieves 66.65% average accuracy vs 65.66% for Adam
- VLoRP shows superior or comparable performance to state-of-the-art methods including LoRA, GaLore, and LMS on MMLU and GSM8K tasks
- Finer granularity (larger c, smaller r) consistently outperforms coarser configurations under fixed memory budgets

## Why This Works (Mechanism)

### Mechanism 1: Projection Granularity Controls Estimation Variance
Finer-grained projection improves gradient estimation quality under fixed memory budgets by reducing the dimensionality of each row before projection. Since projection operates row-wise, smaller target dimensions mean fewer parameters to estimate per sample, reducing variance even when using fewer samples.

### Mechanism 2: Low-Rank Projection as Row-Wise Forward Gradient Estimation
LoRP methods can be interpreted as unbiased stochastic gradient estimators with bounded variance. The projected-back gradient is an average of rank-1 forward gradient estimates per row, providing unbiasedness with variance controlled by both rank and row dimension.

### Mechanism 3: ProjFactor Separates First-Moment and Second-Moment Storage Strategies
ProjFactor stores first moments in subspace while factorizing second moments, maintaining Adam-like adaptation with significantly reduced memory. This preserves superior dynamics while matching SS memory efficiency.

## Foundational Learning

- **Low-Rank Gradient Projection (LoRP)**
  - Why needed here: VLoRP extends LoRP by adding granularity control; understanding baseline helps clarify improvements
  - Quick check question: Given gradient G ∈ R^(4096×4096) and projection rank r=8, what is the memory ratio between storing G versus storing G_s = GP?

- **Forward Gradient Estimation**
  - Why needed here: Paper reinterprets LoRP through this lens; determines variance and bias analysis
  - Quick check question: Why does the forward gradient estimator (g^T v)v provide an unbiased estimate of g, and how does averaging over multiple v reduce variance?

- **Adam's First and Second Moments**
  - Why needed here: ProjFactor modifies how these moments are stored; understanding standard Adam clarifies trade-offs
  - Quick check question: In Adam, what do m_t and v_t represent, and why does v_t require more memory than m_t for same parameter matrix?

## Architecture Onboarding

- **Component map**:
  - Gradient Computation -> Reshape(G, [nc, m/c]) -> Project via P -> Store G̃_s -> Accumulate -> Project back G̃^o = G̃_s P^T -> ProjFactor Update -> Δ computation -> Reshape back to [n,m] -> Parameter Update

- **Critical path**:
  1. Forward-backward pass computes G for parameter W
  2. Reshape G → G̃ based on granularity factor c
  3. Project: G̃_s = G̃ P (store this only)
  4. Accumulate across mini-batches
  5. Project back: G̃^o = G̃_s P^T
  6. Update ProjFactor states (m̃_s, ṽ_r, ṽ_c)
  7. Compute Δ and reshape back to update W

- **Design tradeoffs**:
  - **Granularity vs Rank**: Under fixed M, larger c (finer granularity) with smaller r generally outperforms larger r with coarser granularity
  - **SS vs OS Schemes**: OS performs better but stores full-rank optimizer states; ProjFactor approximates OS with reduced memory
  - **Projection Update Frequency**: Updating P every step introduces high variance; too infrequent constrains updates to fixed subspace (τ=20-30 works well)

- **Failure signatures**:
  - Loss spikes early in training → Check warmup steps
  - Performance plateaus below baseline → Granularity may be too coarse; try increasing c
  - Throughput drops periodically → SVD-based methods have periodic overhead; VLoRP uses random projections without this issue

- **First 3 experiments**:
  1. Granularity sweep under fixed M: Train with M=256, test (c=256, r=1), (c=16, r=16), (c=1, r=256) on Commonsense170k
  2. ProjFactor vs OS vs SS comparison: Plot loss curves for all three optimizers
  3. Memory profiling: Measure GPU memory for Adam, LoRA, GaLore, VLoRP+ProjFactor on LLaMA2-7B with batch size 16, seq length 1024

## Open Questions the Paper Calls Out

### Open Question 1
Does the VLoRP framework maintain its memory efficiency and performance advantages when applied to large-scale pre-training tasks, as opposed to just fine-tuning? The paper's empirical validation is restricted to fine-tuning tasks where low-rank gradient assumptions are well-established, but gradients during pre-training are typically denser and less structured.

### Open Question 2
How does the performance of VLoRP scale when applied to significantly larger parameter models (e.g., 70B+ parameters) in terms of stability and convergence speed? The experimental scope is limited to models up to 7B parameters, leaving behavior on state-of-the-art massive models unverified.

### Open Question 3
Can the granularity factor c be adapted dynamically per layer or over time to further optimize the trade-off between memory usage and gradient approximation accuracy? The paper defines c as a "global hyperparameter," implying a fixed configuration across all layers and time steps.

## Limitations

- The O(1/T) convergence proof assumes gradients remain approximately low-rank throughout training, which may not hold for all tasks
- Memory efficiency claims assume uniform parameter distribution across layers, but LLMs often have heterogeneous layer sizes
- Theoretical assumptions about Gaussian projections maintaining Johnson-Lindenstrauss properties may not hold under finite-precision arithmetic

## Confidence

- **High Confidence**: Memory reduction claims (70% vs Adam) and ProjFactor optimizer's performance relative to baselines are directly measured and reproducible
- **Medium Confidence**: Theoretical convergence guarantees depend on assumptions about gradient low-rankness and bounded variance that may not hold universally
- **Low Confidence**: Optimal granularity selection (c values) across different tasks, as this appears to be problem-dependent and not systematically explored

## Next Checks

1. **Gradient Low-Rankness Analysis**: Track the actual rank distribution of gradients during training across different tasks (Commonsense, MMLU, GSM8K) to validate whether low-rank projection remains effective throughout fine-tuning.

2. **Cross-Model Generalization Test**: Apply VLoRP+ProjFactor to models beyond LLaMA2-7B (e.g., OPT, BLOOM) with varying parameter counts and architectures to assess whether reported memory-accuracy trade-offs generalize.

3. **Mixed-Precision Stability Evaluation**: Conduct controlled experiments comparing VLoRP's numerical stability in bfloat16 versus float32 training, particularly focusing on the impact of granularity factor c on floating-point error accumulation.