---
ver: rpa2
title: Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs
arxiv_id: '2510.17364'
source_url: https://arxiv.org/abs/2510.17364
tags:
- video
- tokens
- visual
- attention
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free approach for efficient streaming
  video understanding using large language models (LLMs). The method addresses the
  computational challenges of processing hour-long videos in real-time by selecting
  only the most relevant visual tokens based on LLM attention scores, reducing visual
  information by approximately 95%.
---

# Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs

## Quick Facts
- **arXiv ID:** 2510.17364
- **Source URL:** https://arxiv.org/abs/2510.17364
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art streaming video QA with 2-3% accuracy improvement while reducing memory by 11GB and latency by nearly 1 second

## Executive Summary
This paper introduces a training-free approach for efficient streaming video understanding using large language models (LLMs). The method addresses computational challenges of processing hour-long videos in real-time by selecting only the most relevant visual tokens based on LLM attention scores, reducing visual information by approximately 95%. A recurrent mechanism processes short video clips while maintaining temporal coherence, and caption-based retrieval enables efficient question answering. The approach achieves state-of-the-art performance on streaming video benchmarks while requiring no additional training.

## Method Summary
The method processes long videos by splitting them into short clips and using attention-based token selection to reduce visual data by 94-95%. For each 16-frame clip, the system generates captions and computes attention scores between caption tokens and visual tokens, selecting only the top 196 tokens (6% of 3,136) based on aggregated attention weights. A FIFO queue maintains selected tokens from previous clips to preserve temporal context. For question answering, the system retrieves relevant captions using Maximal Marginal Relevance (MMR) instead of raw visual tokens, leveraging LLMs' strength in text-based reasoning. The complete pipeline is training-free and model-agnostic, requiring only a standard video-LLM backbone.

## Key Results
- Outperforms previous methods by 2-3% on RVS-Ego and RVS-Movie streaming benchmarks
- Reduces memory usage by 11GB and latency by nearly 1 second compared to previous best method
- Achieves state-of-the-art performance while maintaining model-agnostic approach with no additional training required

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention-based pruning discards ~95% of visual tokens with minimal performance loss
- **Mechanism:** Computes attention matrices between generated captions and visual tokens, aggregates scores across layers/heads, retains top 196 tokens
- **Core assumption:** High attention weights from generated text correlate with semantic relevance of visual tokens
- **Evidence:** Table 2 shows 6% retention outperforms uniform sampling and matches full-model performance
- **Break condition:** Poor caption generation leads to selection of irrelevant tokens

### Mechanism 2
- **Claim:** FIFO queue of selected tokens creates recurrent visual memory for temporal coherence
- **Mechanism:** Selected tokens are prepended to next clip's visual tokens, functioning as compressed visual memory
- **Core assumption:** Compressed visual tokens contain sufficient detail to bridge context gaps between clips
- **Evidence:** Table 4 shows 3-4% performance drop when recurrency is removed
- **Break condition:** Small context window forces eviction of critical memories

### Mechanism 3
- **Claim:** Text-based retrieval using captions outperforms visual token retrieval
- **Mechanism:** Retrieves top-K captions via MMR instead of visual features for question answering
- **Core assumption:** LLMs reason better over text than long visual contexts
- **Evidence:** Table 5 shows captions significantly outperform visual tokens (57.7 vs 48.4 accuracy)
- **Break condition:** Caption generation misses critical details or hallucinates

## Foundational Learning

- **Visual Tokenization & Context Windows**
  - **Why needed here:** Fixed context windows require understanding how frames convert to patch tokens
  - **Quick check question:** If processing 32 frames with 196 tokens each, how many tokens are generated and how does this fit in 4k context?

- **Attention Score Aggregation**
  - **Why needed here:** Core selection logic averages attention weights across layers and heads
  - **Quick check question:** Why average attention scores across layers rather than taking max or using only final layer?

- **Maximal Marginal Relevance (MMR)**
  - **Why needed here:** Used for retrieving captions while handling redundancy
  - **Quick check question:** How does MMR differ from simple cosine similarity ranking?

## Architecture Onboarding

- **Component map:** Frame Input → Encoding → [Concat with Past Memory Tokens] → LLM Forward Pass → Caption Generation → Attention Calculation (Select top 6%) → Update Memory Buffer
- **Critical path:** Frame Input → Encoding → [Concat with Past Memory Tokens] → LLM Forward Pass → Caption Generation → **Attention Calculation** (Select top 6%) → Update Memory Buffer
- **Design tradeoffs:**
  - Fixed 6% retention simplifies memory management but may over-compress complex scenes
  - Caption-based memory is semantically efficient but discards visual nuances
- **Failure signatures:**
  - Spatial drift: spatial information is "manipulated or lost" during selection
  - Redundant retrieval: without MMR, recurrent captioning may produce repetitive text
- **First 3 experiments:**
  1. Validate selection logic by replicating "NextQA-valset" experiment comparing uniform sampling vs. attention-based selection at 6% retention
  2. Run streaming pipeline on RVS-Ego with FIFO buffer disabled to quantify 3-4% performance drop
  3. Compare retrieving "Visual Tokens" vs. "Captions" for Q&A to confirm text-based approach superiority

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does explicit reintroduction of spatial positional information for selected tokens improve fine-grained spatio-temporal reasoning?
- **Basis:** Appendix E notes that spatial information is lost and developing improved strategies to retain spatial information is an open direction
- **Why unresolved:** Authors chose to discard spatial layout to maintain efficiency and model-agnosticism
- **What evidence would resolve it:** Comparison of object tracking or localization accuracy between current method and variant using positional encodings

### Open Question 2
- **Question:** Can semantic-based memory selection strategies outperform the current temporal FIFO queue?
- **Basis:** Limitations section suggests exploring semantic rather than purely temporal memory selection strategies
- **Why unresolved:** FIFO may discard semantically critical tokens from older frames due to context limits
- **What evidence would resolve it:** Benchmarking semantic-relevance retrieval mechanism against FIFO baseline on long-horizon QA tasks

### Open Question 3
- **Question:** Can attention-based selection be optimized through integration into training pipeline?
- **Basis:** Limitations section lists integrating method into training pipelines as future work
- **Why unresolved:** Training-free approach relies on attention patterns from short-clip pretraining that may not be optimal for long-term memory
- **What evidence would resolve it:** Performance comparison between training-free method and version fine-tuned to optimize attention-based selection

## Limitations
- Relies heavily on caption quality for attention-based token selection, with poor captioning leading to selection of irrelevant tokens
- Spatial information is manipulated or lost during selection, potentially impacting tasks requiring precise localization
- Claims of model-agnostic approach untested across different LLM architectures beyond LLaVA-OneVision-7B

## Confidence
- **High confidence:** Memory reduction (11GB saved) and latency improvements (nearly 1 second faster) are directly measurable
- **Medium confidence:** State-of-the-art performance claims (2-3% improvement) depend on specific implementation details and evaluation protocols
- **Low confidence:** Claims about method being "training-free" and "model-agnostic" require further validation across different architectures

## Next Checks
1. **Attention selection robustness test:** Run selection mechanism on videos with varying content complexity to verify 6% retention rate is optimal across different video types
2. **Cross-model generalization validation:** Implement same pipeline using different video-LLM backbone to confirm model-agnostic claim and identify architecture-specific limitations
3. **End-to-end streaming performance:** Deploy complete pipeline on hour-long videos with real-time constraints to measure actual latency and memory usage in streaming conditions