---
ver: rpa2
title: Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache
  Diagnosis in Primary Care
arxiv_id: '2512.04207'
source_url: https://arxiv.org/abs/2512.04207
tags:
- multi-agent
- headache
- clinical
- system
- secondary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-agent clinical decision support system
  for detecting secondary headache red flags in primary care. The system uses seven
  domain-specific agents coordinated by an orchestrator to evaluate clinical vignettes
  against red flag criteria.
---

# Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care

## Quick Facts
- arXiv ID: 2512.04207
- Source URL: https://arxiv.org/abs/2512.04207
- Reference count: 0
- Multi-agent system achieves F1 scores up to 0.605 on secondary headache red flag detection

## Executive Summary
This study presents a multi-agent clinical decision support system for detecting secondary headache red flags in primary care. The system uses seven domain-specific agents coordinated by an orchestrator to evaluate clinical vignettes against red flag criteria. When evaluated on 90 expert-annotated cases using five open-source LLMs, the multi-agent system with clinical practice guideline-based prompting achieved F1 scores up to 0.605, outperforming single-LLM baselines. Performance gains were most pronounced in smaller models (8-20B parameters), suggesting architectural specialization compensates for limited model capacity.

## Method Summary
The system employs an orchestrator agent that routes clinical vignettes to seven specialist agents, each evaluating a specific red flag (thunderclap, meningismus, papilledema, temporal arteritis, systemic illness, focal deficits, first/worst headache ≥40 years). Two prompting strategies were tested: QPrompt (minimal question) and GPrompt (clinical practice guideline-based definitions and indicators). The system uses LangGraph for orchestration, includes manual fan-out to ensure all agents execute, and employs multi-strategy JSON parsing for robustness. Five open-source LLMs were evaluated across 90 expert-annotated cases.

## Key Results
- Multi-agent system with GPrompt achieved F1 scores up to 0.605, outperforming single-LLM baselines
- Performance gains most pronounced in smaller models (8-20B parameters), suggesting architectural specialization compensates for limited model capacity
- Single-LLM GPrompt did not outperform single-LLM QPrompt, indicating GPrompt is more effective in multi-agent settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Architectural decomposition compensates for limited model capacity in smaller LLMs.
- Mechanism: By dividing diagnostic reasoning into seven domain-specific agents, each specialist only needs to evaluate a narrow criterion set rather than holding all red-flag knowledge simultaneously.
- Core assumption: Smaller models fail on complex multi-domain tasks due to capacity constraints, not fundamental reasoning inability.
- Evidence anchors:
  - Performance gains most pronounced in smaller models (8-20B parameters)
  - Qwen-8b's F1 increased from 0.557 (Single-LLM QPrompt) to 0.594 (Multi-agent GPrompt)

### Mechanism 2
- Claim: Clinical practice guideline-based prompting synergizes with multi-agent architecture but not with single-LLM baselines.
- Mechanism: GPrompt provides explicit definitions, decision rules, and key indicators per red-flag domain. In multi-agent settings, each agent receives only its relevant GPrompt slice, reducing prompt length and focusing attention.
- Core assumption: Prompt length and relevance directly affect LLM reasoning quality; irrelevant prompt content degrades performance.
- Evidence anchors:
  - Single-LLM GPrompt did not outperform single-LLM QPrompt
  - GPrompt consists of medical criteria derived from established clinical practice guidelines

### Mechanism 3
- Claim: Manual fan-out and robustness mechanisms prevent cascading failures from orchestrator routing errors.
- Mechanism: The orchestrator may fail to invoke all relevant agents. A manual fan-out function inspects completed vs. pending agents and forces execution of any missing specialists.
- Core assumption: Orchestrator errors are omission errors (missing agents) rather than commission errors (wrong agents); fan-out corrects omissions.
- Evidence anchors:
  - Manual fan-out function ensures robustness given variability of LLM
  - Multiple robustness strategies including multi-strategy JSON parsing

## Foundational Learning

- **Multi-Agent Orchestration Patterns**: Understanding orchestrator-specialist architectures is prerequisite to grasping why decomposition helps and where it fails.
  - Quick check: Can you explain the difference between an orchestrator that routes tasks vs. one that negotiates between agents?

- **Clinical Red Flags and Decision Support**: The system's value hinges on mapping clinical guidelines to agent logic; without understanding red-flag semantics, you cannot validate agent outputs.
  - Quick check: Name three secondary headache red flags and explain why they require urgent evaluation.

- **Prompt Engineering for Structured Output**: The system relies on JSON-formatted orchestrator outputs and yes/no specialist responses; prompt design directly determines parseability.
  - Quick check: What prompt constraints ensure an LLM outputs valid JSON without extraneous text?

## Architecture Onboarding

- **Component map**: Orchestrator -> 7 Specialist Agents (Thunderclap, Meningismus, Papilledema, Temporal_Arteritis, Systemic_Illness, Focal_Deficits, First_Worst_Headache) -> Manual Fan-Out -> Aggregation Layer

- **Critical path**: Vignette input → Orchestrator parses and routes → Routed specialists execute in parallel → Fan-out fires for any missing agents → Aggregation produces final multi-label classification

- **Design tradeoffs**:
  - Completeness vs. efficiency: Fan-out guarantees coverage but may invoke irrelevant agents if orchestrator under-routes
  - Interpretability vs. complexity: More agents = more granular rationales, but harder to debug cascading errors
  - GPrompt detail vs. prompt length: Richer GPrompts improve accuracy but may overwhelm single-LLM baselines

- **Failure signatures**:
  - Orchestrator outputs malformed JSON → multi-strategy parsing triggers; if all fail, routing halts
  - Specialist agent outputs non-binary answer → downstream aggregation may misclassify
  - Smaller models produce inconsistent outputs → fan-out compensates, but latency increases

- **First 3 experiments**:
  1. Baseline replication: Run single-LLM QPrompt vs. multi-agent QPrompt on 20 held-out cases to confirm architectural gain requires GPrompt
  2. Ablate fan-out: Disable manual fan-out and measure drop in recall; quantify how often orchestrator omits relevant agents
  3. Prompt length sensitivity: Gradually increase GPrompt verbosity per agent and plot F1 vs. token count for single-LLM vs. multi-agent

## Open Questions the Paper Calls Out
None

## Limitations
- Small expert-annotated dataset (90 cases) with single annotator labels may not capture full clinical variability
- Manual fan-out mechanism introduces potential bias by forcing agent execution regardless of orchestrator confidence
- Computational costs and inference latency not reported, critical for clinical deployment

## Confidence

- **High confidence**: Multi-agent architecture demonstrably improves F1 scores over single-LLM baselines, particularly for smaller models
- **Medium confidence**: Architectural decomposition compensates for limited model capacity is supported but requires more ablation studies
- **Low confidence**: Robustness of manual fan-out mechanism and its impact on clinical decision-making accuracy not thoroughly validated

## Next Checks
1. Ablate fan-out: Disable the manual fan-out function and measure the drop in recall and F1 scores to quantify how often the orchestrator fails to invoke all relevant agents
2. Dataset generalization: Evaluate the system on an external, multi-annotator dataset to assess robustness to different clinical writing styles and red flag presentations
3. Prompt structure ablation: Test whether the architectural gains persist when using identical prompt templates (QPrompt vs. GPrompt) in both single-LLM and multi-agent settings to isolate the effect of architecture vs. prompting