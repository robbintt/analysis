---
ver: rpa2
title: Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs
arxiv_id: '2601.01069'
source_url: https://arxiv.org/abs/2601.01069
tags:
- algorithm
- regret
- lemma
- have
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-stationary parametric bandits
  and Markov Decision Processes (MDPs) using a weighted strategy. The authors propose
  a refined analysis framework that simplifies the derivation and improves the efficiency
  of weight-based algorithms.
---

# Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs

## Quick Facts
- **arXiv ID:** 2601.01069
- **Source URL:** https://arxiv.org/abs/2601.01069
- **Reference count:** 40
- **One-line primary result:** A refined analysis framework simplifies weighted strategies for non-stationary parametric bandits and MDPs, eliminating the need for problem-specific weighted concentration tools and improving regret bounds.

## Executive Summary
This paper proposes a refined analysis framework for weighted strategies in non-stationary parametric bandits and Markov Decision Processes (MDPs). The key innovation is using the same local norm ($\|x\|_{V_{t-1}^{-1}}$) to control both the bias (from parameter drift) and variance (from noise) in the estimation error, simplifying the derivation and eliminating the need for complex, problem-specific weighted concentration inequalities. This unified approach leads to a simpler algorithm that only maintains one covariance matrix instead of two, while achieving improved regret bounds. The framework is applied to linear bandits, generalized linear bandits, self-concordant bandits, and non-stationary MDPs with function approximation, including linear mixture MDPs and multinomial logit (MNL) mixture MDPs.

## Method Summary
The method employs a weighted strategy where past data is discounted by a factor $\gamma^{t-s}$ to adapt to non-stationary environments. The algorithm uses weighted regularized least squares (or weighted QMLE for GLB/SCB) to estimate the current parameter. The key insight is that by using the same $V_{t-1}^{-1}$-norm for both the bias and variance terms in the estimation error, standard self-normalized concentration inequalities can be directly applied without modification. This allows the algorithm to maintain only one covariance matrix $V_t$, simplifying implementation and improving efficiency. The UCB action selection criterion maximizes the optimistic reward estimate plus a confidence bonus based on the norm $\|x\|_{V_{t-1}^{-1}}$.

## Key Results
- Achieves regret bound of $O(k_\mu^{5/4} c_\mu^{-3/4} d^{3/4} P_T^{1/4} T^{3/4})$ for Generalized Linear Bandits, improving upon previous results.
- Achieves regret bound of $O(Hd\Delta^{1/4}T^{3/4})$ for non-stationary linear mixture MDPs.
- Provides the first dynamic regret bound for non-stationary MNL mixture MDPs.
- Simplifies the algorithm by requiring only one covariance matrix instead of two, while maintaining or improving regret bounds.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single weighted local norm ($\|x\|_{V_{t-1}^{-1}}$) is sufficient to control both the bias (from parameter drift) and variance (from noise) in the estimation error of weighted linear bandits.
- **Mechanism:** Previous approaches used different norms for bias (l2-norm) and variance (a complex norm involving a second covariance matrix) to mimic sliding-window analysis. By developing a refined bias analysis (Lemma 7) that extracts parameter variations using the same covariance matrix $V_{t-1}$ used for variance, the paper unifies the estimation error decomposition (Eq. 7). This alignment allows the standard self-normalized concentration to be applied directly to the variance term without complex modifications to the local norm.
- **Core assumption:** The weighted factors decay geometrically ($w_{t,s} = \gamma^{t-s}$), ensuring the impact of past data is systematically discounted.
- **Evidence anchors:**
  - [abstract] "The authors propose a refined analysis framework using the same local norm for both components, eliminating the need for weighted concentration tools."
  - [section III-C] "As an improvement, we directly use the same $V_{t-1}^{-1}$-norm to control both parts... Previous analysis [2] had to use different local norms: using $l_2$-norm in the bias part..."

### Mechanism 2
- **Claim:** Standard self-normalized concentration inequalities apply to weighted strategies without modification.
- **Mechanism:** The variance term of the estimation error involves the sum $\sum w_{t-1,s}\eta_s X_s$. By defining rescaled noise $\tilde{\eta}_s = \sqrt{w_{t-1,s}}\eta_s$ and features $\tilde{X}_s = \sqrt{w_{t-1,s}}X_s$, the problem maps directly to the standard setup. Since weights $w \le 1$, the rescaled noise remains R-sub-Gaussian, allowing the direct application of standard theorems (e.g., Theorem 7 in Appendix G) rather than deriving a specialized weighted version.
- **Core assumption:** The stochastic noise $\eta_s$ is conditionally R-sub-Gaussian, and the weights are bounded $w_{t,s} \le 1$.
- **Evidence anchors:**
  - [section III-C] "Consequently, we can directly apply the self-normalized concentration (Theorem 7) to control the variance term, without requiring the weighted version... proposed in Theorem 1 of [2]."

### Mechanism 3
- **Claim:** The algorithmic complexity reduces from maintaining two covariance matrices to one while preserving regret bounds.
- **Mechanism:** Because the analysis unifies the local norm (Mechanism 1), the Upper Confidence Bound (UCB) action selection criterion only requires computing $\|x\|_{V_{t-1}^{-1}}$. This eliminates the need for the auxiliary matrix $\tilde{V}_t$ (used in previous algorithms like D-LinUCB) that was required to calculate the complex norm for the variance term.
- **Core assumption:** The estimation error bound (Lemma 1) derived from the unified analysis accurately captures the true error dynamics.
- **Evidence anchors:**
  - [abstract] "...leads to a simpler algorithm that only maintains one covariance matrix instead of two while achieving the same regret bound."
  - [section III-B] "Thus, our algorithm is more efficient... since it only needs to maintain one covariance matrix instead of two."

## Foundational Learning

- **Concept:** **Local Norms (Ellipsoidal Norms)**
  - **Why needed here:** The paper's core innovation is changing which local norm ($\|\cdot\|_A$) is used to measure the size of the bias and variance vectors. Understanding that $\|x\|_{V^{-1}} \le 1$ defines an ellipsoid is crucial for grasping why maintaining $V_t$ is central to the algorithm's "confidence."
  - **Quick check question:** How does the shape of the ellipsoid defined by $V_t^{-1}$ change as more data is collected with weights $\gamma < 1$?

- **Concept:** **Dynamic Regret**
  - **Why needed here:** The objective is not static regret (beating a fixed best policy) but dynamic regret (beating a sequence of optimal policies). The mechanism relies on balancing the "bias" from the changing optimal policy against the "variance" of learning.
  - **Quick check question:** If the environment changed very abruptly (low path length $P_T$ but high change count $\Gamma_T$), how might the "gradual drift" assumption of the weighted strategy compare to a restart strategy?

- **Concept:** **Self-Concordance**
  - **Why needed here:** The paper extends the framework to Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). Self-concordance provides curvature information that allows improving the dependence on the parameter $c_\mu$ (minimum derivative of the link function).
  - **Quick check question:** Why does knowing the reward function has a bounded third derivative (self-concordance) help tighten the confidence radius compared to a generic Lipschitz GLM?

## Architecture Onboarding

- **Component map:** Estimator (Weighted Regularized Least Squares) -> Covariance Matrix ($V_t$) -> UCB Scoring (Optimistic reward + Confidence Bonus) -> Hyperparameter (Discount factor $\gamma$)
- **Critical path:** The recursive update of $V_t$ and the efficient computation of $\|x\|_{V_{t-1}^{-1}}$ (often via Woodbury matrix identity or Cholesky decomposition update) are the computational bottlenecks.
- **Design tradeoffs:**
  - **Single vs. Double Matrix:** This paper argues for single $V_t$ (more efficient, simpler code) vs. previous methods using two (more complex analysis, potentially tighter bounds in specific edge cases).
  - **Projection (GLB/SCB):** For GLB, a projection step onto the convex constraint set $\Theta$ is needed. The refined analysis allows a simpler projection (Eq 12) than previous works which required complex constraint optimization.
- **Failure signatures:**
  - **Inefficient Recursion:** If $V_t$ is updated non-recursively (re-calculating the sum over history), the algorithm will be $O(d^2 t)$ per round instead of $O(d^2)$, causing timeouts on long horizons.
  - **Untuned $\gamma$:** A fixed $\gamma$ too close to 1 will cause high lag (bias) in rapidly changing environments; too small will cause high variance and noise sensitivity.
  - **Numerical Instability:** Recursive updates of $V_t$ and its inverse must use regularization ($\lambda I_d$) to prevent singularity as weights decay.
- **First 3 experiments:**
  1.  **Stationary Baseline:** Implement the basic WeightUCB and verify it recovers the standard LinUCB/OFUL regret bound ($\tilde{O}(d\sqrt{T})$) when $\gamma=1$ or $P_T=0$.
  2.  **Rotating Non-Stationarity:** Create a synthetic "rotating theta" environment (as described in Section VII) where $\theta_t$ moves linearly. Plot cumulative dynamic regret for different fixed $\gamma$ values to visualize the bias-variance tradeoff.
  3.  **GLB/Logistic Bandit:** Implement the GLB-WeightUCB variant with the simple projection. Compare the regret against the baseline algorithm (BVD-GLM-UCB) specifically tracking the dependence on $c_\mu$ and dimension $d$.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can an adaptive weight-based algorithm be designed to achieve the minimax optimal dynamic regret bound of $\tilde{O}(P_T^{1/3}T^{2/3})$ without prior knowledge of the environment's non-stationarity (path length $P_T$)?
  - **Basis:** The authors explicitly state in Section III.B and the Conclusion that designing such an adaptive algorithm is "very challenging" and they "leave this as an important open question for future study."
  - **Why unresolved:** At each round, the learner only receives one data pair, which is insufficient for real-time updates of the continuous discounted factor $\gamma_t \in [0,1]$, making it harder than the binary restart decisions used in existing optimal algorithms like MASTER.
  - **What evidence would resolve it:** A novel algorithm that adaptively tunes the discounted factor using only streaming data and a theoretical proof showing it achieves the optimal $\tilde{O}(P_T^{1/3}T^{2/3})$ regret rate.

- **Open Question 2:** What is the minimax optimal dynamic regret rate for non-stationary parametric bandits when the arm set varies over time?
  - **Basis:** The authors note in Section III.B that while their weighted strategy can handle time-varying arm sets, "it remains unclear whether optimal regret can be achieved," and reiterate in the Conclusion that "the minimax rate remains open for time-varying arm sets."
  - **Why unresolved:** The current optimal lower bounds ($\Omega(P_T^{1/3}T^{2/3})$) and optimal algorithms (e.g., MASTER) rely on the assumption of a fixed arm set, limiting their applicability to the time-varying case.
  - **What evidence would resolve it:** The derivation of a specific lower bound for the time-varying arm setting or the proposal of an algorithm proven to match the optimal rate in this specific setting.

- **Open Question 3:** Can more refined characterizations of gradual environment changes be developed to better distinguish the theoretical performance of weighted strategies from restart strategies?
  - **Basis:** In the Conclusion, the authors suggest that the standard path length $P_T$ "may not be precise enough" as it mixes gradual and abrupt changes, and propose future research "explore more refined characterizations of gradual changes" (e.g., inspired by Sobolev classes).
  - **Why unresolved:** Path length $P_T$ is a general metric that does not specifically penalize abrupt shifts or reward smoothness, failing to theoretically explain why weighted strategies often outperform restart strategies empirically in slowly evolving environments.
  - **What evidence would resolve it:** A new non-stationarity measure tailored to gradual drift and a theoretical analysis demonstrating that weighted strategies achieve strictly better regret bounds than restart-based methods under this specific measure.

## Limitations

- The paper primarily focuses on algorithmic improvement and theoretical analysis, lacking extensive empirical validation across diverse non-stationary bandit algorithms and real-world datasets.
- The efficiency gains (maintaining one matrix vs. two) are primarily in terms of algorithmic complexity and implementation simplicity, not necessarily in wall-clock time, as the dominant cost (matrix updates/inversions) remains $O(d^3)$ per round.
- The practical sensitivity to hyperparameter tuning (e.g., the discount factor $\gamma$) in highly non-stationary regimes is not thoroughly explored.

## Confidence

- **High:** The core theoretical claim that the unified local norm analysis simplifies the algorithm without sacrificing regret bounds. The derivation of the estimation error bound (Lemma 1) and its application to various bandit and MDP settings appears rigorous.
- **Medium:** The claim of improved regret bounds (e.g., $O(k_\mu^{5/4} c_\mu^{-3/4} d^{3/4} P_T^{1/4} T^{3/4})$ for GLB). While the theoretical improvement is clear, the practical significance compared to the baseline requires empirical validation on diverse problem instances.
- **Medium:** The assertion that the framework is "simpler" and more "efficient" to implement. This is true in terms of code complexity and maintaining fewer data structures, but the actual runtime performance depends on the specific implementation details of the matrix operations.

## Next Checks

1. **Hyperparameter Robustness:** Conduct experiments to assess the sensitivity of the algorithm's performance to the choice of the discount factor $\gamma$ across different levels of non-stationarity (measured by $P_T$ and $\Gamma_T$). This would validate the practical guidance for tuning this critical parameter.
2. **Computational Benchmarking:** Implement both the single-matrix algorithm (from this paper) and the double-matrix baseline (e.g., D-LinUCB) and measure their actual wall-clock runtimes and memory usage on problems with varying $d$ and $T$ to quantify the claimed efficiency gains.
3. **Broader Empirical Comparison:** Extend the experimental section to include comparisons against other prominent non-stationary bandit algorithms (e.g., restart strategies, sliding-window methods) on a wider range of synthetic and real-world datasets to assess the relative performance of the weighted strategy in practice.