---
ver: rpa2
title: 'CitiLink: Enhancing Municipal Transparency and Citizen Engagement through
  Searchable Meeting Minutes'
arxiv_id: '2601.18374'
source_url: https://arxiv.org/abs/2601.18374
tags:
- minutes
- citilink
- meeting
- municipal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CitiLink is a platform that transforms unstructured municipal meeting
  minutes into structured, searchable data using LLMs and IR techniques. It extracts
  metadata, discussion topics, and voting outcomes from 120 anonymized Portuguese
  council minutes, indexing them in MongoDB for full-text and faceted search.
---

# CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes

## Quick Facts
- arXiv ID: 2601.18374
- Source URL: https://arxiv.org/abs/2601.18374
- Reference count: 14
- 120 anonymized Portuguese council minutes processed with 0.84 macro F1 metadata extraction and 0.67 macro F1 voting outcomes

## Executive Summary
CitiLink is a platform that transforms unstructured municipal meeting minutes into structured, searchable data using LLMs and IR techniques. It extracts metadata, discussion topics, and voting outcomes from 120 anonymized Portuguese council minutes, indexing them in MongoDB for full-text and faceted search. The system achieves 0.84 macro F1 for metadata extraction and 0.67 macro F1 for voting outcomes, while usability tests with municipal staff showed intuitive navigation and high satisfaction. The platform supports multilingual access and includes a human-in-the-loop validation process, demonstrating the potential of NLP and IR to improve municipal transparency and citizen engagement.

## Method Summary
The system uses Gemini 2.0 Flash with prompt engineering to extract three hierarchical layers from plain-text minutes: metadata (participants, location, date, meeting type), discussion subjects, and voting outcomes. Extracted data is cross-referenced with predefined MongoDB collections for entity normalization, then indexed in MongoDB Atlas for full-text search using BM25 ranking and faceted filtering. A React frontend provides search interface while a restricted back-office enables municipal staff to upload minutes and validate LLM outputs before publication. The 120-minute corpus spans six Portuguese municipalities with ground truth annotations available for evaluation.

## Key Results
- Metadata extraction achieves 0.84 macro F1 score using Gemini 2.0 Flash
- Voting outcome extraction achieves 0.67 macro F1 score, showing room for improvement
- Subject matching uses BERTimbau embeddings with ROUGE-L 0.31 and BLEU 0.21 scores
- Usability testing with municipal staff shows high satisfaction and intuitive navigation
- System supports 22,702 "in favor" vs 161 "against" votes, indicating class imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with prompt engineering can reliably extract structured metadata from unstructured bureaucratic documents.
- Mechanism: Gemini 2.0 Flash receives plain-text minutes and extracts three hierarchical layers: (1) metadata (participants, location, date, meeting type), (2) discussion subjects, and (3) voting outcomes (in favor, against, abstention). Cross-referencing with predefined collections normalizes entities across documents.
- Core assumption: Bureaucratic Portuguese meeting minutes contain sufficiently consistent linguistic patterns for LLM extraction, despite formal style and administrative jargon.
- Evidence anchors:
  - [abstract] "The system employs LLMs to extract metadata, discussed subjects, and voting outcomes"
  - [section] "Each minute is provided to the LLM as plain text, which, using prompt engineering, extracts three layers of information"
  - [corpus] Related work (VotIE, MiNER) confirms active research in municipal minute extraction, validating the problem framing
- Break condition: Highly irregular document formats or municipalities with divergent voting terminology will degrade extraction quality below usable thresholds.

### Mechanism 2
- Claim: MongoDB with BM25 ranking enables effective full-text and faceted search over structured municipal data.
- Mechanism: Extracted data is indexed in MongoDB Atlas, supporting BM25 lexical ranking for query relevance and faceted filtering by municipality, topic, party, and participant through the React frontend.
- Core assumption: Users' information needs align with the extracted facets and full-text search sufficiently captures query intent.
- Evidence anchors:
  - [abstract] "indexing them in MongoDB for full-text and faceted search"
  - [section] "All data are stored in MongoDB Atlas to support full-text and faceted search"
  - [corpus] Corpus evidence on MongoDB-specific IR performance for municipal data is weak—related papers do not detail storage/ranking choices
- Break condition: Complex multi-hop queries (e.g., "which councilors voted against health proposals in 2024?") may exceed BM25 capabilities.

### Mechanism 3
- Claim: Human-in-the-loop validation by municipal staff improves data quality and system trustworthiness.
- Mechanism: A restricted back-office allows municipalities to upload minutes and validate LLM-extracted data before publication, creating an oversight layer that catches extraction errors.
- Core assumption: Domain experts can efficiently identify and correct extraction errors; staff time investment is feasible.
- Evidence anchors:
  - [abstract] "includes a human-in-the-loop validation process"
  - [section] "a restricted back-office where municipalities can upload minutes and validate extracted data, ensuring human-in-the-loop oversight"
  - [corpus] Corpus evidence on human-in-the-loop validation specifically for municipal minute extraction is weak—related systems emphasize full automation
- Break condition: Staff workload constraints or insufficient training reduce validation coverage, allowing errors to persist.

## Foundational Learning

- Concept: BM25 Ranking Algorithm
  - Why needed here: The system relies on BM25 for relevance scoring in full-text search across meeting minutes and discussion subjects.
  - Quick check question: Given documents of varying lengths (minutes range significantly), how does BM25's length normalization affect retrieval compared to raw TF-IDF?

- Concept: Faceted Search Architecture
  - Why needed here: Users filter by municipality, topic, party, and participant across 120+ indexed minutes.
  - Quick check question: How would you structure MongoDB indexes to support both full-text search and efficient faceted filtering without degrading query latency?

- Concept: Prompt Engineering for Structured Extraction
  - Why needed here: LLM extraction quality depends on prompt design for three distinct output types (metadata, subjects, votes).
  - Quick check question: What prompt strategies could reduce confusion between "voted in favor" and "proposed for voting" in free-form narrative text?

## Architecture Onboarding

- Component map:
  - Raw `.txt` minutes → Gemini 2.0 Flash (prompt-based extraction) → Structured JSON → Cross-reference validation → MongoDB Atlas → React frontend

- Critical path:
  1. Anonymized `.txt` minute upload
  2. LLM extraction (Gemini 2.0 Flash)
  3. Entity cross-referencing against predefined collections
  4. Human validation in back-office
  5. MongoDB indexing
  6. Public query via React frontend

- Design tradeoffs:
  - Gemini 2.0 Flash (proprietary) vs. open-source alternatives—paper explicitly notes future work to integrate open-source LLMs
  - 0.67 voting F1 suggests automation vs. accuracy tradeoff; human validation compensates
  - 120-document corpus prioritizes demonstration scope over production-scale coverage

- Failure signatures:
  - Macro F1 of 0.67 for voting outcomes indicates complex voting narratives remain challenging
  - Subject extraction required embedding-based matching (ROUGE-L: 0.31, BLEU: 0.21) since LLM outputs lack text offsets
  - 22,702 "in favor" vs. 161 "against" votes suggests class imbalance may affect model calibration

- First 3 experiments:
  1. Stratify extraction evaluation by municipality to identify format-specific failure modes and prompt refinement opportunities.
  2. Benchmark Gemini 2.0 Flash against open-source alternatives (e.g., LLaMA, Mistral) on identical extraction tasks to inform cost/accuracy tradeoffs.
  3. Measure query latency and relevance satisfaction with real users (municipal staff or journalists) performing targeted vs. exploratory searches to validate BM25 adequacy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source language models achieve performance comparable to proprietary models like Gemini 2.0 Flash across the specific extraction layers (metadata, subjects, votes)?
- Basis in paper: [explicit] The authors state, "In future work, we plan to develop and integrate open-source language models across the different layers."
- Why unresolved: The current system relies exclusively on the proprietary Gemini model; open-source alternatives have not yet been tested or benchmarked within the pipeline.
- What evidence would resolve it: A comparative evaluation benchmarking the macro F1 scores of open-source models against the current Gemini baseline using the same dataset.

### Open Question 2
- Question: How does the platform’s usability and impact differ when evaluated by external stakeholders (citizens and journalists) compared to internal municipal staff?
- Basis in paper: [explicit] The authors plan to "expand evaluation through citizen and journalist feedback via questionnaires and focus groups, enabling a broader assessment of CitiLink’s usability."
- Why unresolved: Current usability testing was restricted to administrative staff and minute-producers; the interface has not been validated by the primary transparency target audience.
- What evidence would resolve it: Qualitative and quantitative results from usability sessions and focus groups involving journalists and general citizens.

### Open Question 3
- Question: What technical improvements are required to close the performance gap between structured metadata extraction and complex voting outcome extraction?
- Basis in paper: [inferred] The paper reports a macro F1 of 0.67 for voting outcomes versus 0.84 for metadata, explicitly stating there is "room for improvement" due to the greater complexity of the voting task.
- Why unresolved: The current pipeline struggles with the nuance of voting positions relative to simpler metadata fields, and specific solutions to improve this score were not identified.
- What evidence would resolve it: Improved F1 scores for voting extraction resulting from new prompt engineering strategies or fine-tuned models.

## Limitations
- Evaluation based on only 120 anonymized minutes from 6 Portuguese municipalities, limiting generalizability
- 0.67 macro F1 for voting outcomes indicates notable extraction errors, particularly for complex voting narratives
- Subject matching relies on embedding similarity (ROUGE-L: 0.31, BLEU: 0.21), suggesting outputs may lack precise text alignment with ground truth
- Critical implementation details remain unspecified including exact prompt templates and MongoDB configuration

## Confidence
- **High Confidence**: MongoDB's capability for full-text and faceted search is well-established, though specific configuration details are missing
- **Medium Confidence**: LLM extraction can handle Portuguese bureaucratic minutes with prompt engineering, but quality varies significantly by extraction type (0.84 vs 0.67 F1)
- **Low Confidence**: Human-in-the-loop validation is effective without evidence on staff workload constraints or validation coverage requirements

## Next Checks
1. Stratify extraction evaluation by municipality to identify format-specific failure modes and prompt refinement opportunities, particularly focusing on the 0.67 voting F1 gap.
2. Benchmark Gemini 2.0 Flash against open-source alternatives (e.g., LLaMA, Mistral) on identical extraction tasks to assess cost/accuracy tradeoffs and validate the proprietary model choice.
3. Conduct user studies with municipal staff performing both targeted (e.g., "find all votes by Councilor X") and exploratory searches to validate BM25 ranking effectiveness and measure query latency under realistic loads.