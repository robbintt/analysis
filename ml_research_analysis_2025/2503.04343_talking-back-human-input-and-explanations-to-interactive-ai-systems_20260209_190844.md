---
ver: rpa2
title: Talking Back -- human input and explanations to interactive AI systems
arxiv_id: '2503.04343'
source_url: https://arxiv.org/abs/2503.04343
tags:
- explanations
- user
- human
- more
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to interactive AI systems
  where humans provide explanations for their decisions to AI systems, not just receive
  explanations from them. The authors explore various forms of human inputs to AI,
  including feature constraints, importance measures, intermediate features, and free
  text, and discuss how these can be incorporated into machine learning algorithms.
---

# Talking Back -- human input and explanations to interactive AI systems

## Quick Facts
- **arXiv ID**: 2503.04343
- **Source URL**: https://arxiv.org/abs/2503.04343
- **Authors**: Alan Dix; Tommaso Turchi; Ben Wilson; Anna Monreale; Matt Roach
- **Reference count**: 24
- **Primary result**: Proposes bidirectional human-AI explanation where humans provide explanations to AI systems, not just receive them

## Executive Summary
This paper introduces a novel approach to interactive AI systems where humans explain their decisions to AI systems rather than merely receiving explanations. The authors explore multiple forms of human input including feature constraints, importance measures, intermediate features, and free text, and discuss how these can be incorporated into machine learning algorithms. The work suggests that human explanations can guide ML models toward automated judgments and explanations that align more closely with human concepts, opening new research directions in human-AI interaction and synergistic systems.

## Method Summary
The paper proposes conceptual frameworks for incorporating various types of human explanations into machine learning algorithms without providing concrete implementations. Methods include constraining decision trees to include user-specified features, modifying backpropagation to weight features by user-assigned importance, clamping neural network nodes to encode intermediate features, and parsing free-text explanations. The approach extends interactive machine learning beyond labeling to include explanation-rich feedback, suggesting both algorithmic modifications and architectural considerations for bidirectional human-AI dialogue.

## Key Results
- Human explanations can guide ML models toward decisions that align with human reasoning patterns
- Feature importance measures from humans create shared vocabulary for human-AI dialogue
- Intermediate features generated by users may help bridge semantic gaps between raw data and human concepts
- The approach opens new research directions in synergistic human-AI systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-provided feature constraints may guide ML models toward decisions that align with human reasoning patterns.
- Mechanism: Users specify which features influenced their classification (e.g., "I selected this row because salary > 10,000"). These constraints are incorporated into the learning process by forcing specific features to appear in decision paths (for trees) or modifying fitness functions (for genetic algorithms).
- Core assumption: Humans can accurately articulate which features drove their decisions, and these articulated features correspond to genuinely relevant decision criteria.
- Evidence anchors:
  - [abstract] "human explanations can guide machine learning models toward automated judgments and explanations that align more closely with human concepts"
  - [Section 3.1] Describes QbB with local feature explanations where users specify "I chose this row because of this column value"
  - [corpus] Related work on explanatory interactive machine learning (Tesó & Kersting) supports the broader direction but empirical validation for this specific mechanism is limited
- Break condition: If users cannot reliably identify which features actually drove their decisions (post-hoc rationalization bias), or if constrained features conflict with optimal classification boundaries.

### Mechanism 2
- Claim: Feature importance measures provided by humans could create a shared vocabulary for human-AI dialogue about reasoning.
- Mechanism: Users assign continuous importance values (e.g., SHAP-style: white blood cell count = 0.8, fever = 0.4). These modify backpropagation learning rates for different features or boost splitting scores in gradient-boosted trees, encouraging the model to weight features similarly to humans.
- Core assumption: Human-assigned importance values reflect meaningful reasoning patterns that generalize beyond individual instances.
- Evidence anchors:
  - [Section 3.3] "This approach provides a natural vocabulary for dialogue between human and AI systems, as both can express their reasoning in terms of feature importance distributions"
  - [Section 4.2] Proposes modifying backpropagation and splitting criteria to incorporate user importance values
  - [corpus] No direct corpus validation found; this is proposed architecture
- Break condition: If different experts provide contradictory importance values for similar cases without clear contextual factors explaining the divergence.

### Mechanism 3
- Claim: Intermediate features generated by users may help bridge the semantic gap between raw input data and human-interpretable concepts.
- Mechanism: Experts introduce new vocabulary (e.g., "irregular heart rhythm") not present in raw features. Neural networks can "clamp" arbitrary nodes in pinch-point layers to known intermediate values during training, propagating this semantic knowledge both forward and backward.
- Core assumption: Intermediate features are learnable from raw data and consistently identifiable by the same expert across cases.
- Evidence anchors:
  - [Section 3.4] Describes doctors using terms like "irregular heart rhythm" mid-way between input data and final classification
  - [Section 4.3/Figure 7] Details clamping mechanism where backprop uses known intermediate values as delta signals
  - [corpus] Concept-based XAI work (neighboring paper on multi-dimensional concept discovery) addresses related problems but from the AI-to-human direction
- Break condition: If intermediate features cannot be reliably derived from input data, or if experts use terms inconsistently.

## Foundational Learning

- **SHAP (SHapley Additive exPlanations)**
  - Why needed here: The paper proposes reversing SHAP-style visualizations—letting humans manipulate feature importance bars rather than just viewing AI-generated ones.
  - Quick check question: Can you explain how a SHAP value of +0.3 for a feature would be interpreted in a classification context?

- **Interactive Machine Learning (IML)**
  - Why needed here: The proposal extends IML beyond labeling to include explanation-rich feedback, building on systems like Query-by-Browsing and skeptical learning.
  - Quick check question: How does active learning's "uncertainty sampling" differ from the "skeptical learning" counterfactual challenges described in the paper?

- **Pinch-point layers in neural networks**
  - Why needed here: The paper proposes clamping nodes at these layers to encode user-defined intermediate features.
  - Quick check question: Where in a typical encoder-decoder architecture would you expect semantic concepts to be most concentrated?

## Architecture Onboarding

- **Component map**:
  - Explanation capture layer -> Explanation parser -> Constraint-aware learner -> Dual feedback loops -> Versioning system

- **Critical path**:
  1. User provides classification + explanation for a data point
  2. Explanation parser extracts feature constraints/importance/intermediate features
  3. Constraint-aware learner incorporates explanation into model update
  4. System generates updated model + own explanation
  5. User can critique system explanation, cycling back to step 1

- **Design tradeoffs**:
  - **Expert vs. novice explanations**: Expert explanations may carry more weight for model accuracy; novice feedback better evaluates explanation comprehensibility. The paper notes this requires context tracking.
  - **Local vs. global explanations**: Local explanations (case-specific) may conflict across cases; global explanations risk overgeneralization.
  - **Incremental vs. batch integration**: Incremental allows real-time adaptation but may destabilize models; batch provides stability but delays feedback incorporation.

- **Failure signatures**:
  - **Explanation contradiction loop**: System keeps oscillating between different feature weightings as different users provide conflicting explanations for similar cases.
  - **Post-hoc rationalization capture**: Model learns users' stated reasons (which may be biased/incomplete) rather than their actual decision patterns.
  - **Vocabulary explosion**: Unconstrained intermediate feature creation leads to fragmented, non-generalizable concepts.

- **First 3 experiments**:
  1. Extend Query-by-Browsing with local feature explanation capture (as proposed in Section 7), testing whether constrained queries better match user intent than unconstrained learned queries.
  2. Implement importance-weighted backpropagation on a controlled classification task, comparing models trained with vs. without human importance measures on alignment with human reasoning (not just accuracy).
  3. Test the neural clamping mechanism with a single intermediate feature on a dataset where ground-truth intermediate concepts exist (e.g., visual features before object classification), validating whether clamping actually encodes the intended semantics.

## Open Questions the Paper Calls Out
None

## Limitations
- No empirical validation of proposed mechanisms; remains at conceptual level
- Technical specifications for algorithmic modifications are abstract with no implementation details
- No validation methodology or metrics to measure whether model explanations "align more closely with human concepts"
- Uncertainty about handling conflicting explanations from different users or contexts

## Confidence

- **High confidence**: The conceptual value of bidirectional human-AI explanation as a research direction; the identification of multiple explanation types (feature constraints, importance measures, intermediate features, free text) as valid human inputs.
- **Medium confidence**: The general feasibility of modifying ML algorithms to incorporate human explanations; the potential for explanations to create shared vocabulary between humans and AI.
- **Low confidence**: Specific algorithmic implementations (particularly neural clamping and feature-constrained tree induction); quantitative claims about improved alignment with human concepts.

## Next Checks

1. Implement a minimal proof-of-concept for feature-constrained decision trees on a standard dataset, comparing accuracy and explanation alignment between constrained and unconstrained models.
2. Conduct a user study where experts provide importance measures for a classification task, then test whether models trained with these measures produce explanations that experts find more aligned with their reasoning.
3. Validate the neural clamping mechanism using a dataset with known intermediate concepts (e.g., visual features in image classification), testing whether clamped models actually learn and utilize the intended semantic features.