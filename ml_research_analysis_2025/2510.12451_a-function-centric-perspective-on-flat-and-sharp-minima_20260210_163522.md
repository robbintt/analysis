---
ver: rpa2
title: A Function Centric Perspective On Flat and Sharp Minima
arxiv_id: '2510.12451'
source_url: https://arxiv.org/abs/2510.12451
tags:
- sharpness
- loss
- generalisation
- metrics
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the widely-held view that flatter minima
  in deep neural networks consistently yield better generalization and safety performance.
  Through extensive empirical studies on both synthetic single-objective optimization
  problems and high-dimensional vision tasks (CIFAR-10, CIFAR-100, TinyImageNet) using
  ResNet, VGG, and ViT architectures, the authors show that sharpness is better understood
  as a function-dependent property rather than a universal proxy for generalization.
---

# A Function Centric Perspective On Flat and Sharp Minima

## Quick Facts
- arXiv ID: 2510.12451
- Source URL: https://arxiv.org/abs/2510.12451
- Authors: Israel Mason-Williams; Gabryel Mason-Williams; Helen Yannakoudakis
- Reference count: 40
- Key outcome: This paper challenges the widely-held view that flatter minima in deep neural networks consistently yield better generalization and safety performance.

## Executive Summary
This paper challenges the widely-held view that flatter minima in deep neural networks consistently yield better generalization and safety performance. Through extensive empirical studies on both synthetic single-objective optimization problems and high-dimensional vision tasks (CIFAR-10, CIFAR-100, TinyImageNet) using ResNet, VGG, and ViT architectures, the authors show that sharpness is better understood as a function-dependent property rather than a universal proxy for generalization. They find that models trained with standard regularisation techniques (SAM, weight decay, data augmentation) typically converge to sharper minima yet often outperform flatter, unregularised baselines across generalisation, calibration, robustness, and functional consistency metrics. Sharpness-aware methods like SAM, contrary to their flatness-promoting motivation, frequently increase sharpness but still improve performance by enabling the learning of more complex, well-generalising functions. The study demonstrates that there is no "goldilocks zone" for sharpness—optimal geometry depends on task complexity and inductive biases. Overall, the results support a function-centric view where solution geometry reflects learned function complexity rather than flatness alone.

## Method Summary
The study employs a multi-faceted experimental design combining synthetic single-objective optimization with real-world vision tasks. For synthetic experiments, MLPs are trained on six benchmark functions (Sphere, Rosenbrock, Rastrigin, Beale, Booth, Three-Hump Camel, Himmelblau) with identical hyperparameters to isolate function-dependent geometric properties. For vision tasks, ResNet-18, VGG-19, and ViT architectures are trained on CIFAR-10, CIFAR-100, and TinyImageNet using six controlled training conditions: Baseline, Baseline+SAM, Augmentation, Augmentation+SAM, Weight Decay, and Weight Decay+SAM. Each condition uses matched seeds for controlled comparison. Sharpness is measured using Fisher-Rao Norm, Relative Flatness, and SAM-Sharpness—reparameterization-invariant metrics that capture global geometry. Safety evaluations include Expected Calibration Error (ECE), corruption robustness on CIFAR-C datasets, and prediction disagreement for functional consistency. The experimental protocol systematically varies hyperparameters including SAM's ρ parameter to explore the sharpness-generalization relationship.

## Key Results
- Models trained with standard regularisation techniques (SAM, weight decay, data augmentation) typically converge to sharper minima yet often outperform flatter, unregularised baselines across generalisation, calibration, robustness, and functional consistency metrics.
- Sharpness-aware methods like SAM, contrary to their flatness-promoting motivation, frequently increase sharpness but still improve performance by enabling the learning of more complex, well-generalising functions.
- There is no "goldilocks zone" for sharpness—optimal geometry depends on task complexity and inductive biases, as shown by the non-monotonic relationship between SAM's ρ parameter and performance.

## Why This Works (Mechanism)

### Mechanism 1: Function Complexity Governs Solution Geometry
- Claim: Solution geometry (sharpness/flatness) reflects the complexity of the learned function rather than directly determining generalization quality.
- Mechanism: Tasks requiring tighter decision boundaries or more expressive functions naturally converge to sharper minima, while simpler functions permit flatter solutions. The sharpness at a minimum is constrained by the intrinsic curvature of the target function being approximated.
- Core assumption: Neural networks approximate target functions with geometry commensurate to function complexity.
- Evidence anchors: [abstract] "sharpness is better understood as a function-dependent property rather than a reliable indicator of poor generalisation"; [section 4, Table 2] Shows Rosenbrock function has sharp global minimum (Hessian trace 1002.0) while Sphere has flat global minimum (Hessian trace 4.0), both achieving zero loss; [corpus] "Sharp Minima Can Generalize: A Loss Landscape Perspective On Data" corroborates that sharp minima can emerge with more training data and still generalize well.

### Mechanism 2: Regularization Increases Function Complexity (Not Flatness)
- Claim: Standard regularization techniques (SAM, weight decay, data augmentation) improve performance by enabling more complex, well-generalizing functions, often producing sharper minima despite being theoretically motivated as flatness-promoting.
- Mechanism: Regularization expands the hypothesis space reachable during training, allowing models to learn functions with more intricate decision boundaries. These functions require sharper minima by nature. SAM specifically promotes local robustness within neighborhoods rather than global flatness.
- Core assumption: Better performance under regularization reflects genuine function complexity increase, not merely optimization artifacts.
- Evidence anchors: [section 6, Tables 3, 5, 7] Regularized models consistently show higher Fisher-Rao Norm, SAM-sharpness, and Relative Flatness while achieving better test accuracy, calibration (ECE), and corruption robustness; [section 6] "Augmentation+SAM achieves the best performance across evaluations while also being the sharpest model"; [corpus] Corpus literature predominantly assumes flatness is beneficial; this paper provides counterevidence.

### Mechanism 3: Local Robustness Dissociates from Global Sharpness
- Claim: SAM's local robustness objective (minimizing worst-case loss within small neighborhoods) does not guarantee globally flat solutions as measured by reparameterization-invariant metrics.
- Mechanism: SAM perturbs weights within a radius ρ and minimizes the worst-case loss locally. This creates local flatness in critical directions but allows high global curvature in other directions. Reparameterization-invariant metrics (Fisher-Rao, Relative Flatness) capture this global structure.
- Core assumption: Local robustness in weight space translates to meaningful generalization improvements independent of global sharpness.
- Evidence anchors: [section 6, Table 8] Varying SAM's ρ parameter shows that larger ρ increases sharpness while reducing generalization gap—up to a point where ρ=0.5 harms accuracy despite highest sharpness; [section 6] "SAM often increases Fisher-Rao norm, Relative Flatness, and SAM-sharpness—highlighting that sharper solutions can still emerge"; [corpus] "VASSO" and other SAM variants assume flatness mediates generalization; this paper challenges the causal chain.

## Foundational Learning

- **Concept: Reparameterization-Invariant Sharpness Metrics**
  - Why needed here: Standard sharpness measures can be arbitrarily manipulated through network reparameterization without changing function behavior. This paper uses Fisher-Rao Norm, Relative Flatness, and SAM-sharpness which are invariant to such manipulations, making cross-model comparisons meaningful.
  - Quick check question: Given two networks with identical input-output behavior but different weight magnitudes, would a reparameterization-invariant metric yield the same sharpness value?

- **Concept: Loss Landscape Geometry and Hessian-Based Curvature**
  - Why needed here: Sharpness quantifies local curvature around minima via Hessian eigenvalues, trace, or condition number. Understanding this connects the paper's single-objective optimization examples (where Hessian properties at global minima vary by function) to deep network behavior.
  - Quick check question: If a loss function's Hessian has large positive eigenvalues at a minimum, is that minimum "sharp" or "flat"?

- **Concept: Generalization Gap vs. Safety Metrics (Calibration, Robustness)**
  - Why needed here: The paper evaluates models beyond accuracy, using Expected Calibration Error (ECE) and corruption robustness. These capture whether sharpness correlates with broader reliability properties, not just train-test accuracy gaps.
  - Quick check question: A model achieves 95% accuracy but assigns 99% confidence to all predictions—is it well-calibrated?

## Architecture Onboarding

- **Component map:**
  - Data preparation -> Training pipeline -> Sharpness computation -> Safety evaluation
  - Synthetic functions -> MLP training -> Hessian analysis -> Geometric comparison
  - Vision datasets -> CNN/ViT training -> Fisher-Rao/Relative Flatness -> ECE/Corruption/Disagreement

- **Critical path:**
  1. Implement matched-seed training pipeline across all six controls
  2. Compute sharpness metrics on converged models (use full training set for Fisher-Rao/SAM-sharpness; subset for Relative Flatness on larger datasets)
  3. Run safety evaluations on held-out test sets and corruption benchmarks

- **Design tradeoffs:**
  - **Sharpness metric choice:** Fisher-Rao is fast but lower-resolution; Relative Flatness has strongest generalization correlation (per Petzka et al.) but is computationally prohibitive for large models
  - **Augmented vs. standard data for sharpness calculation:** Paper shows minimal difference (Appendix F.1), but augmented data may better reflect the training distribution the model actually saw
  - **SAM ρ hyperparameter:** Larger ρ increases sharpness and can improve performance up to a point, then degrades (Table 8: ρ=0.25 optimal for CIFAR-10 ResNet-18)

- **Failure signatures:**
  - Aggregating sharpness trends across heterogeneous architectures without controlling for inductive biases can invert conclusions (Simpson's Paradox mentioned in Section 6)
  - Assuming flatter = better may cause you to reject well-regularized, high-performing models
  - Loss landscape visualizations (2D projections) can appear flatter even when sharpness metrics increase—trust metrics over visualizations

- **First 3 experiments:**
  1. Replicate the single-objective optimization study (Section 4): Train an MLP on Sphere vs. Rosenbrock functions with identical hyperparameters. Verify that sharpness differs despite matched training loss, confirming function-dependence of geometry.
  2. Reproduce CIFAR-10 ResNet-18 six-control experiment (Table 3): Verify that Baseline is flattest but worst-performing; Augmentation+SAM is sharpest and best-performing. Check ECE and corruption accuracy trends.
  3. SAM ρ sweep (Table 8): Train ResNet-18 with ρ ∈ {0.0, 0.005, 0.025, 0.05, 0.25, 0.5}. Confirm non-monotonic relationship between sharpness and performance—there is no "goldilocks zone."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the function-centric relationship between sharpness and generalization hold across domains beyond image classification, particularly in natural language processing and reinforcement learning?
- Basis in paper: [inferred] The authors restrict empirical validation to vision tasks (CIFAR-10/100, TinyImageNet) with ResNet, VGG, and ViT architectures, noting this "ground[s] it in the vision domain" without testing other modalities.
- Why unresolved: The findings may be specific to inductive biases of convolutional and vision transformer architectures; whether the same sharpness-function complexity relationship exists for sequential data or policy learning remains unknown.
- What evidence would resolve it: Replication of the experimental protocol on NLP benchmarks (e.g., GLUE, language modeling) and RL environments, measuring whether regularized models similarly converge to sharper minima with improved safety metrics.

### Open Question 2
- Question: How can "function complexity" be formally defined and measured independently of sharpness metrics to validate the proposed function-centric interpretation?
- Basis in paper: [inferred] The paper posits that "sharpness is better understood as a function-dependent property" reflecting "the complexity of the learned function," but offers no independent quantification of function complexity beyond proxying it through sharpness itself.
- Why unresolved: Without an independent measure, the circularity problem remains: sharpness reflects function complexity, but function complexity is inferred from sharpness.
- What evidence would resolve it: Development and validation of a formal function complexity measure (e.g., based on VC dimension, Rademacher complexity, or information-theoretic quantities) that predicts optimal sharpness levels a priori.

### Open Question 3
- Question: What is the precise theoretical mechanism by which SAM improves generalization when it empirically increases global sharpness under reparameterization-invariant metrics?
- Basis in paper: [explicit] The authors "reconcile SAM's local robustness objective with increased global sharpness" but acknowledge this creates tension with SAM's original flatness-promoting motivation, stating SAM "enables good generalization and safety not solely by flattening."
- Why unresolved: The paper documents the empirical phenomenon across multiple architectures but lacks a theoretical framework explaining why promoting local robustness within a small neighborhood yields better solutions despite higher global sharpness.
- What evidence would resolve it: Formal analysis of SAM's optimization trajectory relating local perturbation robustness to global geometric properties, potentially through decomposition of sharpness into directional components.

## Limitations

- The analysis focuses primarily on image classification tasks with specific architectures (ResNet, VGG, ViT), potentially limiting generalizability to other domains and architectures.
- The computational cost of Relative Flatness limits its use to smaller models and subsets of data, potentially missing important geometric features in larger systems.
- The paper does not explore the relationship between sharpness and generalization across diverse problem domains (language, robotics, etc.) or architectures with fundamentally different inductive biases (RNNs, graph neural networks).

## Confidence

- **High confidence**: Function-dependent nature of sharpness (synthetic experiments consistently show sharp global minima achieving zero loss), and the dissociation between SAM's local robustness objective and global flatness metrics (SAM increases sharpness while improving generalization in all experiments)
- **Medium confidence**: The claim that there is no "goldilocks zone" for sharpness—while the ρ sweep shows non-monotonicity, the optimal sharpness value may depend on task complexity in ways not fully characterized
- **Medium confidence**: The mechanism that regularization enables more complex functions rather than merely promoting flatness—the paper shows regularized models are sharper and better, but the causal relationship between function complexity and geometry requires further investigation

## Next Checks

1. **Cross-domain generalization**: Replicate the six-control experiments on non-vision tasks (e.g., language modeling with transformers or tabular regression) to test whether sharpness-generalization patterns hold across problem domains
2. **Architecture-agnostic sharpness**: Apply the same analysis to architectures with different inductive biases (RNNs for sequential data, GNNs for graph-structured data) to verify that the function-centric view of sharpness applies universally
3. **Sharpness manipulation experiments**: Design modified SAM variants that explicitly minimize global sharpness metrics (Fisher-Rao or Relative Flatness) rather than local robustness, and compare their generalization performance to standard SAM to test whether local robustness alone is sufficient