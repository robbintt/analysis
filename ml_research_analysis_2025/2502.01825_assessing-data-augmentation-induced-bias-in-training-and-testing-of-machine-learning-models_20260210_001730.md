---
ver: rpa2
title: Assessing Data Augmentation-Induced Bias in Training and Testing of Machine
  Learning Models
arxiv_id: '2502.01825'
source_url: https://arxiv.org/abs/2502.01825
tags:
- data
- augmentation
- test
- testing
- augmented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates data augmentation-induced bias in machine
  learning models for flaky test classification. Using the FlakyCat dataset and an
  adapted SMOTE augmentation method, the authors conducted two experiments comparing
  model performance on augmented versus original test cases.
---

# Assessing Data Augmentation-Induced Bias in Training and Testing of Machine Learning Models

## Quick Facts
- arXiv ID: 2502.01825
- Source URL: https://arxiv.org/abs/2502.01825
- Reference count: 12
- Primary result: 12% F1 improvement from augmentation, 8% systematic bias gap

## Executive Summary
This study investigates data augmentation-induced bias in machine learning models for flaky test classification. Using the FlakyCat dataset and an adapted SMOTE augmentation method, the authors conducted two experiments comparing model performance on augmented versus original test cases. Results show an average F1 score improvement of 12% when using augmented data in training, but also reveal an 8% systematic performance gap between augmented and truly independent test cases. The bias varies across flaky test categories, ranging from 5% to 13%. The findings suggest that while augmentation improves overall performance, it introduces patterns that may not represent real-world scenarios. The authors recommend using separate non-augmented validation sets and adopting category-specific augmentation strategies for more reliable model evaluation.

## Method Summary
The study employs the FlakyCat dataset containing flaky Java test cases across five categories: Async, Test Order Dependency (TOD), Time, Concurrency (Conc), and Unordered Collections (UC). An adapted SMOTE technique generates synthetic samples through syntactic mutations including variable name changes, constant replacements, test method name alterations, and unused variable declarations. The FlakyXBert model architecture combines CodeBERT encoder with a Siamese network classifier using contrastive loss. Two experiments are conducted: Phase A (original data only) and Phase B (augmented data with strict split integrity), plus an additional test comparing performance on augmented variants versus independent test cases.

## Key Results
- 12% average F1 score improvement when using augmented data in training (48% to 60%)
- 8% systematic performance gap between augmented and truly independent test cases
- Category-specific bias ranges from 5% (TOD) to 13% (UC), with TOD showing minimal bias

## Why This Works (Mechanism)

### Mechanism 1
Data augmentation with adapted SMOTE improves classification performance for flaky test detection when augmented samples are kept within the same split as their originals. Augmentation expands the training distribution by generating syntactic variants (variable names, constants, method names, unused declarations) that preserve the semantic flakiness properties. This increases the effective sample size and exposes the model to controlled variations, strengthening the learned decision boundaries around flakiness-related features. Core assumption: The adapted SMOTE mutations preserve the true flakiness-causing patterns while introducing only superficial syntactic changes that do not create misleading signals.

### Mechanism 2
Augmented samples create systematic performance inflation when they share lineage with training data, producing an 8% average F1 gap between augmented variants and truly independent test cases. The model learns to recognize augmentation-specific artifacts (e.g., the statistical patterns of randomly generated replacement tokens) that serve as proxy signals for classification. These artifacts are present in augmented training samples and their testing counterparts, but absent in genuinely independent test cases, creating a measurable generalization gap. Core assumption: The 8% gap represents recognition of augmentation artifacts rather than legitimate generalization; the alternative explanation (augmented samples being genuinely easier to classify due to noise reduction) is not ruled out.

### Mechanism 3
Augmentation-induced bias varies systematically across flaky test categories (5% to 13%), with Test Order Dependency showing minimal bias while Unordered Collections shows maximal bias. Different flakiness categories have distinct code pattern characteristics that interact differently with SMOTE-style mutations. TOD patterns (dependencies between tests) may be more structurally distinctive and generalize naturally, while UC patterns (unordered collection comparisons) may rely more on surface-level features that overlap with augmentation artifact patterns. Core assumption: The category-specific bias differences reflect underlying code structure interactions rather than sampling variance or category-specific data quantity differences.

## Foundational Learning

- **SMOTE (Synthetic Minority Over-sampling Technique)**: Why needed here: The paper uses an adapted SMOTE variant as its sole augmentation method. Understanding how SMOTE generates synthetic samples by interpolating between minority class instances is essential to interpret both the performance gains and the bias mechanisms. Quick check question: Given two code samples A and B with the same flakiness category, how would standard SMOTE create a synthetic sample, and what semantic preservation risks does this introduce for source code?

- **Data Leakage in Augmented Datasets**: Why needed here: The core methodological failure mode the paper addresses is augmented variants of the same original appearing in both training and testing splits. Recognizing this leakage pattern is prerequisite to understanding the experimental design. Quick check question: If dataset contains original test T and augmented variants T', T'', what split configurations prevent leakage, and what configurations introduce it?

- **Contrastive Learning with Siamese Networks**: Why needed here: The FlakyXBert model uses a Siamese network architecture with contrastive loss. Understanding how contrastive objectives shape embeddings to cluster similar flakiness categories while separating dissimilar ones clarifies why augmentation artifacts could be amplified in the learned representation space. Quick check question: In a Siamese network trained with contrastive loss on flaky test pairs, what happens to the embedding of an augmented sample if the augmentation preserves the label but introduces systematic token patterns?

## Architecture Onboarding

- **Component map**: FlakyCat dataset (v0/v1/v2) -> CodeBERT encoder -> Siamese network head -> Contrastive loss module -> F1 evaluation
- **Critical path**: 1) Verify data split integrity: no original sample's augmented variants cross train/test boundaries 2) Confirm CodeBERT checkpoint loaded correctly and frozen/unfrozen as intended 3) Monitor contrastive loss convergence across 200 epochs 4) Validate F1 score calculation uses per-category averaging to handle class imbalance
- **Design tradeoffs**: Strict split vs. data efficiency: Keeping all variants (v0, v1, v2) in same split reduces training data diversity but prevents leakage; the paper recommends this tradeoff; Category-specific vs. uniform augmentation: Paper evidence suggests category-specific strategies could reduce bias, but requires per-category tuning overhead; Separate validation set overhead: Maintaining non-augmented holdout reduces available training data but provides unbiased performance estimates
- **Failure signatures**: Training F1 high, independent test F1 low (>8% gap): Indicates augmentation artifact overfitting; High variance across cross-validation folds: Suggests data split contamination or insufficient sample sizes; TOD category underperforming relative to others: May indicate this category's patterns are not well-captured by current augmentation strategy
- **First 3 experiments**: 1) Baseline replication: Reproduce Experiment 1 Phase A (original data only) and Phase B (augmented data with strict split) on FlakyCat to validate the 12% improvement claim and establish local baseline 2) Leakage quantification test: Intentionally create a contaminated split where v1/v2 augmentations of training originals appear in test set; measure performance inflation magnitude to calibrate bias sensitivity 3) Category-specific augmentation ablation: Apply augmentation only to underperforming categories (Async, Time) while leaving TOD unaugmented; measure whether targeted augmentation reduces variance in per-category bias levels

## Open Questions the Paper Calls Out

- **Do other augmentation methods, such as mutation-based or LLM-based approaches, introduce systematic biases comparable to those observed with the adapted SMOTE technique?** [explicit] Section V states, "more studies of augmentation techniques, such as SMOTE and mutation-based methods, are needed to understand not just the benefits of augmentation, but also the risk of bias." This study isolated a specific adapted SMOTE technique applied to the FlakyCat dataset; the bias characteristics of other popular augmentation strategies remain unknown. Replicating the Experiment 2 methodology (comparing performance on augmented training variants vs. independent test cases) using mutation-based and LLM-based augmentation on the same dataset would resolve this.

- **Does the observed 8% performance gap generalize to other software engineering tasks such as defect prediction or fault localization?** [explicit] Section III.F suggests, "Researchers in areas such as defect prediction and fault localization can benefit from assess[ing] bias in augmented data." The findings are specific to flaky test classification; it is unclear if the bias magnitude is a property of the classification task or the augmentation method. Conducting similar bias assessments on datasets for defect prediction and fault localization that utilize data augmentation would resolve this.

- **What specific augmentation strategies can minimize the performance gap for high-bias categories like Unordered Collections while maintaining benefits for low-bias categories like Test Order Dependency?** [inferred] The authors find a variance in bias across categories (5% to 13%) and recommend "category-specific augmentation strategies," but do not define what those strategies should be. The study quantified the bias produced by a "one-size-fits-all" adapted SMOTE method but did not develop tailored solutions for specific code patterns. Designing and testing category-aware augmentation heuristics that selectively mutate code elements based on the specific flaky test category would resolve this.

## Limitations

- The study's primary limitation is the narrow scope of augmentation methods (only adapted SMOTE with syntactic mutations), which may not generalize to other augmentation strategies or domains.
- The 8% bias gap measurement assumes the independent test set truly represents deployment conditions, but this assumption is not empirically validated.
- Category-specific bias findings (5-13% range) may be influenced by varying sample sizes across categories (Async: 298 train/78 test vs. Time: 100 train/22 test), introducing statistical power differences that could confound the observed variation patterns.

## Confidence

- **High**: The 12% F1 improvement from augmentation is well-supported by direct experimental results across all categories
- **Medium**: The 8% systematic bias gap is demonstrated but the mechanism (augmentation artifact recognition vs. genuine difficulty differences) is not definitively established
- **Medium**: Category-specific bias variation is observed but sample size differences across categories create uncertainty about whether the pattern reflects true mechanism or measurement noise

## Next Checks

1. **Cross-dataset generalization test**: Apply the same augmentation and evaluation methodology to a different software testing dataset to verify whether the 8% bias gap persists across domains and augmentation strategies
2. **Independent test set validation**: Conduct a deployment simulation where the model trained with augmented data is evaluated on completely new, independently collected flaky test cases to validate the 8% gap represents true deployment performance rather than dataset-specific artifacts
3. **Category-specific augmentation ablation**: Systematically remove augmentation from different categories to quantify how much of the 5-13% bias variation is driven by augmentation artifacts versus inherent category difficulty differences