---
ver: rpa2
title: RDF-Based Structured Quality Assessment Representation of Multilingual LLM
  Evaluations
arxiv_id: '2504.21605'
source_url: https://arxiv.org/abs/2504.21605
tags:
- context
- knowledge
- multilingual
- llms
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an RDF-based framework for systematically
  evaluating multilingual LLM quality across four context conditions (complete, incomplete,
  conflicting, and no context) in German and English. The approach uses a structured
  vocabulary to capture responses and assess knowledge leakage, where models favor
  training data over provided context.
---

# RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations

## Quick Facts
- arXiv ID: 2504.21605
- Source URL: https://arxiv.org/abs/2504.21605
- Reference count: 5
- The framework uses RDF-based structured representation to evaluate multilingual LLM quality across four context conditions, revealing 89-93% context adherence and enabling systematic analysis of knowledge leakage behaviors.

## Executive Summary
This paper introduces an RDF-based framework for systematically evaluating multilingual LLM quality across four context conditions (complete, incomplete, conflicting, and no-context) in German and English. The approach uses a structured vocabulary to capture responses and assess knowledge leakage, where models favor training data over provided context. Experiments with GPT-4o-mini and Gemini-2.0-Flash in fire safety domain revealed that models predominantly adhere to given context (89-93% replication rate) even when incorrect, with English models handling incomplete information better and German models showing stronger baseline knowledge without context. Paired statistical comparisons showed no significant differences between models in most conditions, though Cohen's κ indicated high agreement in error replication. The framework demonstrates sufficient expressiveness to capture all assessment facets and enables standardized, FAIR-aligned analysis of multilingual LLM behaviors and knowledge conflicts.

## Method Summary
The method employs a structured RDF vocabulary with 14 classes and 57 properties to represent multilingual LLM evaluation data. For each question across four context conditions (complete, incomplete, conflicting, no-context) in German and English, LLM responses are captured as RDF triples linked to validation results. The framework uses zero-shot, system-first prompting with manual validation against ground truth. Analysis employs paired McNemar testing and Cohen's kappa to compare model behaviors, while SPARQL queries enable systematic exploration of knowledge leakage patterns where models override context with training data.

## Key Results
- Models demonstrated 89-93% adherence to provided context even when incorrect, with knowledge leakage rates of only 7-11%
- English models handled incomplete information better than German models, while German models showed stronger baseline knowledge without context
- Paired statistical comparisons revealed no significant differences between models in most conditions, with Cohen's κ indicating high agreement in error replication
- The RDF vocabulary was sufficient to express every assessment facet encountered in the 28-question study

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured context manipulation reveals LLM knowledge source prioritization patterns.
- Mechanism: By exposing the same question across four controlled conditions—complete, incomplete, conflicting, and no-context—the framework creates contrastive pressure that exposes whether models defer to provided context or training data. When context contradicts training knowledge, response alignment with context indicates context adherence; alignment with factual truth indicates knowledge leakage.
- Core assumption: LLMs exhibit consistent prioritization behaviors within context conditions that are detectable through binary correctness validation.
- Evidence anchors:
  - [abstract] "Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English."
  - [section 4] "These conditions were chosen to assess how LLMs prioritize context versus training knowledge, revealing behaviors like context adherence or knowledge leakage."
  - [corpus] Related work (Xie et al., Tan et al.) documents LLM bias toward context even when incorrect, supporting the mechanism's theoretical basis.
- Break condition: If models exhibit high variance in prioritization within the same context condition, the contrastive detection mechanism degrades.

### Mechanism 2
- Claim: RDF semantic structure with language-tagged literals enables systematic cross-lingual consistency analysis.
- Mechanism: The vocabulary represents each answer with `:hasText` language tags (de/en), linked to questions via `:hasGivenFor`, materials via `:hasUsedMaterial`, and validation via `:hasValidationResult`. This graph structure permits SPARQL queries that filter by language while maintaining referential integrity across the evaluation chain.
- Core assumption: Assessment facets encountered in evaluation can be fully expressed by the 14 classes and 57 properties defined in the T-Box.
- Evidence anchors:
  - [section 3] "Multilinguality is supported via language-tagged literals for :hasText. Relationships such as :hasGivenFor (:Question to :Answer), :hasUsedMaterial (:Answer to :Material), and :hasValidationResult (:Answer to :ValidationResult) enable SPARQL queries."
  - [section 4] "demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study."
  - [corpus] Parajudica paper demonstrates comparable RDF/SPARQL-based rule evaluation for context-dependent compliance, validating the representational approach.
- Break condition: If novel assessment facets emerge that cannot be expressed without schema extension, the vocabulary requires modification before reuse.

### Mechanism 3
- Claim: Paired McNemar testing with Cohen's kappa isolates model disagreement patterns beyond chance.
- Mechanism: For each question/context/language tuple, binary correctness labels create 2×2 contingency tables comparing model pairs. McNemar's exact test identifies asymmetric discordance (one model correct where the other fails), while Cohen's kappa measures overall agreement controlling for chance—revealing whether models converge on similar error patterns.
- Core assumption: Binary correctness labels sufficiently capture the validation dimension; partial correctness is collapsed to valid/invalid.
- Evidence anchors:
  - [section 4.1] "For each model pair (Gemini-2.0-Flash vs. GPT-4o-mini) we built the 2×2 contingency table... and computed McNemar's exact test... ∆-accuracy... Cohen's κ as a measure of overall agreement beyond chance."
  - [tables 1-2] Show κ = 0.781 (German conflicting) indicating high agreement in error replication, vs. κ = 0.143 (German incomplete) indicating low agreement under information gaps.
  - [corpus] Weak direct corpus evidence for this specific statistical pairing approach in LLM evaluation; mechanism relies on classical statistical theory.
- Break condition: When discordant cell counts fall below 5 (b+c<5), McNemar tests become unreliable—necessitating descriptive-only interpretation.

## Foundational Learning

- Concept: **RDF Triples and SPARQL Query Patterns**
  - Why needed here: The entire framework is expressed as subject-predicate-object triples; understanding how to traverse `:Question → :Answer → :ValidationResult` chains via SPARQL is prerequisite to reproducing the analysis.
  - Quick check question: Given a SPARQL query selecting `?answer` where `?question :hasGivenFor ?answer` and `?answer :isValid false`, what does the result set represent?

- Concept: **Knowledge Leakage in RAG Systems**
  - Why needed here: The framework's core detection target is when LLMs override provided context with training data; understanding this phenomenon is necessary to interpret "conflicting context" condition results.
  - Quick check question: If an LLM answers correctly when given conflicting (incorrect) context, which knowledge source did it prioritize?

- Concept: **McNemar's Test for Paired Nominal Data**
  - Why needed here: The paper's statistical comparison between models relies on McNemar's exact test; interpreting p-values and contingency tables requires understanding discordant pair analysis.
  - Quick check question: In a 2×2 contingency table comparing two models, what does an asymmetric distribution in cells (b) and (c) indicate?

## Architecture Onboarding

- Component map:
  ```
  Question (14 classes, 57 properties in T-Box)
      ↓ :hasGivenFor
  Answer ← :hasUsedMaterial ← Material
      ↓ :hasValidationResult
  ValidationResult (isValid, matchesFactual, etc.)
  ```
  Vocabulary available at: `http://purl.org/sqare#`

- Critical path:
  1. Define questions with language-tagged literals (`:hasText "Frage"@de`)
  2. Create four material variants per question (complete/incomplete/conflicting/none)
  3. Collect LLM responses, instantiate `:Answer` nodes with context linkage
  4. Apply validation against ground truth, populate `:ValidationResult`
  5. Execute SPARQL queries for knowledge leakage detection

- Design tradeoffs:
  - RDF vs. flat CSV: Gains queryability, reasoning, and KG linkage; costs complexity and storage overhead
  - Binary validation (`isValid`) vs. graded scoring: Simplicity for statistical pairing; loses nuance for partial correctness
  - 28-question pilot vs. larger benchmark: Demonstrates vocabulary sufficiency; limited statistical power (wide CIs in tables)

- Failure signatures:
  - `b + c < 5` in contingency tables → McNemar test undefined; fall back to descriptive statistics
  - κ undefined → Perfect agreement or complete disagreement in all cases; no variance to measure
  - Vocabulary insufficiency → Assessment facet cannot be expressed; requires T-Box extension with new property/class

- First 3 experiments:
  1. **Reproduce fire safety evaluation**: Download vocabulary from `http://purl.org/sqare#`, instantiate 5 questions across all four context conditions, validate that SPARQL queries correctly return knowledge leakage instances (conflicting context with correct factual answers).
  2. **Language swap test**: Run the same 5 questions in a language not tested in the paper (e.g., French or Spanish) to verify vocabulary expressiveness extends beyond de/en without schema modification.
  3. **Model disagreement profiling**: Using the paired comparison method, test two additional models and compute McNemar p-values; identify which context condition produces the highest discordance rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RDF-based framework perform when applied to low-resource languages with different linguistic structures?
- Basis in paper: [explicit] Future work explicitly states plans to "add evaluations for low-resource languages"
- Why unresolved: Current study only tested German and English, both high-resource Germanic languages with similar structures
- What evidence would resolve it: Application of the framework to typologically diverse low-resource languages, demonstrating vocabulary expressiveness and revealing whether observed context prioritization patterns hold

### Open Question 2
- Question: Can knowledge leakage detection be refined beyond binary classification to capture subtle gradations of context versus training data prioritization?
- Basis in paper: [explicit] Future work calls for efforts to "refine leakage metrics" and "improve knowledge leakage detection"
- Why unresolved: Current approach captures 7-11% leakage rates but may miss nuanced blending behaviors where models partially incorporate both sources
- What evidence would resolve it: Development of granular leakage scoring validated against controlled experiments with known ground truth about training data influence

### Open Question 3
- Question: Does the 89-93% context prioritization rate generalize across domains beyond fire safety?
- Basis in paper: [inferred] Paper claims framework "generalizes to applications like educational content creation" without empirical validation beyond the single fire safety domain
- Why unresolved: 28-question study in one well-structured domain may not represent domains with ambiguous or contested knowledge
- What evidence would resolve it: Multi-domain evaluation using identical RDF vocabulary across fields with varying knowledge structures and accuracy requirements

### Open Question 4
- Question: What mechanisms explain the observed language-specific differences in handling incomplete information?
- Basis in paper: [inferred] Authors report English models handle incomplete information better while German models show stronger baseline knowledge, but statistical tests yielded wide confidence intervals and non-significant results
- Why unresolved: Sample size limitations (b + c < 5 in multiple conditions) prevented meaningful statistical inference about true cross-lingual differences
- What evidence would resolve it: Larger-scale controlled study isolating training data composition, prompt translation effects, and model architecture influences

## Limitations
- Small sample size: 28 questions provide limited statistical power with wide confidence intervals that may not generalize to other domains
- Model coverage: Only two models (GPT-4o-mini, Gemini-2.0-Flash) tested; findings may not extend to other architectures or training regimes
- Domain specificity: Fire safety domain may produce idiosyncratic LLM behaviors not representative of broader knowledge domains

## Confidence
- **High confidence**: RDF vocabulary expressiveness and SPARQL query patterns (directly demonstrated in results and traceable to explicit vocabulary definitions)
- **Medium confidence**: Paired statistical methodology (established techniques, but application to LLM evaluation lacks extensive precedent in corpus)
- **Medium confidence**: Context manipulation mechanism revealing knowledge source prioritization (supported by related work on LLM context bias, but specific behavioral patterns require replication)

## Next Checks
1. **Vocabulary extensibility test**: Apply the RDF framework to a new domain (e.g., medical knowledge) and document whether the existing 14 classes and 57 properties suffice without schema modification
2. **Statistical sensitivity analysis**: Vary question sample size and compute how confidence intervals and McNemar p-values change; identify minimum sample size for stable model comparisons
3. **Cross-linguistic generalization**: Execute the full evaluation protocol in a third language (e.g., Spanish or French) to verify that multilinguality support via language-tagged literals maintains consistency without vocabulary extension