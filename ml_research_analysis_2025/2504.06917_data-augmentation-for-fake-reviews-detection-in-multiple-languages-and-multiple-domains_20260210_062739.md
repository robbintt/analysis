---
ver: rpa2
title: Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple
  Domains
arxiv_id: '2504.06917'
source_url: https://arxiv.org/abs/2504.06917
tags:
- data
- fake
- reviews
- review
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data augmentation approach for detecting
  fake reviews across multiple languages and domains. The authors generate synthetic
  fake reviews using large language models fine-tuned on existing datasets, then use
  these augmented datasets to train fake review detectors.
---

# Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains

## Quick Facts
- arXiv ID: 2504.06917
- Source URL: https://arxiv.org/abs/2504.06917
- Authors: Ming Liu; Massimo Poesio
- Reference count: 27
- Primary result: Data augmentation improves fake review detection accuracy by 0.3-10.9 percentage points across English and Chinese datasets

## Executive Summary
This paper presents a data augmentation approach for detecting fake reviews across multiple languages (English, Chinese) and domains (books, hotels, restaurants). The method uses large language models (OPT for English, GLM-10b-Chinese for Chinese) fine-tuned on existing datasets to generate synthetic fake reviews, which are then used to train fake review detectors. The approach demonstrates consistent improvements in detection accuracy, with gains ranging from 0.3pp on DeRev to 10.9pp on Amazon datasets. The method is particularly effective when generating fake reviews from real reviews and vice versa, suggesting the generated data closely mimics the style of source data.

## Method Summary
The approach involves fine-tuning language models on target domain data, then generating synthetic reviews using an interpolation method where the first and last sentences are provided as input. The generator produces 10 candidates for each interpolation, which are then ranked using a Coherence Ranker. The selected reviews are iterated through the generation process 3 times to produce 5-sentence outputs. The augmented dataset is combined with original data using different labeling strategies, and classifiers (SVM and RoBERTa) are trained on these augmented datasets to detect fake reviews.

## Key Results
- Data augmentation improves detection accuracy by 0.3-10.9 percentage points across all tested domains and languages
- Best performance achieved when generating fake reviews from real reviews and labeling them as fake
- F1 score of 96.15 achieved on DeRev test set
- Method works consistently across different languages (English and Chinese) and domains (books, hotels, restaurants)

## Why This Works (Mechanism)
The approach leverages the ability of large language models to capture domain-specific writing styles and generate synthetic reviews that closely mimic the characteristics of real and fake reviews. By fine-tuning on existing datasets, the generator learns the stylistic patterns and linguistic features that distinguish different types of reviews. The interpolation method ensures that generated reviews maintain coherence by connecting meaningful start and end points. The Coherence Ranker selects the most fluent and contextually appropriate generations, improving the quality of synthetic data. When this high-quality synthetic data is used for training, classifiers learn more robust decision boundaries that generalize better to unseen data.

## Foundational Learning
- **Language Model Fine-tuning**: Why needed - To adapt pre-trained models to domain-specific review writing styles. Quick check - Verify perplexity on held-out domain data decreases after fine-tuning.
- **Text Interpolation**: Why needed - To generate coherent synthetic reviews by connecting meaningful start and end points. Quick check - Manually inspect generated reviews for logical flow between first and last sentences.
- **Coherence Ranking**: Why needed - To select the most fluent and contextually appropriate generated reviews from multiple candidates. Quick check - Compare coherence scores of top-ranked vs random selections.
- **Data Augmentation Strategy**: Why needed - To expand training data and improve classifier generalization. Quick check - Verify class balance and label consistency in augmented datasets.
- **Multi-language Model Selection**: Why needed - To handle different linguistic characteristics across languages. Quick check - Compare detection performance when using language-specific vs cross-lingual models.

## Architecture Onboarding

**Component Map:** LLM fine-tuning -> Text interpolation generation -> Coherence ranking -> Data labeling -> Classifier training -> Evaluation

**Critical Path:** Fine-tuning -> Generation -> Ranking -> Training. The quality of generated data directly impacts classifier performance, making the fine-tuning and generation steps most critical.

**Design Tradeoffs:** 
- Using interpolation vs. unconditional generation provides more control but may limit diversity
- Ranking candidates vs. using all generated data improves quality but reduces quantity
- Language-specific models vs. multilingual models provides better performance but requires more resources

**Failure Signatures:** 
- Poor coherence in generated reviews indicates issues with interpolation inputs or ranking
- No improvement from augmentation suggests incorrect labeling strategy or insufficient quality in generated data
- Overfitting to synthetic data indicates poor generalization capabilities

**3 First Experiments:**
1. Fine-tune OPT-6.7B on Amazon book reviews and generate 1,000 synthetic reviews using interpolation
2. Implement Coherence Ranker and evaluate top-1 vs random selection on generated review quality
3. Train RoBERTa classifier on original data vs augmented data to verify improvement in detection accuracy

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does utilizing state-of-the-art LLMs (e.g., GPT-4 or Llama-3) significantly improve the quality and utility of synthetic reviews compared to the OPT and GLM models used in this study?
- Basis in paper: [explicit] Section 6.5 states, "We expect that the results will be improved even more using a more recent LLM, that is expected to produce text of higher quality."
- Why unresolved: The study utilized OPT and GLM-10b; newer models with stronger instruction-following capabilities and semantic consistency were not tested.
- What evidence would resolve it: A comparative evaluation of fake review detection accuracy using training data generated by newer LLMs versus the study's baselines.

### Open Question 2
- Question: Does varying the length and format of generated reviews improve the robustness of fake review detectors compared to the fixed-length strategy currently employed?
- Basis in paper: [explicit] Section 6.2 notes the limitation that generated datasets currently have "unified" length and format, suggesting future work could make them "closer to that collected by humans."
- Why unresolved: The current interpolation method fixes the review length (e.g., 5 sentences), which may not reflect the natural variance of genuine reviews.
- What evidence would resolve it: Experiments comparing detection performance when training on fixed-length augmented data versus variable-length augmented data.

### Open Question 3
- Question: How does the semantic correctness and factual specificity of generated reviews affect the generalization capabilities of the trained detector?
- Basis in paper: [inferred] Section 4.4 analyzes generated outputs and notes that the OPT model often produces thematically consistent text but lacks specific details (e.g., about a book's actual content) because it has not "read" the book.
- Why unresolved: It is unclear if detectors trained on "hallucinated" or generic content transfer well to detecting sophisticated, content-rich fake reviews written by humans.
- What evidence would resolve it: Evaluating detector performance on test sets stratified by the factual specificity and semantic richness of the deceptive reviews.

## Limitations
- Lack of specific implementation details for critical components like the Coherence Ranker and fine-tuning hyperparameters
- Generated reviews may lack factual specificity and semantic correctness despite stylistic similarity
- Fixed-length generation strategy doesn't reflect natural variance in genuine reviews
- Limited evaluation to binary classification without considering more nuanced fake review categories

## Confidence
- **High confidence**: The general methodology and core findings are clearly described and consistent across all four datasets tested
- **Medium confidence**: Reported accuracy improvements are specific, but reproducibility depends on unspecified implementation details
- **Low confidence**: Effectiveness of interpolation method relative to other augmentation strategies is difficult to assess without ablation studies

## Next Checks
1. Implement the interpolation generation pipeline with Coherence Ranker and compare generated review quality (BLEU and coherence scores) against reported values to validate the generation component
2. Reproduce the RoBERTa classifier training on augmented data with different labeling strategies to verify which configuration yields optimal performance
3. Conduct an ablation study comparing the proposed interpolation method against simpler augmentation approaches (back-translation, paraphrasing) to assess whether added complexity provides measurable benefits