---
ver: rpa2
title: 'From Internal Representations to Text Quality: A Geometric Approach to LLM
  Evaluation'
arxiv_id: '2509.25359'
source_url: https://arxiv.org/abs/2509.25359
tags:
- text
- synthetic
- metrics
- instruct
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that geometric properties of internal model
  representations serve as reliable proxies for evaluating generated text quality.
  The authors validate metrics including Maximum Explainable Variance, Effective Rank,
  Intrinsic Dimensionality, MAUVE score, and Schatten Norms across different layers
  of six LLMs, showing that Intrinsic Dimensionality and Effective Rank can serve
  as universal assessments of text naturalness and quality.
---

# From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation

## Quick Facts
- arXiv ID: 2509.25359
- Source URL: https://arxiv.org/abs/2509.25359
- Reference count: 40
- Key outcome: Geometric properties of internal model representations serve as reliable reference-free proxies for evaluating text quality

## Executive Summary
This paper introduces a novel reference-free approach to LLM evaluation by analyzing geometric properties of internal representations. The authors demonstrate that metrics such as Effective Rank and Intrinsic Dimensionality can reliably distinguish between human and machine-generated text without requiring human-annotated datasets. Their approach works across different model architectures and sizes, showing that these geometric properties reflect inherent text characteristics rather than model-specific artifacts.

## Method Summary
The method extracts hidden state matrices X^(l) from MLP blocks (after activation, before residual connection) across all layers of tester models. For each representation matrix, geometric metrics are computed including Maximum Explainable Variance (MEV), Effective Rank, Intrinsic Dimensionality (via multiple estimators), Resultant Length, Schatten Norms, and MAUVE score. These metrics are averaged across layers using s_R(X_g) = (1/L)ΣR(X^(l)_g) to produce a single score per text sample. The approach is validated against traditional metrics like BLEURT, ROUGE-L, and GPT-2 perplexity, showing strong correlations.

## Key Results
- Effective Rank and Intrinsic Dimensionality serve as universal assessments of text naturalness and quality
- Different models consistently rank text from various sources in the same order based on geometric properties
- Strong correlations between geometric metrics and established external metrics (ERank↔GPT-PPL: ρ=-0.76, MEV↔GPT-PPL: ρ=0.81)
- Geometric metrics effectively distinguish human from machine-generated text without requiring reference data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geometric metrics capture intrinsic text properties rather than model-specific artifacts, enabling cross-model consistency in quality assessment.
- Mechanism: All trained LLMs develop similar internal geometries for processing text complexity. Higher-quality text exhibits higher Effective Rank and Intrinsic Dimensionality because natural language has greater complexity and less directional collapse in representation space. Cross-model Spearman correlations exceed 0.947, indicating metrics reflect text-intrinsic properties.
- Core assumption: All sufficiently trained language models develop similar internal geometries for processing text complexity, regardless of architecture or size.
- Evidence anchors:
  - [abstract] "different models consistently rank text from various sources in the same order based on these geometric properties"
  - [section 4.1] Spearman correlations among all tester models exceed 0.947 minimum
  - [corpus] Rethinking LLM-as-a-Judge (arXiv:2601.22588) independently shows small models can leverage internal representations for evaluation
- Break condition: Would fail for severely undertrained models, languages/domains far from training distribution, or fundamentally different architectures not tested.

### Mechanism 2
- Claim: Effective Rank and Intrinsic Dimensionality serve as reference-free proxies for text naturalness by measuring representational diversity.
- Mechanism: Effective Rank computes entropy over normalized singular values. Human text produces higher ERank because tokens occupy more orthogonal directions in representation space. AI-generated text tends toward anisotropy (tokens cluster along fewer directions), yielding lower ERank. Strong negative correlation with GPT-Perplexity (ρ=-0.76) suggests isotropic representations associate with fluent outputs.
- Core assumption: Lower perplexity and higher semantic coherence correspond to text that humans perceive as more natural.
- Evidence anchors:
  - [section 3.5] Defines Effective Rank formula: ERank(X^(l)) = exp(-Σ p_k log p_k) where p_k = σ_k / Σ σ_i
  - [section 4.4] Figure 4 shows ERank correlates negatively with GPT-PPL (ρ=-0.76) and positively with BLEURT (ρ=0.40)
  - [corpus] SR-GRPO paper (arXiv:2512.02807) uses stable rank as a reward signal for alignment
- Break condition: Would fail for highly technical text where "naturalness" diverges from fluency, or for intentionally stylistic text with constrained forms.

### Mechanism 3
- Claim: MEV and Resultant Length detect synthetic text by measuring anisotropy—the degree to which representations collapse toward a dominant direction.
- Mechanism: MEV measures σ₁² / Σ σᵢ² (variance explained by first principal component). Human text shows lower MEV because representations are more evenly distributed across directions. Generated text exhibits higher anisotropy (MEV ~0.42-0.46 for synthetic vs. lower for original), reflecting model bias toward certain representational directions. Strong positive correlation with GPT-Perplexity (ρ=0.81) indicates anisotropic representations link to less fluent generation.
- Core assumption: Anisotropy in representations reflects token distribution patterns that distinguish human from machine-generated text.
- Evidence anchors:
  - [section 4.2] "metrics such as MEV and Resultant Length are lower for original text, reflecting its lower anisotropy compared to synthetic outputs"
  - [section 4.4] MEV correlates positively with GPT-PPL (ρ=0.81) and length variability
  - [corpus] Corpus does not contain direct replications of MEV-as-quality-proxy
- Break condition: Would fail for well-calibrated models specifically trained to reduce anisotropy, or for human text that is formulaic/repetitive.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: All geometric metrics except Resultant Length derive from SVD of the representation matrix X^(l). Understanding how singular values capture variance along orthogonal directions is essential to interpret MEV, Effective Rank, and Schatten norms.
  - Quick check question: If σ₁ accounts for 60% of total variance, what does that imply about MEV and representational diversity?

- Concept: Intrinsic Dimensionality estimation
  - Why needed here: The paper uses multiple ID estimators (MLE, MOM, MADA, CorrInt) as quality metrics. Understanding that ID estimates the manifold dimensionality of data—not just the ambient space dimension—is critical for interpreting why lower ID correlates with predictability.
  - Quick check question: Why might randomized text show higher ID than coherent text, according to Viswanathan et al. (2025)?

- Concept: Anisotropy in embedding spaces
  - Why needed here: The paper positions MEV and Resultant Length as anisotropy measures. Understanding why transformer embeddings tend toward anisotropy (tokens cluster in narrow cones) explains why synthetic text shows higher values.
  - Quick check question: In a perfectly isotropic representation space, what would Resultant Length equal?

## Architecture Onboarding

- Component map: Data pipeline -> Tester model T inference -> Metric computation R -> Aggregation -> Ranking/Correlation
- Critical path: Extracting hidden states at the correct location (MLP output, pre-residual) -> computing SVD accurately -> averaging across layers appropriately
- Design tradeoffs:
  - **Layer averaging vs. layer-specific analysis**: Averaging (Eq. 1) simplifies comparison but loses layer-wise patterns (Figures 8-9 show interesting mid-layer peaks for ERank)
  - **Small vs. large tester models**: Small models (0.5B) are efficient but may have less stable representations; paper shows remarkable consistency, but edge cases may exist
  - **Reference-free vs. reference-based**: Reference-free is the goal, but validation still requires correlation with reference-based metrics (BLEURT, MAUVE)
- Failure signatures:
  - Non-English text shows smaller gaps between human/synthetic metrics (Figure 5), indicating reduced sensitivity
  - Schatten norm correlates weakly with quality metrics (Figure 4), likely reflecting architectural effects rather than text quality
  - MAUVE shows weak correlations (|ρ| < 0.45) with geometric metrics, suggesting it captures orthogonal properties
- First 3 experiments:
  1. Reproduce ranking consistency: Select 2-3 tester models (different sizes) and 3-4 generator models; compute Effective Rank and MEV across layers; verify Spearman correlation > 0.90 between tester rankings
  2. Validate correlation with external metrics: For a fixed generator model, compute geometric metrics and traditional metrics (perplexity, BLEURT); replicate the correlation pattern in Figure 4 (ERank↔GPT-PPL negative, MEV↔GPT-PPL positive)
  3. Test language sensitivity: Compare metric gaps between human and generated text for English vs. a non-English language; expect smaller gaps for non-English per Figure 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do geometric metrics maintain their reliability for text quality evaluation across languages beyond English, German, and Russian, and why is the performance gap between human and synthetic text smaller for non-English languages?
- Basis in paper: [explicit] Authors state "the observed smaller performance gap between human and synthetic text for non-English languages suggests potential cultural or linguistic biases" and "validity of text quality evaluation for non-English text with proposed approach requires additional investigation."
- Why unresolved: Only three languages tested; German and Russian show attenuated metric gaps compared to English, but root cause unclear.
- What evidence would resolve it: Systematic evaluation across typologically diverse languages with analysis of whether the issue stems from multilingual model representations or linguistic properties.

### Open Question 2
- Question: Can geometric metrics reliably evaluate text quality in domains beyond movie reviews and generation tasks beyond paraphrasing?
- Basis in paper: [explicit] Authors acknowledge "their absolute reliability as universal proxies for text quality, especially across a wider array of languages and domains beyond English, German, and Russian movie reviews, requires further validation."
- Why unresolved: Experimental design constrained to single domain (movie reviews) and single task (rewriting while preserving meaning).
- What evidence would resolve it: Validation across diverse domains (scientific writing, news, code, dialogue) and diverse tasks (summarization, translation, open-ended generation).

### Open Question 3
- Question: Which specific geometric metrics or combinations thereof provide the most robust and generalizable proxy for text quality across varying model architectures?
- Basis in paper: [inferred] Paper tests 11 geometric metrics (Schatten, MEV, ERank, Resultant Length, MAUVE, MLE, MOM, MADA, CorrInt) but correlations with quality metrics vary substantially. The optimal metric selection remains undetermined.
- Why unresolved: Different metrics capture different aspects of representation geometry; no systematic ablation or combination analysis performed.
- What evidence would resolve it: Ablation study across metrics with formal comparison of predictive power for held-out quality benchmarks.

### Open Question 4
- Question: How does the correlation between geometric metrics and text quality scale with the number and diversity of generator models evaluated?
- Basis in paper: [explicit] Authors note "our correlation analysis... is based on aggregated scores from a relatively small set of generator models. This aggregation... limits the statistical power for calculating precise p-values and may mask model-specific nuances."
- Why unresolved: Only eight generator models tested; aggregation may obscure model-specific patterns.
- What evidence would resolve it: Large-scale study with dozens of generator models enabling per-model correlation analysis with proper statistical significance testing.

## Limitations
- Reduced sensitivity for non-English text suggests language-specific training distribution effects that could limit universal applicability
- Schatten norm results showing weak correlation with quality metrics indicate careful metric selection is required
- While metrics work as quality proxies, the mechanism by which they capture "naturalness" versus other text properties remains underspecified

## Confidence
- **High confidence**: Cross-model ranking consistency (ρ > 0.94) and correlation patterns with established metrics (ERank↔perplexity, MEV↔perplexity)
- **Medium confidence**: Universal applicability across languages and domains; mechanism explaining why anisotropy captures synthetic text detection
- **Medium confidence**: Practical implementation details (exact ID estimator parameters, layer selection strategy)

## Next Checks
1. **Language distribution sensitivity test**: Apply the geometric metrics to text from languages with varying degrees of representation in pre-training data; quantify the relationship between pre-training exposure and metric sensitivity to human vs. synthetic text differences.

2. **Domain generalization experiment**: Test the metrics on specialized domains (legal, medical, technical) where text naturalness may diverge from general fluency; measure whether ERank and ID still correlate with human quality judgments in these constrained contexts.

3. **Architecture boundary test**: Apply the methodology to non-transformer architectures (RNNs, CNNs, or hybrid models) to determine whether the claimed "universal" nature of geometric properties extends beyond the tested transformer-based models.