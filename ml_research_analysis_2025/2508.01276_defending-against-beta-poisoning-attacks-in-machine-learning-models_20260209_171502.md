---
ver: rpa2
title: Defending Against Beta Poisoning Attacks in Machine Learning Models
arxiv_id: '2508.01276'
source_url: https://arxiv.org/abs/2508.01276
tags:
- poisoning
- samples
- defense
- mean
- defenses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of defending against Beta Poisoning
  attacks, a type of poisoning attack that degrades model accuracy by making training
  data linearly nonseparable. The authors propose four defense strategies based on
  observed characteristics of poisoning samples, such as their close proximity to
  each other and their location near the mean of the target class.
---

# Defending Against Beta Poisoning Attacks in Machine Learning Models

## Quick Facts
- arXiv ID: 2508.01276
- Source URL: https://arxiv.org/abs/2508.01276
- Reference count: 31
- Primary result: Four defenses against Beta Poisoning achieve near-perfect accuracy/F1 scores on MNIST and CIFAR-10.

## Executive Summary
This paper addresses Beta Poisoning attacks that degrade model accuracy by making training data linearly nonseparable. The authors propose four defense strategies based on observed characteristics of poisoning samples, including their close proximity to each other and location near the mean of the target class. Experimental evaluations using MNIST and CIFAR-10 datasets demonstrate that KPB and MDT achieve perfect accuracy and F1 scores, while CBD and NCC also provide strong defensive capabilities. The paper provides insights into defense behaviors under varying conditions and identifies open questions for future research.

## Method Summary
The paper proposes four defenses against Beta Poisoning attacks: KPB (kNN Proximity-Based Defense), NCC (Neighborhood Class Comparison), CBD (Clustering-Based Defense), and MDT (Mean Distance Threshold). These defenses exploit observed characteristics of poisoning samples, particularly their tendency to cluster near the target class mean. The defenses use distance metrics, neighborhood analysis, and clustering to identify and filter suspicious samples. Experimental evaluations were conducted on MNIST and CIFAR-10 datasets with 20% poison rate, measuring accuracy, precision, recall, and F1-score of the defense mechanism itself.

## Key Results
- KPB and MDT defenses achieved perfect accuracy and F1 scores across all tested datasets and poison rates.
- CBD and NCC also demonstrated strong defensive capabilities, though with varying precision levels.
- NCC showed lower precision (0.55 on CIFAR-10) due to false positives on legitimate boundary samples.
- Performance degraded slightly on complex datasets like CIFAR-100, suggesting limitations in high-variance scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Based Filtering (KPB)
If poisoning samples are generated via linear combinations of prototypes, they exhibit higher local density (lower average distance to neighbors) than legitimate samples. KPB calculates the average distance of a sample to its k-nearest neighbors and filters out samples with unusually low average distances, under the assumption that these are the tightly clustered poisoning points.

### Mechanism 2: Target-Class Mean Localization (MDT)
If the attack optimizes likelihood P(x_p|y_t) using a Gaussian KDE, poisoning samples will congregate near the mean of the target class y_t. MDT computes the mean of the target class y_t and measures the distance of non-target class samples to this mean, flagging samples labeled y_nt that are anomalously close to the mean of y_t.

### Mechanism 3: Neighborhood Label Discrepancy (NCC)
If a poisoning sample is labeled y_nt but located near the mean of y_t, its immediate neighbors will share its label while its extended neighbors will differ. NCC compares the majority class label of the k-nearest neighbors against the 2k-nearest neighbors, flagging samples where there's a discrepancy in majority labels.

## Foundational Learning

- **Concept: Linear Separability**
  - Why needed here: Beta Poisoning specifically aims to degrade model accuracy by making the training dataset linearly nonseparable.
  - Quick check question: How does placing a sample with label y_nt near the mean of y_t disrupt the linear separability of the dataset?

- **Concept: Kernel Density Estimation (KDE)**
  - Why needed here: The Beta Poisoning attack uses Gaussian KDE to estimate P(x_p|y_t).
  - Quick check question: Why does optimizing for maximum likelihood under a Gaussian KDE inevitably pull poisoning samples toward the target class mean?

- **Concept: k-Nearest Neighbors (kNN)**
  - Why needed here: Three of the four defenses rely on distance calculations and neighborhood logic.
  - Quick check question: In the KPB defense, how does the choice of k affect the defense's ability to distinguish between a tight cluster of poisons and a dense cluster of legitimate data?

## Architecture Onboarding

- **Component map:** Input Dataset -> Pre-processing (mean calculation, kNN graph) -> Detectors (KPB, NCC, CBD, MDT) -> Filter (flagged samples) -> Output Sanitized Dataset
- **Critical path:** The determination of the distance threshold τ is the most critical step. For KPB and MDT, performance is binary: a τ set too low misses attacks; a τ set too high drops legitimate data.
- **Design tradeoffs:**
  - Precision vs. Recall: NCC offers high recall but lower precision. MDT/KPB offer perfect scores but may be brittle to distribution shifts.
  - Stability: CBD and MDT rely on the mean of the target class (y_t) because it is "unaffected" by the attack.
- **Failure signatures:**
  - High False Positives (NCC): Legitimate samples near the decision boundary are flagged.
  - Sensitivity to Distribution: On complex datasets, performance degrades as the "tight cluster" assumption weakens.
- **First 3 experiments:**
  1. Threshold Sweep (KPB/MDT): Run defenses on MNIST with τ ranging from 0 to 10 to replicate the "sweet spot" (3-5 for KPB, 5-7 for MDT).
  2. Cross-Dataset Generalization: Train defense parameters on MNIST and test directly on CIFAR-10 to test dataset-agnostic τ values.
  3. Class Overlap Stress Test: Visualize CBD/MDT performance on CIFAR-10 with high class overlap to observe break conditions.

## Open Questions the Paper Calls Out
- Can Beta Poisoning attacks be algorithmically modified to evade the proposed proximity and mean-distance based defenses?
- Are the proposed defenses effective against bilevel optimization-based poisoning attacks?
- How can the Neighborhood Class Comparison (NCC) defense be refined to reduce false positives for legitimate samples near decision boundaries?

## Limitations
- Performance may degrade on datasets with overlapping classes or high class variance.
- The assumption that target class mean is "unaffected" by attack may not hold under stronger or adaptive poisoning strategies.
- The exact number of prototypes (k) used in the Beta Poisoning attack is unspecified, affecting reproducibility.

## Confidence
- **High Confidence:** The core observation that poisoning samples cluster near the target class mean is well-supported by experiments.
- **Medium Confidence:** Claims of "perfect" scores may not generalize to more complex datasets with overlapping classes.
- **Low Confidence:** The robustness of mean-based defenses against adaptive attacks that target the mean itself is untested.

## Next Checks
1. Test MDT and CBD against an attacker that explicitly attempts to shift the target class mean during poisoning.
2. Train τ on MNIST and evaluate directly on CIFAR-100 to assess generalization without retraining.
3. Evaluate defenses on a binary subset of CIFAR-10 with classes known to overlap (e.g., cat vs. dog) to stress-test false positive rates.