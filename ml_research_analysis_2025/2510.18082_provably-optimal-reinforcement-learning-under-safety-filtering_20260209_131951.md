---
ver: rpa2
title: Provably Optimal Reinforcement Learning under Safety Filtering
arxiv_id: '2510.18082'
source_url: https://arxiv.org/abs/2510.18082
tags:
- safety
- safe
- policy
- learning
- filter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first formal proof that safety filtering
  in reinforcement learning (RL) does not inherently sacrifice asymptotic performance,
  resolving a long-standing misconception in safe RL. The authors formalize safety-critical
  RL using a Safety-Critical Markov Decision Process (SC-MDP) requiring categorical
  avoidance of failure states, and introduce a Filtered MDP where a perfect safety
  filter ensures all actions are safe.
---

# Provably Optimal Reinforcement Learning under Safety Filtering

## Quick Facts
- arXiv ID: 2510.18082
- Source URL: https://arxiv.org/abs/2510.18082
- Reference count: 40
- This paper proves safety filtering in RL doesn't inherently sacrifice asymptotic performance, resolving a long-standing misconception in safe RL.

## Executive Summary
This paper resolves a fundamental question in safe reinforcement learning: can safety filtering be applied without sacrificing asymptotic performance? The authors prove that a "perfect" safety filter—one that only blocks unsafe actions—enables standard RL algorithms to converge to the optimal safe policy while maintaining categorical safety guarantees. The key insight is that safety filtering acts as a time-invariant operator that preserves the convergence properties of RL algorithms while ensuring all actions remain within a maximal controlled-invariant safe set.

The framework introduces Safety-Critical MDPs (SC-MDPs) where failure states must be categorically avoided, and Filtered MDPs where a safety filter ensures all actions are safe. The main theoretical result proves that any optimal policy in the filtered MDP achieves the same asymptotic return as the best safe policy in the SC-MDP. Experiments on Safety Gymnasium validate these findings, showing zero safety violations with a rollout-based filter while matching or exceeding performance of baselines.

## Method Summary
The method trains RL policies in environments where all actions pass through a safety filter that guarantees categorical safety. The safety filter is implemented either as a value-based neural network approximating the safety value function from Hamilton-Jacobi reachability, or as a rollout-based filter that simulates H=100 future steps to verify safety. The task policy (SAC) proposes actions, which are then filtered to ensure safety before execution. Training uses standard RL algorithms without modification, and the same filter is deployed during execution.

## Key Results
- Safety filtering preserves asymptotic performance: optimal policies in filtered MDPs match best safe policies in SC-MDPs
- Standard RL convergence guarantees carry over to safety-filtered environments
- Rollout-based filter achieves zero safety violations in Safety Gymnasium experiments while matching or exceeding baseline performance
- Value-based filters can cause violations due to approximation errors, highlighting the importance of filter quality

## Why This Works (Mechanism)

### Mechanism 1: Safety-Performance Separation via Operator Equivalence
- Claim: Optimizing in filtered environment yields asymptotically optimal executed policy
- Mechanism: Perfect filter only modifies unsafe actions. Bellman operator for filtered MDP equals constrained optimal Bellman operator within safe set, sharing the same unique fixed point
- Core assumption: Filter is "perfect" or "least-restrictive" (Definition 2), allowing all safe actions and only blocking unsafe ones
- Evidence anchors:
  - [abstract] "any optimal policy in the filtered MDP... achieves the same asymptotic return as the best safe policy in the SC-MDP"
  - [page 15] "Taking the supremum over $a \in A$ post-composition by $\phi$ equals the supremum over $a \in A_{safe}(s)$. Therefore, we have $T^*_{M_\phi} = T^*_{MSC}$."

### Mechanism 2: Convergence Preservation via Stationary Transform
- Claim: Standard RL algorithms converge on safety-filtered environments without modification
- Mechanism: Safety filter introduces time-invariant mapping from proposed to executed actions. Filtered MDP remains stationary MDP with bounded rewards
- Core assumption: Filter $\phi$ and environment dynamics $P$ are time-invariant (Assumption 1.3, 1.4)
- Evidence anchors:
  - [page 6] "ALG also converges on the corresponding filtered MDP $M_\phi$."
  - [page 15] "Every step in the convergence proof of ALG on M applies to $M_\phi$ with the substitution $(P, r) \mapsto (P_\phi, r_\phi)$."

### Mechanism 3: Categorical Safety via Rollout Verification
- Claim: Rollout-based filter achieves zero safety violations
- Mechanism: Simulates proposed action followed by fallback policy for fixed horizon. Permits action only if simulation reaches verified safe terminal state
- Core assumption: Reliable forward model exists for rollout, and verified "stop" policy exists
- Evidence anchors:
  - [page 8] "valid safety filter... since coming to a stop is a known terminal safe state."
  - [page 9] "our rollout-based filter recorded zero safety violations in both tasks."

## Foundational Learning

- Concept: **Controlled Invariant Sets**
  - Why needed here: The entire theoretical guarantee hinges on the existence of a "Maximal Controlled-Invariant Safe Set" ($\Omega^*$)—the set of states from which the agent can forever avoid failure using some control policy
  - Quick check question: If an agent is inside a safe set but driving at full speed toward a wall with no braking distance left, is it in a controlled invariant set?

- Concept: **Bellman Operators**
  - Why needed here: The core proof (Mechanism 1) is established by equating the Bellman operator of the filtered environment with the constrained optimal operator
  - Quick check question: What property must a Bellman operator have to guarantee a unique optimal value function? (Answer: Contraction mapping)

- Concept: **Reachability Analysis**
  - Why needed here: The paper's "safety filter" is implemented using Hamilton-Jacobi reachability (or neural approximations), which computes the safe set $\Omega^*$
  - Quick check question: What does the safety value function represent in HJ reachability? (Answer: The minimum distance to the failure set over time, or the cost-to-go)

## Architecture Onboarding

- Component map:
  - Task Policy (SAC) -> Safety Filter -> Environment
  - Safety Filter -> Safety Value Network + Fallback Policy

- Critical path: The loop latency is dominated by the Safety Filter. If using a rollout filter with horizon H=100, you simulate 100 steps internally for every single real environment step. This is the bottleneck for real-time deployment.

- Design tradeoffs:
  - Value-based vs. Rollout Filter: Value-based is fast (single forward pass) but prone to approximation errors causing violations. Rollout is computationally expensive but provides hard guarantees (zero violations) if model is accurate
  - Permissiveness: More conservative filter is easier to train but reduces effective state space, potentially degrading task performance if it blocks critical maneuvers

- Failure signatures:
  - Chattering: Task Policy proposes $a_{bad}$, Filter forces $a_{safe}$. State changes slightly; Task Policy again proposes $a_{bad}$. Result: robot jitters or makes no progress
  - Drift Violations: Value-based filter neural network may fail to generalize to OOD states, causing agent to slip into failure set

- First 3 experiments:
  1. Sanity Check (Tabular/Grid): Verify Q-values of filtered MDP converge to constrained optimal Q-values in small gridworld where "perfect filter" is trivial to define
  2. Latency Profiling: Benchmark interaction frequency (FPS) when using Rollout Filter (H=100) vs. Value-based Filter vs. No Filter to quantify compute overhead
  3. Ablation on Filter Conservativeness: Artificially restrict safe set (e.g., increase safety margin δ) to verify if performance degrades, confirming sensitivity to "least-restrictive" assumption

## Open Questions the Paper Calls Out
- Characterizing and optimizing the convergence rate for reinforcement learning under safety filtering remains an important direction for future work
- Extending the framework to handle approximate (imperfect) safety filters rather than theoretical "perfect" filters
- Generalizing the approach to scenarios where initialization occurs outside the maximal controlled-invariant safe set

## Limitations
- Critical dependence on filter perfection: theoretical guarantees require "perfect" filter that may not be achievable in practice
- Computational overhead: rollout-based filter requires simulating 100 future steps per real environment interaction
- Model dependence: rollout filter requires accurate forward dynamics model that may not be available in real-world systems

## Confidence
- High Confidence: Theoretical framework and proofs connecting filtered MDPs to safety-constrained MDPs are mathematically rigorous
- Medium Confidence: Experimental validation on Safety Gymnasium convincingly shows zero violations with rollout filtering and performance parity with baselines
- Low Confidence: Practical recipe for deployment assumes availability of perfect safety filters, which may not be achievable in real-world applications

## Next Checks
1. Filter conservativeness ablation: Systematically vary safety margin parameter δ and measure trade-off between violation frequency and task performance
2. Real-world dynamics validation: Implement safety filter using learned dynamics model rather than ground-truth simulation to evaluate model error propagation
3. Transfer to different safety specifications: Test framework on tasks with different failure modes beyond proximity to static obstacles to assess generality of safety-performance separation claim