---
ver: rpa2
title: Probabilistic Multi-Agent Aircraft Landing Time Prediction
arxiv_id: '2512.08281'
source_url: https://arxiv.org/abs/2512.08281
tags:
- aircraft
- prediction
- landing
- traffic
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic multi-agent aircraft landing
  time prediction framework that jointly estimates landing times and associated uncertainties
  for multiple aircraft. The approach extends a Multi-Agent Inverted Transformer (MAIFormer)
  architecture with a Gaussian Parameter Decoder to output landing time distributions
  instead of point estimates.
---

# Probabilistic Multi-Agent Aircraft Landing Time Prediction

## Quick Facts
- arXiv ID: 2512.08281
- Source URL: https://arxiv.org/abs/2512.08281
- Reference count: 40
- Multi-agent landing time prediction with uncertainty, MAE 6.19s, perfect sequence consistency

## Executive Summary
This paper introduces a probabilistic multi-agent aircraft landing time prediction framework that jointly estimates landing times and associated uncertainties for multiple aircraft. The approach extends a Multi-Agent Inverted Transformer (MAIFormer) architecture with a Gaussian Parameter Decoder to output landing time distributions instead of point estimates. Using ADS-B trajectory data from Incheon International Airport, the model significantly outperforms baseline methods including linear regression, LightGBM, and XGBoost. The proposed model achieves a mean absolute error of 6.19 seconds for landing time prediction and perfect arrival sequence consistency (Spearman's ρ = 1.0, Kendall's τ = 1.0), demonstrating superior accuracy and reliability. The model's agent attention scores provide interpretable insights into air traffic control patterns and decision-making processes.

## Method Summary
The model processes N aircraft × T timesteps × 3 features (lat, lon, alt) through an MAIFormer encoder with masked multivariate attention for individual trajectory learning and agent attention for inter-aircraft interactions. An Inverted Embedding technique converts input dimensions to enable efficient self-attention. The Gaussian Parameter Decoder outputs mean and standard deviation for each aircraft's remaining flight time using 4 linear layers. Training uses NLL loss to balance accuracy and uncertainty calibration. The model was trained on 8 months of Incheon arrival data with sliding window preprocessing and validated against MLR, LightGBM, and XGBoost baselines.

## Key Results
- MAE of 6.19 seconds for landing time prediction
- Perfect arrival sequence consistency (Spearman's ρ = 1.0, Kendall's τ = 1.0)
- Significantly outperforms linear regression, LightGBM, and XGBoost baselines
- Predicted uncertainties effectively captured operational complexity and scenario-dependent variability

## Why This Works (Mechanism)

### Mechanism 1: Dual-Attention Separation for Spatio-Temporal and Social Learning
- Claim: Separating individual trajectory learning from inter-agent interaction learning improves both prediction accuracy and interpretability.
- Mechanism: Masked multivariate attention restricts each aircraft to attend only to its own variates (lat/lon/alt), preventing cross-contamination during individual trajectory encoding. Agent attention then operates on concatenated agent tokens to capture ATCo-mediated interactions like radar vectoring and sequencing.
- Core assumption: Aircraft trajectories are influenced by two distinct processes—individual kinematics and social/ATCo interventions—that benefit from separate encoding stages.
- Evidence anchors: [abstract] "aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring"; [Section II.B.2] "the masked multivariate attention aims to model the movement of each individual aircraft, whereas the agent attention aims to capture the social interactions between them"

### Mechanism 2: Negative Log-Likelihood Training for Calibrated Uncertainty
- Claim: Training with NLL rather than point-estimation loss produces uncertainty estimates that reflect operational complexity.
- Mechanism: NLL loss decomposes into an uncertainty penalty (discouraging inflated variance) and prediction error (accuracy term normalized by variance). This forces the model to output larger variances only when prediction errors are genuinely high.
- Core assumption: Landing time distributions can be approximated as unimodal Gaussian distributions per aircraft, with conditional independence across agents.
- Evidence anchors: [Section II.B.4] "The NLL loss can be interpreted as the sum of the two components... balance the trade-off between accuracy and uncertainty"; [Section IV.B] "predicted uncertainties effectively captured the inherent variability... unstructured scenario produced considerably higher uncertainty"

### Mechanism 3: Layer-wise Coarse-to-Fine Attention Refinement
- Claim: Stacked attention layers exhibit hierarchical behavior—early layers focus on local traffic complexity, deeper layers refine based on aircraft with similar predicted landing times.
- Mechanism: Each layer's agent attention scores reveal what the model attends to. Layer 1 attends to aircraft near the airport; Layers 2-3 shift attention to aircraft with similar predicted remaining flight times.
- Core assumption: ATCo decision-making follows a hierarchical pattern (coarse situational awareness → fine-grained sequencing).
- Evidence anchors: [Section IV.C] "the first layer contributes to coarse-grained predictions, whereas the latter layers refine the prediction at a fine-grained level"; [Figure 8] Layer-wise attention visualizations showing attention shift from nearby aircraft to similarly-timed aircraft

## Foundational Learning

- Concept: Self-Attention and Transformer Architecture
  - Why needed here: The entire MAIFormer encoder relies on self-attention; understanding Q/K/V projections and masking is essential for debugging attention patterns.
  - Quick check question: Can you explain why masking prevents Agent 1's variates from attending to Agent 2's variates in Equation 6?

- Concept: Probabilistic Regression vs. Point Estimation
  - Why needed here: The model outputs distribution parameters (μ, σ) rather than single values; understanding NLL loss and uncertainty calibration is critical.
  - Quick check question: Why does NLL penalize both large errors AND large variances, and how does this differ from MSE loss?

- Concept: Multi-Agent Systems and Air Traffic Control Operations
  - Why needed here: Interpreting results requires understanding why aircraft influence each other (separation assurance, wake turbulence categories, radar vectoring).
  - Quick check question: Why would two aircraft at similar distances from the airport have different predicted landing times in a multi-agent scenario?

## Architecture Onboarding

- Component map: Input (N×T×3) → Inverted Embedding (N×3×T) → Scene Embedding + Type Embedding → [L×] Masked Multivariate Attention → Agent Attention → LayerNorm → FFN → Gaussian Parameter Decoder (4 linear layers: 768→512→256→128→2) → Output: (μ₁,σ₁), (μ₂,σ₂), ..., (μₙ,σₙ)

- Critical path: The mask matrix M in Equation 6 is the key implementation detail—if masking is incorrect, cross-aircraft contamination will degrade individual trajectory learning.

- Design tradeoffs:
  - Marginal vs. joint distribution: GPD assumes conditional independence; full covariance modeling would capture inter-agent uncertainty correlations but increases complexity from O(N) to O(N²).
  - Unimodal Gaussian vs. mixture models: Simpler implementation but may miss multimodal patterns (e.g., holding decisions).
  - Number of layers L=3: Chosen empirically; deeper may overfit, shallower may miss hierarchical patterns.

- Failure signatures:
  - Constant σ predictions: NLL not balancing properly; check learning rate scheduling and variance initialization.
  - Poor sequence consistency despite low MAE: Model accurate on average but missing interaction patterns; verify agent attention is functioning.
  - Attention collapse (uniform attention): Agent attention not discriminating; may indicate insufficient training data diversity.

- First 3 experiments:
  1. **Ablation on attention masking**: Remove mask M to verify dual-attention contribution (expect MAE degradation and sequence consistency drop).
  2. **Distribution assumption test**: Replace Gaussian with mixture of Gaussians or quantile regression on a subset; compare calibration using reliability diagrams.
  3. **Attention validation**: If possible, compare layer-wise attention patterns against domain expert annotations of which aircraft ATCos prioritize in similar scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can replacing the unimodal Gaussian assumption with multimodal or asymmetric distribution models improve prediction accuracy in complex terminal operations?
- Basis in paper: [explicit] The authors state, "we assume that the predicted landing times follow a unimodal Gaussian distribution. However, in practice, the landing time distribution may exhibit multimodal or asymmetric characteristics."
- Why unresolved: The current Gaussian Parameter Decoder is architecturally constrained to output a single mean and variance, preventing the capture of multi-modal outcomes.
- What evidence would resolve it: Comparative results using Mixture Density Networks or quantile regression on the same Incheon dataset showing better fit for non-Gaussian ground truth data.

### Open Question 2
- Question: Do the agent attention scores learned by the model align with the actual visual attention and decision-making focus of human Air Traffic Controllers?
- Basis in paper: [explicit] The authors note that "attention scores should be validated against human air traffic controllers’ attention patterns, such as those measured through eye-tracking experiments, to confirm their alignment."
- Why unresolved: The paper claims interpretability based on logical consistency with ATC actions (e.g., shortcuts), but lacks empirical comparison to human cognitive data.
- What evidence would resolve it: A user study correlating the model's attention heatmaps with eye-tracking data from controllers managing identical traffic scenarios.

### Open Question 3
- Question: To what extent does incorporating meteorological data (e.g., wind vectors, storm cells) improve probabilistic landing time accuracy compared to trajectory-only inputs?
- Basis in paper: [explicit] The conclusion states, "our model was developed solely using trajectory data. Incorporating additional factors, such as weather information, may further improve the predictive performance."
- Why unresolved: While the analysis suggests weather impacts uncertainty (e.g., Figure 7), the model currently lacks explicit weather features to quantify this relationship.
- What evidence would resolve it: Ablation studies on the Incheon dataset augmented with AMOS weather data, demonstrating reduced error or better-calibrated uncertainty during adverse conditions.

## Limitations

- External validity limited to single airport (ICN) with specific arrival flow structure; performance on different airport configurations untested.
- Gaussian distributional assumption may not capture multimodal or skewed landing time distributions from complex operations like holding patterns.
- Interpretability claims based on attention patterns lack external validation against human ATCo eye-tracking or cognitive models.

## Confidence

**High confidence**: The model architecture is clearly specified and implements established techniques (MAIFormer with NLL training). The quantitative results (MAE, sequence metrics) are directly computed from the test set and show clear improvement over baselines.

**Medium confidence**: The mechanism explanations for why dual-attention and NLL training work are theoretically sound but lack extensive empirical validation beyond the presented results. The attention interpretability findings are suggestive but not conclusively proven.

**Low confidence**: Claims about operational interpretability and ATCo decision-making patterns require external validation that is not provided. The distributional assumption validity is asserted but not tested against alternatives.

## Next Checks

1. **External airport validation**: Evaluate the trained model on arrival data from a different airport with contrasting characteristics (e.g., multiple runways, different traffic patterns) to assess generalizability beyond Incheon.

2. **Distribution assumption testing**: Replace the Gaussian decoder with a mixture density network or quantile regression approach on a subset of data, then compare calibration using reliability diagrams and proper scoring rules.

3. **Attention pattern validation**: If operational data permits, compare the layer-wise attention patterns against human ATCo eye-tracking data or expert annotations of which aircraft receive priority in similar scenarios.