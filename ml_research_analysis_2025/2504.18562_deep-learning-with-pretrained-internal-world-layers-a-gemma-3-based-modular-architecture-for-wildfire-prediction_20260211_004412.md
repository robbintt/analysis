---
ver: rpa2
title: 'Deep Learning with Pretrained ''Internal World'' Layers: A Gemma 3-Based Modular
  Architecture for Wildfire Prediction'
arxiv_id: '2504.18562'
source_url: https://arxiv.org/abs/2504.18562
tags:
- wildfire
- learning
- layers
- internal
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a modular deep learning architecture that
  reuses frozen middle layers of the pretrained Gemma 3 model to predict wildfire
  occurrences. The method projects tabular environmental data into Gemma's hidden
  space using a lightweight, trainable input network, then passes it through two frozen
  transformer layers, and finally maps the output to a binary prediction.
---

# Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction

## Quick Facts
- arXiv ID: 2504.18562
- Source URL: https://arxiv.org/abs/2504.18562
- Reference count: 40
- Primary result: 0.9433 recall, 0.8838 F1, 0.9344 AUC on Moroccan wildfire dataset using 5M trainable params

## Executive Summary
This paper introduces a modular deep learning architecture that leverages frozen middle layers of the pretrained Gemma 3 model to predict wildfire occurrences from tabular environmental data. The method projects features into Gemma's hidden space using a lightweight trainable input network, then passes them through two frozen transformer layers before mapping to binary predictions. Evaluations show the approach achieves high recall and F1 scores while using only 5 million trainable parameters out of 37.7 million total, outperforming several baselines. The results demonstrate that reusing pretrained transformer layers can improve sensitivity and data efficiency for wildfire forecasting.

## Method Summary
The architecture processes 276 tabular environmental features through a 4-branch PMFFNN input network that splits features into groups of ~69, processes each branch through dense layers, and concatenates to 1152 dimensions matching Gemma 3's hidden size. This output passes through three trainable FFN layers before being reshaped into a pseudo-sequence and processed by frozen Gemma 3 decoder layers 8-9 using grouped-query attention. A classifier head then maps the output to wildfire probability. Training uses dual AdamW optimizers with weighted binary cross-entropy, mixed precision, and early stopping on a Moroccan wildfire dataset with 30-day sliding windows.

## Key Results
- Achieves 0.9433 recall, 0.8838 F1, and 0.9344 AUC on 2022 holdout set
- Uses only 5 million trainable parameters (13.3% of total) while maintaining high performance
- Outperforms FFN baseline (113K params) and other methods like CNN and RNN variants
- Ablation studies confirm frozen transformer layers consistently improve representations

## Why This Works (Mechanism)

### Mechanism 1
Frozen pretrained transformer mid-layers provide transferrable relational inductive biases that improve feature representations for tabular environmental prediction. Gemma 3's layers 8-9, trained on diverse multimodal data, encode generalizable attention patterns that reweight and contextualize input features without parameter updates. The frozen layers act as a fixed "feature refiner" operating on projected embeddings.

### Mechanism 2
Parallel multi-path input projection reduces information bottleneck when mapping heterogeneous tabular features to a unified hidden space. The PMFFNN splits 276 features into 4 parallel branches (~69 features each), processes each through Dense→ReLU→LayerNorm, then concatenates to 1152-dim output. This preserves feature-group structure rather than flattening all features through a single dense path.

### Mechanism 3
Extreme parameter freezing (86.7% frozen) acts as regularization, enabling high recall on limited wildfire data without overfitting. Only 5M of 37.7M parameters update during training. Frozen weights prevent gradient-driven adaptation to noise, forcing trainable layers to learn task-specific mappings while relying on fixed pretrained representations for feature transformation.

## Foundational Learning

- **Transfer learning with frozen backbones**: Understanding that pretrained weights can be reused without updates requires distinguishing between feature extraction (frozen) and fine-tuning (unfrozen). Quick check: Can you explain why freezing parameters reduces overfitting risk on small datasets?
- **Transformer attention mechanics**: The paper uses Gemma 3 decoder layers; understanding multi-head self-attention, key/query/value projections, and residual connections is necessary to debug the frozen layer's behavior. Quick check: What does the attention operation compute, and how does LayerNorm stabilize training?
- **Class-imbalanced binary classification**: Wildfire occurrence is rare; the paper uses weighted BCE (w1=2.0) and undersampling. Understanding precision/recall tradeoffs is critical for interpreting F1 and AUC. Quick check: If recall increases but precision drops, what does that imply about the classifier's threshold behavior?

## Architecture Onboarding

- **Component map**: Tabular tensor (B, 276) → 4-branch PMFFNN → concat (B, 1152) → 3-layer FFN → projection → frozen Gemma-3 layers 8-9 → classifier MLP → sigmoid
- **Critical path**: Feature projection alignment to H=1152 is non-negotiable (Gemma hidden size); temporal window w=30 days must match training data format; frozen layer weights must be loaded exactly from Gemma 3-1B checkpoint
- **Design tradeoffs**: Recall vs. precision (Internal World model optimizes for high recall at cost of lower precision); parameter budget (5M trainable params enables training on 8GB GPU but limits model capacity); frozen vs. fine-tuned (freezing avoids catastrophic forgetting but may underperform on out-of-distribution conditions)
- **Failure signatures**: Domain gap (Gemma's pretraining may encode irrelevant priors for North African meteorology); label noise (MODIS/VIIRS omission errors on cloudy days propagate to targets); temporal granularity (30-day window may miss multi-seasonal fuel accumulation dynamics)
- **First 3 experiments**: 1) Baseline sanity check: Train FFN-3L from scratch on same data to verify data pipeline; 2) Ablation of frozen layers: Replace Gemma layers 8-9 with identity mapping or random initialized transformer layers; 3) Layer depth sweep: Test layers 6-7, 10-11, or single layer (8 only) to identify optimal frozen depth

## Open Questions the Paper Calls Out

- Are the performance gains specific to Gemma 3, or do they generalize to other large pretrained transformers (e.g., LLaMA-3, Mixtral)? The study exclusively utilizes Gemma 3-1B, leaving the transferability of the "internal world" hypothesis to other model families untested.
- Does low-rank adaptation (LoRA) of the frozen internal layers yield better calibration or accuracy than strictly freezing them? The current methodology keeps the transformer blocks completely frozen to minimize overfitting risk and parameter count.
- Does expanding the temporal input window beyond 30 days improve the modeling of multi-seasonal fuel dynamics? The experimental setup is constrained to a fixed 30-day sliding window, which may miss long-range temporal dependencies.

## Limitations

- Domain transfer risk: The paper assumes Gemma 3's pretrained attention patterns generalize to Moroccan environmental data, but no ablation demonstrates that frozen layers are causally responsible for the recall gain
- Architectural sensitivity: The 4-branch PMFFNN design choice lacks external validation—no comparison to single-branch projection or alternative feature-groupings is provided
- Label quality constraints: MODIS/VIIRS-derived wildfire labels are subject to omission errors under cloud cover, potentially biasing the 2022 holdout set

## Confidence

- **High Confidence**: Parameter efficiency claims and reproducibility of training setup
- **Medium Confidence**: The claim that frozen Gemma layers improve recall/F1 over baselines rests on ablation studies within the paper but lacks external validation
- **Low Confidence**: The mechanism that mid-layer frozen transformers encode transferable "internal world" representations for tabular environmental prediction is largely theoretical

## Next Checks

1. **Ablation of Frozen Layers**: Replace Gemma layers 8-9 with either identity mapping or randomly initialized transformer layers; retrain and compare recall/F1 to determine if pretrained weights are necessary
2. **Single-Dataset Cross-Validation**: Apply the same frozen-layer architecture to a different environmental dataset (e.g., California wildfire data) to test domain generalization of the "internal world" hypothesis
3. **Layer Depth Sensitivity**: Systematically test freezing layers 6-7, 10-11, or single layer (8 only) instead of layers 8-9; record AUC, recall, and training stability to identify optimal frozen depth