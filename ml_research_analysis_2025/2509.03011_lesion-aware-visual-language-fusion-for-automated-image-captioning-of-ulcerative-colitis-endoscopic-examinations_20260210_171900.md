---
ver: rpa2
title: Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative
  Colitis Endoscopic Examinations
arxiv_id: '2509.03011'
source_url: https://arxiv.org/abs/2509.03011
tags:
- captioning
- image
- attention
- grad-cam
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a lesion-aware image captioning framework\
  \ for ulcerative colitis (UC) endoscopy that integrates ResNet feature extraction,\
  \ Grad-CAM lesion localization, and CBAM-enhanced attention with a T5-based decoder.\
  \ Clinical metadata\u2014such as MES scores, vascular patterns, and bleeding indicators\u2014\
  are injected as structured natural-language prompts to guide caption generation."
---

# Lesion-Aased Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations

## Quick Facts
- **arXiv ID**: 2509.03011
- **Source URL**: https://arxiv.org/abs/2509.03011
- **Reference count**: 21
- **Primary result**: Introduces lesion-aware image captioning for UC endoscopy with 84.7% MES classification accuracy and BLEU-4 of 0.87

## Executive Summary
This work presents a novel lesion-aware image captioning framework for ulcerative colitis (UC) endoscopic examinations that integrates deep visual feature extraction, lesion localization, and clinical metadata conditioning. The system combines ResNet for feature extraction, Grad-CAM for lesion detection, and CBAM-enhanced attention mechanisms with a T5-based decoder to generate structured, clinically relevant captions. Clinical metadata such as MES scores, vascular patterns, and bleeding indicators are injected as structured natural-language prompts to guide caption generation. The model demonstrates significant improvements in both MES classification (84.7% accuracy) and captioning quality (BLEU-4 of 0.87, ROUGE-L of 0.85) compared to existing baselines.

## Method Summary
The framework employs a multi-stage approach combining visual feature extraction, lesion localization, and text generation. ResNet extracts deep visual features from endoscopic images, while Grad-CAM identifies and highlights lesion regions. These localized features are processed through CBAM (Convolutional Block Attention Module) to enhance lesion-aware attention mechanisms. Clinical metadata, including Mayo Endoscopic Subscore (MES), vascular patterns, and bleeding indicators, is formatted as structured natural-language prompts and fed into the T5-based decoder. The decoder generates structured captions that align with clinical practice, supporting both MES classification and lesion tagging tasks. The system is trained on a 2,355-image UC dataset and validated through ablation studies and clinical metric evaluations.

## Key Results
- Achieves 84.7% MES classification accuracy, surpassing prior baselines (77.8%)
- Improves captioning quality with BLEU-4 of 0.87 and ROUGE-L of 0.85
- Ablation studies confirm lesion-aware attention and metadata conditioning as key performance drivers
- Generates structured, interpretable descriptions aligned with clinical practice

## Why This Works (Mechanism)
The system leverages multimodal fusion by combining visual features, lesion localization, and clinical metadata to generate context-aware captions. ResNet extracts hierarchical visual features, while Grad-CAM identifies lesion regions, providing spatial attention. CBAM enhances this attention by recalibrating feature maps, focusing on clinically relevant regions. The T5 decoder integrates these visual cues with structured clinical metadata (e.g., MES scores, vascular patterns) formatted as natural-language prompts. This conditioning ensures captions are both descriptive and clinically actionable. The lesion-aware attention mechanism prioritizes abnormal regions, improving the relevance and accuracy of generated descriptions.

## Foundational Learning
- **ResNet feature extraction**: Extracts hierarchical visual features from endoscopic images. *Why needed*: Captures fine-grained visual details for accurate lesion representation. *Quick check*: Verify feature maps highlight mucosal abnormalities.
- **Grad-CAM lesion localization**: Generates heatmaps highlighting lesion regions. *Why needed*: Provides spatial attention for lesion-aware processing. *Quick check*: Confirm heatmaps align with clinically annotated lesions.
- **CBAM attention enhancement**: Re-calibrates feature maps to emphasize lesion regions. *Why needed*: Improves focus on clinically relevant areas. *Quick check*: Validate attention weights correlate with lesion severity.
- **T5-based decoder with metadata conditioning**: Integrates visual and clinical data to generate structured captions. *Why needed*: Ensures captions are clinically interpretable and actionable. *Quick check*: Compare generated captions against clinical reports for accuracy.

## Architecture Onboarding

**Component map**: ResNet -> Grad-CAM -> CBAM -> T5 Decoder

**Critical path**: Image input → ResNet feature extraction → Grad-CAM lesion localization → CBAM attention enhancement → T5 decoder with metadata conditioning → Structured caption output

**Design tradeoffs**: 
- ResNet provides robust feature extraction but may miss subtle lesions; Grad-CAM addresses this by localizing abnormalities.
- CBAM enhances attention but adds computational overhead; its benefits outweigh costs for clinical accuracy.
- T5 decoder with metadata conditioning ensures clinical relevance but requires structured metadata, limiting flexibility in unstructured settings.

**Failure signatures**:
- Misaligned Grad-CAM heatmaps lead to irrelevant captions.
- Over-reliance on metadata may produce generic descriptions if visual cues are weak.
- CBAM may amplify noise in low-quality images, degrading caption quality.

**Exactly 3 first experiments**:
1. Validate Grad-CAM heatmaps against clinical annotations to ensure lesion localization accuracy.
2. Test caption quality with and without CBAM to quantify attention enhancement benefits.
3. Evaluate T5 decoder performance with varying levels of metadata completeness to assess robustness.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Clinical validation beyond MES classification is limited, raising questions about generalizability to other UC phenotypes.
- Evaluation metrics (BLEU-4, ROUGE-L) are standard for image captioning but their clinical interpretability for UC reporting is unverified.
- The reliance on structured metadata as natural-language prompts may limit flexibility in real-world clinical settings where such metadata is not always available or standardized.

## Confidence
- **High**: The model's architecture (ResNet + Grad-CAM + CBAM + T5) is technically sound and the reported MES classification accuracy (84.7%) is a clear, measurable improvement over baselines (77.8%).
- **Medium**: The integration of clinical metadata and lesion-aware attention is innovative and supported by ablation studies, but the clinical utility of the generated captions beyond MES classification is not fully demonstrated.
- **Low**: The generalizability of the model to other UC phenotypes, endoscopic contexts, or real-world clinical workflows is uncertain due to limited validation.

## Next Checks
1. **Clinical annotation study**: Conduct a blinded study with gastroenterologists to assess the clinical accuracy and utility of the generated captions compared to manual reports.
2. **Cross-dataset evaluation**: Test the model on external UC datasets or other gastrointestinal conditions to evaluate generalizability and robustness.
3. **Component ablation with larger datasets**: Perform a more granular ablation study on larger, more diverse datasets to isolate the contribution of each model component (e.g., Grad-CAM, CBAM, metadata conditioning) to overall performance.