---
ver: rpa2
title: 'Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory
  Insights and Recommendations'
arxiv_id: '2505.06620'
source_url: https://arxiv.org/abs/2505.06620
tags:
- clinicians
- clinical
- explanations
- these
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the integration of explainable AI (XAI) in
  medical devices, focusing on enhancing trust and safety in clinical decision support
  systems (CDSS). An expert working group, including clinicians, regulators, and data
  scientists, evaluated AI models and XAI methods for predicting heart attack risk.
---

# Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations

## Quick Facts
- arXiv ID: 2505.06620
- Source URL: https://arxiv.org/abs/2505.06620
- Reference count: 40
- Key outcome: XAI methods improved clinician trust and diagnostic accuracy, but over-reliance on AI outputs raised safety concerns; simpler models with effective XAI and comprehensive training are recommended.

## Executive Summary
This study examined the integration of explainable AI (XAI) in medical devices, focusing on enhancing trust and safety in clinical decision support systems (CDSS). An expert working group, including clinicians, regulators, and data scientists, evaluated AI models and XAI methods for predicting heart attack risk. Key findings revealed that while XAI methods improved clinician trust and diagnostic accuracy, over-reliance on AI outputs posed safety concerns. Regulators emphasized the importance of interpretability and model simplicity, while clinicians preferred clear explanations aligned with clinical knowledge. The study recommended using simpler models with effective XAI, ensuring comprehensive stakeholder training, and validating AI tools against clinical standards to ensure safe and effective integration into healthcare settings.

## Method Summary
The study involved an expert working group of eight clinicians, two regulators, and two data scientists. They evaluated AI models and XAI methods for predicting heart attack risk. The working group analyzed local and global explanation methods, including LIME, ExMatrix, and surrogate models, to understand their impact on clinician trust and diagnostic accuracy. The study highlighted the need for effective explanations that align with clinical knowledge and the importance of model interpretability for regulatory compliance.

## Key Results
- XAI methods improved clinician trust and diagnostic accuracy in heart attack risk prediction.
- Over-reliance on AI outputs posed safety concerns, highlighting the need for balanced AI-CDSS integration.
- Regulators emphasized interpretability and model simplicity, while clinicians preferred explanations aligned with clinical knowledge.

## Why This Works (Mechanism)
The study's approach works by bridging the gap between technical AI capabilities and clinical needs through multidisciplinary collaboration. By involving clinicians, regulators, and data scientists, the research ensures that XAI methods are both technically sound and clinically relevant. The focus on model interpretability and explanation clarity helps align AI outputs with clinical decision-making processes, fostering trust and safety.

## Foundational Learning
- **XAI Methods**: Techniques like LIME and SHAP provide local explanations, helping clinicians understand AI predictions. Why needed: To build trust and enable informed clinical decisions. Quick check: Compare explanation clarity and impact on clinician trust across methods.
- **Model Interpretability**: Simpler models with clear explanations are preferred by regulators and clinicians. Why needed: To ensure regulatory compliance and clinical usability. Quick check: Evaluate model complexity versus explanation effectiveness.
- **Stakeholder Training**: Comprehensive training programs are essential for effective AI-CDSS integration. Why needed: To mitigate over-reliance risks and promote balanced AI use. Quick check: Assess training impact on clinician-AI interaction quality.

## Architecture Onboarding
- **Component Map**: AI Model -> XAI Method -> Clinician Interface -> Clinical Decision
- **Critical Path**: AI model prediction -> XAI explanation generation -> Clinician review -> Clinical decision
- **Design Tradeoffs**: Simpler models vs. complex models with better performance; clear explanations vs. detailed technical insights
- **Failure Signatures**: Over-reliance on AI outputs, misinterpretation of explanations, regulatory non-compliance
- **First Experiments**: 1) Compare clinician trust with different XAI methods. 2) Evaluate model interpretability impact on regulatory approval. 3) Assess training effectiveness on AI-CDSS integration.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How do clinicians' years of experience and background knowledge of AI specifically influence their interaction with and trust in AI-CDSS?
- **Basis in paper:** [explicit] The authors state in the Discussion, "Future work will include a pilot study with a broader scope to assess how clinicians’ experience levels and background knowledge of AI affect their interaction with AI-CDSS."
- **Why unresolved:** The current pilot study involved a small sample of eight clinicians, which did not allow for stratification or statistical analysis based on seniority or technical literacy.
- **What evidence would resolve it:** A large-scale user study that segments participants by experience level and prior AI exposure, comparing their diagnostic accuracy and trust calibration metrics.

### Open Question 2
- **Question:** Do the observed trust calibration errors and automation bias persist or evolve during longitudinal use of AI-CDSS in clinical settings?
- **Basis in paper:** [explicit] The Discussion notes, "Observing practitioners’ interactions with these systems over a longer period will also be beneficial to address potential challenges and opportunities in AI/XAI medical devices."
- **Why unresolved:** The study utilized a one-off interaction design where clinicians reviewed cases sequentially, failing to capture how trust and reliance change over time with repeated exposure.
- **What evidence would resolve it:** Longitudinal field studies or randomized trials tracking clinician behavior and agreement rates with AI over weeks or months of active use.

### Open Question 3
- **Question:** What visualization formats or summary styles can deliver clinically relevant explanations without imposing the high time costs associated with methods like LIME?
- **Basis in paper:** [inferred] The results section highlights that "Clinicians argued that... these visualizations would require significant time to interpret... This complexity could hinder their practical use in fast-paced clinical environments."
- **Why unresolved:** While the study identified that current visualizations (e.g., LIME bars, ExMatrix) are time-consuming, it did not test alternative, more concise summary interfaces.
- **What evidence would resolve it:** A comparative study measuring time-to-diagnosis and cognitive load for various XAI visualization formats versus the standard LIME/feature importance plots.

### Open Question 4
- **Question:** Can advanced methods like SHAP or restricted counterfactual generators (e.g., DiCE) overcome the stability issues and impractical recommendations found in LIME and ExMatrix?
- **Basis in paper:** [explicit] The authors suggest in the Discussion that "employing other well-known methods for local explanations, such as SHAP... and counterfactual methods like DiCE, which allow restrictions... could be beneficial."
- **Why unresolved:** The study identified stability issues with LIME and the generation of impossible counterfactuals (e.g., changing age) with ExMatrix, but did not evaluate the proposed alternatives.
- **What evidence would resolve it:** Benchmarking experiments comparing the stability and clinical feasibility of counterfactuals generated by DiCE against ExMatrix.

## Limitations
- The study's generalizability is limited by its focus on a single heart attack risk prediction use case.
- The expert working group composition may not fully represent all stakeholder perspectives in diverse clinical settings.
- The study does not address long-term safety outcomes or real-world performance variations across different healthcare environments.

## Confidence
- High confidence: XAI methods can improve clinician trust and diagnostic accuracy
- Medium confidence: Over-reliance on AI outputs poses safety concerns
- Medium confidence: Simpler models with effective XAI are preferable to complex models
- Low confidence: Current regulatory frameworks adequately address XAI integration challenges

## Next Checks
1. Conduct longitudinal studies across multiple healthcare institutions to assess real-world safety outcomes and performance variations of XAI-integrated medical devices.
2. Perform controlled experiments comparing different XAI methods across diverse clinical scenarios to determine optimal approaches for various medical applications.
3. Develop and validate comprehensive training programs for stakeholders, measuring their effectiveness in promoting appropriate AI tool utilization and mitigating over-reliance risks.