---
ver: rpa2
title: Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation
arxiv_id: '2510.22107'
source_url: https://arxiv.org/abs/2510.22107
tags:
- rainbow
- image
- images
- latent
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Rainbow, a conditional image generation framework
  that captures uncertainty in input prompts and generates diverse, plausible images.
  Rainbow addresses the limitation of traditional methods that rely on random seeds
  or prompt diversification, which fail to adequately capture true diversity and uncertainty.
---

# Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation

## Quick Facts
- arXiv ID: 2510.22107
- Source URL: https://arxiv.org/abs/2510.22107
- Reference count: 40
- Key outcome: Rainbow framework captures uncertainty in input prompts and generates diverse, plausible images using latent graph decomposition with GFlowNets

## Executive Summary
This paper introduces Rainbow, a conditional image generation framework that addresses the limitation of traditional methods in capturing true diversity and uncertainty. By decomposing input conditions into diverse latent representations through a latent graph and sampling diverse trajectories using Generative Flow Networks (GFlowNets), Rainbow generates multiple plausible images from the same prompt. The method demonstrates improved diversity metrics and image quality across natural images, 3D brain MRIs, and chest X-rays, with applications to any pretrained conditional generative model.

## Method Summary
Rainbow decomposes input conditions into diverse latent representations by constructing a latent graph and using GFlowNets to sample diverse trajectories over this graph. The framework trains a GFlowNet (Q_GFN) to sample M parallel trajectories, each building S edges, where each edge has a learned embedding. These trajectories are decoded via an RNN-based graph decoder (Q_D) into condition representations, which are blended with the original condition using parameter γ. The final representations are fed into a frozen pretrained conditional generative model (typically an LDM) to produce diverse outputs. The training objective combines a detailed balance loss for the GFlowNet with the diffusion loss from the generative model.

## Key Results
- Rainbow achieves an Inception Score of 10.45 on natural images compared to 9.93 for the best baseline
- Demonstrates higher Vendi Score for diversity metrics while maintaining or improving image quality (lower FID)
- Successfully generates diverse and plausible medical images (brain MRIs and chest X-rays) with maintained anatomical coherence
- Applicable to any pretrained conditional generative model regardless of condition type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GFlowNets produce diverse high-reward trajectories proportionally, preserving multiple modes rather than converging to a single optimum.
- Mechanism: The detailed balance objective enforces flow conservation where terminal state flow F(s_T) ∝ R(x), causing sampling probability p(x) ∝ R(x) and distributing probability mass across all high-reward regions.
- Core assumption: The reward function R(x) = e^(-MSE(ε, ε̂)) meaningfully correlates with both image quality and trajectory diversity.
- Break condition: If the reward landscape is unimodal or has a dominant peak, GFlowNets may still concentrate sampling, reducing practical diversity gains.

### Mechanism 2
- Claim: Latent graph trajectories encode interpretable semantic concepts that can be manipulated to control generated image attributes.
- Mechanism: Edge embeddings are randomly initialized then trained without predefined semantics, with the RNN-based decoder processing edge sequences and through end-to-end training with the diffusion loss, edges cluster into meaningful patterns that persist across trajectories.
- Core assumption: Edge order and combinations in trajectories carry semantic information that the RNN can learn to decode.
- Break condition: If the graph is too sparse or the RNN capacity insufficient, edges may not form coherent semantic clusters, leading to unpredictable edits.

### Mechanism 3
- Claim: Blending diverse decoded representations with the original condition (via γ) preserves prompt fidelity while enabling controlled variation.
- Mechanism: Final condition representations are computed as ĉ = γQ_D(trajectories) + (1-γ)c, ensuring generated images remain grounded in the original condition while the graph-derived components inject structured diversity.
- Core assumption: The original condition embedding c contains the core semantic information that should not be entirely overwritten.
- Break condition: If γ is too high, generations may drift from the original condition; if too low, diversity gains diminish.

## Foundational Learning

- Concept: Flow Matching and Detailed Balance in GFlowNets
  - Why needed here: Understanding how forward/backward policies and state flows are trained to sample proportionally to rewards is essential for debugging trajectory quality and diversity.
  - Quick check question: Can you explain why detailed balance loss uses squared log-ratios rather than direct probability matching?

- Concept: Latent Diffusion Model (LDM) architecture
  - Why needed here: Rainbow operates on top of pretrained LDMs; understanding the encoder-decoder structure and conditioning mechanisms is prerequisite.
  - Quick check question: In an LDM, where is the condition c injected during the denoising process?

- Concept: Graph representation learning with RNNs
  - Why needed here: Trajectories are sequences of edge indices processed by an RNN to produce condition representations.
  - Quick check question: Why might an RNN be preferred over a simple pooling operation for processing edge sequences in this context?

## Architecture Onboarding

- Component map: Condition Encoder (E_C) -> Graph Generator (Q_GFN) -> Graph Decoder (Q_D) -> Frozen LDM
- Critical path: Input condition → E_C → initial representation c → Q_GFN samples M trajectories (each building S edges sequentially) → Each trajectory → edge embeddings → RNN → projection → blended with c via γ → M condition representations → LDM with M noise samples → M images → Images → reward computation → GFlowNet loss backprop
- Design tradeoffs: Higher M increases diversity but raises memory/compute costs linearly; higher sparsity ρ reduces capacity for complex semantic combinations; γ controls fidelity-diversity balance
- Failure signatures: Low diversity (similar images across M samples) suggests GFlowNet loss converging too fast; condition drift indicates γ may be too high or graph decoder not learning meaningful representations; edge clustering failure suggests insufficient RNN capacity or training duration
- First 3 experiments: 1) Replicate natural image experiment on Flickr30k subset with M=10, N=10, tracking IS and VS alongside GFlowNet loss curves; 2) Ablate γ ∈ {0.2, 0.5, 0.8} to characterize fidelity-diversity frontier; 3) Visualize edge activation patterns across trajectories for prompts with known ambiguity

## Open Questions the Paper Calls Out

- **Question:** How can the training pipeline be optimized to reduce the high computational resources required for updating M trajectories in parallel?
- **Question:** Can latent graph edges be automatically mapped to specific semantic attributes without requiring post-hoc manual clustering or intervention?
- **Question:** How can the framework be constrained to guarantee anatomical plausibility in generated medical images to ensure clinical utility?

## Limitations
- Higher computational resources required for training due to parallel updates of M trajectories
- Need to enhance latent graphs' interpretability more automatically
- Requires anatomical plausibility tests and checks before clinical application

## Confidence
- High confidence in experimental methodology and metric calculations
- Medium confidence in diversity claims, as interpretation depends heavily on specific prompts and reward landscape
- Low confidence in interpretability of latent graph mechanism, as semantic clustering claims are qualitative and not systematically validated

## Next Checks
1. **Semantic Edge Analysis**: For ambiguous prompts, visualize and quantify activation patterns of the first K edges across M trajectories, computing pairwise cosine similarity between edge embeddings to verify semantic clustering without manual supervision.

2. **γ Sensitivity Grid**: Conduct systematic ablation study with γ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on held-out prompts from each dataset, plotting fidelity vs. diversity to characterize the Pareto frontier.

3. **Diversity Under Varying M**: Test Rainbow with M ∈ {5, 10, 20, 40, 80} on the same prompt set, measuring VS, IS, and generation time to quantify the diversity-quality tradeoff curve.