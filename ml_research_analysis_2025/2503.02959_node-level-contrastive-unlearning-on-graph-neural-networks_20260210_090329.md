---
ver: rpa2
title: Node-level Contrastive Unlearning on Graph Neural Networks
arxiv_id: '2503.02959'
source_url: https://arxiv.org/abs/2503.02959
tags:
- unlearning
- nodes
- graph
- embeddings
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Node-CUL, a novel node-level contrastive
  graph unlearning framework for GNNs. It addresses the challenge of removing target
  nodes' influence while maintaining model utility by directly optimizing the embedding
  space.
---

# Node-level Contrastive Unlearning on Graph Neural Networks

## Quick Facts
- arXiv ID: 2503.02959
- Source URL: https://arxiv.org/abs/2503.02959
- Reference count: 40
- This paper introduces Node-CUL, a novel node-level contrastive graph unlearning framework for GNNs that achieves superior test accuracy and lowest unlearn scores across multiple datasets.

## Executive Summary
This paper introduces Node-CUL, a novel node-level contrastive graph unlearning framework for GNNs. It addresses the challenge of removing target nodes' influence while maintaining model utility by directly optimizing the embedding space. Node-CUL employs two complementary components: node representation unlearning, which contrasts embeddings of unlearning nodes with remaining nodes and neighbors to push them toward decision boundaries, and neighborhood reconstruction, which modifies neighbor embeddings to remove unlearning nodes' influence and preserve utility. Experiments on multiple datasets (Cora-ML, PubMed, Citeseer, CS) with GCN, GAT, and GIN models show Node-CUL achieves superior test accuracy (e.g., 87.65% on Cora-ML vs. 85.92% for GNNDelete) and lowest unlearn scores (e.g., 2.62 vs. 51.75 for GNNDelete), indicating effective unlearning. LiRA membership inference attack results confirm strong privacy preservation (AUC ~0.5), and Node-CUL demonstrates comparable efficiency to state-of-the-art methods. The framework's effectiveness and versatility are validated across various graph unlearning scenarios.

## Method Summary
Node-CUL addresses node-level graph unlearning by optimizing the embedding space through two complementary components: node representation unlearning and neighborhood reconstruction. The framework employs a contrastive loss to push unlearning node embeddings away from their same-class neighbors and pull them toward different-class remaining nodes, while simultaneously updating neighbor embeddings to remove the unlearning node's influence and preserve utility. The method uses an iterative optimization approach with a termination condition based on accuracy equivalence between unlearning nodes and a validation subset, ensuring effective unlearning without catastrophic forgetting. Experiments across multiple datasets (Cora-ML, PubMed, Citeseer, CS) with various GNN architectures demonstrate superior performance compared to state-of-the-art methods in terms of test accuracy, unlearn scores, and privacy preservation as measured by LiRA AUC.

## Key Results
- Node-CUL achieves superior test accuracy (e.g., 87.65% on Cora-ML vs. 85.92% for GNNDelete)
- Lowest unlearn scores (e.g., 2.62 vs. 51.75 for GNNDelete), indicating effective unlearning
- Strong privacy preservation confirmed by LiRA membership inference attack results (AUC ~0.5)

## Why This Works (Mechanism)

### Mechanism 1: Representation Unlearning via Contrastive Push-Pull
- **Claim:** If node embeddings are pushed away from their original class cluster and pulled toward decision boundaries, the model treats them as "unseen."
- **Mechanism:** The method defines a contrastive loss $L_U$. It pushes the unlearning node's embedding ($h_i$) away from its immediate neighbors with the same class ("positive" set $P(v_i)$) and pulls it toward remaining nodes with different classes ("negative" set $N(v_i)$). This isolates the node representationally.
- **Core assumption:** The model assumes that "unseen" nodes naturally reside near decision boundaries, whereas "seen" nodes are tightly clustered with their neighbors and class.
- **Evidence anchors:**
  - [abstract] "Directly optimizing the embedding space can effectively remove the target nodes' information."
  - [section 4] Equation 5 shows the specific push/pull loss formulation maximizing similarity to negative samples over positive ones.
  - [corpus] Weak direct support; related works focus on node removal efficiency but not this specific contrastive push-pull dynamic.
- **Break condition:** If the unlearning nodes have no neighbors of the same class (isolated initially), the "push" force is undefined.

### Mechanism 2: Influence Isolation via Neighborhood Reconstruction
- **Claim:** If the embeddings of neighboring nodes are updated to rely solely on their remaining neighbors, the utility lost by removing the target node is recovered.
- **Mechanism:** The method applies a reconstruction loss $L_N$. It pulls the embeddings of $k$-hop neighbors closer to their *other* neighbors (excluding the unlearning node). This effectively "rewrites" the neighbor's local topology to ignore the gap left by the deleted node.
- **Core assumption:** A node's representation is a function of its neighbors; removing a neighbor requires re-aggregating the remaining neighbor features to approximate the original utility.
- **Evidence anchors:**
  - [section 4] "We optimize embeddings of the $k$-hop neighbors... by pulling them closer to their remaining neighbors."
  - [page 11] Table 9 (Appendix) shows test accuracy drops significantly without this component (e.g., Cora-ML drops from 87.65% to 85.09%).
  - [corpus] "Re-understanding Graph Unlearning through Memorization" suggests unlearning efficacy is tied to understanding influence, supporting the need to handle neighbors.
- **Break condition:** If the unlearning node's neighbors are fully disconnected from the rest of the graph (only connected to the unlearning node), reconstruction anchors are missing.

### Mechanism 3: Convergence via Distribution Alignment
- **Claim:** Unlearning is complete when the model's accuracy on unlearning nodes degrades to match its accuracy on unseen test nodes.
- **Mechanism:** An explicit termination condition stops training when $Acc(f'(V_u)) \le Acc(f'(V_{eval}))$. This prevents "over-unlearning" (forcing incorrect predictions) or "under-unlearning" (leaving residual knowledge).
- **Core assumption:** A perfectly unlearned model should treat a deleted node exactly as a node it has never seen (random guess or low confidence).
- **Evidence anchors:**
  - [page 5] Equation 9 defines the termination condition based on accuracy equivalence.
  - [section 5.3] "The algorithm terminates as soon as unlearn accuracy drops below the test accuracy... results in the GNN making slightly less confident predictions."
  - [corpus] "Unlearning Inversion Attacks" implies that successful unlearning resists membership inference, which requires careful calibration to avoid distinct "unlearned" artifacts.
- **Break condition:** If the test set accuracy is extremely low (model is weak), the termination condition might trigger too early.

## Foundational Learning

- **Concept: Message Passing / Neighborhood Aggregation**
  - **Why needed here:** Node-CUL relies on the fact that GNNs learn representations by aggregating features from neighbors. You cannot unlearn a node without addressing how that node's features have "leaked" into its neighbors' embeddings via this aggregation.
  - **Quick check question:** If you remove a node's feature vector but don't touch its neighbors, why does the model still "know" the node? (Answer: The neighbors carry aggregated echoes of the node's features).

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** The core algorithm uses a contrastive objective to manipulate embeddings. You need to distinguish between "positive" pairs (pull together) and "negative" pairs (push apart) to understand how the loss sculpts the embedding space.
  - **Quick check question:** In Node-CUL, do we treat same-class neighbors as positive or negative examples for the unlearning node? (Answer: Negative/Repulsiveâ€”we want to break the association).

- **Concept: Membership Inference Attacks (LiRA)**
  - **Why needed here:** The paper uses LiRA AUC to prove unlearning. You need to understand that an AUC of 0.5 means the attacker cannot distinguish between members and non-members (random guess), which is the goal.
  - **Quick check question:** Is an AUC of 0.0 desirable? (Answer: No, 0.0 implies perfect inversion/information leakage; 0.5 is random/ideal privacy).

## Architecture Onboarding

- **Component map:**
  - Sampler -> Encoder ($f_E$) -> Unlearning Head -> Reconstruction Head -> Controller

- **Critical path:**
  1. Sample unlearning batch $B_u$ and remaining batch $B_r$.
  2. Forward pass to get embeddings for unlearning nodes, their neighbors, and remaining nodes.
  3. Compute $L_U$: Maximize distance to neighbors/same-class, minimize distance to different-class.
  4. Compute $L_N$: Recursively align neighbors with their other neighbors (excluding unlearning node).
  5. Backward pass: Update model weights.
  6. Check if $Acc(V_u) \le Acc(V_{eval})$; if yes, stop.

- **Design tradeoffs:**
  - **Iterative vs. One-shot:** Node-CUL is iterative (multiple steps), making it slower than one-shot influence methods (GIF) but more effective at preserving utility (higher Test Accuracy).
  - **Optimization Scope:** It optimizes the *entire* model rather than adding auxiliary layers (like GNNDelete), ensuring no persistent "unlearning state" is stored during inference.

- **Failure signatures:**
  - **Catastrophic Forgetting:** Test accuracy ($Acc_{test}$) drops significantly.
    - *Diagnosis:* Weight $\gamma$ or $\beta$ for Cross Entropy ($L_C$) is too low; model focuses only on destruction ($L_U$).
  - **Privacy Leakage:** LiRA AUC $\gg 0.5$.
    - *Diagnosis:* Termination condition met too early; embeddings not pushed close enough to decision boundary.
  - **Unlearn Score High:** ($Acc_{test} - Acc_{unlearn}$) is large.
    - *Diagnosis:* Unlearning nodes pushed too far (random noise) or not enough; check Termination Condition calibration.

- **First 3 experiments:**
  1. **Ablation on Reconstruction:** Run Node-CUL without $L_N$ on Cora-ML. Expect to see Test Accuracy drop (Table 9 confirms this) to prove utility maintenance relies on neighbor re-alignment.
  2. **Scaling Test:** Increase unlearning ratio from 10% to 60% (Table 4). Verify if $L_N$ prevents the utility collapse seen in GNNDelete.
  3. **Attack Robustness:** Run LiRA (Table 2). Confirm AUC $\approx 0.5$ to ensure the model isn't just "forgetting" but is effectively indistinguishable from a retrained model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Node-CUL framework be generalized to effectively handle edge unlearning or simultaneous node-edge deletion scenarios?
- Basis in paper: [explicit] The conclusion states, "In the future, we aim to extend the work for edge unlearning and general unlearning of both nodes and edges."
- Why unresolved: The current formulation and experiments focus strictly on node-level unlearning, leaving the adaptation of the contrastive loss and neighborhood reconstruction for edge-specific topological changes unexplored.
- What evidence would resolve it: A modified Node-CUL algorithm and experiments demonstrating unlearn efficacy and model utility on standard edge unlearning benchmarks.

### Open Question 2
- Question: To what extent does the reliance on a labeled validation set for the termination condition limit the framework's applicability in strict transductive settings where test labels are unavailable?
- Basis in paper: [inferred] Section 4 defines the termination condition using a subset of test nodes ($V_{eval}$) and their labels, assuming access that may violate the privacy constraints of a true transductive setting.
- Why unresolved: The paper does not analyze performance degradation or alternative stopping criteria if the ground truth labels for the validation set are inaccessible.
- What evidence would resolve it: An analysis of unlearning performance when the termination condition is triggered by unsupervised metrics (e.g., embedding distribution shift) rather than labeled accuracy.

### Open Question 3
- Question: Can theoretical unlearning guarantees be established for Node-CUL's contrastive approach on non-linear GNNs, bridging the gap with certified unlearning methods?
- Basis in paper: [inferred] Section 2 notes that certified unlearning provides mathematical guarantees for linear GNNs, whereas Node-CUL focuses on empirical efficacy for non-linear GNNs without providing formal bounds.
- Why unresolved: The paper demonstrates empirical success but lacks a theoretical framework to quantify the maximum error or privacy leakage risk after the contrastive update.
- What evidence would resolve it: A formal proof bounding the distance between the Node-CUL model and a gold-standard retrained model in the parameter space.

## Limitations
- The framework requires a labeled validation set for the termination condition, which may not be available in strict transductive settings.
- The method's performance on edge unlearning and simultaneous node-edge deletion scenarios remains unexplored.
- Theoretical unlearning guarantees for non-linear GNNs are not established, unlike certified unlearning methods for linear models.

## Confidence
- **High Confidence:** Test accuracy results and comparison with baselines (GNNDelete, GIF) on Cora-ML, PubMed, and Citeseer.
- **Medium Confidence:** LiRA AUC results indicating privacy preservation, as the methodology is clear but implementation details may vary.
- **Low Confidence:** Ablation study results on the necessity of the reconstruction component, due to missing hyperparameters and potential variability in implementation.

## Next Checks
1. **Hyperparameter Sensitivity:** Test Node-CUL with varying $V_{eval}$ sizes and temperature ($\tau$) values to assess impact on termination and privacy metrics.
2. **Cross-Dataset Generalization:** Apply Node-CUL to a new graph dataset (e.g., Amazon Computers) to validate robustness beyond the four datasets used in the paper.
3. **Runtime Scalability:** Measure training time and memory usage for unlearning 50% of nodes to confirm efficiency claims against state-of-the-art methods.