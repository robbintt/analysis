---
ver: rpa2
title: Implementing a Sharia Chatbot as a Consultation Medium for Questions About
  Islam
arxiv_id: '2512.16644'
source_url: https://arxiv.org/abs/2512.16644
tags:
- chatbot
- system
- islamic
- user
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research implemented a Sharia chatbot using Q-Learning reinforcement
  learning combined with Sentence-Transformers for semantic embedding, trained on
  a 25,000-entry Islam QA dataset from authentic Islamic sources. The system achieved
  87% semantic accuracy in functional testing across fiqh, aqidah, ibadah, and muamalah
  topics.
---

# Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam

## Quick Facts
- arXiv ID: 2512.16644
- Source URL: https://arxiv.org/abs/2512.16644
- Reference count: 29
- Primary result: Sharia chatbot achieved 87% semantic accuracy using Q-Learning + Sentence-Transformers on 25k Islamic QA dataset

## Executive Summary
This research implements a Sharia chatbot using Q-Learning reinforcement learning combined with Sentence-Transformers for semantic embedding, trained on a 25,000-entry Islam QA dataset from authentic Islamic sources. The system achieved 87% semantic accuracy in functional testing across fiqh, aqidah, ibadah, and muamalah topics. The chatbot was developed with a Flask API backend and Flutter mobile frontend, demonstrating potential for enhancing religious literacy and digital da'wah. While effective for closed-domain queries, limitations include static learning and dataset dependency, suggesting opportunities for future enhancement through continuous adaptation and multi-turn conversation support.

## Method Summary
The method combines Sentence-Transformers (paraphrase-multilingual-MiniLM-L12-v2) for semantic embeddings with tabular Q-Learning for answer selection. The system encodes 25,000 Islamic QA pairs, computes cosine similarity between user queries and dataset questions, then refines candidate selection using learned Q-values. Training uses rewards based on cosine similarity thresholds (>0.8), updating Q-values via the standard Q-Learning equation. The hybrid approach merges semantic proximity with reinforcement-guided preferences, selecting the optimal answer from the JSON dataset. Long answers are summarized using TF-IDF-based extractive summarization.

## Key Results
- Achieved 87% semantic accuracy across fiqh, aqidah, ibadah, and muamalah topics
- Strong generalization to different linguistic expressions in 100 testing scenarios
- Maintained <3 second response time with Flask API and Flutter frontend

## Why This Works (Mechanism)

### Mechanism 1
Q-Learning improves answer selection by learning which responses yield higher semantic rewards across repeated query-answer pairings. The system constructs a Q-Table mapping state embeddings (user questions encoded via Sentence-Transformers) to actions (candidate answers). During training, Q-values are updated using: Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') − Q(s,a)], where the reward r is derived from cosine similarity between the input question and stored questions. Over iterations, Q-values converge toward responses with higher semantic alignment.

### Mechanism 2
Sentence-Transformers enable the chatbot to capture semantic meaning beyond lexical overlap, supporting cross-lingual and paraphrased query understanding. The model paraphrase-multilingual-MiniLM-L12-v2 maps questions into fixed-dimensional dense vectors (~384 dimensions). These embeddings position semantically similar questions near each other in vector space, allowing cosine similarity to identify meaning-aligned pairs regardless of word choice.

### Mechanism 3
Combining cosine similarity scores with learned Q-values produces a hybrid ranking that outperforms either method alone. At inference, incoming queries are embedded and compared to dataset embeddings via cosine similarity. The Q-Table provides additional preference weights learned during training. The final response selection merges both signals, prioritizing answers with high similarity AND high Q-values.

## Foundational Learning

- Concept: Q-Learning basics (state, action, reward, Q-table, learning rate α, discount factor γ)
  - Why needed here: The paper directly applies tabular Q-Learning to map question embeddings to answer selections. Without understanding the update equation and convergence behavior, you cannot debug why certain answers are preferred.
  - Quick check question: Given a state s and action a, write the Q-Learning update step. What happens if α is too high?

- Concept: Sentence embeddings and cosine similarity
  - Why needed here: The entire retrieval pipeline depends on embedding questions and computing similarity. Understanding vector space geometry is essential for tuning thresholds and diagnosing retrieval failures.
  - Quick check question: Two sentences with cosine similarity 0.85 are considered "similar." What does this value represent geometrically? What threshold would you set for "relevant" vs. "irrelevant"?

- Concept: Closed-domain vs. open-domain chatbot limitations
  - Why needed here: The authors explicitly note the system is closed-domain with static learning. Recognizing this boundary prevents unrealistic expectations and guides future architecture decisions (e.g., RLHF, multi-turn context).
  - Quick check question: A user asks a question outside the training domain (e.g., contemporary crypto finance). How should the system respond, and why does the current architecture struggle?

## Architecture Onboarding

- Component map: Flutter mobile app (client) → HTTP POST → Flask API server → Q-Learning model + Sentence-Transformers → JSON dataset (25,000 QA pairs) → Response returned via API
- Critical path: User query received by Flask API → Query embedded via Sentence-Transformers → Cosine similarity computed against cached embeddings → Top candidates refined using Q-Table values → Best answer retrieved from JSON dataset and returned
- Design tradeoffs: Tabular Q-Learning vs. Deep Q-Networks (simpler but doesn't scale to continuous state spaces); Static Q-Table vs. online learning (locks in knowledge post-training); Extractive summarization for long answers (reduces verbosity but may omit nuance)
- Failure signatures: Low cosine similarity (<0.5) for all candidates → query likely out-of-domain; Q-values uniform or zero → training didn't converge; API timeout >3 seconds → embedding computation bottleneck; Relevant but not jurisprudentially precise → dataset lacks authoritative depth
- First 3 experiments: 1) Threshold sensitivity analysis: Vary cosine similarity threshold (0.6, 0.7, 0.8) and measure accuracy vs. coverage; 2) Ablation study: Run inference using only cosine similarity vs. hybrid ranking; 3) Out-of-domain detection: Inject 50 questions unrelated to Islam and measure false-positive rate

## Open Questions the Paper Calls Out

- Can Reinforcement Learning from Human Feedback (RLHF) be effectively integrated to enable dynamic Q-value updates and continuous model adaptation without retraining?
- Does replacing tabular Q-Learning with Deep Q-Networks (DQN) improve scalability and performance when handling continuous semantic representations?
- How can semantic context tracking be implemented to maintain coherence during multi-turn religious consultations?

## Limitations

- Static learning approach with fixed Q-Table that cannot adapt to new questions or user interactions without retraining
- Closed-domain architecture that struggles with out-of-domain queries and lacks generalization beyond training data
- Underspecified Q-Table construction from continuous embeddings, making exact reproduction difficult

## Confidence

- **High Confidence**: Technical architecture (Flask API + Flutter frontend) is clearly specified and implementable; Sentence-Transformers for semantic embedding is well-established
- **Medium Confidence**: 87% semantic accuracy claim based on authors' testing, but evaluation methodology and test data are not publicly available for independent verification
- **Low Confidence**: Q-Learning implementation details are too sparse for reproduction; mechanism for mapping continuous embeddings to discrete Q-Table states is unexplained

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rate (α) and discount factor (γ) across a grid (α ∈ {0.01, 0.1, 0.5}, γ ∈ {0.9, 0.95, 0.99}) and measure accuracy impact to identify optimal settings and verify robustness.

2. **Ablation Study Against Baselines**: Compare the hybrid cosine+RL approach against three alternatives: (a) pure cosine similarity ranking, (b) TF-IDF retrieval baseline, and (c) random selection, using the same test set to quantify the marginal value of Q-Learning.

3. **Out-of-Domain Detection Validation**: Create a test set of 100 questions unrelated to Islamic jurisprudence and measure false-positive rate. Implement a minimum confidence threshold (max cosine similarity < 0.6 triggers "I cannot answer") and report precision/recall trade-offs.