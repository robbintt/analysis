---
ver: rpa2
title: Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point
  Clouds
arxiv_id: '2511.02395'
source_url: https://arxiv.org/abs/2511.02395
tags:
- moving
- segmentation
- radar
- data
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of moving object segmentation
  in sparse and noisy radar point clouds, which is crucial for autonomous driving
  systems. The proposed approach employs self-supervised contrastive representation
  learning with supervised fine-tuning to improve label efficiency.
---

# Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds

## Quick Facts
- **arXiv ID:** 2511.02395
- **Source URL:** https://arxiv.org/abs/2511.02395
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art moving object segmentation on View-of-Delft and RadarScenes datasets using only 1% annotated data.

## Executive Summary
This paper presents a self-supervised learning approach for moving object segmentation in sparse and noisy radar point clouds. The method employs contrastive representation learning with supervised fine-tuning to dramatically improve label efficiency. A novel clustering-based contrastive loss function, enhanced with dynamic points removal filtering, enables the model to learn motion-aware representations without extensive manual annotations. The approach achieves state-of-the-art performance, outperforming self-supervised baselines by over 25 percentage points in moving IoU using just 1% of labeled data.

## Method Summary
The approach uses a two-stage training process with a modified Radar Instance Transformer backbone. First, contrastive representation learning is performed using a novel Motion-Aware Contrastive Loss (MACL) that groups points into clusters and computes distances between cluster centroids. The clusters are refined using a Dynamic Points Removal filter to separate moving and static points. Second, the pretrained backbone is fine-tuned with a small MLP head using limited annotated data. The method leverages HDBSCAN clustering with cluster selection epsilon of 0.1 and a RANSAC-based DPR filter with threshold of 0.5 to create motion-aware pseudo-labels for self-supervised training.

## Key Results
- Achieves state-of-the-art performance on View-of-Delft and RadarScenes datasets
- Using only 1% of annotated data, outperforms self-supervised baseline by over 25 absolute percentage points in moving IoU
- Both clustering and cluster refinement components are essential for successful motion-aware representation learning
- Performance gains are particularly significant on sparse radar data compared to dense lidar point clouds

## Why This Works (Mechanism)

### Mechanism 1
Grouping points into cluster centroids for contrastive learning improves robustness to radar sparsity compared to point-wise contrastive losses. The method aggregates individual noisy points into cluster centroids using HDBSCAN before computing the contrastive loss. By calculating the L2-distance between centroids in the representation space rather than individual points, the loss function reduces the influence of high-frequency noise and outliers inherent in radar data. Points spatially close and kinematically similar belong to the same object, and their averaged representation is more semantically stable than isolated points.

### Mechanism 2
Algorithmic refinement of clusters using Dynamic Points Removal (DPR) filters is essential for generating "motion-aware" rather than just "spatial" representations. The loss function utilizes DPR (a RANSAC-based velocity filter) to split spatially coherent clusters into purely "moving" or "static" sub-clusters. This enforces a separation in the feature space based on motion labels derived from the Doppler velocity, allowing the network to pre-learn the distinction between static backgrounds and dynamic objects without human labels. The DPR algorithm provides sufficiently accurate pseudo-labels by reliably identifying deviations from the RANSAC velocity profile.

### Mechanism 3
Decoupling representation pretraining from the segmentation head improves label efficiency for the downstream task. The framework uses a two-stage process where a student-teacher architecture first trains the backbone to output meaningful representations via the contrastive loss. Then, the backbone is frozen or fine-tuned with a lightweight MLP head using sparse annotations. This leverages the structure of the unannotated data to initialize weights closer to the optimal solution, as the features learned to minimize the contrastive loss are transferable to the pixel-wise classification task of moving object segmentation.

## Foundational Learning

- **Concept: Contrastive Representation Learning**
  - **Why needed here:** The core mechanism relies on minimizing distance between "positive" pairs (same motion state) and maximizing it for "negative" pairs.
  - **Quick check question:** Can you explain how a Student-Teacher framework (using EMA updates) prevents representation collapse compared to standard supervised training?

- **Concept: Radar Signal Processing (Doppler & RCS)**
  - **Why needed here:** The method exploits Doppler velocity for clustering refinement; understanding its radial limitation is critical for debugging failure cases.
  - **Quick check question:** Why might a car moving perpendicular to the radar beam fail to trigger the "moving" cluster refinement, and how does the network handle this?

- **Concept: Density-Based Clustering (HDBSCAN)**
  - **Why needed here:** The loss is computed on cluster centroids, not points. Understanding how ε and minimum cluster size affect noise filtering is vital.
  - **Quick check question:** If HDBSCAN classifies valid moving object points as "noise" (label -1), how does your data pipeline handle them during loss calculation?

## Architecture Onboarding

- **Component map:** Input Point Cloud -> SAFE Module -> Representation Vectors -> Algorithmic Cluster Matching -> MACL Loss -> Pretrained Backbone -> MLP Head -> Binary Mask

- **Critical path:**
  1. **Preprocessing:** Ego-motion compensation is strictly required; uncompensated Doppler will break the DPR filtering logic.
  2. **Clustering:** HDBSCAN must run on the spatial+velocity features of the Student input; Teacher labels are derived from Student centroids.
  3. **Refinement:** DPR splits clusters. If this step is skipped, the network learns only spatial features.

- **Design tradeoffs:**
  - **Centroid vs. Point Loss:** Computing loss on centroids is faster and robust to noise but loses fine-grained per-point detail.
  - **Cluster Granularity:** A lower ε in HDBSCAN creates many small clusters (risk of over-segmentation); higher ε merges distinct objects (risk of mixing static/moving points before refinement).

- **Failure signatures:**
  - **Collapse:** Moving IoU stagnates at ~50% (random guessing). Check if DPR threshold is too tight/loose for the current sensor noise profile.
  - **Overfitting to Static:** High Static IoU (~99%) but 0% Moving IoU. This implies the loss is not successfully pushing moving clusters apart; verify that positive/negative matching logic handles the "moving" class correctly.
  - **Noise Amplification:** Visualized masks look like "salt-and-pepper" noise. The clustering min-size may be too small, treating noise points as valid clusters.

- **First 3 experiments:**
  1. **Sanity Check (DPR Ablation):** Run pretraining with "w/o DPR" setting on a small data slice to confirm performance drops (validating the pipeline).
  2. **Label Efficiency Curve:** Fine-tune the pretrained backbone vs. training from scratch using 1%, 10%, and 100% of View-of-Delft data to replicate the efficiency curve.
  3. **Cluster Visualization:** Visualize the HDBSCAN clusters overlaying the point cloud to ensure the "refinement" step actually splits moving cars from the static road surface.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the learned motion-aware representation transfer effectively to moving instance segmentation tasks?
- **Basis in paper:** The authors explicitly modified the Radar Instance Transformer by removing the instance transformer head to focus solely on semantic segmentation.
- **Why unresolved:** It is unclear if the pretraining preserves sufficient intra-class discriminability to separate individual moving objects after the fine-tuning phase.
- **What evidence would resolve it:** Re-attaching the instance head to the pretrained backbone and evaluating on a moving instance segmentation benchmark.

### Open Question 2
- **Question:** Can the reliance on the heuristic Dynamic Points Removal (DPR) filter be replaced by a learnable module?
- **Basis in paper:** Section III.C relies on the algorithmic DPR filter (using RANSAC) to generate pseudo-labels for cluster refinement.
- **Why unresolved:** The method depends on the accuracy of the DPR filter; if the RANSAC ego-motion estimation fails, the contrastive loss receives incorrect supervision signals.
- **What evidence would resolve it:** Analyzing performance degradation under forced DPR failures or successfully replacing the filter with a trainable ego-motion estimation network.

### Open Question 3
- **Question:** At what scale of annotated data does the performance gain from self-supervised pretraining diminish?
- **Basis in paper:** Section IV.B observes that the performance margin over the baseline is smaller on the larger RadarScenes dataset compared to View-of-Delft.
- **Why unresolved:** The experiments suggest that large-scale supervised data may reduce the need for pretraining, but the specific saturation point remains unidentified.
- **What evidence would resolve it:** A scaling law analysis evaluating the performance gap between pretrained and scratch models across a logarithmic range of annotated data fractions.

## Limitations
- The approach relies heavily on the quality of unsupervised clustering and DPR filtering, which are sensitive to radar sensor characteristics
- DPR algorithm's effectiveness depends on the assumption that static points dominate the scene, which may not hold in dense urban environments
- Method's reliance on Doppler velocity limits detection of motion perpendicular to the radar beam

## Confidence
- **High Confidence:** The label efficiency claims (25+ absolute percentage points improvement with 1% data) and the ablations showing clustering and DPR contributions are well-supported by quantitative results
- **Medium Confidence:** The theoretical mechanism of contrastive learning via cluster centroids is sound, but the specific adaptation to radar's sparsity/noise characteristics would benefit from more extensive ablation studies
- **Medium Confidence:** The transfer learning assumption (that motion-aware representations generalize to segmentation) is supported by results but could be further validated with cross-dataset experiments

## Next Checks
1. **Cross-Scene Generalization:** Test the pretrained model on a new radar dataset with different environmental conditions (e.g., urban vs. highway) to validate the robustness of the learned representations.
2. **Parameter Sensitivity Analysis:** Systematically vary HDBSCAN epsilon and DPR thresholds to quantify their impact on moving object IoU and identify optimal settings for different sensor configurations.
3. **Perpendicular Motion Detection:** Create a controlled test case with objects moving perpendicular to the radar beam to measure the false negative rate and evaluate whether the network can infer motion from spatial context alone.