---
ver: rpa2
title: 'CourtPressGER: A German Court Decision to Press Release Summarization Dataset'
arxiv_id: '2512.09434'
source_url: https://arxiv.org/abs/2512.09434
tags:
- press
- summarization
- court
- legal
- hier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CourtPressGER addresses the challenge of generating citizen-oriented
  summaries from German court decisions by providing a 6.4k dataset of aligned judgments
  and press releases. The core method uses synthetic prompts to guide LLMs in producing
  comparable press releases, evaluated through automatic metrics, LLM-as-judge, and
  expert ranking.
---

# CourtPressGER: A German Court Decision to Press Release Summarization Dataset

## Quick Facts
- **arXiv ID:** 2512.09434
- **Source URL:** https://arxiv.org/abs/2512.09434
- **Reference count:** 40
- **Primary result:** 6.4k German court decision to press release summarization dataset with hierarchical processing for smaller models

## Executive Summary
CourtPressGER introduces a dataset of 6,432 German court decisions paired with human-drafted press releases and synthetic prompts for LLM summarization. The core method uses LLM-generated synthetic prompts to guide models in producing press releases matching the style and structure of reference documents. Large models (GPT-4o, Llama-3-70B) achieve ROUGE-1 scores around 37-38% and strong human alignment, while smaller models require hierarchical summarization for long judgments. Human-drafted releases rank highest, with LLM outputs needing minor edits for practical use.

## Method Summary
The method involves generating synthetic prompts using Claude 3.7 Sonnet to capture the structural and stylistic requirements of press releases. For long judgments exceeding context windows, hierarchical summarization recursively summarizes chunks and merges them. Six models are benchmarked: GPT-4o, Llama-3-70B, Llama-3-8B, Mistral-7B, EuroLLM-9B, and Teuken-7B (fine-tuned with stacked SFT). Evaluation combines reference-based metrics (ROUGE, BERTScore), factual consistency checks (QAGS, FactCC), and LLM-as-judge using Claude 3.7 Sonnet.

## Key Results
- Large models (GPT-4o, Llama-3-70B) achieve ROUGE-1 scores of 37-38% and strong human alignment (Spearman's ρ = -0.939)
- Smaller models require hierarchical summarization for long judgments but achieve reasonable quality
- Human-drafted press releases rank highest in quality evaluations
- Factual consistency remains challenging, with FactCC scores around 0.49-0.51 even for top models

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Prompts for Style Alignment
Synthetic prompts potentially align model outputs closer to the specific structural and stylistic requirements of court press releases than generic instructions. An advanced LLM reverse-engineers a "synthetic prompt" by analyzing the alignment between a source judgment and a reference press release, explicitly encoding instructions regarding length, audience level, and topical focus.

### Mechanism 2: Hierarchical Summarization for Long Documents
Hierarchical summarization enables smaller models (7B–9B parameters) to process documents exceeding their context windows while mitigating coherence loss better than simple truncation. The system splits long judgments into chunks, generates "Level-0" summaries, and merges them in a tree-like structure to preserve salient entities across the document span.

### Mechanism 3: LLM-as-Judge for Domain Evaluation
LLM-as-judge evaluation using high-parameter models serves as a reliable proxy for human expert ranking in this specific domain. A capable LLM evaluates generated text against a reference on a 1-10 scale across dimensions like factual correctness and clarity, showing strong negative correlation with human rankings.

## Foundational Learning

### Concept: Context Window Constraints vs. Document Length
- **Why needed here:** The average judgment (~10,810 tokens) exceeds the training/inference limits of many smaller open-source models (e.g., 4k or 8k tokens). Understanding this constraint is essential for selecting between single-pass (large model) and hierarchical (small model) architectures.
- **Quick check question:** If you switch from GPT-4o to a 7B parameter model with a 4096 token limit, why can't you simply feed the entire judgment at once?

### Concept: Abstractive vs. Extractive Summarization
- **Why needed here:** The goal is "citizen-oriented" communication. Merely extracting sentences (extractive) preserves legal jargon, whereas generating new text (abstractive) allows for "lay-friendly narrative."
- **Quick check question:** Why might an extractive summary containing the phrase "The appellant contests the admissibility of the cassation" fail the "clarity" requirement for a press release?

### Concept: Factual Consistency Metrics (e.g., QAGS, FactCC)
- **Why needed here:** Generative models hallucinate. Standard n-gram overlap (ROUGE) does not measure truthfulness. These metrics verify if the summary is entailed by the source text.
- **Quick check question:** If a generated press release invents a date not present in the judgment, would ROUGE or FactCC be more likely to flag this issue?

## Architecture Onboarding

### Component map:
Raw Court Judgment + Synthetic Prompt -> Preprocessing (Chunking for smaller models) -> Generation Engine (LLM) -> Evaluation Suite (Reference-based + Reference-free)

### Critical path:
1. Prompt Synthesis: Generate decision-specific prompt (using Claude 3.7)
2. Hierarchical Processing (Conditional): If input > Context Limit, chunk -> summarize chunks -> merge summaries
3. Final Generation: Model outputs the draft press release

### Design tradeoffs:
- **Large vs. Small Models:** Large models (GPT-4o) offer higher quality and simpler pipelines (single-pass) but incur higher cost/API dependency. Small models (Teuken) allow local deployment and data privacy but require complex hierarchical pipelines and fine-tuning, yet still yield lower "clarity" scores.
- **Evaluation:** Human evaluation is the gold standard but unscalable. LLM-as-judge is scalable and correlates well with humans but risks systematic bias.

### Failure signatures:
- **Metric Mismatch:** High ROUGE score but low human utility (the paper cites prior work showing ROUGE doesn't capture legally salient content)
- **Hallucination:** The generated text includes "enriched information" (correct facts not in source) or false facts, which FactCC/QAGS may flag
- **Coherence Loss:** In hierarchical setups, the final summary lists facts without a unifying narrative thread

### First 3 experiments:
1. **Validation of Pipeline:** Run 10 judgments through the hierarchical Teuken-7B pipeline and 10 through full-text GPT-4o. Compare the "Coherence" and "Factual Correctness" scores using the provided LLM-as-Judge prompts.
2. **Prompt Ablation:** Generate summaries using a generic prompt (e.g., "Summarize this for a newspaper") vs. the synthetic prompt for the same case. Quantify the difference in "Structure" scores.
3. **Error Analysis on Factuality:** Select the 5 lowest-scoring summaries by FactCC. Manually inspect to determine if errors are due to translation issues (English metrics on German text) or actual model hallucinations.

## Open Questions the Paper Calls Out

### Open Question 1: German-Specific Factual Consistency Metrics
How can German-specific factual consistency metrics be developed to properly evaluate legal text summarization without the biases introduced by English-origin metrics like QAGS and FactCC? Current metrics were developed on English datasets and may not handle German compound words, case-based grammar, or jurisdiction-specific legal terminology.

### Open Question 2: Impact of Synthetic Prompts
What impact do synthetic prompts have on model performance compared to generic prompts? Claude-generated synthetic prompts were used uniformly across all models to ensure fair comparison, but prompt quality was not independently benchmarked.

### Open Question 3: Improving Factual Consistency
Can factual consistency in LLM-generated legal press releases be improved to match human-drafted references? Even GPT-4o and Llama-3-70B achieve only ~0.49–0.51 on FactCC and 0.26–0.29 on QAGS, indicating substantial factual gaps remain relative to human-authored releases.

## Limitations
- The study relies on automatic evaluation metrics that may not fully capture the nuances of legal text quality, particularly when applied to German-language content
- The LLM-as-judge correlation with human evaluation was established on only 14 cases, raising questions about generalizability
- The paper does not address potential bias in the synthetic prompt generation process

## Confidence

**High Confidence:** The core finding that large language models (GPT-4o, Llama-3-70B) outperform smaller models in generating court press releases is well-supported by the empirical results and multiple evaluation metrics.

**Medium Confidence:** The effectiveness of synthetic prompts in guiding model outputs is plausible based on the results, but the mechanism by which prompts improve quality over generic instructions is not deeply explored.

**Medium Confidence:** The hierarchical summarization approach for smaller models is validated through improved coherence, but the specific hyperparameters and trade-offs are underspecified, limiting reproducibility.

## Next Checks

1. **Expand Human Evaluation:** Conduct human evaluation on a larger, randomly sampled subset of the test set to confirm the LLM-as-judge correlation and assess the generalizability of the quality rankings.

2. **Factual Consistency Audit:** Perform a manual audit of the 50 lowest-scoring summaries by FactCC to determine whether errors stem from translation artifacts (English metrics on German text) or genuine model hallucinations.

3. **Prompt Ablation Study:** Systematically compare the performance of synthetic prompts against a set of diverse generic prompts (e.g., varying in length, tone, and focus) to isolate the specific benefits of the synthetic approach.