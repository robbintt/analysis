---
ver: rpa2
title: 'When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language
  Models'
arxiv_id: '2507.13868'
source_url: https://arxiv.org/abs/2507.13868
tags:
- factual
- heads
- counterfactual
- knowledge
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how vision-language models resolve conflicts
  between visual input and internal factual knowledge. The authors construct WHOOPS-AHA!,
  a dataset of multimodal counterfactual queries designed to provoke such conflicts,
  pairing unusual images with prompts that encourage commonsense responses.
---

# When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2507.13868
- **Source URL**: https://arxiv.org/abs/2507.13868
- **Reference count**: 34
- **Key outcome**: Vision-language models resolve factual-visual conflicts via specialized attention heads; targeted interventions can steer predictions toward knowledge or visual input.

## Executive Summary
This paper investigates how vision-language models resolve conflicts between visual input and internal factual knowledge. The authors construct WHOOPS-AHA!, a dataset of multimodal counterfactual queries designed to provoke such conflicts, pairing unusual images with prompts that encourage commonsense responses. Using logit inspection, they identify a small set of attention heads that mediate these conflicts, with counterfactual heads attending more to image tokens. Targeted interventions on these heads can shift model predictions toward either factual knowledge or visual context. Attention-based attribution from conflict-resolution heads more precisely identifies image regions driving counterfactual predictions than gradient-based methods. For example, in LLaVA-NeXT, factual accuracy increased from 22% to 74% when boosting factual heads and suppressing counterfactual heads, and ablation of top-ranked image patches improved factual accuracy to around 80%. These results reveal localized mechanisms for multimodal conflict resolution and provide a foundation for interpretable, controllable interventions in vision-language models.

## Method Summary
The authors construct WHOOPS-AHA!, a dataset of 436 multimodal counterfactual queries, each containing an unusual image and a prompt designed to trigger conflict between visual evidence and internal knowledge. They apply Logit Lens to intermediate layers of LLaVA-NeXT and Gemma3 models to identify attention heads that mediate factual vs. counterfactual predictions. Top-20 factual and counterfactual heads are targeted with multiplicative attention scaling (λ ∈ [-3, 3]) at the final token position. The effectiveness of attention-based versus gradient-based attribution is compared through patch ablation studies, measuring factual accuracy recovery.

## Key Results
- Factual accuracy in LLaVA-NeXT increased from 22% to 74% when boosting factual heads and suppressing counterfactual heads.
- Attention-based patch attribution outperformed gradient-based methods in identifying image regions driving counterfactual predictions.
- Ablation of top-attended image patches increased factual accuracy to around 80%.
- The distribution of factual prevalence per head shows only a small subset exhibits strong alignment with either factual or counterfactual tokens.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A small subset of attention heads in late layers function as mediators for cross-modal conflicts.
- **Mechanism:** By applying the Logit Lens to intermediate residual streams, the paper identifies heads that disproportionately increase the logit probability of either "factual" (internal knowledge) or "counterfactual" (visual evidence) tokens. This localization suggests that the competition between modalities is resolved by specific circuits rather than distributed uniformly.
- **Core assumption:** The Logit Lens projection of hidden states to vocabulary space accurately reflects the functional contribution of that layer to the final output.
- **Evidence anchors:**
  - [abstract] "...localize with logit inspection a small set of heads that control the conflict."
  - [section 4.2] "The distribution shows that only a small subset of heads exhibit a strong, consistent alignment with tfact or tcofa."
  - [corpus] [Investigating The Functional Roles of Attention Heads...] supports the general hypothesis that attention heads have specialized functional roles in VLMs (FMR 0.64).
- **Break condition:** If interventions on these heads failed to shift model predictions, the localization would be correlational rather than causal.

### Mechanism 2
- **Claim:** Modulating attention weights at the final token position causally controls reliance on visual vs. parametric knowledge.
- **Mechanism:** The paper employs a targeted intervention: amplifying attention to image tokens in "counterfactual heads" while suppressing attention to text tokens in "factual heads" (and vice versa). This bidirectional scaling (λ) shifts the probability mass between the conflicting tokens.
- **Core assumption:** The mechanism of conflict resolution is primarily carried by the attention flow to the final token, rather than deep processing in earlier layers or MLP blocks alone.
- **Evidence anchors:**
  - [abstract] "...modifying these heads, we can steer the model towards its internal knowledge or the visual inputs."
  - [section 4.3] "For LLaV A-NeXT... factual accuracy increases to 74% [from 22%]... confirming that these heads causally influence..."
  - [corpus] Corpus evidence for specific conflict intervention is weak; related work focuses on general hallucination or robustness (e.g., SegSub) rather than causal steering via heads.
- **Break condition:** If random heads produced similar accuracy shifts, the specific causal role of the identified heads would be invalidated.

### Mechanism 3
- **Claim:** Counterfactual predictions are driven by semantically meaningful, localized image patches identified via direct attention.
- **Mechanism:** "Counterfactual heads" attend significantly more to image tokens (approx. 60% vs 22% avg). By mapping these high-attention weights back to image patches and ablating them, the model reverts to factual predictions, proving the visual signal is localized.
- **Core assumption:** High attention weights correlate with causal importance (saliency) in this specific context, distinct from standard gradient sensitivity.
- **Evidence anchors:**
  - [abstract] "...attention from such heads pinpoints localized image regions driving visual overrides, outperforming gradient-based attribution..."
  - [section 4.4] "Ablation of visual patches identified through attention-based attribution leads to a sharp... increase in factual accuracy."
  - [corpus] [V-SEAM] aligns with using attention modulation for visual semantic editing/causality (FMR 0.56).
- **Break condition:** If attention-based ablation performed equivalently to random ablation, the "localization" claim would be false.

## Foundational Learning
- **Concept: Logit Lens / Patchscope**
  - **Why needed here:** The paper relies on projecting intermediate layer outputs to the vocabulary space to "read the mind" of the model before the final output layer. Without this, you cannot identify which heads favor "sun" vs "moon."
  - **Quick check question:** Can you explain why projecting a hidden state through the unembedding matrix W_U helps localize information processing?
- **Concept: Residual Stream Decomposition**
  - **Why needed here:** The analysis separates the contributions of Attention blocks vs. MLP blocks. Understanding that x_l = x_{l-1} + a_l + m_l is essential for attributing the "factual prevalence" to specific components.
  - **Quick check question:** Does the residual stream in a Transformer typically grow or shrink in magnitude through layers, and how does layer normalization affect this inspection?
- **Concept: Attention Head Attribution**
  - **Why needed here:** The intervention strategy requires identifying "factual" vs "counterfactual" heads. You must understand how to aggregate the effect of a head h on the logit of a specific token t.
  - **Quick check question:** If an attention head attends 80% to a specific image patch, does that guarantee the patch is causing the prediction, or could it be correlational?

## Architecture Onboarding
- **Component map:** Vision Encoder (CLIP) -> Text Tokenizer -> Projection to LLM space -> LLM Backbone (Attention + MLP) -> Output
- **Critical path:**
  1. **Dataset Construction (WHOOPS-AHA!):** Ensure the prompt has a high-probability factual completion in text-only mode (e.g., "moon") that flips in image+text mode (e.g., "sun").
  2. **Localization:** Run Logit Lens on the final token to find heads maximizing the logit difference between the factual and counterfactual token.
  3. **Intervention:** Apply multiplicative scaling (λ) to the attention matrix row corresponding to the final token.
- **Design tradeoffs:**
  - **Gradient vs. Attention Attribution:** The paper argues attention is more precise but potentially noisier for general features, whereas gradients are broader but less precise for specific counterfactual anomalies.
  - **Intervention Strength (λ):** High λ increases steering accuracy but degrades text coherence (KL divergence spikes); optimal balance found at ≈ 20 heads.
- **Failure signatures:**
  - **Random Steering:** Intervening on random heads yields no accuracy shift (control check).
  - **Degraded Generation:** If λ > 10, the model outputs repetitive/nonsensical text (Table 1).
  - **Modality Leak:** If MLPs are not considered, the "internal knowledge" signal might be underestimated, as MLPs strongly favor factual recall.
- **First 3 experiments:**
  1. **Reproduce the "Conflict Shift":** Take a sample prompt (e.g., "The wolf is howling at the..."), run text-only to verify "moon" is top logit, then add the counterfactual image to verify the shift to "sun."
  2. **Head Localization:** Implement the Logit Lens on the final token across all layers. Plot the "Factual Accuracy" per head to verify the emergence of distinct "factual" (blue) and "counterfactual" (red) clusters in late layers.
  3. **Visual Ablation:** Using the top counterfactual heads, mask the top 10% of attended image patches (set embeddings to 0) and check if the prediction reverts to the factual token.

## Open Questions the Paper Calls Out
- **Open Question 1:** Do the identified conflict-resolution mechanisms persist in naturalistic, non-synthetic multimodal inputs, or are they specific to the high-contrast counterfactuals in WHOOPS-AHA!? The paper notes the dataset uses synthetic inputs and suggests future extensions to more naturalistic data could further validate the findings.
- **Open Question 2:** What specific roles do MLP layers and the visual encoder play in the broader circuit of resolving visual-textual conflicts, beyond the mediation by attention heads? The paper states intervention methods do not capture contributions from MLP layers or visual encoders, identifying this as a promising direction.
- **Open Question 3:** Do the identified attention heads maintain causal control over conflict resolution during free-form text generation, or are their effects limited to single-token prediction contexts? The paper mentions the analysis uses a representative token per example and suggests future work could explore more model behavior across full generations.

## Limitations
- The Logit Lens projections are treated as direct indicators of functional role, but this assumes the unembedding matrix faithfully recovers semantic intent at each layer.
- The intervention experiments demonstrate causal influence, yet the scaling factor λ was optimized for accuracy without explicit constraints on generation quality until KL divergence exceeded thresholds.
- The attention-based attribution outperforms gradients in this specific counterfactual setting, but the paper does not test whether this advantage generalizes to other conflict types or whether attention-based saliency correlates with human judgment in broader contexts.

## Confidence
- **High Confidence**: The existence of specialized attention heads that mediate factual vs. visual conflicts, and the causal effect of targeted interventions on these heads.
- **Medium Confidence**: The superiority of attention-based attribution over gradient-based methods for identifying causal image patches.
- **Low Confidence**: The assumption that Logit Lens projections at intermediate layers directly reflect the model's "preference" for factual vs. counterfactual tokens.

## Next Checks
1. **Cross-Conflict Generalization**: Apply the same Logit Lens and intervention pipeline to WHOOPS-AHA! variants where the conflict is not "sun vs. moon" but other factual-visual pairs (e.g., "cat vs. dog"). Verify whether the same heads mediate different conflict types or if conflict resolution is modular.
2. **Attention vs. Gradient Ablation Robustness**: Systematically compare attention-based and gradient-based patch ablations across a range of τ thresholds and gradient variants (vanilla, smooth, integrated). Report both accuracy recovery and correlation with human-annotated causal regions.
3. **Temporal Stability of Head Roles**: Freeze the model after initial analysis, then repeat the Logit Lens and intervention experiments after 1000 additional training steps (if fine-tuning data is available) or after pruning low-magnitude weights. Assess whether factual/counterfactual head identities remain stable or shift with model updates.