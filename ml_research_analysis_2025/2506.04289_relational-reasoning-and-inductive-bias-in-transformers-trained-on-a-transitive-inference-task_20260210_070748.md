---
ver: rpa2
title: Relational reasoning and inductive bias in transformers trained on a transitive
  inference task
arxiv_id: '2506.04289'
source_url: https://arxiv.org/abs/2506.04289
tags:
- transitive
- learning
- training
- in-context
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models perform transitive
  inference, a relational reasoning task requiring inference about indirectly related
  items, under different learning regimes. The authors compare in-weights learning
  (IWL) where models store information in network parameters, versus in-context learning
  (ICL) where models utilize information from input sequences.
---

# Relational reasoning and inductive bias in transformers trained on a transitive inference task

## Quick Facts
- **arXiv ID:** 2506.04289
- **Source URL:** https://arxiv.org/abs/2506.04289
- **Reference count:** 40
- **Primary result:** Transformers exhibit distinct inductive biases for transitive inference depending on whether information is stored in weights (IWL) or utilized from input context (ICL), with pre-training on structured tasks enabling in-context relational reasoning.

## Executive Summary
This paper investigates how transformer models perform transitive inference under different learning regimes, comparing in-weights learning (IWL) where models store information in parameters versus in-context learning (ICL) where models utilize input sequences. The authors find that IWL naturally induces generalization biases toward transitive inference, showing symbolic distance effects and terminal item effects despite training only on adjacent pairs. In contrast, ICL models develop match-and-copy strategies that fail to encode hierarchical relationships. However, pre-training transformers on in-context linear regression tasks enables successful in-context generalizable transitive inference without forming induction circuits. These findings are validated in large language models, demonstrating that linear geometric prompts improve transitive inference while circular geometries impair performance.

## Method Summary
The authors designed controlled experiments using synthetic transitive structures where models learn pairwise comparisons of items on a latent dimension (e.g., A > B > C > D > E). They trained transformers under two regimes: IWL where information is encoded in model parameters through standard training, and ICL where models must reason about relationships from input sequences. Performance was evaluated on non-adjacent pairs (e.g., A vs C, A vs D) to test transitive reasoning. The study also examined transformer behavior on large language models using geometric prompts and analyzed internal circuit representations to understand reasoning strategies. Pre-training interventions involved training transformers on linear regression tasks before evaluating transitive inference performance.

## Key Results
- IWL models exhibit symbolic distance effects (performance increasing with hierarchical separation) and terminal item effects (better performance on extreme items), despite only training on adjacent pairs
- ICL models develop match-and-copy induction circuits that fail to encode hierarchical relationships and do not generalize transitively
- Pre-training on in-context linear regression tasks enables transformers to perform in-context generalizable transitive inference without forming induction circuits
- Linear geometric prompts improve transitive inference in LLMs while circular geometries (violating transitivity) impair performance, especially when models cannot rely on stored knowledge

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how transformers represent and utilize relational information differently under IWL versus ICL regimes. In IWL, the model learns to compress the hierarchical structure into its parameters, creating representations that naturally capture transitive relationships. This results in symbolic distance effects because items separated by more steps have more distinct representations, and terminal items are better represented as they anchor the hierarchy. In ICL, without stored hierarchical knowledge, transformers must construct reasoning strategies on-the-fly, leading to match-and-copy circuits that bypass true relational understanding. Pre-training on structured tasks appears to provide the representational scaffolding needed for in-context relational reasoning.

## Foundational Learning
- **Transitive inference**: The ability to infer relationships between items not directly compared (e.g., if A > B and B > C, then A > C). Why needed: Forms the core cognitive capability being tested.
- **In-weights learning (IWL)**: Learning where information is stored in model parameters through training. Why needed: Enables models to develop internal representations of hierarchical structure.
- **In-context learning (ICL)**: Learning where models utilize information from input sequences without parameter updates. Why needed: Tests the model's ability to reason about relationships in real-time.
- **Symbolic distance effects**: Performance improvement as the hierarchical separation between items increases. Why needed: Indicates whether models encode the underlying structure rather than memorizing specific pairs.
- **Terminal item effects**: Better performance on extreme items in a hierarchy. Why needed: Suggests models have learned the directional properties of the ordering.
- **Induction circuits**: Internal mechanisms that implement rule-based strategies rather than true understanding. Why needed: Helps explain why ICL models fail at transitive reasoning.

## Architecture Onboarding

**Component Map:**
Input Sequence -> Attention Mechanism -> Feed-Forward Networks -> Output Layer -> Parameter Storage (IWL) OR Context Window (ICL)

**Critical Path:**
The critical path for transitive inference differs fundamentally between IWL and ICL. In IWL, information flows from parameter storage through the attention mechanism to output, with hierarchical representations stored during training. In ICL, the path must construct reasoning strategies on-the-fly through attention patterns and context utilization without relying on stored knowledge.

**Design Tradeoffs:**
- IWL trades computational efficiency at inference for better generalization to transitive relationships
- ICL provides flexibility for novel relationships but fails to develop structural understanding
- Pre-training on structured tasks enables in-context reasoning but requires additional training overhead
- Match-and-copy strategies are computationally simple but fail on truly novel transitive relationships

**Failure Signatures:**
- ICL models consistently fail on non-adjacent pairs despite correct performance on adjacent pairs
- Match-and-copy circuits manifest as attention patterns that copy known relationships rather than inferring new ones
- Performance degrades on circular geometries that violate transitivity
- Terminal item advantages disappear when models cannot rely on stored knowledge

**Three First Experiments:**
1. Test IWL vs ICL performance on hierarchies with multiple incomparable elements to assess generalization beyond simple linear structures
2. Analyze attention patterns in ICL models to confirm match-and-copy strategies versus alternative reasoning mechanisms
3. Evaluate whether partial pre-training on structured tasks is sufficient to enable in-context transitive reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to synthetic transitive structures, raising questions about applicability to complex or naturalistic relational reasoning tasks
- Match-and-copy circuit analysis may not capture all possible in-context reasoning strategies
- Pre-training intervention results may reflect task-specific adaptation rather than general principles of inductive bias formation

## Confidence
- **High Confidence:** IWL models exhibit symbolic distance and terminal item effects; pre-trained transformers show improved in-context transitive reasoning
- **Medium Confidence:** Match-and-copy strategies underlie ICL failures; pre-training induces representations that scaffold in-context relational reasoning
- **Low Confidence:** Transformers "naturally induce" transitive inference bias under IWL (appears contingent on specific training conditions)

## Next Checks
1. Test IWL and ICL models on non-linear transitive structures (e.g., partially ordered sets with multiple incomparable elements) to determine whether symbolic distance effects generalize beyond simple linear hierarchies.
2. Conduct ablation studies on the specific pre-training tasks to isolate which aspects of linear regression training (e.g., pattern regularity, numerical reasoning, or geometric consistency) are necessary for inducing transitive reasoning capabilities.
3. Evaluate whether alternative architectural modifications to transformers (e.g., attention mechanisms that explicitly track hierarchical depth) can enable ICL models to perform transitive inference without requiring pre-training on structured tasks.