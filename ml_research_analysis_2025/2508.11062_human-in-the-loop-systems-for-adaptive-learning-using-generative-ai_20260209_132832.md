---
ver: rpa2
title: Human-in-the-Loop Systems for Adaptive Learning Using Generative AI
arxiv_id: '2508.11062'
source_url: https://arxiv.org/abs/2508.11062
tags:
- feedback
- learning
- systems
- students
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Human-in-the-Loop (HITL) generative AI system
  for personalized learning that integrates student feedback into AI-generated solutions
  through predefined feedback tags. The system uses prompt engineering and a Retrieval-Augmented
  Generation (RAG) architecture to adapt content in real time based on student critiques.
---

# Human-in-the-Loop Systems for Adaptive Learning Using Generative AI

## Quick Facts
- arXiv ID: 2508.11062
- Source URL: https://arxiv.org/abs/2508.11062
- Reference count: 21
- Primary result: Static onboarding metadata achieved highest personalization (Adaptability 5.43), while RAG achieved near-perfect correctness (9.70); live feedback showed only modest impact due to low usage (9.5%).

## Executive Summary
This paper presents a Human-in-the-Loop (HITL) generative AI system for personalized learning that integrates student feedback into AI-generated solutions through predefined feedback tags. The system uses prompt engineering and a Retrieval-Augmented Generation (RAG) architecture to adapt content in real time based on student critiques. Evaluation across four parallel pipelines (Personalized + Feedback, Personalized, RAG, and LLM) showed that static onboarding metadata produced the highest personalization scores (Adaptability 5.43), while RAG achieved near-perfect correctness (9.70). Live feedback tags showed only modest impact (Adaptability 4.55), likely due to low usage (9.5% of turns) and limited granularity. The findings suggest that combining static user profiles with retrieval grounding offers the strongest framework for personalized, adaptive learning, while highlighting the need for richer, more engaging feedback mechanisms.

## Method Summary
The system uses a Flask web service with MongoDB backend to implement four parallel query pipelines: (1) Personalized + Feedback with metadata and tags, (2) Personalized with metadata only, (3) RAG with textbook retrieval (top 10 passages, cosine similarity ≥ 0.8), and (4) plain LLM baseline. Users complete an onboarding questionnaire providing experience level, learning style, goals, and challenges, which are stored in the prompt template. The RAG pipeline retrieves relevant textbook passages for grounding. Feedback tags (Excellent to Terrible) are displayed inline after each bot reply and injected into the next turn's prompt for the Personalized + Feedback pipeline. All responses are logged and evaluated post-hoc using a GPT-based scorer measuring Correctness, Clarity, Readability, and Adaptability.

## Key Results
- Static onboarding metadata achieved highest personalization (Adaptability 5.43) compared to live feedback integration (4.55).
- RAG pipeline achieved near-perfect factual correctness (9.70) versus LLM baseline (7.89).
- Live feedback tags were used sparingly (9.5% of turns), showing only modest impact on personalization.
- Four-pipeline comparison enabled controlled evaluation of personalization, feedback, and retrieval mechanisms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Static onboarding metadata provides reliable, persistent personalization signals that outperform real-time feedback for adapting AI responses.
- Mechanism: User profiles (experience level, learning style, goals) captured at session start are injected into the prompt template via LangChain, conditioning every response on stable learner characteristics. This avoids the signal-noise problem of sparse, inconsistent live feedback.
- Core assumption: Learner preferences and background remain sufficiently stable across a session to justify one-time capture.
- Evidence anchors:
  - [abstract] "The system uses a tagging technique and prompt engineering to personalize content"
  - [section IV, Table II] "Personalized" pipeline achieved highest Adaptability (5.43) vs. "Personalized + Feedback" (4.55)
  - [corpus] Related work (Valencia-Vallejo et al.) shows metacognitive scaffolding improves self-efficacy, but doesn't address persistence of onboarding signals specifically—weak direct corpus support
- Break condition: If learner needs shift dramatically mid-session (e.g., domain switch, frustration spike), static profiles may mismatch and require re-onboarding or dynamic profile updates.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) anchoring in domain-specific textbooks reduces hallucinations and improves factual correctness more reliably than prompt-only personalization.
- Mechanism: The RAG pipeline retrieves the top 10 textbook passages (cosine similarity ≥ 0.8) from a MongoDB vector index, concatenates them with chat history, and uses this context to ground generation—explicitly omitting personalization fields to isolate retrieval effects.
- Core assumption: The vector index contains high-quality, domain-relevant content and similarity thresholds correctly surface useful passages.
- Evidence anchors:
  - [section III.C] "RAG pipeline retrieves the top ten textbook passages (cosine-similarity ≥ 0.8)"
  - [section IV, Table II] RAG Correctness = 9.70 vs. LLM baseline = 7.89
  - [corpus] Izacard et al. (Atlas) demonstrate RAG improves few-shot learning; Lewis et al. show RAG reduces hallucination—supportive but not domain-specific to education
- Break condition: If retrieval corpus is sparse, outdated, or poorly indexed, RAG may retrieve irrelevant passages, degrading both correctness and user trust.

### Mechanism 3
- Claim: Live feedback tags showed only modest impact due to low engagement and coarse granularity, not inherent mechanism failure.
- Mechanism: Five predefined tags (Excellent → Terrible) are displayed inline after each bot reply. Selection is recorded and fed into the next streaming request. However, tags are applied at turn-end (not mid-stream), limiting immediacy.
- Core assumption: Students will consistently engage with feedback UI if it's available.
- Evidence anchors:
  - [section IV] "Live feedback was used sparingly—only 19 tags were recorded across all turns" (9.5% usage)
  - [section V] "Our five-level tag set, applied at the end of each turn, may lack the granularity and immediacy needed"
  - [corpus] Van der Kleij et al. meta-analysis shows feedback effectiveness depends on timing and specificity—supports granularity critique but doesn't validate this specific tag design
- Break condition: If feedback fatigue exceeds perceived value, or tags don't map to actionable prompt modifications, the loop decouples from system behavior and users abandon it.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Understanding how external knowledge retrieval grounds LLM outputs is essential for diagnosing correctness vs. adaptability tradeoffs.
  - Quick check question: Can you explain why RAG improves factual accuracy but may not improve personalization without additional signals?

- Concept: **Prompt engineering with variable injection**
  - Why needed here: The system uses a unified prompt template with toggleable metadata/feedback fields; understanding this is critical for modifying personalization logic.
  - Quick check question: How would you add a new user attribute (e.g., "preferred detail level") to the prompt without breaking the four-pipeline comparison?

- Concept: **Human-in-the-Loop feedback dynamics**
  - Why needed here: The paper's central hypothesis is that student feedback improves AI responses; understanding feedback loop design (capture → transformation → injection) reveals why live tags underperformed.
  - Quick check question: What three factors would you test to diagnose why feedback tag usage was only 9.5%?

## Architecture Onboarding

- Component map: Frontend (Flask web service with chatbot UI, onboarding questionnaire, inline feedback buttons) -> Backend (MongoDB for session persistence + vector index) -> Four-pipeline query manager -> LangChain streaming calls

- Critical path:
  1. User completes onboarding questionnaire → metadata stored under session key
  2. User asks question → query manager spawns 4 parallel sessions (base key + suffixes)
  3. Each pipeline generates response using its configuration (metadata on/off, retrieval on/off)
  4. Responses displayed; feedback tags recorded for "Personalized + Feedback" pipeline only
  5. Feedback injected into next turn's prompt for that pipeline
  6. All turns logged; post-hoc evaluation runs GPT scorer

- Design tradeoffs:
  - Parallel pipelines enable controlled comparison but increase latency and cost (4x LLM calls per turn)
  - Static onboarding is simple but can't capture evolving needs
  - Five-level tags are easy to implement but lack specificity (e.g., no "too verbose" or "wrong code")
  - End-of-turn feedback injection is architecturally simple but limits real-time steering

- Failure signatures:
  - Low feedback usage (<10%): suggests UI friction, low perceived value, or tag fatigue
  - RAG retrieves irrelevant passages: check similarity threshold, embedding quality, corpus coverage
  - Adaptability scores diverge significantly from user self-reports: indicates evaluation metric may not capture subjective fit
  - Pipeline responses are identical: verify metadata/feedback fields are actually being injected into prompts

- First 3 experiments:
  1. **Granularity test**: Replace 5-level ordinal tags with multi-dimensional tags (e.g., clarity, correctness, detail-level) and measure usage rate and Adaptability change. Hypothesis: finer-grained tags increase both engagement and signal quality.
  2. **Mid-stream injection test**: Inject feedback immediately after tag selection (before next turn) via prompt append. Compare Adaptability scores to end-of-turn injection.
  3. **Profile staleness test**: After N turns, prompt users to re-rate experience level or goals. Compare Adaptability trajectories for refreshed vs. static profiles to identify when onboarding data decays.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hierarchical feedback taxonomy integrated mid-stream significantly increase user engagement and Adaptability scores compared to the current end-of-turn schema?
- Basis in paper: [explicit] The authors state the five-level tag set "may lack the granularity and immediacy" and suggest future work should involve "integrating tags mid-prompt" and a "richer, hierarchical feedback taxonomy."
- Why unresolved: The current implementation suffered from low engagement (tags used in only 9.5% of turns) and modest Adaptability gains, limiting the ability to assess the true potential of live feedback.
- What evidence would resolve it: A comparative study showing a statistically significant increase in tag usage rates and Adaptability scores when using mid-stream, granular tags versus static, end-of-turn buttons.

### Open Question 2
- Question: How strongly do GPT-based evaluation scores correlate with human expert assessments regarding the "Adaptability" and "Correctness" of educational responses?
- Basis in paper: [explicit] The authors note the evaluation "relies on a GPT-based evaluator for all scoring, which may introduce both topical and evaluator bias," and propose migrating to a "multi-annotator evaluation platform."
- Why unresolved: The study lacked human raters, meaning the validity of the high scores (e.g., Correctness 9.70 for RAG) remains unverified against human judgment.
- What evidence would resolve it: An inter-rater reliability analysis (e.g., Cohen's Kappa) comparing the GPT evaluator's scores against a panel of human instructors for the same response set.

### Open Question 3
- Question: What is the correlation between specific feedback tag usage patterns and longitudinal student learning gains or retention?
- Basis in paper: [explicit] Future work sections propose "correlation analyses between tag usage and learning gains" to determine the educational efficacy of the system.
- Why unresolved: The current study evaluated output quality metrics (Correctness, Adaptability) but did not measure the causal link between the feedback loop behavior and actual academic performance over time.
- What evidence would resolve it: Longitudinal data from real classroom deployments showing a statistically significant positive correlation between the frequency/type of feedback tags used and student performance on standardized assessments.

## Limitations
- The 9.5% feedback usage rate means conclusions about feedback effectiveness are based on sparse data, making it difficult to distinguish between mechanism failure and poor UI design.
- The GPT-based evaluator lacks ground-truth human judgments for cross-validation, leaving open whether automated scores align with student perceptions of quality.
- The "near-perfect" RAG correctness (9.70) may reflect the narrow, textbook-bounded domain rather than generalizable LLM grounding.

## Confidence
- **High confidence**: Static onboarding metadata improves personalization (Adapt. 5.43 vs. 4.55) because the design and results are internally consistent and match intuition about persistent user profiles.
- **Medium confidence**: RAG improves factual correctness (9.70) but this is domain-constrained; generalizability to broader domains is unknown.
- **Low confidence**: Live feedback tags have only modest impact—the low usage rate makes it impossible to distinguish between tag design flaws and inherent ineffectiveness.

## Next Checks
1. **Evaluate feedback UI with A/B testing**: Replace ordinal tags with multi-dimensional tags (clarity, correctness, detail) and measure usage rates and Adaptability changes; diagnose whether granularity or UI placement drives low engagement.
2. **Validate GPT evaluator with human ground truth**: Have human raters score a subset of responses on the same four metrics; compute correlation with GPT scores to assess evaluation reliability.
3. **Test profile staleness**: After N turns, prompt users to re-rate experience level or goals; compare Adaptability trajectories for refreshed vs. static profiles to identify when onboarding data decays.