---
ver: rpa2
title: 'Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level
  Code Generation'
arxiv_id: '2509.01129'
source_url: https://arxiv.org/abs/2509.01129
tags:
- problems
- code
- problem
- solverank
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SolveRank, a solution-aware retrieval model
  designed to enhance code generation performance on competitive programming problems.
  Unlike existing retrieval methods that focus on surface-level semantic similarity,
  SolveRank uses contrastive learning to identify problems with logically equivalent
  solutions, even when problem statements differ in wording or narrative context.
---

# Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation

## Quick Facts
- **arXiv ID**: 2509.01129
- **Source URL**: https://arxiv.org/abs/2509.01129
- **Reference count**: 6
- **Primary result**: SolveRank achieves 40.6% MRR improvement and up to 20% Pass@1 rate boost on difficult competitive programming problems by using contrastive learning to retrieve problems with logically equivalent solutions.

## Executive Summary
This paper introduces SolveRank, a solution-aware retrieval model designed to enhance code generation performance on competitive programming problems. Unlike existing retrieval methods that focus on surface-level semantic similarity, SolveRank uses contrastive learning to identify problems with logically equivalent solutions, even when problem statements differ in wording or narrative context. To train the model, synthetic problems are generated using DeepSeek-R1 and verified for logical equivalence by GPT-4o, with these serving as positive samples and BM25/random retrievals as negatives. Experiments on the xCodeEval dataset show that SolveRank significantly outperforms state-of-the-art retrieval methods, achieving 40.6% MRR improvement, and boosts code generation pass@1 rates by up to 20% on difficult problems. The method proves especially beneficial when problems require deep understanding of underlying solutions rather than simple semantic matching.

## Method Summary
SolveRank is a dense retrieval model that uses contrastive learning to identify competitive programming problems with logically equivalent solutions. The training pipeline generates synthetic problem pairs using DeepSeek-R1, which creates variations of existing problems while preserving their algorithmic logic. GPT-4o then verifies the logical equivalence of these synthetic pairs, filtering out failures. The model is trained using InfoNCE loss with synthetic logic-equivalent problems as positives and BM25-retrieved/random problems as negatives. At inference, the trained retriever fetches top-K reference problems, which are formatted as context for a downstream code generator like GPT-4o.

## Key Results
- SolveRank achieves 40.6% MRR improvement over state-of-the-art retrieval methods on the xCodeEval benchmark.
- The model boosts code generation pass@1 rates by up to 20% on difficult problems (problems requiring complex algorithmic understanding).
- Performance degrades on medium-difficulty problems where semantic retrieval methods prove more effective.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Solution-aware retrieval improves code generation performance on difficult competitive programming problems more effectively than semantic similarity-based retrieval.
- Mechanism: SolveRank uses contrastive learning to map problems that share the same underlying algorithmic logic (solution-level similarity) close together in the embedding space, while pushing apart problems that are semantically similar but algorithmically different. This allows a code generator to receive reference examples that demonstrate the correct solution approach, rather than potentially misleading surface-level matches.
- Core assumption: The quality and relevance of retrieved reference examples directly influences the code generation model's ability to solve a problem, especially for complex tasks requiring specific algorithmic insight.
- Evidence anchors:
  - [abstract] "SolveRank uses contrastive learning to identify problems with logically equivalent solutions, even when problem statements differ in wording or narrative context."
  - [abstract] "Experiments... show that SolveRank significantly outperforms state-of-the-art retrieval methods... and boosts code generation pass@1 rates by up to 20% on difficult problems."
  - [section 1] "for simple problems(<=1400), LLMs can directly generate correct code without RAG. But for more challenging problems(>1400), RAG proves to be highly beneficial."
  - [corpus] No direct corpus evidence compares solution-aware vs. semantic retrieval; OJBench notes general LLM difficulty on competitive programming.

### Mechanism 2
- Claim: A two-model pipeline using DeepSeek-R1 for generation and GPT-4o for verification can produce high-quality synthetic training data for solution-aware retrieval.
- Mechanism: DeepSeek-R1 is prompted to generate variations of a problem with different backgrounds but the same solution logic. GPT-4o is then used as a discriminator to verify that the generated problem shares the same core abstraction and algorithmic structure, filtering out failures. This creates a dataset of "positive pairs" for training.
- Core assumption: A strong LLM (DeepSeek-R1) can effectively rewrite problem statements while preserving logical structure, and another strong LLM (GPT-4o) can reliably verify this logical equivalence without human annotation.
- Evidence anchors:
  - [abstract] "synthetic problems are generated using DeepSeek-R1 and verified for logical equivalence by GPT-4o, with these serving as positive samples..."
  - [section 2.2] "To ensure true logical equivalence, we apply GPT-4o as an automatic verifier... A statistical comparison of synthetic and original problems is provided in Appendix 6."
  - [corpus] No corpus evidence validates this specific generation-verification pipeline for this task.

### Mechanism 3
- Claim: Forcing a retriever to discriminate between hard negatives (BM25/random retrieved problems) and synthetic positives during training creates a more robust embedding space for identifying deep logical similarity.
- Mechanism: The training objective uses InfoNCE loss with synthetic logic-equivalent problems as positives and a mix of BM25-retrieved (hard semantic negatives) and random problems as negatives. This forces the model to learn features that go beyond lexical overlap (which BM25 captures) to identify the algorithmic core of a problem.
- Core assumption: The selected hard negatives (BM25 results) effectively represent the common failure mode of semantic retrievers (getting distracted by surface similarity), making them ideal for teaching the model what *not* to retrieve.
- Evidence anchors:
  - [section 2.3] "The positive samples are drawn from the synthetic dataset Pq, while the negative samples Nq... consist of the top-5 retrieved by BM25 and 20 randomly sampled problems from the training corpus."
  - [section 3.2] "BM25 focuses on lexical overlap, leading to irrelevant results... SolveRank, through contrastive learning, captures algorithmic alignment and reasoning logic..."
  - [corpus] No corpus evidence directly discusses this specific negative sampling strategy.

## Foundational Learning

- **Dense Passage Retrieval (DPR) and Bi-Encoders**
  - Why needed here: SolveRank is built upon a DPR-style architecture, using a dual-encoder to map problems into a shared embedding space. Understanding how this dense retrieval works, as opposed to sparse methods like BM25, is fundamental to grasping the paper's approach.
  - Quick check question: How does a bi-encoder architecture differ from a cross-encoder in terms of computational cost at inference time, and why is the bi-encoder preferable for a retrieval task?

- **Contrastive Learning and InfoNCE Loss**
  - Why needed here: The core training mechanism of SolveRank is contrastive learning. The InfoNCE loss is the specific mathematical formulation used to pull positive pairs together and push negative pairs apart in the embedding space.
  - Quick check question: What is the effect of the temperature hyperparameter (Ï„) in the InfoNCE loss on the learned embedding space?

- **Retrieval-Augmented Generation (RAG) for Code**
  - Why needed here: The entire premise of the work is to improve code generation via RAG. Understanding how retrieved context is formatted and fed into a generative model is essential.
  - Quick check question: In a RAG pipeline, what are the potential downsides of retrieving irrelevant or noisy context for a large language model?

## Architecture Onboarding

- **Component map**: Synthetic Data Generator (DeepSeek-R1) -> Logical Verifier (GPT-4o) -> Retriever Training (SolveRank) -> Inference Pipeline (retrieval + code generation)
- **Critical path**: The entire system hinges on the **Synthetic Data Generator & Verifier**. If this stage produces low-quality data, the trained retriever will be flawed, and the entire pipeline fails. The quality of the retriever is the key bottleneck for improving downstream code generation on hard problems.
- **Design tradeoffs**: The primary tradeoff is between the cost/complexity of synthetic data generation and the performance gain. The paper relies on two proprietary, high-capability models for data creation, which may be expensive. A simpler tradeoff is choosing the difficulty threshold for RAG: using it on easy problems can hurt performance due to distraction.
- **Failure signatures**:
  1. **Semantic Hallucination**: The retriever returns problems with similar narrative themes but different algorithms, leading the generator astray.
  2. **Over-Abstraction**: The retriever focuses so much on deep logic that it retrieves examples that are too complex for medium-difficulty problems, causing a weaker generator to misapply complex logic.
  3. **Verification Failure**: The synthetic data contains "positive" pairs that are not actually logically equivalent, introducing noise into the training process.
- **First 3 experiments**:
  1. **Ablation on Verifier**: Retrain SolveRank using the raw output from DeepSeek-R1 *without* the GPT-4o verification step to measure the impact of noisy synthetic data.
  2. **Hard Negative Ablation**: Retrain SolveRank using only random negatives instead of the mix of BM25 and random negatives, to quantify the benefit of the specific hard-negative mining strategy.
  3. **Cross-Domain Generalization**: Evaluate the trained SolveRank model on a different dataset like APPS to test if the learned "solution-aware" embeddings generalize to a different distribution of competitive programming problems.

## Open Questions the Paper Calls Out

- **Can the SolveRank framework effectively generalize to broader software engineering tasks or multi-language code generation beyond competitive programming?**
  - Basis in paper: [explicit] The Limitations section states, "The generalizability of our framework to broader code generation domains, such as software engineering tasks or multi-language corpora, remains to be validated in future."
  - Why unresolved: The experiments were exclusively conducted on the xCodeEval benchmark, which focuses specifically on competitive programming tasks and specific algorithm classes.
  - What evidence would resolve it: Evaluation results showing SolveRank's performance on datasets representing industrial software engineering tasks (e.g., CodeSearchNet) or corpora spanning multiple programming languages.

- **How can reinforcement learning be integrated into the ranking process to further enhance retrieval performance?**
  - Basis in paper: [explicit] The Conclusion section explicitly lists this as a direction: "Future work will explore the reinforcement learning for ranking improvement."
  - Why unresolved: The current model relies on contrastive learning with static synthetic samples; the authors have not yet explored or defined the reward mechanisms or policy updates required for a reinforcement learning approach.
  - What evidence would resolve it: A modified training pipeline utilizing reinforcement learning that demonstrates superior MRR or Pass@1 scores compared to the current contrastive learning approach.

- **Is it possible to construct a dynamic retrieval strategy that switches between semantic and solution-aware retrieval based on problem characteristics to avoid the performance degradation seen in medium-difficulty tasks?**
  - Basis in paper: [inferred] The paper notes in Section 3.3 that SolveRank performs worse on medium problems because it introduces "unnecessary abstraction," whereas semantic retrievers work better for "implementation and greedy problems."
  - Why unresolved: The current implementation always applies solution-aware retrieval, which appears to act as noise for problems that rely on surface-level context or simulation rather than deep algorithmic logic.
  - What evidence would resolve it: A mechanism that classifies problem types (e.g., logic-heavy vs. implementation-heavy) and selectively applies SolveRank, resulting in recovered performance for medium-difficulty tasks.

## Limitations

- The paper does not specify the exact pre-trained encoder checkpoint used for the DPR model, which could affect reproducibility of results.
- The reported performance degradation on medium-difficulty problems indicates the method may not be universally beneficial across all problem types.
- The synthetic data generation pipeline, while described clearly, lacks external validation or ablation studies demonstrating its necessity.

## Confidence

- **High Confidence**: The core mechanism of using contrastive learning to learn solution-level similarity (Mechanism 1) and the specific performance improvements on xCodeEval (40.6% MRR, 20% Pass@1 on hard problems).
- **Medium Confidence**: The synthetic data generation pipeline using DeepSeek-R1 and GPT-4o verification is described clearly, but lacks external validation or ablation studies demonstrating its necessity.
- **Low Confidence**: The generalization claim to other competitive programming datasets (like APPS) is stated but not empirically tested in the paper.

## Next Checks

1. **Ablation Study**: Reproduce the retriever training without the GPT-4o verification step to quantify the impact of noisy synthetic data on retrieval performance.
2. **Negative Sampling Analysis**: Train a variant using only random negatives instead of the BM25 + random mix to isolate the contribution of hard negative mining.
3. **Cross-Dataset Evaluation**: Evaluate the trained SolveRank model on a different competitive programming dataset (e.g., APPS) to test generalization of the learned solution-aware embeddings.