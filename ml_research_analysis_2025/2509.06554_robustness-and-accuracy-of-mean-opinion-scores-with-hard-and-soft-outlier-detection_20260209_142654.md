---
ver: rpa2
title: Robustness and accuracy of mean opinion scores with hard and soft outlier detection
arxiv_id: '2509.06554'
source_url: https://arxiv.org/abs/2509.06554
tags:
- outlier
- methods
- detection
- quality
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive empirical worst-case analysis
  for evaluating outlier detection methods in subjective quality assessment. Traditional
  robustness measures based on RMSD between clean and noisy datasets proved unreliable
  indicators of actual accuracy.
---

# Robustness and accuracy of mean opinion scores with hard and soft outlier detection

## Quick Facts
- arXiv ID: 2509.06554
- Source URL: https://arxiv.org/abs/2509.06554
- Authors: Dietmar Saupe; Tim Bleile
- Reference count: 16
- This paper presents a comprehensive empirical worst-case analysis for evaluating outlier detection methods in subjective quality assessment.

## Executive Summary
This paper addresses the critical problem of protecting subjective quality assessment from malicious manipulation by presenting an adversarial framework for evaluating outlier detection methods. The authors demonstrate that traditional robustness metrics based on RMSD between clean and noisy datasets are unreliable indicators of actual accuracy. Using genetic algorithms to generate worst-case adversarial attacks, they systematically identify the most vulnerable scenarios for each detection method. The comprehensive empirical analysis reveals that simple hard outlier removal methods (NLL, MAZ, HB) significantly outperform complex soft methods (SUREAL, ZREC) in adversarial settings, with NLL achieving the lowest mean RMSE of 0.217.

## Method Summary
The paper employs a black-box adversarial attack framework using genetic algorithms to stress-test outlier detection methods in subjective quality assessment. The GA evolves attack matrices representing malicious observer ratings to maximize distortion of mean opinion scores, using RMSE against ground truth as the fitness function. The evaluation uses simulated datasets (250 runs) drawn from KonIQ-10k distributions with parameters I=30 observers, J=20 stimuli, and K=5 attackers. The framework compares both hard methods (HB, MAZ, NLL) and soft methods (SUREAL, ZREC, etc.) using multiple metrics including RMSE, RMSD, FPR, FNR, and remaining attacker influence (RAI).

## Key Results
- Traditional RMSD robustness metrics proved unreliable, with LPCC showing highest robustness but lowest accuracy
- Hard outlier detection methods (HB, MAZ, NLL) achieved superior worst-case performance with RMSEs around 0.2
- NLL with threshold 1.31 achieved the lowest mean RMSE of 0.217, significantly outperforming soft methods
- Soft methods (SUREAL, ZREC) showed remaining attacker influence > 0.2, indicating attackers remained significantly influential

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Black-box adversarial attacks generated via genetic algorithms effectively reveal worst-case vulnerabilities that synthetic noise tests miss.
- **Mechanism:** The GA treats ratings of malicious observers as a genome (K×J matrix) and evolves attacks over 300 generations using selection, crossover, and mutation to maximize RMSE between reconstructed MOS and ground truth.
- **Core assumption:** Simulation model derived from KonIQ-10k accurately approximates real-world subject behavior and biases.
- **Evidence anchors:** [Abstract] describes evolutionary optimization maximizing distortion; [Section III.B] details GA configuration; [Corpus] shows weak/neighbors focus on generic outlier detection.

### Mechanism 2
- **Claim:** Iterative hard outlier removal based on NLL provides superior resistance to adversarial manipulation compared to soft-weighting or standard ITU methods.
- **Mechanism:** Calculates likelihood of observer ratings given current consensus distribution, iteratively removing observer with maximum NLL until all remaining fall below threshold (NLL ≤ 1.31).
- **Core assumption:** Initial probability distribution calculation is not immediately corrupted by attackers before first iteration.
- **Evidence anchors:** [Section II.A] describes iterative NLL process; [Table II] shows NLL achieves mean RMSE of 0.217; [Corpus] missing.

### Mechanism 3
- **Claim:** Traditional robustness metrics (RMSD) are fundamentally decoupled from actual accuracy (RMSE vs ground truth).
- **Mechanism:** A method can appear "robust" by being unresponsive to input changes, rejecting almost all observers while having poor accuracy. LPCC was most "robust" against spammers but had worst accuracy.
- **Core assumption:** Access to reliable ground truth via simulation is possible to calculate RMSE.
- **Evidence anchors:** [Abstract] states traditional RMSD measures proved unreliable; [Table I] shows LPCC with Robustness Rank 1 and Accuracy Rank 7; [Corpus] missing.

## Foundational Learning

- **Concept:** Absolute Category Rating (ACR) & Mean Opinion Score (MOS)
  - **Why needed here:** The entire paper revolves around defending the MOS derived from ACR against distortion.
  - **Quick check question:** If an observer rates a stimulus as "5" but the ground truth MOS is "2", how does this impact the RMSE calculation?

- **Concept:** Genetic Algorithms (GA) / Evolutionary Optimization
  - **Why needed here:** The paper uses GA as the "stress test" engine to generate worst-case attacks.
  - **Quick check question:** Why is a "black-box" attack using a GA more dangerous to a defense system than a predefined "random clicker" attack?

- **Concept:** False Positive Rate (FPR) vs. False Negative Rate (FNR) in Outlier Detection
  - **Why needed here:** Results show methods fail due to either high FPR (rejecting good users) or high FNR (keeping attackers).
  - **Quick check question:** In subjective quality assessment, is it better to risk rejecting a valid user (High FPR) or keeping a malicious one (High FNR)?

## Architecture Onboarding

- **Component map:** Data Generator -> Adversarial Optimizer (GA) -> Detection & Reconstruction Core -> Evaluator
- **Critical path:** The interaction between the Adversarial Optimizer and the Detection Core, where GA exploits any thresholding logic in the Detection Core.
- **Design tradeoffs:**
  - Complexity vs. Robustness: Low-complexity hard methods outperform complex soft methods in worst-case scenarios
  - Sensitivity vs. Stability: High sensitivity improves accuracy in adversarial settings but risks data loss in clean settings
- **Failure signatures:**
  - LPCC Mode: Extremely high RMSD stability but massive error (RMSE > 1.0) from rejecting nearly all observers (FPR > 0.9)
  - Soft Method Collapse: Soft methods show RAI > 0.2, meaning attackers still significantly skew results (RMSE > 0.3)
- **First 3 experiments:**
  1. Baseline Reproduction: Run code on KonIQ-10k simulation with NLL and LPCC methods to verify LPCC's paradoxical performance
  2. Threshold Sweep: Vary NLL termination threshold between 0.5 and 2.0 to plot FPR vs Adversarial RMSE trade-off
  3. Attacker Saturation: Increase attackers K from 1 to 10 to determine "break point" where NLL/MAZ methods fail

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance ranking of outlier detection methods change when evaluated against common, non-adversarial noise compared to worst-case adversarial attacks?
  - **Basis in paper:** [explicit] Section V suggests studying accuracy for restricted, more common forms of unreliability
  - **Why unresolved:** Current study focused exclusively on stress-testing methods using genetic algorithms
  - **What evidence would resolve it:** Comparative study evaluating RMSE on datasets with stochastic noise models representing fatigue or lapses in attention

- **Open Question 2:** To what extent does the choice of SUREAL subject model as ground truth generator bias the performance evaluation of outlier detection methods?
  - **Basis in paper:** [explicit] Section V notes ground truth "is derived from statistical subject model in SUREAL, which may still be a simplification"
  - **Why unresolved:** Simulation framework generates synthetic ratings based on SUREAL parameters, potentially favoring similar methods
  - **What evidence would resolve it:** Repeating adversarial analysis using alternative ground truth models or datasets with known physical ground truths

- **Open Question 3:** Can differentiable optimization methods effectively replace combinatorial genetic algorithms for generating adversarial attacks on continuous VAS data?
  - **Basis in paper:** [explicit] Section V suggests replacing combinatorial optimization with faster differentiable methods for VAS data
  - **Why unresolved:** Proposed framework relies on genetic algorithm for discrete ACR ratings, which is computationally expensive
  - **What evidence would resolve it:** Implementation of gradient-based attack framework for VAS data and comparison of convergence speed and attack success rate

## Limitations
- The ecological validity of simulation model depends on KonIQ-10k distributions accurately capturing real-world subject behavior
- GA's ability to find true global worst cases is limited by search space size and computational budget (300 generations, population 150)
- The 1.31 threshold for NLL termination lacks theoretical justification and appears tuned to simulation results

## Confidence

- **High Confidence:** Demonstration that traditional RMSD robustness metrics are decoupled from accuracy is well-supported by direct comparison in Table I showing LPCC's paradoxical performance
- **Medium Confidence:** GA framework's effectiveness in finding worst-case attacks is demonstrated but could be limited by local optima or incomplete search
- **Low Confidence:** Claims about real-world applicability depend heavily on simulation model's fidelity to actual subjective quality assessment scenarios

## Next Checks

1. **Cross-Dataset Validation:** Apply adversarial framework to subjective quality datasets from different domains (audio, text, multimedia) to verify NLL's superiority generalizes beyond image-based KonIQ-10k distributions

2. **Adaptive Attack Testing:** Implement adaptive attack that specifically targets NLL termination threshold by mimicking inlier distributions until final iteration, measuring whether NLL's FPR increases under sustained adversarial pressure

3. **Ground Truth Fidelity Analysis:** Systematically vary simulation parameters (subject count I, attacker count K, stimulus count J) and measure how correlation between adversarial RMSE and real-world detection performance changes, establishing confidence bounds on simulation-based conclusions