---
ver: rpa2
title: Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid
  Care Management
arxiv_id: '2509.09655'
source_url: https://arxiv.org/abs/2509.09655
tags:
- coverage
- harm
- fg-farl
- learning
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Feasibility-Guided Fair Adaptive Reinforcement
  Learning (FG-FARL), an offline reinforcement learning method for equitable care
  coordination in Medicaid programs. The approach calibrates per-group safety thresholds
  to reduce harm while equalizing coverage or harm across protected subgroups.
---

# Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management

## Quick Facts
- **arXiv ID:** 2509.09655
- **Source URL:** https://arxiv.org/abs/2509.09655
- **Reference count:** 18
- **Primary result:** Offline RL method for equitable Medicaid care coordination with calibrated per-group safety thresholds and fairness dials

## Executive Summary
This paper introduces Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline reinforcement learning method designed to improve equity in Medicaid care coordination. The approach calibrates per-group safety thresholds to minimize harm while equalizing coverage or harm across protected subgroups. Using longitudinal Medicaid data, FG-FARL demonstrates comparable overall performance to standard baselines while significantly improving fairness metrics across demographic groups including age, race, and sex.

## Method Summary
FG-FARL operates as an offline reinforcement learning framework that explicitly incorporates fairness constraints and feasibility screening. The method adapts per-group safety thresholds based on historical data patterns, allowing for differential risk tolerance across protected subgroups. It includes explicit "fairness dials" that can be adjusted to prioritize either equitable coverage or equitable harm reduction. The algorithm uses off-policy value estimation for evaluation and includes mechanisms to screen out potentially harmful actions based on learned feasibility bounds.

## Key Results
- Comparable overall value to behavior cloning and HACO baselines (off-policy estimates: -0.157 vs -0.157-0.158)
- Reduced subgroup value disparities across protected demographic groups
- More equitable safe-set coverage across age, race, and sex groups

## Why This Works (Mechanism)
The method works by explicitly incorporating fairness constraints into the offline RL framework through per-group safety threshold calibration. By learning feasibility bounds from historical data, the algorithm can identify which actions are likely to be harmful for specific subgroups. The fairness dials allow practitioners to prioritize different equity objectives (coverage vs harm reduction) based on policy goals. The approach leverages the structure of Medicaid data to create personalized safety thresholds that account for varying risk profiles across demographic groups.

## Foundational Learning
- **Offline RL fundamentals**: Needed to understand how FG-FARL operates without online exploration; quick check: review behavior cloning vs model-based offline RL approaches
- **Feasibility screening**: Critical for identifying safe actions in high-stakes healthcare settings; quick check: understand how historical data bounds are used to screen actions
- **Group fairness metrics**: Essential for evaluating equity across protected subgroups; quick check: review demographic parity, equal opportunity, and related fairness definitions
- **Off-policy evaluation**: Used to estimate performance without online deployment; quick check: understand importance sampling and related OPE techniques
- **Reinforcement learning safety**: Important for high-stakes healthcare applications; quick check: review safe RL frameworks and constraint satisfaction
- **Healthcare policy considerations**: Context for why equity matters in Medicaid programs; quick check: understand Medicaid care coordination challenges

## Architecture Onboarding
**Component map:** Historical data -> Feasibility bounds -> Safety threshold calibration -> RL policy learning -> Fairness constraints -> Action selection
**Critical path:** Data preprocessing -> Feasibility bound estimation -> Per-group threshold calibration -> Policy optimization with fairness constraints -> Off-policy evaluation
**Design tradeoffs:** Prioritizes safety and fairness over potentially higher but riskier performance; explicit fairness dials trade off between coverage and harm reduction
**Failure signatures:** Distribution shift causing poor feasibility bounds; insufficient historical data for certain subgroups; fairness constraints creating infeasible policies
**First experiments:**
1. Validate feasibility bounds on held-out historical data
2. Test threshold calibration sensitivity to different fairness weights
3. Evaluate subgroup performance on synthetic data with known ground truth

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalization of fairness improvements across multiple demographic axes and the potential for compounded disparities in intersectional subpopulations. The authors note that interactions between protected attributes were not explored in their analysis.

## Limitations
- Performance relies entirely on off-policy value estimation without on-policy validation
- Only compared against basic baselines (behavior cloning and HACO), not recent fair RL methods
- Uses retrospective observational data without randomized treatment assignment, risking unmeasured confounding
- Does not explore intersectional subgroups or interactions between protected attributes

## Confidence
- **High confidence:** Per-group safety threshold calibration is technically sound; explicit fairness dials provide practical implementation path
- **Medium confidence:** Comparable value and reduced disparities are plausible but unverified without on-policy testing
- **Low confidence:** Claims about generalization across all subgroups and absence of unintended harms, as these depend heavily on data characteristics and unmeasured factors

## Next Checks
1. Conduct on-policy evaluation or A/B testing to confirm reported value estimates under actual deployment conditions
2. Replicate experiments with synthetic/semi-synthetic dataset where ground-truth treatment effects are known to isolate confounding effects
3. Test method's behavior on intersectional subgroups (e.g., race × age × sex) to identify hidden disparities