---
ver: rpa2
title: 'gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed
  Computing for Exact Gaussian Processes on 10 Million Data Points'
arxiv_id: '2512.06143'
source_url: https://arxiv.org/abs/2512.06143
tags:
- kernel
- points
- covariance
- data
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling Gaussian Process (GP)
  regression to massive datasets (over 10 million points) while maintaining exactness,
  high accuracy, and flexibility in kernel design. The core method, gp2Scale, introduces
  a new class of non-stationary compactly supported kernels that allow the GP to naturally
  discover sparse structure in the covariance matrix.
---

# gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points

## Quick Facts
- **arXiv ID:** 2512.06143
- **Source URL:** https://arxiv.org/abs/2512.06143
- **Reference count:** 28
- **Primary result:** Exact GP inference on 10 million points using compactly-supported non-stationary kernels with distributed computing

## Executive Summary
This paper presents gp2Scale, a framework for exact Gaussian Process regression on massive datasets (10+ million points) by introducing a new class of compactly-supported non-stationary kernels. The method discovers natural sparsity in the covariance matrix through kernel design, enabling efficient computation via sparse linear algebra solvers. By leveraging distributed computing for covariance matrix assembly and using Block-Metropolis-Hastings MCMC for hyperparameter optimization, gp2Scale achieves superior or competitive approximation performance compared to state-of-the-art methods across multiple benchmark datasets.

## Method Summary
gp2Scale uses compactly-supported non-stationary kernels (Wendland and bump functions) to convert dense covariance matrices into sparse ones by enforcing zero correlation beyond learned radii or conditions. The method employs distributed computing where dataset blocks are processed by workers that return sparse COO format results, assembled into CSR format on the host. Block-Metropolis-Hastings MCMC samples hyperparameters, with MINRES solvers handling the sparse linear systems. The framework maintains exactness while achieving favorable scaling, demonstrated on datasets ranging from 2,000 to 10 million points.

## Key Results
- Achieved RMSE of 22.85 on 3D temperature dataset with 10 million points, outperforming Vecchia's 28.65
- Demonstrated exact GP inference on 10 million points using 1024 A100 GPUs
- Showed competitive or superior performance vs. SVGP, VNNGP, SKI, and Vecchia across all benchmark datasets
- Maintained exactness while achieving sparsity ratios of 10^-5 to 10^-3 in covariance matrices

## Why This Works (Mechanism)

### Mechanism 1: Sparsity Discovery via Compact Support
The method converts dense covariance matrices into sparse ones by using compactly supported kernels that return exactly zero for points outside learned radii. This "natural sparsity" allows storage in sparse formats and avoids O(N^3) dense matrix inversion. The underlying data must contain uncorrelated point pairs for efficiency gains; if all points are globally correlated, the matrix remains dense and computational benefits disappear.

### Mechanism 2: Far-Field Interaction via Bump Functions
Standard compact kernels mute long-range correlations, but gp2Scale introduces bump functions and delta masks that act as non-stationary switches. These allow activation of correlations between distant point sets even when spatially separated by regions of zero correlation, modeling sparse discrete long-range dependencies rather than smooth global fields.

### Mechanism 3: Distributed Covariance Assembly
The O(N^2) bottleneck of calculating covariance elements is mitigated by parallelizing evaluation and assembly. The dataset partitions into blocks processed by distributed workers that return sparse COO results immediately, reducing communication overhead. The host assembles these into a global sparse CSR matrix for the solver.

## Foundational Learning

- **Sparse Matrix Formats (COO vs. CSR):** Workers return COO format for easy distributed assembly, but the host requires CSR for efficient arithmetic operations with the MINRES solver. Why does this format conversion matter for computational efficiency?
- **The "Nugget" vs. Numerical Stability:** Adding Matérn kernel or diagonal dominance to the delta-kernel ensures invertibility of the matrix. Without this diagonal term, rank-1 modifications from bump functions could leave the matrix singular. Why is $\sigma^2_s k_W$ added to the rank-1 bump product in Equation 8?
- **Stationary vs. Non-Stationary Kernels:** Stationary kernels depend only on distance |x_i - x_j|, while these depend on location x_i, allowing modeling of changing behavior across the domain. How does the kernel in Equation 6 use $\Sigma(x)$ to change the correlation length scale based on location?

## Architecture Onboarding

- **Component map:** Data Partitioner -> Dask Workers (GPUs) -> Host Node -> BMH-MCMC Controller
- **Critical path:** Covariance matrix assembly remains O(N^2) operations, making distributed computation the rate-limiting step despite fast sparse linear solves.
- **Design tradeoffs:** Exactness vs. hardware requirements (1024 A100s for 10M dataset); flexibility vs. tuning complexity for bump functions.
- **Failure signatures:** Memory overflow if learned support radii expand (matrix re-densifies); oversmoothing if MCMC fails or bump functions are suppressed.
- **First 3 experiments:**
  1. Unit Test (1D Synthetic): Implement f₁(x) example, verify distributed COO assembly matches dense numpy for N=2000
  2. Sparsity Profiling: Run Topography dataset (20k points), measure sparsity ratio, confirm log-determinant calculation speedup
  3. Scale Test (Distributed): Run 3D Temperature dataset on increasing subsets (10k, 100k, 1M) to verify scaling efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does gp2Scale's reliance on discovering natural sparsity hold up in high-dimensional datasets where data scarcity prevents points from falling within compact support radii?
- **Basis in paper:** The Conclusion notes that on the 8-D CA Housing dataset, "sparsely distributed data points" made it "challenging to discover naturally occurring sparsity," limiting the method's value compared to Vecchia.
- **Why unresolved:** The paper identifies this as a failure case but does not offer a solution for high-dimensional domains where the "curse of dimensionality" inevitably makes data sparse.
- **What evidence would resolve it:** Benchmarking gp2Scale on datasets with varying intrinsic dimensionality to identify the density thresholds required to maintain the advantages of exact inference.

### Open Question 2
- **Question:** Can the selection of the number of bump function terms ($U$ and $P$) be automated to ensure optimal rank and sparsity without relying on heuristics?
- **Basis in paper:** Appendix C.3 states the kernel "has a practical shortcoming: the choice of the number of terms" and suggests starting with bumps disabled, implying the current process requires manual tuning or intuition.
- **Why unresolved:** There is no mechanism described to dynamically adjust the number of bump functions ($U$) or their components ($P$) based on the data structure during training.
- **What evidence would resolve it:** A modification to the framework that integrates the selection of $U$ and $P$ into the marginal likelihood optimization or uses a pruning/growth strategy during MCMC.

### Open Question 3
- **Question:** What is the convergence behavior and accuracy ceiling of the Block-Metropolis-Hastings sampler when running a full optimization on the 10-million data point regime?
- **Basis in paper:** Section 5.5 states the 10M point run was "well-performing but not yet optimal" because only 100 MCMC iterations were completed due to time constraints.
- **Why unresolved:** It is unclear if the competitive results (RMSE 22.85) represent the limit of the method's accuracy or if significantly more compute time would yield diminishing returns.
- **What evidence would resolve it:** A completed run involving thousands of MCMC iterations on the 10M dataset with convergence diagnostics (e.g., Gelman-Rubin statistics) to confirm posterior stability.

## Limitations

- High computational resource requirements (1024 A100 GPUs for 10M dataset) create significant barriers to practical adoption
- Method struggles with high-dimensional datasets where sparse data distribution prevents natural sparsity discovery
- Critical implementation details (MCMC configuration, bump function initialization, chunk sizing) remain underspecified, impacting reproducibility

## Confidence

- **High Confidence:** Core mechanism of using compactly-supported kernels to induce sparsity is well-established; distributed COO assembly is standard practice
- **Medium Confidence:** Superior performance claims on benchmarks appear robust, though exact hyperparameter tuning remains opaque
- **Low Confidence:** Bump function implementation details and effectiveness across varied domains are not fully transparent; temperature dataset results rely on domain-specific initialization

## Next Checks

1. Reproduce the 1D synthetic experiment (f₁(x)) to verify RMSE ≈ 0.107 and confirm sparsity discovery mechanism works for controlled test case
2. Profile sparsity ratios across five benchmark datasets (targeting 10^-5 to 10^-3 range) to validate Wendland and bump kernels are discovering natural sparsity
3. Test robustness to initialization by running MNIST experiment with random vs. domain-informed hyperparameter starting points to assess sensitivity to bump function placement strategy