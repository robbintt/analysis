---
ver: rpa2
title: 'Beyond the Black Box: Interpretability of LLMs in Finance'
arxiv_id: '2505.24650'
source_url: https://arxiv.org/abs/2505.24650
tags:
- financial
- interpretability
- feature
- features
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mechanistic interpretability for large language
  models (LLMs) in finance, addressing the need for transparency in AI-driven decision-making.
  By reverse-engineering model internals, the approach dissects activations and circuits
  to reveal how specific features influence predictions, enabling both observation
  and intervention.
---

# Beyond the Black Box: Interpretability of LLMs in Finance

## Quick Facts
- arXiv ID: 2505.24650
- Source URL: https://arxiv.org/abs/2505.24650
- Authors: Hariom Tatsat; Ariye Shater
- Reference count: 9
- One-line primary result: Mechanistic interpretability framework improves LLM transparency in financial tasks, reducing sentiment misclassification by 296 instances through feature steering.

## Executive Summary
This paper introduces mechanistic interpretability for large language models (LLMs) in finance, addressing the critical need for transparency in AI-driven financial decision-making. By reverse-engineering model internals through techniques like sparse autoencoders, logit attribution, and activation patching, the authors reveal how specific features influence predictions. The framework enables both observation and intervention in model behavior, demonstrating improved interpretability across tasks including sentiment analysis, credit risk assessment, bias detection, and hallucination mitigation.

## Method Summary
The framework combines Logit Lens for observing prediction evolution across layers, attribution patching for identifying causal attention heads, and sparse autoencoders (SAEs) for feature decomposition. SAEs disentangle polysemantic activations into interpretable feature vectors, while feature steering modifies model outputs by injecting latent vectors. The methodology was validated using GPT-2 and Gemma models on financial prompts, sentiment datasets, and credit risk assessments, with steering interventions reducing misclassification rates by 296 instances.

## Key Results
- Feature steering reduced sentiment classification errors by 296 instances in financial news analysis
- Attribution patching identified layers 8 and 10 as critical for financial reasoning in GPT-2
- SAE features successfully mapped to financial concepts like credit risk and sentiment
- Bias ratio improved through targeted feature interventions

## Why This Works (Mechanism)

### Mechanism 1
Sparse Autoencoders decompose polysemantic neuron activations into interpretable, monosemantic feature vectors. SAEs expand the activation space and enforce an L₁ sparsity penalty, forcing the model to represent input data using a sparse set of dictionary features. Core assumption: features are linearly represented in activation space and can be disentangled from superposition. Evidence: Section 3.1 defines SAE architecture and loss function; Section 6.1.1 demonstrates extraction of finance-specific features.

### Mechanism 2
Feature steering enables causal modification of model outputs without retraining. A steering vector is injected into the SAE's hidden state, amplifying or suppressing specific feature directions during forward pass. Core assumption: scaling a specific latent direction corresponds linearly to scaling the semantic concept it represents. Evidence: Section 3.1.3 formulates steering equation; Section 6.3.2 shows steering "credit risk" features reduced misclassification rates by 296 instances.

### Mechanism 3
Attribution patching identifies specific attention heads responsible for correct financial reasoning. By running clean and corrupted prompts and patching activations, one measures the change in logit difference. A significant change indicates a specific head is causally necessary for the prediction. Core assumption: logit difference is a reliable proxy for reasoning correctness. Evidence: Section 5.2 describes clean/corrupted prompt structure; Section 5.2 Figure 8 heatmaps identify layers 8 and 10 as critical for financial prompts.

## Foundational Learning

- **Superposition & Polysemanticity**: Neurons respond to multiple unrelated concepts, creating compression problems that SAEs solve. Why needed: understanding this compression problem explains why SAEs are needed to untangle features. Quick check: If a neuron activates for both "Apple Inc." and "Granny Smith apples," how does an SAE separate these into distinct features?

- **Residual Stream & Logit Lens**: The residual stream accumulates information layer-by-layer, and Logit Lens projects intermediate states to vocabulary space. Why needed: understanding this accumulation is essential to interpret how predictions evolve. Quick check: Why might Logit Lens show a "buy" prediction forming at layer 8 but solidifying only at layer 12?

- **Causal Tracing / Intervention**: Distinguishes between correlation and causation in feature importance. Why needed: unlike correlation-based importance, this paper uses intervention to determine if changing a neuron actually causes a change in assessment. Quick check: In attribution patching, why compare clean and corrupted runs rather than just analyzing the clean run?

## Architecture Onboarding

- **Component map**: GPT-2/Gemma (Transformer) -> Logit Lens/SAEs (Probe) -> Attribution Patching/Feature Steering (Intervention) -> Confusion Matrices/Bias Ratios (Validation)

- **Critical path**: Hook into Transformer's residual stream or MLP output → Train/load pre-trained Sparse Autoencoder → Identify feature indices for target concepts → Inject steering vector during inference

- **Design tradeoffs**: Larger SAE dictionaries offer granular features but increase compute; higher steering magnitude ensures effect visibility but risks output incoherence; patching by head is precise but computationally heavy

- **Failure signatures**: High SAE reconstruction loss indicates feature capture failure; steering invariance suggests feature not causal or timing issue; interpretability illusion occurs when SAE features appear monosemantic but react to unrelated text clusters

- **First 3 experiments**: 1) Logit Lens Baseline: Run financial prompts through GPT-2 and plot probability evolution across layers; 2) Feature Discovery: Load Gemma Scope SAE, pass credit risk documents, identify top 5 activating latent features; 3) Steering Validation: Take mixed sentiment financial news dataset, steer identified "credit risk" feature, measure confusion matrix shift

## Open Questions the Paper Calls Out

- **Hybrid Approach Integration**: How can mechanistic interpretability frameworks effectively integrate unstructured LLM processing with traditional statistical modeling of financial time series data? The paper identifies this as promising future work but doesn't implement such a system. Resolution would require a working framework demonstrating mathematical combination of SAE features with quantitative time-series inputs.

- **Optimal Steering Magnitude**: What methodologies can determine optimal magnitude for feature steering to prevent entangled representations or unintended trade-offs? While steering works, the relationship between strength and model integrity is unpredictable. Resolution would require metrics or guardrails quantifying the trade-off between steering effectiveness and representational damage.

- **Scalability Automation**: How can interpretability techniques be automated to scale effectively for large models where manual circuit analysis is infeasible? Current techniques rely heavily on human evaluation, which doesn't scale with model parameter count. Resolution would require successful application of automated circuit discovery on frontier models without human verification for every feature.

## Limitations
- Scalability challenges for larger models beyond GPT-2 and Gemma-2B remain unaddressed
- Dataset specifics and training hyperparameters are not fully specified, limiting reproducibility
- Generalization of feature steering effectiveness across different financial contexts is uncertain
- Potential adversarial scenarios where malicious actors might exploit interpretability insights are not discussed

## Confidence
- **High confidence**: Mechanistic interpretability framework using Logit Lens and attribution patching is well-established and reproducible
- **Medium confidence**: Effectiveness of feature steering for reducing misclassification rates is demonstrated but may not generalize to all financial tasks
- **Low confidence**: Claims about bias detection and hallucination mitigation lack detailed experimental validation

## Next Checks
1. **Dataset transparency validation**: Request and verify exact financial datasets used, including annotation guidelines, inter-annotator agreement scores, and class distribution statistics
2. **Hyperparameter sensitivity analysis**: Systematically vary SAE sparsity penalties, steering magnitudes, and attribution patching thresholds to establish robustness and identify optimal ranges
3. **Generalization benchmark**: Test the same interpretability pipeline on a larger financial LLM (e.g., BloombergGPT or FinGPT) to validate scalability and effectiveness across model architectures