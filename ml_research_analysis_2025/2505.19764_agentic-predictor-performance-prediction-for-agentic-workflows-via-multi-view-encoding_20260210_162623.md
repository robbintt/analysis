---
ver: rpa2
title: 'Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View
  Encoding'
arxiv_id: '2505.19764'
source_url: https://arxiv.org/abs/2505.19764
tags:
- agentic
- workflow
- performance
- workflows
- agenticpredictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgenticPredictor, a lightweight performance
  prediction framework for agentic workflows that leverages multi-view encoding and
  cross-domain unsupervised pretraining. The framework addresses the challenges of
  workflow heterogeneity and limited labeled data by encoding agentic workflows through
  graph structures, code semantics, and prompt embeddings, then pretraining these
  representations across diverse domains before fine-tuning on small labeled datasets.
---

# Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding

## Quick Facts
- arXiv ID: 2505.19764
- Source URL: https://arxiv.org/abs/2505.19764
- Authors: Patara Trirat; Wonyong Jeong; Sung Ju Hwang
- Reference count: 34
- Key outcome: AgenticPredictor achieves up to 12.12% improvement in prediction accuracy and 15.16% improvement in workflow utility over strong baselines through multi-view encoding and cross-domain unsupervised pretraining.

## Executive Summary
This paper introduces AgenticPredictor, a lightweight performance prediction framework for agentic workflows that leverages multi-view encoding and cross-domain unsupervised pretraining. The framework addresses the challenges of workflow heterogeneity and limited labeled data by encoding agentic workflows through graph structures, code semantics, and prompt embeddings, then pretraining these representations across diverse domains before fine-tuning on small labeled datasets. Experiments on a benchmark spanning code generation, math problem solving, and reasoning tasks show that AgenticPredictor achieves up to 12.12% improvement in prediction accuracy and 15.16% improvement in workflow utility over strong baselines. Ablation studies confirm the effectiveness of the multi-view design and pretraining, with consistent performance gains observed even under low-label regimes.

## Method Summary
AgenticPredictor employs a multi-view encoding approach to represent agentic workflows, combining graph structure (agent connectivity), code semantics (implementation logic), and prompt text (behavioral intent). Each view is processed by specialized encoders—a GNN for graph structure, MLPs for code and prompt views—then aggregated via learned attention. The framework optionally employs cross-domain unsupervised pretraining using reconstruction and contrastive objectives to structure the latent space before fine-tuning on small labeled datasets. A lightweight MLP predictor trained on the joint representation (workflow encoding + task embedding) predicts success probability, enabling efficient workflow ranking without costly LLM evaluations.

## Key Results
- Multi-view encoding achieves 84.38% accuracy, outperforming any two-view combination (max 82.15%)
- Cross-domain pretraining maintains ~73% accuracy at 10% label ratio versus ~70% for baselines
- Workflow utility improves by up to 15.16% over baselines through predictor-as-ranker setup
- Consistent performance gains across code generation, math problem solving, and reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-view encoding captures complementary workflow characteristics that single-view representations miss.
- Mechanism: Three specialized encoders process distinct modalities—graph structure (agent connectivity), code semantics (implementation logic), and prompt text (behavioral intent)—then aggregate via learned attention. This reduces representational bias by ensuring structural, computational, and semantic signals each contribute to the final embedding.
- Core assumption: Each view provides non-redundant predictive signal about workflow success; the aggregation layer can learn to weight views appropriately per task.
- Evidence anchors:
  - [abstract]: "multi-view representation learning of agentic systems by incorporating code architecture, textual prompts, and interaction graph features"
  - [section 3.3/Table 4]: Ablation shows full model (84.38% accuracy) outperforms any two-view combination (max 82.15%), confirming synergistic contribution.
  - [corpus]: Related work (GNNs as Predictors, FLORA-Bench) uses single graph-view encoding; no direct comparison of multi-view vs. single-view in corpus.
- Break condition: If code and prompt views provide highly correlated signals for a task domain, multi-view gains may diminish toward single-view baselines.

### Mechanism 2
- Claim: Cross-domain unsupervised pretraining enables sample-efficient fine-tuning when labeled workflow-performance pairs are scarce.
- Mechanism: Pretrain encoders on unlabeled workflows across domains using reconstruction (L_rec) and contrastive (L_con) objectives. Contrastive loss pulls together representations of workflows solving the same task successfully while pushing apart dissimilar ones. This creates semantically structured latent space before any performance labels are seen.
- Core assumption: Structural and semantic similarities between workflows correlate with performance similarity; pretraining objectives capture transferable structure.
- Evidence anchors:
  - [abstract]: "pretraining these representations across diverse domains before fine-tuning on small labeled datasets"
  - [section 4.3/Figure 3]: At 10% label ratio, AgenticPredictor+ maintains ~73% accuracy vs. ~70% for baselines; gap persists across all ratios tested.
  - [corpus]: No corpus papers evaluate unsupervised pretraining for workflow prediction; mechanism remains specific to this work.
- Break condition: If pretraining domains are too dissimilar from target domain, transfer may hurt rather than help (negative transfer not tested in paper).

### Mechanism 3
- Claim: A lightweight predictor trained on pretrained embeddings can rank candidate workflows accurately enough to replace expensive LLM executions during search.
- Mechanism: Joint representation F = [Z, T] concatenates workflow embedding with task encoder output. Binary classifier (MLP) predicts success probability. During search, sample K candidates, predict scores, evaluate only top-k—transforming random search into guided selection without complex heuristics.
- Core assumption: Relative ranking quality matters more than absolute probability calibration; predictor errors are systematic rather than random (won't consistently promote bad candidates).
- Evidence anchors:
  - [section 3.5]: "predictor-as-ranker setup transforms random search into a label-efficient guided procedure without requiring complex heuristics"
  - [Table 3]: Utility scores (ranking quality) improve up to 15.16% over baselines; accuracy improves up to 12.12%.
  - [corpus]: JudgeFlow uses evaluation signals for optimization; Shapley value paper analyzes component contributions—both assume evaluation access, not prediction.
- Break condition: If predictor is systematically biased toward certain workflow patterns (e.g., always preferring deeper graphs), search will converge to suboptimal regions.

## Foundational Learning

- Concept: **Graph Neural Networks (GNNs) for structured representation**
  - Why needed here: Graph encoder captures agent connectivity and message-passing patterns; must understand how GNNs aggregate neighbor information.
  - Quick check question: Given a 3-agent linear chain vs. fully-connected topology, would a 2-layer GNN produce different embeddings?

- Concept: **Contrastive learning objectives**
  - Why needed here: Pretraining uses contrastive loss to structure latent space; understanding positive/negative pair construction is essential.
  - Quick check question: If all workflows for a task are positive pairs regardless of success/failure, what inductive bias would the model learn?

- Concept: **Multi-modal fusion strategies**
  - Why needed here: Three encoder outputs must be combined; concatenation + MLP is chosen, but alternatives exist (attention, gating).
  - Quick check question: Why might simple concatenation fail if one view's embedding magnitude dominates others?

## Architecture Onboarding

- Component map:
  Input: Workflow W = (V, E, P, C) + Task description T
    ├─ Graph Encoder: GNN layers → CrossGraphAttn → AttenPool → Z_G
    ├─ Code Encoder: MLP on code embeddings → Z_C  
    ├─ Prompt Encoder: MLP on text embeddings → Z_P
    └─ Task Encoder: Pretrained LM embedding → T
  Fusion: Z = MLP([Z_G, Z_C, Z_P]), F = MLP([Z, T])
  Output: Performance predictor MLP → P(success)

  Pretraining (optional):
    Encoder → Latent Z → Decoder → Reconstruct (G, C, P)
    + Contrastive loss on Z

- Critical path:
  1. **Data preparation**: Collect unlabeled workflows for pretraining; small labeled set (workflow, task, pass/fail) for fine-tuning.
  2. **Pretraining phase**: Train encoder-decoder on reconstruction + contrastive objectives (20 epochs, batch 32 per paper).
  3. **Predictor training**: Freeze or fine-tune encoder; train classifier on labeled pairs using binary cross-entropy.
  4. **Deployment**: For new task, encode task + candidate workflows → rank by predicted score → evaluate top-k.

- Design tradeoffs:
  - **Pretraining vs. from-scratch**: Pretraining helps most at low label ratios (Figure 3); with abundant labels, direct supervision may suffice.
  - **Encoder complexity**: Paper uses 2-layer GNN, MLP for code/prompt; deeper encoders may overfit given limited workflow diversity.
  - **Binary vs. regression**: Paper uses binary success prediction; regression could capture nuanced performance but requires more labels.

- Failure signatures:
  - Predictions collapse to majority class (imbalanced pass/fail in training data).
  - Utility low despite reasonable accuracy → predictor not ranking well (check calibration).
  - Large domain gap between pretraining and target → negative transfer (validate pretraining helps via ablation).
  - Graph encoder dominates fusion → check embedding magnitudes, consider normalization.

- First 3 experiments:
  1. **Reproduce single-domain baseline**: Train predictor on one domain (e.g., code generation) without pretraining. Establish accuracy/utility baseline.
  2. **Ablate views**: Remove code, prompt, and graph encoders one at a time. Confirm each contributes (should replicate Table 4 pattern).
  3. **Vary label ratio**: Train with 10%, 30%, 50% of labels with and without pretraining. Verify pretraining advantage scales inversely with label availability (Figure 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can AgenticPredictor be extended to support multi-objective optimization by simultaneously predicting workflow success and operational costs (e.g., latency, token usage)?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "Future work includes expanding to multi-objective optimization (e.g., balancing accuracy and cost)."
- **Why unresolved:** The current framework is trained and evaluated exclusively on binary success/failure classification and does not model continuous cost variables.
- **What evidence would resolve it:** Demonstration of the predictor ranking workflows based on a Pareto frontier of success probability versus execution cost or latency.

### Open Question 2
- **Question:** Does incorporating dynamic execution data, such as temporal traces or user feedback, improve predictive accuracy over the current static multi-view encoding?
- **Basis in paper:** [explicit] The authors identify the need for "incorporating richer views such as temporal traces and user feedback" to handle complex, real-world settings.
- **Why unresolved:** The current methodology relies on static features (code, graph structure, prompts) and does not capture behavioral dynamics during workflow execution.
- **What evidence would resolve it:** An ablation study showing performance gains when runtime execution logs or feedback loops are included as input views.

### Open Question 3
- **Question:** How can the cross-domain pretraining strategy be refined to ensure robust generalization across structurally diverse reasoning tasks, such as multi-hop question answering?
- **Basis in paper:** [inferred] Appendix A.3 shows that while AgenticPredictor transfers well to DROP, it underperforms on HotpotQA compared to baselines, suggesting a gap in handling specific reasoning structures.
- **Why unresolved:** The current pretraining objectives may not capture the structural nuances required for multi-hop reasoning, leading to inconsistent transfer performance.
- **What evidence would resolve it:** Experiments showing consistent performance across both DROP and HotpotQA without task-specific fine-tuning, or an analysis explaining the feature discrepancy causing the HotpotQA performance drop.

## Limitations
- Framework effectiveness depends on quality of multi-view inputs, but extraction methodology from raw workflows is not fully specified
- Binary prediction setup may oversimplify nuanced performance differences, limiting utility in scenarios where relative performance rankings matter more
- Cross-domain pretraining effectiveness not tested for negative transfer when source and target domains differ substantially

## Confidence
- **High Confidence**: Multi-view encoding design and contribution to accuracy improvements (supported by ablation studies showing consistent gains over single-view baselines)
- **Medium Confidence**: Cross-domain pretraining effectiveness (mechanism appears sound, but lack of negative transfer testing and specific pretraining details reduce confidence)
- **Medium Confidence**: Predictor-as-ranker utility claims (accuracy improvements are clear, but ranking quality depends on domain-specific performance distributions not fully characterized)

## Next Checks
1. **Cross-domain transfer robustness**: Systematically test pretraining on dissimilar domains (e.g., pretrain on code, fine-tune on math) to identify negative transfer boundaries
2. **View redundancy analysis**: Measure pairwise correlations between graph, code, and prompt embeddings to quantify information overlap and optimal fusion strategies
3. **Calibration validation**: Evaluate predictor calibration (e.g., reliability diagrams) to ensure ranking quality isn't compromised by systematic bias in predicted probabilities