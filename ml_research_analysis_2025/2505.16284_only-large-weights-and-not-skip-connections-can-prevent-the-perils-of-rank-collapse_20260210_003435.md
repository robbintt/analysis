---
ver: rpa2
title: Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank
  Collapse
arxiv_id: '2505.16284'
source_url: https://arxiv.org/abs/2505.16284
tags:
- arxiv
- follows
- step
- attention
- softm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the representational strength of transformer
  models with small weights. It introduces the notion of "layer collapse," showing
  that Self-Attention Networks with small weights can be approximated well by a network
  with only a single layer, even when skip connections are present.
---

# Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse

## Quick Facts
- arXiv ID: 2505.16284
- Source URL: https://arxiv.org/abs/2505.16284
- Reference count: 30
- One-line primary result: Small weight norms in transformers cause "layer collapse," where deep networks can be approximated by single-layer networks with O(η) error.

## Executive Summary
This paper investigates the representational strength of transformer models with small weights. It introduces the notion of "layer collapse," showing that Self-Attention Networks with small weights can be approximated well by a network with only a single layer, even when skip connections are present. The key result demonstrates that large weights, not skip connections, are crucial for preventing representational weaknesses like layer collapse. This finding challenges the conventional wisdom that skip connections are primarily responsible for preventing expressiveness limitations in deep transformers.

## Method Summary
The paper develops a theoretical framework to analyze layer collapse in Self-Attention Networks. It introduces the Res function to measure rank-1 proximity and proves perturbation properties for key functions (Res, α, softmax) used in the analysis. The main technique involves showing that when weight matrices have ℓ∞ norm bounded by η, attention outputs remain close to unperturbed values. The proof proceeds by iteratively removing layers while maintaining O(η) approximation error, ultimately showing any L-layer network can be approximated by a single-layer network with bounded error.

## Key Results
- Layer collapse occurs when weight matrices have ℓ∞ norm bounded by η, allowing approximation by single-layer networks with O(η)·∥X∥∞ error.
- Skip connections do not prevent layer collapse - they only prevent rank collapse.
- The O(η) error bound holds for any input X, demonstrating a fundamental representational weakness of small-weight transformers.
- The result is tight: networks with large weights (η > 1) do not experience layer collapse.

## Why This Works (Mechanism)

### Mechanism 1: Softmax Perturbation Boundedness Under Small Weights
- Claim: When weight matrix entries are bounded in ℓ∞ norm, the attention softmax output changes by at most O(η) under perturbations.
- Mechanism: The paper proves that |softm(a+b)_i - softm(a)_i| ≤ 2(e^ε - 1) when ||b||_∞ ≤ ε. Since small weights produce small attention score perturbations (via the β normalization factor), the softmax outputs remain close to unperturbed values.
- Core assumption: Weight matrices satisfy ||W_q||_∞, ||W_k||_∞, ||W_v||_∞ ≤ η with η < 1.
- Evidence anchors:
  - [abstract]: "proves that if the weight matrices have bounded infinity norm, then there exists a single-layer network that approximates the original network well"
  - [section 4.3]: Lemma 4.4 provides the core perturbation bound: ||softm(a+b) - softm(a)||_∞ ≤ 2(e^ε - 1)
  - [corpus]: Limited direct corpus evidence on this specific perturbation analysis; related work discusses rank collapse but not layer collapse
- Break condition: When η >> 1, the perturbation bounds fail and layer collapse no longer occurs.

### Mechanism 2: Res Function Captures Rank-1 Proximity
- Claim: The Res(Z) = Z - 1_n·y^T operation extracts how far a matrix is from being constant across tokens.
- Mechanism: By subtracting the best rank-1 approximation (constant row pattern), Res measures residual token-wise variation. The paper shows that attention with small weights contracts this residual: ||Res(SAtt(X))||_∞ ≤ K·||Res(X)||_∞.
- Core assumption: The matrix E = β·Res(X)·W·Res(X)^T is θ-balanced (Definition 3.5), which holds when β ≤ 1/(||Res(X)||_∞²·η²).
- Evidence anchors:
  - [section 3.2]: Definition 3.4 formally defines the Res function
  - [section 5.1]: Lemma 5.1 proves the contraction property connecting Res(SAtt()) to Res()
  - [corpus]: "Two failure modes of deep transformers" paper discusses related rank collapse phenomena
- Break condition: When inputs have high residual variation that cannot be balanced, or when β normalization fails.

### Mechanism 3: Iterative Layer Removal with Cumulative O(η) Error
- Claim: Any L-layer Self Attention Network with bounded weights can be approximated by a single-layer network with O(η) entry-wise error.
- Mechanism: Remove layers one at a time. Lemma C.1 shows removing one layer changes the next layer's output by O(η·ε_0). Accumulating across L-1 removals yields the final bound. Skip connections cannot prevent this because the attention contributions themselves collapse.
- Core assumption: Each layer's Lipschitz constant remains bounded (C ≤ 6η in the analysis).
- Evidence anchors:
  - [section 5.3]: Theorem 5.3 proof sketch explicitly describes the iterative removal process
  - [section C.1]: Lemma C.1 handles the multi-head case with error bound ε_0 = 3g(2Hε)
  - [corpus]: Weak corpus evidence; this specific layer-collapse-to-single-layer construction is novel to this paper
- Break condition: Very deep networks (large L) or many heads (large H) may cause accumulated error to exceed bounds unless η is scaled down accordingly.

## Foundational Learning

- Concept: **Softmax shift-invariance**
  - Why needed here: Critical for understanding why constant row terms can be ignored in attention score analysis (Fact 3.2: softm(x) = softm(x + a·1_n)).
  - Quick check question: If you add 5 to every element of a 3-element vector [1, 2, 3], does the softmax output change?

- Concept: **Matrix norm distinctions (ℓ_∞ vs ℓ_1)**
  - Why needed here: The paper's main theorem uses ℓ_∞ norm bounds, while prior work (Dong et al.) used ℓ_1 norm. The ℓ_∞ bound can be much smaller, making the result applicable where prior results weren't.
  - Quick check question: For a 100×100 matrix with all entries equal to 0.1, what are ||A||_∞ and ||A||_1?

- Concept: **Rank collapse vs Layer collapse distinction**
  - Why needed here: Prior work showed skip connections prevent rank collapse