---
ver: rpa2
title: 'TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of
  LLMs'
arxiv_id: '2506.23423'
source_url: https://arxiv.org/abs/2506.23423
tags:
- fine-tuning
- tuco
- llama
- contribution
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TuCo, a method for measuring the contribution
  of fine-tuning to individual LLM responses using intermediate hidden states. The
  authors propose decomposing a fine-tuned model into pre-training and fine-tuning
  components, and show that scaling the fine-tuning component modulates model behavior
  and performance.
---

# TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs

## Quick Facts
- arXiv ID: 2506.23423
- Source URL: https://arxiv.org/abs/2506.23423
- Authors: Felipe Nuti; Tim Franzmeyer; João Henriques
- Reference count: 40
- Primary result: Method to measure fine-tuning contribution using intermediate hidden states, validated on multiple open-source LLMs

## Executive Summary
This paper introduces TuCo, a method for measuring how much fine-tuning contributes to individual LLM responses by decomposing models into pre-training and fine-tuning components. The authors propose an exact decomposition using residual stream analysis and show that scaling the fine-tuning component modulates model behavior. They validate TuCo on multiple open-source LLMs, demonstrating it can detect jailbreaks, predict low-resource language performance, and identify prompts that attenuate fine-tuning effects. The method successfully predicts jailbreak success with an AUC score of 0.87 for Llama 13B, suggesting that reducing fine-tuning influence plays a role in attack success.

## Method Summary
TuCo measures fine-tuning contribution by decomposing a fine-tuned LLM into pre-training (PTC) and fine-tuning (FTC) components at each layer. For a given input, the method runs the fine-tuned model to capture intermediate hidden states, then computes PTC as the output of the pre-trained model's layer on these activations. FTC is the difference between fine-tuned and pre-trained layer outputs. TuCo is calculated as the ratio of accumulated FTC to total component magnitudes across all layers, using only the last token. The method requires both pre-trained and fine-tuned model weights and can optionally scale FTC during inference to modulate behavior without retraining.

## Key Results
- TuCo successfully discriminates between web text and chat-like prompts with AUC > 0.80 across most models
- FTC^α-scaling controls model behavior, with agreement changes up to 24% for religious beliefs on Llama2 13B
- TuCo is lower for low-resource languages and adversarial attacks, decreasing with attack intensity
- TuCo predicts jailbreak success with AUC 0.87 for Llama 13B, suggesting attacks work by attenuating fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Residual Stream Decomposition
Any fine-tuned transformer can be exactly decomposed into pre-training and fine-tuning components at each layer. The residual architecture allows additive decomposition where PTC is the pre-trained layer output and FTC is the difference. This holds for any transformer with residual connections.

### Mechanism 2: Grönwall Bound for Component Influence
The relative magnitude of FTC to PTC bounds the divergence between fine-tuned and pre-trained outputs. Using discrete Grönwall inequality, the paper proves that this ratio controls maximum distance, providing theoretical justification for TuCo as a meaningful metric.

### Mechanism 3: FTC Scaling Controls Behavior
Modulating FTC by scaling factor α directly controls model behaviors without retraining. The modified forward pass x_{l+1} = x_l + PTC(x_l, l) + α·FTC(x_l, l) demonstrates that FTC magnitude is causally linked to behavioral expression.

### Mechanism 4: Jailbreaks Attenuate FTC
Adversarial attacks succeed by implicitly reducing TuCo, making the model behave more like its pre-trained version. Jailbreak techniques produce inputs with lower TuCo values, reducing FTC contribution relative to PTC and increasing reliance on pre-trained capabilities.

## Foundational Learning

- **Concept: Residual Connections in Transformers**
  - Why needed here: The decomposition relies entirely on the additive nature of transformer layers where each layer's output is added to the residual stream.
  - Quick check question: In a standard transformer block, how is the output of the attention layer combined with the input?

- **Concept: Discrete Grönwall Inequality**
  - Why needed here: Provides the theoretical bound linking FTC magnitude to output divergence, justifying TuCo as a meaningful metric.
  - Quick check question: If a sequence satisfies a_{n+1} ≤ B·a_n + ξ_n, what does Grönwall's inequality bound?

- **Concept: Lipschitz Continuity**
  - Why needed here: Required for the Grönwall bound; layer normalization ensures this property, making the theoretical analysis valid.
  - Quick check question: Why does layer normalization make a function bounded and Lipschitz?

## Architecture Onboarding

- **Component map**: Pre-trained Model (T^PT_φ) -> Fine-tuned Model (T^FT_Θ) -> Intermediate hidden states x^FT_l -> PTC_l = f^PT(x^FT_l, l) -> FTC_l = f^FT(x^FT_l, l) - PTC_l -> Accumulated I_PTC and I_FTC -> TuCo = ||I_FTC||_1 / (||I_PTC||_1 + ||I_FTC||_1)

- **Critical path**: 1) Run forward pass on fine-tuned model storing all intermediate hidden states x^FT_l, 2) For each layer l, compute PTC_l = f^PT(x^FT_l, l) and FTC_l = f^FT(x^FT_l, l) - PTC_l, 3) Accumulate last-token projections I_FTC and I_PTC, 4) Compute TuCo, 5) (Optional) Modify forward pass to use PTC + α·FTC at each layer.

- **Design tradeoffs**: Compute cost requires two forward passes, doubling inference cost; last-token focus avoids dilution across long contexts but may miss token-level effects; L1 norm chosen for robustness.

- **Failure signatures**: Models without residual architecture make decomposition inapplicable; missing pre-trained weights prevent TuCo computation; very high TuCo on jailbreaks suggests safety fine-tuning is not primary defense; inconsistent α-behavior indicates entangled fine-tuning effects.

- **First 3 experiments**: 1) Compute TuCo on 100 samples each from OpenWebText and HH-RLHF, expecting AUC > 0.8 distinguishing them; 2) Compute TuCo on harmful prompts with/without GCG attacks and benign prompts, evaluating AUC for discriminating successful jailbreaks; 3) Implement FTC^α-Scaling on Model Written Evaluations, varying α ∈ [0.75, 1.25] and measuring agreement changes.

## Open Questions the Paper Calls Out

### Open Question 1
Can TuCo be integrated into real-time adversarial attack prevention mechanisms in user-facing LLM applications? The paper demonstrates TuCo can predict jailbreak success but does not implement or evaluate any defense mechanism using this signal.

### Open Question 2
What are the internal model mechanisms that cause certain prompts (low-resource languages, adversarial suffixes) to reduce TuCo? The paper documents the phenomenon but does not explain mechanistically why these inputs attenuate the fine-tuning component.

### Open Question 3
How does TuCo behave across different fine-tuning methods (SFT, RLHF, DPO) and can it predict which fine-tuning approaches are more robust to jailbreaks? The paper evaluates models with different fine-tuning approaches but does not systematically compare TuCo distributions across methodologies.

### Open Question 4
Does TuCo scale reliably to frontier models (70B+ parameters), and does the relationship between TuCo and jailbreak success hold at scale? The paper evaluates only models up to 13B parameters, and empirical validation at larger scales is absent.

## Limitations

- Requires access to both pre-trained and fine-tuned weights, limiting applicability to models where original weights are available
- Effectiveness across diverse model families beyond LLaMA architectures remains untested
- Provides scalar measure of fine-tuning contribution but doesn't explain which aspects of fine-tuning are most influential for specific behaviors

## Confidence

**High Confidence**: FTC^α-Scaling behavioral control mechanism - strong empirical support with statistically significant results across multiple tasks and models

**Medium Confidence**: Jailbreak detection capability - AUC scores are strong but interpretation assumes specific safety architecture

**Medium Confidence**: Language resource sensitivity - reported differences are consistent but corpus quality differences could confound interpretation

## Next Checks

1. **Layer-wise FTC Analysis**: Analyze which specific layers contribute most to safety behaviors versus general capabilities, testing whether uniform treatment of layers is justified.

2. **Cross-Architecture Generalization**: Apply TuCo to transformer variants beyond standard LLMs (e.g., Mamba, RWKV) to verify whether residual-based decomposition holds universally.

3. **FTC Localization**: Conduct ablation studies where FTC is selectively disabled for specific layers or attention heads to determine whether fine-tuning effects are distributed uniformly or concentrated in particular components.