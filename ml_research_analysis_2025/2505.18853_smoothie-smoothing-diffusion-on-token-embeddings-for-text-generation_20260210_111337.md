---
ver: rpa2
title: 'Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation'
arxiv_id: '2505.18853'
source_url: https://arxiv.org/abs/2505.18853
tags:
- diffusion
- generation
- text
- process
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SMOOTHIE, a text diffusion method that combines
  the strengths of continuous latent space and categorical simplex-based approaches.
  The key idea is to represent tokens using vectors of negative squared Euclidean
  distances to all other token embeddings, allowing the model to progressively smooth
  these distances during diffusion based on semantic similarity.
---

# Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation

## Quick Facts
- arXiv ID: 2505.18853
- Source URL: https://arxiv.org/abs/2505.18853
- Reference count: 40
- State-of-the-art BERTScore of 68.0 on XSum summarization compared to 65.6 for embedding-based diffusion

## Executive Summary
This paper proposes SMOOTHIE, a novel text diffusion method that bridges continuous latent space and categorical simplex-based approaches by representing tokens as vectors of negative squared Euclidean distances to all other token embeddings. The method progressively smooths these distances during diffusion based on semantic similarity, maintaining both the discrete nature of text and semantic relationships between tokens. Evaluated on four sequence-to-sequence generation tasks (summarization, question generation, text simplification, paraphrase generation), SMOOTHIE outperforms existing diffusion-based models and achieves state-of-the-art performance among diffusion methods.

## Method Summary
SMOOTHIE represents tokens using vectors of negative squared Euclidean distances to all other token embeddings, allowing the model to progressively smooth these distances during diffusion based on semantic similarity. The method uses a noise scheduler that adds more noise at early stages of the forward process and includes a hyperparameter (δ) controlling the stochasticity of generation. Training is performed using mean squared error loss with a 12-layer Transformer decoder architecture featuring UNet-style skip connections. The approach maintains both discrete token nature and semantic relationships while achieving superior performance compared to both standard embedding space and categorical simplex approaches.

## Key Results
- Achieves BERTScore of 68.0 on XSum summarization compared to 65.6 for embedding-based diffusion and 62.1 for simplex-based diffusion
- Outperforms existing diffusion-based models on most evaluated sequence-to-sequence tasks
- Ablation studies confirm the proposed distance-based latent space yields better performance than both standard embedding space and categorical simplex approaches

## Why This Works (Mechanism)
The key innovation lies in the distance-based latent space representation that captures semantic relationships between tokens through negative squared Euclidean distances. During diffusion, this representation allows for semantically meaningful smoothing operations - tokens with similar meanings have their distances reduced in a way that preserves semantic structure. The noise scheduler's design, which adds more noise early in the forward process, enables more aggressive exploration of the latent space initially while allowing finer refinement later. The MSE loss on these distance representations provides stable training signals that respect the geometric structure of the latent space.

## Foundational Learning
- **Diffusion probabilistic models**: Stochastic processes that gradually add noise to data and learn to reverse this process for generation. Needed for understanding the core mechanism; quick check: can you explain the forward and reverse processes?
- **Token embedding spaces**: Continuous vector representations where semantically similar tokens are closer together. Needed to understand how semantic relationships are encoded; quick check: what properties should good token embeddings have?
- **Negative squared Euclidean distance**: A metric that measures dissimilarity between points, with smaller values indicating greater similarity. Needed for the novel latent space representation; quick check: how does this differ from standard Euclidean distance?
- **UNet architectures**: Convolutional architectures with skip connections that preserve spatial information across scales. Needed for understanding the model structure; quick check: what's the purpose of skip connections in UNet?
- **BERTScore**: An evaluation metric that computes similarity using contextual embeddings. Needed for understanding performance metrics; quick check: how does BERTScore differ from BLEU or ROUGE?
- **Sequence-to-sequence generation**: Tasks that transform input sequences into output sequences, such as summarization or translation. Needed for understanding the evaluation tasks; quick check: what are common challenges in seq2seq generation?

## Architecture Onboarding
**Component map**: Input sequence -> Token embeddings -> Distance matrix computation -> UNet-style Transformer decoder -> Noise scheduler -> Generated sequence

**Critical path**: Token embeddings → Distance matrix computation → UNet-style Transformer decoder → Reconstruction

**Design tradeoffs**: The distance-based representation provides semantic smoothing but requires O(V²) memory for vocabulary size V. MSE loss offers stable training but may not fully capture discrete reconstruction needs. The early-noise scheduler enables exploration but deviates from standard schedules.

**Failure signatures**: Performance degradation with larger vocabularies due to memory constraints, unstable training with poor-quality embeddings, generation artifacts when δ is improperly tuned.

**Three first experiments**:
1. Compare reconstruction quality using different distance metrics (Euclidean vs. cosine similarity)
2. Test different noise scheduler schedules (linear, cosine, early-noise)
3. Evaluate impact of embedding quality by using different pre-trained embedding models

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Distance-based representation may face scalability challenges with larger vocabularies due to O(V²) memory requirements for distance matrix operations
- Evaluation primarily focuses on sequence-to-sequence tasks, leaving uncertainty about performance on open-ended generation or long-form text
- Method's dependence on high-quality token embeddings suggests potential vulnerability to embedding quality variations across different domains or languages

## Confidence
- State-of-the-art performance claims: High
- Technical innovation claims: Medium
- Scalability claims: Low

## Next Checks
1. Evaluate scalability by testing the method with vocabularies of different sizes (e.g., 10K, 50K, 100K tokens) to measure memory and computational requirements, particularly for the distance matrix operations
2. Test cross-lingual generalization by evaluating on multilingual datasets and comparing performance when using embeddings trained on different languages or with different embedding models
3. Conduct human evaluation studies comparing SMOOTHIE outputs with baseline methods across multiple quality dimensions (fluency, coherence, semantic preservation) to validate automatic metric improvements