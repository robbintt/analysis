---
ver: rpa2
title: Is Grokking a Computational Glass Relaxation?
arxiv_id: '2505.11411'
source_url: https://arxiv.org/abs/2505.11411
tags:
- grokking
- entropy
- training
- generalization
- wand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates grokking through a statistical physics
  lens, interpreting neural networks as physical systems and framing grokking as a
  computational glass relaxation process. By sampling the Boltzmann entropy landscape
  of transformer models on modular arithmetic tasks, the authors find no entropy barrier
  between memorization and generalization states, challenging previous claims of grokking
  as a first-order phase transition.
---

# Is Grokking a Computational Glass Relaxation?

## Quick Facts
- arXiv ID: 2505.11411
- Source URL: https://arxiv.org/abs/2505.11411
- Reference count: 40
- Key outcome: This paper investigates grokking through a statistical physics lens, interpreting neural networks as physical systems and framing grokking as a computational glass relaxation process.

## Executive Summary
This paper investigates grokking through a statistical physics lens, interpreting neural networks as physical systems and framing grokking as a computational glass relaxation process. By sampling the Boltzmann entropy landscape of transformer models on modular arithmetic tasks, the authors find no entropy barrier between memorization and generalization states, challenging previous claims of grokking as a first-order phase transition. They observe a significant high-entropy advantage under grokking, with equilibrium states exhibiting much better generalization than classically trained networks. The authors also develop a physics-inspired optimizer, WanD, based on Wang-Landau Molecular Dynamics, which can eliminate grokking and find high-norm generalizing solutions without regularization.

## Method Summary
The authors employ statistical physics techniques to analyze grokking in transformer models. They sample the Boltzmann entropy landscape of neural networks trained on modular arithmetic tasks to study the relationship between entropy and generalization. The Wang-Landau Molecular Dynamics (WanD) optimizer is introduced as a physics-inspired approach that combines energy-based sampling with molecular dynamics to explore the loss landscape more effectively than traditional gradient-based methods.

## Key Results
- No entropy barrier found between memorization and generalization states, challenging first-order phase transition claims
- Equilibrium states show significant high-entropy advantage with better generalization than classically trained networks
- WanD optimizer eliminates grokking and finds high-norm generalizing solutions without regularization

## Why This Works (Mechanism)
The paper frames neural networks as physical systems where grokking emerges as a computational glass relaxation process. By sampling the Boltzmann entropy landscape, the authors reveal that generalization states correspond to high-entropy regions rather than low-energy minima. This challenges conventional wisdom about optimization and suggests that traditional gradient descent may get trapped in suboptimal low-entropy configurations. The WanD optimizer leverages this insight by actively exploring high-entropy regions of the landscape, finding generalizing solutions that classical methods miss.

## Foundational Learning
- **Boltzmann entropy**: Measure of disorder in physical systems - needed to quantify the complexity of neural network configurations; quick check: higher entropy states can correspond to better generalization
- **Glass relaxation dynamics**: Slow approach to equilibrium in disordered systems - provides framework for understanding the delayed generalization in grokking; quick check: systems may take exponentially long to find global optima
- **Wang-Landau sampling**: Monte Carlo method for estimating density of states - enables efficient exploration of neural network loss landscapes; quick check: allows uniform sampling across energy levels

## Architecture Onboarding
The transformer architecture consists of multiple self-attention layers followed by feed-forward networks. The critical path involves token embeddings → self-attention → feed-forward → residual connections → next layer. Design tradeoffs include attention mechanism depth versus computational cost. Failure signatures include memorization without generalization and prolonged training plateaus. Three first experiments: 1) Test WanD on simple modular arithmetic tasks, 2) Compare entropy landscapes of memorizing vs generalizing states, 3) Measure training time reduction with WanD versus AdamW.

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on specific transformer architectures and modular arithmetic tasks limits generalizability
- Computational overhead of physics-inspired sampling compared to standard optimization remains unclear
- Relationship between entropy landscapes and generalization in deeper, more complex networks not fully established

## Confidence
- High confidence in the absence of entropy barrier between memorization and generalization states in tested transformer models
- High confidence in observation that equilibrium states show better generalization than classically trained networks
- High confidence in WanD optimizer's ability to find high-norm generalizing solutions without regularization in tested scenarios
- Medium confidence in broader interpretation of grokking as computational glass relaxation across different architectures
- Medium confidence in WanD's ability to eliminate grokking in general settings
- Medium confidence in scalability of physics-inspired optimization to larger, more complex models

## Next Checks
1. Test the WanD optimizer on a diverse set of real-world datasets and network architectures to assess its generalizability and computational efficiency
2. Investigate the relationship between entropy landscapes and generalization in deeper transformer models and other network architectures
3. Examine the impact of different initialization schemes on the observed entropy-generalization relationship and grokking behavior