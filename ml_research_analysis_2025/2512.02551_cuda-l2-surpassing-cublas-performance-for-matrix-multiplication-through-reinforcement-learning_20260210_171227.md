---
ver: rpa2
title: 'CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement
  Learning'
arxiv_id: '2512.02551'
source_url: https://arxiv.org/abs/2512.02551
tags:
- cuda-l2
- kernels
- cuda
- copy
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CUDA-L2, a system that combines large language
  models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision
  General Matrix Multiply (HGEMM) CUDA kernels. The key challenge addressed is that
  manually optimizing matmul kernels across diverse matrix sizes and GPU architectures
  is extremely difficult due to the vast configuration space and architecture-specific
  constraints.
---

# CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.02551
- Source URL: https://arxiv.org/abs/2512.02551
- Authors: Songqiao Su; Xiaofei Sun; Xiaoya Li; Albert Wang; Jiwei Li; Chris Shum
- Reference count: 40
- Primary result: Outperforms cuBLAS and cuBLASLt across 1,000 matmul configurations with +19.2% to +26.0% speedups

## Executive Summary
CUDA-L2 combines large language models with reinforcement learning to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels, achieving significant performance improvements over highly-optimized libraries like cuBLAS and cuBLASLt. The system addresses the fundamental challenge that manual optimization of matmul kernels across diverse matrix sizes and GPU architectures is impractical due to the vast configuration space and architecture-specific constraints. Through systematic exploration guided by execution speed as a reward signal, CUDA-L2 discovers non-obvious optimization strategies that even heavily-tuned libraries cannot match.

## Method Summary
The system employs a multi-stage reinforcement learning approach: first training on general CUDA kernels, then specializing in HGEMM optimization. The RL agent uses execution speed as a reward signal and receives NCU profiling metrics as contextual information to guide optimization decisions. The model learns to select between different implementation abstractions (raw WMMA vs. CuTe) based on problem size and automatically discovers optimization techniques like block swizzle and multi-stage pipelining. Generated kernels are evaluated through both offline continuous execution and server mode intermittent execution scenarios.

## Key Results
- Outperforms torch.matmul by +22.0% (offline) and +28.7% (server mode)
- Surpasses cuBLAS with optimal layout configuration by +19.2% (offline) and +26.0% (server)
- Beats cuBLASLt-heuristic by +16.8% (offline) and +22.4% (server)
- Shows decreasing speedups for larger matrices as GPU compute becomes saturated

## Why This Works (Mechanism)

### Mechanism 1: RL-Guided Search of Configuration Space
The system uses execution speed as a reward signal to train an LLM via contrastive reinforcement learning (GRPO), systematically exploring configuration spaces impractical for manual tuning. The multi-stage training progresses from general CUDA kernels to HGEMM specialization, allowing discovery of non-obvious parameter combinations that optimize memory throughput and compute utilization.

### Mechanism 2: Profile-Guided Synthesis with Contextual Awareness
During HGEMM RL, the model receives NCU profiling metrics (memory throughput, SM occupancy, cache efficiency) as context, moving optimization from black-box execution time to granular hardware bottleneck understanding. This enables the model to associate specific code patterns with hardware performance counters and synthesize kernels that resolve issues like low occupancy or cache thrashing.

### Mechanism 3: Adaptive Abstraction Selection
The system automatically selects different implementation abstractions based on input dimensions - lightweight abstractions for smaller matrices and sophisticated frameworks like CuTe for larger ones. The RL reward naturally guides this choice, with simpler kernels winning for small problems due to overhead considerations, while advanced features become necessary for large matrices to achieve high throughput.

## Foundational Learning

**Concept: Tiled Matrix Multiplication & Memory Hierarchy**
- Why needed: Understanding memory bandwidth constraints is crucial for GEMM optimization, as the paper's discussion of tile sizes, shared memory, and registers is built on this concept
- Quick check: Explain why loading tiles from global memory to shared memory and then to registers is faster than operating directly from global memory

**Concept: Tensor Cores and WMMA**
- Why needed: Tensor Cores are the core compute unit for half-precision GEMM on modern GPUs; the paper mentions using "raw WMMA" and "CuTe's abstractions" built on Tensor Core operations
- Quick check: What is the primary performance benefit of Tensor Cores over standard CUDA cores for HGEMM?

**Concept: Reinforcement Learning with GRPO**
- Why needed: GRPO is the core optimization algorithm used to update the policy (LLM) based on reward (kernel speed)
- Quick check: In the context of CUDA-L2, what represents the "state", "action", and "reward"?

## Architecture Onboarding

**Component map:**
Foundation Model (DeepSeek 671B) -> General Kernel RL Module -> HGEMM-Specialized RL Module -> Compilation & Evaluation Sandbox

**Critical path:** The HGEMM RL module is the critical innovation, depending on reward signal quality and the model's ability to interpret NCU metrics. Correctness checks are vital to prevent reward hacking.

**Design tradeoffs:**
- CuTe vs. Raw WMMA: CuTe offers concise code but abstraction overhead; raw WMMA is verbose but allows fine-grained control
- Exploration vs. Exploitation: Must explore novel techniques while exploiting known good patterns
- Offline vs. Server Mode: Kernels optimized for continuous execution may not be optimal for intermittent execution due to thermal dynamics

**Failure signatures:**
- Compilation or Runtime Errors: Generated kernel is invalid
- Numerical Deviation: Output exceeds acceptable deviation from reference
- Performance Regression: Kernel is slower than baseline

**First 3 experiments:**
1. Benchmark torch.matmul, cuBLAS, and cuBLASLt-AutoTuning across a subset of 1,000 configurations to verify evaluation script
2. Use pre-trained model to generate kernels for fixed configurations, checking compilation and correctness via binary match
3. Run RL loop for single configuration, logging generated code, execution time, NCU metrics, and computed reward to verify reward calculation

## Open Questions the Paper Calls Out

### Open Question 1
Will optimization strategies transfer effectively to GPU architectures beyond A100 (Hopper, Ada Lovelace, Blackwell)? The framework is designed for broad applicability but ongoing work is needed to extend to other architectures.

### Open Question 2
Can the approach achieve meaningful speedups for very large matrix dimensions where GPU compute is already saturated? Speedups decrease significantly for large problems, suggesting convergence to baseline as matrices saturate GPU throughput.

### Open Question 3
What is the computational cost of the RL training pipeline and is it economically justified? The paper describes complex multi-stage training but provides no details on training duration, GPU hours consumed, or cost-benefit analysis.

### Open Question 4
Can the framework extend effectively to other precision formats (BF16, FP32, INT8) or mixed-precision operations? The paper focuses exclusively on HGEMM, noting that different precisions lead to different register pressure and optimization strategies.

## Limitations
- Scale of generalization: Optimized on specific 1,000 configurations using NVIDIA GPUs; performance on unseen sizes or AMD GPUs unknown
- Hardware dependency: Performance gains tied to specific NVIDIA architecture characteristics
- Complexity of RL pipeline: Two-stage training quality highly dependent on training corpus and reward function design
- Potential for overfitting: May be tuned to specific test set rather than learning robust generalizable strategies

## Confidence

**High Confidence:** Core methodology (LLM + RL for code optimization) is sound with verifiable speedups over torch.matmul and cuBLAS within tested configuration space.

**Medium Confidence:** Profile-guided synthesis and adaptive abstraction selection mechanisms are logically sound but lack detailed ablation studies to quantify exact contributions.

**Medium Confidence:** Comparison to cuBLASLt-AutoTuning is strong but difficult to directly validate due to undisclosed tuning details.

## Next Checks

1. Run ablation study comparing CUDA-L2 with and without NCU profiling context to measure impact of profile-guided synthesis
2. Evaluate best-performing CUDA-L2 kernels on different NVIDIA GPU architecture to measure performance retention and degradation
3. Generate kernels for 100 random configurations outside original 1,000 to assess model's ability to generalize beyond training set