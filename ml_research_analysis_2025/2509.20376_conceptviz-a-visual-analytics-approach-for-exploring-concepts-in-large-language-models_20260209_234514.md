---
ver: rpa2
title: 'ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language
  Models'
arxiv_id: '2509.20376'
source_url: https://arxiv.org/abs/2509.20376
tags:
- feature
- features
- activation
- system
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ConceptViz is a visual analytics system that bridges the gap between\
  \ Sparse Autoencoder (SAE) features and human-understandable concepts in large language\
  \ models. The system implements an Identification \u21D2 Interpretation \u21D2 Validation\
  \ pipeline that enables users to query SAEs using concepts of interest, interactively\
  \ explore concept-to-feature alignments, and validate correspondences through model\
  \ behavior verification."
---

# ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models

## Quick Facts
- arXiv ID: 2509.20376
- Source URL: https://arxiv.org/abs/2509.20376
- Reference count: 40
- Key outcome: ConceptViz bridges SAE features and human concepts through an Identification ⇒ Interpretation ⇒ Validation pipeline, demonstrating effectiveness in concept exploration with high user study ratings (4.42-4.67/5).

## Executive Summary
ConceptViz is a visual analytics system designed to make Sparse Autoencoder (SAE) features in large language models more interpretable by connecting them to human-understandable concepts. The system implements a three-stage pipeline: Identification (querying SAEs for relevant features), Interpretation (exploring feature distributions and semantic meanings), and Validation (verifying causal relationships through activation steering). Through two usage scenarios and a user study with 12 participants, ConceptViz demonstrates its effectiveness in helping users explore and interpret SAE features, receiving high ratings for usability and interpretability.

## Method Summary
ConceptViz implements an Identification ⇒ Interpretation ⇒ Validation pipeline for exploring concepts in SAEs. The system uses semantic similarity ranking to retrieve relevant SAE models, hierarchical feature visualization with topic extraction for exploration, and interactive causal validation via activation steering. Users can query SAEs using concepts of interest, explore concept-to-feature alignments through a hierarchical visual interface, and validate correspondences by manipulating feature activations and observing model behavior changes.

## Key Results
- ConceptViz received high ratings (4.42-4.67/5) in user study for helping users understand feature distributions, interpret semantic meanings, and verify concept-feature relationships
- Users expressed willingness to use the system again, with positive qualitative feedback about its effectiveness
- The system successfully demonstrated concept exploration through two usage scenarios covering diverse application domains

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity Ranking for SAE Retrieval
The system uses multi-threshold semantic similarity ranking to identify SAE models containing features aligning with user concepts. It embeds both user queries and pre-computed SAE feature explanations using text-embedding-3-large, computes cosine similarity, and ranks SAE models based on their contribution to Top-K most similar features across multiple thresholds (K=10, 100, 1000). This mechanism assumes automated feature explanations accurately capture feature semantics for effective semantic search.

### Mechanism 2: Hierarchical Feature Visualization with Topic Extraction
SAE features are organized into hierarchical visual structures using UMAP for 2D projection and agglomerative clustering at multiple granularities (10, 30, 90 clusters). Class-based TF-IDF (c-TF-IDF) extracts representative keywords for each cluster, while hierarchical color coordination maintains visual coherence across zoom levels. This enables efficient global-to-local exploration and interpretation of thousands of features.

### Mechanism 3: Interactive Causal Validation via Activation Steering
Causal relationships between SAE features and concepts are validated by manipulating feature activations through steering. The system extracts feature steering vectors from SAE decoders and adds scaled versions to model activations during inference. Users compare outputs across different steering strengths to verify concept causality, assuming decoder vectors directly influence model output logits in ways that align with feature semantics.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed here: The entire system is built on SAE output, decomposing dense neuron activations into sparse, interpretable features
  - Quick check question: If an SAE has a hidden dimension of 16,384 features and an L0 sparsity of 50, what is the average number of active features per token?

- **Concept: Activation Steering**
  - Why needed here: Core mechanism for causal validation, where adding vectors to model activations can bias output
  - Quick check question: To increase the probability of generating text about "negative emotions", would you add or subtract the corresponding feature's steering vector?

- **Concept: UMAP (Uniform Manifold Approximation and Projection)**
  - Why needed here: Primary method for creating visual feature maps, preserving meaningful semantic relationships in 2D projection
  - Quick check question: UMAP uses a "fuzzy simplicial set" to approximate the topological structure of high-dimensional data. True or False?

## Architecture Onboarding

- **Component map**: Concept Query & Retrieval (A1, A2) → Feature Abstraction & Visualization (B1, B2) → Interactive Analysis & Validation (C1, C2)
- **Critical path**: Real-time Activation Analysis (C1) requires dual-forward-pass through base LLM and SAE encoder, making it computationally intensive and critical for responsive user experience
- **Design tradeoffs**: 
  - Relies on pre-computed automated feature explanations for speed but inherits explainer model biases
  - UMAP computationally expensive for very large SAEs, requiring distributed preprocessing for scalability
  - Hierarchical color scheme trades exact color differentiation for visual continuity across zoom levels
- **Failure signatures**:
  - Semantic Mismatch: Query returns completely unrelated SAE and features, indicating semantic embedding failure
  - Steering Failure: No discernible change or random changes in output after steering, suggesting broken causal link
  - Visual Incoherence: Feature cloud shows no discernible clusters or has nonsensical topic labels
- **First 3 experiments**:
  1. Baseline Semantic Search Test: Query clear concept (e.g., "Eiffel Tower") and verify top SAE contains directly relevant features
  2. Steering Causality Test: Select feature with clear interpretation, use neutral prompt, verify positive steering generates concept-related output
  3. Anomaly Detection Test: Examine activation-similarity matrix for selected feature, inspect high-activation, low-similarity segments for secondary concepts

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on automated feature explanations (GPT-4o-mini) introduces potential error propagation and systematic biases
- UMAP-based visualization assumes semantic relationships translate meaningfully to 2D projection, which may not hold for all SAEs
- Steering validation depends on SAE decoder quality and may not capture all feature effects (contextual or compositional)

## Confidence

- **High Confidence**: The Identification ⇒ Interpretation ⇒ Validation pipeline structure is sound and well-implemented, with strong empirical support from user study results
- **Medium Confidence**: Semantic similarity ranking and hierarchical visualization work well for tested SAEs (10-30 layers, 16K-64K features), but scalability to larger models remains uncertain
- **Low Confidence**: Steering validation's ability to capture all relevant feature effects and generalizability of UMAP visualization across diverse SAE architectures

## Next Checks

1. **Semantic Explanation Accuracy Test**: Manually annotate sample SAE feature explanations and compare against automated ones to quantify accuracy rates and identify systematic biases
2. **Steering Generalizability Study**: Test activation steering across diverse concepts and measure consistency of causal effects using quantitative metrics beyond qualitative observation
3. **Cross-SAE Visualization Comparison**: Apply same UMAP and clustering pipeline to SAEs from different model architectures to assess visualization generalizability and identify architecture-specific limitations