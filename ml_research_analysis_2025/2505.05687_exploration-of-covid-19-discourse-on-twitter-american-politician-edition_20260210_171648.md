---
ver: rpa2
title: 'Exploration of COVID-19 Discourse on Twitter: American Politician Edition'
arxiv_id: '2505.05687'
source_url: https://arxiv.org/abs/2505.05687
tags:
- political
- party
- bigram
- more
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores COVID-19 discourse on Twitter by U.S. politicians,
  comparing Republican and Democratic messaging patterns.
---

# Exploration of COVID-19 Discourse on Twitter: American Politician Edition

## Quick Facts
- arXiv ID: 2505.05687
- Source URL: https://arxiv.org/abs/2505.05687
- Reference count: 0
- Best model achieved 92.7% accuracy distinguishing Democratic vs Republican COVID-19 messaging

## Executive Summary
This study analyzes how U.S. politicians communicated about COVID-19 on Twitter, revealing distinct partisan messaging patterns. Researchers collected 64,568 tweets from 60 political figures, filtering to 18,050 COVID-related posts, and applied machine learning to classify messaging by party affiliation. Democrats emphasized health impacts and precautions, while Republicans focused on political updates and monitoring. The research demonstrates how computational methods can quantify political polarization in crisis communication and suggests potential applications for analyzing international leader discourse.

## Method Summary
The study collected tweets from 60 U.S. political figures (governors and 2020 candidates) spanning November 2019 to November 2020, filtering for COVID-related content using specific keywords. Researchers applied preprocessing including tokenization, stopword removal, and both stemming and lemmatization. They experimented with bag-of-words and bigram models using CountVectorizer and TfidfVectorizer, training LinearSVC and Naïve Bayes classifiers. The dataset was split into training (80%), development (10%), and test (10%) sets, with final models trained on combined training and development data.

## Key Results
- Best model achieved 92.7% accuracy using LinearSVC with bigram features and CountVectorizer on lemmatized data
- Democrats emphasized pandemic casualties, health precautions, and action-oriented recommendations
- Republicans focused more on political updates, media briefings, and monitoring virus progression
- Most keywords overlapped across parties, with misclassification stemming from frequency differences

## Why This Works (Mechanism)

### Mechanism 1: Lexical Frequency Disparity as a Proxy for Ideology
The model exploits statistical tendencies in word usage between parties - Democrats use health-casualty terms like "death" and "hospitalize" more frequently, while Republicans use procedural terms like "briefing" and "update." This frequency disparity creates separable feature distributions that LinearSVC can classify.

### Mechanism 2: N-gram Contextualization of Action Phrases
Bigram models capture collocations like "wear mask" that provide stronger classification signals than unigrams alone. This preserves the context of public health directives that distinguish Democratic messaging patterns.

### Mechanism 3: Vectorization-Normalization Interaction
Bag-of-Words with TfidfVectorizer highlights distinctive terms by downweighting common words, while Bigrams with CountVectorizer preserves raw frequency signals. This pairing prevents "scattering" attention across too many features.

## Foundational Learning

- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Crucial for understanding why TfidfVectorizer worked for Bag-of-Words but not Bigrams. IDF penalizes common words - if a word appears in every document (e.g., "COVID"), its TF-IDF score will be low.
- **Lemmatization vs. Stemming**: The paper experiments with both (e.g., "distanc" vs "distance"). Stemming groups related words by chopping suffixes, while lemmatization uses dictionary forms. Bigram models might prefer Stemming when counting phrase frequencies to group variations like "distancing" and "distance."
- **LinearSVC (Support Vector Classification)**: This finds a hyperplane separating Democrat and Republican vectors based on weighted features. It "cares" about feature presence/absence, not grammatical structure of sentences.

## Architecture Onboarding

- **Component map**: Data Ingestion (snscrape) -> Preprocessing (NLTK tokenizer, stopword removal, custom filter) -> Feature Extraction (Scikit-Learn Vectorizers) -> Classification (LinearSVC/Naïve Bayes)
- **Critical path**: The Vectorization Step is most brittle - switching from Count to TF-IDF caused ~6% accuracy drop for Bigram models. Changing defaults without re-tuning breaks performance.
- **Design tradeoffs**: LinearSVC offers interpretability through coefficient analysis versus black-box neural models. Bigrams provide better context but create 10x larger feature space (131k vs 12k features), increasing computational cost and sparsity.
- **Failure signatures**: Misclassification from high keyword overlap - model guesses based on frequency biases when terms like "update" or "public health" appear in both parties. The model cannot capture sentiment nuances behind neutral terms.
- **First 3 experiments**:
  1. Validate the discrepancy between reported 92.7% results (BoW+Tfidf vs Bigram+Count)
  2. Stopword sensitivity analysis - remove all stopwords vs keeping negations to quantify impact on "left vs right" boundary
  3. Temporal drift test - train on Jan-May data, test on Oct-Nov to see if "distinct" keywords shift causes performance degradation

## Open Questions the Paper Calls Out

- Can the keyword-based classification approach be generalized to distinguish COVID-19 discourse among international political leaders by country rather than U.S. political party?
- Does the usage of pandemic-related terminology by politicians correlate with external variables such as COVID-19 case trends or specific legislative policies?
- Would incorporating context-aware language models reduce the misclassification rates observed in frequency-based approaches for ambiguous tweets?

## Limitations
- Limited generalizability to non-pandemic topics or international contexts
- Vocabulary dependency on 2020-specific COVID terminology may create temporal drift
- Dataset may not capture full spectrum of political discourse, missing local officials and opposition voices

## Confidence

- **High Confidence (Mechanistic Understanding)**: Core classification pipeline and accuracy figures are well-documented and reproducible
- **Medium Confidence (Comparative Claims)**: Partisan messaging differences supported by frequency analysis but lack deeper semantic validation
- **Low Confidence (Cross-Cultural Applicability)**: Global extension suggestions are speculative given different political systems and pandemic responses

## Next Checks

1. **Temporal Stability Test**: Train on January-May 2020 data, test on October-November 2020 to measure performance degradation
2. **Vocabulary Shift Analysis**: Systematically remove high-performing keywords and measure accuracy decline to quantify dependency
3. **Cross-Topic Transfer**: Apply trained COVID-19 classifier to non-pandemic political dataset to test generalizability beyond health crises