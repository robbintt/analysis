---
ver: rpa2
title: Agentic Workflows for Conversational Human-AI Interaction Design
arxiv_id: '2501.18002'
source_url: https://arxiv.org/abs/2501.18002
tags:
- user
- users
- design
- goals
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper explores how agentic workflows can help users and designers
  navigate ambiguity and transience in conversational human-AI interaction (CHAI).
  The authors developed a design probe with three stages: contextualization, goal
  formulation, and prompt articulation, supported by AI agents such as User Proxies,
  Contextual Personas, and Goal Refinement agents.'
---

# Agentic Workflows for Conversational Human-AI Interaction Design

## Quick Facts
- arXiv ID: 2501.18002
- Source URL: https://arxiv.org/abs/2501.18002
- Reference count: 40
- Primary result: Agentic workflows help users navigate ambiguity in conversational human-AI interaction through decoupled goal formulation and persona-based context

## Executive Summary
This paper introduces agentic workflows to address ambiguity and transience in conversational human-AI interaction (CHAI). The authors developed a design probe with three stages: contextualization, goal formulation, and prompt articulation, supported by AI agents including User Proxies, Contextual Personas, and Goal Refinement agents. Tested over four iterations with 10 users, the probe revealed that users benefited from decoupled goal formulation and prompt articulation stages, personalized prompts based on user personas improved relevance, and iterative goal refinement through merging and decomposition was valuable. The probe served as a "Needfinding Machine," revealing both explicit and latent user needs.

## Method Summary
The study employed a research-through-design approach with four iterative probes deployed over four weeks to 10 participants. Users interacted with a Streamlit web app featuring a Microsoft Phi-3.5-vision SLM, Sentence-BERT embeddings for semantic search, and a persona hub dataset. The workflow began with contextualization (image captioning and retrieval), proceeded through goal formulation with persona-based agents, and concluded with goal refinement and prompt articulation. User interactions were logged and analyzed through reflexive thematic analysis and affinity diagramming of 47 conversations.

## Key Results
- Decoupling goal formulation from prompt articulation reduced cognitive load and improved goal clarity
- Persona-based agents (User Proxies, Contextual Personas) generated more relevant recommendations by anchoring in user context
- Iterative goal refinement through merging and decomposition helped users converge on actionable sub-goals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling goal formulation from prompt articulation reduces cognitive load and improves goal clarity in CHAI.
- Mechanism: A two-stage workflow first helps users refine and select goals (with agent assistance), then generates tailored prompts. This separates divergent exploration from convergent articulation, reducing trial-and-error prompting.
- Core assumption: Users often have ambiguous or incomplete goals; separating clarification from execution reduces the "intentionality gap."
- Evidence anchors:
  - [abstract] "users benefited from the decoupled goal formulation and prompt articulation stages"
  - [section 4.3.3] "Participants welcomed the possibility to clarify their goals before receiving prompt recommendations... this step of first identifying goals helps me explore more potential ideas"
  - [corpus] Related work on contextualized reasoning (arXiv:2601.13115) supports staged processing for evolving user intent, though not directly validated for this decoupling pattern.
- Break condition: When users already have highly specific goals, the extra stage adds friction without benefit.

### Mechanism 2
- Claim: Persona-based agents (Contextual Personas, User Proxies) improve recommendation relevance by anchoring suggestions in user context.
- Mechanism: Role-play prompting animates retrieved personas to generate goal/prompt suggestions. User Proxies incorporate pre-survey data (routines, interests); Contextual Personas draw from semantic similarity matches to uploaded images.
- Core assumption: Personas capture enough relevant user attributes to steer recommendations meaningfully.
- Evidence anchors:
  - [abstract] "personalized prompts based on user personas improved relevance"
  - [section 4.2.2] "Each retrieved persona was animated through role-play prompts, resulting in agents semantically related to the context"
  - [corpus] PersonaRAG (arXiv:2407.09394) demonstrates user-centric retrieval-generation alignment, supporting—but not proving—this mechanism.
- Break condition: When persona attributes are accurate but irrelevant to the immediate context (e.g., product design expertise when choosing wine), recommendations misalign.

### Mechanism 3
- Claim: Iterative goal refinement through merging and decomposition helps users converge on actionable sub-goals.
- Mechanism: A Goal Refinement agent combines user-selected and user-defined goals, decomposing complex ones into smaller, actionable components. Users iteratively update selections until satisfied.
- Core assumption: Goals are often too broad or multiple; decomposition reduces ambiguity and enables incremental progress.
- Evidence anchors:
  - [abstract] "goal refinement through iterative merging and decomposition was valuable"
  - [section 4.4.2] "Goal Refinement agent, which combined user-selected goal recommendations with user-defined goals and then performed a decomposition operation"
  - [corpus] Limited direct corpus evidence for this specific decomposition mechanism in CHAI.
- Break condition: When recommendations are already well-aligned, iterative refinement may feel redundant.

## Foundational Learning

- Concept: Role-Play Prompting
  - Why needed here: Agents animate personas by adopting roles (e.g., "You are [persona]") to modulate behavior and generate contextually relevant suggestions.
  - Quick check question: Can you explain how role-play prompting differs from standard instruction-following in LLMs?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Semantic search retrieves relevant persona descriptions from a dataset; these ground agent suggestions in external knowledge rather than model weights alone.
  - Quick check question: What component retrieves and what component generates in a RAG pipeline?

- Concept: Research-through-Design (RtD)
  - Why needed here: The probe was iteratively designed with users, allowing emergent findings to shape subsequent versions.
  - Quick check question: How does RtD differ from hypothesis-driven experimental validation?

## Architecture Onboarding

- Component map:
  Frontend (Streamlit) -> Image upload/captioning -> Embedding (Sentence-BERT) -> Persona retrieval (cosine similarity) -> Agent generation (role-play) -> Goal selection/refinement -> Prompt generation -> Export

- Critical path:
  1. User uploads image → captioned by SLM → embedded → top-3 personas retrieved
  2. User provides pre-survey data → User Proxy persona constructed
  3. Agents generate goal recommendations → user selects/edits
  4. Goal Refinement agent merges/decomposes selected goals
  5. Agents generate prompt recommendations → user edits → export to external chat

- Design tradeoffs:
  - SLM vs. LLM: Lower capability but sustainable and controllable; no JSON formatting or system messages
  - Context granularity: Too much context causes fixation on irrelevant details; too little reduces relevance
  - Automation vs. user control: Full automation reduces effort but risks misalignment; user control improves alignment at cost of friction

- Failure signatures:
  - Recommendations coherent with context but irrelevant to user's actual goal (goal mismatch)
  - Persona accurate but not situationally relevant (fixation on wrong attributes)
  - Users bypass workflow entirely and write prompts from scratch

- First 3 experiments:
  1. A/B test: Compare goal-prompt decoupling vs. single-stage workflow on task completion time and user satisfaction.
  2. Context granularity sweep: Vary number of context types (image only, image+location, image+location+time) and measure recommendation relevance ratings.
  3. Model substitution: Replace Phi-3.5-vision with a larger model (e.g., Phi-4) and compare goal refinement quality and reasoning capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of goal refinement agents differ when implemented with large language models (LLMs) versus the small language model (SLM) used in this study?
- Basis in paper: [explicit] The authors note the use of Phi-3.5 (an SLM) "potentially limiting performance in reasoning tasks" and suggest "Future work should consider larger or specialized models."
- Why unresolved: SLMs have constrained reasoning capabilities compared to LLMs, and it is unclear if the agentic workflow structure compensates for model size or if larger models are required for complex goal decomposition.
- What evidence would resolve it: A comparative study measuring goal satisfaction and prompt quality using the identical workflow architecture deployed on both SLM and LLM backends.

### Open Question 2
- Question: What mechanisms can optimally balance the "dose-effect gradient" of contextualization to prevent irrelevant context from increasing ambiguity?
- Basis in paper: [inferred] The discussion highlights a "dose-effect gradient" where "excessive contextual dimensions may cause the AI system to fixate on irrelevant details," yet the study did not finalize a method to automate this balance.
- Why unresolved: The authors identified that too much context is detrimental, but relied on manual user input to filter context, leaving the automated optimization of context granularity unsolved.
- What evidence would resolve it: Experiments testing automated context-filtering algorithms that measure the reduction in "irrelevant recommendations" and user cognitive load.

### Open Question 3
- Question: How do agentic workflows influence the formation of recurring goals and sustained engagement in long-term usage?
- Basis in paper: [explicit] The limitations section states, "Longer-term studies could better capture recurring goals and integrate the app into users' routines."
- Why unresolved: The four-week study duration focused on transient interactions; it is unknown if the workflow supports the transition from transient needs to long-lasting, recurring user behaviors.
- What evidence would resolve it: A longitudinal field study (e.g., 6 months) analyzing the persistence of user goals and the evolution of prompt articulation strategies over time.

## Limitations

- Sample size of 10 participants limits statistical power and external validity
- Critical workflow components (role-play prompts, goal refinement algorithms) are not fully specified
- Multiple concurrent interventions make it difficult to isolate causal effects of individual mechanisms

## Confidence

- High confidence: Users benefited from decoupled goal formulation and prompt articulation (supported by direct quotes and clear mechanism)
- Medium confidence: Persona-based agents improved recommendation relevance (mechanism plausible but attribution unclear due to concurrent interventions)
- Low confidence: Iterative goal refinement through merging/decomposition was valuable (limited direct evidence, corpus support minimal)

## Next Checks

1. A/B Workflow Test: Compare goal-prompt decoupling vs. single-stage workflow with 30+ participants, measuring task completion time and satisfaction scores.

2. Context Granularity Sweep: Systematically vary context types (image only, image+location, image+location+time) with 20 participants per condition to measure recommendation relevance impact.

3. Model Capability Substitution: Replace Phi-3.5-vision with a larger model (Phi-4 or similar) and test whether goal refinement quality and reasoning capability improve significantly.