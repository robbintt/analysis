---
ver: rpa2
title: 'The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous
  Coding Agents Are Reshaping Software Engineering'
arxiv_id: '2507.15003'
source_url: https://arxiv.org/abs/2507.15003
tags:
- agents
- software
- code
- autonomous
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AIDev, the first large-scale dataset capturing
  how autonomous coding agents operate in real-world software development. It spans
  456,000 pull requests by five leading agents across 61,000 repositories and 47,000
  developers, offering an empirical foundation for studying AI-native software engineering
  workflows.
---

# The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering

## Quick Facts
- **arXiv ID**: 2507.15003
- **Source URL**: https://arxiv.org/abs/2507.15003
- **Reference count**: 40
- **Key outcome**: AIDev dataset reveals agents accelerate code submission but lag human PR acceptance rates, exposing trust gaps in SE 3.0 workflows.

## Executive Summary
This paper introduces AIDev, the first large-scale dataset capturing autonomous coding agents in real-world software development. Spanning 456,000 pull requests from five leading agents across 61,000 repositories and 47,000 developers, it provides empirical evidence of AI-native engineering workflows. Key findings reveal agents increase throughput by prioritizing simpler changes but face lower acceptance rates than human contributions, highlighting trust and quality gaps. The dataset enables new research into agent benchmarking, readiness, collaboration modeling, and AI governance.

## Method Summary
The authors mined GitHub's REST API using targeted search queries to identify PRs from autonomous coding agents (OpenAI Codex, Devin, GitHub Copilot, Cursor, Claude Code). They filtered repositories to retain those with >500 stars, creating a high-quality subset (AIDev-pop) for rigorous statistical analysis. PRs were classified into task types using GPT-4.1-mini, and metrics like acceptance rates, turnaround times, and cyclomatic complexity changes were computed. Statistical significance was tested using Mann-Whitney U tests and Cliff's delta for effect size.

## Key Results
- Agents increase development throughput by disproportionately targeting simpler code modifications.
- Provider-aligned review loops (e.g., Copilot bot reviewing Copilot PRs) accelerate triage but may propagate provider-specific blind spots.
- Agents succeed on synthetic benchmarks but fail integration acceptance due to lack of "soft" context like maintainability and style.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autonomous agents increase development throughput by disproportionately targeting structurally simpler code modifications.
- **Mechanism:** Agents decompose complex goals into lower-risk, minimal-patch tasks (e.g., documentation, boilerplate) rather than deep refactoring, reducing cognitive load per submission.
- **Core assumption:** Cyclomatic complexity change is a valid proxy for the "intricacy" or risk of a code change.
- **Evidence anchors:**
  - [abstract] Agents "prioritize throughput over complexity" and produce "structurally simpler" code changes.
  - [section 4.3, Finding #8] In the OpenHFT case study, a developer submitted 164 Agentic-PRs in 3 days (vs. 176 Human-PRs over 3 years), yet only 9.1% of Agentic-PRs introduced cyclomatic complexity changes compared to 23.3% for humans.
  - [corpus] *Agentic Refactoring: An Empirical Study of AI Coding Agents* supports this by examining how agents handle (or avoid) complex refactoring tasks.
- **Break condition:** If agents begin to successfully execute high-complexity architectural refactoring at rates comparable to humans, the "simple-change" throughput mechanism will no longer explain their productivity.

### Mechanism 2
- **Claim:** Provider-aligned review loops function as a mechanism for accelerated triage, trading potential bias for speed.
- **Mechanism:** When an autonomous agent (e.g., GitHub Copilot) submits a PR, it is frequently reviewed by a bot from the same provider (e.g., `copilot-pull-request-reviewer[bot]`), creating a closed, automated feedback loop that reduces human latency for initial triage.
- **Core assumption:** Faster closure times in these loops indicate successful triage rather than merely superficial automated approval.
- **Evidence anchors:**
  - [section 4.2, Finding #6] GitHub Copilot shows a marked shift toward hybrid human-bot review (37.4% of PRs).
  - [section 4.2, Finding #7] `copilot-pull-request-reviewer[bot]` is responsible for 91.0% of bot reviews on GitHub Copilot PRs.
  - [corpus] *Security in the Age of AI Teammates* likely investigates whether these loops propagate security vulnerabilities (weak evidence: title suggests relevance to security risks in agentic PRs).
- **Break condition:** If human reviewers consistently override or reject the "first-pass" assessments of these aligned bots, the efficiency mechanism is broken, and the loop creates net negative noise.

### Mechanism 3
- **Claim:** Agents achieve functional correctness in isolation (benchmarks) but fail integration acceptance due to a lack of "soft" context (maintainability/style).
- **Mechanism:** Agents succeed at resolving isolated logic issues (high SWE-bench scores) but fail to capture implicit project constraints—such as architectural conventions, documentation standards, or team-specific idioms—resulting in lower merge rates in the wild.
- **Core assumption:** Human PRs inherently capture these soft constraints better than current agents.
- **Evidence anchors:**
  - [abstract] PRs are "less frequently accepted than human contributions, exposing trust and quality gaps."
  - [section 4.1, Finding #2] Agents lag human acceptance rates by 15-40 percentage points, particularly in complex tasks like feature development, contrasting sharply with >70% success rates on synthetic benchmarks.
  - [corpus] *How AI Coding Agents Modify Code* (neighbor) likely provides further detail on the structural differences in code modifications that drive this rejection.
- **Break condition:** If future agents incorporate full-repository context (not just snippets) into their planning phase, the gap between benchmark performance and real-world acceptance should narrow significantly.

## Foundational Learning

- **Concept:** **SE 3.0 (Agentic Software Engineering)**
  - **Why needed here:** This paper defines the transition from SE 2.0 (AI as an assistant/copilot) to SE 3.0 (AI as an autonomous teammate). Understanding this distinction is critical to interpreting why agents are submitting their own PRs rather than just suggesting code snippets.
  - **Quick check question:** Does the tool require a human to type code (SE 2.0), or does it plan, execute, and submit changes autonomously based on a goal (SE 3.0)?

- **Concept:** **Code Complexity Metrics (Cyclomatic Complexity)**
  - **Why needed here:** The paper uses changes in cyclomatic complexity to argue that agents "prioritize throughput over complexity." You must understand this metric to evaluate the claim that agents avoid intricate implementations.
  - **Quick check question:** If an agent refactors a large function into five smaller ones, does the total cyclomatic complexity of the system likely increase, decrease, or stay the same?

- **Concept:** **Benchmark Ecological Validity**
  - **Why needed here:** The paper highlights a massive disparity between synthetic benchmark performance (SWE-bench) and real-world PR acceptance. Understanding why benchmarks fail to predict reality is key to the paper's critique of current evaluation methods.
  - **Quick check question:** Why would a model that solves 70% of GitHub issues on a benchmark have a much lower acceptance rate when submitting PRs to real, active repositories?

## Architecture Onboarding

- **Component map:** GitHub REST API -> Identification Layer (search queries) -> Dataset Schema (pull_request, pr_reviews, pr_comments, pr_commit_details, repository, user) -> Analysis Layer (GPT-4.1-mini, Statistical analysis)
- **Critical path:**
  1. **Filtering:** Narrowing 456K PRs down to `AIDev-pop` (repos with >500 stars) to ensure data quality.
  2. **Classification:** Labeling PRs by task type (feat, fix, docs, etc.) to compare like-for-like against humans.
  3. **Metric Calculation:** Computing acceptance rates and turnaround times to establish the "trust gap."
- **Design tradeoffs:**
  - **Volume vs. Quality:** The authors trade the breadth of the full AIDev dataset (456K PRs) for the reliability of the filtered `AIDev-pop` (7K PRs) for rigorous statistical comparison.
  - **Automated Labeling:** Relying on GPT-4.1-mini for task classification scales the analysis but introduces potential label noise compared to manual annotation.
- **Failure signatures:**
  - **Superficial Review:** PRs closed in <30 minutes with no comments (likely "rubber stamp" or bot-only triage).
  - **Quality Drift:** High submission volume coupled with a high rejection rate (indicative of "spammy" agent behavior).
  - **Attribution Loss:** Agentic PRs attributed to human users (e.g., OpenAI Codex) without clear metadata, breaking accountability.
- **First 3 experiments:**
  1. **Failure Mode Analysis:** Sample rejected PRs from the `pr_reviews` table to manually categorize the reasons for rejection (e.g., logic error, style violation, lack of tests).
  2. **Latency Distribution Profiling:** Plot the resolution time for `GitHub Copilot` specifically to investigate the "heavy-tailed latency" mentioned in Research Direction #3.
  3. **Provider Bias Check:** Compare the acceptance rates of PRs reviewed by provider-aligned bots (e.g., Copilot bot reviewing Copilot PR) vs. neutral bots (e.g., CodeRabbit) to test the "closed loop" risk hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the significantly faster review time for Agentic-PRs indicate superficial scrutiny or genuine confidence in the code?
- **Basis in paper:** [explicit] Finding #5 notes OpenAI Codex PRs close 10x faster than human PRs, raising concerns about review depth versus efficiency.
- **Why unresolved:** The paper measures speed but lacks qualitative metrics on the thoroughness of the review process or the cognitive load on reviewers.
- **Evidence to resolve:** Correlating review speed with post-merge defect density or analyzing the volume and depth of review comments on Agentic-PRs.

### Open Question 2
- **Question:** Why do agents with high success rates on synthetic benchmarks (e.g., SWE-bench) fail to achieve comparable acceptance rates in real-world repositories?
- **Basis in paper:** [explicit] Finding #2 highlights a stark disparity between benchmark performance (>70% success) and real-world PR acceptance rates (15-40% lower than humans).
- **Why unresolved:** Current benchmarks focus on functional correctness but fail to capture "human-centric" factors like code style, maintainability, and project context.
- **Evidence to resolve:** A comparative analysis of rejected Agentic-PRs against benchmark solutions to identify missing evaluation dimensions like readability or architectural fit.

### Open Question 3
- **Question:** Does the agent behavior of prioritizing throughput over complexity lead to increased long-term technical debt?
- **Basis in paper:** [explicit] Finding #8 shows agents produce structurally simpler code (lower cyclomatic complexity changes) to maximize output volume.
- **Why unresolved:** While immediate throughput is measurable, the paper notes that the long-term maintainability and architectural impact of these "simpler" contributions are unknown.
- **Evidence to resolve:** Longitudinal studies tracking code churn, bug density, and refactoring effort in repositories with high Agentic-PR volume over time.

## Limitations
- The dataset focuses on GitHub repositories with >500 stars, which may not represent the broader software development landscape, including private or smaller-scale projects.
- The analysis assumes cyclomatic complexity is a sufficient proxy for code intricacy, which may not capture all dimensions of software complexity.
- Automated classification using GPT-4.1-mini introduces potential label noise that could affect task-type distributions and comparisons.

## Confidence
- **High Confidence**: Findings related to throughput differences (e.g., submission volume, acceptance rate gaps) are supported by robust statistical analysis and large-scale data.
- **Medium Confidence**: Conclusions about agent behavior (e.g., prioritization of simpler changes) are plausible but depend on the validity of cyclomatic complexity as a complexity proxy.
- **Low Confidence**: Claims about the efficacy of provider-aligned review loops are based on descriptive statistics and require further investigation to rule out superficial automation.

## Next Checks
1. **Manual Validation of Classification**: Randomly sample 100 PRs and manually reclassify them to assess the accuracy of GPT-4.1-mini labels and quantify potential noise in task-type distributions.
2. **Cross-Repository Analysis**: Expand the analysis to include repositories with fewer than 500 stars to evaluate whether findings generalize to smaller or less popular projects.
3. **Complexity Metric Comparison**: Supplement cyclomatic complexity with alternative metrics (e.g., Halstead complexity, cognitive complexity) to validate the claim that agents prioritize simpler changes.