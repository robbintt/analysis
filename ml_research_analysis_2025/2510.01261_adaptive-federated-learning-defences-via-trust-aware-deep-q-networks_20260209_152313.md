---
ver: rpa2
title: Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks
arxiv_id: '2510.01261'
source_url: https://arxiv.org/abs/2510.01261
tags:
- accuracy
- client
- trust
- learning
- rounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of defending federated learning
  systems against poisoning and backdoor attacks under partial observability. The
  core method introduces a trust-aware Deep Q-Network (DQN) that frames defense as
  a partially observable sequential decision problem.
---

# Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks

## Quick Facts
- **arXiv ID:** 2510.01261
- **Source URL:** https://arxiv.org/abs/2510.01261
- **Reference count:** 30
- **Primary result:** DQN defense achieves 61.48% test accuracy with 41.35% backdoor attack success rate on CIFAR-10

## Executive Summary
This work addresses federated learning poisoning and backdoor attacks under partial observability by framing defense as a sequential decision problem. The approach uses a trust-aware Deep Q-Network that accumulates evidence across rounds through Bayesian belief tracking while optimizing a long-horizon robustness-accuracy objective. Experiments show that sequential belief updates enable detection even when individual signals are weak, and that DQN outperforms linear controllers and policy gradient methods. The method demonstrates reinforcement learning as a principled foundation for robust federated learning defenses.

## Method Summary
The defense operates in federated learning rounds where clients submit model updates that may contain backdoors. A DQN agent selects trust adjustment actions (increase, reduce, hold) based on anomaly signals extracted from each client's update: directional alignment with global model, magnitude deviation from typical updates, and validation accuracy impact. These signals feed Bayesian belief updates about client trustworthiness, which combine with DQN actions to compute trust scores for weighted aggregation. The DQN is trained online using a reward function balancing accuracy, trust accuracy, action cost, and attack success rate. The framework uses CIFAR-10 partitioned among 10 non-IID clients with 20% malicious clients injecting backdoor attacks.

## Key Results
- DQN achieves 61.48% test accuracy with 41.35% ASR, outperforming random (16% accuracy) and linear-Q controllers
- Sequential belief updates maintain accuracy while ASR increases when observability is reduced, showing robustness to weaker signals
- Validation cues contribute most to detection quality, but sequential updates preserve clean accuracy even when validation signals are removed
- DQN is the only agent consistently escaping chance-level performance, demonstrating advantages of off-policy value-based control

## Why This Works (Mechanism)

### Mechanism 1: Sequential Belief Accumulation Under Partial Observability
Accumulating evidence across rounds enables detection of malicious clients even when per-round signals are weak. The server maintains Bayesian posterior beliefs about client trustworthiness that update recursively: $b^t_i = P(s^t_i | o^{1:t}_i) \propto P(o^t_i | s^t_i) b^{t-1}_i$. This allows benign and malicious clients to separate gradually rather than requiring strong single-round detectors. Core assumption: malicious clients cannot consistently produce updates indistinguishable from benign across all three signal dimensions over multiple rounds.

### Mechanism 2: Multi-Signal Evidence Fusion
Combining directional, magnitude, and validation signals provides complementary evidence that no single signal can capture alone. Three anomaly metrics are computed: cosine similarity between client update and global model, deviation of update norm from median, and validation accuracy change when applying update. These concatenate into observation vector feeding belief updates. Core assumption: at least one signal will deviate for malicious updates even if others appear normal.

### Mechanism 3: Off-Policy DQN for Long-Horizon Credit Assignment
Off-policy value-based RL with replay buffers outperforms on-policy and linear methods for FL defense under non-stationarity. DQN parameterizes $Q(s,a;\theta)$ trained via temporal-difference loss with experience replay and target networks. The reward balances accuracy, trust accuracy, action cost, and attack success rate. Core assumption: the defense decision horizon extends beyond immediate round rewards—early trust decisions affect later aggregation quality.

## Foundational Learning

- **Partially Observable Markov Decision Process (POMDP)**: The core formulation treats client honesty as latent state that cannot be directly observed. Understanding POMDPs is essential to grasp why belief tracking (not direct detection) is the right approach. *Quick check:* If you could directly observe client honesty each round, would this still be a POMDP?

- **Bayesian Belief Updates**: The recursive posterior update $b^t_i \propto P(o^t_i | s^t_i) b^{t-1}_i$ is the mathematical core of trust accumulation. Without this, you cannot understand how evidence compounds across rounds. *Quick check:* What happens to belief if $P(o^t_i | malicious) = P(o^t_i | benign)$ for all observations?

- **Deep Q-Learning with Experience Replay**: The defense policy is learned via DQN. Understanding replay buffers, target networks, and $\epsilon$-greedy exploration is necessary to debug training instability. *Quick check:* Why does experience replay help under non-stationary client distributions?

## Architecture Onboarding

- **Component map:** Client update → Anomaly Signal Extractor → Bayesian Belief Updater → State Constructor → DQN Policy Network → Trust Mapper → Trust-Weighted Aggregator → Global Model Update → Reward Computation → DQN Training

- **Critical path:** Client update → anomaly signals → belief update → DQN action selection → trust score update → weighted aggregation → global model update → reward computation → DQN training step

- **Design tradeoffs:**
  - Signal budget vs. privacy: Full signals require validation set access; directional-only is weakest but most privacy-preserving
  - Exploration vs. security: High $\epsilon$ early improves policy but allows more malicious influence during exploration
  - Trust decay rate ($\lambda$) vs. false positives: Aggressive penalty catches attackers faster but may exclude benign non-IID clients

- **Failure signatures:**
  - Belief collapse: All clients converge to similar beliefs → check if signal likelihoods are uninformative
  - ASR drift despite stable accuracy: Policy may be optimizing accuracy at expense of robustness → increase $\alpha_{attack}$
  - Q-value explosion: Check reward scaling and gradient clipping

- **First 3 experiments:**
  1. **Sanity check:** Run with 0% malicious clients; verify accuracy converges to baseline and trust scores remain high
  2. **Signal ablation:** Compare Full vs. Directional-only with 20% malicious; quantify ASR increase per dropped signal
  3. **Controller comparison:** Replicate Linear-Q vs. DQN comparison; verify DQN convergence within ~20 rounds

## Open Questions the Paper Calls Out

- **Scalability to high-dimensional data:** How does the defense scale to ImageNet or language tasks with larger model architectures? The authors focused on CIFAR-10 with lightweight CNN and note extending to larger vision or multimodal tasks is necessary.

- **Real-world system noise:** Does the defense maintain robustness under communication latency and heterogeneous client hardware? Simulators cannot fully capture real-world complexity including communication delays, heterogeneous hardware, or user behavior.

- **Reconnecting Malicious Clients:** How robust is the sequential belief tracking against attackers who reset their trust history by dropping out and rejoining? The method relies on accumulating evidence over time, which RMCs could bypass by restoring initial trust scores.

- **Server-side validation dependency:** Can the defense maintain robustness without server-side validation set for the validation impact signal? While sequential updates mitigate weaker signals, reliance on clean server data is a strong assumption in privacy-critical FL.

## Limitations

- **Implementation details underspecified:** Bayesian likelihood functions, validation set protocol, and reward calculation formulas lack precise specifications
- **Single attack type focus:** Experiments limited to backdoor attacks; generalization to other poisoning strategies remains untested
- **Simulator-based evaluation:** Results from AIJACK simulator may not translate directly to real-world federated learning deployments

## Confidence

- **High confidence:** Sequential belief accumulation provides robustness under partial observability; multi-signal fusion improves detection; DQN outperforms linear controllers
- **Medium confidence:** Specific performance numbers depend on exact implementation details; belief update dynamics under extreme non-IID distributions
- **Low confidence:** Generalization to other attack types beyond backdoor attacks; scalability to larger client populations

## Next Checks

1. **Signal Ablation Study:** Run controlled experiments comparing Full (3-signal), Directional-only, and Validation-only variants with fixed malicious client proportion to quantify individual signal contributions

2. **Belief Calibration Analysis:** Plot belief trajectories over time for both benign and malicious clients to verify that posterior updates correctly separate client types

3. **Reward Function Sensitivity:** Systematically vary α_perf, α_attack, α_cost, α_trust weights to identify sensitivity thresholds where accuracy-robustness tradeoffs emerge