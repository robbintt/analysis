---
ver: rpa2
title: 'L0: Reinforcement Learning to Become General Agents'
arxiv_id: '2506.23667'
source_url: https://arxiv.org/abs/2506.23667
tags:
- agent
- training
- code
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces L0, a scalable end-to-end pipeline for training\
  \ general-purpose autonomous agents using reinforcement learning. The key innovation\
  \ is NB-Agent, which operates in a \u201Ccode-as-action\u201D fashion via a REPL-style\
  \ Think-Code-Observe loop, allowing the agent to execute Python code within a stateful\
  \ environment and use it as a form of external memory."
---

# L0: Reinforcement Learning to Become General Agents

## Quick Facts
- **arXiv ID:** 2506.23667
- **Source URL:** https://arxiv.org/abs/2506.23667
- **Reference count:** 1
- **Primary result:** L0 trains general-purpose agents using RL, achieving 80% accuracy on SimpleQA and 41% on HotpotQA with Qwen2.5-7B-Instruct.

## Executive Summary
L0 introduces a scalable end-to-end pipeline for training general-purpose autonomous agents using reinforcement learning. The key innovation is NB-Agent, which operates in a "code-as-action" fashion via a REPL-style Think-Code-Observe loop, allowing the agent to execute Python code within a stateful environment and use it as a form of external memory. This design addresses the limitations of existing multi-turn RL pipelines by enabling complex, multi-token actions and preserving context across steps. The training framework employs a novel agentic policy gradient with verifiable multi-faceted rewards (including final answer correctness, format compliance, and code execution) and a scalable sandboxed infrastructure for parallelized rollouts.

## Method Summary
L0 trains autonomous agents using reinforcement learning with a "code-as-action" approach. The NB-Agent enters a Think-Code-Observe loop, generating reasoning traces followed by Python code that executes in a stateful Jupyter kernel, capturing outputs as observations. The training employs agentic policy gradient with token-level normalization, verifiable multi-faceted rewards (final answer correctness, format compliance, code execution), and dynamic sampling to manage training stability. The system uses Qwen2.5-7B-Instruct as the base model, trains on 20K filtered QA pairs, and leverages a decoupled GPU/CPU architecture with sandboxed workers for parallel rollouts.

## Key Results
- Qwen2.5-7B-Instruct accuracy improves from 30% to 80% on SimpleQA benchmark
- HotpotQA EM increases from 22% to 41% with L0 training
- Agentic scaffold alone provides strong baseline, further amplified by RL training
- Outperforms supervised and RL baselines on multi-hop QA tasks

## Why This Works (Mechanism)

### Mechanism 1: Code-as-Action via REPL Stateful Execution
The NB-Agent's Think-Code-Observe loop executes Python code in a persistent Jupyter kernel, using the REPL environment as external memory. This enables multi-step reasoning with variable persistence across steps, outperforming single-tool paradigms. The core assumption is that the base LLM has sufficient coding proficiency to generate valid Python code. If the model fails to generate executable code in >30% of attempts, learning degrades due to noisy observations.

### Mechanism 2: Agentic Policy Gradient with Token-Level Normalization
The policy gradient operates at action-sequence level (complete code blocks), normalized by token count to prevent longer trajectories from dominating gradients. DAPO-style token-level normalization and batch-step-wise advantage normalization stabilize training. The core assumption is that reward signals propagate meaningfully across 10+ step trajectories. If discount factor Î³ is too low (<0.9), long-horizon credit assignment fails; if too high, variance explodes.

### Mechanism 3: Multi-Faceted Verifiable Rewards
Rewards decompose into final answer correctness, format compliance, and code execution success, providing dense automatic learning signals. The core assumption is that tasks have ground-truth answers amenable to exact match scoring and that code execution errors correlate with poor reasoning. If tasks require subjective or multi-valid answers, EM-based rewards provide misleading signals.

## Foundational Learning

- **Policy Gradient / REINFORCE**: The entire L0 training framework builds on âˆ‡log Ï€(a|s)Â·Ã‚. Quick check: Why does REINFORCE require baseline subtraction (advantage) to reduce variance, and what happens if you skip normalization?

- **REPL and Stateful Execution**: NB-Agent's core loop assumes a persistent Python namespace. Quick check: What is the difference between executing code in a fresh Python subprocess vs. a persistent Jupyter kernel, and how does this affect agent memory?

- **KL Divergence Penalty in RL**: The paper mentions an explicit KL penalty to prevent policy drift. Quick check: Why does a KL penalty help prevent "reward hacking" where the policy exploits spurious reward signals?

## Architecture Onboarding

- **Component map:** Inference Server (GPU) hosting policy model -> HTTP (stateless) -> Agent Worker Pool (CPU, Bubblewrap) with Jupyter kernels + Sandbox -> Training Engine with Agentic Policy Gradient
- **Critical path:** Task dispatched from controller to worker â†’ Worker queries inference server â†’ LLM generates (ðŸ’­, <code>) â†’ Worker executes code in sandboxed kernel â†’ Output wrapped as <output>, appended to trajectory â†’ context_watcher checks token budget â†’ Loop until submit_final_answer() or timeout â†’ Trajectory batched for training
- **Design tradeoffs:** Bubblewrap vs Docker (Bubblewrap safer, less tooling maturity), decoupled inference adds 10-50ms latency but enables independent scaling, on-policy only (simpler but requires more samples)
- **Failure signatures:** Training collapse on hard tasks (response length explosion, format/execution rewards crash), redundant search queries not decreasing (learning signal not propagating), context window overflow without recovery
- **First 3 experiments:** 1) Validate scaffold baseline on HotpotQA with Qwen2.5-7B-Instruct, 2) Train reward ablation variants (full, final only, final+format), 3) Reproduce collapse scenario on hard_dataset subset with dynamic sampling disabled

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does the "code-as-action" framework transfer to non-retrieval domains like software engineering or GUI navigation? The authors limit evaluation to factuality QA benchmarks, though the scaffold is domain-agnostic.
- **Open Question 2:** Can the "degenerate policy" observed in high-difficulty tasks be mitigated through algorithmic improvements rather than data filtering? The paper uses dynamic sampling to discard bad trajectories but doesn't address root causes of instability.
- **Open Question 3:** Does reliance on code execution success as a reward signal incentivize the agent to generate "hacky" code that runs without errors but doesn't contribute to reasoning? The paper doesn't analyze whether successful execution correlates with meaningful reasoning steps.

## Limitations

- Code-as-Action Robustness: The approach assumes reliable LLM code generation; if >30% of code blocks fail, learning degrades significantly
- Reward Signal Quality: Fully automatic rewards assume unambiguous ground-truth answers; for subjective tasks, EM-based rewards provide misleading gradients
- Scalability Complexity: Decoupled CPU/GPU architecture adds 10-50ms HTTP latency per step and requires non-trivial DevOps infrastructure

## Confidence

- **High Confidence**: Multi-faceted verifiable reward design is sound and proven in RLHF/self-play training
- **Medium Confidence**: Code-as-action mechanism is innovative but depends on LLM coding proficiency not quantified in paper
- **Medium Confidence**: Agentic policy gradient with token-level normalization addresses real instability but lacks ablation studies isolating component impacts

## Next Checks

1. **Coding Failure Rate Measurement**: Instrument NB-Agent to log code generation success/failure rates on held-out validation set. If failure rate >20%, implement error recovery and retrain.

2. **Reward Component Ablation**: Train four variants (full reward, final answer only, final+format, final+execution). Compare HotpotQA EM after 1000 steps. If format/execution rewards contribute <5% gain, simplify reward function.

3. **Latency Sensitivity Analysis**: Measure end-to-end task completion time with 1, 4, 16 workers. Plot completion time vs. worker count. If marginal returns diminish (<10% gain from 4â†’16 workers), optimize worker startup time or batch requests.