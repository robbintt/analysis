---
ver: rpa2
title: 'Evaluating the cognitive reality of Spanish irregular morphomic patterns:
  Humans vs. Transformers'
arxiv_id: '2507.21556'
source_url: https://arxiv.org/abs/2507.21556
tags:
- l-shaped
- human
- participants
- responses
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether transformer models generalize Spanish
  morphomic patterns like humans by comparing their performance to human behavioral
  data from Nevins et al. (2015).
---

# Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers

## Quick Facts
- arXiv ID: 2507.21556
- Source URL: https://arxiv.org/abs/2507.21556
- Authors: Akhilesh Kakolu Ramarao; Kevin Tang; Dinah Baer-Henney
- Reference count: 30
- Primary result: Transformer models show opposite response preferences to humans when generalizing Spanish morphomic patterns

## Executive Summary
This study compares transformer model performance to human behavioral data on Spanish morphomic pattern generalization. The researchers trained transformer models on Spanish verb data with different frequency distributions of regular and irregular verbs, then tested them on nonce verb production tasks using the same items as a previous human study by Nevins et al. (2015). The results reveal that while transformers achieve higher stem accuracy than humans (52-57% vs 16%), they exhibit opposite response preferences - humans consistently favor regular forms while models prefer irregular forms, with preferences increasing proportionally to irregular verb frequency in training data.

The study finds that models trained on natural and balanced distributions show sensitivity to phonological similarity between test items and real Spanish L-shaped verbs, mirroring a limited aspect of human generalization. However, the overall conclusion is that morphomic patterns are more productive in models than humans, who treat irregulars as exceptions. The artificial experimental setup that constrains models to only tested verb forms is identified as a potential factor inflating performance metrics compared to naturalistic language use.

## Method Summary
The study compared transformer model performance to human behavioral data from Nevins et al. (2015) on Spanish morphomic pattern generalization. Transformer models were trained on three frequency conditions: 10%L-90%NL, 50%L-50%NL, and 90%L-10%NL, where L represents irregular verbs and NL represents regular verbs. The models were then tested on nonce verb production tasks using the exact same items as the human study. Performance was measured in terms of stem accuracy and response preferences between regular and irregular forms. The study specifically examined whether models would show similar generalization patterns to humans, particularly regarding sensitivity to phonological similarity between test items and real Spanish L-shaped verbs.

## Key Results
- Transformers achieved higher stem accuracy than humans (52-57% vs 16%) on nonce verb production tasks
- Humans consistently favored regular forms while models preferred irregular forms, with model preferences increasing proportionally to irregular verb frequency in training data
- Models trained on natural and balanced distributions showed sensitivity to phonological similarity between test items and real Spanish L-shaped verbs, mirroring a limited aspect of human generalization

## Why This Works (Mechanism)
Unknown

## Foundational Learning
- Spanish morphomic patterns - Irregular verb inflection patterns in Spanish that follow systematic rather than purely phonological rules
  - Why needed: Forms the target phenomenon being studied for cognitive reality
  - Quick check: Can be verified through examination of Spanish verb conjugation tables showing irregular patterns

- Transformer models - Neural network architecture using self-attention mechanisms for sequence processing
  - Why needed: The computational models being compared to human performance
  - Quick check: Can be validated through model architecture documentation and implementation details

- Frequency effects in morphological learning - How the distribution of regular vs irregular forms in training data affects generalization patterns
  - Why needed: Central to understanding the experimental design with three different frequency conditions
  - Quick check: Can be examined through training data statistics and model performance across conditions

## Architecture Onboarding

Component map: Spanish verb data -> Transformer model -> Stem accuracy and preference outputs -> Comparison with human data

Critical path: Training data preparation -> Model training on different frequency conditions -> Testing on identical nonce items as human study -> Performance evaluation and comparison

Design tradeoffs: The study prioritizes controlled experimental comparison by using identical test items as the human study, but this creates artificial constraints that may inflate model performance compared to naturalistic use

Failure signatures: Large performance gap between models and humans (52-57% vs 16% stem accuracy) indicates models may be overfitting to the constrained experimental setup rather than demonstrating true cognitive similarity

First experiments:
1. Test model performance on naturalistic text corpora to assess whether high accuracy rates persist outside controlled experimental conditions
2. Implement human-like constraints in model training (limited exposure to irregular forms, contextual dependencies) to isolate factors driving performance gap
3. Conduct cross-linguistic validation with languages having different morphological systems to determine pattern generalizability beyond Spanish

## Open Questions the Paper Calls Out
None

## Limitations
- Artificial experimental setup constrains models to only tested verb forms, potentially inflating performance metrics compared to naturalistic language use
- Stark performance gap between models and humans (52-57% vs 16% stem accuracy) may reflect differences in training methodology rather than fundamental cognitive differences
- Limited finding of phonological sensitivity in models only mirrors one aspect of complex human generalization patterns documented in original human study

## Confidence
- Comparative performance analysis: Medium confidence
- Claims about cognitive reality of morphomic patterns: Low confidence

## Next Checks
1. Test model performance on naturalistic text corpora to assess whether the high accuracy rates persist outside controlled experimental conditions
2. Implement human-like constraints in the model training (e.g., limited exposure to irregular forms, contextual dependencies) to isolate which factors drive the performance gap
3. Conduct cross-linguistic validation with languages having different morphological systems to determine if the pattern generalizes beyond Spanish verb inflection