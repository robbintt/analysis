---
ver: rpa2
title: 'Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and
  Revealing Method-Dependent Performance Patterns'
arxiv_id: '2509.03176'
source_url: https://arxiv.org/abs/2509.03176
tags:
- evaluation
- attribution
- threshold
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the fundamental flaw in attribution method
  evaluation: threshold selection bias that can reverse method rankings by over 200
  percentage points. Current single-threshold evaluation protocols introduce systematic
  artifacts that undermine scientific conclusions about method performance.'
---

# Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns

## Quick Facts
- **arXiv ID:** 2509.03176
- **Source URL:** https://arxiv.org/abs/2509.03176
- **Reference count:** 13
- **Primary result:** Threshold selection bias can reverse attribution method rankings by over 200 percentage points; AUC-IoU framework eliminates this bias.

## Executive Summary
This paper addresses a fundamental flaw in explainable AI evaluation: threshold selection bias that systematically distorts method rankings. Current single-threshold evaluation protocols introduce artifacts that undermine scientific conclusions about attribution method performance. The authors introduce a threshold-free evaluation framework using Area Under the Curve for Intersection over Union (AUC-IoU) that computes attribution quality across the complete threshold spectrum rather than at arbitrary points. Systematic evaluation of seven attribution methods on dermatological imaging reveals that conventional approaches yield contradictory results, while the new framework provides stable, reproducible rankings. Size-stratified analysis shows performance varies up to 269% across lesion scales, demonstrating that method selection cannot rely on aggregate metrics alone.

## Method Summary
The study evaluates seven attribution methods (XRAI, LIME, GradCAM, Vanilla IG, Blur IG, SmoothGrad IG, Guided IG) on the HAM10000 dermoscopic image dataset. A ResNet-18 model is trained for binary melanoma classification with frozen early layers and fine-tuned later layers. Attribution maps are generated for 500 test images (167 melanoma, 333 non-melanoma). The novel evaluation computes IoU between attribution maps and ground truth segmentation masks across 19 thresholds (0.05-0.95), integrating to calculate AUC-IoU. Statistical significance is assessed using Wilcoxon signed-rank tests with Holm-Bonferroni correction. Temperature scaling (T=2.28) is applied for LIME probability inputs.

## Key Results
- Threshold selection alone can alter attribution method rankings by over 200 percentage points
- XRAI achieves 31% improvement over LIME and 204% improvement over vanilla Integrated Gradients
- Performance varies up to 269% across lesion scales, with GradCAM showing the most dramatic size sensitivity
- AUC-IoU framework eliminates ranking instability inherent in single-threshold evaluation

## Why This Works (Mechanism)

### Mechanism 1: Threshold-Spectrum Integration Neutralizes Binarization Bias
- **Claim:** Aggregating evaluation metrics across a full spectrum of binarization thresholds eliminates the ranking reversals caused by arbitrary threshold selection.
- **Mechanism:** Attribution methods produce continuous heatmaps with different signal distributions (concentrated vs. diffuse). Standard evaluation binarizes these at a single point, systematically favoring methods whose signal density aligns with that specific cutoff. By computing AUC-IoU across τ ∈ [0.05, 0.95], the framework integrates performance over all possible cutoffs, preventing methods from appearing superior simply because their peak signal intensity matches an arbitrary evaluation threshold.
- **Core assumption:** Attribution quality is a cumulative property observable across the entire relevance spectrum, rather than a binary property identifiable at a single optimal cutoff.
- **Evidence anchors:** Abstract notes threshold choice can alter rankings by over 200 percentage points; Table 5 shows Vanilla IG varies 235.6 percentage points between low and high thresholds.
- **Break condition:** This mechanism fails if ground truth relevance is binary and sparse, where partial overlap has no semantic meaning.

### Mechanism 2: Region-Based Aggregation Reduces Fragmentation Penalty
- **Claim:** Region-based attribution methods (XRAI) achieve higher IoU scores because they evaluate relevance over coherent segments rather than individual pixels, aligning structurally with ground truth segmentation masks.
- **Mechanism:** Pixel-wise gradient methods often generate noisy or fragmented attribution maps where important pixels are scattered. The IoU metric penalizes fragmentation (scattered pixels dilute the intersection/union ratio). XRAI groups pixels into regions based on attribution density before scoring, effectively smoothing the heatmap to match the contiguous nature of biological lesions.
- **Core assumption:** Diagnostically relevant features correspond to contiguous image regions rather than dispersed textures or edges.
- **Evidence anchors:** Table 2 shows XRAI achieves highest mean AUC-IoU (0.1844), significantly outperforming pixel-based Vanilla IG (0.0606).
- **Break condition:** This advantage degrades if the target feature is defined by texture rather than a solid mass, as region aggregation might smooth over critical details.

### Mechanism 3: Scale-Dependency of Coarse vs. Fine Localization
- **Claim:** The relative performance of coarse localization methods (Grad-CAM) versus fine-grained methods (IG variants) is conditional on target size.
- **Mechanism:** Grad-CAM relies on low-resolution activation maps, providing coarse localization insufficient to precisely bound small lesions, resulting in low IoU. As lesion size increases, the coarse resolution becomes sufficient to capture the bulk of the target, leading to dramatic performance gains (269% improvement) that fine-grained methods do not exhibit to the same degree.
- **Core assumption:** The spatial resolution of the network's internal activation maps limits the localization precision for small objects.
- **Evidence anchors:** Table 4 shows GradCAM improves from 0.046 (small) to 0.171 (large); Figure 3 visualizes the steep upward slope for GradCAM vs. flatter trajectory for Blur IG.
- **Break condition:** This mechanism breaks if small lesions are identified purely by global context rather than local pixel boundaries.

## Foundational Learning

- **Concept: Intersection over Union (IoU)**
  - **Why needed here:** This is the core evaluation metric. You cannot interpret the results without understanding that IoU measures the overlap between the "explanation" (attribution map) and the "truth" (segmentation mask), penalizing both missed areas and excess coverage.
  - **Quick check question:** If a model highlights the entire image as "important," it might have high sensitivity but low IoU. Why?

- **Concept: Binarization Thresholding (τ)**
  - **Why needed here:** The paper's central critique is that converting a continuous heatmap to a binary mask requires picking a cutoff. Understanding that low thresholds capture more area (diffuse) and high thresholds capture only peaks (concentrated) is essential to grasping the "bias."
  - **Quick check question:** A gradient-based method produces high-magnitude peaks. Would a low threshold (τ=0.1) or high threshold (τ=0.9) likely result in a "noisier" binary mask for this method?

- **Concept: Attribution Paradigms (Gradient vs. Perturbation vs. Region)**
  - **Why needed here:** The paper compares methods based on their fundamental mechanics. You need to know that Gradient-based methods backpropagate error, Perturbation-based methods modify inputs to see output changes, and Region-based methods build upon these but aggregate results over segments.
  - **Quick check question:** Which paradigm would you expect to be most computationally expensive if you had to run inference thousands of times per image to generate a single explanation?

## Architecture Onboarding

- **Component map:** Data Loader -> Base Model (ResNet-18) -> Attribution Engine (7 methods) -> Evaluation Loop (19 thresholds) -> Metric Calculator (IoU + AUC integration)
- **Critical path:** The Evaluation Loop is the novel component. Do not simply compute the IoU at the "best" threshold. You must implement the trapezoidal integration of IoU scores across the full range of τ ∈ [0.05, 0.95] to reproduce the paper's findings.
- **Design tradeoffs:**
  - Cost: This framework requires 19× the metric calculations of a standard evaluation
  - Sensitivity: The choice of IoU implies that "tightness of fit" to a mask is the definition of quality. If the ground truth masks are poor quality, the evaluation signal degrades
- **Failure signatures:**
  - Zero IoU Trap: If the heatmap and mask are misaligned or the heatmap is empty, IoU is 0. If IoU is 0 for all τ, AUC-IoU is 0
  - Scaling Artifacts: The paper resizes masks to 224×224. If resizing is not handled carefully, boundary pixels may distort the IoU calculation
  - LIME Invariance: If you observe LIME's performance changing drastically with threshold, check the implementation; the paper suggests LIME should be relatively threshold-invariant due to its superpixel nature
- **First 3 experiments:**
  1. Sanity Check (Threshold Sensitivity): Take a single image. Plot IoU vs. Threshold (τ) for Vanilla IG and LIME on the same graph. Verify that IG shows a steep decay while LIME is flatter.
  2. Ranking Stability: Calculate rankings using τ=0.3 vs. τ=0.7. Confirm that the "best" method changes depending on the threshold chosen (reproducing the "200 percentage point" reversal claim).
  3. Scale Stratification: Bin your test set into "small" and "large" targets (using mask pixel count). Compare GradCAM vs. XRAI performance on the small bin to validate the claim that GradCAM struggles with small lesions.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the AUC-IoU framework generalize effectively to multi-class classification tasks and non-medical imaging domains?
  - **Basis in paper:** The authors note the study is "limited to a single dataset and binary classification task; generalization to other modalities, multi-class settings, and non-medical domains requires further validation."
  - **Why unresolved:** The current validation is restricted to binary melanoma classification on the HAM10000 dataset.
  - **What evidence would resolve it:** Replicating the evaluation framework on multi-class datasets (e.g., ImageNet) or other modalities (e.g., radiology, natural scenes).

- **Open Question 2:** Does threshold selection bias similarly distort faithfulness metrics (e.g., Deletion/Insertion) and human evaluation results?
  - **Basis in paper:** "IoU alone may not fully capture attribution quality; future work should examine threshold bias in faithfulness, human evaluation, and downstream task metrics."
  - **Why unresolved:** The study focused exclusively on Intersection over Union (IoU) against ground truth segmentation masks.
  - **What evidence would resolve it:** Studies applying threshold-free analysis to perturbation-based faithfulness scores or conducting user studies to test for threshold-dependent perception bias.

- **Open Question 3:** Can efficient approximations be developed to mitigate the computational overhead of calculating AUC-IoU?
  - **Basis in paper:** "The computational overhead of threshold-free evaluation (19× metric calculations) poses practical challenges for large-scale studies, motivating development of efficient approximations."
  - **Why unresolved:** The current method requires calculating IoU at 19 uniform thresholds, increasing processing time significantly.
  - **What evidence would resolve it:** Algorithms using adaptive threshold sampling or Monte Carlo integration that achieve equivalent statistical power with fewer calculations.

## Limitations

- Evaluation framework's conclusions are fundamentally dependent on ground truth segmentation mask quality and resolution
- Study focuses exclusively on dermatological imaging, constraining generalizability to other domains
- Computational cost of threshold-free evaluation (19× metric calculations) represents a practical barrier for widespread adoption

## Confidence

**High Confidence Claims:**
- Threshold selection significantly impacts attribution method rankings (200+ percentage point variations documented)
- XRAI outperforms other methods on aggregate metrics with statistical significance
- Performance varies substantially across lesion size categories (up to 269% differences)

**Medium Confidence Claims:**
- Region-based methods inherently produce better IoU scores due to structural alignment with segmentation masks
- Scale-dependency mechanism explaining GradCAM's performance variation
- Temperature scaling requirement for LIME probability inputs

**Low Confidence Claims:**
- Generalizability of threshold-free framework to non-medical domains
- Claims about "fairness" in attribution evaluation (not rigorously defined beyond overlap metrics)

## Next Checks

1. **Mask Quality Sensitivity Analysis:** Systematically degrade ground truth mask quality (add noise, vary boundary precision) and measure impact on method rankings. This validates whether the framework is measuring attribution quality or mask-artifact correlation.

2. **Cross-Domain Replication:** Apply the threshold-free evaluation framework to a non-medical dataset (e.g., satellite imagery or natural scene classification) to test generalizability of the threshold-bias findings and method performance patterns.

3. **Computational Efficiency Study:** Implement and benchmark approximation methods (e.g., adaptive threshold sampling or importance-weighted integration) to reduce the 19× computational overhead while maintaining ranking stability.