---
ver: rpa2
title: 'Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback'
arxiv_id: '2508.15757'
source_url: https://arxiv.org/abs/2508.15757
tags:
- optimization
- configuration
- training
- search
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language-Guided Tuning (LGT) addresses the challenge of configuration
  optimization in machine learning, which involves tuning model architecture, training
  strategy, feature engineering, and hyperparameters. Traditional methods treat these
  dimensions independently and lack interpretability.
---

# Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback

## Quick Facts
- arXiv ID: 2508.15757
- Source URL: https://arxiv.org/abs/2508.15757
- Reference count: 10
- Primary result: 23.3% accuracy improvement and 49.3% error reduction across six diverse datasets

## Executive Summary
Language-Guided Tuning (LGT) addresses configuration optimization in machine learning by introducing a multi-agent system that uses textual gradients from Large Language Models to complement numerical optimization. The system coordinates architecture, training strategy, feature engineering, and hyperparameter tuning through three specialized LLM agents that create a self-improving feedback loop. Across six diverse datasets, LGT achieves significant performance gains while maintaining interpretability through natural language reasoning about optimization decisions.

## Method Summary
LGT employs three LLM agents - Advisor, Evaluator, and Optimizer - working in a coordinated loop to optimize machine learning configurations. The Advisor analyzes training history and generates configuration changes via natural language, which are parsed into structured modifications. The Evaluator assesses whether changes improved performance, and the Optimizer refines the Advisor's prompts based on accumulated outcomes, creating a meta-learning loop. The system operates at epoch boundaries, using DeepSeek API with temperature 0.2 to generate textual gradients that complement numerical optimization across architecture, training strategy, feature engineering, and hyperparameters.

## Key Results
- Achieves up to 23.3% absolute accuracy improvement and 49.3% error reduction compared to established methods
- Demonstrates consistent performance gains across six diverse datasets (MNIST, CIFAR-10, Iris, Water Potability, House Price, Wine Quality)
- Shows 99.99% AUC on MNIST with significantly fewer training epochs than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Textual gradients from LLMs complement numerical optimization by surfacing semantic relationships between configuration dimensions that pure gradient descent cannot capture. The Advisor generates natural-language rationales for configuration changes that are parsed into structured modifications, providing qualitative feedback that captures interdependencies.

Core assumption: LLMs can reliably map training dynamics to configuration adjustments via natural language reasoning. Evidence: Performance improvements across datasets, though the paper doesn't prove LLM reasoning quality.

### Mechanism 2
Coordinated multi-dimensional optimization across architecture, training strategy, feature engineering, and hyperparameters outperforms independent tuning by exploiting interdependencies. Each configuration dimension has its own optimization function but shares a common training history, enabling coordinated adjustments.

Core assumption: Configuration dimensions are non-separable and benefit from joint optimization. Evidence: Performance gains when treating dimensions jointly vs. independently.

### Mechanism 3
Prompt optimization via the Optimizer agent creates a self-improving meta-loop that increases advisor effectiveness over iterations. The Optimizer generates modifications to the Advisor's prompt based on optimization history, creating a learning loop over the optimization trajectory.

Core assumption: LLMs can synthesize lessons from trials to improve their own prompting strategies. Evidence: Theoretical convergence analysis, though not empirically validated for long-term stability.

## Foundational Learning

- **Bayesian Optimization Surrogates**: Needed to understand what LGT replaces or augments; helps contextualize GP surrogate models and acquisition functions
  - Quick check: Can you explain why expected improvement acquisition balances exploration vs. exploitation?

- **Multi-Agent LLM Orchestration**: Prerequisite to understand LGT's three-agent architecture and communication protocols
  - Quick check: What happens if the Evaluator's success signal is noisy or delayed - how does the Optimizer respond?

- **Prompt Engineering for Structured Output**: Essential for implementing LGT since prompts must enforce structured formats extractable by code
  - Quick check: How would you design a prompt to ensure the Advisor outputs valid JSON configuration changes rather than free-form text?

## Architecture Onboarding

- **Component map**: Training loop → Advisor → parsed Δc_t → apply config → Evaluator → success signal → Optimizer → prompt modification → append to history
- **Critical path**: 1. Train one epoch → collect metrics Mt 2. Advisor generates Δc_t from (Mt, c_t) 3. Apply c_{t+1} = c_t + Δc_t 4. Evaluator assesses success_t from (Mt, c_t, M_base, c_base) 5. Optimizer updates Advisor prompt if convergence stalls 6. Append to history H_t and repeat
- **Design tradeoffs**: LLM API latency vs. optimization frequency (epoch-level only), prompt complexity vs. parse reliability, temperature setting affects determinism vs. exploration
- **Failure signatures**: Unparseable Advisor outputs cause config to remain unchanged, Evaluator oscillation leads to unstable prompt updates, convergence stalls when configuration space is exhausted
- **First 3 experiments**: 1. Reproduce MNIST ablation to verify relative contributions and 98.99% accuracy target 2. Stress-test parsing robustness by injecting malformed outputs 3. Ablate Optimizer agent to measure meta-loop contribution vs. baseline architecture

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical approximation gap (δ) in the convergence analysis be characterized or bounded relative to standard numerical optimization? The paper establishes the existence of the gap but leaves the magnitude of error introduced by discrete textual reasoning unquantified.

### Open Question 2
Does the computational overhead of multi-agent LLM inference invalidate efficiency gains for large-scale datasets? While the method reduces training epochs, API call latency could become the bottleneck on tasks like ImageNet.

### Open Question 3
How sensitive is the feedback loop stability to noise or hallucinations in the Evaluator agent's binary success signal? The paper demonstrates successful optimization but doesn't analyze failure modes where textual gradients are semantically incorrect but syntactically valid.

## Limitations

- Core assumption that LLM-generated textual gradients reliably map training dynamics remains empirically demonstrated but not theoretically validated
- Heavy API costs (estimated 1500+ DeepSeek calls for full reproduction) may limit practical adoption despite performance gains
- Meta-learning convergence depends on bounded prompt updates, but long-term stability and failure modes are not empirically validated

## Confidence

- **High confidence**: Multi-agent architecture is well-specified and reproducible; performance improvements vs. baselines are measurable and significant
- **Medium confidence**: Textual gradient mechanism works empirically but lacks rigorous validation of LLM reasoning quality and parse reliability
- **Low confidence**: Claims about self-improving meta-loop convergence are theoretical assertions without empirical validation of long-term stability

## Next Checks

1. **Parsing robustness test**: Inject malformed Advisor outputs into the pipeline and measure recovery rate vs. silent failure, tracking impact on convergence speed and final performance
2. **Optimizer ablation study**: Run identical experiments with Optimizer disabled across all 6 datasets to quantify meta-loop contribution beyond the baseline Advisor-Evaluator architecture
3. **LLM reasoning quality audit**: Manually evaluate whether Advisor's rationales match actual training dynamics and whether proposed changes would be reasonable to human experts