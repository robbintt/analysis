---
ver: rpa2
title: 'Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring'
arxiv_id: '2509.25438'
source_url: https://arxiv.org/abs/2509.25438
tags:
- intrinsic
- exploration
- reward
- logp
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learning Progress Monitoring (LPM), a novel
  intrinsic motivation method for exploration in reinforcement learning. LPM addresses
  the "noisy-TV problem" where agents get stuck observing unlearnable random sources
  by rewarding learning progress rather than prediction error or novelty.
---

# Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring

## Quick Facts
- arXiv ID: 2509.25438
- Source URL: https://arxiv.org/abs/2509.25438
- Reference count: 16
- Primary result: Introduces LPM, a dual-network intrinsic motivation method that rewards learning progress instead of prediction error, demonstrating superior robustness to aleatoric uncertainty compared to state-of-the-art exploration baselines.

## Executive Summary
This paper introduces Learning Progress Monitoring (LPM), a novel intrinsic motivation method for exploration in reinforcement learning that addresses the "noisy-TV problem" where agents get stuck observing unlearnable random sources. LPM rewards learning progress—the reduction in prediction error over time—rather than raw prediction error or novelty. The method employs a dual-network architecture where a dynamics model predicts next observations while an error model predicts the expected prediction error of the previous dynamics model iteration. This design naturally filters out aleatoric uncertainty (unlearnable noise) while rewarding epistemic uncertainty (learnable novelty), demonstrated through superior performance across multiple domains including faster convergence in noisy MNIST experiments, higher state coverage in 3D maze environments, and improved extrinsic rewards in Atari games compared to state-of-the-art baselines.

## Method Summary
LPM introduces a dual-network architecture where a dynamics model $f_\theta$ predicts next observations and an error model $g_\phi$ predicts the expected prediction error of the previous dynamics model iteration. The intrinsic reward is calculated as the difference between these errors, measuring learning progress. The method uses a fixed-size replay queue $D$ storing (observation, action, error) tuples and updates both models every $N$ environment steps. During each step, the agent computes the current error using the latest dynamics model, pushes this to the queue, and when the update cycle triggers, trains the dynamics model on the replay buffer and the error model on the queue. This design ensures the intrinsic reward is monotonically related to information gain while filtering out unlearnable noise.

## Key Results
- In Noisy MNIST experiments, LPM achieved faster convergence to zero intrinsic reward under stochastic transitions compared to baselines, demonstrating robustness to aleatoric uncertainty
- In MiniWorld 3D maze environments, LPM achieved higher state coverage than competing methods during pure exploration tasks
- Across Atari games with action noise variants, LPM demonstrated improved extrinsic rewards compared to state-of-the-art exploration methods including AMA, EME, EDT, Ensemble, IDF, and RND

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rewarding the reduction in prediction error (learning progress) rather than raw error naturally filters out unlearnable noise (aleatoric uncertainty), preventing the "noisy-TV" trap.
- **Mechanism:** Standard curiosity methods reward high prediction error (surprise). Since random noise is inherently unpredictable, it yields perpetually high error, trapping the agent. LPM rewards the *derivative* of error. If a source is unlearnable (random noise), the error never decreases, so the intrinsic reward $r^i_t$ remains zero. If a source is learnable, the error drops, generating positive reward.
- **Core assumption:** The environment contains a mix of learnable dynamics (epistemic uncertainty) and unlearnable stochasticity (aleatoric uncertainty), and the agent's model capacity is sufficient to learn the learnable components.
- **Evidence anchors:**
  - [abstract] "rewards learning progress... effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions."
  - [section 3] "ri_t = E_D[\epsilon^{(\tau-1)}] - \epsilon^{(\tau)}... intuitively captures the prediction accuracy improvement."
  - [corpus] Related work "Exploration by Random Distribution Distillation" addresses exploration via prediction error (RND), which LPM specifically contrasts against regarding noise robustness.
- **Break condition:** If the "unlearnable" source actually becomes learnable (e.g., a pseudo-random number generator the model cracks), LPM will suddenly start rewarding it.

### Mechanism 2
- **Claim:** Using an error model to predict the *expected* prediction error of the *previous* model iteration is necessary to ensure the intrinsic reward is a monotonic indicator of Information Gain (IG).
- **Mechanism:** A naive "learning progress" implementation might calculate the difference between the current error and the *pointwise* error of the old model on the new data. The paper proves (Theorem 4.2) that this pointwise approach lacks a deterministic monotonic relationship with IG. By learning a separate error model $g_\phi$ to estimate the *expectation* of the previous error, LPM satisfies the monotonicity requirement ($r^i \geq \frac{1}{c}IG$).
- **Core assumption:** The error model $g_\phi$ can successfully converge to approximate the expected error of the dynamics model $f_\theta$.
- **Evidence anchors:**
  - [section 4.2] "Theorem 4.2... necessity of expectation in intrinsic reward for monotonicity... the expectation in the first term of ri is necessary."
  - [section 3] "We introduce a separate error model... which predicts the error of the dynamics model before the last updating step."
- **Break condition:** If the error model $g_\phi$ is under-parameterized or trained on insufficient data, it fails to approximate the expectation, potentially breaking the theoretical guarantees and causing unstable rewards.

### Mechanism 3
- **Claim:** Decoupling the dynamics model update from the policy update via a fixed "update cycle" $N$ stabilizes the "learning progress" signal.
- **Mechanism:** Learning progress is measured between model version $\tau-1$ and $\tau$. If the model updates every step, the "previous" version is a moving target that changes too rapidly for stable comparison. By updating $f_\theta$ only every $N$ steps, the agent collects a consistent buffer of errors relative to a fixed model version, allowing meaningful measurement of progress when the update finally occurs.
- **Core assumption:** The update cycle $N$ is large enough to populate the buffer $D$ but small enough that the policy doesn't overfit to an outdated model.
- **Evidence anchors:**
  - [section 3] "We use t to denote the environment time steps and \tau to denote model updating steps... i-th model updating step happens in the (N*i)-th environment step."
  - [algorithm 1] "if t mod N = 0 then Update \tau..."
- **Break condition:** If $N$ is too small, the error buffer $D$ may not be sufficiently diverse to train $g_\phi$, leading to noisy reward estimates.

## Foundational Learning

- **Concept:** **Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** LPM is fundamentally a mechanism to separate these two. Without understanding that "noise" (aleatoric) cannot be reduced while "novelty" (epistemic) can, the motivation for LPM is unclear.
  - **Quick check question:** Does a coin flip generate epistemic or aleatoric uncertainty for a standard neural network?

- **Concept:** **Intrinsic Motivation (Curiosity)**
  - **Why needed here:** LPM is a specific instance of intrinsic motivation. Understanding the standard baseline (Prediction Error = Reward) is required to see why LPM modifies it to (Reduction in Prediction Error = Reward).
  - **Quick check question:** In a standard curiosity setup (ICM/RND), would a TV playing static receive a high or low intrinsic reward?

- **Concept:** **Information Gain (KL Divergence)**
  - **Why needed here:** The paper grounds LPM's theoretical superiority in its monotonic relationship to Information Gain.
  - **Quick check question:** If a new observation changes your belief about the world drastically, is the Information Gain high or low?

## Architecture Onboarding

- **Component map:**
  - Dynamics Model ($f_\theta$) -> Predicts $\hat{o}_{t+1}$
  - Error Model ($g_\phi$) -> Predicts scalar expected error $E[\epsilon]$
  - Replay Buffer $B$ -> Stores $(o, a, o')$ for $f_\theta$
  - Queue $D$ -> Fixed-size queue storing $(o, a, \epsilon^{(\tau)}_t)$ for $g_\phi$
  - Policy $\pi$ -> Standard actor-critic (e.g., PPO/A2C) modified with $r_t = r^e_t + \beta r^i_t$

- **Critical path:**
  1. Agent takes step, stores transition in $B$
  2. Agent computes current error $\epsilon^{(\tau)}$ using *current* $f_\theta$; pushes $(o, a, \epsilon^{(\tau)})$ to $D$
  3. If update step ($t \% N == 0$):
     - **Crucial:** Update $f_\theta$ on $B$ (model becomes $f^{(\tau+1)}$)
     - **Crucial:** Update $g_\phi$ on $D$ (training target is the error of the *old* model $f^{(\tau)}$)
  4. Compute Intrinsic Reward: $r^i = g_\phi(current\_state) - \text{current\_error}$

- **Design tradeoffs:**
  - **Dual Network Overhead:** Requires maintaining two models ($f$ and $g$) and two buffers ($B$ and $D$). Paper claims this is cheaper than Ensemble methods but more expensive than single-model RND.
  - **Buffer Size $d$:** Queue $D$ must be large enough to capture the distribution of errors, but small enough to flush out "ancient" errors that are no longer relevant to the current model state.

- **Failure signatures:**
  - **Zero Reward Collapse:** If the Dynamics Model learns too fast (or is too large), the "current error" drops to match the "predicted error" immediately, yielding $r^i \approx 0$ and halting exploration.
  - **Negative Rewards:** If the Error Model overestimates the previous error, or if the Dynamics Model regresses (current error > previous error), $r^i$ becomes negative.
  - **Lagging Error Estimates:** If the Error Model is stale (trained on errors from $\tau-5$ instead of $\tau-1$), the "learning progress" signal will be inaccurate.

- **First 3 experiments:**
  1. **Noisy MNIST Sanity Check:** Replicate the "Det vs Stoch" setup. Verify that LPM rewards drop to zero for the stochastic digit stream (noise) but not the deterministic stream (signal).
  2. **Hyperparameter Sensitivity ($N$):** Vary the update cycle $N$ (e.g., update every step vs every 100 steps). Check if frequent updates destabilize the error model training because the "target distribution" of errors shifts too fast.
  3. **Ablation on Error Model:** Replace the learned Error Model $g_\phi$ with a naive buffer lookup (just storing the exact error from the previous run). Compare performance to verify the paper's claim that the *expectation* (function approximation) is necessary for monotonicity.

## Open Questions the Paper Calls Out

- **Question:** Does the theoretical monotonicity correspondence between LPM intrinsic reward and information gain hold under non-Gaussian observation likelihoods or correlated noise distributions?
  - **Basis in paper:** [explicit] The authors state in Section 4: "While it is difficult to analyze the robustness of the above Theorems in the absence of the i.i.d. Gaussian observation model assumption..."
  - **Why unresolved:** The theoretical proofs (Theorem 4.1) rely specifically on an i.i.d. Gaussian observation model to link Mean Squared Error (MSE) to the log-likelihood and information gain. Real-world noise often violates these assumptions (e.g., heavy-tailed sensor noise).
  - **What evidence would resolve it:** A theoretical extension of Theorem 4.1 to non-Gaussian distributions, or empirical demonstrations of LPM's robustness in environments specifically designed with correlated or heavy-tailed noise processes.

- **Question:** How does LPM perform in continuous control domains or environments with compact state vectors, as opposed to the high-dimensional visual domains tested?
  - **Basis in paper:** [inferred] The empirical evaluation focuses exclusively on high-dimensional visual inputs (MNIST, MiniWorld RGB, Atari), and the implementation details (Appendix B) describe architectures using ConvTranspose decoders for image reconstruction.
  - **Why unresolved:** LPM is validated on tasks where visual prediction is difficult. It is unclear if the "learning progress" signal remains robust and effective when the dynamics model operates on low-dimensional state vectors where prediction errors might decrease too rapidly or differ in distribution.
  - **What evidence would resolve it:** Benchmarking LPM on standard continuous control suites (e.g., MuJoCo, PyBullet) using both pixel-based and state-based observations.

- **Question:** What is the sensitivity of LPM to the hyperparameters governing the temporal dynamics, specifically the update cycle $N$ and the error buffer size $d$?
  - **Basis in paper:** [inferred] Algorithm 1 relies on a specific cadence where the model updates every $N$ steps and the error model samples from a buffer $D$ of size $d$.
  - **Why unresolved:** If the update cycle $N$ is too short, the difference between the current and previous model errors might be negligible (yielding low reward). If the buffer size $d$ is too small, the estimate of the expected previous error $E_D[\epsilon]$ might be high-variance.
  - **What evidence would resolve it:** An ablation study showing performance and intrinsic reward variance across a range of values for $N$ and $d$.

## Limitations
- The error model's effectiveness depends critically on the queue size $d$ and update cycle $N$, but the paper doesn't provide a systematic analysis of these hyperparameters across environments.
- The theoretical guarantee (Theorem 4.2) assumes the error model $g_\phi$ converges to the true expected error, but convergence rates and stability under function approximation error are not quantified.
- The comparison with state-of-the-art methods shows LPM's advantage in noisy environments, but the relative performance gap is not consistently quantified across all baselines and domains.

## Confidence
- **High Confidence:** LPM's core mechanism (rewarding learning progress vs. prediction error) effectively addresses the noisy-TV problem in controlled settings (Noisy MNIST).
- **Medium Confidence:** LPM demonstrates consistent improvements in exploration and extrinsic reward across MiniWorld and Atari, though baseline implementation details remain unclear.
- **Low Confidence:** Theoretical claims about information gain monotonicity hold under idealized assumptions but may not fully translate to practical function approximation settings.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary queue size $d$ and update cycle $N$ to identify stable operating regions and failure modes.
2. **Error Model Ablation:** Compare LPM with a simplified version using buffer lookup instead of learned error model to isolate the contribution of function approximation to theoretical guarantees.
3. **Noise Robustness Scaling:** Test LPM's performance under increasing levels of stochasticity (e.g., gradually increasing the probability of random transitions in Noisy MNIST) to identify where the method begins to fail.