---
ver: rpa2
title: 'DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current
  Effects'
arxiv_id: '2505.00961'
source_url: https://arxiv.org/abs/2505.00961
tags:
- policy
- dolce
- learning
- bias
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DOLCE addresses off-policy evaluation and learning under support
  violations in contextual bandits by leveraging lagged contexts to construct lag-marginalized
  importance weights, achieving exact bias cancellation when the reward-model residual
  is conditionally mean-zero given the lagged context and action. DOLCE decomposes
  the objective into a support-robust lagged correction term and a current, model-based
  term, and uses softmin aggregation across multiple candidate lags weighted by an
  approximate local correctness score.
---

# DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects

## Quick Facts
- arXiv ID: 2505.00961
- Source URL: https://arxiv.org/abs/2505.00961
- Reference count: 40
- Primary result: DOLCE achieves exact bias cancellation under residual invariance and maintains near-nominal confidence interval coverage as overlap violations intensify

## Executive Summary
DOLCE addresses off-policy evaluation and learning under support violations in contextual bandits by leveraging lagged contexts to construct lag-marginalized importance weights. The method achieves exact bias cancellation when the reward-model residual is conditionally mean-zero given the lagged context and action. DOLCE decomposes the objective into a support-robust lagged correction term and a current, model-based term, and uses softmin aggregation across multiple candidate lags weighted by an approximate local correctness score.

## Method Summary
DOLCE estimates value functions in contextual bandits when current-context overlap between logging and target policies fails but lagged-context overlap holds. It constructs lag-marginalized importance weights by replacing current-context propensity ratios with ratios computed at lagged contexts. A moment-targeted residual invariance (MTRI) estimator promotes the desired invariance using only logged lag-augmented data. The method uses softmin aggregation across multiple candidate lags weighted by approximate local correctness scores. DOLCE combines these components in a cross-fitted doubly robust-style estimator that remains unbiased when either lag propensities or reward models are correct.

## Key Results
- DOLCE substantially reduces bias and maintains near-nominal confidence interval coverage as overlap violations intensify in synthetic experiments
- DOLCE yields more reliable gradient directions and improved policy optimization under severe support violations
- In a real-world ICU dataset, DOLCE provides stable evaluation estimates and learned policies with comparable or slightly improved performance relative to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lag-marginalized importance weights enable estimation when current-context overlap fails but lag-context overlap holds.
- Mechanism: Standard IPS uses πθ(A|X)/π0(A|X), which becomes undefined when π0(A|X)=0. DOLCE replaces this with lag-marginalized ratios wk(x(k),a) = π̄θ,k(a|x(k))/π̄0,k(a|x(k)), requiring overlap only at the lagged context level. Under current-action sufficiency (A⊥⊥X(k)|X), the logging policy conditions only on current context, so lag-marginal propensities remain well-defined even when current-context support collapses.
- Core assumption: Lag overlap (Assumption 3.2) — target actions have positive probability under logging policy when marginalized to lagged context.
- Break condition: If lag overlap also fails, the estimator reverts to undefined or requires extrapolation.

### Mechanism 2
- Claim: Residual invariance enables exact bias cancellation even with misspecified reward models.
- Mechanism: The bias of lag-k DOLCE is E[Σa {π0(a|X)wk(X(k),a) - πθ(a|X)}Δk(X,X(k),a)]. If Δk = qk - q̃k is σ(X(k),A)-measurable (does not vary with current context once lag and action are fixed), then wk(X(k),a)EP[π0(a|X)|X(k)] - EP[πθ(a|X)|X(k)] = 0 by construction, cancelling the bias term exactly.
- Core assumption: Residual invariance (Assumption 3.3) — the reward-model error depends only on (X(k),A), not on X.
- Break condition: If residual variance with respect to current context is large, bias may not cancel fully.

### Mechanism 3
- Claim: MTRI training promotes residual invariance using only logged lag-augmented data without pairwise matching.
- Mechanism: Residual invariance is equivalent to orthogonality between the residual Δ and all functions in L2,0(Gk) (centered test functions). MTRI adds a minimax moment penalty sup_f EP[(R-q')f]² to the reward model loss, which lower-bounds the ALC score (conditional variance of residual). Minimizing this drives residual variance toward zero.
- Core assumption: The critic class Fk is rich enough to approximate L2,0(Gk), and cross-fitting prevents overfitting.
- Break condition: If critic class is too weak or cross-fitting fails, MTRI may not enforce invariance effectively.

## Foundational Learning

- Concept: **Importance sampling / IPS estimators**
  - Why needed here: DOLCE generalizes IPS by replacing current-context ratios with lag-marginalized ratios. Without understanding why IPS requires overlap (π0(a|x)>0 for all target actions), the motivation for lag-based correction is unclear.
  - Quick check question: Given logged data from π0, can IPS estimate V(πθ) when πθ assigns positive probability to an action that π0 never takes at context x?

- Concept: **Doubly robust estimation**
  - Why needed here: DOLCE has a DR-like structure combining lag-weighted residuals and a model-based term. The bias cancellation relies on understanding how DR achieves robustness to either propensity or reward model misspecification.
  - Quick check question: If the reward model q̃ is correct but propensities are wrong, is DR biased? If propensities are correct but q̃ is wrong?

- Concept: **Positivity/overlap in causal inference**
  - Why needed here: The entire paper addresses overlap violations. Understanding that unbiased IPS requires common support between logging and target policies is essential to see why DOLCE's lag-overlap relaxation matters.
  - Quick check question: In a treatment effect setting, what happens to inverse-probability-weighted estimation if some units with covariate X=x always receive treatment A=0?

## Architecture Onboarding

- Component map: Cross-fitting data splits → Lag propensity estimators π̄0,k(a|x(k)) → Lag target marginal estimators π̄θ,k(a|x(k)) → MTRI reward models q̃k(X, X(k), A) → ALC score estimators → Softmin aggregator with weights αk ∝ exp{-ALCk/τ}

- Critical path: Split data into Kcf folds; for each fold j and lag k: fit π̄0,k, π̄θ,k, q̃k on out-of-fold data; compute per-sample influence functions ϕ̂k,i using fold-specific nuisances; aggregate across lags using softmin-weighted ALC scores; construct Wald CIs from cross-fitted variance

- Design tradeoffs: More lags → more robust to individual lag failures but increased computation and potential variance; smaller τ → more selective softmin but higher sensitivity to ALC estimation noise; weight clipping → variance reduction but introduces bias if clipping is frequent

- Failure signatures: Near-zero effective sample size indicates lag overlap may also be violated; ALC scores near zero for all lags suggests residual invariance satisfied but check if reward model is trivially fitting; divergent estimates across lags indicates one or more lags have severe violations

- First 3 experiments: 1) Synthetic overlap violation sweep with r ∈ {0%, 25%, 50%, 75%}; 2) Residual invariance ablation with controlled interaction term η > 0; 3) Lag sensitivity analysis with artificially corrupted lag propensity estimator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is DOLCE to violations of the current-action sufficiency assumption when logging policies depend on lag information?
- Basis: Assumption 3.1 requires π₀(a|X, X⁽ᵏ⁾) = π₀(a|X), enabling the key bias-cancellation argument. The paper acknowledges this as a "standard premise" but does not analyze settings where it fails.
- Why unresolved: Theoretical guarantees rely on this conditional independence; no sensitivity analysis or experiments test violation scenarios.
- Evidence: Experiments with logging policies explicitly constructed to depend on lagged contexts would quantify robustness.

### Open Question 2
- Question: Can the lag-based decomposition framework be extended to full sequential decision-making (MDPs) beyond single-step contextual bandits?
- Basis: The method leverages temporal structure for bandits, but no discussion addresses sequential settings where rewards and transitions depend on trajectory history.
- Why unresolved: The bias-cancellation mechanism uses single-step importance weights and residual invariance; multi-step extension requires addressing compounded support violations.
- Evidence: Theoretical analysis or empirical evaluation in finite-horizon or infinite-horizon RL benchmarks.

### Open Question 3
- Question: What are principled methods for selecting the softmin temperature τ, MTRI penalty λ, and critic class Fₖ?
- Basis: Algorithm 1 introduces these hyperparameters but provides no guidance; default values are used without systematic sensitivity analysis.
- Why unresolved: Performance may depend on these choices, particularly the trade-off between variance reduction and residual-invariance enforcement.
- Evidence: Ablation studies or data-adaptive selection procedures demonstrating impact on bias, variance, and coverage.

### Open Question 4
- Question: How does DOLCE perform when lag-overlap (Assumption 3.2) also degrades, not just current-context overlap?
- Basis: The method requires "overlap at the level of a lagged context," but experiments only induce violations at the current context.
- Why unresolved: The theory establishes guarantees under lag overlap, but practical robustness when lag overlap also fails remains uncharacterized.
- Evidence: Synthetic experiments with controlled lag-overlap violation rates would establish operational boundaries.

## Limitations
- The core mechanism relies on lagged contexts providing sufficient overlap where current contexts fail, which is unverifiable from logged data alone
- The MTRI penalty's effectiveness depends critically on the critic class being rich enough to approximate required orthogonality conditions, but architectures are not specified
- DOLCE assumes current-action sufficiency, which may not hold when logging policies depend on lag information

## Confidence

**High confidence**: The bias cancellation mechanism when residual invariance holds — straightforward algebra in Proposition B.3
**Medium confidence**: Lag-marginalized importance weights work when current overlap fails — supported by theory but relies on unverifiable lag overlap assumptions
**Medium confidence**: MTRI successfully promotes residual invariance — theoretical equivalence established, but practical effectiveness depends on critic class richness and cross-fitting quality

## Next Checks

1. **Overlap propagation analysis**: Systematically vary the temporal correlation ρ between current and lagged contexts to quantify how quickly overlap violations at t affect overlap at t-k

2. **Critic class sensitivity**: Compare MTRI performance across different critic architectures (linear, kernel, neural) to establish minimum requirements for effective residual invariance enforcement

3. **Single-lag baseline comparison**: Implement a version of DOLCE using only the optimal lag (oracle selection) to quantify the actual benefit of softmin aggregation versus oracle knowledge of the best lag