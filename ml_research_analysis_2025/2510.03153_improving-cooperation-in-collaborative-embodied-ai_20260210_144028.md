---
ver: rpa2
title: Improving Cooperation in Collaborative Embodied AI
arxiv_id: '2510.03153'
source_url: https://arxiv.org/abs/2510.03153
tags:
- base
- agents
- prompt
- collaborative
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an enhanced version of CoELA, a framework for
  collaborative embodied agents powered by large language models (LLMs). The authors
  developed and tested various prompting strategies to improve agent cooperation and
  communication.
---

# Improving Cooperation in Collaborative Embodied AI

## Quick Facts
- arXiv ID: 2510.03153
- Source URL: https://arxiv.org/abs/2510.03153
- Reference count: 22
- Primary result: Prompt optimization significantly improves multi-agent collaboration efficiency in embodied AI systems

## Executive Summary
This paper presents an enhanced version of CoELA, a framework for collaborative embodied agents powered by large language models (LLMs). The authors developed and tested various prompting strategies to improve agent cooperation and communication. Key methods included structured reasoning prompts, instruction-focused messages, and multi-shot dialogue examples. These were evaluated across multiple LLMs including Llama 3.1, DeepSeek r1, Mistral, and Gemma3. Results showed that combining an improved baseline prompt with CPrompt4 significantly enhanced efficiency, reducing step counts and turn counts in collaborative tasks. For example, the best configuration improved Gemma3's efficiency by 22% over the original system. The paper also introduced a TTS-enabled chat GUI for real-time dialogue visualization, aiding system development and demonstration.

## Method Summary
The researchers evaluated prompt engineering strategies across three LLM modules in the CoELA framework: Planning, Communication, and Action. They tested four LLM models (Llama 3.1 8B, DeepSeek r1 8B, Mistral 7B, Gemma 3 4B) using 10 episodes (5 tasks × 2 variations) in VirtualHome and Habitat 3.0 environments. The primary optimization combined an Improved Baseline prompt with CPrompt4, which merges instruction removal with one-shot dialogue examples. The framework was implemented using Ollama for local LLM execution, with a TTS-enabled chat GUI for real-time dialogue visualization. Statistical significance was assessed using t-tests where applicable.

## Key Results
- Improved Baseline + CPrompt4 combination reduced step counts by 18-22% for Gemma3 and Llama 3.1 compared to baseline
- Gemma3 (4B parameters) achieved comparable efficiency to Llama 3.1 (8B parameters) when properly prompted
- DeepSeek r1 showed the least sensitivity to prompt engineering, continuing internal reasoning despite constraints
- The system reduced verbose agent outputs and improved action-focused communication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompts with explicit instructions reduce verbose outputs and improve task efficiency in multi-agent coordination.
- Mechanism: The Improved Baseline prompt replaces vague requests ("please help me choose") with directive instructions ("You should select the most efficient action based on the goal"). This constrains the LLM's output space toward goal-relevant responses, reducing fuzzy matches and redundant actions.
- Core assumption: LLMs default to verbose, explanatory outputs that introduce noise in agent-to-agent communication; explicit constraints can override this tendency.
- Evidence anchors:
  - [abstract] "combining an improved baseline prompt with CPrompt4 significantly enhanced efficiency, reducing step counts and turn counts"
  - [Section 5.1] "Adds explicit instructions to encourage more goal-focused decisions"
  - [Section 7] "the base prompt often led to verbose outputs... In contrast, the revised prompts encouraged more concise, action-focused communication"
- Break condition: Models with strong inherent reasoning priors (e.g., DeepSeek r1) may continue generating internal reasoning despite prompt constraints, limiting effectiveness.

### Mechanism 2
- Claim: Combining instruction-focused prompts with one-shot dialogue examples yields greater efficiency gains than either strategy alone.
- Mechanism: CPrompt4 merges instruction removal (CPrompt1) with one-shot examples (CPrompt2). The instruction removal suppresses formatting/verbosity; the example anchors message tone and structure. Together, they produce concise, properly formatted messages.
- Core assumption: LLMs benefit from both negative constraints (what not to do) and positive demonstrations (what to emulate).
- Evidence anchors:
  - [Section 5.2] "CPrompt 4 = CPrompt 1 + CPrompt 2: Merges the instruction-removal from CPrompt 1 with the one-shot example from CPrompt 2"
  - [Table 1] Improved Base + CPrompt4 achieved lowest step counts for Gemma3 (65.5) and Llama 3.1 (64.6)
  - [corpus] MiTa framework (arXiv:2601.22974) similarly addresses "memory inconsistency and agent behavioral conflicts" through structured multi-agent coordination
- Break condition: Multi-shot examples (CPrompt3) showed inconsistent results—extended context may introduce noise for some models.

### Mechanism 3
- Claim: Prompt effectiveness is model-dependent; smaller architectures with modern training can match larger models when properly prompted.
- Mechanism: Gemma3 (4B parameters) achieved comparable or superior efficiency to Llama 3.1 (8B) under optimized prompts, suggesting architectural and training improvements compensate for parameter count in collaborative scenarios.
- Core assumption: Model size alone does not determine collaborative reasoning capability; prompt-model fit matters significantly.
- Evidence anchors:
  - [Section 7] "Gemma 3, despite being a smaller model (4B parameters), delivered performance comparable to Llama 3.1 (8B parameters)"
  - [Section 7.1] t-test showed Llama 3.1 improvement was statistically significant (p=0.018), Gemma3 showed practical but not statistically significant gains (p=0.62)
  - [corpus] VIKI-R (arXiv:2506.09049) explores "perception-driven reasoning and scalable cooperation strategies" in embodied multi-agent settings, reinforcing that architecture-design choices impact cooperation
- Break condition: DeepSeek r1's reasoning behavior was "less sensitive to prompt engineering"—inherent model tendencies may resist external shaping.

## Foundational Learning

- Concept: **ReAct-style reasoning-action integration**
  - Why needed here: CoELA's Planning Module interprets world state and generates action sequences. Understanding how LLMs interleave reasoning with action selection is prerequisite to designing effective planning prompts.
  - Quick check question: Can you explain why asking an LLM to "think step-by-step" before selecting an action might improve multi-step task performance?

- Concept: **Episodic memory in multi-agent systems**
  - Why needed here: CoELA uses three memory levels (semantic, episodic, procedural). Episodic memory tracks prior interactions—essential for coherent multi-turn collaboration.
  - Quick check question: If Agent A requests help at turn 3, what memory component lets Agent B recall this request at turn 7?

- Concept: **Fuzzy matching in LLM action selection**
  - Why needed here: The Action prompt must ensure LLM outputs align with executable action formats. Vague outputs cause "fuzzy matches" that degrade efficiency.
  - Quick check question: Why might an LLM output "grab the cup" fail when the action space only contains `[pick_up(object_id)]`?

## Architecture Onboarding

- Component map:
CoELA Framework (5 modules per agent)
├── Perception → Environment state input
├── Memory → Semantic/Episodic/Procedural stores
├── Planning (LLM-powered) → Action sequence generation
├── Communication (LLM-powered) → Inter-agent messaging
└── Execution → Low-level action dispatch

+ Ollama Integration → Local LLM runtime (Llama3.1, DeepSeek, Mistral, Gemma3)
+ TTS + Chat GUI → Real-time dialogue visualization

- Critical path:
  1. Agent receives goal + environment state via Perception
  2. Memory module retrieves relevant context (episodic + semantic)
  3. Planning Module prompts LLM with Improved Base prompt → generates action plan
  4. Communication Module prompts LLM with CPrompt4 → generates coordination message
  5. Execution Module dispatches selected action
  6. Other agent receives message, cycles through its own modules

- Design tradeoffs:
  - Smaller models (Gemma3 4B) vs. larger (Llama 3.1 8B): Smaller models reduce compute but require careful prompt optimization
  - Multi-shot vs. one-shot examples: Multi-shot provides richer context but may introduce noise; one-shot balances guidance with simplicity
  - Instruction removal vs. structured reasoning: Instruction removal improves conciseness; structured reasoning may help complex tasks but increases verbosity

- Failure signatures:
  - High turn count with low action progress → agents stuck in negotiation loops; likely prompt too open-ended
  - Verbose outputs with internal reasoning → prompt constraints insufficient; try CPrompt1-style instruction removal
  - Inconsistent action formats → Action prompt needs one-shot example; add explicit format demonstration
  - Model ignores prompt constraints (esp. DeepSeek r1) → inherent model behavior may require different prompting paradigm or model swap

- First 3 experiments:
  1. **Baseline calibration**: Run all 4 models with base prompts across 10 episodes; establish step count and turn count baselines. Verify results match paper's Table 1-2 ranges.
  2. **Improved Base + CPrompt4 validation**: Apply optimized prompt combination to Gemma3 and Llama 3.1; confirm 18-22% efficiency improvement. Use t-test to validate significance.
  3. **Ablation test**: Run CPrompt1-only and CPrompt2-only conditions to isolate which component (instruction removal vs. one-shot example) drives CPrompt4's gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the optimized prompting strategies transfer to human-agent collaboration scenarios, where natural human communication patterns may differ from agent-agent dialogue?
- Basis in paper: [explicit] The authors state: "In future developments we plan to extend our evaluation beyond agent-agent collaboration to include human-agent interaction using similar prompting strategies."
- Why unresolved: All current experiments involved only AI agents (Alice and Bob) communicating with each other; human communication may be less structured, more ambiguous, or require different turn-taking conventions than the optimized agent dialogue patterns.
- What evidence would resolve it: Comparative experiments using the Improved Base + Cprompt4 configuration in mixed human-agent teams, measuring step counts, turn counts, and task success rates against agent-only baselines.

### Open Question 2
- Question: Can the prompting strategies that improved 2-agent collaboration scale effectively to ad-hoc teams of n≥3 agents with dynamic role assignment?
- Basis in paper: [explicit] The authors explicitly mention plans to extend to "ad-hoc team formations of n≥3 agents (e.g. 2 robots assisting 1 human)."
- Why unresolved: Communication complexity increases non-linearly with team size; prompts optimized for dyadic interaction may not handle multi-party coordination, role negotiation, or distributed task allocation effectively.
- What evidence would resolve it: Experiments with 3+ agent teams measuring whether the same prompt optimizations maintain efficiency gains, or if new coordination prompts are needed.

### Open Question 3
- Question: To what extent do these findings generalize to larger LLMs (beyond 8B parameters) and to physical robot deployment outside simulation environments?
- Basis in paper: [inferred] Models were "intentionally capped below 8B parameters to ensure that the entire system could run efficiently on a single GPU setup," and all testing occurred in VirtualHome and Habitat 3.0 simulations.
- Why unresolved: Smaller models may respond differently to prompt engineering than larger models with different emergent capabilities; sim-to-real transfer introduces perception noise, actuation constraints, and latency that may affect the observed prompt effectiveness.
- What evidence would resolve it: Ablation studies with larger models (e.g., Llama 70B) and real-world robot experiments using the same prompt configurations.

## Limitations
- Exact task definitions and environment configurations are not specified in the paper
- Statistical testing protocol details are minimal beyond mentioning t-tests for some comparisons
- DeepSeek r1 showed inherent resistance to prompt constraints, suggesting model-specific behavior limits generalizability

## Confidence
- **High confidence**: That prompt engineering can meaningfully improve multi-agent coordination efficiency
- **Medium confidence**: That instruction-focused prompts + one-shot examples is the optimal combination
- **Low confidence**: That these optimizations generalize to other embodied tasks or agent architectures

## Next Checks
1. **Ablation testing**: Run CPrompt1-only and CPrompt2-only conditions to isolate which component drives CPrompt4's gains, and test whether instruction removal or one-shot examples alone achieve comparable results.

2. **Model behavior analysis**: For DeepSeek r1 specifically, log and analyze instances where internal reasoning appears despite prompt constraints to understand whether this is inherent model behavior or prompt formulation issues.

3. **Generalization testing**: Apply the Improved Base + CPrompt4 combination to a different embodied task (not from the original 10 episodes) to assess whether efficiency gains transfer beyond the trained scenarios.