---
ver: rpa2
title: 'Mugi: Value Level Parallelism For Efficient LLMs'
arxiv_id: '2601.10823'
source_url: https://arxiv.org/abs/2601.10823
tags:
- figure
- nonlinear
- approximation
- gemm
- array
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Mugi: Value Level Parallelism For Efficient LLMs

## Quick Facts
- **arXiv ID:** 2601.10823
- **Source URL:** https://arxiv.org/abs/2601.10823
- **Reference count:** 40
- **Primary result:** 2.07× higher throughput than a 32nm systolic array for LLM inference

## Executive Summary
Mugi is a hardware architecture that leverages Value Level Parallelism (VLP) to accelerate both nonlinear operations (softmax, SiLU, GELU) and asymmetric GEMM (BF16-INT4) in LLM inference. By splitting values into Sign-Mantissa-Exponent and using temporal coding with LUTs, Mugi achieves significant energy and area savings while maintaining accuracy within 0.1 perplexity of baseline models. The design is optimized for small-batch, GQA-based LLM workloads.

## Method Summary
Mugi uses an event-based cycle-accurate simulator to evaluate its VLP architecture against systolic arrays. The evaluation involves profiling input distributions for nonlinear operations to determine LUT windows, applying value-centric approximation with mantissa rounding and temporal subscription, and reusing the PE array for both nonlinear and GEMM operations. RTL synthesis at 45nm (400MHz) provides area and energy estimates, with results validated on Llama 2 (7B, 13B, 70B GQA), Whisper, SwinV2, and ViViT models.

## Key Results
- 2.07× higher throughput than 32nm systolic array for LLM inference
- 668× lower energy for nonlinear operations (softmax, SiLU, GELU)
- 0.1 perplexity degradation compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
Value-level parallelism approximates nonlinear functions by splitting inputs into Sign-Mantissa-Exponent, rounding the mantissa, and using temporal coding to select pre-computed LUT values. This approach leverages clustered exponent distributions (LUT window) to focus accuracy on high-magnitude inputs.

### Mechanism 2
Mugi maps INT4 weights to array rows and BF16 inputs to columns, enabling efficient small-batch LLM inference. This transposed mapping aligns with WOQ quantization and GQA group sizes, achieving high utilization for batch sizes of 8.

### Mechanism 3
Reusing the PE array for both nonlinear operations and GEMM reduces area and embodied carbon without sacrificing throughput. The same hardware handles selection logic for nonlinear ops and accumulation for GEMM, eliminating the need for separate vector units.

## Foundational Learning

- **Concept: Temporal Coding (VLP)**
  - **Why needed:** Fundamental to Mugi's arithmetic - values are represented by cycle counts when signals spike
  - **Quick check:** If an input has a mantissa of 3, at which cycle does the Temporal Converter assert a spike?

- **Concept: Asymmetric Quantization (WOQ/KVQ)**
  - **Why needed:** Mugi optimizes for BF16 inputs and INT4 weights - understanding this precision split is crucial for grasping format customization
  - **Quick check:** Why does mapping INT4 weights to rows allow Mugi to support small-batch LLM inference better than mapping FP8 activations to rows?

- **Concept: Grouped Query Attention (GQA)**
  - **Why needed:** Mugi fixes its array width to 8 to match GQA group size - this alignment is critical for utilization gains
  - **Quick check:** How does the GQA group size determine the optimal array width in Mugi?

## Architecture Onboarding

- **Component map:** M-proc/E-proc -> TC -> iSRAM -> PE Array -> PP
- **Critical path:** Input -> M-proc (Round Mantissa) -> TC (Generate Spike) -> TC + iSRAM -> PE Array (Select LUT row via AND gate) -> PE Array -> PP (Select final value based on Exponent spike)
- **Design tradeoffs:**
  - LUT Window Size: Larger window covers more values (better accuracy) but increases iSRAM size and latency
  - Array Width: Fixed at 8, optimized for Batch=8/GQA, may bottleneck with larger batch sizes
- **Failure signatures:**
  - Perplexity Spike: Occurs if LUT window is not tuned to specific layer's input distribution
  - Under-utilization: Occurs if batch size is not a multiple of array width (8) during GEMM ops
- **First 3 experiments:**
  1. Accuracy Tuning: Run end-to-end inference on Llama 2 7B, sweep LUT window range to find per-layer perplexity sweet spot
  2. Throughput Scaling: Benchmark Mugi vs. Systolic Array at varying batch sizes (1, 8, 16) to validate small-batch efficiency
  3. Energy Profiling: Isolate nonlinear operation energy consumption to verify 668x improvement over precise vector arrays

## Open Questions the Paper Calls Out
None

## Limitations
- Mugi's efficiency depends on clustered exponent distributions in nonlinear inputs - may lose advantage if distributions become uniform
- Fixed 8-column array creates structural mismatch for large-batch inference or standard attention without GQA
- Array reuse for nonlinear and GEMM operations may create system-level bottlenecks under extreme fused workloads

## Confidence

- **High Confidence:** Mechanical description of VLP (Sign-Mantissa-Exponent decomposition, TC-based temporal coding, LUT-based selection) is clearly defined and implementable
- **Medium Confidence:** 2.07× throughput improvement based on simulation assuming optimal batch sizes (8) and GQA - real-world speedup depends on workload characteristics
- **Low Confidence:** Absolute perplexity values reported are sensitive to LUT window tuning - lack of generalizable heuristic for "sliding window" selection limits portability

## Next Checks

1. **Distribution Robustness Test:** Profile exponent distributions for Softmax and SiLU/GELU on a second, independently trained LLM (e.g., MPT or Falcon) to verify clustering assumption holds

2. **Array Width Scalability Test:** Modify simulation to benchmark Mugi with variable array widths (4, 8, 16, 32) under both GQA (Batch=8) and standard attention (Batch=1) conditions

3. **Fused Kernel Throughput Test:** Create synthetic benchmark interleaving heavy nonlinear computation with GEMM operations in fused kernel, measure Mugi's array reuse design vs. heterogeneous design with dedicated nonlinear accelerators