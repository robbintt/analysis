---
ver: rpa2
title: Advancing Safe Mechanical Ventilation Using Offline RL With Hybrid Actions
  and Clinically Aligned Rewards
arxiv_id: '2506.14375'
source_url: https://arxiv.org/abs/2506.14375
tags:
- action
- actions
- policy
- reward
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses optimizing mechanical ventilation (MV) settings
  in intensive care using offline reinforcement learning (RL). A key challenge is
  handling MV's hybrid (continuous and discrete) actions, where discretizing continuous
  settings causes exponential action space growth and reconstruction-induced distribution
  shifts.
---

# Advancing Safe Mechanical Ventilation Using Offline RL With Hybrid Actions and Clinically Aligned Rewards

## Quick Facts
- arXiv ID: 2506.14375
- Source URL: https://arxiv.org/abs/2506.14375
- Authors: Muhammad Hamza Yousuf; Jason Li; Sahar Vahdati; Raphael Theilen; Jakob Wittenstein; Jens Lehmann
- Reference count: 17
- This work addresses optimizing mechanical ventilation (MV) settings in intensive care using offline reinforcement learning (RL). A key challenge is handling MV's hybrid (continuous and discrete) actions, where discretizing continuous settings causes exponential action space growth and reconstruction-induced distribution shifts. To overcome this, the study develops constrained and factored action space methods for discrete settings and extends SOTA offline RL algorithms (IQL and EDAC) to natively handle hybrid actions. It also introduces a clinically aligned reward function based on ventilator-free days and physiological safety ranges, selected via multi-objective optimization. Experiments on large ICU datasets show HybridIQL achieves superior performance and policy coverage, avoids distribution shift, and generalizes well, while discrete methods benefit from the proposed optimizations. The approach advances safe, scalable AI-driven MV optimization for real-world deployment.

## Executive Summary
This paper tackles the challenge of optimizing mechanical ventilation settings in intensive care units using offline reinforcement learning. The key innovation is handling hybrid action spaces (continuous and discrete) natively, avoiding the pitfalls of discretization like exponential action space growth and distribution shifts. The authors introduce constrained and factored action space methods for discrete settings and extend leading offline RL algorithms (IQL and EDAC) to handle hybrid actions. They also develop a clinically aligned reward function based on ventilator-free days and physiological safety ranges, selected through multi-objective optimization. Experiments on large ICU datasets demonstrate that HybridIQL outperforms discrete action baselines, achieving superior performance, better policy coverage, and improved generalization.

## Method Summary
The study optimizes 6 mechanical ventilation settings (mode, respiratory rate, tidal volume, driving pressure, PEEP, FiO2) using offline RL on a combined dataset from MIMIC-IV, eICU, and HiRID (12,572 patients, 1.25M MV hours). The method handles hybrid actions by decomposing them into discrete and continuous components, applying constrained/factored action spaces for discrete settings, and extending IQL and EDAC algorithms to natively process hybrid actions. A clinically aligned reward function is introduced, combining ventilator-free days (VFD) with weighted indicators for physiological parameters in safe ranges (pH, MAP, PaO2, SaO2, PaCO2, HR, SpO2). The model is trained on 80% of the data and evaluated on 20%, using Fitted Q-Evaluation (V^π) and policy coverage (d_π) as primary metrics. HybridIQL is shown to outperform discrete action baselines while avoiding distribution shift.

## Key Results
- HybridIQL achieves superior performance and policy coverage compared to discrete action methods.
- The method avoids distribution shift by natively handling hybrid actions without discretization.
- Clinically aligned rewards based on VFD and physiological safety ranges improve policy performance.
- The approach generalizes well across diverse ICU datasets and patient populations.

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of hybrid action spaces in mechanical ventilation. Discretizing continuous settings leads to exponential action space growth and reconstruction-induced distribution shifts, which degrade performance. By natively handling hybrid actions, the method avoids these issues. The constrained and factored action space methods for discrete settings further optimize the action space, reducing computational complexity. The clinically aligned reward function, combining VFD and physiological safety ranges, ensures that the learned policy aligns with clinical goals and safety considerations. This combination of technical innovations and clinical alignment enables superior performance and generalization.

## Foundational Learning
- **Offline RL**: Learning from fixed datasets without environment interaction. Why needed: MV data is observational, and online exploration is unsafe. Quick check: Verify dataset contains diverse MV episodes with varying settings and outcomes.
- **Hybrid Actions**: Handling both continuous and discrete actions in RL. Why needed: MV settings include both continuous (e.g., tidal volume) and discrete (e.g., mode) parameters. Quick check: Confirm action space decomposition into discrete and continuous components.
- **Distribution Shift**: The mismatch between training and deployment data distributions. Why needed: Reconstruction from discretized actions can cause the policy to select OOD actions. Quick check: Compare action distributions between policy and behavior model.
- **Fitted Q-Evaluation (FQE)**: Offline policy evaluation method. Why needed: Allows evaluation without online deployment, crucial for safety. Quick check: Verify FQE estimates align with clinical outcomes.
- **Policy Coverage (d_π)**: Measures how much the learned policy covers the behavior policy. Why needed: Ensures the policy selects actions similar to clinicians. Quick check: Compare d_π between HybridIQL and discrete baselines.
- **Clinically Aligned Rewards**: Rewards based on clinical outcomes and safety. Why needed: Ensures the policy aligns with clinical goals. Quick check: Verify reward components reflect clinical priorities (VFD, physiological ranges).

## Architecture Onboarding
- **Component Map**: Data Preprocessing -> Reward Engineering -> HybridIQL/EDAC Training -> FQE Evaluation -> Policy Coverage Analysis
- **Critical Path**: Data Preprocessing -> HybridIQL Training -> FQE Evaluation
- **Design Tradeoffs**: Native hybrid action handling vs. discretization (complexity vs. performance), clinical reward alignment vs. simplicity, large dataset vs. computational cost
- **Failure Signatures**: Low policy coverage (d_π) indicating OOD action selection, poor FQE performance suggesting suboptimal policy, distribution shift in discrete action reconstructions
- **First Experiments**:
  1. Train HybridIQL on a subset of the data and evaluate FQE performance.
  2. Compare policy coverage (d_π) between HybridIQL and discrete action baselines.
  3. Analyze action distributions to detect potential distribution shift.

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that 1-hour timesteps sufficiently capture MV dynamics remains unverified.
- The clinical reward design's impact on policy performance is not fully explored through ablation studies.
- Generalization to patient subgroups not well-represented in the training data is not explicitly tested.

## Confidence
- **Primary Claim (HybridIQL's Superior Performance)**: Medium
- **Clinical Safety Claims**: Medium
- **Generalization Claims**: Medium

## Next Checks
1. Conduct ablation studies isolating the impact of constrained/factored action space optimization vs. native hybrid action handling in offline RL.
2. Validate physiological safety ranges by correlating reward-weighted parameters with documented adverse events in the dataset.
3. Perform out-of-distribution tests using patient subgroups not well-represented in the training data to assess generalization.