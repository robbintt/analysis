---
ver: rpa2
title: VLMs Can Aggregate Scattered Training Patches
arxiv_id: '2506.03614'
source_url: https://arxiv.org/abs/2506.03614
tags:
- visual
- stitching
- vlms
- patches
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether vision-language models (VLMs) can
  reconstruct visual content from scattered patches, enabling potential adversarial
  attacks. The authors introduce "visual stitching," the ability of VLMs to integrate
  visual information spread across multiple training samples sharing the same textual
  descriptions.
---

# VLMs Can Aggregate Scattered Training Patches

## Quick Facts
- arXiv ID: 2506.03614
- Source URL: https://arxiv.org/abs/2506.03614
- Authors: Zhanhui Zhou; Lingjie Chen; Chao Yang; Chaochao Lu
- Reference count: 40
- VLMs can integrate visual information spread across training samples sharing the same text descriptions

## Executive Summary
This study investigates whether vision-language models (VLMs) can reconstruct visual content from scattered patches, enabling potential adversarial attacks. The authors introduce "visual stitching," the ability of VLMs to integrate visual information spread across multiple training samples sharing the same textual descriptions. They conduct experiments on three synthetic datasets (food, animal, landmark) where images are split into patches and paired with unique IDs. After fine-tuning on these patch-text pairs, models demonstrate strong image-based stitching (low mean rank) even with small patches, and non-trivial reference-based stitching, though the latter is less reliable. They also show that small patches from harmful images often evade moderation filters, and fine-tuning on these can lead VLMs to generate misleading "safe" or "unsafe" labels for dangerous content. The results highlight visual stitching as both a generalization strength and a safety concern.

## Method Summary
The authors create three synthetic datasets (food, animal, landmark) with 20 images each, assigning unique synthetic IDs to each image. Images are split into patches at grid factors f ∈ {1, 2, 4, 8}. VLMs are fine-tuned on patch-ID pairs using a template format where the patch is paired with text indicating the associated ID. Two stitching tasks are evaluated: image-based (verbalize ID from full image) and reference-based (verbalize ID from text reference alone). Mean rank of the correct ID among 20 candidates serves as the primary metric. For safety experiments, patches from harmful images are filtered through moderation APIs, then used to fine-tune models to generate "safe" or "unsafe" labels, demonstrating potential manipulation of safety classifications.

## Key Results
- VLMs demonstrate strong image-based stitching with mean ranks well below random baseline (20) even for small patches (f=8)
- Reference-based stitching shows non-trivial performance but degrades significantly with smaller patches, approaching random at f=8
- Qwen2-VL-7B consistently outperforms other models in both stitching tasks
- Small patches from harmful content can evade moderation filters, and fine-tuning on these patches enables VLMs to generate misleading safety labels

## Why This Works (Mechanism)
Visual stitching works because VLMs can integrate distributed visual information across training samples sharing the same textual context. When patches from the same image share an ID, the model learns to associate these fragments with a common concept, enabling reconstruction from scattered inputs. The cross-attention mechanisms in VLMs allow them to relate textual descriptions to visual features, facilitating the aggregation of information across multiple training instances.

## Foundational Learning
- **Visual stitching**: The ability to integrate visual information spread across training samples sharing the same text description
  - Why needed: Core concept being investigated; explains how VLMs can reconstruct content from scattered patches
  - Quick check: Can the model correctly identify IDs from patch combinations that were never seen together during training?

- **Mean rank evaluation**: Ranking candidates by predicted probability and measuring the position of the correct answer
  - Why needed: Primary metric for assessing stitching capability across different patch sizes and models
  - Quick check: Is the mean rank significantly below the random baseline of 20 for the given dataset size?

- **Patch-based adversarial attacks**: Using fragmented harmful content to evade moderation filters and manipulate model outputs
  - Why needed: Demonstrates practical safety implications of visual stitching capabilities
  - Quick check: Do small patches from harmful images consistently bypass moderation APIs while retaining harmful information?

## Architecture Onboarding

**Component map**: Input patches → VLM encoder → Cross-attention mechanism → Textual output (ID prediction)

**Critical path**: Patch encoding → cross-modal attention → text generation → candidate ranking

**Design tradeoffs**: The study uses synthetic datasets for controlled experimentation, sacrificing ecological validity for experimental control. Fine-tuning on patch-ID pairs enables stitching but may not reflect natural VLM training.

**Failure signatures**: High mean ranks indicate stitching failure; reference-based stitching fails more dramatically than image-based, especially with smaller patches. Performance drops when positional information is added to patches.

**Exactly 3 first experiments**:
1. Test Qwen2-VL-7B on image-based stitching with f=2 patches to establish baseline performance
2. Evaluate reference-based stitching with f=4 patches to assess cross-patch integration capability
3. Measure moderation filter evasion rate for patches from harmful images at different sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on synthetic datasets with controlled patch fragmentation, limiting generalizability to natural images
- The safety experiments demonstrate controlled conditions but don't fully characterize real-world attack scenarios or adversary capabilities
- The study doesn't establish whether visual stitching represents a novel capability beyond existing VLM generalization patterns

## Confidence
- **High confidence**: VLMs exhibit image-based stitching (integrating patch information when full image is visible)
- **Medium confidence**: VLMs demonstrate non-trivial reference-based stitching (inferring IDs from textual descriptions alone)
- **Medium confidence**: Safety implications are demonstrated under controlled conditions but depend on real-world deployment scenarios

## Next Checks
1. Test visual stitching capabilities on naturally occurring images where patches from different images of the same class are mixed
2. Evaluate whether defensive fine-tuning on complete images can mitigate the visual stitching attack surface
3. Verify whether reference-based stitching results generalize to VLMs not explicitly trained on image-text pairs