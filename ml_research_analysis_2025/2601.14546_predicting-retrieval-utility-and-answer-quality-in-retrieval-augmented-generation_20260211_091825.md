---
ver: rpa2
title: Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation
arxiv_id: '2601.14546'
source_url: https://arxiv.org/abs/2601.14546
tags:
- context
- answer
- quality
- retrieval
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two novel prediction tasks for retrieval-augmented
  generation (RAG): retrieval performance prediction (RPP) and generation performance
  prediction (GPP). RPP estimates the utility of retrieved documents as RAG context,
  while GPP directly predicts the quality of generated answers.'
---

# Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2601.14546
- Source URL: https://arxiv.org/abs/2601.14546
- Authors: Fangzheng Tian; Debasis Ganguly; Craig Macdonald
- Reference count: 40
- Key outcome: The paper proposes two novel prediction tasks for RAG - retrieval performance prediction (RPP) and generation performance prediction (GPP) - and demonstrates that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.

## Executive Summary
This paper introduces two novel prediction tasks for retrieval-augmented generation (RAG): retrieval performance prediction (RPP) and generation performance prediction (GPP). The authors propose adapting query performance prediction (QPP) techniques to estimate both the utility of retrieved documents as RAG context and the quality of generated answers. They develop a framework that combines reader-centric predictors (like perplexity), query-agnostic predictors (document quality/readability), and QPP-inspired features using linear regression models. Experiments on the Natural Questions dataset demonstrate that combining predictors from multiple categories consistently outperforms single-category approaches, with context perplexity being particularly effective for answer quality prediction.

## Method Summary
The authors formulate two prediction tasks: RPP estimates the utility of retrieved documents as RAG context, while GPP directly predicts answer quality. They hypothesize that QPP approaches can be adapted for these tasks and supplement them with reader-centric (perplexity) and query-agnostic (document quality/readability) predictors. The framework trains linear regression models combining multiple predictor categories. Pre-generation predictors estimate context utility before LLM processing, while post-generation predictors analyze the LLM's output. The study uses the Natural Questions dataset with a quantized Llama-3-8B model and evaluates predictor effectiveness through correlation with actual performance metrics.

## Key Results
- Combining predictors from multiple feature categories (QPP, reader-centric, query-agnostic) yields the most accurate estimates of RAG performance
- Adding context perplexity consistently improves prediction accuracy for answer quality
- Document quality metrics provide modest additional gains when combined with other predictors
- Post-generation answer perplexity further improves both RPP and GPP accuracy compared to pre-generation predictors alone
- Pre-generation predictor accuracy declines as context size increases from 2 to 10 documents

## Why This Works (Mechanism)
The effectiveness stems from leveraging multiple information sources about document and context quality. QPP-inspired features capture retrieval-specific signals, reader-centric predictors like perplexity measure how well the context aligns with language model expectations, and query-agnostic metrics assess inherent document quality. By combining these complementary perspectives through linear regression, the model captures both retrieval effectiveness and generation readiness. The post-generation approach benefits from having the actual answer, allowing direct assessment of generation quality through perplexity.

## Foundational Learning

Query Performance Prediction (QPP)
- Why needed: QPP provides established techniques for estimating retrieval effectiveness without ground truth judgments
- Quick check: QPP methods correlate retrieval scores with expected relevance, enabling quality estimation before expensive annotation

Reader-centric predictors
- Why needed: These metrics assess how well documents align with language model expectations and generation patterns
- Quick check: Perplexity measures the uncertainty of a language model when processing text, indicating context quality

Query-agnostic document quality
- Why needed: Document quality metrics provide stable signals about information value independent of specific queries
- Quick check: Readability scores and other quality metrics capture document characteristics that affect generation performance

Linear regression ensemble
- Why needed: Simple linear models can effectively combine diverse predictor types while maintaining interpretability
- Quick check: Coefficient analysis reveals which predictor categories contribute most to accurate performance estimation

## Architecture Onboarding

Component map:
Document Retriever -> Context Builder -> Linear Regression Predictor -> Performance Estimate
Natural Questions Dataset -> Query Analyzer -> Feature Extractor -> Predictor Training
Language Model (Llama-3-8B) -> Answer Generator -> Post-generation Analysis

Critical path:
Query -> Document Retrieval -> Context Assembly -> Predictor Feature Extraction -> Performance Prediction -> Answer Generation (for post-generation analysis)

Design tradeoffs:
- Pre-generation vs. post-generation prediction: Pre-generation enables early optimization but is less accurate; post-generation is more accurate but requires full generation
- Single vs. multiple predictor categories: Multiple categories improve accuracy but increase computational overhead
- Linear vs. non-linear modeling: Linear regression offers interpretability but may miss complex interactions

Failure signatures:
- Poor correlation with actual performance indicates missing critical predictor categories
- Degradation with increasing context size suggests feature extraction struggles with long contexts
- Inconsistent performance across query types indicates dataset-specific overfitting

First experiments:
1. Evaluate single predictor category performance to establish baseline correlations
2. Test linear regression combinations of two predictor categories to identify most complementary pairs
3. Compare pre-generation and post-generation predictor accuracy on held-out test queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are the proposed RPP and GPP predictors across a broader range of datasets, tasks, and LLM architectures?
- Basis in paper: The conclusion states that "evaluating RPP and GPP across a broader range of datasets and LLMs is important for future work, as context utility may vary with task and model choice."
- Why unresolved: The current study limits its experimental scope to the Natural Questions dataset and a specific quantized Llama-3-8B model.
- What evidence would resolve it: Reporting consistent prediction accuracy (Spearman's $\rho$) across diverse benchmarks (e.g., MS MARCO, HotpotQA) and different model families (e.g., GPT-4, Mistral).

### Open Question 2
- Question: How can these estimators be integrated into adaptive RAG workflows to optimize retrieval or generation parameters per query?
- Basis in paper: The introduction explicitly excludes the application of these predictions, leaving "the downstream integration of these estimators into adaptive RAG systems for future research."
- Why unresolved: The paper focuses on defining the prediction tasks and validating feature correlations but does not implement a feedback loop to adjust the RAG pipeline.
- What evidence would resolve it: A study demonstrating that using RPP/GPP scores to dynamically select $k$ (number of documents) or switch retrieval models improves final answer quality or efficiency.

### Open Question 3
- Question: Can pre-generation prediction accuracy be stabilized as the number of retrieved documents (context size) increases?
- Basis in paper: The results (Figure 3) show a consistent decline in pre-generation predictor accuracy as context size $k$ increases from 2 to 10, attributed to the difficulty of analyzing long concatenated contexts.
- Why unresolved: While ensembling mitigates the issue slightly, the paper does not propose a method to counteract the degradation caused by context length.
- What evidence would resolve it: A pre-generation approach that maintains high correlation with utility targets even as $k$ exceeds 10 documents.

## Limitations
- Experimental scope limited to Natural Questions dataset, raising questions about performance across different query types and domains
- Focus on specific RAG architecture (FiD) limits applicability to other generation models
- Linear regression approach assumes additive effects of predictors, potentially missing complex interactions between features

## Confidence

**High confidence**: The empirical finding that combining predictors from multiple categories improves prediction accuracy compared to single-category approaches

**Medium confidence**: The relative effectiveness of specific predictor types (context perplexity vs. document quality metrics)

**Low confidence**: The generalizability of predictor effectiveness across different RAG architectures and domains

## Next Checks

1. Evaluate predictor combinations on diverse datasets beyond Natural Questions, including open-domain and domain-specific queries
2. Test the proposed predictor framework with different RAG architectures (e.g., reranker-based, generative models beyond FiD)
3. Investigate non-linear modeling approaches to capture potential interactions between predictor categories and improve prediction accuracy