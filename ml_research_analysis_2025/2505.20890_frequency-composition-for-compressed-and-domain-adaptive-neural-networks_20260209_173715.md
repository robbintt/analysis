---
ver: rpa2
title: Frequency Composition for Compressed and Domain-Adaptive Neural Networks
arxiv_id: '2505.20890'
source_url: https://arxiv.org/abs/2505.20890
tags:
- coda
- domain
- tent
- training
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoDA tackles the dual challenge of model compression and domain
  adaptation for on-device neural networks. It introduces a frequency composition
  framework that combines low-frequency component quantization-aware training (LFC
  QAT) with frequency-aware batch normalization (FABN) for test-time adaptation (TTA).
---

# Frequency Composition for Compressed and Domain-Adaptive Neural Networks

## Quick Facts
- **arXiv ID**: 2505.20890
- **Source URL**: https://arxiv.org/abs/2505.20890
- **Reference count**: 40
- **Primary result**: CoDA achieves up to 7.96%p and 5.37%p accuracy improvements over full-precision TTA baselines while reducing model size by 4-16×

## Executive Summary
CoDA tackles the dual challenge of model compression and domain adaptation for on-device neural networks. It introduces a frequency composition framework that combines low-frequency component quantization-aware training (LFC QAT) with frequency-aware batch normalization (FABN) for test-time adaptation (TTA). During training, CoDA uses only low-frequency components to learn generalizable features, while at test time it adapts using full-frequency data, treating low and high frequencies differently. This allows the model to maintain robustness under domain shifts while remaining compact.

## Method Summary
CoDA trains quantized models exclusively on low-frequency image components, then adapts them at test time using a frequency-aware batch normalization scheme. The training phase applies 2D Fourier transform to input images, filters out high-frequency components using a radius-based low-pass filter, and trains the quantized model (LSQ/LQ) on the reconstructed low-frequency images. At test time, the model processes full-frequency data but decomposes each batch normalization layer's input features into low-frequency and high-frequency components. The low-frequency statistics are initialized from the source domain and updated gradually via exponential moving average, while high-frequency statistics are computed from the current batch. This frequency-aware treatment enables effective adaptation while preserving source-domain knowledge.

## Key Results
- Up to 7.96%p accuracy improvement on CIFAR10-C over full-precision TTA baselines
- Up to 5.37%p accuracy improvement on ImageNet-C with 4-16× model size reduction
- Synergy between LFC QAT and FABN exceeds individual component gains (15.8%p combined vs. 8.42%p sum of parts on ImageNet-C)
- Maintains performance with batch sizes as small as 32, where standard TTA methods degrade

## Why This Works (Mechanism)

### Mechanism 1
Training quantized models exclusively on low-frequency components produces more domain-invariant representations than full-frequency training, improving generalization under distribution shift. The 2D Fourier transform decomposes images into low-frequency components (LFC) capturing slowly varying patterns and high-frequency components (HFC) capturing rapid transitions. By applying quantization-aware training only to LFC-reconstructed images, the compressed model learns features that transfer better across domains because LFC exhibits smaller inter-domain distances than HFC. Core assumption: Labels generated by humans primarily reflect LFC information, making LFC more semantically aligned with classification tasks.

### Mechanism 2
Frequency-aware batch normalization (FABN) enables effective test-time adaptation by treating LFC and HFC statistics independently, preserving source-domain knowledge while incorporating target-domain details. At test time, FABN decomposes each BN layer's input feature into flfc and fhfc. For LFC, it initializes statistics from source training and updates via exponential moving average. For HFC, which was never learned during training, it uses only current batch statistics. The final normalization combines both: μt = μlfc,t + μhfc,t, σt² = σlfc,t² + σhfc,t². Core assumption: LFC statistics are stable and require gradual adaptation; HFC statistics are domain-specific and can be estimated from single batches.

### Mechanism 3
The synergy between LFC QAT and FABN produces greater robustness gains than either component alone because LFC-trained models have flatter loss landscapes that require less adaptation. LFC QAT produces a quantized model with smoother, more convex loss landscapes. This malleable state means BN statistics are closer to target-domain optima at initialization. FABN's gradual LFC adaptation then requires smaller adjustments, while HFC statistics are computed from scratch without conflicting with stored knowledge. Core assumption: The regularization effect of quantization complements frequency filtering—both reduce model capacity to fit domain-specific noise.

## Foundational Learning

- Concept: **2D Discrete Fourier Transform and Frequency Filtering**
  - Why needed here: CoDA uses FFT to decompose images into frequency components. Understanding how low-pass and high-pass filtering affects image content is essential for configuring the radius parameter r.
  - Quick check question: Given a 32×32 image, what frequency components would a low-pass filter with radius r=8 preserve versus r=4?

- Concept: **Batch Normalization Statistics (Running Mean/Variance vs. Batch Statistics)**
  - Why needed here: FABN's core innovation is treating LFC and HFC BN statistics differently. Understanding standard BN operation (training vs. inference modes) is prerequisite.
  - Quick check question: In standard BN, what statistics are used during training versus inference? How does NORM-style TTA modify this?

- Concept: **Quantization-Aware Training (QAT) vs. Post-Training Quantization (PTQ)**
  - Why needed here: CoDA builds on QAT methods (LSQ, LQ). Understanding how quantization parameters (step size) are learned during training helps explain why LFC QAT produces different representations.
  - Quick check question: How does QAT differ from PTQ in terms of when quantization parameters are determined? What regularization effect does the paper attribute to QAT?

## Architecture Onboarding

- Component map:
  - **Training Pipeline**: Input → FFT → Low-pass filter (radius r) → Inverse FFT → LFC image → Standard QAT (LSQ/LQ) → Quantized model with LFC-trained BN statistics
  - **Test Pipeline**: Input batch → Forward pass through model → At each BN layer: FFT on features → Split into flfc/fhfc → FABN normalization → Combine → Continue forward pass
  - **FABN Module**: Maintains EMA state for LFC (μlfc,t, σlfc,t), computes batch stats for HFC (μhfc,t, σhfc,t), combines for final normalization

- Critical path:
  1. Choose radius r based on input image size (paper uses r=8 for CIFAR10, r=56 for ImageNet—proportional to input dimension)
  2. Train quantized model on LFC-only data using any QAT method
  3. At deployment, apply FABN: initialize LFC stats from source, compute HFC stats per batch
  4. Set EMA coefficient α (controls LFC adaptation speed; paper uses α=0.1)

- Design tradeoffs:
  - **Smaller radius r**: More aggressive LFC filtering → better domain generalization but may lose semantic information needed for complex tasks. Paper shows r=4 on CIFAR10 degrades performance vs. r=8.
  - **Higher α**: Faster LFC adaptation → better for rapid domain shifts but risk overwriting source knowledge
  - **Batch size**: FABN's HFC statistics rely on batch estimates; small batches (<32) may produce noisy HFC normalization

- Failure signatures:
  - **Accuracy collapse on clean test data**: Radius r too aggressive, discarding task-relevant LFC information
  - **No improvement from FABN**: Batch size too small for reliable HFC statistics, or target domain shift is primarily in LFC (not HFC)
  - **Degradation under continual domain shifts**: EMA accumulating errors; consider resetting LFC statistics periodically

- First 3 experiments:
  1. **Reproduce Table 1 baseline**: Train ResNet-26 on CIFAR10 with LSQ (2-bit) using full-frequency vs. LFC (r=8). Verify that LFC QAT maintains comparable clean accuracy (target: ~80.82% vs. 86.11% FP baseline).
  2. **Validate FABN ablation**: Take the LFC-trained model and test on CIFAR10-C with (a) standard BN, (b) FABN-LFC only, (c) full FABN. Confirm synergy effect from Table 9.
  3. **Batch size sensitivity test**: Run FABN on CIFAR10-C with batch sizes [16, 32, 64, 128]. Compare against NORM/TENT baselines. Expect CoDA to maintain performance at smaller batch sizes per Figure 6.

## Open Questions the Paper Calls Out

### Open Question 1
Can the frequency filtering radius $r$ be optimized adaptively during training rather than being fixed as a heuristic hyperparameter? The paper establishes efficacy of specific fixed values but does not explore if the optimal frequency cutoff evolves as the model trains or varies across different layers.

### Open Question 2
Does the CoDA framework transfer effectively to Vision Transformers (ViTs) or other non-convolutional architectures? The evaluation exclusively lists CNN architectures, leaving the interaction between low-frequency quantization and self-attention mechanisms unexplored.

### Open Question 3
What is the theoretical link between quantization noise and frequency decomposition that causes LFC QAT to yield flatter loss landscapes? The paper empirically demonstrates LFC QAT results in flatter loss landscapes but does not derive a theoretical explanation for why removing HFC specifically regularizes the quantized parameter space so effectively.

## Limitations
- Assumes domain shifts primarily affect high-frequency components, but provides limited empirical validation across diverse shift types
- FABN's HFC statistics rely on batch estimates, which may become unreliable with very small batch sizes or severely corrupted data
- Choice of radius r is heuristic (proportional to input dimension) without theoretical justification for the scaling relationship

## Confidence
- High confidence: LFC QAT produces domain-invariant representations (strong empirical support from CIFAR10-C results showing 11-15%p improvements)
- Medium confidence: FABN's separate treatment of LFC/HFC statistics is optimal (supported by SSE analysis but lacks ablation studies on alternative strategies)
- Medium confidence: Synergy between LFC QAT and FABN exceeds individual component gains (statistically significant in ImageNet-C but mechanism is somewhat hand-wavy)

## Next Checks
1. Test CoDA's performance when target domain shifts primarily affect low-frequency structure (e.g., systematically vary DC component, apply global brightness/contrast changes)
2. Evaluate FABN sensitivity to batch size by testing with [16, 32, 64, 128] on ImageNet-C and comparing against NORM/TENT baselines
3. Conduct ablation study comparing FABN's EMA strategy vs. alternative approaches (e.g., no EMA for LFC, different initialization strategies)