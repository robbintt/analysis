---
ver: rpa2
title: On the Expressive Power of Floating-Point Transformers
arxiv_id: '2601.16450'
source_url: https://arxiv.org/abs/2601.16450
tags:
- floating-point
- theorem
- then
- such
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressive power of floating-point transformers
  under IEEE 754 arithmetic, where real parameters and exact operations are replaced
  by finite floating-point numbers and inexact machine operations with round-off errors.
  The authors prove that floating-point transformers can represent a class of non-permutation-equivariant
  functions without positional encoding by exploiting the non-associativity of floating-point
  addition.
---

# On the Expressive Power of Floating-Point Transformers

## Quick Facts
- **arXiv ID:** 2601.16450
- **Source URL:** https://arxiv.org/abs/2601.16450
- **Reference count:** 40
- **Key outcome:** Floating-point transformers can represent non-permutation-equivariant functions without positional encoding by exploiting floating-point non-associativity, but cannot represent all permutation-equivariant functions for large sequences due to summation convergence.

## Executive Summary
This paper establishes fundamental differences between floating-point and real transformers by analyzing the expressive power of transformers under IEEE 754 arithmetic. The authors prove that floating-point transformers can exploit the non-associativity of floating-point addition to recover token order information without positional encoding, but face inherent limitations in representing permutation-equivariant functions when sequence lengths exceed precision-dependent thresholds. The work identifies a minimal equivariance structure (πn(1,2)-equivariance) that all floating-point transformers must possess due to left-associative softmax computation, and demonstrates that standard additive positional encodings can harm representability due to non-injectivity.

## Method Summary
The authors analyze floating-point transformers by replacing real arithmetic operations with IEEE 754 floating-point operations throughout the architecture. They define a mathematical framework for correctly rounded floating-point computation and establish conditions on precision parameters. The analysis leverages properties of floating-point addition (non-associativity, convergence under repetition) and carefully examines how these properties affect the internal computations of self-attention layers and feed-forward networks. The work proves several representability theorems by constructing specific functions that can or cannot be represented under floating-point constraints, and characterizes the minimal equivariance structure that arises from the implementation of softmax.

## Key Results
- Floating-point transformers can represent non-permutation-equivariant functions without positional encoding by exploiting floating-point non-associativity
- Universal approximation of permutation-equivariant functions is guaranteed only for sequences with n ≤ 6·2^p - 2 tokens
- All floating-point transformers are π_n(1,2)-equivariant due to left-associative softmax computation
- Additive positional encodings are non-injective and can harm representability compared to concatenative encodings

## Why This Works (Mechanism)

### Mechanism 1: Non-associativity of Floating-Point Addition Enables Order Sensitivity
Floating-point transformers exploit the non-associativity of floating-point addition to break permutation equivariance. In standard self-attention, the aggregation V⊗σ(K^T⊗Q) uses left-associative floating-point summation. Different summation orders (caused by token permutations) produce different results because floating-point addition is not associative. This allows the attention layer to identify token ordering without positional encoding, but only when input tokens are distinct. The mechanism requires standard IEEE 754 precision (2 ≤ p ≤ 2^(q-1) - 3) and distinct inputs.

### Mechanism 2: Convergence of Repeated Floating-Point Addition Limits Representability
Repeated floating-point addition of identical or similar values converges to a fixed value due to rounding errors. This saturation effect means that certain input patterns differing only in token counts beyond a threshold produce identical aggregated values in the attention mechanism. The paper establishes that for sequence lengths n ≥ 9·2^p, floating-point transformers cannot represent all permutation-equivariant functions. This impossibility only applies for long sequences; for n ≤ 6·2^p - 2, universal approximation is guaranteed.

### Mechanism 3: Minimal Equivariance Structure from Left-Associative Softmax
All floating-point transformers are guaranteed to be π_n(1,2)-equivariant, meaning they preserve the order when only swapping the first two tokens. This irreducible equivariance constraint arises from left-associative floating-point summation in softmax and matrix operations. While summation order generally matters, swapping the first two operands in a left-associative sum does not change the order of subsequent additions, creating this specific minimal equivariance that cannot be eliminated without changing the arithmetic model.

## Foundational Learning

- **IEEE 754 Floating-Point Arithmetic**: The analysis depends on specific floating-point properties: mantissa (p), exponent (q), rounding modes, and non-associativity of operations. These explain why real-arithmetic theoretical results fail. Quick check: In IEEE 754 double-precision (p=52, q=11), is (a⊕b)⊕c guaranteed to equal a⊕(b⊕c)?

- **Permutation Equivariance**: This is the core symmetry property of standard transformers that this paper proves is broken. Understanding f(πX) = πf(X) is essential to appreciate the significance of floating-point transformers being non-S_n-equivariant but π_n(1,2)-equivariant. Quick check: If a function f takes [x₁, x₂, x₃] and returns [x₂, x₁, x₃], is it permutation equivariant?

- **Transformer Architecture (Self-Attention and MLP)**: The proofs manipulate internal computations of attention (softmax, key/query/value matrices) and feed-forward networks. Basic understanding of sequence processing is required to follow the representability constructions. Quick check: In a standard self-attention layer, what is the primary role of the softmax function applied to the scaled dot-product of keys and queries?

## Architecture Onboarding

- **Component map**: Floating-Point Parameters (W) -> Floating-Point Operations (⊕, ⊗, σ) -> Attention Layer (AT) -> Feed-Forward Network (FF) -> Output

- **Critical path**: (1) Grasp floating-point operation definitions and non-associativity. (2) Understand how this within attention aggregation leads to π_n(1,2)-equivariance but breaks general S_n-equivariance. (3) Understand how repeated summation convergence limits representability for long sequences.

- **Design tradeoffs**:
    - Precision vs. Sequence Length: Universal approximation guaranteed only for n ≤ 6·2^p - 2. Lower precision (FP8) drastically restricts maximum sequence length vs. higher precision (FP32, BF16).
    - Positional Encoding: Standard additive encoding (X + [p₁, ..., pₙ]) is non-injective and harms representability. Concatenative encoding ([X^T, p]^T) avoids information loss.

- **Failure signatures**:
    - Equivariance Failure: Output changing for non-adjacent token permutations (with distinct elements) is expected behavior, not a bug.
    - Long-Sequence Saturation: For very long sequences with repetitive patterns, model becomes insensitive to small token count changes.
    - Positional Encoding Artifacts: Additive encodings may fail to distinguish certain inputs since x⊕z is non-injective.

- **First 3 experiments**:
    1. Verify π_n(1,2)-equivariance: Train a small floating-point transformer; measure output difference for original vs. first-two-token-swapped input.
    2. Probe Representability Thresholds: For simulated low-precision float, train transformers to fit a simple permutation-equivariant function (e.g., count specific token). Vary sequence length n across theoretical threshold (6·2^p).
    3. Compare Positional Encodings: Compare additive vs. concatenative positional encodings on a task requiring token distinction.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical thresholds (6·2^p and 9·2^p) may shift with different implementation details or hardware optimizations
- Analysis focuses on single layers rather than deep architectures with residual connections
- Results depend on idealized IEEE 754 behavior without accounting for hardware-specific rounding patterns

## Confidence

**High Confidence:**
- Floating-point transformers are guaranteed to be π_n(1,2)-equivariant (Theorem 4)
- Floating-point transformers cannot represent all permutation-equivariant functions for large n (Theorem 2)
- Additive positional encodings are non-injective and can harm representability (Proposition 1)

**Medium Confidence:**
- Floating-point transformers can represent non-permutation-equivariant functions without positional encoding (Theorem 1)
- The specific thresholds (6·2^p and 9·2^p) precisely delimit the universal approximation boundary
- The identified minimal equivariance structure is fundamental and irreducible

**Low Confidence:**
- Practical implications for standard transformer architectures (typically FP32/BF16)
- Performance degradation in real-world scenarios due to these theoretical limitations
- Specific recommendations for positional encoding design in floating-point transformers

## Next Checks

1. **Hardware-Realistic Implementation Study**: Implement the theoretical framework using actual IEEE 754 floating-point hardware with varying precisions (FP8, BF16, FP32). Measure whether observed behavior matches theoretical predictions for the π_n(1,2)-equivariance and convergence thresholds.

2. **Deep Architecture Validation**: Construct deep floating-point transformer models and systematically test whether the theoretical limitations identified for single layers persist or amplify. Specifically, examine if residual connections and normalization layers mitigate or exacerbate the representability constraints.

3. **Input Distribution Sensitivity Analysis**: Conduct controlled experiments varying input token distinctiveness across the theoretical domain. Quantify the relationship between input similarity, sequence length, and the model's ability to recover order information without positional encoding, particularly near the theoretical thresholds.