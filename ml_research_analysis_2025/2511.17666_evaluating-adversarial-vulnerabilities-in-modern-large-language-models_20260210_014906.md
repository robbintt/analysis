---
ver: rpa2
title: Evaluating Adversarial Vulnerabilities in Modern Large Language Models
arxiv_id: '2511.17666'
source_url: https://arxiv.org/abs/2511.17666
tags:
- safety
- attack
- adversarial
- vulnerabilities
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the security of Google's Gemini 2.5 Flash and
  OpenAI's GPT-4o mini under adversarial jailbreak attempts using self-generated and
  cross-model attack prompts. Four attack types (direct injection, role-playing, context
  manipulation, obfuscation) were applied across five harmful content categories,
  with results measured by a severity score (1-5).
---

# Evaluating Adversarial Vulnerabilities in Modern Large Language Models

## Quick Facts
- arXiv ID: 2511.17666
- Source URL: https://arxiv.org/abs/2511.17666
- Authors: Tom Perel
- Reference count: 8
- Primary result: GPT-4o mini showed weaker safety resilience (average severity 2.16) than Gemini 2.5 Flash (1.96) under adversarial jailbreak attempts

## Executive Summary
This study compared the security of Google's Gemini 2.5 Flash and OpenAI's GPT-4o mini under adversarial jailbreak attempts. The research employed self-generated and cross-model attack prompts across four attack types (direct injection, role-playing, context manipulation, obfuscation) and five harmful content categories. Results were measured by a severity score (1-5), revealing that GPT-4o mini had a higher average severity score than Gemini, indicating weaker safety resilience. Context manipulation and role-playing attacks proved most effective, while Gemini demonstrated perfect resistance to obfuscation attacks.

## Method Summary
The study conducted 320 total attack trials (160 per model) using automated adversarial prompt generation. Four attack vectors were tested across five harmful content categories with two bypass methods (self/cross), resulting in 20 trials per condition per model. System prompts instructed models to generate adversarial prompts targeting themselves or the other model. Generated prompts were executed against target models with temperature set to 0.7, and outputs were scored using a severity rubric ranging from 0 (refusal) to 5 (total jailbreak).

## Key Results
- GPT-4o mini had higher average severity score (2.16) than Gemini (1.96)
- Context manipulation and role-playing were most effective attack types
- Gemini showed perfect resistance to obfuscation attacks
- Self-bypass and cross-bypass attacks had equal overall success (35.6%), but self-bypass was more effective for semantic attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contextual framing (Context Manipulation) bypasses safety filters by exploiting the "Use-Mention" distinction.
- **Mechanism:** The attack embeds a harmful request within a benign narrative frame (e.g., "write a scene for a novel"). This dilutes the activation of safety triggers, causing the model to prioritize the "helpful writer" persona over the "safety" objective. The model processes the harmful content as a mention rather than an instruction to use.
- **Core assumption:** Safety alignment training (RLHF) relies heavily on trigger keywords or direct intent recognition, which can be confused by complex narrative structures.
- **Evidence anchors:**
  - [abstract]: "Context manipulation and role-playing were most effective..."
  - [section 2.1.3 & 5.1]: Defines the "Use-Mention" distinction and notes that "safety trigger is diluted by the surrounding benign text."
  - [corpus]: *Dark LLMs* and *BitBypass* corroborate that sophisticated obfuscation and context shifting exploit fundamental alignment gaps.
- **Break condition:** Models implement context-aware intent recognition that weights potential harm regardless of narrative wrapper.

### Mechanism 2
- **Claim:** Models generate more effective adversarial prompts against themselves (Self-Bypass) than external models do for semantic attacks.
- **Mechanism:** A model possesses an implicit mapping of its own linguistic boundaries and alignment flaws. When prompted to "red-team" itself, it leverages this internal knowledge to craft narratives (Role-Playing, Context Manipulation) that precisely bypass its specific reinforcement weights.
- **Core assumption:** The model's generative capability includes access to or understanding of its own refusal boundaries (dark knowledge of alignment).
- **Evidence anchors:**
  - [abstract]: "Self-bypass was more effective for semantic attacks."
  - [section 4.3]: "Self-Bypass proved superior for semantic attacks... suggests that a model 'knows' its own linguistic nuances best."
  - [corpus]: *AutoAdv* supports the viability of automated adversarial prompting frameworks.
- **Break condition:** Safety training explicitly masks or decouples the model's generative access from its internal refusal boundary distributions.

### Mechanism 3
- **Claim:** Disparity in Obfuscation resilience is caused by differences in preprocessing and text normalization layers.
- **Mechanism:** Gemini 2.5 Flash resists obfuscation (e.g., Base64, Leetspeak) because inputs likely pass through a normalization layer that translates or "cleans" the input before it reaches the reasoning engine. GPT-4o mini's vulnerability suggests a more direct token-to-generation mapping that preserves adversarial noise.
- **Core assumption:** Defensive layers exist prior to the main transformer inference, acting as a sanitizer for encoding-based attacks.
- **Evidence anchors:**
  - [section 4.2]: "Gemini 2.5 Flash demonstrated remarkable resilience... failing to fall for a single obfuscation attack."
  - [section 5.2]: "It is likely that Gemini... employs a more aggressive text normalization layer... neutralizing Base64 or Leetspeak attacks."
  - [corpus]: *BitBypass* discusses "Bitstream Camouflage," suggesting encoding attacks remain a vector for models without robust normalization.
- **Break condition:** Target model employs multi-modal or encoding-agnostic input processing that neutralizes syntax manipulation.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** RLHF is the primary method for safety alignment discussed in the paper. Understanding that safety is a dynamic trade-off between "helpful" and "harmless" is required to grasp why jailbreaking works.
  - **Quick check question:** How does the "Use-Mention" distinction confuse a model trained primarily to be helpful?

- **Concept: Adversarial Taxonomy (Prompt Injection)**
  - **Why needed here:** The paper classifies attacks into Direct, Role-Playing, Context Manipulation, and Obfuscation. Distinguishing between semantic attacks (Role/Context) and syntax attacks (Obfuscation) is critical for interpreting the results.
  - **Quick check question:** Why is Direct Injection considered a "control" group rather than a viable attack vector in modern evaluations?

- **Concept: Tokenization & Normalization**
  - **Why needed here:** The "Obfuscation Disparity" mechanism relies on understanding how raw text (like Base64) is converted to tokens and whether a model "normalizes" this input before reasoning.
  - **Quick check question:** If a model uses aggressive text normalization, why would an encoding attack (like Leetspeak) fail?

## Architecture Onboarding

- **Component map:** Attacker Module -> Target Interface -> Evaluation Pipeline
- **Critical path:** 1. Define harmful content category 2. Select Bypass Method 3. Attacker Module generates prompt 4. Target Interface executes prompt 5. Evaluation Pipeline assigns Severity Score
- **Design tradeoffs:**
  - Self-Bypass: Highly effective for semantic vulnerabilities but requires the model to have "dark knowledge" of its own flaws; high automation potential (35.6% success)
  - Cross-Bypass: Better for finding generalizable transferability of attacks; simulates external threat landscape
  - Automation vs. Nuance: The study uses automated generation (scalable) but notes that "Use-Mention" distinctions require nuanced semantic understanding which automated systems might struggle to differentiate perfectly from safe content
- **Failure signatures:**
  - Semantic Drift: High severity scores (3-5) in Context Manipulation indicate the model is prioritizing narrative coherence over safety
  - Token Sensitivity: Non-zero severity in Obfuscation (for GPT-4o mini) indicates a lack of input normalization
  - Refusal Loop: Severity Score of 0 indicates successful alignment; direct injection should result in this
- **First 3 experiments:**
  1. Baseline Injection Test: Run Direct Injection prompts against your model to verify basic keyword/refusal filters are active (Expected: ~0% success)
  2. Context Manipulation Probe: Instruct the model to write a fictional story involving a restricted topic to test the "Use-Mention" boundary
  3. Self-Bypass Audit: Prompt the model to generate a prompt that would "violate its own safety guidelines" to see if it can identify its own semantic weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can safety alignment techniques be improved to distinguish between fictional "mention" of harm and actionable "use" of harm without stifling model utility?
- **Basis in paper:** [explicit] Section 5.1 identifies the "Use-Mention" distinction as a fundamental limitation in current RLHF approaches, noting that models often prioritize creative instructions over safety triggers when faced with Context Manipulation.
- **Why unresolved:** The paper highlights the failure mode but offers no technical solution to patch this semantic gap while maintaining the model's creative writing capabilities.
- **What evidence would resolve it:** A new alignment methodology or loss function that significantly reduces the success rate of Context Manipulation attacks compared to current RLHF baselines.

### Open Question 2
- **Question:** What specific pre-processing or architectural mechanisms are responsible for Gemini's perfect resilience to obfuscation attacks compared to GPT-4o mini?
- **Basis in paper:** [inferred] Section 5.2 notes the "Obfuscation Disparity" and speculates that Gemini likely employs "aggressive text normalization" or "secondary screening," but the exact cause remains unidentified.
- **Why unresolved:** The study was conducted as a black-box comparison; the authors could not inspect the internal pre-processing pipelines or tokenization defenses of the proprietary models.
- **What evidence would resolve it:** A white-box analysis or ablation study isolating the input normalization layers of both models to identify the specific component that neutralizes encoded inputs (e.g., Base64, Leetspeak).

### Open Question 3
- **Question:** Can "Self-Bypass" loops be effectively integrated into the training pipeline to create "Constitutionally" self-correcting models?
- **Basis in paper:** [explicit] Section 5.3 suggests that the viability of Self-Bypass implies expensive human red-teaming could be replaced by automated systems where models identify their own weaknesses during training.
- **Why unresolved:** The study measured the success of attacks but did not implement or test a training loop that utilizes these failures for self-correction.
- **What evidence would resolve it:** Empirical results from a training run where adversarial prompts generated via Self-Bypass are used as feedback, demonstrating a measurable reduction in vulnerability over successive iterations.

## Limitations
- Automated adversarial generation may not capture full spectrum of human adversarial creativity
- Severity scoring relies on single evaluation criterion without reporting inter-rater reliability
- Study focuses on only two models, limiting generalizability across LLM landscape
- Analysis doesn't account for potential confounding factors like model temperature or system context

## Confidence

**High Confidence (Multiple evidence anchors, mechanistic plausibility):**
- Gemini 2.5 Flash demonstrates stronger safety resilience than GPT-4o mini
- Context manipulation and role-playing attacks are most effective overall
- Self-bypass attacks are more effective than cross-bypass for semantic attacks
- Gemini shows perfect resistance to obfuscation attacks while GPT-4o mini is vulnerable

**Medium Confidence (Single evidence anchor or limited mechanistic detail):**
- The "Use-Mention" distinction explains context manipulation effectiveness
- Models possess implicit knowledge of their own linguistic boundaries
- Disparity in obfuscation resilience is caused by differences in preprocessing layers

**Low Confidence (Speculative mechanisms, minimal evidence):**
- The specific internal mapping of alignment flaws that enables self-bypass
- The exact nature of Gemini's normalization layer that neutralizes encoding attacks

## Next Checks
1. **Severity Scoring Validation:** Implement a blind inter-rater reliability study where multiple human evaluators score a subset of outputs using the severity rubric. Calculate Cohen's kappa to assess consistency and identify ambiguous cases requiring rubric refinement.

2. **Obfuscation Mechanism Confirmation:** Test both models with a gradient of obfuscation techniques (Base64, hex, leetspeak, simple encoding) while instrumenting input preprocessing to determine whether Gemini applies normalization layers that strip or transform adversarial encodings before reasoning.

3. **Cross-Model Transferability Analysis:** Conduct a focused experiment comparing self-bypass vs cross-bypass success rates across multiple model pairs (e.g., Gemini→GPT, GPT→Gemini) to determine whether the observed advantage of self-bypass is specific to model-pair dynamics or represents a generalizable phenomenon.