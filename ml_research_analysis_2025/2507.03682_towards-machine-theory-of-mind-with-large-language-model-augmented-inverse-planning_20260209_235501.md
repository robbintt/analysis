---
ver: rpa2
title: Towards Machine Theory of Mind with Large Language Model-Augmented Inverse
  Planning
arxiv_id: '2507.03682'
source_url: https://arxiv.org/abs/2507.03682
tags:
- laip
- agent
- room
- restaurant
- hypotheses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LLM-AUGMENTED INVERSE PLANNING (LAIP), a hybrid
  model that combines large language models (LLMs) with Bayesian inverse planning
  for Theory of Mind (ToM) tasks. The method uses LLMs to generate hypotheses about
  an agent's beliefs and desires, then employs inverse planning to compute posterior
  probabilities of these mental states based on observed actions.
---

# Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning

## Quick Facts
- arXiv ID: 2507.03682
- Source URL: https://arxiv.org/abs/2507.03682
- Authors: Rebekah A. Gelpí; Eric Xue; William A. Cunningham
- Reference count: 40
- Primary result: Hybrid LLM-Bayesian model achieves 48.4% posterior probability on correct hypothesis vs 11.9% for zero-shot baselines in restaurant preference tasks

## Executive Summary
This paper introduces LLM-AUGMENTED INVERSE PLANNING (LAIP), a hybrid approach that combines large language models with Bayesian inverse planning for Theory of Mind (ToM) tasks. The method uses LLMs to generate hypotheses about an agent's beliefs and desires, then employs inverse planning to compute posterior probabilities of these mental states based on observed actions. LAIP addresses the scalability limitations of traditional Bayesian models while avoiding the brittleness of pure LLM approaches to ToM reasoning.

In controlled experiments with restaurant preference tasks, LAIP significantly outperformed zero-shot LLM baselines and chain-of-thought prompting, achieving 48.4% posterior probability on the correct hypothesis versus 11.9% for zero-shot CoT when inferring preferences. The model also showed strong performance on the MMToM-QA benchmark (67.5% accuracy vs 34% baseline), demonstrating its effectiveness in both constrained and open-ended scenarios.

## Method Summary
LAIP uses LLMs as hypothesis generators and likelihood estimators within a Bayesian inverse planning framework. The method first generates N hypotheses about an agent's preferences and beliefs with associated priors. At each timestep, it estimates the likelihood of each possible action under each hypothesis using the LLM. After observing the actual action, Bayes' rule updates the posterior probability of each hypothesis. The model can compute posteriors mathematically or via LLM prompting. LAIP was tested across multiple LLM sizes (GPT-4o, GPT-4o-mini, GPT-3.5, LLaMA 3-8B/70B, Mixtral, Gemma 2) using 20 hypotheses per run in restaurant preference tasks and MMToM-QA benchmark.

## Key Results
- LAIP achieved 48.4% posterior probability on correct hypothesis vs 11.9% for zero-shot CoT in restaurant preference tasks
- On MMToM-QA benchmark, LAIP reached 67.5% accuracy vs 34% for GPT-4 baseline
- Strong correlation with optimal Bayesian models (r=0.94) and low Jensen-Shannon divergence (0.011-0.015)
- Smaller LLMs showed larger gains from mathematical posterior computation (Cohen's d = 1.59 for Gemma 2 vs d = -0.03 for GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1
Separating hypothesis generation from inference improves ToM reasoning by leveraging LLM world knowledge while avoiding heuristic shortcuts. The LLM samples candidate hypotheses from implicit representations of language and world knowledge, then a separate Bayesian inference step computes posteriors. This decomposition prevents the LLM from jumping to conclusions based on surface patterns. If hypothesis space is poorly generated (biased, insufficiently diverse, or missing key alternatives), posteriors will be wrong regardless of inference quality.

### Mechanism 2
Mathematical posterior computation outperforms LLM-based computation, especially for smaller models, because it enforces normative probabilistic reasoning. After the LLM generates likelihoods P(A|H) for each action-hypothesis pair, Bayes' rule is applied algorithmically rather than through prompting. Smaller LLMs show larger gains from this (Cohen's d = 1.59 for Gemma 2 vs d = -0.03 for GPT-4o). If likelihood estimates are systematically miscalibrated (e.g., overconfident), mathematical posteriors will amplify errors.

### Mechanism 3
Belief state tracking enables context-appropriate inference when actions don't directly reveal preferences. The model maintains beliefs about what the observed agent knows (e.g., which restaurants are visible/closed), then conditions likelihood estimates on those beliefs. This allows correctly inferring Thai/Indian preferences despite inconsistent choices due to contextual constraints. If belief state updates are incorrect (e.g., not tracking what the agent has observed), likelihood estimates will be systematically wrong.

## Foundational Learning

- **Concept: Bayesian Inverse Planning**
  - Why needed here: Core mathematical framework. Requires understanding how to "invert" a forward model (from mental states to actions) into an inverse model (from actions to mental states).
  - Quick check question: Given P(action|preference) and a prior P(preference), how would you compute P(preference|observed_action)?

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The restaurant task is a POMDP—agents don't know restaurant availability until they observe it. Understanding belief states over hidden variables is essential.
  - Quick check question: Why would an agent's action differ between a fully observable environment and a partially observable one with the same ground truth?

- **Concept: The Frame Problem in AI**
  - Why needed here: Traditional Bayesian ToM models require manual specification of hypothesis spaces. LLMs "sidestep" this by sampling from learned representations.
  - Quick check question: What happens to a Bayesian ToM model if the true hypothesis isn't in the predefined hypothesis space?

## Architecture Onboarding

- **Component map:**
  Environment State → LLM Hypothesis Generator (20 hypotheses + priors)
                    ↓
  For each timestep:
    Agent State + Visibility → LLM Belief State Tracker
                             ↓
    For each hypothesis H_i:
      State + H_i → LLM Action Likelihood Generator → P(A|H_i)
                             ↓
    Observed Action O → Posterior Calculator (Bayes rule) → P(H|O)
                             ↓
    Update P(H) for next timestep

- **Critical path:** LLM likelihood generation. Each hypothesis requires a separate LLM call to estimate P(actions|hypothesis). With 20 hypotheses × multiple timesteps, this is the computational bottleneck and primary failure point.

- **Design tradeoffs:**
  - Mathematical vs. LLM posterior: Mathematical is more reliable for smaller models but requires likelihood normalization; LLM-posterior works well for GPT-4o but fails for smaller models.
  - Hypothesis count: More hypotheses capture more possibilities but increase cost and may dilute probability mass. Paper uses 20 as a fixed default.
  - Action space constraint: Constrained action spaces (Studies 1-2) enable comparison to optimal models; unconstrained (Study 3) requires similarity-based matching via cosine similarity.

- **Failure signatures:**
  - LLM generates non-mutually-exclusive hypotheses → probability mass improperly distributed
  - Likelihood estimates lack calibration → posteriors unreliable
  - Missing key hypothesis → correct answer unavailable (frame problem returns)
  - Small models computing own posteriors → systematic reasoning errors (JSD 0.17+ vs 0.01 for math computation)

- **First 3 experiments:**
  1. Replicate Study 1 trajectory with a single hypothesis set (20 items) to validate pipeline—confirm posterior concentrates on correct hypothesis when Japanese restaurant is closed (target: ~48% vs 12% baseline).
  2. Test computational scaling: measure latency and cost for 20 hypotheses × 5 timesteps across model sizes; identify if hypothesis count can be reduced without performance loss.
  3. Implement belief state ablation: remove belief tracking and measure JSD increase on MMToM-QA goal-inference subset to quantify mechanism 3's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
Can Sequential Monte Carlo (SMC) methods be effectively integrated into the LAIP framework to optimize the trade-off between computational cost and hypothesis coverage? The authors state that future work "should consider how methods such as sequential Monte Carlo (SMC) can combine importance sampling methods... which may further optimize the high cost of sampling and evaluating hypotheses." The current implementation uses a fixed number of hypotheses; it is unknown if dynamic resampling and particle filtering would degrade the quality of LLM-generated hypotheses or improve efficiency. A comparative analysis of token usage and inference accuracy between the current fixed-hypothesis LAIP model and an SMC-augmented version on long-horizon tasks would resolve this.

### Open Question 2
To what extent does relying on LLMs for hypothesis generation propagate social biases or stereotypical assumptions into the Theory of Mind reasoning process? The discussion notes that using LLMs as samplers "has the potential to result in the proposal of biased or stereotypical hypotheses or actions that have the potential to be propagated and entrenched." The paper focuses on preference inference accuracy in controlled environments but does not quantify or audit the diversity and neutrality of the generated hypothesis space. An audit of the generated hypotheses in open-ended scenarios using bias benchmarks to measure the frequency of stereotypical versus neutral priors would resolve this.

### Open Question 3
Does the strong correlation between LAIP and optimal Bayesian models align with actual human inference patterns in the absence of direct human comparison data? The authors note that "we do not have direct human performance data for this specific tasks" but suggest that high correlation (r=0.94) with optimal models implies human-like reasoning based on prior literature. While the model is "optimal," human reasoning often relies on sub-optimal heuristics; without direct comparison, it is unclear if LAIP captures human specific failure modes or intuitive jumps. Running the "Restaurant" and "Alice" tasks with human participants to calculate the correlation between LAIP's posteriors and human likelihood judgments directly would resolve this.

## Limitations

- The restaurant preference task represents a narrow slice of real-world mental state inference, limiting generalizability
- Hypothesis generation depends critically on LLM's implicit world knowledge and may fail silently if true hypothesis falls outside training distribution
- Strong performance on MMToM-QA is encouraging but this benchmark still focuses on structured, discrete mental state prediction rather than open-ended social reasoning

## Confidence

- **High confidence**: LAIP's superior performance over zero-shot LLM baselines in controlled restaurant tasks (48.4% vs 11.9% posterior probability)
- **Medium confidence**: LAIP's scalability benefits for smaller LLMs and its effectiveness on MMToM-QA benchmark
- **Low confidence**: Generalizability to open-ended, real-world ToM scenarios with complex belief structures and continuous action spaces

## Next Checks

1. Test LAIP on out-of-distribution preferences (e.g., regional cuisines, dietary restrictions) where training data coverage is likely sparse to measure robustness to hypothesis space limitations
2. Conduct ablation studies varying hypothesis count (5, 10, 15, 20, 25) to quantify the tradeoff between coverage and computational cost
3. Implement LAIP on a multi-agent coordination task where agents must infer not just preferences but dynamic beliefs about other agents' mental states in real-time