---
ver: rpa2
title: Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue
  Settings
arxiv_id: '2502.11007'
source_url: https://arxiv.org/abs/2502.11007
tags:
- response
- local
- cloud
- inference
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying large language
  models (LLMs) in resource-constrained environments, where local devices face computational
  and memory limitations, while cloud deployment incurs high latency and costs. The
  authors propose TMO, a local-cloud inference offloading system with Three-M Offloading:
  Multi-modal, Multi-task, and Multi-dialogue.'
---

# Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings

## Quick Facts
- **arXiv ID**: 2502.11007
- **Source URL**: https://arxiv.org/abs/2502.11007
- **Reference count**: 31
- **Primary result**: TMO system achieves significant improvements in response quality, latency, and cost by optimizing local vs. cloud LLM inference with multi-modal data selection across multi-task, multi-dialogue scenarios.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) in resource-constrained environments by proposing TMO, a local-cloud inference offloading system. TMO uses a lightweight local LLM for simple tasks and a cloud LLM for complex, multi-modal tasks, optimized through resource-constrained reinforcement learning (RCRL). The system estimates response quality using nearest neighbor strategies and computes task-modality associations using CLIP embeddings. Evaluation on a new M4A1 dataset demonstrates significant improvements over baselines in response quality, latency, and usage cost while adhering to resource constraints.

## Method Summary
The TMO system formulates LLM inference offloading as a Markov Decision Process where states capture LLM choices, modality history, and task types. Actions select between local and cloud inference and determine which multi-modal data sources to use. The system employs RCRL to optimize this decision-making process while adhering to latency and cost constraints through penalty terms. Response quality is estimated offline using k-NN interpolation from the M4A1 dataset, and task-modality relevance is computed using CLIP embeddings. The policy network is trained on 30k timesteps using A2C/PPO/DQN algorithms with constraint penalty mechanisms.

## Key Results
- TMO significantly improves response quality, latency, and usage cost compared to baselines
- The system effectively balances local and cloud LLM inference with multi-modal data selection
- RCRL optimization with constraint penalties successfully adheres to resource budgets while maximizing cumulative reward
- CLIP-based task-modality association enables selective modality upload, reducing unnecessary data transmission

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Resource-constrained reinforcement learning can jointly optimize LLM selection (local vs. cloud) and modality selection across multi-dialogue interactions.
- Mechanism: TMO formulates the offloading problem as a Markov Decision Process where state captures (LLM choice, modality history, task type), actions select (LLM, modalities), and rewards combine response quality, latency, cost, and task-modality association scores. Resource constraints are enforced via penalty terms rather than hard constraints.
- Core assumption: The reward function weights can be tuned to balance competing objectives, and cumulative reward maximization over dialogue sequences aligns with user preferences.
- Evidence anchors: [abstract] states the RCRL strategy optimizes inference location and multi-modal data sources; [Section 3.3.3] defines constrained optimization with penalty terms.
- Break condition: If local LLM response quality approaches cloud LLM quality, the system may default to local inference regardless of modality needs.

### Mechanism 2
- Claim: Nearest neighbor interpolation enables robust offline estimation of response quality for unseen state-action pairs.
- Mechanism: For pending (state, action) pairs, retrieve k nearest neighbors from the offline dataset using Euclidean distance, then compute weighted average of their known response scores with inverse-distance weighting.
- Core assumption: The state-action embedding space is meaningful such that nearby pairs have similar response quality characteristics.
- Evidence anchors: [abstract] mentions nearest neighbor strategy for handling uncertainties; [Section 3.4.2] defines the weighted k-NN estimation formula.
- Break condition: If the dataset lacks coverage of certain task-modality-LLM combinations, k-NN estimates may be unreliable.

### Mechanism 3
- Claim: CLIP-based task-modality association scoring enables selective modality upload.
- Mechanism: Use pre-trained CLIP text and image encoders to embed prompts and modalities into a shared space. Compute normalized cross-modality similarity between text prompt and each available modality.
- Core assumption: CLIP embeddings capture semantic relevance between textual tasks and visual modalities in LLM assistant scenarios.
- Evidence anchors: [Section 3.5] defines association score as cosine similarity; [Section 1.1] discusses understanding relationships between modalities, tasks, and dialogues.
- Break condition: If task-modality relevance doesn't align with actual LLM performance gains from those modalities, the association score may misguide selection.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation**
  - Why needed here: RCRL frames offloading as sequential decision-making where each dialogue's choice affects future state. Understanding states, actions, transitions, rewards, and discount factors is essential.
  - Quick check question: Given state st = (local LLM, {overhead camera}, "recommendation task"), what action space is available for at+1?

- Concept: **Constraint handling in RL (Lagrangian relaxation)**
  - Why needed here: TMO uses penalty terms to enforce latency and cost constraints rather than hard constraints. Understanding soft vs. hard constraint enforcement helps interpret policy behavior.
  - Quick check question: If λ = 0, what happens to constraint satisfaction? If λ → ∞, what behavior emerges?

- Concept: **Offline RL with uncertain rewards**
  - Why needed here: Training uses pre-collected M4A1 dataset with estimated response scores. Unlike online RL, the policy cannot explore and receive true rewards, making uncertainty estimation critical.
  - Quick check question: Why does the paper use k-NN estimation instead of training a response predictor model?

## Architecture Onboarding

- Component map:
  Local LLM (Phi-3-mini) -> RCRL Policy Network -> Cloud LLM (GPT-4o) or Local LLM
  CLIP Encoders -> Task-Modality Association Scoring
  k-NN Response Estimator -> Reward Signal
  Judge LLM (GPT-4o) -> Dataset Creation (offline only)

- Critical path:
  1. User prompt + available modalities arrive
  2. CLIP computes association scores for each modality
  3. Current state constructed: (previous LLM choices, modality history, task type)
  4. Policy network outputs action: (local/cloud, modality subset)
  5. If local: run Phi-3-mini with text only
  6. If cloud: upload selected modalities + prompt, receive response
  7. Update state for next dialogue

- Design tradeoffs:
  - Local vs. Cloud LLM capability gap: Larger gap → more offloading but higher costs
  - Constraint tightness (ξⱼ): Tighter constraints → more local inference, lower quality
  - Reward weights (α, β): Higher α prioritizes quality; higher βψ/βφ prioritizes efficiency
  - k-NN neighborhood size (k): Larger k → smoother estimates but potentially biased

- Failure signatures:
  - Constraint violation cascade: Policy exceeds budget early, then forced into suboptimal local-only decisions
  - Modality over-selection: Uploading all modalities when task doesn't require them
  - Modality under-selection: Never uploading modalities, missing visual context
  - OOD estimation failure: Encountering task-modality combinations absent from training data

- First 3 experiments:
  1. Validate reward component contributions: Run ablation with βΛ = 0, βψ = 0, βφ = 0 separately to confirm each metric's impact on final policy behavior
  2. Stress test constraint handling: Systematically vary latency constraint (5s to 60s) and cost constraint ($0.01 to $0.10), measuring violation rates and reward degradation
  3. Profile device heterogeneity: Deploy on Jetson Nano vs. Jetson Orin NX, measuring local latency changes and whether policy adapts LLM selection appropriately

## Open Questions the Paper Calls Out

- How can multiple users collaboratively share resources for LLM inference while preserving privacy and ensuring fair resource allocation? The paper states future research will investigate multi-user collaborative inference through resource sharing, but current TMO operates in single-user settings with challenges of resource contention, privacy risks, and fairness in allocation.

- How can the system optimally select among multiple cloud servers with heterogeneous capabilities, pricing models, and latency characteristics? Future work will examine multi-server adaptive selection based on latency, cost, and other considerations, extending beyond the current single cloud LLM endpoint assumption.

- What safety layer mechanisms can guarantee hard constraint satisfaction under extremely limited resource budgets? The paper acknowledges struggles under extremely limited resources and states establishing safety layers is essential, as the current penalty-based approach provides soft constraints that may be violated.

## Limitations

- The system depends critically on the M4A1 dataset, which is not publicly accessible, limiting reproducibility and external validation.
- k-NN response estimation may degrade in high-dimensional state-action spaces or with sparse coverage of task-modality combinations.
- Constraint handling via penalty terms may lead to inconsistent satisfaction across different constraint regimes.
- CLIP-based association scoring assumes semantic relevance between modalities and tasks generalizes well to unseen scenarios.

## Confidence

- **High confidence**: The core MDP formulation and constraint handling mechanism are well-established approaches adapted appropriately to the LLM offloading context.
- **Medium confidence**: The effectiveness of the nearest neighbor response estimation and CLIP-based modality association in this specific application, given the lack of direct evidence in the corpus.
- **Medium confidence**: The empirical improvements shown on the M4A1 dataset, limited by the absence of ablation studies isolating individual components and potential overfitting to the specific hardware platform used.

## Next Checks

1. **Dataset coverage analysis**: Quantify the distribution of state-action pairs in M4A1 to identify gaps where k-NN estimation may be unreliable. Measure average nearest-neighbor distance across different task-modality combinations.

2. **Constraint satisfaction stress test**: Systematically vary constraint tightness (latency 5s-60s, cost $0.01-$0.10) and measure violation rates. Compare performance against hard constraint formulations.

3. **Cross-device generalization**: Deploy the trained policy on a different hardware platform (e.g., Jetson Orin NX vs. Jetson Nano) and measure changes in local latency and policy adaptation. Assess whether the policy overfits to specific device characteristics.