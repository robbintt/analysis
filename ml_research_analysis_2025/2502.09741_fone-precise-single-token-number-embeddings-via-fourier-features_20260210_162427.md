---
ver: rpa2
title: 'FoNE: Precise Single-Token Number Embeddings via Fourier Features'
arxiv_id: '2502.09741'
source_url: https://arxiv.org/abs/2502.09741
tags:
- fone
- digit
- number
- arxiv
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models typically represent numbers using multiple
  tokens, requiring aggregation to interpret numerical values. This tokenization fragmentation
  makes training and inference less efficient and adversely affects performance on
  number-related tasks.
---

# FoNE: Precise Single-Token Number Embeddings via Fourier Features

## Quick Facts
- arXiv ID: 2502.09741
- Source URL: https://arxiv.org/abs/2502.09741
- Reference count: 40
- Primary result: Single-token Fourier-based number embeddings achieve 100% accuracy on addition, subtraction, and multiplication tasks while using 3-6× fewer tokens than baseline methods

## Executive Summary
This paper introduces FoNE (Fourier Number Embedding), a novel method for representing numerical values in language models as single tokens using Fourier features. Unlike traditional subword tokenization that fragments numbers across multiple tokens, FoNE encodes each number with only two embedding dimensions per digit, preserving numerical value without fragmentation. The method demonstrates significant improvements in both training efficiency and task accuracy, requiring 64× less data to achieve 99% accuracy on 6-digit decimal addition compared to baseline approaches while using fewer tokens per number.

## Method Summary
FoNE maps numerical values into embedding space using Fourier features, creating a compact single-token representation for each number. The approach uses two embedding dimensions per digit, encoding both the digit value and its positional information through Fourier transforms. This design eliminates the need for aggregation or reconstruction that typically follows multi-token number representations. The method is evaluated across addition, subtraction, and multiplication tasks, showing superior performance in terms of accuracy, data efficiency, and computational requirements compared to traditional subword and digit-wise embeddings.

## Key Results
- Achieved 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication tasks
- Required 64× less training data to reach 99% accuracy on 6-digit decimal addition compared to subword embeddings
- Used 3× and 6× fewer tokens per number than subword and digit-wise embeddings respectively
- Demonstrated perfect accuracy where baseline methods typically achieve lower performance

## Why This Works (Mechanism)
FoNE leverages Fourier features to create a continuous, compact embedding space that preserves numerical relationships without fragmentation. By mapping numbers to a lower-dimensional space using sinusoidal functions, the method captures both magnitude and positional information in a way that neural networks can learn arithmetic operations more efficiently. The single-token representation eliminates the need for aggregation across multiple tokens, reducing computational overhead and improving numerical reasoning accuracy.

## Foundational Learning
- Fourier feature mapping: Transforms numerical values into a higher-dimensional space using sinusoidal functions to capture periodic patterns; needed for compact representation of numerical relationships; quick check: verify sinusoidal encoding preserves numerical order
- Single-token embedding strategy: Represents entire numbers as single tokens rather than sequences; needed to eliminate aggregation overhead; quick check: confirm single-token vs multi-token performance gap
- Positional encoding through Fourier features: Uses frequency-based encoding to capture digit positions; needed to maintain place value information; quick check: test with reversed digit positions

## Architecture Onboarding

**Component map**: Input number -> Fourier feature transformation -> Single embedding token -> Arithmetic operation learning -> Output

**Critical path**: The transformation from raw numerical input through Fourier features to the final embedding vector is the most critical component, as errors here propagate through all subsequent operations.

**Design tradeoffs**: Single-token representation improves efficiency but may limit capacity for very large numbers (>15 digits require chunking); two-dimensional embedding per digit balances compactness with representational power.

**Failure signatures**: Accuracy degradation on very large numbers (>15 digits), where chunking strategy reintroduces fragmentation-like effects; potential loss of precision for numbers exceeding float64 range.

**Three first experiments**:
1. Compare FoNE performance on 6-digit addition vs traditional subword tokenization across varying training dataset sizes
2. Test FoNE on non-arithmetic numerical tasks (comparisons, sorting) to validate generalization
3. Measure computational overhead of Fourier feature transformation during training and inference

## Open Questions the Paper Calls Out
- How can FoNE be efficiently integrated into existing pre-trained LLMs without causing catastrophic forgetting of semantic knowledge or requiring prohibitive retraining costs?
- Does the Fourier-based embedding improve performance on tasks requiring continuous numerical reasoning, such as time-series forecasting or physics simulation, or is it limited to symbolic arithmetic?
- Why does the chunking strategy for large numbers (>15 digits) fail to achieve the perfect accuracy seen in standard inputs, and can this be mitigated?

## Limitations
- Perfect 100% accuracy claims require independent verification as such performance is unusual for numerical reasoning tasks
- Comparison framework focuses narrowly on subword and digit-wise embeddings without exploring alternative numerical representation approaches
- Computational overhead of Fourier feature transformation during training and inference is not explicitly quantified

## Confidence
- High: Demonstrated improvements in arithmetic accuracy and data efficiency compared to baseline embedding methods are well-supported by experimental results
- Medium: Claim of 100% accuracy across all tested numerical operations requires independent replication
- Low: Broader applicability to non-arithmetic numerical tasks and performance with different number ranges or bases remains untested

## Next Checks
1. Replicate the 100,000 test example experiments with independent implementation to verify the claimed 100% accuracy across all three arithmetic operations
2. Test FoNE on numerical reasoning tasks beyond basic arithmetic, including comparison operations and symbolic manipulation
3. Quantify the exact computational overhead of the Fourier feature transformation during both training and inference to determine net efficiency gains