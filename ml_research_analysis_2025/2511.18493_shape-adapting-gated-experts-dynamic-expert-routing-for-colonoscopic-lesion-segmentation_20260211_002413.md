---
ver: rpa2
title: 'Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion
  Segmentation'
arxiv_id: '2511.18493'
source_url: https://arxiv.org/abs/2511.18493
tags:
- expert
- experts
- routing
- shared
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of segmenting heterogeneous
  tissue structures in gigapixel histopathology images, where traditional CNN-Transformer
  hybrids rely on static computation graphs and struggle with varying input complexity.
  The proposed Shape-Adapting Gated Experts (SAGE) framework dynamically reconfigures
  backbones into adaptive expert architectures using hierarchical gating to route
  features between shared and specialized experts, complemented by a Shape-Adapting
  Hub that harmonizes representations across CNN and Transformer modules.
---

# Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation

## Quick Facts
- arXiv ID: 2511.18493
- Source URL: https://arxiv.org/abs/2511.18493
- Reference count: 40
- Primary result: SAGE-UNet achieves 95.57%, 95.16%, and 94.17% Dice scores on EBHI, DigestPath, and GlaS datasets respectively

## Executive Summary
This paper addresses the challenge of segmenting heterogeneous tissue structures in gigapixel histopathology images, where traditional CNN-Transformer hybrids rely on static computation graphs and struggle with varying input complexity. The proposed Shape-Adapting Gated Experts (SAGE) framework dynamically reconfigures backbones into adaptive expert architectures using hierarchical gating to route features between shared and specialized experts, complemented by a Shape-Adapting Hub that harmonizes representations across CNN and Transformer modules. SAGE-UNet achieves state-of-the-art Dice scores of 95.57%, 95.16%, and 94.17% on EBHI, DigestPath, and GlaS datasets, respectively, demonstrating robust generalization and improved efficiency through dynamic expert selection.

## Method Summary
SAGE-UNet combines ConvNeXt and ViT encoders in parallel, with each block treated as a potential expert. A hierarchical router uses Semantic Affinity Routing (SAR) with two-level gating to select between shared and specialized experts. The Shape-Adapting Hub (SA-Hub) enables cross-architecture routing by transforming CNN features to token sequences and vice versa. Dual-path fusion with learnable scalars blends main-path (backbone) and expert-path outputs. The model is trained in two stages: initial uniform learning rate followed by discriminative fine-tuning with different rates for shared vs. non-shared parameters.

## Key Results
- Achieved state-of-the-art Dice scores of 95.57%, 95.16%, and 94.17% on EBHI, DigestPath, and GlaS datasets
- Demonstrated superior generalization with Test B performance showing robustness to domain shifts
- Showed sigmoid gating outperforms softmax in dual-path fusion by approximately 0.5% DSC

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Top-K Expert Routing
Two-level gating improves upon flat MoE by first deciding expert type (shared vs. specialized), then selecting specific experts. A scalar gate $g_s$ biases routing logits toward shared experts when high, or fine-grained experts when low. CNN layers prefer shared experts ($g_s > 0.5$); Transformer layers remain near-neutral. Core assumption: inputs vary in semantic complexity; simple features benefit from general-purpose experts while complex patterns require specialization.

### Mechanism 2: Shape-Adapting Hub (SA-Hub)
Enables CNN and Transformer experts to interoperate despite incompatible tensor shapes (spatial maps vs. token sequences). $S_{in}$ performs architectural normalization before expert execution; $S_{out}$ reconstructs the output to match main-path dimensions. Core assumption: feature semantics can be preserved across shape transformations with learned adapters.

### Mechanism 3: Dual-Path Fusion with Learnable Scalar
Blending main-path and expert-path outputs via learned $\alpha_i$ preserves pretrained knowledge while enabling adaptive refinement. $z_i = \alpha_i \cdot z^{(main)}_i + (1-\alpha_i) \cdot z^{(expert)}_i$. Not all inputs require expert refinement; some can rely on backbone alone. Sigmoid gating outperforms softmax by avoiding forced expert competition.

## Foundational Learning

- **Concept: Sparse Mixture-of-Experts (SMoE)**
  - Why needed: SAGE extends SMoE with hierarchical routing; understanding load-balancing loss and router collapse is essential
  - Quick check: Given $M=20$ experts and $K=4$, what fraction of experts can remain inactive before load-balancing loss becomes critical?

- **Concept: CNN vs. Transformer Inductive Biases**
  - Why needed: The model routes between ConvNeXt (local, translation-equivariant) and ViT (global, attention-based) experts based on input complexity
  - Quick check: For a texture-heavy histology patch with weak long-range structure, should the router favor CNN or Transformer experts at early layers?

- **Concept: Load-Balancing Auxiliary Loss**
  - Why needed: Prevents router collapse where few experts dominate; $L_{balance} = M \sum f_j P_j$ encourages uniform expert utilization
  - Quick check: If $f_j = 0.8$ for one expert and near-zero for others, will $L_{balance}$ increase or decrease, and what behavior does this incentivize?

## Architecture Onboarding

- **Component map:** Input → Patch Embedding → Stage 0-3 (ConvNeXt blocks) → SAR router → SA-Hub adapters → Top-K expert selection → Expert execution → Dual-path fusion → Decoder

- **Critical path:** Input passes through patch embedding and initial ConvNeXt stages. Simultaneously, router computes logits via SAR plus hierarchical gate. Top-K experts are activated, SA-Hub transforms features, executes experts, and reconstructs outputs. Dual-path fusion via $\alpha_i$ combines backbone and expert contributions. Decoder aggregates skip connections to produce segmentation output.

- **Design tradeoffs:** K (activated experts): Higher K improves diversity but increases compute. S (shared experts): More shared experts provide robust common knowledge. Gating function: Sigmoid outperforms softmax (independent activation vs. competition).

- **Failure signatures:** Router collapse (few experts handle >80% of tokens; $L_{balance}$ stagnates near zero). SA-Hub bottleneck (gradients vanish through adapters; expert outputs become noisy). Path imbalance ($\alpha_i$ stuck near 1 or 0). Domain shift degradation (Test B performance drops significantly).

- **First 3 experiments:** 1) Ablate hierarchical gating: Force $g_s = 0.5$ and measure DSC drop on GlaS Test B. 2) SA-Hub stress test: Remove SA-Hub and allow only homogeneous-expert routing; compare Dice on heterogeneous tissue patches. 3) Expert utilization profiling: Log Top-K activation frequencies per expert across all datasets; verify no expert has <5% activation rate.

## Open Questions the Paper Calls Out

### Open Question 1
Does the computational overhead of hierarchical gating and Shape-Adapting Hub negate efficiency gains from sparse expert activation during real-time inference? The paper claims "improved efficiency through dynamic expert selection" but lacks inference latency benchmarks against static baselines like TransUNet or Swin-UNet.

### Open Question 2
Does input-adaptive routing cause prediction inconsistencies when stitching adjacent patches for gigapixel WSI reconstruction? The method processes WSIs by extracting overlapping patches, and routing is "input-adaptive," potentially leading to discontinuous features at patch boundaries that aren't analyzed in the current evaluation.

### Open Question 3
Can Semantic Affinity Routing generalize to non-histopathology medical domains where the distinction between local (CNN) and global (Transformer) features differs significantly? All experiments are restricted to 2D H&E stained colonoscopic datasets, leaving unverified whether the router learns meaningful affinity for modalities like MRI, CT, or 3D volumetric data.

## Limitations
- SA-Hub implementation details are unspecified, particularly the exact architecture of Sin/Sout adapters
- Router load-balancing validation is missing empirical data on expert utilization distribution
- Generalization beyond colonic histopathology is unverified, as all datasets are from similar tissue morphology

## Confidence
- **Hierarchical Two-Level Expert Routing**: Medium Confidence - Ablation shows sigmoid gating outperforms softmax, but direct validation of hierarchical gating mechanism is missing
- **SA-Hub Enabling Cross-Architecture Routing**: Low Confidence - No ablation removes SA-Hub to show degradation, nor comparison against homogeneous-expert routing
- **State-of-the-Art Performance**: High Confidence - Strong Dice scores are numerically consistent and training protocol is detailed enough for replication

## Next Checks
1. Ablate hierarchical gating: Force $g_s = 0.5$ throughout training and measure DSC drop on GlaS Test B to quantify hierarchical routing contribution
2. SA-Hub stress test: Remove SA-Hub and restrict routing to homogeneous-expert pairs only; compare Dice scores on heterogeneous tissue patches
3. Expert utilization profiling: Log Top-K activation frequencies per expert across all three datasets; verify no expert has <5% activation rate and distribution is reasonably uniform