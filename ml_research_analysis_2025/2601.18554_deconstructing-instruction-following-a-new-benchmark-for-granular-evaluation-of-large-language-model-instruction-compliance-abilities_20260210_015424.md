---
ver: rpa2
title: 'Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation
  of Large Language Model Instruction Compliance Abilities'
arxiv_id: '2601.18554'
source_url: https://arxiv.org/abs/2601.18554
tags:
- constraints
- given
- constraint
- compliance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MOSAIC, a modular synthetic benchmark for
  granular evaluation of instruction-following in large language models. Unlike prior
  work that conflates task accuracy with compliance or uses artificial constraints,
  MOSAIC uses dynamically generated prompts with up to 20 complex, real-world-oriented
  constraints decoupled from the task.
---

# Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities

## Quick Facts
- **arXiv ID**: 2601.18554
- **Source URL**: https://arxiv.org/abs/2601.18554
- **Reference count**: 18
- **Primary result**: Introduces MOSAIC, a modular synthetic benchmark revealing significant variation in how LLMs handle different instruction constraints, with positional biases and model-specific weaknesses

## Executive Summary
This paper introduces MOSAIC, a modular synthetic benchmark designed to provide granular evaluation of instruction-following capabilities in large language models. Unlike previous benchmarks that conflate task accuracy with compliance or rely on artificial constraints, MOSAIC uses dynamically generated prompts with up to 20 complex, real-world-oriented constraints that are decoupled from the task itself. The benchmark evaluates compliance through single constraint, pairwise constraint, and position-based metrics, revealing that instruction-following is not monolithic but exhibits significant variation across different constraint types and positions.

The study finds that models exhibit substantial differences in handling various constraints, with specific weaknesses identified for certain models (e.g., Qwen3 and DeepSeek-R1 struggle with list formatting). The analysis uncovers positional biases such as primacy and recency effects, as well as constraint synergies and conflicts that were previously obscured by high-level benchmarks. These findings demonstrate that granular evaluation can provide critical diagnostic insights for developing more reliable LLMs, challenging the assumption that instruction-following is a uniform capability.

## Method Summary
The MOSAIC benchmark generates synthetic prompts by dynamically combining a base task with multiple real-world-oriented constraints. Each prompt contains up to 20 constraints covering areas like formatting, language requirements, exclusion rules, and structural specifications. The evaluation framework assesses compliance through three metrics: single constraint evaluation measures individual constraint adherence, pairwise constraint analysis examines constraint interactions and synergies/conflicts, and position-based metrics investigate primacy and recency effects. The benchmark was tested across multiple LLM families including GPT-4, Claude-3, Llama-3, Qwen2, and DeepSeek models, with results showing substantial variation in constraint handling across different model families and individual constraints.

## Key Results
- Compliance varies significantly across constraint types, with models showing specific weaknesses (Qwen3/DeepSeek-R1 struggle with list formatting)
- Positional effects are evident, with primacy and recency biases affecting constraint adherence
- Constraint synergies and conflicts emerge, where certain constraint combinations enhance or impair compliance
- Granular evaluation reveals failure points invisible to high-level benchmarks, providing diagnostic insights

## Why This Works (Mechanism)
The MOSAIC benchmark works by decoupling task performance from constraint compliance, allowing researchers to isolate and evaluate instruction-following as a distinct capability. By using dynamically generated synthetic prompts with multiple real-world-oriented constraints, the benchmark creates a controlled environment where constraint interactions and positional effects can be systematically studied. The modular design enables testing of individual constraint types and their combinations, revealing patterns that would be obscured in task-specific evaluations.

## Foundational Learning
- **Instruction-following vs task accuracy**: Why needed - to distinguish between understanding what to do and following specific execution requirements; Quick check - measure task completion separately from constraint compliance
- **Constraint synergy/conflict analysis**: Why needed - to understand how multiple instructions interact and affect overall compliance; Quick check - test pairwise combinations of constraints to identify patterns
- **Positional bias in instruction processing**: Why needed - to determine if instruction order affects adherence probability; Quick check - vary constraint ordering across multiple prompts
- **Synthetic vs naturalistic instruction evaluation**: Why needed - to balance experimental control with real-world applicability; Quick check - compare synthetic benchmark results with performance on actual user instructions
- **Model-specific constraint weaknesses**: Why needed - to identify particular areas where different models struggle; Quick check - aggregate compliance rates across constraint types for each model family

## Architecture Onboarding

### Component Map
Synthetic prompt generator -> Constraint evaluator -> Position analyzer -> Model compliance scorer -> Diagnostic reporter

### Critical Path
Prompt generation → Model response → Constraint extraction → Compliance checking → Position analysis → Performance reporting

### Design Tradeoffs
The benchmark prioritizes experimental control and systematic evaluation over ecological validity. Synthetic constraints provide consistent testing conditions but may not fully capture the ambiguity and complexity of natural instructions. The modular design enables granular analysis but requires significant computational resources for comprehensive testing across multiple constraint combinations.

### Failure Signatures
- Consistent failure on specific constraint types indicates model-specific weaknesses
- Position-dependent compliance patterns suggest processing biases
- Constraint interaction failures reveal architectural limitations in handling multiple requirements
- High task accuracy with low compliance indicates successful task completion despite poor instruction-following

### 3 First Experiments
1. Test baseline compliance on single constraints to establish individual capability profiles
2. Evaluate pairwise constraint combinations to identify synergies and conflicts
3. Analyze compliance patterns across different constraint positions to measure primacy/recency effects

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic constraints may not fully represent the complexity of natural instructions
- Results may be biased by the fixed constraint ordering used in the benchmark
- Analysis focuses on English-language prompts, limiting cross-lingual generalizability
- The benchmark's controlled environment may not capture real-world instruction-following challenges

## Confidence
- **High**: Claims about positional biases and constraint-specific model weaknesses are well-supported by systematic evaluation across multiple models
- **Medium**: Diagnostic value of granular evaluation depends on synthetic constraints adequately representing real-world complexity
- **Low**: Universal primacy/recency effects may be artifacts of the fixed constraint ordering rather than inherent processing biases

## Next Checks
1. Validate MOSAIC findings using naturalistic instructions from real user interactions to test whether constraint-specific weaknesses persist outside synthetic prompts
2. Conduct ablation studies varying constraint order to distinguish genuine positional effects from artifacts of the current fixed ordering
3. Test whether identified model-specific weaknesses (e.g., Qwen3's list formatting issues) correlate with performance degradation on actual user-facing applications