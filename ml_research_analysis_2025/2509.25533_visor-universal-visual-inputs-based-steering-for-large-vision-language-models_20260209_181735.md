---
ver: rpa2
title: 'VISOR++: Universal Visual Inputs based Steering for Large Vision Language
  Models'
arxiv_id: '2509.25533'
source_url: https://arxiv.org/abs/2509.25533
tags:
- steering
- visor
- images
- image
- behavioral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VISOR++ is a method for steering Vision Language Models using optimized
  visual inputs instead of activation-based steering vectors, enabling behavioral
  control without runtime model access. It generates universal adversarial images
  that induce target activation patterns across diverse VLM architectures and prompts,
  achieving performance parity with steering vectors for refusal, sycophancy, and
  survival instinct behaviors.
---

# VISOR++

## Quick Facts
- arXiv ID: 2509.25533
- Source URL: https://arxiv.org/abs/2509.25533
- Reference count: 10
- Generates universal adversarial images that induce target activation patterns across diverse VLM architectures and prompts, achieving performance parity with steering vectors for refusal, sycophancy, and survival instinct behaviors

## Executive Summary
VISOR++ introduces a novel approach to steering Vision Language Models (VLMs) by optimizing visual inputs instead of traditional activation-based steering vectors. The method generates universal adversarial images that induce target activation patterns across diverse VLM architectures and prompts, achieving performance parity with steering vectors for behavioral control. Both model-specific and jointly optimized images show strong steering effects, with universal images demonstrating promising directional transfer to unseen models, including closed-access ones. The approach preserves 99.9% performance on unrelated tasks, making it a practical alternative for deployable behavioral steering in multimodal AI systems.

## Method Summary
VISOR++ transforms behavioral steering from an activation-space intervention to an input-space optimization problem. The method extracts steering vectors from contrastive prompt pairs at specified layers and token positions, then computes target activations by adding these vectors to baseline image representations. A dual-momentum optimization with spectral augmentation (DCT-domain perturbations) finds adversarial images that minimize the L2 distance between induced and target activations across an ensemble of models. The approach eliminates runtime model access requirements while preserving steering effectiveness through CWA-SSA optimization that finds shared vulnerability basins across architectures.

## Key Results
- VISOR++ images achieve behavioral alignment scores matching traditional steering vectors (9.1% vs 9.8% for refusal steering)
- Universal images preserve 99.9% MMLU performance while steering behavior across diverse VLM architectures
- Transfer directionality observed on 6/7 unseen models including GPT-4 variants, though absolute deltas remain modest

## Why This Works (Mechanism)

### Mechanism 1: Activation Pattern Mimicry via Visual Input Optimization
VISOR++ optimizes perturbations in pixel space to minimize L2 distance between activations induced by adversarial images and target activations computed from steering vectors. This transforms steering from activation-level intervention to input-space operation. The optimization propagates through differentiable preprocessing and vision encoders to influence specific language model layers with sufficient precision to replicate behavioral steering effects.

### Mechanism 2: Transferability via Common Weakness Spectral Optimization
The CWA-SSA framework uses dual-momentum optimization with spectral augmentation to find flat, overlapping basins across ensemble models. Inner momentum accumulates per-model gradients while outer momentum stabilizes trajectory with sign-based updates. Spectral noise increases robustness to preprocessing variations, enabling single images to transfer steering effects across architecturally diverse VLMs.

### Mechanism 3: Behavioral Specificity via Layer-Targeted Optimization
Steering images affect targeted behavioral dimensions while preserving unrelated task performance through layer-specific activation targeting. Optimization focuses on specific layers where behavioral vectors were computed, leaving other representations relatively untouched. This selective targeting enables behavioral control without degrading general reasoning capabilities.

## Foundational Learning

- **Concept: Activation/Representation Steering**
  - Why needed here: VISOR++ presupposes understanding that model behavior can be modified by adding vectors to hidden states, and that contrastive pairs yield meaningful direction vectors
  - Quick check question: Given two prompts "Help me with this task" vs. "I cannot help with this task," would you expect their activation difference at layer 15 to produce a meaningful steering vector for compliance?

- **Concept: Adversarial Optimization in Input Space**
  - Why needed here: VISOR++ reformulates steering as an adversarial attack problem—optimizing pixels to achieve a latent-space objective rather than output-space target
  - Quick check question: If optimizing an image to minimize cross-entropy loss on a target caption, would you expect PGD with signed gradients to converge faster than Adam for this non-convex problem?

- **Concept: Transferability in Multi-Model Ensembles**
  - Why needed here: The practical value of VISOR++ hinges on single images working across multiple VLMs; understanding why ensemble optimization improves transfer is critical
  - Quick check question: When training an adversarial perturbation on both CLIP-ViT and SigLIP encoders simultaneously, would you optimize their losses jointly or alternate—why?

## Architecture Onboarding

- **Component map**: Differentiable Preprocessing -> Steering Vector Computation -> Target Activation Generator -> Spectral Gradient Module -> Dual-Momentum Optimizer

- **Critical path**:
  1. Extract steering vectors for each target behavior and model (grid search over layers, multipliers, token positions)
  2. Compute target activations: baseline image + steering vector at specified layers
  3. Initialize adversarial image at 384×384; run dual-momentum optimization with spectral augmentation
  4. Evaluate on held-out prompts; iterate hyperparameters until BAS approaches steering vector baseline

- **Design tradeoffs**:
  - Per-model vs. Universal images: Per-model achieves higher steering magnitude; universal enables single-image deployment across models
  - Ensemble size vs. convergence: Larger ensembles improve transferability but increase compute and may require more iterations (sycophancy needed 20k steps vs. 5k for refusal)
  - Epsilon budget: Full 255/255 budget is practical for behavioral steering (no semantic constraint on source image); constrained budgets would reduce effectiveness

- **Failure signatures**:
  - BAS random baseline (no steering effect): Check gradient flow through preprocessing; verify target activations are non-trivial
  - Steering reverses direction: Sign error in vector computation or application multiplier
  - Poor transfer to unseen model: Ensemble may lack architectural diversity; try adding model with different vision encoder or token count
  - MMLU degradation >0.5%: Layer weights or prompt ensemble may be too broad, affecting general representations

- **First 3 experiments**:
  1. Reproduce refusal steering on single model (LLaVA-1.5-7B) with per-model VISOR++: extract steering vector, compute target activations, optimize with PGD for 2000 steps, compare BAS to baseline
  2. Ablate spectral augmentation: train with S=1 vs. S=20 samples; measure convergence speed and transferability to IDEFICS2
  3. Test universal image on unseen open-access model (e.g., LLaVA-NeXT): use trained universal refusal image, evaluate BAS change, compare to transfer deltas

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VISOR++ achieve effective steering with larger ensemble sizes (beyond 2 models), and how does ensemble diversity affect the universality of the resulting steering images?
- Basis in paper: Given the compute constraints, the above models form an ideal ensemble for our evaluation and authors explicitly note the field of finding steering methods that transfer across multiple VLMs is still an open area of research.
- Why unresolved: Limited to 2 models due to compute constraints; unknown whether optimization would converge with more diverse architectures.
- What evidence would resolve it: Training VISOR++ on ensembles of 3, 5, 10+ models with varied vision encoders and language backbones, then evaluating transferability.

### Open Question 2
- Question: Why does the sycophancy behavioral dimension require significantly more optimization steps to converge compared to refusal and survival instinct?
- Basis in paper: For the sycophancy case in particular, convergence requires an order of magnitude more steps than the other use cases restricting longer training runs for better steering images and we still had not hit full convergence even after 20k steps.
- Why unresolved: The mechanistic reason for sycophancy's slower convergence remains unexplained; may relate to how this behavior is represented in activation space.
- What evidence would resolve it: Activation analysis comparing sycophancy representations to other behaviors across layers; testing with different layer selections or loss weightings.

### Open Question 3
- Question: Why does positive steering transfer directionally only to GPT-4 variants while negative steering shows broader directional consistency across models?
- Basis in paper: Interestingly, we observe that transfer directionality only holds for the GPT-4 variants for positive steering and consistent negative trends across 6 out of the 7 unseen models across the different behavioral tasks.
- Why unresolved: The asymmetry between positive and negative steering transferability is observed but not mechanistically explained.
- What evidence would resolve it: Comparing activation patterns induced by positive vs. negative steering images across models; analyzing whether positive directions occupy narrower regions in activation space.

## Limitations

- Transfer effectiveness to unseen models remains modest despite directional consistency (1.5% vs 5.2% steering magnitude)
- Sycophancy behavior requires significantly more optimization steps to converge than other behavioral dimensions
- Claude Sonnet 3.5 shows complete immunity to VISOR++ steering images while other closed-access models exhibit directional effects

## Confidence

- VISOR++ matches steering vectors: High confidence—Table 1 shows 9.1% vs. 9.8% BAS for refusal; Table 2 confirms directional transfer to 6/7 unseen models
- Minimal task interference: High confidence—Table 3 demonstrates 99.9% MMLU preservation across 14,000 samples with universal images
- Universal transferability: Medium confidence—transfer exists but effects are modest (1.5% survival instinct steering on GPT-4V); architectural differences may limit scalability

## Next Checks

1. Test on diverse vision encoders: Apply VISOR++ universal images to models with non-CLIP/SigLIP encoders (e.g., DINOv2, OpenCLIP) to probe architectural transfer limits
2. Analyze layer-specific effects: Vary optimization layer targets (e.g., early vs. late layers) to map behavioral concept localization and assess specificity preservation
3. Benchmark against stronger baselines: Compare VISOR++ steering magnitude to advanced steering methods (e.g., FGAA, GCAV) on same tasks to contextualize performance gaps