---
ver: rpa2
title: Improving ML Training Data with Gold-Standard Quality Metrics
arxiv_id: '2512.20577'
source_url: https://arxiv.org/abs/2512.20577
tags:
- data
- tagging
- agreement
- taggers
- krippendorff
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that high-quality hand-tagged training
  data can be achieved by monitoring inter-rater agreement metrics (Krippendorff's
  alpha) across multiple tagging iterations rather than requiring multiple tags per
  item. The authors collected paraphrase similarity ratings from legal text using
  five legal professionals, with periodic monitoring datasets (60-150 pairs) reviewed
  by experienced attorneys who provided targeted feedback.
---

# Improving ML Training Data with Gold-Standard Quality Metrics

## Quick Facts
- arXiv ID: 2512.20577
- Source URL: https://arxiv.org/abs/2512.20577
- Authors: Leslie Barrett; Michael W. Sherman
- Reference count: 13
- Key outcome: High-quality hand-tagged training data achieved by monitoring inter-rater agreement metrics (Krippendorff's alpha) across multiple tagging iterations rather than requiring multiple tags per item.

## Executive Summary
This study demonstrates that high-quality hand-tagged training data can be achieved by monitoring inter-rater agreement metrics (Krippendorff's alpha) across multiple tagging iterations rather than requiring multiple tags per item. The authors collected paraphrase similarity ratings from legal text using five legal professionals, with periodic monitoring datasets (60-150 pairs) reviewed by experienced attorneys who provided targeted feedback. While initial burn-in showed acceptable agreement (>0.8), the moving variance of Krippendorff's alpha decreased throughout the 3-month data collection period, indicating continued tagger improvement. This suggests conventional burn-in periods may be insufficient for ambiguous tasks, and that ongoing monitoring with targeted interventions produces more reliable data than single-point agreement measurements or multiple tags per item.

## Method Summary
The authors implemented a quality monitoring system for legal text paraphrase similarity ratings using Krippendorff's alpha as the primary metric. The method involved five rounds of task design with experienced attorneys until achieving α > 0.8, followed by burn-in with five legal professionals tagging 500 education pairs. Production used single tags per item with periodic monitoring datasets (60-150 pairs, twice weekly) where all taggers labeled the same items. Experienced attorneys reviewed disagreements and provided targeted feedback. The key innovation was tracking moving variance of Krippendorff's alpha rather than relying solely on absolute agreement values.

## Key Results
- Moving variance of Krippendorff's alpha decreased throughout the 3-month data collection period, indicating continued tagger improvement
- Initial burn-in showed acceptable agreement (>0.8) but variance continued declining well beyond burn-in period
- Targeted expert feedback on disagreed monitoring items drove improvement more effectively than generic retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Declining variance in inter-rater agreement metrics across monitoring iterations signals improving tagger consistency, not just acceptable quality.
- Mechanism: Multiple agreement measurements over time capture convergence toward stable tagging behavior. Single-point measurements (conventional burn-in) miss ongoing improvement; moving variance reveals whether agreement is stabilizing or fluctuating.
- Core assumption: Tagger performance is not fixed after initial training—it continues to evolve with feedback and practice, particularly for ambiguous tasks.
- Evidence anchors:
  - [abstract] "moving variance of Krippendorff's alpha decreased throughout the 3-month data collection period, indicating continued tagger improvement"
  - [section 3] "variance drops at both the 8th and 16th monitoring dataset, and appears to converge at a very small value at the 20th monitoring dataset"
  - [corpus] Weak direct corpus support; neighbor papers focus on tagging architectures rather than longitudinal quality monitoring.
- Break condition: If monitoring dataset sample sizes are too small, individual Krippendorff's alpha values may be biased toward outliers, obscuring true variance trends.

### Mechanism 2
- Claim: Targeted expert feedback on disagreed items in monitoring datasets drives tagger improvement more effectively than generic retraining.
- Mechanism: Expert review identifies specific misunderstanding patterns in the monitoring subset; targeted feedback corrects those patterns without requiring wholesale instruction changes or tagger replacement.
- Core assumption: Disagreements in monitoring data are representative of disagreements in the full dataset, and experts can diagnose root causes from examples.
- Evidence anchors:
  - [abstract] "monitoring datasets... reviewed by experienced attorneys who provided targeted feedback"
  - [section 3] "We believe the primary driver of decreasing moving variance... was the review of disagreeing tags... these interventions did result in more consistent tagging"
  - [corpus] No direct corpus evidence for this specific feedback mechanism.
- Break condition: If feedback conversations are not documented, causal attribution between specific interventions and agreement improvements becomes uncertain.

### Mechanism 3
- Claim: Replacing "multiple tags per item" with "single tag per item plus periodic multi-tagger monitoring" can produce high-quality data at lower cost for specialized domains.
- Mechanism: Expert taggers are expensive; full multi-tagging doubles or quintuples labeling cost. Monitoring datasets sample agreement without requiring redundancy on every item, provided tagger performance is tracked and corrected.
- Core assumption: Taggers remain consistent across monitored and unmonitored items, and feedback generalizes beyond the monitoring subset.
- Evidence anchors:
  - [abstract] "high-quality hand-tagged training data can be achieved... rather than requiring multiple tags per item"
  - [section 1] "we specifically avoid replacing taggers or tagging every work item multiple times... Neither of these assumptions is necessarily the case, especially when a tagging task requires special expertise"
  - [corpus] Weak corpus support; neighbor papers do not address cost-quality tradeoffs in labeling.
- Break condition: If unmonitored items systematically differ from monitored items (e.g., harder cases avoided in monitoring), quality estimates become unreliable.

## Foundational Learning

- Concept: **Krippendorff's alpha**
  - Why needed here: This is the core metric for measuring inter-rater agreement; understanding its formula, interpretation, and sensitivity to sample size is essential for implementing the monitoring system.
  - Quick check question: Can you explain why Krippendorff's alpha is preferred over Cohen's kappa for multi-rater, ordinal data with potential missing values?

- Concept: **Moving variance / rolling statistics**
  - Why needed here: The paper's key insight is that variance in agreement—not just point estimates—indicates quality. Implementing this requires understanding how to compute and interpret rolling variance.
  - Quick check question: If you observe Krippendorff's alpha of 0.82, 0.79, 0.85, 0.81 across four monitoring periods, what does high vs. low moving variance tell you about tagger stability?

- Concept: **Burn-in period limitations**
  - Why needed here: Conventional practice assumes taggers are ready after initial training; this paper challenges that assumption for ambiguous tasks, which affects project planning.
  - Quick check question: For a new labeling task with inherent ambiguity, what signals would tell you that burn-in is insufficient and ongoing monitoring is required?

## Architecture Onboarding

- Component map:
  Task design phase -> Tagger education phase -> Production phase with monitoring datasets -> Expert feedback loop

- Critical path:
  1. Achieve Krippendorff's alpha >0.8 on task design rounds before recruiting production taggers
  2. Establish stable moving average during education before production begins
  3. Set up monitoring dataset sampling proportional to each tagger's production volume
  4. Implement variance tracking from monitoring point 5 onward (need 5 data points for moving variance)

- Design tradeoffs:
  - **Monitoring dataset size**: Smaller sets (60) are cheaper but noisier; larger sets (150) are more stable but consume expert time. Paper suggests moving metrics partially mitigate small-sample noise.
  - **Feedback frequency**: Twice-weekly was used; less frequent may miss improvement opportunities, more frequent may be unsustainable.
  - **Assumption**: The paper could not confirm which specific interventions caused improvement—feedback content was not logged.

- Failure signatures:
  - Moving variance of alpha remains high or increases over time (taggers not converging)
  - Alpha values oscillate without trend (task may be fundamentally ambiguous or instructions unclear)
  - Feedback does not reduce disagreements on similar item types (instructions may need revision, not just tagger coaching)

- First 3 experiments:
  1. **Pilot monitoring**: Label a small production batch (500-1000 items) with monitoring datasets at 2x/week; compute moving variance from period 5 onward to validate convergence pattern.
  2. **Feedback documentation**: Log the content of all expert-to-tagger feedback; correlate specific intervention topics with subsequent variance drops to test the hypothesized mechanism.
  3. **Sample size sensitivity**: Run parallel monitoring with 60 vs. 100 vs. 150 pairs for 4-6 weeks; compare variance stability to calibrate cost-quality tradeoff for your domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What constitutes an appropriate threshold for "reliable" agreement when variance-based metrics are used instead of fixed-point benchmarks like Landis & Koch's 0.8?
- Basis in paper: [explicit] The authors propose that "a 'reliable' result may be one in which the metric's variance is reduced by a certain amount over a given set of iterations rather than a fixed point that applies unilaterally."
- Why unresolved: The paper demonstrates declining variance but does not define how much reduction is sufficient or how to operationalize this threshold across different tasks.
- What evidence would resolve it: Repeating the variance-monitoring methodology across multiple tagging tasks with known ground truth to identify variance-reduction thresholds that predict downstream ML model performance.

### Open Question 2
- Question: What is the optimal size and frequency for monitoring datasets to minimize outlier bias while remaining cost-effective?
- Basis in paper: [explicit] The authors note that "some of our sample sizes were small, potentially biasing Krippendorff's alpha towards outliers" and state they "would consider collecting monitoring datasets with a larger number of work items" in the future.
- Why unresolved: The paper used 60-150 sentence pairs twice weekly but did not systematically test whether larger or more frequent samples would improve reliability estimates.
- What evidence would resolve it: A controlled experiment varying monitoring dataset sizes and collection frequencies while measuring the stability and predictive validity of agreement metrics.

### Open Question 3
- Question: To what extent does the observed decline in variance reflect tagger learning versus reduction in task ambiguity through instruction refinement?
- Basis in paper: [inferred] The authors attribute improved consistency to "targeted feedback" from attorneys, but they acknowledge having "no record of the content of conversations" and cannot isolate which specific interventions drove improvement.
- Why unresolved: Without systematic tracking of intervention content and timing, the causal mechanism remains unclear.
- What evidence would resolve it: Logging all feedback interventions with timestamps and content, then correlating specific feedback types with subsequent changes in agreement metrics at the tagger level.

### Open Question 4
- Question: Does reduced variance in inter-rater agreement directly translate to improved performance of ML models trained on the resulting data?
- Basis in paper: [inferred] The paper focuses on agreement metrics as a proxy for data quality but does not evaluate whether models trained on data collected with variance-based monitoring outperform models trained on data from standard burn-in approaches.
- Why unresolved: The causal link between agreement variance and downstream ML utility remains assumed rather than empirically validated.
- What evidence would resolve it: Training identical model architectures on datasets collected with and without variance-based monitoring, then comparing performance on held-out test sets with gold-standard labels.

## Limitations

- The monitoring approach requires domain experts who can both perform the task and provide targeted feedback, limiting scalability
- The causal link between specific feedback interventions and improved agreement is not fully validated due to lack of systematic intervention tracking
- The monitoring dataset selection methodology is underspecified, raising concerns about whether monitoring pairs are representative of production items

## Confidence

- High confidence: Krippendorff's alpha is appropriate for measuring inter-rater agreement on ordinal similarity ratings
- Medium confidence: Declining moving variance indicates ongoing tagger improvement rather than one-time training effects
- Low confidence: Targeted expert feedback is the primary driver of agreement improvement (no systematic intervention tracking)

## Next Checks

1. Implement a controlled experiment comparing monitoring-only (single tag + periodic multi-tagger review) vs. full multi-tagging (every item tagged by multiple experts) to quantify cost-quality tradeoffs
2. Document and categorize all expert feedback interventions, then correlate intervention types with subsequent variance reduction to validate the feedback mechanism
3. Test monitoring dataset sampling strategies (random vs. stratified by predicted difficulty) to determine optimal design for detecting true agreement trends