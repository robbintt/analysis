---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and
  Benchmark'
arxiv_id: '2402.02242'
source_url: https://arxiv.org/abs/2402.02242
tags:
- vision
- peft
- tuning
- proceedings
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey and benchmark of parameter-efficient
  fine-tuning (PEFT) methods for pre-trained vision models. The authors categorize
  existing methods into four types: addition-based, partial-based, unified-based,
  and multi-task tuning.'
---

# Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and Benchmark

## Quick Facts
- **arXiv ID:** 2402.02242
- **Source URL:** https://arxiv.org/abs/2402.02242
- **Reference count:** 40
- **Key outcome:** Comprehensive survey and benchmark of 25 PEFT algorithms across 24 image recognition, 3 video action recognition, and 3 dense prediction tasks, proposing a unified PPT metric.

## Executive Summary
This paper provides a comprehensive survey and benchmark of parameter-efficient fine-tuning (PEFT) methods for pre-trained vision models. The authors categorize existing methods into four types: addition-based, partial-based, unified-based, and multi-task tuning. They introduce V-PEFT Bench, a unified benchmark evaluating 25 PEFT algorithms across 24 image recognition, 3 video action recognition, and 3 dense prediction tasks. The benchmark proposes a Performance-Parameter Trade-off (PPT) metric for fair comparison. Key results show that DTL achieves the best PPT on VTAB-1k, while Mona surpasses full fine-tuning on MS COCO. The authors also highlight challenges like explainability, extending PEFT to generative models, simplifying hyperparameter selection, and incorporating differential privacy.

## Method Summary
The paper systematically evaluates 25 PEFT algorithms using a unified benchmark (V-PEFT Bench) across three domains: image recognition (VTAB-1k), video action recognition (SSv2, HMDB51), and dense prediction (MS COCO, ADE20K, PASCAL VOC). The benchmark uses ViT-B/16 (ImageNet-21K), Video Swin-B, and Swin-B/L as backbones. The primary metric is Performance-Parameter Trade-off (PPT), which balances task performance against the number of trainable parameters. The evaluation includes methods like DTL, Mona, AdaptFormer, LoRA, and various prompt-based approaches, comparing them against full fine-tuning and linear probing baselines.

## Key Results
- DTL achieves the best PPT on VTAB-1k image recognition tasks
- Mona surpasses full fine-tuning performance on MS COCO dense prediction
- Almost all PEFT algorithms outperform full fine-tuning on limited data regimes
- Different PEFT methods show varying effectiveness across task categories (Natural, Specialized, Structured)

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis Space Restriction via Regularization
- **Claim:** In low-data regimes (e.g., VTAB-1k), restricting trainable parameters acts as a regularizer, preventing catastrophic overfitting.
- **Evidence:** Section 5.4 shows PEFT algorithms outperform full fine-tuning (85.8M params, 65.57 accuracy) vs. DTL (0.04M params, 74.58 accuracy).
- **Break condition:** If downstream task requires features absent from pre-training data.

### Mechanism 2: Low-Rank Approximation of Task Shift
- **Claim:** Weight updates for task adaptation inhabit low-dimensional subspaces.
- **Evidence:** Section 3.2.2 describes LoRA's low-rank decomposition (W' = W + BA).
- **Break condition:** Tasks requiring high-dimensional spatial reasoning may struggle with extreme rank compression.

### Mechanism 3: Attention Manipulation via Prompt Injection
- **Claim:** Modifying input sequence or attention keys/values steers frozen model representations.
- **Evidence:** Section 3.1.2 describes VPT's learnable prompts concatenated with path embedding.
- **Break condition:** Insufficient prompt length limits capacity to encode complex task instructions.

## Foundational Learning

- **Concept: Transfer Learning vs. Fine-Tuning**
  - **Why needed here:** To distinguish between "Linear Probing" and various PEFT methods
  - **Quick check:** If I freeze the backbone and only train the final classification layer, am I doing PEFT? (Answer: Yes, Partial-based/Specification tuning)

- **Concept: Attention Mechanisms (Q, K, V)**
  - **Why needed here:** Essential for understanding where Adapters and Prefix Tuning are inserted
  - **Quick check:** In Prefix Tuning, do we modify the Query vector? (Answer: Typically Key and Value vectors are prepended)

- **Concept: Rank and Bottlenecks**
  - **Why needed here:** To grasp how LoRA and Adapters achieve efficiency
  - **Quick check:** In a standard Adapter, what happens to the feature dimension between down-projection and up-projection? (Answer: Reduced to bottleneck dimension k, then restored)

## Architecture Onboarding

- **Component map:** Input -> Backbone (Frozen) -> PEFT Module -> Head
- **Critical path:** Image → Patches → Embedding → PEFT intervention → Fusion (residual) → Backbone stream → Classification logit
- **Design tradeoffs:**
  - PPT metric balances performance vs. parameter count
  - Adapters add latency; LoRA does not (merged weights)
  - Side-tuning saves memory by avoiding backprop through backbone
- **Failure signatures:**
  - Overfitting: High train accuracy, low test accuracy on small datasets
  - Underfitting on Structured tasks: Poor performance on counting/depth tasks
  - Catastrophic forgetting: Model loses pre-training capabilities
- **First 3 experiments:**
  1. Baseline sanity check: Linear Probe vs. Full Fine-tuning on VTAB-1k
  2. PPT evaluation: DTL vs. VPT-Deep on FGVC datasets (CUB-200)
  3. Visual inspection: Attention maps for failing cases (e.g., SVHN with frozen backbone)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the interpretability of visual PEFT methods, specifically visual prompts, be improved?
- **Basis:** Section 6.1 states visual prompts are "difficult to translate into an understandable format"
- **What evidence would resolve it:** Techniques mapping learned visual prompt tokens to semantic concepts

### Open Question 2
- **Question:** How can PEFT techniques be effectively adapted for autoregressive visual generative models?
- **Basis:** Section 6.2 notes little research on PEFT for AR models
- **What evidence would resolve it:** Successful application on standard autoregressive visual generation models

### Open Question 3
- **Question:** Can automated techniques reduce reliance on manual hyperparameter tuning for PEFT?
- **Basis:** Section 6.3 highlights heavy dependence on hyperparameter configurations
- **What evidence would resolve it:** Meta-learning framework identifying optimal PEFT configurations across diverse datasets

### Open Question 4
- **Question:** How can differential privacy be integrated into PEFT methods without overhead?
- **Basis:** Section 6.4 identifies integration of large-scale model fine-tuning with differential privacy as a compelling avenue
- **What evidence would resolve it:** PEFT-specific DP algorithm maintaining accuracy with rigorous privacy guarantees

## Limitations
- Dataset bias: VTAB-1k's limited samples may favor regularization-heavy PEFT methods
- Metric simplification: PPT assumes equal importance of performance and parameter count across all applications
- Architecture lock-in: Results primarily based on Vision Transformers, may not transfer to convolutional architectures

## Confidence
- **High Confidence:** Categorization of PEFT methods is well-grounded; overfitting prevention on small datasets is empirically supported
- **Medium Confidence:** LoRA's low-rank approximation effectiveness is theoretically supported but optimal rank values vary
- **Low Confidence:** DTL's "best PPT on VTAB-1k" claim requires careful interpretation for larger datasets

## Next Checks
1. **Architecture Generalization Test:** Reproduce top 3 methods (DTL, Mona, AdaptFormer) on ConvNeXt or CNN-based architecture
2. **Dataset Scale Validation:** Evaluate same PEFT methods on full ImageNet-1k training set (not 1k-sample VTAB subset)
3. **Ablation on Rank and Capacity:** Systematically vary rank parameter in LoRA and bottleneck dimension across three task categories