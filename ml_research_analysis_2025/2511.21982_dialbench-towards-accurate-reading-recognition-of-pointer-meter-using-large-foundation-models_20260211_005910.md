---
ver: rpa2
title: 'DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large
  Foundation Models'
arxiv_id: '2511.21982'
source_url: https://arxiv.org/abs/2511.21982
tags:
- reading
- meter
- pointer
- dataset
- dial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of automated pointer meter reading
  in smart power systems, which is critical for real-time monitoring but hindered
  by reflections, occlusions, varying angles, and thin pointer markings. To address
  the lack of large-scale datasets, the authors introduce RPM-10K, a benchmark dataset
  of 10,730 meter images covering 300+ dial types under diverse environmental conditions.
---

# DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models

## Quick Facts
- **arXiv ID**: 2511.21982
- **Source URL**: https://arxiv.org/abs/2511.21982
- **Reference count**: 40
- **Primary result**: MRLM achieves 62.4% accuracy within 1% error and 70.9% within 5° angular error on RPM-10K dataset

## Executive Summary
This paper addresses the challenge of automated pointer meter reading in smart power systems, a critical task for real-time monitoring that is hindered by reflections, occlusions, varying angles, and thin pointer markings. The authors introduce RPM-10K, a benchmark dataset of 10,730 meter images covering 300+ dial types under diverse environmental conditions. They propose MRLM, a vision-language framework that explicitly injects physical relations into perception via Key Feature Mining, Mixture-of-Experts, and language-labeled supervision. MRLM achieves state-of-the-art accuracy, significantly advancing physics-aware AI for industrial vision tasks.

## Method Summary
The authors develop MRLM (MeterReading Large Model), a vision-language framework that explicitly encodes physical relationships in pointer meter reading. The system uses a CLIP visual encoder followed by a Key Feature Mining module that applies cross-attention with 6 dial archetype templates to enhance feature extraction. This is followed by a Mixture-of-Experts layer for conditional robustness across different visual degradation conditions, and an LLM decoder that generates numeric readings. The model is trained on the RPM-10K dataset with text-only numeric labels using cross-entropy loss, achieving superior accuracy compared to baseline approaches.

## Key Results
- MRLM achieves 62.4% accuracy within 1% error and 70.9% within 5° angular error on RPM-10K
- Outperforms existing models significantly on challenging conditions including reflections, occlusions, and varying angles
- KFM module improves accuracy from 59.7% to 68.1% by explicitly encoding physical meter relationships
- MoE routing provides +9.1% absolute gain on high exposure conditions but -8.3% on occlusion

## Why This Works (Mechanism)

### Mechanism 1: Entity Grounding via Cross-Attentional Key Feature Mining
The KFM module constructs a query vector from concatenated CLIP features of 6 canonical meter archetypes, which cross-attends to raw image features. This acts as a filter amplifying features semantically aligned with physical meter components while suppressing irrelevant backgrounds. The core assumption is that CLIP space contains distinct embeddings for pointers and scales that can serve as valid structural queries.

### Mechanism 2: Conditional Robustness via Mixture-of-Experts Routing
A gating network routes the enhanced feature representation to a sparse subset of experts, allowing specific expert weights to optimize for distinct coupling relations under different visual conditions. This prevents gradient interference between conflicting noise patterns. The dataset must contain sufficient samples of each degradation type for the router to learn distinct utilization strategies.

### Mechanism 3: Direct Numeric Regression over Chain-of-Thought
Training directly on numeric text labels yields higher regression accuracy than augmenting with intermediate reasoning traces. The hypothesis is that CoT supervision encourages the LLM to optimize for linguistic fluency at the expense of precise numeric mapping. By using only final numeric labels, the model reduces the task to image regression, forcing the visual encoder to compress geometric information directly into numeric token embeddings.

## Foundational Learning

- **Cross-Attention in VLMs**: Understanding Query/Key/Value roles is essential to debug why the model might ignore a pointer. Quick check: If the "template" (Query) represents a "perfect pointer," what does the Attention Map highlight in a blurry image?
- **Mixture-of-Experts Routing**: The system relies on soft-gating to handle varying environments. Quick check: Why might a router collapse into using only one expert even if multiple are available?
- **Supervision Signals for Regression vs. Generation**: The paper explicitly rejects CoT for direct numeric regression. Quick check: How does predicting the token "6.45" differ mathematically from regressing the float value 6.45?

## Architecture Onboarding

- **Component map**: Input Image (224x224) + Text Label → CLIP Encoder → KFM Module (Cross-Attention with Archetype Templates) → MLP → MoE Layer (Router + Experts) → Q-Former → LLM Decoder → Numeric Output
- **Critical path**: The KFM module is the critical differentiator. If the archetype features do not align with input features via cross-attention, subsequent modules receive ungrounded noise. This is where "physics" enters the pipeline.
- **Design tradeoffs**: Latency increases from ~580ms (Baseline/KFM-only) to ~688ms (Full MoE). Using 6 specific archetypes improves accuracy on known types but risks overfitting. Dropping CoT improves numeric accuracy but removes explainability.
- **Failure signatures**: Occlusion performance decreases by 12% relative, indicating MoE router may be actively harmful in obstructed environments. The model struggles with unseen dial types, suggesting it learns archetype matching rather than general geometric reasoning.
- **First 3 experiments**: 1) KFM Ablation on Archetype Zero-Shot: Test on dial type not in 6 archetypes. 2) MoE Router Visualization: Visualize expert activation patterns for High Exposure vs Occlusion. 3) Numeric Precision Stress Test: Compare MRLM against pure regression CNN head.

## Open Questions the Paper Calls Out
1. **Reinforcement Learning Integration**: Can RL be effectively integrated to improve accuracy and generalization beyond supervised fine-tuning? The authors identify RL as a promising but unexplored avenue for handling complex reasoning in this domain.
2. **Visual Reasoning Traces**: How can Chain-of-Thought be incorporated without degrading numerical regression accuracy required for precise meter reading? CoT has adverse effects on tasks sensitive to numerical regression.
3. **Zero-Shot Generalization**: What modifications are required to achieve robust zero-shot generalization for meter types unseen during training? The model struggles with images it has never seen before despite using large foundation models.

## Limitations
- The model demonstrates excellent performance on known dial archetypes but explicitly struggles with unseen meter types, suggesting potential overfitting to the 6 template archetypes
- MoE routing actively harms performance on occluded images (-12% relative), indicating the conditional routing mechanism may not generalize well to compound degradation scenarios
- The KFM module relies on 6 specific dial archetypes as queries, making reproduction difficult without exact template images

## Confidence
- **High Confidence**: Core architectural contributions (KFM cross-attention, MoE routing, numeric supervision) are well-specified and produce measurable improvements
- **Medium Confidence**: Interpretation of why CoT supervision harms numeric accuracy is plausible but could benefit from additional ablation studies
- **Low Confidence**: Claims about handling arbitrary dial types beyond training distribution, given explicit acknowledgment of generalization limitations

## Next Checks
1. **Archetype Generalization Test**: Evaluate MRLM on a held-out dial type not present in the 6 training archetypes to quantify template-dependency limitation
2. **MoE Router Analysis**: Visualize expert activation patterns across different degradation conditions to verify whether distinct experts are actually being selected
3. **Numeric Precision Benchmark**: Compare MRLM against a pure regression CNN head (no LLM) to isolate whether the LLM decoder contributes to or detracts from precise fractional reading accuracy