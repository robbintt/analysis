---
ver: rpa2
title: 'Spelling-out is not Straightforward: LLMs'' Capability of Tokenization from
  Token to Characters'
arxiv_id: '2506.10641'
source_url: https://arxiv.org/abs/2506.10641
tags:
- llms
- character
- token
- knowledge
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs can spell out tokens character-by-character accurately, but
  struggle with complex character-level tasks. Analysis shows token embeddings lack
  full character-level information beyond the first character, with later characters
  reconstructed in intermediate and higher Transformer layers.
---

# Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters

## Quick Facts
- **arXiv ID**: 2506.10641
- **Source URL**: https://arxiv.org/abs/2506.10641
- **Reference count**: 9
- **Primary result**: LLMs can spell out tokens character-by-character accurately, but struggle with complex character-level tasks

## Executive Summary
This paper investigates whether large language models (LLMs) can accurately spell out tokens character-by-character, revealing that while LLMs can perform this task, it is not a straightforward extraction of character-level information from token embeddings. The study finds that token embeddings lack full character-level information beyond the first character, with later characters being reconstructed in intermediate and higher Transformer layers rather than being directly available. A key discovery is the identification of a distinct "breakthrough" layer where character knowledge becomes reliably detectable.

The research demonstrates that spelling-out is a learned capability rather than simple information extraction, supported by multiple analytical approaches including probing classifiers, knowledge neuron analysis, and attention pattern examination. The findings are validated across four different LLMs (Llama-3-8B, Mistral-7B, Gemma-7B, and Qwen-7B), showing consistent patterns in how character-level information emerges through the model's layers.

## Method Summary
The authors employ multiple analytical techniques to investigate character-level information in LLMs. They use probing classifiers to detect character-level knowledge at different layers, conduct knowledge neuron analysis to identify specific neurons carrying character information, and examine attention patterns to understand how information flows through the model. The study evaluates four LLMs with similar model sizes (7B parameters) across English text, systematically analyzing how character information is represented and reconstructed through different layers of the Transformer architecture.

## Key Results
- Token embeddings contain only first-character information, with subsequent characters reconstructed in intermediate/higher layers
- A distinct "breakthrough" layer exists where character knowledge becomes reliably detectable across all evaluated models
- Knowledge neurons and attention patterns align with this breakthrough layer, confirming spelling-out is a learned task
- Probing classifiers, neuron-level analyses, and attention weight inspections validate findings across four LLMs

## Why This Works (Mechanism)
Spelling-out capability emerges through learned transformations in Transformer layers rather than direct extraction from token embeddings. The mechanism involves progressive reconstruction of character information through intermediate layers, with attention mechanisms and specific neurons developing the capacity to map token representations to their constituent characters. This learned capability is evidenced by the alignment between breakthrough layers identified through probing, knowledge neurons, and attention patterns.

## Foundational Learning
- **Tokenization fundamentals**: Understanding how text is split into tokens and the limitations of token-level representations is crucial for interpreting why character-level information must be reconstructed
- **Transformer layer dynamics**: Knowledge of how information transforms across layers explains why character knowledge emerges progressively rather than being present in embeddings
- **Probing classifier methodology**: This technique is essential for detecting latent knowledge in model representations at different layers
- **Knowledge neuron analysis**: Understanding how specific neurons encode particular types of information allows identification of character-related neurons
- **Attention mechanism patterns**: Analyzing attention weights reveals how information flows and is reconstructed through the model

## Architecture Onboarding
- **Component map**: Token embeddings -> Transformer layers (progressive reconstruction) -> Breakthrough layer -> Character-level output
- **Critical path**: Token embedding → Transformer layers → Breakthrough layer → Character reconstruction
- **Design tradeoffs**: The study reveals that token-level representations sacrifice complete character information for efficiency, requiring learned reconstruction mechanisms
- **Failure signatures**: Inability to reconstruct complex character patterns, particularly for characters beyond the first position
- **3 first experiments**: 1) Verify breakthrough layer exists across different model scales, 2) Test character reconstruction on non-English languages, 3) Conduct ablation studies on identified knowledge neurons

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the findings: whether these patterns generalize to larger models and different architectures, how the mechanism works for languages with different writing systems, and what specific computational mechanisms enable character reconstruction in intermediate layers.

## Limitations
- Analysis limited to only four LLMs with similar model sizes (7B parameters), raising questions about generalizability to larger models and different architectures
- Focus exclusively on English text, leaving open whether findings apply to languages with different orthographic or morphological properties
- Does not fully characterize the specific computational mechanisms or intermediate representations used for character reconstruction

## Confidence
- High confidence: Spelling-out is learned rather than directly extracted from embeddings, supported by convergent evidence from multiple analytical methods
- Medium confidence: Existence and precise location of breakthrough layer, as layer identification may vary with different probing methodologies
- Medium confidence: Generalization of findings to different model sizes, architectures, and languages due to limited scope of evaluation

## Next Checks
1. Evaluate the breakthrough layer phenomenon across a broader range of model scales (including frontier models) and architectures (including recurrent and convolutional models) to assess generalizability
2. Test the character-level reconstruction capabilities on multiple languages with different writing systems (logographic, agglutinative, and right-to-left scripts) to determine if the learned patterns are language-specific
3. Conduct ablation studies by modifying or removing specific attention heads or neurons identified as carrying character information to empirically verify their causal role in spelling-out capability