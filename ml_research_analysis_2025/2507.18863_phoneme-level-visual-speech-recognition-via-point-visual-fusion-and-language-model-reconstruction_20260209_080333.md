---
ver: rpa2
title: Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language
  Model Reconstruction
arxiv_id: '2507.18863'
source_url: https://arxiv.org/abs/2507.18863
tags:
- visual
- speech
- phonemes
- phoneme
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visual speech recognition (V-ASR),
  which is challenging due to the absence of auditory cues and visual ambiguity of
  phonemes (visemes). Existing methods often predict words or characters directly
  from visual cues, suffering from high error rates due to viseme ambiguity and requiring
  large amounts of pre-training data.
---

# Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction

## Quick Facts
- arXiv ID: 2507.18863
- Source URL: https://arxiv.org/abs/2507.18863
- Reference count: 36
- Primary result: Achieves 17.4% WER on LRS2 and 21.0% WER on LRS3 using phoneme-level two-stage fusion framework

## Executive Summary
This paper addresses the challenge of visual speech recognition (V-ASR) by proposing a two-stage framework that first predicts phonemes from visual input and then reconstructs them into words using a language model. The approach tackles the inherent ambiguity of visemes (visually identical phonemes) by leveraging linguistic context for disambiguation. The method combines visual appearance features with geometric landmark motion features through a point-visual fusion mechanism, achieving state-of-the-art results on the LRS2 and LRS3 datasets.

## Method Summary
The proposed method uses a two-stage architecture: Stage 1 employs a V-ASR encoder that fuses visual appearance features (from 3D CNN + ResNet-18 + Conformer) with facial landmark motion features (from ST-GCN + Conformer) via an MLP layer, outputting predicted phonemes with hybrid CTC/Attention loss. Stage 2 uses NLLB-1.3B encoder-decoder to reconstruct phoneme sequences back to words, trained with error-augmented phoneme-to-text pairs. The system processes 96×96 mouth ROI crops and 117 facial landmarks per frame, targeting 39 ARPAbet phoneme classes plus special tokens.

## Key Results
- Achieves 17.4% WER on LRS2 dataset
- Achieves 21.0% WER on LRS3 dataset
- PV-ASR outperforms V-ASR on LRS2 (17.4% vs 17.7% WER)
- NLLB outperforms BART (27.9% WER) and decoder-only models (30.4% WER) on LRS2

## Why This Works (Mechanism)

### Mechanism 1: Phoneme-Level Intermediate Representation
The decomposition into visual-to-phoneme mapping followed by phoneme-to-text reconstruction reduces training complexity while providing interpretable intermediate representations. Each stage specializes: Stage 1 learns visual-phonetic correspondences across 39 phoneme classes, while Stage 2 leverages linguistic knowledge for disambiguation. This works because phonemes preserve acoustic information while being learnable from visual cues.

### Mechanism 2: Dual-Stream Landmark-Visual Feature Fusion
Fusing appearance-based visual features with geometric landmark features improves robustness to speaker-specific facial characteristics and lighting variations. The V-ASR encoder captures pixel-level appearance and temporal dynamics, while the P-ASR encoder models structural geometry of lip-region landmarks. These complementary streams are fused via MLP, enabling the model to leverage both texture and articulatory motion patterns.

### Mechanism 3: Encoder-Decoder LLM for Contextual Denoising and Reconstruction
Treating phoneme sequences as a "low-resource symbolic language" and using an encoder-decoder LLM enables error-tolerant sentence reconstruction. NLLB's bidirectional encoder processes the entire phoneme sequence with full attention, capturing contextual dependencies that help identify and correct phoneme-level errors. The decoder then generates fluent text, with error injection during training further improving robustness.

## Foundational Learning

- **Concept: Viseme-to-Phoneme Many-to-One Mapping**
  - Why needed here: Understanding that multiple phonemes (e.g., /p/, /b/, /m/) produce visually identical lip shapes is essential for grasping why direct visual-to-word prediction fails and why a phoneme-intermediate approach with language model disambiguation is proposed.
  - Quick check question: Why can't a purely visual model distinguish "back" from "pack"?

- **Concept: Hybrid CTC/Attention Architecture**
  - Why needed here: The paper uses both CTC loss (for alignment-free temporal supervision) and cross-entropy loss (for attention-based decoding). Understanding this trade-off is critical for implementing Stage 1 training correctly.
  - Quick check question: What does CTC loss provide that cross-entropy alone does not?

- **Concept: Encoder-Decoder vs. Decoder-Only LLMs**
  - Why needed here: The paper explicitly chooses NLLB (encoder-decoder) over LLaMA (decoder-only), claiming bidirectional attention in the encoder provides better denoising. This architectural distinction affects model selection and training strategy.
  - Quick check question: Why might an encoder-decoder model handle noisy phoneme inputs better than a decoder-only model?

## Architecture Onboarding

- **Component map**: Video frames → 96×96 mouth ROI (V-ASR) + MediaPipe 117 lip landmarks (P-ASR) → 3D CNN + ResNet-18 front-end → Conformer temporal backend → ST-GCN (6 blocks) + Conformer → MLP fusion → CTC/Attention decoder → 39 phoneme classes + `<unk>`, `<space>` → NLLB-1.3B encoder-decoder → English sentences

- **Critical path**:
  1. Video frames simultaneously feed V-ASR and P-ASR encoders
  2. Both streams frozen during initial fine-tuning (visual feature extractor weights from pre-training)
  3. Fusion + CTC/Attention produces phoneme sequence predictions
  4. NLLB trained separately with error-augmented phoneme-text pairs
  5. Inference: Stage 1 → phonemes → NLLB beam search → final sentence

- **Design tradeoffs**:
  - PV-ASR outperforms V-ASR on LRS2 (17.4% vs 17.7% WER) but underperforms on LRS3 (21.0% vs 18.9% WER) due to landmark detection failures in longer/more complex videos
  - NLLB-1.3B chosen over BART (lower WER) and decoder-only models (better denoising)
  - 10-20% error injection during LLM training balances robustness vs. overfitting to noise

- **Failure signatures**:
  - Profile/occluded faces → zero-padded landmarks → PV-ASR WER degradation
  - Phoneme segmentation errors → LLM outputs grammatically correct but semantically wrong words (e.g., "WE'LL" vs "WE'D")
  - Viseme confusion persisting through LLM (e.g., "FLOAT" vs "FLAWED") when context insufficient

- **First 3 experiments**:
  1. **Baseline validation**: Run V-ASR (visual-only) and PV-ASR (fused) on LRS2/LRS3 test sets to reproduce reported WERs and identify which stream contributes more per dataset.
  2. **Ablation on error injection**: Train NLLB with 0%, 10%, and 20% phoneme error augmentation to verify optimal augmentation level and confirm BART's >25% WER without augmentation.
  3. **Landmark quality analysis**: Manually inspect LRS3 failure cases where MediaPipe detection fails to quantify the proportion of zero-padded frames and their correlation with WER degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning face detection model parameters effectively minimize undetected facial points in challenging scenes (e.g., profile views in LRS3), thereby lowering the PV-ASR Word Error Rate (WER) to match or beat the V-ASR baseline? The current PV-ASR method underperforms the V-ASR method on LRS3 (21.0% vs 18.9% WER), which the authors attribute to MediaPipe failures on profile or occluded frames causing zero-padding noise.

### Open Question 2
Does pre-training the NLLB model on a Wikipedia corpus prior to fine-tuning on LRS2/LRS3 yield a significant reduction in Word Error Rate compared to the current direct fine-tuning approach? While BART underwent curriculum training, the NLLB models did not; the potential benefit of additional textual pre-training for the NLLB component remains unverified in the current results.

### Open Question 3
Can this two-stage phoneme-centric framework be successfully generalized to other languages, such as German or Japanese, leveraging NLLB's multilingual pre-training capabilities? The study is currently restricted to English. It is unknown if the phoneme-to-text reconstruction robustness transfers to languages with different phoneme sets, viseme-to-phoneme mappings, or syntactic structures.

## Limitations

- **Landmark detection reliability**: Significant performance gap on LRS3 (21.0% vs 18.9% WER) suggests MediaPipe FaceMesh failures under profile views and occlusion, with fusion mechanism effectiveness depending on consistent landmark quality.
- **Ablation depth**: Lacks systematic ablations on fusion architecture (MLP dimensions), error augmentation rates, or alternative encoder-decoder models; 10-20% error injection rate appears heuristic.
- **Phoneme segmentation ambiguity**: Acknowledges but doesn't quantify how often phoneme segmentation errors propagate through NLLB, with robustness claims relying on qualitative examples rather than systematic error analysis.

## Confidence

- **High confidence**: The two-stage decomposition mechanism (phoneme prediction + LLM reconstruction) is well-supported by the 17.4%/21.0% WER results and explicit error analysis examples showing LLM denoising.
- **Medium confidence**: The dual-stream fusion architecture's benefits are partially validated (marginal LRS2 gains) but undermined by LRS3 degradation, suggesting dataset-specific limitations not fully characterized.
- **Low confidence**: The optimality of NLLB-1.3B choice and 10-20% error augmentation rate lacks comparative ablation studies against other encoder-decoder models or augmentation strategies.

## Next Checks

1. **Landmark quality impact analysis**: Quantify MediaPipe detection failure rates across LRS2/LRS3 and correlate with WER degradation. Visualize landmark coverage distribution and test alternative face tracking methods.

2. **Systematic error propagation study**: Analyze Stage 1 phoneme error types (substitution/deletion/insertion) and their frequency of correction by NLLB. Measure WER sensitivity to specific phoneme confusions (viseme pairs like /p/-/b/-/m/).

3. **Fusion architecture ablation**: Compare MLP fusion dimensions, attention-based fusion, and single-stream baselines on both datasets to isolate architectural contributions beyond pre-trained weights.