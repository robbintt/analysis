---
ver: rpa2
title: 'Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy
  Regulation'
arxiv_id: '2510.02249'
source_url: https://arxiv.org/abs/2510.02249
tags:
- uni00000013
- uni00000011
- uni00000048
- reasoning
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overthinking problem in large language
  models, where they generate unnecessarily long reasoning chains even for simple
  problems, reducing efficiency and adaptability. The authors introduce Token Entropy
  Cumulative Average (TECA), a metric that measures exploration extent throughout
  the reasoning process by averaging token entropy.
---

# Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation

## Quick Facts
- arXiv ID: 2510.02249
- Source URL: https://arxiv.org/abs/2510.02249
- Authors: Tianyi Jiang; Yi Bin; Yujuan Ding; Kainian Zhu; Fei Ma; Jingkuan Song; Heng Tao Shen
- Reference count: 40
- Primary result: Reduces response length by up to 71% on simpler datasets without sacrificing accuracy

## Executive Summary
This paper addresses the overthinking problem in large language models, where they generate unnecessarily long reasoning chains even for simple problems, reducing efficiency and adaptability. The authors introduce Token Entropy Cumulative Average (TECA), a metric that measures exploration extent throughout the reasoning process by averaging token entropy. Based on this, they propose a novel "Explore Briefly, Then Decide" paradigm with a Cumulative Entropy Regulation (CER) mechanism. CER uses TECA to dynamically guide models to conclude their reasoning process at the optimal point, suppressing excessive exploration while preserving necessary reasoning ability.

Experiments on math benchmarks show that CER effectively reduces response length—by up to 71% on simpler datasets—without sacrificing accuracy, and outperforms existing methods. The method demonstrates adaptive reasoning length adjustment based on problem complexity, addressing overthinking by teaching models to briefly explore and then determine answers efficiently.

## Method Summary
The method introduces TECA as a metric for measuring exploration extent during reasoning, calculated as the cumulative average of token entropy throughout generation. Using the VERL framework with GRPO and LoRA fine-tuning, CER implements a custom reward combining accuracy with TECA-based exploration rewards. The segmented reward structure gives full accuracy reward for incorrect answers but combines accuracy and TECA rewards only for correct answers, encouraging models to explore briefly and then decide decisively. The approach is evaluated on GSM8K, MATH500, AIME24, and AIME25 benchmarks using Qwen3-4B and Qwen3-8B models.

## Key Results
- Reduces response length by up to 71% on simpler datasets while maintaining accuracy
- Outperforms existing methods in balancing reasoning efficiency and accuracy
- Demonstrates adaptive reasoning length adjustment based on problem complexity
- Shows effectiveness across multiple math benchmarks (GSM8K, MATH500, AIME24, AIME25)

## Why This Works (Mechanism)
The CER mechanism works by introducing a novel exploration metric (TECA) that quantifies how much the model is exploring versus concluding during reasoning. By rewarding models for lower final TECA values when correct, the approach encourages models to explore briefly and then commit to answers rather than engaging in unnecessarily long reasoning chains. The segmented reward structure ensures that exploration is only encouraged when it leads to correct answers, preventing premature termination. This addresses overthinking by teaching models to recognize when sufficient exploration has occurred and when to conclude reasoning.

## Foundational Learning
**Token Entropy Calculation**: Measures uncertainty in token predictions using H_t = -Σ p_{t,j} log p_{t,j} from softmax logits
- Why needed: Quantifies exploration vs exploitation behavior during reasoning
- Quick check: Should increase initially during exploration phase then decrease as model converges to answer

**Cumulative Average Metrics**: TECAt = (1/t) Σ_{s=1}^{t} H_s computes running average of token entropy
- Why needed: Provides smoothed measure of exploration extent over entire reasoning process
- Quick check: Should show initial rise during exploration, peak, then decline as reasoning concludes

**Reinforcement Learning with Verifiable Rewards (VERL)**: Framework for optimizing models with reward signals that can be automatically verified
- Why needed: Enables training with custom rewards like TECA that require generation completion
- Quick check: Should converge to policy that maximizes both accuracy and TECA-based exploration rewards

**Segmented Reward Structures**: Different reward calculations based on correctness outcome
- Why needed: Ensures exploration is only rewarded when it leads to correct answers
- Quick check: Incorrect answers should only receive accuracy-based reward, correct answers should combine both

**GRPO (Group Relative Policy Optimization)**: Variant of PPO for RLHF that optimizes groups of samples together
- Why needed: Enables stable policy updates with custom reward structures
- Quick check: KL divergence should remain stable during training to prevent reward hacking

## Architecture Onboarding

Component Map:
Qwen3-4B/8B model -> LoRA adapters (rank=64, alpha=32) -> VERL framework -> GRPO optimizer -> Custom CER reward manager -> TECA computation module

Critical Path:
During inference, model generates tokens with associated entropy values. TECA is computed in real-time as cumulative average. At generation end, CER reward manager calculates r_te = e^(-TECA_{-1}) + 1. Segmented reward combines accuracy and TECA only for correct answers. This reward flows back through GRPO to update LoRA parameters.

Design Tradeoffs:
- Exploration vs efficiency: Higher TECA rewards encourage more exploration but increase response length
- Reward balance: Too much emphasis on TECA can cause premature answers, too little maintains overthinking
- Model capacity: Larger models may require different TECA thresholds for optimal performance

Failure Signatures:
- TECA reward dominates: Model terminates prematurely, accuracy drops significantly
- Entropy collapse: TECA remains flat low throughout, indicating skipped exploration entirely
- Inconsistent behavior: Different response lengths for similar difficulty problems

First Experiments:
1. Run TECA computation on baseline model outputs to establish typical exploration patterns
2. Test segmented reward structure with synthetic data to verify correct reward assignment
3. Evaluate CER on a small subset of GSM8K with manual TECA verification

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to math benchmarks, raising questions about generalizability to other reasoning domains
- Method relies on custom reward engineering which may require problem-specific tuning
- TECA as an exploration metric may not capture all aspects of reasoning quality beyond math problems

## Confidence
High confidence in methodological framework and implementation details provided
Medium confidence in empirical results due to narrow evaluation scope
Medium-Low confidence in claims about broad applicability beyond math problems

## Next Checks
1. Test CER on non-math reasoning tasks (e.g., commonsense reasoning, code generation) to verify generalizability of the overthinking mitigation approach
2. Conduct ablation studies removing the TECA component to isolate its specific contribution versus the reinforcement learning framework itself
3. Compare against alternative exploration metrics (e.g., variance in intermediate reasoning steps, attention entropy) to establish TECA's relative effectiveness