---
ver: rpa2
title: Integration of LLM Quality Assurance into an NLG System
arxiv_id: '2501.16078'
source_url: https://arxiv.org/abs/2501.16078
tags:
- text
- system
- quality
- correct
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an LLM-based Quality Assurance system that
  identifies and suggests corrections for grammatical and spelling errors in NLG-generated
  text. The system uses an LLM to analyze text and link corrections back to the original
  rule set via unique IDs, enabling users to make informed decisions about applying
  suggested fixes.
---

# Integration of LLM Quality Assurance into an NLG System

## Quick Facts
- **arXiv ID**: 2501.16078
- **Source URL**: https://arxiv.org/abs/2501.16078
- **Reference count**: 29
- **Primary result**: LLM-based QA system achieves 94-98% precision in identifying ungrammatical text but shows language-dependent recall (55-92%) and mixed improvement authenticity (31-62%)

## Executive Summary
This paper presents an LLM-based Quality Assurance system that identifies and suggests corrections for grammatical and spelling errors in NLG-generated text. The system uses an LLM to analyze text and link corrections back to the original rule set via unique IDs, enabling users to make informed decisions about applying suggested fixes. Evaluated on basketball match reports in three languages (French, German, Spanish), the system achieved high precision (94-98%) and moderate recall (55-92%) in identifying ungrammatical text. While suggestion quality was rated highly (89-95), the authenticity of revisions was lower (31-62%), indicating a need for improvement. The authors propose a human-in-the-loop workflow to address these limitations and plan future enhancements like dynamic prompts and additional quality checks.

## Method Summary
The system uses GPT-4 to evaluate NLG-generated text by identifying grammar and spelling errors and suggesting corrections linked to original rule set IDs. The approach processes basketball match reports in French, German, and Spanish (10 reports per language), using an LLM QA system with temperature 0.5 and an LLM Evaluator with temperature 0. The system outputs structured JSON corrections with ID mappings and container-level explanations. Performance is measured through precision and recall for error detection, suggestion quality ratings (0-100), and improvement proportion (share of revisions judged better than original). The evaluation relies on LLM self-evaluation without human validation.

## Key Results
- High precision in error detection: 94-98% across all three languages
- Language-dependent recall: 55% (German), 85% (Spanish), 92% (French)
- Strong suggestion quality: 89-95% average ratings
- Mixed improvement authenticity: 31-62% of revisions judged as improvements
- Systematic hallucination issues, particularly in German, where over-correction of error-free text occurred

## Why This Works (Mechanism)

### Mechanism 1: Source-Level Traceability via Unique IDs
Linking LLM corrections to original rule set IDs enables fixing errors at their source, improving all downstream generated texts rather than individual instances. The NLG system structures text as "variable units" with unique IDs, and the LLM QA system maps problematic sections back to these IDs, allowing users to modify the rule configuration itself.

### Mechanism 2: Constrained LLM Output Format
Enforcing structured JSON output with specific fields (revised_text, explanation, intention, ID mapping) reduces hallucination and enables systematic processing. The prompt explicitly instructs the LLM to map corrections to structural IDs, creating accountability and traceability.

### Mechanism 3: Human-in-the-Loop Gatekeeping
Keeping humans as final arbiters addresses the authenticity gap where LLM suggestions score high on quality but lower on actual improvement. The system positions itself as an "editing assistant" rather than automated correction, with users reviewing and accepting/rejecting suggestions.

## Foundational Learning

- **Concept: NLG Pipeline Stages (Document Planning → Micro Planning → Surface Realization)**
  - **Why needed here**: The system operates at the Surface Realization stage. Understanding where grammar/spelling issues arise helps identify what can be fixed via rule modifications.
  - **Quick check question**: If an NLG system produces factually incorrect statements, would fixing it at the Surface Realization stage address the problem?

- **Concept: Variable Units and Agreement in NLG**
  - **Why needed here**: The system uses "variable units" with configurable grammar features that must agree across the text. Errors often come from agreement failures between data-driven variables and template text.
  - **Quick check question**: In the sentence "Fortuna won with 2 goals against Borussia," which words come from variable units and which require grammatical agreement configuration?

- **Concept: LLM-as-Evaluator (Self-Evaluation)**
  - **Why needed here**: The system uses GPT-4 to evaluate GPT-4 outputs. Understanding the limitations of LLM self-evaluation is critical for interpreting reported metrics.
  - **Quick check question**: What are two risks of using the same LLM family for both generation and evaluation?

## Architecture Onboarding

- **Component map**: NLG System -> LLM QA System (GPT-4, temp=0.5) -> LLM Evaluator (GPT-4, temp=0) -> Human Review Interface -> Rule Set Updater

- **Critical path**: Generate representative text subset → Extract rule set JSON with variable units and IDs → Prompt LLM QA with text + rule set → Structured correction JSON → Run LLM Evaluator for metrics → Present corrections to human reviewer → Propagate accepted fixes to rule set

- **Design tradeoffs**:
  - Single vs. language-specific prompts: Current system uses same prompt for all languages; German shows lower recall (55%) vs. French (92%) and Spanish (85%)
  - Temperature settings: QA system uses 0.5 (more creative), Evaluator uses 0 (deterministic)
  - Automation level: Human-in-the-loop increases reliability but limits scalability
  - Evaluation method: LLM-as-evaluator enables rapid iteration but lacks human validation

- **Failure signatures**:
  - Low recall (German: 55%): Many ungrammatical sentences not detected
  - Low improvement proportion (German: 31%): Revisions not perceived as improvements
  - Hallucination example: LLM claims "seiner" must agree with "masculine genitive" when "Leistung" is feminine dative
  - Over-correction: LLM revises error-free text sections

- **First 3 experiments**:
  1. Language-specific prompt optimization: Create tailored prompts for each language with native speaker-validated grammar rules
  2. Few-shot prompting for revision authenticity: Include 2-3 examples of high-quality revisions in the prompt
  3. Human vs. LLM evaluator alignment: Compare LLM Evaluator judgments against human annotations on 30 sentences

## Open Questions the Paper Calls Out

### Open Question 1
Does a full human-in-the-loop workflow effectively improve text quality through iterative feedback? The current evaluation is limited to a single correction task and does not measure the cumulative effect of user feedback on subsequent text generations.

### Open Question 2
Can the proposed framework successfully evaluate and correct text dimensions beyond grammar and spelling? The current system architecture and prompts are fine-tuned specifically for correctness, and it is unclear if the same approach transfers to subjective stylistic dimensions.

### Open Question 3
To what extent does dynamic, language-specific prompting improve the recall and revision authenticity of the system? The paper identifies the correlation between static prompts and poor performance but does not experimentally validate the proposed optimization.

## Limitations

- Performance varies significantly across languages, with German showing substantially lower recall (55%) and improvement authenticity (31%) compared to French and Spanish
- The LLM-as-evaluator approach lacks human validation, raising questions about the reliability of reported metrics
- The system's effectiveness depends on the quality and structure of the underlying rule set, which isn't fully specified in the paper
- Hallucination issues persist, with the LLM suggesting corrections to error-free text and providing grammatically incorrect explanations

## Confidence

- **High confidence**: The mechanism of linking corrections to rule set IDs via unique identifiers is technically sound and well-demonstrated
- **Medium confidence**: The overall approach of using LLM for QA in NLG systems is valid, but language-specific performance issues suggest the current implementation needs refinement
- **Low confidence**: The evaluation methodology using LLM self-evaluation, while practical, lacks human validation to confirm metric reliability

## Next Checks

1. **Human evaluator validation**: Compare LLM Evaluator judgments against human annotations on 30-50 sentences across all three languages to establish ground truth reliability and identify systematic evaluation biases

2. **Language-specific prompt optimization**: Develop and test tailored prompts for each language with native speaker input, measuring recall improvement and hallucination reduction compared to the current single-prompt approach

3. **A/B testing with human-in-the-loop**: Conduct a controlled experiment where one group uses the system with human review while another uses automated correction, measuring actual quality improvement and user trust levels