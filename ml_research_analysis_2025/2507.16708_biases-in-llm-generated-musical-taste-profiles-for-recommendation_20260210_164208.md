---
ver: rpa2
title: Biases in LLM-Generated Musical Taste Profiles for Recommendation
arxiv_id: '2507.16708'
source_url: https://arxiv.org/abs/2507.16708
tags:
- user
- profiles
- music
- genre
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Model (LLM)-generated user
  profiles for music recommendation, focusing on whether users perceive them as accurate
  representations of their tastes. Profiles were generated from listening histories
  using three LLMs (Llama, DeepSeek, Gemini) across four time windows (30-365 days).
---

# Biases in LLM-Generated Musical Taste Profiles for Recommendation

## Quick Facts
- **arXiv ID**: 2507.16708
- **Source URL**: https://arxiv.org/abs/2507.16708
- **Reference count**: 40
- **Primary result**: User ratings of LLM-generated music profiles vary significantly by model and user characteristics, with specialist users and older listeners rating profiles higher, while downstream recommendation performance correlates weakly with perceived accuracy.

## Executive Summary
This study evaluates how Large Language Models generate user taste profiles from music listening histories and whether users perceive these profiles as accurate representations of their preferences. Using three different LLMs (Llama, DeepSeek, Gemini) and four time windows (30-365 days), the research found significant model-specific biases in profile quality. Specialist users with concentrated preferences rated profiles more positively than generalists, and older users/listeners of older music consistently gave higher ratings. The study revealed a concerning decoupling between user-perceived profile accuracy and actual recommendation performance, suggesting that profiles feeling "accurate" to users may not translate to better algorithmic recommendations. These findings highlight fairness concerns and the need for debiasing strategies in LLM-based recommendation systems.

## Method Summary
The study used Deezer user listening histories (1 year from April 1, 2024) with streams >30s to generate natural language profiles. A 2-step sampling strategy selected top-n artists then top tracks per artist (15 pairs total). Three LLMs (Llama 3.2, DeepSeek-R1, Gemini 2.0 Flash) generated profiles using a metadata prompt with temperature 0.8. User ratings (1-7 Likert scale) were collected from 64 participants. Analysis included linear regression with 5K bootstrap, two-way ANOVA, and Doubly Robust ATE estimation. Downstream evaluation used a bi-encoder trained on 100K triplets to compute recall@10 and NDCG@10, which were correlated with user ratings.

## Key Results
- User ratings varied significantly by LLM model, with Llama profiles rated highest
- Specialist users (high GS-score) rated profiles more positively than generalists
- Older users and listeners of older music rated profiles higher
- Genre and country characteristics biased profile quality (Rap received negative ATE, Metal positive)
- Downstream recommendation performance correlated weakly with user ratings
- Time window (30-365 days) had minimal impact on profile quality

## Why This Works (Mechanism)

### Mechanism 1
Profile recognition is biased by user preference concentration; "specialist" users rate profiles higher than "generalists." Specialists have narrower, more cohesive preferences that are effectively captured by a fixed-size item sample (n=15). Generalists have diverse tastes; a fixed sample fails to represent the full variance, leading to lower perceived accuracy. If the number of sampled items (n) is increased significantly and generalist ratings do not improve, the mechanism implies an LLM summarization limit rather than a sampling limit.

### Mechanism 2
LLMs exhibit model-specific content biases (genre/country) that affect user perception independent of the user's actual taste. LLMs inherit priors from training data. The study finds that profiles with high "Rap" content systematically receive lower ratings while "Metal" receives higher ratings, even after controlling for user preference. This suggests the LLM generates descriptions for these genres that users find less representative or lower quality. If a different LLM reverses these genre biases, the mechanism is likely training-data specific rather than structural to the task.

### Mechanism 3
Perceived profile accuracy (user rating) is decoupled from downstream recommendation utility (Recall/NDCG). "Human-readable" accuracy relies on explicit entity matching. "Algorithmic utility" relies on embedding similarity in a latent space. A profile that feels accurate may lack the semantic nuance to act as an effective query for retrieval. If an LLM is fine-tuned specifically to optimize Recall@10 and user ratings subsequently drop, the decoupling is confirmed.

## Foundational Learning

- **Concept: Generalist-Specialist Score (GS-Score)**
  - Why needed here: To diagnose why profiles fail for certain users. The paper uses this to prove that low ratings aren't randomâ€”they correlate with taste diversity.
  - Quick check question: Does the system perform worse for users with high entropy in their listening history?

- **Concept: Doubly Robust Estimation (ATE)**
  - Why needed here: To distinguish between "User likes Rap" (taste) and "LLM writes bad Rap summaries" (model bias). This causal inference method isolates the model's contribution to the rating.
  - Quick check question: Is the low rating for a genre caused by the user's preference or the model's handling of that genre?

- **Concept: Implicit vs. Explicit Feedback**
  - Why needed here: The system relies on streaming history (implicit, noisy) rather than likes/dislikes. This necessitates the "2-step sampling strategy" to filter signal from noise.
  - Quick check question: How does the system define a "positive interaction" to feed the LLM? (Answer: >30s play count).

## Architecture Onboarding

- **Component map**: Input (User Listening History) -> Preprocessor (2-Step Sampler) -> Generator (LLM with Metadata Prompt) -> Evaluator A (Human Rating) -> Evaluator B (Bi-Encoder for Recommendation)
- **Critical path**: The **Sampling Strategy**. Feeding raw history exceeds context limits and introduces noise. The move from "random tracks" to "top tracks per top artist" stabilizes the LLM input.
- **Design tradeoffs**:
  - Time Windows: 30 vs 365 days had "minimal impact" on ratings. You likely do not need complex temporal segmentation for static profiles.
  - Model Selection: Llama yielded highest user ratings but DeepSeek/Llama had different bias profiles. No single model dominated all metrics.
- **Failure signatures**:
  - Hallucinations: LLMs inventing track names or misattributing genres (noted in user feedback).
  - The "Generalist Gap": Low ratings for users with low GS-scores (diverse tastes).
  - Genre Penalization: Unexpectedly low ratings for Rap/World music profiles compared to Rock/Metal.
- **First 3 experiments**:
  1. Run the ATE analysis on your current LLM generator to identify which genres or user demographics are systematically penalized.
  2. Test n=15 vs n=30 sampled items specifically for "Generalist" users (low GS-score) to see if ratings improve.
  3. Correlate user ratings with CTR or dwell time in a live setting to verify if the "weak correlation" with offline Recall holds in production.

## Open Questions the Paper Calls Out

### Open Question 1
Does decoupling profile generation for user-facing transparency from backend recommendation optimization effectively resolve the misalignment between perceived accuracy and algorithmic utility? The study identified the misalignment (weak correlation between ratings and recall@10) but did not test the proposed decoupling strategy. A comparative user study measuring trust and recommendation quality in systems using coupled vs. decoupled profiles would resolve this.

### Open Question 2
Do the observed negative biases against specific genres, such as rap, stem primarily from deficiencies in item metadata quality or from intrinsic biases within the LLMs? The Doubly Robust estimation isolated the effect (negative ATE for rap) but could not disentangle the source (input data vs. model processing). An ablation study using standardized, high-quality metadata for the same set of items across multiple models would resolve this.

### Open Question 3
Can targeted fine-tuning using user-corrected feedback effectively reduce the representation gap between specialist and generalist users? The study quantified the bias favoring specialists but did not implement or evaluate remediation techniques like feedback loops. Iterative experiments where users edit profiles, measuring if the rating gap between user groups narrows after model refinement, would resolve this.

## Limitations

- Study relies on a user sample (N=64) that may not fully represent global music listening populations
- GS-score computation depends on Deezer-specific playlist co-occurrence data not fully specified
- Weak correlation between user ratings and algorithmic performance raises questions about profile utility
- Cultural context differences that might affect profile interpretation across regions were not accounted for

## Confidence

- **High confidence**: Model-specific bias findings (Llama outperforming others, genre-based ATE variations) are well-supported by statistical analysis
- **Medium confidence**: Generalist-specialist mechanism is theoretically sound but relies on assumptions about sampling strategy limits
- **Medium confidence**: Decoupling of perceived accuracy from algorithmic utility is observed but practical implications remain unclear

## Next Checks

1. Apply the Doubly Robust ATE estimation framework to your own LLM-generated profiles to identify genre and demographic biases specific to your user base
2. Test whether increasing the 15-item sampling limit for generalist users (low GS-score) improves profile ratings, distinguishing between sampling vs. summarization limitations
3. Deploy profiles in a live recommendation setting and measure correlation between user ratings and actual engagement metrics (CTR, dwell time) to validate whether the "weak correlation" with offline metrics persists in production