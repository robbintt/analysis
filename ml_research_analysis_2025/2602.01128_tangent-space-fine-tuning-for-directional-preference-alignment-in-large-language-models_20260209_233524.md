---
ver: rpa2
title: Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language
  Models
arxiv_id: '2602.01128'
source_url: https://arxiv.org/abs/2602.01128
tags:
- preference
- ts-dpo
- alignment
- helpfulness
- verbosity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tangent-Space Direct Preference Optimization
  (TS-DPO), a method for enabling large language models to balance multiple human
  preference dimensions such as helpfulness, safety, and verbosity through principled
  and controllable alignment. Unlike existing methods that collapse feedback into
  a single scalar reward, TS-DPO learns separate update directions in the model's
  tangent space for each preference axis.
---

# Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models

## Quick Facts
- **arXiv ID**: 2602.01128
- **Source URL**: https://arxiv.org/abs/2602.01128
- **Reference count**: 5
- **Primary result**: TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO for multi-objective alignment.

## Executive Summary
This paper introduces Tangent-Space Direct Preference Optimization (TS-DPO), a method for enabling large language models to balance multiple human preference dimensions such as helpfulness, safety, and verbosity through principled and controllable alignment. Unlike existing methods that collapse feedback into a single scalar reward, TS-DPO learns separate update directions in the model's tangent space for each preference axis. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO.

## Method Summary
TS-DPO freezes a pretrained model and learns update directions (τ vectors) in the tangent space for each preference axis. These directions are optimized independently using DPO loss on separate datasets. At inference, the final model parameters are computed as a linear combination of the base parameters and the learned tangent directions, enabling continuous control over preference trade-offs without retraining. The method uses first-order linearization to ensure that preference directions combine additively in function space.

## Key Results
- TS-DPO achieves higher helpfulness at comparable verbosity compared to Task-Vector DPO
- Produces smooth and well-behaved Pareto frontiers with monotonic trade-off behavior
- Canonical Correlation Analysis shows TS-DPO produces more disentangled preference directions than standard DPO
- Broader coverage of Pareto-optimal points compared to scalarized approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-order linearization of the model around base parameters enables additive composition of preference directions.
- Mechanism: By constraining updates to the tangent space via f(x; θ₀ + Δθ) ≈ f(x; θ₀) + J_θ₀(x)Δθ, the learned directions behave as linear vectors that combine predictably rather than interacting non-linearly in weight space.
- Core assumption: The loss landscape near the pretrained weights has sufficiently low curvature that first-order approximation remains valid across the range of preference interpolations.
- Evidence anchors:
  - [abstract] "linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks"
  - [Section 3.1] "preference learning can be expressed entirely in terms of the update direction Δθ when base model is frozen"
  - [corpus] Weak direct corpus support; related work (Ortiz-Jimenez et al. 2023) is cited but not in provided neighbors
- Break condition: Large extrapolation coefficients (λ > 3 in Affine-2) show degraded helpfulness, suggesting the linear approximation breaks down far from the base point.

### Mechanism 2
- Claim: Training separate tangent directions per preference objective yields disentangled, near-orthogonal parameter updates.
- Mechanism: Each objective is optimized independently with its own dparams, constrained to act through the same frozen Jacobian, encouraging the model to find distinct functional pathways for each preference dimension.
- Core assumption: Preferences can be meaningfully decomposed into independent axes that do not require shared representation.
- Evidence anchors:
  - [Section 4.4] "TS-DPO consistently achieves even lower similarity [between helpfulness and verbosity], implying a cleaner separation"
  - [Section 4.4, Figure 3] "TS-DPO spectrum exhibits a faster decay than DPO... indicating sharper and more disentangled functional separation"
  - [corpus] "D-STEER" paper describes DPO as "low rank steering mechanism"—consistent with the idea that preference updates occupy constrained subspaces
- Break condition: If preference axes are inherently correlated (e.g., helpfulness requires some verbosity), forcing orthogonality may limit achievable performance.

### Mechanism 3
- Claim: Linear interpolation of tangent-space directions produces smooth, predictable traversal of the Pareto frontier at inference.
- Mechanism: The final model θ(λ) = θ₀ + λ₁τ_help + λ₂τ_verb can be evaluated with any (λ₁, λ₂) without retraining, turning preference control into a continuous parameter selection problem.
- Core assumption: The combination coefficients λ map monotonically to behavioral intensity along each preference axis.
- Evidence anchors:
  - [Section 4.3] "TS-DPO curves produce higher helpfulness for comparable or lower verbosity... Increasing λ moves the TS-DPO models smoothly along the frontier"
  - [abstract] "linearly combined at inference to generate user-specified behaviors without additional optimization"
  - [corpus] "Robust Preference Alignment via Directional Neighborhood Consensus" frames preferences as "high-dimensional vector where different directions represent trade-offs"—conceptually aligned
- Break condition: Task-Vector DPO (direct weight combination without tangent-space training) shows "non-linear behavior" and "discontinuous" movement, indicating that composition fails without the linearization constraint.

## Foundational Learning

- Concept: **Tangent space and first-order Taylor expansion**
  - Why needed here: The entire method hinges on approximating the model as locally linear around θ₀. Without this, composition of updates has no theoretical justification.
  - Quick check question: Given a function f(θ), write the first-order approximation at θ₀. What does the Jacobian J_θ₀ represent geometrically?

- Concept: **Jacobian-vector products (JVP) vs. full Jacobian materialization**
  - Why needed here: The implementation computes J_θ₀·v efficiently without storing J. Understanding this is essential for debugging and computational cost analysis.
  - Quick check question: Why is JVP O(forward-pass) while materializing J is O(params × outputs)?

- Concept: **Pareto optimality in multi-objective optimization**
  - Why needed here: The paper's core evaluation is coverage of the Pareto frontier. You must understand why scalarization (single reward) yields only one point, not the full trade-off surface.
  - Quick check question: If you scalarize objectives as r = α·r₁ + (1-α)·r₂ and sweep α from 0 to 1, are you guaranteed to recover all Pareto-optimal points? Why or why not?

## Architecture Onboarding

- Component map: Frozen Base Model (θ₀) -> LinearizedModel wrapper (functorch) -> Forward pass: f(x) + JVP(func0, params0, dparams) -> DPO loss per preference axis -> Inference: θ(λ) = θ₀ + λ₁τ_help + λ₂τ_verb

- Critical path:
  1. Convert pretrained model to functional form via `make_functional_with_buffers`
  2. Initialize `dparams` as zero-valued `nn.ParameterList`
  3. Implement forward as `out + JVP contribution`
  4. Train two separate models (or two `dparams` sets) on disjoint preference datasets
  5. At inference, materialize combined weights or compute JVP with combined dparams

- Design tradeoffs:
  - **Compute vs. modularity**: JVP adds ~40% training overhead vs. standard DPO (2.5h → 3.5h on V100), but eliminates retraining for new trade-offs
  - **Trainable layers**: Paper only trains last 16 transformer layers + LM head; full-model tangent updates may be redundant or unstable
  - **Convex vs. affine mixing**: Convex (λ₁ + λ₂ = 1) is safer; affine-2 extrapolation explores more frontier but risks breakdown

- Failure signatures:
  - **Collapsed helpfulness** with high verbosity λ: Affine-2 DPO shows helpfulness dropping to 0.33 (Table 1)—linear approximation failing
  - **Non-smooth Pareto curves**: Task-Vector DPO shows discontinuous jumps; TS-DPO should be smooth if working
  - **High cosine similarity between τ vectors**: If > 0.3, directions are not disentangled; check dataset quality or learning rate

- First 3 experiments:
  1. **Validate JVP implementation**: Train single-objective TS-DPO on helpfulness only; verify loss decreases and checkpoint loads correctly
  2. **Convex interpolation sanity check**: Sweep (λ, 1-λ) for λ ∈ {0.0, 0.5, 1.0}; plot helpfulness vs. verbosity to confirm monotonic trade-off
  3. **Ablate against Task-Vector DPO**: Replicate the paper's comparison—train standard DPO on same data, compute weight deltas, combine directly; verify TS-DPO produces smoother frontier

## Open Questions the Paper Calls Out
None

## Limitations
- The linear approximation breaks down for large extrapolation coefficients (λ > 3), limiting the range of controllable behaviors
- Requires training separate models per preference, increasing computational overhead compared to single-model approaches
- Assumes preferences are naturally decomposable into orthogonal axes, which may not hold for all alignment scenarios

## Confidence

| Claim | Confidence |
|-------|------------|
| Tangent-space linearization enables additive composition of preference directions | High |
| Separate tangent directions yield more disentangled updates than scalarization | High |
| Method scales to more than two preference axes | Medium |

## Next Checks

1. Test extrapolation limits systematically by sweeping λ₁, λ₂ across wider ranges and measuring where linearization breaks down
2. Evaluate multi-axis alignment (3+ preferences) to verify scalability of the tangent-space approach
3. Compare TS-DPO against gradient-based multi-objective methods like Pareto MTL to establish relative advantages