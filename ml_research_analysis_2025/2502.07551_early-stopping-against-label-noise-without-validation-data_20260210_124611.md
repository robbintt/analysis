---
ver: rpa2
title: Early Stopping Against Label Noise Without Validation Data
arxiv_id: '2502.07551'
source_url: https://arxiv.org/abs/2502.07551
tags:
- label
- learning
- training
- wave
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Early Stopping Against Label Noise Without Validation Data

## Quick Facts
- arXiv ID: 2502.07551
- Source URL: https://arxiv.org/abs/2502.07551
- Authors: Suqin Yuan; Lei Feng; Tongliang Liu
- Reference count: 17
- Primary result: Label Wave achieves 1.02% average accuracy improvement over validation-based early stopping on CIFAR-10/100 with 40% symmetric noise

## Executive Summary
This paper proposes Label Wave, an early stopping method for deep learning with noisy labels that eliminates the need for validation data. The approach tracks prediction changes on the training set using the Prediction Changes (PC) metric, identifying the epoch where model predictions stabilize before overfitting to mislabeled examples. The method demonstrates superior performance compared to hold-out validation across multiple datasets, noise types, and learning with noisy labels (LNL) algorithms.

## Method Summary
Label Wave monitors prediction fluctuations on the training set to identify the optimal early stopping point without requiring validation data. The method tracks how often training examples change their predicted labels between consecutive epochs, computing a Prediction Changes (PC) metric. A local minimum in PC indicates the transition point where the model has learned clean patterns but hasn't yet begun overfitting to mislabeled examples. Moving average smoothing improves robustness, and the approach preserves all training data for learning rather than splitting into train/validation sets.

## Key Results
- Label Wave outperforms validation-based early stopping across 7 LNL methods on CIFAR-10/100 with 40% symmetric noise
- Higher Kendall τ correlation with test accuracy compared to hold-out validation (0.2835 vs 0.2517 on average)
- Consistently achieves better test accuracy than validation-based methods across validation set sizes from 250 to 16,000
- Robust performance across symmetric and instance-dependent noise types on multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tracking prediction fluctuations on the training set identifies the transition from learning clean patterns to overfitting mislabeled data.
- Mechanism: The **Prediction Changes (PC)** metric counts training examples whose predicted labels differ between consecutive epochs. When PC reaches its first local minimum, this marks the epoch where model fitting performance is most stable—before the model begins significantly fitting mislabeled examples, which destabilizes predictions on clean data.
- Core assumption: DNNs learn simple/clean patterns before memorizing mislabeled examples; fitting mislabeled data impairs overall fitting performance on the training set itself.
- Evidence anchors:
  - [abstract] "minimum fluctuations in predictions typically occur at the training epoch before the model excessively fits mislabeled data"
  - [Section 3.2] "fitting mislabeled examples impairs not only the generalization performance but also the overall model's fitting performance"
  - [corpus] Related work on label noise (arXiv:2505.18909) confirms label noise fundamentally alters feature learning dynamics, but does not validate the PC-to-overfitting correlation directly.
- Break condition: If training lacks a "learning confusing patterns" stage (e.g., very low noise, strong regularization, or clean data), PC may not exhibit a clear local minimum, and early stopping may be unnecessary.

### Mechanism 2
- Claim: Moving average smoothing of PC improves robustness against stochastic training noise while preserving the signal of the transition point.
- Mechanism: Compute PC′ over a sliding window of k recent epochs (default k=3 per sensitivity analysis). This filters epoch-to-epoch variance while preserving the underlying trend. A patience parameter p allows waiting for confirmation that a local minimum has been reached.
- Core assumption: The true underlying PC signal follows a smooth U-shaped curve; noise is zero-mean and epoch-independent.
- Evidence anchors:
  - [Section 3.3, Eq. 3] "By averaging PCt over the recent k epochs... we derive a more stable version, denoted as PC′t"
  - [Appendix E] Sensitivity analysis shows k=3 yields strongest negative correlation (−0.9637) with test accuracy; correlation remains strong across k∈{1,2,3,5,10}.
  - [corpus] No direct corpus evidence on optimal moving average window for noisy-label early stopping.
- Break condition: If k is set too large relative to the number of epochs before overfitting begins, the smoothed signal may lag and miss the optimal stopping point.

### Mechanism 3
- Claim: Label Wave outperforms hold-out validation because it preserves full training data and correlates better with true test accuracy.
- Mechanism: By eliminating validation split, all training data is used for learning. The PC metric, computed on training predictions, exhibits higher Kendall τ correlation with test accuracy than hold-out validation, especially under high noise and limited validation set sizes.
- Core assumption: The correlation between PC and test error is sufficiently strong and consistent across noise types, architectures, and datasets.
- Evidence anchors:
  - [Section 4.3, Figure 4] Label Wave shows higher Kendall τ correlation and test accuracy than hold-out validation across set sizes 250–16,000 and noise rates 20–60%.
  - [Tables 2–3] Label Wave consistently outperforms validation-based early stopping across 7 LNL methods on CIFAR-10/100 with 40% noise.
  - [corpus] GRADSTOP (arXiv:2508.19028) proposes posterior-sampling-based early stopping without validation data, but targets general overfitting, not label noise specifically.
- Break condition: If the dataset is very small or noise rate is extremely high (>80%), PC signal may be too noisy to identify a reliable minimum.

## Foundational Learning

- Concept: **Memorization effect in DNNs**
  - Why needed here: The entire method relies on DNNs first learning simple/clean patterns before memorizing mislabeled examples. Without this asymmetry, PC would not exhibit a characteristic minimum.
  - Quick check question: Can you explain why deep networks tend to learn clean labels faster than random/noisy labels?

- Concept: **Early stopping and model selection**
  - Why needed here: Label Wave is fundamentally an early stopping criterion. Understanding the bias-variance tradeoff and why stopping before overfitting improves generalization is prerequisite.
  - Quick check question: What happens to training error vs. test error as training progresses past the optimal stopping point?

- Concept: **Label noise types (symmetric vs. instance-dependent)**
  - Why needed here: The paper evaluates Label Wave under both noise types. Understanding how noise structure affects learning dynamics helps interpret when the method is applicable.
  - Quick check question: What is the difference between symmetric label noise and instance-dependent label noise?

## Architecture Onboarding

- Component map:
  - Training loop -> Prediction tracker -> PC calculator -> Moving average module -> Early stopping controller -> Checkpoint manager

- Critical path:
  1. Initialize: θ ← θ₀, buffer ← empty, best_PC′ ← ∞, patience_counter ← 0.
  2. For each epoch t: run forward pass on full training set; store predictions ȳt.
  3. Compute PCt = count(ȳt ≠ ȳt−1).
  4. Update moving average buffer; compute PC′t.
  5. If PC′t < best_PC′: update best checkpoint, reset patience_counter.
  6. Else: increment patience_counter; if patience_counter ≥ p, halt and return best checkpoint.

- Design tradeoffs:
  - **k (moving average window)**: Larger k → smoother signal but slower response. Paper finds k=3 optimal; recommend range 2–5.
  - **p (patience)**: Larger p → more confidence in minimum but risk of slight overfitting. Default not explicit; suggest p=5–10 epochs.
  - **Checkpoint storage frequency**: Store every epoch vs. only at new minima. Latter saves storage but requires tracking parameters.

- Failure signatures:
  - **No clear PC minimum observed**: May indicate very low noise, clean data, or strong regularization. In this case, early stopping may not be needed.
  - **PC minimum occurs too early**: May indicate high learning rate or batch size causing prediction instability; reduce LR or increase k.
  - **Significant gap between Label Wave stopping point and global maximum test accuracy**: Check if noise rate is extremely high (>70%) or dataset is very small.

- First 3 experiments:
  1. **Reproduce CIFAR-10 + 40% symmetric noise baseline**: Train ResNet-18, track PC and test accuracy. Verify PC minimum aligns with test accuracy peak within ~1 epoch.
  2. **Ablate moving average window k**: Compare k∈{1,2,3,5,10} on CIFAR-10 + 40% noise. Measure correlation between PC′ and test accuracy; confirm k=3 is near-optimal.
  3. **Compare against hold-out validation**: On CIFAR-10 + 40% noise, run Label Wave vs. 5%/10%/20% hold-out validation. Measure test accuracy gap; confirm Label Wave matches or exceeds validation-based selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical mechanisms underlie the transition at "Point 2," where the model shifts from "misguided generalization" to overfitting, and can new metrics be designed to capture this shift?
- Basis in paper: [explicit] Appendix A states that the marked transition at Point 2 "cannot be satisfactorily explained by solely relying on our predefined stability and variability metrics. This presents an intriguing open question for further investigation."
- Why unresolved: The current metrics (stability and variability) identify Point 1 (early stopping) effectively but fail to theoretically explain or detect the subsequent stabilization phase (Point 2).
- What evidence would resolve it: A theoretical framework or a new quantitative metric that accurately predicts the stabilization of test error following the "learning confusing patterns" stage.

### Open Question 2
- Question: How can the Label Wave method be adapted to determine appropriate early stopping points in scenarios with very low label noise or benign overfitting?
- Basis in paper: [explicit] Appendix C.3 notes that the "Label Wave method is not applicable in very low or no label noise" because the stage of "learning confusing patterns" may be absent, making the original method unable to identify an appropriate stopping point.
- Why unresolved: The method relies on detecting fluctuations caused by fitting mislabeled data; if noise is too low or regularization prevents the "confusing patterns" stage, the prediction change metric may not exhibit the required local minimum.
- What evidence would resolve it: A modified version of the algorithm that functions effectively on datasets with noise rates significantly below 10% or where validation error consistently decreases.

### Open Question 3
- Question: Can an advanced early stopping metric be devised that surpasses the capabilities of the Prediction Changes (PC) metric?
- Basis in paper: [explicit] The Conclusion states, "Looking forward, there is potential to devise an advanced early stopping metric that could surpass the capabilities of the prediction changes metric, thereby enhancing the Label Wave method."
- Why unresolved: While PC is simple and computationally efficient, it is a heuristic based on empirical observation; more complex metrics might capture the dynamics of fitting performance with higher precision.
- What evidence would resolve it: A new metric that demonstrates a higher Kendall $\tau$ correlation with test accuracy or selects models with higher generalization performance than the PC metric across the same benchmarks.

## Limitations
- Method assumes a clear local minimum in PC curve exists, which may not hold for very clean datasets or extremely high noise rates (>70%)
- Performance depends on appropriate selection of hyperparameters k and p, though the paper provides empirical guidance
- Theoretical guarantees for PC-minimum correlation with optimal test accuracy are not proven, relying instead on empirical observation

## Confidence
- High confidence in Mechanism 1 (PC minimum identifies transition point) - supported by strong empirical evidence across multiple datasets and noise types
- Medium confidence in Mechanism 2 (moving average smoothing) - ablation studies show k=3 is optimal, but theoretical justification for this specific value is limited
- Medium confidence in Mechanism 3 (Label Wave vs. validation) - correlation with test accuracy is consistently higher, but validation-based methods may still outperform in specific low-noise scenarios

## Next Checks
1. Test Label Wave on extremely small datasets (n<1000) to verify PC signal remains detectable when training examples are scarce
2. Evaluate performance on noise rates exceeding 60% to identify the breaking point where PC minimum becomes unreliable
3. Compare Label Wave against validation-based early stopping on real-world noisy datasets (e.g., WebVision) rather than synthetic noise to assess practical utility