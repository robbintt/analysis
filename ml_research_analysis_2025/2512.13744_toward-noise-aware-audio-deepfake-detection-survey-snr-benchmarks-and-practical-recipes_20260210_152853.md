---
ver: rpa2
title: 'Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical
  Recipes'
arxiv_id: '2512.13744'
source_url: https://arxiv.org/abs/2512.13744
tags:
- noise
- detection
- speech
- noisy
- spoof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of audio deepfake detection under
  realistic noisy conditions. It introduces a controlled framework using MS-SNSD noise
  corpus mixed with ASVspoof 2021 utterances at varying SNRs, and evaluates state-of-the-art
  SSL encoders (WavLM, Wav2Vec2, MMS) on both binary and four-class tasks.
---

# Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes

## Quick Facts
- arXiv ID: 2512.13744
- Source URL: https://arxiv.org/abs/2512.13744
- Authors: Udayon Sen; Alka Luqman; Anupam Chattopadhyay
- Reference count: 24
- Primary result: SNR-aware finetuning of SSL encoders reduces EER by 10-15 percentage points under 10-0 dB noise conditions

## Executive Summary
This paper addresses the critical challenge of audio deepfake detection in realistic noisy conditions. The authors introduce a controlled evaluation framework using MS-SNSD noise corpus mixed with ASVspoof 2021 utterances at varying SNR levels, benchmarking state-of-the-art SSL encoders (WavLM, Wav2Vec2, MMS) on both binary and four-class tasks. Their core contribution is an SNR-based augmentation strategy with on-the-fly multi-condition training during finetuning, which significantly improves robustness to noise compared to frozen SSL models. The four-class supervision helps disentangle authenticity and corruption cues, though performance degrades sharply under severe noise when models are not adapted.

## Method Summary
The paper proposes a controlled evaluation framework for audio deepfake detection under noise conditions. The method involves mixing ASVspoof 2021 utterances with MS-SNSD noise corpus at varying SNR levels (20 dB down to 0 dB). Three state-of-the-art SSL encoders (WavLM, Wav2Vec2, MMS) are evaluated in both binary (Real vs Spoof) and four-class (Real+Clean, Real+Noisy, Spoof+Clean, Spoof+Noisy) settings. The core technique is on-the-fly multi-condition training with SNR-based augmentation during finetuning, allowing models to learn noise-invariant features while preserving spoof detection capabilities.

## Key Results
- Finetuning reduces EER by approximately 10-15 percentage points at 10-0 dB SNR across all backbone models
- WavLM generally outperforms Wav2Vec2 and MMS, showing best average performance under noisy conditions
- Four-class supervision improves disentanglement of authenticity and corruption cues compared to binary classification
- Frozen SSL models show sharp performance degradation under noise, dropping from clean EERs of 0.6-1.2% to 12-15% at 0 dB SNR
- Real+Noisy utterances are often misclassified as Spoof+Noisy, though four-class supervision mitigates this effect

## Why This Works (Mechanism)
The approach works by exposing models to diverse noise conditions during training, enabling them to learn representations that preserve spoof artifacts even when corrupted by additive noise. The SNR-aware finetuning strategy allows the model to adapt its decision boundaries to account for varying signal quality, while the four-class supervision explicitly teaches the model to distinguish between authenticity and noise-related degradation.

## Foundational Learning
- **Signal-to-Noise Ratio (SNR)**: The ratio of signal power to noise power, critical for understanding audio quality degradation. Why needed: Forms the basis for controlled noise evaluation and augmentation strategies.
- **Self-Supervised Learning (SSL) in Audio**: Pretraining methods like WavLM and Wav2Vec2 that learn representations from unlabeled audio data. Why needed: These provide strong feature extractors that can be adapted for downstream tasks.
- **Anti-Spoofing in ASV**: Techniques to distinguish between genuine human speech and synthetic/generated speech. Why needed: The core detection task being evaluated under noise conditions.
- **Four-Class Classification**: Explicit supervision distinguishing Real+Clean, Real+Noisy, Spoof+Clean, and Spoof+Noisy samples. Why needed: Helps disentangle authenticity from corruption cues that are confounded in binary classification.
- **MS-SNSD Noise Corpus**: A standardized noise dataset used for controlled evaluation. Why needed: Provides consistent, reproducible noise conditions for benchmarking.
- **Equal Error Rate (EER)**: A metric measuring the point where false acceptance rate equals false rejection rate. Why needed: Standard metric for anti-spoofing performance evaluation.

## Architecture Onboarding

**Component Map**: Raw Audio -> SSL Encoder (WavLM/Wav2Vec2/MMS) -> Feature Extractor -> Classification Head (Binary/Four-class) -> EER Score

**Critical Path**: The most important path is from the SSL encoder through to the classification head, where learned representations must preserve spoof artifacts while being robust to noise corruption.

**Design Tradeoffs**: 
- Frozen vs finetuned SSL models: Frozen models are faster to deploy but significantly underperform under noise
- Binary vs four-class supervision: Binary is simpler but conflates authenticity with noise effects; four-class provides better disentanglement at the cost of task complexity
- SNR levels during training: Lower SNRs improve robustness but may degrade clean performance

**Failure Signatures**: 
- Sharp EER increase from clean to noisy conditions (>10 percentage points)
- Confusion between Real+Noisy and Spoof+Noisy samples
- Inconsistent performance across different SSL backbones

**First Experiments to Run**:
1. Compare frozen WavLM performance at 0 dB SNR against finetuned WavLM on same conditions
2. Evaluate binary vs four-class classification on Real+Noisy vs Spoof+Noisy confusion
3. Test finetuning with different SNR ranges (e.g., only 10-20 dB vs full 0-20 dB) to find optimal noise exposure

## Open Questions the Paper Calls Out

### Open Question 1
Can corruption-aware SSL pretraining objectives—jointly spanning noise, reverb, codec, and channel perturbations—yield representations that preserve spoof artifacts under severe distortion better than post-hoc finetuning?

Basis in paper: Section V proposes "multi-corruption SSL pretraining" as a promising direction, noting that representations may preserve spoof artefacts even under severe distortion.

Why unresolved: Current work only applies on-the-fly noise augmentation during finetuning; no experiments test whether incorporating corruption awareness during pretraining improves robustness.

What evidence would resolve it: Pretrain SSL encoders with multi-corruption objectives and compare spoof detection EER curves against standard encoders with augmentation-only finetuning.

### Open Question 2
How do compounded degradations (e.g., codec compression plus reverberation plus additive noise) jointly affect deepfake detection, and do robustness gains from single-factor training transfer?

Basis in paper: Section V states future work should "incorporate multi-factor stress tests that jointly vary codecs, room acoustics, channels, and device characteristics."

Why unresolved: This study isolates additive noise via SNR sweeps; real-world degradations are coupled and non-stationary.

What evidence would resolve it: Systematic benchmarks varying two or more corruption types simultaneously, reporting per-condition and joint EER degradation.

### Open Question 3
Does explicit noise-invariant representation learning (e.g., adversarial denoising, multi-view consistency losses) further reduce Real+Noisy / Spoof+Noisy confusion beyond four-class supervision?

Basis in paper: Error analysis notes that "Real+Noisy utterances are often misclassified as Spoof+Noisy" and "explicit four-class supervision mitigates this effect but does not fully eliminate the problem."

Why unresolved: Four-class labeling helps disentanglement but does not guarantee noise-invariant features; no architectural or loss-based invariance mechanisms were tested.

What evidence would resolve it: Ablations comparing standard finetuning against models trained with adversarial noise removal or contrastive multi-view losses on the same four-class task.

## Limitations
- Controlled synthetic noise framework may not capture real-world non-stationary noise patterns
- Focus on English-language data limits generalizability to other languages and cultural contexts
- Assumes access to clean reference signals for augmentation, which may not be available in practical deployment
- Computational overhead of on-the-fly augmentation may challenge resource-constrained applications

## Confidence

**High**: SNR-aware finetuning effectiveness (supported by consistent improvements across multiple backbones and SNR levels)

**Medium**: WavLM superiority (shows best average performance but varies by condition)

**Medium**: Four-class supervision benefits (improvement observed but task complexity trade-offs not fully explored)

## Next Checks
1. Evaluate model performance on real-world noisy audio recordings with varying acoustic environments and background noise types
2. Test the finetuning approach with only noisy references available, simulating realistic deployment conditions
3. Assess cross-lingual transferability by evaluating models on non-English deepfake datasets under identical noise conditions