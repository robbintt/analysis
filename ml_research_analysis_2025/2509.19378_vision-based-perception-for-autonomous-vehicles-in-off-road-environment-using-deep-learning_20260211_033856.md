---
ver: rpa2
title: Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using
  Deep Learning
arxiv_id: '2509.19378'
source_url: https://arxiv.org/abs/2509.19378
tags:
- page
- figure
- learning
- segmentation
- off-road
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of visual perception for autonomous
  vehicles operating in off-road environments with low visibility conditions, such
  as rain, dust, and night. The research proposes a Configurable Modular Segmentation
  Network (CMSNet) framework to enable different architectural arrangements for semantic
  segmentation, allowing detection of trafficable areas and obstacles without predefined
  track boundaries.
---

# Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning

## Quick Facts
- arXiv ID: 2509.19378
- Source URL: https://arxiv.org/abs/2509.19378
- Authors: Nelson Alves Ferreira Neto
- Reference count: 0
- One-line primary result: Configurable CNN architectures can segment trafficable areas and obstacles in off-road environments with real-time embedded inference.

## Executive Summary
This thesis develops a Configurable Modular Segmentation Network (CMSNet) framework for visual perception in off-road autonomous vehicles, where traditional lane-based detection fails due to undefined track boundaries. The research creates the Kamino dataset with nearly 12,000 labeled images from Brazilian off-road tracks, including adverse conditions like dust, rain, and night. CMSNet enables different architectural arrangements for semantic segmentation, allowing detection of trafficable areas and obstacles. The study systematically evaluates how deep learning algorithms behave under varying visibility impairments and achieves real-time embedded inference using TensorRT optimization.

## Method Summary
The method employs a CMSNet framework built on MobileNetV2 backbone with configurable pyramid modules (SPP, ASPP, GPP) to segment off-road environments into trafficable areas and obstacles. Training uses the Kamino dataset (5,523 images for experiments) with 200 epochs, polynomial learning rate decay, and extensive data augmentation including synthetic fog and noise. The architecture balances accuracy and inference speed through output stride selection (OS8 for accuracy, OS16 for speed) and employs TensorRT layer fusion for embedded deployment on ARM platforms, achieving 21 FPS performance.

## Key Results
- CMSNet configurations achieved high accuracy in segmenting obstacles and trafficable ground in off-road environments
- Output stride 8 performed best in adverse conditions while output stride 16 offered the best inference speed
- Field tests demonstrated successful real-time performance on embedded hardware, achieving 21 FPS on ARM platforms
- Models trained on Kamino dataset showed better performance than urban-trained models when deployed in off-road conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulating the receptive field via configurable pyramid modules allows the network to balance global context understanding against local detail preservation, which is critical when track boundaries are undefined.
- Mechanism: The CMSNet framework swaps Spatial Pyramid Pooling (SPP), Atrous Spatial Pyramid Pooling (ASPP), or Global Pyramid Pooling (GPP) modules. By applying parallel filters at multiple scales (e.g., rates 6, 12, 18 for ASPP), the network aggregates multi-scale context before upsampling, helping to distinguish "trafficable" areas from "background" even when textures are similar.
- Core assumption: The geometric structure or subtle texture differences of the trafficable region differ sufficiently from the surrounding terrain to be captured by dilated convolutions.
- Evidence anchors:
  - [abstract] "CMSNet configurations were trained to segment obstacles and trafficable ground... without predefined track boundaries."
  - [section 3.2.1.6] Describes ASPP using expansion rates to capture global/local context.
  - [corpus] Neighbors like *AnyNav* highlight that off-road navigation suffers from "unstructured environments," supporting the need for multi-scale context over rigid lane detection.
- Break condition: If the off-road terrain is visually uniform (e.g., a sand dune with no distinct path), the global context provides no discriminative signal, causing the segmentation to fail or hallucinate paths.

### Mechanism 2
- Claim: Training on a dataset explicitly containing visibility impairments (dust, rain, night) induces robustness that urban-only models lack.
- Mechanism: The "Kamino" dataset includes nearly 12,000 images with labeled adverse conditions. By minimizing the loss function across these diverse distributions (day vs. night vs. dust), the CNN learns feature invariants (e.g., edge gradients or shapes) rather than relying solely on color intensity, which varies wildly in low light or dust.
- Core assumption: The degradation caused by rain/dust is not so extreme as to completely obscure structural edges required for segmentation.
- Evidence anchors:
  - [abstract] "A new dataset, Kamino... includes images... emulating a mine under adverse visibility."
  - [section 4.1.3] Shows mIoU degradation curves; while performance drops in extreme conditions, it remains functional compared to urban-trained models.
  - [corpus] *MCRL4OR* notes the difficulty of scaling dense annotations for off-road; this mechanism relies on the assumption that the 12k labeled images are sufficient to span the variance.
- Break condition: If sensor noise (e.g., heavy Gaussian noise) overwhelms the signal-to-noise ratio beyond the training distribution, the model's mIoU degrades significantly (observed in Section 4.1.4).

### Mechanism 3
- Claim: Reducing output stride (OS) and utilizing TensorRT layer fusion enables real-time inference on embedded hardware by trading slight accuracy for latency reduction.
- Mechanism: Configuring the backbone (MobileNetV2) to an Output Stride of 16 (OS16) instead of 8 reduces the resolution of feature maps processed, lowering MAC operations. Furthermore, fusing CNN layers via TensorRT/CUDA eliminates framework overhead and memory bottlenecks.
- Core assumption: The embedded hardware (ARM/CUDA) supports the specific fused kernels required for the modified MobileNetV2 depthwise separable convolutions.
- Evidence anchors:
  - [abstract] "Output stride 16 offering the best inference speed... achieved 21 FPS on ARM platforms."
  - [section 4.1.7] "CM3-TRT has achieved 21 FPS in Drive PX2... The standard deviation Ïƒ... was only 0.16%."
  - [corpus] *Verti-Bench* emphasizes the need for generalizable off-road benchmarks; here, the mechanism relies on hardware-specific optimization rather than algorithmic novelty alone.
- Break condition: If the target hardware lacks the specific vector acceleration (SIMD) assumed by the TensorRT optimizer, the latency gains may vanish or layer fusion may fail.

## Foundational Learning

- Concept: **Semantic Segmentation vs. Object Detection**
  - Why needed here: In off-road environments, obstacles (rocks, bushes) and drivable surfaces rarely fit into neat bounding boxes. Segmentation provides a pixel-wise mask for the "trafficable area" required for path planning.
  - Quick check question: Can you distinguish between a discrete obstacle (object detection) and a continuous surface type (semantic segmentation)?

- Concept: **Output Stride and Atrous Convolution**
  - Why needed here: Standard pooling reduces resolution (OS), losing spatial details needed for precise edge detection. Atrous convolution expands the receptive field without downsampling, crucial for maintaining the definition of track boundaries.
  - Quick check question: Does increasing the output stride (e.g., from 8 to 16) increase or decrease the spatial resolution of the output feature map?

- Concept: **Transfer Learning and Domain Shift**
  - Why needed here: The paper explicitly notes that models trained on urban datasets (Cityscapes) fail on unpaved roads. Understanding why (distribution shift in textures/colors) explains the necessity of the Kamino dataset.
  - Quick check question: Why would a model trained on asphalt perform poorly on sand, even if both are "roads"?

## Architecture Onboarding

- Component map:
  - Input Image -> Modified MobileNetV2 Backbone -> CMSNet Context Module (ASPP/GPP/SPP) -> Bilinear Upsampling -> Segmentation Mask

- Critical path:
  1. Input Image -> **Backbone** (Extract features, apply atrous conv)
  2. Features -> **Pyramid Module** (Aggregate context at multiple scales)
  3. Context -> **Bilinear Upsampling** (Restore resolution)
  4. **Shortcuts** (Optional, if OS16 is used, to recover lost edge detail)

- Design tradeoffs:
  - **ASPP vs. GPP**: Use ASPP if accuracy is paramount (better mIoU in dust/night). Use GPP if inference speed is critical (highest FPS).
  - **OS8 vs. OS16**: OS8 preserves spatial details (better for obstacle boundaries) but is computationally expensive. OS16 is faster but may lose fine-grained obstacle details.

- Failure signatures:
  - **"Ghost" obstacles**: In heavy dust or synthetic fog, the model hallucinates obstacles where none exist due to noise patterns matching training data features.
  - **Uniform Terrain Failure**: If the track and non-track areas have identical texture/color (e.g., dry riverbed), the model may classify everything as "trafficable."

- First 3 experiments:
  1. **Baseline Benchmark**: Run CMSNet-M2 (OS8, ASPP) on the Kamino validation set to establish the upper bound for accuracy (mIoU).
  2. **Latency vs. Accuracy**: Compare CMSNet-M2 vs. CMSNet-M3 (OS16, GPP) inference time (FPS) on the target embedded hardware to quantify the speed trade-off.
  3. **Domain Shift Test**: Run a standard pre-trained DeepLab (urban) model on a Kamino off-road image to visually confirm the failure mode of urban models in this domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fusion of thermal and RGB cameras significantly improve segmentation performance in extreme visibility conditions where RGB-only models fail?
- Basis in paper: [explicit] The Conclusion section lists "trying some approaches combining thermal cameras and RGB ones" as a suggestion for future development.
- Why unresolved: The current research relied exclusively on RGB data and concluded that segmentation quality degrades under extreme conditions.
- What evidence would resolve it: A comparative study evaluating mIoU between RGB-only and thermal-fused models in heavy dust, rain, and total darkness.

### Open Question 2
- Question: To what extent do specialized loss functions (e.g., Focal Loss, Dice Loss) improve the segmentation of rare classes in off-road environments?
- Basis in paper: [explicit] The Conclusion suggests "trying cost functions... as Focal Loss and Dice Loss" to address the class imbalance noted in the dataset.
- Why unresolved: The study excluded rare classes (e.g., animals, bikes) from training due to insufficient samples and high imbalance.
- What evidence would resolve it: Experiments comparing standard cross-entropy loss against Focal/Dice Loss on the rare classes present in the Kamino dataset.

### Open Question 3
- Question: Do vision transformer architectures outperform the proposed CNN-based CMSNet framework in off-road semantic segmentation tasks?
- Basis in paper: [explicit] The Conclusion lists "applying transformers-networks for semantic segmentation" as a recommendation for future work.
- Why unresolved: The research focused strictly on Convolutional Neural Networks (CNNs) and modular arrangements like SPP and ASPP.
- What evidence would resolve it: A benchmark comparing a transformer-based segmentation model against the CMSNet configurations on the Kamino dataset.

## Limitations
- The Kamino dataset, while substantial for off-road work, is modest compared to urban datasets and may not capture all rare edge cases
- Real-time performance claims (21 FPS) are hardware-specific and may not translate to other embedded platforms without significant optimization
- The study does not address catastrophic forgetting when fine-tuning pre-trained models on Kamino or explore continual learning approaches

## Confidence
- **High Confidence**: The architectural framework (CMSNet with configurable pyramid modules) and its impact on accuracy vs. speed trade-offs are well-supported by systematic experiments
- **Medium Confidence**: The robustness claims under adverse visibility conditions are supported by the Kamino dataset but may be overfit to specific degradation patterns
- **Low Confidence**: The specific TensorRT optimization parameters and exact data augmentation ranges are not fully specified, making faithful reproduction challenging

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate the trained CMSNet-M2 model on a completely different off-road dataset (e.g., from a different geographic region or collected by different sensors) to assess true generalization beyond the Kamino domain.

2. **Ablation on Data Augmentation**: Systematically remove specific augmentation types (fog, noise, rotation) to quantify their individual contributions to model robustness, particularly under adverse visibility conditions.

3. **Hardware Portability Benchmark**: Port the TensorRT-optimized model to a different embedded platform (e.g., NVIDIA Jetson Orin) and measure the FPS degradation to establish the portability limits of the real-time performance claims.