---
ver: rpa2
title: Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive
  Reinforcement Learning
arxiv_id: '2511.05407'
source_url: https://arxiv.org/abs/2511.05407
tags:
- user
- satisfaction
- minority
- majority
- seeker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of modeling diverse user preferences
  in dialogue systems, particularly for satisfaction estimation where minority and
  majority users may have different feedback patterns. The authors propose a framework
  that captures both individual and group-level preferences through interpretable
  reasoning chains (CoPeR) and unsupervised clustering (M2PC) to distinguish majority
  and minority user groups.
---

# Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.05407
- Source URL: https://arxiv.org/abs/2511.05407
- Reference count: 40
- One-line result: Preference-adaptive RL framework improves minority user satisfaction prediction in dialogue systems through interpretable reasoning and unsupervised clustering

## Executive Summary
This work addresses the challenge of modeling diverse user preferences in dialogue systems, particularly for satisfaction estimation where minority and majority users may have different feedback patterns. The authors propose a framework that captures both individual and group-level preferences through interpretable reasoning chains (CoPeR) and unsupervised clustering (M2PC) to distinguish majority and minority user groups. They integrate these components into a preference-adaptive reinforcement learning framework (PAda-PPO) that jointly optimizes for satisfaction across diverse users. Experiments on the Emotional Support Conversation dataset show consistent improvements in user satisfaction estimation, particularly for underrepresented user groups, with the proposed method achieving higher weighted and macro F1 scores compared to baseline approaches.

## Method Summary
The approach combines structured reasoning with preference-aware clustering and reinforcement learning. First, CoPeR synthesizes interpretable reasoning chains using GPT-4.1-mini to capture user-specific intent-strategy alignment. These rationales supervise fine-tuning of a Llama-3-8B-Instruct model via LoRA. Second, M2PC applies EM clustering to discover majority/minority user groups based on dialogue perplexity without requiring labeled group membership. Finally, PAda-PPO integrates these components through group-specific reference models in the KL regularization term, routing each input to the appropriate reference model based on perplexity. The framework is trained on the ESConv dataset, with evaluation focusing on per-class F1 scores for both minority and majority user groups.

## Key Results
- CoPeR approach achieves highest performance with weighted and macro F1 scores of 0.65 and 0.64 respectively
- M2PC improves minority group F^low_1 from 0.24 to 0.60 through unsupervised clustering
- PAda-PPO with diversity-aware KL regularization demonstrates consistent improvements across all evaluation metrics
- Overall performance reaches weighted F1 of 0.64 and macro F1 of 0.63 on test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reasoning chains improve satisfaction prediction by making user-specific intent-strategy alignment explicit.
- Mechanism: The UCoT prompt forces the model to decompose satisfaction into four steps: (1) infer user intent, (2) identify supporter strategy, (3) evaluate alignment, (4) predict score. CoPeR supervises this reasoning during SFT, conditioning score prediction on intermediate reasoning.
- Core assumption: Users assign satisfaction based on whether the supporter's strategy matches their underlying intent and emotional state.
- Evidence anchors:
  - [abstract] "Chain-of-Personalized-Reasoning (CoPeR) to capture individual preferences through interpretable reasoning chains"
  - [section 6.2] "CoPeR approach, which extends UCoT by additionally supervising the model to generate user-specific reasoning as output, achieves the highest performance in both weighted and macro F1 scores"
  - [corpus] Limited corpus support; related work focuses on reward modeling rather than reasoning-chain supervision.
- Break condition: If synthesized rationales from GPT-4.1-mini are inaccurate (Section 6.1 reports 63-74% accuracy), supervision signal degrades. Paper acknowledges this as a key limitation.

### Mechanism 2
- Claim: Perplexity-based EM clustering separates majority/minority users without labeled group membership.
- Mechanism: M2PC initializes two models, assigns users to whichever model yields lower perplexity on their dialogues (E-step), then fine-tunes each model on its assigned users (M-step). This iteratively discovers latent preference groups.
- Core assumption: Users with similar preferences produce dialogue distributions that different models fit with different perplexities.
- Evidence anchors:
  - [abstract] "expectation-maximization-based Majority-Minority Preference-Aware Clustering (M2PC) algorithm that discovers distinct user groups in an unsupervised manner"
  - [section 6.3.1] "M2PC improves F^low_1 from 0.24 to 0.60... for the minority group, M2PC leads to significant improvements in low satisfaction prediction"
  - [corpus] Corpus lacks direct comparables for perplexity-based user clustering in dialogue.
- Break condition: If groups overlap significantly in perplexity space, routing becomes unstable. Section 6.3.2 notes M2PC "does not model some subgroups of minority users well."

### Mechanism 3
- Claim: Group-specific reference models in KL regularization preserve minority preferences during RL optimization.
- Mechanism: Standard PPO uses a single reference model for KL penalty. PAda-PPO routes each input to either G^Major or G^Minor based on perplexity, computing KL divergence against the appropriate group reference. This prevents the policy from collapsing toward majority-only behaviors.
- Core assumption: Separate reference models encode group-specific preference patterns that single references would overwrite.
- Evidence anchors:
  - [abstract] "preference-adaptive reinforcement learning framework (PAda-PPO) that jointly optimizes alignment with both individual and group preferences"
  - [section 4.4] "A divergence penalty was utilized to prevent the policy from diverging significantly from human-like reference behaviors in each group"
  - [corpus] LoRe paper mentions low-rank reward modeling for personalization but uses different mechanism.
- Break condition: If CoPeR reasoning conflicts with routing signals, KL penalty increases and destabilizes training (Section 6.4 notes this for CoPeR+PAda-PPO combination).

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**:
  - Why needed here: UCoT/CoPeR builds directly on CoT by structuring reasoning about user intent and strategy. Understanding standard CoT helps debug why prompts work or fail.
  - Quick check question: Can you explain how step-by-step reasoning differs from direct prediction in terms of gradient flow during training?

- **Expectation-Maximization (EM) Algorithm**:
  - Why needed here: M2PC uses EM to iteratively assign users and update models. Without this foundation, debugging cluster convergence is impossible.
  - Quick check question: What happens if the E-step assignments oscillate between iterations rather than stabilizing?

- **PPO with KL Regularization**:
  - Why needed here: PAda-PPO modifies standard PPO by adding group-conditional KL. You need to understand baseline PPO to isolate where modifications matter.
  - Quick check question: Why does KL divergence need a reference model, and what happens if the reference is too different from the initial policy?

## Architecture Onboarding

- **Component map**: UCoT Prompt Generator -> CoPeR Synthesizer -> SFT Model G(θ) -> M2PC Clustering Module -> PAda-PPO Trainer

- **Critical path**: CoPeR synthesis quality → SFT model reasoning capability → M2PC cluster separation → PAda-PPO stability. Errors propagate forward; fixing late stages without fixing early ones wastes effort.

- **Design tradeoffs**:
  - **Synthesized vs. human rationales**: Paper uses GPT-4.1-mini for scalability but acknowledges noise. Human annotation would improve quality at high cost.
  - **Two groups vs. more**: Binary majority/minority split is heuristic; Table 3 shows finer subgroups exist but aren't explicitly modeled.
  - **Perplexity routing vs. explicit user features**: Unsupervised approach works without demographic data but may misclassify users near decision boundaries.

- **Failure signatures**:
  - High logical accuracy but low strategy accuracy in CoPeR synthesis → model learns coherent but wrong reasoning
  - EM cluster sizes don't stabilize → perplexity signal too weak for clean separation
  - PAda-PPO loss oscillates → KL penalty coefficient λ_KL may be too high relative to reward signal

- **First 3 experiments**:
  1. **Validate CoPeR synthesis**: Sample 50 training examples, manually check whether reasoning aligns with ground-truth satisfaction. Target >70% logical consistency.
  2. **Ablate M2PC initialization**: Run EM with different random seeds and cluster counts (k=10,20,30). Check if final group assignments correlate with heuristic majority/minority labels.
  3. **Isolate KL routing effect**: Compare PAda-PPO against standard PPO with single reference model, measuring F1 separately on identified minority vs. majority users in validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can higher-quality or human-annotated reasoning chains improve PAda-PPO training stability and final performance compared to GPT-4.1-mini synthesized CoPeR rationales?
- Basis in paper: [explicit] Authors state: "Improving the quality of CoPeR synthesis will be done in future work" and note that synthesized rationales can conflict with preference routing, causing higher KL divergence penalties that destabilize training.
- Why unresolved: GPT-synthesized rationales achieve only 63.44% supporter-strategy accuracy and 74.16% logical accuracy, introducing noise; no comparison with human-authored reasoning exists.
- What evidence would resolve it: Ablation comparing PAda-PPO trained with human-annotated reasoning chains versus synthetic chains on the same test splits.

### Open Question 2
- Question: Do distinct preference subgroups within the minority population require separate models or specialized clustering initialization to improve satisfaction estimation?
- Basis in paper: [explicit] Authors acknowledge: "M2PC does not model some subgroups of minority users well... Future work is needed to better account for the nuanced preference patterns among these underrepresented subgroups."
- Why unresolved: Table 3 shows some minority subgroups achieving weighted F1 as low as 0.10–0.34 while others reach 1.00, suggesting high intra-group heterogeneity not captured by binary clustering.
- What evidence would resolve it: Extending M2PC to K>2 clusters with subgroup-specific analysis showing improved macro F1 across all minority subpopulations.

### Open Question 3
- Question: How sensitive is the majority/minority partition threshold (currently 60% high-satisfaction proportion) to downstream model performance?
- Basis in paper: [inferred] The paper states: "we heuristically classified users whose proportion of high satisfaction scores exceeded 60% as belonging to a 'majority' population"—no justification or sensitivity analysis is provided for this threshold.
- Why unresolved: Different thresholds could change cluster composition (currently 81.4% majority vs. 18.6% minority) and affect M2PC routing accuracy.
- What evidence would resolve it: Systematic evaluation across thresholds (e.g., 50%, 55%, 60%, 65%, 70%) measuring validation F1 and cluster stability metrics.

## Limitations

- The quality of synthesized CoPeR rationales introduces uncertainty, with GPT-4.1-mini achieving only 63-74% logical accuracy
- Binary majority/minority clustering heuristic using 60% threshold is arbitrary and may not capture full preference heterogeneity
- PAda-PPO framework shows training instability when combining CoPeR with preference routing, suggesting potential conflicts between reasoning supervision and perplexity-based assignment

## Confidence

- **High Confidence**: The core claim that structured reasoning improves satisfaction prediction is well-supported by empirical results showing CoPeR's superior F1 scores across both weighted and macro metrics.
- **Medium Confidence**: The unsupervised clustering approach effectively separates user groups, as evidenced by the dramatic improvement in F^low_1 for minority users, though the method's sensitivity to initialization and parameter choices remains partially explored.
- **Medium Confidence**: Preference-adaptive RL provides measurable improvements in minority satisfaction prediction, though the interaction effects with CoPeR synthesis quality introduce uncertainty about the robustness of these gains.

## Next Checks

1. **CoPeR Synthesis Validation**: Manually evaluate 100 randomly sampled synthesized reasoning chains against ground-truth satisfaction scores to quantify logical consistency and identify systematic failure modes in the GPT-4.1-mini generation process.

2. **M2PC Robustness Analysis**: Run the EM clustering algorithm with 10 different random seeds and compare cluster assignments, sizes, and resulting satisfaction prediction performance to assess stability and sensitivity to initialization.

3. **KL Routing Stability Test**: Isolate the PAda-PPO routing mechanism by comparing group-specific vs. single-reference KL regularization on a held-out validation set, measuring both minority F1 improvement and training stability metrics (KL divergence variance, loss oscillation).