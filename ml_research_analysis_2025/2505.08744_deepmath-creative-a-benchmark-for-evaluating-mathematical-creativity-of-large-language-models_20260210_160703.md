---
ver: rpa2
title: 'DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large
  Language Models'
arxiv_id: '2505.08744'
source_url: https://arxiv.org/abs/2505.08744
tags:
- mathematical
- problems
- reasoning
- creativity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DeepMath team introduces DeepMath-Creative, a benchmark to
  evaluate mathematical creativity in large language models (LLMs), addressing the
  lack of datasets for assessing creative problem-solving beyond basic reasoning tasks.
  The benchmark includes constructive problems in algebra, topology, geometry, and
  analysis, focusing on novel example construction, original reasoning, and counterexample
  development.
---

# DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models

## Quick Facts
- arXiv ID: 2505.08744
- Source URL: https://arxiv.org/abs/2505.08744
- Reference count: 15
- The DeepMath team introduces DeepMath-Creative, a benchmark to evaluate mathematical creativity in large language models (LLMs), addressing the lack of datasets for assessing creative problem-solving beyond basic reasoning tasks.

## Executive Summary
The DeepMath team introduces DeepMath-Creative, a benchmark to evaluate mathematical creativity in large language models (LLMs), addressing the lack of datasets for assessing creative problem-solving beyond basic reasoning tasks. The benchmark includes constructive problems in algebra, topology, geometry, and analysis, focusing on novel example construction, original reasoning, and counterexample development. Experiments on mainstream models (O3 Mini, Claude-3-7-Sonnet, Gemini-2.0-Flash, DeepSeek R1, Qwen QwQ-32B) reveal that even under lenient scoring criteria, the best model (O3 Mini) achieves only 70% accuracy, primarily on undergraduate-level problems. Performance sharply declines on complex tasks, with models often producing flawed reasoning, incorrect directions, or overly verbose outputs lacking convergence. These results indicate that current LLMs' creative abilities rely more on memorized pattern recombination than genuine insight, highlighting the need for further advancements in mathematical creativity.

## Method Summary
DeepMath-Creative is a benchmark designed to evaluate mathematical creativity in large language models through constructive problem-solving tasks. The benchmark comprises 179 expert-designed problems spanning algebra (50%), topology (15%), analysis (35%), and geometry, with approximately 60% undergraduate and 40% master's-level difficulty. Problems require either constructing proofs or counterexamples to given propositions, using a bidirectional prompt format: "Problem description + If the proposition holds, please prove it; if not, provide a counterexample." Five mainstream models were evaluated via API using standardized prompts: GPT O3-mini, Claude-3-7-Sonnet, Gemini-2.0-Flash, DeepSeek R1, and Qwen QwQ-32B. Performance was scored manually by math experts using a three-tier rubric (0 for wrong direction, 0.5 for flawed but correct direction, 1 for complete and correct) based on direction accuracy and process accuracy.

## Key Results
- Even under lenient scoring, the best-performing model (O3 Mini) achieved only 70% accuracy, primarily on undergraduate-level problems
- Performance sharply declines on master's-level problems and complex tasks, with models producing flawed reasoning, incorrect directions, or non-convergent verbose outputs
- Current LLMs' mathematical creativity relies more on memorized pattern recombination than genuine creative insight

## Why This Works (Mechanism)
The bidirectional problem format requires models to first determine whether a proposition is true or false before constructing either a proof or counterexample. This dual requirement captures both creative direction selection and constructive reasoning, revealing whether models possess genuine insight or merely apply memorized patterns. The manual expert scoring with nuanced 0/0.5/1 rubric distinguishes between fundamental misunderstanding (wrong direction) and partial progress (correct direction with flawed execution), providing detailed insight into model limitations.

## Foundational Learning
- Constructive mathematics: Creating mathematical objects to prove or disprove propositions. Needed because standard benchmarks focus on deduction rather than creation. Quick check: Can the model generate a valid counterexample to "every continuous function is differentiable"?
- Mathematical creativity dimensions: Original reasoning, novel example construction, counterexample development. Needed because creativity involves more than pattern matching. Quick check: Does the model identify when a proposition is false before constructing a counterexample?
- Bidirectional problem format: Requiring models to choose between proof or counterexample. Needed because creative insight includes recognizing problem direction. Quick check: Does the model correctly identify proposition truth value before proceeding?

## Architecture Onboarding

### Component Map
Problem Dataset -> Prompt Template -> LLM API -> Output Processing -> Manual Expert Scoring -> Performance Metrics

### Critical Path
Problem formulation → API evaluation → Manual scoring → Performance analysis

### Design Tradeoffs
- Manual expert scoring provides nuanced evaluation but introduces subjectivity
- Bidirectional format captures creative direction choice but increases evaluation complexity
- Focus on constructive problems captures creativity but may miss other creative dimensions

### Failure Signatures
- Wrong directional choice (proving when counterexample needed or vice versa)
- Spurious reasoning (correct conclusion via invalid intermediate steps)
- Non-convergent verbose outputs (excessive length without clear conclusion)

### 3 First Experiments
1. Evaluate a simple algebra problem: "Is every subgroup of a cyclic group cyclic? Prove or provide counterexample."
2. Test direction identification: "Is the product of two continuous functions always continuous?"
3. Assess counterexample construction: "Can a continuous function on a closed interval fail to achieve its maximum?"

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the scalability of current LLMs' mathematical creativity, including whether specialized training data focused on constructive problem-solving could improve performance, how model architecture influences creative reasoning capabilities, and what specific mathematical domains pose the greatest challenges for creative insight development.

## Limitations
- Manual scoring methodology introduces potential subjectivity in distinguishing between "minor inaccuracies" and "significant flaws" in mathematical reasoning
- The evaluation focuses primarily on constructive problems, potentially missing other dimensions of mathematical creativity
- Performance metrics may vary due to potential variability in expert grader calibration

## Confidence
- High confidence in the overall conclusion that current LLMs demonstrate limited mathematical creativity beyond pattern recognition
- Medium confidence in the specific performance metrics due to potential variability in manual scoring and the absence of the complete benchmark dataset for independent verification

## Next Checks
1. Obtain and independently evaluate the complete DeepMath-Creative benchmark dataset using multiple expert graders to assess inter-rater reliability in the 0/0.5/1 scoring system
2. Test additional model families beyond the five evaluated, particularly those specifically trained with mathematical reasoning objectives, to establish whether performance limitations are universal across architectures
3. Conduct ablation studies varying prompt engineering parameters (temperature, few-shot examples, problem complexity) to isolate the impact of evaluation methodology on measured creative ability