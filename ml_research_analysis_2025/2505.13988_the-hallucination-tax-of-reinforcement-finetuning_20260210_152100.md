---
ver: rpa2
title: The Hallucination Tax of Reinforcement Finetuning
arxiv_id: '2505.13988'
source_url: https://arxiv.org/abs/2505.13988
tags:
- unanswerable
- arxiv
- math
- questions
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical unintended consequence of reinforcement
  finetuning (RFT) on large language models: a significant degradation in refusal
  behavior, causing models to confidently produce hallucinated answers to unanswerable
  questions. To systematically study this "hallucination tax," the authors introduce
  SUM (Synthetic Unanswerable Math), a dataset of high-quality, multi-step reasoning
  math problems designed to be unanswerable due to missing, ambiguous, or contradictory
  information.'
---

# The Hallucination Tax of Reinforcement Finetuning

## Quick Facts
- arXiv ID: 2505.13988
- Source URL: https://arxiv.org/abs/2505.13988
- Reference count: 29
- Primary result: Standard RFT reduces refusal rates by >80%; 10% SUM augmentation restores appropriate refusal behavior with minimal accuracy loss

## Executive Summary
This paper identifies a critical unintended consequence of reinforcement finetuning (RFT) on large language models: a significant degradation in refusal behavior, causing models to confidently produce hallucinated answers to unanswerable questions. To systematically study this "hallucination tax," the authors introduce SUM (Synthetic Unanswerable Math), a dataset of high-quality, multi-step reasoning math problems designed to be unanswerable due to missing, ambiguous, or contradictory information. Experiments across four open-source LLMs and eight benchmarks show that standard RFT reduces refusal rates by more than 80%. The authors demonstrate that augmenting RFT with just 10% SUM data substantially restores appropriate refusal behavior, enabling models to reason about their own uncertainty and knowledge boundaries, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach generalizes to out-of-domain tasks, including factual question answering, by leveraging inference-time compute.

## Method Summary
The authors introduce SUM (Synthetic Unanswerable Math), a dataset of unanswerable math problems generated by transforming answerable problems from DeepScaleR using an LLM (o3-mini). They train four models (Qwen2.5-7B, Qwen2.5-Math-1.5B, Llama-3.1-8B-Instruct) using veRL's PPO implementation with a modified reward function that rewards both correct answers to answerable problems and appropriate refusals to unanswerable ones. Training uses 200 optimization steps with 8×A100 GPUs, mixing SUM data at various ratios (0%, 1%, 10%, 30%, 50%) into DeepScaleR. Evaluation measures refusal rates on three unanswerable benchmarks (SUM-test, UMWP, SelfAware) and accuracy on five answerable math benchmarks (GSM8K, MATH-500, OlympiadBench, Minerva, AMC23).

## Key Results
- Standard RFT reduces refusal rates by more than 80% across all tested models and benchmarks
- Augmenting RFT with 10% SUM data restores refusal behavior, with refusal rates jumping from ~0.01 to 0.73 on SUM-test for Qwen2.5-7B
- The approach generalizes to out-of-domain tasks: refusal rate on factual QA (SelfAware) improves from 0.01 to 0.94 with 10% SUM augmentation
- Higher SUM ratios (30-50%) further improve refusal but degrade accuracy on answerable tasks by 0.05-0.15

## Why This Works (Mechanism)

### Mechanism 1: Reward Misalignment Drives Overconfident Hallucination
Standard RFT reward structures implicitly penalize abstention, causing models to produce confident answers even when questions are fundamentally unanswerable. RFT optimizes expected reward via equation (1). For answerable problems, reward=1 only for correct answers; refusals receive 0 reward. This asymmetrically reinforces answer-production behavior while never positively reinforcing epistemic humility. Over training steps, the policy shifts toward always generating substantive outputs.

### Mechanism 2: Unified Reward Signal Enables Refusal Learning
A single scalar reward function that symmetrically treats correct answers AND appropriate refusals enables models to learn both behaviors simultaneously. The modified reward function (equation 3) assigns reward=1 when: (a) problem is answerable AND answer is correct, OR (b) problem is unanswerable AND model refuses. This creates balanced gradient signals. The categorization function c(x,y,ŷ) in equation (2) detects refusal via exact match of "I don't know" in boxed output, making the signal unambiguous.

### Mechanism 3: Inference-Time Compute Enables Cross-Domain Generalization
Training on synthetic unanswerable math teaches models to reason about information insufficiency, generalizing to out-of-domain tasks like factual QA without domain-specific training data. SUM problems require multi-step reasoning to identify information gaps (e.g., deleted key conditions, ambiguous specifications). During training, models learn to allocate inference-time compute toward solvability assessment before committing to answers. This meta-reasoning skill—generating internal analysis chains about whether sufficient information exists—transfers across reasoning domains.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) for language model finetuning**
  - Why needed here: The entire RFT framework uses PPO (section 4.2). Understanding how policy gradients with clipped objective work is essential to grasp why reward signal changes directly affect model behavior.
  - Quick check question: Can you explain why PPO's clipping mechanism prevents excessive policy updates during finetuning?

- **Concept: Reward shaping and alignment in reinforcement learning**
  - Why needed here: The core insight is that standard RFT rewards fail to incentivize refusal. Understanding how reward design shapes emergent behavior is critical for diagnosing and fixing the hallucination tax.
  - Quick check question: What happens to learned behavior when an RL agent receives zero reward for a desirable action but positive reward only for a different action?

- **Concept: Chain-of-thought reasoning and inference-time compute**
  - Why needed here: The paper claims models "leverage inference-time compute to reason about their own uncertainty." This requires understanding how autoregressive generation can implement multi-step reasoning before producing final outputs.
  - Quick check question: How does allowing a model more tokens before its final answer enable more complex reasoning behaviors?

## Architecture Onboarding

- **Component map:**
  SUM Data Generator (o3-mini) -> Training Data Mixer (DeepScaleR + SUM) -> PPO Training Loop (veRL) -> Reward Function (equations 2-3) -> Evaluation Suite (8 benchmarks)

- **Critical path:**
  1. Generate SUM data using o3-mini with quality validation (section 3.3 reports 86.93% correctness rate)
  2. Append refusal instruction ("\boxed{I don't know.}") to all SUM questions
  3. Mix SUM into DeepScaleR at 10% replacement ratio
  4. Train with PPO using unified reward function (equation 3)
  5. Evaluate refusal rate on unanswerable benchmarks + accuracy on answerable benchmarks

- **Design tradeoffs:**
  - **Mixing ratio**: 10% SUM balances refusal restoration with minimal accuracy loss (Table 2); 30-50% improves refusal further but degrades answerable accuracy by 0.05-0.15 (Figure 3)
  - **Model selection**: Instruction-tuned models (Qwen-7B-Instruct, Llama-8B-Instruct) learn refusal faster but show more accuracy fluctuation; base models more stable but slower convergence
  - **Reward design**: Exact-match refusal detection is brittle but unambiguous; semantic matching could improve flexibility but introduce reward hacking risks

- **Failure signatures:**
  - **Over-refusal**: High SUM ratios (50%) cause models to refuse answerable questions—accuracy drops on AMC23 from 0.57 to ~0.47 for Qwen2.5-7B-Instruct
  - **Under-refusal**: 0% or 1% SUM yields near-zero refusal rates (<0.10) on unanswerable benchmarks
  - **Domain overfitting**: If refusal generalizes to in-domain but not out-of-domain (SelfAware), model learned surface patterns rather than meta-reasoning

- **First 3 experiments:**
  1. **Reproduce hallucination tax baseline**: Train Qwen2.5-7B on DeepScaleR with 0% SUM; verify refusal rate drops to <0.05 on SUM-test and UMWP
  2. **Ablate mixing ratios**: Train separate models at 1%, 10%, 30% SUM; plot refusal rate vs. answerable accuracy trade-off curves (replicate Figure 3)
  3. **Cross-domain generalization test**: Evaluate 10% SUM-trained model on SelfAware (factual QA); verify refusal improvement exceeds +0.50 compared to baseline (Table 2 shows +0.93 for Qwen2.5-7B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training on synthetic unanswerable math problems generalize to safety-critical domains such as medical, legal, or clinical decision-making where refusal behavior is essential?
- Basis in paper: [explicit] The Limitations section states "further evaluation is needed to assess whether these generalization benefits extend to other domains, such as commonsense reasoning, legal QA, or clinical decision-making."
- Why unresolved: The paper only evaluates generalization to factual QA (SelfAware), not to domains where incorrect answers carry higher stakes.
- What evidence would resolve it: Evaluation of SUM-trained models on domain-specific unanswerable question benchmarks in medical, legal, or clinical contexts, measuring both refusal rates and downstream safety outcomes.

### Open Question 2
- Question: How do different forms of instruction tuning and prior alignment affect a model's predisposition to hallucinate or abstain after RFT?
- Basis in paper: [explicit] Section 6.2 notes "our findings also raise questions about how different forms of instruction tuning and prior alignment affect a model's predisposition to hallucinate or abstain—an area that remains underexplored in the RFT literature."
- Why unresolved: The paper observes different learning dynamics between instruction-tuned and non-instruction-tuned models but does not systematically isolate the effects of prior alignment methods.
- What evidence would resolve it: A controlled ablation study comparing RFT behavior across models with varied alignment histories (RLHF vs. DPO vs. no alignment, different instruction-tuning datasets).

### Open Question 3
- Question: Can curriculum learning or adaptive reward shaping dynamically balance refusal behavior and accuracy throughout RFT training, eliminating the need for manual ratio tuning?
- Basis in paper: [explicit] Section 6.2 proposes "future work may explore curriculum learning or adaptive reward shaping to dynamically balance refusal and correctness throughout training."
- Why unresolved: The current approach requires manual tuning of the SUM mixing ratio (10% worked best), which may not generalize across datasets or model scales.
- What evidence would resolve it: Demonstration of a curriculum or adaptive reward method that achieves comparable or better refusal-accuracy trade-offs without requiring dataset-specific hyperparameter search.

## Limitations
- The refusal mechanism relies on exact string matching for "I don't know" outputs, which may not capture semantically equivalent refusals
- The synthetic data generation via o3-mini introduces potential bias from the generator's own reasoning patterns
- Generalization to out-of-domain tasks shows promising initial results but lacks extensive validation across diverse domains beyond mathematics and QA

## Confidence
- **High confidence**: The core observation that RFT degrades refusal behavior is well-supported by consistent results across four models and three unanswerable benchmarks
- **Medium confidence**: The 10% SUM mixing ratio represents an optimal trade-off, though empirical rather than theoretically derived
- **Low confidence**: Claims about "inference-time compute" enabling reasoning about uncertainty could benefit from more direct measurement of reasoning processes

## Next Checks
1. **Reward function robustness test**: Replace exact string matching with semantic refusal detection and measure impact on refusal rates and potential reward hacking
2. **Multi-domain generalization**: Evaluate the 10% SUM-trained models on non-mathematical, non-QA tasks (e.g., code generation, creative writing) to test the breadth of refusal generalization
3. **Temporal stability analysis**: Track refusal rates and accuracy across extended inference-time compute budgets (varying token limits) to confirm that models consistently allocate compute toward solvability assessment