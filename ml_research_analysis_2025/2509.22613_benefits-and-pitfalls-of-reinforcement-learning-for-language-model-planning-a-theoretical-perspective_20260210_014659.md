---
ver: rpa2
title: 'Benefits and Pitfalls of Reinforcement Learning for Language Model Planning:
  A Theoretical Perspective'
arxiv_id: '2509.22613'
source_url: https://arxiv.org/abs/2509.22613
tags:
- training
- arxiv
- planning
- preprint
- q-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes reinforcement learning (RL) for language model
  planning through theoretical and empirical lenses. It shows that supervised fine-tuning
  (SFT) memorizes co-occurrence patterns and fails to generalize due to lack of transitivity
  learning, while RL improves via exploration.
---

# Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective

## Quick Facts
- **arXiv ID**: 2509.22613
- **Source URL**: https://arxiv.org/abs/2509.22613
- **Reference count**: 28
- **Primary result**: RL outperforms SFT for planning primarily through exploration, not gradient dynamics; PG suffers diversity collapse while Q-learning preserves it but requires careful reward design

## Executive Summary
This paper provides theoretical and empirical analysis of reinforcement learning for language model planning, comparing supervised fine-tuning (SFT) with policy gradient (PG) and Q-learning approaches. The work demonstrates that SFT memorizes co-occurrence patterns without learning transitivity, while RL methods improve generalization through exploration. However, PG methods suffer from diversity collapse where output variety decreases even at 100% training accuracy, while Q-learning preserves diversity but requires carefully designed process rewards to avoid reward hacking.

## Method Summary
The paper analyzes planning as graph path-finding where language models generate sequences representing paths between nodes. The experimental setup uses synthetic Erdős-Rényi graphs with 100 nodes and 15% edge probability. Training data is generated via random walks for SFT, then RL fine-tuning is applied using either policy gradient or Q-learning. The model is a one-layer, single-head Transformer with 120-dimensional embeddings. Policy gradient uses on-policy rollouts with optional KL regularization, while Q-learning uses off-policy updates with process rewards that check adjacency and target reachability.

## Key Results
- PG with exploration discovers new valid paths absent from initial training data, enabling better generalization than SFT
- PG without KL regularization exhibits diversity collapse—output entropy decreases monotonically even after 100% training accuracy
- Q-learning with process rewards converges to graph structure and preserves diversity, while outcome-only rewards cause reward hacking
- Empirical validation on synthetic graphs and Blocksworld confirms theoretical predictions about accuracy, diversity, and structure recovery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RL outperforms SFT for planning primarily through exploration-driven data augmentation, not gradient dynamics
- **Mechanism**: Policy Gradient loss on correct paths is mathematically equivalent to SFT loss on that subset (Theorem 4.1). The performance gap emerges because on-policy exploration discovers new valid paths absent from the initial training set, effectively expanding $D_{SFT} \to \bigcup_t D_{RL,t} \cap \mathcal{P}$
- **Core assumption**: Assumption 3.1 (logits depend only on target/current node pairs)
- **Evidence anchors**: [abstract] "RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization"; [Section 4.1] Theorem 4.1 and Theorem 4.2 show PG converges to correct paths for training pairs via gradient, but generalization requires exploration
- **Break condition**: If exploration is constrained (e.g., low temperature, small batch), PG degrades toward continual SFT performance

### Mechanism 2
- **Claim**: Policy Gradient without KL regularization exhibits diversity collapse—output entropy decreases monotonically even after 100% training accuracy
- **Mechanism**: Theorem 4.3 proves $\text{KL}(U_{C(i,j)} \| \text{softmax}(f^t(i,j)))$ is non-decreasing in expectation. Gradients push incorrect logits toward $-\infty$ but do not balance among correct options. The model converges to a single path per source-target pair
- **Core assumption**: Assumption 3.1; infinite training; no KL term ($\lambda=0$)
- **Evidence anchors**: [abstract] "PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained"; [Section 4.1] Theorem 4.3: formal KL divergence bound; Figure 2c shows empirical diversity decline
- **Break condition**: Adding KL regularization with $\lambda > 0$ stabilizes diversity near the base model's distribution (Theorem 4.4), but caps training accuracy

### Mechanism 3
- **Claim**: Q-learning with process rewards converges to graph structure and preserves diversity; outcome-only rewards cause reward hacking
- **Mechanism**: With outcome rewards only, Theorem 5.1 shows all logits $f(i,j)[k]$ for fixed target $i$ collapse to a constant $c_i$ (loss of structure). Process rewards ($R(u,m) = \delta_{u_{m+1}=u_2} - \delta_{(u_m,u_{m+1})\notin E}$) yield Theorem 5.2 convergence: $f^*(i,j)[k] \to 1$ for adjacent+reachable nodes, $0$ or $-1$ otherwise. All feasible actions converge to equal logits, preserving diversity
- **Core assumption**: Persistent exploration (Assumption 5.1); linear Transformer or Assumption 3.1
- **Evidence anchors**: [abstract] "Q-learning offers two key advantages: it preserves output diversity at convergence and supports off-policy learning"; [Section 5.1] Theorems 5.1–5.3; Figure 4 shows logits of feasible nodes converging to uniform high values
- **Break condition**: If process rewards are unavailable or adjacency checks are noisy, Q-learning may not recover structure; off-policy data distribution shift could violate persistent exploration

## Foundational Learning

- **Concept**: **Reachability vs. Adjacency Matrices ($R$ vs. $A$)**
  - Why needed here: The paper models planning as graph path-finding. $A[u,v]=1$ indicates a direct edge; $R[s,t]=1$ indicates path existence. SFT learns co-occurrence (weak proxy for $A$), while Q-learning with process rewards theoretically recovers both
  - Quick check question: Given nodes $a \to b \to c$, what is $A[a,c]$? What is $R[a,c]$?

- **Concept**: **On-policy vs. Off-policy RL**
  - Why needed here: PG is on-policy (data from current model); Q-learning is off-policy (can use any data). This matters for practical training with quantized models or large batches (VeRL framework mention)
  - Quick check question: If you train with data generated by a stale model checkpoint, which algorithm degrades gracefully?

- **Concept**: **KL Regularization Tradeoff**
  - Why needed here: $\lambda$ controls accuracy-diversity balance in PG (Theorem 4.4). High $\lambda$ preserves base model diversity but limits gains; $\lambda=0$ maximizes accuracy but causes collapse
  - Quick check question: If base model has 60% accuracy and you observe diversity collapse at $\lambda=0$, what happens to test accuracy as training continues?

## Architecture Onboarding

- **Component map**: SFT Stage -> PG/Q-learning Stage -> Graph Abstraction
- **Critical path**: 
  1. Construct graph representation of planning domain (e.g., Blocksworld config graph, tool dependency graph)
  2. Generate SFT data via random walks; train base model
  3. Choose RL method:
     - PG: Set $\lambda$ based on diversity requirements; monitor entropy
     - Q-learning: Implement process rewards (adjacency check + target bonus); use $\epsilon$-exploration
  4. Evaluate on held-out source-target pairs ($D_{Test}$)

- **Design tradeoffs**:
  - PG + KL ($\lambda > 0$): Stable diversity, capped accuracy, sensitive to base model quality
  - PG no KL ($\lambda = 0$): Max accuracy, guaranteed diversity collapse, poor generalization on long training
  - Q-learning + outcome: Reward hacking, accuracy collapse (do not use)
  - Q-learning + process: Preserves diversity, off-policy compatible, requires adjacency supervision

- **Failure signatures**:
  - SFT-only: High training accuracy, poor test accuracy; spurious edges in learned adjacency heatmap (Figure 1b)
  - PG diversity collapse: Train accuracy $\to 100\%$, output distinct paths $\to 1$ per query (Figure 2c)
  - Q-learning outcome-only: Train/test accuracy $\to 0$ (Figure 3a)
  - Over-regularized PG: Test accuracy plateaus below optimal (Figure 2d, high $\lambda$)

- **First 3 experiments**:
  1. **SFT baseline**: Train on $D_{SFT}$, evaluate adjacency recovery (heatmap) and test accuracy. Confirm co-occurrence memorization (Theorem 3.1)
  2. **PG diversity sweep**: Run PG with $\lambda \in \{0, 10^{-4}, 10^{-3}, 10^{-2}\}$. Plot train accuracy, test accuracy, and output diversity over time. Verify Theorem 4.3 and 4.4 predictions
  3. **Q-learning reward comparison**: Train Q-learning with outcome-only vs. process rewards. Plot convergence trajectories (Figure 3a); inspect logits heatmap (Figure 4) for structure recovery

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies on strong assumptions about model architecture (Assumption 3.1) that may not hold for deeper or more complex LLMs
- Q-learning's success depends heavily on having correct process rewards that require explicit knowledge of graph adjacency structure
- Empirical validation is limited to synthetic graphs and Blocksworld, with no evidence for complex real-world planning tasks
- The theoretical framework doesn't address noisy or incomplete reward signals common in practical applications

## Confidence
- **Mechanism 1 (RL Generalization via Exploration)**: High Confidence
- **Mechanism 2 (PG Diversity Collapse)**: High Confidence
- **Mechanism 3 (Q-learning Advantages)**: Medium Confidence
- **Overall Claim (RL > SFT for Planning)**: Medium Confidence

## Next Checks
1. Validate theoretical claims on deeper Transformer models to assess robustness when Assumption 3.1 is violated
2. Conduct systematic study on Q-learning's performance using various reward function designs including noisy adjacency checks and sparse rewards
3. Apply methodology to a standard planning benchmark with complex graph structure to assess practical performance outside synthetic settings