---
ver: rpa2
title: Overview of the TREC 2023 deep learning track
arxiv_id: '2507.08890'
source_url: https://arxiv.org/abs/2507.08890
tags:
- queries
- passage
- marco
- ranking
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Overview of the TREC 2023 deep learning track

## Quick Facts
- arXiv ID: 2507.08890
- Source URL: https://arxiv.org/abs/2507.08890
- Reference count: 3
- Primary result: None specified in paper

## Executive Summary
The TREC 2023 Deep Learning Track continued its focus on passage and document ranking using the MS MARCO V2 corpus. The track featured both full ranking and top-100 reranking tasks, with document ranking as a secondary task using labels inferred from passage judgments. A key innovation was the inclusion of synthetic queries generated by LLMs (T5 and GPT-4) alongside human-generated queries, with human filtering applied to ensure usability. The track observed that LLM prompting-based ranking approaches outperformed traditional neural language models (nnlm) when applied to candidate subsets, suggesting a shift toward hybrid retrieval-LLM architectures.

## Method Summary
The track used the MS MARCO V2 corpus (138M passages, 11.9M documents) with a test set of 700 queries (human held-out plus synthetic). Participants could either perform full ranking from the corpus or reranking of top-100 candidates provided via Pyserini. Document relevance was inferred from passage judgments by taking the maximum passage relevance score for each document. The primary evaluation metric was NDCG@10, with NDCG@100 and MAP as secondary metrics. Synthetic queries were generated using T5 (fine-tuned) and GPT-4 (zero-shot), with human filtering applied before evaluation. Near-duplicate passage clusters and passage-to-document mappings were provided to support deduplication and label propagation.

## Key Results
- LLM prompting-based ranking approaches outperformed traditional neural language models (nnlm) on both passage and document ranking tasks
- Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of τ = 0.8487
- Most top-performing prompt-based runs combined neural first-stage retrieval with LLM-based reranking of candidates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM prompting-based ranking can outperform pre-trained neural language models ("nnlm") on passage and document ranking tasks when applied to candidate subsets.
- Mechanism: Systems use traditional or neural retrieval to generate a small candidate set from the full corpus (e.g., 138M passages), then apply expensive LLM prompting (e.g., GPT-4) to rerank or score only those candidates, combining scalable retrieval with high-quality LLM relevance estimation.
- Core assumption: The candidate generator retrieves most relevant items into the top-k, so LLM reranking can improve ordering without processing the full corpus.
- Evidence anchors:
  - [abstract]: "runs using Large Language Model (LLM) prompting in some way outperformed runs that use the 'nnlm' approach, which was the best approach in the previous four years"
  - [section 4]: "We can refer to the participant papers to see how the expensive model was used, but we might expect that trad and nnlm methods were used to retrieve a small number of candidates from the 138 million passage corpus, then the LLM approach was applied to a few results per query"
  - [corpus]: Weak direct evidence; corpus neighbors focus on track overviews, not detailed prompt-based architectures.
- Break condition: If candidate retrieval recall is poor (relevant items not in top-k), or if LLM prompting cost/latency is prohibitive at intended deployment scale.

### Mechanism 2
- Claim: Synthetic queries generated by LLMs (T5, GPT-4) can produce test collections with system rankings similar to those from human queries, provided humans filter for usability.
- Mechanism: Generate synthetic queries from seed passages using T5 (fine-tuned) and GPT-4 (zero-shot), have human assessors reject unreasonable or uninformative queries, then evaluate systems using depth-10 pooling on both synthetic and human queries.
- Core assumption: Human filtering removes low-quality synthetic queries, and remaining queries reflect realistic retrieval challenges without systematic bias toward systems using similar models.
- Evidence anchors:
  - [abstract]: "Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of τ = 0.8487"
  - [section 5]: "Amongst the 48 T5-generated queries, 13 (27.1%) of them were selected and amongst the 49 queries generated using GPT-4, 18 (36.7%) of them were selected"
  - [corpus]: No strong comparative evidence on synthetic query reliability from neighbors.
- Break condition: If synthetic queries systematically favor models similar to the generator (bias), or if human filtering effort outweighs benefits.

### Mechanism 3
- Claim: Propagating passage-level relevance judgments to source documents can reduce annotation cost while maintaining reasonable document ranking evaluation.
- Mechanism: Judges evaluate only passages; for each document, infer its relevance as the maximum judgment across all its constituent passages, avoiding separate document-level annotation.
- Core assumption: Document relevance is well-approximated by the best passage it contains; passages are correctly mapped to their source documents.
- Evidence anchors:
  - [abstract]: "with passage ranking as primary and document ranking as a secondary task (using labels inferred from passage)"
  - [section 2.2]: "Previous work has shown that such an approach results in reasonable quality relevance judgments [Wu et al., 2019], and our study on the 2021 test collections further validated this"
  - [corpus]: No direct counter-evidence; neighbors do not address this mechanism.
- Break condition: If relevant document content is spread across multiple passages such that no single passage is highly relevant, or if passage-document mappings are noisy.

## Foundational Learning

- Concept: Neural ranking model categories ("trad", "nn", "nnlm", "prompt")
  - Why needed here: The track categorizes runs to compare approaches; understanding distinctions is essential for interpreting results and choosing architectures.
  - Quick check question: Can you explain why "nnlm" (pre-trained neural models) differs from "prompt" (LLM prompting) in how they use training data?

- Concept: Test collection reusability and judgment completeness
  - Why needed here: The track design aims for reusable test collections; incomplete judgments bias evaluation toward systems that retrieve judged documents.
  - Quick check question: Why does using queries held out from corpus construction improve test collection difficulty and reusability?

- Concept: Kendall's τ and system ordering agreement
  - Why needed here: Used to quantify whether synthetic and human queries produce similar system rankings.
  - Quick check question: If τ = 0.8487 between synthetic and human query evaluations, what does that imply about synthetic query reliability?

## Architecture Onboarding

- Component map:
  - Candidate retrieval (BM25, SPLADE, dense retrievers) → Reranking (nnlm or prompt-based LLM) → Score aggregation → Ranked output
  - Query sampling pipeline: held-out MS MARCO queries + synthetic T5/GPT-4 queries → human filtering → depth-10 pooling → NIST assessors → qrels
  - Evaluation: trec_eval with expanded qrels (including near-duplicate propagation)

- Critical path:
  1. Understand the task: passage full-ranking vs. reranking (top-100 provided)
  2. Build or select candidate retriever; ensure it can scale to 138M passages
  3. Integrate LLM prompt-based reranker; design prompts for relevance scoring
  4. Submit runs with correct metadata (run type, training data, external resources)

- Design tradeoffs:
  - Cost vs. quality: GPT-4 reranking is expensive; limit to top-k candidates
  - Synthetic vs. human queries: Synthetic queries reduce human query collection effort but require human filtering
  - Passage vs. document judgment: Passage-only judgment reduces cost but assumes passage-document mapping is accurate

- Failure signatures:
  - Low recall in candidate retrieval: Top-k misses relevant items; LLM reranking cannot recover
  - Prompt misalignment: LLM scores do not correlate with human relevance; system underperforms
  - Qrel sparsity: Incomplete judgments cause unfair metric comparisons across runs
  - Near-duplicate explosion: Without deduplication, judgment budget wasted on redundant passages

- First 3 experiments:
  1. Baseline retrieval comparison: Run BM25 vs. SPLADE vs. dense retriever on MS MARCO v2; measure recall@100 on held-out queries.
  2. LLM reranking ablation: Given fixed top-100 candidates, compare GPT-4 prompt-based reranking vs. nnlm reranker (e.g., monoT5); measure NDCG@10.
  3. Synthetic query validation: Generate synthetic queries from seed passages; compare system rankings using synthetic vs. human queries on a shared test set; compute Kendall's τ.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetically generated queries reliably replace human queries for the construction of reusable test collections across different retrieval tasks?
- Basis in paper: [explicit] The authors state in the conclusion and analysis sections that initial results suggest synthetic queries lead to similar evaluation outcomes, but "more analysis is needed to validate this, which is left as future work."
- Why unresolved: While system ordering agreement was high (τ=0.8487), the study was limited to a specific set of MS MARCO v2 queries and required human effort to filter out unusable synthetic queries.
- What evidence would resolve it: A comparative study across multiple, diverse test collections showing that evaluation rankings derived from unfiltered or lightly filtered synthetic queries remain consistent with those derived entirely from human user queries.

### Open Question 2
- Question: Is task-specific fine-tuning (on datasets like MS MARCO) strictly necessary for prompt-based Large Language Model (LLM) ranking systems to achieve state-of-the-art performance?
- Basis in paper: [explicit] The authors note that while prompt-based runs outperformed "nnlm" runs, "most top runs using the 'prompt' approach did self report as using the MS MARCO training data." They explicitly interest in "how well a prompt approach can work... in a system that has no use of MS MARCO for fine tuning any dense retrieval or ranking stages."
- Why unresolved: The top-performing "prompt" runs utilized a hybrid approach where LLMs were likely used for reranking candidates retrieved by models fine-tuned on MS MARCO, making it difficult to isolate the zero-shot efficacy of the LLM component alone.
- What evidence would resolve it: Ablation studies comparing end-to-end prompt-based retrieval systems with and without MS MARCO-specific fine-tuning on the candidate generation stages.

### Open Question 3
- Question: Does the use of specific models (e.g., GPT-4 or T5) for synthetic query generation introduce a latent evaluation bias favoring systems that utilize the same underlying architectures?
- Basis in paper: [explicit] The paper investigates this specific risk, noting: "We did not see clear evidence of bias... However, more analysis is needed to validate this."
- Why unresolved: The evaluation showed only slight over-estimation or under-estimation of performance for specific model types, but the statistical significance and generalizability of these observations remain unclear due to the limited sample size of 82 queries.
- What evidence would resolve it: Large-scale statistical testing on a wider variety of query sets and retrieval systems to determine if lexical or semantic artifacts specific to the generating model correlate artificially with higher retrieval scores for that same model family.

## Limitations
- The paper presents track results rather than novel empirical claims, limiting direct verification
- Incomplete documentation of exact LLM prompting strategies and parameters used in top-performing systems
- Lack of detailed comparison between synthetic and human query generation quality beyond τ statistics

## Confidence
- Synthetic query evaluation reliability: Medium (based on moderate τ but limited bias analysis)
- LLM prompting superiority: Low-Medium (stated as observed result, but prompting details unspecified)
- Passage-to-document relevance propagation: Medium (cited prior validation, but no new metrics reported)

## Next Checks
1. Replicate the passage retrieval comparison: run BM25 vs. SPLADE vs. dense retriever on MS MARCO v2 and measure recall@100 on held-out queries
2. Implement and test a simple GPT-4 prompt-based reranker on top-100 candidates; compare against nnlm baseline on the official 82-query qrels
3. Generate synthetic queries from MS MARCO passages using T5 and GPT-4; compare system rankings against human queries on a shared test subset and compute Kendall's τ