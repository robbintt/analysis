---
ver: rpa2
title: 'Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding'
arxiv_id: '2511.06376'
source_url: https://arxiv.org/abs/2511.06376
tags:
- function
- positional
- lemma
- theorem
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the universal approximation property (UAP)
  of single-layer Transformers in vocabulary in-context learning (VICL), focusing
  on the role of positional encoding. Without positional encoding, VICL in single-layer
  Transformers does not possess the UAP.
---

# Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding

## Quick Facts
- arXiv ID: 2511.06376
- Source URL: https://arxiv.org/abs/2511.06376
- Reference count: 40
- Without positional encoding, VICL in single-layer Transformers does not possess the universal approximation property (UAP).

## Executive Summary
This paper investigates the universal approximation property (UAP) of single-layer Transformers in vocabulary in-context learning (VICL), focusing on the role of positional encoding. The key finding is that VICL without positional encoding cannot achieve UAP, but with appropriate positional encoding, UAP becomes possible. The paper provides several sufficient conditions for positional encoding, including cases with ReLU or softmax activation functions. These findings reveal the critical importance of positional encoding for achieving universal approximation in Transformers from an approximation theory perspective.

## Method Summary
The paper proves that single-layer Transformers performing VICL can achieve universal approximation when equipped with appropriate positional encoding. The method establishes an equivalence between single-layer Transformers and one-hidden-layer feed-forward neural networks under specific sparse matrix structure assumptions. The proofs use the Kronecker Approximation Theorem to show that dense positional encodings enable the construction of a "dense" effective weight space from discrete vocabulary tokens. The paper provides constructive proofs showing how to build context matrices that enable arbitrary function approximation.

## Key Results
- VICL without positional encoding cannot achieve universal approximation property (Theorem 6)
- With dense positional encoding, VICL can approximate any continuous function to arbitrary precision (Theorem 7)
- ReLU activations require global density of positional encodings, while exponential/softmax only need local density (Theorem 8)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context demonstrations function as weight parameters of an implicit feed-forward neural network (FNN), allowing fixed Transformer to approximate varied functions without weight updates.
- **Mechanism:** Context tokens (X, Y) map to weight matrices (W, A) of a one-hidden-layer FNN. Self-attention computes Y σ(XᵀBᵀC x̃), mirroring FNN form A σ(Wx + b).
- **Core assumption:** Assumption 1 holds with sparse partition structure and non-singular matrices.
- **Evidence anchors:** [abstract] "context can serve as a control parameter for the model..."; [section 2.3] "Lemma 3... implies that single-layer Transformers... and FNNs... are equivalent."
- **Break condition:** If relationship between input and output in context cannot form arbitrary linear combinations, or if Assumption 1 is violated.

### Mechanism 2
- **Claim:** Positional Encoding allows Transformers with finite vocabularies to achieve UAP by constructing a "dense" effective weight space from discrete tokens.
- **Mechanism:** Finite vocabulary restricts possible weights to finite set. Dense positional encoding set P makes Vₓ + P dense in Rᵈ, allowing approximation of any real-valued weight vector required by universal approximation theorem.
- **Core assumption:** Positional encoding set P must be countably infinite and dense in target domain.
- **Evidence anchors:** [abstract] "...when appropriate positional encodings are used, UAP becomes possible."; [section 4] Theorem 7 states S dense in Rᵈᵡ enables UAP.
- **Break condition:** If positional encodings are removed or PE set is finite/bounded such that S is not dense.

### Mechanism 3
- **Claim:** Exponential activation functions allow UAP with strictly weaker density requirements for positional encodings compared to ReLU.
- **Mechanism:** For ReLU, weights must span global range requiring global density of PE. For exponential activations, Taylor expansions allow approximation using derivatives around single point, requiring only local density of PE.
- **Core assumption:** Activation function is exponential or softmax, and domain is compact.
- **Evidence anchors:** [section 4] Theorem 8 differentiates conditions for ReLU vs. Exponential; [section E.5] Lemma 16 shows finite differences approximate derivatives.
- **Break condition:** If using ReLU with locally dense but globally sparse PE, or insufficient context length for finite difference approximations.

## Foundational Learning

- **Concept:** Universal Approximation Property (UAP)
  - **Why needed here:** Central theoretical metric measuring ability to approximate any continuous function to arbitrary precision.
  - **Quick check question:** Does a model with finite vocabulary but infinite context length satisfy UAP without positional encoding? (Answer: No, Theorem 6).

- **Concept:** Kronecker Approximation Theorem
  - **Why needed here:** Explains how discrete combinations like q√2 ± l can be dense in real number line, showing how discrete tokens approximate continuous weights.
  - **Quick check question:** Why is density of set S = Vₓ + Pₓ mathematically necessary for proofs? (Answer: To approximate continuous weight vectors wᵢ of target FNN).

- **Concept:** Absolute vs. Relative Positional Encoding (APE/RPE)
  - **Why needed here:** Paper focuses on APE; notes standard RoPE/RPE formulations often fail density conditions required for proof.
  - **Quick check question:** Why might RoPE struggle with this specific UAP proof mechanism? (Answer: Rotation matrices generally don't generate dense subset like additive encodings).

## Architecture Onboarding

- **Component map:** Input Layer (Finite Vocabulary Embedding + Dense Positional Encoding) → Core (Single-Layer Self-Attention with activation) → Constraint (Assumption 1 structure)

- **Critical path:**
  1. Define Vocabulary V (finite)
  2. Inject Dense PE (crucial step; standard Sinusoidal may not suffice)
  3. Construct Context (X, Y)
  4. Attention mechanism maps Context → Implicit Weights

- **Design tradeoffs:**
  - ReLU vs. Exponential: ReLU requires unbounded/dense global PE (harder to implement); Exponential/Softmax only needs local density (easier to satisfy)
  - Context Length vs. Precision: Proofs require potentially unbounded context length n to achieve ε → 0 approximation

- **Failure signatures:**
  - Vocabulary Bottleneck: Without PE, outputs restricted to finite-dimensional function space, leading to irreducible error
  - Sparse PE: If PE not dense, "weight" approximation fails and UAP is lost

- **First 3 experiments:**
  1. Verify Non-UAP without PE: Train/freeze single-layer Transformer on regression task using only finite vocabulary without positional encoding. Show error plateaus regardless of context length.
  2. Dense PE Injection: Implement positional encoding scheme ensuring {v + p | v ∈ V, p ∈ P} dense in [-1,1]ᵈ. Compare approximation error against standard Sinusoidal PE.
  3. Activation Ablation: Compare ReLU vs. Softmax transformers on "Dense PE" setup. Verify if Softmax requires fewer distinct positional encodings (local density) to converge compared to ReLU.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multi-layer Transformers with finite vocabularies and positional encoding achieve UAP?
- **Basis in paper:** [explicit] "Future research should extend these findings to multi-layer Transformers..."
- **Why unresolved:** Paper restricts analysis to single-layer; multi-layer architectures introduce attention layer interactions requiring new proof techniques.
- **What evidence would resolve it:** Theoretical proof extending Theorems 7-8 to multi-layer cases, or counterexample showing depth alone is insufficient.

### Open Question 2
- **Question:** Do UAP results extend to relative positional encodings (RPEs) and rotary positional embeddings (RoPE)?
- **Basis in paper:** [explicit] "Future research should extend these findings to... general positional encodings (such as RPEs and RoPE)..." and Appendix E.4 states density-based argument doesn't directly apply to RoPE.
- **Why unresolved:** RoPE's rotation operation acts on distinct 2D subspaces; induced family of matrices doesn't generate dense subset under current framework.
- **What evidence would resolve it:** Alternative proof techniques specific to rotation-based encodings, or characterization of which RPE formulations permit UAP.

### Open Question 3
- **Question:** Can softmax-activation Transformers with positional encoding and finite vocabulary achieve UAP?
- **Basis in paper:** [explicit] "For softmax Transformers, our analysis... highlighted their connection to Transformers with exponential activations. However, extending this connection... proves challenging and requires more sophisticated techniques."
- **Why unresolved:** Normalization operation in softmax creates coupling between terms that breaks constructive proof technique used for element-wise activations.
- **What evidence would resolve it:** Construction showing how softmax normalization can be controlled through positional encoding choices, or proof that UAP is impossible under softmax.

### Open Question 4
- **Question:** What are minimal density requirements on positional encoding sets for UAP in practical finite-context scenarios?
- **Basis in paper:** [inferred] Paper assumes unbounded context length and strong density conditions, but notes "in practical applications, input sequences are finite and are typically truncated."
- **Why unresolved:** Gap between theoretical assumption of countably infinite positional encodings and finite-context practical implementations remains unexplored.
- **What evidence would resolve it:** Bounds relating context length, vocabulary size, and achievable approximation error for specific function classes.

## Limitations

- Theoretical scope relies heavily on equivalence between single-layer Transformers and one-hidden-layer FNNs under specific sparse matrix structure assumptions
- Doesn't specify concrete positional encoding functions that satisfy density requirements while remaining computationally tractable
- Proofs require potentially unbounded context length to achieve arbitrary precision, making practical feasibility assessment difficult

## Confidence

**High Confidence Claims:**
- VICL without positional encoding cannot achieve UAP
- With appropriate positional encoding, VICL can achieve UAP
- Mechanism linking context tokens to implicit FNN weights is mathematically sound under stated assumptions

**Medium Confidence Claims:**
- Kronecker Approximation Theorem application for density arguments
- Specific conditions on vocabulary structure required for proofs
- Practical implications of ReLU vs. exponential activation differences

**Low Confidence Claims:**
- Direct applicability to standard Transformer architectures without modification
- Specific positional encoding schemes satisfying density requirements in practice
- Resource requirements for achieving meaningful approximation precision

## Next Checks

1. **Empirical UAP Verification:** Implement single-layer Transformer architecture following paper's specifications and test on regression tasks with known continuous functions. Measure whether approximation error plateaus without positional encoding (validating Theorem 6) and decreases with dense positional encoding (validating Theorem 7).

2. **Practical Positional Encoding Design:** Design and test positional encoding schemes that provably generate dense sets S = Vₓ + Pₓ. Evaluate both theoretical density properties and empirical approximation performance against standard sinusoidal positional encodings on benchmark VICL tasks.

3. **Assumption Relaxation Study:** Experiment with different initialization schemes and matrix structures to test boundaries of Assumption 1. Measure how relaxing sparse partition structure affects UAP capability and approximation quality, providing practical guidance on architectural design.