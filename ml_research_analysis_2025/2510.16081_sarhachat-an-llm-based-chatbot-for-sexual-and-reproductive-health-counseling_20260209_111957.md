---
ver: rpa2
title: 'SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling'
arxiv_id: '2510.16081'
source_url: https://arxiv.org/abs/2510.16081
tags:
- sarhachat
- health
- medical
- conversational
- chatbot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SARHAchat addresses the challenge of providing accurate, empathetic,
  and user-centered AI-driven counseling for sensitive sexual and reproductive health
  topics. It integrates a bi-level memory system combining short-term dialogue context
  with an external long-term knowledge base of verified contraceptive guidelines,
  structured reasoning chains, and guided response generation via large language models.
---

# SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling

## Quick Facts
- arXiv ID: 2510.16081
- Source URL: https://arxiv.org/abs/2510.16081
- Reference count: 10
- One-line primary result: 98.22% medical safety pass rate with zero contraindicated recommendations and only 3 critical information errors

## Executive Summary
SARHAchat is an LLM-based chatbot designed to provide safe, empathetic, and accurate counseling for sexual and reproductive health, specifically contraceptive recommendations. It employs a bi-level memory system that combines short-term dialogue context with an external long-term knowledge base of verified CDC guidelines, structured reasoning chains, and guardrails to ensure medical safety and conversational quality. Evaluation shows significant improvements over a baseline system, with high medical safety pass rates and user satisfaction, demonstrating strong potential for clinical deployment in pre-clinical contraceptive care.

## Method Summary
The system uses a bi-level memory architecture with short-term dialogue context and an external long-term knowledge base of CDC contraceptive guidelines. A 5-stage conversational flow (initial, preference, health screening, recommendation, PDF generation) is managed by a stage tracker and reinforced by guardrails. Structured reasoning chains are injected into prompts to guide the LLM toward clinically valid recommendations. AWS serverless functions handle orchestration, while GPT-4 generates responses. The system produces downloadable PDF summaries for clinical use.

## Key Results
- Medical Safety Pass Rate: 98.22% (vs 85.21% baseline)
- Zero contraindicated recommendations (vs 13 baseline)
- Only 3 critical information errors (vs 11 baseline)
- Conversational Quality: 98.82% satisfactory (vs 89.35% baseline)

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Grounding via Bi-Level Memory
Separating short-term dialogue context from verified long-term medical knowledge reduces hallucination and improves recommendation accuracy. The system queries a structured key-value store containing current CDC guidelines rather than relying on LLM parametric memory. This grounds contraceptive recommendations in externally verified criteria.

### Mechanism 2: Structured Reasoning Chains with Thought Injection
Explicit step-by-step reasoning chains injected into prompts constrain LLM outputs to clinically valid paths. A reasoning chain—considering user preferences, medical history, and weighted decision criteria—is prepended to the generation prompt, guiding the LLM to produce justified recommendations rather than free-form responses.

### Mechanism 3: Stage-Gated Conversation Flow with Guardrails
Enforcing a five-stage conversational structure with explicit guardrails reduces omission errors and ensures systematic screening. A stage tracker manages transitions; guardrails submodule 1 verifies question-asking behavior, and submodule 2 checks that recommendations incorporate health screening data and preferences before output.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Understands how external knowledge is dynamically retrieved and injected into LLM context windows to reduce hallucination.
  - Quick check question: Can you explain how a retrieved CDC guideline snippet would be incorporated into a prompt for contraceptive recommendation?

- Concept: Chain-of-Thought Prompting / Thought Injection
  - Why needed here: The system relies on explicit reasoning chains to guide generation; understanding prompt structure is essential for modifying decision logic.
  - Quick check question: How would you modify the reasoning chain template to add a new contraindication criterion?

- Concept: Healthcare Chatbot Evaluation Taxonomy
  - Why needed here: Distinguishing medical safety metrics (contraindications, omissions) from conversational quality metrics (empathy, coherence) is necessary for interpreting results and designing future evaluations.
  - Quick check question: What is the difference between a "critical information error" and a "conversational quality" failure in this system?

## Architecture Onboarding

- Component map: Frontend UI -> Stage tracker -> Short-term memory (AWS S3) -> Long-term memory (key-value store) -> RAG retrieval -> Reasoning chain construction -> Thought injection -> GPT-4 API -> Guardrails (2 submodules) -> Response output -> PDF generation

- Critical path: 1. User input received → Stage tracker identifies current phase 2. Short-term memory extracts relevant user factors 3. Long-term memory queried for applicable CDC criteria 4. Structured reasoning chain constructed with retrieved data 5. Thought injection into LLM prompt → Response generated 6. Guardrails verify output before delivery 7. Profile PDF generated at final stage

- Design tradeoffs: Structured 5-stage flow increases safety but may reduce conversational flexibility compared to open-ended chat; external knowledge base ensures accuracy but requires expert maintenance and synchronization with guideline updates; guardrails add latency and complexity but catch critical failures before user exposure

- Failure signatures: Health screening omission (conversation advances without required medical history questions); Contraindicated recommendation (method suggested despite MEC criteria ruling it out); Critical information error (incomplete or incorrect side-effect/contraindication details provided); Stage stall (user deflection causes repeated questioning without progress)

- First 3 experiments: 1. Replicate baseline vs. SARHAchat comparison on held-out synthetic dialogues to verify reported safety and quality differentials 2. Ablate the long-term memory component (use LLM-only) to quantify hallucination reduction contribution 3. Inject adversarial user inputs (conflicting health information, ambiguous preferences) to test guardrails robustness and stage transition logic

## Open Questions the Paper Calls Out

### Open Question 1
Does SARHAchat maintain its high medical safety and conversational quality when deployed to real-world patient populations compared to the synthetic and professional-based evaluation? The current results reflect performance in a controlled environment with expert or simulated users, which may not capture the linguistic variability, health literacy levels, or unpredictability of real patients. Results from a pilot study or clinical trial measuring safety pass rates and satisfaction scores with actual patients would resolve this.

### Open Question 2
Does the integration of SARHAchat-generated summaries into clinical workflows significantly reduce provider administrative burden or improve consultation efficiency? While the chatbot successfully generates profiles, it is unproven whether these outputs are trusted by clinicians or effectively save time during the actual clinical encounter. A comparative study measuring provider time-on-task and perceived utility when using SARHAchat summaries versus standard intake procedures would resolve this.

### Open Question 3
Are there disparities in the system's recommendation accuracy or empathetic engagement across different demographic groups or health literacy levels? The aggregate "Satisfactory" rating of 98.82% may mask specific failure modes for underrepresented groups or complex medical histories within the synthetic dataset. Stratified analysis of the failure cases and quality scores across race, gender, and medical complexity variables within the existing synthetic dataset would resolve this.

## Limitations

- Knowledge base completeness and update latency: The system's safety gains depend on the external knowledge base containing complete, current CDC guidelines. If guidelines change faster than the knowledge base is updated, or if retrieval misses critical contraindications, safety metrics will degrade.

- Reasoning chain completeness: The structured reasoning chain template is asserted to encode clinical decision logic, but no explicit validation is provided that it covers all contraceptive eligibility criteria. If the chain omits criteria, LLM-guided recommendations may still fail.

- Baseline implementation ambiguity: The comparison to a "naive prompting" baseline lacks detail on prompt structure, context window usage, or knowledge integration method, making it difficult to isolate the contribution of each architectural component.

## Confidence

- High confidence: Safety and quality metrics improvement (98.22% vs 85.21% medical safety pass rate; 98.82% vs 89.35% conversational quality satisfactory). The reduction in contraindicated recommendations (0 vs 13) and critical information errors (3 vs 11) is directly reported and unambiguous.

- Medium confidence: Causal attribution to bi-level memory and reasoning chains. While metrics improve, the paper does not perform ablation studies to quantify the individual contributions of memory architecture, reasoning chain, or guardrails.

- Low confidence: Generalizability beyond contraceptive counseling. The architecture and evaluation are narrowly scoped; extension to other SRH topics or clinical domains is untested.

## Next Checks

1. **Ablation study of long-term memory**: Disable external knowledge base retrieval and compare safety/quality metrics to confirm hallucination reduction is due to RAG, not LLM parametric knowledge.

2. **Reasoning chain completeness audit**: Cross-check the reasoning chain template against full CDC MEC and SPR criteria to identify any missing eligibility rules.

3. **Adversarial input robustness test**: Feed the system synthetic user profiles with conflicting health information or ambiguous preferences to test guardrails resilience and stage transition logic under edge cases.