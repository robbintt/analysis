---
ver: rpa2
title: On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification
  for LLMs
arxiv_id: '2510.04439'
source_url: https://arxiv.org/abs/2510.04439
tags:
- uncertainty
- probability
- answers
- sequences
- unobserved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantifying uncertainty in
  large language models (LLMs), which is crucial for detecting hallucinations and
  improving reliability in safety-critical applications. Current entropy-based uncertainty
  quantification methods, such as Predictive Entropy and Semantic Entropy, estimate
  uncertainty using only the probabilities of observed output sequences, neglecting
  the probability mass of unobserved but possible sequences.
---

# On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs

## Quick Facts
- arXiv ID: 2510.04439
- Source URL: https://arxiv.org/abs/2510.04439
- Authors: Lucie Kunitomo-Jacquin; Edison Marrese-Taylor; Ken Fukuda
- Reference count: 5
- Primary result: EOS-UP achieves comparable performance to state-of-the-art methods while maintaining strong performance with single sample

## Executive Summary
This paper addresses a fundamental limitation in sample-based uncertainty quantification for large language models (LLMs): the failure to account for probability mass in unobserved sequences. Current entropy-based methods estimate uncertainty only from observed samples, implicitly assuming the missing probability is negligible. The authors propose Unobserved Probability (UP), which directly incorporates P(Ā|x) = 1 - Σ p(s|x) into uncertainty quantification. Two variants are presented: EOS-UP, which properly accounts for end-of-sequence tokens to ensure valid probability spaces, and LN-UP, which uses conventional sequence probability calculations. Experiments on TriviaQA with falcon-40b-instruct demonstrate that EOS-UP achieves performance comparable to Predictive Entropy while maintaining strong performance even with a single sample, whereas LN-UP degrades significantly as sample count increases.

## Method Summary
The proposed Unobserved Probability (UP) method quantifies uncertainty by computing the probability mass of unobserved sequences: P(Ā|x) = 1 - Σ p(s|x). Two variants are presented: EOS-UP, which includes the end-of-sequence (EOS) token probability to ensure sequences are mutually exclusive events, and LN-UP, which uses standard sequence probability calculation without EOS inclusion or length normalization. EOS-UP requires white-box access to extract individual token probabilities and EOS probabilities from the LLM. The method samples M sequences from the model, computes each sequence's probability, sums observed probabilities, and returns the complement as the uncertainty score. Experiments compare EOS-UP against state-of-the-art methods (Predictive Entropy, Semantic Entropy, Deterministic Semantic Entropy) on the TriviaQA dataset using the falcon-40b-instruct model.

## Key Results
- EOS-UP achieves performance comparable to Predictive Entropy (state-of-the-art) while maintaining strong performance even with single sample (M=1)
- LN-UP variant performs poorly, particularly as sample count increases, showing dramatic performance degradation
- Results demonstrate that accounting for unobserved sequence probability is critical for effective uncertainty quantification, especially in low-sample scenarios
- EOS-UP shows consistent performance across different sample counts while other methods require multiple samples to achieve comparable results

## Why This Works (Mechanism)

### Mechanism 1
Incorporating the probability of unobserved sequences improves uncertainty estimation, particularly in low-sample regimes. Current entropy-based methods estimate uncertainty only from observed samples, implicitly assuming the unobserved probability mass is negligible. The proposed UP method explicitly computes P(Ā|x) = 1 - Σ p(s|x), which captures epistemic uncertainty from limited sampling. When model confidence is high, observed sequences concentrate probability mass, reducing P(Ā|x); under high uncertainty, probability mass disperses, increasing P(Ā|x).

### Mechanism 2
Proper sequence probability calculation requires including the EOS token to ensure mutual exclusivity of sequences in the probability space. Without EOS token inclusion, shorter sequences are not mutually exclusive with their extensions (e.g., "vatican" and "vatican city" overlap). The EOS token terminates sequences, creating disjoint events. Formally: p(s|x) = Πᵢ p(tᵢ|t<tᵢ,x) × p(EOS|t≤N,x), ensuring Σ p(s|x) ≤ 1 and enabling valid P(Ā|x) computation.

### Mechanism 3
Length normalization corrupts probability mass estimation, degrading unobserved probability as a UQ signal. Length normalization (log p'(s|x) = (1/N) Σ log p(tᵢ|t<tᵢ,x)) produces values that do not sum to 1 across sequences, violating probability axioms. This distortion compounds as sample count M increases, causing LN-UP's observed performance collapse.

## Foundational Learning

- Concept: Predictive Entropy estimation via Monte Carlo sampling
  - Why needed here: The baseline methods (E, SE, DSE) all estimate entropy from sampled sequences; understanding their limitations motivates the UP approach.
  - Quick check question: Given M=3 sampled sequences with probabilities [0.4, 0.3, 0.2], what is the naive predictive entropy, and what probability mass is unaccounted for?

- Concept: Autoregressive language model probability decomposition
  - Why needed here: The paper's sequence probability calculations (Equations 2, 3, 6) depend on understanding how token-level conditional probabilities compose.
  - Quick check question: Why does the product of conditional probabilities p(tᵢ|t<tᵢ,x) yield a joint probability that decreases with sequence length?

- Concept: Aleatoric vs. epistemic uncertainty distinction
  - Why needed here: The paper argues that sample-based methods introduce epistemic uncertainty from limited sampling; P(Ā|x) captures this missing information component.
  - Quick check question: If you increase the number of samples M, which type of uncertainty should decrease, and how should P(Ā|x) change?

## Architecture Onboarding

- Component map:
  - Sampling module -> Probability calculator -> Unobserved mass estimator -> UQ output

- Critical path:
  1. Prompt LLM with question → collect M sampled sequences
  2. Extract token-level conditional probabilities (requires white-box access)
  3. Compute each sequence's probability using Equation 6 (EOS-inclusive)
  4. Sum observed probabilities → compute complement
  5. Output P(Ā|x) as uncertainty measure

- Design tradeoffs:
  - EOS-UP vs. LN-UP: EOS-UP requires extracting EOS token probabilities (implementation-dependent) but yields valid probability spaces; LN-UP is simpler but degrades with more samples
  - Sample count M: More samples improve entropy-based methods but reduce P(Ā|x)'s marginal contribution; EOS-UP remains stable at M=1
  - Prompt style: SHORT prompts yield higher AUROC in experiments (Figure 2 top vs. bottom), possibly due to reduced sequence variability

- Failure signatures:
  - P(Ā|x) near 1.0 with high model confidence: Indicates probability extraction errors or EOS miscalibration
  - LN-UP outperforming EOS-UP: Suggests implementation bug in EOS probability extraction
  - Identical AUROC for all methods at M=1: Expected (monotonic relationship to p(s₁|x)); if not observed, check ranking logic

- First 3 experiments:
  1. Replicate EOS-UP vs. LN-UP comparison on TriviaQA subset (n=100) with M=1,5,10 to validate the performance divergence pattern.
  2. Ablate EOS token inclusion: Compute P(Ā|x) with and without EOS probability multiplication on a controlled example (e.g., the "vatican"/"vatican city" case) to verify mutual exclusivity effects.
  3. Test sample efficiency: Compare EOS-UP vs. Predictive Entropy AUROC at M=1 only; EOS-UP should match or exceed E without requiring multiple samples.

## Open Questions the Paper Calls Out
None

## Limitations

- Model Access Dependency: EOS-UP requires white-box access to extract individual token probabilities and EOS token probabilities, limiting applicability to closed models where such access is unavailable.
- Dataset and Model Generalization: Results are demonstrated only on TriviaQA dataset with falcon-40b-instruct model; performance may vary significantly across different domains and model families.
- Temperature and Sampling Configuration: Experiments use fixed temperature T=1, top-K=50, and nucleus sampling p=0.9; different sampling strategies may impact the effectiveness of the unobserved probability signal.

## Confidence

**Claim 1**: "Unobserved probability mass meaningfully improves uncertainty quantification, especially at low sample counts" - **High Confidence**
- Strong experimental evidence shows EOS-UP matches state-of-the-art methods at M=1 while maintaining performance across sample counts

**Claim 2**: "EOS token inclusion is necessary for proper probability space construction" - **High Confidence**
- Mathematical proof provided for mutual exclusivity with EOS
- Empirical validation shows LN-UP degradation with increased samples

**Claim 3**: "Length normalization corrupts probability mass estimation and degrades UQ performance" - **High Confidence**
- Direct experimental comparison shows LN-UP consistently underperforming EOS-UP
- Monotonic degradation pattern as M increases observed

## Next Checks

1. Cross-model validation: Implement EOS-UP on multiple LLM architectures (e.g., LLaMA, Mistral, GPT-2) and evaluate on diverse datasets (e.g., Natural Questions, SQuAD, GSM8K) to assess generalization.

2. Closed-model approximation study: Develop and evaluate approximation methods for EOS-UP when white-box access is unavailable, such as estimating EOS probabilities from sequence length distributions.

3. Probability space validation: Conduct statistical tests on the EOS-UP probability space to verify mutual exclusivity and proper normalization properties. Measure EOS token calibration across different sequence types.