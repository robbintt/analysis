---
ver: rpa2
title: Multi-Agent Inverse Q-Learning from Demonstrations
arxiv_id: '2503.04679'
source_url: https://arxiv.org/abs/2503.04679
tags:
- learning
- reward
- agent
- multi-agent
- mamql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of inferring reward functions
  in multi-agent general-sum games from expert demonstrations, a problem that becomes
  increasingly complex due to non-stationarity and variance from multiple agents.
  The proposed method, Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL),
  learns marginalized critics for each agent, allowing the use of generalized Boltzmann
  policies and enabling effective balancing of cooperative and competitive objectives.
---

# Multi-Agent Inverse Q-Learning from Demonstrations

## Quick Facts
- arXiv ID: 2503.04679
- Source URL: https://arxiv.org/abs/2503.04679
- Reference count: 40
- Primary result: Method learns reward functions in multi-agent general-sum games from demonstrations, achieving 2-5x improvement in sample efficiency and convergence speed over baselines.

## Executive Summary
This paper addresses the challenge of inferring reward functions in multi-agent general-sum games from expert demonstrations. The proposed method, Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), learns marginalized critics for each agent, enabling the use of generalized Boltzmann policies and effective balancing of cooperative and competitive objectives. By connecting marginalized critics to single-agent soft-Q IRL, MAMQL provides a direct optimization criterion that scales well to multi-agent settings.

## Method Summary
MAMQL learns marginalized critics Q̄ᵢ(s, aᵢ) for each agent, where the critic is marginalized over other agents' policies. The method uses a direct Q-learning objective with a concave regularizer (Pearson χ²) rather than adversarial training, and recovers rewards via MSE constraints. Policies are extracted as Boltzmann distributions over the marginalized critics. The approach handles both cooperative and competitive objectives through the generalized Boltzmann formulation, and uses a one-sample estimate for the marginalized value function to avoid exponential computational complexity.

## Key Results
- MAMQL outperforms previous methods in average reward, sample efficiency, and reward recovery
- Achieves 2-5x improvements in convergence speed and sample efficiency
- Shows superior robustness, particularly in environments requiring complex coordination
- Demonstrates 2-4x faster convergence than baselines across Gems, Overcooked, and Highway-Env environments

## Why This Works (Mechanism)

### Mechanism 1: Marginalized Critics Reduce Multi-Agent Complexity to Single-Agent Form
For each agent i, MAMQL computes Q̄ᵢ(s, aᵢ) = E[a₋ᵢ~π₋ᵢ][Q(s, aᵢ, a₋ᵢ)], converting the joint action-value function into a per-agent critic. This marginalization satisfies a soft Bellman condition equivalent to the single-agent case, allowing direct application of IQ-Learn's concave optimization objective. The core assumption is that other agents' policies π₋ᵢ are approximately stationary or can be treated as fixed during each update step.

### Mechanism 2: Generalized Boltzmann Policies Balance Cooperative and Competitive Objectives
The policy πᵢ(aᵢ|s) ∝ exp(λ Q̄ᵢ(s, aᵢ)) emerges from optimal response conditions where each agent optimally responds to fixed π₋ᵢ modulated by rationality parameter λ. This formulation implicitly encodes dependence on other agents through the marginalized critic without requiring explicit joint policy optimization, naturally capturing mixed cooperative-competitive equilibria.

### Mechanism 3: Non-Adversarial Optimization via Direct Q-Learning Objective
MAMQL uses L(ψ) = (1-γ)E[V(s₀)] - E[φ(R̄)] where φ is a concave regularizer, directly optimizing the Q-function to match expert state-action occupancy. This sidesteps min-max instability from adversarial training, instead using MSE constraints for reward reconstruction.

## Foundational Learning

- **Soft Q-Learning / Maximum Entropy RL**: MAMQL's foundation rests on soft Bellman equations and Boltzmann policies. Understanding entropy-regularized RL is essential for grasping the connection between marginalized critics and single-agent IRL.
  - Quick check: Can you explain why adding entropy to the RL objective makes the optimal policy expressible as exp(Q(s,a))/Z(s)?

- **Markov Games (Stochastic Games)**: The problem formulation uses n-agent Markov games with joint actions, transition functions T(s,aⁿ), and per-agent rewards. Understanding why Nash equilibria (not single optimal policies) are the solution concept is essential.
  - Quick check: In a general-sum Markov game, why does an agent's optimal policy depend on other agents' policies?

- **Inverse Reinforcement Learning (IRL) Basics**: The goal is reward recovery from demonstrations, not policy learning directly. The distinction between IRL (inferring R) and imitation learning (learning π directly) matters for interpreting results.
  - Quick check: Why might recovering the reward function be preferable to directly cloning expert actions?

## Architecture Onboarding

- **Component map**: Marginalized Critic Networks Q̄ᵢ(s, aᵢ) -> Reward Networks Rᵢ(s, a) -> Policy Extraction (implicit via Boltzmann)

- **Critical path**: 
  1. Sample expert transitions (s, a, s') from expert buffer
  2. Sample rollout transitions from policy buffer using current policies
  3. Update critics via Eq. 8 using biased V(s') approximation
  4. Update reward functions via MSE constraint (Eq. 9)
  5. Extract policy as Boltzmann over updated marginalized critics

- **Design tradeoffs**: 
  - Online vs. Offline: Online rollouts preferred for better performance; pure offline possible but degraded
  - Biased one-sample V(s'): Avoids |A|^{n-1} computation but introduces approximation error
  - Buffer size: Scaled to ~400× horizon length; smaller buffers degrade sample efficiency

- **Failure signatures**: 
  - Policy collapse to single equilibrium (e.g., only collecting purple gems in Gems) → insufficient critic marginalization or learning rate issues
  - Divergent reward values → check reward regularization β in Eq. 9
  - Slow convergence relative to reported → verify expert data quality and coverage

- **First 3 experiments**: 
  1. Sanity check on Gems (small dataset): Train with 500-1000 expert steps; verify agents learn to collect both individual and shared gems
  2. Ablation of biased V(s') approximation: Compare against multi-sample or full enumeration to quantify approximation error
  3. Scalability test in Highway-Env: Run with 2, 4, and 6 agents; measure whether sample efficiency claims hold as n increases

## Open Questions the Paper Calls Out

### Open Question 1
Can MAMQL be adapted to effectively capture systematic human biases and suboptimality that violate the generalized Boltzmann policy assumption? The current framework assumes expert rationality proportional to reward (generalized Boltzmann), which often fails for human data due to systematic biases.

### Open Question 2
To what extent can integrating more expressive architectures into MAMQL enable learned policies to surpass expert performance on high-complexity tasks? The current work focuses on matching expert behavior, and standard IRL/IL methods typically cap performance at the expert level.

### Open Question 3
How does the approximation error from the one-sample estimate of the marginalized critic impact performance as the number of agents or action space size increases? While effective for tested environments (up to 4 agents), the robustness of this bias in environments with significantly larger action spaces or agent counts is not validated.

## Limitations
- Performance depends on expert demonstrations following Boltzmann-rational behavior, which may not hold for human demonstrators
- The biased one-sample approximation for marginalized value functions introduces approximation error that isn't systematically evaluated
- Scalability to larger action spaces or more than 4 agents remains untested, as does performance in sparse reward environments

## Confidence
- **High confidence**: Theoretical connection between marginalized critics and single-agent soft-Q IRL
- **High confidence**: Policy extraction via Boltzmann over marginalized critics
- **Medium confidence**: Empirical performance claims (2-5x improvements, 2-4x faster convergence)
- **Medium confidence**: Robustness to non-stationary expert policies given biased V(s') approximation
- **Medium confidence**: Reward recovery quality given unknown reward network architecture and regularization strength

## Next Checks
1. Sanity check on Gems with small dataset: Train MAMQL on 500-1000 expert steps and verify agents learn to collect both individual and shared gems (not just purple), confirming proper marginalization and reward recovery.

2. Ablation of biased V(s') approximation: Compare MAMQL's one-sample V(s') against multi-sample (k=5) and full enumeration (small action spaces) to quantify the approximation error and its impact on convergence speed.

3. Scalability test in Highway-Env: Run experiments with 2, 4, and 6 agents to verify whether the claimed sample efficiency improvements (2-5x) scale with agent count, and measure the point where marginalization approximation breaks down.