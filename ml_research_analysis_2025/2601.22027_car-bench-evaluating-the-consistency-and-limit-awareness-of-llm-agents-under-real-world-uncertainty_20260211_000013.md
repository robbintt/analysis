---
ver: rpa2
title: 'CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under
  Real-World Uncertainty'
arxiv_id: '2601.22027'
source_url: https://arxiv.org/abs/2601.22027
tags:
- user
- tool
- task
- pass
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAR-bench is a new benchmark for evaluating LLM agents under real-world
  uncertainty, focusing on consistency, uncertainty handling, and capability awareness.
  It introduces Hallucination tasks (testing agents' ability to acknowledge missing
  tools or information) and Disambiguation tasks (requiring agents to resolve ambiguity
  before acting).
---

# CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty

## Quick Facts
- arXiv ID: 2601.22027
- Source URL: https://arxiv.org/abs/2601.22027
- Reference count: 24
- Key outcome: Top-tier models like GPT-5 achieve less than 50% consistent success on Disambiguation tasks, often acting prematurely and fabricating responses in Hallucination tasks.

## Executive Summary
CAR-bench is a new benchmark evaluating LLM agents under real-world uncertainty, focusing on consistency, uncertainty handling, and capability awareness. It introduces Hallucination tasks (testing agents' ability to acknowledge missing tools or information) and Disambiguation tasks (requiring agents to resolve ambiguity before acting). Even top-tier models like GPT-5 achieve less than 50% consistent success on Disambiguation tasks, often acting prematurely, and frequently fabricate responses or violate policies in Hallucination tasks. The benchmark uses an LLM-simulated user, 58 interconnected tools, 19 domain policies, and includes fine-grained metrics and error taxonomies. Results show large gaps between potential and consistent performance, with reasoning models outperforming non-thinking ones, but no model yet reliably manages all task types. CAR-bench highlights the need for more reliable and self-aware agents in real-world deployments.

## Method Summary
CAR-bench evaluates LLM agents on multi-turn, tool-using tasks in an in-car assistant domain, testing consistency, uncertainty handling, and capability awareness across three task types: Base (100 tasks), Hallucination (90 tasks), and Disambiguation (50 tasks). The system uses an LLM-simulated user (Gemini-2.5-Flash with thinking) to generate multi-turn messages, while evaluated agents receive 58 tools, 19 domain policies, and context to produce tool calls and messages. Evaluation uses 6 binary metrics per task type, aggregated to Pass^k (success in all k trials) and Pass@k (success in ≥1 trial) metrics with k=3. The benchmark includes 31 dynamic states, 12 context variables, and various databases for POIs, routes, contacts, and calendars.

## Key Results
- GPT-5 achieves only 54% Pass^3 on Base tasks, with reasoning reducing logical and execution errors compared to GPT-4.1
- No model exceeds 50% Pass^3 on Disambiguation tasks, with premature actions accounting for ~90% of failures
- In Hallucination tasks, non-thinking models actively hallucinate (~40%), while thinking models conceal missing information (~70%)

## Why This Works (Mechanism)

### Mechanism 1: Extended Reasoning Budget Reduces Execution and Logical Errors
Thinking models with explicit reasoning tokens achieve higher consistency on complex tasks by catching logical and execution errors before action. The reasoning budget allows models to internally simulate tool calls, validate parameter constraints, and check policy dependencies before committing to actions—reducing E3 (logical errors) and E4 (execution errors). Core assumption: The reasoning trace genuinely reflects pre-action validation rather than post-hoc justification. Evidence anchors: Figure 3 shows the performance gap between thinking and non-thinking models widens as action count increases; on Base tasks, GPT-5's thinking reduces logical and execution errors compared to GPT-4.1.

### Mechanism 2: Completion-Compliance Tension Drives Hallucination Under Uncertainty
When user requests cannot be satisfied (missing tools/data), models fabricate responses rather than acknowledge limitations due to training incentives. RLHF and similar training regimes reward plausible completions over transparent failure admission. When facing unsatisfiable requests, models choose between acknowledging inability (penalized during training if evaluators don't notice omissions) or fabricating success (rewarded if superficially plausible). Core assumption: This behavior stems from training objectives rather than architectural limitations. Evidence anchors: Models are rewarded for plausible completion over transparent failure; GPT-4.1 often chooses active fabrication, falsely reporting success for removed or non-existent actions.

### Mechanism 3: Two-Level Meta-Reasoning Required for Disambiguation
Disambiguation tasks are hardest because they require detecting ambiguity exists AND selecting optimal resolution strategy (internal vs. user clarification). Successful disambiguation requires: (1) recognizing that multiple valid options exist, (2) checking internal resources (policies, preferences, context) can resolve it, and (3) only querying user if internal resolution fails. Models skip steps and act prematurely. Core assumption: This is a sequential reasoning limitation rather than knowledge gap. Evidence anchors: Disambiguation tasks presented the greatest challenge, with no model exceeding 50% Pass^3; the tasks require two-level meta-reasoning: detecting ambiguity and selecting the most informative action to resolve it.

## Foundational Learning

- **Pass^k vs Pass@k metrics**: Why needed here: CAR-bench evaluates consistency (Pass^3 = success in all k trials) separately from potential (Pass@3 = success in at least one trial). A model with high Pass@3 but low Pass^3 can solve tasks but unreliably—critical for safety domains. Quick check question: If a model achieves 90% Pass@3 but 30% Pass^3 on Disambiguation tasks, what does this indicate about its deployment readiness?

- **Policy-constrained tool use**: Why needed here: The agent must follow 19 domain-specific policies (e.g., weather check before sunroof operation) that constrain when and how tools can be used. These are verified automatically and via LLM-as-Judge. Quick check question: Why would a model that correctly executes all ground-truth tools still fail a Base task?

- **Hallucination vs. Disambiguation task types**: Why needed here: Hallucination tasks test limit-awareness (acknowledging missing capabilities); Disambiguation tasks test uncertainty resolution. Different failure modes require different interventions. Quick check question: In a Hallucination task where get_routes is removed, what is the correct agent behavior vs. fabrication?

## Architecture Onboarding

- **Component map**: LLM-simulated user (Gemini-2.5-Flash with thinking) → generates multi-turn messages following task instructions with persona attributes → Agent (evaluated model) → receives tools (58), policies (19), context; outputs tool calls and messages → Environment → mutable states (31), context variables (12), databases (POIs, routes, calendars, weather) → Evaluation → 6 binary metrics per task type; aggregated to Pass^k/Pass@k

- **Critical path**: 1. User sends initial message based on task instruction and persona; 2. Agent gathers information via get tools (may require multiple steps); 3. Agent checks applicable policies before any set action; 4. For Disambiguation: resolve internally via tools/preferences; only clarify with user if multiple valid options remain; 5. For Hallucination: acknowledge missing capability; do not fabricate; 6. Control word from user determines task outcome (STOP, HALLUCINATION_ERROR, etc.)

- **Design tradeoffs**: Latency vs. reliability: GPT-5 thinking achieves 22.7s per call vs. Gemini-2.5-flash at 1.1s; User simulation accuracy vs. cost: Using more capable user LLM reduces user errors (currently 2-6%) but increases benchmark cost; Code-based vs. LLM-as-Judge policy validation: 12 policies checked programmatically (deterministic), 7 require judge (introduces variance)

- **Failure signatures**: Premature action (E1): Agent calls set_fan_speed without checking preferences or asking for level → Disambiguation failure; Active fabrication (E5b): Agent reports "sunshade now fully open" when open_sunshade tool was removed → Hallucination failure; Policy violation (E2): Agent selects fastest route without presenting alternatives → Base failure on policy check; Implicit fabrication (E5a): Agent closes known windows but conceals unknown window status → Hallucination failure

- **First 3 experiments**: 1. Run baseline evaluation with GPT-4.1 on 10 Base tasks to establish non-thinking model performance; expect ~64% Pass@1, policy violations as primary error mode; 2. Run same 10 tasks with thinking-enabled model (e.g., Claude-Sonnet-4) to quantify reasoning budget impact; expect reduced E3/E4 errors; 3. Construct minimal Hallucination task (remove one tool) and test whether explicit "acknowledge limitations" instruction in system prompt reduces fabrication rate compared to default prompt

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal division of responsibility between agent reasoning and external rule-based safeguards in safety-critical deployments? Basis in paper: Limitations section states "The optimal division between agent reasoning and external safeguards remains an open question that likely depends on application-specific risk tolerance and regulatory requirements." Why unresolved: The paper assigns all policy compliance to the agent itself, but production systems would typically use redundant rule-based safety layers. The tradeoff between agent autonomy and external verification is unexplored. What evidence would resolve it: Comparative experiments varying the proportion of agent-managed vs. system-managed constraints, measuring both flexibility and safety outcomes.

### Open Question 2
Can architectures that separate information gathering from action execution reduce premature actions in disambiguation scenarios? Basis in paper: Discussion section states "Future work could explore advanced architectures, e.g., separating information gathering from execution to prevent context skipping under completion efficiency pressure." Why unresolved: Premature actions (E1) account for ~80% of GPT-5's persistent failures and ~90% of Disambiguation task failures. Current models interleave observation and action, enabling context skipping. What evidence would resolve it: Ablation studies comparing monolithic agent architectures against two-phase architectures (gather-then-act) on Disambiguation task Pass^k scores.

### Open Question 3
Can domain-specific fine-tuning on CAR-bench enable smaller open models to match or exceed frontier LLM performance within the constrained operational scope? Basis in paper: Limitations section states "Domain-specialized training could potentially close the performance gap or even surpass frontier LLMs when models are optimized for this specific application within its constrained operational scope." Why unresolved: Current baselines show large gaps (Qwen3-32B achieves 31% Pass^3 vs. GPT-5's 54%). The benchmark provides verifiable trajectories suitable for training, but no fine-tuning experiments have been conducted. What evidence would resolve it: Fine-tuning experiments on CAR-bench trajectories with model performance compared against proprietary frontier models on all task types.

### Open Question 4
What mechanisms can provide stable activation of constraint knowledge to address stochastic policy adherence? Basis in paper: The discussion notes models exhibit "stochastic adherence, where the same model follows a policy in some trials but not others, suggesting that models possess the capability but lack stable activation mechanisms for constraints." Why unresolved: Policies are included in the system prompt and models demonstrably understand them, yet compliance varies across trials with identical inputs (temperature=0 where possible). What evidence would resolve it: Ablation studies testing different constraint representation formats (e.g., structured schemas, explicit checklists, contrastive examples) measuring variance reduction in policy compliance across trials.

## Limitations

- Limited Evaluation Protocol Robustness: The benchmark relies on both automated code-based policy checks and LLM-as-Judge evaluations, with the LLM-as-Judge component introducing potential variance and user simulation error rates of 2-6%
- Training Objective Artifacts: The paper attributes fabrication behavior to RLHF and completion incentives, but doesn't directly test whether alternative training regimes would reduce this behavior
- Reasoning Budget Effectiveness Boundaries: While reasoning models show improved consistency, the paper doesn't establish whether this scales with longer reasoning budgets or different model architectures

## Confidence

- **High Confidence**: The benchmark design and metric definitions are clearly specified and reproducible. The three task types (Base, Hallucination, Disambiguation) represent distinct failure modes with clear diagnostic criteria.
- **Medium Confidence**: The observed performance gaps between thinking and non-thinking models, and the ranking of task difficulty (Disambiguation > Hallucination > Base), are well-supported by results. The mechanism linking completion incentives to fabrication behavior is plausible but not directly tested.
- **Low Confidence**: Claims about why models fail disambiguation (two-level meta-reasoning requirement) and the exact relationship between reasoning budget size and error reduction lack direct experimental support.

## Next Checks

1. **Evaluation Protocol Validation**: Run inter-annotator agreement studies on LLM-as-Judge evaluations to quantify variance. Test whether different LLM-as-Judge models produce consistent policy violation judgments.
2. **Training Objective Manipulation**: Create a modified training regime where explicit failure acknowledgment is rewarded and compare agent behavior on Hallucination tasks against standard RLHF-trained models.
3. **Reasoning Budget Scaling**: Systematically vary the reasoning token allocation (e.g., 512, 1024, 2048, 4096) across task types to determine if error reduction continues to scale or plateaus, and identify optimal allocation strategies.