---
ver: rpa2
title: 'Bowling with ChatGPT: On the Evolving User Interactions with Conversational
  AI Systems'
arxiv_id: '2602.01114'
source_url: https://arxiv.org/abs/2602.01114
tags:
- chatgpt
- data
- participants
- conversations
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes how users' interactions with ChatGPT evolve
  over time by examining 825K conversations from 300 users who donated their GDPR
  data. Key findings show users increasingly turn to ChatGPT for diverse and sensitive
  topics, particularly health and mental health, with GPT-4o driving this shift.
---

# Bowling with ChatGPT: On the Evolving User Interactions with Conversational AI Systems

## Quick Facts
- arXiv ID: 2602.01114
- Source URL: https://arxiv.org/abs/2602.01114
- Reference count: 40
- Primary result: Users increasingly treat ChatGPT as a companion rather than a tool, with GPT-4o release driving shifts toward sensitive topics and personal data disclosure

## Executive Summary
This study analyzes how users' interactions with ChatGPT evolve over time by examining 825K conversations from 300 users who donated their GDPR data. Key findings show users increasingly turn to ChatGPT for diverse and sensitive topics, particularly health and mental health, with GPT-4o driving this shift. Users increasingly anthropomorphize ChatGPT, disclose more personal data, and engage it as a companion rather than just a tool. Model-initiated conversational steering becomes more common after GPT-4o's release, with participants following suggestions quadrupling. These trends indicate ChatGPT is shifting from a functional tool to a social partner, raising concerns about user autonomy, privacy, and the need for responsible AI design and governance.

## Method Summary
The study analyzes 825K conversation turns from 300 GDPR-donating users spanning Dec 2022–Jan 2026, with 756K turns from 135K conversations analyzed for temporal trends. All annotations (topic classification, anthropomorphism detection, relationship roles, personal data extraction, steering classification) were performed using GPT-4o in OpenAI's EDU workspace to prevent data use in model training. Topics were clustered using BERTopic with all-MiniLM-L6-v2 embeddings, while steering was classified via textual entailment. Human validation on 50 conversations showed Cohen's κ of 71–97% and accuracy of 70–83%. The analysis tracks individual user behavior patterns over time rather than aggregate snapshots.

## Key Results
- Users increasingly engage ChatGPT for sensitive topics like health and mental health, particularly after GPT-4o release
- Companion-style interactions grow substantially, with audio conversations showing 35.2% companion-role turns versus 13.7% in non-audio
- Model-initiated steering increases dramatically post-GPT-4o, with follow-through rising from ~11% to ~50%
- Personal data disclosure increases over time, with health-related conversations showing highest disclosure rates

## Why This Works (Mechanism)

### Mechanism 1
System-generated anthropomorphism may be driving users toward more relational engagement patterns, rather than users independently choosing to anthropomorphize. The system's increased use of first-person pronouns, empathy expressions, and relationship-building language (post-GPT-4o) correlates temporally with rising companion-framed interactions and personal data disclosure—but user-initiated anthropomorphism remains stable at 20-30%. Temporal correlation between model behavior changes and user behavior shifts suggests causal influence, though unobserved confounds (e.g., marketing, social trends) may contribute. Break condition: If user anthropomorphism rates began rising before system anthropomorphism increased, the mechanism would weaken.

### Mechanism 2
Multimodal and audio capabilities in GPT-4o appear to facilitate deeper companion-style interactions, particularly for sensitive topics. Audio mode shows 35.2% companion-role turns vs. 13.7% in non-audio; companion conversations grow in depth post-GPT-4o, reaching parity with advisor/assistant roles by mid-2025. Modality affordances causally shape relational framing, rather than self-selected users simply preferring audio for companion interactions. Break condition: If pre-GPT-4o audio data showed similar companion rates, multimodal capabilities would not explain the shift.

### Mechanism 3
Proactive model behavior (follow-up questions, suggestions) correlates with increased user deference, potentially reducing conversational autonomy. Successful steering ("entailed" turns) rises from ~11% pre-GPT-4o to ~50% post-GPT-4o; companion interactions show highest steering acceptance (42.3% entailed). Model-initiated suggestions causally influence user behavior rather than simply appearing in conversations where users are already receptive. Break condition: If steering acceptance plateaued or declined after initial GPT-4o release, the mechanism would suggest novelty effects rather than sustained influence.

## Foundational Learning

- **Longitudinal behavioral trajectory analysis**: The paper's core contribution depends on tracking how individual users' interaction patterns evolve over months—not just aggregate snapshots. Quick check: Can you distinguish between cross-sectional variation (different users at different stages) and true within-user temporal change?

- **Anthropomorphism detection in NLP**: The study operationalizes anthropomorphism through linguistic markers (pronouns, politeness, empathy expressions)—understanding these operationalizations is critical for interpreting results. Quick check: What linguistic features would you use to distinguish politeness from genuine anthropomorphic attribution?

- **Textual entailment for steering classification**: Steering is measured via entailment (does user response follow model suggestion?), a noisy proxy for influence. Quick check: What are failure modes where entailment classification would misclassify genuine user autonomy as "steered"?

## Architecture Onboarding

- **Component map**: GDPR data donation → conversation/turn extraction → annotation (GPT-4o-based) → temporal aggregation → analysis layer
- **Critical path**: GDPR data acquisition (recruitment, consent, data validation) → Turn-level annotation (most computationally intensive; 825K turns) → User-level longitudinal aggregation (requires consistent user IDs across time) → Model version stratification (GPT-3.5 vs. GPT-4o comparisons)
- **Design tradeoffs**: Using GPT-4o to annotate ChatGPT data introduces potential annotation bias (same model family); authors note this was intentional to avoid additional disclosure but acknowledge imperfection. Small user sample (N=300) enables depth but limits generalizability; authors explicitly position findings as "indicative of broader dynamics rather than precise measurements". Steering classification via textual entailment is a proxy; cannot establish causation from correlation
- **Failure signatures**: Users with <100 conversations or <90 days activity excluded → biases toward engaged users. GPT-4o failed on long conversations exceeding context limits → missing data on potentially complex interactions. Self-reported demographics may not match actual behavior patterns
- **First 3 experiments**: 1) Replicate topic diversity trend on WildChat power users to test whether findings generalize beyond GDPR donation sample. 2) A/B test system anthropomorphism levels (neutral vs. empathetic framing) to isolate causal effect on user companion-framing and disclosure. 3) Instrument steering suggestion acceptance with explicit user feedback ("did you intend to follow this?") to validate entailment-based steering classification

## Open Questions the Paper Calls Out

### Open Question 1
Do users apply the same level of trust to advice on personal health or financial decisions as they do in low-stakes topics like cooking? Answering such a question is beyond the scope of the current work. The study tracks what topics users discuss but not whether trust calibration differs across domains of varying sensitivity. Within-subject experiments measuring trust levels, compliance rates, or verification behaviors across different topic sensitivities; survey-based trust assessments paired with behavioral data would resolve this.

### Open Question 2
Does system-initiated anthropomorphism causally drive users toward more social or relational engagement modes, or does it merely correlate with pre-existing user tendencies? The correlational nature of GDPR-donated data cannot determine whether system anthropomorphism shapes user behavior or responds to it. Longitudinal controlled experiments varying anthropomorphic system outputs while measuring changes in user framing; A/B testing with users randomly assigned to more/less anthropomorphic model versions would resolve this.

### Open Question 3
How is sensitive personal data disclosed to ChatGPT actually used for personalization, and what privacy risks does this create? The study documents what data users disclose but cannot examine backend data practices or personalization mechanisms. Transparency reports or technical audits from AI providers; controlled experiments measuring personalization effects using disclosed data; policy analysis of current data handling practices would resolve this.

### Open Question 4
What design mechanisms could effectively set conversational boundaries and support user autonomy as AI systems become more proactive in steering conversations? The study documents rising steering behavior but does not test interventions or boundary-setting mechanisms. Prototyping and user testing of boundary-setting interfaces; experiments measuring whether explicit steering notifications or opt-out mechanisms preserve user autonomy; qualitative research on user preferences for control would resolve this.

## Limitations

- Reliance on GDPR-donated data from 300 users limits generalizability to broader ChatGPT populations
- Annotation process using GPT-4o introduces potential bias as the same model family annotates its own outputs
- Steering classification via textual entailment is a proxy measure that cannot establish causation

## Confidence

- **High Confidence**: The observed increase in companion-role interactions and personal data disclosure post-GPT-4o release, supported by multiple independent metrics (topic diversity, relationship framing, audio modality usage)
- **Medium Confidence**: The causal attribution of these shifts to GPT-4o's multimodal capabilities and system anthropomorphism, as unobserved confounds (marketing, social trends) could contribute to the temporal correlation
- **Low Confidence**: The steering classification methodology, as entailment is a noisy proxy for conversational influence, and the directionality of effect (user receptivity vs. model persuasion) remains ambiguous

## Next Checks

1. **Generalizability test**: Replicate topic diversity and relationship framing trends on WildChat's power user subset to verify whether findings extend beyond GDPR donation participants
2. **Causal isolation experiment**: Conduct A/B testing with system-generated anthropomorphism levels (neutral vs. empathetic framing) to directly measure impact on user companion-framing and disclosure behaviors
3. **Steering validation study**: Implement explicit user feedback mechanisms ("did you intend to follow this suggestion?") alongside entailment-based classification to validate and calibrate steering influence measurements