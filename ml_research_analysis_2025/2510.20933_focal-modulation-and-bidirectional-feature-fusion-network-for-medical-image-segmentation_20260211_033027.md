---
ver: rpa2
title: Focal Modulation and Bidirectional Feature Fusion Network for Medical Image
  Segmentation
arxiv_id: '2510.20933'
source_url: https://arxiv.org/abs/2510.20933
tags:
- segmentation
- image
- medical
- khan
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Focal Modulation and Bidirectional Feature
  Fusion Network (FM-BFF-Net) for medical image segmentation. The key idea is to combine
  convolutional and transformer-based components to capture both local and global
  features effectively.
---

# Focal Modulation and Bidirectional Feature Fusion Network for Medical Image Segmentation

## Quick Facts
- arXiv ID: 2510.20933
- Source URL: https://arxiv.org/abs/2510.20933
- Authors: Moin Safdar; Shahzaib Iqbal; Mehwish Mehmood; Mubeen Ghafoor; Tariq M. Khan; Imran Razzak
- Reference count: 40
- Primary result: Proposes FM-BFF-Net combining CNN and transformer components, achieving Jaccard index of 92.96% and Dice coefficient of 97.38% on Kvasir-SEG polyp dataset

## Executive Summary
This paper presents FM-BFF-Net, a hybrid architecture combining convolutional and transformer components for medical image segmentation. The model addresses challenges in lesion detection by employing focal modulation attention to emphasize informative regions, bidirectional feature fusion to integrate encoder-decoder representations, and a Vision Transformer module at the bottleneck for capturing long-range dependencies. Experimental results on eight public datasets demonstrate state-of-the-art performance across polyp, skin lesion, and ultrasound image segmentation tasks.

## Method Summary
FM-BFF-Net employs an EfficientNetV2S1 encoder followed by a bottleneck Vision Transformer Module, then a decoder with bidirectional feature fusion. The focal modulation attention block refines feature representations by computing attention weights from Global Average and Max Pooling outputs, applying learnable modulation factors and sigmoid gating. The bidirectional feature fusion module merges encoder and decoder features across scales using parallel convolutional pathways and element-wise operations. The architecture aggregates skip connections with focal modulation before fusion, preserving fine-grained details while incorporating semantic context. Training uses Adam optimizer with learning rate scheduling and early stopping, evaluated on eight diverse medical image datasets.

## Key Results
- Achieved Jaccard index of 92.96% and Dice coefficient of 97.38% on Kvasir-SEG polyp segmentation
- Outperformed state-of-the-art methods including UNet++ and Swin-Unet across all tested datasets
- Demonstrated strong performance on ISIC skin lesion datasets (2016-2018) and BUSI/DDTI ultrasound datasets
- Showed 4.82% Jaccard drop when tested on CVC-ColonDB (domain shift from Kvasir-SEG)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Focal modulation adaptively refines feature representations by emphasizing informative regions while suppressing irrelevant context.
- **Mechanism:** The Focal Modulation unit computes attention weights by fusing Global Average Pooling (GAP) and Global Max Pooling (GMP) outputs, then applies a learnable modulation factor (γ_p) and sigmoid gating. This creates spatially-varying emphasis that is multiplied with intermediate features.
- **Core assumption:** Medical segmentation accuracy depends on distinguishing lesion boundaries from background—a task requiring dynamic, region-specific attention rather than uniform processing.
- **Evidence anchors:** [abstract] "employs a focal modulation attention mechanism to refine context awareness" [section III-A, Eq. 11] FM = γ_p[σ(f_1×1(Re((GAP(i_2) − GMP(i_2)) × α))) × i_2] [corpus] Limited direct evidence on focal modulation in segmentation; related work (SWinMamba, DCAT) uses attention but not focal-specific variants. Weak corpus support for this exact mechanism.
- **Break condition:** If lesion-to-background contrast is extremely low (as noted in failure cases), the modulation signal may fail to distinguish signal from noise, degrading refinement.

### Mechanism 2
- **Claim:** Bidirectional feature fusion enables richer semantic representation by allowing encoder and decoder features to interact across scales.
- **Mechanism:** BiFFM takes two inputs (s_n from aggregated skip connections, B_n^enc from encoder). It applies GAP to both, processes through parallel convolutional pathways (1×3/3×1 convolutions and channel shuffling), then fuses via element-wise multiplication and concatenation. This bidirectional flow preserves fine-grained encoder details while integrating decoder reconstruction context.
- **Core assumption:** Multi-scale features contain complementary information—encoder features capture local details; decoder features carry semantic context—and explicit fusion improves boundary precision.
- **Evidence anchors:** [abstract] "introduces a bidirectional feature fusion module that enables efficient interaction between encoder and decoder representations across scales" [section III-B, Eq. 15] f_fuse = [i_1 ⊗ x_1 ⊗ x_2] © [i_2 ⊗ x_1 ⊗ x_2] [corpus] BiPVL-Seg also uses "bidirectional progressive vision-language fusion" with global-local alignment, suggesting fusion strategies are a broader trend. Moderate corpus support.
- **Break condition:** If skip connections carry noise (e.g., from poor-quality inputs or domain shift), fusion amplifies artifacts rather than signal.

### Mechanism 3
- **Claim:** Hybrid CNN-Transformer architecture captures both local spatial features and long-range dependencies, addressing scale and boundary challenges in medical images.
- **Mechanism:** The encoder (EfficientNetV2S1 CNN) extracts local features; the Vision Transformer Module (ViTM) at the bottleneck applies token-based self-attention (TSA) and global spatial attention (GSA) to capture cross-position relationships across the entire feature map.
- **Core assumption:** Medical lesions vary significantly in size, shape, and texture; pure CNNs lack sufficient receptive field for large structures, while pure transformers may miss fine boundary details.
- **Evidence anchors:** [abstract] "combines convolutional and transformer-based components to capture both local and global features effectively" [section I] "Transformers can be used with CNNs to capture local and global features, eliminating border and scale difficulties" [corpus] Multiple recent works (H2Former, TransUNet, Swin-Unet) use CNN-transformer hybrids for medical segmentation, validating the general approach. Strong corpus support.
- **Break condition:** If the transformer's self-attention operates on insufficiently preprocessed features (e.g., noisy or under-trained encoder outputs), long-range dependencies may be spurious or misleading.

## Foundational Learning

- **Concept:** Encoder-Decoder Architecture with Skip Connections
  - **Why needed here:** FM-BFF-Net follows a U-Net-style structure where encoder features must be preserved and fused with decoder outputs. Understanding spatial resolution tradeoffs is essential.
  - **Quick check question:** Given a 256×256 input with 4 encoder downsampling stages (2× each), what is the spatial dimension at the bottleneck?

- **Concept:** Self-Attention Mechanics (Query-Key-Value)
  - **Why needed here:** The ViTM module uses scaled dot-product attention (QK^T/√d_k)V. Understanding how attention weights capture feature similarity is critical for debugging transformer components.
  - **Quick check question:** In self-attention, if Q and K are normalized but V contains outliers, what happens to the output?

- **Concept:** Feature Pyramid and Multi-Scale Fusion
  - **Why needed here:** BiFFM operates across multiple encoder stages. Understanding how features at different resolutions combine (concatenation vs. addition vs. attention) informs design choices.
  - **Quick check question:** When fusing a 64×64 feature map with a 128×128 map via concatenation, what preprocessing is required?

## Architecture Onboarding

- **Component map:** Input → EfficientNetV2S1 Encoder (4 blocks, B^enc_1 to B^enc_4) → FMCAB on skip connections → Aggregated skip (s_1 to s_4) → ViTM (TSA + GSA) → f_enc → FRM → BiFFM (fuses with s_4) → FRM → B^dec_1 → repeat for 4 stages → Output: B^dec_4 → 1×1 conv + sigmoid

- **Critical path:** The FMCAB-modulated skip connections (s_n) → BiFFM fusion → decoder FRMs. Errors here propagate through all decoder stages.

- **Design tradeoffs:** EfficientNetV2S1 chosen for computational efficiency vs. larger backbones (ResNet-101, ViT-L); Two FRM modules per decoder block (one with upsampling, one without) add capacity but increase parameters; Aggregated skip connections (concatenating s_{n-1} with current stage) enrich context but require careful memory management.

- **Failure signatures:** Low-contrast boundaries (Figures 11-12): Model produces over-segmentation or missed lesions; Domain shift (e.g., CVC-ColonDB vs. Kvasir-SEG): Performance drops on out-of-distribution data (Jaccard 88.14% vs. 92.96%); Small lesions: If focal modulation fails to detect signal, small structures may be suppressed entirely.

- **First 3 experiments:**
  1. Baseline validation: Reproduce Kvasir-SEG results (Jaccard ~92.96%) using provided training setup (merged ClinicDB + Kvasir, 80:20 split, Adam optimizer, lr=0.001). Verify data augmentation and early stopping.
  2. Ablation study: Disable FMCAB (replace with identity) and measure performance drop. Expected: boundary precision degrades, Jaccard decreases.
  3. Cross-dataset generalization: Train on Kvasir-SEG, evaluate on CVC-ColonDB without fine-tuning. Assess domain shift robustness and identify failure modes.

## Open Questions the Paper Calls Out

- **Question:** Can FM-BFF-Net be effectively adapted for 3D volumetric and multimodal medical image segmentation?
- **Basis in paper:** [explicit] The conclusion explicitly suggests future work should "explore adapting M-BFF-Net to 3D volumetric and multimodal medical images."
- **Why unresolved:** The current architecture is designed and tested exclusively on 2D datasets (e.g., Kvasir-SEG, ISIC, BUSI), lacking mechanisms for volumetric context.
- **What evidence would resolve it:** Evaluation of a modified 3D FM-BFF-Net on volumetric datasets (e.g., BraTS) demonstrating retained accuracy and feasible memory usage.

- **Question:** How does integrating uncertainty quantification impact the clinical reliability of FM-BFF-Net predictions?
- **Basis in paper:** [explicit] The conclusion states future work involves "integrating uncertainty quantification mechanisms" to provide "confidence estimates" for "clinically interpretable segmentation outcomes."
- **Why unresolved:** The current model outputs deterministic masks without confidence scores, limiting utility in ambiguous cases identified in the "Limitations" section.
- **What evidence would resolve it:** Implementation of uncertainty layers and evaluation using calibration metrics (e.g., Expected Calibration Error) on low-contrast images.

- **Question:** Does the proposed architecture maintain sufficient computational efficiency for resource-constrained deployment?
- **Basis in paper:** [inferred] The introduction emphasizes the need for "simplicity of computing" in "resource-limited locations," but the experimental section lacks metrics on model size, FLOPs, or inference time.
- **Why unresolved:** While EfficientNetV2 is used as a backbone, the added Vision Transformer and FMCAB modules increase complexity, potentially conflicting with the goal of efficiency.
- **What evidence would resolve it:** Reporting parameter counts and latency benchmarks compared against lightweight baselines.

## Limitations
- The paper lacks critical implementation details including input resolution, batch size, FMCAB hyperparameters, and exact loss function formulation
- Performance shows 4.82% Jaccard drop on CVC-ColonDB, indicating potential overfitting to training distribution
- No uncertainty quantification or error analysis provided to assess clinical reliability
- Computational efficiency metrics (parameters, FLOPs, inference time) are not reported

## Confidence
- **High confidence:** The hybrid CNN-Transformer architecture approach (Mechanism 3) is well-supported by corpus evidence and follows established design patterns
- **Medium confidence:** The focal modulation mechanism (Mechanism 1) is theoretically sound but lacks direct corpus validation for this specific implementation
- **Medium confidence:** The bidirectional fusion strategy (Mechanism 2) is reasonable but the specific implementation details require empirical validation

## Next Checks
1. Perform systematic ablation studies disabling FMCAB, BiFFM, and ViTM components to quantify individual contributions to performance
2. Test cross-dataset generalization by training on one domain (e.g., Kvasir-SEG) and evaluating on multiple out-of-distribution datasets (CVC-ColonDB, BUSI, ISIC series)
3. Conduct failure case analysis on low-contrast boundary examples to identify whether focal modulation or fusion mechanisms break down first