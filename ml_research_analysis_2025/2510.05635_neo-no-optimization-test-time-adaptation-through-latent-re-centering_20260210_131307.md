---
ver: rpa2
title: 'NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering'
arxiv_id: '2510.05635'
source_url: https://arxiv.org/abs/2510.05635
tags:
- accuracy
- samples
- adaptation
- adapt
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NEO, a hyperparameter-free test-time adaptation
  method that re-centers embeddings to the origin without requiring backpropagation
  or access to source data. NEO achieves strong performance on ImageNet-C (59.2% accuracy
  on ViT-Base), ImageNet-R (60.3%), and ImageNet-S (47.2%), outperforming seven baselines
  while using minimal compute.
---

# NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering

## Quick Facts
- arXiv ID: 2510.05635
- Source URL: https://arxiv.org/abs/2510.05635
- Reference count: 40
- Primary result: Achieves 59.2% accuracy on ImageNet-C with ViT-Base while reducing inference time by 63% and memory usage by 9%

## Executive Summary
NEO introduces a hyperparameter-free test-time adaptation method that re-centers embeddings to the origin without backpropagation or source data access. The approach leverages neural collapse properties to correct distribution shifts by globally re-centering embeddings, achieving strong performance across multiple benchmarks (ImageNet-C: 59.2%, ImageNet-R: 60.3%, ImageNet-S: 47.2%) while significantly reducing computational overhead. NEO is robust to limited data (adapts with as few as one sample or one class) and extends naturally to continual adaptation scenarios.

## Method Summary
NEO operates by tracking the global mean of target embeddings during inference and subtracting this mean from all incoming embeddings before classification. The method uses a running average to compute $\tilde{\mu}_G$ across batches, then re-centers the penultimate layer embeddings (input to the final classifier) by subtracting this mean. This simple operation corrects for distribution shifts without requiring backpropagation, optimizer states, or source data. The approach is theoretically grounded in neural collapse, where under cross-entropy loss with regularization and balanced classes, the source embedding centroid is at the origin, making origin-recentering mathematically equivalent to global alignment.

## Key Results
- ImageNet-C (ViT-Base): 59.2% accuracy (vs. 55.6% baseline), outperforming seven baselines
- ImageNet-R: 60.3% accuracy, demonstrating cross-dataset robustness
- ImageNet-Sketch: 47.2% accuracy with minimal compute overhead
- Inference efficiency: 63% reduction in inference time, 9% reduction in memory usage on Jetson Orin Nano

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Input distribution shifts cause structural displacement of embeddings that can be corrected by global re-centering at the origin.
- Mechanism: Distribution-shifted inputs produce embeddings h(x̃) that are systematically displaced from source embeddings h(x). This displacement concentrates in a small number of dimensions (<20 dimensions account for largest shift in 95% of samples for contrast corruption). NEO estimates the global centroid shift ˜µG from test samples and subtracts it: h(x̃) - ˜µG, restoring alignment with the source distribution.
- Core assumption: Test samples come from the same shifted distribution throughout adaptation; the shift is shared across classes.
- Evidence anchors:
  - [section 4.1] "For all corruptions, 80% of data has less than 50 dimensions as their highest magnitude change, signifying the possible existence of a globally shared shift across samples and classes."
  - [section 4.2] "The largest increase in cosine similarity is caused by the global alignment... global alignment greatly reduces the difference in norms."
  - [corpus] Weak corpus support—related TTA papers focus on optimization-based or pseudo-labeling approaches, not geometric re-centering.
- Break condition: If test distribution is non-stationary or samples come from multiple different shifts simultaneously, the single global shift estimate becomes unreliable.

### Mechanism 2
- Claim: Under neural collapse, the source embedding centroid is at the origin, making origin-recentering mathematically equivalent to global alignment.
- Mechanism: Neural collapse (Papyan et al., 2020) emerges during the terminal phase of training. Proposition 4.2 states: under cross-entropy loss with regularization and balanced classes, the global centroid µG = 0. Therefore, the true global shift ∆G = ˜µG - µG = ˜µG. This means centering corrupted embeddings at the origin (subtracting ˜µG) is exactly the correction needed.
- Core assumption: The pre-trained model exhibits neural collapse properties; training used cross-entropy with regularization and approximately balanced classes.
- Evidence anchors:
  - [section 4.4] "Under the assumption of the unconstrained features model... and balanced classes, we have ∆G = ˜µG - µG = ˜µG."
  - [section 4.3] "Under neural collapse assumptions with cross-entropy loss, the assigned class is solely determined by the cosine similarity of embeddings and classifier weights."
  - [corpus] Neural collapse is well-documented in the literature (Papyan et al., 2020), but the paper acknowledges it has not been investigated for TTA specifically.
- Break condition: Models trained with MSE loss require modified treatment (see Appendix A.1); highly imbalanced training data may violate the balanced-class assumption.

### Mechanism 3
- Claim: Avoiding backpropagation and storing only a single vector (˜µG) eliminates the computational overhead typical of TTA methods.
- Mechanism: NEO requires only: (1) forward pass through encoder to get embeddings, (2) running average update, (3) vector subtraction before classification. No gradient computation, no optimizer state, no per-sample storage. The update rule is O(d) where d is embedding dimension.
- Core assumption: The embedding dimension d is manageable (384-1024 for tested ViT models); the averaging operation does not create numerical instability.
- Evidence anchors:
  - [abstract] "NEO reduces inference time by 63% and memory usage by 9% compared to baselines."
  - [Figure 7] NEO's elapsed time and peak memory match "No Adapt" baseline on Jetson Orin Nano.
  - [corpus] Related efficient TTA methods (T3A, LAME) also avoid backpropagation but NEO is the only fully hyperparameter-free method per Table 1.
- Break condition: Numerical precision issues could accumulate if running for millions of batches without reset (mitigated by NEO-Continual's exponential moving average).

## Foundational Learning

### Neural Collapse
- Why needed here: The theoretical justification that µG = 0 depends on understanding NC1-NC4 properties. Without this, the origin-recentering seems arbitrary.
- Quick check question: Can you explain why NC2 (convergence to simplex ETF) implies that class means are equidistant from the origin under balanced training?

### Test-Time Adaptation (TTA)
- Why needed here: Context for why this problem matters—models degrade under distribution shift, and TTA addresses this without source data or labels.
- Quick check question: What is the difference between "fully TTA" and methods that require source data access? Why does this matter for deployment?

### Cosine Similarity and Linear Classification
- Why needed here: Proposition 4.1 shows that under neural collapse, classification reduces to maximizing cosine similarity between embedding and weight vectors. Re-centering improves this alignment.
- Quick check question: Why does the L2 norm of embeddings affect confidence but not the predicted class in this setting?

## Architecture Onboarding

### Component map
Encoder h (ViT-S/B/L) -> Embeddings h(x) (d-dimensional) -> NEO Layer (tracks ˜µG, subtracts it) -> Classifier θ (linear head) -> Predictions y

### Critical path
1. Input batch B (shape: b × m) → Encoder h → Embeddings h(B) (shape: b × d)
2. Update ˜µG ← (i-1)/i · ˜µG + 1/i · Avg(h(B))
3. Re-center: h(B) - ˜µG → Classifier θ → Predictions y

### Design tradeoffs
- **Simple averaging vs. EMA**: Standard NEO assumes stationary shift; NEO-Continual (with α hyperparameter) handles evolving distributions but loses "hyperparameter-free" property
- **Penultimate layer only**: Paper focuses on last-layer embeddings; intermediate layers not explored
- **ViT architectures only**: Experiments limited to Vision Transformers; CNNs not evaluated (architectural assumption about BatchNorm not required for NEO)

### Failure signatures
- **MSE-trained models**: Require modified bias handling (Algorithm 2 in Appendix A.1)—failing to adjust causes systematic prediction errors
- **Severely imbalanced test distributions**: If test classes are highly skewed, ˜µG estimate may not represent true global shift
- **Non-collapsed models**: Theoretical guarantees weaken; empirical performance may degrade

### First 3 experiments
1. **Validation on single-corruption adaptation**: Test NEO on one corruption type (e.g., Gaussian noise) with 64 samples, measure accuracy improvement over baseline. Expected: 1-3% gain per Figure 5a.
2. **Cross-corruption transfer**: Adapt on corruption A, test on corruption B. Use Figure 6 to identify which corruption pairs share similar ˜µG vectors (e.g., noise types transfer well to other noise types).
3. **Resource profiling on target hardware**: Implement the single-line change (replace nn.Linear with NEO layer), measure latency and memory on your deployment device. Compare against Table 1 baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NEO maintain its efficiency and performance advantages when applied to non-Transformer architectures, such as Convolutional Neural Networks (CNNs) or MLP-Mixers?
- Basis in paper: [explicit] The authors state that while the method is architecture-agnostic, they "evaluate on vision transformer architectures, and leave other architecture choices for the future."
- Why unresolved: The paper exclusively validates the method on Vision Transformers (ViT-S, B, L), leaving the interaction between NEO and the inductive biases of other architectures unexplored.
- What evidence would resolve it: Empirical benchmarking of NEO on CNNs (e.g., ResNet) and MLP-based models under the same distribution shifts.

### Open Question 2
- Question: Can the latent re-centering mechanism be effectively applied to intermediate layers of the network rather than solely the penultimate layer?
- Basis in paper: [explicit] The paper notes the analysis is "inherently limited to understanding the activations from the penultimate layer and leave[s] the investigations for other layers for future scope."
- Why unresolved: Distribution shifts likely alter the geometry of intermediate feature maps; correcting these earlier representations might further improve alignment or capture different types of corruption.
- What evidence would resolve it: A layer-wise analysis of centroid shifts and accuracy changes when applying the re-centering operation at various depths of the encoder.

### Open Question 3
- Question: To what degree does the strictness of neural collapse in the source model determine the success of re-centering at the origin?
- Basis in paper: [inferred] The theoretical justification (Proposition 4.2) relies on µG = 0 due to neural collapse, yet the experiments use "regular, publicly available models" which may not strictly satisfy this terminal phase condition.
- Why unresolved: It is unclear if the method succeeds because neural collapse holds approximately in practice, or if the global mean is a sufficiently robust proxy even without the theoretical guarantees.
- What evidence would resolve it: Correlating the "collapse" metrics (NC1-NC4) of various pre-trained models with the accuracy gains achieved by NEO.

## Limitations

- Theoretical claims rely on neural collapse properties that have not been specifically investigated for test-time adaptation scenarios
- Method evaluation focuses exclusively on Vision Transformers, leaving performance on CNNs and other architectures unexplored
- Limited analysis of performance degradation under rapidly changing non-stationary distributions

## Confidence

- **High Confidence**: Computational efficiency claims (63% inference time reduction, 9% memory reduction) are well-supported by direct measurements on Jetson Orin Nano
- **Medium Confidence**: ImageNet-C and ImageNet-R adaptation performance (59.2% and 60.3% accuracy) is robust across seven baselines, but single-corruption vs. full-suite evaluation differences warrant careful interpretation
- **Low Confidence**: Theoretical guarantees under neural collapse assumptions are mathematically sound but rely on idealized conditions (balanced classes, specific training objectives) that may not hold in all practical deployments

## Next Checks

1. Test NEO's performance degradation when adapting to rapidly changing distributions versus stationary ones, measuring accuracy drop and centroid drift rates
2. Evaluate NEO on ResNet architectures to determine if the method generalizes beyond Vision Transformers or if there are architectural dependencies
3. Implement NEO-Continual with various EMA hyperparameters (α) to quantify the trade-off between adaptation flexibility and hyperparameter sensitivity