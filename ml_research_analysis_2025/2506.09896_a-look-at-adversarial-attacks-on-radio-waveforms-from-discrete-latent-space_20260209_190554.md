---
ver: rpa2
title: A look at adversarial attacks on radio waveforms from discrete latent space
arxiv_id: '2506.09896'
source_url: https://arxiv.org/abs/2506.09896
tags:
- attack
- adversarial
- attacks
- data
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether a vector-quantized variational autoencoder
  (VQ-VAE) can suppress adversarial attacks on radio waveforms by analyzing the latent
  space structure. The method uses FGSM and PGD attacks targeting amplitude modulations,
  with phase-preserving variants to maintain signal integrity.
---

# A look at adversarial attacks on radio waveforms from discrete latent space

## Quick Facts
- arXiv ID: 2506.09896
- Source URL: https://arxiv.org/abs/2506.09896
- Reference count: 26
- Primary result: VQ-VAE substantially reduces adversarial attack effectiveness on RF waveforms, especially for stronger perturbations, as measured by classification accuracy recovery

## Executive Summary
This paper investigates whether vector-quantized variational autoencoders (VQ-VAEs) can suppress adversarial attacks on radio waveforms by analyzing the structure of the discrete latent space. The authors generate adversarial examples using FGSM and PGD attacks targeting amplitude modulations, with phase-preserving variants to maintain signal integrity. By passing attacked samples through a trained VQ-VAE and analyzing reconstruction quality and latent space properties, they demonstrate that quantization into a learned codebook substantially mitigates attack effectiveness. The study reveals modulation-dependent vulnerabilities and provides evidence that latent space analysis can detect adversarial perturbations through codebook usage shifts.

## Method Summary
The method trains a VQ-VAE on clean radio frequency (RF) waveforms from six modulation classes (4ASK, 8PAM, 16PSK, 32-QAM-X, 2FSK, OFDM256), then evaluates its ability to reconstruct adversarial examples generated via FGSM and PGD attacks at varying perturbation strengths (ε). A separate classifier is trained on clean data to assess reconstruction quality. The VQ-VAE uses stochastic vector quantization with a 128-entry codebook of 512-dimensional vectors, mapping 64-latent positions per sample. Performance is measured through classification accuracy recovery, Hamming distance, and Set distance metrics between latent codes, along with I/Q plane visualizations and codebook usage histograms.

## Key Results
- VQ-VAE reconstructions achieve near-clean classification accuracy for moderate ε values (0.01-0.2), substantially outperforming direct attack samples
- Stronger attacks (ε ≥ 0.3) cause mode collapse and reconstruction failure, particularly for high-order modulations like 32-QAM-X
- Codebook usage analysis reveals that attacks populate normally-unused codeword indices, detectable through increased Set distance
- Phase-preserving attacks (FGSM1) are notably ineffective against phase-based modulations, while amplitude-only attacks (FGSM2) show stronger performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrete vector quantization acts as a projection filter that suppresses adversarial perturbations by mapping attacked inputs to learned codebook entries trained on clean data.
- **Mechanism:** The VQ-VAE encoder produces latent vectors Ze, which are quantized to discrete codewords from a codebook Q learned during training on clean RF samples. Adversarial perturbations that fall outside the learned distribution are rounded to the nearest valid codeword, effectively denoising the input before reconstruction.
- **Core assumption:** Adversarial perturbations manifest as off-manifold deviations that do not align with learned codebook entries.
- **Evidence anchors:** [abstract] "VQV AE substantially decreases the effectiveness of the attack"; [section II-C] "stochastic vector quantization is used as a regularizer" mapping slices of Ze to codewords via learned posterior P(ZQ[i]=k|x); [corpus] Weak direct evidence; related work [21] cited in paper notes "removing redundant features mitigates the attack, which is why autoencoders are good defense mechanisms"
- **Break condition:** If attack strength ε exceeds ~0.2-0.3 (approaching SNR_a ≈ 0), mode collapse occurs and reconstruction fails to recover class structure.

### Mechanism 2
- **Claim:** Stochastic quantization via learned discrete posteriors provides regularization that limits adversarial transfer through the latent bottleneck.
- **Mechanism:** Rather than deterministic nearest-neighbor assignment, the model samples from a learned distribution over candidate codewords (Eq. 4). This stochasticity prevents gradient-based attacks from precisely controlling latent representations.
- **Core assumption:** The sampling process introduces sufficient variance to disrupt adversarial gradient paths while preserving semantic content.
- **Evidence anchors:** [section II-C] "Stochastic vector quantization is used as a regularizer to prevent the infamous mode collapse"; [section III] "the same x will produce slightly different Zq in different evaluations"—normalization required to separate attack effects from natural randomness; [corpus] No direct corpus support for this specific mechanism in RF domain
- **Break condition:** If codebook size is too small relative to modulation complexity, quantization error overwhelms both clean and attacked reconstruction.

### Mechanism 3
- **Claim:** Codebook usage histograms reveal attack presence through activation of normally-unused codeword indices.
- **Mechanism:** Clean samples from a given modulation class use a characteristic subset of codebook entries. Adversarial perturbations push latent representations toward "insignificant codewords" outside this support, measurable via Set distance metric.
- **Core assumption:** Attack perturbations distribute differently than natural signal variation across codebook entries.
- **Evidence anchors:** [section IV] "we see some probability mass in the range of 'insignificant codewords' (codewords outside of support of the original latents), suggesting the presence of noise"; [Fig. 8 description] "arrows show ranges outside of the support of normal latents, which get populated by the attack, thus increasing Set distance"; [corpus] Weak; related work on statistical detection methods cited but not directly addressing discrete latent shifts
- **Break condition:** If training data distribution shifts (e.g., multipath channel effects), baseline codebook usage changes independently, reducing detection reliability.

## Foundational Learning

- **Concept: Vector-Quantized Variational Autoencoder (VQ-VAE)**
  - Why needed here: Core architecture; must understand encoder→quantizer→decoder pipeline and how discrete bottlenecks enable compression and reconstruction.
  - Quick check question: Can you explain why quantizing to a discrete codebook differs from continuous latent compression?

- **Concept: I/Q Signal Representation**
  - Why needed here: RF signals represented as complex numbers; attacks independently perturb I and Q channels or preserve phase relationships.
  - Quick check question: Given I=0.7 and Q=0.7, what is the phase angle and amplitude?

- **Concept: FGSM and PGD Adversarial Attacks**
  - Why needed here: Understanding how perturbations are generated (single-step vs. iterative, gradient-based) informs why VQ-VAE can suppress them.
  - Quick check question: Why would PGD typically succeed more often than FGSM given the same ε budget?

## Architecture Onboarding

- **Component map:** Input I/Q tensors (1024 samples each) -> VQ-VAE encoder -> 64 latent positions × 512-dim -> Stochastic quantizer -> 128-entry codebook -> Decoder -> Reconstruction

- **Critical path:** 1. Train VQ-VAE on clean X_train until reconstruction loss stabilizes; 2. Train classifier on same X_train (target: 100% accuracy); 3. Generate adversarial examples X_af1, X_af2, X_ap at varying ε; 4. Pass attacked samples through VQ-VAE, collect Z_q and reconstructions; 5. Evaluate classifier accuracy on reconstructions; compute Hamming/Set distances

- **Design tradeoffs:** Codebook size (qs=128): Larger increases representation capacity but dilutes class-specific support, potentially aiding attacks on low-order modulations; Stochastic vs. deterministic quantization: Stochastic aids regularization but requires normalized distance metrics to isolate attack effects; ε threshold: Below ~0.2, VQ-VAE reliably recovers; above, mode collapse risk increases sharply

- **Failure signatures:** 32-QAM-X and phase-based modulations show poor recovery at ε=0.3 even after VQ-VAE processing; 2FSK shows high Set distance (novel codeword indices activated) despite classification resilience; Accuracy cliff appears when SNR_a approaches zero (ε ≈ std(X_train)=0.25)

- **First 3 experiments:** 1. Replicate accuracy recovery curve: Train VQ-VAE on 6 modulation classes, test classifier accuracy on reconstructions across ε∈{0.01,0.06,0.1,0.2,0.3} for all three attack types; 2. Codebook usage visualization: For one modulation (e.g., 8PAM), plot histogram of codeword indices at each ε level; identify "insignificant" regions populated only under attack; 3. Hamming vs. Set distance analysis: Compute both normalized distances for each modulation class; test hypothesis that high Set distance with low Hamming indicates attack-driven sampling from redundant codebook regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a multi-feature attack detector be effectively developed using discrete latent space statistics combined with waveform space data?
- Basis in paper: [explicit] The conclusion states, "In future work, we aim to develop a multi-feature attack detector, which will leverage our findings about the discrete space and the statistics from the waveform space."
- Why unresolved: The current study only analyzes the properties of the latent space and reconstruction accuracy; it does not implement or test a detection system.
- What evidence would resolve it: The successful implementation of a detector that achieves high detection rates on adversarial examples by fusing the analyzed metrics.

### Open Question 2
- Question: Does VQ-VAE-based mitigation remain effective when applied to multipath-affected signals or over-the-air data?
- Basis in paper: [inferred] The authors mention they studied multipath-affected signals but omitted these results "due to space limits," focusing solely on clean, high-SNR data.
- Why unresolved: The paper restricts its evaluation to clean signals obtained after channel estimation, leaving the robustness of the method in realistic channel conditions unverified.
- What evidence would resolve it: Classification accuracy and latent distance metrics calculated on datasets containing multipath fading and channel noise.

### Open Question 3
- Question: Does tailoring the codebook size to specific modulation complexities reduce the vulnerability to attacks that exploit redundant codebook indices?
- Basis in paper: [inferred] The authors conjecture that the high dimension of the shared codebook allows attacks to map low-order modulations (like 2FSK) to "redundant" indices.
- Why unresolved: The paper observes the phenomenon (increased Set distance) but does not experimentally vary the codebook architecture to confirm the hypothesis.
- What evidence would resolve it: Comparative experiments using modulation-specific or adaptive codebook sizes showing reduced Set distances under attack.

## Limitations
- Critical implementation details missing: VQ-VAE architecture specifics (layer configurations, channel dimensions), training hyperparameters (learning rate, batch size, epochs, β weight), and dataset size
- Classifier architecture underspecified beyond high-level layer descriptions
- Analysis limited to 6 modulation classes with specific SNR conditions, limiting generalizability
- Stochastic quantization mechanism effectiveness depends heavily on learned posterior distribution, which is not fully characterized

## Confidence
- **High Confidence:** The core finding that VQ-VAE reduces adversarial attack effectiveness, particularly for stronger perturbations, is well-supported by classification accuracy metrics
- **Medium Confidence:** The codebook usage analysis showing attack-induced activation of normally-unused codeword indices is plausible but relies on unstated assumptions about clean data distribution stability
- **Medium Confidence:** The differential resilience across modulation types (e.g., 8PAM recovery vs. 32-QAM-X failure) is supported by results but requires understanding of underlying reasons beyond the presented analysis

## Next Checks
1. **Replication of Accuracy Recovery Curve:** Train VQ-VAE on the specified 6 modulation classes and reproduce the classification accuracy on reconstructions across all ε values for FGSM1, FGSM2, and PGD attacks. Verify the accuracy cliff at ε ≈ 0.2-0.3 and compare clean vs. attacked reconstruction performance.

2. **Codebook Usage Visualization Under Attack:** For 8PAM and 2FSK, generate codebook usage histograms at each ε level. Confirm that attack samples populate codeword indices outside the clean support range, validating the Set distance analysis. Check if these "insignificant codewords" correlate with classification accuracy drops.

3. **Phase-Preservation Attack Effectiveness:** Systematically test FGSM1 (phase-preserving) against FGSM2 (independent I/Q) on phase-based modulations (16PSK, 32-QAM-X). Verify the claim that phase-preserving attacks fail to degrade performance, and analyze why VQ-VAE cannot recover these modulations at high ε.