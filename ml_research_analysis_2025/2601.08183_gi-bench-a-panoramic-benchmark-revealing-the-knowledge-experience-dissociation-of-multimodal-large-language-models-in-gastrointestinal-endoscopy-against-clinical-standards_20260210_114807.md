---
ver: rpa2
title: 'GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation
  of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical
  Standards'
arxiv_id: '2601.08183'
source_url: https://arxiv.org/abs/2601.08183
tags:
- clinical
- lesion
- performance
- mllms
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GI-Bench is a comprehensive benchmark for evaluating multimodal\
  \ large language models (MLLMs) in gastrointestinal endoscopy. It covers five clinical\
  \ tasks\u2014anatomical localization, lesion identification, diagnosis, findings\
  \ description, and management recommendations\u2014across 20 lesion categories."
---

# GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards

## Quick Facts
- **arXiv ID**: 2601.08183
- **Source URL**: https://arxiv.org/abs/2601.08183
- **Reference count**: 0
- **Primary result**: GI-Bench benchmark reveals MLLMs excel in knowledge tasks but underperform in spatial localization compared to trainees

## Executive Summary
GI-Bench is a comprehensive benchmark designed to evaluate multimodal large language models (MLLMs) in gastrointestinal endoscopy across five clinical tasks: anatomical localization, lesion identification, diagnosis, findings description, and management recommendations. The benchmark covers 20 lesion categories and evaluates twelve MLLMs against human endoscopists using standardized metrics including Macro-F1, mIoU, and Likert scores. The evaluation reveals a critical dissociation between knowledge-based performance and spatial reasoning capabilities, with models matching junior endoscopists in diagnostic reasoning but significantly underperforming in lesion localization tasks.

The study exposes a "fluency-accuracy paradox" where MLLMs generate reports with superior linguistic readability but significantly lower factual accuracy compared to human practitioners. This finding suggests that current MLLMs function at an "advanced beginner" level in clinical endoscopy, excelling at knowledge retrieval and description but lacking the spatial precision and clinical intuition required for autonomous diagnostic decision-making. The benchmark provides a standardized framework for assessing MLLM performance in medical imaging contexts and establishes performance baselines for future model development in gastrointestinal endoscopy applications.

## Method Summary
The GI-Bench framework was developed through a systematic process involving clinical expert consultation to identify five core endoscopic tasks and 20 representative lesion categories. Twelve state-of-the-art MLLMs were evaluated using a standardized dataset of endoscopic images, with performance metrics including Macro-F1 for classification tasks, mean Intersection over Union (mIoU) for spatial localization, and Likert scales for report quality assessment. Human performance baselines were established by comparing model outputs against three junior endoscopists and three trainees, with evaluations conducted in a controlled clinical environment to ensure consistency. The benchmark employed a multi-modal evaluation approach, assessing both the visual understanding capabilities and natural language generation quality of each model across all five clinical task domains.

## Key Results
- Gemini-3-Pro achieved state-of-the-art performance matching junior endoscopists in diagnostic reasoning (Macro-F1 0.641)
- Models significantly underperformed trainees in spatial lesion localization (mIoU 0.345 vs 0.506)
- Generated reports showed superior linguistic readability but significantly lower factual accuracy
- Current MLLMs function as "advanced beginners," excelling in knowledge retrieval but lacking spatial precision

## Why This Works (Mechanism)
The performance patterns observed in GI-Bench stem from fundamental architectural differences between knowledge-based reasoning and spatial understanding in multimodal systems. MLLMs leverage their extensive pretraining on textual medical knowledge to excel at diagnostic classification and descriptive tasks, where pattern recognition and semantic understanding are paramount. However, the spatial reasoning required for accurate lesion localization demands precise coordinate mapping and geometric understanding that current multimodal architectures struggle to integrate with their language generation capabilities.

## Foundational Learning
- **Multimodal representation fusion**: Combining visual and textual features is essential for endoscopic analysis; quick check: verify feature alignment across modalities
- **Spatial reasoning in medical imaging**: Precise coordinate mapping is critical for lesion localization; quick check: evaluate mIoU consistency across lesion types
- **Clinical task decomposition**: Breaking complex endoscopic workflows into discrete evaluation components enables systematic benchmarking
- **Knowledge-grounded generation**: Language models must balance factual accuracy with linguistic fluency; quick check: compare report readability vs factual precision

## Architecture Onboarding
- **Component map**: Image encoder -> Spatial reasoning module -> Text decoder -> Clinical task adapter
- **Critical path**: Visual feature extraction → Spatial localization → Diagnostic reasoning → Report generation
- **Design tradeoffs**: Balance between fine-grained spatial precision and broad semantic understanding
- **Failure signatures**: Spatial misalignment in lesion boundaries, knowledge hallucination in rare conditions
- **3 first experiments**: 1) Cross-model ablation of spatial modules, 2) Domain adaptation on rare lesion types, 3) Temporal sequence evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of six human evaluators may not capture full expertise variability
- Potential subjective bias in human evaluation of report quality and factual accuracy
- Controlled clinical environment may not represent real-world endoscopic complexity

## Confidence
- Medium confidence in performance gaps between junior endoscopists and trainees due to limited sample size
- High confidence in general pattern of superior knowledge-based vs spatial reasoning performance
- Medium confidence in "fluency-accuracy paradox" due to potential subjective evaluation bias

## Next Checks
- Independent replication with larger, more diverse panel of endoscopists across all experience levels
- Longitudinal evaluation of model performance on video sequences rather than static images
- Prospective clinical deployment studies to measure actual impact on diagnostic accuracy and patient outcomes