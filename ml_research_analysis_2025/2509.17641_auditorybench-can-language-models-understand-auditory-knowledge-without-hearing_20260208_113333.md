---
ver: rpa2
title: 'AuditoryBench++: Can Language Models Understand Auditory Knowledge without
  Hearing?'
arxiv_id: '2509.17641'
source_url: https://arxiv.org/abs/2509.17641
tags:
- auditory
- reasoning
- imagine
- knowledge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AuditoryBench++, a benchmark for evaluating
  auditory knowledge and reasoning in text-only settings. It addresses the gap in
  auditory commonsense understanding for language models, which struggle with tasks
  like pitch, loudness, and sound-source associations without direct audio input.
---

# AuditoryBench++

## Quick Facts
- **arXiv ID**: 2509.17641
- **Source URL**: https://arxiv.org/abs/2509.17641
- **Reference count**: 0
- **Primary result**: Introduces AuditoryBench++ benchmark and AIR-CoT method, achieving 82.67% accuracy on auditory context reasoning (+21.34% over standard fine-tuning)

## Executive Summary
This paper introduces AuditoryBench++, a benchmark for evaluating auditory knowledge and reasoning in text-only settings. It addresses the gap in auditory commonsense understanding for language models, which struggle with tasks like pitch, loudness, and sound-source associations without direct audio input. The benchmark includes five tasks ranging from basic comparisons to complex contextual reasoning, constructed through rigorous filtering and verification. The paper proposes AIR-CoT, a novel auditory imagination reasoning method that enables language models to dynamically generate and process auditory embeddings during inference, significantly outperforming both standard language models and models augmented with auditory knowledge.

## Method Summary
AIR-CoT uses a two-stage approach: first detecting spans requiring auditory reasoning via special tokens, then injecting relevant auditory knowledge through audio embeddings. Stage 1 performs supervised fine-tuning to teach the model to generate `[imagine]` tokens around text spans needing auditory reasoning. Stage 2 freezes the model and trains only a 2-layer MLP projector to align CLAP text embeddings to the language model's hidden dimension. During inference, when the model generates `[/imagine]`, it pauses generation, retrieves a CLAP embedding for the span text, projects it through the MLP, and injects this auditory knowledge before resuming reasoning. The method is evaluated on AuditoryBench++, a comprehensive benchmark with 6,732 samples across five auditory reasoning tasks.

## Key Results
- AIR-CoT achieves 82.67% accuracy on auditory context reasoning task, outperforming standard fine-tuning by 21.34 percentage points
- Strong performance on pitch comparison (83.89%, +8.25%) and animal sound recognition tasks
- Demonstrates significant improvement over off-the-shelf language models across all benchmark tasks
- Shows method's effectiveness in enabling end-to-end auditory imagination within reasoning chains

## Why This Works (Mechanism)

### Mechanism 1: Auditory Span Detection via Special Tokens
LLMs can learn to identify text spans requiring auditory knowledge through supervised fine-tuning with special tokens. During Stage 1 SFT, the model generates `[imagine]...[/imagine]` tokens around spans demanding auditory reasoning, with loss computed only on these tokens. This forces the model to learn auditory-knowledge boundaries before reasoning. Core assumption: Auditory reasoning requirements are locally detectable from linguistic context alone.

### Mechanism 2: Embedding Injection via Imagination Module
Pre-trained audio-text embeddings can provide "auditory imagination" to LLMs when injected at inference time. When the model emits `[/imagine]`, generation pauses, CLAP text encoder produces an embedding from the span text, a 2-layer MLP projects it to the LLM's hidden dimension, and this embedding replaces the token. Core assumption: CLAP embeddings capture task-relevant acoustic properties like pitch relations and sound-source semantics.

### Mechanism 3: Two-Stage Training Decomposition
Separating span detection (Stage 1) from knowledge injection (Stage 2) prevents interference and stabilizes learning. Stage 1 trains the full LLM to emit `[imagine]` tokens, while Stage 2 freezes the LLM and trains only the MLP projector. Core assumption: Span detection is a prerequisite that should not be retrained during embedding alignment.

## Foundational Learning

- **Concept: CLAP (Contrastive Language-Audio Pretraining)**
  - Why needed here: CLAP provides the "imagination" embeddings by aligning audio and text in shared space, enabling text→audio embedding retrieval without actual audio.
  - Quick check question: Can CLAP distinguish "high pitch" from "low pitch" based on text alone?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: AIR-CoT extends CoT by inserting auditory "imaginations" into reasoning chains. Understanding standard CoT is prerequisite.
  - Quick check question: How does inserting intermediate reasoning steps affect final answer accuracy in LLMs?

- **Concept: Special Token Injection**
  - Why needed here: The `[imagine]` mechanism requires vocabulary extension and embedding replacement at specific token positions during decoding.
  - Quick check question: What happens to hidden states when a token's embedding is replaced mid-generation?

## Architecture Onboarding

- **Component map**: LLM Backbone (Qwen2.5 7B) → Imagination Module (CLAP text encoder → 2-layer MLP) → Special Tokens ([imagine], [/imagine])

- **Critical path**:
  1. Data prep: LLM generates training data with `[imagine]`-marked spans using Qwen2.5-32B for annotation
  2. Stage 1: Fine-tune Qwen2.5-7B to emit special tokens (10 epochs, lr=1e-5)
  3. Stage 2: Freeze LLM, train MLP projector only (10 epochs, lr=1e-4)
  4. Inference: Decode → detect `[/imagine]` → pause → encode span with CLAP → project → inject → resume decoding

- **Design tradeoffs**:
  - End-to-end vs. Cascade: AIR-CoT is end-to-end (model controls when to imagine) vs. Imagine to Hear's cascade (external retrieval first)
  - CLAP text encoder vs. audio encoder: Using text-to-embedding enables text-only operation but sacrifices acoustic fidelity
  - MLP-only Stage 2: Freezing LLM preserves span detection but limits embedding integration depth

- **Failure signatures**:
  - Random-level performance on comparison tasks: Indicates span detection or embedding injection failure
  - Strong pitch/animal, weak duration/loudness: Expected—CLAP lacks temporal/amplitude encoding
  - No `[imagine]` tokens generated during inference: Stage 1 training failed or special tokens not added to vocabulary

- **First 3 experiments**:
  1. Baseline probe: Run off-the-shelf Qwen2.5-7B on AuditoryBench++ pitch comparison to confirm knowledge gap
  2. Stage 1 validation: After Stage 1 training, verify `[imagine]` token generation frequency and span accuracy on held-out examples
  3. Embedding ablation: Replace CLAP embeddings with random vectors in Stage 2 to isolate whether performance gains come from auditory knowledge vs. additional trainable parameters

## Open Questions the Paper Calls Out

- **Open Question 1**: How can audio embedding models be enhanced to encode temporal duration and amplitude information for effective duration and loudness comparison tasks?
  - Basis in paper: The paper states: "We leave this challenging problem for future work" and notes that "duration and loudness depend on temporal and amplitude cues that these representations do not capture well, with duration being particularly challenging since current embeddings do not reflect the time axis."
  - Why unresolved: Current audio representations like CLAP are primarily semantic-focused and lack mechanisms to encode temporal extent or signal intensity properties.
  - What evidence would resolve it: Modified audio encoders producing embeddings that explicitly encode duration and loudness, validated through improved performance on these specific comparison tasks.

- **Open Question 2**: Would AIR-CoT's two-stage training paradigm generalize effectively to different base language model architectures and parameter scales?
  - Basis in paper: The implementation exclusively uses Qwen2.5 7B for both training stages, leaving unclear whether the approach depends on specific architectural properties or scale requirements.
  - Why unresolved: The span detection and knowledge injection mechanisms may interact differently with varying model capacities and attention patterns.
  - What evidence would resolve it: Systematic evaluation across multiple base models (e.g., LLaMA, Phi, Mistral) and sizes demonstrating consistent transferability or identifying architectural dependencies.

- **Open Question 3**: Can the auditory imagination mechanism complement or integrate with direct audio processing in multimodal settings?
  - Basis in paper: The paper focuses solely on text-only settings, noting this reflects scenarios where "direct audio input is unavailable," but does not explore how AIR-CoT interacts with actual audio inputs in LALMs.
  - Why unresolved: The relationship between imagined auditory knowledge and perceptual audio processing remains unexamined—it is unclear whether they would conflict or synergize.
  - What evidence would resolve it: Comparative experiments evaluating AIR-CoT on multimodal LALMs with and without audio input, measuring whether imagination enhances or interferes with direct auditory perception.

## Limitations

- **CLAP embedding coverage**: The method cannot solve tasks requiring acoustic attributes outside CLAP's semantic space, particularly duration and loudness comparisons
- **Training data quality**: The method relies on Qwen2.5-32B to annotate training data with `[imagine]` tokens, introducing potential cascading errors if the large model misidentifies auditory spans
- **Single benchmark evaluation**: All evaluation uses AuditoryBench++, leaving uncertainty about generalization to other auditory reasoning scenarios or real-world applications

## Confidence

**High Confidence Claims**:
- AIR-CoT architecture and training procedure are correctly described and implementable
- Performance improvements on pitch comparison and auditory context reasoning tasks are accurately reported
- CLAP embeddings do not capture duration/loudness information (acknowledged limitation)

**Medium Confidence Claims**:
- AIR-CoT significantly outperforms standard fine-tuning on auditory tasks (21.34% improvement on context reasoning)
- The two-stage training approach is beneficial (no ablation evidence provided)
- Span detection via special tokens is an effective mechanism (assumes training data quality)

**Low Confidence Claims**:
- AIR-CoT would generalize to broader auditory reasoning tasks beyond the benchmark
- The method could be extended to handle duration/loudness with different embeddings (unverified)
- The approach represents the optimal solution for text-only auditory reasoning (no comparison to alternatives)

## Next Checks

1. **Training Data Quality Audit**: Manually sample 100 annotated examples from the Qwen2.5-32B-generated training data and verify `[imagine]` token placement accuracy to quantify the cascading error risk from the annotation pipeline.

2. **Embedding Ablation Study**: Replace CLAP embeddings with random vectors in Stage 2 training while keeping all other components identical to isolate whether performance gains come from actual auditory knowledge vs. additional trainable parameters in the MLP projector.

3. **Joint Training Comparison**: Implement a variant that trains span detection and embedding alignment simultaneously (vs. staged approach) to determine if the two-stage decomposition is truly beneficial or just conventional.