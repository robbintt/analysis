---
ver: rpa2
title: 'Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning
  from Multi-Mode Supervision'
arxiv_id: '2512.15489'
source_url: https://arxiv.org/abs/2512.15489
tags:
- reasoning
- python
- training
- mathematical
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nemotron-Math addresses the challenge of generating diverse, high-quality
  mathematical reasoning supervision by leveraging multi-mode generation from gpt-oss-120b.
  The dataset combines 85K curated AoPS problems with 262K StackExchange-Math questions,
  producing 7.5M solution traces across three reasoning modes (high, medium, low)
  with and without Python tool-integrated reasoning (TIR).
---

# Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision

## Quick Facts
- **arXiv ID**: 2512.15489
- **Source URL**: https://arxiv.org/abs/2512.15489
- **Reference count**: 35
- **Primary result**: Nemotron-Math dataset enables 100% maj@16 accuracy on AIME24/25 benchmarks with Qwen3-8B/30B-A3B under Python TIR high reasoning mode.

## Executive Summary
Nemotron-Math addresses the challenge of generating diverse, high-quality mathematical reasoning supervision by leveraging multi-mode generation from gpt-oss-120b. The dataset combines 85K curated AoPS problems with 262K StackExchange-Math questions, producing 7.5M solution traces across three reasoning modes (high, medium, low) with and without Python tool-integrated reasoning (TIR). Controlled experiments show that Nemotron-Math significantly outperforms prior datasets on AIME 2024/2025 benchmarks, with up to 13.1% accuracy improvement. Incorporating StackExchange-Math improves robustness on HLE-Math without harming competition performance. A sequential bucketed training strategy accelerates 128K-context fine-tuning by 2–3× with minimal accuracy loss.

## Method Summary
Nemotron-Math generates a large-scale mathematical reasoning dataset by combining 85K AoPS problems with 262K StackExchange-Math questions, producing 7.5M solution traces via gpt-oss-120b. The dataset features three reasoning modes (high, medium, low) with and without Python TIR. Fine-tuning uses Qwen3-8B or Qwen3-30B-A3B with AdamW, learning rate 2e-4, and global batch size 2048. Sequential bucketed training (16K→32K→64K→128K context) accelerates 128K-context fine-tuning by 2–3×. Filtering removes trivial problems (pass rate <0.8 using low-mode solutions), and answers are validated via Qwen2.5-32B-Instruct LLM judge. The final 128K bucket balances reasoning modes to prevent collapse.

## Key Results
- Nemotron-Math achieves 100% maj@16 accuracy on AIME24/25 for both Qwen3-8B and Qwen3-30B-A3B under Python TIR high reasoning mode.
- Incorporating StackExchange-Math improves HLE-Math robustness without harming competition performance.
- Sequential bucketed training accelerates 128K-context fine-tuning by 2–3× with minimal accuracy loss.
- Nemotron-Math outperforms prior datasets on AIME 2024/2025 benchmarks by up to 13.1% accuracy.

## Why This Works (Mechanism)
Nemotron-Math leverages multi-mode supervision to capture diverse reasoning depths, enabling student models to learn flexible problem-solving strategies. Python TIR integration provides tool-based reasoning capabilities, while the combination of AoPS and StackExchange-Math ensures both competition-focused and open-domain generalization. The sequential bucketed training strategy enables efficient long-context fine-tuning by progressively increasing context length while maintaining accuracy.

## Foundational Learning
- **Multi-mode supervision**: Three reasoning depths (high/medium/low) enable flexible problem-solving. Needed to capture diverse solution strategies and prevent overfitting to a single reasoning pattern. Quick check: Verify that each mode produces distinct solution lengths and approaches.
- **Python tool-integrated reasoning (TIR)**: Embeds Python code execution for numerical and symbolic computation. Needed for problems requiring precise calculations or symbolic manipulation. Quick check: Confirm that TIR solutions execute correctly and match expected outputs.
- **Sequential bucketed training**: Progressive context length increase (16K→32K→64K→128K) with parallelism configs. Needed to accelerate long-context fine-tuning while maintaining accuracy. Quick check: Monitor accuracy and training speed at each bucket transition.
- **LLM-based filtering and verification**: Uses Qwen2.5-32B-Instruct to filter trivial problems and validate answers. Needed to ensure dataset quality and remove degenerate solutions. Quick check: Test filtering consistency across multiple LLM judges.
- **Reasoning mode balancing**: Explicitly balances high/medium/low modes in final 128K bucket. Needed to prevent mode collapse and maintain diversity. Quick check: Track mode distribution at long context lengths.
- **Dataset composition**: Combines AoPS (competition math) with StackExchange-Math (open-domain). Needed for both benchmark performance and generalization. Quick check: Evaluate performance on AoPS-only vs. combined datasets.

## Architecture Onboarding

### Component Map
gpt-oss-120b (teacher) -> Multi-mode generation -> Nemotron-Math dataset -> Qwen3-8B/30B-A3B (student) -> Fine-tuning pipeline -> AIME/HMMT/HLE-Math benchmarks

### Critical Path
1. Generate 7.5M solution traces with multi-mode and TIR configurations
2. Filter trivial problems and validate answers via LLM judge
3. Fine-tune Qwen3 models using sequential bucketed training
4. Balance reasoning modes in final 128K bucket
5. Evaluate on AIME24/25, HMMT, and HLE-Math benchmarks

### Design Tradeoffs
- **Efficiency vs. accuracy**: Sequential bucketed training provides 2–3× speedup but introduces 1–3% accuracy gap versus full-length joint training
- **Dataset diversity vs. focus**: Combining AoPS and StackExchange-Math improves HLE-Math but may dilute competition-specific patterns
- **Filtering stringency vs. coverage**: Strict filtering removes trivial problems but may discard edge cases

### Failure Signatures
- Mode collapse in long-context buckets (medium/low modes generating high-depth reasoning)
- Degraded HLE-Math performance if training only on AoPS
- Accuracy loss if teacher model quality or access is limited

### First Experiments
1. Generate a small subset of multi-mode traces and verify distinct reasoning patterns
2. Test bucketed training on 32K context to confirm acceleration before scaling to 128K
3. Compare AoPS-only vs. AoPS+StackExchange-Math performance on HLE-Math benchmark

## Open Questions the Paper Calls Out

### Open Question 1
Can the mode-collapse phenomenon in sequential bucketed training be fully prevented while retaining efficiency gains? The authors note that imbalance in the 128K bucket causes medium/low reasoning modes to "collapse toward uniformly long, high-depth reasoning" and lose their intended distinction, even with their balancing workaround. The paper's mitigation preserves mode diversity but does not establish whether collapse can be fundamentally prevented without sacrificing the 2–3× speedup. A sweep across multiple AoPS:StackExchange ratios (e.g., 90:10, 75:25, 50:50, 25:75) with joint evaluation on both competition benchmarks and open-domain tasks.

### Open Question 2
What is the optimal mixing ratio between AoPS and StackExchange-Math data for balancing competition performance and open-domain generalization? The controlled experiment uses a fixed 50% replacement of AoPS with StackExchange-Math, but the optimal proportion remains unexplored despite clear trade-offs in the results. The paper demonstrates that adding StackExchange-Math improves HLE-Math without harming AIME/HMMT, but does not characterize the full Pareto frontier across different mixing ratios.

### Open Question 3
Does training exclusively on gpt-oss-120b-generated trajectories limit student models to the teacher's reasoning patterns? All 7.5M traces derive from a single teacher model; the paper does not compare against data from multiple teachers or verify whether students acquire capabilities absent in gpt-oss-120b. While Nemotron-Math outperforms OpenMathReasoning (generated by DeepSeek-R1), the observed gains may reflect data quality differences rather than teacher-independent learning.

### Open Question 4
Is the 1–3% accuracy gap between sequential bucketed training and full-length joint training inherent to the progressive approach or remediable? Table 5 shows consistent minor degradation for bucketed training; the paper states the gap is "without significant accuracy loss" but does not investigate whether the gap can be eliminated. The design space of bucket boundaries, stage-wise learning rates, and curriculum strategies remains unexplored.

## Limitations
- Dependency on gpt-oss-120b model access and quality for supervision generation
- Potential mode collapse in long-context buckets requiring careful balancing
- 1–3% accuracy gap versus full-length joint training due to sequential bucketing
- Filtering and verification pipeline details not fully specified, raising quality concerns

## Confidence

**High Confidence**: Nemotron-Math significantly outperforms prior datasets on AIME 2024/2025 benchmarks (supported by direct experimental comparison).

**Medium Confidence**: Sequential bucketed training accelerates 128K-context fine-tuning by 2–3× (based on parallelism configs but lacks explicit sample counts).

**Low Confidence**: 100% maj@16 accuracy on AIME24/25 achievable for both Qwen3-8B and Qwen3-30B-A3B under Python TIR high reasoning mode (sensitive to model access and prompt fidelity).

## Next Checks

1. **Replicate dataset generation pipeline** using an available, comparable teacher model to verify multi-mode supervision quality and diversity without gpt-oss-120b.

2. **Perform mode balance analysis during bucketed training** by monitoring relative frequencies of high, medium, and low reasoning modes at each context length; adjust sampling to prevent mode collapse and verify accuracy gains.

3. **Test impact of filtering and verification pipeline** through ablation studies: train models with and without pass-rate filtering and LLM judge steps, measuring effects on competition and HLE-Math benchmarks.