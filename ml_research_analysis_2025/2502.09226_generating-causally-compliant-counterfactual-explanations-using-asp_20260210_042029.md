---
ver: rpa2
title: Generating Causally Compliant Counterfactual Explanations using ASP
arxiv_id: '2502.09226'
source_url: https://arxiv.org/abs/2502.09226
tags:
- counterfactual
- causal
- cogs
- rules
- counterfactuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces CoGS, a framework for generating realistic
  counterfactual explanations in automated decision-making systems. Unlike existing
  approaches that ignore causal dependencies, CoGS produces both a counterfactual
  solution and a path of achievable interventions respecting causal constraints.
---

# Generating Causally Compliant Counterfactual Explanations using ASP

## Quick Facts
- arXiv ID: 2502.09226
- Source URL: https://arxiv.org/abs/2502.09226
- Authors: Sopam Dasgupta
- Reference count: 19
- Primary result: Framework generates 240, 112, and 78 causally consistent counterfactual sets on German, Adult, and Car datasets respectively

## Executive Summary
CoGS introduces a framework for generating causally compliant counterfactual explanations in automated decision-making systems. Unlike existing approaches that ignore causal dependencies, CoGS produces both a counterfactual solution and a path of achievable interventions respecting causal constraints. The method models feature knowledge and decision rules using Answer Set Programming (ASP) and s(CASP), solving a planning problem where the goal state is a causally consistent counterfactual. Experiments demonstrate the framework can generate multiple sets of counterfactuals with execution times ranging from 1126 to 3236 milliseconds.

## Method Summary
CoGS generates counterfactual explanations by modeling the problem as a planning task in Answer Set Programming. The framework uses FOLD-SE to learn decision rules and potential causal dependencies from training data, which users then validate before incorporation into the ASP knowledge base. The s(CASP) solver searches for a sequence of valid interventions from the initial negative state to a positive counterfactual state, ensuring all causal constraints are respected throughout the path. This approach produces not just the final counterfactual solution but also a step-by-step intervention path that users can follow to achieve the desired outcome.

## Key Results
- Generated 240 counterfactual sets on German Credit dataset with 3236ms execution time
- Generated 112 counterfactual sets on Adult dataset with 2156ms execution time
- Generated 78 counterfactual sets on Car Evaluation dataset with 1126ms execution time
- All generated counterfactuals respect causal constraints between features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: s(CASP) models causal dependencies through program completion, transforming implication rules into biconditional rules
- Mechanism: s(CASP) converts "if" rules (P ⇒ Q) into "if and only if" rules ((P ⇒ Q) ∧ (¬P ⇒ ¬Q)), creating bidirectional constraints
- Core assumption: Feature dependencies can be correctly identified and encoded as logical rules
- Evidence anchors: Abstract states "CoGS computes paths that respect the causal constraints among features"; section 2 explains s(CASP)'s program completion approach
- Break condition: Misspecified causal rules propagate errors through all generated counterfactuals

### Mechanism 2
- Claim: CoGS treats counterfactual generation as a planning problem with causally consistent goal states
- Mechanism: States are encoded as feature-value pairs; s(CASP) searches for valid transition sequences from negative to positive outcomes
- Core assumption: Search space remains tractable under ASP's goal-directed execution
- Evidence anchors: Abstract mentions "path of achievable interventions"; section 3 describes the planning framework
- Break condition: Deep causal chains may cause timeout or memory exhaustion

### Mechanism 3
- Claim: FOLD-SE discovers potential causal dependencies from data that users verify before incorporation
- Mechanism: Rule-based machine learning outputs human-readable logical rules for expert validation
- Core assumption: Domain experts can distinguish causal from correlational relationships
- Evidence anchors: Abstract mentions "automatically discover potential dependencies"; section 1 describes FOLD-SE integration
- Break condition: Approval of non-causal correlations creates incorrect intervention sequences

## Foundational Learning

- Concept: Answer Set Programming (ASP) fundamentals
  - Why needed here: CoGS is implemented entirely in s(CASP), requiring understanding of ASP's declarative syntax and stable model semantics
  - Quick check question: Can you write an ASP rule that encodes "if feature A has value X, then feature B must have value Y"?

- Concept: Causal intervention vs. observation
  - Why needed here: Distinguishes direct actions from causal actions, explaining why standard counterfactuals fail
  - Quick check question: For features [age, education_level, income], which pairs might have causal relationships requiring careful intervention ordering?

- Concept: Planning as search in state space
  - Why needed here: CoGS frames counterfactual generation as planning, requiring understanding of state representation and transition operators
  - Quick check question: If you have 10 binary features, what is the size of the state space, and what constraints reduce the searchable subset?

## Architecture Onboarding

- Component map: [Training Data] → FOLD-SE → [Decision Rules D + Candidate Causal Rules C] → [Expert Review] → [Approved Causal Rules] → [ASP Knowledge Base] → [s(CASP) Solver] → [Counterfactual Path]

- Critical path: FOLD-SE rule learning → expert causal validation → ASP rule encoding → s(CASP) query execution. Errors in early stages compound.

- Design tradeoffs:
  - Completeness vs. speed: 1126–3236ms execution times with acknowledged scalability concerns
  - Automation vs. human-in-loop: Full automation requires trusting learned causal rules
  - Path minimality vs. interpretability: Shorter paths are computationally easier but may skip intermediate states

- Failure signatures:
  - No counterfactual found: Unreachable positive class, over-restrictive causal constraints, or search timeout
  - Unrealistic counterfactuals: Incorrect causal rules; verify FOLD-SE output against domain knowledge
  - Excessive execution time (>5s): Deep causal chains or large feature domains

- First 3 experiments:
  1. Reproduce on German dataset with provided FOLD-SE rules; verify output matches paper's Table 1 path and 240 counterfactual sets
  2. Remove all causal rules from ASP knowledge base; compare counterfactual quality and execution time
  3. Add a spurious causal rule (e.g., "age determines education_level"); trace how this propagates into invalid intervention paths

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with execution times ranging from 1126 to 3236 milliseconds
- Dependence on accurate causal rule specification through FOLD-SE and expert validation
- Computational limits when causal dependencies create deep chains

## Confidence
- Counterfactual quality claims: Medium - demonstrates causal compliance but lacks systematic evaluation against ground-truth causal structures
- Execution time scalability: Low - explicit acknowledgment of computational limits with no mitigation strategies provided
- Framework effectiveness: Medium - successful on three datasets but limited scope

## Next Checks
1. Verify reproducibility by running CoGS on German dataset and confirming 240 counterfactual sets with ~3236ms execution time
2. Test the impact of removing causal constraints by comparing counterfactual generation with and without causal rules
3. Evaluate the effect of incorrect causal rules by injecting spurious dependencies and observing intervention path changes