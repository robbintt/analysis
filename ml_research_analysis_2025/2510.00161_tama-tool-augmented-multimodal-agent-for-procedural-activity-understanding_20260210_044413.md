---
ver: rpa2
title: 'TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding'
arxiv_id: '2510.00161'
source_url: https://arxiv.org/abs/2510.00161
tags:
- tool
- answer
- question
- tools
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TAMA, a training-free agentic framework for
  procedural activity understanding that enables interleaved multimodal reasoning
  through multimedia-returning tools. The approach uses a vision-language model (VLM)
  as an agent to flexibly explore video content and instructions by calling tools
  that return either images or text.
---

# TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding

## Quick Facts
- **arXiv ID**: 2510.00161
- **Source URL**: https://arxiv.org/abs/2510.00161
- **Reference count**: 28
- **Primary result**: Training-free agentic framework for procedural activity understanding using multimedia-returning tools, showing GPT-5 achieves 14.1% improvement over naive approach on ProMQA-Assembly dataset.

## Executive Summary
This paper introduces TAMA, a training-free agentic framework for procedural activity understanding that enables interleaved multimodal reasoning through multimedia-returning tools. The approach uses a vision-language model (VLM) as an agent to flexibly explore video content and instructions by calling tools that return either images or text. Experiments on the ProMQA-Assembly dataset show that TAMA improves performance for certain models, particularly GPT-5 and MiMo-VL, with GPT-5 achieving a 14.1% improvement over the naive approach. Ablation studies confirm the effectiveness of multimedia-returning tools and agentic flexible tool selection.

## Method Summary
TAMA is a training-free agentic framework where a VLM agent orchestrates multimodal reasoning through tool use. The agent uses four multimedia-returning tools: `sample_frame` to extract frames from video, `zoom_in` to focus on specific regions, `check_instruction` to query assembly instructions, and `check_final_picture` to verify target assembly. The framework operates through an iterative loop where the agent generates thoughts and tool calls, executes tools to obtain images or text, and appends both to the conversation context. This enables interleaved multimodal reasoning where the agent can "think with images" rather than converting all visual information to text.

## Key Results
- GPT-5 achieves 14.1% improvement over the naive approach on ProMQA-Assembly dataset
- Multimedia-returning tools improve performance across multiple models (GPT-5 mini: 59.0→63.7, Gemini 2.5 Flash: 48.2→52.4, Qwen2.5-VL 32B: 39.0→42.1)
- Agentic flexible tool selection outperforms fixed workflows in most cases
- MiMo-VL 7B showed slight degradation (50.9→49.6) with multimedia tools, highlighting model-specific differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimedia-returning tools preserve information that would otherwise be lost in vision-to-text conversion.
- Mechanism: Tools return original modalities (images as images, text as text) rather than semantic grounding via captioning. This allows the VLM agent to perceive raw visual evidence directly, supporting what the paper calls "thinking with images."
- Core assumption: The VLM agent has sufficient visual reasoning capability to interpret returned images; information bottleneck occurs primarily during forced visual-to-text conversion.
- Evidence anchors:
  - [abstract] "making use of multimedia-returning tools... tool calls that return original modalities—images or text—rather than semantic-grounded text"
  - [section 5.1] Ablation shows GPT-5 mini (59.0→63.7), Gemini 2.5 Flash (48.2→52.4), and Qwen2.5-VL 32B (39.0→42.1) improve with multimedia tools over text-only variants
  - [corpus] Related work on multimodal assembly assistants (LEGO Co-builder) similarly emphasizes fine-grained visual reasoning challenges
- Break condition: If a model's visual encoder is weak or if task-relevant information is primarily textual, multimedia returns may not help; MiMo-VL 7B showed slight degradation (50.9→49.6).

### Mechanism 2
- Claim: Interleaved multimodal reasoning enables iterative evidence gathering akin to human procedural verification.
- Mechanism: The agent produces a thought, calls a tool, receives output, appends both to context, and continues. This loop allows progressive refinement—sampling frames, zooming, checking instructions—rather than single-pass prediction.
- Core assumption: The model can maintain coherent reasoning state across multi-turn tool invocations; context window is sufficient for accumulated thoughts and images.
- Evidence anchors:
  - [abstract] "TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools"
  - [section 3.3] "we feed a prompt with a question... generate a tool call with a thought process... append both the model output and the tool output to the previous input"
  - [corpus] ProMQA-Assembly dataset explicitly requires aligning video recordings with instructions—a task well-suited to iterative verification
- Break condition: If the model's reasoning degrades with longer contexts (Sun et al., 2025 citation in paper), or if tool-use prompts are not followed correctly (observed with Qwen2.5-VL 32B and InternVL3 38B).

### Mechanism 3
- Claim: Agentic flexible tool selection outperforms predefined workflows by adapting tool use to question-specific needs.
- Mechanism: Rather than fixing tool execution order (workflow), the agent decides dynamically which tools to call, how many times, and in what sequence. This allows, e.g., multiple zoom_in calls for fine-grained inspection or skipping instruction checks when irrelevant.
- Core assumption: The model has sufficient zero-shot tool-use capability to select appropriate tools without demonstration examples.
- Evidence anchors:
  - [abstract] "ablation studies confirm the benefits of... agentic flexible tool selection over fixed workflows"
  - [section 5.2] All 6 workflow permutations degraded performance vs. TAMA for GPT-5 mini and MiMo-VL 7B; only one Gemini permutation matched TAMA
  - [corpus] Related agent reliability frameworks (e.g., "When Agents Fail to Act") highlight tool invocation reliability as a key concern
- Break condition: If the model lacks robust tool-selection training, it may call tools inefficiently or inappropriately; InternVL3 38B and Qwen2.5-VL 32B showed issues following ReAct-style prompting.

## Foundational Learning

- Concept: **ReAct-style reasoning loops (Thought → Action → Observation → Thought...)**
  - Why needed here: TAMA implements this pattern for multimodal agents; understanding the cycle is essential for debugging agent traces.
  - Quick check question: Can you trace one full loop from the GPT-5 example in Table 3, identifying thought, tool call, and observation?

- Concept: **Vision-Language Model as Agent (not just as tool)**
  - Why needed here: Prior work often uses VLMs as captioning tools for text-only agents; TAMA uses VLMs as the orchestrating agent itself.
  - Quick check question: What is the architectural difference between a text-LLM agent calling a VLM tool vs. a VLM agent calling frame-sampling tools?

- Concept: **Training-free vs. fine-tuned tool use**
  - Why needed here: TAMA operates zero-shot; understanding what capabilities transfer without training informs model selection.
  - Quick check question: Which models in Table 2 benefited from TAMA, and what might explain why others did not?

## Architecture Onboarding

- Component map: VLM Agent -> Tool Layer (4 tools) -> Context Manager -> Execution Loop
- Critical path:
  1. Question received → Agent generates thought + tool call
  2. Tool executed locally → Returns image(s) or text
  3. Output appended to conversation → Fed back to agent
  4. Repeat until answer generated or max turns reached (h=10)
- Design tradeoffs:
  - **Multimedia vs. text tools**: Preserves information but increases token/image costs
  - **Agentic vs. workflow**: Flexible but unpredictable; may require turn limits and cut-in messages
  - **Frame sampling count**: Too few frames risks missing evidence (Gemini 2.5 Flash undersampled); too many risks context overload
- Failure signatures:
  - **Premature answering**: Model outputs answer before sufficient evidence (mitigated by minimum turns i=5)
  - **Tool call without reasoning**: Observed with InternVL3/Qwen2.5-VL—model skips thoughts, calls tools mechanically
  - **Excessive sampling without zooming**: High frame count without targeted inspection suggests poor tool selection
- First 3 experiments:
  1. Replicate TAMA vs. Naive vs. Reasoning on ProMQA-Assembly subset with GPT-5 mini; verify 5%+ improvement margin
  2. Run text-tool ablation (Section 5.1) on one model; confirm multimedia tools help for that model
  3. Trace tool usage patterns for a failing case (e.g., InternVL3); identify if issue is prompt adherence or tool selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific pre-training or post-training methodologies effectively induce interleaved multimodal reasoning capabilities in VLMs?
- Basis in paper: [explicit] The authors state on page 8 that "further investigation would be needed to understand what training contributes to interleaved multimodal reasoning," noting that results for Claude 4 Sonnet suggest textual reasoning strength may not directly correlate with multimodal reasoning capability.
- Why unresolved: The study is training-free, and performance varies unpredictably across models with different architectures and training data, making the source of "thinking with images" capability unclear.
- What evidence would resolve it: Comparative benchmarks of models specifically fine-tuned on interleaved image-text sequences versus those trained on standard captioning tasks.

### Open Question 2
- Question: Does the TAMA framework generalize to procedural domains with higher temporal density or different constraints, such as cooking or biological experiments?
- Basis in paper: [explicit] Page 4 explicitly states, "We leave it to future work to apply our method to ProMQA(-cooking) or other datasets."
- Why unresolved: The experiments are limited to the ProMQA-Assembly dataset, and it is unknown if the current set of multimedia tools (e.g., zoom, frame sampling) is sufficient for tasks with more fluid or rapid motions found in other domains.
- What evidence would resolve it: Evaluation results on datasets like ProMQA-Cooking or FineBio using the TAMA framework.

### Open Question 3
- Question: Can supervised fine-tuning resolve the inability of open-weight models to follow the required ReAct-style prompting for interleaved reasoning?
- Basis in paper: [inferred] Page 8 notes that models like Qwen2.5-VL and InternVL3 "sometimes failed to follow the intended ReAct-style prompting" and "refused to output any reasoning," suggesting they require "additional tuning."
- Why unresolved: The zero-shot prompting approach exposes a behavioral gap in open-source models where they struggle to maintain a chain of thought while calling tools.
- What evidence would resolve it: Performance metrics of open-weight models after fine-tuning on tool-use trajectories that enforce interleaved reasoning steps.

### Open Question 4
- Question: Can the high inference cost and latency of the TAMA framework be reduced through reasoning path distillation without significant performance degradation?
- Basis in paper: [inferred] Page 17 identifies cost and efficiency as a limitation, noting the framework involves "multiple inferences" and suggesting "distillation to smaller models" or "shorter... reasoning paths" as potential solutions.
- Why unresolved: The current agent architecture is inherently expensive compared to single-pass methods, limiting its practical deployment.
- What evidence would resolve it: A study showing that a smaller student model trained on TAMA's reasoning trajectories can achieve comparable accuracy with a single-pass inference.

## Limitations
- Performance gains are model-dependent, with MiMo-VL 7B showing degradation
- Open-weight models struggle with ReAct-style prompting without additional tuning
- Inference cost and latency are high due to multiple tool calls and reasoning steps
- Zero-shot approach may not generalize well to all procedural domains or model architectures

## Confidence
- **High Confidence**: Multimedia-returning tools preserve information vs. text-only (supported by ablation across 3 models showing consistent gains)
- **Medium Confidence**: Agentic flexible tool selection improves performance (supported by most workflows underperforming TAMA, but one Gemini permutation matching suggests edge cases)
- **Low Confidence**: Visual reasoning loop is universally beneficial (model-specific failures like MiMo-VL 7B and tool-use issues in InternVL3 indicate brittleness)

## Next Checks
1. **Replicate Multimedia vs. Text Ablation**: Run the text-tool ablation (Section 5.1) on a second model (e.g., Gemini 2.5 Flash) to confirm multimedia tools consistently improve performance.
2. **Trace a Failing Case**: Select an InternVL3 38B failure from the dataset; log outputs to verify if the issue is prompt adherence (missing thoughts) or poor tool selection.
3. **Judge Prompt Replication**: Implement the LLM-as-a-judge prompt (Figure 12) using a strong judge model (e.g., GPT-4o) and rerun evaluation on a subset to check for score variance.