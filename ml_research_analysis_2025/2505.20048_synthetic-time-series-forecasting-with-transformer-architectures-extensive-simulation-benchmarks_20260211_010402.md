---
ver: rpa2
title: 'Synthetic Time Series Forecasting with Transformer Architectures: Extensive
  Simulation Benchmarks'
arxiv_id: '2505.20048'
source_url: https://arxiv.org/abs/2505.20048
tags:
- patch
- noisy
- rmse
- time
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper conducts an extensive benchmark of three Transformer-based\
  \ time series forecasting architectures: Autoformer, Informer, and PatchTST. Each\
  \ architecture is evaluated across three variants\u2014Minimal, Standard, and Full\u2014\
  to compare their performance under controlled conditions."
---

# Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks

## Quick Facts
- arXiv ID: 2505.20048
- Source URL: https://arxiv.org/abs/2505.20048
- Authors: Ali Forootani; Mohammad Khosravi
- Reference count: 40
- Key outcome: Benchmark of three Transformer architectures (Autoformer, Informer, PatchTST) across synthetic signals with variants and noise; introduces Deep Koopformer for improved forecasting stability

## Executive Summary
This paper conducts an extensive benchmark of three Transformer-based time series forecasting architectures: Autoformer, Informer, and PatchTST. Each architecture is evaluated across three variants—Minimal, Standard, and Full—to compare their performance under controlled conditions. The study covers over 1500 experiments using 10 synthetic signals with varying patch lengths and forecast horizons, both under clean and noisy conditions. Results show that PatchTST Standard consistently achieves the best performance across most scenarios, while Autoformer variants excel in trend-dominated signals, especially under noise. Informer variants, though computationally efficient, are less robust in noisy settings. Additionally, the authors propose Deep Koopformer, a Koopman-enhanced Transformer that integrates operator-theoretic latent state modeling, demonstrating improved stability and forecasting accuracy for nonlinear and chaotic dynamical systems. This hybrid approach advances both interpretability and performance in time series forecasting.

## Method Summary
The study benchmarks Autoformer, Informer, and PatchTST architectures using 10 synthetic time series signals with controlled patch lengths and forecast horizons. Each architecture is tested in three variants: Minimal, Standard, and Full. Experiments include clean and noise-injected conditions to assess robustness. Deep Koopformer is introduced as a hybrid model combining Transformer layers with Koopman operator theory for modeling latent dynamics in nonlinear systems. Over 1500 experiments are conducted, comparing accuracy and computational efficiency across architectures and variants.

## Key Results
- PatchTST Standard achieves the best overall performance across most synthetic scenarios
- Autoformer variants excel in trend-dominated signals, particularly under noisy conditions
- Informer variants offer computational efficiency but reduced robustness in noisy environments
- Deep Koopformer improves forecasting stability and accuracy for nonlinear and chaotic dynamical systems

## Why This Works (Mechanism)
The paper's approach works by leveraging the strengths of Transformer architectures for sequence modeling while introducing structural adaptations (Autoformer's decomposition, Informer's attention efficiency, PatchTST's patch-based processing) and integrating Koopman operator theory to capture latent dynamics in nonlinear systems. The extensive synthetic benchmarks allow systematic isolation of architectural effects across signal types and noise conditions.

## Foundational Learning
- **Transformer architectures**: Attention-based models for sequence modeling; needed for capturing long-range dependencies in time series
- **Koopman operator theory**: Framework for analyzing nonlinear dynamical systems via linear operators in observable space; enables modeling of latent dynamics
- **Synthetic signal generation**: Controlled signal synthesis for benchmarking; allows isolation of architectural performance under known conditions
- **Noise injection mechanisms**: Systematic addition of perturbations to assess model robustness; critical for evaluating real-world applicability
- **Patch-based processing**: Division of sequences into fixed-length segments; improves computational efficiency and locality handling
- **Attention mechanisms**: Scaled dot-product attention with causal masking; enables selective focus on relevant historical data

## Architecture Onboarding

**Component map**: Input -> Patch Segmentation -> Transformer Encoder -> Attention/Decomposition -> Forecasting Head -> Output

**Critical path**: Signal → Patch Segmentation → Encoder Layers → Attention/Decomposition → Forecast Output

**Design tradeoffs**: Autoformer balances decomposition accuracy with computational cost; Informer prioritizes efficiency over robustness; PatchTST optimizes for patch-based locality; Deep Koopformer trades additional complexity for improved stability

**Failure signatures**: Informer degrades significantly under noise; Autoformer struggles with non-stationary signals; PatchTST may miss long-range dependencies; Deep Koopformer requires careful initialization

**First experiments**: 1) Benchmark Autoformer Minimal on clean trend signal; 2) Test Informer Full under maximum noise; 3) Evaluate PatchTST Standard with varying patch lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend heavily on synthetic signal generation, which may not capture real-world distributional properties
- Synthetic benchmarks cover 10 signal types, but generalizability to production datasets remains uncertain
- Noise injection mechanism is described but not quantified in terms of realistic signal-to-noise ratios or domain-specific perturbation types

## Confidence
- Performance rankings across architectures: **Medium** - well-controlled synthetic benchmarks, but real-world transfer validity unknown
- Deep Koopformer improvements: **Low-Medium** - novel hybrid method with limited external validation
- Noise robustness conclusions: **Medium** - systematic noise testing, but noise models may not reflect actual data corruption patterns

## Next Checks
1. Replicate key findings on at least two real-world benchmark datasets with known ground truth (e.g., electricity load, traffic flow)
2. Conduct ablation studies isolating the contribution of Koopman operator layers in Deep Koopformer
3. Test the architectures on out-of-distribution signal types not included in the original 10 synthetic categories