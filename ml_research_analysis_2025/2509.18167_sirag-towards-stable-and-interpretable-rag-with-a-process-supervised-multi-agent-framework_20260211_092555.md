---
ver: rpa2
title: 'SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent
  Framework'
arxiv_id: '2509.18167'
source_url: https://arxiv.org/abs/2509.18167
tags:
- generator
- retriever
- agent
- each
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of suboptimal coordination between
  retrievers and generators in Retrieval-Augmented Generation (RAG) systems, where
  retrievers often return irrelevant or redundant documents, and generators fail to
  fully leverage retrieved evidence. To bridge this gap, the authors propose a process-supervised
  multi-agent framework called SIRAG, which introduces two lightweight agents: a Decision
  Maker that determines when to continue retrieval or stop for answer generation,
  and a Knowledge Selector that filters retrieved documents to retain only the most
  useful evidence.'
---

# SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework

## Quick Facts
- **arXiv ID**: 2509.18167
- **Source URL**: https://arxiv.org/abs/2509.18167
- **Reference count**: 24
- **Primary result**: SIRAG achieves 48.23% average accuracy across 4 datasets by introducing lightweight agents to coordinate retriever-generator interactions

## Executive Summary
SIRAG addresses the misalignment between retrievers and generators in RAG systems by introducing a process-supervised multi-agent framework. The approach uses two lightweight agents - a Decision Maker that determines when to retrieve or stop for generation, and a Knowledge Selector that filters retrieved documents - trained jointly with reinforcement learning and process-level supervision from an LLM-as-a-Judge. The framework demonstrates higher accuracy, more stable convergence, and interpretable reasoning trajectories compared to standard RAG baselines on both single-hop and multi-hop question answering benchmarks.

## Method Summary
SIRAG introduces two lightweight agents (Decision Maker and Knowledge Selector) to coordinate retriever-generator interactions in RAG systems. The framework employs tree-structured rollout with forced exploration at the root, where an LLM-as-a-Judge provides process-level rewards for each intermediate action. Both agents are trained jointly using PPO with centralized value function, combining system rewards (final answer correctness) with process rewards (intermediate action quality). The approach requires no modification to the retriever or generator, making it a plug-and-play solution.

## Key Results
- Achieves 48.23% average accuracy across 4 datasets (NQ, PopQA, 2WikiMultiHopQA, HotpotQA)
- Shows 9.2% improvement on multi-hop HQA benchmark and 1.8% on single-hop PopQA
- Demonstrates more stable convergence during training compared to standard RAG baselines
- Provides interpretable reasoning trajectories through agent decision logging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lightweight intermediary agents improve retriever-generator coordination without modifying either component.
- **Mechanism**: Decision Maker chooses between Retrieve or Stop&Generate based on question state and evidence, while Knowledge Selector filters retrieved documents to retain only relevant evidence. This creates an adaptive control loop where agents learn to optimize retrieval depth and filtering based on query complexity.
- **Core assumption**: The retriever and generator are fundamentally misaligned due to independent development, and a learned intermediate policy can bridge this gap more effectively than static heuristics.
- **Evidence anchors**: [abstract] "the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence" [section II.B] "Each agent operate in a partially observable environment and interacts sequentially, forming an action trajectory"
- **Break condition**: If retriever quality is already high for the target domain, or if generator is already robust to noisy context, the added agent overhead may not justify marginal gains.

### Mechanism 2
- **Claim**: Process-level supervision via LLM-as-a-Judge provides denser training signal than outcome-only rewards, improving credit assignment across multi-step reasoning.
- **Mechanism**: For each intermediate action, a strong LLM assigns a score R_process ∈ [0,1] evaluating action reasonableness. Final credit combines: R_credit = α · R(τ) + β · R_process, where R(τ) is the sparse system-level reward.
- **Core assumption**: A strong LLM can reliably evaluate intermediate reasoning quality, and its judgments correlate with downstream task success.
- **Evidence anchors**: [abstract] "evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness" [section III.D] "removing the tree-structured rollout (and LLM-as-a-judge credit assignment) leads to unstable performance during the RL phase"
- **Break condition**: If the judge LLM's preferences diverge from task-specific success criteria, or if judge evaluations are inconsistent across similar actions, the process rewards may introduce noise rather than signal.

### Mechanism 3
- **Claim**: Tree-structured rollout with PPO enables effective exploration of reasoning paths and stable end-to-end multi-agent training.
- **Mechanism**: The framework forces exploration of multiple strategies at the tree root while expanding deeper levels stochastically. Each path becomes a training trajectory. PPO optimizes both agent policies jointly using the credit rewards, with a centralized value function V_φ that estimates returns across agents.
- **Core assumption**: Diverse trajectory exploration during training yields policies that generalize to test-time queries, and PPO's stability properties transfer to this multi-agent setting.
- **Evidence anchors**: [section II.C] "For each input question, the Decision Maker is forced to explore multiple reasoning strategies at the top level... This results in a decision tree where each path corresponds to a possible reasoning trajectory"
- **Break condition**: If rollout depth or breadth is insufficient for the task's reasoning complexity, or if PPO hyperparameters are misconfigured, training may converge to suboptimal policies.

## Foundational Learning

- **Concept: Reinforcement Learning with Sparse Rewards**
  - Why needed here: The system reward R(τ) is only observed at trajectory end (answer correctness). Understanding why sparse rewards cause credit assignment problems—and how reward shaping (process supervision) addresses this—is essential to grasp why SIRAG's LLM-as-a-Judge matters.
  - Quick check question: If a 5-step retrieval trajectory produces a correct answer but step 3 was actually detrimental, what would pure outcome-based reward do vs. process supervision?

- **Concept: Multi-Agent Policy Gradient Methods**
  - Why needed here: SIRAG trains two agents (DM, KS) jointly. The centralized value function and per-agent policy optimization require understanding how credit flows across agents and why independent optimization may fail.
  - Quick check question: Why does SIRAG use a centralized V_φ rather than separate value functions per agent? What would happen if agents were trained independently?

- **Concept: PPO and the Clipping Objective**
  - Why needed here: The paper uses PPO's clipped surrogate objective (Equation 6) to ensure stable updates. Understanding why clipping prevents destructive policy updates helps explain SIRAG's claimed stable convergence.
  - Quick check question: What does ε control in the PPO clipping term, and what symptom would appear if ε were set too large?

## Architecture Onboarding

- **Component map**: Question q → [Decision Maker (0.5B params)] → (action: Retrieve or Stop&Generate) → Retrieve → Query → [Retriever R(q)] → Retrieved docs → [Knowledge Selector (0.5B params)] → Filtered docs → Evidence pool → [Generator G] → Answer

- **Critical path**:
  1. Warm-up phase: Collect initial trajectories using Qwen2.5-7B-Instruct to bootstrap agent policies via supervised learning
  2. Rollout collection: For each question, expand tree with forced exploration at root; record all (state, action, reward) tuples
  3. Process supervision: Query LLM-as-a-Judge for each intermediate action to obtain R_process
  4. Credit assignment: Compute R_credit = α·R(τ) + β·R_process for each node
  5. PPO update: Optimize DM and KS policies using clipped objective with centralized value function

- **Design tradeoffs**:
  - Agent size (0.5B) vs. capability: Smaller agents enable edge deployment but may lack reasoning capacity for complex queries
  - Rollout depth/breadth vs. compute: Deeper trees explore more strategies but increase training cost
  - α/β weighting: The trade-off between system reward and process reward is critical
  - Judge LLM choice: Using a stronger judge improves signal quality but increases cost

- **Failure signatures**:
  - Runaway retrieval: DM never chooses Stop&Generate, exhausts retrieval budget
  - Empty evidence passed to generator: KS filters too aggressively
  - Training instability: Loss oscillates or diverges
  - No improvement over baseline: Agents learn trivial policies

- **First 3 experiments**:
  1. Reproduce warm-up → RL transition: Train supervised warm-up only, then compare to full RL training on a held-out validation set (50 questions from HotpotQA)
  2. Ablate process supervision: Compare full SIRAG vs. "w/o LLM judge" variant on multi-hop questions
  3. Probe agent behavior: Log DM action distribution and KS retention rates across single-hop vs. multi-hop questions

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SIRAG's performance scale when trained on full training sets rather than only 100 randomly sampled questions per dataset?
  - Basis in paper: [explicit] "For each dataset, we only use 100 randomly sampled questions instead of the full training set."
  - Why unresolved: Performance on full datasets could reveal training instabilities, overfitting, or computational bottlenecks not visible in the sampled setting.
  - What evidence would resolve it: Experiments comparing SIRAG trained on full HotpotQA (~90K examples) against the sampled approach, measuring convergence time and final accuracy.

- **Open Question 2**: To what extent does LLM-as-a-Judge introduce biases or systematic errors in credit assignment that could misguide agent training?
  - Basis in paper: [inferred] The paper relies on LLM judges to provide process-level rewards but does not analyze cases where judge evaluations disagree with ground-truth action quality.
  - Why unresolved: If the judge systematically undervalues certain correct actions, the trained agents may learn suboptimal policies that are difficult to detect through final accuracy alone.
  - What evidence would resolve it: A comparative analysis of judge scores versus human annotations on intermediate actions, plus ablation studies using different judge models.

- **Open Question 3**: What is the computational overhead of the tree-structured rollout strategy compared to single-path exploration, and how does it scale with tree depth?
  - Basis in paper: [inferred] The paper mentions "tree-structured rollout strategy to explore diverse reasoning paths" but provides no analysis of inference time or token costs relative to baseline RAG systems.
  - Why unresolved: Multi-path exploration inherently multiplies retrieval and generation calls; practical deployment requires understanding whether accuracy gains justify increased latency and API costs.
  - What evidence would resolve it: Detailed profiling of inference time, number of retrievals, and LLM calls per question across different tree depths.

## Limitations
- Results show diminishing returns on single-hop tasks (+1.8% on PopQA), suggesting the approach may not universally improve simpler RAG systems
- Computational overhead of tree-structured rollout increases inference time and LLM call costs, with no detailed efficiency analysis provided
- The reliability of LLM-as-a-Judge evaluations and their correlation with task success remains unverified through calibration studies

## Confidence
- **High confidence**: The core mechanism of using lightweight agents to coordinate retriever-generator interaction is technically sound and well-specified
- **Medium confidence**: The process supervision approach via LLM-as-a-Judge likely improves credit assignment, but the reliability of judge evaluations and their correlation with task success remains unverified
- **Low confidence**: The scalability claims (edge deployment, plug-and-play modularity) lack empirical support for different retriever/generator combinations and real-world deployment scenarios

## Next Checks
1. **Judge reliability test**: Run cross-validation where multiple LLM-as-a-Judge instances score the same trajectories. Calculate inter-judge agreement and correlation with final task success to quantify process reward noise.
2. **Agent capability scaling**: Re-run experiments with larger agent models (1B, 3B parameters) on multi-hop tasks to measure performance gains and determine if 0.5B is a bottleneck for complex reasoning.
3. **Deployment robustness**: Evaluate SIRAG with different retriever types (sparse vs. dense) and generator sizes (3B, 7B) to test true plug-and-play modularity and identify compatibility constraints.