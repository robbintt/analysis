---
ver: rpa2
title: 'Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case
  Study on ABB Circuit Breakers'
arxiv_id: '2505.17520'
source_url: https://arxiv.org/abs/2505.17520
tags:
- chunking
- retrieval
- context
- page
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Retrieval-Augmented Generation (RAG) systems
  for ABB circuit breaker documentation, addressing challenges in accuracy and contextual
  relevance for engineering applications. Three RAG pipelines (OpenAI GPT4o, Cohere,
  Anthropic Claude) were tested using domain-specific datasets and three chunking
  methods (basic, paragraph-per-page, by-title).
---

# Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers

## Quick Facts
- arXiv ID: 2505.17520
- Source URL: https://arxiv.org/abs/2505.17520
- Authors: Salahuddin Alawadhi; Noorhan Abbas
- Reference count: 16
- Primary result: Claude RAG pipeline with paragraph-per-page chunking achieved highest faithfulness (0.8556) for ABB circuit breaker documentation

## Executive Summary
This study evaluates Retrieval-Augmented Generation (RAG) systems for ABB circuit breaker documentation, addressing challenges in accuracy and contextual relevance for engineering applications. Three RAG pipelines (OpenAI GPT4o, Cohere, Anthropic Claude) were tested using domain-specific datasets and three chunking methods (basic, paragraph-per-page, by-title). RAGAS metrics showed variability in performance, with Claude's paragraph-per-page chunking achieving the highest faithfulness (0.8556), while Cohere's paragraph-per-page method excelled in context recall (0.7705). Despite achieving high precision in some configurations, the systems exhibited limitations in factual faithfulness and completeness, particularly in retrieving critical technical details. The study underscores the need for iterative improvements in RAG systems, such as hybrid chunking, enhanced retrieval mechanisms, and knowledge graph integration, to meet the stringent demands of electrical engineering tasks.

## Method Summary
The study tested three RAG pipelines across three chunking strategies using 12 ABB Emax E1.2 circuit breaker PDFs (537,708 tokens) and 31 domain-specific QA pairs. Chunking methods included basic fixed-window, paragraph-per-page boundary enforcement, and by-title semantic grouping. Three embedding models were used: OpenAI ada-002, Cohere embed-english-v3.0, and Voyage voyage-3. RAGAS metrics (faithfulness, context recall/precision, answer relevancy) evaluated performance at temperature=0 with top-k=10 retrieval. The methodology specifically targeted technical accuracy for circuit breaker specifications, dip-switch configurations, and fault settings.

## Key Results
- Claude RAG with paragraph-per-page chunking achieved highest faithfulness (0.8556)
- Cohere with paragraph-per-page chunking excelled in context recall (0.7705)
- GPT4o with by-title chunking achieved highest answer relevancy (0.8021) and context precision (1.0000)
- All configurations struggled with specific technical details like dip-switch positions and resistor values
- Temperature=0 setting prevented inconsistent responses but couldn't fix retrieval gaps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing page boundaries during chunking (Paragraph-per-Page) appears to improve factual consistency (faithfulness) in technical documents containing mixed text and diagrams.
- **Mechanism:** By preventing chunks from splitting across pages, the method likely preserves the spatial relationship between a technical diagram (e.g., a circuit schematic) and its explanatory text. This reduces the "semantic fragmentation" that occurs when related visual and textual data are separated during vectorization.
- **Core assumption:** The layout of the source PDF (ABB documentation) aligns semantic units (paragraphs/diagrams) within single pages rather than spanning them.
- **Evidence anchors:**
  - [abstract] Mentions "paragraph-per-page chunking achieving the highest faithfulness (0.8556)."
  - [section] Page 5 notes this method enforces boundaries via `multipage_sections=False` to balance chunk size and segmentation.
  - [corpus] Weak direct support in corpus neighbors; however, *MuaLLM* suggests hybrid contextual RAG is vital for circuit design, implying structure matters.
- **Break condition:** If documents have long tables or schematics spanning multiple physical pages, this chunking method may sever critical context, reducing recall.

### Mechanism 2
- **Claim:** Increasing chunk size based on semantic structure (By-Title) improves answer relevancy and context precision, but may reduce context recall compared to paragraph-based methods.
- **Mechanism:** "By-Title" chunking aggregates text under logical headings (up to 4000 chars). This provides the LLM with a broader, topic-complete context window during generation, allowing it to synthesize answers from a comprehensive section rather than fragmented snippets.
- **Core assumption:** The user query aligns well with the document's structural hierarchy (titles/headers).
- **Evidence anchors:**
  - [section] Table 3 shows GPT4o with "By Title" chunking achieved the highest Answer Relevancy (0.8021) and Context Precision (1.0000).
  - [section] Page 5 describes `max_characters=4000` for By-Title vs `1500` for Paragraph-per-Page.
  - [corpus] *An Analysis of Hyper-Parameter Optimization Methods for RAG* (corpus neighbor) confirms that chunk size is a critical hyper-parameter impacting performance.
- **Break condition:** If a section under a title is massive, the chunk may exceed the embedding model's ability to capture fine-grained details, or the retrieval step may retrieve a broad section where the specific answer is buried.

### Mechanism 3
- **Claim:** Deterministic generation (Temperature = 0) is required for reproducibility in engineering contexts, but does not guarantee factual correctness (faithfulness) if retrieval fails.
- **Mechanism:** Setting temperature to 0 forces the model to always select the highest probability token. This eliminates variance in responses to the same query, which is critical for safety validation. However, if the retrieved context lacks the answer, the model will still consistently generate a plausible but potentially incorrect response (hallucination) or an "I don't know" refusal.
- **Core assumption:** The embedding retrieval step successfully fetches the relevant context.
- **Evidence anchors:**
  - [section] Page 7 states Temperature 0 was used "to prevent the dissemination of inconsistent or erroneous information."
  - [section] Page 13-14 shows that despite optimized settings, models failed to identify dip-switch positions or returned "I don't know" (GPT4o By-Title), proving generation settings cannot fix retrieval gaps.
  - [corpus] *Federated RAG* (corpus neighbor) highlights the broader challenge of factual grounding and consistency in distributed systems.
- **Break condition:** If the retrieval system fails to fetch the necessary context, a temperature of 0 results in consistent "unawareness" or consistent, confident errors, rather than creative exploration that might accidentally find the right path (though creative exploration is generally undesirable in engineering).

## Foundational Learning

- **Concept:** **RAGAS Metrics (Faithfulness vs. Context Recall)**
  - **Why needed here:** The paper relies on these metrics to determine the "best" pipeline. Faithfulness measures if the answer is supported by retrieved context (critical for safety), while Context Recall measures if all relevant information was retrieved (critical for completeness).
  - **Quick check question:** If a system has 100% Faithfulness but 10% Context Recall, is it safe for troubleshooting? (Answer: No, it might be factually correct regarding the small slice it sees, but it misses critical warnings or steps).

- **Concept:** **Semantic Chunking vs. Fixed-Window Chunking**
  - **Why needed here:** The study explicitly compares "Basic" (fixed/simple) against "By-Title" and "Paragraph-per-Page" (semantic/structural). Understanding this distinction is key to optimizing the pipeline.
  - **Quick check question:** Why might a "Basic" chunker split a crucial warning label away from the warning text? (Answer: Because it cuts strictly on character count, ignoring semantic or layout boundaries).

- **Concept:** **Embedding Models & Vector Stores**
  - **Why needed here:** The study tests different combinations (OpenAI+ada-002 vs. Claude+Voyage). Performance depends on how well the embedding captures domain-specific nuance.
  - **Quick check question:** Why can't you use a generic English embedding model for highly specialized electrical engineering schematics without performance loss? (Answer: Generic models may not map technical jargon or relationships—like "Ekip DIP LSI/LSIG"—effectively into the vector space).

## Architecture Onboarding

- **Component map:** PDF Parsing -> Chunking (Basic / Para-per-Page / By-Title) -> Chroma Vector Store -> Embedding (Cohere/Claude/OpenAI) -> Cosine similarity ranking (Top K=10) -> LLM (GPT4o / Cohere / Claude) -> RAGAS Evaluation

- **Critical path:**
  1. **PDF Parsing:** Successfully extracting text and potentially table boundaries is the first failure point.
  2. **Chunking Strategy:** Selecting the method that preserves technical context (e.g., Claude + Paragraph-per-Page).
  3. **Context Window:** Ensuring the retrieved chunks (up to 4000 tokens) fit the model's input limit without truncating the answer.

- **Design tradeoffs:**
  - **Claude (High Faithfulness) vs. Cohere (High Recall):** Claude is safer (less likely to hallucinate), while Cohere retrieves more data (better for broad research).
  - **Chunk Size:** Large chunks (By-Title) increase relevancy/precision but may lower recall (needle in a haystack); Small chunks increase recall granularity but may lose context.

- **Failure signatures:**
  - **"I don't know" responses:** Occurred in GPT4o "By-Title" (Q3). Indicates retrieval failure or context window ambiguity.
  - **Specific Numeric Hallucinations:** E.g., stating "390 Ω" instead of "120 Ω". Indicates the model is guessing based on similar but irrelevant retrieved context.
  - **Incomplete Procedures:** Missing specific dip-switch positions. Indicates the chunk contained the "what" but not the "how."

- **First 3 experiments:**
  1. **Chunking A/B Test:** Run the same 10 questions through Claude using "Basic" vs. "Paragraph-per-Page" chunking on a single manual to quantify the Faithfulness gap.
  2. **Retrieval Inspection:** For a specific failure case (e.g., the dip-switch question), manually inspect the Top 10 retrieved chunks to see if the data exists. If it exists but wasn't used, the prompt or context window is the issue. If it doesn't exist, the embedding is the issue.
  3. **Hybrid Strategy Prototype:** Implement a "By-Title + Summary" approach where large sections are summarized to see if it improves Context Recall without sacrificing Faithfulness.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation dataset (31 QA pairs) may not capture full technical complexity
- Proprietary ABB documentation restricts reproducibility and generalizability
- Absence of multimodal grounding (text+diagrams) for schematic interpretation
- No analysis of real-time performance constraints or deployment costs

## Confidence
- **High confidence:** The comparative performance rankings across chunking strategies and LLMs are well-supported by the experimental data
- **Medium confidence:** The specific mechanism explanations (e.g., why Paragraph-per-Page improves faithfulness) are plausible but not definitively proven
- **Low confidence:** The generalizability of these findings to other technical domains or document types beyond ABB circuit breaker manuals

## Next Checks
1. Test the "Claude + Paragraph-per-Page" configuration on a different technical domain (e.g., HVAC systems or automotive engineering) to validate cross-domain performance
2. Implement a multimodal RAG pipeline that can process both text and diagrams from the same PDFs to measure performance improvements for schematic-related queries
3. Conduct a longitudinal study measuring the performance degradation over time as the documentation corpus grows, particularly focusing on context window limitations