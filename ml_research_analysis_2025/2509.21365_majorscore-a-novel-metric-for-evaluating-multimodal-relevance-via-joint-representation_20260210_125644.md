---
ver: rpa2
title: 'MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation'
arxiv_id: '2509.21365'
source_url: https://arxiv.org/abs/2509.21365
tags:
- multimodal
- majorscore
- data
- similarity
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MAJORScore, a novel evaluation metric for
  multimodal relevance via joint representation learning. It addresses the limitation
  of existing metrics that only assess two modalities by proposing a unified latent
  space to encode multiple modalities.
---

# MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation

## Quick Facts
- arXiv ID: 2509.21365
- Source URL: https://arxiv.org/abs/2509.21365
- Reference count: 40
- The paper introduces MAJORScore, a novel evaluation metric for multimodal relevance via joint representation learning that increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistent modality compared to baseline methods.

## Executive Summary
This paper introduces MAJORScore, a novel evaluation metric for multimodal relevance that addresses limitations of existing metrics which only assess two modalities. The method proposes a unified latent space to encode multiple modalities (video, text, and audio) using the C-MCR model, enabling more balanced and interpretable similarity computations. Experiments on VGGSound and VITAS datasets demonstrate MAJORScore's effectiveness in distinguishing consistent from inconsistent multimodal content, showing significant improvements over baseline methods like CLIPScore and CLAPScore.

## Method Summary
MAJORScore computes multimodal relevance by encoding video frames (averaged from 1 fps sampling), text, and audio into a unified latent space using C-MCR, then calculating cosine similarities between video-text and text-audio pairs. These bimodal similarities are aggregated using sum, product, or average functions to produce the final score. The method also introduces FairScore to measure similarity balance across modality pairs. The approach is evaluated on VGGSound and VITAS datasets, comparing consistent versus inconsistent multimodal samples.

## Key Results
- MAJORScore increases by 26.03%-64.29% for consistent modality samples compared to baseline methods
- MAJORScore decreases by 13.28%-20.54% for inconsistent modality samples compared to baseline methods
- C-MCR shows better similarity fairness with lower Mean Diff (0.0933 vs 0.0869) and Cohen's d (0.4684 vs 0.6959) for consistent cases

## Why This Works (Mechanism)

### Mechanism 1
A unified latent space reduces cross-modal scoring imbalance compared to naively combining separate bimodal models. MAJORScore uses C-MCR to bind CLIP and CLAP embedding spaces into a shared latent space, enabling cosine similarity computation on a common scale rather than comparing scores from incomparable embedding distributions. The textual connection mechanism in C-MCR successfully aligns visual and audio representations semantically, not just geometrically.

### Mechanism 2
Aggregating bimodal similarity scores (product, sum, average) provides robust tri-modal relevance estimation. MAJORScore computes separate cosine similarities for video-text and text-audio pairs, then combines them via f(CMCR_vt, CMCR_ta) where f ∈ {sum, product, average}, allowing the metric to capture overall consistency while remaining interpretable. Tri-modal relevance decomposes into bimodal relationships; higher-order interactions (video-audio direct relationship) are captured indirectly through text as intermediary.

### Mechanism 3
Similarity fairness (balanced scores across modality pairs) indicates more reliable multimodal consistency. FairScore measures imbalance: the sum of absolute differences between all C(M,2) bimodal similarity pairs, normalized. Lower FairScore indicates more balanced cross-modal relationships. Well-aligned multimodal content should exhibit similar consistency levels across all modality pairs.

## Foundational Learning

- **Concept: Contrastive Learning (CLIP, CLAP)** - Why needed here: The paper builds on CLIP and CLAP as pretrained encoders; understanding how contrastive pretraining creates aligned embedding spaces is prerequisite to understanding why naive combination fails. Quick check question: Given an image and caption, how would CLIP compute their similarity score?

- **Concept: Joint Representation Learning** - Why needed here: C-MCR's core contribution is creating a unified latent space; understanding joint representation goals (shared semantics across modalities) is essential. Quick check question: What is the difference between concatenating features from separate encoders vs. learning a joint embedding space?

- **Concept: Cosine Similarity in High-Dimensional Spaces** - Why needed here: All MAJORScore computations rely on cosine similarity; understanding its properties (scale-invariance, range [-1,1]) is necessary for interpreting results. Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing embeddings from different modalities?

## Architecture Onboarding

- **Component map:** Video (1 fps frames) -> CLIP encoder -> averaged frame embeddings; Text -> C-MCR text encoder; Audio -> C-MCR audio encoder; All encoders -> unified latent space; Cosine similarity calculator (V-T and T-A pairs) -> Aggregation layer (sum/product/average + FairScore calculator) -> Output MAJORScore

- **Critical path:** Frame extraction quality from video (1 fps sampling may miss temporal details); C-MCR encoder alignment quality (inherited from pretrained model, no fine-tuning); Text quality as intermediary (ambiguous text propagates errors to both similarity computations)

- **Design tradeoffs:** Using C-MCR vs. ImageBind/Meta-Transformer: C-MCR leverages existing CLIP/CLAP but is limited to 3 modalities; alternatives support more modalities but may require more compute; Product vs. sum vs. average aggregation: Product penalizes any low similarity more heavily; sum/average are more forgiving; Assumption: The paper does not compare aggregation methods empirically—choice is task-dependent

- **Failure signatures:** High variance between CMCR_vt and CMCR_ta for supposedly consistent samples (indicates encoder misalignment); MAJORScore fails to distinguish consistent from inconsistent samples (check FairScore distribution); Negative MAJORScore on clearly aligned content (possible encoder failure or preprocessing issue)

- **First 3 experiments:** 1) Reproduce Table I analysis: Compare CLIP, CLAP, and C-MCR scores on 10-20 manually verified samples to confirm embedding space alignment; 2) Consistency discrimination test: Sample 100 triplets from VGGSound (50 consistent, 50 with shuffled text), compute MAJORScore and baseline; verify separation improvement matches claimed 26-64% range; 3) FairScore validation: On VITAS generated samples, correlate FairScore with human judgment of multimodal coherence

## Open Questions the Paper Calls Out

### Open Question 1
Can MAJORScore effectively scale to evaluate multimodal consistency across four or more modalities using emerging joint representation models? The current experimental validation is restricted to video, text, and audio (N=3); it is unclear if the similarity aggregation methods remain stable or effective as the number of modality pairs increases combinatorially.

### Open Question 2
Is there an adaptive aggregation function that outperforms the static sum, product, and average methods for combining bimodal similarity scores? The paper proposes three distinct aggregation strategies but does not identify a single optimal method or explore dynamic weighting based on modality characteristics.

### Open Question 3
How robust is MAJORScore when the intermediate text modality is noisy, low-quality, or missing? The metric's reliance on text as the central connecting node creates a potential single point of failure; if the text is ambiguous or mislabeled, it may disproportionately penalize the overall relevance score even if video and audio are well-aligned.

## Limitations
- C-MCR model availability and exact checkpoint details are unspecified, making direct replication difficult without external model access
- Limited ablation studies on aggregation function choice (sum vs product vs average) and FairScore's relationship to human judgment
- The methodology relies on text as an intermediary modality, creating potential single point of failure when text quality is poor

## Confidence
- **High Confidence:** Mechanism 1 (unified latent space reduces cross-modal scoring imbalance) - supported by quantitative evidence showing CLIP/CLAP score differences of 0.73 for identical samples
- **Medium Confidence:** Mechanism 2 (bimodal aggregation captures tri-modal relevance) - supported by 26.03%-64.29% improvement metrics, but lacks comparison of aggregation strategies
- **Low Confidence:** Mechanism 3 (similarity fairness as reliability indicator) - no direct corpus support or human validation studies for FairScore effectiveness

## Next Checks
1. Verify C-MCR embedding alignment by computing score differences between CLIP, CLAP, and C-MCR on 10-20 manually verified samples
2. Test MAJORScore's discrimination ability by creating controlled consistent vs inconsistent triplets from VGGSound and measuring separation improvement
3. Validate FairScore's correlation with human judgment by having annotators rate multimodal coherence on VITAS samples and comparing against computed FairScore values