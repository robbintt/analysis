---
ver: rpa2
title: 'ParaScopes: What do Language Models Activations Encode About Future Text?'
arxiv_id: '2511.00180'
source_url: https://arxiv.org/abs/2511.00180
tags:
- arxiv
- outline
- text
- parascope
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ParaScopes, a framework for decoding paragraph-
  and document-scale planning information from language model activations. The authors
  propose Residual Stream Decoders as a method to extract future content from current
  model activations, testing the hypothesis that language models encode forward-looking
  information.
---

# ParaScopes: What do Language Models Activations Encode About Future Text?

## Quick Facts
- **arXiv ID**: 2511.00180
- **Source URL**: https://arxiv.org/abs/2511.00180
- **Reference count**: 40
- **Primary result**: Introduces Residual Stream Decoders that extract paragraph-scale planning information from language model activations, achieving performance comparable to baselines with 5+ tokens of future context.

## Executive Summary
This paper introduces ParaScopes, a framework for decoding paragraph- and document-scale planning information from language model activations. The authors propose Residual Stream Decoders as a method to extract future content from current model activations, testing the hypothesis that language models encode forward-looking information. They develop two main approaches: Continuation ParaScope, which generates text from modified activations, and TAE ParaScope, which maps activations to structured embeddings. Experiments with Llama 3.2 3B show that both methods successfully decode information equivalent to 5+ tokens of future context, with TAE ParaScope better preserving general subject matter and Continuation ParaScope better capturing specific details.

## Method Summary
ParaScopes extracts information about upcoming text from current language model activations at paragraph boundaries. The framework uses two complementary approaches: Continuation ParaScope extracts residual stream activations at "\n\n" tokens and injects them into a blank context to generate future paragraphs, while TAE ParaScope trains a linear map from normalized residual stream differences to SONAR embeddings, then decodes to text. Both methods are evaluated against baselines using Qwen 3 embeddings, BLEURT-20, and LLM-as-judge rubrics for subject and detail matching.

## Key Results
- Both Continuation and TAE ParaScope methods achieve cosine similarity scores comparable to cheat-5 baseline (0.50), with TAE reaching ~0.55
- TAE ParaScope shows better subject matter preservation (76% related domain matches) while Continuation better captures specific details (15% achieve moderate depth vs. 3% for TAE)
- Layer-wise analysis reveals planning signals concentrate in middle layers (60-80% depth), with cosine similarity deltas of 0.15-0.25
- Token-wise analysis shows sharp planning signal emergence at paragraph transition tokens, jumping from 0.12 to 0.48 topic similarity
- Outline-level planning shows weak performance, near random for Llama 3.2 3B

## Why This Works (Mechanism)

### Mechanism 1: Residual Stream Encoding of Future Context
The residual stream at paragraph boundaries encodes decodable information about upcoming content equivalent to ~5 tokens of forward visibility. The transformer's residual stream accumulates representations across layers, and at transition tokens ("\n\n"), the model integrates context in a way that makes future paragraph content linearly decodable from middle-layer activations.

### Mechanism 2: Layer-wise Specialization for Planning
Planning-relevant signals concentrate in middle layers (60-80% of model depth). Causal scrubbing experiments show that swapping activations at different depths produces predictable changes, with middle-layer swaps having the largest effect on future paragraph content.

### Mechanism 3: Just-in-Time Plan Formation at Transition Tokens
The model primarily forms or refines paragraph-level plans at the "\n\n" transition token, with limited evidence of extended look-ahead. Token-wise analysis shows a sharp shift in residual stream content at paragraph boundaries, suggesting myopic planning behavior.

## Foundational Learning

- **Concept**: Residual Stream in Transformers
  - Why needed here: The entire ParaScope framework operates on residual stream activations; understanding how these accumulate across layers is essential for interpreting layer-wise results.
  - Quick check question: Can you explain why the residual stream at layer L contains information from all previous layers?

- **Concept**: Linear Probing and Decodability
  - Why needed here: TAE ParaScope relies on linear maps from activations to embeddings; understanding what linear probes can/cannot extract determines interpretability claims.
  - Quick check question: What are the limitations of linear probes for detecting model representations?

- **Concept**: Text Auto-Encoders (SONAR)
  - Why needed here: TAE ParaScope maps residual stream activations to SONAR embedding space; understanding what SONAR encodes helps interpret decoded outputs.
  - Quick check question: What information might a text auto-encoder preserve vs. lose compared to the original text?

## Architecture Onboarding

- **Component map**: Residual stream extraction at "\n\n" -> Continuation injection into blank context OR TAE linear mapping to SONAR embeddings -> Text generation/decoding -> Evaluation via embeddings and LLM judgment

- **Critical path**: 1) Dataset generation: FineWeb-Edu -> Gemma 2 27B prompts -> Llama 3.2 3B generations -> paragraph splits at "\n\n" 2) Activation extraction at "\n\n" tokens 3) Train TAE linear map on 100K samples 4) Generate and evaluate against baselines

- **Design tradeoffs**: Continuation vs. TAE: Continuation requires no training, preserves more detail but inconsistent; TAE more stable for subject matching but loses specifics. Linear vs. MLP probes: MLP highly correlated with linear (τ=0.82) with no significant improvement. Evaluation metrics: Cosine/BLEURT provide automated scoring; LLM-as-judge adds nuance but introduces model dependency.

- **Failure signatures**: Low cosine similarity (<0.25) indicates poor probe training or insufficient planning signal. High variance in Continuation outputs is expected behavior. TAE outputs matching subject but missing details reflects linear map limitations. Outline RSD near-random performance suggests limited long-horizon planning in 3B model.

- **First 3 experiments**: 1) Replicate baseline comparison: Run Continuation and TAE ParaScope on subset; verify mean cosine similarity ~0.43 and ~0.55 respectively against cheat-5 baseline (0.50) 2) Layer-wise scrubbing test: Implement causal scrubbing on controlled prompt pair; confirm middle-layer dominance (layers 25-35 of 42 show 0.15-0.25 deltas) 3) Token-wise dynamics check: Extract residuals at positions ±10 around "\n\n"; apply Continuation ParaScope; verify sharp transition in topic similarity (pre: 0.12, at: 0.48, post: 0.65)

## Open Questions the Paper Calls Out

- **Open Question 1**: Are the decoded planning signals causally necessary for model generation, or could they be spurious correlations detectable by sophisticated probes? The paper demonstrates decodability but uses observational probing methods; causal intervention studies would be needed to distinguish genuine planning from correlational artifacts.

- **Open Question 2**: Does planning scale with model size, and do larger models exhibit longer-horizon or more structured planning? Only one small model was tested; the weak outline-level results may reflect model capacity rather than fundamental planning limitations.

- **Open Question 3**: Is planning information concentrated at paragraph boundaries (particularly "\n\n" tokens), or is it distributed across preceding tokens? Temporal dynamics analysis suggests just-in-time planning, but the analysis window was limited to ±10 tokens around transitions.

## Limitations
- The paper primarily tests on a single 3B parameter model, limiting generalizability claims
- Outline-level planning shows near-random performance, suggesting limited long-horizon planning capability
- Linear probe approach in TAE ParaScope may miss non-linearly encoded planning information

## Confidence

**High Confidence**: Layer-wise specialization findings (60-80% depth concentration) and token-level transition dynamics (0.12→0.48 topic similarity jump at "\n\n"). These are supported by specific quantitative measurements and causal scrubbing results.

**Medium Confidence**: Paragraph-level decodability claims. While cosine similarities are above random baselines, the distinction between genuine planning and learned correlations remains unresolved.

**Low Confidence**: Outline-level planning results and the generalizability claim to larger models. The outline experiments show random performance, and the 3B model's myopic behavior may not extend to larger architectures.

## Next Checks

1. **Probe Robustness Test**: Evaluate TAE ParaScope on out-of-distribution prompts and measure cosine similarity degradation. Compare against control probes trained on shuffled paragraph pairs to quantify spurious correlation.

2. **Non-Linear Probe Comparison**: Implement and test MLP probes against the linear TAE method. Measure improvement in detail preservation metrics and conduct ablation studies on hidden layer sizes.

3. **Architecture Scaling Study**: Run preliminary tests on Llama 3.1 8B and 70B using the same ParaScope framework. Compare layer-wise planning signal concentration and transition dynamics to identify whether the 60-80% depth pattern holds across scales.