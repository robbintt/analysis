---
ver: rpa2
title: 'Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs
  Human Fallacy Detection'
arxiv_id: '2510.09695'
source_url: https://arxiv.org/abs/2510.09695
tags:
- arguments
- emotional
- fallacy
- emotions
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first systematic computational analysis
  of how AI-driven emotional framing affects human fallacy detection and argument
  convincingness. Using large language models (LLMs), the authors inject six emotions
  (anger, disgust, fear, enjoyment, sadness, surprise) into fallacious arguments via
  four framing strategies (storytelling, imagery, vivid language, loaded words), preserving
  logical structure.
---

# Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection

## Quick Facts
- arXiv ID: 2510.09695
- Source URL: https://arxiv.org/abs/2510.09695
- Reference count: 40
- Primary result: LLM-driven emotional framing reduces human fallacy detection by 14.5% F1

## Executive Summary
This study systematically investigates how AI-driven emotional framing affects human ability to detect logical fallacies. Using large language models, the authors inject six emotions (anger, disgust, fear, enjoyment, sadness, surprise) into fallacious arguments through four framing strategies while preserving logical structure. Human experiments with 1,000 arguments revealed that emotionally framed arguments significantly reduced fallacy detection performance, with loaded words strategy being most detrimental. The research demonstrates that emotions like enjoyment support detection while fear and sadness impair it, highlighting risks of AI-driven emotional manipulation in generating persuasive misinformation.

## Method Summary
The authors used the LOGIC dataset (1,245 fallacious arguments) filtered to 5 fallacy types and 538 standalone arguments. Four LLMs (qwen3-32b, ds-llama-70b, gpt-4o-mini, llama-3.3-70b) were selected from eight models based on reasoning preservation and coherence. These models applied three framing strategies (storytelling, imagery, loaded words) to inject six emotions into arguments. Human annotators (3 per batch, majority voting) evaluated fallacy detection, perceived emotions, and convincingness. The authors also annotated reasoning preservation and coherence for each synthetic argument.

## Key Results
- LLM-framed emotional arguments reduced fallacy detection F1 by 14.5% and accuracy by 5.5% versus original arguments
- Loaded words strategy most detrimental to detection despite adding fewest tokens
- Humans detected fallacies best when perceiving enjoyment (F1=0.528), worst with fear (0.283) and sadness (0.337)
- Enjoyment, fear, and sadness correlated with higher convincingness ratings
- Emotion match rates remained low (23.2%-52.9%) between target and perceived emotions

## Why This Works (Mechanism)

### Mechanism 1: Emotional Interference with Logical Reasoning
Emotional framing reduces fallacy detection accuracy by impairing cognitive evaluation of argument structure. Emotions alter attentional scope and critical thinking capacity. Negative emotions (fear, sadness) narrow attention and impair detection (F1=0.283–0.337), while positive emotions like enjoyment broaden attentional scope and support detection (F1=0.528).

### Mechanism 2: Dual Pathway to Perceived Convincingness
Convincingness operates through two distinct emotional pathways depending on valence. Positive emotions (enjoyment) increase openness to persuasion while maintaining logical scrutiny. Negative emotions (fear, sadness) increase convincingness by impairing critical evaluation rather than enhancing argument quality.

### Mechanism 3: Expression-Perception Gap in LLM Emotional Framing
LLMs can modify emotional expression while preserving logical structure, but intended emotions frequently mismatch human perception. LLMs successfully increase emotional appeal (94% of cases) and preserve reasoning (89%), but emotion match rates remain low (26–50%) due to gap between textual emotional cues and subjective reader interpretation.

## Foundational Learning

- **Logical Fallacy Types**: Understanding premise-conclusion structures and common reasoning errors (faulty generalization, ad hominem, ad populum, false causality, circular claim). Why needed here: The entire paradigm requires preserving fallacy structure while manipulating emotional content; without this, you cannot isolate emotional effects from logical structure changes. Quick check question: Can you identify why "Everyone believes X, therefore X is true" is fallacious without changing the conclusion?

- **Ekman's Six Basic Emotions**: Anger, disgust, fear, enjoyment, sadness, surprise as discrete emotional categories. Why needed here: The experimental design depends on systematically injecting these specific emotions and measuring their differential effects. Quick check question: What distinguishes fear from anxiety in terms of target specificity?

- **Emotional Framing Strategies**: Four text modification approaches (storytelling adds sentences; imagery paints mental pictures; vivid language modifies word choices; loaded words uses connotative vocabulary). Why needed here: Strategy selection directly affects reasoning preservation and emotional intensity; loaded words disrupted detection most despite adding fewest tokens. Quick check question: Which strategy would you use to add emotion while minimizing text length increase?

## Architecture Onboarding

- **Component map**: Original argument -> LLM emotional framing (strategy + target emotion) -> Human perception (emotion, fallacy type, convincingness) -> Aggregated labels -> Performance comparison (original vs. synthetic)

- **Critical path**: Original argument → LLM emotional framing (strategy + target emotion) → Human perception (emotion, fallacy type, convincingness) → Aggregated labels → Performance comparison (original vs. synthetic)

- **Design tradeoffs**:
  - Imagery/storytelling: Higher emotional appeal but lower coherence (0.77–0.87 vs. 0.95–0.96)
  - Loaded words: Most disruptive to detection but adds fewest tokens
  - Vivid language: Highest coherence but lowest reasoning preservation (excluded from final study)
  - o3-mini: Strong reasoning but coherence failures (0.60 vs. 0.86–0.98)

- **Failure signatures**:
  - Low emotion match (target ≠ perceived emotion): 74% mismatch rate
  - Reasoning disruption: Check if premise-conclusion relationship changes
  - Coherence collapse: o3-mini scored 0.00 on imagery coherence
  - Length inflation: Storytelling/imagery add 20+ tokens, potentially diluting fallacy signal

- **First 3 experiments**:
  1. Reproduce the framing pipeline: Run qwen3-32b on 25 arguments with loaded words strategy for all 6 emotions; verify reasoning preservation rate approaches reported 1.00
  2. Validate emotion-perception gap: Have 3 independent annotators label perceived emotions on 50 synthetic arguments; expect 25–50% match with target emotions
  3. Test strategy comparison: Generate 20 arguments each using loaded words vs. imagery with ds-llama-70b; measure whether loaded words produces lower F1 scores despite shorter length

## Open Questions the Paper Calls Out

### Open Question 1
How do emotions affect LLM behavior in argumentation tasks, specifically when evaluating or being convinced by fallacious arguments? Future work could explore the effects of emotions on LLM behavior in argumentation. For example, Payandeh et al. (2024) find that LLMs are substantially more often convinced when presented with logical fallacies in a multi-round debate setting, but do not consider the role of emotions.

### Open Question 2
Do the observed effects of emotional framing on human fallacy detection generalize to real-world arguments with greater length, topic diversity, and structural complexity? The study used short, standalone arguments from online educational resources, which are simpler than authentic public discourse.

### Open Question 3
What specific mitigation methods can effectively safeguard against malicious use of AI-driven emotional framing in fallacious content? The authors explicitly call for future work to investigate mitigation methods to safeguard against malicious applications and note their findings raise risks of misuse for generating persuasive misinformation, propaganda, or manipulative content.

## Limitations
- Emotion-perception gap: 74% mismatch between target and perceived emotions raises questions about whether intended manipulations occurred
- Dataset limitations: Arguments are short and from educational resources, potentially limiting generalizability to real-world discourse
- Reasoning preservation: 89% preservation rate suggests some logical structure changes occurred during framing

## Confidence
- **High**: The core effect of emotional framing reducing fallacy detection performance (F1=0.283-0.528 across emotions)
- **Medium**: The specific mechanism linking positive emotions to improved detection and negative emotions to impairment (based on theoretical literature more than direct empirical validation)
- **Low**: The generalizability beyond the six Ekman emotions and four framing strategies tested, particularly for complex arguments or different cultural contexts

## Next Checks
1. **Emotion Match Validation**: Conduct independent human annotation of perceived emotions on 100 synthetic arguments to quantify the expression-perception gap and test whether different emotions (regardless of target) drive detection differences
2. **Length vs. Strategy Effect**: Generate 50 arguments each using loaded words (shortest) and imagery (longest) strategies with the same model; test whether detection differences persist after controlling for token count
3. **Reasoning Preservation Audit**: Have three independent annotators verify reasoning preservation on 50 overlapping arguments from different model-strategy combinations; check if preservation rates vary by emotion type or correlate with detection performance