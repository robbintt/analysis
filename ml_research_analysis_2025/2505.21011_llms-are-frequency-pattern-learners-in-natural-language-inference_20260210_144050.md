---
ver: rpa2
title: LLMs are Frequency Pattern Learners in Natural Language Inference
arxiv_id: '2505.21011'
source_url: https://arxiv.org/abs/2505.21011
tags:
- frequency
- llms
- bias
- datasets
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanisms behind LLMs' improved performance
  on Natural Language Inference (NLI) tasks through fine-tuning. The authors identify
  a consistent frequency bias in NLI datasets where predicates in hypotheses occur
  more frequently than those in premises for positive entailment cases.
---

# LLMs are Frequency Pattern Learners in Natural Language Inference

## Quick Facts
- arXiv ID: 2505.21011
- Source URL: https://arxiv.org/abs/2505.21011
- Authors: Liang Cheng; Zhaowei Wang; Mark Steedman
- Reference count: 14
- Key outcome: LLMs exploit frequency bias in NLI datasets and perform poorly on frequency-adversarial instances.

## Executive Summary
This paper investigates why LLMs improve on Natural Language Inference (NLI) tasks through fine-tuning. The authors discover a consistent frequency bias in NLI datasets where predicates in hypotheses occur more frequently than those in premises for positive entailment cases. Through controlled experiments, they demonstrate that LLMs exploit this frequency bias during inference and that fine-tuning amplifies this reliance. When evaluated on frequency-adversarial instances (where the bias is reversed), both standard and fine-tuned LLMs show significant performance drops, with fine-tuned models exhibiting larger gaps. The authors also find a correlation between frequency bias and textual entailment, showing that hypernyms occur more frequently than their hyponyms.

## Method Summary
The authors fine-tune multiple LLMs (DeepSeek-R1-Distill-Llama-8B, Mistral-7B, LLaMA-3-8B-instruct, LLaMA-3-70B-instruct) on RTE, MNLI, and Entailment Graphs using LoRA with lr=1e-4, 12 epochs, rank=8, dropout=0.05. They evaluate on the Levy/Holt dataset, partitioning it into frequency-consistent (hypothesis predicate more frequent) and frequency-adversarial (premise predicate more frequent) subsets. Predicate frequencies are computed using the WordFreq library, focusing on verbal predicates only. Performance is measured as AUC scores comparing accuracy on consistent vs. adversarial subsets.

## Key Results
- Both standard and fine-tuned LLMs perform better on frequency-consistent examples than frequency-adversarial ones
- Fine-tuned models show significantly larger performance gaps between consistent and adversarial cases
- Frequency bias correlates with entailment, as hypernyms occur more frequently than hyponyms in WordNet
- Models become more vulnerable to frequency-adversarial inference cases after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exploit corpus frequency patterns as a proxy for semantic entailment during NLI inference.
- Mechanism: The model learns that higher-frequency predicates in hypotheses correlate with positive entailment labels in training data, then applies this statistical regularity as a decision shortcut at inference time rather than performing compositional logical reasoning.
- Core assumption: Predicate frequency differences between premise and hypothesis carry signal that partially overlaps with valid semantic generalization relationships.
- Evidence anchors:
  - [abstract] "LLMs exploit frequency bias for inference and perform poorly on adversarial instances."
  - [section 4.2, Table 2] "Both standard and fine-tuned LLMs can make correct predictions when test samples are consistent with the frequency bias. In contrast, when this bias is reduced or adversarial, the likelihood of incorrect predictions increases."
  - [corpus] Related work "Neutralizing Bias in LLM Reasoning using Entailment Graphs" documents attestation bias in NLI, supporting the broader pattern of shortcut learning.
- Break condition: When frequency-adversarial examples are presented (hypothesis predicates less frequent than premise), accuracy drops substantially, indicating the mechanism is being misapplied.

### Mechanism 2
- Claim: Fine-tuning on NLI datasets amplifies model reliance on frequency bias rather than building robust reasoning circuits.
- Mechanism: Gradient updates during fine-tuning reinforce the easiest learnable patterns first; frequency correlations are high-signal/low-complexity, so they receive disproportionate weight while deeper semantic reasoning patterns remain under-learned.
- Core assumption: NLI datasets contain exploitable frequency biases that are learned faster than true semantic entailment relationships.
- Evidence anchors:
  - [abstract] "Fine-tuned LLMs exhibit significantly increased reliance on this bias, suggesting that they are learning these frequency patterns from datasets."
  - [section 4.3, Table 4] Fine-tuned models show larger performance gaps between frequency-consistent and frequency-adversarial cases (e.g., Mistral-7B: -8.08 → -24.82 after EG-tuning).
  - [corpus] "Improving the OOD Performance of Closed-Source LLMs on NLI" confirms in-distribution fine-tuning gains correspond to OOD performance drops.
- Break condition: Models that have undergone less NLI fine-tuning show smaller accuracy gaps between bias-consistent and bias-adversarial examples.

### Mechanism 3
- Claim: Frequency bias correlates with semantic generalization because hypernyms (more general concepts) naturally occur more frequently than hyponyms.
- Mechanism: In natural language corpora, abstract/general predicates appear more often than specific ones; NLI datasets disproportionately contain specific→general entailments; LLMs learn frequency as a valid proxy for the generalization gradient.
- Core assumption: The statistical distribution of predicates in pretraining corpora reflects genuine semantic hierarchies.
- Evidence anchors:
  - [section 4.4, Table 5] WordNet analysis shows hypernyms (12.18 avg frequency) occur ~3× more than hyponyms (4.13).
  - [section 4.4] "This finding suggests that frequency bias may serve as a proxy for the generalization gradient, whereby inferences from lower-frequency to higher-frequency predicates reflect a generalization from specific to more abstract concepts."
  - [corpus] Limited direct corpus evidence; related work on entailment graphs supports the directionality claim but not specifically the frequency mechanism.
- Break condition: If entailment requires specific→specific or general→specific reasoning (rather than specific→general), the frequency proxy produces incorrect predictions.

## Foundational Learning

- Concept: **Frequency Bias Detection**
  - Why needed here: Understanding how to measure predicate frequency differences between premise/hypothesis pairs is essential for diagnosing model brittleness and designing adversarial tests.
  - Quick check question: Given premise "The athlete sprinted" and hypothesis "The person moved," which direction does the frequency bias point, and would you expect a frequency-sensitive model to predict entail or no-entail?

- Concept: **Shortcut Learning in Neural Networks**
  - Why needed here: The paper's core finding is that NLI fine-tuning teaches exploitable shortcuts rather than robust reasoning; this background helps interpret the amplification effect.
  - Quick check question: If a model achieves 85% accuracy on a benchmark but fails when a surface-level statistical cue is inverted, what does this suggest about its learned representations?

- Concept: **Hypernym/Hyponym Semantic Hierarchies**
  - Why needed here: The frequency-entailment correlation depends on understanding that generalization flows up semantic hierarchies (hyponym→hypernym), and that NLI datasets encode this pattern.
  - Quick check question: In the pair "X whispered to Y" / "X talked to Y," which predicate is the hypernym, which occurs more frequently in natural text, and what entailment label should hold?

## Architecture Onboarding

- Component map:
  - Predicate extraction module -> Frequency lookup -> Bias computation -> Dataset partitioner -> Fine-tuning pipeline

- Critical path:
  1. Extract predicates from NLI training data → compute frequency statistics → verify bias exists in dataset
  2. Fine-tune models on bias-containing NLI data
  3. Evaluate on Levy/Holt test set partitioned by bias direction
  4. Compare accuracy gaps (consistent vs. adversarial) between base and fine-tuned models

- Design tradeoffs:
  - Using verbal predicates only (not noun phrases) focuses analysis but may miss other frequency-sensitive patterns
  - Levy/Holt dataset chosen for single-predicate structure enabling cleaner analysis, but may not reflect complex multi-predicate NLI
  - LoRA fine-tuning is parameter-efficient but may produce different behaviors than full fine-tuning

- Failure signatures:
  - Large accuracy gap between bias-consistent and bias-adversarial subsets (>15 points suggests strong reliance)
  - Fine-tuned models showing worse adversarial performance than base models indicates bias amplification
  - Models predicting based on frequency direction rather than semantic validity when presented with synthetic adversarial pairs

- First 3 experiments:
  1. **Baseline frequency audit**: Compute predicate frequencies on your target NLI dataset using the `wordfreq` library; report the average bias for Entail vs. No-Entail classes to confirm the pattern exists before any model training.
  2. **Adversarial robustness test**: Take a held-out test set, partition by bias direction, and evaluate both base and fine-tuned models; quantify the gap (Δ = consistent - adversarial) as your brittleness metric.
  3. **Ablation with balanced data**: Construct a mini training set with frequency bias explicitly neutralized (equal frequency premises and hypotheses); compare fine-tuned performance to standard NLI data to isolate the bias contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do extremely large-scale models (>70B parameters) exhibit reduced or different reliance on frequency bias compared to smaller models?
- Basis in paper: [explicit] The authors state in the Limitations section: "our experiments are limited to a range of smaller LLMs with 8B variants and a single extremely large-scale model, LLaMA-3-70B. This restricts our ability to draw conclusions about extremely large models."
- Why unresolved: Only one large model was tested, and scaling laws may fundamentally change how models learn and exploit statistical shortcuts versus genuine reasoning patterns.
- What evidence would resolve it: Systematic evaluation of models across a broader scale range (e.g., 8B, 70B, 405B, 1T+ parameters) on both frequency-consistent and frequency-adversarial inference tasks.

### Open Question 2
- Question: Can LLMs be trained to achieve robust inference performance while minimizing reliance on frequency bias as a shortcut?
- Basis in paper: [inferred] The paper demonstrates vulnerability to frequency-adversarial cases but does not explore mitigation strategies. The finding that fine-tuning amplifies bias reliance suggests current training paradigms reinforce rather than address this limitation.
- Why unresolved: It remains unclear whether de-biasing training data, adding adversarial examples, or architectural modifications could preserve the benefits of frequency-pattern learning while ensuring robustness when bias is inverted.
- What evidence would resolve it: Experiments comparing models trained on frequency-balanced NLI datasets versus standard datasets, measuring both overall performance and robustness to adversarial cases.

### Open Question 3
- Question: Does the correlation between frequency bias and entailment hold across languages with different linguistic properties?
- Basis in paper: [inferred] All experiments use English-only datasets (WordNet, MNLI, RTE, etc.). The relationship between hypernym frequency and semantic generalization may not generalize to languages with different morphological or lexical structures.
- Why unresolved: The frequency-entailment correlation may be an artifact of English corpus statistics rather than a universal property of natural language inference.
- What evidence would resolve it: Cross-linguistic analysis of frequency bias in NLI datasets and multilingual model evaluation on frequency-adversarial instances across typologically diverse languages.

## Limitations

- The generalizability of frequency bias findings to complex, multi-predicate NLI tasks remains untested
- The causal link between frequency patterns and semantic generalization lacks direct empirical validation
- The focus on verbal predicates may miss other exploitable patterns in NLI datasets

## Confidence

- **High confidence**: The existence of frequency bias in NLI datasets and its correlation with entailment labels is well-established through corpus analysis.
- **Medium confidence**: The mechanism that frequency serves as a proxy for semantic generalization (hypernym/hyponym relationships) is plausible given WordNet analysis but requires additional validation.
- **Low confidence**: The paper doesn't adequately address whether the frequency bias is specific to NLI or would manifest similarly in other reasoning tasks.

## Next Checks

1. **Generalization to Multi-Predicate NLI**: Test the frequency bias hypothesis on datasets like MNLI or ANLI that contain complex premise-hypothesis pairs with multiple predicates. Measure whether the frequency-adversarial performance gap persists and correlates with the number of predicates per example.

2. **Frequency-Neutral Training**: Train models on NLI datasets with explicit frequency balancing (ensuring premise and hypothesis predicates have similar frequencies). Compare performance on standard vs. adversarial test sets to quantify the isolated contribution of frequency bias to overall accuracy.

3. **Semantic vs. Statistical Validation**: Design experiments that disentangle semantic generalization from pure frequency learning. For instance, create synthetic examples where hypernym-hyponym relationships exist but frequency patterns are reversed, or where frequency patterns exist but no valid entailment holds.