---
ver: rpa2
title: Large Scale Retrieval for the LinkedIn Feed using Causal Language Models
arxiv_id: '2510.14223'
source_url: https://arxiv.org/abs/2510.14223
tags:
- member
- retrieval
- item
- embeddings
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper describes a retrieval system for LinkedIn\u2019s feed\
  \ that uses a fine-tuned causal language model to generate high-quality embeddings\
  \ for both members and content, replacing a multi-source retrieval pipeline. The\
  \ system uses a dual-encoder architecture with LLaMA 3 and optimizes for engagement-based\
  \ relevance."
---

# Large Scale Retrieval for the LinkedIn Feed using Causal Language Models

## Quick Facts
- **arXiv ID**: 2510.14223
- **Source URL**: https://arxiv.org/abs/2510.14223
- **Reference count**: 9
- **Primary result**: Replaced multi-index retrieval with unified CLM-based dual-encoder, achieving +0.8% revenue and +0.2% daily unique professional interactions in online A/B test

## Executive Summary
This paper describes LinkedIn's transition from a multi-index retrieval system to a unified embedding-based approach using a fine-tuned causal language model (LLaMA 3 3B) for their feed recommendation pipeline. The dual-encoder architecture generates embeddings for both members and content, optimizing for engagement-based relevance through InfoNCE loss with mixed negatives. Offline experiments showed improved recall@10 when using mean pooling, quantized numerical features, and matryoshka representations. An online A/B test demonstrated significant improvements in revenue (+0.8%) and daily unique professional interactions (+0.2%), with particularly strong gains among newer members. The approach simplifies the retrieval stack while improving relevance and engagement.

## Method Summary
The system uses LLaMA-3 3B as a shared dual-encoder to generate dense embeddings for both members and content items. Member prompts include profile information and truncated positive interaction history, while content prompts contain metadata and text. The model is trained with InfoNCE loss using in-batch negatives plus two hard negatives per member (impressed-but-not-engaged items). Mean pooling over all tokens produces the final embeddings, with matryoshka representation learning enabling flexible dimensionality. A key innovation was quantizing numerical popularity features to percentages (1-100), which significantly improved feature encoding. The system retrieves 2000 candidates from hundreds of millions of items with sub-50ms latency at several thousand QPS.

## Key Results
- Offline: Improved recall@10 using InfoNCE loss with matryoshka embeddings
- Online A/B test: +0.8% revenue increase and +0.2% daily unique professional interactions
- Larger gains observed among newer members (lower liquidity cohorts)
- Minimal performance drop when reducing embedding dimensionality using matryoshka representations

## Why This Works (Mechanism)
The dual-encoder architecture enables efficient similarity search by pre-computing member and content embeddings, replacing the complex multi-index retrieval pipeline. InfoNCE loss with hard negatives ensures the model learns to distinguish between engaging and non-engaging content for each member. Mean pooling captures contextual information across the entire prompt, while matryoshka representations provide flexibility in balancing accuracy and efficiency. Quantizing numerical features into discrete buckets (1-100) makes the model better at encoding popularity signals, addressing the challenge that raw counts can overwhelm other features.

## Foundational Learning
- **Dense Retrieval**: Why needed: Replaces expensive re-ranking with pre-computed embeddings for scalability; Quick check: Verify sub-50ms latency at target QPS
- **Dual-Encoder Architecture**: Why needed: Enables separate encoding of queries and documents for efficient retrieval; Quick check: Confirm cosine similarity computation works bidirectionally
- **InfoNCE Loss**: Why needed: Contrastive objective that learns to separate positive pairs from negatives; Quick check: Monitor training loss convergence and embedding separation
- **Matryoshka Representations**: Why needed: Provides nested embeddings for flexible dimensionality at inference; Quick check: Measure recall@10 drop when reducing dimensions
- **Hard Negative Mining**: Why needed: Improves model discrimination by including challenging non-engaging examples; Quick check: Verify recall improvement with hard negatives vs only in-batch
- **Mean Pooling**: Why needed: Captures context across entire prompt rather than just final token; Quick check: Compare performance with last-token pooling baseline

## Architecture Onboarding

**Component Map**: Member prompt generator -> CLM dual-encoder -> Embedding store -> Cosine similarity search -> Ranking model

**Critical Path**: Prompt generation → Embedding computation → Index search → Candidate ranking

**Design Tradeoffs**: 
- CLM vs traditional encoders: Better feature interaction modeling vs higher computational cost
- Mean pooling vs last-token: Better context capture vs simpler implementation
- Matryoshka vs fixed embeddings: Flexibility vs complexity
- Hard negatives vs only in-batch: Better discrimination vs increased training complexity

**Failure Signatures**: 
- Low recall with raw count features (near-zero correlation with popularity)
- Performance drop with last-token pooling (5-12% worse)
- Poor performance on new users (if positive-only history insufficient)

**3 First Experiments**:
1. Implement quantized count features (1-100) and measure correlation improvement with popularity
2. Compare mean pooling vs last-token pooling on recall@10 metric
3. Stratify online metrics by member tenure to quantify new user gains

## Open Questions the Paper Calls Out
None

## Limitations
- Security concerns prevent sharing exact prompt templates and special tokens
- Oracle ranking model details not specified, limiting offline evaluation validity
- Revenue impact measured in aggregate without detailed stratification by member type

## Confidence
- **High confidence**: Core architecture (dual-encoder CLM, InfoNCE loss, matryoshka embeddings), quantization approach
- **Medium confidence**: Quantitative results (recall@10 improvements, online metrics) due to missing oracle model details
- **Low confidence**: Causal attribution of specific improvements to design choices

## Next Checks
1. Implement quantized count features (1-100) and measure cosine similarity correlation with popularity before and after quantization
2. Train two versions of dual-encoder (mean pooling vs last-token) and measure Recall@10 gap
3. Re-analyze online A/B test results by stratifying metrics across member tenure buckets (<30 days, 30-90 days, >90 days)