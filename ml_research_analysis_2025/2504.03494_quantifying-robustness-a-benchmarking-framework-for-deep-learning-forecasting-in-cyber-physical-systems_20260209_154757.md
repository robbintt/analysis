---
ver: rpa2
title: 'Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting
  in Cyber-Physical Systems'
arxiv_id: '2504.03494'
source_url: https://arxiv.org/abs/2504.03494
tags:
- robustness
- performance
- forecasting
- time
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of practical robustness evaluation
  methods for deep learning forecasting models in industrial Cyber-Physical Systems.
  While deep learning shows promise for time series forecasting in CPS, concerns about
  robustness under real-world disturbances limit adoption.
---

# Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems

## Quick Facts
- **arXiv ID**: 2504.03494
- **Source URL**: https://arxiv.org/abs/2504.03494
- **Reference count**: 40
- **Key outcome**: A new robustness evaluation framework reveals that while simple models like DLinear achieve best forecasting accuracy, recurrent architectures like LSTM demonstrate highest robustness to realistic CPS disturbances.

## Executive Summary
This paper addresses the critical gap in evaluating deep learning forecasting models for industrial Cyber-Physical Systems under realistic operational disturbances. The authors develop a systematic robustness framework that quantifies model performance degradation across ten realistic disturbance types including sensor drift, noise, and irregular sampling. The framework introduces a standardized robustness score that integrates relative performance degradation across severity levels, enabling fair comparison across diverse CPS datasets. Empirical evaluation of nine popular DL architectures across six real-world CPS datasets reveals a fundamental accuracy-robustness tradeoff, with DLinear achieving best baseline performance but lowest robustness, while LSTM demonstrates superior resilience to disturbances.

## Method Summary
The framework trains DL models on CPS time series forecasting tasks with 70/15/15 train/val/test splits, then evaluates robustness by injecting ten realistic disturbance types at varying severity levels. For each disturbance and severity combination, the framework computes relative performance (disturbed MSE/original MSE), integrates this over severity levels (1% increments), then multiplies scores across all disturbance types to produce a final robustness score. The approach uses standardized data preprocessing, early stopping, and hyperparameter tuning via grid search. The evaluation covers six CPS datasets and nine DL architectures including RNNs, Transformers, and State Space Models.

## Key Results
- DLinear achieves best baseline forecasting accuracy (MSE 0.2587) but lowest robustness score (0.2754)
- LSTM demonstrates highest robustness score (0.6411) with moderate baseline accuracy (MSE 0.5016)
- Transformer offers balanced compromise between accuracy and robustness across datasets
- Robustness scores vary significantly across datasets, highlighting need for multi-dataset evaluation
- Simple linear models are highly susceptible to disturbances despite strong clean performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The robustness score quantifies model degradation under disturbances by integrating relative performance across severity levels and disturbance types.
- Mechanism: The framework computes relative performance (disturbed MSE / original MSE) at increasing severity levels s ∈ [0,1], integrates across severity via discrete summation (1% steps), then multiplies scores across all disturbance types. This creates a unified metric where failure on any single disturbance scenario heavily penalizes the overall score.
- Core assumption: The selected disturbances (drift, noise, missing data, etc.) adequately represent real-world CPS operational challenges, and their effects compound rather than cancel.
- Evidence anchors:
  - [abstract] "The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets"
  - [section 3, Eq. 6-7] Rd(f, µ) = E[∫₀¹ µrel(f, d, s, X, Y) ds]; R(f, µ) = ∏_{d∈D} Rd(f, µ)
  - [corpus] Weak direct support—related papers (Windmann et al. 2023, Dix et al. 2023) examine data quality effects but lack standardized scoring mechanisms.
- Break condition: When disturbances are mutually exclusive in practice; when multiplication over-penalizes rare, correlated failures; when 1% severity granularity misses discontinuous failure modes.

### Mechanism 2
- Claim: Recurrent architectures (LSTM, GRU, RIMs) achieve higher robustness scores due to sequential state processing with gating mechanisms.
- Mechanism: Gating in LSTM/GRU allows selective retention of relevant history while discarding corrupted or irrelevant inputs. When disturbances affect early time steps, recurrent models can "recover" by emphasizing more recent, undisturbed observations through learned gate dynamics.
- Core assumption: Disturbances are localized in time rather than uniformly affecting the entire input sequence.
- Evidence anchors:
  - [abstract] "Recurrent architectures like LSTM demonstrate higher robustness scores"
  - [section 7] "This could be attributed to their design, which inherently emphasizes recent temporal dependencies, enabling them to better withstand disturbances affecting earlier time steps"
  - [corpus] Federated Learning paper (arXiv:2501.16666) notes sensor reliability varies in distributed CPS, supporting importance of robust sequential processing, but does not validate gating mechanism directly.
- Break condition: When disturbances affect the forecast horizon (Y) rather than input (X); when tasks require precise long-range dependencies that gating discards.

### Mechanism 3
- Claim: Simpler linear models achieve superior baseline accuracy but exhibit lower robustness due to overfitting to training distribution patterns.
- Mechanism: DLinear decomposes time series into trend and seasonal components with linear projections. This captures training distribution structure efficiently but lacks capacity to adapt when inputs deviate from learned patterns—small input perturbations propagate directly to outputs without non-linear buffering.
- Core assumption: Training data is clean and representative of ideal conditions, creating an accuracy-robustness tradeoff that standard evaluation misses.
- Evidence anchors:
  - [abstract] "While simpler models like DLinear achieve better baseline forecasting performance, they show lower robustness"
  - [section 6.1] "DLinear achieves the best overall forecasting performance"
  - [section 6.2] "Despite demonstrating strong forecasting performance, DLinear exhibits the lowest robustness score, indicating high susceptibility to disturbances"
  - [corpus] Explainable AI paper (arXiv:2601.16074) discusses ML reliability in industrial CPS but does not address this specific tradeoff.
- Break condition: When training data includes augmented disturbances; when explicit regularization (dropout, noise injection) is applied during training.

## Foundational Learning

- Concept: **Distributional Robustness vs. Adversarial Robustness**
  - Why needed here: The paper explicitly rejects adversarial perturbations as unrealistic for CPS, instead focusing on natural distribution shifts (sensor drift, noise, missing data).
  - Quick check question: Why would a model robust to adversarial noise still fail under gradual sensor drift?

- Concept: **Relative Performance Metrics**
  - Why needed here: The robustness score uses µrel = µ(original) / µ(disturbed), not absolute performance. This isolates degradation from baseline accuracy.
  - Quick check question: If Model A has MSE=0.1 and Model B has MSE=0.5 on clean data, but both degrade to MSE=1.0 under disturbance, which has higher robustness score?

- Concept: **CPS-Specific Data Quality Issues**
  - Why needed here: The 10 disturbance types (drift, dying signal, flat sensor, etc.) reflect real industrial sensor failure modes, not generic noise.
  - Quick check question: Why is "MissingData" disturbance more challenging than "Noise" for most architectures in this benchmark?

## Architecture Onboarding

- Component map: CSV/Parquet → standardize on training stats → 70/15/15 split with 1% purge gaps → Model zoo (9 architectures) → Disturbance layer (10 types, 10% sensors, s∈[0,1]) → MSE → relative performance → severity integral → score product

- Critical path:
  1. Standardize data using training split statistics only (prevents leakage)
  2. Train model with early stopping (patience=5 on validation MSE)
  3. For each disturbance d and severity s: inject disturbance → compute µrel
  4. Integrate µrel over s for each d (Riemann sum with 1% steps)
  5. Multiply Rd across all d for final R(f, µ)

- Design tradeoffs:
  - Score multiplication vs. averaging: Multiplication ensures single-scenario failure dominates; averaging may hide critical weaknesses
  - 10% sensor subset: Reduces computation but may miss correlated multi-sensor failures
  - 1% severity granularity: Approximates continuous integral; coarser steps miss sharp degradation points

- Failure signatures:
  - R(f, µ) → 0: Model collapses on one or more disturbance types
  - High validation-test MSE gap: Poor generalization, likely correlated with low robustness
  - DLinear signature: Best test MSE (0.2587) but worst robustness (0.2754)
  - LSTM signature: Moderate test MSE (0.5016) but best robustness (0.6411)
  - Mamba signature: High variance across datasets (±0.73 test MSE, ±0.38 robustness)

- First 3 experiments:
  1. Replicate LSTM vs. DLinear comparison on Electricity dataset to validate robustness-accuracy tradeoff on familiar data
  2. Ablate disturbance types individually on your chosen model to identify which failure modes are most critical for your application
  3. Add disturbance augmentation during training (noise injection, random masking) and measure robustness score improvement

## Open Questions the Paper Calls Out

- Question: How does the robustness of deep learning forecasting models change when facing correlated, system-wide failures compared to the individual sensor-level faults currently tested?
  - Basis in paper: [explicit] Section 7 states that "correlated system-wide failures, actuator-side or adversarial attacks remain outside our scope" as the framework currently injects faults individually.
  - Why unresolved: Real-world industrial failures often involve cascading effects across multiple sensors and actuators, which the current independent disturbance methodology does not capture.
  - What evidence would resolve it: An evaluation using an extended framework that injects simultaneous, correlated disturbances across multiple sensors and actuators to compare against the current baseline scores.

- Question: To what extent can disturbance-aware training or adversarial augmentation mitigate the high fragility observed in accurate linear models like DLinear?
  - Basis in paper: [explicit] Section 7 suggests "incorporating disturbance-aware training or adversarial augmentation" as a necessary direction for future work.
  - Why unresolved: The paper identifies a trade-off where simple models achieve high accuracy but low robustness; it is unknown if specific training regimes can break this trade-off.
  - What evidence would resolve it: A comparative study showing the robustness scores of DLinear and Transformers before and after being fine-tuned with the specific disturbance profiles defined in the paper.

- Question: Do physics-informed neural networks or graph-based architectures provide a better trade-off between forecasting accuracy and robustness than the evaluated data-driven models?
  - Basis in paper: [explicit] Section 7 explicitly calls for broadening model coverage to include "interpretable statistical or physics-informed baselines [and] graph-based architectures."
  - Why unresolved: The current study is limited to standard deep learning architectures (RNNs, Transformers, SSMs) and does not assess models that integrate domain knowledge or structural inductive biases.
  - What evidence would resolve it: Benchmark results applying the proposed robustness score $R(f, \mu)$ to physics-informed or graph-based models across the six CPS datasets.

## Limitations

- The robustness framework's applicability is constrained by its assumption that realistic CPS disturbances follow the 10 types modeled, which may not capture all operational failure modes in specific industrial settings.
- The multiplicative scoring mechanism could overemphasize rare but catastrophic failures while underrepresenting common, minor degradations.
- The benchmark focuses on univariate forecasting tasks, limiting generalizability to multivariate CPS control scenarios where disturbances propagate through system interconnections.

## Confidence

- **High confidence**: The empirical finding that DLinear achieves best baseline accuracy but lowest robustness (0.2587 vs 0.2754 scores) across multiple datasets
- **Medium confidence**: The claim that LSTM achieves highest robustness (0.6411) due to sequential state processing, as this mechanism hasn't been validated through ablation studies
- **Medium confidence**: The framework's comprehensiveness, as the 10 disturbance types appear representative but haven't been validated against real industrial CPS failure logs

## Next Checks

1. Apply the robustness framework to a custom industrial CPS dataset with known failure patterns to test framework sensitivity to application-specific disturbances
2. Conduct ablation studies removing gating mechanisms from LSTM to isolate whether recurrent processing or gating specifically drives robustness
3. Implement Bayesian optimization hyperparameter tuning instead of grid search to verify that the reported 10,000+ models achieved near-optimal performance for each architecture-dataset pair