---
ver: rpa2
title: Unified Semantic and ID Representation Learning for Deep Recommenders
arxiv_id: '2502.16474'
source_url: https://arxiv.org/abs/2502.16474
tags:
- semantic
- codebook
- tokens
- items
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of traditional ID-based and
  semantic token-based recommendation systems by proposing a unified framework that
  combines both token types. The method integrates semantic tokens, learned via RQ-VAE,
  with low-dimensional ID tokens to capture both shared and unique item characteristics.
---

# Unified Semantic and ID Representation Learning for Deep Recommenders

## Quick Facts
- arXiv ID: 2502.16474
- Source URL: https://arxiv.org/abs/2502.16474
- Reference count: 40
- One-line primary result: Unified framework combining semantic and ID tokens achieves 6%-17% performance gains while reducing embedding size by 80%+ over ID-only methods.

## Executive Summary
This paper addresses the limitations of traditional ID-based and semantic token-based recommendation systems by proposing a unified framework that combines both token types. The method integrates semantic tokens, learned via RQ-VAE, with low-dimensional ID tokens to capture both shared and unique item characteristics. It also employs cosine similarity in earlier layers for embedding decoupling and Euclidean distance in the final layer for item distinction. Experimental results on three benchmark datasets show significant performance improvements (6%-17% across metrics) while reducing token size by over 80% compared to ID-only methods.

## Method Summary
The proposed method combines semantic tokens (learned via RQ-VAE quantization) with low-dimensional ID tokens to create unified item representations. It uses Sentence-T5 to generate 768-dim text embeddings, compresses them to 64-dim semantic tokens through a 3-layer RQ-VAE with hybrid distance metrics (Cosine for early layers, Euclidean for final layer), and concatenates these with 8-dim ID embeddings. The entire framework is trained end-to-end with a composite loss function. The approach reduces embedding parameters by 80%+ while improving recommendation performance across standard metrics.

## Key Results
- Achieves 6%-17% performance improvements across HR@k, NDCG@k, and MRR metrics compared to ID-only methods
- Reduces token size by over 80% by replacing 64-dim ID embeddings with 8-dim ID + 64-dim semantic tokens
- The unified distance function improves codebook activation from 97% to 100% and unique coverage from 70% to 83%
- Ablation study shows 8-dim ID tokens provide optimal balance between distinction and redundancy

## Why This Works (Mechanism)

### Mechanism 1
Replacing high-dimensional ID embeddings with low-dimensional ID tokens plus semantic tokens reduces parameter redundancy while preserving item distinctiveness. The framework utilizes Residual Quantized Variational Autoencoders (RQ-VAE) to quantize content embeddings into discrete semantic tokens that capture shared attributes across items (generalization). Simultaneously, it assigns a significantly reduced ID vector to each item to capture unique attributes (memorization). These are concatenated to form a unified representation. The core assumption is that traditional ID embeddings contain significant redundancy and that a small ID dimension (e.g., 8) is sufficient to distinguish items that share semantic tokens. Evidence shows 80%+ token reduction with maintained performance. If ID dimension is reduced too aggressively (e.g., to 0), the model may fail to distinguish between similar items, causing ranking metrics like NDCG to drop.

### Mechanism 2
Employing a hybrid distance function (Cosine then Euclidean) during the quantization search improves codebook utilization and item coverage compared to using a single metric. In the RQ-VAE codebook lookup, the model uses Cosine Similarity in the first L-1 layers to decouple accumulated embeddings by finding orthogonal directions. It switches to Euclidean (L2) distance in the final layer to precisely distinguish unique items in the Cartesian space. The core assumption is that Cosine Similarity effectively handles dense/accumulated residual vectors, whereas Euclidean distance offers superior precision for the final distinct selection. Evidence shows Cosine achieves high codebook activation (97%+) but lower unique coverage (70%), whereas the unified approach achieves 100% activation and 83% coverage. If Euclidean distance is used in early layers, codebook activation drops significantly (5.86% in first layer), leading to underutilization of the semantic space.

### Mechanism 3
End-to-end joint optimization aligns the semantic codebook learning with the recommendation objective. Instead of pre-training semantic tokens separately, the framework minimizes a composite loss function: Recommendation Loss (accuracy) + RQ-VAE Loss (quantization commitment) + Reconstruction Loss (semantic fidelity). This forces the semantic tokens to be useful for the downstream recommendation task rather than just text reconstruction. The core assumption is that gradient flow from the recommendation loss through the quantization bottleneck is necessary to learn "recommendation-aware" semantic tokens. The framework is trained in an end-to-end manner, jointly optimizing three key objectives. If the reconstruction loss or quantization loss overwhelms the recommendation loss, the tokens may represent text semantics perfectly but fail to predict user preference (low HR/MRR).

## Foundational Learning

- **Concept: Residual Quantization (RQ-VAE)**
  - **Why needed here:** This is the core technique for generating semantic tokens. It represents an item as a sequence of discrete codes by recursively quantizing the residual error from previous layers.
  - **Quick check question:** Can you explain how the "residual" in RQ-VAE differs from standard Vector Quantization (VQ-VAE), and why it allows for a more compact representation?

- **Concept: Metric Spaces (Cosine vs. Euclidean)**
  - **Why needed here:** The paper's novelty relies on the geometric properties of these two metrics. Understanding that Cosine measures angle (orientation) while Euclidean measures magnitude (distance) is crucial to understanding why they are applied at different depths of the quantization process.
  - **Quick check question:** Why might Cosine Similarity fail to distinguish two items that are far apart in magnitude but share the same direction?

- **Concept: Sequential Recommendation (Transformer-based)**
  - **Why needed here:** The unified tokens are fed into a SASRec (Self-Attentive Sequential Recommendation) model. Understanding positional embeddings and attention masks is required to integrate the new tokenization scheme.
  - **Quick check question:** How does the input dimension of the SASRec model change when switching from standard One-Hot ID embeddings to the proposed Unified Tokenization?

## Architecture Onboarding

- **Component map:** Item Text → RQ-VAE Quantization → [Semantic Token + Low-Dim ID] → SASRec → Prediction

- **Critical path:** The logic flow is: *Item Text → RQ-VAE Quantization → [Semantic Token + Low-Dim ID] → SASRec → Prediction.* The critical implementation detail is the **custom lookup loop** in the Residual Quantizer that switches the distance metric at the final layer.

- **Design tradeoffs:**
  - **ID Dimension (D_id):** Lower dimensions save space (85% reduction) but risk collapsing distinct items. The paper suggests D_id=8 is a sweet spot (Figure 7).
  - **Codebook Size (K):** Larger K increases capacity but risks "degeneration" (Figure 13), where codebooks become sparse/unused. The paper suggests K=128 or 256.

- **Failure signatures:**
  - **Codebook Collapse:** If visualizing codebook usage shows only a few active indices (Figure 13), the learning rate or commitment loss β in RQ-VAE needs tuning.
  - **Duplication:** If distinct items map to identical tokens, check if the ID dimension is too low or if the final layer is still using Cosine similarity (which has lower unique coverage).

- **First 3 experiments:**
  1. **Sanity Check (Metric Swap):** Run RQ-VAE quantization on the validation set using Cosine-only, Euclidean-only, and the Unified method. Plot "Percentage of Activated Codebooks" and "Coverage of Unique Items" to reproduce Table 1.
  2. **Ablation (ID Dim):** Train the recommender with ID dimensions {0, 4, 8, 16, 64}. Verify that performance peaks at low dimensions (e.g., 8) and drops at 0 (semantic only) or 64 (redundant), reproducing Figure 7.
  3. **Token Efficiency:** Compare the total parameter count (Embedding Matrix Size) of the baseline vs. the proposed method to verify the claimed 80% reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the semantic tokenization process be refined to eliminate the remaining ~17% rate of duplicate item mappings without solely relying on ID tokens for distinction?
- **Basis in paper:** Section 3.2 explicitly states: "Despite the improvements, our proposed method still results in approximately 17% duplicate items... This issue arises when sentence embeddings for certain items are too similar to be distinguished."
- **Why unresolved:** The authors identify this as a limitation that is "difficult to completely eliminate" and currently rely on the low-dimensional ID token component to mitigate the issue, rather than resolving the root cause in the semantic quantization.
- **What evidence would resolve it:** A modification to the RQ-VAE mechanism or a contrastive loss term that reduces the duplication rate to near 0% in the semantic tokens alone, or improves the final Hit Rate without increasing ID dimensionality.

### Open Question 2
- **Question:** How can codebook degeneration (redundancy) be prevented when scaling the semantic codebook size beyond 256 entries?
- **Basis in paper:** Appendix A.5 notes that while performance peaks at size 128, "codebooks begin to degenerate and be redundant when codebook size is greater than 256," limiting the model's capacity to scale.
- **Why unresolved:** The paper experiments with size but does not propose a solution (such as specific regularization techniques) to maintain codebook utilization (activation) as the codebook capacity increases.
- **What evidence would resolve it:** An experiment using larger codebook sizes (e.g., 512 or 1024) demonstrating high codebook activation rates (utilization) and improved metric performance over the current optimal size of 128/256.

### Open Question 3
- **Question:** Is the fixed assignment of distance metrics (Cosine for early layers, Euclidean for the final layer) optimal, or could a learnable dynamic weighting of these metrics improve performance?
- **Basis in paper:** The method applies a heuristic where cosine similarity is used for the first two layers and Euclidean for the third (Section 3.2), based on the observation that they excel at different tasks (decoupling vs. distinction).
- **Why unresolved:** The current approach is a hard-coded architectural choice based on empirical visualization; the paper does not explore if a soft, learnable combination of the two metrics might be more adaptive.
- **What evidence would resolve it:** A comparison showing that a model with learnable distance metric weights outperforms the static "Cosine-Cosine-Euclidean" configuration on NDCG and MRR metrics.

### Open Question 4
- **Question:** Does the unified framework maintain its 6-17% performance improvement and token reduction efficiency in non-product domains (e.g., news or media) with sparser textual descriptions?
- **Basis in paper:** The evaluation is restricted to three Amazon Product datasets (Beauty, Sports, Toys), which feature rich metadata (titles, brands, categories) ideal for the Sentence-T5 semantic encoder.
- **Why unresolved:** It is unclear if the reliance on rich semantic embeddings holds true for domains where item descriptions are shorter, noisier, or less structured, potentially widening the gap between semantic and ID tokens.
- **What evidence would resolve it:** Experimental results on non-product benchmarks (e.g., news recommendation) showing that the unified method still significantly outperforms ID-only baselines while retaining the 80% token size reduction.

## Limitations
- The framework still results in approximately 17% duplicate item mappings that must be resolved by ID tokens alone
- Codebook degeneration occurs when scaling beyond 256 entries, limiting capacity expansion
- Performance on non-product domains with sparse textual descriptions remains unverified

## Confidence
- **Performance improvements (6%-17% across metrics):** High confidence - Multiple ablations on three datasets with standard metrics
- **80% token reduction claim:** High confidence - Direct comparisons showing parameter counts and demonstration that 8-dimensional ID tokens are sufficient
- **Unified distance function efficacy:** Medium confidence - Table 1 shows clear improvements, but results depend on dataset characteristics
- **End-to-end optimization benefits:** Medium confidence - Theoretically sound but lacks ablation studies comparing with pre-trained semantic tokens

## Next Checks
1. **Metric ablation validation:** Implement RQ-VAE with three variants (Cosine-only, Euclidean-only, Unified) on the validation set and reproduce Table 1's codebook activation and unique coverage metrics to verify the claimed 100% vs 97% and 83% vs 70% improvements.

2. **ID dimension sensitivity test:** Systematically vary the ID embedding dimension (0, 4, 8, 16, 32, 64) while keeping semantic tokens fixed, and measure HR@k, NDCG@k, and MRR to confirm the optimal 8-dimensional sweet spot and validate the claimed performance degradation at extremes.

3. **Parameter efficiency verification:** Calculate and compare the total embedding parameter counts (ID lookup table + semantic reconstruction parameters) between baseline (64-dim ID) and proposed method (8-dim ID + semantic tokens) across all three datasets to verify the 80%+ reduction claim.