---
ver: rpa2
title: Caption Injection for Optimization in Generative Search Engine
arxiv_id: '2511.04080'
source_url: https://arxiv.org/abs/2511.04080
tags:
- multimodal
- content
- g-seo
- optimization
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of optimizing content visibility
  in generative search engines (GSEs) by extending the task from unimodal to multimodal
  scenarios. The proposed method, Caption Injection, is the first multimodal G-SEO
  approach that integrates visual semantics into textual content by extracting captions
  from images and injecting them into text through a three-stage pipeline: structural
  generation, alignment refinement, and semantic injection.'
---

# Caption Injection for Optimization in Generative Search Engine

## Quick Facts
- arXiv ID: 2511.04080
- Source URL: https://arxiv.org/abs/2511.04080
- Reference count: 40
- Primary result: Proposed Caption Injection method achieves 1.85% and 1.09% relative improvements in unimodal and multimodal scenarios respectively on MRAMG benchmark.

## Executive Summary
This paper addresses the challenge of optimizing content visibility in generative search engines (GSEs) by extending the task from unimodal to multimodal scenarios. The proposed method, Caption Injection, is the first multimodal G-SEO approach that integrates visual semantics into textual content by extracting captions from images and injecting them into text through a three-stage pipeline: structural generation, alignment refinement, and semantic injection. The method was evaluated on the MRAMG benchmark under both unimodal and multimodal settings using the G-Eval metric. Results show that Caption Injection significantly outperforms text-only G-SEO baselines, achieving relative improvements of 1.85% and 1.09% in unimodal and multimodal scenarios, respectively. The study demonstrates the effectiveness of multimodal integration in enhancing content visibility within GSEs.

## Method Summary
Caption Injection is a prompt-engineering-based method that optimizes content for generative search engines by incorporating visual semantics. The method takes source text and images as input, processes them through three stages: (1) Structural Generation where a VLM extracts Object, Action, and Scene elements from images; (2) Alignment Refinement where an LLM rewrites captions to align with source text context; and (3) Semantic Injection where the refined caption is inserted into the optimal position within the source text. The method was evaluated using GLM-4-9B as the simulation backbone for GSE response generation, with Qwen-2.5-VL-7B used for caption generation when needed. The pipeline aims to enhance "subjective visibility" - the likelihood that content is synthesized into the final generated answer.

## Key Results
- Caption Injection outperforms text-only G-SEO baselines with 1.85% and 1.09% relative improvements in unimodal and multimodal scenarios respectively
- The method shows effectiveness across 6 datasets (Wit, Wiki, Web, Arxiv, Recipe, Manual) on the MRAMG benchmark
- Ablation studies confirm the necessity of each pipeline stage, with refined captions outperforming original VLM-generated captions
- Performance degrades on long-document datasets (Manual/Recipe), highlighting this as an open research problem

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Projection
Integrating visual semantics into textual content enhances the "subjective visibility" of content sources in GSEs more effectively than text-only optimization. The method projects visual features into natural language space via captioning, allowing the GSE's LLM to "perceive" and reason over visual information. This enriches semantic density of the source content, increasing the likelihood of citation or synthesis.

### Mechanism 2: Alignment-Driven Refinement
Raw image captions are insufficient for optimization; they must be refined to align with specific source text context. The two-step process (Structural Generation → Alignment Refinement) filters and rewrites captions to ensure they preserve syntactic structure suitable for LLM attention mechanisms while grounding them in document reality.

### Mechanism 3: Contextual Injection Strategy
The placement of visual-textual information matters; controlled insertion into text segments with high relevance maximizes impact. The method uses an LLM to identify optimal insertion points within existing text, maintaining fluency and coherence while introducing visual semantic cues exactly where contextually appropriate.

## Foundational Learning

- **Concept: Generative Search Engine Optimization (G-SEO)** - Why needed: This is the core problem space. Unlike traditional SEO (ranking links), G-SEO focuses on getting content *synthesized* into an AI-generated answer. Quick check: How does "subjective visibility" differ from "search ranking"? (Answer: Visibility refers to presence/influence of content in final generated text, not position on results page.)

- **Concept: Retrieval-Augmented Generation (RAG) & MRAG** - Why needed: The paper assumes the target system uses RAG architecture. The method specifically targets the "Generation" phase context window. Quick check: In the context of this paper, does the method optimize the *retriever* or the *generator*? (Answer: It optimizes the content source to be better utilized by the *generator* during synthesis.)

- **Concept: Vision-Language Models (VLMs) & Captioning** - Why needed: Stage 1 of the pipeline relies on VLMs to translate images into "Object, Action, Scene" schema. Quick check: Why can't we just pass the image directly to the search engine? (Answer: Many GSE workflows convert images to text internally anyway, so pre-aligning this text gives a control advantage.)

## Architecture Onboarding

- **Component map:** Source Text + Source Image → Structural Generator (VLM) → Alignment Refiner (LLM) → Semantic Injector (LLM) → Optimized Source
- **Critical path:** The **Alignment Refiner** is the most sensitive component. It requires a prompt that strictly enforces relevance, and if this fails, the injection creates hallucinations.
- **Design tradeoffs:** Automation vs. Quality (automated injection vs. quality control), Shallow vs. Deep Fusion (current "shallow mapping" vs. deeper feature fusion), Model dependency (relies on specific GLM-4-9B and Qwen-2.5-VL models).
- **Failure signatures:** Long-context dilution (fails on Manual dataset with avg 6,365 chars), Visual-Textual Mismatch (if image is decorative and unrelated to text).
- **First 3 experiments:**
  1. Reproduce the Ablation: Run pipeline on MRAMG-Arxiv subset comparing "Original Caption" vs. "Refined Caption" to verify lift from alignment stage.
  2. Stress Test Context Length: Apply method to "Manual" dataset to analyze exactly where optimization fails.
  3. Metric Sensitivity: Evaluate optimized content using G-Eval 2.0 but swap underlying Judge LLM to check robustness.

## Open Questions the Paper Calls Out

### Open Question 1
How can G-SEO methods be adapted to effectively optimize extremely long-form content (e.g., manuals averaging 6,365+ characters), where current methods show minimal or negative improvement? The paper explicitly identifies long-text optimization as an open research problem after noting all methods exhibited weakest improvements on MRAMG-Manual.

### Open Question 2
What deeper cross-modal feature fusion mechanisms can surpass the "shallow mapping level" of caption injection to achieve more unified multimodal semantic space for G-SEO? The authors state current integration remains at a relatively shallow mapping level and suggest future work should focus on deeper cross-modal feature fusion.

### Open Question 3
How can G-SEO methods account for and adapt to the inherent, unmodeled semantic preferences of black-box GSEs to improve optimization alignment? The authors identify the unmodeled GSE preference toward input data as a limitation and suggest exploring cross-model optimization strategies.

### Open Question 4
Why do VLM-generated captions occasionally outperform human-aligned refined captions in multimodal GSE settings, and what does this reveal about the misalignment between LLM semantic spaces and human-perceived meaning? From ablation study: VLM-generated captions occasionally outperform refined ones due to semantic alignment preference of the LLMs.

## Limitations

- Performance degrades significantly on long-document datasets (Manual/Recipe), suggesting the method may not generalize well to real-world scenarios where content is often lengthy.
- The method assumes images are semantically relevant to the source text, but doesn't address scenarios where images are purely decorative or unrelated to content.
- Results rely heavily on prompt engineering rather than model training, making them sensitive to specific prompt formulations that aren't fully specified in the text.

## Confidence

- **High Confidence**: The core mechanism of caption injection and the 3-stage pipeline design are well-explained and supported by ablation studies. The performance improvement over text-only baselines is measurable and consistent across multiple datasets.
- **Medium Confidence**: The effectiveness of the alignment refinement stage is demonstrated, but the specific prompts and LLM models used for this stage are not explicitly detailed, creating some uncertainty about reproducibility.
- **Low Confidence**: The generalization of results to real-world GSEs beyond the GLM-4-9B simulation, and the method's robustness to irrelevant or decorative images, remain unproven.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the prompts for the alignment refinement and semantic injection stages to quantify how sensitive performance is to prompt formulation.

2. **Cross-Model Evaluation**: Test the optimized content using different underlying LLM judges (not just GLM-4-9B) to verify that the "subjective visibility" gains are not artifacts of the specific evaluation model.

3. **Image Relevance Stress Test**: Evaluate the method on datasets where images have varying degrees of relevance to the source text to assess performance degradation when visual-textual alignment is weak.