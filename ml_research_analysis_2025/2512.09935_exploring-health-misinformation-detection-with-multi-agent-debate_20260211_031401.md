---
ver: rpa2
title: Exploring Health Misinformation Detection with Multi-Agent Debate
arxiv_id: '2512.09935'
source_url: https://arxiv.org/abs/2512.09935
tags:
- debate
- stage
- evidence
- agent
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses health misinformation detection by proposing
  a two-stage framework that combines automated agreement scoring with multi-agent
  debate. In the first stage, LLMs retrieve and evaluate evidence articles, computing
  an aggregated agreement score to classify claims.
---

# Exploring Health Misinformation Detection with Multi-Agent Debate

## Quick Facts
- arXiv ID: 2512.09935
- Source URL: https://arxiv.org/abs/2512.09935
- Authors: Chih-Han Chen; Chen-Han Tsai; Yu-Shao Peng
- Reference count: 7
- Key outcome: Two-stage framework combining evidence agreement scoring and multi-agent debate achieves +0.8 F1 on TREC-Health and +1.4 F1 on HealthFC, improving precision-recall balance over strong baselines.

## Executive Summary
This paper proposes a novel two-stage framework for health misinformation detection that leverages both automated evidence agreement scoring and multi-agent debate reasoning. The approach addresses the challenge of reliably identifying health misinformation by first aggregating evidence-based agreement scores, then engaging multiple agents in structured debate when evidence is inconclusive. Experiments across three health datasets demonstrate consistent improvements over existing methods, particularly in balancing precision and recall. The framework offers both improved detection accuracy and enhanced explainability through collaborative reasoning.

## Method Summary
The proposed framework operates in two stages. In stage one, LLMs retrieve relevant evidence articles and evaluate their agreement with the input claim, computing an aggregated agreement score for classification. When evidence is insufficient or contradictory, stage two activates multiple agents to engage in structured debate, reasoning through conflicting information to produce well-justified verdicts. This approach combines automated consistency checking with collaborative reasoning to enhance both reliability and explainability in health misinformation detection.

## Key Results
- Two-stage framework achieves +0.8 F1 score improvement on TREC-Health dataset
- Demonstrates +1.4 F1 score improvement on HealthFC dataset
- Shows better balance between precision and recall compared to strong baselines

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual approach: automated evidence agreement scoring provides reliable classification when strong evidence exists, while multi-agent debate enables deeper reasoning when evidence is conflicting or insufficient. By combining these complementary mechanisms, the system can handle both straightforward and complex misinformation scenarios more effectively than single-stage approaches.

## Foundational Learning
- Evidence Retrieval Systems: Critical for finding relevant articles to support or refute claims; quick check: evaluate retrieval precision and recall on domain-specific health content.
- Multi-Agent Debate: Enables structured reasoning over conflicting information; quick check: assess debate coherence and logical consistency of generated arguments.
- Agreement Scoring: Aggregates evidence consistency for reliable classification; quick check: verify scoring correlates with expert human judgments.
- LLM Reasoning Capabilities: Underpins both evidence evaluation and debate quality; quick check: benchmark reasoning depth and accuracy on medical knowledge.
- Precision-Recall Balance: Key metric for misinformation detection performance; quick check: analyze trade-offs across different decision thresholds.

## Architecture Onboarding

Component Map: Claim -> Evidence Retrieval -> Agreement Scoring -> (Debate Agents) -> Final Verdict

Critical Path: The critical path begins with claim input, proceeds through evidence retrieval and agreement scoring, and branches to multi-agent debate only when evidence is inconclusive. The final verdict combines both stages' outputs.

Design Tradeoffs: The framework trades computational complexity (multiple LLM calls for debate) for improved accuracy and explainability. It balances between automated scoring efficiency and collaborative reasoning depth, requiring careful threshold tuning to minimize debate invocation while maximizing detection reliability.

Failure Signatures: Primary failure modes include poor evidence retrieval leading to insufficient or irrelevant articles, LLM reasoning errors in complex medical contexts, and debate agents converging on incorrect conclusions due to biased initial positions or flawed reasoning chains.

First Experiments:
1. Benchmark evidence retrieval precision on domain-specific health content
2. Evaluate agreement scoring accuracy against expert human annotations
3. Test debate agent reasoning quality on simple verifiable health claims

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on evidence retrieval quality and domain coverage
- Multi-agent debate dependent on underlying LLM's medical reasoning capabilities
- Current focus on text-based claims may not generalize to multimodal health misinformation
- Computational complexity due to multiple LLM invocations

## Confidence

High Confidence:
- Overall framework architecture and precision-recall balance improvements

Medium Confidence:
- Absolute performance improvements may vary with different model configurations
- Real-world deployment effectiveness of collaborative reasoning

## Next Checks

1. Evaluate framework on additional health misinformation datasets not used in training
2. Conduct expert medical review of debate-generated verdicts
3. Perform ablation studies with different retrieval strategies to assess impact on detection performance