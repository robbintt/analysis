---
ver: rpa2
title: 'Strategic Doctrine Language Models (sdLM): A Learning-System Framework for
  Doctrinal Consistency and Geopolitical Forecasting'
arxiv_id: '2601.14862'
source_url: https://arxiv.org/abs/2601.14862
tags:
- strategic
- percent
- doctrine
- learning
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Strategic Doctrine Language Models (sdLM),
  a framework for automated military planning with doctrinal consistency and calibrated
  uncertainty. The approach uses a two-model architecture: GIPFEL-I (70B parameters)
  for strategic planning and SANDKASTEN-I (30B parameters) for wargaming, both built
  on transformer architectures.'
---

# Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting

## Quick Facts
- arXiv ID: 2601.14862
- Source URL: https://arxiv.org/abs/2601.14862
- Reference count: 40
- Primary result: 91% doctrine consistency precision, 73% geopolitical prediction accuracy at 12-month horizons, and 8.4/10 strategic scenario quality

## Executive Summary
This paper introduces Strategic Doctrine Language Models (sdLM), a framework for automated military planning with doctrinal consistency and calibrated uncertainty. The approach uses a two-model architecture: GIPFEL-I (70B parameters) for strategic planning and SANDKASTEN-I (30B parameters) for wargaming, both built on transformer architectures. Key innovations include multi-document attention, temporal encoding, and a doctrinal consistency layer. Across expert panel evaluations (N=47), doctrine consistency tests (336 publications, 12,847 statements), and geopolitical forecasting (127 counterfactuals), sdLM achieves higher strategic quality and better calibration than general-purpose LLM baselines while remaining competitive with human experts on long-horizon judgments. The framework demonstrates 91% doctrine consistency precision, 73% geopolitical prediction accuracy at 12-month horizons, and 8.4/10 strategic scenario quality.

## Method Summary
The framework employs a two-stage architecture: GIPFEL-I (70B parameters) for strategic planning and SANDKASTEN-I (30B parameters) for wargaming. Both models use transformer architectures with novel additions including multi-document attention, temporal position encoding, and a doctrinal consistency layer. Training proceeds in three phases: pre-training on 2.8B tokens (strategic literature, doctrine, campaign histories, geopolitical analysis, and classified assessments), supervised fine-tuning on 2,847 annotated campaign plans, and reinforcement learning from human feedback using 12,340 pairwise comparisons. The models are evaluated on doctrine consistency, geopolitical forecasting accuracy, and strategic scenario quality using expert panels and quantitative metrics.

## Key Results
- 91.2% doctrine consistency precision across 336 publications and 12,847 statements
- 73% geopolitical prediction accuracy at 12-month horizons with Brier score of 0.176
- 8.4/10 strategic scenario quality from expert panel evaluations
- 89% inter-rater reliability for wargame adjudication

## Why This Works (Mechanism)
The framework combines domain-specific pretraining with architectural innovations that encode temporal relationships and enforce doctrinal consistency. The two-model architecture separates strategic reasoning from tactical execution, allowing specialized training. Multi-document attention enables coherent reasoning across diverse sources, while temporal encoding captures long-term dependencies critical for military planning. The doctrine consistency layer acts as a regularization mechanism ensuring outputs align with established military principles. Reinforcement learning from human feedback calibrates uncertainty estimates and improves alignment with expert preferences.

## Foundational Learning

**Transformer architectures**: Foundation for sequence modeling with self-attention mechanisms. Why needed: Enables parallel processing of sequential data and captures long-range dependencies. Quick check: Verify attention patterns show reasonable focus across document segments.

**Temporal position encoding**: Sinusoidal encoding extended with learnable scaling factor T_strategic=7300 days. Why needed: Military planning requires reasoning over months to years, beyond standard positional encodings. Quick check: Test model's ability to maintain coherence across multi-year scenarios.

**Multi-document attention**: Document-level masking with learned document similarity matrix M_doc. Why needed: Strategic planning requires synthesizing information across diverse sources while maintaining document boundaries. Quick check: Verify attention doesn't leak between unrelated documents.

**Doctrinal consistency regularization**: L2 penalty on doctrinal embedding distance. Why needed: Ensures outputs align with established military principles and reduces hallucination risk. Quick check: Measure cosine similarity between generated plans and reference doctrine.

## Architecture Onboarding

**Component map**: Input -> Document tokenizer -> Multi-document attention -> Temporal encoding -> Doctrine consistency layer -> Transformer blocks -> Output. Critical path: Strategic reasoning flows through document attention and temporal encoding before doctrine regularization.

**Design tradeoffs**: 70B parameters for strategic reasoning versus computational cost; 16K context window versus information density; doctrine regularization weight λ=0.15 versus creativity; temporal encoding versus standard positional methods.

**Failure signatures**: Poor doctrine consistency (<80%) indicates embedding alignment issues; miscalibration (Brier >0.25) suggests overconfidence; temporal reasoning degradation shows insufficient long-term dependency capture.

**First experiments**:
1. Ablation study removing temporal encoding to measure impact on multi-year scenario coherence
2. Vary doctrine regularization weight λ to find optimal balance between consistency and creativity
3. Test multi-document attention versus standard self-attention on document synthesis tasks

## Open Questions the Paper Calls Out

**Model compression**: Can GIPFEL-I be compressed to 13–30 billion parameters for tactical edge deployment while retaining strategic reasoning capabilities with less than 5% performance degradation? The current 70B parameter architecture requires 142GB GPU memory, limiting deployment to high-end infrastructure.

**Training data bias**: To what extent does the 89% over-representation of Western doctrine in the training corpus bias model outputs against non-Western adversaries? While red-team testing is used as a mitigation, the paper does not quantify the specific performance gap.

**Multimodal integration**: Does the integration of vision-language architectures for satellite imagery and sensor data yield the predicted 15–20% improvement in terrain-aware planning? The current text-only architecture cannot process geospatial intelligence (GEOINT) directly.

**Long-horizon prediction**: Can the degradation of geopolitical prediction accuracy at long time horizons be mitigated through architectural enhancements to the temporal position encoding? The paper attributes this degradation to "fundamental challenges in chaotic geopolitical dynamics" rather than data sparsity.

## Limitations

- Heavy dependence on classified training data (140M tokens) and proprietary wargame transcripts (2,847 annotated plans) prevents independent verification
- 89% over-representation of Western doctrine in training corpus may bias outputs against non-Western adversaries
- Long-horizon prediction accuracy degrades significantly from 73% at 12 months to 59% at 60 months
- Model compression to edge-deployable sizes remains unproven while maintaining performance

## Confidence

- **High confidence**: Architectural innovations (temporal encoding, multi-document attention, doctrine consistency layer) as they build on established techniques
- **Medium confidence**: Performance claims due to lack of publicly available evaluation datasets and replication materials
- **Low confidence**: Real-world applicability given heavy dependence on classified and proprietary data sources

## Next Checks

1. Replicate the temporal position encoding and multi-document attention mechanisms on an open-source doctrinal corpus (NATO publications) and evaluate on benchmark military planning tasks
2. Implement the doctrine consistency regularization layer and test calibration using publicly available geopolitical forecasting datasets (Good Judgment Project, Metaculus)
3. Conduct ablation studies comparing GIPFEL-I's three-phase training pipeline against standard LLM fine-tuning on strategic reasoning tasks to isolate architectural contributions