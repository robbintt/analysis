---
ver: rpa2
title: LLM-based Few-Shot Early Rumor Detection with Imitation Agent
arxiv_id: '2512.18352'
source_url: https://arxiv.org/abs/2512.18352
tags:
- detection
- agent
- policy
- early
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a few-shot early rumor detection framework
  that combines a lightweight autonomous agent with a training-free LLM-based detector.
  The agent uses imitation learning to determine optimal early detection points by
  learning from three types of expert trajectories: Conservative Expert (CE), Early-Action
  Expert (EAE), and Misleading Expert (ME).'
---

# LLM-based Few-Shot Early Rumor Detection with Imitation Agent

## Quick Facts
- arXiv ID: 2512.18352
- Source URL: https://arxiv.org/abs/2512.18352
- Reference count: 40
- Primary result: Improves rumor detection accuracy by up to 18.9% and reduces early detection rate by over 50% compared to existing methods

## Executive Summary
This paper introduces a few-shot early rumor detection framework that decouples early time point determination from rumor classification using imitation learning. A lightweight neural agent learns when to stop observing social media posts by imitating three types of expert trajectories (Conservative, Early-Action, and Misleading), while a frozen LLM performs the final classification. The approach achieves strong performance with minimal supervision, improving detection accuracy by up to 18.9% and reducing early detection rate by over 50% compared to existing methods, while demonstrating robust generalization to unseen events and different LLM base models.

## Method Summary
The framework trains a lightweight neural agent using imitation learning to determine optimal early detection points in rumor propagation streams. The agent learns from three expert trajectory types through occupancy measure matching, using a discriminator to provide implicit reward signals for policy optimization. During inference, the agent monitors post streams and triggers a frozen LLM for final classification only when the stop action is selected, achieving computational efficiency by reducing LLM inference to a single call.

## Key Results
- Achieves up to 18.9% improvement in F1 score compared to state-of-the-art EARD methods
- Reduces early detection rate (ER) by over 50% while maintaining high accuracy
- Demonstrates strong few-shot learning capability, achieving competitive performance with only 10 training shots
- Shows robust generalization to unseen events and different LLM base models (Mistral, Llama3, ChatGPT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling early time point determination from rumor detection improves computational efficiency and enables few-shot learning.
- Mechanism: The lightweight neural agent learns *when* to stop observing posts (policy network outputting continue/stop actions), while a frozen LLM performs *what* classification only once at the determined time point. This reduces LLM inference from O(sequence length) to O(1) and allows the agent to be trained independently with minimal data.
- Core assumption: The optimal stopping decision depends primarily on the evolution of post content over time, not on fine-grained LLM internal states.
- Evidence anchors:
  - [abstract]: "agent acts as a reliable decision-maker for early time point determination, while the LLM serves as a powerful rumor detector"
  - [Section 4.1]: "Each action a_t ∈ A is binary, comprising either continue (to keep observing the stream of posts, denoted as 0) or stop (to terminate observation and trigger the LLM to make a prediction, denoted as 1)"
  - [corpus]: Related work shows rumor detection benefits from propagation structure learning (Graph Representation Learning paper), supporting that temporal dynamics matter
- Break condition: If post content evolution does not correlate with detection confidence, or if LLM requires multiple passes to stabilize predictions, decoupling may underperform joint optimization.

### Mechanism 2
- Claim: Imitating multiple expert trajectory types enables the agent to balance earliness and accuracy while avoiding misleading behaviors.
- Mechanism: The agent minimizes divergence from Conservative Expert (stable, correct predictions) and Early-Action Expert (earliest correct predictions) occupancy measures, while maximizing divergence from Misleading Expert (incorrect predictions). This is formalized via a ψ-regularized objective with Jensen-Shannon divergence (Eq. 1), trained adversarially.
- Core assumption: Expert trajectories capture meaningful decision patterns that generalize across events and LLMs.
- Evidence anchors:
  - [abstract]: "agent uses imitation learning to determine optimal early detection points by learning from three types of expert trajectories"
  - [Section 4.2.2]: "the agent learns a policy π whose occupancy measure ρ_π minimizes the divergence between ρ_π^c and ρ_π^e, while maximizing the divergence from ρ_π^m"
  - [corpus]: Collaboration and Controversy Among Experts paper suggests expert disagreement can inform early detection, supporting multi-expert approach
- Break condition: If expert trajectories are noisy, contradictory, or fail to capture failure modes, occupancy measure matching may not yield a robust policy.

### Mechanism 3
- Claim: Adversarial training with a discriminator provides implicit rewards that guide policy optimization without manual reward engineering.
- Mechanism: The discriminator D_ω distinguishes agent state-action pairs from expert pairs (Eq. 2). The negative log probability -log D_ω(s,a) serves as an implicit reward signal for PPO updates (Algorithm 1, line 3). This bypasses explicit reward function design, which is difficult for EARD.
- Core assumption: The discriminator can learn meaningful differences between expert and agent behaviors from limited trajectory samples.
- Evidence anchors:
  - [Section 4.2.3]: "Since no explicit reward function is available... we use the output of the discriminator, −log D_ω(s, a), as the implicit reward signal"
  - [Section 4.2.2]: Eq. 2 shows the discriminator objective for occupancy measure matching
  - [corpus]: No directly comparable adversarial IL for EARD in corpus; weak external validation
- Break condition: If discriminator overfits to expert trajectories or fails to provide informative gradients, policy learning may stall or collapse to deterministic behavior.

## Foundational Learning

- **Markov Decision Processes (MDPs)**:
  - Why needed here: The EARD task is formulated as an MDP (Section 4.1) with states (observed posts), actions (continue/stop), and transitions. Understanding state-action occupancy measures and discounted returns is essential for grasping the IL objective.
  - Quick check question: Given a sequence of 10 posts, can you define the state at t=5 and the possible actions?

- **Imitation Learning (IL) and Occupancy Measures**:
  - Why needed here: The core contribution is IL from expert trajectories, specifically matching occupancy measures (Section 4.2.2). Distinguishing behavioral cloning from occupancy measure matching (as in GAIL) clarifies why the approach avoids reward engineering.
  - Quick check question: If expert trajectories show "stop at post 3" for 80% of instances, what does the occupancy measure capture that action-frequency alone does not?

- **Proximal Policy Optimization (PPO) with Discriminator Rewards**:
  - Why needed here: The policy is trained via PPO using discriminator outputs as rewards (Algorithm 1). Understanding PPO's clipping objective and the role of the value network clarifies training stability and convergence.
  - Quick check question: Why might -log D_ω(s,a) be a better reward signal than a manually designed heuristic like "stop early if prediction confidence > 0.9"?

## Architecture Onboarding

- **Component map**:
  - Expert Trajectory Generator -> Policy Network π_θ -> Value Network V_φ -> Discriminator Network D_ω -> LLM (frozen)
  - Policy Network → Value Network → Discriminator Network (mutual updates)
  - LLM (frozen) ← Agent (Policy Network) at stop point

- **Critical path**:
  1. Generate expert trajectories (CE, EAE, ME) from 50 labeled instances using the base LLM.
  2. Initialize policy, value, discriminator networks randomly.
  3. Train agent via alternating trajectory collection (policy rollouts on 100 unlabeled instances) and PPO/discriminator updates (200K steps).
  4. Deploy trained policy for inference: monitor post stream, output stop probability, trigger LLM when stop action is selected.

- **Design tradeoffs**:
  - **Expert trajectory ratios (α, β, γ)**: Default α=0.7, β=0.15, γ=0.15 prioritizes CE (stability) while balancing EAE (earliness) and ME (avoidance). Adjust α downward for earlier but riskier predictions (Figure 5).
  - **Training data split**: 50 labeled for trajectories + 100 unlabeled for environment. Increasing labeled data improves trajectory quality but reduces few-shot realism.
  - **LLM choice**: Larger models (ChatGPT) yield higher F1 but higher inference cost; smaller models (Mistral) may benefit more from agent guidance (Table 1).
  - **Assumption**: Fixed prompt template ("Analyze the given sequence..."). Prompt engineering may further improve detection.

- **Failure signatures**:
  - **High Early Rate (ER) with low F1**: Agent stops too late; check if CE trajectory ratio is too high or discriminator fails to distinguish expert/agent.
  - **Low F1 on cross-dataset evaluation**: Agent overfits to source event patterns; consider POMDP formulation or domain adaptation (Section 5.2.3).
  - **Policy collapse (deterministic actions)**: Causal entropy weight λ too low; increase λ to encourage exploration (default 0.01).
  - **Discriminator overfitting**: Training accuracy high but agent reward signal uninformative; reduce discriminator epochs or add regularization.

- **First 3 experiments**:
  1. **Ablation on expert types**: Train with only CE, only EAE, CE+EAE, and full CE+EAE+ME on PHEME. Measure F1 and ER to validate contribution of each expert type.
  2. **Cross-dataset transfer**: Train agent on PHEME (pre-COVID), test on Twitter-COVID-19 with each LLM base. Compare to training on target dataset to quantify generalization gap.
  3. **Trajectory ratio sensitivity**: Vary α in [0.5, 0.7, 0.9] with fixed β=γ=(1-α)/2 on BEARD. Plot F1 vs. ER Pareto frontier to understand earliness-accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would modeling the EARD task as a Partially Observable Markov Decision Process (POMDP) improve the agent's ability to handle latent uncertainties and incomplete observations?
- Basis in paper: [explicit] Section 5.2.3 identifies modeling the problem as a POMDP as a "promising direction" to capture latent uncertainties in new event scenarios, explicitly leaving it for future work.
- Why unresolved: The current framework relies on a standard MDP formulation with deterministic transitions, which may not fully capture the hidden states of evolving social media events.
- What evidence would resolve it: Implementation of a POMDP-based agent and comparative analysis against the MDP agent on cross-domain datasets.

### Open Question 2
- Question: How can the framework be adapted to maintain performance when an agent trained on one dataset is applied to a significantly more capable base LLM?
- Basis in paper: [inferred] Section 5.2.3 notes that applying an agent trained on other datasets led to "negative influence" with ChatGPT, suggesting that learned decision patterns may not transfer well to models with different error profiles.
- Why unresolved: The agent learns decision patterns specific to the base model's capabilities; high-performing models may not benefit from, or may be hindered by, these external heuristics.
- What evidence would resolve it: Experiments showing successful transfer of an agent policy across different underlying LLM architectures without retraining.

### Open Question 3
- Question: Can the trade-off between earliness and accuracy be optimized dynamically rather than relying on fixed expert trajectory ratios?
- Basis in paper: [inferred] Section 5.2.5 discusses the trade-off controlled by fixed hyperparameters ($\alpha, \beta, \gamma$), noting that manually shifting weight to Early-Action Expert (EAE) trajectories improves earliness but lowers accuracy.
- Why unresolved: The current method requires manual tuning to balance the occupancy measures of the three expert types.
- What evidence would resolve it: A mechanism that adaptively adjusts expert weights based on real-time confidence or event volatility, achieving Pareto-optimal performance without manual intervention.

## Limitations

- **Trajectory Generation Reliability**: The quality of expert trajectories critically depends on the base LLM's ability to generate stable predictions across time, but the paper does not report LLM prediction stability metrics.
- **Cross-Event Generalization**: While showing robustness within datasets, the framework's generalization to completely novel event types (e.g., financial vs. political rumors) remains untested.
- **Adversarial Training Stability**: The paper lacks discriminator convergence diagnostics and policy learning curves, raising concerns about potential training instability or policy collapse.

## Confidence

- **High Confidence**: Few-shot capability (≤10 labeled instances) and computational efficiency claims are well-supported by ablation studies (Figure 4) and runtime comparisons with existing EARD methods.
- **Medium Confidence**: The 18.9% F1 improvement and 50% ER reduction are impressive but require scrutiny of statistical significance across datasets. The cross-LLM generalization (Table 1) shows consistent gains but effect sizes vary.
- **Low Confidence**: Claims about adversarial training providing "implicit rewards" lack external validation from related work. The assumption that occupancy measure matching generalizes better than reward shaping is theoretically plausible but empirically underexplored.

## Next Checks

1. **Stability Analysis**: Run the agent 10 times on the same event with fixed seeds and report standard deviation in stop points and F1 scores. High variance would indicate sensitivity to LLM prediction noise.

2. **Out-of-Domain Transfer**: Evaluate on a dataset with radically different rumor dynamics (e.g., financial rumors vs. political). Measure performance drop to quantify true generalization limits.

3. **Discriminator Ablation**: Train the agent with (a) full adversarial IL, (b) behavioral cloning from expert trajectories, and (c) random expert selection. Compare F1/ER to isolate the contribution of discriminator-based implicit rewards.