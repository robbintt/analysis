---
ver: rpa2
title: Effective and Efficient Schema-aware Information Extraction Using On-Device
  Large Language Models
arxiv_id: '2505.14992'
source_url: https://arxiv.org/abs/2505.14992
tags:
- extraction
- schema
- information
- dlisc
- schemas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual-LoRA with Incremental Schema Caching
  (DLISC), a two-stage on-device information extraction framework that uses specialized
  LoRA adapters for schema identification and extraction, along with caching to reduce
  redundant computation. DLISC significantly outperforms RAG-based baselines on CrossNERAI
  and DuEE-Fin datasets, achieving F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B)
  on CrossNERAI, and maintaining high precision and F1 in both schema identification
  and extraction on DuEE-Fin.
---

# Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models

## Quick Facts
- **arXiv ID:** 2505.14992
- **Source URL:** https://arxiv.org/abs/2505.14992
- **Reference count:** 5
- **Primary result:** Dual-LoRA with Incremental Schema Caching (DLISC) achieves F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) on CrossNER_AI, outperforming RAG baselines by 8.4-17.8% absolute F1.

## Executive Summary
This paper introduces DLISC, a two-stage on-device information extraction framework that uses specialized LoRA adapters for schema identification and extraction, combined with caching to reduce redundant computation. The framework achieves significant improvements in both effectiveness (F1 scores) and efficiency (latency reduction) compared to RAG-based baselines across multiple on-device LLMs. DLISC demonstrates that decomposing schema-aware extraction into specialized modules with caching mechanisms can substantially improve performance while maintaining practical deployment feasibility on resource-constrained devices.

## Method Summary
DLISC employs a Dual-LoRA architecture where two separate LoRA modules are fine-tuned for schema identification and extraction tasks. The identification LoRA (θI) retrieves relevant schemas for a given query, while the extraction LoRA (θE) performs information extraction using the identified schemas. These modules are merged with a frozen base LLM during inference. The framework incorporates Incremental Schema Caching to store and reuse previously computed KV representations of schemas, reducing redundant computation. This two-stage approach with caching is designed specifically for on-device deployment, addressing the challenges of hallucinations, context limits, and latency in schema-aware information extraction.

## Key Results
- DLISC achieves F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) on CrossNER_AI, outperforming RAG baselines by 8.4-17.8% absolute F1.
- On DuEE-Fin dataset, DLISC maintains high precision (0.2982) in identification and F1 (0.7746) in extraction with TinyLlama-1.1B.
- Incremental Schema Caching reduces latency from 1.38-6.02 seconds to 0.66-4.59 seconds per sample (52% reduction for Llama-3.2-1B, 24% for Qwen2.5-3B).

## Why This Works (Mechanism)

### Mechanism 1: Dual-LoRA Task Decomposition
- **Claim:** Decomposing information extraction into separate identification and extraction stages with specialized LoRA modules improves extraction accuracy compared to single-stage RAG-based approaches.
- **Mechanism:** Two LoRA adapters (θI, θE) are independently optimized for schema identification and schema-aware extraction, then merged with the frozen base LLM (θ) during inference via `θ'I = Merge(θI, θ)` and `θ'E = Merge(θE, θ)`. Each module specializes without interference.
- **Core assumption:** The identification and extraction tasks have sufficiently different optimization landscapes that separate parameter sets outperform shared parameters.
- **Evidence anchors:**
  - [abstract]: "DLISC adopts an Identification LoRA module for retrieving the most relevant schemas to a given query, and an Extraction LoRA module for performing information extraction based on the previously selected schemas."
  - [section]: Table 3 shows DLISC achieves highest Precision (0.2982) in identification and highest F1 (0.7746) in extraction compared to all RAG baselines on DuEE-Fin with TinyLlama.
  - [corpus]: Related work (Liang et al., 2025 - ASEE, arXiv:2505.08690) uses similar two-stage RAG decomposition, supporting but not proving LoRA-specific benefits.
- **Break condition:** If schemas are highly overlapping or identification becomes near-trivial, maintaining two LoRA modules adds overhead without meaningful specialization gains.

### Mechanism 2: Incremental Schema Caching for KV Reuse
- **Claim:** Caching previously computed schema representations reduces redundant attention computation and accelerates inference without sacrificing extraction quality.
- **Mechanism:** When a matched schema S is identified, the system checks a schema cache pool. If cached, pre-computed KV representations are reused. If not, inference proceeds normally and the computed cache is stored. The extraction uses `R = θ'E(ME(Cache) + S(Cache) + Q)`.
- **Core assumption:** Schema reuse is sufficiently frequent in the target workload that cache hits provide meaningful latency reduction.
- **Evidence anchors:**
  - [abstract]: "Incremental Schema Caching is incorporated to reduce redundant computation, substantially improving efficiency."
  - [section]: Table 2 shows latency reduction from 1.38–6.02s to 0.66–4.59s per sample (52% reduction for Llama-3.2-1B, 24% for Qwen2.5-3B).
  - [corpus]: Weak corpus evidence—no corpus papers directly validate ISC for schema caching. KV-cache optimization (Luohe et al., 2024) and Prompt Cache (Gim et al., 2024) are cited as inspiration but not empirically validated for this specific use case.
- **Break condition:** If the workload has near-zero schema reuse (e.g., every query requires unique schemas), caching overhead adds latency without benefit.

### Mechanism 3: LoRA-Based Schema Identification vs. External Retrieval
- **Claim:** Using a LoRA-adapted LLM for schema identification outperforms external retrieval methods (BM25, embedding-based rerankers) by leveraging the LLM's task-specific semantic understanding.
- **Mechanism:** The Identification LoRA (θI) is fine-tuned to map queries to relevant schemas using learned internal representations, rather than relying on general-purpose similarity metrics that may not capture extraction-specific relevance.
- **Core assumption:** The base LLM has sufficient capacity and the training data adequately covers schema-query relationships that general retrievers miss.
- **Evidence anchors:**
  - [abstract]: "Extensive experiments across multiple information extraction datasets demonstrate notable improvements in both effectiveness and efficiency."
  - [section]: Table 1 shows DLISC achieves F1 of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) vs. 0.2341–0.3968 for all RAG baselines on CrossNER_AI.
  - [corpus]: Mixed corpus evidence—corpus papers (e.g., PARSE, arXiv:2510.08623) focus on schema optimization for extraction, not direct comparison of LoRA-based vs. retrieval-based schema identification.
- **Break condition:** If identification training data is insufficient or schemas are highly diverse with sparse coverage, θI may overfit to seen patterns and fail to generalize.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The entire DLISC architecture depends on understanding how LoRA adapters enable parameter-efficient fine-tuning by adding small trainable low-rank matrices to frozen LLM weights.
  - Quick check question: Can you explain why LoRA allows multiple task-specific adapters to share a single base LLM at inference time?

- **Concept: KV-Cache in Transformer Inference**
  - Why needed here: Incremental Schema Caching extends the KV-cache concept; understanding how attention keys/values are computed, stored, and reused across tokens is essential.
  - Quick check question: What computational savings occur when you reuse KV-cache from a shared prompt prefix across multiple requests?

- **Concept: Schema-Aware Information Extraction**
  - Why needed here: The paper's core task is extracting structured entities/relations guided by predefined schemas; schemas act as extraction templates that constrain output format.
  - Quick check question: How does providing an explicit schema to an LLM change its extraction behavior compared to open-ended extraction without constraints?

## Architecture Onboarding

- **Component map:** Base LLM (θ) <- Identification LoRA (θI) -> Schema Cache Pool <- Extraction LoRA (θE) <- Meta Prompts (MI, ME)

- **Critical path:**
  1. Query Q → θ'I + MI → Matched Schemas S
  2. Check S against Schema Cache Pool (hit/miss)
  3. If miss: θ'E + ME + S + Q → Result R, then cache S
  4. If hit: θ'E + ME(cached) + S(cached) + Q → Result R (faster)

- **Design tradeoffs:**
  - Two-stage pipeline vs. end-to-end: Modularity vs. added latency
  - Cache pool size vs. memory: Larger cache improves hit rate but consumes edge-device memory
  - LoRA rank: Higher rank = more capacity but larger adapter files

- **Failure signatures:**
  - Low cache hit rate: Schema diversity exceeds cache capacity; consider cache eviction policy
  - Identification precision drops: θI undertrained or schema definitions ambiguous
  - Extraction hallucinations: θE lacks task-specific tuning; schemas underspecified

- **First 3 experiments:**
  1. Reproduce Table 1: Compare DLISC vs. BM25/BGE-Reranker/LLM-Embedder on CrossNER_AI with Llama-3.2-1B to validate baseline F1 improvements.
  2. Ablate caching: Measure per-sample latency with and without Incremental Schema Caching on a held-out test set to quantify efficiency gains.
  3. Vary cache size: Test different schema cache pool sizes (e.g., 10, 50, 100 schemas) to identify the saturation point for your target workload.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DLISC framework generalize to other parameter-efficient fine-tuning methods, such as Prefix Tuning, Series Adapters, or Parallel Adapters?
- Basis in paper: [explicit] The "Limitations" section states the method currently employs only LoRA adapters, leaving the generalization to other adapter types unexplored.
- Why unresolved: The experimental design and architectural implementation are restricted to the LoRA paradigm, making performance with other tuning strategies unknown.
- What evidence would resolve it: Comparative experiments implementing the dual-stage approach using alternative adapter architectures against the LoRA baseline.

### Open Question 2
- Question: How does DLISC perform in terms of memory footprint and energy consumption when deployed on actual physical edge devices?
- Basis in paper: [explicit] The authors acknowledge in the "Limitations" section that they were "unable to deploy on real edge devices due to computational resource constraints."
- Why unresolved: Current results report latency (seconds/sample), but real-world edge deployment involves hardware constraints like thermal throttling and strict RAM limits not captured in the current setup.
- What evidence would resolve it: Benchmarking results on physical mobile hardware (e.g., smartphones) measuring peak memory usage and battery drain during inference.

### Open Question 3
- Question: Can the current architecture maintain effectiveness and efficiency when applied to complex multilingual and cross-lingual information extraction scenarios?
- Basis in paper: [explicit] The "Limitations" section notes the implementation "lacks support for more complex multilingual and cross-lingual scenarios."
- Why unresolved: Schema caching and identification may face different challenges when aligning schemas across languages, potentially negating the efficiency gains seen in monolingual tests.
- What evidence would resolve it: Evaluation of the model on standard multilingual IE datasets, reporting F1 scores and latency for cross-lingual tasks.

## Limitations

- The paper lacks validation of Incremental Schema Caching with cache hit rate data, making it unclear how often schema reuse actually occurs in real workloads.
- The evaluation does not include comparisons with state-of-the-art schema-aware IE methods from recent literature (e.g., PARSE, ASEE), limiting confidence in the claimed superiority over RAG baselines.
- The framework has not been deployed on actual physical edge devices, leaving questions about real-world memory footprint, energy consumption, and performance under hardware constraints unanswered.

## Confidence

**High Confidence:** The effectiveness of LoRA-based fine-tuning for schema-aware tasks is well-established in the literature. The Dual-LoRA decomposition strategy (separate identification and extraction modules) is theoretically sound and aligns with established practices in multi-stage information extraction systems.

**Medium Confidence:** The reported F1 improvements over RAG baselines are credible given the methodology, but the lack of comparison with state-of-the-art schema-aware IE methods from recent literature (e.g., PARSE, ASEE) limits confidence in the claimed superiority. The latency reduction claims are supported by direct measurements but lack context about typical cache hit rates.

**Low Confidence:** The Incremental Schema Caching mechanism's real-world effectiveness is uncertain without cache hit rate data or validation across diverse workloads. The paper does not address potential edge cases such as schema conflicts, cache eviction strategies, or memory constraints on actual on-device deployment scenarios.

## Next Checks

1. **Reproduce Table 1 results** comparing DLISC vs. RAG baselines on CrossNER_AI with Llama-3.2-1B to validate the reported F1 improvements (0.4179 vs. 0.2341-0.3968).

2. **Measure cache hit rates** across the DuEE-Fin test set to quantify actual reuse frequency and validate that the reported latency reductions are attributable to Incremental Schema Caching rather than other factors.

3. **Compare against recent schema-aware IE methods** (e.g., PARSE, ASEE) to establish whether Dual-LoRA's improvements are competitive with the current state-of-the-art in schema-aware information extraction.