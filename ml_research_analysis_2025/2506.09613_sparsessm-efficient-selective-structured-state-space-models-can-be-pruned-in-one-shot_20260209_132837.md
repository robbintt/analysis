---
ver: rpa2
title: 'SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned
  in One-Shot'
arxiv_id: '2506.09613'
source_url: https://arxiv.org/abs/2506.09613
tags:
- pruning
- mamba
- sparsessm
- sparsity
- sparsegpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SparseSSM, the first training-free pruning
  framework for selective state-space models like Mamba. The method extends the optimal
  brain surgeon (OBS) framework by deriving an approximate second-order saliency score
  that aggregates Hessian-trace information across time steps for the time-shared
  state-transition matrix, incorporating component sensitivity analysis for feed-forward
  network pruning, and enabling extension to semi-structured and structured sparsity.
---

# SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot

## Quick Facts
- arXiv ID: 2506.09613
- Source URL: https://arxiv.org/abs/2506.09613
- Reference count: 40
- Primary result: 50% sparsity without fine-tuning, maintaining zero-shot accuracy

## Executive Summary
SparseSSM introduces the first training-free pruning framework for selective state-space models like Mamba. The method extends the optimal brain surgeon (OBS) framework by deriving an approximate second-order saliency score that aggregates Hessian-trace information across time steps for the time-shared state-transition matrix, incorporating component sensitivity analysis for feed-forward network pruning, and enabling extension to semi-structured and structured sparsity. Experiments show that SparseSSM can prune 50% of SSM weights without fine-tuning and achieves state-of-the-art performance, maintaining zero-shot accuracy on multiple benchmarks.

## Method Summary
SparseSSM is a one-shot, training-free pruning framework for Mamba models that operates through three key phases: (1) during a forward pass, it accumulates hidden-state statistics across time steps, (2) it computes per-timestep importance scores using a diagonal Hessian approximation (A²_log × Σh²) and aggregates pruning decisions via frequency-based mask selection, and (3) it applies sensitivity-aware sparsity allocation to FFN layers by ranking modules based on Hessian trace and assigning lower sparsity to higher-sensitivity components. The method targets both the SSM module's time-shared state-transition matrix and feed-forward network components, enabling up to 50% sparsity without fine-tuning while maintaining accuracy.

## Key Results
- Prunes 50% of SSM weights without fine-tuning, maintaining zero-shot accuracy on PIQA, OpenBookQA, Winogrande, and ARC benchmarks
- Achieves 1.72× SSM speedup with column-wise structured pruning at 50% sparsity
- Outperforms MP and SparseGPT baselines by 4× in perplexity when pruning whole model at 50% sparsity

## Why This Works (Mechanism)

### Mechanism 1: Approximate Second-Order Saliency for Time-Shared SSM Parameters
The method adapts OBS by deriving a closed-form Hessian approximation for diagonal SSM matrices. Under the diagonal assumption, the per-parameter importance reduces to I ∝ A² × Σh² across batches and time steps (Theorem 1). This captures how each state dimension amplifies error through recurrent multiplication without computing full Hessians.

### Mechanism 2: Hierarchical Mask Aggregation via Pruning Frequency
Instead of averaging importance scores, SparseSSM counts how often each parameter falls in the bottom-K candidates across all time steps, then selects the most frequently flagged indices. This deferred-commitment approach prevents early, irreversible decisions that would conflict with later timesteps.

### Mechanism 3: Sensitivity-Aware Sparsity Allocation for FFN Layers
in_proj and out_proj exhibit higher Hessian trace and larger reconstruction error. The sparsity formula (Eq. 7) assigns lower sparsity to higher-sensitivity modules by ranking modules and allocating within an allowable deviation interval [1−p−α, 1−p+α].

## Foundational Learning

- **Concept**: Optimal Brain Surgeon (OBS) framework
  - Why needed: SparseSSM is built on OBS principles (second-order Taylor expansion, Hessian-based saliency ε = w²/H_mm). Understanding OBS clarifies why diagonal Hessian approximation matters.
  - Quick check: Given a parameter with weight 0.1 and diagonal Hessian entry 10, versus weight 0.5 and Hessian 100, which would OBS prune first?

- **Concept**: Selective State Space Models (SSMs) and discretization
  - Why needed: Mamba's A is time-shared and discretized via exp(δA). Pruning decisions must account for this recurrence; standard layer-wise methods assume feedforward independence.
  - Quick check: If A were not time-shared (independent per timestep), would mask aggregation still be necessary?

- **Concept**: Hessian-trace as sensitivity proxy
  - Why needed: The method uses trace(H) to rank module sensitivity. This connects curvature to pruning tolerance.
  - Quick check: Would a module with near-zero Hessian trace be more or less safe to prune aggressively?

## Architecture Onboarding

- **Component map**: SSM module contains A_log (diagonal, time-shared), δ (stride), B, C; FFN branch: conv1d → in_proj → (x_proj → dt_proj produces δ, B, C) → selective scan → out_proj

- **Critical path**: (1) Forward pass accumulates h² statistics, (2) Per-timestep importance computed via A²_log ⊙ S, (3) Frequency-based mask selection finalizes A_log pruning, (4) FFN layers pruned with SparseGPT backbone + sensitivity-adjusted sparsity

- **Design tradeoffs**: Calibration samples (N_sample): 64 balances quality vs. time; Sensitivity width (α): 0.04 used; Sparsity: 50% yields near-zero degradation on zero-shot for SSM-only

- **Failure signatures**: Perplexity explodes (>1000) → incorrect mask aggregation; SparseGPT collapse on SSM → naive OBS without time-sharing adaptation; in_proj pruning causes disproportionate drop → over-pruning high-sensitivity module

- **First 3 experiments**: (1) Reproduce SSM-only pruning on Mamba-370M at 50% sparsity; (2) Ablate mask aggregation: frequency-based vs. L2 pooling; (3) Validate structured extension: column-wise pruning on A_log's second axis at 50% sparsity

## Open Questions the Paper Calls Out

### Open Question 1
Can the SparseSSM framework be extended to structured pruning of the entire Mamba architecture to achieve substantial end-to-end inference acceleration? The current method successfully prunes the SSM module but lacks a comprehensive strategy to structurally prune all components in a way that translates to significant wall-clock speedup for the full model.

### Open Question 2
Does the derived Hessian-trace saliency score generalize effectively to other selective state-space architectures or time-varying models beyond Mamba? The theoretical derivation and empirical validation are tailored specifically to Mamba's discretization and diagonal structure.

### Open Question 3
What specific hardware-aware optimizations are required to fully exploit the sparsity patterns generated by SparseSSM during inference? While the paper achieves theoretical parameter reduction, it does not define the specific kernels or hardware mappings needed to realize these gains on GPUs or specialized accelerators.

## Limitations
- The diagonal Hessian approximation assumes ∂L/∂h remains approximately constant across time steps, which may break down in deeper models or those with complex long-range dependencies
- Frequency-based mask aggregation lacks theoretical grounding and may fail when critical parameters are active only at rare timesteps
- Sensitivity analysis relies on Hessian-trace as a proxy measured only during calibration, which may not generalize to diverse deployment scenarios

## Confidence
- **High confidence**: SSM-only pruning at 50% sparsity maintaining zero-shot accuracy (validated by direct comparison with MP and SparseGPT baselines in Table 1)
- **Medium confidence**: Structured sparsity extension (1.72× speedup) and frequency-based mask aggregation (4× perplexity reduction vs L2 pooling)
- **Low confidence**: Generalizability of sensitivity-aware FFN pruning across diverse tasks and model scales

## Next Checks
1. Test diagonal Hessian approximation limits by varying hidden-state magnitude uniformity across dimensions and verifying importance score discriminative power
2. Validate frequency aggregation robustness by creating synthetic pruning scenarios where critical parameters are active only at rare timesteps
3. Apply the same sensitivity analysis framework to vision SSM models (e.g., MambaViT) to determine if Hessian-trace remains a reliable proxy for pruning tolerance across modalities