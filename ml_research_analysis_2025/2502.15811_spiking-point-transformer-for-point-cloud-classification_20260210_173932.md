---
ver: rpa2
title: Spiking Point Transformer for Point Cloud Classification
arxiv_id: '2502.15811'
source_url: https://arxiv.org/abs/2502.15811
tags:
- point
- spiking
- time
- points
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Spiking Point Transformer (SPT), the first
  transformer-based spiking neural network for 3D point cloud classification. SPT
  integrates a Queue-Driven Sampling Direct Encoding (Q-SDE) method that reduces computational
  costs while retaining key support points, and a Hybrid Dynamics Integrate-and-Fire
  Neuron (HD-IF) that combines multiple neural dynamic models to simulate selective
  neuron activation.
---

# Spiking Point Transformer for Point Cloud Classification

## Quick Facts
- arXiv ID: 2502.15811
- Source URL: https://arxiv.org/abs/2502.15811
- Reference count: 7
- Primary result: First transformer-based SNN for 3D point cloud classification achieving 91.43% OA on ModelNet40

## Executive Summary
This paper introduces the Spiking Point Transformer (SPT), the first transformer-based spiking neural network for 3D point cloud classification. SPT integrates three key innovations: Queue-Driven Sampling Direct Encoding (Q-SDE) to reduce computational costs while retaining key support points, a Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF) that combines multiple neural dynamic models to simulate selective neuron activation, and spiking local self-attention that replaces MAC-heavy transformer operations with spike-driven accumulate operations. The method achieves state-of-the-art performance on ModelNet40 (91.43% OA) and ScanObjectNN (78.03% OA) datasets in the SNN domain while consuming at least 6.4× less energy than its ANN counterpart.

## Method Summary
SPT builds on Point Transformer architecture but replaces all non-spiking components with spiking equivalents. The input point cloud (N=1024) passes through Q-SDE encoding, which maintains Ns support points while progressively dequeueing subsets across T time steps to reduce redundancy. HD-IF neurons integrate four dynamic models (IF, LIF, EIF, PLIF) with a gating network that learns to selectively activate different neuron types based on spatial-temporal context. Spiking local self-attention computes Query, Key, Value in spike form using KNN neighborhoods, enabling binary spike accumulation instead of floating-point multiplication. The architecture cascades multiple Spiking Point Transformer Blocks (SPTB) and Spiking Transition Down Blocks (STDB) for hierarchical feature extraction, followed by classification heads.

## Key Results
- Achieves 91.43% OA on ModelNet40 (SOTA for SNNs on 3D point cloud classification)
- Achieves 78.03% OA on ScanObjectNN (real-world scans)
- Consumes at least 6.4× less energy than ANN counterpart (13.3mJ vs 84.7mJ)
- Q-SDE reduces training memory from 15.3G to 9.7G and inference memory from 9.3G to 5.2G

## Why This Works (Mechanism)

### Mechanism 1: Queue-Driven Sampling Direct Encoding (Q-SDE)
Q-SDE reduces memory and compute costs by distributing point coverage across time steps rather than naively replicating the full point cloud. Instead of repeating N points across T time steps (computational cost O(T×N)), Q-SDE maintains Ns "support points" while progressively dequeueing/dequeueing subsets of Np points. This exploits the sparsity of point clouds—each time step differs by only Np points, enabling neurons to leverage temporal dynamics with reduced redundancy.

### Mechanism 2: Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF)
HD-IF improves accuracy by learning to selectively activate among multiple neuron dynamic models (IF, LIF, EIF, PLIF) based on spatial-temporal context. A gating network computes weights for each neuron type's membrane potential contribution. During training, dense weighted fusion allows gradient flow through all models. During inference, only Top-2 models are activated, reducing compute while preserving adaptive dynamics.

### Mechanism 3: Spiking Local Self-Attention
Spiking local self-attention replaces MAC-heavy transformer attention with spike-driven accumulate operations. SPTB computes spike-form Query, Key, Value via KNN-local neighborhoods. Binary spikes enable AC instead of MAC (0.9pJ vs. 4.6pJ per 32-bit operation). Relative position encoding (δ) is preserved in spike form. HD-IF pre-processes membrane potentials before attention.

## Foundational Learning

- **Spiking Neuron Dynamics (IF/LIF/EIF/PLIF)**: HD-IF explicitly fuses these models; understanding membrane potential integration, leakage, exponential terms, and learnable time constants is prerequisite to diagnosing why one neuron type activates over another. Quick check: Given an input current of 0.4 and threshold 0.5, which neuron type reaches threshold fastest: IF (no leak) or LIF (with leak)?

- **Point Transformer Local Self-Attention**: SPT builds directly on Point Transformer's KNN-based local attention; understanding how vector attention and relative position encoding work is essential before grasping their spiking adaptation. Quick check: Why does Point Transformer use KNN neighborhoods rather than global attention for point clouds?

- **Direct Encoding in SNNs**: Q-SDE is positioned as an improvement over standard direct encoding; the baseline assumption (repeat input T times) must be understood to appreciate the queue-driven modification. Quick check: What is the memory cost scaling of direct encoding with respect to time steps T and input dimension N?

## Architecture Onboarding

- **Component map**: Input (Point cloud P) → Q-SDE → Pe → MLP Module → SPTB → STDB → SPTB → ... → Classification Head
- **Critical path**: Q-SDE encoding correctness → HD-IF gating → Firing rate balance per layer
- **Design tradeoffs**: Ns (support points) vs. memory, T (time steps) vs. latency, neuron type selection vs. compute
- **Failure signatures**: Accuracy plateaus/drops as T increases, firing rate > 40%, HD-IF gates converge to single neuron dominant
- **First 3 experiments**:
  1. Train SPT with T=1 vs. T=4 Q-SDE512 to verify memory reduction and accuracy gain
  2. Replace HD-IF with each single neuron type to confirm HD-IF matches/exceeds best single neuron
  3. Profile spike sparsity per layer; adjust Vth to maintain 15-25% firing rate for energy-accuracy balance

## Open Questions the Paper Calls Out
- Can SPT be effectively extended to dense prediction tasks such as 3D semantic segmentation and object detection while maintaining energy efficiency?
- Does the computational overhead of HD-IF negate its theoretical energy benefits when deployed on physical neuromorphic hardware?
- How does Q-SDE perform on large-scale, outdoor LiDAR point clouds with non-uniform density and significant noise?

## Limitations
- HD-IF gating mechanism architecture is unspecified, raising questions about reproducibility
- Data augmentation specifics are not detailed beyond "consistent with Point Transformer"
- Surrogate gradient function for spiking operations is not explicitly specified

## Confidence
- **High Confidence**: Energy efficiency claims (6.4× reduction), overall accuracy results (91.43% on ModelNet40, 78.03% on ScanObjectNN), and Q-SDE memory reduction
- **Medium Confidence**: Claims about HD-IF selective activation improving accuracy over single neurons, and spike-driven attention preserving transformer's discriminative power
- **Low Confidence**: Specific mechanism of how HD-IF gates learn to differentiate neuron dynamics, and exact conditions under which queue-driven sampling outperforms direct encoding

## Next Checks
1. Profile HD-IF gating network outputs during training to verify it learns distinct weights for different neuron types
2. Train baseline with T=1 (no queue sampling) and compare accuracy/memory to Q-SDE with T=4
3. Systematically vary Vth from 0.3 to 0.7 and measure the trade-off between accuracy and energy consumption