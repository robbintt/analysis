---
ver: rpa2
title: 'MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via
  Motion Feature Matching'
arxiv_id: '2502.13234'
source_url: https://arxiv.org/abs/2502.13234
tags:
- motion
- video
- diffusion
- videos
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses motion customization in text-to-video diffusion
  models, where the goal is to control both object movements and camera framing based
  on a reference video. The key challenge is that existing pixel-level approaches
  using frame differences fail to accurately capture complex motion.
---

# MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching

## Quick Facts
- **arXiv ID**: 2502.13234
- **Source URL**: https://arxiv.org/abs/2502.13234
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art motion customization via feature-level attention matching, outperforming pixel-level baselines on CLIP-T, ImageReward, and motion discrepancy metrics.

## Executive Summary
This paper addresses motion customization in text-to-video diffusion models, where the goal is to control both object movements and camera framing based on a reference video. The key challenge is that existing pixel-level approaches using frame differences fail to accurately capture complex motion. The proposed MotionMatcher solves this by fine-tuning diffusion models at the feature level using motion feature matching. Instead of reconstructing pixel values, it extracts high-level spatio-temporal motion features from attention maps in the diffusion model - cross-attention maps for camera framing and temporal self-attention maps for object movements. These features are then matched during fine-tuning to learn the target motion.

## Method Summary
MotionMatcher fine-tunes text-to-video diffusion models by matching high-level motion features extracted from attention maps rather than pixel values. It uses a frozen copy of the same T2V model to extract cross-attention (CA) and temporal self-attention (TSA) maps from the predicted and ground-truth noisy videos. These attention maps are concatenated and matched via L2 loss during LoRA fine-tuning, enabling accurate motion learning while preserving text alignment and avoiding content leakage.

## Key Results
- Achieves 30.43 CLIP-T score vs 30.31 for best baseline
- ImageReward score of 0.2301 vs 0.4650 for baselines
- Motion discrepancy of 0.0330 vs 0.0544 for baselines
- 60-65% human preference over baselines across video quality, text alignment, and motion alignment

## Why This Works (Mechanism)

### Mechanism 1: Feature-Level Motion Matching Overcomes Pixel-Level Limitations
- Claim: Aligning high-level motion features rather than pixel differences enables accurate motion learning without content leakage.
- Mechanism: The motion feature matching objective minimizes L2 distance between motion features extracted from predicted and ground-truth noisy videos.
- Core assumption: Motion is inherently a high-level spatio-temporal concept that cannot be captured by pixel-level frame differences.
- Evidence anchors: [abstract] "Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features"; [section 3.2] Frame differences "do not accurately represent motion" — same motion yields different pixel changes in different color channels.

### Mechanism 2: Pre-trained T2V Model Serves as Noisy-Latent Feature Extractor
- Claim: Pre-trained diffusion models naturally extract meaningful features from noisy latent videos without training-inference gap.
- Mechanism: Use a frozen copy of the same T2V diffusion model to extract attention maps from both predicted and ground-truth videos at matching noise levels.
- Core assumption: Attention maps in pre-trained diffusion models encode spatio-temporal motion information that persists across noise levels.
- Evidence anchors: [section 3.2] "pre-trained diffusion models are capable of extracting high-level semantics and structural information from visual data"; [Appendix C, Table 3] Retrieval experiment: motion features achieve 32.78% AP vs 8.20% for DDPM latent values.

### Mechanism 3: Dual Attention Decomposition Captures Complementary Motion Aspects
- Claim: Cross-attention maps encode camera framing; temporal self-attention maps encode object dynamics.
- Mechanism: CA maps correlate spatial-temporal coordinates with text tokens for scene composition. TSA maps compute self-attention between frame pairs at each spatial location for inter-frame dynamics.
- Core assumption: These two attention mechanisms provide orthogonal motion information that jointly represents complete motion.
- Evidence anchors: [section 3.3] "we discover that the temporal self-attention (TSA) maps in T2V diffusion models can capture detailed object movements"; [Table 2, ablation] Removing CA: motion discrepancy 0.0330→0.0360; removing TSA: 0.0330→0.0693.

## Foundational Learning

- **Concept: Temporal attention in video diffusion models**
  - Why needed here: Understanding how temporal self-attention processes frame-axis information is critical for extracting object movement cues.
  - Quick check question: How does temporal self-attention differ from spatial self-attention in T2V architectures?

- **Concept: Latent space diffusion and noisy video processing**
  - Why needed here: The method operates entirely in latent space at arbitrary noise levels — standard feature extractors fail here.
  - Quick check question: Why can't pre-trained vision models extract features from noisy latent videos without modification?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tuning strategy preserves pre-trained knowledge while adapting to new motion with ~30× fewer parameters.
  - Quick check question: What happens to text alignment if you full fine-tune vs. LoRA fine-tune for motion customization?

## Architecture Onboarding

- **Component map**: Zeroscope T2V model -> LoRA adapters -> Attention map hooks (downblock.2) -> Motion feature extractor -> L2 loss -> Backprop to LoRA only

- **Critical path**: 1) Input reference video → DDIM inversion → initial noise z_T 2) Forward pass with LoRA → predicted noisy video vθ_t 3) Extract CA+TSA from vθ_t and ground-truth v̂_t (frozen extractor, stop gradient after extraction) 4) Compute L2 loss on attention maps → backprop to LoRA only

- **Design tradeoffs**: 
  - Training time vs. quality: Feature extraction adds ~2× training time (15 min vs. 8 min on RTX 4090)
  - Timestep weighting: First 500 timesteps weighted 1, last 500 weighted 0 — prioritizes motion-defining early denoising
  - Attention resolution: 12×12 from downblock.2 — faster but may lose fine-grained motion details

- **Failure signatures**:
  - Content leakage: Generated video replicates reference appearance (check if β in DDIM inversion is too high)
  - Position errors without dynamics errors: CA extraction failing (verify λ_CA weight)
  - Dynamics errors without position errors: TSA extraction failing (verify λ_TSA weight)
  - Text alignment collapse: LoRA rank too high or learning rate too aggressive

- **First 3 experiments**:
  1. Sanity check: Reproduce retrieval experiment (Appendix C) — motion features should achieve >30% AP on motion similarity task
  2. Ablation validation: Train with CA-only, TSA-only, both — confirm both contribute with TSA having larger impact on motion discrepancy
  3. Complex motion test: Compare against VMC/MotionDirector on fast displacement + rotation videos — expect 0.0330 vs. 0.0544 motion discrepancy gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can motion customization be achieved without relying on DDIM-inverted noise to eliminate the risk of content leakage from the reference video?
- Basis in paper: [explicit] The authors identify the reliance on DDIM-inverted noise as a limitation that "introduces a potential risk of content leakage" and explicitly designate addressing this as a direction for future research.
- Why unresolved: The current framework depends on inverted noise for initial alignment, a common trait in existing approaches that inherently carries the leakage risk.
- What evidence would resolve it: A modified sampling strategy that achieves comparable motion alignment scores without using inverted noise initialization.

### Open Question 2
- Question: Is it possible to reduce the training latency introduced by the motion feature extractor while maintaining high-fidelity motion capture?
- Basis in paper: [explicit] The authors note that using the pre-trained T2V model as a feature extractor introduces "additional latency," increasing fine-tuning time compared to pixel-level baselines.
- Why unresolved: The method prioritizes feature quality and accessibility using the full T2V model, accepting the computational cost.
- What evidence would resolve it: The development of a lightweight or distilled feature extractor that reduces training time to pixel-level benchmarks without increasing Motion Discrepancy.

### Open Question 3
- Question: How does the method perform when the cross-attention maps derived from the reference text prompt do not semantically align with the target text prompt?
- Basis in paper: [inferred] The method extracts camera framing cues from cross-attention maps based on the text prompt $y$. If the target prompt describes a different scene or object, the spatial correlation maps may not transfer meaningfully.
- Why unresolved: The framework assumes the text prompt provides sufficient spatial grounding, but does not explicitly handle semantic drift between the reference and target prompts.
- What evidence would resolve it: Evaluation results on a dataset where reference and target prompts describe semantically distinct subjects (e.g., "car" vs. "person").

## Limitations

- **Architecture dependency**: Method only validated on Zeroscope, generalization to other T2V architectures requires identifying appropriate attention extraction layers
- **Training efficiency**: Feature extraction adds ~2× training time compared to pixel-level baselines
- **DDIM inversion reliance**: Current framework depends on DDIM-inverted noise, introducing potential content leakage risk

## Confidence

- **High Confidence**: The core methodology of feature-level motion matching over pixel-level differences is sound and well-supported by ablation studies showing both CA and TSA contribute to motion quality
- **Medium Confidence**: The dual attention decomposition (CA for framing, TSA for object movement) is supported by ablation results but could be architecture-dependent rather than fundamental
- **Low Confidence**: Claims about avoiding content leakage and achieving motion-only transfer lack quantitative validation beyond qualitative assertions

## Next Checks

1. **Architecture Transfer Test**: Apply MotionMatcher to a different T2V architecture (e.g., Step-Video-T2V) and verify whether the same attention map extraction points (downblock.2) capture motion effectively, or if layer-specific tuning is required.

2. **Content Leakage Quantification**: Implement SSIM or LPIPS-based appearance similarity metrics between generated videos and reference videos across the test set, and compare against pixel-level baselines to provide quantitative evidence for content leakage prevention.

3. **Motion Feature Ablation Study**: Beyond the existing CA/TSA ablation, test the contribution of individual attention heads/layers to isolate which specific attention mechanisms are most critical for different motion types (fast displacement vs. rotation vs. camera motion).