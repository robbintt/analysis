---
ver: rpa2
title: Hardware optimization on Android for inference of AI models
arxiv_id: '2511.13453'
source_url: https://arxiv.org/abs/2511.13453
tags:
- accuracy
- int8
- quantization
- table
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates AI model performance on an Android tablet\
  \ using heterogeneous computing (CPU, GPU, NPU) for object detection (YOLO) and\
  \ image classification (ResNet). The NPU achieved up to 130\xD7 speedup over single-threaded\
  \ CPU baseline, making it optimal for low-latency edge AI."
---

# Hardware optimization on Android for inference of AI models

## Quick Facts
- arXiv ID: 2511.13453
- Source URL: https://arxiv.org/abs/2511.13453
- Reference count: 12
- Primary result: NPU achieves up to 130× speedup over single-threaded CPU for AI inference on Android tablets

## Executive Summary
This study evaluates AI model performance on an Android tablet using heterogeneous computing (CPU, GPU, NPU) for object detection (YOLO) and image classification (ResNet). The NPU achieved up to 130× speedup over single-threaded CPU baseline, making it optimal for low-latency edge AI. INT8 quantization worked best for ResNet balancing speed and accuracy, while FP16 was preferred for YOLO due to significant accuracy loss with INT8. YOLO11 outperformed YOLOv8 in smaller models with better speed/accuracy trade-offs. Dynamic quantization failed on NPU due to lack of native support. The study establishes optimal configurations for edge AI deployment, with NPU + INT8/FP16 being the most effective combination depending on model type.

## Method Summary
The authors tested AI models (YOLOv8, YOLO11, ResNet18/50/101/152) on a Samsung Galaxy Tab S9 with Snapdragon 8 Gen 2 SoC. They evaluated three accelerators (CPU, GPU, NPU) with four data types (FP32, FP16, INT8, INT16) and two quantization approaches (static and dynamic). Models were converted from PyTorch to ONNX to TFLite LiteRT format. Performance metrics included latency (10 runs, mean reported), accuracy, and energy consumption. A Pareto analysis identified optimal configurations balancing speed and accuracy.

## Key Results
- NPU achieved up to 130× speedup over single-threaded CPU baseline for AI inference
- INT8 quantization worked best for ResNet balancing speed and accuracy, while FP16 was preferred for YOLO due to significant accuracy loss with INT8
- Dynamic quantization failed on NPU due to lack of native support for runtime type conversion
- YOLO11 outperformed YOLOv8 in smaller models with better speed/accuracy trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** NPUs achieve substantial speedups (up to 130× over single-threaded CPU) for AI inference when models use static, pre-computed data types.
- **Mechanism:** The NPU architecture features dedicated tensor, scalar, and vector execution units connected to shared internal memory. "Micro Tile Inferencing" partitions model layers and dispatches operations to specialized units for parallel execution, minimizing data transfer latency.
- **Core assumption:** Model operations can be statically mapped to the NPU's execution units without runtime type conversion.
- **Evidence anchors:**
  - [abstract] "The NPU achieved up to 130× speedup over single-threaded CPU baseline, making it optimal for low-latency edge AI."
  - [section II.A] "The SoC also includes a dedicated NPU featuring specialized units for tensor, scalar, and vector operations, all connected to a shared internal memory... Micro Tile Inferencing... partitioning the AI model layers and dispatching each specific operation to the corresponding dedicated execution unit."
  - [corpus] "Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution" supports heterogeneous processor benefits but does not validate NPU micro-architecture specifics.
- **Break condition:** Operations requiring runtime data type conversion (e.g., Dynamic Quantization) break this mechanism, forcing CPU fallback and costly transfers.

### Mechanism 2
- **Claim:** INT8 quantization effectiveness depends on task complexity—classification tolerates it well, but object detection suffers significant accuracy loss.
- **Mechanism:** Object detection requires both localization and classification information. Reduced precision (INT8) loses fine-grained spatial information critical for bounding box regression, causing ~6.5-8.7 mAP loss for YOLO models versus <3% accuracy loss for ResNet.
- **Core assumption:** Accuracy degradation from quantization scales with the information density required by the task.
- **Evidence anchors:**
  - [abstract] "INT8 quantization worked best for ResNet balancing speed and accuracy, while FP16 was preferred for YOLO due to significant accuracy loss with INT8."
  - [section IV.B] "Lower precision, like INT8 quantization, significantly hurts object detection models (YOLOv8) because this task demands more information (localization and classification) than simpler tasks like image classification."
  - [corpus] "Optimizing LLMs Using Quantization for Mobile Execution" discusses PTQ for compression but does not validate task-dependent accuracy patterns.
- **Break condition:** Tasks requiring high spatial precision or fine-grained regression outputs will show unacceptable accuracy degradation with INT8.

### Mechanism 3
- **Claim:** Dynamic Quantization fails on NPU due to architectural mismatch—mixed-precision operations require CPU intervention, negating acceleration.
- **Mechanism:** Dynamic Quantization uses mixed precisions quantized at runtime. The NPU lacks native support for dynamic type conversion, forcing CPU to perform casting with costly data transfers between NPU and CPU for each precision change.
- **Core assumption:** Data transfer overhead between CPU and NPU dominates inference time when mixed-precision operations are required.
- **Evidence anchors:**
  - [abstract] "Dynamic quantization failed on NPU due to lack of native support."
  - [section IV.A] "The NPU accelerator lacks native support for such dynamic data type conversion. Consequently, the model must rely on the CPU to perform casting operations, requiring costly data transfers between the NPU and CPU for each precision change."
  - [corpus] Weak corpus evidence—related papers do not address dynamic quantization NPU compatibility.
- **Break condition:** Future NPU architectures with native mixed-precision support could enable dynamic quantization acceleration.

## Foundational Learning

- **Concept: Heterogeneous Computing (CPU/GPU/NPU)**
  - Why needed here: Different processors excel at different workloads—CPU for general logic, GPU for parallel floating-point, NPU for tensor operations. Choosing the wrong accelerator wastes potential speedup.
  - Quick check question: If your model requires frequent runtime type conversions, which processor should you avoid?

- **Concept: Quantization Types (Static vs. Dynamic, FP32/FP16/INT8)**
  - Why needed here: Quantization choice directly impacts both latency and accuracy. Static quantization pre-computes weights offline; dynamic quantizes activations at runtime. The paper shows this choice is model-dependent.
  - Quick check question: Why would INT8 work for ResNet but fail for YOLO object detection?

- **Concept: Pareto Front Analysis for Multi-Objective Optimization**
  - Why needed here: Edge AI deployment requires balancing conflicting objectives (latency, accuracy, power). Pareto analysis identifies non-dominated configurations where you cannot improve one metric without degrading another.
  - Quick check question: If two configurations have the same latency but different accuracy, which appears on the Pareto Front?

## Architecture Onboarding

- **Component map:** PyTorch Model → ONNX (intermediate) → TFLite (.tflite) with quantization → LiteRT 1.4 (NPU) vs LiteRT 2.0.2 (CPU/GPU) → Samsung Galaxy Tab S9 (Snapdragon 8 Gen 2)

- **Critical path:**
  1. Model conversion: PyTorch → ONNX → TFLite (use `onnx2tf` for quantization control)
  2. LiteRT version selection: v1.4 for NPU, v2.0.2 for CPU/GPU (API compatibility)
  3. Quantization selection: INT8 for classification, FP16 for detection tasks
  4. Accelerator selection: NPU for maximum speed, GPU for consistency

- **Design tradeoffs:**
  - **INT8 vs FP16 on NPU:** INT8 gives ~2× faster inference (129.6× vs 66.4× speedup for ResNet18) but may lose 6.5-8.7 mAP on detection
  - **NPU vs GPU:** NPU achieves 47-298× speedup but incompatible with Dynamic Quantization; GPU more flexible with ~15-38× speedup
  - **Model size vs quantization tolerance:** Larger models (ResNet152) lose only 0.20% accuracy with INT8 vs 2.94% for smaller models (ResNet18)

- **Failure signatures:**
  - Dynamic Quantization on NPU: Speedup drops to ~2× (vs 60-100× for static), indicating CPU fallback
  - Full INT8 (FINT8) on ResNet: Accuracy crashes to 0.08% (quantization failure)
  - INT16 quantization: Latency 791-882ms across all devices (unoptimized kernels)
  - Framework conversion loss: ~0.83-1.77% accuracy loss inherent in PyTorch→LiteRT pipeline

- **First 3 experiments:**
  1. **Baseline measurement:** Run FP32 model on CPU single-threaded to establish reference latency and accuracy metrics.
  2. **Accelerator sweep:** Test same FP32 model on GPU16 and NPU to measure raw acceleration potential without quantization confounds.
  3. **Quantization tolerance test:** For your specific model, test INT8 vs FP16 on NPU and measure accuracy degradation to determine acceptable operating point using Pareto analysis.

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to single Android device (Samsung Galaxy Tab S9 with Snapdragon 8 Gen 2), restricting generalizability
- Evaluation focuses only on YOLO and ResNet models, potentially missing task-specific behaviors for other architectures
- Dynamic quantization failure on NPU observed but underlying hardware-software interaction mechanism remains incompletely characterized

## Confidence
- **High Confidence:** NPU speedup claims (130× over single-threaded CPU), INT8 vs FP16 accuracy tradeoffs for specific models, YOLO11 performance improvements over YOLOv8
- **Medium Confidence:** Task-dependent quantization tolerance mechanism, NPU architecture-specific limitations for dynamic quantization
- **Low Confidence:** Generalizability to other device SoCs, behavior with non-CNN architectures, long-term stability of quantization configurations

## Next Checks
1. **Cross-platform validation:** Test the same model configurations on at least two additional Android devices with different NPUs (e.g., Google Tensor, MediaTek Dimensity) to assess hardware dependency.
2. **Architecture expansion:** Evaluate transformer-based models (BERT, ViT) and segmentation models (UNet) to determine if task-complexity quantization patterns hold beyond the tested CNN architectures.
3. **Dynamic quantization root cause analysis:** Profile CPU-NPU data transfers during dynamic quantization to quantify the overhead and identify specific hardware-software bottlenecks preventing NPU acceleration.