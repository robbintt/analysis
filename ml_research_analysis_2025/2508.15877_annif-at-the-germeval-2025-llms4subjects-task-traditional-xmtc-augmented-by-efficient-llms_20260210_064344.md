---
ver: rpa2
title: 'Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented
  by Efficient LLMs'
arxiv_id: '2508.15877'
source_url: https://arxiv.org/abs/2508.15877
tags:
- synthetic
- records
- data
- system
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes an automated subject indexing system for bibliographic
  records using efficient large language models (LLMs). The system combines traditional
  multi-label classification with LLM-based synthetic data generation and ranking.
---

# Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs

## Quick Facts
- arXiv ID: 2508.15877
- Source URL: https://arxiv.org/abs/2508.15877
- Reference count: 16
- Primary result: First place in both quantitative (nDCG@20=0.5697) and qualitative evaluations in the GermEval-2025 LLMs4Subjects Task 2

## Executive Summary
This paper describes an automated subject indexing system for bibliographic records that combines traditional extreme multi-label text classification (XMTC) with efficient large language models (LLMs). The system achieved first place in the GermEval-2025 LLMs4Subjects Task by using LLM-based synthetic data generation and ranking to augment limited training data. Metadata records were translated into German and English using multiple small, efficient LLMs, and synthetic training data was generated to improve traditional classifier performance. The approach demonstrates that combining efficient LLMs with traditional NLP methods provides competitive performance while balancing computational efficiency and indexing quality.

## Method Summary
The system uses LLM translation (Aya 8B for German, Gemma 3 4B for English) to create monolingual records, then generates synthetic training data through one-shot prompting with multiple LLMs. Three base XMTC models are trained: Omikuji Bonsai (using 2×original + 4×synthetic), XTransformer (using original + 2×synthetic), and MLLM (using original only). These are combined into weighted ensembles with optimized exponents, and optionally enhanced with LLM re-ranking of top candidates. The pipeline achieves state-of-the-art performance on the GermEval-2025 subject indexing task.

## Key Results
- Achieved first place with nDCG@20=0.5697 in quantitative evaluation
- Synthetic data augmentation improved performance with diminishing returns after 3-4 synthetic datasets
- LLM-based re-ranking improved nDCG scores by 0.01-0.03 compared to simple ensembles
- Language-specific model selection (A8 for German, G4 for English) provided optimal quality-efficiency tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Augmentation via Multi-LLM Generation
LLM-generated synthetic training records improve traditional XMTC model performance through one-shot prompting that creates new title/abstract pairs with existing subjects plus one randomly added GND term. The approach shows diminishing returns after 3-4 synthetic datasets as the semantic coherence of artificially constructed label combinations becomes questionable.

### Mechanism 2: LLM-based Re-ranking of Ensemble Candidates
External LLM re-ranking of top-K ensemble predictions provides complementary relevance judgments that improve final ranking quality. The approach assigns 0-100 relevance scores to top 100 candidates, with final scores combining LLM and ensemble scores through weighted formulas.

### Mechanism 3: Quality-Efficiency Optimization for Task-Specific Model Selection
Scoring LLMs on combined quality and throughput metrics enables optimal model selection for different pipeline stages. The approach uses nDCG@20 + α·throughput optimization to select different optimal models for German (A8) vs English (G4) translation based on task-specific efficiency requirements.

## Foundational Learning

- **Concept**: Extreme Multi-Label Text Classification (XMTC)
  - Why needed here: Core task formulation—assigning relevant subjects from 200,035 possible GND labels to bibliographic records, where each record typically receives multiple labels.
  - Quick check question: How does XMTC differ from standard multi-class classification? (Answer: XMTC handles label spaces orders of magnitude larger with sparse, multi-label outputs requiring specialized algorithms.)

- **Concept**: Partitioned Label Trees (Bonsai/Parabel)
  - Why needed here: Enables efficient inference over 200K+ labels by recursively partitioning the label space into tree structures, reducing complexity from O(|L|) to O(log|L|).
  - Quick check question: Why do partitioned trees scale better than flat classifiers? (Answer: They avoid computing scores for all labels simultaneously by traversing only relevant tree paths.)

- **Concept**: Ensemble Weighting with Exponents
  - Why needed here: Different backends produce scores on different scales; exponents normalize these distributions before weighted combination.
  - Quick check question: What problem do weighting exponents solve that simple weights don't? (Answer: Exponents adjust for non-linear score distributions, preventing models with higher-variance outputs from dominating.)

## Architecture Onboarding

- **Component map**: vLLM translation (A8→German, G4→English) → synthetic data generation (4 LLMs × 2 languages) → base models (Bonsai, MLLM, XTransformer) → weighted ensembles (BM simple/BMX simple/BMX LLM) → final predictions

- **Critical path**:
  1. Translate train/dev records to monolingual German and English
  2. Generate 4 synthetic datasets per language using top-performing translation LLMs
  3. Train Bonsai on 2×original + 4×synthetic; XTransformer on 1×original + 2×synthetic; MLLM on original only
  4. Run `annif hyperopt` (400 combinations) to optimize weights and exponents
  5. Optional: Add LLM ranking layer with separate hyperparameter optimization

- **Design tradeoffs**:
  - M24 vs Q4 for re-ranking: +0.0066 nDCG@20 but ~6× model size
  - Synthetic data volume: Diminishing returns after 3 sets (Figure 4)
  - Ensemble complexity: BMX LLM requires GPU inference; BM simple runs CPU-only
  - Language-specific vs unified models: Separate de/en models outperform bilingual but double maintenance

- **Failure signatures**:
  - Synthetic data over-sampling causes plateau/degradation (Figure 4 shows inflection point)
  - LLM ranking weight >0.2 overpowers classifier signal
  - Translation model mismatch drops nDCG by ~0.03-0.05
  - Exponent values outside [0.5, 1.5] indicate unstable score distributions

- **First 3 experiments**:
  1. Establish baseline: Train single Bonsai model on original German data only; measure F1@5 and nDCG@20 on dev set
  2. Synthetic data ablation: Add synthetic datasets one at a time (1→2→3→4) and plot nDCG trajectory to identify plateau point for your data
  3. Ensemble validation: Compare BM simple vs BMX simple vs BMX LLM (using Q4) on held-out dev records; quantify re-ranking benefit vs inference cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would constraining synthetic training data generation to ensure semantic coherence between added random subjects and existing document content improve XMTC model performance?
- Basis in paper: Authors recognize that random addition of GND terms may cause seemingly unrelated subjects to appear together, but did not want to place too much constraint here.
- Why unresolved: Current one-shot approach adds randomly chosen subjects without verifying semantic compatibility with document content.
- What evidence would resolve it: Comparative experiments using constrained vs. unconstrained synthetic data generation, measuring nDCG@20 and F1@5 on held-out test sets.

### Open Question 2
- Question: Why does the Bonsai backend show substantially greater improvement from synthetic training data compared to XTransformer, and does this generalize to other transformer-based XMTC models?
- Basis in paper: Authors note Bonsai improved a lot while XTransformer improvements were less dramatic without providing explanation.
- Why unresolved: Differential response suggests fundamental differences in how tree-based vs. transformer-based architectures exploit augmented training data.
- What evidence would resolve it: Ablation studies analyzing feature distributions in synthetic data and their alignment with each model's learning mechanism.

### Open Question 3
- Question: What is the theoretical justification for the weighting exponent parameter in ensemble fusion, and can optimal exponent values be predicted from base model score distributions?
- Basis in paper: The enhanced ensemble uses weighting exponents (ranging 0.63–1.17) determined through hyperparameter optimization without theoretical grounding.
- Why unresolved: The formula $f = \sum_{k=1}^{n} w_k \cdot x_k^{p_k}$ is introduced empirically without theoretical justification.
- What evidence would resolve it: Analysis of score calibration across backends and theoretical/experimental validation of exponent effects on ranking quality.

## Limitations

- Synthetic data generation may produce semantically inconsistent label combinations that could confuse classifiers
- Ensemble optimization via hyperopt on development data risks overfitting despite minimal dev-test gap
- Hardware dependency on high-end GPUs (A100 80GB) limits accessibility and may not translate well to smaller compute environments

## Confidence

- **High confidence**: Effectiveness of combining traditional XMTC methods with LLM-based synthetic data generation and re-ranking (supported by nDCG@20=0.5697 first place)
- **Medium confidence**: Optimal number of synthetic datasets (3-4) and specific model choices (A8 for German, G4 for English) may be dataset-specific
- **Low confidence**: Quality-efficiency optimization formula (score = nDCG@20 + α·throughput) lacks broader corpus validation across varied document types and languages

## Next Checks

1. **Synthetic data coherence validation**: Manually inspect 100 synthetic records to verify that randomly added GND terms create semantically plausible subject combinations, and measure whether these combinations improve decision boundary learning or introduce noise.

2. **Cross-dataset generalization**: Apply the same pipeline to a different bibliographic dataset (e.g., arXiv or PubMed) with different subject vocabularies to test whether the synthetic data and ensemble strategies transfer or require re-tuning.

3. **Efficiency-cost tradeoff analysis**: Profile inference latency and compute costs for the BMX LLM approach versus simpler ensembles on commodity hardware (e.g., RTX 4090) to determine practical deployment viability and identify potential bottlenecks.