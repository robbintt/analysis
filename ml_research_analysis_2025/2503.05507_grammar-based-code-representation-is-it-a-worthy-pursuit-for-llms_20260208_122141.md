---
ver: rpa2
title: 'Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?'
arxiv_id: '2503.05507'
source_url: https://arxiv.org/abs/2503.05507
tags:
- code
- grammar-based
- representation
- representations
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grammar-based code representations are explored for billion-scale
  LLMs, where syntax errors are rare. A series of models (1.3B, 1.5B, 7B) is trained
  using grammar rules alongside tokens.
---

# Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?

## Quick Facts
- arXiv ID: 2503.05507
- Source URL: https://arxiv.org/abs/2503.05507
- Reference count: 27
- Primary result: Grammar-based representations improve semantic differentiation and code generation performance on billion-scale LLMs, achieving significant gains on HumanEval(+) and MBPP(+) benchmarks over token-only baselines.

## Executive Summary
This paper explores whether grammar-based code representations can improve billion-scale LLM performance on code generation tasks. The authors propose GrammarCoder, which incorporates grammar rules alongside tokens in the input representation. By training models on serialized code via preorder AST traversal with interleaved grammar rules, they demonstrate significant performance improvements on HumanEval(+) and MBPP(+) benchmarks compared to token-only baselines. The key insight is that grammar-based representations amplify subtle semantic differences that appear minimal at the token level, helping models distinguish correct from incorrect code more effectively.

## Method Summary
The method involves parsing code into Abstract Syntax Trees (ASTs) using Tree-sitter, then serializing them via preorder traversal to extract grammar rule sequences interleaved with terminal tokens. The vocabulary is extended to include grammar rule IDs (Vgrammar = Vnormal + Vrule). Models are trained with next-token prediction loss over these mixed sequences using a standard decoder-only transformer architecture. The training pipeline includes continued pre-training on 10B tokens from TheStackV2 (Python) and 0.5B code textbook tokens, followed by instruction tuning on 6B instruction tokens.

## Key Results
- GrammarCoder-1.3B-Base achieves 71.8 Pass@1 on HumanEval(+) vs DeepSeek-Coder-1.3B-Base at 66.8
- Grammar-based representations show 91.18% higher edit distances compared to token-only representations for semantically similar code pairs
- GrammarCoder-1.3B-Base achieves 79.30 F1 vs DeepSeek-Coder-1.3B-Base 67.06 F1 (+18.25%) on CodeNanoFix semantic classification

## Why This Works (Mechanism)

### Mechanism 1
Grammar-based representation amplifies subtle semantic differences that appear minimal at the token level. When code undergoes minor token-level changes (e.g., missing parentheses affecting operator precedence), the AST structure changes significantly. Grammar rules explicitly encode this structural shift via preorder traversal, while token sequences may show only 1-2 character differences. Evidence shows token-level edit distance 14.33 vs grammar-based 27.43 (91.18% increase), with operator precedence errors showing token edit distance=2, grammar edit distance=6.

### Mechanism 2
Grammar rules provide explicit structural constraints that reduce invalid code generation paths. By training on grammar rule sequences interleaved with terminals, the model learns to generate syntactically valid AST structures autoregressively. Each grammar rule token constrains subsequent valid productions, helping the model maintain structural validity during generation.

### Mechanism 3
Grammar-based representation improves semantic classification by making structural differences more salient. In semantic classification tasks, grammar-based models achieve higher F1 scores because the representation explicitly differentiates structurally distinct but token-similar code (e.g., `a + b % 2` vs `(a + b) % 2`). GrammarCoder-1.3B-Base achieves 79.30 F1 vs DeepSeek-Coder-1.3B-Base 67.06 F1 (+18.25%) on CodeNanoFix classification.

## Foundational Learning

- **Abstract Syntax Tree (AST) and Preorder Traversal**
  - Why needed: Grammar-based representation is derived from AST preorder traversal; understanding how code maps to trees is essential.
  - Quick check: Given `return a + b * c`, can you sketch the AST and identify which grammar rules would appear in preorder traversal?

- **Edit Distance in Sequence Space**
  - Why needed: The paper's core hypothesis rests on edit distance differences between representations.
  - Quick check: If sequence A transforms to sequence B via 3 insertions and 2 deletions, what is the edit distance?

- **Grammar Rule as Token (Vocabulary Extension)**
  - Why needed: The model doesn't generate ASTs directly—it generates token sequences where some tokens are grammar rules.
  - Quick check: If base vocabulary has 32K tokens and there are 1.2K grammar rules, what is the extended vocabulary size?

## Architecture Onboarding

Raw Code → Tree-sitter Parser → AST → Preorder Traversal → Grammar Rule Sequence + Terminal Tokens → Tokenizer (Vgrammar) → Model Input

Critical path:
1. Parse all training code through Tree-sitter; discard unparsable samples
2. Extract grammar rules via AST preorder traversal
3. Interleave grammar rules with BPE-tokenized terminals
4. Extend vocabulary with grammar rule IDs
5. Train with next-token prediction loss over mixed sequences

Design tradeoffs:
- Pros: Amplifies semantic differences; enforces structural validity
- Cons: Cannot process incomplete/malformed code; higher preprocessing cost; vocabulary expansion increases embedding layer size
- Key insight: Grammar-based models trained on ~10B tokens underperformed OpenCoder on MBPP+ (trained on 900B tokens), suggesting data scale still matters

Failure signatures:
- High syntax error rate on generation → Check if grammar rules are being predicted
- Low data utilization during preprocessing → Many samples failing Tree-sitter parse
- No performance gain over token baseline → Verify grammar rules carry signal

First 3 experiments:
1. Replicate edit distance analysis: Take 100 error-correct pairs from CodeNanoFix; compute token vs grammar edit distances; verify ~90% amplification
2. Ablate grammar rule vocabulary: Train model with grammar-based representation but random-shuffle rule IDs; if performance drops, confirms rules carry signal
3. Perturbation test: Take correct code, introduce single-character semantic errors (missing parens, wrong operator); measure if grammar representation shows larger changes than token representation

## Open Questions the Paper Calls Out

### Open Question 1
Can grammar-based models match state-of-the-art token-based models when trained on comparable data scales (hundreds of billions of tokens)? The current study used only 10B tokens versus OpenCoder's 900B, explicitly noting this limitation and calling for future work with larger datasets.

### Open Question 2
How can grammar-based representations be adapted to handle incomplete or syntactically invalid code snippets that fail AST parsing? The strict requirement for valid syntax creates rigidity that limits real-world applicability, particularly for interactive scenarios with incomplete code.

### Open Question 3
Does the semantic differentiation benefit generalize to programming languages with different structural constraints? The study focuses exclusively on Python, leaving open whether the mechanism works equally well for languages with more complex or different grammars.

## Limitations
- The causal link between representation changes and improved generation is indirect—the paper shows correlation and benchmark results but not direct measurement of semantic error reduction in generated code
- Findings may not generalize beyond Python's relatively simple grammar to languages with more complex grammars or different error patterns
- Grammar-based approaches may require even larger datasets to compete with token-only models at scale, as evidenced by performance gaps when comparing 10B vs 900B token training

## Confidence
- **High**: Empirical results showing performance gains on HumanEval(+) and MBPP(+) benchmarks are robust and directly measured
- **Medium**: Mechanism explanations are plausible and supported by evidence, but the causal chain from representation changes to generation improvements is not fully established
- **Low**: Claims about general applicability across different programming languages, code domains, or model scales beyond tested configurations

## Next Checks
1. Take 100 generated samples from both token-only and grammar-based models on HumanEval(+); classify semantic vs syntactic errors to verify grammar-based models actually reduce semantic errors
2. Implement grammar-based pipeline for JavaScript using Tree-sitter; train on 10B JavaScript tokens and evaluate on JavaScript-specific benchmark to test generalization
3. Train both token-only and grammar-based models on progressively smaller subsets (1B, 5B, 10B tokens) to measure performance curves and determine data efficiency advantages