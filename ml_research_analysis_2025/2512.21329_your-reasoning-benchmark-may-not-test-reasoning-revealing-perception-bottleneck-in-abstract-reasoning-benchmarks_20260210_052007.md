---
ver: rpa2
title: 'Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck
  in Abstract Reasoning Benchmarks'
arxiv_id: '2512.21329'
source_url: https://arxiv.org/abs/2512.21329
tags:
- reasoning
- perception
- stage
- performance
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the interpretation of ARC-style reasoning
  benchmarks as pure tests of "fluid intelligence." It hypothesizes that performance
  gaps between humans and AI models stem primarily from perception limitations rather
  than reasoning deficiencies. To test this, the authors introduce a two-stage pipeline
  that separates perception (converting images to natural language descriptions) from
  reasoning (inducing and applying rules).
---

# Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks

## Quick Facts
- **arXiv ID**: 2512.21329
- **Source URL**: https://arxiv.org/abs/2512.21329
- **Reference count**: 9
- **Primary result**: Separating perception from reasoning via a two-stage pipeline reveals perception as the dominant performance bottleneck in ARC-style benchmarks, with ~80% of failures attributed to perception errors.

## Executive Summary
This paper challenges the interpretation of ARC-style reasoning benchmarks as pure tests of "fluid intelligence," arguing that performance gaps between humans and AI models stem primarily from perception limitations rather than reasoning deficiencies. The authors introduce a two-stage pipeline that separates perception (converting images to natural language descriptions) from reasoning (inducing and applying rules). Experiments on Mini-ARC, ACRE, and Bongard-LOGO show that the two-stage approach with dedicated perception significantly outperforms standard end-to-end models. Manual inspection reveals approximately 80% of failures stem from perception errors, suggesting ARC-style benchmarks conflate perceptual and reasoning challenges, potentially overstating reasoning deficiencies in current AI systems.

## Method Summary
The authors propose a two-stage pipeline to evaluate whether perception or reasoning limitations explain performance gaps in ARC-style benchmarks. In Stage 1 (Perception), a vision-language model converts each image to natural language descriptions independently, preventing cross-image signal leakage. In Stage 2 (Reasoning), another model induces rules from demonstration descriptions and applies them to test input descriptions. The pipeline is evaluated under two settings: Same-Model (using GPT-4o or LLaVA-1.5 for both stages) and Stronger-Model Perception (using a stronger VLM for perception, weaker for reasoning). Error attribution involves manual inspection of reasoning traces, categorizing failures into four types: Perception (Demo), Reasoning (Inductive), Perception (Test), and Reasoning (Deductive).

## Key Results
- Two-stage pipeline with dedicated perception improves performance by 10-15 percentage points on Mini-ARC and ACRE compared to end-to-end approaches.
- Stronger perception models paired with weaker reasoning models approach the performance of strong end-to-end models, indicating perception capability explains most of the model-to-model performance gap.
- Manual inspection reveals approximately 80% of failures stem from perception errors (86.4% in Mini-ARC one-stage, 76.3% in ACRE, 65.8% in Bongard-LOGO).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating perception from reasoning via a two-stage pipeline reveals that perception is the dominant performance bottleneck in ARC-style benchmarks.
- Mechanism: Images are independently converted to natural language descriptions (no cross-image signal leakage), then a reasoning model operates purely on these descriptions. This isolates inductive reasoning from visual processing failures.
- Core assumption: Natural language descriptions adequately preserve task-relevant structure while eliminating visual perception challenges.
- Evidence anchors: [abstract] "This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks." [Section 3.2] "The transformation is applied independently to each image in isolation, ensuring that it reduces perceptual difficulty without introducing inductive cues."

### Mechanism 2
- Claim: Fine-grained error attribution across four task-solving steps shows perception errors account for approximately 80% of failures.
- Mechanism: Manual inspection of model reasoning traces categorizes each failure into: Perception (Demo), Reasoning (Inductive), Perception (Test), or Reasoning (Deductive), with errors attributed to the earliest failed step.
- Core assumption: Human annotators can reliably distinguish perception failures from reasoning failures in model outputs.
- Evidence anchors: [abstract] "Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors." [Table 5] Mini-ARC shows 86.4% perception errors in one-stage; ACRE shows 76.3%; Bongard-LOGO shows 65.8%.

### Mechanism 3
- Claim: Stronger perception models paired with weaker reasoning models approach the performance of strong end-to-end models, indicating perception capability explains most of the model-to-model performance gap.
- Mechanism: Substitute a stronger VLM (e.g., GPT-4o, o1) for the perception stage while keeping a weaker VLM for reasoning. Compare against both weak and strong end-to-end baselines.
- Core assumption: The performance gap between strong and weak models primarily reflects perception rather than reasoning differences.
- Evidence anchors: [Section 4.3, Table 4] ACRE: LLaVA-1.5 end-to-end scores 22%, but GPT-4o (perception) + LLaVA-1.5 (reasoning) scores 82.5%, approaching GPT-4o end-to-end at 93%.

## Foundational Learning

- Concept: Fluid vs. Crystallized Intelligence
  - Why needed here: ARC benchmarks claim to test fluid intelligence (novel problem-solving) rather than crystallized intelligence (accumulated knowledge). Understanding this distinction is essential to interpreting why perception bottlenecks matter.
  - Quick check question: Can you explain why a benchmark designed to minimize knowledge priors might still inadvertently favor humans through innate visual perception?

- Concept: Inductive vs. Deductive Reasoning in Multi-Step Tasks
  - Why needed here: The error attribution framework separates inductive reasoning (inferring rules from demonstrations) from deductive reasoning (applying inferred rules to test inputs). Confusing these leads to misdiagnosing failure sources.
  - Quick check question: Given demonstration pairs (x₁,y₁), (x₂,y₂) and test input x₃, which step involves induction and which involves deduction?

- Concept: Cross-Image Signal Leakage
  - Why needed here: The two-stage pipeline's validity depends on preventing information from multiple images from combining during perception. Without this isolation, the pipeline would not cleanly separate perception from reasoning.
  - Quick check question: If a perception model were shown all demonstration images simultaneously, what inductive signals might leak into the descriptions?

## Architecture Onboarding

- Component map: Perception Stage (g_X, g_Y) -> Reasoning Stage (h) -> Error Attribution Module
- Critical path:
  1. Define transformations g_X and g_Y with prompts that specify human-aligned perceptual features (objects, colors, positions, and shapes).
  2. Apply transformations independently to each image—never batch multiple images into one perception call.
  3. Construct enriched task representation êT preserving original images alongside descriptions.
  4. Run reasoning model on êT; collect reasoning traces.
  5. For failures, trace back to earliest failed step via output inspection.

- Design tradeoffs:
  - Natural language vs. structured representations: Language is interpretable but may lose precise spatial information. The paper acknowledges this is a convenient choice, not claimed as optimal.
  - Manual vs. automated error attribution: Manual inspection provides fine-grained insight but doesn't scale. Automated methods remain future work.
  - Same-model vs. stronger-model perception: Same-model comparisons isolate the perception bottleneck within one model; stronger-model comparisons reveal cross-model gaps.

- Failure signatures:
  - Perception (Demo) failure: Model describes objects incorrectly (e.g., misidentifies colors, misses objects, wrong spatial relations) in demonstration images.
  - Reasoning (Inductive) failure: Descriptions are correct but model infers an incorrect or incomplete rule.
  - Perception (Test) failure: Demonstration reasoning is correct, but test input is misperceived.
  - Reasoning (Deductive) failure: Rule and test perception are correct, but application produces wrong output.

- First 3 experiments:
  1. Run GPT-4o end-to-end on Mini-ARC, then run two-stage pipeline with GPT-4o for both stages. Expect 10-15 percentage point improvement if perception is the bottleneck.
  2. Use GPT-4o for perception and a smaller model (e.g., LLaVA-1.5) for reasoning on ACRE. Compare against both models end-to-end to quantify how much of the strong model's advantage is perceptual.
  3. Randomly sample 50 failed tasks from each dataset. Manually classify errors into four categories. Verify that >60% are perception-related in one-stage, and that two-stage gains come primarily from reducing perception errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What intermediate representation best separates perception from reasoning in abstract reasoning tasks—natural language, symbolic representations, or structured programmatic descriptions?
- Basis in paper: [explicit] The authors state: "We do not claim that natural language is the best representation for isolating reasoning, nor that it preserves all aspects of the original task structure without distortion."
- Why unresolved: The paper demonstrates that natural language descriptions improve performance by reducing perception errors, but does not compare against alternative representations that might preserve task structure more faithfully or enable more precise error attribution.
- What evidence would resolve it: A systematic comparison of multiple intermediate representations (natural language, symbolic, programmatic) on the same ARC-style benchmarks, measuring both downstream task performance and the fidelity of task structure preservation.

### Open Question 2
- Question: Does the perception bottleneck generalize to textual and other multimodal reasoning benchmarks beyond ARC-style visual abstraction tasks?
- Basis in paper: [explicit] The authors state: "We do not claim that perception is the dominant bottleneck for all multimodal or textual reasoning tasks."
- Why unresolved: The study focuses exclusively on three ARC-style datasets with grid-based or simple geometric visuals; it remains unclear whether perception-versus-reasoning error distributions differ for benchmarks like MATHVISTA or textual reasoning suites.
- What evidence would resolve it: Applying the two-stage pipeline methodology to diverse reasoning benchmarks (e.g., MATHVISTA, GSM8K, GPQA) and comparing error attribution distributions across modalities and task types.

### Open Question 3
- Question: Is there a perception sufficiency threshold beyond which reasoning becomes the dominant bottleneck?
- Basis in paper: [inferred] The paper shows perception improvements yield large gains, but does not explore whether further perception enhancements eventually yield diminishing returns, or if reasoning capacity becomes limiting once perception is "good enough."
- Why unresolved: The experiments compare discrete model tiers but do not parametrically vary perception quality to identify potential phase transitions in bottleneck dominance.
- What evidence would resolve it: A controlled experiment progressively enhancing perception quality (e.g., using oracle descriptions or human annotations) while holding reasoning model constant, then analyzing the inflection point where performance gains plateau.

## Limitations
- The core assumption that natural language descriptions preserve task-relevant visual structure is untested.
- The ~80% perception error finding relies on manual inspection of model reasoning traces, which doesn't scale and may have inter-annotator variability.
- Small prompt variations for the perception stage could significantly affect description quality and thus measured performance differences.

## Confidence
- **High Confidence**: The empirical observation that separating perception from reasoning yields 10-15 percentage point improvements on Mini-ARC and ACRE.
- **Medium Confidence**: The interpretation that perception bottlenecks explain most of the performance gap between humans and AI models on ARC-style benchmarks.
- **Medium Confidence**: The claim that stronger perception models paired with weaker reasoning models approach strong end-to-end performance.

## Next Checks
1. Run the two-stage pipeline on a subset of tasks where human subjects provide the natural language descriptions. If performance remains similar, this validates that the bottleneck is truly perceptual rather than linguistic.
2. Apply the same two-stage methodology to a non-ARC benchmark with different visual reasoning demands (e.g., visual analogies or spatial reasoning tasks). Consistent perception bottlenecks across domains would strengthen the generalizability claim.
3. Develop an automated metric for distinguishing perception from reasoning failures by correlating description quality (measured via automated image-text alignment scores) with downstream reasoning success. Compare automated classifications against the manual error attribution schema.