---
ver: rpa2
title: 'ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized
  Node Features and Labels'
arxiv_id: '2506.02134'
source_url: https://arxiv.org/abs/2506.02134
tags:
- graph
- privacy
- node
- explanations
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses privacy vulnerabilities in explainable graph
  learning systems where public feature explanations, combined with differentially
  private node features and labels, can enable graph structure reconstruction attacks.
  The authors propose ReconXF, a novel attack framework that adapts explanation-based
  reconstruction methods by incorporating denoising mechanisms to handle privacy noise
  while exploiting structural signals in explanations.
---

# ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels

## Quick Facts
- arXiv ID: 2506.02134
- Source URL: https://arxiv.org/abs/2506.02134
- Authors: Rishi Raj Sahoo; Rucha Bhalchandra Joshi; Subhankar Mishra
- Reference count: 21
- This work demonstrates that public feature explanations enable graph structure reconstruction even when node features and labels are protected by differential privacy, achieving 5-15% AUC improvements over baseline attacks.

## Executive Summary
This paper introduces ReconXF, a novel graph reconstruction attack framework that exploits public feature explanations combined with differentially private node features and labels. The authors demonstrate that even when privacy-preserving mechanisms protect auxiliary data, explanation-based attacks can recover substantial graph structure by leveraging homophilic patterns encoded in explanations. Through experiments on multiple datasets, ReconXF shows significant improvements over existing reconstruction methods in privatized settings, revealing critical privacy vulnerabilities in current explainable graph learning systems.

## Method Summary
ReconXF reconstructs graph structure by combining public feature explanations with differentially private node features and labels. The framework uses Multi-Bit encoding with MB-rectifier for feature debiasing, Randomized Response for label privatization, and two adjacency generators (FullParam for small graphs, MLP-Diag for large). The model incorporates h-hop aggregation and denoised classification with RR regularization on predicted labels. Joint optimization minimizes a combined loss across adjacency generation, explanation reconstruction, and classification tasks.

## Key Results
- ReconXF achieves 5-15% AUC improvements over baseline attacks in privatized settings
- Performance degrades significantly on heterophilic graphs (Bitcoin-α, Ogbn-arXiv)
- GLime explanations show lower structural sensitivity compared to Grad and Grad-I methods
- Debiasing mechanisms partially restore reconstruction performance under strong privacy guarantees (ε=0.01)

## Why This Works (Mechanism)

### Mechanism 1
Feature explanations leak structural information even when auxiliary data is privatized. In homophilic networks, connected nodes exhibit similar feature importance patterns. Gradient-based explanations (Grad, Grad-I) aggregate neighborhood information during message-passing, encoding structural signals that persist despite feature/label privatization. Adversaries exploit correlation between explanation similarity and edge existence.

### Mechanism 2
Denoising autoencoders can recover structural signals from privatized inputs. Local differential privacy (MB-encoding for features, randomized response for labels) adds bounded noise. The MB-rectifier debiases privatized features by correcting expected distortion. The adjacency generator learns to denoise by optimizing reconstruction against the classification objective, effectively filtering noise that doesn't contribute to downstream task performance.

### Mechanism 3
Applying randomized response to predicted labels acts as regularization that improves reconstruction. Adding noise to predicted labels during training creates an expectation over noise realizations, smoothing gradient updates. This reduces variance from any single noisy label flip, preventing the model from fitting spurious connections that only appear consistent under specific noise configurations.

## Foundational Learning

- **Local Differential Privacy with Multi-bit Encoding**: Understanding how features are perturbed and what debiasing is required before reconstruction. Quick check: Given ε=0.01, can you explain why raw privatized features degrade baseline attacks but debiasing partially restores utility?

- **Message Passing in GNNs**: Explanations inherit neighborhood information through gradient computation; h-hop aggregation controls information propagation during attack. Quick check: Why does h=2 vs h=16 produce different reconstruction quality under privatized inputs?

- **Feature Explanation Methods (Grad, Grad-I, GLime)**: Different explainers have varying structural sensitivity and noise robustness, directly affecting attack success. Quick check: Which explainer would you expect to be most vulnerable to privatization noise, and why?

## Architecture Onboarding

- **Component map**: (EX, X', Y') → MB-rectifier → Adjacency Generators → h-hop Aggregate → Denoised Classification → Fusion
- **Critical path**: Verify privatization pipeline produces expected noise levels → Confirm debiasing reduces feature distortion → Validate adjacency generators produce meaningful edge estimates → Check classification loss provides useful gradient signal
- **Design tradeoffs**: FullParam vs MLP-Diag (arbitrary patterns vs scalability), higher h-hop (more context vs over-smoothing), lower ε (stronger privacy vs infeasible reconstruction)
- **Failure signatures**: AUC ≈ 0.5 on homophilic graphs (debiasing not applied or explanation mismatch), OoM on large graphs (FullParam instead of MLP-Diag), no improvement on heterophilic graphs (expected weak structural signal)
- **First 3 experiments**: 1) Reproduce Cora results with Grad explanations at ε_x=ε_y=0.01; 2) Ablate debiasing and confirm performance drop; 3) Sweep h-hop values on Citeseer to identify optimal neighborhood size

## Open Questions the Paper Calls Out

- **Question 1**: How does applying differential privacy to explanation generation process itself affect reconstruction vulnerability, and what is the resulting privacy-utility trade-off? The paper assumes explanations are public and non-privatized, leaving the impact of noisy explanations unexplored.

- **Question 2**: Can explanation-based attacks succeed when the adversary only accesses a differentially privatized version of the graph structure rather than assuming the structure is completely hidden? The current threat model assumes complete structural ignorance.

- **Question 3**: What unified privacy frameworks are required to secure graph learning systems where features, labels, structure, and explanations are all potential leakage vectors? Current defenses isolate specific components, but the paper shows combined protection is necessary.

## Limitations

- The attack framework relies heavily on homophilic graph assumptions, showing significantly reduced effectiveness on heterophilic graphs like Bitcoin-α and Ogbn-arXiv
- Privacy-utility tradeoff is poorly characterized across the full range of privacy budgets
- Denoising mechanisms lack extensive ablation studies to isolate their individual contributions

## Confidence

- **High confidence**: Core mechanism of exploiting public explanations for graph reconstruction is well-supported by experiments
- **Medium confidence**: Effectiveness of denoising mechanisms demonstrated but not thoroughly validated through ablation studies
- **Low confidence**: Generalizability to heterophilic graphs and extreme privacy budgets (ε < 0.01) is insufficiently explored

## Next Checks

1. **Heterophily stress test**: Run ReconXF on Bitcoin-α and Ogbn-arXiv with varying ε values to quantify exact performance degradation on non-homophilic graphs.

2. **Ablation of denoising components**: Remove MB-rectifier and RR regularization individually and in combination to isolate their contributions versus a simple baseline.

3. **Privacy budget sweep**: Systematically evaluate ReconXF across a broader range of privacy budgets (ε = 0.001, 0.01, 0.1, 1.0, 10.0) to map the full privacy-utility tradeoff curve.