---
ver: rpa2
title: 'Privacy Amplification Through Synthetic Data: Insights from Linear Regression'
arxiv_id: '2506.05101'
source_url: https://arxiv.org/abs/2506.05101
tags:
- privacy
- data
- then
- synthetic
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates privacy amplification through synthetic
  data generation in linear regression. The authors show that when an adversary controls
  the seed of the generative model, a single synthetic data point can leak as much
  information as releasing the model itself.
---

# Privacy Amplification Through Synthetic Data: Insights from Linear Regression

## Quick Facts
- arXiv ID: 2506.05101
- Source URL: https://arxiv.org/abs/2506.05101
- Reference count: 40
- Key finding: Synthetic data can amplify privacy beyond model release when generated from random inputs, with amplification scaling as O(1/d) for one-dimensional outputs and O(√n/d) for multiple outputs

## Executive Summary
This paper investigates privacy amplification through synthetic data generation in linear regression settings. The authors demonstrate that when synthetic data is generated from random inputs to a differentially private model, releasing a limited number of synthetic points can provide stronger privacy guarantees than directly releasing the model parameters. Critically, the privacy amplification scales favorably with the input dimension d, offering O(1/d) amplification for single outputs and O(√n/d) for multiple outputs. However, this benefit disappears entirely if an adversary controls the seed used for synthetic data generation, highlighting the importance of randomization in the process.

## Method Summary
The paper analyzes privacy amplification through synthetic data by studying linear regression models trained with differential privacy. Two main scenarios are examined: (1) synthetic data generated from random inputs, and (2) synthetic data generated from adversary-controlled seeds. The analysis uses f-differential privacy (trade-off functions) as the primary privacy accounting framework, with Rényi differential privacy bounds derived for interpretation. The theoretical results rely on Gaussian approximations and multivariate central limit theorems to establish convergence rates for privacy amplification.

## Key Results
- Releasing synthetic data from random inputs to a private linear regression model provides stronger privacy guarantees than releasing the model itself when the number of points is small relative to input dimension
- When an adversary controls the seed input to the generative model, a single synthetic data point can leak as much information as releasing the entire model
- Privacy amplification scales as O(1/d) for one-dimensional outputs and O(√n/d) for multiple outputs, where d is the input dimension and n is the output dimension

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Releasing synthetic data from random inputs provides stronger privacy guarantees than directly releasing model parameters when l << d.
- **Mechanism:** Random input seeds Z ~ N(0, σ_z²) are multiplied with the noisy model (σ_θN + v), where N is Gaussian noise. The product VZ converges (as d→∞) to a Gaussian with variance scaling as dσ_θ² + ||v||², transforming mean-shift leakage into variance-shift leakage that diminishes as d increases.
- **Core assumption:** The adversary does not observe the random inputs Z; only synthetic outputs VZ are released.
- **Evidence anchors:**
  - [abstract]: "when synthetic data is generated from random inputs, releasing a limited number of synthetic data points amplifies privacy beyond the model's inherent guarantees"
  - [section 4.2, Theorem 4.3]: Trade-off function T(VZ, WZ) converges at O(1/d) rate to Gaussian trade-off function with different variances
  - [corpus]: Related work on privacy amplification by subsampling/iteration exists (Balle et al., 2018; Feldman et al., 2018), but this specific synthetic-data amplification is novel
- **Break condition:** If the adversary learns the random seed Z, amplification vanishes; if d < max{n, l}, multivariate CLT bounds become uninformative.

### Mechanism 2
- **Claim:** When an adversary controls the seed input to the generative model, a single synthetic data point can leak as much information as releasing the entire model.
- **Mechanism:** The adversary chooses z to maximize ||μz||/||z||, selecting z as the right singular vector corresponding to σ_max(μ). For rank-1 shifts (adjacent datasets under label DP), this achieves T(Vz, Wz) = T(V, W), matching post-processing upper bound.
- **Core assumption:** The adversary knows the privacy mechanism and can choose arbitrary query vectors z ∈ R^d.
- **Evidence anchors:**
  - [abstract]: "if an adversary controls the seed of the generative model, a single synthetic data point can leak as much information as releasing the model itself"
  - [section 3.1, Proposition 3.1]: For any fixed z, there exist adjacent datasets where T(Vz, Wz) = T(V, W)
  - [corpus]: Corpus weak for this specific adversarial seed attack; most related work assumes API-level query access without seed control
- **Break condition:** If the defender restricts seed choices or adds additional noise at inference time, the attack may not achieve maximum leakage.

### Mechanism 3
- **Claim:** The product of a shifted Gaussian matrix with an independent Gaussian matrix converges to a Gaussian distribution in total variation distance, with rate O(√(nl/d)) when d ≥ max{n, l}.
- **Mechanism:** Decompose (σ_θN + v)Z into independent summands; apply multivariate CLT via Theorem 4.2 (Bally & Caramellino, 2016). The shift v only affects the variance of the limiting Gaussian, not the convergence rate, because the Gaussian Z averages out the shift across dimensions.
- **Core assumption:** d is sufficiently large relative to n and l; the noise scale σ_θ and input scale σ_z are fixed.
- **Evidence anchors:**
  - [section 4.3, Theorem 4.5]: TV((σ_θN + v)Z, σ_θ√(d-s)G + vZ') ≤ C'√(nls/(d-s))
  - [section 4.2, Lemma 4.3]: Univariate case achieves O(1/d) convergence
  - [corpus]: Li & Woodruff (2021) proved Gaussian product convergence (cited in paper), but without the shift analysis
- **Break condition:** If d ≤ C'' max{n,l}^(1/2) min{n,l}^(3/2), TV distance ≥ 2/3 and Gaussian approximation fails (Theorem 4.7).

## Foundational Learning

- **Concept: Trade-off Functions (f-DP)**
  - Why needed here: This is the primary privacy accounting framework used throughout; all bounds are expressed as T(P, Q) ≥ f
  - Quick check question: Given two distributions P and Q, what does T(P, Q)(α) = 0.9 mean for the Type II error when Type I error is α?

- **Concept: Post-Processing Property of DP**
  - Why needed here: Establishes the baseline upper bound that synthetic data cannot exceed; the paper's contribution is showing this bound can be loose
  - Quick check question: If M is ε-DP and f is any randomized function, what is the privacy guarantee of f(M(D))?

- **Concept: Rényi Differential Privacy (RDP)**
  - Why needed here: Used to interpret amplification bounds numerically; RDP divergences are easier to compose and compare than trade-off functions
  - Quick check question: How does D_α(P, Q) relate to the standard ε-DP guarantee?

## Architecture Onboarding

- **Component map:** Private model training -> Private model parameters (V) -> Synthetic generation with random seeds Z -> Release VZ (synthetic data)
- **Critical path:** Input dimension d must dominate output dimension n and number of released points l for meaningful amplification. The condition d ≥ max{n, l} is necessary for CLT bounds to hold.
- **Design tradeoffs:**
  - Higher d → stronger amplification but potentially lower utility (model may be undertrained)
  - More released points l → more utility but O(1/√d) amplification instead of O(1/d)
  - Revealing the random seed Z → zero amplification; must treat Z as secret
- **Failure signatures:**
  - If amplification appears too good (e.g., diverging from post-processing bound by >10×), verify that Z is truly hidden from the adversary
  - If RDP bounds diverge to +∞, the trade-off function may have zf < 1; use the composite bound h(α) = max{T(V,W)(α), g̃(α)} instead of g̃ alone
  - If d is small (< 10× max{n,l}), CLT approximation may be invalid; empirical privacy auditing recommended
- **First 3 experiments:**
  1. **Baseline verification:** Implement output perturbation for linear regression, generate synthetic data with random Z, and numerically estimate the trade-off function T(VZ, WZ) via Monte Carlo. Compare to theoretical bounds from Theorem 4.3.
  2. **Adversarial seed attack:** Implement the attack from Proposition 3.1—choose z as the top singular vector of the estimated shift μ. Verify that T(Vz, Wz) ≈ T(V, W) for constructed adjacent datasets.
  3. **Scaling study:** Vary d from 10 to 1000 while fixing n=1, l=1. Plot log(Rényi divergence) vs log(d) to verify O(1/d) scaling. Then vary l to confirm O(1/√d) scaling for multiple points.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical results require d ≥ max{n, l} for CLT bounds to hold, which may not be satisfied in low-dimensional settings
- Privacy amplification relies critically on keeping the random seed Z secret from the adversary - this assumption may be challenging to enforce in practice
- Empirical validation on real datasets is limited; the paper focuses on theoretical bounds rather than practical privacy-utility tradeoffs

## Confidence
- **Medium** - The paper provides rigorous theoretical analysis, but several assumptions limit practical applicability. The main theoretical results require d ≥ max{n, l} for CLT bounds to hold, which may not be satisfied in low-dimensional settings.
- **High** - The adversarial seed attack (Mechanism 2) is proven optimal for rank-1 shifts, but its applicability to general DP settings remains unclear.
- **Medium** - While the paper establishes theoretical bounds on privacy amplification, empirical validation on real datasets is limited.

## Next Checks
1. **Adversarial Seed Robustness:** Implement the adversarial seed attack from Proposition 3.1 on a real linear regression dataset and verify that releasing a single synthetic point with controlled seed indeed leaks as much information as releasing the full model parameters. Test variations where the seed is partially revealed or corrupted.

2. **Low-Dimensional Scaling:** Design experiments specifically for d < max{n, l} to understand when the CLT approximation breaks down. Measure actual privacy leakage via Rényi divergence and compare against theoretical bounds. Identify the minimum d required for meaningful amplification.

3. **Real-World Privacy-Utility Tradeoff:** Apply the synthetic data generation framework to a benchmark dataset (e.g., medical data with linear regression tasks). Release varying numbers of synthetic points and measure both privacy (via RDP or DP-SGD auditing) and utility (prediction accuracy on held-out real data). Compare against baseline of releasing model parameters directly.