---
ver: rpa2
title: 'When Thinking Drifts: Evidential Grounding for Robust Video Reasoning'
arxiv_id: '2510.06077'
source_url: https://arxiv.org/abs/2510.06077
tags:
- video
- reasoning
- visual
- person
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the "Visual Thinking Drift"
  problem in video reasoning, where Chain-of-Thought (CoT) prompting causes models
  to generate verbose but ungrounded reasoning chains that hallucinate visual details.
  The authors introduce Visual Evidence Reward (VER), a reinforcement learning framework
  that rewards reasoning traces explicitly grounded in visual evidence.
---

# When Thinking Drifts: Evidential Grounding for Robust Video Reasoning

## Quick Facts
- **arXiv ID:** 2510.06077
- **Source URL:** https://arxiv.org/abs/2510.06077
- **Reference count:** 40
- **Primary result:** Visual Evidence Reward (VER) framework improves video reasoning accuracy by up to 9.0% absolute points across 10 benchmarks.

## Executive Summary
This paper identifies and addresses the "Visual Thinking Drift" problem in video reasoning, where Chain-of-Thought (CoT) prompting causes models to generate verbose but ungrounded reasoning chains that hallucinate visual details. The authors introduce Visual Evidence Reward (VER), a reinforcement learning framework that rewards reasoning traces explicitly grounded in visual evidence. An auxiliary LLM judges the alignment between reasoning steps and observable video content, providing a binary reward signal. Across 10 diverse video understanding benchmarks, the proposed Video-VER model consistently achieves top performance, improving accuracy by up to 9.0% absolute points and averaging +4.0% gains over base models. The results demonstrate that grounding reasoning in visual evidence, rather than encouraging verbosity, is essential for robust video intelligence.

## Method Summary
The VER framework extends GRPO with an evidence-based reward. The policy model (Qwen2.5-VL-7B) generates CoT reasoning and final answer; trained via SFT then GRPO. Visual evidence is generated offline by Qwen2.5-VL-72B using inverted prompting (video + GT answer → visual facts). An LLM judge (Llama-3.1-70B-Instruct) provides binary grounding assessment. The composite reward combines accuracy, length (320-512 tokens), format, and evidence reward (α=0.3). Training uses 8 H200 GPUs, GRPO group size G=8, RL iters=2,000.

## Key Results
- VER improves accuracy by up to 9.0% absolute points across 10 video reasoning benchmarks
- Video-VER consistently achieves top performance, averaging +4.0% gains over base models
- Question-dependent visual evidence (QD-VE) outperforms generic captions on 9/10 benchmarks
- More frames improve performance (Table 3) but increase compute; 32 frames optimal for most benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Visual Thinking Drift Detection
Chain-of-Thought prompting in video MLLMs can degrade accuracy by amplifying language priors over visual evidence, especially in open-source models. In autoregressive generation, the softmax at each step combines a language prior term and a visual likelihood term. The paper observes that ‖W_lang‖ ≫ ‖W_vis‖ in practice, and as generation proceeds, self-attention increasingly focuses on already-generated tokens rather than video features. A single hallucinated visual detail propagates through subsequent reasoning steps with no backward correction. The failure rate scales roughly linearly with chain length T (≈ Tε for small per-step error ε).

### Mechanism 2: Visual Evidence Reward (VER)
A binary reward signal indicating whether reasoning traces reference verifiable visual evidence can shift policy behavior toward grounded reasoning. An auxiliary LLM judge evaluates alignment between generated CoT and question-dependent visual evidence, outputting e_i ∈ {0,1}. The evidence-augmented reward r_evid = r_base + α if both correct and grounded (α=0.3), else r_base only. GRPO optimizes the policy using group-relative advantages with KL regularization.

### Mechanism 3: Inverted Prompting for Evidence Generation
Generating visual evidence conditioned on (question, ground-truth answer) pairs produces lower-entropy, more relevant supervision than standard CoT generation. Rather than sampling p(c_{1:T}, a | q, v) where both chain and answer are uncertain, the evidence generator samples p(e_{1:K} | q, a, v)—conditioning on the known correct answer to retrieve minimal supporting visual facts. This inverted structure forces evidence to directly explain the answer rather than exploring a broader reasoning space.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Why needed here: The VER framework extends GRPO with an evidence-based reward. Understanding baseline GRPO (group sampling, advantage normalization, clipped objectives) is prerequisite to grasping how evidence rewards integrate. Quick check: Can you explain why GRPO uses group-relative advantages rather than absolute rewards?

- **Visual-Linguistic Grounding in MLLMs**: Why needed here: The paper's core claim is that ungrounded reasoning drifts. Understanding what "grounding" means—referencing specific, verifiable visual content—is essential. Quick check: Given a CoT trace mentioning "the person wears a red shirt," what would constitute evidence that this claim is grounded vs. hallucinated?

- **Self-Consistency Decoding**: Why needed here: Appendix A shows majority voting over multiple CoT samples improves accuracy, revealing that CoT traces are stochastic. This motivates the need for explicit grounding rather than relying on sampling diversity. Quick check: Why does self-consistency improve accuracy but not address the root cause of visual thinking drift?

## Architecture Onboarding

- **Component map**: Policy model (Qwen2.5-VL-7B) -> Evidence generator (Qwen2.5-VL-72B, offline) -> LLM judge (Llama-3.1-70B-Instruct) -> Policy update

- **Critical path**: 1) SFT phase on CoT-annotated data bootstraps basic reasoning capability 2) Generate visual evidence for GRPO dataset using inverted prompting (offline) 3) GRPO training: sample G=8 responses per question, compute accuracy + evidence rewards, normalize within group, update policy

- **Design tradeoffs**: Question-dependent evidence vs. generic captions (QD-VE outperforms on 9/10 benchmarks but requires running a strong MLLM per question); Binary vs. continuous reward (binary is more robust to noise but provides weaker gradient signal); Frame count (8/16/32) (more frames improve performance but increase compute; 32 frames optimal for most benchmarks)

- **Failure signatures**: Incomplete frame sampling (if critical visual details are missed during encoding, even grounded reasoning fails); LLM judge errors (false positives reward ungrounded traces; false negatives waste training signal); Teacher hallucinations propagate (if inverted prompting generates plausible-but-false evidence, policy may learn incorrect grounding patterns)

- **First 3 experiments**: 1) Ablate evidence type: Compare QD-VE vs. VC vs. no evidence reward on 2-3 benchmarks to validate grounding mechanism 2) Vary frame count: Test 8/16/32 frames to confirm scalability and identify compute-accuracy tradeoff for your hardware 3) Inspect judge reliability: Sample 50 training examples, manually verify LLM judge outputs correlate with human grounding assessment

## Open Questions the Paper Calls Out

- How can the Visual Evidence Reward framework be effectively adapted for open-ended tasks like free-form video QA, where fixed rule-based rewards are inapplicable? The conclusion states that "Extending this framework to open-ended tasks, such as free-form QA... presents an exciting and crucial avenue for future work."

- Can the Visual Evidence Reward mechanism maintain grounding in very long videos (e.g., hours long) where critical information is sparse or temporally distant? The limitations section notes that "extending robust, fine-grained reasoning to scenarios with very long videos... remains an important direction for future work."

- To what extent does the performance of Video-VER depend on the specific capabilities or biases of the auxiliary LLM judge used to determine the evidence reward? The authors acknowledge that "the quality of the reward signal is inherently connected to the chosen LLM's capabilities" and suggests optimizing judge models is a future need.

## Limitations

- The entire VER framework depends on Llama-3.1-70B-Instruct correctly identifying grounded vs. ungrounded reasoning, but systematic error rates for the judge remain unquantified
- The inverted prompting strategy assumes Qwen2.5-VL-72B reliably extracts question-relevant visual facts when given the answer, but systematic analysis of teacher hallucination rates is absent
- Performance gains may not generalize to domains or video types not represented in the Video-R1 training datasets

## Confidence

- **High Confidence**: The empirical observation that CoT prompting often degrades video reasoning accuracy (up to 4.4% drops observed), and that grounding reasoning in visual evidence improves performance across multiple benchmarks
- **Medium Confidence**: The mechanism explaining why visual thinking drift occurs (softmax combining language prior and visual likelihood, with language prior dominating), and the proposed solution of binary evidence rewards
- **Low Confidence**: The specific design choices for the evidence reward (binary vs. continuous, α=0.3 weight, inverted prompting details) and their sensitivity to hyperparameter variations

## Next Checks

1. **Judge Error Analysis**: Manually annotate 200 random training examples with human grounding judgments, then compute precision/recall/F1 for the LLM judge. This quantifies the reliability of the binary reward signal and identifies whether false positives or false negatives dominate.

2. **Teacher Hallucination Audit**: For 100 randomly selected evidence generation examples, verify whether each generated visual fact is actually present in the video frames. Calculate the hallucination rate and test whether policy performance correlates with evidence veracity.

3. **Domain Generalization Test**: Evaluate Video-VER on at least two video reasoning datasets not used in training (e.g., TGIF-QA, PororoQA) to assess whether grounding-based reasoning generalizes beyond the Video-R1 domain. Compare against both base CoT and strong non-CoT baselines.