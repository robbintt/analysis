---
ver: rpa2
title: 'FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority
  Groups'
arxiv_id: '2502.06695'
source_url: https://arxiv.org/abs/2502.06695
tags:
- spurious
- fairdropout
- group
- neurons
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairDropout, a method that improves generalization
  for minority groups in datasets with spurious correlations. The authors observe
  that models trained with empirical risk minimization (ERM) tend to memorize minority
  group examples, leading to poor worst-group accuracy.
---

# FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority Groups

## Quick Facts
- arXiv ID: 2502.06695
- Source URL: https://arxiv.org/abs/2502.06695
- Reference count: 15
- This paper introduces FairDropout, a method that improves generalization for minority groups in datasets with spurious correlations.

## Executive Summary
This paper introduces FairDropout, a method that improves generalization for minority groups in datasets with spurious correlations. The authors observe that models trained with empirical risk minimization (ERM) tend to memorize minority group examples, leading to poor worst-group accuracy. Building on this insight, FairDropout applies example-tied dropout to redirect memorization to specific neurons, which are then dropped during inference. This approach redistributes the "memorization burden" more fairly across examples, improving worst-group accuracy without requiring group labels during training. The method is evaluated on a benchmark suite spanning vision, language, and healthcare tasks, including CelebA, MetaShift, Waterbirds, MultiNLI, and MIMIC-CXR. FairDropout consistently outperforms ERM and state-of-the-art methods for handling spurious correlations and class imbalance, achieving the highest worst-group accuracy on four of the five datasets. Notably, FairDropout can be combined with classifier retraining techniques like DFR, further boosting performance. The results demonstrate that FairDropout effectively reduces reliance on spurious features and enhances robustness to subpopulation shifts.

## Method Summary
FairDropout addresses spurious correlations by isolating memorization to a dedicated neuron pool that can be pruned at inference. A parameter-free FairDropout layer is inserted into the network, splitting neurons into "generalizing" (`pgen`) and "memorizing" (`1 - pgen`) paths. During training, an example-tied dropout allocates a fixed, randomly sampled subset of memorizing neurons to each example. This "fair" allocation forces the model to learn spurious correlations within this designated pool. At inference, the entire memorizing pool is dropped, forcing the model to rely on the generalizing neurons. The method requires no group labels during training and is evaluated across vision, language, and healthcare tasks, consistently outperforming ERM and state-of-the-art methods for handling spurious correlations and class imbalance.

## Key Results
- FairDropout achieves the highest worst-group accuracy on four of five benchmark datasets (CelebA, MetaShift, Waterbirds, MultiNLI, MIMIC-CXR).
- The method improves worst-group accuracy without requiring group labels during training.
- FairDropout can be combined with classifier retraining techniques like DFR for further performance gains.
- Consistently outperforms ERM and state-of-the-art methods for handling spurious correlations and class imbalance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minority group examples are disproportionately memorized by specific, localized neurons, which can be identified and removed to improve generalization.
- **Mechanism:** Under ERM, models generalize well for majority groups but memorize atypical examples from minority groups using a small set of "critical" neurons. These neurons are disproportionately responsible for the model's prediction on minority examples but have a smaller impact on overall training accuracy. Dropping these neurons forces the model to rely on generalizing features instead.
- **Core assumption:** Memorization can be localized to a limited number of neurons, and minority group examples are more prone to this than majority ones.
- **Evidence anchors:**
  - [abstract] "Building on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout... to redirect this memorization to specific neurons that we subsequently drop out during inference."
  - [section 3.2] "...(i) the number of neurons required to flip each prediction from the minority group is considerably lower than the corresponding number for majority groups... (ii) these neurons have a smaller effect on training worst-group accuracy compared to those from the majority group."
  - [corpus] Corpus signals are weak for this specific mechanism; evidence is primarily from the paper's analysis.

### Mechanism 2
- **Claim:** FairDropout isolates memorization into a dedicated neuron pool, which is then pruned at inference.
- **Mechanism:** A parameter-free FairDropout layer is inserted into the network, bifurcating the neuron stream into "generalizing" (`pgen`) and "memorizing" (`1 - pgen`) paths. During training, an example-tied dropout allocates a fixed, randomly sampled subset of memorizing neurons to each example. This "fair" allocation forces the model to learn spurious correlations within this designated pool. At inference, the entire memorizing pool is dropped.
- **Core assumption:** The example-tied allocation and dedicated pool successfully contain the spurious features, leaving the generalizing neurons cleaner.
- **Evidence anchors:**
  - [abstract] "...aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference."
  - [section 3.3] "As an example-tied dropout, FairDropout is a layer without learnable parameters, that divides neurons into two types... During testing, the memorizing ones are dropped."
  - [corpus] Corpus signals are weak; related work focuses on re-weighting or robust optimization rather than this isolation mechanism.

### Mechanism 3
- **Claim:** The method improves worst-group accuracy by correcting the overfitting/memorization behavior that ERM exhibits on minority examples.
- **Mechanism:** The paper's analysis shows a large generalization gap for minority groups under ERM, indicative of overfitting/memorization. By surgically removing the neurons responsible for this behavior at test time, the model's reliance on spurious, memorized patterns is eliminated, leading to a significant rebound in worst-group accuracy.
- **Core assumption:** The neurons identified as "memorizing" are primarily responsible for the poor generalization on minority groups.
- **Evidence anchors:**
  - [abstract] "...reducing memorization of minority group examples... Experiments... show FairDropout significantly improves worst-group accuracy..."
  - [section 3.2] "We observe that for approximately 75% of these drops, the worst-group accuracy significantly improves."
  - [corpus] The problem of ERM exploiting spurious features is confirmed by related titles like *Mitigating Shortcut Learning...*.

## Foundational Learning

**Concept:** **Spurious Correlations and Subpopulation Shift**
- **Why needed here:** This is the core problem the paper solves. ERM models exploit features (e.g., water background) that correlate with labels (e.g., waterbird) in training but fail on minority groups where the correlation doesn't hold (e.g., landbird on water).
- **Quick check question:** Why does optimizing for average accuracy often fail for minority groups in real-world datasets?

**Concept:** **Memorization vs. Generalization in Deep Learning**
- **Why needed here:** The paper's central hypothesis is that minority group failure is a memorization problem. Understanding how memorization can be isolated to specific neurons is critical for grasping the FairDropout logic.
- **Quick check question:** In the paper's analysis, what is the key difference in how "critical neurons" affect minority vs. majority group predictions?

**Concept:** **Example-Tied Dropout**
- **Why needed here:** This is the novel technical contribution. Unlike standard dropout, it assigns a fixed set of "memorizing" neurons to each example, creating a controllable path for spurious features that can be pruned.
- **Quick check question:** How does the allocation of neurons in FairDropout differ from standard dropout during training?

## Architecture Onboarding

**Component map:** A FairDropout layer is inserted after an intermediate layer (e.g., a ResNet block or BERT projection). It has no learnable parameters. It splits the incoming activations into a "generalizing" stream and a "memorizing" stream, applying example-tied dropout to the latter.

**Critical path:**
1. **Placement:** Insert `FairDropout` after a chosen layer.
2. **Training:** Pass data through the model. For each example, the layer activates a fixed random subset of memorizing neurons. Train with standard ERM.
3. **Inference:** The layer operates in "test mode," dropping all neurons designated as memorizing (`1 - pgen`). The final prediction uses only the generalizing neurons.

**Design tradeoffs:**
- **Placement:** Later layers may isolate spurious features better but could also disrupt abstract representations. Earlier layers may be too low-level. This must be tuned.
- **Hyperparameters:** `pgen` controls the trade-off between model capacity for generalization vs. memorization. A low `pgen` may discard useful features; a high `pgen` may not leave enough capacity for memorization to be isolated.

**Failure signatures:** If test-time worst-group accuracy does not improve, it suggests the memorizing neurons were not effectively isolated or the spurious features were not captured in the designated pool.

**First 3 experiments:**
1. **Replicate Figure 2:** Train an ERM model and run the neuron-criticality analysis. Confirm that minority examples rely on fewer neurons and that dropping them has a smaller impact on training accuracy than for majority examples.
2. **Sanity Check on CelebA:** Implement FairDropout and train on CelebA. Verify the training-mode vs. test-mode behavior shown in Figure 5 (training mode mimics ERM, test mode shows improved worst-group accuracy).
3. **Hyperparameter Sweep:** Perform a sweep over `pgen`, `pmem`, and layer position using worst-class accuracy on the validation set as the guiding metric, as recommended in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does FairDropout inadvertently degrade performance in learning scenarios where memorization is strictly beneficial, such as in long-tailed distributions where rare examples must be retained?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations section that "memorization can sometimes be beneficial for generalization... an aspect that warrants further investigation, particularly for certain applications and datasets."
- **Why unresolved:** The current study evaluates FairDropout primarily on subpopulation shift benchmarks where memorization is linked to spurious correlations, without testing on datasets designed to require memorization for optimal generalization.
- **What evidence would resolve it:** Empirical results from datasets specifically constructed to measure the benefits of memorization (e.g., those with rare but informative examples) comparing FairDropout against standard ERM.

### Open Question 2
- **Question:** To what extent do the "generalizing neurons" in the FairDropout architecture remain free of spurious correlations compared to standard ERM representations?
- **Basis in paper:** [explicit] The paper states that "FairDropout implicitly assumes that generalizing neurons are less likely to memorize examples... this hypothesis requires further exploration."
- **Why unresolved:** While the drop in test error suggests the mechanism works, the paper does not provide a direct analysis of the learned features within the generalizing neurons to confirm they are uncontaminated by spurious attributes.
- **What evidence would resolve it:** A representation learning analysis (e.g., probing classifiers) applied specifically to the activations of the generalizing neurons to quantify their dependence on spurious features versus core features.

### Open Question 3
- **Question:** Is there a systematic heuristic for determining the optimal layer depth for FairDropout insertion based on the semantic level of the spurious feature?
- **Basis in paper:** [inferred] The authors note that "optimal placement... depends on the dataset, as spurious correlations exhibit task-specific levels of abstraction," and currently rely on hyperparameter tuning to find the position.
- **Why unresolved:** The method treats the layer position as a tunable hyperparameter rather than deriving it from the network's architecture or the nature of the data, leaving the relationship between feature hierarchy and optimal placement unclear.
- **What evidence would resolve it:** A controlled ablation study correlating the abstraction level of the spurious feature (e.g., texture vs. background vs. textual negation) with the performance of FairDropout inserted at different network depths.

## Limitations
- The method's success critically depends on identifying a clear split between memorizing and generalizing neurons, which may not hold for all architectures or datasets.
- The hyperparameter search (pgen, pmem, layer position) is essential and could be costly for larger models.
- The paper acknowledges that in cases where memorization aids generalization (e.g., certain out-of-distribution shifts), FairDropout could harm performance.

## Confidence

**High Confidence:** The core mechanism of using example-tied dropout to isolate spurious memorization, and the observation that minority group examples rely on fewer critical neurons, is well-supported by the paper's analysis and experiments.

**Medium Confidence:** The broad effectiveness across diverse tasks (vision, language, healthcare) is demonstrated, but the specific performance gains may vary depending on the dataset's structure and the quality of hyperparameter tuning.

**Low Confidence:** The method's robustness to different network architectures (beyond ResNet-50 and BERT) and its performance in extremely long-tail distribution scenarios are not fully explored.

## Next Checks

1. **Neuronal Criticality Verification:** Reproduce the analysis from Section 3.2 to confirm that minority group examples in a new dataset are indeed predicted by a smaller, more localized set of neurons compared to majority groups.

2. **Memorizaton vs. Generalization Ablation:** Design an experiment where FairDropout is applied to a dataset where memorization is known to be beneficial. Measure if worst-group accuracy degrades, as the paper suggests could happen.

3. **Architecture Scaling Test:** Implement FairDropout on a larger, more complex architecture (e.g., ViT for images or RoBERTa for text) and assess if the method's effectiveness scales or if new challenges emerge.