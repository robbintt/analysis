---
ver: rpa2
title: 'PowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large
  Language Models'
arxiv_id: '2501.07639'
source_url: https://arxiv.org/abs/2501.07639
tags:
- power
- graph
- grid
- llms
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces PowerGraph-LLM, the first framework to solve
  Optimal Power Flow (OPF) problems using Large Language Models (LLMs). The approach
  combines graph and tabular representations of power grids to effectively query LLMs,
  capturing complex relationships and constraints in power systems.
---

# PowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models

## Quick Facts
- arXiv ID: 2501.07639
- Source URL: https://arxiv.org/abs/2501.07639
- Reference count: 9
- Introduces first LLM-based framework for Optimal Power Flow (OPF) problems

## Executive Summary
This study introduces PowerGraph-LLM, the first framework to solve Optimal Power Flow (OPF) problems using Large Language Models (LLMs). The approach combines graph and tabular representations of power grids to effectively query LLMs, capturing complex relationships and constraints in power systems. A new implementation of in-context learning and fine-tuning protocols for LLMs is introduced, tailored specifically for the OPF problem. The results show that larger models perform better, with fine-tuning significantly improving accuracy and reducing invalid outputs. Graph representations become more effective than tabular ones after fine-tuning. The fine-tuned Llama-8b model achieves MSE values of 6.42×10⁻³ for generators, 6.18×10⁻² for slack buses, and 6.29×10⁻⁴ for buses, with a validity rate of 99.7%.

## Method Summary
The framework uses IEEE 9-bus and 30-bus grids with 65,000 context pairs plus 1,000 evaluation pairs. Power grid components (buses, generators, loads, lines, slack) are represented with features X_b, X_l, X_g, X_s, X_e, and labels Y_g, Y_s, Y_b. Loads are mutated ±20% uniformly. The method employs Llama-3.1-8B/70B-Instruct and GPT-4o-mini/4o models with in-context learning using 65 examples plus 1 query in a 4096-token context. LoRA fine-tuning with r=8, α=16 is applied, using Q4_K_M quantization and ollama v0.5.0 inference. Hugging Face is used for fine-tuning. The approach compares graph versus tabular representations and evaluates MSE for generators, slack buses, and buses, along with validity rate for parseable JSON outputs.

## Key Results
- Fine-tuned Llama-8b achieves MSE values of 6.42×10⁻³ for generators, 6.18×10⁻² for slack buses, and 6.29×10⁻⁴ for buses
- Post-fine-tuning validity rate reaches 99.7% for parseable JSON outputs
- Graph representations outperform tabular ones only after fine-tuning
- Larger models (Llama-3.1-70B) perform better than smaller variants

## Why This Works (Mechanism)
None specified in the paper.

## Foundational Learning
- Optimal Power Flow (OPF): The fundamental problem of determining optimal dispatch of power generation to meet demand while respecting physical and operational constraints. Needed because it's the core problem being solved.
- Graph Embeddings for Power Systems: Representing power grid topology and electrical relationships as graph structures rather than raw numerical tables. Needed because the paper claims graph representations become superior after fine-tuning.
- In-Context Learning vs Fine-Tuning: Two approaches for adapting LLMs to specific tasks - in-context learning uses examples in the prompt, while fine-tuning updates model weights. Needed because the paper systematically compares both approaches.
- LoRA Fine-Tuning: Low-Rank Adaptation technique that efficiently fine-tunes LLMs by modifying smaller adapter layers rather than full model weights. Needed because this is the specific fine-tuning method employed.
- MSE and Validity Metrics: Mean Squared Error measures prediction accuracy while validity rate measures JSON parseability of outputs. Needed because these are the primary evaluation metrics used.

## Architecture Onboarding

**Component Map**
IEEE Grid Data -> Feature Extraction -> Graph/Tabular Representation -> LLM (Llama-3.1 or GPT-4o) -> OPF Solution Prediction -> MSE/Validity Evaluation

**Critical Path**
Grid data preparation and feature extraction → Representation choice (graph/tabular) → LLM inference with in-context learning → Fine-tuning (if applicable) → Solution prediction and evaluation

**Design Tradeoffs**
- Graph vs tabular representation: Tabular performs better pre-fine-tuning, graph superior post-fine-tuning
- Model size: Larger models (70B) outperform smaller ones (8B) but at higher computational cost
- In-context learning vs fine-tuning: Fine-tuning significantly improves accuracy and reduces invalid outputs

**Failure Signatures**
- High invalid output rate (>50%) without fine-tuning, especially with Llama models
- Graph representation underperforms tabular without fine-tuning
- Physical constraint violations may still occur despite high validity rates

**First Experiments**
1. Generate dataset using PandaPower/MatPower with SafePowerGraph-style embeddings for 9-bus and 30-bus grids
2. Run in-context inference with 65 examples + 1 query using Llama-3.1-8B-Instruct and evaluate MSE/validity
3. Apply LoRA fine-tuning and compare performance between graph and tabular representations

## Open Questions the Paper Calls Out
1. Can the framework maintain high validity and low MSE on large-scale grids (e.g., IEEE 118-bus) given token limitations?
2. What mechanistic interaction causes fine-tuning to invert relative performance of graph versus tabular representations?
3. Does the framework require post-processing to guarantee physical feasibility of OPF solutions?

## Limitations
- Missing fine-tuning hyperparameters (learning rate, batch size, epochs, optimizer) prevent exact reproduction
- Exact feature schema and mutation procedure for grid embeddings not specified
- Complete JSON schema and system prompt text unavailable

## Confidence
- Fine-tuning significantly improves accuracy: Medium
- Graph representations become superior post-fine-tuning: Medium  
- This is first LLM-based OPF framework: High (verifiable through literature)
- Scalability to large grids: Low (not tested beyond 30-bus)

## Next Checks
1. Implement fine-tuning procedure with systematic hyperparameter sweeps to verify performance improvements
2. Conduct controlled experiments comparing graph vs tabular representations across different model sizes and fine-tuning conditions
3. Test framework on additional grid sizes (e.g., 14-bus, 57-bus) to evaluate scalability and generalizability