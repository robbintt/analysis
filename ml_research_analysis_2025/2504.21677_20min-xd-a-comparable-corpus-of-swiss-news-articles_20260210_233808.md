---
ver: rpa2
title: '20min-XD: A Comparable Corpus of Swiss News Articles'
arxiv_id: '2504.21677'
source_url: https://arxiv.org/abs/2504.21677
tags:
- articles
- similarity
- article
- dataset
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents 20min-XD, a French-German document-level comparable
  corpus of news articles sourced from the Swiss online news outlet 20 Minuten/20
  minutes. The dataset comprises around 15,000 article pairs spanning 2015 to 2024,
  automatically aligned based on semantic similarity using a paraphrase-multilingual-mpnet
  model with intersection alignment strategy at a threshold of 46.
---

# 20min-XD: A Comparable Corpus of Swiss News Articles

## Quick Facts
- arXiv ID: 2504.21677
- Source URL: https://arxiv.org/abs/2504.21677
- Reference count: 8
- 15,000 French-German article pairs from 2015-2024, aligned using semantic similarity with intersection strategy at threshold 46

## Executive Summary
This paper presents 20min-XD, a document-level comparable corpus of news articles from the Swiss outlet 20 Minuten/20 minutes. The dataset contains approximately 15,000 French-German article pairs spanning 2015-2024, automatically aligned based on semantic similarity using a paraphrase-multilingual-mpnet model. The resulting corpus exhibits a broad spectrum of cross-lingual similarity, from near-translations to loosely related articles, making it valuable for research on cross-lingual text similarity, bitext mining, and cross-lingual difference recognition.

## Method Summary
The corpus was constructed by scraping 593,897 articles from 20 Minuten/20 minutes FR and DE editions (2015-2024), extracting title, lead, and content. Articles were encoded using paraphrase-multilingual-mpnet, with cosine similarity computed between French-German pairs sharing the same publication date. The intersection alignment strategy was applied at threshold 46, determined by F1 optimization on a small validation set of 14 manually aligned pairs. Post-processing removed error pages and same-language duplicates, with final sentence-level alignment performed using the same model and strategy.

## Key Results
- 15,000 French-German article pairs covering 2015-2024
- F1 score of 64.7 achieved with paraphrase-multilingual-mpnet + intersection strategy
- Weak positive correlations: cosine similarity vs. AlignRatio (r=0.145 FR, r=0.103 DE) and vs. monotonicity (r=0.147)
- Distribution of similarity scores is right-skewed, ranging from near-translations to loosely related articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding only title and lead text provides sufficient signal for cross-lingual document alignment while avoiding sequence length constraints.
- Mechanism: The paraphrase-multilingual-mpnet model encodes concatenated title+lead as dense vectors; cosine similarity quantifies semantic relatedness between French-German article pairs.
- Core assumption: Title and lead capture the essential topical content that distinguishes comparable articles from unrelated ones.
- Evidence anchors:
  - [section 3.2] "concatenating the article's title and lead provides a sufficiently strong signal for document alignment"
  - [section 4.2.2] Weak positive correlation (r=0.145 FR, r=0.103 DE) between document cosine similarity and AlignRatio suggests title+lead alignment partially predicts full-text overlap
  - [corpus] Related work on comparable corpora (Building and Aligning Comparable Corpora) supports topic-based alignment approaches
- Break condition: If articles share events but have misleading headlines, or if headlines differ in focus while bodies align, this mechanism degrades.

### Mechanism 2
- Claim: The intersection alignment strategy (mutual best-match) produces higher-quality 1:1 alignments than permissive n:n strategies.
- Mechanism: A French article aligns to a German article only if each is the other's highest-similarity match above threshold—bidirectional optimality constraint.
- Core assumption: Comparable articles in this news domain typically have one primary counterpart per day.
- Evidence anchors:
  - [section 3.2.2] Intersection strategy achieved highest F1 (64.7) on validation set vs. union (54.1) and above-threshold (54.1)
  - [section 3.1] Validation set "only contains 1:1 pairings" supporting the appropriateness of this constraint
  - [corpus] No direct corpus evidence on alignment strategy comparisons; related work focuses on sentence-level alignment
- Break condition: If legitimate n:n article relationships exist (e.g., multiple regional variants of the same story), intersection will miss them.

### Mechanism 3
- Claim: Threshold selection via F1 optimization on a small validation set generalizes to the full corpus despite validation set noise.
- Mechanism: Iterate thresholds 0–100 in 0.5 steps; select threshold maximizing F1 against manually aligned gold pairs; apply threshold 46 globally.
- Core assumption: The validation set (14 pairs from one day) is representative of the decade-spanning corpus.
- Evidence anchors:
  - [section 3.3] "paraphrase-multilingual-mpnet with the alignment strategy intersection at a similarity score threshold of 46, outperforms all other models"
  - [section 3.3] Authors note: "number of samples in our validation set is small... could lead to statistical noise"
  - [corpus] Weak evidence—no corpus papers validate threshold transferability across time periods
- Break condition: If topic distribution or writing style shifted significantly between validation day (unspecified) and other years, optimal threshold may vary.

## Foundational Learning

- Concept: **Comparable vs. parallel corpora**
  - Why needed here: 20min-XD explicitly contains non-translations ranging from near-parallel to loosely related—understanding this spectrum is essential for appropriate use.
  - Quick check question: If you need exact translation pairs for MT training, would you use this dataset as-is or filter it first?

- Concept: **Cosine similarity in embedding space**
  - Why needed here: The entire alignment methodology rests on cosine similarity between multilingual sentence embeddings as a proxy for semantic relatedness.
  - Quick check question: Why might two articles about the same event score lower than two paraphrased sentences on this metric?

- Concept: **Alignment strategy constraints (1:1, n:m, intersection, union)**
  - Why needed here: Paper compares five strategies; choosing among them requires understanding the precision-recall tradeoff each imposes.
  - Quick check question: If you want maximum recall for bitext mining, which strategy would you choose, and what quality risk does it introduce?

## Architecture Onboarding

- Component map:
  - Data scraper -> Preprocessor -> Encoder -> Aligner -> Threshold filter -> Post-processor -> Sentence aligner

- Critical path: Embedding quality -> threshold selection -> intersection constraint -> post-processing filters. Errors in embedding (e.g., domain mismatch) propagate through all downstream steps.

- Design tradeoffs:
  - Title+lead vs. full text: Efficiency vs. granularity (paper acknowledges full-text could improve accuracy)
  - Intersection vs. union: Precision vs. recall (F1 64.7 vs. 54.1 on validation)
  - Fixed threshold vs. adaptive: Simplicity vs. potential topic-specific optimization

- Failure signatures:
  - High-similarity pairs with same-language content (caught by post-processing)
  - Same-topic different-event pairs (e.g., financial crisis articles about different companies)
  - Single-sentence alignments causing spurious monotonicity extremes (clusters at ±1.0 in Figure 5)

- First 3 experiments:
  1. Reproduce validation set F1 scores with different random seeds to quantify threshold stability noise.
  2. Sample 50 pairs from low-similarity tail (46–55 range) and manually assess whether they represent false positives or legitimate loose comparable pairs.
  3. Compare title+lead alignment against full-text embedding on a subset using gte-multilingual-base (which supports longer contexts) to quantify the information loss from the shortcut.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating full article text, rather than just titles and leads, significantly improve the granularity and accuracy of semantic alignment?
- Basis in paper: [explicit] Section 5.1 states that using only titles and leads was a resource-saving constraint and that future work should investigate full-text comparison to capture narrative structure.
- Why unresolved: The current alignment method was constrained by the sequence length limitations of older encoder models and computational resources.
- What evidence would resolve it: A comparative evaluation where a long-context model (e.g., gte-multilingual-base) aligns documents using full text versus the current title+lead method.

### Open Question 2
- Question: Do the monotonicity clusters near scores of -1.00 and 1.00 strictly correspond to article pairs with very few aligned sentences, or are they driven by specific topics?
- Basis in paper: [explicit] Section 4.2.4 notes distinct clusters at these extremes and suggests they may indicate articles with only one or two aligned sentences, a pattern worth investigating further.
- Why unresolved: The paper reports the correlation (r=0.147) and visualizes the clusters but does not perform a qualitative breakdown of the articles constituting these specific clusters.
- What evidence would resolve it: A statistical analysis correlating monotonicity scores with the count of aligned sentences per pair, combined with a topic classification of the outliers.

### Open Question 3
- Question: Can the 20min-XD corpus be effectively extended with fine-grained annotations to support the task of automatic cross-lingual difference recognition?
- Basis in paper: [explicit] Section 5.3 proposes extending the corpus with paragraph- or token-level annotations to enable research into detecting semantic differences between related documents.
- Why unresolved: The dataset currently provides document- and sentence-level alignments but lacks the token-level labels or difference categories required for training difference recognition models.
- What evidence would resolve it: An annotation study creating a "silver standard" of textual differences and a downstream evaluation of a model trained to detect these diffs.

## Limitations
- Small validation set (14 pairs from one day) may not represent decade-long corpus diversity
- Title+lead encoding may miss semantic alignment present in full article bodies
- Intersection strategy may exclude legitimate n:n article relationships in real-world news coverage

## Confidence
- **High confidence**: Corpus construction methodology is clearly specified and reproducible; F1 score comparisons across alignment strategies are transparent and verifiable
- **Medium confidence**: Threshold transferability from single-day validation set to full corpus is methodologically sound but potentially brittle given topic/style evolution over ten years
- **Low confidence**: Quality distribution of full corpus beyond validation set cannot be independently verified without manual annotation of representative sample

## Next Checks
1. **Threshold stability analysis**: Reproduce validation set F1 scores with 10-fold bootstrapping on the 14 pairs to quantify threshold sensitivity and statistical noise.
2. **Representative sampling audit**: Manually annotate 100 randomly selected article pairs from final corpus (stratified by similarity score) to assess precision across similarity spectrum.
3. **Temporal drift assessment**: Compare alignment performance on validation set against 5 additional days randomly sampled from 2015, 2019, and 2023 to test threshold generalizability across time periods.