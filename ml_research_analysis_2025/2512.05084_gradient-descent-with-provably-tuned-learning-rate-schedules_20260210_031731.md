---
ver: rpa2
title: Gradient Descent with Provably Tuned Learning-rate Schedules
arxiv_id: '2512.05084'
source_url: https://arxiv.org/abs/2512.05084
tags:
- most
- degree
- learning
- functions
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how to tune hyperparameters in gradient-based\
  \ optimization, especially in multi-task settings where optimal parameters vary\
  \ across tasks. The main technical contribution is showing that for a broad class\
  \ of non-convex, non-smooth functions\u2014including piecewise-polynomial and piecewise-Pfaffian\
  \ functions\u2014the dual cost function (number of steps to convergence as a function\
  \ of step-size) has only a logarithmic number of pieces in the function complexity."
---

# Gradient Descent with Provably Tuned Learning-rate Schedules

## Quick Facts
- **arXiv ID:** 2512.05084
- **Source URL:** https://arxiv.org/abs/2512.05084
- **Reference count:** 28
- **Primary result:** Shows tuning step-size and learning-rate schedules for non-convex, non-smooth functions has sample complexity bounds matching convex case up to logarithmic factors, via piecewise monotonicity of convergence steps.

## Executive Summary
This paper analyzes the sample complexity of tuning hyperparameters (especially step-size) in gradient-based optimization for non-convex, non-smooth functions. The core insight is that for piecewise-polynomial or piecewise-Pfaffian functions—including neural networks with ReLU, sigmoid, and tanh activations—the number of gradient descent steps to convergence, as a function of step-size, has only a logarithmic number of "pieces" in the function complexity. This bounded piecewise structure allows application of uniform convergence theorems, yielding sample complexity bounds that match the best known bounds for convex functions up to logarithmic factors. The analysis extends to tuning multiple hyperparameters simultaneously, such as full learning-rate schedules, with gracefully scaling bounds.

## Method Summary
The method analyzes the sample complexity of learning hyperparameters (step-size, momentum, initialization, learning-rate schedules) in gradient-based optimization. It leverages the observation that for a broad class of non-convex, non-smooth functions (piecewise-polynomial or piecewise-Pfaffian), the number of steps to convergence T(η) is piecewise constant with a bounded number of discontinuities as a function of step-size η. By expressing gradient descent iterates as functions of η and counting the critical intervals where convergence behavior changes, the paper bounds the pseudo-dimension of the cost function class. This pseudo-dimension directly yields sample complexity bounds for uniform convergence. The framework applies to vanilla gradient descent and momentum variants, and extends to tuning entire learning-rate schedules.

## Key Results
- **Logarithmic Pieces:** For piecewise-polynomial/Pfaffian functions, the number of pieces in the convergence cost function T(η) is bounded by O((Δ^H) log(H)) where Δ is the polynomial degree and H is the max iteration count.
- **Sample Complexity:** Achieves Õ(H³/ε²) sample complexity for step-size tuning, matching best-known convex function bounds up to logarithmic factors.
- **Schedule Tuning:** Extends to learning full learning-rate schedules with Õ(H⁴/ε²) sample complexity, scaling gracefully with schedule length H.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework ensures sample complexity bounds by analyzing the piecewise monotonicity of convergence steps, not relying on convexity or smoothness.
- **Mechanism:** The analysis tracks how the number of steps to convergence T(η) changes as η varies, proving T(η) is piecewise constant with bounded discontinuities, enabling uniform convergence theorems.
- **Core assumption:** The function class is piecewise-polynomial or piecewise-Pfaffian, ensuring a finite number of pieces.
- **Evidence anchors:** Abstract and page 4 discuss piecewise monotonicity; AutoGD work is contrasted.
- **Break condition:** Non-Pfaffian periodic activations (e.g., sin, cos) invalidate the bounded-piece analysis.

### Mechanism 2
- **Claim:** The location of the i-th iterate x_i can be expressed as a polynomial/Pfaffian function of η, allowing exact counting of critical intervals.
- **Mechanism:** By induction, the update rule preserves the algebraic structure, so if f is polynomial of degree Δ, x_i is a polynomial in η, limiting roots of convergence conditions.
- **Core assumption:** The function f and its gradients are piecewise-polynomial or Pfaffian.
- **Evidence anchors:** Page 5 and 7 detail the polynomial/Pfaffian structure of iterates.
- **Break condition:** If the loss function destroys the polynomial/Pfaffian dependence on η (e.g., infinite oscillation), the counting argument fails.

### Mechanism 3
- **Claim:** The pseudo-dimension of the cost function class provides a tight bound on sample complexity.
- **Mechanism:** The dual cost function is piecewise constant; Lemma 2.2 relates pieces to pseudo-dimension, and Theorem 2.1 translates this to Õ(H³/ε²) sample complexity.
- **Core assumption:** The convergence cost is bounded (by H) and the learning rate space is continuous.
- **Evidence anchors:** Page 3 defines pseudo-dimension and Lemma 2.2; page 4 applies Theorem 3.1.
- **Break condition:** If the dual losses have infinite pieces, the pseudo-dimension becomes infinite, and sample complexity guarantees vanish.

## Foundational Learning

- **Concept: Pseudo-dimension (Pdim)**
  - **Why needed here:** It quantifies the complexity of learning a hyperparameter from a finite sample of tasks, generalizing VC-dimension to real-valued functions.
  - **Quick check question:** If the cost function class had infinite Pdim, could we still guarantee uniform convergence on a finite sample?

- **Concept: Pfaffian Functions**
  - **Why needed here:** This is the most general function class analyzed, including polynomials, exponentials, and common activations (sigmoid, tanh), defining the boundary of theoretical applicability.
  - **Quick check question:** Does the Pfaffian property allow the inclusion of periodic activation functions like sin(x)?

- **Concept: Data-Driven Algorithm Design**
  - **Why needed here:** This paradigm frames hyperparameter tuning as a learning problem over a distribution of tasks, moving beyond worst-case to average-case guarantees.
  - **Quick check question:** Does the framework assume tasks are drawn i.i.d. from a distribution D, or analyze a single task in isolation?

## Architecture Onboarding

- **Component map:** Task distribution D over instances (x, f) -> Hyperparameters (η, γ) -> Gradient Descent/Momentum GD solver -> Cost ℓ(η, x, f) = Iterations to converge (bounded by H) -> Analysis Engine (bounds Pdim via piecewise structure).

- **Critical path:** Verify function class is Piecewise-Pfaffian → Establish x_i as function of η → Bound pieces of ||∇f|| < θ → Apply Lemma 2.2 → Derive Sample Complexity.

- **Design tradeoffs:**
  - **Generality vs. Bounds:** Extending to Pfaffian functions increases sample complexity (logarithmic factors) vs. simple polynomials.
  - **Cost Definition:** Optimizing for "steps to converge" yields different bounds than optimizing for "validation loss," though both are handled.

- **Failure signatures:**
  - **Non-convergence:** If GD doesn't converge within H, cost is capped at H; analysis remains valid but optimizes for best possible within horizon.
  - **Unsupported Activations:** Using non-Pfaffian activations (e.g., sine in SIRENs) invalidates the piecewise counting argument.

- **First 3 experiments:**
  1. **Synthetic Polynomial Validation:** Generate random polynomial functions and initial points. Tune η on a sample of tasks and verify the generalization gap matches the Õ(H³/ε²) bound.
  2. **Activation Boundary Test:** Train a small ReLU network (piecewise polynomial) and a Sigmoid network (piecewise Pfaffian) on MNIST subsets. Compare samples required to find a "good" η against theoretical logarithmic dependencies.
  3. **Schedule Tuning Stress Test:** Tune a full learning rate schedule (H parameters) vs. a single scalar η. Verify if sample complexity scales as H⁴ vs H³ as predicted by Theorems 4.1 vs 3.1.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the analysis be extended to neural networks with periodic trigonometric activation functions (e.g., sine/cosine)?
  - **Basis:** Section 3.2 explicitly identifies these as non-examples of Pfaffian functions.
  - **Why unresolved:** The proof relies on Khovanskii's Theorem for counting components of Pfaffian functions, which doesn't cover periodic functions.
  - **What evidence would resolve it:** A proof of sample complexity bounds for a function class including periodic activations, or a demonstration that piecewise monotonicity fails.

- **Open Question 2:** Can the framework tune hyperparameters for properties beyond convergence speed, like adversarial robustness or distribution shift performance?
  - **Basis:** Appendix A identifies these as "relevant future directions."
  - **Why unresolved:** Current results focus on minimizing validation loss or iteration count, not incorporating robustness constraints.
  - **What evidence would resolve it:** Deriving sample complexity bounds for tuning where the objective includes a robustness or distribution-shift penalty term.

- **Open Question 3:** Is the Õ(H⁴) sample complexity bound for learning the learning-rate schedule tight, or can it be improved to match Õ(H³) for a fixed step-size?
  - **Basis:** Theorem 3.2 establishes Õ(H³) for step-size, while Theorem 4.1 establishes higher Õ(H⁴) for the schedule, with no lower bound provided.
  - **Why unresolved:** The schedule proof may introduce slack not present in the single parameter analysis.
  - **What evidence would resolve it:** A lower bound proof showing Ω(H⁴) is necessary, or an improved upper bound reducing dependence to Õ(H³).

## Limitations
- Analysis is limited to piecewise-polynomial/Pfaffian functions; excludes periodic activations like sine/cosine.
- Theoretical bounds are asymptotic and hide significant constants in Õ notation; practical tightness unclear.
- No empirical validation provided to assess the practical implications of the theoretical results.

## Confidence
- **High Confidence:** The piecewise-polynomial structure of iterates and application of pseudo-dimension to bound sample complexity are mathematically rigorous given assumptions.
- **Medium Confidence:** Extension to piecewise-Pfaffian functions is valid but introduces potentially loose logarithmic dependencies.
- **Medium Confidence:** Claim of matching convex function bounds up to logarithmic factors is correct in form, but practical implications for non-convex optimization remain to be validated empirically.

## Next Checks
1. **Empirical Piecewise Structure Verification:** For a synthetic polynomial of known degree, plot the dual cost function ℓ(η) vs. step-size η and count the number of pieces. Verify it scales as O(Δ^H) as predicted by the theory.

2. **Activation Class Boundary Test:** Implement a small neural network with sine activations (non-Pfaffian). Attempt to apply the framework and verify that the analysis breaks down (e.g., infinite pieces in the cost function).

3. **Sample Complexity Stress Test:** For a fixed synthetic task distribution, measure the actual number of samples required to find an η that achieves (ε, δ)-uniform convergence. Compare this to the theoretical bound Õ(H³/ε²) to assess tightness.