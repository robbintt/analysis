---
ver: rpa2
title: When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual
  Euphemism Detection
arxiv_id: '2508.11831'
source_url: https://arxiv.org/abs/2508.11831
tags:
- fine-tuning
- language
- languages
- sequential
- xlm-r
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates cross-lingual transfer for euphemism detection
  using sequential fine-tuning, comparing it with monolingual and simultaneous fine-tuning
  approaches across five languages: English, Spanish, Chinese, Turkish, and Yoruba.
  Using XLM-R and mBERT, the study examines how performance is affected by language
  pairings, typological features, and pretraining coverage.'
---

# When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection

## Quick Facts
- arXiv ID: 2508.11831
- Source URL: https://arxiv.org/abs/2508.11831
- Reference count: 7
- Sequential fine-tuning with high-resource L1 significantly improves L2 euphemism detection, especially for low-resource languages like Yoruba and Turkish.

## Executive Summary
This paper investigates cross-lingual transfer for euphemism detection using sequential fine-tuning, comparing it with monolingual and simultaneous fine-tuning approaches across five languages: English, Spanish, Chinese, Turkish, and Yoruba. Using XLM-R and mBERT, the study examines how performance is affected by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 significantly improves L2 euphemism detection, particularly for low-resource languages like Yoruba and Turkish, with XLM-R achieving larger gains but also showing greater sensitivity to pretraining gaps and catastrophic forgetting, while mBERT yields more stable but lower improvements. These findings demonstrate sequential fine-tuning as a simple yet effective strategy for enhancing euphemism detection in multilingual models, especially in low-resource settings.

## Method Summary
The study employs sequential fine-tuning, where models are first fine-tuned on a high-resource language (L1) for euphemism detection, then further fine-tuned on a low-resource language (L2). This is compared against monolingual fine-tuning (only L2) and simultaneous fine-tuning (jointly on L1 and L2). Experiments use XLM-R and mBERT as multilingual backbone models. Datasets are constructed for five languages, with Yoruba and Turkish representing low-resource scenarios. Performance is evaluated using F1-score, and the impact of typological similarity, pretraining coverage, and catastrophic forgetting is analyzed.

## Key Results
- Sequential fine-tuning significantly improves euphemism detection in low-resource languages (Yoruba, Turkish) compared to monolingual and simultaneous fine-tuning.
- XLM-R achieves higher gains than mBERT but is more prone to catastrophic forgetting and sensitive to pretraining data gaps.
- Language pairing and typological features influence transfer success, with higher-resource L1 languages providing better initial adaptation.

## Why This Works (Mechanism)
Sequential fine-tuning leverages knowledge from a high-resource language to bootstrap learning in a low-resource language. By first adapting the model to euphemism detection in a resource-rich setting, the model acquires general semantic and contextual patterns useful for euphemism recognition. When fine-tuned on the target low-resource language, the model can more efficiently learn language-specific euphemism cues, benefiting from both cross-lingual transfer and task-specific adaptation. This approach mitigates data scarcity and improves generalization in low-resource settings.

## Foundational Learning
- **Euphemism Detection**: Identifying indirect or polite expressions that replace harsh or taboo terms. *Why needed*: Core task being evaluated; requires nuanced semantic understanding. *Quick check*: Are datasets annotated with euphemism labels and validated for linguistic accuracy?
- **Cross-Lingual Transfer**: Applying knowledge from one language to improve performance in another. *Why needed*: Central mechanism for leveraging high-resource language data. *Quick check*: Are language pairs chosen to test both related and unrelated language families?
- **Sequential Fine-Tuning**: Sequentially adapting a pretrained model to a task in one language, then further adapting to another language. *Why needed*: Main experimental condition to assess transfer benefits. *Quick check*: Is the order of fine-tuning (L1→L2 vs L2→L1) varied to test transfer asymmetry?
- **Catastrophic Forgetting**: Loss of previously learned knowledge when adapting to new tasks or languages. *Why needed*: Explains performance degradation in XLM-R during sequential fine-tuning. *Quick check*: Are metrics tracked after each fine-tuning stage to quantify forgetting?
- **Multilingual Models (XLM-R, mBERT)**: Transformer-based models pretrained on multiple languages. *Why needed*: Backbone architectures enabling cross-lingual transfer. *Quick check*: Are models compared under identical hyperparameter settings?
- **Typological Features**: Structural and grammatical properties of languages (e.g., word order, morphology). *Why needed*: Influences ease of transfer between languages. *Quick check*: Are typological distances quantified and correlated with transfer performance?

## Architecture Onboarding

**Component Map**: Pretrained Multilingual Model → Task-Specific Fine-Tuning (L1) → Cross-Lingual Fine-Tuning (L2) → Evaluation

**Critical Path**: The model is first adapted to euphemism detection in L1, then sequentially fine-tuned on L2 data. Performance on L2 is the primary outcome measure.

**Design Tradeoffs**: Sequential fine-tuning trades off potential forgetting of L1 knowledge for improved L2 performance. XLM-R offers higher capacity and better transfer but at risk of instability; mBERT is more stable but less effective at transfer.

**Failure Signatures**: Catastrophic forgetting in XLM-R manifests as sharp drops in L1 performance after L2 fine-tuning. Poor transfer in mBERT shows as minimal gains from sequential vs. monolingual fine-tuning.

**3 First Experiments**:
1. Compare sequential vs. monolingual fine-tuning for Yoruba euphemism detection using XLM-R.
2. Evaluate catastrophic forgetting by measuring L1 performance after sequential fine-tuning on L2.
3. Test if typological similarity between L1 and L2 correlates with transfer improvement.

## Open Questions the Paper Calls Out
None.

## Limitations
- Evaluation is limited to euphemism detection, restricting generalizability to other NLP tasks.
- Only five languages are studied, which may not capture full diversity of language families and scripts.
- Does not address cultural or contextual factors influencing euphemism usage across languages.
- Analysis of catastrophic forgetting is limited to observable performance drops, without deeper investigation into mitigation strategies.

## Confidence
- High confidence: Sequential fine-tuning improves cross-lingual euphemism detection, especially for low-resource languages.
- Medium confidence: XLM-R outperforms mBERT for this task, though sensitivity to pretraining gaps and architecture differences introduces uncertainty.
- Medium confidence: Sequential fine-tuning is a simple yet effective strategy, but alternative transfer methods and hyperparameter tuning are not explored.

## Next Checks
1. Replicate the sequential fine-tuning approach on a broader set of NLP tasks (e.g., sentiment analysis, named entity recognition) to assess generalizability beyond euphemism detection.
2. Conduct experiments with additional language pairs, including those from underrepresented families or with complex script variations, to better understand typological influences on cross-lingual transfer.
3. Investigate mitigation strategies for catastrophic forgetting observed in XLM-R, such as elastic weight consolidation or rehearsal methods, to improve stability during sequential fine-tuning.