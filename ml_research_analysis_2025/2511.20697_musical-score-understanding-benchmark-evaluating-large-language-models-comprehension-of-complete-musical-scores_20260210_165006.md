---
ver: rpa2
title: 'Musical Score Understanding Benchmark: Evaluating Large Language Models''
  Comprehension of Complete Musical Scores'
arxiv_id: '2511.20697'
source_url: https://arxiv.org/abs/2511.20697
tags:
- level
- musical
- questions
- scores
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Musical Score Understanding Benchmark (MSU-Bench) is the first
  large-scale, human-curated evaluation framework for assessing Large Language Models
  (LLMs) and Vision-Language Models (VLMs) on complete musical scores. It comprises
  1,800 generative question-answer pairs from 150 complete scores spanning multiple
  composers, organized into four hierarchical levels of comprehension.
---

# Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores

## Quick Facts
- arXiv ID: 2511.20697
- Source URL: https://arxiv.org/abs/2511.20697
- Authors: Congren Dai; Yue Yang; Krinos Li; Huichi Zhou; Shijie Liang; Zhang Bo; Enyang Liu; Ge Jin; Hongran An; Haosen Zhang; Peiyuan Jing; KinHei Lee; Zhenxuan Zhang; Xiaobing Li; Maosong Sun
- Reference count: 40
- Key outcome: First large-scale, human-curated benchmark for evaluating LLMs/VLMs on complete musical scores across four comprehension levels

## Executive Summary
The Musical Score Understanding Benchmark (MSU-Bench) addresses the gap in evaluating multimodal models on complete musical scores by providing 1,800 generative QA pairs from 150 scores spanning Baroque, Classical, and Romantic periods. The benchmark reveals that current models perform substantially better on textual ABC notation (40-50% accuracy) than on visual PDF representations (20% accuracy), with accuracy dropping sharply at higher comprehension levels. Fine-tuning with LoRA improves performance across both modalities while preserving general knowledge, establishing a foundation for advancing multimodal reasoning in musical understanding.

## Method Summary
The benchmark uses 150 complete musical scores (PDF + MusicXML) converted to ABC notation for textual QA, with 1,800 human-curated generative QA pairs (450 per comprehension level). Models are evaluated zero-shot across 15+ architectures, with LoRA fine-tuning (rank 8) applied to select models for 20 epochs. Evaluation uses an LLM-as-a-Judge ensemble (ChatGPT-5, Claude Sonnet 4, Gemini 2.5 Pro) with majority voting for semantic equivalence scoring. The framework measures per-level accuracy and Level-wise Success Rate (LSR) to capture hierarchical dependencies, with additional MMLU evaluation to assess knowledge preservation after fine-tuning.

## Key Results
- Current models achieve 40-50% accuracy on textual ABC notation versus 20% on visual PDF representations
- LSR declines steeply with comprehension depth, falling below 10% by Level 2 and nearly vanishing by Level 3
- LoRA fine-tuning improves musical comprehension while preserving MMLU scores (STEM: 72.63→74.09, Humanities: 81.44→81.54)
- Batched questioning improves performance by ~5-10% compared to sequential questioning

## Why This Works (Mechanism)

### Mechanism 1
ABC notation provides a more reliable input modality for musical score understanding than visual PDF representations. ABC notation explicitly encodes bar indices, pitch, rhythm, and articulation using structured ASCII tokens (e.g., `%1`, `!tenuto!G`, `[B,DG]`), which align with LLMs' pattern-matching strengths on sequential symbolic data. This bypasses the visual localisation challenge where VLMs struggle to identify bar positions from raw score images.

### Mechanism 2
Multi-level musical comprehension exhibits compounding error dynamics where failures at lower levels propagate to higher levels. Level-wise Success Rate (LSR) declines steeply because higher-level tasks (harmony, form) depend on accurate localisation established at Level 2 (bar identification). An incorrect bar index invalidates subsequent harmonic analysis, causing cascading failures rather than independent errors.

### Mechanism 3
LoRA fine-tuning improves musical score understanding without catastrophic forgetting of general knowledge. Low-rank adaptation modifies a small subset of parameters, preserving pretrained representations for general tasks while adjusting domain-specific pathways. The paper reports MMLU scores remain stable or improve slightly post-adaptation.

## Foundational Learning

- **ABC Notation Syntax**
  - Why needed here: All textual experiments use ABC notation; understanding header fields (`T:`, `C:`, `K:`) and body syntax (`!tenuto!`, `[B,DG]`, `%1`) is prerequisite for debugging tokenisation and prompt design
  - Quick check question: Given `K:Bb` and `[B,DG]`, what chord is implied and in what key?

- **Bar Localisation in Scores**
  - Why needed here: The paper identifies localisation as the primary VLM failure mode; understanding how models map visual regions to bar numbers is critical for interpreting error patterns
  - Quick check question: In a 4-staff orchestral score, which visual cues distinguish bar 7 from bar 8?

- **Level-wise Success Rate (LSR) Metric**
  - Why needed here: LSR captures compounding errors better than per-level accuracy; understanding its formulation (`Correct(Q1:l)/|Q1:l|`) is necessary for replication and extension
  - Quick check question: If a model answers Level 1-2 correctly for 30 scores but only 5 of those survive Level 3, what is LSR(3)?

## Architecture Onboarding

- **Component map:**
  - PDF pages processed by vision encoder (e.g., Qwen2.5-VL's ViT)
  - ABC notation tokenised as text
  - LLM backbone (Qwen3, GPT-5, etc.) receives either modality
  - LoRA adapters (rank 8) inserted at attention layers for fine-tuning
  - LLM-as-a-Judge ensemble (ChatGPT-5, Claude Sonnet 4, Gemini 2.5 Pro) with majority voting

- **Critical path:**
  1. Score ingestion (PDF→image OR MusicXML→ABC conversion)
  2. Question prompting with score context (12 questions per score, batched preferred)
  3. Model inference with temperature=0.95, top-p=0.7
  4. Answer evaluation via 3-model judge ensemble

- **Design tradeoffs:**
  - **PDF vs ABC input**: PDF preserves visual authenticity but suffers localisation errors; ABC provides explicit structure but requires conversion pipeline
  - **Batched vs sequential questions**: Batched improves accuracy (~5-10% gain per Figure 12) but increases token length, potentially excluding longer scores
  - **LoRA rank**: Rank 8 balances adaptation capacity with preservation; lower ranks may underfit, higher risks overfitting

- **Failure signatures:**
  - **Localisation hallucination**: Model claims "bar 7" features not present in score (Figure 1a)
  - **LSR collapse**: Near-zero scores at Level 3+ indicate fundamental localisation failure rather than reasoning limitation
  - **Title-only shortcut**: If model achieves >30% accuracy given only title (no score content), indicates memorisation over reasoning (Table 3 shows this is limited: ~17-24%)

- **First 3 experiments:**
  1. Establish baseline by running zero-shot evaluation on 3 models (1 LLM, 2 VLMs) across all 4 levels using the public MSU-Bench split; measure both per-level accuracy and LSR to identify localisation bottleneck
  2. Isolate modality effect by converting PDF test set to ABC and comparing same model (e.g., Qwen2.5-VL-3B) performance on both formats; quantify localisation error rate independently
  3. Validate LoRA preservation by fine-tuning Qwen3-4B on MSU-Bench training split, then evaluating on both MSU-Bench test and MMLU (5-subject subset) to confirm no catastrophic forgetting; track whether STEM vs Humanities diverge

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What architectural or training methodologies are required to bridge the substantial performance gap between visual (PDF) and textual (ABC) modalities, enabling models to interpret raw sheet music reliably without symbolic aids?
**Basis in paper:** [explicit] The authors explicitly state in the "Limitations" section that the "substantial performance gap observed between textual and visual modalities highlights the limitations of current models in interpreting raw sheet music without symbolic aid."
**Why unresolved:** Current VLMs struggle significantly with visual PDF inputs (approx. 20% accuracy) compared to textual ABC inputs (approx. 40-50%), indicating a fundamental failure in extracting structured musical semantics from visual data.
**What evidence would resolve it:** Demonstration of a VLM architecture that achieves parity or near-parity performance on the MSU-Bench visual task compared to the textual task, without relying on intermediate Optical Music Recognition (OMR) conversion to text.

### Open Question 2
**Question:** To what extent do the hierarchical reasoning capabilities observed in Western art music generalize to non-Western, contemporary, or avant-garde musical scores with distinct notational systems?
**Basis in paper:** [explicit] In the "Limitations" section, the authors acknowledge that "the scope of MSU-Bench is primarily centered on the Western art music canon... We seek to broaden this musical scope to encompass non-Western and contemporary genres in future."
**Why unresolved:** The current benchmark is restricted to Baroque, Classical, and Romantic periods, leaving the models' ability to handle different notational logics (e.g., microtonality, graphic notation) unknown.
**What evidence would resolve it:** Evaluation results from a benchmark extension containing non-Western and contemporary scores, showing sustained accuracy across all four comprehension levels.

### Open Question 3
**Question:** Does the observed performance boost from "joint" (batched) questioning stem from true hierarchical reasoning or simply from increased context length providing more opportunities for error correction?
**Basis in paper:** [inferred] The authors note in the Empirical Results that "posing questions jointly yields better performance than presenting them sequentially, suggesting that current models can effectively leverage hierarchical reasoning." However, the specific mechanism (reasoning vs. context redundancy) is not isolated.
**Why unresolved:** While the paper establishes that batched questions improve scores, it does not confirm if the model is genuinely using the answer from a Level 1 question to solve a Level 4 question (reasoning) or if the expanded context merely helps it locate relevant features (context window effects).
**What evidence would resolve it:** Ablation studies analyzing attention mechanisms during batched inference to verify if lower-level question-answer pairs explicitly inform the token generation of higher-level questions.

## Limitations

- The substantial performance gap between visual and textual modalities highlights fundamental limitations in models' ability to interpret raw sheet music without symbolic aids
- The benchmark scope is primarily centered on Western art music canon, restricting generalization to non-Western and contemporary genres
- The hierarchical compounding error model assumes strict dependency between levels without testing alternative reasoning strategies that bypass lower-level failures

## Confidence

**High confidence:** The empirical observation that ABC notation outperforms PDF representations for musical score understanding, supported by direct accuracy measurements across multiple models and comprehension levels.

**Medium confidence:** The hierarchical compounding error model, based on observed LSR decline but lacking controlled experiments isolating localisation from reasoning capabilities.

**Medium confidence:** LoRA fine-tuning effectiveness, with knowledge preservation demonstrated on limited MMLU subset but broader generalization unverified.

## Next Checks

1. Control experiment: Evaluate models on identical musical comprehension tasks using ABC notation versus standard text descriptions (removing musical-specific syntax) to isolate the contribution of ABC structure from musical pretraining exposure.

2. Dependency test: Design experiments where models receive correct bar indices but incorrect local details, or vice versa, to quantify the actual contribution of each level to higher-level reasoning rather than assuming strict hierarchy.

3. Generalization assessment: Expand post-fine-tuning evaluation to full MMLU benchmark and out-of-distribution musical pieces to verify that LoRA adaptation preserves general reasoning capabilities without musical domain overfitting.