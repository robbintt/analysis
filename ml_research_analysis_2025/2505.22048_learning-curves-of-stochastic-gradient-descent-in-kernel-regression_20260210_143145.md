---
ver: rpa2
title: Learning Curves of Stochastic Gradient Descent in Kernel Regression
arxiv_id: '2505.22048'
source_url: https://arxiv.org/abs/2505.22048
tags:
- have
- where
- step
- kernel
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes single-pass stochastic gradient descent (SGD)
  for kernel regression under the source condition, where the optimal predictor may
  lie outside the reproducing kernel Hilbert space (RKHS). The study focuses on inner-product
  kernels over the unit sphere and characterizes exact convergence rates of the excess
  risk under various scaling regimes between sample size $n$ and input dimension $d$.
---

# Learning Curves of Stochastic Gradient Descent in Kernel Regression

## Quick Facts
- arXiv ID: 2505.22048
- Source URL: https://arxiv.org/abs/2505.22048
- Reference count: 40
- Primary result: SGD with exponentially decaying step sizes achieves minimax-optimal convergence rates for kernel regression across all smoothness levels and scaling regimes

## Executive Summary
This paper provides a comprehensive analysis of single-pass stochastic gradient descent (SGD) for kernel regression under source conditions where the optimal predictor may lie outside the reproducing kernel Hilbert space. The study focuses on inner-product kernels over the unit sphere and characterizes exact convergence rates of the excess risk under various scaling regimes between sample size and input dimension. The key insight is that exponentially decaying step sizes, common in deep learning practice, enable SGD to overcome the saturation phenomenon that affects kernel ridge regression when problems are highly smooth. The analysis establishes both upper and lower bounds, showing that this simple modification achieves optimal performance across all regimes.

## Method Summary
The authors analyze single-pass SGD with exponentially decaying step sizes for kernel regression under the source condition framework. The method operates on inner-product kernels over the unit sphere, where the target function satisfies f* = L^s g with g in the RKHS and s > 0. The exponentially decaying schedule takes the form η_t = η_0 / (1 + λ η_0 t) where λ is the regularization parameter. The analysis tracks the evolution of the SGD iterate through its interaction with the spectral properties of the kernel matrix, establishing precise convergence rates for the excess risk. The framework allows comparison with kernel ridge regression and demonstrates conditions under which SGD outperforms KRR, particularly in highly smooth regimes where KRR saturates.

## Key Results
- SGD with exponentially decaying step sizes achieves minimax-optimal convergence rates up to constants across all scaling regimes (d/n ratios) and smoothness levels
- The exponentially decaying schedule prevents the saturation phenomenon that affects kernel ridge regression in highly smooth problems
- For misspecified problems with 0 < s < 1, SGD with constant step size and iterate averaging is optimal
- A lower bound shows that SGD with averaging cannot achieve optimality when s > 2 in asymptotic settings

## Why This Works (Mechanism)
The exponentially decaying step size schedule allows SGD to balance exploration and exploitation throughout the optimization process. In early iterations, larger steps enable rapid movement toward the solution, while later iterations with smaller steps provide fine-grained convergence. This schedule effectively adapts to the spectral decay of the kernel matrix, maintaining progress even when the problem is highly smooth (large s) where fixed-step methods would stall. The single-pass nature without replacement ensures each data point is used exactly once, creating a natural annealing effect that complements the step size decay. For moderately smooth problems (0 < s < 1), constant step sizes with averaging work better because the noise in the gradient estimates averages out without needing aggressive annealing.

## Foundational Learning
- **Reproducing Kernel Hilbert Space (RKHS)**: A Hilbert space of functions where point evaluation is a continuous linear functional, characterized by a positive definite kernel. Needed to understand the function space and regularization framework. Quick check: Verify Mercer's theorem applies to the kernel being used.
- **Source Condition**: Assumes the target function can be expressed as f* = L^s g for some g in the RKHS and smoothness parameter s > 0. Provides the mathematical framework for analyzing convergence rates. Quick check: Determine whether the smoothness parameter s is known or needs to be estimated from data.
- **Spectral Decay of Kernel Matrices**: Describes how the eigenvalues of the kernel matrix decrease, typically polynomially or exponentially. Critical for understanding convergence behavior. Quick check: Compute or estimate the spectral decay rate for the specific kernel and data distribution.
- **Minimax Optimality**: A convergence rate is minimax optimal if no algorithm can achieve a better worst-case rate over a given function class. Provides the benchmark for algorithm comparison. Quick check: Compare the obtained rate with known lower bounds for the function class.
- **Iterate Averaging**: A technique where the final prediction is the average of all intermediate iterates rather than just the last one. Can improve convergence in noisy settings. Quick check: Determine whether averaging provides benefits for the specific smoothness regime and step size schedule.
- **Inner-product Kernels on Unit Sphere**: Kernels that depend only on the inner product of inputs, restricted to the unit sphere. Simplifies analysis while maintaining practical relevance. Quick check: Verify the data satisfies the unit sphere constraint or can be normalized appropriately.

## Architecture Onboarding

Component Map:
Kernel -> Gram Matrix -> Eigenvalue Decomposition -> Step Size Schedule -> SGD Updates -> Excess Risk Bound

Critical Path:
Data generation (unit sphere, inner-product kernel) → Gram matrix computation → Spectral analysis → Step size parameter selection (η_0, λ) → Single-pass SGD execution → Convergence rate verification

Design Tradeoffs:
The choice between exponentially decaying and constant step sizes represents a fundamental tradeoff between handling highly smooth problems (exponential decay) versus moderately smooth problems (constant with averaging). Single-pass SGD trades multiple passes for computational efficiency but requires careful step size scheduling. The unit sphere constraint simplifies analysis but limits direct applicability to general data.

Failure Signatures:
- If s is misestimated, the step size schedule may be inappropriate, leading to suboptimal rates
- For s > 2 with averaging, the method will provably fail to achieve optimal rates
- When d << n (underparameterized regime), the analysis breaks down
- Non-inner-product kernels may not exhibit the same convergence behavior

Three First Experiments:
1. Verify convergence rates on synthetic data with known s values across different d/n ratios
2. Test sensitivity to s estimation error by perturbing the assumed smoothness parameter
3. Compare exponentially decaying versus constant step sizes with averaging for various s values

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis requires inner-product kernels on the unit sphere, limiting practical applicability
- Exponentially decaying step sizes may be overly aggressive in practice, potentially causing premature convergence
- The single-pass without replacement assumption differs from common minibatch implementations
- The analysis focuses on the overparameterized regime (d ≥ n), not covering underparameterized cases

## Confidence
High: Main convergence rate claims, minimax optimality results, KRR saturation comparison
Medium: Practical implications of exponentially decaying step sizes, finite-sample performance
Low: Applicability to non-inner-product kernels, performance in underparameterized regime

## Next Checks
1. Empirically verify the theoretical convergence rates on synthetic data with known smoothness parameters across different d/n ratios
2. Test the performance of exponentially decaying step sizes versus iterate averaging on real-world datasets where the source condition may only approximately hold
3. Extend the analysis to non-inner-product kernels and assess whether the exponentially decaying schedule maintains its advantages