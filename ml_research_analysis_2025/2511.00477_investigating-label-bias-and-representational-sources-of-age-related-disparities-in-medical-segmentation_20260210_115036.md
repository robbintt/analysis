---
ver: rpa2
title: Investigating Label Bias and Representational Sources of Age-Related Disparities
  in Medical Segmentation
arxiv_id: '2511.00477'
source_url: https://arxiv.org/abs/2511.00477
tags:
- bias
- labels
- segmentation
- fairness
- breast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates age-related algorithmic bias in breast
  cancer tumor segmentation using the MAMA-MIA dataset. The authors systematically
  audit automated labels and establish a baseline of age-related performance disparities,
  finding that models perform significantly worse on younger patients.
---

# Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation

## Quick Facts
- arXiv ID: 2511.00477
- Source URL: https://arxiv.org/abs/2511.00477
- Reference count: 0
- Primary result: Age-related algorithmic bias in breast cancer tumor segmentation amplified by 66% when training on biased machine-generated labels

## Executive Summary
This study systematically investigates age-related algorithmic bias in breast cancer tumor segmentation using the MAMA-MIA dataset. The authors demonstrate that models perform significantly worse on younger patients, and through controlled experiments show this bias originates from intrinsic representational differences in the data rather than label quality or simple case difficulty imbalance. Critically, they reveal that training on biased, machine-generated labels amplifies the bias by 66%, creating dangerous clinical implications. The work introduces a framework for diagnosing bias sources in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.

## Method Summary
The study employs a controlled experimental framework on the MAMA-MIA DCE-MRI dataset (n=1,506) with paired Gold-Standard (expert) and Silver-Standard (nnU-Net-generated) labels. Age cohorts are stratified into Young (≤40, n=349), Middle (40-55, n=754), and Older (≥55, n=400). Using nnU-Net 3d_fullres with 1000 epochs and 5-fold age-stratified cross-validation, the authors conduct five controlled experiments: establishing baseline bias, quantifying Biased Ruler effect (comparing silver vs. gold evaluation), testing label-sensitivity, testing difficulty-balance, and measuring bias amplification from biased-label training. Key metrics include Dice score, HD95, Demographic Parity Difference (DPD), Disparate Impact Ratio (DIR), and statistical tests via OLS regression and ANOVA.

## Key Results
- Models trained on biased, machine-generated labels show 66% wider fairness gap than gold-standard trained models
- Silver-standard labels inflate observed age-related bias by 40% compared to gold-standard evaluation (Biased Ruler effect)
- Difficulty-balancing experiments fail to close fairness gap, demonstrating representational rather than distributional bias source
- M-BIASED-INPUT achieves DIR=0.7895, below the 0.8 adverse-impact threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Validating models against biased machine-generated labels systematically misrepresents true performance disparities across demographic subgroups.
- Mechanism: Silver-standard labels contain age-related bias; when used as evaluation benchmarks, they inflate observed bias by 40% compared to gold-standard evaluation, creating a "Biased Ruler" that masks or exaggerates actual model fairness.
- Core assumption: Gold-standard expert annotations are substantially less biased than automated labels for the same cases.
- Evidence anchors:
  - [abstract] "reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model's actual bias"
  - [section 2.3, Experiment 1] "observed performance bias... 40% higher than the true bias" when evaluating against silver vs. gold labels; DPD increased from 0.0802 to 0.1060
  - [corpus] Limited direct corpus support; related paper "Who Does Your Algorithm Fail?" investigates bias in same dataset but not label-bias effects specifically
- Break condition: If gold-standard labels themselves contain systematic age-related annotation errors, the "true" performance baseline would also be compromised.

### Mechanism 2
- Claim: Age-related segmentation disparities originate from qualitative representational differences in the data distribution, not from quantitative imbalance of difficult cases or label quality sensitivity.
- Mechanism: Younger patients exhibit anatomically distinct tumor characteristics (66% larger volume, 70% greater variance) that are intrinsically harder to learn; balancing difficulty counts across age groups fails to close the fairness gap because the hard cases are qualitatively different, not just more numerous.
- Core assumption: Difficulty-tier stratification (based on Dice and expert ratings) captures the relevant difficulty dimensions affecting model learning.
- Evidence anchors:
  - [abstract] "systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance"
  - [section 2.3, Experiment 3] M-DIFF-BAL with matched easy/hard distributions showed unchanged fairness gap (§=0.0361 vs. baseline 0.0399), "refuting the hypothesis that a simple quantitative imbalance of difficult cases causes the bias"
  - [corpus] Limited corpus support for this specific causal chain; related fairness papers focus on classification rather than segmentation mechanisms
- Break condition: If the difficulty-tier classification system systematically mischaracterizes case difficulty for younger patients, the "qualitative difference" conclusion may conflate measurement artifact with true representational shift.

### Mechanism 3
- Claim: Training segmentation models on biased machine-generated labels amplifies existing fairness disparities by ~66%.
- Mechanism: Silver-standard labels encode age-related bias; models trained on them learn and intensify these systematic errors, widening the fairness gap from 0.0399 (gold-trained) to 0.0661 (silver-trained), pushing DIR below the 0.8 adverse-impact threshold.
- Core assumption: The amplification is caused by bias in labels themselves rather than by other differences between silver and gold labels (e.g., annotation protocol differences).
- Evidence anchors:
  - [abstract] "training on biased, machine-generated labels amplifies the bias by 66%"
  - [section 2.3, Experiment 4] M-BIASED-INPUT trained on silver labels showed "fairness gap widened by 66% relative to M-BASELINE" with DIR dropping to 0.7895, "below the standard threshold"
  - [corpus] Neighbor paper on "Mutual Evidential Deep Learning" notes pseudo-label bias issues in semi-supervised segmentation, providing indirect support for label-induced bias propagation
- Break condition: If silver labels differ from gold labels in ways unrelated to age (e.g., different annotation tools or protocols), amplification may reflect methodology artifacts rather than bias learning.

## Foundational Learning

- Concept: **Demographic Parity Difference (DPD) and Disparate Impact Ratio (DIR)**
  - Why needed here: These are the formal fairness metrics used throughout the paper to quantify age-related disparities; DPD measures absolute gaps, DIR measures relative disparities with a 0.8 threshold for adverse impact.
  - Quick check question: Given DPD(Y|O)=0.1146 and DIR(Y|O)=0.7895 for M-BIASED-INPUT, does this model meet or violate the four-fifths rule, and for which group?

- Concept: **Label bias vs. Representational bias distinction**
  - Why needed here: The paper's central methodological contribution is disentangling whether performance gaps come from annotation quality (label bias) or from intrinsic data characteristics (representational bias); different interventions are required for each.
  - Quick check question: If balancing the number of hard cases across age groups fails to close the fairness gap, which type of bias does this evidence support?

- Concept: **Bias amplification in iterative training pipelines**
  - Why needed here: Many medical imaging datasets use semi-automated annotations; understanding how bias propagates when models are trained on biased labels is critical for preventing feedback loops that worsen disparities over successive model generations.
  - Quick check question: A model trained on nnU-Net-generated silver labels shows 66% wider fairness gap than a gold-standard trained model. What does this imply about using automated annotations for future dataset creation?

## Architecture Onboarding

- Component map:
  - Data layer (MAMA-MIA DCE-MRI) -> Gold/Silver label pairs -> Age-stratified cohorts (Young, Middle, Older) -> 5-fold stratified cross-validation -> nnU-Net 3d_fullres training -> Dual evaluation (Gold/Silver) -> Fairness metrics computation

- Critical path:
  1. Establish bias baseline in silver labels via fairness audit (Experiment 0)
  2. Quantify Biased Ruler effect by comparing observed vs. true performance (Experiment 1)
  3. Test label-sensitivity hypothesis via targeted label swaps (Experiment 2)
  4. Test difficulty-imbalance hypothesis via difficulty-balanced training (Experiment 3)
  5. Measure bias amplification from biased-label training (Experiment 4)

- Design tradeoffs:
  - Using age-stratified cross-validation ensures subgroup representation but reduces per-fold sample sizes for Young group (n~70 per fold)
  - Difficulty-tier system combines qualitative ratings with quantitative metrics; may miss cases that are hard for different reasons
  - Focusing on Young vs. Older contrast maximizes signal but may obscure Middle-group patterns

- Failure signatures:
  - DIR dropping below 0.8 indicates adverse impact requiring intervention
  - Large discrepancy between observed performance (vs. silver) and true performance (vs. gold) signals Biased Ruler effect
  - Unchanged fairness gap after difficulty balancing signals representational rather than distributional bias source

- First 3 experiments:
  1. Replicate Experiment 1 (Biased Ruler): Train balanced model, evaluate against both gold and silver labels; confirm 40% inflation of observed bias
  2. Replicate Experiment 3 (Difficulty Balance): Train M-DIFF-BAL with matched easy/hard distributions; verify fairness gap persistence
  3. Extend to new subgroup: Apply same framework to breast density categories or institution ID to test generalizability of bias mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific interventions targeting qualitative representational differences can effectively mitigate age-related bias in medical segmentation?
- Basis in paper: [explicit] The conclusion states "Future work must address qualitative representational interventions rather than rebalancing strategies" after demonstrating that difficulty-balancing failed to eliminate bias.
- Why unresolved: The paper isolates qualitative distributional differences as the root cause but does not propose or test specific mitigation approaches.
- What evidence would resolve it: Controlled experiments testing targeted interventions (e.g., age-specific data augmentation, architecture modifications, or curriculum learning) that reduce the fairness gap below statistical significance.

### Open Question 2
- Question: Do these age-related bias findings generalize to other sensitive attributes (race, ethnicity, socioeconomic status) in breast cancer segmentation?
- Basis in paper: [inferred] The study focuses exclusively on age as the sensitive attribute, though other demographic disparities in breast cancer diagnostics are well-documented in the literature cited.
- Why unresolved: The MAMA-MIA dataset metadata may contain additional demographic variables that were not analyzed; the framework was only validated on age stratification.
- What evidence would resolve it: Applying the same controlled experimental framework to audit other demographic attributes in the same or similar datasets.

### Open Question 3
- Question: What specific auditing protocols can effectively detect and prevent bias propagation in automated annotation pipelines before clinical deployment?
- Basis in paper: [explicit] The conclusion calls for "rigorous auditing protocols using high-quality benchmarks must be established to detect and prevent bias propagation in automated data pipelines."
- Why unresolved: The paper demonstrates the problem (66% bias amplification from biased labels) but provides no procedural guidelines for pipeline auditing.
- What evidence would resolve it: Development and validation of a standardized auditing checklist or protocol that reliably flags label bias before model training.

## Limitations

- Generalizability uncertainty: Findings may not extend to other medical imaging tasks or modalities beyond breast cancer DCE-MRI segmentation
- Gold-label assumption: The study assumes gold-standard labels are unbiased, which may not hold for complex tumor boundary annotations
- Single dataset focus: The analysis is limited to one dataset (MAMA-MIA), constraining external validity across different clinical settings

## Confidence

**High confidence**: The Biased Ruler effect (40% inflation of observed bias when using silver labels) is robustly demonstrated with clear statistical evidence and controlled experiments.

**Medium confidence**: The representational bias mechanism (qualitative distributional differences) is well-supported within this dataset, but the difficulty-tier stratification methodology has some opacity in how it handles cases that may be hard for different reasons.

**Medium confidence**: The 66% amplification effect from biased-label training is clearly quantified, though the causal attribution to bias rather than methodological differences between gold and silver annotations requires additional validation.

## Next Checks

1. **Cross-dataset validation**: Replicate the bias auditing framework on a different medical segmentation dataset (e.g., prostate or liver segmentation) to test generalizability of the Biased Ruler and representational bias mechanisms.

2. **Gold-label bias assessment**: Conduct a sensitivity analysis where gold-standard annotations are independently reviewed by multiple experts to quantify potential bias in the "ground truth" used as the reference standard.

3. **Temporal bias analysis**: Train models on sequential subsets of the MAMA-MIA dataset (by acquisition date) to determine if the bias amplification effect compounds over time in iterative annotation pipelines.