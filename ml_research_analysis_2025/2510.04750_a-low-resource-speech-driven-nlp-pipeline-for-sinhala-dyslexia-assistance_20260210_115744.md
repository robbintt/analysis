---
ver: rpa2
title: A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance
arxiv_id: '2510.04750'
source_url: https://arxiv.org/abs/2510.04750
tags:
- correction
- sinhala
- system
- error
- dyslexia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The system addresses the lack of assistive tools for Sinhala-speaking
  adults with dyslexia by combining Whisper, SinBERT, mT5, Mistral, and gTTS into
  a real-time speech-driven NLP pipeline. The modular design classifies dyslexic errors,
  applies grammatical correction, and provides spoken feedback.
---

# A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance

## Quick Facts
- arXiv ID: 2510.04750
- Source URL: https://arxiv.org/abs/2510.04750
- Reference count: 15
- System combines Whisper, SinBERT, mT5, Mistral, and gTTS into a real-time speech-driven NLP pipeline for Sinhala dyslexia assistance

## Executive Summary
This paper presents a real-time, speech-driven NLP assistive system designed for Sinhala-speaking adults with dyslexia. The modular pipeline integrates Whisper for speech-to-text transcription, SinBERT for error classification, mT5 for grammatical correction, Mistral for fluency refinement, and gTTS for text-to-speech output. Developed in response to the lack of assistive tools for Sinhala—a morphologically complex, low-resource language—the system achieves 66% STT accuracy, 70% correction accuracy, and operates within a target latency of 2.5 seconds per input. The approach leverages publicly available models and a synthetic dataset of 3,000 samples, enabling practical support for dyslexic users in underrepresented linguistic contexts.

## Method Summary
The system addresses the absence of assistive tools for Sinhala-speaking adults with dyslexia by combining established NLP models into a speech-driven pipeline. Whisper transcribes spoken input, SinBERT classifies errors into four types (omission, insertion, substitution, reversal), mT5 performs grammatical correction, and Mistral refines fluency before gTTS provides spoken feedback. The authors generated a parallel dataset of 3,000 samples from the OpenSLR SLR63 Sinhala Read Speech corpus, applying rule-based transformations to simulate dyslexic errors. The pipeline operates in 2.5 seconds per input, with Whisper achieving 66% accuracy (34% WER), SinBERT fine-tuned for classification, and mT5 + Mistral delivering 70% correction accuracy with a GLEU score of 57%. Public code and dataset are available for replication.

## Key Results
- Whisper transcription achieves 66% accuracy (34% WER) on Sinhala speech
- mT5 + Mistral combination yields 70% correction accuracy and GLEU score of 57%
- Complete pipeline operates within 2.5 seconds per input with 65% overall system accuracy
- Modular design enables error classification and real-time spoken feedback for dyslexic users

## Why This Works (Mechanism)
The pipeline leverages the strengths of each component: Whisper's robust zero-shot speech recognition for Sinhala, SinBERT's fine-tuned classification of dyslexic error types, mT5's grammatical correction capabilities adapted to Sinhala's morphological complexity, and Mistral's fluency refinement. The modular architecture allows for targeted error handling and rapid feedback, critical for dyslexic users. The use of quantized models and efficient orchestration ensures the 2.5-second latency target, while the synthetic dataset enables supervised training despite the scarcity of real dyslexic writing in Sinhala.

## Foundational Learning
- **Speech-to-text transcription for low-resource languages**: Whisper's zero-shot capability is crucial for Sinhala, a language with limited annotated speech data. Quick check: Verify Whisper's transcription accuracy on a held-out Sinhala speech subset.
- **Error classification with SinBERT**: Fine-tuning a BERT-based model on synthetic dyslexic errors enables targeted correction. Quick check: Assess SinBERT's classification accuracy on the test split.
- **Grammatical correction with mT5**: Adapting mT5 to Sinhala's morphological complexity supports effective error correction. Quick check: Measure mT5's correction accuracy and GLEU score.
- **Fluency refinement with Mistral**: Post-processing with Mistral improves grammatical output coherence. Quick check: Compare correction quality with and without Mistral refinement.
- **Synthetic data generation for low-resource scenarios**: Rule-based transformations simulate dyslexic errors for supervised learning. Quick check: Inspect synthetic error diversity and coverage.
- **Real-time pipeline orchestration**: Efficient integration and latency management are essential for user experience. Quick check: Profile end-to-end latency and identify bottlenecks.

## Architecture Onboarding
- **Component map**: Speech input → Whisper (STT) → SinBERT (error classification) → mT5 (grammatical correction) → Mistral (fluency refinement) → gTTS (TTS output)
- **Critical path**: Whisper transcription (1.2s) → SinBERT classification (60ms) → mT5 + Mistral correction (900ms) → gTTS synthesis (300ms)
- **Design tradeoffs**: Use of zero-shot Whisper and quantized models balances accuracy with latency; synthetic data enables supervised learning despite real-world scarcity; modular design supports future personalization
- **Failure signatures**: Whisper WER > 40% indicates transcription issues; SinBERT misclassification suggests synthetic data limitations; mT5 overcorrection signals prompt or data mismatch; pipeline latency > 2.5s reveals bottlenecks
- **First experiments**: 1) Run Whisper on held-out Sinhala speech to verify 66% accuracy; 2) Fine-tune SinBERT on the training split and evaluate classification accuracy; 3) Execute the full pipeline on the test set and compare correction accuracy and GLEU to reported values

## Open Questions the Paper Calls Out
- How does system performance generalize from synthetically generated dyslexic errors to authentic writing patterns from real Sinhala-speaking dyslexic adults? The synthetic dataset may not capture the full variability of real-world dyslexic writing.
- Can personalized correction mechanisms that track user-specific error histories significantly improve accuracy beyond the current 70% correction rate? The current system does not adapt to individual users' persistent error patterns.
- Does expressive TTS with prosody and emotional modulation improve comprehension and engagement for adult dyslexic users compared to the flat gTTS output? The current system lacks expressive TTS features critical for adult comprehension.

## Limitations
- Synthetic error generation may not fully capture the natural variability of real-world dyslexic writing patterns, limiting generalization.
- Unspecified fine-tuning settings for SinBERT and prompts for mT5 + Mistral hinder reproducibility and may affect reported accuracy.
- No evaluation with naturalistic speech data or clinical populations; effectiveness for diverse Sinhala-speaking dyslexic users remains untested.

## Confidence
- **High confidence** in the modular architecture and feasibility of combining Whisper, SinBERT, mT5, Mistral, and gTTS for a speech-driven NLP pipeline in Sinhala.
- **Medium confidence** in the reported accuracy metrics, given reliance on synthetic errors and absence of fine-tuning details for SinBERT.
- **Low confidence** in the system's real-world performance for diverse Sinhala-speaking dyslexic users, as there is no evaluation with naturalistic speech data or clinical populations.

## Next Checks
1. Replicate the full pipeline with explicit fine-tuning hyperparameters for SinBERT and prompts for mT5 + Mistral to verify correction accuracy and GLEU scores.
2. Evaluate Whisper transcription on a held-out subset of naturalistic or noisy Sinhala speech to assess robustness beyond synthetic errors.
3. Conduct a pilot user study with Sinhala-speaking adults with dyslexia to measure practical latency, usability, and correction effectiveness under realistic conditions.