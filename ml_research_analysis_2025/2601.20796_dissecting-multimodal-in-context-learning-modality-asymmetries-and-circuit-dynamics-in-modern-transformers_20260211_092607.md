---
ver: rpa2
title: 'Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit
  Dynamics in modern Transformers'
arxiv_id: '2601.20796'
source_url: https://arxiv.org/abs/2601.20796
tags:
- multimodal
- accuracy
- data
- learning
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how transformers learn to associate information
  across modalities from in-context examples. Through controlled experiments on small
  transformers trained on synthetic classification tasks, the study isolates the effects
  of data statistics and model architecture on multimodal in-context learning (ICL).
---

# Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers

## Quick Facts
- arXiv ID: 2601.20796
- Source URL: https://arxiv.org/abs/2601.20796
- Authors: Yiran Huang; Karsten Roth; Quentin Bouniot; Wenjia Xu; Zeynep Akata
- Reference count: 40
- Primary result: Pretraining on high-diversity primary modality data enables multimodal ICL with surprisingly low-complexity secondary modality

## Executive Summary
This work investigates how transformers learn to associate information across modalities from in-context examples. Through controlled experiments on small transformers trained on synthetic classification tasks, the study isolates the effects of data statistics and model architecture on multimodal in-context learning (ICL). The key finding is a fundamental learning asymmetry: when a transformer is pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis reveals that both unimodal and multimodal ICL rely on induction-style mechanisms that copy labels from matching in-context exemplars, with multimodal training refining and extending these circuits across modalities.

## Method Summary
The study uses synthetic classification tasks with controlled statistics (Zipf skew α≈1, burstiness B, class diversity K) to systematically investigate multimodal ICL. Transformers are trained on unimodal and multimodal sequences where each example includes query tokens and context examples with labels. The researchers vary data complexity through class diversity and burstiness, test different positional encodings (APE vs. RoPE), and examine the role of induction circuits through ablation studies and attention visualization. They also study modality alignment by comparing scenarios with and without feature projectors or encoders.

## Key Results
- Pretraining on high-diversity primary modality (K1=8192) enables strong multimodal ICL with low-complexity secondary modality (K2=256, 32x less)
- Both unimodal and multimodal ICL rely on induction-style circuits that copy labels from matching context exemplars
- RoPE positional encodings weaken induction circuit formation, requiring higher data complexity to achieve comparable ICL performance
- Multimodal training refines existing induction circuits rather than creating new ones for cross-modal alignment

## Why This Works (Mechanism)

### Mechanism 1: Induction Circuit Formation and Refinement
- **Claim**: Both unimodal and multimodal ICL rely on induction-style circuits that copy labels from matching in-context exemplars; multimodal training refines these circuits rather than creating new ones.
- **Mechanism**: A two-layer circuit where Layer 1 forms "previous-token heads" that copy token information, and Layer 2 forms "induction heads" that match query features to context exemplars and retrieve their labels.
- **Core assumption**: The induction circuit architecture is task-agnostic and transferable across modalities once established.
- **Evidence anchors**:
  - [abstract] "Mechanistic analysis reveals that both unimodal and multimodal ICL rely on induction-style mechanisms that copy labels from matching in-context exemplars"
  - [Section 4.4.2] Correlation analysis shows IndStrength2 (label matching) is the strongest correlate of multimodal ICL accuracy (ρ=0.70), while CLA drops to 0.02—indicating the circuit already exists and only needs refinement
  - [Table 3] Causal validation: knocking out induction head drops ICL accuracy from 0.970 to 0.062
- **Break condition**: If context length increases significantly without corresponding data complexity, induction patterns become diffuse (Appendix A.2.2 shows ICL accuracy degrades for all PE types as N increases).

### Mechanism 2: Modality Asymmetry via Pretrained Scaffold
- **Claim**: When the decoder is pretrained on high-diversity primary modality data, the secondary modality requires substantially lower data complexity to achieve multimodal ICL.
- **Mechanism**: Primary modality pretraining installs the foundational ICL circuit; secondary modality training maps new features onto this pre-existing scaffold rather than learning ICL from scratch.
- **Core assumption**: The primary modality's embedding space provides a structured manifold that the secondary modality can align to.
- **Evidence anchors**:
  - [abstract] "when a transformer is pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge"
  - [Section 4.1, Figure 4a] With K1=8192, only K2=256 is needed for strong ICL—32x less class diversity required
  - [Appendix A.3.7] Early-fusion joint training reverses the asymmetry, confirming it emerges from pretraining order and sequence geometry, not modality properties
- **Break condition**: If training both modalities from scratch jointly (early fusion), the asymmetry reverses—the modality positionally adjacent to labels becomes primary.

### Mechanism 3: RoPE Suppresses Induction Circuit Formation
- **Claim**: Rotary Position Embeddings raise the data complexity threshold for ICL by weakening the inductive bias for offset-based token copying operations.
- **Mechanism**: RoPE's multiplicative rotational structure lacks discrete positional offsets, making it harder for attention to learn "attend to token at position n-1" patterns critical for induction circuits.
- **Core assumption**: ICL fundamentally requires learning fixed-offset attention patterns (previous-token, induction).
- **Evidence anchors**:
  - [abstract] "Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL"
  - [Section 3.3, Figure 2c-d] APE shows sharp previous-token and induction patterns; RoPE exhibits diffuse attention with the same peak location but weaker signal
  - [Appendix A.2.2, Figure 12] RoPE and ALiBi cluster together with weaker induction than APE; performance gap narrows at high data complexity, suggesting sufficient data can compensate
- **Break condition**: High data complexity (K·√B) can compensate for RoPE's weaker inductive bias, restoring near-perfect ICL at short context lengths.

## Foundational Learning

- **Concept: Induction Heads**
  - **Why needed here**: The core mechanism enabling ICL—understanding how models learn to retrieve and copy labels from context is essential for debugging and improving multimodal ICL.
  - **Quick check question**: Can you identify which attention heads in your model show strong attention from label tokens to immediately preceding tokens (previous-token pattern) and from queries to matching context labels (induction pattern)?

- **Concept: In-Context Learning vs. In-Weight Learning Trade-off**
  - **Why needed here**: Data statistics determine whether models memorize (IWL) or reason from context (ICL); this trade-off governs when multimodal ICL will emerge.
  - **Quick check question**: Is your training data high-diversity (many classes, Zipf skew α≈1) or low-diversity? High diversity favors ICL, low favors memorization.

- **Concept: Cross-Modal Alignment**
  - **Why needed here**: Even with good induction circuits, multimodal ICL fails if the secondary modality features don't align with the primary modality's embedding space.
  - **Quick check question**: What is the CKA similarity between your projected secondary modality features and primary modality class prototypes? Values below ~0.1 indicate alignment problems.

## Architecture Onboarding

- **Component map**:
  Primary modality (M1) → Token embedding → Decoder layers (pretrained on high-diversity M1 data)
  Secondary modality (M2) → Optional pretrained encoder → MLP projector → M1 embedding space → Decoder layers
  Decoder: 2+ layers with RMSNorm, SiLU, RoPE or APE, multi-head attention

- **Critical path**:
  1. Pretrain decoder on primary modality with K≥4096 classes, burstiness B≥2, Zipf α≈1
  2. Add projector (and optionally encoder) for secondary modality
  3. Jointly train projector (+ optional fine-tuning of encoder/decoder) on multimodal sequences

- **Design tradeoffs**:
  - APE vs. RoPE: APE provides stronger induction circuit formation; RoPE offers better length generalization but requires higher data complexity
  - Encoder vs. larger projector: Pretrained encoder provides better alignment than parameter-matched larger projector, especially for high-dimensional M2 features
  - Fine-tuning scope: Training all components (encoder+projector+decoder) yields highest accuracy but requires more data

- **Failure signatures**:
  - ICL accuracy stuck near random despite high training accuracy → Data complexity too low (increase K or B)
  - Strong unimodal ICL but weak multimodal ICL → Alignment gap (add encoder or check feature dimensions)
  - High IWL but low ICL on swapped-label evaluation → Model memorizing rather than using context (increase data diversity)
  - Diffuse attention patterns with RoPE → Expected behavior; increase data complexity or accept performance ceiling

- **First 3 experiments**:
  1. **Baseline sanity check**: Train unimodal decoder with APE, K=4096, B=4. Verify ICL accuracy >0.9 on novel classes and swapped-label evaluation.
  2. **Positional encoding comparison**: Same setup with RoPE. Expect ~10-20% ICL degradation; confirm induction patterns are visible but weaker in attention visualization.
  3. **Multimodal minimal viable**: Pretrain decoder on M1 (K1=8192), add simple projector for M2 (K2=256, B=4). Verify that low-complexity M2 achieves strong multimodal ICL (>0.9), confirming asymmetry principle.

## Open Questions the Paper Calls Out
None

## Limitations
- All experiments use synthetically generated classification tasks with controlled statistics, which may not transfer to real-world multimodal scenarios
- Experiments are conducted exclusively on small decoders (typically 2 layers), potentially limiting scalability conclusions
- The study does not address multimodal tasks involving language, vision, or audio in their natural forms

## Confidence
**High Confidence**:
- Induction circuit formation is necessary for ICL: Multiple causal interventions (head knockouts) and correlation analyses consistently show that induction strength is the strongest predictor of ICL accuracy (ρ=0.70) and that disrupting these circuits severely degrades performance
- RoPE weakens induction patterns: Systematic comparisons across positional encodings with consistent attention visualizations show that RoPE produces more diffuse attention distributions, particularly affecting previous-token and induction patterns, while maintaining the same peak locations

**Medium Confidence**:
- Modality asymmetry from pretraining order: The experimental evidence strongly supports that pretraining on high-diversity primary modality enables low-complexity secondary modality to achieve multimodal ICL, but this conclusion relies on synthetic data and controlled sequence construction
- Data complexity thresholds compensate for RoPE: While the data shows performance gaps narrow at high K·√B, the exact scaling relationships and whether this fully compensates for RoPE's inductive bias limitations requires further validation

**Low Confidence**:
- Circuit refinement vs. new circuit formation: The conclusion that multimodal training refines existing induction circuits rather than creating new ones is primarily based on correlation analysis. Direct mechanistic evidence of circuit modification at the parameter level is limited.

## Next Checks
1. **Real Multimodal Dataset Validation**: Apply the modality asymmetry framework to a real multimodal dataset (e.g., image-text pairs from CLIP or natural audio-visual data). Measure whether pretraining on high-diversity visual data enables low-complexity audio features to achieve strong multimodal ICL, and characterize the alignment gap through CKA analysis between projected features and class prototypes.

2. **Scaling Laws for RoPE Compensation**: Systematically vary data complexity (K and B) across multiple model sizes to quantify how much data complexity is required to fully compensate for RoPE's weakening of induction circuits. Identify the exact scaling relationship between K·√B and context length N for achieving target ICL accuracy thresholds.

3. **Circuit Evolution in Larger Models**: Extend mechanistic analysis to 12-24 layer transformers to determine whether induction circuits still dominate ICL performance or whether deeper hierarchical mechanisms emerge. Use causal tracing and attention intervention studies to map how multimodal ICL circuits evolve across depth in larger architectures.