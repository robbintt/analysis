---
ver: rpa2
title: 'Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM
  Evaluators'
arxiv_id: '2509.03647'
source_url: https://arxiv.org/abs/2509.03647
tags:
- self-preference
- steering
- arxiv
- bias
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates self-preference bias in large language models
  used as evaluators, where models tend to favor their own outputs over those of others.
  The authors introduce a dataset that distinguishes between illegitimate self-preference
  (bias), legitimate self-preference (correct preference), and unbiased agreement
  using ensemble "gold" judges.
---

# Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators

## Quick Facts
- **arXiv ID:** 2509.03647
- **Source URL:** https://arxiv.org/abs/2509.03647
- **Reference count:** 20
- **Key outcome:** Steering vectors can reduce illegitimate self-preference by up to 97%, but show instability on legitimate self-preference and unbiased agreement.

## Executive Summary
This paper investigates self-preference bias in large language models used as evaluators, where models tend to favor their own outputs over those of others. The authors introduce a dataset that distinguishes between illegitimate self-preference (bias), legitimate self-preference (correct preference), and unbiased agreement using ensemble "gold" judges. They construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. These vectors are applied at inference time to mitigate self-preference bias without retraining. The results show that steering vectors can reduce illegitimate self-preference by up to 97%, significantly outperforming prompting and Direct Preference Optimization (DPO) baselines. However, the vectors show instability in preserving legitimate self-preference and unbiased agreement, suggesting that self-preference may be represented non-linearly or with multiple directions in activation space.

## Method Summary
The paper constructs steering vectors to mitigate self-preference bias in LLM evaluators through two approaches: (1) CAA, which computes the difference in residual stream activations between unbiased and biased prompt completions, and (2) an optimization-based method that learns an additive vector via gradient descent to maximize unbiased completions while suppressing biased ones. The vectors are applied at inference time by adding them to the residual stream at specific layers (14-16). The method requires a dataset with ground truth labels distinguishing illegitimate self-preference (bias), legitimate self-preference (correct), and unbiased agreement, created using an ensemble of diverse "gold" judges (Phi-4, DeepSeek V3, Claude 3.5-Sonnet).

## Key Results
- Steering vectors reduce illegitimate self-preference by up to 97% compared to baselines
- Optimization-based steering performs comparably to CAA with far fewer examples (data-efficient)
- Context-unaware vectors (hidden authorship) outperformed aware vectors, suggesting linear encoding
- Steering vectors are unstable on legitimate self-preference and unbiased agreement

## Why This Works (Mechanism)

### Mechanism 1: Linear Direction Extraction via Contrastive Pairs
Illegitimate self-preference appears to be at least partially encoded in a linear direction within the model's activation space, which can be isolated and suppressed. Contrastive Activation Addition (CAA) computes the difference in residual stream activations between "positive" (unbiased) and "negative" (biased) prompt completions. Averaging these differences yields a steering vector representing the "self-preference" direction. Subtracting this vector during inference shifts the model away from biased behavior. The core assumption is that the bias manifests predominantly as a linear addition to the residual stream activation. If the bias were represented purely non-linearly or distributed diffusely, linear vector subtraction would fail to change the output or would require infeasibly large multipliers that degrade coherence.

### Mechanism 2: Targeted Behavioral Perturbation via Optimization
An optimization-based approach can learn a steering vector that specifically maximizes the probability of an "unbiased" completion while suppressing the "biased" one, offering data efficiency over CAA. Instead of averaging activations (CAA), this method optimizes an additive vector $h$ via gradient descent. It minimizes a loss function that explicitly increases the likelihood of the "gold" response ($Y^+$) and decreases the likelihood of the self-preferred response ($Y^-$). There exists a specific activation perturbation that flips the preference without damaging the model's fundamental judging capabilities. If the loss landscape is too rugged or the vector overfits to the specific training examples, the vector will fail to generalize to unseen summaries.

### Mechanism 3: Disentanglement of Quality and Bias
Self-preference is not a monolithic behavior; it can be decomposed into "illegitimate" (bias) and "legitimate" (quality), allowing for targeted mitigation. The paper constructs a dataset using "gold judges" (ensemble of diverse models) to label instances where the self-model is right (legitimate) vs. wrong (illegitimate). This creates the signal required to train vectors that correct error without destroying accuracy. An ensemble of external models (Phi-4, DeepSeek, Claude) provides a sufficiently objective ground truth to distinguish bias from quality. If the "gold judges" share systematic biases with the model being steered, the resulting vector will reinforce rather than mitigate the error.

## Foundational Learning

- **Concept: Residual Stream & Activation Patching**
  - **Why needed here:** The intervention acts directly on the residual stream (hidden states) at specific layers. Without understanding that information flows accumulatively through layers, the mechanics of "steering" via vector addition will be opaque.
  - **Quick check question:** If you add a vector to layer 14, does it affect the weights of layer 13? (Answer: No, it only affects downstream layers).

- **Concept: Positional and Ordering Bias**
  - **Why needed here:** The paper explicitly notes that "ordering bias" confounded their CAA signal and required specific experimental controls (running prompts with swapped orderings). Understanding this is critical to generating clean data.
  - **Quick check question:** If a model prefers Summary A over Summary B 70% of the time, but also prefers the summary listed first 70% of the time, how can you isolate the true preference? (Answer: Swap positions and average the results).

- **Concept: Steering Multipliers**
  - **Why needed here:** The magnitude of the steering vector (multiplier) controls the trade-off between bias removal and model stability. Too high a value causes divergence; too low has no effect.
  - **Quick check question:** In Figure 2, why does the probability of choosing "other" change as the multiplier moves from -0.5 to 0.5?

## Architecture Onboarding

- **Component map:** XSUM articles → Summaries (Llama vs GPT-3.5) → Gold Labels (Phi-4/DeepSeek/Claude Ensemble) → Vector Factory (CAA/Optimization) → Llama-3.1-8B-Instruct + Hook (Layer 14/15/16) → Evaluation Metrics
- **Critical path:** The creation of the **Gold Label Dataset** is the bottleneck. Without high-quality ground truth to distinguish legitimate vs. illegitimate preference, the vectors cannot be trained or reliably evaluated.
- **Design tradeoffs:**
  - CAA vs. Optimization: CAA is simple but requires more data and is sensitive to noise (ordering bias). Optimization is data-efficient (works with fewer examples) but requires hyperparameter tuning (lr, iterations) and is prone to overfitting.
  - Effectiveness vs. Stability: Aggressive steering (high multiplier) flips illegitimate bias effectively but also flips legitimate preferences (over-correction).
- **Failure signatures:**
  - Over-Steering: The model flips to preferring the *other* model's summary even when its own is objectively better (high LSP flip rate).
  - Coherence Loss: The model generates nonsensical text if the steering vector magnitude is too high or applied to the wrong layer.
  - Signal Noise: CAA fails to converge if ordering bias in the prompt pairs is not randomized or accounted for.
- **First 3 experiments:**
  1. **Layer Sensitivity Scan:** Inject the optimization-based vector at layers 10 through 20 to identify the "self-preference" critical layer (paper suggests 14-16, but verify for your specific model checkpoint).
  2. **Multiplier Sweep:** Plot "P(other)" vs. "Steering Multiplier" (similar to Fig 2) on a held-out set to find the sweet spot where illegitimate bias drops but legitimate preference remains stable.
  3. **Generalization Test:** Train the vector on XSUM (news) and test on a different domain (e.g., the APPS coding dataset mentioned in Appendix E) to see if the bias representation is universal or domain-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are illegitimate self-preference, legitimate self-preference, and unbiased agreement encoded as distinct directions (linear or non-linear) in the residual stream?
- **Basis in paper:** [explicit] "This could mean that illegitimate self-preference is linearly encoded while the other two are different directions either linearly or nonlinearly encoded in the residual stream. Future work should explore this possibility in depth..."
- **Why unresolved:** Steering vectors achieved 97% effectiveness on illegitimate bias but showed instability on legitimate preferences and agreement, suggesting qualitatively different representations that current single-direction methods cannot jointly preserve.
- **What evidence would resolve it:** Mechanistic interpretability analyses (e.g., probing classifiers, activation patching) across layers to map how each preference type is represented, plus testing whether multi-direction steering can jointly optimize all three.

### Open Question 2
- **Question:** Can individual (pointwise) evaluations replace pairwise comparisons to reduce ordering bias and improve steering signal quality?
- **Basis in paper:** [explicit] "Another major confounding factor for CAA in particular was the ordering bias models exhibit in the pairwise setting... Future work should incorporate individual rather than pairwise evaluations for self-preference to circumvent this issue."
- **Why unresolved:** Pairwise setups introduce positional bias that may obstruct the steering signal, particularly for legitimate self-preference and agreement cases.
- **What evidence would resolve it:** Constructing a pointwise evaluation framework where models score summaries independently, then comparing steering vector effectiveness and stability against the pairwise approach.

### Open Question 3
- **Question:** Do self-preference steering vectors generalize across task domains beyond news summarization?
- **Basis in paper:** [inferred] The paper evaluates only on XSUM (news summarization) and briefly mentions preliminary investigations in other domains (Appendix E), but does not report cross-domain results.
- **Why unresolved:** Self-preference bias may manifest differently across tasks (e.g., code generation, dialogue, reasoning), and domain-specific features may require different steering strategies.
- **What evidence would resolve it:** Testing steering vectors constructed from XSUM on diverse tasks (code, math, creative writing) and reporting cross-domain transfer, plus training domain-specific vectors for comparison.

## Limitations
- Ground truth assumption: The assumption that gold judge ensembles provide objective ground truth is critical but unverified.
- Linear representation: The paper suggests self-preference may be represented non-linearly across multiple directions, which linear steering vectors cannot fully address.
- Domain generalization: The effectiveness metrics rely on a single evaluation dataset (XSUM), raising questions about domain generalization.

## Confidence
- **High Confidence:** The existence of self-preference bias in LLM evaluators is well-established. The CAA and optimization methodologies are clearly specified and reproducible.
- **Medium Confidence:** The claim that steering vectors reduce illegitimate self-preference by up to 97% is supported by results, but the instability on legitimate preferences suggests the solution is incomplete.
- **Low Confidence:** The assumption that gold judge ensembles provide objective ground truth is critical but unverified.

## Next Checks
1. **Ground Truth Validation:** Test the gold judge ensemble against human annotators on a subset of examples to verify their objectivity and identify any systematic biases that could propagate through the vector training process.
2. **Layer Sensitivity Verification:** Conduct systematic layer-by-layer analysis of vector application (layers 10-20) to confirm the paper's claim that layers 14-16 are optimal, and identify whether different bias components manifest at different layers.
3. **Domain Generalization Test:** Evaluate the trained steering vectors on a completely different domain (e.g., APPS coding dataset) to determine whether self-preference representations are universal or domain-specific.