---
ver: rpa2
title: Benchmarking Large Language Models on Homework Assessment in Circuit Analysis
arxiv_id: '2506.06390'
source_url: https://arxiv.org/abs/2506.06390
tags:
- student
- answer
- final
- correct
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks large language models (LLMs) on homework
  assessment for an undergraduate circuit analysis course. The authors develop a novel
  dataset of real student solutions and official reference solutions in LaTeX format,
  and evaluate three LLMs (GPT-3.5 Turbo, GPT-4o, and Llama 3 70B) across five metrics:
  completeness, method, final answer, arithmetic error, and units.'
---

# Benchmarking Large Language Models on Homework Assessment in Circuit Analysis

## Quick Facts
- **arXiv ID:** 2506.06390
- **Source URL:** https://arxiv.org/abs/2506.06390
- **Reference count:** 26
- **Primary result:** GPT-4o and Llama 3 70B outperform GPT-3.5 Turbo in assessing circuit analysis homework, with GPT-4o showing more consistent performance and Llama 3 70B having higher false positive rates.

## Executive Summary
This paper benchmarks three large language models (GPT-3.5 Turbo, GPT-4o, and Llama 3 70B) on homework assessment for an undergraduate circuit analysis course. The authors develop a novel dataset of real student solutions and official reference solutions in LaTeX format, evaluating the models across five metrics: completeness, method, final answer, arithmetic error, and units. The study finds that GPT-4o and Llama 3 70B significantly outperform GPT-3.5 Turbo, with GPT-4o showing more consistent performance and Llama 3 70B having higher false positive rates. The research identifies limitations of current LLMs in circuit diagram recognition, handwritten solution recognition, and mathematical calculations, providing insights for future development of reliable personalized tutors for circuit analysis education.

## Method Summary
The authors benchmarked three LLMs on homework assessment for an undergraduate circuit analysis course using a novel dataset of 283 real student solutions across 119 problems from the "Introduction to Electric Circuits" textbook (Svoboda & Dorf, 9th ed). All solutions were converted to LaTeX format via Mathpix with manual proofreading. The models were evaluated using zero-shot prompting with a specific template containing the official solution, student solution, and correct final answer. Five binary classification metrics were used: completeness, method, final answer, arithmetic error, and units, with outputs classified as "Correct," "Partially Correct," or "Incorrect" by a human expert.

## Key Results
- GPT-4o and Llama 3 70B significantly outperform GPT-3.5 Turbo across all five metrics
- GPT-4o shows more consistent performance across different problem types
- Llama 3 70B has higher false positive rates, particularly in marking incomplete solutions as complete
- Current LLMs struggle with circuit diagram recognition and handwritten solution interpretation

## Why This Works (Mechanism)
The study leverages LLMs' ability to parse and compare structured mathematical content in LaTeX format to assess student solutions against official reference answers. By providing both the reference solution and the student's work, the models can identify discrepancies in methodology, final answers, and intermediate calculations. The five-metric evaluation framework allows for granular assessment of different aspects of student work, from overall completeness to specific technical errors.

## Foundational Learning
- **LaTeX format processing:** Why needed - Enables precise representation of mathematical expressions and circuit diagrams; Quick check - Verify models can accurately parse and interpret LaTeX equations
- **Zero-shot prompting:** Why needed - Allows evaluation without model fine-tuning; Quick check - Test prompt template with known correct/incorrect examples
- **Circuit analysis fundamentals:** Why needed - Essential for understanding the problem domain and evaluating model performance; Quick check - Review basic circuit analysis principles and common student errors
- **Binary classification metrics:** Why needed - Provides standardized evaluation framework; Quick check - Validate metric definitions with human experts

## Architecture Onboarding

**Component Map:** Dataset (LaTeX solutions) -> LLM API (GPT-3.5 Turbo, GPT-4o, Llama 3 70B) -> Evaluation Metrics (5 binary classifications) -> Human Expert Validation

**Critical Path:** Student solution input -> LaTeX preprocessing -> LLM prompt generation -> Model inference -> Metric classification -> Expert validation

**Design Tradeoffs:** Zero-shot prompting (easier implementation, potentially lower accuracy) vs. fine-tuning (higher accuracy, requires more data and resources); LaTeX format (precise mathematical representation, limited to text-based input) vs. image input (includes diagrams, harder for LLMs to parse)

**Failure Signatures:** False positives in completeness assessment (Llama 3 70B), arithmetic error hallucinations, difficulty interpreting circuit diagrams, confusion with alternative solution methods

**First Experiments:**
1. Test prompt template with a set of known correct and incorrect student solutions to verify metric classifications
2. Compare model performance on simple vs. complex circuit problems to identify problem type dependencies
3. Evaluate model sensitivity to different LaTeX formatting styles for the same mathematical content

## Open Questions the Paper Calls Out

**Open Question 1:** Can customized prompt engineering or fine-tuning mitigate specific LLM failure modes in circuit analysis, such as unit conversion errors and false incompleteness assessments? (Basis: Section 6.1 proposes developing customized prompts; Evidence needed: Comparative evaluation showing reduced error rates)

**Open Question 2:** How can information be effectively extracted from circuit diagrams to enable LLMs to independently solve circuit analysis problems? (Basis: Section 6.2 notes current inability to interpret diagrams; Evidence needed: Benchmark where LLM solves problems using only raw circuit diagram images)

**Open Question 3:** Does the proposed five-metric evaluation framework generalize effectively to other engineering disciplines? (Basis: Section 6.3 suggests expanding to other courses; Evidence needed: Successful replication on datasets from distinct engineering fields)

## Limitations
- Dataset unavailability due to student privacy constraints requires simulation of student solutions
- No specification of model inference hyperparameters (temperature, top_p values)
- Subjective evaluation boundaries between "partially correct" and "incorrect" classifications without detailed rubric

## Confidence
- **High Confidence:** Relative performance ranking (GPT-4o > Llama 3 70B > GPT-3.5 Turbo) based on consistent numerical differences
- **Medium Confidence:** Absolute performance percentages dependent on simulated dataset
- **Low Confidence:** Claims about specific failure modes in diagram interpretation as these were limitations rather than systematically measured

## Next Checks
1. Replicate the study using a different textbook or problem set to verify whether observed performance patterns generalize beyond Svoboda & Dorf source material
2. Implement a standardized rubric for distinguishing "partially correct" from "incorrect" classifications and apply it across multiple human evaluators to measure inter-rater reliability
3. Test whether fine-tuning Llama 3 70B on the circuit analysis dataset reduces its false positive rate while maintaining high true positive rates