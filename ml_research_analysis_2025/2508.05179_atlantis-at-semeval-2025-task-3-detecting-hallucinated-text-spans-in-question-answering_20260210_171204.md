---
ver: rpa2
title: 'ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question
  Answering'
arxiv_id: '2508.05179'
source_url: https://arxiv.org/abs/2508.05179
tags:
- question
- data
- text
- retrieval
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the ATLANTIS team's approach to SemEval-2025
  Task 3, which focuses on detecting hallucinated text spans in question answering
  systems. The task involves identifying incorrect or misleading content generated
  by large language models (LLMs), a critical challenge for deploying reliable NLG
  systems.
---

# ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering

## Quick Facts
- arXiv ID: 2508.05179
- Source URL: https://arxiv.org/abs/2508.05179
- Reference count: 14
- Primary result: ATLANTIS team achieved top rankings in detecting hallucinated text spans in LLM-generated QA answers, with Gemini 1.5 Pro ranking first in Spanish, third in English, and fifth in German.

## Executive Summary
This paper presents the ATLANTIS team's approach to SemEval-2025 Task 3, focusing on detecting hallucinated text spans in question answering systems. The task addresses the critical challenge of identifying incorrect or misleading content generated by large language models. The team explored multiple methods both with and without external context from Wikipedia, using few-shot prompting with Gemini 1.5 Pro, fine-tuning token-level classifiers (XLM-RoBERTa), and retrieval-augmented generation systems. Results show that retrieval consistently improved performance across all languages, with Gemini 1.5 Pro achieving the best overall results while fine-tuned models demonstrated competitive performance despite being significantly smaller.

## Method Summary
The team employed three main approaches for hallucination detection. Without retrieval, they used few-shot prompting with Gemini 1.5 Pro and fine-tuned a token-level classifier (XLM-RoBERTa) on synthetic data generated using the MKQA dataset. With retrieval, they integrated Wikipedia context via a RAG system using hybrid search (BAAI/bge-large-en-v1.5 + BM25), reranking, and clustering to select relevant passages. The detection models then used this context to identify factual inconsistencies. The synthetic data generation involved injecting hallucinations into correct answers using Gemini 1.5 Pro, creating ~48,000 samples across four languages. Models were evaluated using IoU (Intersection over Union) for span detection, with Gemini 1.5 Pro showing superior performance while fine-tuned models achieved competitive results, particularly in German and French.

## Key Results
- Gemini 1.5 Pro achieved top rankings across languages, ranking first in Spanish, third in English, and fifth in German
- Retrieval consistently improved performance across all languages when added to both LLM prompting and fine-tuned models
- Fine-tuned models, particularly XLM-RoBERTa, achieved competitive performance (IoU 0.53 in German, 0.50 in French) despite being significantly smaller than LLM baselines
- Spanish consistently underperformed compared to other languages across all model types

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Factual Grounding
- **Claim:** External context from Wikipedia provides a verifiable knowledge base that enables comparison between generated text and ground truth.
- **Mechanism:** The retrieval module extracts relevant chunks from Wikipedia (312 tokens with 100-token overlap), reranks them with a cross-encoder, and uses clustering to select the most relevant passages. This context is then provided to both LLM prompts and fine-tuned models, allowing them to identify factual inconsistencies by direct comparison.
- **Core assumption:** The relevant factual information exists in the indexed Wikipedia corpus and can be successfully retrieved; query translation preserves semantic meaning.
- **Evidence anchors:**
  - [abstract]: "Notably, our approaches achieved top rankings... This work highlights the importance of integrating relevant context to mitigate hallucinations"
  - [Section 3.2.2]: "Comparing with the previous section, the scores are better in all languages, without exception" when retrieval is added
  - [corpus]: REFIND (arXiv:2502.13622) similarly uses retrieval-augmented factuality detection, supporting this mechanism's validity
- **Break condition:** When retrieval MAP scores drop significantly (e.g., 0.62 for French vs. 0.81 for English), performance degrades; when relevant facts don't exist in the indexed corpus or query translation introduces errors.

### Mechanism 2: Token-Level Hallucination Classification via Synthetic Training
- **Claim:** Hallucination detection can be framed as a sequence labeling task where each token is classified as hallucinated ('I-H') or not ('O'), enabling precise span identification.
- **Mechanism:** XLM-RoBERTa-large is fine-tuned on synthetically generated data (~48,000 samples across 4 languages) where hallucinations are deliberately injected. A linear classification head predicts token-level labels. The model learns to associate contextual patterns (question + retrieved chunk + answer) with hallucinated spans.
- **Core assumption:** Synthetic hallucination patterns generated by Gemini 1.5 Pro generalize to real-world hallucinations; token-level labels capture the full extent of hallucinated spans.
- **Evidence anchors:**
  - [Section 2.3.2]: "The span hallucination task can be also casted as a more classical token classification task"
  - [Section 3.2.2]: "XLM-RoBERTa, significantly smaller, achieves a score that is relatively close to Llama-3.2-3B in German (0.53 vs 0.57)"
  - [corpus]: Weak direct corpus evidence for token-classification specifically; related work (Zhou et al., 2020) cited for machine translation hallucination detection
- **Break condition:** When synthetic data quality is poor (contains errors, biases, or unrealistic hallucination patterns), or when the 512-token context window truncates critical context (only top-1 chunk used).

### Mechanism 3: Few-Shot Prompting with Structured Output
- **Claim:** Large LLMs (Gemini 1.5 Pro) can identify hallucinated spans through in-context learning when provided with examples and explicit output format constraints.
- **Mechanism:** A custom prompt includes 2 examples with labeled hallucinations in JSON format, the question, model output, and retrieved context. The LLM identifies factual inaccuracies and misspellings, returning structured JSON that is parsed to extract span positions. Temperature=0.0 ensures consistency.
- **Core assumption:** The LLM's parametric knowledge combined with retrieved context enables discrimination between accurate and hallucinated content; JSON output can be reliably parsed.
- **Evidence anchors:**
  - [Section 2.2.2]: "Our objective was to maximize the capabilities of an LLM by first detecting hallucinations, then implement custom functions to extract the hallucinated spans"
  - [Section 3.2.2]: "Gemini 1.5 Pro still outperforms the fine-tuned approaches on 3 over 4 languages"
  - [corpus]: Multiple SemEval-2025 Task 3 participants (UCSC, MSA) use LLM-based prompting approaches, indicating broad applicability
- **Break condition:** When prompt design doesn't align with dataset characteristics; when retrieved context contains conflicting information; when language-specific nuances aren't captured by simple example substitution.

## Foundational Learning

- **Concept: Intersection over Union (IoU) for Span Detection**
  - **Why needed here:** The task evaluation uses IoU to measure overlap between predicted and ground-truth hallucinated spans. Understanding IoU is critical for interpreting model performance (e.g., IoU of 0.53 in German means ~53% overlap).
  - **Quick check question:** If a model predicts hallucinated span [0, 20] but ground truth is [10, 30], what's the IoU?

- **Concept: Retrieval-Augmented Generation (RAG) Pipeline**
  - **Why needed here:** The retrieval module (hybrid search → reranking → clustering) directly impacts downstream hallucination detection. Poor retrieval (MAP 0.62 for French) limits even the best detection models.
  - **Quick check question:** Why might a cross-encoder reranker improve retrieval quality over embedding-only search?

- **Concept: Synthetic Data Generation for Low-Resource Tasks**
  - **Why needed here:** No annotated training data was provided; the team generated ~48,000 synthetic samples. Understanding injection strategies and quality tradeoffs is essential for replicability.
  - **Quick check question:** What risks arise when training on synthetic hallucinations generated by the same class of model being evaluated?

## Architecture Onboarding

- **Component map:**
  - Retrieval Module: Wikipedia (English, Nov 2023) → Chunking (312 tokens, 100 overlap) → Hybrid Search (BAAI/bge-large-en-v1.5 + BM25 + distribution fusion) → Cross-encoder reranking → K-means clustering → Top-k chunks
  - Query Translation: Mistral-7B-Instruct-v0.2 translates non-English queries to English
  - Detection Models:
    - Path A: Gemini 1.5 Pro + 2-shot prompt (JSON output)
    - Path B: XLM-RoBERTa-large + linear classifier (512-token limit, top-1 chunk only)
    - Path C: Llama-3.2-3B-Instruct + LoRA fine-tuning (FAVA-style editing)

- **Critical path:**
  1. Query input → (translation if non-English) → retrieval
  2. Retrieved context + question + LLM output → detection model
  3. Predicted hallucinated spans → IoU evaluation

- **Design tradeoffs:**
  - **Gemini 1.5 Pro:** Best performance (avg IoU 0.54 with retrieval) but high API cost and latency; no fine-tuning needed
  - **XLM-RoBERTa-large:** Smaller (0.49 avg IoU), faster inference, but limited to 512 tokens (only top-1 chunk); requires synthetic data generation
  - **Llama-3.2-3B:** Competitive (0.54 avg IoU), can be deployed on-premise, but requires LoRA fine-tuning infrastructure
  - **Retrieval language:** Only English Wikipedia indexed; translation step introduces error for other languages

- **Failure signatures:**
  - IoU drops significantly when retrieval MAP < 0.65 (French: 0.62 MAP → 0.50 IoU for Gemini with retrieval)
  - Token classifier under-confident; requires threshold tuning below 0.5 default
  - Spanish consistently underperforms (0.37 IoU for XLM-RoBERTa with retrieval vs. 0.53 for German)

- **First 3 experiments:**
  1. **Baseline validation:** Run the overlap-based method (Section 2.2.1) on English test data to establish a lower bound before implementing complex retrieval.
  2. **Ablation on context window size:** Test XLM-RoBERTa with top-1, top-3, and top-5 chunks (truncated/prioritized to fit 512 tokens) to quantify retrieval context impact.
  3. **Cross-lingual transfer analysis:** Compare monolingual vs. multilingual fine-tuning (Table 3) for XLM-RoBERTa on a held-out language to determine if multilingual training provides regularization benefits.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does a translation-based retrieval pipeline perform better than multilingual embedding models for non-English queries?
  - **Basis in paper:** [explicit] Section 3.3 notes the authors relied on LLM translation but "This approach could have been compared with multilingual embedding models."
  - **Why unresolved:** The paper only evaluated translating queries to English rather than using native multilingual search indices.
  - **What evidence would resolve it:** Comparative analysis of retrieval metrics (MAP@5) and downstream hallucination detection IoU using multilingual embeddings versus the implemented translation method.

- **Open Question 2:** Does filtering for the most relevant sentences improve token-level classification compared to using the top chunk?
  - **Basis in paper:** [explicit] Section 3.3 identifies the limited context window as a limitation and suggests a "potential solution would be to filter and include only the most relevant sentences."
  - **Why unresolved:** The current model only sees the top-1 chunk, potentially missing facts distributed across other chunks.
  - **What evidence would resolve it:** Experiments contrasting the current input strategy against a method extracting top-k sentences from multiple retrieved chunks.

- **Open Question 3:** Do models trained on synthetic hallucinations generalize to datasets with a balanced mix of correct and hallucinated answers?
  - **Basis in paper:** [explicit] The Conclusion states approaches "would need to be validated on a balanced dataset, containing also a significant part of non-hallucinated answers."
  - **Why unresolved:** Current validation focuses on detecting errors; performance on correct answers (false positive rates) remains uncertain.
  - **What evidence would resolve it:** Evaluation of the fine-tuned XLM-RoBERTa and Llama models on a curated dataset with an equal distribution of correct and incorrect samples.

## Limitations

- **Synthetic Data Quality Uncertainty:** The token-level classifier's performance depends entirely on the quality and representativeness of synthetic hallucinations generated by Gemini 1.5 Pro, with no validation of synthetic-to-real generalization.
- **Language Coverage Limitations:** The multilingual approach relies on query translation to English and English Wikipedia indexing, creating potential semantic loss and retrieval performance degradation for non-English languages.
- **Context Window Constraints:** The XLM-RoBERTa classifier uses only the top-1 chunk (512 tokens max), which may truncate critical context without quantitative analysis of coverage impact.

## Confidence

- **High Confidence:** Retrieval consistently improves performance across all languages (directly supported by comparative IoU results showing improvement when retrieval is added).
- **Medium Confidence:** Gemini 1.5 Pro's superior performance generalizes across languages (results show consistent ranking advantages but only 4 languages tested).
- **Low Confidence:** Smaller fine-tuned models can achieve competitive performance with proper synthetic training (supported by specific results but lacks ablation studies on synthetic data quality impact).

## Next Checks

1. **Synthetic Data Validation:** Generate a small manually-annotated test set of real hallucinations across 2-3 languages and evaluate the XLM-RoBERTa classifier trained on synthetic data. Measure performance drop to quantify synthetic-to-real generalization gap. If IoU drops >20%, investigate synthetic data quality or generation prompts.

2. **Context Window Ablation:** Systematically vary the number of retrieved chunks provided to XLM-RoBERTa (top-1, top-3, top-5) while maintaining the 512-token limit. Measure IoU improvement and identify the marginal benefit of additional context. If top-3 provides >80% of top-5 performance, consider it a practical tradeoff.

3. **Cross-Lingual Transfer Analysis:** Train XLM-RoBERTa on a multilingual corpus (all 4 languages) versus monolingual models and evaluate on a held-out language from the same family (e.g., Italian or Portuguese). Measure performance difference to determine if multilingual training provides cross-lingual benefits or simply increases data volume.