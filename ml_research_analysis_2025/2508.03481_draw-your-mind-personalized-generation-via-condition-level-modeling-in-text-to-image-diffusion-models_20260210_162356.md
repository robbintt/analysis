---
ver: rpa2
title: 'Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image
  Diffusion Models'
arxiv_id: '2508.03481'
source_url: https://arxiv.org/abs/2508.03481
tags:
- user
- diffusion
- personalized
- generation
- drum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personalized image generation
  in text-to-image diffusion models, which often struggle to incorporate individual
  user preferences accurately due to limited input token capacity and reliance on
  prompt-level modeling. The proposed method, DrUM, introduces a novel approach that
  integrates user profiling with a transformer-based adapter to enable personalized
  generation through condition-level modeling in the latent space.
---

# Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2508.03481
- Source URL: https://arxiv.org/abs/2508.03481
- Authors: Hyungjin Kim; Seokho Ahn; Young-Duk Seo
- Reference count: 34
- One-line primary result: CLIP score improvements up to +34.83% via condition-level personalization without model fine-tuning

## Executive Summary
This paper addresses personalized image generation challenges in text-to-image diffusion models by introducing DrUM, a method that integrates user profiling with transformer-based adapters to enable condition-level modeling in latent space. By leveraging coreset sampling for efficient preference extraction and a guidance mechanism for fine-grained control, DrUM achieves superior performance across multiple foundation models like Stable Diffusion and FLUX without requiring additional fine-tuning. The approach overcomes token capacity limitations by operating in the latent condition space, allowing integration of richer preference information than prompt-level token sequences permit.

## Method Summary
DrUM introduces a three-component approach to personalized T2I generation: (1) Coreset sampling algorithm selects representative user preference prompts using CLIP similarity and preference intensities; (2) PeCA (Personalized Conditioning Adapter) - a transformer adapter with 10 cross-attention layers - modifies text encoder outputs to fuse target and historical conditions; (3) Guidance mechanism decouples softmax across conditions with weighting α for fine-grained control. The method trains on CC3M captions using cosine similarity loss to reconstruct target conditions, then applies the learned adapter to frozen T2I models (SD/FLUX) during inference. Evaluation on PIP and MovieLens datasets shows CLIP score improvements up to +34.83% and enhanced text alignment while preserving creativity and diversity.

## Key Results
- CLIP score improvements of up to +34.83% compared to baselines
- Enhanced text alignment while maintaining target semantics
- Superior performance across multiple foundation models (Stable Diffusion, FLUX) without requiring fine-tuning
- Effective personalization without overwhelming target concept, validated through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Condition-level modeling bypasses token capacity limits
- Claim: Operating in the latent condition space allows integration of richer preference information than prompt-level token sequences permit.
- Mechanism: PeCA attaches to text encoders and uses cross-attention layers to fuse historical prompt embeddings with target embeddings before they reach the diffusion model—effectively concatenating conditions in latent space rather than token space.
- Core assumption: Cross-attention can preserve and transfer preference-relevant semantic patterns across embeddings without requiring explicit token representation.
- Evidence anchors: [abstract] "enables personalized generation through condition-level modeling in the latent space"; [section 4.3] "overcoming constraints imposed by prompt-level limitations"; [corpus] Related work relies on prompt-level rewriting with LLMs.

### Mechanism 2: Coreset sampling preserves preference diversity while reducing redundancy
- Claim: Greedy approximation of minimum enclosing ball selects representative prompts that capture semantic spread without exhaustive processing.
- Mechanism: Iteratively selects samples maximizing distance from already-chosen points (weighted by preference intensity), ensuring coverage of the user's semantic preference space.
- Core assumption: CLIP embedding distances correlate with preference-relevant semantic differences; preference intensities indicate importance weighting.
- Evidence anchors: [abstract] "leveraging coreset sampling for efficient user preference extraction"; [section 4.2] "exclude unnecessary references, reducing computational overhead while enhancing personalization"; [figure 3] Shows coreset sampling outperforming random/uniform sampling.

### Mechanism 3: Decoupled attention guidance enables fine-grained personalization control
- Claim: Independent softmax for reference vs target conditions prevents attention dilution and allows explicit control over personalization intensity.
- Mechanism: Splits attention computation—target gets weight (1-α), references share weight α proportionally to their softmax scores × preference intensities. Uses unconditional embeddings as query to anchor to text encoder's central distribution.
- Core assumption: Target and reference conditions occupy separable attention spaces that should not compete in a single softmax normalization.
- Evidence anchors: [abstract] "guidance mechanism for fine-grained control"; [section 4.4] "computation of attention scores for multiple reference conditions and the target condition is decoupled"; [table 3] Ablation shows removing guidance drops target CLIP score from 25.13 to 19.04.

## Foundational Learning

- **Cross-attention in transformers**: Queries from one modality attend to keys/values from another. DrUM uses cross-attention to fuse target and historical condition embeddings.
  - Why needed here: PeCA's core operation is cross-attention over concatenated conditions.
  - Quick check question: Can you explain why softmax is applied to the dot-product of Q and K before multiplying by V?

- **Classifier-free guidance**: Conditioning technique that blends conditional and unconditional predictions to strengthen adherence to conditions.
  - Why needed here: DrUM's guidance mechanism is inspired by CFG; uses unconditional embeddings as query input.
  - Quick check question: What happens to output diversity when guidance scale increases?

- **Text encoder token limitations**: Models like OpenCLIP (77 tokens) and T5 (512 tokens) truncate long prompts, losing information.
  - Why needed here: DrUM's motivation—prompt-level modeling cannot capture extended user histories.
  - Quick check question: Why does truncation particularly hurt personalized generation compared to single-prompt generation?

## Architecture Onboarding

- **Component map**: Historical prompts → Coreset Sampling (Algorithm 1) → Sampled prompts → Text Encoder → PeCA (cross-attention layers) → Fused embeddings → Guidance Mechanism (α-weighted decoupled attention) → Personalized condition → Frozen T2I model → Output image

- **Critical path**: Coreset sampling efficiency → PeCA training convergence → Guidance α calibration. If any stage fails, personalization degrades without visible error.

- **Design tradeoffs**:
  - Sampling ratio (10% default): Higher captures more history but increases PeCA sequence length and computation
  - α (0.1-0.3 in experiments): Controls target vs history balance; domain-dependent
  - PeCA depth (10 cross-attention layers): Deeper may capture more patterns but risks distribution drift

- **Failure signatures**:
  - Target semantics lost (e.g., "clothes" attribute missing in FLUX baseline, recovered by DrUM)
  - Over-smoothing toward average preferences when guidance removed
  - Complex product descriptions causing semantic conflicts

- **First 3 experiments**:
  1. **Ablate guidance (α=0)**: Generate with guidance disabled, compare CLIP scores to baseline. Expected: target score drops, history score rises—confirms guidance mechanism is load-bearing.
  2. **Vary sampling ratio**: Test 5%, 10%, 20%, 50% on Text align metric. Expected: 10% sweet spot; lower loses information, higher adds noise.
  3. **Cross-encoder test**: Train PeCA on OpenCLIP ViT-L, apply to ViT-bigG without retraining. Expected: Performance maintains if adapter learns generalizable patterns; degrades if overfit.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does increasing the depth of the Personalized Conditioning Adapter (PeCA) affect the model's ability to capture complex patterns compared to the current shallow cross-attention implementation?
  - Basis: "Our architecture employs only a small number of cross-attention layers... a more in-depth exploration with deeper architectures remains a subject for future research."
  - Why unresolved: Current design uses few layers to demonstrate reproducibility, leaving potential performance gains unexplored.
  - Evidence needed: Comparative study evaluating deeper adapter variants versus baseline on complex user profiles.

- **Open Question 2**: Can optimized modeling approaches be developed to improve performance on dense, structured inputs like product information where DrUM currently struggles?
  - Basis: "DrUM showed a slight decrease in ML, likely due to the complexity of product information" and "Future work will explore more optimized architectures and modeling approaches for complex inputs."
  - Why unresolved: Method faces difficulty with rich, complex product details (e.g., movie metadata), leading to performance trade-offs.
  - Evidence needed: Improved CLIP scores and text alignment on MovieLens dataset using modified architecture for complex data.

- **Open Question 3**: Does dynamically adjusting the number of reference prompts or refining token-level information improve personalization fidelity?
  - Basis: "adjusting the number of references or refining token-level information may be beneficial, though this remains to be explored further."
  - Why unresolved: Current experiments use fixed sampling ratios; impact of variable reference counts or granular token refinement is unknown.
  - Evidence needed: Ablation studies showing relationship between variable sampling sizes, token refinement methods, and resulting text alignment scores.

## Limitations

- **Architecture specification gaps**: Critical PeCA implementation details including hidden dimensions, attention head counts, and MLP layer sizes are omitted, making exact reproduction difficult.
- **Dataset preprocessing ambiguity**: Training methodology describes using CC3M captions "grouped into user histories" but doesn't specify how individual captions become multi-prompt user histories.
- **Limited domain testing**: Performance variation between domains shows method struggles with complex product information, suggesting limited robustness across use cases.

## Confidence

**High confidence**: Core mechanism of condition-level modeling via PeCA cross-attention fusion is technically sound with verifiable CLIP score improvements (+34.83%) and qualitative examples.
**Medium confidence**: Coreset sampling algorithm's effectiveness depends on CLIP embedding distances accurately reflecting semantic preference differences, which may not hold universally.
**Low confidence**: Guidance mechanism's decoupled softmax approach appears novel but superiority over simpler alternatives is not rigorously established through comprehensive ablation studies.

## Next Checks

- **Check 1: Ablation of guidance mechanism**: Remove the decoupled softmax and test with standard weighted average attention. Compare target CLIP scores, history CLIP scores, and generation diversity to confirm that independent softmax is load-bearing.
- **Check 2: Cross-encoder generalization**: Train PeCA on one text encoder (e.g., OpenCLIP ViT-L) and apply it to a different encoder (e.g., T5) without retraining. Measure performance degradation to assess whether PeCA learns generalizable patterns or overfits.
- **Check 3: Distribution preservation testing**: Generate images using neutral prompts (no personalization) with and without PeCA enabled. Compare CLIP similarity to original prompts and visual quality metrics to verify that PeCA modifications don't degrade base model performance.