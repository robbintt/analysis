---
ver: rpa2
title: 'ORION: Teaching Language Models to Reason Efficiently in the Language of Thought'
arxiv_id: '2511.22891'
source_url: https://arxiv.org/abs/2511.22891
tags:
- reasoning
- zhang
- wang
- arxiv
- slpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of current Large Reasoning
  Models (LRMs) which rely on long, verbose chains of thought, leading to high latency,
  redundancy, and incoherent reasoning paths. To tackle this, the authors introduce
  a cognitively motivated framework that trains models to reason in a compact symbolic
  style inspired by the Language of Thought Hypothesis.
---

# ORION: Teaching Language Models to Reason Efficiently in the Language of Thought

## Quick Facts
- arXiv ID: 2511.22891
- Source URL: https://arxiv.org/abs/2511.22891
- Reference count: 40
- Primary result: ORION models achieve 4–16× token reduction, 5× lower latency, and 7–9× lower training costs while maintaining 90–98% of baseline accuracy.

## Executive Summary
ORION introduces a cognitively inspired approach to improve reasoning efficiency in Large Reasoning Models (LRMs) by training them to reason in a compact symbolic format called Mentalese. This format, based on the Language of Thought Hypothesis, encodes abstract reasoning as ultra-compressed, structured tokens rather than verbose natural language chains of thought. To further enhance efficiency, the authors propose Shorter Length Preference Optimization (SLPO), a reinforcement learning method that dynamically rewards concise yet correct reasoning. The resulting ORION models demonstrate dramatic improvements in token efficiency, latency, and training costs while maintaining high accuracy on math and reasoning benchmarks.

## Method Summary
The paper presents a novel framework that combines symbolic reasoning with preference optimization to create more efficient reasoning models. At its core is Mentalese, a symbolic representation inspired by cognitive science that compresses reasoning steps into structured tokens. The SLPO algorithm then fine-tunes models to prefer shorter reasoning paths while maintaining correctness, using a dynamic reward system that avoids rigid length penalties. This approach is applied to both distilled and proprietary base models, resulting in ORION variants that significantly outperform traditional LRMs in efficiency metrics while preserving accuracy on challenging reasoning tasks.

## Key Results
- ORION achieves 4–16× fewer tokens compared to DeepSeek R1 Distilled
- Up to 5× lower inference latency and 7–9× reduction in training costs
- Maintains 90–98% of baseline accuracy while surpassing Claude and ChatGPT-4o by up to 5% accuracy with 2× compression

## Why This Works (Mechanism)
The approach works by aligning language model reasoning with cognitive principles from the Language of Thought Hypothesis. By encoding reasoning in a symbolic, compressed format (Mentalese), the model can process abstract relationships more efficiently than natural language. The SLPO algorithm then reinforces this efficiency by dynamically rewarding correct but concise reasoning paths. This combination addresses the fundamental inefficiency of current LRMs, which generate long, redundant chains of thought that are computationally expensive and often contain unnecessary steps.

## Foundational Learning
- **Language of Thought Hypothesis**: Provides theoretical foundation for symbolic reasoning representation - needed to justify compact symbolic format; quick check: validate that Mentalese tokens capture essential reasoning structure
- **Reinforcement Learning from Human Feedback**: Underlies the SLPO optimization method - needed to train preference for conciseness; quick check: verify reward shaping doesn't compromise accuracy
- **Knowledge Distillation**: Enables transfer of reasoning capabilities to more efficient models - needed to leverage strong base models; quick check: ensure distilled models retain reasoning fidelity
- **Symbolic AI principles**: Informs the design of Mentalese representation - needed to create interpretable compressed reasoning; quick check: test if compressed reasoning remains debuggable
- **Efficient inference optimization**: Guides the overall efficiency goals - needed to justify computational savings claims; quick check: benchmark against standard LRMs on same hardware
- **Multi-task learning**: Supports training across diverse reasoning tasks - needed for generalization; quick check: evaluate on tasks beyond GSM8K and MATH

## Architecture Onboarding

**Component Map**: Base LRM -> Mentalese Encoder -> SLPO Fine-tuning -> ORION Model

**Critical Path**: The core innovation flows from Mentalese encoding through SLPO optimization to create the final ORION model. The Mentalese encoder transforms natural language reasoning into symbolic tokens, while SLPO fine-tunes the model to prefer these compressed representations without losing accuracy.

**Design Tradeoffs**: The paper trades some interpretability (compressed symbolic format) for significant efficiency gains. While Mentalese tokens are more compact, they may be less intuitive for human inspection compared to natural language chains of thought. The dynamic SLPO approach balances this by avoiding overly aggressive compression that could compromise accuracy.

**Failure Signatures**: Potential failure modes include: over-compression leading to loss of critical reasoning steps, symbolic representation becoming too abstract for certain problem types, and the SLPO reward function creating unintended optimization behaviors that favor brevity over correctness.

**First Experiments**:
1. Benchmark ORION against other open-weight LRMs (e.g., Qwen2.5-Reasoning, o1-mini) on diverse reasoning tasks
2. Conduct ablation studies varying compression ratios to identify accuracy degradation points
3. Evaluate Mentalese-encoded reasoning on interpretability tasks requiring step-by-step debugging

## Open Questions the Paper Calls Out
The paper acknowledges several limitations and open questions regarding its approach. The evaluation is primarily focused on GSM8K and MATH benchmarks, leaving uncertainty about generalization to broader reasoning tasks or real-world applications. The comparisons are limited to distilled versions of DeepSeek-R1 and proprietary models, without extensive benchmarking against other open-weight LRMs. Additionally, while the compressed Mentalese format shows strong efficiency benefits, the paper doesn't fully explore potential brittleness when reasoning steps become too compressed, nor does it address whether the compressed format affects model interpretability or debuggability for human users.

## Limitations
- Limited benchmarking scope: Primarily tested on GSM8K and MATH, may not generalize to broader reasoning tasks
- Comparison constraints: Only compared against distilled DeepSeek-R1 and strong proprietary models, not other open-weight LRMs
- Interpretability concerns: Compressed Mentalese format may reduce debuggability and human-in-the-loop verification capabilities
- Potential brittleness: Unclear how the approach handles edge cases or complex reasoning that requires detailed explanation

## Confidence

**Major Claims and Confidence Labels:**
- **High Confidence**: The efficiency gains (4-16× token reduction, 5× latency reduction) are well-supported by the experimental results on GSM8K and MATH
- **Medium Confidence**: The claim that ORION surpasses Claude and ChatGPT-4o by up to 5% accuracy is based on limited comparisons and may not hold across diverse reasoning tasks
- **Low Confidence**: The assertion that ORION achieves 90-98% of baseline accuracy with 7-9× lower training costs is difficult to fully verify without access to the exact training configurations and cost breakdowns

## Next Checks

1. Benchmark ORION against other open-weight LRMs (e.g., Qwen2.5-Reasoning, o1-mini) on a broader set of reasoning tasks beyond GSM8K and MATH
2. Conduct ablation studies to determine the trade-off point where compression begins to degrade reasoning accuracy or coherence
3. Evaluate the robustness of Mentalese-encoded reasoning on tasks requiring interpretability, such as step-by-step debugging or human-in-the-loop verification