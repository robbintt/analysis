---
ver: rpa2
title: Entity Representation Learning Through Onsite-Offsite Graph for Pinterest Ads
arxiv_id: '2508.02609'
source_url: https://arxiv.org/abs/2508.02609
tags:
- graph
- training
- data
- pinterest
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for learning entity representations
  by integrating onsite ad engagement data with opt-in offsite conversion data through
  a large-scale heterogeneous graph. The authors introduce TransRA, a novel Knowledge
  Graph Embedding model that improves upon TransR by designating an anchor space to
  streamline downstream integration.
---

# Entity Representation Learning Through Onsite-Offsite Graph for Pinterest Ads

## Quick Facts
- arXiv ID: 2508.02609
- Source URL: https://arxiv.org/abs/2508.02609
- Reference count: 22
- This paper presents a framework for learning entity representations by integrating onsite ad engagement data with opt-in offsite conversion data through a large-scale heterogeneous graph

## Executive Summary
This paper introduces a comprehensive framework for entity representation learning that integrates onsite ad engagement data with opt-in offsite conversion data through a large-scale heterogeneous graph. The authors propose TransRA, a novel Knowledge Graph Embedding model that improves upon TransR by introducing an anchor space to streamline downstream integration. An attention-based finetuning approach is also developed to effectively adapt large-scale KGE models within Pinterest's ranking models, addressing distribution discrepancies between graph and tabular data. The framework was deployed in Pinterest's Ads Engagement Model, achieving a 2.69% lift in CTR and a 1.34% reduction in CPC.

## Method Summary
The framework integrates onsite ad engagement data with opt-in offsite conversion data through a heterogeneous graph structure. The core innovation is TransRA, a modified TransR model that designates an anchor space to improve downstream integration efficiency. The model incorporates an attention-based finetuning mechanism to bridge distribution discrepancies between graph and tabular data sources. This approach allows for effective adaptation of large-scale Knowledge Graph Embedding models within existing ranking architectures, enabling better utilization of both onsite and offsite signals for ad targeting.

## Key Results
- 2.69% lift in CTR after deploying the framework in Pinterest's Ads Engagement Model
- 1.34% reduction in CPC achieved through the integrated representation learning approach
- Substantial offline and online gains validated the effectiveness of combining onsite and offsite data for ad targeting

## Why This Works (Mechanism)
The framework works by creating a unified representation space that captures both onsite user engagement patterns and offsite conversion behaviors. TransRA's anchor space design provides a stable foundation for integrating heterogeneous data sources, while the attention-based finetuning mechanism dynamically adjusts the learned representations to match the distribution characteristics of the target ranking model. This dual approach addresses the fundamental challenge of combining structured graph data with tabular features in advertising systems, resulting in more accurate entity representations that drive improved ad performance.

## Foundational Learning
- Knowledge Graph Embedding fundamentals: Why needed - to represent complex entity relationships in vector space; Quick check - understand TransE/TransR base models
- Heterogeneous graph construction: Why needed - to unify onsite and offsite data sources; Quick check - verify node/edge type definitions
- Distribution adaptation techniques: Why needed - to bridge graph and tabular data gaps; Quick check - validate attention mechanism effectiveness
- Entity representation learning: Why needed - to capture rich user and item interactions; Quick check - measure embedding quality metrics
- Ranking model integration: Why needed - to apply learned representations in production; Quick check - confirm feature engineering pipeline compatibility

## Architecture Onboarding

**Component Map:**
Data Ingestion -> Heterogeneous Graph Construction -> TransRA Training -> Attention-Based Finetuning -> Ranking Model Integration

**Critical Path:**
The critical path flows from data ingestion through heterogeneous graph construction to TransRA training, as these components must be completed before finetuning and integration can occur. The anchor space design in TransRA is particularly critical as it directly impacts the quality of downstream integration.

**Design Tradeoffs:**
The framework trades computational complexity for representation quality by using a modified TransR model with anchor space. This adds training overhead but provides more stable and generalizable embeddings. The attention-based finetuning approach adds another layer of complexity but effectively addresses distribution mismatches between graph and tabular data.

**Failure Signatures:**
- Poor CTR/CPC improvements suggest issues with graph construction or TransRA training
- Training instability may indicate problems with the anchor space design or attention mechanism hyperparameters
- Integration failures point to feature engineering or model compatibility issues

**First 3 Experiments:**
1. Validate TransRA performance against baseline TransR on a small-scale heterogeneous graph
2. Test attention-based finetuning effectiveness with synthetic distribution discrepancies
3. Measure embedding quality metrics (e.g., link prediction accuracy) before and after finetuning

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary implementation prevents independent verification of claimed 2.69% CTR lift and 1.34% CPC reduction
- Limited ablation studies to quantify specific contributions of anchor space versus other components
- Handling of potential data quality issues in offsite conversion signals not thoroughly addressed
- Assumption that distribution discrepancies can be effectively bridged lacks robustness testing across different entity types

## Confidence
- High confidence in technical novelty of TransRA model and attention-based finetuning approach
- Medium confidence in claimed performance improvements due to limited public validation data
- Medium confidence in general applicability beyond Pinterest's specific context

## Next Checks
1. Conduct ablation studies comparing TransRA against standard TransR and other KGE models across multiple datasets to isolate the contribution of the anchor space design
2. Implement the framework on a public advertising dataset with known ground truth to independently verify the claimed performance improvements
3. Test the attention-based finetuning approach's robustness by systematically varying the distribution discrepancies between graph and tabular data sources