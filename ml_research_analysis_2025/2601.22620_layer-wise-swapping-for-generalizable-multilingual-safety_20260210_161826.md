---
ver: rpa2
title: Layer-wise Swapping for Generalizable Multilingual Safety
arxiv_id: '2601.22620'
source_url: https://arxiv.org/abs/2601.22620
tags:
- safety
- swapping
- language
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multilingual safety alignment
  in low-resource languages by proposing a training-free layer-wise and module-wise
  swapping method. The approach transfers safety alignment from an English safety
  expert to low-resource language experts by composing task vectors and adaptively
  selecting or blending attention and MLP modules based on their degree of specialization.
---

# Layer-wise Swapping for Generalizable Multilingual Safety
## Quick Facts
- arXiv ID: 2601.22620
- Source URL: https://arxiv.org/abs/2601.22620
- Reference count: 14
- Primary result: Safety alignment transfer method for low-resource languages achieves up to 30% reduction in unsafety ratios while maintaining general capabilities

## Executive Summary
This paper addresses the challenge of multilingual safety alignment in low-resource languages by proposing a training-free layer-wise and module-wise swapping method. The approach transfers safety alignment from an English safety expert to low-resource language experts by composing task vectors and adaptively selecting or blending attention and MLP modules based on their degree of specialization. Experiments on Korean, Bengali, Swahili, and Telugu demonstrate significant improvements in multilingual safety while maintaining competitive performance on general benchmarks such as MMMLU, BELEBELE, and MGSM.

## Method Summary
The proposed method leverages a safety-aligned English expert model and applies layer-wise and module-wise swapping to transfer safety knowledge to low-resource language models. The approach works by first identifying task vectors that capture the degree of specialization for each layer and module, then either selecting the most specialized components or blending them to create a hybrid model. This process is entirely training-free and relies on the assumption that safety knowledge can be effectively transferred across languages through this modular composition approach.

## Key Results
- Safety performance comparable or better than GPT-4o safety expert baseline
- Up to 30% reduction in unsafety ratios on MultiJail benchmark
- Maintains competitive scores on MMMLU, BELEBELE, and MGSM benchmarks

## Why This Works (Mechanism)
The method exploits the modular nature of transformer architectures by treating different layers and attention/MLP modules as specialized components that can be selectively combined. By composing task vectors that capture specialization degrees, the approach can identify which components from the safety expert are most relevant for safety alignment and transfer them to the target low-resource model without requiring additional training.

## Foundational Learning
- **Layer-wise specialization**: Different layers in transformers learn different types of representations; needed to identify which layers contain safety-relevant knowledge, quick check: verify layer-specific performance differences
- **Module-wise composition**: Attention and MLP modules can be independently evaluated for specialization; needed to enable fine-grained swapping, quick check: confirm module-level task vector effectiveness
- **Cross-lingual transfer**: Safety knowledge can transfer across languages despite linguistic differences; needed to justify the approach, quick check: validate transfer effectiveness across diverse language families
- **Training-free adaptation**: Model components can be recombined without fine-tuning; needed to make the method practical for low-resource settings, quick check: ensure no performance degradation from lack of fine-tuning
- **Task vector composition**: Specialized knowledge can be captured and transferred through vector representations; needed for systematic swapping, quick check: verify task vector accuracy in identifying specialized components
- **Adaptive selection vs blending**: Both approaches for combining specialized modules; needed to optimize for different scenarios, quick check: compare performance of selection vs blending strategies

## Architecture Onboarding
- **Component map**: English safety expert -> Task vector computation -> Layer/module specialization analysis -> Adaptive selection/blending -> Low-resource language model
- **Critical path**: The core workflow involves computing task vectors for both the safety expert and target model, analyzing specialization degrees, then applying either selection or blending of specialized modules to create the final model
- **Design tradeoffs**: Training-free approach sacrifices some potential performance gains from fine-tuning but enables practical deployment in low-resource settings; selection strategy is simpler but blending may capture more nuanced knowledge
- **Failure signatures**: If cross-lingual transfer fails, safety performance will degrade rather than improve; if module identification is inaccurate, the wrong components may be swapped leading to capability loss
- **First experiments**: 1) Verify task vector computation correctly identifies specialized layers in English model, 2) Test layer-wise swapping on a single low-resource language before module-wise extension, 3) Compare selection vs blending strategies on a small benchmark

## Open Questions the Paper Calls Out
None

## Limitations
- Tested only on four low-resource languages, limiting generalization claims
- Relies on assumption that English safety knowledge transfers effectively to other languages
- Safety improvements measured primarily against single baseline (GPT-4o) and limited datasets

## Confidence
- Safety performance improvements: Medium
- General capability preservation: Medium
- Cross-lingual generalization: Low

## Next Checks
1. Test the layer-wise swapping approach on additional low-resource languages not included in the original study, particularly those with distinct linguistic features from the four tested languages
2. Evaluate safety performance using diverse adversarial testing methods and safety scenarios beyond the MultiJail benchmark to assess robustness of the safety alignment transfer
3. Conduct human evaluations to verify that the automated safety metrics accurately capture real-world safety concerns across different cultural and linguistic contexts