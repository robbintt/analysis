---
ver: rpa2
title: 'EXPERT: An Explainable Image Captioning Evaluation Metric with Structured
  Explanations'
arxiv_id: '2506.24016'
source_url: https://arxiv.org/abs/2506.24016
tags:
- evaluation
- score
- image
- human
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EXPERT, a reference-free image captioning
  evaluation metric that provides structured, criterion-specific explanations for
  scores based on fluency, relevance, and descriptiveness. The authors construct two
  large-scale explanation datasets (Polaris-exp and Nebula-exp) by extending existing
  human judgment datasets, and train a vision-language model using a two-stage evaluation
  template.
---

# EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations

## Quick Facts
- arXiv ID: 2506.24016
- Source URL: https://arxiv.org/abs/2506.24016
- Reference count: 33
- Reference-free metric achieving SOTA correlation with human judgments while providing structured explanations

## Executive Summary
EXPERT is a reference-free image captioning evaluation metric that generates both scores and structured explanations across three criteria: fluency, relevance, and descriptiveness. The authors construct two large-scale explanation datasets (Polaris-exp and Nebula-exp) by extending existing human judgment datasets with GPT-4o-generated structured explanations, then fine-tune a vision-language model using a two-stage evaluation template. EXPERT achieves state-of-the-art correlation with human judgments across benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation.

## Method Summary
EXPERT uses a two-stage evaluation template where a vision-language model first generates a numerical score (0.0-1.0) using score binning with 0.10 granularity, then produces structured explanations for each of three criteria. The model is fine-tuned on explanation datasets created by extending Polaris and Nebula human judgment datasets with GPT-4o-generated structured explanations. Training uses LoRA (rank 128, alpha 256) on LLaVA-1.5-13B with specific hyperparameters for both scoring and explanation generation stages.

## Key Results
- Achieves SOTA performance on 5/6 benchmark datasets among reference-free metrics
- Significantly outperforms existing explainable metrics in human evaluation of explanation quality (consistency: 3.72/4, factuality: 3.84/4, informativeness: 3.72/4)
- Outperforms reference-based metrics on 4 datasets while maintaining competitive performance on the remaining two
- Score binning with 0.10 granularity consistently improves correlation performance across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Two-stage Evaluation Template Decomposition
Separating scoring and explanation generation into sequential stages improves both scoring accuracy and explanation quality compared to unified generation. Stage 1 isolates numerical judgment learning with score binning to reduce tokenization complexity. Stage 2 conditions on the score context, generating criterion-specific justifications. This decomposition allows the model to first establish a judgment anchor before rationalizing it.

### Mechanism 2: Structured Criteria as Evaluation Scaffolds
Enforcing three standardized criteria (fluency, relevance, descriptiveness) improves explanation consistency and alignment with human judgment patterns. The predefined criteria act as attention guides, forcing the model to systematically evaluate distinct quality dimensions rather than generating free-form justifications. Each criterion has explicit definitions embedded in the prompt template, reducing ambiguity.

### Mechanism 3: Supervised Fine-tuning Transfer from High-Quality Explanations
SFT on curated explanation datasets significantly improves both scoring correlation and explanation quality compared to zero-shot prompting. GPT-4o generates structured explanations using carefully designed prompts (with anti-overcriticism specifications for descriptiveness). These become training targets. SFT with LoRA (rank 128) adapts LLaVA-1.5 to internalize evaluation reasoning patterns, not just output format.

## Foundational Learning

- **Vision-Language Model Architecture (LLaVA-style)**
  - Why needed here: EXPERT builds on LLaVA-1.5, which uses a vision encoder + projection layer + LLM decoder. Understanding this architecture clarifies why score smoothing works and why LoRA adaptation is appropriate.
  - Quick check question: Can you explain why VLMs output numerical tokens discretely rather than as continuous values, and how this motivates score binning?

- **Kendall Correlation Coefficients (τb, τc)**
  - Why needed here: Evaluation uses different correlation metrics for different datasets (τb for Flickr8k-CF, τc for others). Understanding when to use tied vs. untied variants matters for proper benchmarking.
  - Quick check question: Why would Kendall-τc be preferred over Spearman correlation for caption evaluation datasets with many tied scores?

- **Score Smoothing via Token Probability Marginalization**
  - Why needed here: The score smoothing formula extracts continuous scores from discrete token probabilities, enabling finer-grained evaluation than single-token generation.
  - Quick check question: Given the formula s = Σ(10^-j × Σ(i × p(i,j))), how does this differ from simple greedy decoding, and why does GPT-4o's tokenizer design undermine this approach?

## Architecture Onboarding

- **Component map:**
  Input: (image, caption) → Scoring Stage Query → [LLaVA-1.5 + LoRA adapter] → Score token probabilities → Score smoothing → Numerical score (0-1) → Explanation Stage Query (with score context) → Structured explanation output

- **Critical path:**
  1. Dataset quality hinges on GPT-4o prompt design (Appendix A shows anti-overcriticism specification is essential)
  2. Score binning (0.10 granularity) directly affects Stage 1 learning efficiency
  3. LoRA rank (128) and alpha (256) control adaptation capacity—insufficient capacity breaks both scoring and explanation quality

- **Design tradeoffs:**
  - Inference time vs. detail: Full EXPERT takes 3.80s vs. 0.36s for score-only (Table 8). For high-throughput evaluation, skip explanation generation.
  - Reference-free vs. reference-based: EXPERT outperforms reference-based metrics on 4 datasets (Table 2) but sacrifices Pascal-50S accuracy. Reference-free is better when human references are unavailable or low-quality.
  - Explanation length vs. informativeness: Longer explanations improve human evaluation scores but increase inference latency. The paper doesn't ablate this directly.

- **Failure signatures:**
  - Overpenalization of concise captions: 45% of errors involve excessive penalization of accurate but detail-sparse captions (Table 9). Training data likely over-represents detailed captions.
  - Small-region factual errors: Explanations sometimes mischaracterize visually minor elements mentioned in captions, degrading informativeness scores.
  - Tokenizer mismatch with score smoothing: When applied to models with different tokenizers (e.g., GPT-4o), score smoothing degrades or fails due to multi-digit token merging.

- **First 3 experiments:**
  1. Reproduce Stage 1 scoring only: Train with scoring template only (no explanation stage) on Polaris-exp. Compare τc correlation on Flickr8k-EX against full two-stage model to isolate explanation stage contribution to scoring accuracy.
  2. Ablate criterion definitions: Remove explicit criterion descriptions from explanation stage query, keeping only criterion names. Evaluate on the 100-sample human evaluation set to measure consistency/factuality degradation.
  3. Test domain transfer: Evaluate EXPERT on out-of-domain caption datasets (e.g., medical images, screen captures) without retraining. Compare against zero-shot LLaVA-1.5 with the same template to assess SFT generalization vs. template-only benefits.

## Open Questions the Paper Calls Out

- Can oversampling concise captions during training reduce EXPERT's bias toward overpenalizing captions that lack sufficient details?
- How can the inference time of explainable evaluation metrics be reduced while maintaining both scoring accuracy and explanation quality?
- Can incorporating explicit region-level visual features improve EXPERT's ability to detect fine-grained details and reduce factual inaccuracies in explanations?

## Limitations

- EXPERT's performance critically depends on the quality of GPT-4o-generated explanations used for training, with potential systematic biases not fully characterized
- The metric shows strong performance on general image captioning datasets but lacks systematic evaluation on specialized domains (medical imaging, technical diagrams)
- Computational cost is significant, with 3.80s inference time and substantial fine-tuning resources required

## Confidence

**High Confidence**: The experimental methodology is rigorous with proper ablation studies, comprehensive benchmarking across 6 datasets, and detailed human evaluation. The correlation improvements over existing metrics are statistically significant and well-documented.

**Medium Confidence**: The two-stage template decomposition mechanism shows consistent improvements, but the exact contribution of each stage to scoring accuracy versus explanation quality remains partially disentangled.

**Low Confidence**: Claims about GPT-4o explanation quality serving as ideal training targets assume no systematic biases in the generated data. The anti-overcriticism specification helps but doesn't guarantee bias-free outputs.

## Next Checks

1. **Bias Analysis of Training Data**: Conduct a systematic error analysis of GPT-4o-generated explanations to identify potential biases in fluency, relevance, and descriptiveness judgments. Compare distributions against human judgments in the original datasets.

2. **Cross-Domain Robustness Testing**: Evaluate EXPERT on specialized captioning datasets (medical reports, technical documentation, creative writing) without retraining to assess domain transfer limitations and identify where the three criteria break down.

3. **Efficiency-Ablation Tradeoff**: Systematically vary LoRA rank (64, 128, 256) and training epochs (1, 2, 3) to identify the minimal configuration that maintains 95% of peak correlation performance, reducing computational overhead while preserving quality.