---
ver: rpa2
title: Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy
  Learning
arxiv_id: '2505.19054'
source_url: https://arxiv.org/abs/2505.19054
tags:
- learning
- policy
- function
- neural
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RANDPOL, an actor-critic algorithm that uses
  randomized neural networks to significantly reduce computational costs in deep reinforcement
  learning. The key innovation is that only the final layer of the networks is trained,
  while all other layers remain randomly initialized.
---

# Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning

## Quick Facts
- arXiv ID: 2505.19054
- Source URL: https://arxiv.org/abs/2505.19054
- Reference count: 10
- Key outcome: RANDPOL achieves PPO-level performance with 2-3x faster wall-clock training by randomizing all network layers except the final one

## Executive Summary
This paper introduces RANDPOL, an actor-critic algorithm that significantly reduces computational costs in deep reinforcement learning by randomizing all neural network layers except the final output layer. The key insight is that only the final layer needs training, while hidden layers remain fixed with random weights drawn from a specific distribution. This approach leverages prior work showing that randomized neural networks can effectively approximate complex functions while requiring far fewer trainable parameters. The algorithm was evaluated on OpenAI Gym environments including LunarLander, BipedalWalker, Hopper, and Humanoid, as well as on a challenging quadruped robot locomotion task using a Unitree Go1 model. Results show that RANDPOL achieves performance comparable to leading algorithms like PPO, A2C, and TRPO, but with substantially faster wall-clock training times—despite requiring more timesteps to converge.

## Method Summary
RANDPOL is an actor-critic algorithm where both the actor and critic are represented by multi-layer perceptrons with randomly initialized weights that remain fixed throughout training. Only the final layer weights are trained, transforming the learning problem into optimizing a linear combination of random features. The critic is trained by solving a convex regression problem with L∞ regularization to bound weight magnitudes, while the actor is updated via gradient ascent on the policy objective. This architecture eliminates backpropagation through hidden layers, dramatically reducing computational cost per update. The algorithm uses Adam optimizer with learning rate 3e-4, batch size 128, and discount factor 0.99, with network architecture [500, 800] hidden units.

## Key Results
- RANDPOL achieves PPO-level performance on LunarLander, BipedalWalker, Hopper, and Humanoid environments
- Wall-clock training time is 2-3x faster than PPO, A2C, and TRPO despite requiring more timesteps to converge
- On the quadruped robot locomotion task with Unitree Go1 model, RANDPOL outperforms off-policy methods DDPG, SAC, and TD3 in both cumulative reward and sample efficiency
- The algorithm demonstrates superior frames-per-second processing while maintaining competitive sample efficiency

## Why This Works (Mechanism)

### Mechanism 1: Random Feature Approximation of Value/Policy Functions
Randomly initialized hidden layers provide sufficient basis functions for approximating optimal policies and value functions in continuous control tasks. Hidden layer weights θ are drawn from a distribution ν(θ) and fixed, transforming learning into finding a linear combination αⱼϕ(x; θⱼ) of random basis functions. This leverages Rahimi & Recht (2008a) showing random features can approximate functions in Reproducing Kernel Hilbert Spaces with bounded error O(1/√K).

### Mechanism 2: Computational Savings via Eliminated Backpropagation
Wall-clock training time decreases substantially because forward passes through fixed random layers are cheaper than full backpropagation. Standard DRL requires computing gradients through all layers per update, while RANDPOL only optimizes α via convex loss or gradient ascent on policy objective.

### Mechanism 3: Convexification of Critic Learning
Fixing hidden layers converts critic training from non-convex to convex optimization, improving stability. With θ fixed, critic training becomes a linear regression problem with unique global optimum, avoiding local minima issues in full backpropagation.

## Foundational Learning

- **Concept: Actor-Critic Architecture**
  - Why needed: RANDPOL builds on standard actor-critic where actor learns π(a|s) and critic learns V(s)
  - Quick check: Can you explain why the critic reduces variance in policy gradient estimates?

- **Concept: Policy Gradient Theorem**
  - Why needed: The actor update derives from the policy gradient theorem, replacing full network gradients with gradients w.r.t. only α′
  - Quick check: What does ∇_θ log π_θ(a|s) represent geometrically?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed: Theoretical guarantees for random feature approximation rely on target functions belonging to an RKHS
  - Quick check: Why does bounded error O(1/√K) matter for choosing network width?

## Architecture Onboarding

- **Component map:**
  Environment → Trajectory Sampling → (s, a, r, s') tuples → Critic: s → [Fixed θ: L1, L2...] → ϕ(s; θ) → α (trainable) → V(s) → Actor: s → [Fixed θ: L1, L2...] → ψ(s; θ) → α′ (trainable) → π(a|s)

- **Critical path:**
  1. Initialize random networks: draw θ from ν(θ), initialize α, α′ small
  2. Collect trajectory of length T using current policy
  3. Compute returns R(x_t) and advantages A(x_t) using critic
  4. Update critic: solve convex regression with L∞ regularization
  5. Update actor: gradient step on policy objective
  6. Repeat until convergence

- **Design tradeoffs:**
  - Hidden layer size vs approximation quality: Larger J_Q, J_π → better approximation but more memory/computation
  - L∞ regularization strength (h): Controls weight boundedness |α_j| ≤ C/J
  - Trajectory length T: Must satisfy T ≫ N_Q, N_π for unbiased advantage estimates

- **Failure signatures:**
  - Slow convergence or instability: Check if L∞ regularization h is appropriate for reward scale
  - Good wall-time but poor final performance: Random features may not span required function class
  - High variance across seeds: Actor initialization or random seed for θ may cause feature quality variance

- **First 3 experiments:**
  1. Validate on LunarLander: Compare RANDPOL vs PPO with matched compute budget
  2. Ablate hidden layer width: Test J_π ∈ {100, 500, 1000} on BipedalWalker
  3. Stress test on sparse reward task: Run on modified environment with sparse rewards

## Open Questions the Paper Calls Out

### Open Question 1
Can the RANDPOL framework be effectively extended to Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)? The authors identify this as future work, noting the current limitation to MLP architectures and the potential for randomized convolutional/recurrent networks in other domains.

### Open Question 2
Can the sample efficiency of RANDPOL be improved to match or exceed baseline algorithms without sacrificing its wall-clock speed advantage? The authors concede that their algorithm "does not outperform other algorithms in terms of sample efficiency" and generally requires more timesteps to converge.

### Open Question 3
Is the empirical performance of RANDPOL robust across a wider variety of high-dimensional discrete action spaces? The paper explicitly restricts its problem formulation and experiments to "continuous MDP problems" and continuous action spaces.

## Limitations

- Limited to continuous action spaces; extension to discrete actions remains unexplored
- Requires more timesteps to converge than baseline algorithms, trading sample efficiency for wall-clock speed
- Theoretical guarantees apply to shallow networks, but paper uses multi-layer randomized networks without established error bounds

## Confidence

- **High confidence:** Computational savings mechanism and convexification of critic learning
- **Medium confidence:** Comparable performance to PPO/A2C/TRPO on tested environments
- **Low confidence:** Generalization to tasks requiring hierarchical abstraction or long-term credit assignment

## Next Checks

1. **Stress test on sparse reward tasks:** Evaluate RANDPOL on modified LunarLander with sparse rewards and compare wall-clock efficiency and final performance against PPO
2. **Compute-optimal width analysis:** Systematically vary J_Q and J_π on BipedalWalker and plot final return vs wall-clock time to identify diminishing returns
3. **Transfer to manipulation tasks:** Test RANDPOL on standard manipulation benchmark (e.g., Fetch environments) with varying reward densities to validate generalization beyond locomotion