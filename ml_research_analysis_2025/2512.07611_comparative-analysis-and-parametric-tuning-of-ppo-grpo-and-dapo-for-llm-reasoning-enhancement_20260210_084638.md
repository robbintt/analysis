---
ver: rpa2
title: Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning
  Enhancement
arxiv_id: '2512.07611'
source_url: https://arxiv.org/abs/2512.07611
tags:
- policy
- grpo
- dapo
- training
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically compares three reinforcement learning
  algorithms (PPO, GRPO, DAPO) for enhancing large language model reasoning capabilities.
  The key findings are: PPO with entropy bonus shows lower accuracy despite encouraging
  exploration; larger group sizes (G=8 vs G=2) in GRPO and DAPO lead to more stable
  training and higher accuracy; DAPO''s token-level loss aggregation produces longer,
  more comprehensive responses compared to GRPO''s sample-level approach; dynamic
  sampling in DAPO does not improve model accuracy while adding computational overhead;
  and all RL-trained models outperform the base model across benchmarks including
  GSM8K, MATH, BBH, and MMLU-Pro, with DAPO (without dynamic sampling) achieving the
  highest accuracy at 53.3% on GSM8K and 30.0% on MMLU-Pro.'
---

# Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement

## Quick Facts
- arXiv ID: 2512.07611
- Source URL: https://arxiv.org/abs/2512.07611
- Authors: Yongsheng Lian
- Reference count: 35
- Primary result: DAPO without dynamic sampling achieved highest accuracy at 53.3% on GSM8K and 30.0% on MMLU-Pro

## Executive Summary
This study systematically compares three reinforcement learning algorithms (PPO, GRPO, DAPO) for enhancing large language model reasoning capabilities. The key findings are: PPO with entropy bonus shows lower accuracy despite encouraging exploration; larger group sizes (G=8 vs G=2) in GRPO and DAPO lead to more stable training and higher accuracy; DAPO's token-level loss aggregation produces longer, more comprehensive responses compared to GRPO's sample-level approach; dynamic sampling in DAPO does not improve model accuracy while adding computational overhead; and all RL-trained models outperform the base model across benchmarks including GSM8K, MATH, BBH, and MMLU-Pro, with DAPO (without dynamic sampling) achieving the highest accuracy at 53.3% on GSM8K and 30.0% on MMLU-Pro.

## Method Summary
The study fine-tunes Qwen2.5-1.5B-Instruct using three RL algorithms on the Countdown Game dataset, then evaluates on GSM8K, MATH, BBH, and MMLU-Pro. PPO uses per-token KL penalty with optional entropy bonus. GRPO estimates advantages relative to peer generations without a value function, using explicit KL penalty in the objective. DAPO shifts to token-level loss aggregation, uses asymmetric clipping, and optionally employs dynamic sampling. Key hyperparameters include learning rates of 1e-6 to 3e-6, group sizes of 2-8 (G=8 optimal), and KL coefficients of 0.005-0.04 (β≈0.0075-0.01 optimal). Training is conducted via the Verl framework with evaluation using lm-eval-harness at max_tokens=2048.

## Key Results
- PPO with entropy bonus shows lower accuracy despite encouraging exploration
- Larger group sizes (G=8 vs G=2) in GRPO and DAPO lead to more stable training and higher accuracy
- DAPO's token-level loss aggregation produces longer, more comprehensive responses compared to GRPO's sample-level approach
- Dynamic sampling in DAPO does not improve model accuracy while adding computational overhead
- All RL-trained models outperform the base model across benchmarks, with DAPO (without dynamic sampling) achieving the highest accuracy at 53.3% on GSM8K and 30.0% on MMLU-Pro

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage Estimation
GRPO and DAPO replace the learned critic (value function) with group-relative normalization, reducing variance and memory overhead while maintaining policy gradient signals. The advantage $\hat{A}_{i,t}$ is calculated by normalizing the reward of a single output against the mean and standard deviation of a group of $G$ outputs generated for the same prompt. This transforms absolute rewards into relative performance metrics. The variance within a group of sampled responses provides a sufficient baseline for estimating advantage, assuming the group size $G$ is large enough to provide statistical significance.

### Mechanism 2: Token-Level Loss Aggregation
DAPO aggregates loss at the token level rather than the sample level to mitigate "reward hacking" where models generate overly short responses to minimize penalty. GRPO normalizes by sequence length ($1/|o_i|$), penalizing long outputs. DAPO normalizes by the total token count across the group ($1/\sum|o_i|$), weighting each token equally. This encourages the model to produce longer reasoning chains required for complex tasks, based on the assumption that longer reasoning chains correlate with higher accuracy for the specific reasoning tasks evaluated.

### Mechanism 3: Stability via Group Size and KL Tuning
Training stability is achieved by increasing group size $G$ and non-monotonic tuning of the KL penalty coefficient $\beta$. A larger $G$ reduces variance in advantage estimation. A moderate $\beta$ constrains policy drift, preventing the model from deviating too far from the reference model, whereas too high $\beta$ stifles learning. The "trust region" for the policy update can be effectively managed through these hyperparameters without strictly enforcing a hard constraint.

## Foundational Learning

- **Concept: Actor-Critic Architecture**
  - Why needed here: PPO uses a Critic (Value Function) to estimate advantages. GRPO/DAPO eliminate this. Understanding what is being removed is necessary to understand the computational efficiency gains described in the paper.
  - Quick check question: Can you explain how GRPO calculates the baseline for advantage estimation if it has no Critic?

- **Concept: KL Divergence**
  - Why needed here: The paper highlights the KL penalty coefficient as a critical tuning knob. Understanding KL divergence as a measure of distribution shift is required to interpret the "non-monotonic" behavior observed in Section 3.3.
  - Quick check question: What happens to the policy if the KL penalty coefficient is set too high versus too low?

- **Concept: Reward Hacking**
  - Why needed here: The paper identifies sample-level loss aggregation as a cause for short, lazy responses.
  - Quick check question: Why does normalizing loss by response length ($1/|o_i|$) encourage the model to generate shorter outputs?

## Architecture Onboarding

- **Component map:** Base Model (Qwen2.5-1.5B-Instruct, Policy $\pi_\theta$) -> Reference Model (Frozen copy, $\pi_{ref}$) -> Sampler (Generates $G$ outputs per prompt) -> Reward Engine (Countdown Game verifier) -> Optimizer (Updates $\pi_\theta$ using PPO/GRPO/DAPO loss calculations)

- **Critical path:** The efficiency gain in GRPO/DAPO comes from skipping the Critic training step. The data flow is: Prompt → Sample Group → Calculate Rewards → Normalize (Group-Relative) → Compute Loss → Update.

- **Design tradeoffs:**
  - Group Size ($G$): Higher $G$ (8 vs 2) improves stability/accuracy but linearly increases inference cost during training
  - Dynamic Sampling: The paper found this DAPO feature increased compute by 25% without accuracy gains; the recommended configuration disables it
  - Clipping: DAPO uses asymmetric clipping ($\epsilon_{low} \neq \epsilon_{high}$) to encourage exploration

- **Failure signatures:**
  - Entropy Collapse: Gradients vanish if all samples in a group yield identical rewards (mitigated by $G \ge 4$)
  - Short Responses: Model produces minimal output (indicates sample-level loss bias or low $\beta$)
  - Training Instability: Spikes in KL divergence or loss (indicates learning rate too high or $G$ too small)

- **First 3 experiments:**
  1. Group Size Ablation: Train GRPO with $G=2$ vs $G=4$ vs $G=8$ on a small subset of data to confirm stability improvements before scaling
  2. KL Sensitivity Scan: Run a parameter sweep on $\beta$ (e.g., $0.0$ to $0.04$) to find the "sweet spot" where validation accuracy peaks before dropping
  3. Loss Aggregation Comparison: Directly compare GRPO (sample-level) vs. DAPO (token-level) loss on response length and reward hacking metrics to verify if token-level is necessary for the specific target domain

## Open Questions the Paper Calls Out

### Open Question 1
Can the Dynamic Sampling strategy be modified to align its surrogate objective improvement with actual task-level accuracy gains? The mechanism causes a misalignment where the optimization landscape improves, but the policy degrades because high-quality samples are discarded, leaving suboptimal responses to define the advantage. An ablation study showing that a modified sampling strategy (e.g., one that preserves high-confidence samples or re-weights advantages) results in both a higher surrogate objective and improved benchmark accuracy would resolve this.

### Open Question 2
Does the marginal performance gain on general benchmarks stem from the narrowness of the Countdown Game's reward signals? The study cannot distinguish whether the performance ceiling on general reasoning tasks is caused by the specialized nature of the Countdown training data or the limited capacity of the 1.5B parameter model. A comparison of model performance when trained on the Countdown Game versus a diverse, multi-domain reasoning dataset, evaluated specifically on MATH and BBH, would resolve this.

### Open Question 3
How does the "Clip-higher" strategy interact with model scale to influence the exploration of connective reasoning tokens? The current experiments do not isolate the asymmetric clipping parameter $\epsilon_{high}$ across varying model sizes, leaving its contribution to the stability-accuracy trade-off unclear. A parameter sweep of $\epsilon_{high}$ on models of varying sizes (e.g., 1.5B vs 7B) to measure the impact on the frequency of connective tokens and final accuracy would resolve this.

## Limitations
- The claimed stability improvements from larger group sizes are demonstrated only on the Countdown Game dataset, with limited cross-task validation
- The entropy bonus's negative impact on accuracy is reported but lacks ablation studies isolating entropy coefficient effects
- The superiority of DAPO over GRPO is established primarily through response length metrics rather than direct accuracy gains on complex reasoning tasks

## Confidence

- **High confidence:** Group size G=8 provides more stable training than G=2 (supported by direct experimental comparison)
- **Medium confidence:** Token-level loss aggregation in DAPO produces longer responses (directly measured but correlation with reasoning quality not fully established)
- **Medium confidence:** Dynamic sampling adds computational overhead without accuracy gains (observed but mechanism unclear)
- **Medium confidence:** KL penalty coefficient tuning is non-monotonic (consistent with RL theory but specific optimal range may be dataset-dependent)

## Next Checks
1. Test group size effects on a broader range of reasoning tasks beyond Countdown Game to verify generalizability
2. Conduct controlled ablation studies on entropy coefficient and dynamic sampling to isolate their individual contributions
3. Measure actual reasoning quality (not just response length) when comparing sample-level vs token-level loss aggregation