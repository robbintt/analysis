---
ver: rpa2
title: 'MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation'
arxiv_id: '2508.01005'
source_url: https://arxiv.org/abs/2508.01005
tags:
- question
- answer
- sub-question
- arxiv
- planner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MAO-ARAG introduces a multi-agent framework that dynamically selects\
  \ and orchestrates different RAG modules\u2014such as query reformulation, retrieval,\
  \ and document selection\u2014to tailor workflows for each query. A planner agent,\
  \ trained via reinforcement learning with a reward balancing F1 score and cost penalties,\
  \ constructs these adaptive pipelines."
---

# MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.01005
- Source URL: https://arxiv.org/abs/2508.01005
- Reference count: 14
- Key outcome: MAO-ARAG achieves 52.91% average F1 score, outperforming best baseline by 3.08 points

## Executive Summary
MAO-ARAG introduces a multi-agent framework that dynamically selects and orchestrates different RAG modules—such as query reformulation, retrieval, and document selection—to tailor workflows for each query. A planner agent, trained via reinforcement learning with a reward balancing F1 score and cost penalties, constructs these adaptive pipelines. Experiments on multiple QA datasets show MAO-ARAG achieves the highest average F1 score (52.91%), outperforming the best baseline by 3.08 points while maintaining reasonable token costs and latency. The framework demonstrates effective balance between answer quality and computational cost.

## Method Summary
MAO-ARAG uses a planner agent (LLM) that dynamically selects and orchestrates executor agents into query-specific workflows. The system is modeled as a Semi-Markov Decision Process where the planner chooses from actions like query decomposition, retrieval, and answer generation. Training uses Proximal Policy Optimization with a composite reward balancing F1 score against cost penalties for tokens, latency, and retrieval calls. The planner can be a 7B model or smaller distilled versions, while executors use GPT-4o-Mini. Workflows are executed recursively when decomposition agents are selected.

## Key Results
- MAO-ARAG achieves 52.91% average F1 score across 7 benchmarks
- Outperforms best baseline by 3.08 F1 points
- Demonstrates effective cost-performance trade-off through α hyperparameter tuning
- 0.5B distilled planner closely matches 7B planner performance

## Why This Works (Mechanism)

### Mechanism 1: Query-Aware Workflow Selection
The system improves performance by mapping query complexity to specific workflow configurations, avoiding the inefficiency of applying complex reasoning to simple queries. A Planner agent (LLM) processes the input question and selects a subset of executor modules from an action space, transforming a static pipeline into a dynamic, query-specific graph. The semantic representation of the query contains sufficient signal for the planner to distinguish between "simple" (lookup) and "complex" (multi-hop reasoning) needs without executing the full pipeline first.

### Mechanism 2: Cost-Regularized Reinforcement Learning (PPO)
MAO-ARAG balances effectiveness and efficiency by penalizing resource consumption directly within the policy optimization loop, rather than treating cost as a post-hoc constraint. The PPO algorithm optimizes the Planner's policy using a composite reward: R = F1 − α·CostPenalty − FormatPenalty. This shapes the policy to maximize F1 score while minimizing token usage, latency, and search API calls. The hyperparameter α can be universally scaled or tuned to reflect the monetary value of a unit of F1 performance versus compute cost.

### Mechanism 3: Decoupled Planner-Executor Architecture
Separating the "Planner" (orchestrator) from the "Executors" (workers) allows for efficient model distillation, enabling small models to orchestrate larger, proprietary models. The framework treats RAG as a Multiagent Semi-Markov Decision Process. A 7B parameter model generates a plan, which is executed by separate agents. This decoupling allows the execution to happen on powerful backbones while the orchestration logic remains lightweight. The context passed between agents retains sufficient information to resolve the initial query without requiring the Planner to read all retrieved documents.

## Foundational Learning

- **Concept: Semi-Markov Decision Process (SMDP)**
  - Why needed here: Standard MDPs assume fixed time steps. RAG workflows are hierarchical and take variable time. SMDPs model these variable durations.
  - Quick check question: How does the system calculate the "duration" T for a workflow that involves parallel sub-queries?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: This is the specific RL algorithm used to train the Planner. Understanding the "clipping" mechanism is crucial to diagnosing training instability or slow convergence.
  - Quick check question: In Equation (8), what is the role of the `clip` function in preventing the policy π_θ from changing too drastically in a single update?

- **Concept: F1 Score vs. Cost Trade-off**
  - Why needed here: The core innovation is the composite reward. Understanding the inverse relationship between accuracy (F1) and efficiency (Cost) is vital for setting the α hyperparameter.
  - Quick check question: If you increase the cost penalty weight α from 0.1 to 0.25, based on Figure 4, should you expect the F1 score to increase or decrease?

## Architecture Onboarding

- **Component map:**
  - Planner: Qwen2.5-7B-Instruct (or distilled 1.5B/0.5B). Inputs Prompt + Question; Outputs Workflow.
  - Executors: QDS (Query Decomposition Serial), QDP (Query Decomposition Parallel), QR (Query Rewriter), RA (Retrieval Agent / Search Engine), DS (Document Selector), AG (Answer Generator), AS (Answer Summarization).
  - Environment: Wikipedia Corpus + E5 Retriever.

- **Critical path:**
  1. Input: User Query q.
  2. Planning: Planner generates a workflow (e.g., QR → RA → AG).
  3. Execution: The system executes modules sequentially.
  4. Recursion: If QDP or QDS is called, new sub-queries are fed back to step 2.
  5. Reward: After max turns or completion, calculate F1 against Ground Truth and Cost based on tokens/turns.

- **Design tradeoffs:**
  - α Setting: Low α = Higher F1, Higher Cost. High α = Lower F1, Lower Cost. (Page 6 shows the curve inflection point is around 0.2).
  - Model Size: A 0.5B planner is cheaper but requires distillation from a 7B teacher.
  - Executor Backbone: Swapping GPT-4o-mini for GPT-3.5-turbo lowers cost but also lowers F1 (Table 2 shows a drop from ~54 to ~48).

- **Failure signatures:**
  - Format Penalty Loops: The planner outputs invalid workflow syntax, triggering R_FP = 1 repeatedly.
  - Runaway Costs: A low α causes the planner to decompose simple questions into 4+ sub-queries unnecessarily.
  - Context Drift: In Serial Decomposition (QDS), if the first sub-answer is wrong, all subsequent dependent sub-questions fail.

- **First 3 experiments:**
  1. Sanity Check: Run MAO-ARAG w/o train (untrained planner) on a small subset of NQ to verify the executor agents function correctly and the code compiles.
  2. Hyperparameter Sweep: Train the planner with α ∈ {0.0, 0.1, 0.2} and plot F1 vs. Token Cost to visualize the Pareto frontier.
  3. Ablation: Replace the Executor backbone (GPT-4o-mini) with a smaller model (e.g., GPT-3.5) to measure the sensitivity of the final F1 score to executor quality versus planner quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the cost penalty formulation be refined to more accurately weight token cost, turn cost, and retrieval call cost relative to each other and to the F1 reward?
- Basis in paper: The conclusion states: "Future work may focus on refining cost penalties to better balance performance and cost." Additionally, Section 4.4 notes that the coarse scaling of RCP components to [0,1] could cause fluctuations, "indicating a potential area for future refinement."
- Why unresolved: The current approach normalizes all three cost terms equally, but real-world scenarios have different monetary weights for each cost type.
- What evidence would resolve it: Experiments comparing learned weightings or per-dollar conversion rates against the uniform scaling baseline, showing reduced fluctuation in performance-cost curves.

### Open Question 2
- Question: Can multiple LLM APIs be dynamically selected as executor backbones within the same workflow to optimize cost-effectiveness per sub-task?
- Basis in paper: The conclusion states: "We also plan to use multiple optional APIs simultaneously as executor backbones, aiming for better results at a lower cost."
- Why unresolved: The current framework uses a single backbone (GPT-4o-mini) for all executors, and the planner only selects workflow structure, not backbone assignments.
- What evidence would resolve it: Extending the planner's action space to include backbone selection, then demonstrating improved F1/cost ratios compared to fixed-backbone configurations.

### Open Question 3
- Question: What minimum instruction-following and initial planning capabilities must a smaller model possess to support direct RL-based planner training without requiring distillation from a larger model?
- Basis in paper: Appendix D states: "it was not possible to train directly based on Qwen2.5-0.5B-Instruct and Qwen2.5-1.5B-Instruct because models of this size have issues with instruction-following capabilities."
- Why unresolved: Smaller models required SFT distillation from the 7B planner before PPO training could succeed; the threshold capability level is unknown.
- What evidence would resolve it: Systematic evaluation of various sub-7B models' instruction-following scores and initial workflow validity rates, correlating these with successful PPO convergence.

### Open Question 4
- Question: Can the cost-performance trade-off hyperparameter α be adaptively selected per-query rather than fixed globally?
- Basis in paper: Section 4.4 shows that α values above 0.2 cause rapid performance decline, and the curves exhibit fluctuations. The fixed α requires manual tuning and cannot adapt to query-specific complexity-cost trade-offs.
- Why unresolved: The current design uses a single global α for all queries, but optimal trade-offs likely vary by query difficulty and type.
- What evidence would resolve it: Implementing a meta-controller or conditioning the planner on α, then comparing per-query adaptive α against fixed α baselines on diverse benchmarks.

## Limitations

- The paper does not specify critical PPO hyperparameters (learning rate, batch size, epochs, discount factors, clipping epsilon, KL penalty), normalization constants for cost metrics, or training duration/stopping criteria.
- The claim that the planner can accurately distinguish query complexity from semantic representation alone is questionable for niche domains where the base LLM may lack sufficient knowledge.
- The system's performance appears sensitive to the α hyperparameter, with rapid F1 degradation when α exceeds 0.2, suggesting potential instability in the cost-accuracy balance.

## Confidence

- **High Confidence:** The experimental results showing MAO-ARAG outperforming baselines by 3.08 F1 points on average, and the documented trade-off between α and performance-cost balance.
- **Medium Confidence:** The mechanism claims about query-aware workflow selection and decoupled architecture, though supported by the text, rely on assumptions about the planner's ability to estimate query difficulty and handle conditional logic.
- **Low Confidence:** The generalizability of the cost-regularized RL approach across different domains and the robustness of the format validation mechanism to prevent planner output errors.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary α across [0.0, 0.1, 0.2, 0.25, 0.3] and plot F1 vs. Token Cost to confirm the claimed inflection point and validate the trade-off curve.
2. **Distilled Model Evaluation:** Compare the performance of the 0.5B and 1.5B distilled planners against the 7B teacher on a held-out validation set to quantify knowledge transfer efficiency.
3. **Failure Mode Testing:** Intentionally corrupt planner outputs with invalid workflow syntax and measure the frequency of FormatPenalty=1 to assess the robustness of the format validation mechanism.