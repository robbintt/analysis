---
ver: rpa2
title: Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture
arxiv_id: '2510.13250'
source_url: https://arxiv.org/abs/2510.13250
tags:
- crowd
- network
- counting
- ieee
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of real-time crowd counting on
  embedded systems, where existing methods suffer from excessive model parameters
  and computational complexity. The authors propose a lightweight stem-encoder-decoder
  architecture designed for super real-time performance.
---

# Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture

## Quick Facts
- arXiv ID: 2510.13250
- Source URL: https://arxiv.org/abs/2510.13250
- Reference count: 40
- Achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1

## Executive Summary
This paper addresses the challenge of real-time crowd counting on embedded systems, where existing methods suffer from excessive model parameters and computational complexity. The authors propose a lightweight stem-encoder-decoder architecture designed for super real-time performance. The stem network uses large convolution kernels to expand the receptive field and extract detailed features. The encoder employs conditional channel weighting and multi-branch local fusion blocks to merge multi-scale features efficiently. The decoder uses feature pyramid networks to address incomplete fusion. The proposed network achieves state-of-the-art inference speed while maintaining competitive accuracy on three benchmark datasets.

## Method Summary
The method employs a stem-encoder-decoder architecture with large convolution kernels (9, 7, 5) in the stem to expand receptive field before downsampling. The encoder uses conditional channel weighting (CCW) and multi-branch local fusion (MLF) blocks to merge multi-scale features efficiently. The decoder employs a feature pyramid network (FPN) to integrate encoder outputs across scales. Training uses Adam optimizer with cosine annealing, MSE loss on density maps, and standard data augmentation. The model is optimized for extreme inference speed on embedded devices while maintaining competitive accuracy on UCF-QNRF, NWPU-Crowd, and ShanghaiTech datasets.

## Key Results
- Achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1
- Maintains competitive accuracy with 0.15M parameters and 1.32G FLOPs
- Demonstrates effective crowd counting on three benchmark datasets (UCF-QNRF, NWPU-Crowd, ShanghaiTech)
- Successfully balances speed and accuracy for embedded system deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large convolution kernels in the stem network may compensate for the loss of detailed spatial information caused by aggressive early downsampling.
- **Mechanism:** The architecture employs kernels of size 9, 7, and 5 immediately after input. This expands the receptive field to capture "detailed head information" before the resolution is reduced to 1/4, mitigating information loss where visual redundancy is high.
- **Core assumption:** Large kernels in early layers can extract sufficient context without the computational burden of deep $3\times3$ stacks or the "gridding artifacts" introduced by dilated convolutions.
- **Evidence anchors:**
  - [Abstract]: "large convolution kernels in the stem network are used to enlarge the receptive field, which effectively extracts detailed head information."
  - [Section 3.1]: "visual information in it is highly redundant in space... large convolution kernels... commendably expand the receptive field."
  - [Table 2]: Shows "Large kernels" outperforming "Dilated kernels" (104.1 MAE vs 110.1 MAE) and "Small kernels."
- **Break condition:** If input images require fine-grained texture discrimination (non-head features) at full resolution, this early compression could destroy necessary signals.

### Mechanism 2
- **Claim:** The combination of Conditional Channel Weighting (CCW) and Multi-branch Local Fusion (MLF) acts as a low-compute substitute for standard heavy convolution, enabling real-time speed.
- **Mechanism:** The encoder replaces standard convolutions with CCW (element-wise weighting) to mix information across resolutions cheaply. MLF aggregates these multi-scale features only at specific downsampling points, strictly limiting the computational graph.
- **Core assumption:** Element-wise channel weighting and local fusion provide sufficient feature expressivity for density estimation compared to full convolution or self-attention mechanisms (used in related Transformer works like RCCFormer/TCFormer).
- **Evidence anchors:**
  - [Section 3.2]: "convolution operation is replaced by the element-by-element weighting operation... to reduce complexity."
  - [Abstract]: "merges multi-scale features with low computational consumption. This part is crucial to the super real-time performance."
- **Break condition:** If complex background interference requires global semantic understanding (which Transformers handle well), this local processing might fail to suppress false positives.

### Mechanism 3
- **Claim:** The Feature Pyramid Network (FPN) decoder recovers global context lost by the encoder's strictly "local" fusion strategy.
- **Mechanism:** Since the MLF blocks in the encoder only fuse locally, the FPN is added to integrate encoder outputs across all scales via up-sampling and lateral connections, correcting "incomplete fusion."
- **Core assumption:** The encoder is too localized to produce a coherent density map on its own; a lightweight top-down decoder is necessary to re-integrate spatial hierarchy.
- **Evidence anchors:**
  - [Section 3.3]: "alleviates the problem of incomplete integration originating from the local fusion in the encoder."
  - [Figure 2]: Shows the FPN sitting atop the encoder stages to produce the final output.
- **Break condition:** If the encoder features are already too sparse or noisy, the FPN's linear fusion (sum/concatenation) may amplify noise rather than signal.

## Foundational Learning

- **Concept: Receptive Field vs. Parameter Efficiency**
  - **Why needed here:** The paper trades depth for width and kernel size. You must understand how a single $9\times9$ kernel covers a larger area than stacked $3\times3$s but with different parameter counts and non-linearity behaviors.
  - **Quick check question:** Why would the authors choose a $9\times9$ kernel over three stacked $3\times3$ kernels in the stem, considering both receptive field and non-linear activation functions?

- **Concept: Channel Weighting (SE-Net / Lite-HRNet logic)**
  - **Why needed here:** The CCW block is the core efficiency driver. Understanding how to compute weights globally and apply them locally (element-wise multiplication) is key to grasping the speedup.
  - **Quick check question:** How does replacing a standard $3\times3$ convolution with a Conditional Channel Weighting block reduce FLOPs while maintaining the ability to select features?

- **Concept: Density Map Regression**
  - **Why needed here:** The network outputs a density map, not a count.
  - **Quick check question:** If the network outputs a $128\times128$ density map where the sum of all pixels is 50.5, what is the predicted crowd count?

## Architecture Onboarding

- **Component map:** Input -> Stem (Large Conv k=9,7,5 -> ShuffleBlock) -> 1/4 Resolution -> Encoder (CCW + MLF) -> Multi-scale branches (1/4, 1/8, 1/16) -> Decoder (FPN) -> Density Map

- **Critical path:** The **Stem -> CCW** path is the most sensitive. The paper notes that while the Stem extracts features, the CCW is crucial for "super real-time performance" (Section 3.2). If latency budgets are tight, profile the CCW implementation first.

- **Design tradeoffs:** The authors explicitly sacrifice ~2-3 MAE (accuracy) for a 5x-10x speed gain over baselines like CSRNet or CACrowdGAN. They also argue against using LLMs or large Transformers (e.g., BLIP-2, LLaVA) for this task, citing that they cannot exceed 0.1 FPS on embedded targets (Section 2.1, Page 5). This is a **speed-first** architecture.

- **Failure signatures:**
  1. **Undercounting in ultra-dense regions:** The paper admits the model "occasionally underestimates counts in extremely dense regions" (Section 4.5).
  2. **Gridding Artifacts:** If you swap large kernels for dilated convolutions to save parameters, expect the "gridding artifacts" mentioned in the ablation study.

- **First 3 experiments:**
  1. **Kernel Ablation (Table 2):** Replace the Stem's large kernels (9,7,5) with standard (3,3,3) to verify the MAE/FPS trade-off on the validation set.
  2. **Hardware Profiling (Table 5):** Run inference on a Jetson TX1 (or equivalent edge device) with $576\times768$ input to confirm the < 15ms latency claim.
  3. **Module Isolation (Table 3):** Run the network with only the Stem vs. Stem+Encoder to observe the jump in MAE (from 237.1 to ~104), validating the necessity of the encoder blocks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Feature Pyramid Network (FPN) decoder be replaced by a more parameter-efficient module to enable deployment on devices with stricter memory constraints?
- **Basis in paper:** [explicit] The authors state: "In the future, we tend to squeeze the network to use a more lightweight decoder instead of FPN, so that they can be used on more marginalized devices."
- **Why unresolved:** While the FPN effectively addresses incomplete feature fusion, its current parameter count (0.085 MB) and computational overhead limit the model's applicability to extremely low-power hardware.
- **What evidence would resolve it:** A study demonstrating a modified architecture with a reduced decoder size that maintains competitive MAE/MSE on benchmarks while running on micro-controllers or older embedded chips.

### Open Question 2
- **Question:** Can the integration of density-aware loss functions mitigate the model's tendency to underestimate counts in extremely dense crowd regions?
- **Basis in paper:** [explicit] The authors note: "This limitation suggests potential directions for future research, such as combining our lightweight model with density-aware loss functions to enhance robustness in ultra-dense scenes."
- **Why unresolved:** The current MSE loss function may not sufficiently penalize errors in high-density areas where head boundaries overlap severely, leading to the observed underestimation.
- **What evidence would resolve it:** Experiments on high-density subsets of NWPU-Crowd or UCF-QNRF showing reduced error rates when training with density-aware or frequency-domain losses compared to the baseline MSE.

### Open Question 3
- **Question:** Can the network maintain "super real-time" performance (e.g., >30 FPS) on embedded devices when processing high-resolution inputs necessary for large-scale scenes?
- **Basis in paper:** [inferred] Table 5 shows that FPS on the NVIDIA Jetson TX1 drops from 71.9 (at 576x768 resolution) to 27.17 (at 1024x1024 resolution), falling below standard real-time video thresholds despite the need for higher resolution to capture distant crowds.
- **Why unresolved:** The paper demonstrates high speed on lower resolutions, but the computational cost scales significantly with input size, challenging the feasibility of high-fidelity counting on edge devices.
- **What evidence would resolve it:** Optimized benchmarks showing frame rates above 30 FPS for 1024x1024 inputs on the same embedded hardware, or architectural adjustments (e.g., variable resolution processing) that balance speed and detection range.

## Limitations

- The MLF block implementation details are insufficiently specified, making exact reproduction challenging.
- The architecture explicitly sacrifices accuracy for speed, with occasional underestimation in extremely dense regions.
- The ablation study comparing large kernels to dilated convolutions doesn't explore other efficient alternatives like depthwise separable convolutions.

## Confidence

**High Confidence:** Hardware performance claims (381.7 FPS on GTX 1080Ti, 71.9 FPS on Jetson TX1) are well-supported by Table 5 and consistent with architectural design choices.

**Medium Confidence:** Accuracy claims are supported by Table 4, though lack statistical significance testing between methods.

**Low Confidence:** Mechanism claims about why large kernels outperform dilated convolutions are plausible but not rigorously proven.

## Next Checks

1. **MLF Block Implementation Verification:** Implement the Multi-branch Local Fusion blocks exactly as described in Figure 2 and verify their behavior through unit tests.

2. **Hardware Profiling on Target Edge Device:** Conduct latency profiling on Jetson TX1 with exact model configuration ($576\times768$ input) to verify <15ms real-time requirement.

3. **Density Map Generation Methodology:** Investigate and document the exact ground truth density map generation process, including kernel size and method used.