---
ver: rpa2
title: 'Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning
  for Bit String Generation'
arxiv_id: '2505.17288'
source_url: https://arxiv.org/abs/2505.17288
tags:
- reward
- class
- should
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides a theoretical comparison of two methods for
  adapting large language models to new tasks: supervised fine-tuning (SFT) and Best-of-N
  (BoN). The study uses bit string generation as a case study and finds that when
  the learning setting is realizable (i.e., the target function is in the model class),
  SFT outperforms BoN due to better dependence on response length in its rate of convergence.'
---

# Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation

## Quick Facts
- arXiv ID: 2505.17288
- Source URL: https://arxiv.org/abs/2505.17288
- Reference count: 40
- This work provides a theoretical comparison of two methods for adapting large language models to new tasks: supervised fine-tuning (SFT) and Best-of-N (BoN)

## Executive Summary
This paper provides a theoretical comparison of supervised fine-tuning (SFT) and Best-of-N (BoN) for adapting large language models to new tasks, using bit string generation as a case study. The study reveals that SFT outperforms BoN when the learning setting is realizable, due to better dependence on response length in its rate of convergence. However, when realizability fails, BoN can achieve better convergence rates depending on the specific failure mode. The analysis shows that SFT is more sensitive to the learning setting and can fail entirely in agnostic cases, while BoN is more robust but potentially slower to converge.

## Method Summary
The paper analyzes two approaches for adapting pre-trained language models: supervised fine-tuning (SFT) and Best-of-N (BoN). SFT involves training the model on labeled examples using gradient descent, while BoN generates multiple samples and selects the best one according to a scoring function. The theoretical framework assumes a bit string generation setting where the model outputs binary strings of length m. The analysis compares the excess risk (difference between model error and Bayes risk) for both methods under different learning scenarios: realizable (target function in model class), misspecified (target close but not in model class), and agnostic (no assumptions about target). The convergence rates are derived in terms of sample size n and response length m.

## Key Results
- In realizable settings, SFT achieves faster convergence rates than BoN due to logarithmic dependence on response length m versus linear dependence
- When realizability fails, BoN can achieve better convergence rates either in sample size n or in dependence on response length m, depending on the failure mode
- SFT's convergence rate degrades significantly in agnostic settings, potentially failing to converge, while BoN remains robust but slower
- The analysis reveals that SFT's advantage in realizable settings comes from its ability to directly minimize the loss rather than relying on selection

## Why This Works (Mechanism)
The mechanism behind these results stems from how each method handles the optimization problem. SFT directly minimizes the expected loss through gradient descent, allowing it to exploit the structure of the problem and achieve better sample complexity. The method benefits from the realizability assumption by finding the optimal hypothesis within the model class. In contrast, BoN relies on sampling and selection, which introduces additional variance that scales with the response length. However, this same limitation becomes an advantage when realizability fails, as BoN can explore the hypothesis space more broadly without being constrained to find a perfect fit.

## Foundational Learning

**Statistical Learning Theory**: Why needed - Provides the framework for analyzing excess risk and convergence rates. Quick check - Verify understanding of risk decomposition into approximation, estimation, and optimization errors.

**Binary Classification with Large Output Spaces**: Why needed - The bit string generation setting extends binary classification to sequences of length m. Quick check - Understand how n-bit strings can be viewed as m independent binary predictions.

**Agnostic vs Realizable Learning**: Why needed - Different assumptions about the relationship between target function and model class lead to different convergence behaviors. Quick check - Distinguish between excess risk bounds in realizable vs agnostic settings.

**Information Theory and Entropy**: Why needed - Used to analyze the sample complexity of learning bit strings of length m. Quick check - Verify understanding of how entropy relates to the number of possible outputs.

## Architecture Onboarding

**Component Map**: Input Distribution -> Sampling Mechanism -> Model (SFT or BoN) -> Output Distribution -> Loss Function -> Risk Estimation

**Critical Path**: For SFT: Data sampling → Gradient computation → Parameter update → Convergence monitoring. For BoN: Data sampling → Multiple sampling from model → Selection via scoring → Best output selection.

**Design Tradeoffs**: SFT trades computational efficiency and sample complexity for sensitivity to learning assumptions, while BoN trades slower convergence for robustness to model mismatch and agnostic settings.

**Failure Signatures**: SFT fails completely in agnostic settings (excess risk doesn't converge to Bayes risk), while BoN fails slowly but steadily with linear dependence on response length.

**Three First Experiments**:
1. Compare SFT and BoN on synthetic bit string generation with known target function to verify theoretical convergence rates
2. Test performance degradation of SFT when target function is slightly outside model class
3. Evaluate robustness of both methods when training data contains label noise

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes a bit string generation setting which may not fully capture real-world language task complexity
- The study focuses on binary output spaces while most language tasks involve larger or continuous output spaces
- Results are derived under specific assumptions about target function and model class that may not hold in practice
- The theoretical framework may overemphasize the severity of SFT's failure in agnostic settings

## Confidence
- **High confidence**: Mathematical proofs showing SFT's advantage in realizable settings with proper dependence on response length
- **Medium confidence**: Comparative analysis of convergence rates when realizability fails, dependent on specific assumptions
- **Medium confidence**: Claim that BoN is more robust to agnostic settings, though severity of SFT's failure needs empirical validation

## Next Checks
1. Empirical validation on real language tasks: Test SFT and BoN on actual text generation tasks (e.g., summarization, translation) to verify whether theoretical advantages of SFT in realizable settings translate to practical performance gains

2. Analysis of partial realizability: Investigate scenarios where the target function is approximately in the model class rather than exactly realizable, bridging the gap between idealized theoretical setting and practical applications

3. Extension to non-binary output spaces: Adapt the theoretical framework to handle larger output vocabularies and measure how convergence rate comparisons change when moving beyond bit strings to actual language generation tasks