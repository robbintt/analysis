---
ver: rpa2
title: Bayesian Optimization for Function-Valued Responses under Min-Max Criteria
arxiv_id: '2512.07868'
source_url: https://arxiv.org/abs/2512.07868
tags:
- functional
- optimization
- design
- across
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MM-FBO, a Bayesian optimization framework
  for function-valued responses under min-max criteria. The core idea is to minimize
  the maximum error across the functional domain using functional principal component
  analysis to represent responses and Gaussian processes to model principal component
  scores.
---

# Bayesian Optimization for Function-Valued Responses under Min-Max Criteria

## Quick Facts
- arXiv ID: 2512.07868
- Source URL: https://arxiv.org/abs/2512.07868
- Authors: Pouya Ahadi; Reza Marzban; Ali Adibi; Kamran Paynabar
- Reference count: 16
- Primary result: MM-FBO framework for min-max optimization of function-valued responses using FPCA and GP surrogates

## Executive Summary
This paper introduces MM-FBO, a Bayesian optimization framework designed for function-valued responses under min-max criteria. The method addresses scenarios where the goal is to minimize the maximum error across an entire functional domain, which is critical in applications like engineering design where worst-case performance matters. The core innovation lies in representing infinite-dimensional functional responses through functional principal component analysis (FPCA), constructing Gaussian process surrogates for the principal component scores, and developing an integrated uncertainty acquisition function that balances worst-case exploitation with exploration.

The framework introduces novel theoretical guarantees including discretization error bounds and consistency results showing convergence to the true min-max objective. Experimental results on synthetic benchmarks and physics-inspired case studies demonstrate that MM-FBO consistently outperforms existing baselines in convergence speed, stability, and worst-case accuracy, with area under normalized regret curves (AUOC) showing improvements of 25-50% compared to standard GP approaches.

## Method Summary
MM-FBO combines functional principal component analysis with Gaussian process surrogates to handle function-valued responses under min-max criteria. The method first applies FPCA to represent functional responses as a finite set of principal component scores, reducing infinite-dimensional problems to tractable scalar regression tasks. Gaussian processes are then trained as surrogates for these scores, enabling closed-form propagation of uncertainty to functional predictions. The acquisition function integrates worst-case expected error with an exploration term based on integrated uncertainty, dynamically balancing exploitation and exploration through an adaptive parameter κ. Theoretical analysis provides discretization error bounds and consistency guarantees, while empirical validation demonstrates superior performance compared to baseline approaches.

## Key Results
- Introduces MM-FBO framework achieving 25-50% improvement in AUOC compared to standard GP baselines
- Provides theoretical guarantees including discretization error bounds and consistency convergence to true min-max objective
- Demonstrates effectiveness on synthetic benchmarks and physics-inspired case studies across different dimensionalities
- Shows robust performance in convergence speed, stability, and worst-case accuracy

## Why This Works (Mechanism)

### Mechanism 1: FPCA-Based Dimensionality Reduction
Infinite-dimensional functional responses can be faithfully represented by a finite set of principal component scores, enabling tractable surrogate modeling. FPCA projects functional responses onto an orthonormal basis {ϕᵢ(λ)}, truncating to M components that capture dominant variability. Each response f(θ,λ) ≈ f̄(λ) + Σᵢ cᵢ(θ)ϕᵢ(λ), reducing the problem to M scalar regression tasks mapping θ → cᵢ(θ). The core assumption is that eigenvalue decay in the Karhunen-Loève expansion is rapid enough that M << T captures most functional variability; scores are approximately independent across components.

### Mechanism 2: Closed-Form Error Distribution via Gaussian Propagation
Pointwise squared error e(θ,λ) = (f(θ,λ) - f*(λ))² follows a noncentral chi-square distribution with tractable moments. With Gaussian posteriors on PC scores, the deviation h(θ,λ) is Gaussian with μₕ and σ²ₕ computable in O(M²) (O(M) if independent). The squared error inherits closed-form moments: E[e] = μ²ₕ + σ²ₕ, Var[e] = 2σ⁴ₕ + 4μ²ₕσ²ₕ, plus cross-λ covariance formulas. The core assumption is that GP posteriors on scores are well-calibrated; residual process r(λ) has known variance structure.

### Mechanism 3: Integrated Uncertainty Acquisition for Min-Max Targeting
An acquisition combining worst-case expected error with integrated uncertainty asymptotically converges to the true min-max objective while enabling efficient exploration. α(θ) = max_λ μₑ(θ,λ) - κ∫_Λ σₑ(θ,λ)dν(λ). The first term exploits worst-case predictions; the second encourages exploration in uncertain regions. Adaptive κ adjusts across iterations to escape local traps. The core assumption is that Theorem 2 conditions hold—uniform mean accuracy, vanishing integrated uncertainty, bounded exploration weight, and existence of a minimizer.

## Foundational Learning

- **Functional Principal Component Analysis (FPCA)**: Core representation enabling dimensionality reduction of function-valued outputs; must understand Karhunen-Loève expansion, eigenvalue truncation, and score extraction. Quick check: Given T=201 discretized wavelengths and M=5 retained components, can you compute how much variance is captured and reconstruct a functional response from scores?

- **Gaussian Process Surrogates with Posterior Moments**: GP posteriors on scores propagate to functional predictions; need fluency with kernel selection, hyperparameter estimation, and posterior mean/variance formulas. Quick check: For a GP with RBF kernel, what happens to posterior variance far from observed design points, and how does this affect the integrated uncertainty term?

- **Min-Max Optimization and Regret Metrics**: The objective g(θ) = sup_λ e(θ,λ) differs fundamentally from integrated error; understanding worst-case vs. average-case optimization is essential. Quick check: Why might a design minimizing integrated error perform poorly under min-max criteria? Give a concrete functional example.

## Architecture Onboarding

- **Component map**: Space-filling design -> FPCA fit -> GP training -> Error statistics computation -> Acquisition evaluation -> Candidate selection -> Evaluation -> Update -> Repeat

- **Critical path**: Initial space-filling design → FPCA fit → GP training → acquisition computation → candidate selection → evaluation → update → repeat until budget exhausted

- **Design tradeoffs**:
  - **M (retained components)**: Higher M captures more variability but increases computational cost (O(M²) per error statistics evaluation) and GP training burden
  - **Grid density T**: Finer grids reduce discretization error (g(θ) - g_T(θ) ≤ ω(h_T)) but increase per-iteration cost
  - **κ strategy**: Aggressive early exploration improves coverage; premature exploitation risks local traps. Paper uses heuristic adaptation; no theoretical guidance on optimal schedule
  - **Assumption: Independence of PC scores**: Simplifies computation (Σ_c diagonal) but may be violated in practice; paper claims robustness to mild violations without quantification

- **Failure signatures**:
  - **Stagnant regret**: κ may be too low (over-exploiting) or too high (under-exploiting); check adaptation heuristics
  - **Exploding acquisition variance**: GP hyperparameters poorly calibrated; reassess kernel choice or increase initial data
  - **FPCA reconstruction error high**: M insufficient; check eigenvalue decay and cumulative variance explained
  - **Slow convergence vs. scalar GP baseline**: Functional structure not beneficial for this problem; reconsider whether min-max is the right objective

- **First 3 experiments**:
  1. **Sanity check on mass-spring-damper (2D)**: Replicate synthetic experiment with n₀=10, budget=50, M=3-5 components. Compare regret trajectory and AUOC against paper's reported values; verify FPCA captures >95% variance
  2. **Ablation on M**: On a single oracle (e.g., heat diffusion), sweep M ∈ {2, 5, 10, 15} and measure AUOC and per-iteration time. Identify knee point where additional components yield diminishing returns
  3. **κ sensitivity analysis**: Fix M and run MM-FBO with static κ ∈ {0.1, 0.5, 1.0} vs. adaptive scheme. Quantify stability (IQR of final regret) and convergence speed to identify if adaptation is essential or a fixed value suffices

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the MM-FBO framework be effectively extended to handle correlated functional principal component scores, potentially utilizing multi-output Gaussian processes?
- **Basis in paper**: [explicit] The conclusion states that "Extending the method to correlated scores through multi-output Gaussian processes remains an important direction, albeit at higher computational costs."
- **Why unresolved**: The current methodology assumes independent GP surrogates for each principal component score to maintain computational tractability, an assumption that may not hold for all functional datasets.
- **What evidence would resolve it**: A derivation of the error statistics and acquisition function under a correlated score model (e.g., using a linear model of coregionalization), along with empirical results showing robustness on datasets with high inter-score correlation.

### Open Question 2
- **Question**: How does the computational efficiency of MM-FBO scale as the number of retained functional principal components ($M$) increases?
- **Basis in paper**: [explicit] The authors note that "the scalability of the approach with respect to the number of retained components M warrants further study, since error statistics grow quadratically with M."
- **Why unresolved**: While the reduction to $M$ scores aids dimensionality reduction, the quadratic cost of calculating error statistics poses a potential bottleneck for complex functional responses requiring many components.
- **What evidence would resolve it**: A theoretical analysis of the computational complexity with respect to $M$ and empirical benchmarks demonstrating the method's runtime and performance on problems with high-dimensional spectral outputs.

### Open Question 3
- **Question**: How can the exploration-exploitation trade-off be characterized and optimized in high-dimensional functional spaces?
- **Basis in paper**: [explicit] The paper states that "additional research is needed to better understand exploration–exploitation trade-offs in high-dimensional functional spaces."
- **Why unresolved**: The current acquisition function relies on a dynamically adapted parameter $\kappa$ using simple heuristics, which may not be optimal or robust when the design space dimensionality increases.
- **What evidence would resolve it**: A theoretical or empirical study analyzing the sensitivity of the convergence rate to the choice of $\kappa$ schedules in high-dimensional settings compared to lower-dimensional benchmarks.

## Limitations

- The paper's main theoretical claims rely heavily on idealized assumptions including smooth functional responses with rapidly decaying eigenvalues and well-calibrated GP posteriors
- Empirical validation is restricted to physics-inspired synthetic benchmarks with known ground truth, limiting generalizability to real-world domains
- The κ adaptation strategy lacks theoretical grounding, and performance improvements depend critically on problem-dependent hyperparameters
- Claims of "20-50% AUOC improvements" are based on synthetic benchmarks that may not capture real-world noise and complexity

## Confidence

- **High confidence**: FPCA-based dimensionality reduction mechanism (well-established in functional data analysis), closed-form error distribution derivation under Gaussian assumptions, and the basic acquisition structure combining worst-case and uncertainty terms
- **Medium confidence**: The consistency theorem's practical relevance given finite budgets and discretization, the claim that M=5-10 components suffice across diverse problems, and the comparative advantage over scalar GP baselines in realistic scenarios
- **Low confidence**: The robustness claim regarding score correlation violations (no quantification provided), the optimality of the κ adaptation heuristic, and the scalability to high-dimensional functional domains (D > 5) where M may need to be large

## Next Checks

1. **Ablation study on FPCA truncation M**: Systematically vary M on a single oracle (e.g., heat diffusion) to quantify variance capture vs. AUOC tradeoff, identifying the optimal M range where functional structure adds value

2. **Robustness to score correlation**: Intentionally introduce correlated PC scores (e.g., via synthetic covariance structure) and measure degradation in convergence speed and worst-case accuracy to test the independence assumption's impact

3. **Real-world application pilot**: Apply MM-FBO to a small-scale experimental design problem (e.g., combustion kinetics or material characterization) to assess performance in the presence of measurement noise, model misspecification, and practical constraints not captured in synthetic benchmarks