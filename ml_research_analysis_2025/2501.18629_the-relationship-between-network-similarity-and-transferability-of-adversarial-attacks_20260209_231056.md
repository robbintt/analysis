---
ver: rpa2
title: The Relationship Between Network Similarity and Transferability of Adversarial
  Attacks
arxiv_id: '2501.18629'
source_url: https://arxiv.org/abs/2501.18629
tags:
- similarity
- attacks
- network
- networks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between network similarity
  and the transferability of adversarial attacks. Using Centered Kernel Alignment
  (CKA) to measure similarity across torchvision models, the authors find that networks
  exhibit moderate overall similarity, with more complex architectures like DenseNet
  showing lower similarity scores.
---

# The Relationship Between Network Similarity and Transferability of Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2501.18629
- **Source URL**: https://arxiv.org/abs/2501.18629
- **Reference count**: 40
- **Primary result**: Network similarity, measured via CKA, can predict adversarial attack transferability success rates with >90% accuracy for black-box and C&W attacks using a DecisionTreeRegressor.

## Executive Summary
This paper investigates the relationship between network similarity and the transferability of adversarial attacks. Using Centered Kernel Alignment (CKA) to measure similarity across torchvision models, the authors find that networks exhibit moderate overall similarity, with more complex architectures like DenseNet showing lower similarity scores. Layer similarity is highest for basic, consistent layers such as DataParallel, Dropout, and Conv2d, while specialized layers show greater variability. Adversarial attack success rates are generally consistent across networks for non-transferred attacks, but vary significantly for transferred attacks, with complex networks being more vulnerable. The authors demonstrate that a DecisionTreeRegressor can predict the success rate of transferred attacks for black-box and Carlini & Wagner attacks with over 90% accuracy, suggesting predictive models may be viable under certain conditions. However, the variability across different data subsets underscores the complexity of these relationships.

## Method Summary
The authors compute representational similarity between torchvision models using Centered Kernel Alignment (CKA) on layer activations, complemented by a novel Diagonal Box Similarity (DBS) metric. They generate adversarial examples using 14 attack types from the Adversarial Robustness Toolbox (ART) on 20 pretrained models, then transfer attacks between networks to measure success rates. A DecisionTreeRegressor is trained to predict transferred attack success rates using similarity scores and layer counts as features, with an 80/20 train/test split. The prediction task targets black-box and C&W attacks, aiming for >90% accuracy (within 1% threshold).

## Key Results
- Network similarity measured by CKA shows moderate overall scores (mean 0.45) with complex architectures like DenseNet exhibiting lower similarity
- Layer similarity is highest for basic layers (DataParallel, Dropout, Conv2d) and lowest for specialized layers
- Transferred attack success rates vary significantly across networks, with complex networks showing higher vulnerability
- DecisionTreeRegressor achieves >90% accuracy in predicting transferred attack success for black-box and C&W attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representational similarity between networks, measured via CKA, provides a signal for predicting adversarial attack transfer success under specific attack conditions.
- Mechanism: CKA compares activation pattern similarity matrices across networks processing identical inputs. When representations align, adversarial perturbations exploiting feature vulnerabilities in one network are more likely to transfer. A DecisionTreeRegressor can map similarity scores + layer counts to transfer success rates.
- Core assumption: Attack transferability is partially encoded in learned representations rather than solely in architectural hyperparameters.
- Evidence anchors:
  - [abstract]: "Network similarity was analyzed using the Centered Kernel Alignment (CKA) metric, revealing moderate overall similarity with a mean score of 0.45"
  - [Section 4.5.2]: "All black-box attacks and the C&W attacks show an accurate prediction of transferred attack success rate... with a Mean Squared Error (MSE) of close to zero"
  - [corpus]: "Defense That Attacks" explores adversarial training's effect on transferability; limited corpus validation for similarity-based prediction models.
- Break condition: Prediction accuracy degrades substantially for white-box attacks (~3% accuracy) where gradient access bypasses similarity-dependent transfer mechanisms.

### Mechanism 2
- Claim: Position-aligned layer comparison via Diagonal Box Similarity (DBS) provides more discriminative similarity scores than global CKA.
- Mechanism: DBS uses Bresenham line algorithm to identify diagonal positions in the similarity matrix, then aggregates scores within a box around each point. This focuses on layers at comparable depths, capturing that early (edge/texture) and final (classification) layers are more conserved across architectures than middle layers.
- Core assumption: Layers at similar normalized depths perform analogous functions regardless of total network depth.
- Evidence anchors:
  - [Section 4.3]: "The DBS scores with box size 5 range from 0.40 to 0.75, while the CKA scores range from 0.32 to 0.57"
  - [Section 4.1]: "The analysis reveals that the first layers consistently have the highest similarity scores... the last layers show an increase in similarity"
  - [corpus]: No direct corpus validation for DBS specifically; this is a novel contribution.
- Break condition: When comparing architectures with fundamentally different depth-function mappings (e.g., dense connections vs. sequential), diagonal alignment may not correspond to functional alignment.

### Mechanism 3
- Claim: Network complexity correlates inversely with robustness against transferred attacks; simpler networks generate more transferable attacks.
- Mechanism: Complex networks (RegNet_X_32GF, DenseNet201) have higher-dimensional decision boundaries with more exploitable features. Attacks generated on simpler networks (AlexNet, SqueezeNet) learn perturbations targeting fundamental features common across architectures.
- Core assumption: Simpler networks learn more universal low-level features; complex networks learn architecture-specific feature combinations.
- Evidence anchors:
  - [Section 4.4]: "RegNet_X_32GF is the most transferable to for all 14 out of 14 attacks... AlexNet is the most transferable from for 8 out of 14 attacks"
  - [Section 4.4]: "When creating attacks on a network, it is more likely that they are transferable to other networks if the network is simpler or smaller"
  - [corpus]: Demontis et al. (2019, cited in paper) discuss transferability explanations; corpus papers focus more on attack methods than complexity-robustness relationships.
- Break condition: VGG11 showed unexpected low transfer-from rates for targeted PGD/FGSM despite being a "simple" network—suggesting complexity alone is insufficient.

## Foundational Learning

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: Core metric quantifying representational similarity; understanding its sensitivity to nonlinear relationships distinguishes it from CCA/SVCCA.
  - Quick check question: Given two networks with CKA=0.45, what would a drop to 0.32 imply about their layer activation patterns?

- Concept: **Adversarial Transferability Categories**
  - Why needed here: The paper distinguishes white-box vs. black-box and targeted vs. non-targeted attacks—these categories show fundamentally different predictability.
  - Quick check question: Why would black-box attacks show higher predictability from similarity scores than white-box attacks?

- Concept: **Regression for Success Rate Prediction**
  - Why needed here: The DecisionTreeRegressor approach uses [similarity score, layer count] as features; understanding feature engineering choices is critical for reproduction.
  - Quick check question: If you added attack type as a categorical feature, would you expect accuracy to improve for mixed-attack evaluation?

## Architecture Onboarding

- Component map:
  - Load pretrained torchvision CNNs → Extract activations for ImageNet subset → Compute pairwise CKA matrices → Derive DBS scores (box size 5) → Generate adversarial examples on source networks → Transfer and evaluate success rates → Train regressor on similarity + metadata → Evaluate with MSE improvement over mean baseline and 1% threshold accuracy

- Critical path:
  1. Load pretrained torchvision CNNs; extract activations for ImageNet subset
  2. Compute pairwise CKA matrices; derive DBS scores (box size 5 recommended)
  3. Generate adversarial examples on source networks; transfer and evaluate success rates
  4. Train regressor on similarity + metadata; evaluate with MSE improvement over mean baseline and 1% threshold accuracy

- Design tradeoffs:
  - Box size 5 balances capturing aligned-layer similarity vs. maintaining discriminative score range
  - DecisionTreeRegressor chosen for interpretability; ensemble methods may improve robustness but reduce insight into decision rules
  - Excluding initial source success rate as feature (intentionally unavailable in realistic black-box scenarios)

- Failure signatures:
  - White-box attack prediction: ~3% accuracy (architecture-specific exploitation breaks similarity signal)
  - High-variance attacks (PGD, FGSM with 19-22% std): prediction accuracy drops
  - DenseNet comparisons: consistently low similarity scores may compress useful signal range

- First 3 experiments:
  1. Reproduce CKA similarity distribution across torchvision models; verify mean≈0.45, std≈0.05
  2. Implement DBS with box sizes [1,5,15]; plot score range vs. box size to validate size-5 selection
  3. Train DecisionTreeRegressor on black-box subset only; confirm >70% accuracy within 1% threshold on held-out network pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the relationships between network similarity and attack transferability consistent across non-CNN domains such as natural language processing or reinforcement learning?
- Basis in paper: [explicit] The conclusion states studies should be expanded to determine if observed patterns are consistent across different domains and applications like text or audio.
- Why unresolved: This study limited its scope to torchvision models and image classification tasks.
- What evidence would resolve it: Replicating the CKA/DBS analysis and transferability prediction on Transformer or recurrent architectures using text or control datasets.

### Open Question 2
- Question: What architectural factors cause specific 2D convolutional layers in VGG BN models to exhibit near-zero similarity scores when compared to layers in other networks?
- Basis in paper: [explicit] Section 4.1 notes that the exact reason for these low scores is unclear and suggests further investigation is required to uncover the underlying cause.
- Why unresolved: The authors observed the anomaly but did not isolate whether it stems from batch normalization, depth, or other structural features.
- What evidence would resolve it: An ablation study modifying VGG BN layers or comparing them against a wider variety of architectures to isolate the variable driving the dissimilarity.

### Open Question 3
- Question: Can a more granular network similarity metric be developed to better capture the comparability of intermediate layers across different architectures?
- Basis in paper: [explicit] The conclusion suggests the goal of future research should be creating a more accurate score capable of comparing intermediate layers to achieve higher similarity within same-architecture groups.
- Why unresolved: Current metrics (CKA, DBS) struggle to differentiate networks effectively, resulting in moderate scores that do not fully correlate with transfer success.
- What evidence would resolve it: A new metric demonstrating higher correlation values with transferred attack success rates specifically for intermediate layers.

## Limitations
- The relationship between similarity and transferability only holds for specific attack types (black-box and C&W), failing dramatically for white-box attacks
- The moderate CKA similarity scores (mean 0.45) indicate that similarity alone is insufficient for universal transferability prediction
- The dataset used for CKA computation is unspecified, making exact reproduction challenging

## Confidence
- **High Confidence**: The CKA similarity distribution findings (mean 0.45, range 0.32-0.57) and the DBS improvement over global CKA (range 0.40-0.75) are well-supported by the methodology and results presented.
- **Medium Confidence**: The prediction accuracy claims for black-box and C&W attacks (>90%) are supported, but the dramatic failure for white-box attacks (~3%) suggests the mechanism has narrow applicability that may not generalize to all attack scenarios.
- **Low Confidence**: The complexity-robustness relationship (simpler networks generate more transferable attacks) shows inconsistencies, particularly with VGG11's unexpected low transfer-from rates despite being classified as "simple."

## Next Checks
1. **Reproduce CKA Distribution**: Compute CKA scores across torchvision models using a standard ImageNet validation subset to verify the reported mean of 0.45 and standard deviation of 0.05.
2. **Validate DBS Sensitivity**: Implement DBS with varying box sizes (1, 5, 15) and plot score distributions to confirm that box size 5 provides optimal discriminative power while maintaining interpretability.
3. **Attack-Specific Prediction**: Train separate DecisionTreeRegressor models for each attack type (black-box, white-box, C&W) to verify that the high accuracy claim specifically applies to black-box and C&W attacks, while white-box predictions fail as reported.