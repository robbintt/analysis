---
ver: rpa2
title: Nonconvex Regularization for Feature Selection in Reinforcement Learning
arxiv_id: '2509.15652'
source_url: https://arxiv.org/abs/2509.15652
tags:
- penalty
- monotone
- learning
- algorithm
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a nonconvex regularization method for feature\
  \ selection in reinforcement learning, addressing the estimation bias issue in conventional\
  \ \u21131 regularization. The method combines least-squares temporal-difference\
  \ (LSTD) learning with a projected minimax concave (PMC) penalty, reformulated as\
  \ a nonmonotone-inclusion problem solved via forward-reflected-backward splitting\
  \ (FRBS) with convergence guarantees."
---

# Nonconvex Regularization for Feature Selection in Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.15652
- **Source URL:** https://arxiv.org/abs/2509.15652
- **Reference count:** 0
- **Primary result:** PMC regularization with FRBS solver significantly outperforms LSTD, LARS-TD, and BPDN in NMSE for RL feature selection with irrelevant features.

## Executive Summary
This paper addresses estimation bias in conventional ℓ₁ regularization for feature selection in reinforcement learning by proposing a nonconvex projected minimax concave (PMC) penalty combined with least-squares temporal-difference (LSTD) learning. The method reformulates the problem as a nonmonotone inclusion solved via forward-reflected-backward splitting (FRBS), achieving convergence guarantees even with hypomonotone operators. Experiments on a 20-state MDP benchmark demonstrate substantial NMSE improvements over state-of-the-art methods, particularly in high-dimensional settings with many irrelevant features.

## Method Summary
The approach combines LSTD with a projected minimax concave (PMC) penalty to perform feature selection while mitigating ℓ₁ regularization's estimation bias. The PMC penalty applies full ℓ₁ regularization on the orthogonal complement of subspace M and a debiasing minimax concave penalty on M. The resulting nonmonotone inclusion problem is solved via FRBS, which handles the nonconvexity through a reflection step. The algorithm requires eigendecomposition of the feature covariance matrix to determine the appropriate subspace M and verify convexity conditions.

## Key Results
- PMC-LSTD achieves significantly lower NMSE than LSTD, LARS-TD, and BPDN on 20-state MDP benchmark
- Performance advantage increases with number of irrelevant features in high-dimensional settings
- The debiasing effect of PMC penalty reduces underestimation bias for large-amplitude weight components
- Convexity is preserved through careful subspace projection even when feature matrix is singular

## Why This Works (Mechanism)

### Mechanism 1: PMC Penalty's Debiasing Effect Reduces Estimation Bias
The projected minimax concave (PMC) penalty Ψ^PMC_{τ,M}(x) = ||x||₁ − τ||·||₁(P_M x) mitigates the underestimation bias inherent in ℓ₁ regularization for large-amplitude weight components. The subtraction of the Moreau envelope of the ℓ₁ norm projected onto subspace M counteracts ℓ₁'s tendency to shrink large coefficients, reducing penalization of larger values while maintaining sparsity.

### Mechanism 2: Subspace Projection Preserves Convexity in Underdetermined Settings
Restricting the MC debiasing effect to subspace M ensures convexity of the regularized objective even when the feature matrix is singular. The PMC penalty applies full ℓ₁ penalty on M^⊥ and the debiasing MC penalty only on M. By choosing M := span(V_{1,q}) where V comes from eigendecomposition of Φ^TΦ, the convexity condition μτ⁻¹ ≤ λ⁺⁺_min(M^T M) can be satisfied.

### Mechanism 3: FRBS Solves Nonmonotone Inclusion via Resolvent Composition
The forward-reflected-backward splitting algorithm converges to solutions of the reformulated nonmonotone inclusion problem even when operators are hypomonotone (ρ < 0). The problem 0 ∈ (A+B)(x) is recast where A = αμ∂||·||₁ − Id (maximally (−1)-monotone) and B = αT + Id (monotone, β-Lipschitz). FRBS update uses resolvent composition with soft-shrinkage operator.

## Foundational Learning

- **Concept: Least-Squares Temporal Difference (LSTD) Learning**
  - Why needed here: The base RL policy evaluation framework being regularized
  - Quick check question: Can you explain why LSTD solves argmin_u (1/2)||Φu − (g + γΦ'w)||²₂ and what matrices Ã, b̃ represent?

- **Concept: Proximal Operators and Moreau Envelopes**
  - Why needed here: The PMC penalty is defined via Moreau envelopes, and FRBS relies on resolvents (generalized proximal operators)
  - Quick check question: What is the relationship between the proximity operator s-Prox_τ f and the Moreau envelope τf? Can you compute s-Prox_τ||·||₁?

- **Concept: Monotone Operator Theory (ρ-monotonicity, Hypomonotonicity)**
  - Why needed here: Convergence analysis requires understanding when T − ρId is monotone, and why standard FBS/Douglas-Rachford fail for nonmonotone operators
  - Quick check question: If A is maximally (−1)-monotone, what does that imply about A + Id?

## Architecture Onboarding

- **Component map:**
  [Sample Buffer] → [Feature Matrix Φ, Φ', g] → [Eigendecomposition Φ^TΦ] → [Subspace M selection]
                                                                              ↓
  [FRBS Solver] ← [Hyperparameters μ, τ, q, α, η_k] ← [Convexity condition check (8)]
       ↓
  [Sparse weights ŵ] → [Q-function estimate Q̂ = Φŵ]

- **Critical path:**
  1. Collect m samples (s_i, r_i, s'_i) via current policy π
  2. Build Ã = Φ^T(Φ − γΦ') and b̃ = Φ^Tg
  3. Compute eigendecomposition of Φ^TΦ to determine M
  4. Verify condition (8): μτ⁻¹ ≤ max{l_q, λ⁺⁺_min}
  5. Run FRBS iterations until convergence
  6. Use ŵ for policy improvement

- **Design tradeoffs:**
  - q selection: Larger q increases debiasing coverage but requires larger τ (weaker debiasing)
  - μ (regularization strength): Higher μ increases sparsity but risks underfitting
  - τ (PMC index): Controls debiasing intensity; τ → ∞ recovers ℓ₁

- **Failure signatures:**
  - NMSE plateau or degradation: Check if condition (8) violated or τ too large
  - Non-convergence: Verify η_k bounds in Proposition 2; check if (A+B)^{-1}(0) is empty
  - Excessive sparsity (zero weights): Reduce μ

- **First 3 experiments:**
  1. Sanity check on 20-state MDP: Reproduce Figure 1 with n_ε = 0 irrelevant features
  2. Scalability test: Fix m = 1000 samples, vary n_ε from 10 to 500
  3. Ablation on subspace dimension q: With fixed μ, τ, vary q ∈ {1, rank(Φ), n/2, n}

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed nonconvex regularization framework be extended to online reinforcement learning settings?
- Basis in paper: The conclusion explicitly lists "extending the method to online learning" as a primary direction for future research.
- Why unresolved: The current work focuses exclusively on a batch algorithm (LSTD) which processes collected samples offline, whereas online learning requires updating parameters incrementally with streaming data.
- What evidence would resolve it: An online variant of the algorithm that maintains convergence guarantees while processing data streams.

### Open Question 2
- Question: Can this method be integrated with the variational framework for Q-function approximation in Reproducing Kernel Hilbert Spaces (RKHS)?
- Basis in paper: The conclusion identifies "integrating it with the variational framework of [33] for Q-function approximation in reproducing kernel Hilbert spaces (RKHS)" as a future direction.
- Why unresolved: The current formulation relies on linear function approximation, and extending the weak convexity properties of the PMC penalty to the non-parametric RKHS setting is non-trivial.
- What evidence would resolve it: A reformulation of the Bellman-residual objective with PMC penalty within the RKHS variational framework.

### Open Question 3
- Question: How can the dimension $q$ of the subspace $M$ be selected adaptively to balance the debiasing effect against the penalty parameter requirements?
- Basis in paper: Remark 1 notes that maximizing the dimension of $M$ is desirable for debiasing, but if $\lambda_{\min}^{++}$ is small, $\tau$ must be large, degrading performance.
- Why unresolved: The paper provides a theoretical bound (Condition 8) but acknowledges a practical trade-off where strict adherence to the maximal subspace might force suboptimal penalty parameters.
- What evidence would resolve it: A heuristic or theoretical rule for selecting $q$ based on the spectral properties of the feature covariance matrix.

### Open Question 4
- Question: Does the FRBS-based PMC-LSTD method maintain its performance advantage in high-dimensional environments with non-linear function approximation (e.g., deep RL)?
- Basis in paper: The paper introduces the "curse of dimensionality" as motivation but validates the approach only on a simple "20-state MDP" benchmark.
- Why unresolved: It is unclear if the computational overhead of the FRBS algorithm and the eigenvalue decomposition required for subspace M scales efficiently to the massive parameter spaces found in deep reinforcement learning.
- What evidence would resolve it: Numerical experiments applying the method to standard high-dimensional control benchmarks integrated with neural network function approximation.

## Limitations
- Hyperparameter sensitivity: Performance depends critically on μ, τ, α, and η_k choices, with τ requiring careful tuning to balance debiasing and convexity
- Scalability to larger MDPs: While experiments show effectiveness in the 20-state benchmark, generalization to high-dimensional or continuous state spaces remains untested
- Convergence guarantees under practical conditions: Theorem 1 requires specific conditions that may not hold in practice

## Confidence
- **High confidence:** FRBS convergence theory and implementation, convexity preservation mechanism via subspace projection, NMSE metric calculation
- **Medium confidence:** PMC penalty's debiasing effect, selection of subspace M and parameter q, practical effectiveness relative to LSTD/BPDN
- **Low confidence:** Generalization beyond 20-state MDP, robustness to hyperparameter choices, computational scalability

## Next Checks
1. Reproduce 20-state MDP baseline: Implement the 20-state MDP with 2 actions and RBF features as described. Verify that LSTD, LARS-TD, and BPDN baselines reproduce the reported NMSE values before testing the PMC-FRBS method.
2. Test convexity condition violation: Deliberately choose τ values that violate condition (8). Measure impact on NMSE and convergence behavior to understand practical sensitivity.
3. Ablation on subspace dimension q: Systematically vary q ∈ {1, 2, rank(Φ), n/2} while holding other hyperparameters constant. Quantify trade-off between debiasing coverage and performance degradation.