---
ver: rpa2
title: 'PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific
  Rubric Generation'
arxiv_id: '2510.21721'
source_url: https://arxiv.org/abs/2510.21721
tags:
- user
- story
- evaluation
- each
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PREFINE extends critique-and-refine (C&R) frameworks to personalized
  story generation by constructing a pseudo-user agent from user interaction history
  and generating user-specific rubrics. The framework enables iterative refinement
  without parameter updates or direct feedback.
---

# PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation

## Quick Facts
- arXiv ID: 2510.21721
- Source URL: https://arxiv.org/abs/2510.21721
- Reference count: 4
- Key outcome: PREFINE achieves higher win rates and statistically significant improvements over baselines in personalized story generation while maintaining general quality comparable to specialized methods.

## Executive Summary
PREFINE introduces a critique-and-refine framework for personalized story generation that operates without parameter updates or direct user feedback. The system constructs a pseudo-user agent from interaction history and generates user-specific rubrics to guide iterative refinement. Evaluations on PerDOC and PerMPST datasets demonstrate PREFINE's effectiveness in aligning generated stories with individual user preferences while maintaining general story quality.

## Method Summary
PREFINE builds on critique-and-refine frameworks by adding two personalization components: a pseudo-user agent constructed from user interaction history, and user-specific rubrics generated for each individual. The system uses an expert agent to abstract raw interaction logs into an Explicit Persona, which grounds the pseudo-user agent's behavior. A separate rubric generator creates 3-5 specific evaluation criteria based on this persona. The iterative loop then critiques generated stories against these user-specific rubrics and refines them accordingly, achieving personalization without model updates or direct feedback.

## Key Results
- PREFINE achieves 67% win rate against Prompt-Persona baseline on PerDOC dataset
- Significant improvements over baselines including Zero-Persona, Prompt-Persona, and Self-Refine
- Maintains general story quality comparable to specialized quality-enhancement methods
- Both pseudo-user agent and user-specific rubrics are essential for effective personalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Abstracting user history into an "Explicit Persona" enables more robust simulation than direct history injection
- **Mechanism:** An expert agent prompt condenses raw interaction logs into a natural language psychological profile that acts as a system prompt for the pseudo-user agent
- **Core assumption:** LLMs can accurately infer stable personality traits and preferences from limited interaction history
- **Evidence anchors:** Expert agent prompting described in Section 3.3; PerDOC and PerMPST datasets provide interaction history
- **Break condition:** The abstraction process hallucinates traits not present in the history, leading to a persona that diverges from the actual user

### Mechanism 2
- **Claim:** User-specific rubrics provide a consistent optimization objective that prevents drift during iterative refinement
- **Mechanism:** Instead of relying on the LLM's implicit definition of "good," PREFINE generates a fixed rubric derived from the persona that forces the critique agent to evaluate against specific, user-aligned constraints
- **Core assumption:** The generated rubric accurately operationalizes the user's subjective preferences into measurable criteria
- **Evidence anchors:** EPER vs EPIR ablation shows methods with Explicit Rubrics improve over iterations while those without stagnate
- **Break condition:** The rubric is too generic or contradictory, failing to guide the refinement agent toward specific user preferences

### Mechanism 3
- **Claim:** Iterative Critique-and-Refine acts as a test-time optimization loop, aligning generic outputs to specific user vectors without gradient updates
- **Mechanism:** The loop converts the difficult problem of "generate perfect personalized text in one shot" into "critique difference" and "apply patch"
- **Core assumption:** The refinement agent can successfully interpret feedback and modify the narrative without degrading coherence
- **Evidence anchors:** 67% win rate against Prompt-Persona, even when starting from the same Zero-Persona baseline
- **Break condition:** The refinement loop degenerates into superficial changes that fail to address the core critique or violates the "fixed premise" constraint

## Foundational Learning

**Concept: Critique-and-Refine (C&R) Frameworks**
- **Why needed here:** PREFINE builds on Self-Refine mechanisms. Understanding that LLMs can self-correct if given a standard is prerequisite to understanding how user-specific rubrics modify that process.
- **Quick check question:** Why does a generic "Self-Refine" loop fail to produce personalized results according to the paper? (Hint: See Section 5.1 comparison of SR vs EPER).

**Concept: In-Context User Modeling**
- **Why needed here:** The system relies on simulating a user (pseudo-user agent) rather than querying one. You must understand how prompts can instantiate specific behaviors or personas.
- **Quick check question:** What is the specific difference between the "Implicit Persona" (IP) and "Explicit Persona" (EP) used in the ablation studies?

**Concept: LLM-as-a-Judge Evaluation**
- **Why needed here:** The results depend on GPT-4o and PerSE validating the outputs. You need to understand the biases inherent in these evaluators to interpret the "Win Rate" data correctly.
- **Quick check question:** How did the authors validate that the LLM judges (PerSE, GPT-4o) aligned with human preferences before relying on them?

## Architecture Onboarding

- **Component map:** History Source -> Expert Agent -> Explicit Persona -> Rubric Generator -> User-Specific Rubric -> Generator (initial story) -> Pseudo-User Agent (critique) -> Refinement Agent (refine) -> Refined Story
- **Critical path:** The *Rubric Generation* phase is the critical junction. If the rubric is flawed, the subsequent 7-step refinement loop optimizes for the wrong objective.
- **Design tradeoffs:** Cost vs. Alignment requires $1 (\text{Expert}) + 1 (\text{Rubric}) + T \times 2 (\text{Critic} + \text{Refine})$ LLM calls per story. Explicit vs. Implicit: Using Explicit Personas requires an extra synthesis step but outperforms Implicit Personas in experiments.
- **Failure signatures:** "Rubric Drift" (rubric too broad), "Over-Refinement" (ignores "Do not change Premise" instruction), "Short-Text Ceiling" (very short texts offer limited refinement surface area).
- **First 3 experiments:** 1) Baseline Replication: Run Zero-Persona and Prompt-Persona on PerDOC to confirm evaluation pipeline. 2) Ablation on Rubric: Implement "EPER" vs. "EPIR" comparison to verify rubric's role in personalization. 3) Iteration Ceiling: Test C&R loop stopping at T=1, T=3, and T=7 to measure marginal gain against cost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does varying the backbone model size or architecture for individual agents affect PREFINE's performance?
- **Basis in paper:** Section 8 (Limitations) explicitly states, "we did not explore how variations in model size or architecture across these components might affect the performance."
- **Why unresolved:** Authors uniformly used LLaMA-3-70B for all agents to manage computational costs, leaving the contribution of specific component capacities unknown.
- **What evidence would resolve it:** Ablation studies utilizing smaller models for specific agents to analyze performance trade-offs.

### Open Question 2
- **Question:** Can the PREFINE framework be effectively transferred to other personalized generation domains, such as dialogue or recommendation systems?
- **Basis in paper:** Section 7 (Conclusion) suggests the design principles "are broadly applicable to other domains that require personalized systems."
- **Why unresolved:** Evaluation was restricted to story generation datasets (PerDOC and PerMPST).
- **What evidence would resolve it:** Applying PREFINE to interactive tasks like personalized dialogue or educational content generation and measuring alignment with user preferences.

### Open Question 3
- **Question:** Does PREFINE maintain robust personalization when scaling to significantly longer user interaction histories?
- **Basis in paper:** Section 4.1 notes that history size was restricted ($N_u=1$, $K=4$) due to context length limitations, relying on very sparse interaction data.
- **Why unresolved:** It is unclear if the current pseudo-user agent construction can effectively aggregate and weigh preferences from extensive user logs without losing critical nuances.
- **What evidence would resolve it:** Experiments using datasets with dense user histories or integrating memory-augmented mechanisms to handle longer context windows.

## Limitations
- Dataset dependence on PerDOC and PerMPST with constrained interaction histories (single user pairs, 4 reviews)
- Limited testing of iterative refinement stability beyond T≤7 iterations
- Unknown scalability to diverse, noisy real-world user logs
- Computational cost of multiple LLM calls per story

## Confidence
- **High**: The C&R framework architecture is sound and the rubric generation step is essential (confirmed by EPER > EPIR ablations)
- **Medium**: The personalization effect is real and measurable, but evaluator alignment and long-term loop stability are uncertain
- **Low**: Claims about robustness to noisy histories and long-term iterative stability lack empirical support

## Next Checks
1. **Evaluator Validation**: Run a small human study (n≥20) comparing PerSE-Llama3-8B vs. human preferences on PREFINE outputs to quantify alignment and identify systematic biases
2. **Extended Refinement Stability**: Execute the C&R loop for T=10, 15, 20 iterations on a subset of PerDOC, tracking rubric adherence, premise preservation, and narrative coherence to detect mode collapse or drift
3. **History Noise Robustness**: Inject synthetic noise (random reviews, conflicting preferences) into PerDOC histories and measure degradation in PREFINE performance to test the expert persona's noise tolerance