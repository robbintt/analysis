---
ver: rpa2
title: Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept
  Discovery
arxiv_id: '2505.20293'
source_url: https://arxiv.org/abs/2505.20293
tags:
- concept
- concepts
- comprehensibility
- explanations
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ECO-Concept, an unsupervised framework for
  discovering human-comprehensible concepts in text classification. The method combines
  a slot attention-based concept extractor with LLM-guided evaluation to iteratively
  refine concept representations.
---

# Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery

## Quick Facts
- **arXiv ID**: 2505.20293
- **Source URL**: https://arxiv.org/abs/2505.20293
- **Reference count**: 27
- **Primary result**: Unsupervised concept discovery achieves classification performance comparable to black-box models while outperforming existing concept-based methods in task accuracy and concept interpretability.

## Executive Summary
This paper introduces ECO-Concept, an unsupervised framework that discovers human-comprehensible concepts for text classification without requiring predefined concept annotations. The method combines slot attention-based concept extraction with LLM-guided evaluation to iteratively refine concept representations. By evaluating concepts using LLM-generated summaries and highlighted segments, the framework ensures the discovered concepts are both task-relevant and interpretable. Experiments on seven diverse datasets demonstrate that ECO-Concept matches black-box model performance while significantly improving interpretability compared to existing concept-based methods.

## Method Summary
ECO-Concept operates in two phases. First, it trains a slot attention-based concept extractor that learns M concept prototypes from text inputs, applying consistency and distinctiveness regularizers to ensure semantic coherence and separation. Second, it enhances concept comprehensibility through LLM feedback: GPT-4o summarizes each concept from top exemplars, and GPT-4o-mini highlights concept-related tokens on held-out examples. The framework then fine-tunes concept prototypes to align slot attention patterns with LLM highlighting, weighted by concept importance. This process iterates until concept meanings stabilize, creating explanations that are both accurate and human-understandable.

## Key Results
- ECO-Concept achieves classification accuracy comparable to black-box models across all seven datasets
- Outperforms existing concept-based methods in concept interpretability metrics (semantics, distinctiveness, consistency)
- Human evaluations show ECO-Concept's concepts are more intuitive and effective for understanding model behavior (4.1-4.6/5.0 agreement ratings with LLM summaries)
- Concept stability achieved within 2-3 iterations of comprehensibility enhancement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slot attention with sparse normalization enables unsupervised discovery of distinct task-relevant concepts.
- Mechanism: Each concept prototype competes to attend to input tokens via sparse softmax normalization across the M concept dimension, forcing different slots to capture different semantic aspects.
- Core assumption: Task-relevant concepts can be decomposed into token-level attention patterns that form coherent semantic clusters.
- Evidence anchors: [abstract] "ECO-Concept first utilizes an object-centric architecture to extract semantic concepts automatically." [Section 3.2] "This normalization introduces competition among slots to attend to each input token."
- Break condition: If slots collapse to attending the same tokens, or if attention becomes uniform across all slots.

### Mechanism 2
- Claim: Concept regularizers constrain prototypes to form semantically coherent and non-redundant representations.
- Mechanism: Consistency loss minimizes variance in concept features for the same concept across samples, while distinctiveness loss maximizes separation between average features of different concepts.
- Core assumption: Semantic coherence in embedding space correlates with human-comprehensible concepts.
- Evidence anchors: [Section 3.2] "For better interpretability, we employ concept regularizers to constrain the training of concept prototypes C." [Section 4.4, Table 5] Ablation shows both regularizers together achieve best performance.
- Break condition: If λ_dist is too large, concepts may become artificially distinct; if λ_con is too large, concepts may become overly rigid.

### Mechanism 3
- Claim: LLM-based comprehensibility feedback closes the gap between learned representations and human-understandable concepts.
- Mechanism: GPT-4o summarizes concepts from top-activated exemplars, GPT-4o-mini highlights concept-related tokens, and MSE between model attention and LLM highlighting becomes a weighted comprehensibility loss.
- Core assumption: LLM summarization and highlighting approximate human judgment of concept comprehensibility.
- Evidence anchors: [abstract] "the comprehensibility of the extracted concepts is evaluated by large language models." [Appendix B, Table 6] Human evaluation shows 4.1-4.6/5.0 agreement ratings with LLM summaries.
- Break condition: If LLM hallucinates patterns not present in exemplars, or if highlighting becomes trivially permissive.

## Foundational Learning

- **Concept: Slot Attention**
  - Why needed here: Core mechanism for unsupervised concept decomposition; without understanding competition-based attention, the extractor's behavior is opaque.
  - Quick check question: Can you explain why sparse softmax across slots (not across tokens) creates competition?

- **Concept: Concept Bottleneck Models (CBM)**
  - Why needed here: ECO-Concept extends CBM from supervised to unsupervised concept learning; the classifier still uses concept activations as the only input.
  - Quick check question: In a standard CBM, what is the role of the bottleneck layer between input and output?

- **Concept: Forward Simulatability**
  - Why needed here: Primary human evaluation metric; measures whether users can correctly predict model outputs given explanations.
  - Quick check question: Why is simulatability a stronger test of explanation quality than subjective ratings alone?

## Architecture Onboarding

- **Component map**: Text Encoder (RoBERTa) -> Concept Extractor (Slot attention with prototypes) -> Classifier (Linear layer) -> Concept Evaluator (GPT-4o/GPT-4o-mini)

- **Critical path**:
  1. Phase 1: Train with L_ce + λ_con·L_con + λ_dist·L_dist until convergence
  2. Phase 2: Freeze encoder, fine-tune prototypes C and classifier with added L_com
  3. Iteratively re-summarize concepts; freeze prototypes when meanings stabilize

- **Design tradeoffs**:
  - Number of concepts M: Paper uses 20; more concepts capture finer distinctions but risk redundancy and semantic dilution
  - LLM cost vs. coverage: Only 100 exemplars per concept highlighted due to API costs
  - Fixed M: Cannot adaptively add/remove concepts during training

- **Failure signatures**:
  - Slot collapse: Multiple slots attend to identical token patterns
  - Semantic drift: Concept meanings shift across iterations without stabilizing
  - Comprehensibility-accuracy trade-off: If λ_com too high, classification accuracy drops

- **First 3 experiments**:
  1. Reproduce on single dataset (e.g., Beer) with M=10 vs. M=20 to verify slot competition behavior and concept diversity
  2. Ablate L_com: Compare Base model vs. ECO-Concept vs. ECO-Concept (w/o L_com) on concept semantics score
  3. Human proxy validation: Sample 20 concepts, compare human vs. LLM highlighting agreement on held-out examples

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the fixed constraint on the number of concept slots be replaced by a dynamic mechanism that adaptively adjusts the quantity of concepts during training?
  - Basis: Authors state in Limitations section they are exploring incremental learning techniques to adjust concept count more flexibly.
  - Evidence needed: A modified ECO-Concept variant utilizing dynamic slot mechanism demonstrating comparable or superior performance without manual tuning.

- **Open Question 2**: How effectively can cost-effective, open-source LLMs replace proprietary API-based models as human proxies for evaluating concept comprehensibility during training?
  - Basis: Paper notes cost limitations prevented simulating concepts on all samples and plans to test more cost-effective open-source LLMs.
  - Evidence needed: Experiments substituting LLM evaluator with open-source models showing consistent correlation with human evaluations and final concept quality.

- **Open Question 3**: Can the ECO-Concept framework be scaled to State-of-the-Art Large Language Models such as LLaMA or Mixtral without losing the interpretability-performance balance achieved with BERT-based backbones?
  - Basis: Authors state they plan to explore concept extraction methods compatible with larger models like LLaMA and Mixtral.
  - Evidence needed: Successful implementation of slot attention and LLM-feedback loop on decoder-only LLM architecture maintaining concept comprehensibility alongside advanced capabilities.

## Limitations

- The method relies heavily on LLM-generated feedback as a proxy for human interpretability, validated only through proxy human studies rather than direct human evaluation across all datasets
- Fixed concept budget (M=20) represents a significant architectural limitation that cannot adapt to task complexity
- Implementation details remain underspecified, particularly slot attention architecture parameters and training hyperparameters
- Computational cost of LLM evaluation limits scalability and creates trade-offs between evaluation fidelity and training expense

## Confidence

**High Confidence (Evidence Strong):**
- ECO-Concept achieves comparable classification accuracy to black-box models while outperforming baselines in concept interpretability metrics
- Slot attention mechanism with sparse normalization effectively creates competitive attention patterns across concept slots
- LLM feedback improves concept comprehensibility as measured by automatic metrics

**Medium Confidence (Evidence Moderate):**
- Human evaluations showing ECO-Concept concepts are more intuitive and effective for understanding model behavior
- Two-phase training procedure with concept regularizers consistently improves concept quality across datasets
- Concept stability after 2-3 iterations of comprehensibility enhancement

**Low Confidence (Evidence Weak):**
- LLM proxy accurately represents human judgment of concept comprehensibility across all domains
- Fixed concept number (M=20) is optimal for all seven diverse datasets
- The method generalizes well to datasets substantially different from the seven tested

## Next Checks

1. **Human Validation of LLM Proxy**: Conduct direct human evaluations comparing LLM-generated concept summaries and highlightings against human judgments on a held-out dataset. Measure agreement rates between LLM and human annotators for concept comprehensibility.

2. **Concept Number Sensitivity Analysis**: Systematically vary M (e.g., M=5, 10, 20, 30, 50) on a representative dataset and measure the trade-off between concept interpretability scores and classification accuracy. Identify whether the fixed M=20 choice represents an optimal balance or a domain-specific compromise.

3. **Cross-Dataset Generalization Test**: Apply ECO-Concept to a dataset substantially different from the seven tested (e.g., legal document classification or medical text analysis) and evaluate whether the unsupervised concept discovery maintains effectiveness. Focus on whether concepts remain interpretable and task-relevant in a new domain with different semantic structures.