---
ver: rpa2
title: Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection
arxiv_id: '2508.11504'
source_url: https://arxiv.org/abs/2508.11504
tags:
- crash
- vehicle
- units
- categorical
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a six-year, multi-level dataset of over\
  \ 2.3 million Ohio crash records and applies AutoML with SHAP-based interpretability\
  \ to predict and explain traffic crash severity. Using JADBio, the analysis identified\
  \ 17 key predictors across demographic, environmental, vehicle, and operational\
  \ domains, including location, posted speed, and occupants\u2019 minimum age."
---

# Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection

## Quick Facts
- arXiv ID: 2508.11504
- Source URL: https://arxiv.org/abs/2508.11504
- Reference count: 40
- Primary result: AutoML with SHAP interpretability achieved 84.9% AUC-ROC on 2.3M Ohio crash records, identifying 17 key predictors for severe crash severity.

## Executive Summary
This study introduces a six-year, multi-level dataset of over 2.3 million Ohio crash records and applies AutoML with SHAP-based interpretability to predict and explain traffic crash severity. Using JADBio, the analysis identified 17 key predictors across demographic, environmental, vehicle, and operational domains, including location, posted speed, and occupants' minimum age. A final Ridge Logistic Regression model achieved an AUC-ROC of 84.9% on a hold-out test set, demonstrating strong generalization while prioritizing interpretability. The approach highlights systemic risk factors over individual behaviors, aligning with Vision Zero's Safe System Approach and supporting data-driven, scalable interventions for traffic safety.

## Method Summary
The study utilized Ohio's crash dataset (2017-2022) containing ~2.3 million vehicle-level records. Data underwent VIN verification, occupant-level validation, and aggregation to vehicle-level records. A Ridge Logistic Regression model was trained using JADBio's AutoML pipeline with 738 model configurations evaluated via stratified 10-fold cross-validation. Feature selection employed SES, LASSO, and Epilogi algorithms, retaining features selected in ≥3 of 4 independent training subsets (75% stability threshold). The final model used 17 stable predictors and achieved 84.9% AUC-ROC on a hold-out test set. SHAP values provided instance-level and global interpretability, quantifying feature contributions to severity predictions.

## Key Results
- Achieved 84.9% AUC-ROC on hold-out test set with Ridge Logistic Regression
- Identified 17 stable predictors across demographic, environmental, vehicle, and operational domains
- Location emerged as most influential predictor, followed by Posted Speed and Contributing Circumstances
- Top 10 features included road surface conditions, collision type, weather, and occupants' minimum age
- Model prioritized systemic risk factors over individual behaviors, supporting Vision Zero approach

## Why This Works (Mechanism)

### Mechanism 1
Automated feature selection with stability filtering identifies a compact, generalizable predictor set from high-dimensional crash data. JADBio's AutoML pipeline evaluates 738 model configurations using stratified 10-fold cross-validation with early stopping, applying multiple feature selection algorithms (SES, LASSO, Epilogi). Features selected in ≥3 of 4 independent training subsets (75% stability threshold) are retained, reducing ~4,000 encoded features to 17 stable predictors. Core assumption: Features consistently selected across stratified subsamples reflect reproducible associations rather than sampling noise or overfitting.

### Mechanism 2
SHAP values provide instance-level and global interpretability that maps predictor importance to actionable safety domains. For each prediction, SHAP decomposes the model output into feature-specific contributions (ϕᵢ), quantifying both magnitude and direction. Average absolute SHAP values across all samples rank global feature importance, while individual SHAP plots reveal how specific feature values (e.g., high posted speed, urban location) push predictions toward severe or non-severe outcomes. Core assumption: The linear Ridge Logistic Regression model's SHAP values faithfully represent feature contributions without significant interaction masking.

### Mechanism 3
Threshold-independent evaluation (AUC-ROC) combined with stratified sampling enables robust modeling under extreme class imbalance. Severe crashes represent ~1.27% of data. The pipeline uses stratified random sampling to preserve class ratios in training subsets, cross-validation folds, and hold-out sets. AUC-ROC measures discrimination across all thresholds, avoiding bias toward the majority class. Core assumption: The minority class signal is sufficiently strong and consistent across strata for the model to learn discriminative patterns.

## Foundational Learning

- **Automated Machine Learning (AutoML) with feature selection**: Why needed here: The pipeline automates model search and variable selection across high-dimensional crash data. Quick check question: Can you explain why manual feature engineering may miss relevant predictors in datasets with thousands of encoded features?

- **SHAP (SHapley Additive exPlanations) values**: Why needed here: SHAP translates model outputs into interpretable, domain-aligned feature contributions. Quick check question: How does a positive SHAP value for a specific feature (e.g., "Posted Speed = 65 mph") affect the predicted probability of a severe crash?

- **Class imbalance handling via stratification and threshold-free metrics**: Why needed here: Severe crashes are rare; standard accuracy metrics would be misleading. Quick check question: Why might accuracy be a poor metric when the positive class represents only 1.27% of samples?

## Architecture Onboarding

- **Component map**: Data Ingestion & Cleaning -> Preprocessing -> AutoML Pipeline (JADBio) -> Stability Aggregation -> Final Model Training -> Interpretation Layer

- **Critical path**: Data quality (VIN/person validation) -> proper aggregation -> stratified sampling -> feature selection stability -> generalization on hold-out

- **Design tradeoffs**: Ridge LR chosen over Random Forest despite slightly lower peak AUC for better interpretability and lower computational cost; 17-feature subset balances predictive power with practical interpretability for policy use; class imbalance addressed via stratification and AUC-ROC rather than resampling

- **Failure signatures**: Unstable feature selection across training subsets suggests overfitting or insufficient sample diversity; large training-test AUC gap indicates overfitting or data leakage; SHAP plots showing counterintuitive directions may signal encoding errors or confounding

- **First 3 experiments**:
  1. Reproduce the 4-subset training pipeline on a smaller region/year to verify feature stability patterns
  2. Compare Ridge LR vs. Random Forest vs. XGBoost on the same 17-feature set to quantify interpretability-performance tradeoff
  3. Apply the trained model to a different state's crash data (if available) to assess geographic transferability and identify domain shift

## Open Questions the Paper Calls Out

- **Can interpretable ensemble or hybrid models improve predictive performance over the Ridge Logistic Regression baseline by capturing non-linear feature interactions without sacrificing transparency?**: The authors state that future work should "explore hybrid and interpretable ensemble models that retain explainability while capturing non-linearity," as the linear model may limit capacity for complex interactions. Unresolved because the study prioritized the linear Ridge model for its transparency and strong baseline performance (84.9% AUC), leaving the potential performance gains of non-linear models untested.

- **Does utilizing synthetic oversampling or customized loss functions significantly improve the model's sensitivity to rare severe crash events compared to the standard training approach?**: The authors acknowledge that the "extreme class imbalance (approximately 100:1)... was not directly addressed," and suggest future work investigate "synthetic oversampling or customized loss functions" to enhance detection. Unresolved because the current reliance on AUC-ROC and standard training may mask poor predictive accuracy for the minority "severe" class despite high overall scores.

- **Can causal inference frameworks successfully elucidate the specific mechanisms of injury severity beyond the associative feature importance provided by SHAP?**: The conclusion suggests "integrating causal inference frameworks could elucidate mechanisms of injury severity and further strengthen the translation of model outputs into actionable public safety strategies." Unresolved because while SHAP values identify which features drive predictions, they do not confirm causal relationships (e.g., whether road contour directly causes severity or is a proxy for other factors).

## Limitations
- Geographic Specificity: Model trained and validated on Ohio data may not generalize to states with different traffic patterns, enforcement practices, or infrastructure
- Temporal Stability: Six-year training window captures recent trends but may miss evolving risk factors (e.g., emerging vehicle technologies, post-pandemic driving behaviors)
- Static Feature Set: 17 predictors selected from 2017-2022 data may become less relevant as road environments and vehicle fleets change over time

## Confidence
- High Confidence: AUC-ROC of 84.9% on hold-out test set, rigorous stratified sampling, and stability-based feature selection demonstrate robust internal validation
- Medium Confidence: SHAP-based interpretability aligns with domain expectations (location, speed, age as key factors), but causal claims remain inferential
- Low Confidence: Generalizability to other states or future years untested; model may capture region-specific or temporally bounded patterns

## Next Checks
1. Cross-State Transferability: Apply the trained model to crash data from neighboring states (e.g., Pennsylvania, Michigan) to test geographic generalization and identify domain shifts
2. Temporal Robustness: Retrain the model on rolling five-year windows (e.g., 2017-2021, 2018-2022) to assess feature stability and predictive performance over time
3. Counterfactual Policy Simulation: Use SHAP values to simulate the impact of hypothetical interventions (e.g., reducing urban speed limits by 5 mph) on predicted severe crash rates