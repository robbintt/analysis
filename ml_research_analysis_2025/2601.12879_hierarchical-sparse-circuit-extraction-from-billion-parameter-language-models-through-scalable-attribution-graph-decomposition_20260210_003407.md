---
ver: rpa2
title: Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models
  through Scalable Attribution Graph Decomposition
arxiv_id: '2601.12879'
source_url: https://arxiv.org/abs/2601.12879
tags:
- circuit
- circuits
- across
- hierarchical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Hierarchical Attribution Graph Decomposition
  (HAGD), a framework for extracting sparse computational circuits from billion-parameter
  language models. The method reduces circuit discovery complexity from exponential
  O(2^n) to polynomial O(n^2 log n) through multi-resolution abstraction hierarchies
  and differentiable circuit search.
---

# Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition

## Quick Facts
- arXiv ID: 2601.12879
- Source URL: https://arxiv.org/abs/2601.12879
- Authors: Mohammed Mudassir Uddin; Shahnawaz Alam; Mohammed Kaif Pasha
- Reference count: 35
- Primary result: Reduces circuit discovery complexity from O(2^n) to O(n² log n) through hierarchical decomposition

## Executive Summary
This paper presents Hierarchical Attribution Graph Decomposition (HAGD), a framework for extracting sparse computational circuits from billion-parameter language models. The method achieves polynomial complexity circuit discovery through multi-resolution abstraction hierarchies, cross-layer transcoders for feature extraction, and graph neural networks for topology prediction. It successfully extracts circuits from models ranging from 117M to 70B parameters with behavioral preservation of 82-97% on algorithmic tasks and 74-88% on natural language benchmarks.

## Method Summary
HAGD integrates cross-layer transcoders that map hidden states to sparse features via TopK/L1 regularization, attribution graph builders that compute gradient-based inter-feature dependencies, hierarchical spectral clustering for multi-resolution decomposition, and GNN meta-learning for topology prediction. The framework trains transcoders on 100M tokens to produce monosemantic features, builds attribution graphs between these features, applies spectral clustering to create hierarchical partitions, uses GNNs to predict circuit membership probabilities, and validates circuits through causal intervention (ablation and activation patching). This approach reduces circuit discovery complexity from exponential O(2^n) to polynomial O(n² log n) while achieving behavioral preservation of 82-97% on algorithmic tasks and 74-88% on natural language benchmarks across architectures from 117M to 70B parameters.

## Key Results
- Reduces circuit discovery complexity from O(2^n) to O(n² log n) via hierarchical decomposition
- Achieves 82-97% behavioral preservation on algorithmic tasks and 74-88% on natural language benchmarks
- Extracts circuits from models ranging from 117M to 70B parameters with 347 nodes and 8,923 edges for Llama-70B
- Shows 52-82% structural similarity in cross-architecture transfer experiments

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Complexity Reduction via Spectral Clustering
The hierarchical decomposition reduces circuit search complexity from O(2^n) to O(n² log n) by exploiting compositional structure in neural computations. Attribution graphs are recursively clustered using spectral decomposition of the normalized graph Laplacian L^(r) = I - D^(-1/2)A^(r)D^(-1/2). At each level, spectral clustering on the smallest eigenvectors partitions vertices into supernodes that minimize inter-cluster edges while preserving intra-cluster connectivity. Circuit search traverses this hierarchy rather than enumerating all subgraphs. Core assumption: neural computations exhibit compositional structure—high-level algorithms decompose into modular subroutines that further decompose into atomic operations.

### Mechanism 2: Cross-Layer Transcoders for Monosemantic Feature Extraction
Cross-layer transcoders produce sparse, interpretable features while explicitly modeling inter-layer dependencies. The encoder maps hidden states to sparse features via f^ℓ = TopK(ReLU(W_E^ℓ h^ℓ + b_E^ℓ), k). A prediction head models cross-layer structure via f̂^(ℓ+1) = σ(W_P^(ℓ→ℓ+1) f^ℓ). The composite loss L = Σ∥h^ℓ - D^ℓ f^ℓ∥² + λ₁∥f^(ℓ+1) - f̂^(ℓ+1)∥² + λ₂∥f^ℓ∥₁ jointly optimizes reconstruction, cross-layer prediction, and sparsity. Core assumption: TopK sparsity encourages monosemantic decomposition; cross-layer prediction captures computational dependencies between layers.

### Mechanism 3: GNN Meta-Learning for Topology Prediction
A graph neural network trained on circuit discovery tasks can predict circuit membership probabilities, guiding hierarchical search toward promising regions. The GNN uses graph attention layers with message passing z_v^(t+1) = MLP(z_v^t + Σ α_uv z_u^t) where attention weights α_uv = exp(a^T[z_u || z_v]) / Σ_w exp(a^T[z_w || z_v]). The output layer predicts membership probability per vertex. Meta-learning across tasks enables generalization to unseen models. Core assumption: circuit topology patterns transfer across architectures; ground-truth labels (from exhaustive search on small models or expert annotation) are available for training.

## Foundational Learning

- **Concept: Sparse Autoencoders and Dictionary Learning**
  - Why needed: Understanding how TopK/L1 regularization encourages monosemantic features, and the reconstruction-sparsity tradeoff
  - Quick check question: Why does TopK sparsity encourage each dictionary element to respond to a single interpretable concept, and what happens when k is set too high?

- **Concept: Spectral Clustering and Graph Laplacians**
  - Why needed: The hierarchical decomposition uses normalized Laplacian eigenvectors to partition vertices
  - Quick check question: Given L = I - D^(-1/2)AD^(-1/2), explain why the smallest non-zero eigenvectors identify partitions that minimize inter-cluster edge weights

- **Concept: Causal Intervention via Ablation and Activation Patching**
  - Why needed: Validation protocol distinguishes causal circuits from correlational artifacts
  - Quick check question: What is the difference between necessity testing (ablation) and sufficiency testing (restricted model evaluation), and why might validation circularity arise when the same data informs both discovery and validation?

## Architecture Onboarding

- **Component map:**
  1. **Cross-Layer Transcoder** → Encoders E^ℓ, decoders D^ℓ, prediction heads P^(ℓ→ℓ+1) producing sparse features f^ℓ
  2. **Attribution Graph Builder** → Computes gradient-based attributions A_{i→j} = (∂f_j/∂f_i) · f_i between features
  3. **Hierarchical Decomposer** → Spectral clustering creates multi-resolution hierarchy G^(0), G^(1), ..., G^(R)
  4. **GNN Circuit Searcher** → Predicts membership probabilities at each hierarchy level
  5. **Causal Validator** → Ablation (necessity) and restricted model (sufficiency) testing

- **Critical path:** Transcoder training (100M tokens) → Attribution graph construction O(L·m·s) → Hierarchical decomposition → GNN-guided search → Validation

- **Design tradeoffs:**
  - Sparsity k (32-128 optimal): Higher k improves reconstruction but reduces interpretability
  - Dictionary expansion m/d = 8: Larger dictionaries increase memory (65,536 features for Llama-70B)
  - Circuit size vs. preservation: Llama-70B circuits have 347 nodes/8,923 edges—may exceed human interpretability

- **Failure signatures:**
  - ACDC OOM on models >1.4B parameters (Table II)
  - Natural language tasks yield lower modularity (0.55-0.78) than algorithmic tasks
  - Cross-family transfer drops to 38-43% on WinoGrande (Table IV)
  - Attention circuits not modeled—critical gap for token-relationship tasks

- **First 3 experiments:**
  1. Reproduce GPT-2 Small modular arithmetic extraction (49 nodes, 97% preservation per Table II) to validate pipeline implementation
  2. Run "No Hierarchy" ablation (Table V) to confirm decomposition contributes ~2-4% preservation improvement
  3. Test Pythia-1.4B to Pythia-2.8B transfer on modular arithmetic to verify expected ~71% transfer coefficient before attempting cross-family experiments

## Open Questions the Paper Calls Out

### Open Question 1
Can attention circuit analysis be integrated with the MLP-focused HAGD framework to provide complete computational coverage?
Basis: The paper states "attention circuits are not modeled, leaving significant computational pathways unanalyzed" and identifies "integration of attention and MLP circuit analysis" as priority future work. Why unresolved: Current framework exclusively traces transcoder features from MLP layers; attention mechanisms perform substantial computation for token-to-token relationships that remain invisible to extraction. What evidence would resolve it: Successful extraction of unified circuits combining attention head patterns with MLP transcoder features, demonstrating behavioral preservation comparable to MLP-only circuits.

### Open Question 2
Does the 15-20% "reconstruction dark matter" in transcoder decomposition contain structured computation relevant to circuit analysis?
Basis: The paper acknowledges "if this residual variance contains structured computation rather than noise, important computational pathways may be invisible to circuit analysis." Why unresolved: Current methodology does not characterize what information resides in unexplained variance or whether it varies systematically across tasks and layers. What evidence would resolve it: Analysis of residual activations showing task-dependent structure, or demonstration that specific behaviors require features outside the reconstructed subspace.

### Open Question 3
Do universal computational motifs exist across diverse architectures, training regimes, and task domains?
Basis: The authors note "claims of 'universal' computational motifs would require substantially broader evidence" beyond the observed 52%-82% structural similarity. Why unresolved: Partial overlap indicates 18%-48% architecture-specific structure; current evidence limited to three model families trained on similar data distributions. What evidence would resolve it: Systematic circuit extraction across fundamentally different architectures (e.g., state-space models, mixture-of-experts) showing convergence to shared topological patterns.

### Open Question 4
How can validation circularity be eliminated when ablation experiments used for circuit discovery also serve for validation?
Basis: The paper identifies that "features may appear necessary because their ablation disrupts computation in ways unrelated to the target behavior." Why unresolved: The causal validation protocol assumes circuit completeness, creating potential circular reasoning where discovery and validation share methodological assumptions. What evidence would resolve it: Development of independent validation protocols (e.g., out-of-distribution behavioral prediction, adversarial circuit perturbation) that do not rely on ablation-based fidelity metrics.

## Limitations

- **Attention circuit exclusion**: The methodology explicitly omits attention mechanisms, creating a 15-20% "dark matter" gap that significantly limits applicability to natural language tasks
- **Ground-truth label scarcity**: GNN meta-learning depends on circuit membership annotations from "exhaustive search on small models or expert annotation" without specified methodology
- **Cross-architecture transfer variability**: Performance drops to 38-43% on WinoGrande when transferring between different architectures, indicating architectural differences may exceed methodological artifacts

## Confidence

**High confidence claims:**
- Hierarchical decomposition reduces complexity from O(2^n) to O(n² log n) via spectral clustering
- Cross-layer transcoders produce interpretable sparse features with 82-97% behavioral preservation on algorithmic tasks
- Cross-family transfer shows moderate similarity (52-82%) for algorithmic tasks

**Medium confidence claims:**
- GNN meta-learning improves circuit discovery beyond hierarchical search alone (lacks ablation evidence)
- 15-20% dark matter represents structured computation rather than noise
- Modularity scores (0.55-0.78 for natural language) indicate meaningful circuit organization

**Low confidence claims:**
- Universal applicability across all LLM scales and tasks (attention omission limits validity)
- Causal validity of extracted circuits (validation protocol may be circular)

## Next Checks

1. **Ablation of GNN component**: Run circuit extraction with and without GNN guidance on GPT-2 Small modular arithmetic. Compare node/edge counts and behavioral preservation to quantify GNN contribution beyond hierarchical search alone.

2. **Attention circuit reconstruction**: Extend attribution graph builder to include attention head contributions. Extract circuits for pronoun resolution tasks and measure behavioral preservation to quantify the "dark matter" gap.

3. **Cross-validation protocol robustness**: Split benchmark datasets in half—use one half for circuit discovery and the other for validation. Compare results to current protocol to assess potential circularity in causal validation.