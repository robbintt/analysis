---
ver: rpa2
title: The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large
  Language Models with a Multi-Framework Approach
arxiv_id: '2504.19255'
source_url: https://arxiv.org/abs/2504.19255
tags:
- moral
- ethical
- reasoning
- scores
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces PRIME, a multi-framework methodology for
  analyzing LLM moral reasoning across consequentialist-deontological frameworks,
  moral foundations theory, and Kohlberg's developmental stages. Applying PRIME to
  six leading LLMs revealed consistent prioritization of care/harm and fairness/cheating
  foundations (mean scores 3.3-4.1) while authority, loyalty, and sanctity received
  notably lower emphasis (1.1-1.7).
---

# The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach

## Quick Facts
- **arXiv ID:** 2504.19255
- **Source URL:** https://arxiv.org/abs/2504.19255
- **Reference count:** 0
- **Key outcome:** Multi-framework analysis reveals consistent LLM prioritization of care/harm and fairness/cheating foundations over authority, loyalty, and sanctity.

## Executive Summary
This study introduces PRIME, a multi-framework methodology for analyzing LLM moral reasoning across consequentialist-deontological frameworks, moral foundations theory, and Kohlberg's developmental stages. Applying PRIME to six leading LLMs revealed consistent prioritization of care/harm and fairness/cheating foundations (mean scores 3.3-4.1) while authority, loyalty, and sanctity received notably lower emphasis (1.1-1.7). Models demonstrated decisive ethical judgments with varying confidence levels (71-86) and showed strong cross-model alignment in moral decisions. The methodology successfully identified systematic patterns in moral foundation preferences and reasoning sophistication while revealing how different models express ethical decision-making reluctance.

## Method Summary
The PRIME framework triangulates LLM moral reasoning across three distinct ethical lenses: consequentialist-deontological analysis, Moral Foundations Theory (MFT) scoring, and Kohlberg's stages of moral development. The methodology employs standardized prompt banks (Trolley, Heinz, Lifeboat scenarios) combined with dual-protocol evaluation (direct questioning and response analysis). Independent raters score LLM outputs across six moral foundation dimensions and six developmental stages, while also measuring confidence levels and reluctance to engage with ethical questions. Nudging protocols test the robustness of model safeguards when ethical boundaries are challenged.

## Key Results
- All evaluated models consistently prioritized care/harm and fairness/cheating foundations with mean scores of 3.3-4.1
- Authority, loyalty, and sanctity foundations received notably lower emphasis (1.1-1.7 mean scores)
- Models demonstrated decisive ethical judgments with confidence levels ranging from 71-86
- Strong cross-model alignment in moral decisions revealed systematic patterns in foundation preferences
- Negative correlation (-2.74 slope) between confidence levels and moral reasoning sophistication

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A multi-framework analytical approach (PRIME) reveals consistent moral priorities in LLMs that single-framework analyses might miss.
- **Mechanism:** By triangulating across three distinct ethical lenses—consequentialist-deontological, Moral Foundations Theory (MFT), and Kohlberg's stages—the framework isolates specific reasoning patterns. For example, while a model might appear neutral in a binary choice, MFT scoring can reveal a strong "care/harm" bias (mean scores 3.3-4.1) versus "authority" (1.1-1.7).
- **Core assumption:** LLMs possess stable, quantifiable "moral preferences" that can be captured through text analysis, rather than generating stochastic or context-dependent ad-hoc justifications.
- **Evidence anchors:**
  - [abstract] "comprehensive methodology for analyzing moral priorities across foundational ethical dimensions"
  - [section] Page 2: "This framework focuses not on evaluating the correctness... but on systematically analyzing how these systems explain and justify their choices."
  - [corpus] "Structured Moral Reasoning in Language Models" (neighbor paper) supports the need for multi-dimensional evaluation to overcome shallow reasoning patterns.
- **Break condition:** If LLM outputs are purely stochastic or heavily prompt-dependent without internal consistency, cross-framework triangulation would yield noisy, non-correlated data rather than the observed "striking patterns of convergence."

### Mechanism 2
- **Claim:** Observed "convergent ethics" (alignment between models and humans) is driven primarily by prioritization of "individualizing" foundations (Care, Fairness) over "binding" foundations (Authority, Loyalty, Sanctity).
- **Mechanism:** LLMs likely mirror the liberal-democratic values prevalent in their Western-centric training data and RLHF (Reinforcement Learning from Human Feedback) processes. The paper notes this results in high scores for harm prevention and equity, but low scores for deference to hierarchy or tradition.
- **Core assumption:** The lower scores for Authority/Loyalty represent a genuine model "preference" or bias, rather than an inability to comprehend complex social hierarchies or nuance in the prompts.
- **Evidence anchors:**
  - [abstract] "all evaluated models demonstrate strong prioritization of care/harm and fairness/cheating foundations while consistently underweighting authority, loyalty, and sanctity"
  - [section] Page 13, Table 2: Shows Authority scores ranging 1.1-1.7 vs Care scores 3.6-3.9.
  - [corpus] Weak/missing explicit mechanism evidence in corpus; neighbor papers focus on evaluation frameworks rather than the specific cause of this specific "individualizing" bias.
- **Break condition:** If models were trained on significantly different cultural data distributions (e.g., emphasized in "BengaliMoralBench" neighbor), the convergence on Care/Fairness dominance would likely weaken or invert.

### Mechanism 3
- **Claim:** High confidence in moral decisions is not a reliable proxy for sophisticated moral reasoning (Kohlberg score).
- **Mechanism:** Models may exhibit "calibrated uncertainty" or "epistemic humility" where lower confidence correlates with higher reasoning complexity (higher Kohlberg stages). Conversely, high confidence (e.g., from Gemini) might indicate simple heuristic matching rather than deep reasoning.
- **Core assumption:** The negative correlation (slope -2.74) implies the models "know what they don't know," rather than simply being confused by complex prompts.
- **Evidence anchors:**
  - [section] Page 9: "The scatter plot... reveals a slight negative correlation... slope = -2.74... models tend to express somewhat lower confidence when engaging with more complex moral reasoning scenarios."
  - [section] Page 10: "Gemini and Perplexity (highest confidence) did not necessarily demonstrate the most sophisticated moral reasoning."
  - [corpus] "Rule-Based Moral Principles for Explaining Uncertainty" (neighbor) aligns with the need to interpret model confidence/uncertainty ethically.
- **Break condition:** If the negative correlation were an artifact of poor prompt formatting for complex questions (causing the model to default to low confidence), the link between confidence and reasoning sophistication would be spurious.

## Foundational Learning

- **Concept: Moral Foundations Theory (MFT)**
  - **Why needed here:** This is the primary lens for quantifying LLM outputs. Without understanding the six axes (Care, Fairness, Loyalty, Authority, Sanctity, Liberty), you cannot interpret the radar charts or the "convergent ethics" claim.
  - **Quick check question:** Can you explain why "Care" and "Fairness" are often grouped as "individualizing" foundations compared to "binding" foundations like "Authority"?

- **Concept: Kohlberg's Stages of Moral Development**
  - **Why needed here:** The paper uses this scale (1-6) to grade the *sophistication* of the model's reasoning, not just the decision itself. It distinguishes a "because I'll be punished" (Pre-conventional) rationale from a "universal principles" (Post-conventional) rationale.
  - **Quick check question:** A model refuses to steal bread even to save a life because "stealing is illegal." Which Kohlberg stage does this reflect? (Answer: Stage 4, maintaining social order).

- **Concept: Theory of Mind in Game Theory**
  - **Why needed here:** The paper utilizes Prisoner's Dilemma and Dictator Games. Understanding that these require modeling an "other" is crucial, as the paper notes LLMs failed to cooperate where humans often do, potentially indicating a lack of reciprocal expectation or "Theory of Mind."
  - **Quick check question:** In the Prisoner's Dilemma results, why might "None of the LLMs chose cooperation" suggest a lack of Theory of Mind or a hyper-risk-averse alignment?

## Architecture Onboarding

- **Component map:** Standardized prompts (Trolley, Heinz, Lifeboat) -> PRIME Framework (3 lenses: consequentialist-deontological, MFT, Kohlberg) -> Dual-protocol evaluation (Direct Questioning + Response Analysis) -> Metrics output (Moral Foundation Scores, Kohlberg Scores, Confidence, Reluctance)

- **Critical path:**
  1. **Standardize Prompts:** Ensure exact consistency across models (Page 7)
  2. **Elicit Response:** Capture initial decision + reasoning + confidence score
  3. **Apply PRIME Scoring:** Independent raters (or automated NLP) score the response against MFT and Kohlberg rubrics
  4. **Nudge/Reluctance Test:** If the model refuses or hesitates, apply "nudging" to see if safeguards break (Page 11)

- **Design tradeoffs:**
  - **Quant vs. Qual:** The framework relies on independent raters (Cohen's kappa) for validity (Page 7). Replacing this with fully automated NLP scoring scales faster but risks missing nuance in "reasoning chains."
  - **Ground Truth:** The paper explicitly avoids "correctness" (Page 2). If you need a system to judge *right vs. wrong*, this architecture is unsuitable; it only maps *priorities*.

- **Failure signatures:**
  - **False Confidence:** High confidence (>80) with low Kohlberg scores (simple reasoning)
  - **Nudge Fragility:** "Guardrails" that dissolve under slight prompt modification (Page 12), indicating the safety layer is superficial rather than intrinsic to the reasoning
  - **Rogue Dimensions:** Authority/Sanctity scores near zero (1.1-1.7) indicate the model may fail to reason about non-Western or traditionalist scenarios

- **First 3 experiments:**
  1. **Reproduce the MFT Radar:** Run the standard Trolley/Heinz prompts against your target model and score *only* the MFT dimensions to see if it replicates the "Care/Fairness" dominance
  2. **Test Nudge Susceptibility:** Take a "refusal" response (high reluctance) and apply the paper's "nudging" strategy to measure how quickly the model abandons its ethical stance
  3. **Correlate Confidence vs. Complexity:** Plot the model's self-reported confidence against the complexity of the prompt (or proxy Kohlberg score) to check for the "negative correlation" mechanism mentioned on Page 9

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes LLMs possess stable moral preferences rather than context-dependent stochastic responses, though the observed convergence patterns provide some validation
- The Western-centric training data bias may artificially inflate individualizing foundation scores while suppressing binding foundations, potentially limiting generalizability to non-Western ethical frameworks
- The reliance on independent raters for scoring introduces inter-rater variability, though Cohen's kappa values suggest acceptable agreement

## Confidence
- **High Confidence:** Observed convergence patterns across six models and the consistent prioritization of care/harm and fairness/cheating foundations (Mechanism 1)
- **Medium Confidence:** The mechanism linking training data bias to individualizing foundation dominance (Mechanism 2) requires further validation across diverse cultural datasets
- **Medium Confidence:** The negative correlation between confidence and moral reasoning sophistication (Mechanism 3) appears robust but may be influenced by prompt complexity artifacts

## Next Checks
1. **Cultural Validation:** Apply PRIME to LLMs trained on non-Western datasets to test whether the Care/Fairness dominance persists across different cultural contexts
2. **Temporal Stability:** Test the same models at different time points to verify whether moral foundation priorities remain stable or shift with fine-tuning
3. **Automated Scoring Validation:** Replace human raters with automated NLP scoring systems to assess scalability while maintaining scoring validity through cross-validation studies