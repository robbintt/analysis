---
ver: rpa2
title: Development and Enhancement of Text-to-Image Diffusion Models
arxiv_id: '2503.05149'
source_url: https://arxiv.org/abs/2503.05149
tags:
- images
- diffusion
- image
- training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research focused on improving text-to-image diffusion models,
  addressing issues of limited sample diversity and training instability. The study
  employed advanced techniques including Classifier-Free Guidance (CFG) and Exponential
  Moving Average (EMA) to enhance the performance of Hugging Face's state-of-the-art
  text-to-image generation model.
---

# Development and Enhancement of Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2503.05149
- Source URL: https://arxiv.org/abs/2503.05149
- Reference count: 20
- Primary result: Enhanced text-to-image diffusion model achieves FID score of 1088.94 compared to baseline 1332.33 using CFG and EMA

## Executive Summary
This research addresses limited sample diversity and training instability in text-to-image diffusion models by implementing Classifier-Free Guidance (CFG) and Exponential Moving Average (EMA) techniques. The enhanced model was trained on CIFAR-100 and evaluated using FID scores, showing significant improvements in image quality, detail, and alignment with textual descriptions. The study establishes new benchmarks in generative AI and provides a foundation for future research in text-to-image generation applications.

## Method Summary
The study employed transfer learning on Hugging Face's CompVis/stable-diffusion-v1-4 model, fine-tuning only the UNet component on CIFAR-100 dataset. CFG was implemented by conditioning the model on both actual text embeddings and unconditional embeddings, while EMA stabilized training by averaging model parameters over time. The model was trained for 5 epochs using AdamW optimizer with learning rate 1×10⁻⁷, and evaluated using Fréchet Inception Distance scores. Key components included CLIPTokenizer, CLIPTextModel, AutoencoderKL, UNet2DConditionModel, and DDPMScheduler.

## Key Results
- FID score improved from 1332.33 (baseline) to 1088.94 (enhanced model)
- Enhanced model produced images with better detail, diversity, and alignment with textual descriptions
- CFG and EMA techniques successfully addressed training instability and limited sample diversity
- Qualitative assessments demonstrated significant improvements in generated image quality

## Why This Works (Mechanism)

### Mechanism 1: Classifier-Free Guidance (CFG)
CFG improves text-image alignment by interpolating between conditional and unconditional noise predictions during sampling. During training, the model learns to denoise both with text embeddings and unconditionally. At inference, the final prediction extrapolates away from unconditional toward conditional using the formula: ε̂ = εθ(xt, t, c) + w(εθ(xt, t, c) − εθ(xt, t, ∅)), where w controls guidance strength. This amplifies text-relevant features without requiring an external classifier.

### Mechanism 2: Exponential Moving Average (EMA)
EMA maintains a time-averaged copy of model parameters to produce more stable final weights with better generalization. After each training step, EMA parameters are updated using: θEMA = αθEMA + (1 − α)θ, where α determines smoothing strength. This reduces weight fluctuations around the optimal point while preserving the signal, leading to smoother convergence and reduced training variability.

### Mechanism 3: Latent Diffusion with Cross-Attention Conditioning
The architecture compresses images into latent space via VAE and conditions denoising on text embeddings for efficient, high-fidelity generation. VAE encoder maps images to lower-dimensional latents; UNet denoises latents step-by-step using DDPMScheduler; text embeddings from CLIPTextModel inject semantic guidance via cross-attention layers within UNet. This enables efficient computation while retaining perceptually relevant information.

## Foundational Learning

- **Concept: Forward and Reverse Diffusion Process**
  - Why needed: The entire generative mechanism depends on understanding how noise is added (forward) and learned to be removed (reverse) over timesteps
  - Quick check: Can you explain why diffusion models learn the reverse process p(xt−1|xt) rather than the forward process?

- **Concept: Guidance Scale Trade-offs**
  - Why needed: CFG introduces a hyperparameter w that directly trades off between prompt adherence and sample diversity/quality
  - Quick check: What happens to image diversity as you increase the CFG scale from 1.0 to 15.0?

- **Concept: Transfer Learning vs. Full Fine-Tuning**
  - Why needed: This project uses transfer learning on the UNet component rather than training from scratch or using LoRA—understanding why affects replication
  - Quick check: Why might full fine-tuning of Stable Diffusion on a small dataset (50K images) cause overfitting compared to transfer learning?

## Architecture Onboarding

- **Component map:** CLIPTokenizer → CLIPTextModel → text embeddings → UNet2DConditionModel (with cross-attention) → AutoencoderKL decoder → output image; DDPMScheduler orchestrates timesteps; EMA maintains shadow copy of UNet weights

- **Critical path:** 1) Verify VAE encode/decode fidelity on CIFAR-100 samples before training 2) Validate text embeddings are correctly shaped and passed to UNet cross-attention 3) Confirm CFG correctly computes both conditional and unconditional forward passes during sampling

- **Design tradeoffs:** Dataset size vs. overfitting: CIFAR-100 (50K, 64×64) is small relative to Stable Diffusion's pretraining data; 5 epochs chosen to prevent overfitting. CFG scale w: Higher w improves prompt adherence but risks artifacts. EMA decay α: Higher decay = more smoothing but slower adaptation.

- **Failure signatures:** Mode collapse (limited diversity): May indicate CFG scale too high or training instability—check sample variance across seeds. Blurry/incoherent outputs: Likely VAE reconstruction issue or insufficient denoising steps. Prompt ignored: CFG not properly implemented (verify unconditional pass is empty string). Training divergence: Learning rate too high for fine-tuning.

- **First 3 experiments:** 1) Baseline sanity check: Generate images with pretrained Stable Diffusion v1-4 on CIFAR-100 prompts without CFG/EMA; record FID and visual quality. 2) Ablate CFG: Enable CFG with varying scales (w = 1, 3, 7, 12) while keeping EMA disabled; measure FID and prompt alignment. 3) Ablate EMA: Train with EMA enabled but CFG disabled (w = 1); compare training loss curves and final FID to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will the stability and diversity improvements from CFG and EMA persist when the model is trained on larger, higher-resolution datasets?
- Basis: The Future Directions section states, "One critical direction involves exploring additional datasets to validate the robustness and generalization capabilities... on diverse and larger datasets."
- Why unresolved: Current study utilized CIFAR-100 (50K images), which may not reflect challenges in industrial-scale datasets like LAION.
- Evidence needed: Successful training and evaluation on large-scale benchmarks (e.g., MS-COCO) with maintained/improved FID scores.

### Open Question 2
- Question: Can the architecture be optimized to support real-time text-to-image generation for interactive applications?
- Basis: Authors identify "real-time text-to-image generation" as future innovation with "profound implications for interactive applications" and "conversational AI systems."
- Why unresolved: Current research prioritized output quality and training stability, but did not evaluate inference latency or computational efficiency.
- Evidence needed: Latency benchmarks demonstrating model's ability to generate images instantaneously in response to text inputs.

### Open Question 3
- Question: Is the reported Fréchet Inception Distance (FID) reduction statistically reliable given the extremely small evaluation sample size?
- Basis: Methodology states FID was "calculated by comparing the six generated images," whereas standard evaluation requires thousands of samples.
- Why unresolved: FID score calculated on only six images is highly susceptible to outliers and random variance, making comparison potentially inconclusive.
- Evidence needed: Re-calculating FID scores using statistically significant sample size (e.g., full CIFAR-100 test set of 10,000 images).

## Limitations
- Specific CFG scale (w) and EMA decay (α) hyperparameters are not specified, making exact replication challenging
- Text prompts used to condition on CIFAR-100 classes are not provided, creating ambiguity about conditioning methodology
- Qualitative improvements lack quantitative backing or systematic human evaluation methodology
- No training/validation loss curves or ablations provided to confirm isolated contributions of CFG versus EMA

## Confidence
- **High confidence**: Architectural components (VAE, UNet, CLIP, CFG equation, EMA equation) are correctly implemented based on standard diffusion model literature
- **Medium confidence**: Reported FID improvement is plausible given methodology, but without specified hyperparameters and exact prompts, magnitude cannot be independently verified
- **Low confidence**: Claims about qualitative improvements (better detail, diversity, and alignment) lack quantitative backing or systematic evaluation methodology

## Next Checks
1. **Hyperparameter sensitivity analysis**: Replicate experiment across range of CFG scales (w = 1, 3, 7, 12) and EMA decay rates (α = 0.999, 0.9999, 0.99999) to identify optimal values and understand isolated effects on FID and visual quality
2. **Ablation study**: Train three variants—baseline (no CFG/EMA), CFG-only, and EMA-only—using identical seeds and prompts to quantify each component's contribution to reported improvements
3. **Diversity quantification**: Beyond FID, measure sample diversity using metrics like LPIPS distance between generated samples and Frechet Video Distance (FVD) to empirically verify claim of improved diversity rather than just prompt alignment