---
ver: rpa2
title: 'Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture
  on Unbounded Data'
arxiv_id: '2512.12325'
source_url: https://arxiv.org/abs/2512.12325
tags:
- regret
- data
- bound
- wealth
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves a pathwise (deterministic) regret bound for\
  \ Robbins' sub-Gaussian mixture strategy on unbounded data. The regret is bounded\
  \ by O(ln ln Vt) on a \"Ville event\" E\u03B1 where the mixture wealth process remains\
  \ bounded by log(1/\u03B1), and this event has probability at least 1-\u03B1 under\
  \ various stochastic assumptions (sub-Gaussian, symmetric, variance-bounded distributions)."
---

# Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data

## Quick Facts
- **arXiv ID:** 2512.12325
- **Source URL:** https://arxiv.org/abs/2512.12325
- **Reference count:** 40
- **Key outcome:** Proves $O(\ln\ln V_T)$ regret for Robbins' sub-Gaussian mixture on unbounded data, valid on a measure-one event where mixture wealth remains bounded

## Executive Summary
This paper establishes almost sure $O(\ln\ln T)$ regret bounds for a sub-Gaussian mixture betting strategy on unbounded data sequences. The key innovation is proving these bounds hold on a measure-one event where the mixture wealth process remains bounded, rather than on all possible paths. By conditioning on high-probability "Ville events" where wealth doesn't explode, the analysis achieves the optimal LIL-type regret rate while maintaining robustness to unknown distributions. This bridges adversarial online learning frameworks with stochastic betting approaches.

## Method Summary
The method uses a mixture martingale approach with Robbins' heavy-tailed prior $\pi(\eta) \sim 1/(\eta (\ln \eta) (\ln \ln \eta)^2)$. The mixture wealth $W_t = \int \exp(\eta S_t - \eta^2 V_t/2) \pi(\eta) d\eta$ is computed via numerical integration over $\eta \in [-1,1]$. Regret is defined as $R_t = L^*_t - \ln W_t$, where $L^*_t$ is maximum hindsight wealth. The strategy tracks the optimal betting parameter tightly enough to achieve $O(\ln\ln V_t)$ regret on a measure-one event where wealth remains bounded by $1/\alpha$.

## Key Results
- Proves $O(\ln\ln V_T)$ regret bound on a "Ville event" $E_\alpha$ with probability $\ge 1-\alpha$
- Achieves almost sure $O(\ln\ln V_T)$ regret on a measure-one event $E_0$
- Extends classical mixture martingale methods to unbounded domains while maintaining pathwise guarantees
- Shows conditional regret bounds bridge adversarial online learning with stochastic betting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If data paths belong to a specific "Ville event" ($E_\alpha$), regret is deterministically bounded by $O(\ln \ln V_T)$, even for unbounded data.
- **Mechanism:** The strategy defines a wealth process $W_t$ (a mixture supermartingale). Ville's inequality implies that with high probability ($1-\alpha$), this wealth remains bounded by $1/\alpha$. By conditioning on this high-probability event (the set of paths where wealth doesn't explode), the analysis excludes the worst-case paths that would otherwise force regret to be linear or unbounded.
- **Core assumption:** The data generating process allows $W_t$ to act as a non-negative supermartingale (e.g., conditionally sub-Gaussian), ensuring the "Ville event" has measure $\ge 1-\alpha$.
- **Evidence anchors:**
  - [abstract] "For every path in a natural 'Ville event' $E_\alpha$, this regret till time $T$ is bounded by..."
  - [section 4.1.3] "...if the data are stochastic... $(W_t)$ is also a non-negative super-martingale. The last statement then follows from Ville's inequality."
  - [corpus] The neighbor paper *"Extensions of Robbins-Siegmund Theorem..."* supports the foundational role of almost supermartingale convergence in stochastic optimization.
- **Break condition:** If the specific wealth process $W_t$ grows without bound (leaves $E_\alpha$), the regret guarantee fails, potentially growing linearly with $V_t$.

### Mechanism 2
- **Claim:** Using Robbins' heavy-tailed prior (specifically $\pi(\eta) \sim 1/(\eta (\ln \eta) (\ln \ln \eta)^2)$) allows the mixture to track the optimal bet tightly enough to achieve the double-logarithmic rate.
- **Mechanism:** The regret bound depends on the ratio of the mixture integral to the optimal likelihood. This specific prior shape balances placing mass near zero (for safety) with placing enough mass at $\eta^*$ to minimize the "cost" of adaptation. The prior mass decays slowly enough to cover the parameter space efficiently as variance $V_t$ grows.
- **Core assumption:** The optimal betting parameter $\eta^*_t$ does not drift permanently outside the effective support of the prior (specifically, $|S_t|/V_t \le 1$ eventually).
- **Evidence anchors:**
  - [section 4] "For the mixture wealth using Robbins' heavy-near-0 prior, we show that the bound on regret... is $O(\ln \ln V_t)$."
  - [section 4.1.1] Lemma 4.3 and 4.4 explicitly link the regret bound to the mass the prior places on an interval around the optimizer $\eta^*_t$.
  - [corpus] Specific corpus evidence for this exact prior mechanism is weak/missing in neighbors; neighbors focus on generic regret or SGD.
- **Break condition:** If $|S_t|/V_t > 1$ (the drift is too large relative to variance), the regret bound switches to a third branch that can be arbitrarily large (linear).

### Mechanism 3
- **Claim:** The mechanism uses cumulative variance $V_t$ rather than time $t$ as the intrinsic clock for the regret bound.
- **Mechanism:** The strategy self-normalizes the sum of observations $S_t$ by the cumulative variance $V_t$. This allows the "concentration" to tighten based on the actual information content (volatility) observed, rather than just the number of steps. This is critical for handling unbounded domains where variance may not scale linearly with time.
- **Core assumption:** The variance process $V_t$ is nonnegative, nondecreasing, and goes to infinity (eventually) to drive the regret bound down.
- **Evidence anchors:**
  - [abstract] "...regret grows only as $O(\ln \ln V_T)$, where $V_T$ is a cumulative variance process."
  - [section 2.1] Defines $V_t$ as a function of $X_1, \dots, X_t$ that is nonnegative and increasing.
  - [corpus] *"Concentration Inequalities for Stochastic Optimization..."* discusses sample-dependent bounds, analogous to using $V_t$ over $t$.
- **Break condition:** If $V_t$ remains bounded (low volatility regime), the $\ln \ln V_T$ term may not dominate, preventing the asymptotic tightening of the bound.

## Foundational Learning

- **Concept: Ville's Inequality**
  - **Why needed here:** This is the mathematical gatekeeper. It connects the stochastic property of the data (supermartingale) to the deterministic bound on the specific subset of paths ($E_\alpha$).
  - **Quick check question:** Can you explain why bounding the *supremum* of a non-negative supermartingale allows us to make probabilistic claims about "good" paths?

- **Concept: Mixture Martingales (Method of Mixtures)**
  - **Why needed here:** The core algorithm integrates simple betting strategies over a prior. Understanding how mixing $\exp(\eta S_t - \eta^2 V_t/2)$ creates a process robust to unknown drift is essential.
  - **Quick check question:** How does the mixture wealth $W_t$ differ from a simple maximum-likelihood wealth process in terms of robustness?

- **Concept: Sub-Gaussian Processes**
  - **Why needed here:** The paper generalizes beyond standard Gaussian assumptions. You must understand that "sub-Gaussian" effectively means "tail decay is dominated by Gaussian," which defines the variance proxy $V_t$.
  - **Quick check question:** Does a sub-Gaussian assumption require the data to have finite variance? (Check Appendix A for the answer regarding symmetric distributions).

## Architecture Onboarding

- **Component map:** Data stream $X_t$ -> Running Sum $S_t$ and Cumulative Variance $V_t$ -> Mixture Wealth $W_t$ (via numerical integration) -> Regret Bound $R_t$ and Confidence Sequence
- **Critical path:** Calculating the mixture integral (Eq. 6). This is the computational bottleneck. The theoretical bound relies on this specific heavy-tailed integration.
- **Design tradeoffs:**
  - **Robbins Prior vs. Gaussian Prior:** The paper notes a trade-off. Gaussian mixtures yield tighter bounds early (small $t$) but $O(\ln T)$ regret eventually. Robbins prior yields $O(\ln \ln T)$ regret eventually (optimal) but may be looser initially.
  - **Assumption:** You trade "validity on all paths" (impossible for unbounded) for "validity on high-probability paths" ($E_\alpha$).
- **Failure signatures:**
  - **Linear Regret:** If you observe linear growth in regret, check if $|S_t|/V_t > 1$. This indicates the "third branch" of Theorem 4.1, where the prior cannot track the optimizer.
  - **Exploding Wealth:** If $W_t$ hits the threshold $1/\alpha$, you have exited the Ville event $E_\alpha$, and the guarantees void.
- **First 3 experiments:**
  1. **Validating the LIL Rate:** Generate i.i.d. Gaussian data and plot $\ln \ln V_t$ vs. actual regret $R_t$ to verify the asymptotic tracking.
  2. **Stress Testing the Ville Event:** Inject adversarial outliers to force $W_t$ large. Verify that regret bounds fail only when $W_t$ exceeds $1/\alpha$.
  3. **Comparing Priors:** Run the Gaussian mixture (Section 3) vs. Robbins mixture (Section 4) on the same stream. Plot the width of the derived Confidence Sequences over time to visualize the $O(\ln T)$ vs. $O(\ln \ln T)$ crossover.

## Open Questions the Paper Calls Out
None

## Limitations
- Regret guarantees hold only on specific "Ville events" rather than all paths, restricting applicability
- Heavy-tailed Robbins prior requires careful numerical integration near η = 0
- Constant c ≥ 6.6e is specified but not optimized for practical performance
- Results assume variance proxy V_t grows unboundedly, which may not hold in low-volatility regimes

## Confidence
- **High confidence:** The mechanism linking Ville's inequality to conditional regret bounds (Mechanism 1) is mathematically rigorous and well-supported
- **Medium confidence:** The asymptotic O(ln ln V_t) regret rate (Mechanism 2) follows logically from the prior construction
- **Medium confidence:** The use of cumulative variance V_t as the intrinsic clock (Mechanism 3) is clearly specified but needs empirical validation

## Next Checks
1. **Numerical Integration Stability**: Implement and test the mixture integral computation for W_t, particularly near the singularity at η = 0, using both Gaussian and Robbins priors on synthetic data
2. **Cross-Prior Comparison**: Generate empirical plots comparing the Gaussian mixture (O(ln T)) versus Robbins mixture (O(ln ln T)) on identical stochastic streams, measuring both regret and confidence sequence width over time
3. **Adversarial Stress Testing**: Construct data sequences that maximize |S_t|/V_t > 1 to verify the switch to linear regret, and sequences that cause W_t to exceed 1/α to confirm breakdown of Ville event guarantees