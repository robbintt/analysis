---
ver: rpa2
title: 'GeoLaux: A Benchmark for Evaluating MLLMs'' Geometry Performance on Long-Step
  Problems Requiring Auxiliary Lines'
arxiv_id: '2508.06226'
source_url: https://arxiv.org/abs/2508.06226
tags:
- uni00000011
- step
- auxiliary
- error
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoLaux addresses the limitations of existing geometry benchmarks
  by introducing a comprehensive dataset of 2,186 plane geometry problems that require
  long-step reasoning (average 6.51 steps, up to 24 steps) and auxiliary line construction
  (41.8% of problems). The benchmark features a novel five-dimensional evaluation
  framework assessing answer correctness, process correctness, process quality, auxiliary
  line impact, and error types.
---

# GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines

## Quick Facts
- **arXiv ID**: 2508.06226
- **Source URL**: https://arxiv.org/abs/2508.06226
- **Reference count**: 40
- **Primary result**: Introduces GeoLaux benchmark with 2,186 geometry problems requiring long-step reasoning (average 6.51 steps) and auxiliary line construction, revealing MLLMs' severe performance degradation in extended reasoning tasks.

## Executive Summary
GeoLaux introduces a comprehensive benchmark to evaluate multimodal large language models' (MLLMs) geometry reasoning capabilities, particularly for long-step problems requiring auxiliary line construction. The benchmark features 2,186 plane geometry problems from Chinese Zhongkao exams with an average of 6.51 reasoning steps and includes a novel five-dimensional evaluation framework. Experiments with 13 state-of-the-art MLLMs reveal that models exhibit severe performance degradation in extended reasoning steps, tend to take shortcuts in proving problems compared to calculation problems, and lack auxiliary line awareness. The benchmark provides critical insights into MLLMs' geometric reasoning limitations and establishes a foundation for improving spatial reasoning capabilities in AI systems.

## Method Summary
GeoLaux employs a five-dimensional evaluation framework assessing answer correctness (ACS), process correctness (PCS), process quality (PQS), auxiliary line impact, and error types. The dataset contains 2,186 plane geometry problems (1,418 calculation, 768 proving) with average 6.51 steps and 41.8% requiring auxiliary lines. Evaluation uses o4-mini as automated step-by-step scorer with reference solutions. Models generate solutions in JSON format with one-shot prompting, then receive binary step scoring and error classification. PQS uses exponential weighting (α=3.5) to differentiate reasoning quality, while auxiliary line evaluation compares performance with/without heuristic prompts. The benchmark includes GeoLaux-mini (330 problems) for testing expensive models.

## Key Results
- MLLMs show severe performance degradation in extended reasoning steps, with nine models exhibiting over 50% PCS drop on problems exceeding 9 steps
- Models demonstrate "laziness" in proving problems, achieving high ACS but low PCS by leveraging given conclusions rather than full derivation
- Thinking models universally benefit from auxiliary line prompts (+39.3 PCS for o3-mini), while non-thinking models show minimal improvement (+3.6 PCS for best non-thinking)
- Auxiliary line construction capability enhancement proves particularly beneficial for overall geometry reasoning improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponentially-weighted process scoring (PQS) differentiates reasoning quality more sensitively than binary correctness.
- Mechanism: Early errors receive higher penalty via exponential decay (weight yi = e^{-i/n}), then tanh normalization spreads scores across [0,1]. This captures that an error at step 2 is worse than at step 12, but with diminishing marginal differences.
- Core assumption: Earlier errors propagate through subsequent steps, making them more consequential for solution quality.
- Evidence anchors:
  - [abstract] "five-dimensional evaluation framework assessing...process quality"
  - [section 4.2] "PQS = tanh(α(PQS′ − 1)) + 1, where α = 3.5"
  - [corpus] Weak/missing - no corpus papers validate this specific weighting scheme.
- Break condition: If errors are independent (early mistakes don't affect later steps), the exponential weighting over-penalizes early failures.

### Mechanism 2
- Claim: Auxiliary line hints selectively boost thinking models by providing spatial reasoning scaffolds they can integrate.
- Mechanism: When models receive auxiliary line construction text + diagram alongside the original problem, thinking models (o3-mini: +39.3 PCS) show large gains while non-thinking models show minimal improvement (+3.6 PCS for best non-thinking). Thinking models appear to treat hints as reasoning starting points; non-thinking models may lack the capacity to incorporate them into coherent chains.
- Core assumption: Thinking models have internal mechanisms to evaluate and integrate external spatial suggestions into their reasoning traces.
- Evidence anchors:
  - [abstract] "enhancing this capability proves particularly beneficial for overall geometry reasoning improvement"
  - [section 5.4] "Thinking models universally benefit from auxiliary line prompts"
  - [corpus] GeoSketch and Geoint-R1 similarly emphasize auxiliary line construction as critical for geometric reasoning.
- Break condition: If the auxiliary line provided is suboptimal or misleading, models that rely heavily on hints may perform worse than unguided baselines.

### Mechanism 3
- Claim: Step-segmented evaluation reveals "laziness" in proof problems where models produce correct answers via flawed processes.
- Mechanism: In proving problems, models exploit the given conclusion to work backwards or guess, yielding high ACS but low PCS. Calculation problems force forward reasoning, reducing this gap. The paper measures this via ACS-PCS divergence.
- Core assumption: Models can detect target conclusions in proof problems and use them as shortcuts without full derivation.
- Evidence anchors:
  - [abstract] "MLLMs tend to take shortcuts in proving problems compared to calculation problems"
  - [section 5.3] "proving problems consistently demonstrate significantly higher ACS but lower PCS"
  - [corpus] Weak/missing - corpus papers don't explicitly address this proving vs. calculation asymmetry.
- Break condition: If evaluation prompts explicitly forbid using the target conclusion as a premise, the effect should diminish.

## Foundational Learning

- Concept: Because-Therefore (∵-∴) reasoning chain structure
  - Why needed here: GeoLaux defines each reasoning step as a complete "because-therefore" pair. Understanding this format is essential for interpreting step segmentation and scoring.
  - Quick check question: Given a geometry solution, can you segment it into discrete ∵-∴ pairs where each "therefore" is a justified conclusion?

- Concept: Process Correctness Score (PCS) vs Answer Correctness Score (ACS)
  - Why needed here: The benchmark explicitly separates final-answer accuracy from reasoning-process validity. High ACS with low PCS indicates potential shortcut-taking or guessing.
  - Quick check question: If a model outputs the correct final answer but uses an invalid theorem in step 3, what are ACS and PCS?

- Concept: Auxiliary line construction taxonomy (simple vs. complex)
  - Why needed here: Simple lines involve point connections; complex lines create new primitives (perpendiculars, extensions, inscribed elements). Models handle these differently.
  - Quick check question: Classify "connect points A and B" vs. "extend line segment AB through point C" by complexity.

## Architecture Onboarding

- Component map:
  Dataset -> One-shot prompting -> JSON solution generation -> Step-by-step evaluation -> Error classification -> Five-dimensional scoring

- Critical path:
  1. Model generates solution in JSON format with "solution" and "short_answer"
  2. Evaluator receives (problem, diagram, reference_solution, model_solution)
  3. Step-by-step scoring produces η = (η1, ..., ηn)
  4. Compute ACS, PCS, PQS using formulas in Section 4
  5. For auxiliary evaluation, inject aux_text + aux_image before step 1

- Design tradeoffs:
  - Using an MLLM (o4-mini) as evaluator enables automation but introduces evaluation-model bias; reference solutions mitigate this
  - Exponential weighting penalizes early errors heavily; α=3.5 tunes the spread but is not empirically optimized
  - GeoLaux-mini (330 problems) enables testing expensive models (o1, o3) but reduces statistical power

- Failure signatures:
  - ACS ≫ PCS: model likely guessed or shortcutted, especially in proof problems
  - First error step clustering at 2-5: indicates systematic early-reasoning failure, not random errors
  - Complex auxiliary line performance ≪ no-auxiliary performance: model resorting to coordinate-brute-force escape routes

- First 3 experiments:
  1. Baseline evaluation: Run target MLLM on full GeoLaux, compute all five dimensions. Compare ACS vs PCS gap for calculation vs proving problems.
  2. Auxiliary line ablation: For problems requiring auxiliary lines, run with and without auxiliary hints. Measure ΔPCS separately for thinking and non-thinking model categories.
  3. Error-type stratification: For each model, compute error-type distribution (figure comprehension, knowledge, calculation, logical reasoning). Identify whether failures cluster in specific categories to guide targeted improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLMs be trained to autonomously construct auxiliary lines rather than relying on coordinate brute-force methods or external prompts?
- Basis in paper: [explicit] Section 5.4 states that enhancing auxiliary line construction awareness and capability is a "critical research direction," noting that models currently lack spatial imagination and use escape mechanisms.
- Why unresolved: Models currently struggle to construct complex auxiliary lines, often resorting to brute-force coordinate systems which increase complexity and computational load.
- What evidence would resolve it: A model architecture or training paradigm that significantly improves performance on the "complex auxiliary line" subset of GeoLaux without requiring heuristic prompts.

### Open Question 2
- Question: What methods can effectively mitigate the "laziness" in proving problems where MLLMs combine correct final answers with incorrect solution processes?
- Basis in paper: [explicit] Section 5.3 identifies this "laziness" as an issue requiring "urgent attention," where models "cheat" by leveraging given conclusions while neglecting reasoning steps.
- Why unresolved: Current models decouple answer generation from rigorous deductive derivation, particularly when the target conclusion is explicitly provided in the problem statement.
- What evidence would resolve it: The development of models that minimize the gap between Answer Correctness Score (ACS) and Process Correctness Score (PCS) specifically on proving problems.

### Open Question 3
- Question: Can external tools (e.g., symbolic solvers) mitigate knowledge and calculation errors without disrupting the logical reasoning flow required for long-step geometry problems?
- Basis in paper: [inferred] Section 5.5 notes that while figure comprehension and logical reasoning are "fundamental bottlenecks," knowledge and calculation errors are "addressable through external tools."
- Why unresolved: While tools can correct arithmetic or formula selection, their integration into a multi-step deductive chain without causing context fragmentation or logical hallucination remains unproven.
- What evidence would resolve it: A tool-augmented MLLM that demonstrates near-zero calculation/knowledge error rates while maintaining or improving Process Quality Scores (PQS) on ultra-long-step problems.

## Limitations

- The exponential weighting scheme in PQS (α=3.5) lacks empirical justification for this specific value
- Performance gaps between thinking and non-thinking models may reflect architectural differences beyond reasoning capabilities
- Dataset focuses exclusively on Chinese Zhongkao exam problems, limiting generalizability to other educational systems

## Confidence

- **High confidence** in the core finding that MLLMs show severe performance degradation on extended reasoning steps
- **Medium confidence** in the proving vs. calculation asymmetry claim
- **Medium confidence** in auxiliary line impact results

## Next Checks

1. **Error propagation validation**: Re-analyze the GeoLaux dataset to determine whether early errors in solutions actually propagate through subsequent steps to justify the exponential weighting assumption.

2. **Cross-cultural generalization test**: Evaluate the same MLLMs on a comparable geometry benchmark from a different educational system to assess whether observed performance patterns generalize beyond Chinese Zhongkao problems.

3. **Auxiliary line mechanism probe**: Design a controlled experiment where models receive both correct and incorrect auxiliary line hints to determine whether improvements stem from genuine integration of spatial reasoning or simple pattern matching.