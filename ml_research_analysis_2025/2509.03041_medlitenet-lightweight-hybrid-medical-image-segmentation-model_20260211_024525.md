---
ver: rpa2
title: 'MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model'
arxiv_id: '2509.03041'
source_url: https://arxiv.org/abs/2509.03041
tags:
- segmentation
- lesion
- medlitenet
- image
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MedLiteNet introduces a lightweight CNN\u2013Transformer hybrid\
  \ for skin lesion segmentation that achieves clinical-grade accuracy with just 3.2\
  \ M parameters. The model employs a MobileNetV2 inverted residual backbone, a bottleneck-level\
  \ cross-scale Local\u2013Global Block fusing local convolution and global Transformer\
  \ features, and a boundary-aware self-attention mechanism to enhance lesion contours."
---

# MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model

## Quick Facts
- arXiv ID: 2509.03041
- Source URL: https://arxiv.org/abs/2509.03041
- Reference count: 4
- Primary result: Achieves 0.897 ± 0.010 Dice and 0.821 ± 0.015 IoU on ISIC 2018 with just 3.2 M parameters

## Executive Summary
MedLiteNet introduces a lightweight CNN–Transformer hybrid for skin lesion segmentation that achieves clinical-grade accuracy with just 3.2 M parameters. The model employs a MobileNetV2 inverted residual backbone, a bottleneck-level cross-scale Local–Global Block fusing local convolution and global Transformer features, and a boundary-aware self-attention mechanism to enhance lesion contours. The decoder integrates ASPP and SCSE modules for multi-scale context and fine-grained detail recovery. On ISIC 2018, MedLiteNet attains 0.897 ± 0.010 Dice and 0.821 ± 0.015 IoU, with 1 ms raw latency for 256×256 images. A three-variant ensemble reaches 0.904 ± 0.012 Dice and 0.830 ± 0.018 IoU while remaining under 10 M parameters.

## Method Summary
The model uses a MobileNetV2 encoder with four stages of inverted residual blocks, followed by a bottleneck-level Local–Global Block that fuses MBConv features with Transformer-processed tokens. A Boundary-Aware Attention module enhances contour detection. The decoder employs ASPP for multi-scale context and SCSE for feature recalibration, with U-Net style upsampling and skip connections. Training uses AdamW optimizer with cosine annealing, combined BCE+Dice loss (0.5/0.5 ratio), and extensive data augmentation including elastic deformation, CLAHE, and noise/blur. The model is trained on ISIC 2018 with 300 epochs, batch size 16, and FP16 mixed precision.

## Key Results
- Single model: 0.897 ± 0.010 Dice, 0.821 ± 0.015 IoU on ISIC 2018 test set
- Ensemble of three variants: 0.904 ± 0.012 Dice, 0.830 ± 0.018 IoU
- Model size: 3.2 M parameters (under 10 M for ensemble)
- Latency: 1 ms for 256×256 images on RTX A6000

## Why This Works (Mechanism)

### Mechanism 1: Bottleneck-Level Local–Global Fusion
Fusing local convolution features with global Transformer features exclusively at the bottleneck layer captures long-range dependencies while minimizing the quadratic computational cost typical of Vision Transformers. The encoder first uses MobileNetV2 inverted residuals to extract local textures. At the lowest spatial resolution, features are flattened into tokens and processed by a lightweight Transformer module to model global context. These are fused via concatenation and a $1 \times 1$ convolution, combining fine local details with holistic context.

### Mechanism 2: Boundary-Aware Attention (BAA)
Explicitly deriving attention weights from spatial gradients enhances the model's focus on lesion contours, mitigating the blurring effect common in downsampling encoders. The BAA module computes a boundary response map alongside standard features. This map acts as an attention mask that amplifies features at the periphery, explicitly signaling the decoder to prioritize edge reconstruction.

### Mechanism 3: Multi-Scale Context Aggregation (ASPP + SCSE)
Parallel multi-scale processing at the decoder restores spatial details lost during encoding and adapts to varying lesion sizes without increasing network depth. The ASPP module applies parallel dilated convolutions with different rates to capture context at multiple scales. The SCSE module recalibrates features channel-wise and spatially to suppress irrelevant background regions.

## Foundational Learning

- **Depthwise Separable Convolution (MobileNetV2)**: The encoder relies entirely on MBConv blocks to achieve the claimed 3.2M parameter count. Understanding the factorization of standard convolution into depthwise and pointwise steps is required to analyze the model's efficiency.
  - Quick check: How does the computational cost of a $3 \times 3$ Depthwise Separable Convolution compare to a standard $3 \times 3$ Convolution given the same input/output channels?

- **Transformer Self-Attention**: The "Global" part of the Local–Global Block relies on computing pairwise attention between tokens. You must understand why this has $O(N^2)$ complexity to appreciate why the authors restrict it to the low-resolution bottleneck.
  - Quick check: Why does the authors' choice to apply the Transformer only at the bottleneck layer (smaller $H \times W$) reduce computational overhead compared to applying it at full resolution?

- **Dilated (Atrous) Convolution**: The ASPP module uses this to enlarge the receptive field without pooling or striding.
  - Quick check: If a $3 \times 3$ kernel has a dilation rate of 2, what is the effective receptive field size covered by the kernel?

## Architecture Onboarding

- **Component map**: Input (256×256 Dermoscopic Image) -> MBConv Encoder (4 stages) -> Bottleneck (Local–Global Block + BAA) -> Context (ASPP + SCSE) -> Decoder (U-Net upsampling) -> Output (Binary Mask)
- **Critical path**: The flow from MBConv Encoder → Bottleneck Fusion → ASPP is the critical path. The skip connections rescue spatial resolution, but the semantic accuracy depends on the bottleneck's ability to mix local/transformer features effectively.
- **Design tradeoffs**: Accuracy vs. Latency (replacing ResNet with MBConv for 90%+ parameter reduction), Context vs. Resolution (Transformer at bottleneck gains global context but risks losing spatial localization)
- **Failure signatures**: Artifact Segmentation (occasional hair/follicle/ruler segmentation), Under-segmentation (in low-contrast regions), Latency (1ms single model, ensemble triples this)
- **First 3 experiments**:
  1. Ablation on BAA: Disable Boundary-Aware Attention module and measure drop in IoU/Dice
  2. Backbone Substitution: Replace MobileNetV2 with ResNet-18 to benchmark performance gap
  3. Noise Robustness: Evaluate on images with synthetic hair/marker overlays to test artifact sensitivity

## Open Questions the Paper Calls Out

1. **Multi-modal extension**: Can MedLiteNet maintain its efficiency when extended to multi-modal imaging or multi-task learning? The paper lists this as a specific future direction but only validates on single-modal dermoscopic images.

2. **Artifact sensitivity**: Can architectural improvements effectively resolve the model's sensitivity to artifacts (hair, rulers) and low-contrast boundaries without relying on external preprocessing? The current BAA module still struggles with these issues.

3. **Real-world deployment**: What is the real-world inference latency and power consumption on actual mobile or edge hardware? The 1ms figure is based on high-end GPU, not representative of mobile deployment.

## Limitations
- Limited dataset evaluation (only ISIC 2018 benchmark)
- Unclear ensemble configuration and test-time augmentation details
- Unverified performance on actual mobile/edge hardware
- Sensitivity to artifacts (hair, rulers) not fully resolved architecturally

## Confidence
- **High Confidence**: Core architectural innovations clearly described, parameter count verifiable, training methodology well-documented
- **Medium Confidence**: Performance metrics well-documented but ensemble details unclear, BAA effectiveness demonstrated empirically but acknowledged limitations exist
- **Low Confidence**: Computational efficiency claims need clarification (especially ensemble latency), artifact robustness not comprehensively evaluated

## Next Checks
1. Ablation Study on Boundary-Aware Attention: Disable BAA module entirely and measure specific performance degradation in Dice/IoU scores
2. Cross-Dataset Generalization: Evaluate trained model on Derm7pt or PH2 dataset without fine-tuning to assess generalization
3. Artifact Robustness Testing: Create synthetic test set with controlled hair, ruler, and compression artifact overlays to systematically measure false positive rate and identify failure modes