---
ver: rpa2
title: 'PromptBridge: Cross-Model Prompt Transfer for Large Language Models'
arxiv_id: '2512.01420'
source_url: https://arxiv.org/abs/2512.01420
tags:
- prompt
- transfer
- promptbridge
- target
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model drifting, where prompts
  optimized for one LLM perform suboptimally on a different model, hindering seamless
  model switching. It introduces PromptBridge, a training-free framework that transfers
  prompts across models without re-optimization by learning a mapping from a small
  set of alignment tasks.
---

# PromptBridge: Cross-Model Prompt Transfer for Large Language Models

## Quick Facts
- arXiv ID: 2512.01420
- Source URL: https://arxiv.org/abs/2512.01420
- Reference count: 40
- One-line primary result: Cross-model prompt transfer framework improves accuracy by 27.39% on SWE-Bench and 39.44% on Terminal-Bench compared to direct transfer

## Executive Summary
This paper addresses the problem of model drifting, where prompts optimized for one LLM perform suboptimally on a different model, hindering seamless model switching. PromptBridge introduces a training-free framework that transfers prompts across models without re-optimization by learning a mapping from a small set of alignment tasks. The framework employs Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to generate model-specific prompts, then learns a cross-model transformation using these calibrated pairs. At test time, it adapts source-model prompts to target models for unseen tasks, achieving substantial accuracy improvements while reducing migration effort.

## Method Summary
PromptBridge operates in three phases: calibration, mapping extraction, and test-time adaptation. During calibration, MAP-RPE uses island-based evolutionary search with a reflection model to optimize prompts for each target model on alignment tasks. The Mapping Extractor (a high-capability LLM) then analyzes prompt pairs to generate a transfer effects summary describing the transformation pattern. Finally, an adapter model uses this summary to convert source-model prompts for unseen tasks to target-model optimized versions. The framework is training-free and requires only a small number of alignment tasks to learn transferable prompt transformations.

## Key Results
- Achieves 27.39% improvement on SWE-Bench and 39.44% on Terminal-Bench compared to direct prompt transfer
- Outperforms Few-Shot In-Context Learning baselines due to capturing stable adaptation patterns rather than superficial imitation
- Shows consistent performance gains across single- and multi-agent settings with various model pairs
- Semantic delta analysis confirms shared latent structures in prompt transformations across different tasks

## Why This Works (Mechanism)

### Mechanism 1
Reflective, metric-driven prompt evolution (MAP-RPE) recovers model-specific performance better than static or random generation because it explicitly optimizes for the target model's behavioral distribution. MAP-RPE maintains an island-based population of prompts, iteratively queries the target model, evaluates responses using quantitative metrics, and uses a reflection model to propose refinements, approximating the optimal prompt for a specific model.

### Mechanism 2
Cross-model transferability relies on the existence of a stable "semantic delta" between source and target prompt optima that is consistent across diverse tasks. The framework assumes that the transformation required to map an optimal prompt from one model to another follows a shared latent structure, justifying a shared mapping function that can be learned from alignment tasks.

### Mechanism 3
Explicitly summarizing transfer rules outperforms Few-Shot In-Context Learning because it forces the Adapter Model to internalize a generalizable strategy rather than mimicking surface-level patterns. By first synthesizing a "Transfer Effects Summary" that guides the adaptation process, the framework reduces sensitivity to specific examples and prevents the imitation of superficial patterns observed in ablation studies.

## Foundational Learning

- **Model Drifting**: The core problem PromptBridge solves—optimal prompts differ across models. Quick check: If Model A achieves 99% with Prompt X and Model B achieves 90% with the same prompt, does increasing Model B's temperature solve the drift? (Answer: No, prompt structure needs adaptation).

- **Island-Based Evolutionary Search**: The engine of the calibration phase (MAP-RPE). Standard gradient descent doesn't work on discrete prompts. Quick check: Why use "islands" instead of one large population? (Answer: To maintain diversity and prevent premature convergence).

- **Semantic Similarity vs. Functional Equivalence**: The paper relies on semantic embeddings to prove transferability, but the final metric is functional correctness. Quick check: If two prompts have high cosine similarity, do they guarantee the same LLM output? (Answer: No, minor semantic changes can cause large behavioral shifts).

## Architecture Onboarding

- **Component map:** Calibration Tasks → MAP-RPE → (p*_source, p*_target) pairs → Mapping Extractor → Transfer Effects Summary → Adapter Model → Target-Optimized Prompt

- **Critical path:** The Mapping Extractor is the single point of failure. If the summary generated is vague or incorrect, the test-time adaptation will fail regardless of the Adapter Model's quality.

- **Design tradeoffs:** Using a weaker model as the Mapping Extractor significantly degrades performance. Performance improves with more calibration questions (n=5 vs n=50) but incurs higher API costs during calibration.

- **Failure signatures:** Mode collapse in evolution (syntactically valid but logically identical prompts), superficial imitation by the adapter model, negative transfer from coding to planning tasks.

- **First 3 experiments:** 1) Establish baseline drift by running Source and Target models on held-out test set using same default prompt. 2) Ablate the "Bridge" by comparing simple Few-Shot ICL vs PromptBridge Summary method. 3) Perform semantic delta check by visualizing embedding similarity of prompt pairs before running full pipeline.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework may fail when transferring between models with fundamentally different instruction-following training paradigms or when applied to domains requiring highly specialized prompt structures
- Calibration phase computational cost is significant, requiring iterative evolutionary search with multiple model queries for each alignment task
- Reliance on high-capability Mapping Extractor (GPT-5) creates practical bottleneck for organizations without access to frontier models

## Confidence

- **High Confidence (80-100%):** Existence of model drifting and effectiveness of evolutionary calibration on coding benchmarks
- **Medium Confidence (60-80%):** Generalizability of semantic delta across diverse domains (limited to coding tasks in current validation)
- **Low Confidence (20-40%):** Framework's ability to transfer between radically different model architectures or data distributions

## Next Checks

1. **Cross-Domain Transferability Test:** Evaluate PromptBridge's performance when transferring prompts between coding and non-coding domains using the same alignment tasks to determine if domain-specific alignment tasks are necessary.

2. **Mapping Extractor Scalability Analysis:** Systematically vary the capability of the Mapping Extractor and measure degradation in adapter model performance to quantify practical constraints.

3. **Computational Cost-Benefit Trade-off:** Measure the relationship between number of alignment tasks and transfer accuracy, then calculate the break-even point where calibration cost exceeds performance gain compared to direct prompt re-optimization.