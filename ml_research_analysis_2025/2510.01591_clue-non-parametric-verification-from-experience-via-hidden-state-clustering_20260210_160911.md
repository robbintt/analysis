---
ver: rpa2
title: 'CLUE: Non-parametric Verification from Experience via Hidden-State Clustering'
arxiv_id: '2510.01591'
source_url: https://arxiv.org/abs/2510.01591
tags:
- reasoning
- arxiv
- hidden
- clue
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLUE presents a non-parametric approach to verify Large Language
  Model (LLM) outputs using hidden-state trajectories. Instead of relying on text-level
  cues or confidence scores, CLUE computes an activation delta for each reasoning
  trace and compares it to success/failure centroids derived from past experiences.
---

# CLUE: Non-parametric Verification from Experience via Hidden-State Clustering

## Quick Facts
- arXiv ID: 2510.01591
- Source URL: https://arxiv.org/abs/2510.01591
- Reference count: 23
- Primary result: CLUE achieves 70.0% top-1 accuracy on AIME 24 using 64 candidates, outperforming majority voting (56.7%)

## Executive Summary
CLUE introduces a training-free method to verify LLM reasoning outputs by analyzing hidden-state trajectories rather than text-level cues. The approach computes activation deltas between reasoning start and end points, then compares these vectors to pre-computed success/failure centroids using nearest-centroid classification. By leveraging the geometric separability of correct versus incorrect reasoning paths in activation space, CLUE achieves strong performance in reranking candidate solutions without requiring model retraining or confidence calibration.

## Method Summary
CLUE extracts hidden-state activations at reasoning start (`) and end (``) tokens, computes layer-wise activation deltas, and classifies traces by Euclidean distance to pre-computed success/failure centroids. The method requires no training, uses layer-averaged distances for robustness, and can transfer verification capabilities across models when using RL-trained centroids to verify SFT outputs.

## Key Results
- Achieves 70.0% top-1 accuracy on AIME 24 with 64 candidates using a 1.5B model
- Outperforms LLM-as-a-judge and confidence-based baselines in solution reranking
- Demonstrates cross-model verification capability: RL model centroids can verify SFT model outputs
- Shows geometric separability is stronger in RL-trained models versus SFT-only models

## Why This Works (Mechanism)

### Mechanism 1
The correctness of a reasoning trace is encoded as a geometrically separable signature in the activation space. CLUE computes an "activation delta" ($\Delta h = h_{end} - h_{start}$) across all layers, representing the transformation induced by reasoning. Comparing this delta to success/failure centroids using Euclidean distance classifies the trace. The core assumption is that correct and incorrect reasoning trajectories differ systematically in activation space.

### Mechanism 2
Reinforcement Learning instills better geometric separability than Supervised Fine-Tuning. RL with verifiable rewards provides explicit negative feedback for incorrect paths, pushing wrong answer states away from correct ones. SFT primarily imitates positive traces without defining "wrongness," leaving representations entangled. The training objective directly alters hidden state manifolds to make correctness separable.

### Mechanism 3
Layer-averaged distance integrates distributed correctness signals better than single-layer probing. Correctness information is distributed: early layers capture semantic features while later layers align with output logits. Averaging Euclidean distance across all layers ensures classification utilizes the full spectrum of reasoning signals rather than overfitting to specific depths.

## Foundational Learning

- **Hidden State Extraction (Activations)**: Required to access model internals at specific token positions. Engineer must know how to attach forward hooks or access `output.hidden_states`. Quick check: Can you identify the specific tensor shape and index for the final layer's hidden state for the token ` Assistant`?

- **Centroid-based Classification (Non-parametric)**: Relies on arithmetic means and distance metrics rather than neural network training. Understanding bias-variance tradeoff is key. Quick check: If you have 10,000 success and 10,000 failure vectors in 4096 dimensions, what is the computational complexity of classifying a new vector?

- **Chain-of-Thought Delimiters**: The activation delta is bounded by reasoning tags (e.g., `...`). The mechanism fails if delimiters are not present or identified correctly. Quick check: Does the method require explicit generation of ` tags, or can boundaries be inferred implicitly?

## Architecture Onboarding

- **Component map**: Experience Generator -> Extraction Hook -> Centroid Manager -> Inference Scorer
- **Critical path**: Extracting $h_{start}$ and $h_{end}$ is most fragile. Must map token indices of reasoning delimiters exactly. If tokenizer merges the ` tag with adjacent text, extraction index will be off, introducing noise into the delta.
- **Design tradeoffs**: Pros include no training required, robust to calibration errors, cross-model transfer. Cons require white-box access, labeled experience set for domain-specific centroids, and explicit reasoning structure.
- **Failure signatures**: SFT-only verification may drop to near random levels. Short reasoning traces (few tokens) reduce signal-to-noise ratio in the delta.
- **First 3 experiments**:
  1. **Sanity Check (Visualization)**: Replicate Figure 1/4. Extract deltas for 100 known-correct and 100 known-incorrect traces on 1.5B model. Run PCA. If clusters are overlapping, setup is broken.
  2. **Hyperparameter Sensitivity (Layer Averaging)**: Test if only the last layer works better than average of all layers. Paper implies averaging is robust, but specific tasks might benefit from weighted averaging.
  3. **Cross-Model Transfer**: Build centroids using strong RL-model (DeepSeek-R1) and use them to rerank outputs from weaker SFT-model. Validates "universality" of geometric signature.

## Open Questions the Paper Calls Out

- Can novel training objectives be designed to explicitly optimize for internal representational clarity, thereby enhancing geometric separability of correct and incorrect reasoning traces?
- Can Supervised Fine-Tuning be regularized to induce geometric separability observed in Reinforcement Learning models, or is contrastive signal of RL strictly necessary?
- Is simple Euclidean distance to centroid sufficient for complex reasoning distributions, or does decision boundary require non-linear modeling?

## Limitations
- Geometric separability assumption may fail for short reasoning traces or tasks lacking clear correctness criteria
- Requires white-box access to internal activations, limiting deployment to open-weight models only
- Performance depends on quality and representativeness of experience dataset for building centroids

## Confidence
**High Confidence**: Centroid-based verification works for mathematics problems with RL-trained model centroids; cross-model verification is effective; layer-averaging provides robust performance.

**Medium Confidence**: Method generalizes to non-mathematical reasoning tasks; approach scales effectively to larger models; geometric signature is universal across different RL training methodologies.

**Low Confidence**: Performance on extremely long reasoning chains with potential activation saturation; effectiveness when experience dataset size drops below 10K samples; behavior with chain-of-thought models using different delimiter conventions.

## Next Checks
1. **Distribution Shift Robustness Test**: Generate experience centroids using one mathematical domain (e.g., algebra) and evaluate verification performance on a different domain (e.g., geometry or calculus).

2. **Chain Length Sensitivity Analysis**: Systematically vary reasoning trace length (from 5 to 50+ steps) and measure how centroid distance distributions change.

3. **Cross-Objective Training Verification**: Train RL models using different reward structures (pure correctness vs. preference optimization vs. mixed objectives) and compare resulting centroid separability.