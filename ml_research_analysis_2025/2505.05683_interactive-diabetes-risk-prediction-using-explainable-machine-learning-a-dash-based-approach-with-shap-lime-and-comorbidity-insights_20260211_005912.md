---
ver: rpa2
title: 'Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A
  Dash-Based Approach with SHAP, LIME, and Comorbidity Insights'
arxiv_id: '2505.05683'
source_url: https://arxiv.org/abs/2505.05683
tags:
- health
- risk
- prediction
- diabetes
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an interactive diabetes risk prediction system
  using the BRFSS dataset and machine learning. A LightGBM model trained on undersampled
  data was selected based on its superior recall performance.
---

# Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights

## Quick Facts
- arXiv ID: 2505.05683
- Source URL: https://arxiv.org/abs/2505.05683
- Reference count: 25
- One-line primary result: LightGBM with undersampling achieved superior recall for diabetes detection, deployed with SHAP/LIME explainability in a Dash web application

## Executive Summary
This study developed an interactive diabetes risk prediction system using the 2015 BRFSS dataset and machine learning. A LightGBM model trained on randomly undersampled data was selected for its superior recall performance in detecting diabetic cases. The system integrates SHAP and LIME methods to provide both global and local interpretability of predictions. Additional features include comorbidity correlation analysis and deployment as a Dash web application with personalized health recommendations, combining predictive accuracy with transparency and user accessibility.

## Method Summary
The method involved preprocessing the BRFSS 2015 dataset (~250,000 records, 22 features) by removing duplicates and converting prediabetic cases to create a binary classification task. Three engineered composite features were created: Lifestyle Score, Healthcare Access Score, and Risk Factor Count. Random undersampling was applied to balance the class distribution, addressing the 84% non-diabetic vs. 16% diabetic imbalance. LightGBM was selected through hyperparameter tuning with GridSearchCV and evaluated via stratified k-fold cross-validation, optimizing for recall. SHAP and LIME were integrated for global and local interpretability respectively, and the system was deployed as a Dash web application with a multi-step form interface.

## Key Results
- Undersampling improved recall performance across multiple models, with LightGBM achieving the best results
- Risk Factor Count emerged as the most influential predictor in SHAP analysis
- The Dash application successfully integrated SHAP global importance plots and LIME local explanations
- The system provides personalized health recommendations alongside predictions

## Why This Works (Mechanism)

### Mechanism 1
- Random undersampling of the majority class improves recall for diabetes detection compared to SMOTE or original data distribution. By reducing the non-diabetic majority class to match the diabetic minority class size, the model assigns proportionally more learning weight to diabetic patterns during training, reducing false negatives. Core assumption: In health screening contexts, missing a true diabetic case carries higher clinical cost than incorrectly flagging a non-diabetic individual.

### Mechanism 2
- Combining SHAP and LIME provides complementary global and local interpretability for model predictions. SHAP computes Shapley values based on cooperative game theory for global importance, while LIME approximates the complex model locally with a simple interpretable model around specific prediction instances. Core assumption: Users require both population-level insights and individual-level explanations to trust and act on model outputs.

### Mechanism 3
- Engineered composite features (Risk Factor Count, Lifestyle Score, Healthcare Access Score) capture aggregated health patterns that improve both prediction and interpretability. Summing or averaging related binary indicators into composite scores creates higher-level abstractions that represent latent health constructs. Core assumption: Health behaviors and chronic conditions cluster in meaningful ways that single binary features cannot fully represent.

## Foundational Learning

- **Concept: Class Imbalance and Sampling Strategies**
  - Why needed here: The BRFSS dataset contains ~84% non-diabetic vs. ~16% diabetic samples. Models trained on imbalanced data tend to predict the majority class, achieving high accuracy but poor recall on the minority (diabetic) class.
  - Quick check question: A model achieves 84% accuracy on this dataset by always predicting "non-diabetic." Is this a useful model for health screening?

- **Concept: Recall-Precision Tradeoff in Healthcare Contexts**
  - Why needed here: The paper explicitly optimizes for recall to minimize missed diabetes cases. Understanding why recall is prioritized over accuracy or precision is essential for evaluating the system design.
  - Quick check question: Why would a public health screening tool prefer 90% recall with 50% precision over 70% recall with 85% precision?

- **Concept: SHAP vs. LIME Explainability**
  - Why needed here: The system deploys both methods for different purposes. SHAP provides consistent global feature rankings; LIME offers intuitive local explanations. Knowing when to use each is critical for interpretation.
  - Quick check question: A clinician asks why a specific patient was flagged as high-risk. Which explanation method is more appropriate to show them?

## Architecture Onboarding

- **Component map:**
  Data Layer (BRFSS 2015 dataset) → Preprocessing Pipeline (duplicate removal, ordinal encoding, binary target conversion) → Feature Engineering Module (Lifestyle Score, Healthcare Access Score, Risk Factor Count) → Sampling Layer (Random undersampling) → Model Layer (LightGBM with GridSearchCV) → Explainability Layer (SHAP global summary plots, LIME local explanations) → Presentation Layer (Dash web application with multi-step form)

- **Critical path:**
  Data ingestion → Preprocessing → Feature engineering → Undersampling → Model training → SHAP/LIME computation → Dash app deployment

- **Design tradeoffs:**
  - Recall vs. Precision: Undersampling maximizes recall but reduces precision, increasing false positives that may cause unnecessary patient anxiety.
  - Interpretability vs. Performance: LightGBM selected over potentially higher-accuracy neural networks to maintain explainability via SHAP/LIME.
  - Precomputation vs. Real-time: Assumption: SHAP and LIME explanations may be computed at inference time, which could impact latency.

- **Failure signatures:**
  - High accuracy but low recall on validation data: Indicates class imbalance not properly addressed.
  - SHAP feature importance contradicts clinical expectations: May signal data leakage or erroneous feature engineering.
  - LIME explanations highly unstable for similar inputs: Local approximation neighborhood may be too small or model decision boundary is highly irregular.
  - Dash application slow on prediction: SHAP/LIME computation happening synchronously during request rather than cached or approximated.

- **First 3 experiments:**
  1. Establish baseline performance: Train LightGBM on original imbalanced data; record accuracy, precision, recall, F1, and ROC-AUC via stratified k-fold cross-validation.
  2. Compare sampling strategies: Train identical models on SMOTE-oversampled and randomly-undersampled versions; use ANOVA on cross-validation recall scores to test for significant differences.
  3. Validate explainability outputs: Select a subset of diabetic cases with known clinical profiles; verify that SHAP identifies expected risk factors (HighBP, HighChol, BMI, GenHlth) as top contributors, and that LIME explanations align with patient histories.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the integration of SHAP and LIME explanations in the Dash application improve decision-making accuracy and user trust among patients and healthcare providers in a live clinical setting?
- Basis in paper: The Conclusion states that future work requires "incorporating user feedback loops and external clinical validation" to strengthen the model's reliability and applicability in healthcare settings.
- Why unresolved: The current study demonstrates technical feasibility and design but lacks empirical data on actual user interaction, behavioral changes, or diagnostic utility in a real-world clinical workflow.
- What evidence would resolve it: Results from a controlled user study or clinical trial measuring user comprehension, trust metrics, and the concordance of model predictions with clinical diagnoses.

### Open Question 2
- Question: How does model performance and feature importance evolve when transitioning from static, self-reported cross-sectional data to longitudinal datasets or real-time wearable device streams?
- Basis in paper: The Future Work section proposes using "longitudinal or more recent datasets for improved temporal relevance" and the "inclusion of real-time wearable data."
- Why unresolved: The current model is trained on a single snapshot (2015 BRFSS) of self-reported data; it is unknown if the "Risk Factor Count" and other engineered features remain stable or if the model generalizes to temporal health patterns.
- What evidence would resolve it: Comparative benchmarking of the model's recall and precision on time-series clinical data versus the static BRFSS baseline, specifically analyzing the stability of SHAP values over time.

### Open Question 3
- Question: How does the exclusion of the "prediabetic" class during preprocessing affect the model's sensitivity to early warning signs compared to a multi-class classification approach?
- Basis in paper: Section III.B notes that "prediabetic entries were excluded" to force a binary classification task, despite the Introduction emphasizing the goal of "early identification."
- Why unresolved: By removing the intermediate class, the model may conflate early risk factors with frank disease, potentially failing to identify the specific behavioral drivers unique to the pre-diabetic stage.
- What evidence would resolve it: A comparative analysis of model recall and SHAP feature distributions specifically for the prediabetic cohort when modeled separately versus when merged into the non-diabetic class.

## Limitations
- Undersampling significantly reduces training data size, potentially compromising model generalizability to unseen populations.
- Specific formulas for engineered features (Lifestyle Score, Healthcare Access Score) are not fully specified, affecting reproducibility.
- Limited validation on external datasets or temporal splits to assess model robustness beyond the 2015 BRFSS cohort.

## Confidence

- **High Confidence**: Undersampling improves recall over original class distribution (supported by experimental results in section 3.2)
- **Medium Confidence**: SHAP+LIME combination provides complementary explanations (mechanism well-established but application-specific validation limited)
- **Medium Confidence**: Engineered composite features improve interpretability (mechanism plausible but lacks comparative ablation studies)

## Next Checks
1. Conduct external validation using 2016-2018 BRFSS data to assess temporal generalizability of the undersampling approach
2. Perform ablation study comparing model performance with and without engineered composite features to quantify their contribution
3. Test alternative sampling ratios (e.g., 2:1, 3:1 non-diabetic:diabetic) to find optimal balance between recall and precision