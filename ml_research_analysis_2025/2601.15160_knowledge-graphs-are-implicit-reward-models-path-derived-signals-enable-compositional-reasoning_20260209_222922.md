---
ver: rpa2
title: 'Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional
  Reasoning'
arxiv_id: '2601.15160'
source_url: https://arxiv.org/abs/2601.15160
tags:
- reasoning
- reward
- compositional
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a post-training pipeline that uses knowledge
  graphs as implicit reward models to enable compositional reasoning in large language
  models. The method combines supervised fine-tuning with reinforcement learning,
  where path-derived rewards from knowledge graph triples encourage models to compose
  intermediate axioms rather than focus solely on final answers.
---

# Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning

## Quick Facts
- arXiv ID: 2601.15160
- Source URL: https://arxiv.org/abs/2601.15160
- Authors: Yuval Kansal; Niraj K. Jha
- Reference count: 32
- Key outcome: Path-derived KG rewards enable 14B model to outperform much larger models on complex multi-hop reasoning

## Executive Summary
This work introduces a post-training pipeline that uses knowledge graphs as implicit reward models to enable compositional reasoning in large language models. The method combines supervised fine-tuning with reinforcement learning, where path-derived rewards from knowledge graph triples encourage models to compose intermediate axioms rather than focus solely on final answers. Training a 14B model on short-hop reasoning paths and evaluating on complex multi-hop queries shows that the approach significantly improves accuracy, especially on the most difficult tasks, outperforming much larger frontier models. The method is robust to adversarial perturbations and demonstrates that grounding reasoning in structured knowledge is a scalable path to intelligent reasoning.

## Method Summary
The approach uses a two-stage pipeline: first supervised fine-tuning (SFT) with LoRA on 19,660 medical QA tasks covering 1-3 hop reasoning paths, then reinforcement learning with Group Relative Policy Optimization (GRPO) on 5,000 tasks using a composite reward combining binary correctness signals with path alignment rewards. The path alignment reward measures token overlap between the model's reasoning trace and ground-truth KG path entities, encouraging compositional reasoning rather than outcome-only focus. The final SFT+RL model achieves 82.20% accuracy on complex multi-hop questions, significantly outperforming both SFT-only and much larger frontier models.

## Key Results
- SFT+RL achieves 82.20% accuracy vs. 70.86% for SFT-only on ICD-Bench
- Outperforms GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro on 4-5 hop questions
- Generalization gap widens with hop length: +7.5% on 4-hop and +11.1% on 5-hop vs. SFT-only
- Robust to adversarial perturbations with shuffled answer options

## Why This Works (Mechanism)

### Mechanism 1: Process-Level Supervision via Path Alignment
Path-derived rewards from KG triples provide process-level supervision that enables compositional generalization beyond training complexity. The path alignment reward R_path measures token overlap between the model's reasoning trace and ground-truth KG path entities, creating dense feedback on intermediate reasoning steps rather than sparse outcome-only signals. This works because KG paths represent verifiable, complete logical chains that models aligning with are reasoning correctly.

### Mechanism 2: SFT Provides Necessary Axiomatic Grounding
SFT provides necessary axiomatic grounding; RL alone is insufficient for compositional reasoning at small model scales. The base model lacks domain knowledge, and zero-RL with 5k-24k examples fails to match SFT-only performance. SFT first installs atomic facts and reasoning patterns; RL then amplifies compositional logic to connect these patterns, following the principle that "SFT memorizes, RL generalizes."

### Mechanism 3: Asymmetric Negative Reinforcement for Stable Learning
Asymmetric negative reinforcement combined with path alignment provides stable learning dynamics for compositional reasoning. Binary reward uses asymmetric penalty (α=0.1 for correct, β=-1.0 for incorrect) per Zhu et al. (2025). This encourages exploration of correct alternate paths while penalizing wrong answers strongly, balancing outcome correctness with process alignment for stable policy optimization.

## Foundational Learning

- **Knowledge Graph Triple Representation (head, relation, tail)**: The entire reward mechanism depends on representing domain knowledge as traversable paths of triples. Understanding that (SMARCB1 mutation, causes, RTPS) is a single hop, and multi-hop paths chain these, is essential. *Quick check*: Given triples (A, treats, B) and (B, causes, C), what 2-hop inference connects A to C?

- **Group Relative Policy Optimization (GRPO)**: The RL stage uses GRPO, which normalizes advantages at the group level and eliminates the need for a separate critic. Understanding how group-level normalization affects reward scaling is critical for debugging. *Quick check*: How does GRPO differ from standard PPO in handling advantage estimation?

- **Compositional Generalization**: The paper's central claim is that training on 1-3 hop paths enables 4-5 hop reasoning. This requires understanding composition as combining learned primitives into novel sequences, not memorizing longer chains. *Quick check*: If a model sees (A→B) and (B→C) paths during training, what type of generalization is required to solve (A→B→C→D)?

## Architecture Onboarding

- **Component map**: Base LLM (Qwen3 14B) → SFT Stage (LoRA, r=16, α=16, lr=2e-4) → SFT Checkpoint → RL Stage (GRPO, lr=8e-6, temp=0.6, G=2) → Final SFT+RL Model

- **Critical path**: The path alignment reward extraction is the most fragile component. It requires: (1) parsing `<think`> blocks from model output, (2) tokenizing and normalizing, (3) matching against ground-truth path tokens from the KG. Errors in any step produce noisy rewards that destabilize training.

- **Design tradeoffs**: RL data budget (5k) vs. SFT budget (19.66k): Authors found larger RL budgets yield diminishing returns and instability; small but high-quality RL is more efficient. LoRA-only RL updates (66.75% accuracy) vs. full-parameter updates (82.20%): Restricted updates insufficient for compositional reasoning. Combining all rewards collapsed to 55.21%: Over-optimization with conflicting signals causes failure.

- **Failure signatures**: Accuracy plateaus or degrades after initial RL epochs → check reward hacking (repetition penalty ϕ_rep=1.15 should help). Model generates fluent reasoning but wrong answers → R_bin weight may be too low; increase β penalty. High accuracy on 2-hop but collapse on 5-hop → SFT coverage may be insufficient; R_path minimum-hit constraint may be too permissive.

- **First 3 experiments**: 1) Reproduce Zero-RL baseline: Apply GRPO directly to base model with 5k examples using R_bin only. Verify that accuracy stays below SFT-only (~65% vs. 70.86%). 2) Ablate reward components: Train SFT→RL with R_path only (no R_bin), then R_bin only (no R_path), then both. Expected: R_path only (~79.29%), R_bin only (~79.54%), both (~82.20%). 3) Hop generalization test: Evaluate on held-out 2-hop, 3-hop, 4-hop, 5-hop splits separately. Verify the gap between SFT-only and SFT+RL widens with hop count (Figure 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
Can knowledge graph-derived reward signals enable compositional reasoning in domains beyond medicine, such as legal reasoning, organic chemistry, or engineering? The authors state: "Any scientific or technical field that can be represented as a structured KG (from organic chemistry to case law) is a candidate for this pipeline. We encourage future research to explore...broader domains." This remains unresolved as the paper only validates in the medical domain.

### Open Question 2
What is the theoretical limit of compositional generalization—can models trained on short paths (1-3 hops) generalize to arbitrarily long reasoning chains (6+ hops)? The paper demonstrates generalization from 1-3 hop training to 4-5 hop testing, but does not explore whether this positive compositional gradient continues or plateaus at longer chain lengths. The ICD-Bench test set only contains 2-5 hop questions.

### Open Question 3
How robust is the path-aligned reward signal to incompleteness or noise in the underlying knowledge graph? The method assumes access to a high-quality, comprehensive KG (UMLS), but real-world KGs often contain missing triples, outdated facts, or incorrect relationships that could mislead reward computation. The paper does not ablate the effect of KG quality on reward signal reliability or model performance.

## Limitations
- Evaluation focuses exclusively on medical domain MCQs, constraining conclusions about broader applicability
- Absolute accuracy on 5-hop queries remains moderate (~55-65%), suggesting room for improvement in handling maximum complexity
- KG paths may not fully represent minimal logical chains needed for medical reasoning, potentially limiting generalizability

## Confidence
**High Confidence**: The SFT+RL pipeline with path alignment rewards outperforms both SFT-only and larger frontier models on the ICD-Bench evaluation. The ablation showing that asymmetric negative reinforcement is crucial for stable learning is well-supported by direct experimental evidence.

**Medium Confidence**: The claim that path-derived rewards enable compositional generalization (training on 1-3 hops → solving 4-5 hops) is supported by the observed performance gap, but could also reflect improved memorization or pattern matching rather than true composition. The mechanism by which path alignment specifically encourages intermediate reasoning steps over outcome-only supervision needs more rigorous validation.

**Low Confidence**: The assertion that knowledge graphs are "implicit reward models" is somewhat metaphorical - the KG provides ground-truth paths for process supervision rather than functioning as an actual reward model. The scalability claim (grounding reasoning in structured knowledge is "scalable") is based on a single domain and model scale, limiting its generalizability.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply the SFT+RL pipeline with path-derived rewards to a non-medical KG domain (e.g., Wikipedia-based commonsense reasoning) and evaluate whether the same compositional generalization benefits emerge.

2. **Path Coverage Ablation**: Systematically vary the completeness of ground-truth KG paths used for reward computation (e.g., using 50%, 75%, 100% of path tokens) and measure the impact on final accuracy.

3. **Alternative Process Supervision Comparison**: Replace KG-derived path alignment with other forms of process supervision (e.g., chain-of-thought alignment from human-annotated reasoning traces) while keeping the same SFT+RL framework.