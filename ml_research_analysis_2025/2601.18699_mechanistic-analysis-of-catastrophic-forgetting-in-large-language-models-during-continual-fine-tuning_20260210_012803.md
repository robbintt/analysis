---
ver: rpa2
title: Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During
  Continual Fine-tuning
arxiv_id: '2601.18699'
source_url: https://arxiv.org/abs/2601.18699
tags:
- forgetting
- gradient
- attention
- task
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study provides the first comprehensive mechanistic analysis
  of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning.
  The authors identify three primary mechanisms: gradient interference in attention
  weights (disrupting 15-23% of heads in lower layers), representational drift in
  intermediate layers (CKA similarity decreases by 0.32-0.47), and loss landscape
  flattening around prior task minima.'
---

# Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning

## Quick Facts
- arXiv ID: 2601.18699
- Source URL: https://arxiv.org/abs/2601.18699
- Reference count: 40
- Primary result: First comprehensive mechanistic analysis of catastrophic forgetting in LLMs, identifying three mechanisms: gradient interference (15-23% attention heads), representational drift (CKA drop 0.32-0.47), and loss landscape flattening (eigenvalue drop ~75%).

## Executive Summary
This study provides the first comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. The authors identify three primary mechanisms: gradient interference in attention weights (disrupting 15-23% of heads in lower layers), representational drift in intermediate layers (CKA similarity decreases by 0.32-0.47), and loss landscape flattening around prior task minima. Key findings include strong correlation between task similarity and forgetting severity (Pearson r=0.87) and predictive power of early gradient alignment metrics (r=-0.79). Attention mechanisms drive early forgetting, with lower layers showing greater susceptibility.

## Method Summary
The study employed sequential fine-tuning across six models (Llama 4 Scout 109B, Llama 4 Maverick 400B, DeepSeek-V3.1 671B, GPT-5.1, Claude Opus 4.5, Gemini 2.5 Pro) using 12 task sequences from 24 NLP tasks across 7 domains. Training used 3-5 epochs per task with batch sizes 32-256, learning rates 1e-5 to 5e-5, and AdamW optimizer. Mechanistic analysis tracked attention weight distances, CKA similarity, gradient cosine similarity between tasks, and Hessian eigenvalues to quantify forgetting mechanisms.

## Key Results
- Gradient interference disrupts 15-23% of attention heads in lower layers during early fine-tuning
- Representational drift causes CKA similarity to decrease by 0.32-0.47 in intermediate layers
- Loss landscape flattening reduces maximum Hessian eigenvalues by ~75% after three subsequent tasks
- Task similarity strongly correlates with forgetting severity (r=0.87)
- Early gradient alignment metrics predict final forgetting (r=-0.79)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Interference in Attention Weights
- Claim: Sequential fine-tuning causes gradient interference concentrated in attention query and key projection matrices, disrupting 15-23% of heads predominantly in lower layers.
- Mechanism: When gradients from new tasks have negative cosine similarity with previous task gradients, attention weights update in directions that degrade prior task circuits. Query/key matrices show 67% conflict rates versus 29% for feedforward weights.
- Core assumption: Gradient direction conflicts during optimization directly translate to functional degradation of learned attention circuits.
- Evidence anchors:
  - [abstract]: "gradient interference in attention weights (disrupting 15-23% of heads in lower layers)"
  - [section 2.3]: "When attention weights were frozen during fine-tuning... forgetting was reduced by 64 percent"
  - [corpus]: "Dynamic Orthogonal Continual Fine-tuning" addresses functional direction drift as a complementary finding
- Break condition: If gradient cosine similarity remains positive (>0.1) throughout first epoch, this mechanism contributes minimally to forgetting.

### Mechanism 2: Representational Drift in Intermediate Layers
- Claim: Fine-tuning causes systematic rotation of dominant principal components in intermediate layers (roughly layers 12-24 in 40-layer models), degrading previously learned feature encodings irrespective of task relevance.
- Mechanism: Leading principal components rotate 35-52 degrees during fine-tuning while higher-order components remain stable. Critically, drift magnitude shows no correlation with task relevance (r=0.12), indicating non-selective reorganization.
- Core assumption: The leading principal components encode critical task-relevant features that cannot be easily recovered once rotated.
- Evidence anchors:
  - [abstract]: "representational drift in intermediate layers (CKA similarity decreases by 0.32-0.47)"
  - [section 2.4]: "CKA scores for intermediate layers decreased by 0.32 to 0.47 compared to 0.08 to 0.14 for upper layers"
  - [corpus]: "Putting a Face to Forgetting" proposes geometric interpretation but with limited empirical validation at scale
- Break condition: If CKA similarity remains above 0.85 across intermediate layers after fine-tuning, representational drift is likely not the primary driver.

### Mechanism 3: Loss Landscape Flattening Around Previous Task Minima
- Claim: Sequential fine-tuning progressively flattens the loss landscape around previous task solutions, with maximum Hessian eigenvalues decreasing by ~75% after three subsequent tasks.
- Mechanism: As optimization proceeds, curvature along previous task-relevant directions diminishes (linearity index increases from 0.28 to 0.71), removing the "restoring forces" that would guide parameters back toward previous task solutions.
- Core assumption: Sharp minima with high curvature provide implicit stability for retaining task knowledge; flattening makes forgetting increasingly irreversible.
- Evidence anchors:
  - [abstract]: "loss landscape flattening around prior task minima"
  - [section 2.6]: "maximum eigenvalues decreasing to 34.2 after training on three subsequent tasks [from 147.3]"; "landscape flattening preceded behavioral forgetting by 1 to 2 epochs"
  - [corpus]: Limited direct corpus evidence on landscape geometry in continual LLM fine-tuning
- Break condition: If curvature regularization maintains Hessian eigenvalues within 20% of original values, this mechanism is substantially suppressed.

## Foundational Learning

- **Gradient Cosine Similarity and Interference**
  - Why needed here: The paper uses gradient alignment as the primary predictive metric (r=-0.79) for forecasting forgetting severity from early training dynamics.
  - Quick check question: Given gradient vectors g₁ and g₂ for two tasks, compute cos(g₁, g₂). What does a value of -0.4 predict about forgetting rate compared to +0.2?

- **Centered Kernel Alignment (CKA) for Representation Similarity**
  - Why needed here: CKA quantifies representational drift, the paper's second mechanism, with decreases of 0.32-0.47 indicating substantial reorganization.
  - Quick check question: If CKA between pre- and post-fine-tuning layer activations drops from 0.92 to 0.55, which layer region is this likely to be and what mechanism does it signal?

- **Hessian Eigenvalue Analysis for Loss Landscape Curvature**
  - Why needed here: The third mechanism is characterized through maximum Hessian eigenvalues as a measure of minimum sharpness and retention stability.
  - Quick check question: A model has max Hessian eigenvalue λ_max = 150 at Task 1 completion. After Task 3, λ_max = 35. What does this indicate about the loss landscape and forgetting reversibility?

## Architecture Onboarding

- **Component map**: Transformer layers 1-12 (attention disruption zone, gradient interference concentrated in Q/K projections) → layers 12-24 (representational drift zone, leading PCs rotate) → global (landscape flattening accumulates across all parameters).

- **Critical path**: First epoch → measure gradient alignment between tasks (predictive r=-0.79) → epochs 1-2 → monitor attention entropy in lower layers (target: identify 15-23% disrupted heads) → epochs 3-5 → track CKA in intermediate layers → epoch 4+ → sample Hessian eigenvalues at task boundaries.

- **Design tradeoffs**: Freezing attention: -64% forgetting but may limit adaptation; curvature regularization: -34% forgetting with +12% convergence cost (more favorable than gradient clipping: -18% forgetting, +27% slowdown); gradient clipping at 0.5 vs 1.0: reduced forgetting but proportionally slower learning.

- **Failure signatures**: First-epoch gradient cosine < -0.3 (2.4x higher forgetting); attention entropy increase >1.8 bits (disrupted head); CKA drop >0.3 in layers 12-24 (significant drift); max Hessian eigenvalue <50% of original (landscape substantially flattened).

- **First 3 experiments:**
  1. Compute gradient alignment between sequential tasks during first 500 steps; validate correlation with final forgetting magnitude (paper reports r=-0.79).
  2. Track attention entropy and weight distance in layers 1-12; identify which heads exceed 2.5σ disruption threshold (target: 15-23% of heads).
  3. Measure CKA and Hessian eigenvalues before/after fine-tuning on a held-out task; quantify representational drift and landscape flattening magnitudes.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the identified mechanisms—gradient interference in attention and representational drift—generalize to state-space models (e.g., Mamba) or encoder-decoder architectures? Basis: [explicit] The Discussion states extending the framework to "alternative architectures like encoder-decoder models, sparse transformers, or state-space models" is necessary to distinguish general principles from architecture-specific phenomena. Why unresolved: The study focused exclusively on transformer decoder and mixture-of-experts (MoE) architectures.

- **Open Question 2**: Can gradient-level or circuit-level compatibility metrics predict forgetting more accurately than surface-level semantic similarity? Basis: [explicit] The Discussion calls for investigating "task similarity metrics beyond surface-level feature overlap, focusing on gradient-level and circuit-level compatibility." Why unresolved: The paradoxical finding that high task similarity correlates with severe forgetting ($r=0.87$) suggests current metrics miss specific parameter-level interference.

- **Open Question 3**: Do optimization dynamics during RLHF or instruction tuning induce distinct forgetting mechanisms compared to the supervised fine-tuning analyzed here? Basis: [explicit] The authors note that "continual learning in reinforcement learning from human feedback or instruction tuning may involve additional mechanisms." Why unresolved: The methodology exclusively examined supervised fine-tuning on distinct NLP tasks.

## Limitations

- Mechanistic decomposition assumes independence between three identified mechanisms, though they likely interact in complex ways during fine-tuning.
- Gradient interference analysis depends on linear approximation validity, which may break down in highly non-convex regions of the loss landscape.
- Hessian eigenvalue measurements at extreme scales (>400B parameters) face computational constraints that may limit precision.

## Confidence

- **High confidence**: Gradient interference as dominant early mechanism (supported by 64% forgetting reduction when attention frozen); loss landscape flattening measurements (eigenvalue drops from 147.3 to 34.2 are substantial and consistent)
- **Medium confidence**: Representational drift characterization (CKA decreases of 0.32-0.47 are well-measured but PCA rotation interpretation remains somewhat speculative)
- **Low confidence**: Integrated mechanistic model coupling timescales (while intuitively compelling, temporal coupling between mechanisms needs more rigorous validation)

## Next Checks

1. Conduct ablation studies isolating each mechanism (attention freezing, curvature regularization, representation stabilization) to quantify individual contributions versus the 15-23% + 0.32-0.47 + 75% reductions
2. Measure mechanism activation patterns across different fine-tuning durations (1-10 epochs) to validate the proposed temporal hierarchy
3. Test mechanism predictions on cross-modal tasks (vision-language models) to assess generalizability beyond text-only domains