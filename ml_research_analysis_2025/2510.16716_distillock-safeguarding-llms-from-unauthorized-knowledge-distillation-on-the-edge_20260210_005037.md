---
ver: rpa2
title: 'DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on
  the Edge'
arxiv_id: '2510.16716'
source_url: https://arxiv.org/abs/2510.16716
tags:
- distillation
- distillock
- knowledge
- data
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DistilLock, a TEE-assisted framework that enables
  privacy-preserving knowledge distillation on edge devices while protecting both
  user data and model intellectual property. The core innovation is using model obfuscation
  with lightweight TEE authorization to prevent unauthorized access to proprietary
  model weights during distillation.
---

# DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge

## Quick Facts
- **arXiv ID**: 2510.16716
- **Source URL**: https://arxiv.org/abs/2510.16716
- **Reference count**: 40
- **Primary result**: TEE-assisted framework that reduces surrogate model attack accuracy to 0.98× black-box baseline (vs 1.69× for white-box attacks) with only 0.44-1.11% TEE FLOP overhead

## Executive Summary
DistilLock addresses the growing threat of unauthorized knowledge distillation on edge devices, where attackers extract proprietary model weights through distillation attacks. The framework combines model weight obfuscation with single-point TEE authorization to protect intellectual property while enabling legitimate fine-tuning. By requiring authorization only at the embedding layer, DistilLock achieves significantly lower TEE overhead (0.44-1.11%) compared to prior methods (3.55-16.40%) while maintaining strong security guarantees.

## Method Summary
DistilLock uses permutation-based weight obfuscation where model owners apply random permutation matrices to transform all weight matrices before deployment. Authorized users receive the inverse permutation inside a TEE enclave, enabling correct forward passes. The TEE performs authorization only once at the embedding layer, leveraging permutation-equivariance properties of LayerNorm and RMSNorm layers to propagate the authorization through the entire network. Unauthorized users receive obfuscated weights that produce random logits, preventing effective knowledge transfer to surrogate models.

## Key Results
- Surrogate model attack accuracy reduced to 0.98× black-box baseline (versus 1.69× for white-box attacks)
- TEE overhead requires only 0.44-1.11% of total FLOPs versus 3.55-16.40% for prior defenses
- Authorized distillation maintains performance while unauthorized attempts collapse to near-random results
- Strong protection across multiple teacher/student model pairs and evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
Permutation-based weight obfuscation preserves forward-pass functionality while hiding model semantics from adversaries. The model owner applies random permutation matrices (π, π_emb) to transform all weight matrices before deployment. When authorized inputs receive the inverse permutation inside the TEE, the mathematical operations yield identical outputs. Without authorization, misaligned permutations produce semantically meaningless activations. Core assumption: adversaries cannot efficiently guess the permutation matrix (probability ~1/d! for dimension d).

### Mechanism 2
Single-point TEE authorization at the embedding layer is sufficient to lock the entire forward pass, achieving 0.44-1.11% TEE FLOP overhead versus 3.55-16.40% for prior methods. The permutation propagates through all transformer blocks automatically due to permutation-equivariance of LayerNorm/RMSNorm layers. TEE computation is needed only once: apply π_emb and π to input embeddings. All subsequent blocks maintain functional equivalence on GPU without further TEE intervention.

### Mechanism 3
Unauthorized distillation collapses because the obfuscated teacher produces near-random logits, providing no useful learning signal. Without TEE authorization, input x passes through the network as x' ≠ xπ, causing misaligned activations throughout. The final classifier produces essentially random logits that bear no systematic relationship to the true distribution, making gradient-based knowledge transfer ineffective.

## Foundational Learning

- **Knowledge Distillation fundamentals**: Understanding why soft targets (temperature-scaled logits) provide more information than hard labels for student training is crucial since the entire threat model assumes attackers use KD to extract model functionality.
- **Trusted Execution Environments security model**: DistilLock's security rests on TEE guarantees—confidentiality and integrity of code/data. The threat model explicitly assumes TEE is "secure and uncompromisable."
- **Transformer architecture equivalence under transformation**: Understanding why W' = π^T W preserves Q' = x'W'_q requires knowing how attention, LayerNorm, and MLP layers compose.

## Architecture Onboarding

- **Component map**: Model Owner (offline) -> generates π, π_emb -> obfuscates weights -> packages W' weights + encrypted π for delivery -> User TEE Enclave (stores encrypted π, performs authorization) -> User GPU (untrusted) (runs obfuscated teacher forward pass, student model training, KD loss computation)
- **Critical path**: 1) Pre-deployment: Obfuscate all weights using random permutation π 2) Runtime authorization: TEE applies π_emb and π to input embeddings 3) GPU forward pass: All transformer blocks execute with obfuscated weights 4) Logit generation: Final classifier produces correct q_T only if step 2 succeeded 5) Cleanup: Remove obfuscated teacher after distillation completes
- **Design tradeoffs**: Security vs Efficiency (smaller OTP noise reduces TEE computation but must maintain entropy), TEE placement (only embedding layer minimizes overhead but assumes permutation propagation robustness), Obfuscation key management (storing π in TEE vs external encrypted storage)
- **Failure signatures**: Unauthorized KD succeeds (Student accuracy improves → π_emb or π leaked), Authorized KD fails (Student accuracy similar to pre-distillation → permutation misalignment), Performance degradation >expected (TEE overhead higher than 1.11% → check OTP implementation)
- **First 3 experiments**: 1) Verify functional equivalence: Run identical inputs through original model vs obfuscated model with correct TEE authorization—logits should match within numerical precision 2) Test unauthorized distillation blocking: Train student on obfuscated teacher outputs without TEE authorization on HellaSwag/CommonSenseQA—confirm accuracy ≤ pre-distillation baseline 3) Benchmark TEE overhead: Measure FLOPs inside TEE vs total FLOPs across LLaMA3.2-3B, Qwen2.5-1.5B—target 0.44-1.11% range

## Open Questions the Paper Calls Out
None

## Limitations
- Security claims rely heavily on TEE integrity, creating fundamental vulnerability if TEE is compromised through side-channel attacks or microarchitectural vulnerabilities
- Limited evaluation of real-world attack scenarios with only 1000 examples per task, potentially underestimating sophisticated extraction attempts
- No testing against membership inference or property inference attacks that could reveal model characteristics without full parameter extraction

## Confidence

**High Confidence**: Functional equivalence proof for permutation-based obfuscation is mathematically sound. TEE overhead measurements (0.44-1.11% vs 3.55-16.40% for baselines) are directly verifiable.

**Medium Confidence**: Unauthorized distillation blocking mechanism (0.98× black-box baseline) is well-supported but assumes standard KD methods. Doesn't evaluate sophisticated extraction techniques.

**Low Confidence**: Long-term security against adaptive attackers is untested. No analysis of whether repeated distillation attempts could gradually reconstruct permutation matrix.

## Next Checks

1. **TEE Integrity Stress Test**: Implement side-channel attack simulation on the authorization module (timing analysis of the embedding layer TEE call) to verify whether permutation leakage occurs through observable timing differences between authorized and unauthorized inputs.

2. **Statistical Analysis Attack**: Develop a statistical attack that analyzes multiple obfuscated output distributions to attempt permutation matrix reconstruction. Test whether 10,000+ inference queries can reveal π structure, and measure the attack success rate versus query count.

3. **Cross-Architecture Transferability**: Evaluate whether knowledge from the obfuscated teacher can be transferred to students with different architectures (e.g., from transformer to LSTM, or different embedding sizes) that might bypass the permutation alignment requirement.