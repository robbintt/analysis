---
ver: rpa2
title: Towards Trustworthy Multimodal Recommendation
arxiv_id: '2602.00730'
source_url: https://arxiv.org/abs/2602.00730
tags:
- multimodal
- modality
- item
- recommendation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses trustworthiness in multimodal recommendation
  systems, where misleading content and unreliable interactions can degrade performance.
  It proposes a plug-and-play modality-level rectification module that learns soft
  correspondences between items and multimodal features via lightweight projection
  and Sinkhorn-based matching, suppressing mismatched signals while preserving consistency.
---

# Towards Trustworthy Multimodal Recommendation

## Quick Facts
- arXiv ID: 2602.00730
- Source URL: https://arxiv.org/abs/2602.00730
- Reference count: 40
- Primary result: Modality-level rectification via Sinkhorn matching improves robustness under modality corruption (e.g., +0.007 Recall@10 on Baby under 20% image corruption)

## Executive Summary
This paper addresses trustworthiness in multimodal recommendation systems where misleading content and unreliable interactions degrade performance. It proposes a plug-and-play modality-level rectification module that learns soft correspondences between items and multimodal features via lightweight projection and Sinkhorn-based matching, suppressing mismatched signals while preserving consistency. The method is integrated into existing recommenders without architectural changes. Extensive experiments show improved robustness under modality corruption and reveal that graph-based edge editing can help or hurt under noisy interactions, depending on prior–signal alignment.

## Method Summary
The method operates as offline preprocessing before standard multimodal recommenders. It first extracts anchor embeddings from a pre-trained LightGCN on the interaction graph, then learns per-modality linear projections using small-loss selection to filter out mismatched pairs. Sparse affinity matrices are constructed via top-K cosine similarity with temperature scaling, then balanced using Sinkhorn-Knopp iterations to produce soft matching matrices. The rectified features are aggregated from the original and matched modality signals with a diagonal mix parameter. The module is plug-and-play, requiring no changes to existing backbone architectures like VBPR, FREEDOM, or LATTICE.

## Key Results
- MR improves robustness under modality corruption, with gains widening as corruption increases (e.g., +0.007 Recall@10 on Baby under 20% image corruption)
- Graph-based edge editing effectiveness depends on prior-signal alignment: helpful under clean interactions, potentially harmful under noise
- Sinkhorn balancing outperforms row-normalization, reducing hubness where few signals attract most items
- Small-loss selection during projection training prevents contamination from mismatched pairs

## Why This Works (Mechanism)

### Mechanism 1: Sinkhorn-based Soft Matching for Correspondence Correction
- Claim: Soft matching corrects modality-item misalignments without hard reassignment
- Mechanism: The module computes sparse affinity between anchor embeddings and projected modality features, then applies Sinkhorn-Knopp iterative scaling to produce approximately doubly-stochastic matching matrices. This yields balanced soft correspondences where each item receives weighted contributions from multiple modality signals, avoiding hubness.
- Core assumption: The interaction graph is more reliable than modality features; anchor embeddings encode true item semantics that mismatched modalities should align toward.
- Evidence anchors:
  - [abstract] "learns soft correspondences between items and multimodal features via lightweight projection and Sinkhorn-based matching, suppressing mismatched signals while preserving semantic consistency"
  - [section 4.2, Step 3] "To obtain a more balanced correspondence that approximates a soft one-to-one assignment, we apply a Sinkhorn–Knopp scaling procedure on A^m and obtain a soft matching matrix P^m... to make P^m approximately doubly-stochastic"
  - [corpus] Related work addresses modality noise via spectral methods but not Sinkhorn matching specifically
- Break condition: If the interaction graph itself is heavily corrupted, anchor embeddings become unreliable, causing rectification to align toward wrong semantics

### Mechanism 2: Small-loss Selection for Robust Projection Training
- Claim: Selecting only high-confidence item-modality pairs during projection training prevents contamination from mismatched pairs
- Mechanism: Within each mini-batch, keep only the lowest-loss (highest cosine similarity between anchor and projected modality) subset at ratio ρ. Since mismatched pairs have lower anchor-modality similarity, they produce higher loss and are excluded from optimization.
- Core assumption: Correctly matched item-modality pairs have higher similarity to anchor embeddings than mismatched pairs on average.
- Evidence anchors:
  - [section 4.2, Step 1] "Since a fraction of observed pairs (i, ẽ^m_i) can be mismatched, directly minimizing the average loss is sensitive. We thus employ a small-loss selection strategy"
  - [section 6.4] "In controlled modality-misalignment stress tests, we set ρ = η_m + 0.05, where η_m is the injected misalignment ratio"
  - [corpus] Limited direct corpus support; addresses modality uncertainty but uses different approach
- Break condition: If corruption creates symmetric similarity, small-loss selection cannot distinguish them

### Mechanism 3: Prior-Signal Alignment Determines Edge Editing Effectiveness
- Claim: Similarity-based edge editing helps only when the collaborative prior aligns with true preferences; under noise, editing can amplify errors
- Mechanism: When interactions are clean, LightGCN prior produces meaningful similarity, so completion adds useful edges and pruning removes noise. When corrupted, the prior is contaminated: completion adds false positives, pruning may remove rare-but-informative edges. Graph propagation amplifies these through multi-hop message passing.
- Core assumption: There exists an underlying "true" preference structure that collaborative filtering attempts to estimate; corruption perturbs this estimation non-randomly.
- Evidence anchors:
  - [abstract] "graph-based edge editing can help or hurt under noisy interactions, depending on prior–signal alignment"
  - [section 5.3] "When there is no interaction noise, the LightGCN prior is generally reliable, and both completion and pruning are typically beneficial. When noise is injected, the effectiveness of edge editing becomes unstable"
  - [corpus] Related works address graph-based multimodal recommendation but do not analyze edge editing failure modes under noise
- Break condition: High interaction corruption rates cause the prior to contain more noise than signal; any editing based on it becomes harmful

## Foundational Learning

- Concept: **Sinkhorn-Knopp Algorithm / Optimal Transport**
  - Why needed here: Core mechanism for computing balanced soft correspondences between items and modality signals. Unlike row-normalization, Sinkhorn produces near-uniform assignment weights across both dimensions.
  - Quick check question: Given an N×N affinity matrix A, what property does a "doubly stochastic" matrix satisfy? Why might row-normalization alone cause "hubness" where a few modality signals attract most items?

- Concept: **Graph Neural Network Message Passing**
  - Why needed here: Understanding error amplification through multi-hop neighbors. Graph-based models construct modality-induced item graphs—if features are corrupted, erroneous neighbor connections propagate signals through the entire graph.
  - Quick check question: If item A has corrupted visual features and connects to items B, C, D via modality-similarity edges, how does 2-hop message passing spread this error to items that share no direct modality similarity with A?

- Concept: **BPR Loss and Implicit Feedback Evaluation**
  - Why needed here: Paper evaluates on BPR-based recommenders. Understanding why "Train-only" edge editing changes the supervision signal differently than "Graph-only" editing changes propagation—and why filtering training positives affects metric interpretation.
  - Quick check question: If you delete training positives and then filter candidates using the edited (smaller) training set, why might Recall appear higher even if true preference learning did not improve?

## Architecture Onboarding

- Component map:
  Raw Modality Features (ẽ^m_i, m ∈ {visual, textual}) → [Anchor Embeddings] ← Pre-trained LightGCN on interaction graph → [Lightweight Projection g_m(·)] → z^m_i (learned, with small-loss selection) → [Sparse Affinity A^m] ← top-K cosine similarity, temperature τ → [Sinkhorn Balancing] → Soft matching matrix P^m (iterative, O(NK)) → [Correspondence Aggregation] → ê^m_i = Σ_j P^m_ij × ẽ^m_j → [Diagonal Prior Mix] → e^m,rect_i = λ × ẽ^m_i + (1-λ) × ê^m_i → Existing Backbone (VBPR/LATTICE/FREEDOM/MGCN/SMORE) — unchanged

- Critical path:
  1. Pre-train LightGCN on interaction graph → obtain normalized anchor embeddings ē_i
  2. Train projection g_m(·) using small-loss selection (only trainable component in rectification)
  3. Compute sparse affinity matrix A^m across all items (top-K, force diagonal inclusion)
  4. Run Sinkhorn iterations to convergence (post-processing, no gradients)
  5. Aggregate rectified features using P^m, apply diagonal mix with λ
  6. Freeze rectified features, train backbone with original objective

- Design tradeoffs:
  - **λ (diagonal mix)**: λ≈0.6 optimal under 20% corruption; λ→1 keeps original (conservative but weak correction); λ→0 fully replaces (aggressive, may over-correct clean signals)
  - **ρ (small-loss keep ratio)**: Must approximate (true_corruption_rate + inherent_noise). Paper uses ρ = η_m + 0.05 in controlled tests
  - **K (top-K neighbors)**: Controls sparsity/computation. Larger K = more candidates for Sinkhorn = O(NK) cost
  - **Temperature τ**: Lower τ = sharper affinities. Paper finds τ ∈ [0.05, 0.2] stable; τ=0.5 over-smooths

- Failure signatures:
  - Clean performance degrades: λ too low (over-rectification), or projection failed (check training loss)
  - No improvement under corruption: ρ set too high (corrupted pairs included in projection training)
  - OOM on large item sets: K too large; use smaller K or batch Sinkhorn
  - "Train-only" editing shows large gains that disappear with consistent filtering: evaluation artifact from smaller filtered candidate set

- First 3 experiments:
  1. **Sanity check on clean data**: Run backbone ±MR on uncorrupted features. Verify Table 2: MR should not degrade clean performance. If it does, λ is too aggressive or projection is overfitting.
  2. **Corruption stress test**: Inject modality corruption at η_m ∈ {0.1, 0.2, 0.3, 0.4, 0.5}. Plot Recall@10 curves. Expect widening gap between base and rectified as corruption increases. Replicate Figure 2 patterns.
  3. **Ablate Sinkhorn**: Replace Sinkhorn with row-normalization. Expect performance drop, especially on datasets with high item similarity variance (hubness). Verify Figure 5 pattern where "w/o Sink" underperforms "Full."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can modality-level rectification be extended to additional modalities beyond image/text, such as audio and video, where temporal alignment introduces new correspondence challenges?
- Basis in paper: [explicit] "Our study focuses on image/text modalities and specific corruption patterns; future work may extend to additional modalities and more realistic trustworthiness failures."
- Why unresolved: The current method assumes static features; temporal modalities require correspondence estimation across both items and time dimensions, and Sinkhorn matching may not scale efficiently to sequential data.
- What evidence would resolve it: Experiments on datasets with audio/video modalities demonstrating robustness under modality corruption with comparable or adapted rectification mechanisms.

### Open Question 2
- Question: How can principled, alignment-aware mechanisms be designed for graph editing that adaptively control the degree of edge modification based on estimated prior reliability?
- Basis in paper: [explicit] "Designing principled, alignment-aware mechanisms for graph editing and propagation under untrustworthy interactions also remains an important direction."
- Why unresolved: Current edge editing is either applied or not; there is no framework to quantify prior reliability and modulate editing intensity accordingly, especially when the collaborative prior itself is contaminated.
- What evidence would resolve it: A method that estimates prior-signal alignment and dynamically adjusts pruning/completion ratios, validated through improved robustness across noise levels.

### Open Question 3
- Question: How can the misalignment rate ρ be reliably estimated in real-world deployment without labeled ground-truth audits?
- Basis in paper: [inferred] The paper uses controlled corruption ratios and notes "ρ can be chosen via a lightweight estimate of the misalignment rate (e.g., random auditing of image–text pairs or simple consistency checks)," but provides no mechanism for unsupervised estimation.
- Why unresolved: The small-loss selection strategy assumes knowledge of the corruption ratio; in practice, platforms lack ground-truth labels for which item–modality pairs are mismatched.
- What evidence would resolve it: An unsupervised estimator that correlates with true corruption levels and enables adaptive rectification without hyperparameter tuning per dataset.

## Limitations
- Key implementation details remain underspecified, including top-K neighbor count, LightGCN anchor training hyperparameters, and projection output dimensions
- Controlled experiments assume known corruption ratios, but real-world deployment requires robust estimation of modality misalignment
- Computational scaling beyond noting LATTICE struggles on the largest dataset is not addressed

## Confidence
- **High**: The core mechanism of Sinkhorn-based soft matching for correspondence correction (supported by explicit algorithmic descriptions and empirical improvements under controlled corruption)
- **Medium**: The effectiveness of small-loss selection for robust projection training (mechanism described but lacks direct ablation showing its necessity)
- **Medium**: The analysis of prior-signal alignment determining edge editing effectiveness (qualitative claims supported by limited experiments, but interaction between edge editing and rectification not fully explored)

## Next Checks
1. **Ablate small-loss selection**: Run MR with ρ=1.0 (no selection) under 20% corruption; expect performance degradation if the mechanism is critical
2. **Estimate corruption without ground truth**: Implement heuristic for estimating η_m from anchor-modality similarity distributions; validate against known corruption ratios
3. **Edge editing + rectification interaction**: Test whether applying MR before/after different edge editing strategies changes the prior-signal alignment effects observed in section 5.3