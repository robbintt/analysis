---
ver: rpa2
title: Learning to Reason as Action Abstractions with Scalable Mid-Training RL
arxiv_id: '2509.25810'
source_url: https://arxiv.org/abs/2509.25810
tags:
- mid-training
- arxiv
- action
- reasoning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unlocking the full potential
  of reinforcement learning (RL) for large language models (LLMs) by formalizing the
  role of mid-training. It presents the first theoretical analysis showing that mid-training
  should identify an action subspace that minimizes both pruning error (approximation
  error from reducing the action space) and post-training RL error (error when planning
  within the pruned space).
---

# Learning to Reason as Action Abstractions with Scalable Mid-Training RL

## Quick Facts
- **arXiv ID**: 2509.25810
- **Source URL**: https://arxiv.org/abs/2509.25810
- **Reference count**: 19
- **Key result**: RA3 improves average HumanEval and MBPP performance by 8 and 4 points over base models and next-token prediction baselines.

## Executive Summary
This paper addresses the challenge of unlocking the full potential of reinforcement learning (RL) for large language models (LLMs) by formalizing the role of mid-training. It presents the first theoretical analysis showing that mid-training should identify an action subspace that minimizes both pruning error (approximation error from reducing the action space) and post-training RL error (error when planning within the pruned space). Based on these insights, the paper proposes Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm that iteratively discovers temporally-consistent latent structures via RL and fine-tunes on bootstrapped data. Experiments on code generation tasks with models ranging from 1B to 8B parameters demonstrate RA3's effectiveness.

## Method Summary
RA3 is a scalable mid-training algorithm that discovers temporally-consistent latent action abstractions via an EM-style optimization. The E-step uses RL to sample latent trajectories that "explain" expert behavior (reward = log likelihood of actions given latents), while the M-step fine-tunes the base LLM on bootstrapped (state, latent, action) triples. A temporal consistency prior and KL penalty regulate the frequency of reasoning. The method operates in the space of action abstractions (e.g., code lines) rather than primitive tokens, reducing the effective planning horizon and accelerating RLVR convergence.

## Key Results
- RA3 improves average performance on HumanEval and MBPP by 8 and 4 points over base models and next-token prediction baselines.
- RA3 achieves faster convergence and higher asymptotic performance in RLVR across multiple benchmarks including HumanEval+, MBPP+, LiveCodeBench, and Codeforces.
- Theoretical analysis shows mid-training effectiveness depends on pruning efficiency and impact on RL convergence, with optimal operation in action abstraction space.

## Why This Works (Mechanism)

### Mechanism 1: Action Subspace Pruning Creates Stronger Policy Priors
Mid-training that identifies a compact action subspace reduces the burden on post-training RL by eliminating suboptimal actions before online interaction begins. The paper proves that the number of expert demonstrations required to prune suboptimal actions scales with the cardinality of the minimal ε-optimal action subset. By defining actions as temporally-extended abstractions rather than primitive tokens, both the total action space and the minimal optimal subset shrink, making pruning more sample-efficient.

### Mechanism 2: Temporal Abstraction Shortens Effective Planning Horizon
Learning in the space of temporally-extended actions accelerates post-training RL convergence by reducing the effective discount factor. Policy iteration converges faster when each Bellman backup spans multiple primitive steps, shrinking the effective horizon. Longer actions (larger duration) reduce the effective discount factor, causing each backup to span multiple primitive steps and accelerating convergence.

### Mechanism 3: Self-Supervised RL Discovers Latent Rationales via Variational Inference
Optimizing a temporal ELBO via EM iterations uncovers hidden reasoning structures that bootstrap more effective supervised fine-tuning. The E-step treats log-likelihood of expert actions as reward, using RL to discover latent sequences that "explain" expert behavior. The M-step fine-tunes on the bootstrapped (state, latent, action) triples, with a KL penalty enforcing that latents function as coherent abstractions.

## Foundational Learning

- **Concept: Markov Options / Temporal Abstractions**
  - Why needed: RA3's action abstractions are a variant of options—understanding how temporally-extended actions affect planning and convergence is prerequisite to grasping Theorems 3.4–3.5.
  - Quick check: Can you explain why an option with duration τ reduces the effective discount factor from γ to γ^τ?

- **Concept: Variational Inference and ELBO**
  - Why needed: The core RA3 algorithm optimizes a sequential ELBO; understanding the E-step (posterior inference) and M-step (parameter update) is essential to follow Section 4.
  - Quick check: In standard VAEs, the ELBO decomposes into reconstruction loss and KL divergence. What is the analogous decomposition in RA3's temporal ELBO?

- **Concept: Policy Iteration Convergence**
  - Why needed: Theoretical analysis of mid-training impact on RL uses policy iteration as the analysis vehicle; convergence rates depend on contraction properties.
  - Quick check: Why does a smaller effective discount factor γ̃ lead to faster convergence in policy iteration?

## Architecture Onboarding

- **Component map**: LLM π → E-step (RL with log-likelihood reward + KL penalty) → collect bootstrapped (s, z, a) → M-step (NTP fine-tuning) → repeat EM iterations
- **Critical path**: Initialize with base LLM and mid-training corpus → E-step: RL to maximize J(π, q) → collect bootstrapped trajectories → M-step: NTP fine-tuning on (s, z, a) → repeat EM iterations until convergence
- **Design tradeoffs**: Penalty c (low → excessive reasoning, high → RA3→NTP), latent max length (expressiveness vs inference cost), truncated return horizon (credit assignment vs coherence)
- **Failure signatures**: CE loss not decreasing during M-step (noisy latents), reasoning frequency >80% (KL penalty too low), RLVR performance not improving over NTP (pruning removed useful actions)
- **First 3 experiments**:
  1. Ablation on penalty c: Sweep c ∈ {0.01, 0.03, 0.05, 0.2} on validation set; plot reasoning frequency vs mean accuracy to find optimal operating point.
  2. EM iteration monitoring: Track CE loss and benchmark accuracy across iterations; verify each M-step reduces loss and improves accuracy.
  3. RLVR transfer test: Take best RA3 and NTP checkpoints; run GRPO on RLVR dataset; compare convergence speed and asymptotic performance.

## Open Questions the Paper Calls Out

- **Question**: Does the RA3 framework generalize to reasoning domains outside of code generation, such as mathematical reasoning or long-horizon agentic tasks?
  - Basis: Authors state they focus on Python code generation despite theoretical framework applying to general MDPs.
  - Why unresolved: Efficacy of temporal variational bound only validated on code data where lines map neatly to logical steps.
  - Evidence needed: Applying RA3 to mathematical benchmarks or agentic navigation tasks and observing convergence rates similar to code domain.

- **Question**: How sensitive is the algorithm to the definition of "primitive actions," specifically choice of code lines versus tokens?
  - Basis: Section 7 defines primitive actions as "a single line of code" for implementation convenience.
  - Why unresolved: Unclear if performance gain comes from algorithm itself or specific granularity of action abstraction.
  - Evidence needed: Comparison study running RA3 with token-level versus line-level actions on same benchmarks.

- **Question**: Do theoretical convergence guarantees derived for policy iteration hold for practical policy gradient methods (GRPO) used in experiments?
  - Basis: Theorem 3.5 analyzes policy iteration convergence, but implementation uses GRPO.
  - Why unresolved: Policy gradient methods have different convergence properties than policy iteration.
  - Evidence needed: Theoretical extension of Theorem 3.5 to policy gradient settings or empirical analysis of convergence variance under GRPO.

## Limitations
- Reliance on proprietary or non-public datasets prevents direct replication and forces dependence on author clarification.
- Theoretical framework assumes expert demonstrations contain sufficient coverage of near-optimal actions, which may not hold for complex code generation tasks.
- Temporal consistency prior and KL penalty hyperparameter c are critical to performance but require careful tuning that may not generalize across domains.

## Confidence

- **High confidence**: Theoretical analysis connecting action subspace pruning to policy priors and convergence (Theorems 3.4-3.5) is well-founded and consistent with established RL theory. EM algorithm specification and training procedure are clearly described.
- **Medium confidence**: Empirical improvements on code generation benchmarks are convincing but limited to specific model sizes (1B-8B) and may not scale to frontier models.
- **Low confidence**: Transferability of action abstractions across different code generation tasks and robustness when expert data coverage is limited remain unclear without additional experiments.

## Next Checks

1. **Dataset reproducibility test**: Construct comparable mid-training corpus from publicly available Python code repositories and verify if RA3 still improves over NTP baselines on HumanEval/MBPP.
2. **Cross-task generalization study**: Evaluate whether action abstractions learned on one code generation task transfer to semantically different tasks without additional mid-training.
3. **Scaling experiment**: Test RA3 with larger models (e.g., Llama-3.1-70B, Qwen2.5-72B) to determine if convergence benefits and performance gains scale proportionally or diminish at frontier sizes.