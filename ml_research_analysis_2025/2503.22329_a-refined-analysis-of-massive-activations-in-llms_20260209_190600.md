---
ver: rpa2
title: A Refined Analysis of Massive Activations in LLMs
arxiv_id: '2503.22329'
source_url: https://arxiv.org/abs/2503.22329
tags:
- activations
- massive
- token
- attention
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of massive activations in large
  language models (LLMs), which can cause numerical instability and impact performance,
  particularly in low-precision training and quantization. It conducts a comprehensive
  analysis of massive activations across various LLM architectures, including both
  GLU-based and non-GLU-based models.
---

# A Refined Analysis of Massive Activations in LLMs

## Quick Facts
- arXiv ID: 2503.22329
- Source URL: https://arxiv.org/abs/2503.22329
- Reference count: 40
- Primary result: Massive activations are not universally detrimental; hybrid strategies like TVR+KV bias can mitigate them while preserving downstream performance.

## Executive Summary
This paper conducts a comprehensive analysis of massive activations in large language models (LLMs), challenging the assumption that all such activations are harmful. Through extensive experiments across GLU and non-GLU architectures, the authors demonstrate that massive activations can function as implicit bias terms and that their suppression doesn't always lead to performance collapse. The study introduces hybrid mitigation strategies combining Target Variance Rescaling with Attention KV bias or Dynamic Tanh, which successfully reduce extreme activation values while maintaining or improving downstream task performance.

## Method Summary
The study analyzes massive activations (defined as max(|h|) > 100 and max(|h|) ≥ 1000× median(|h|)) across various LLM architectures including LLaMA, GPT-2, Phi-2, and others. Mitigation strategies include Attention KV bias (augmenting attention with learnable key-value embeddings), Target Variance Rescaling (TVR) to control weight initialization and updates, and Dynamic Tanh (DyT) to replace normalization layers. Experiments are conducted on LLaMA-1B with specific hyperparameters: AdamW optimizer, LR 6e-4→6e-5, BFloat16 precision, and training on SmolLM corpus. Performance is evaluated using perplexity metrics and downstream task accuracy across seven benchmarks.

## Key Results
- Massive activations exist across LLM architectures but their impact varies significantly between GLU and non-GLU models
- Suppressing massive activations doesn't always degrade performance—non-GLU models like GPT-2 show minimal impact when activations are zeroed
- Hybrid strategies (TVR+KV bias, TVR+DyT) effectively reduce activation magnitudes while maintaining or improving downstream accuracy
- Attention concentration persists even when massive activations are eliminated, suggesting the phenomena are coupled but not identical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Massive activations function as implicit bias terms that concentrate attention on specific tokens, but this behavior is architecture-dependent and not universally necessary for model function.
- Mechanism: A small subset of hidden states reach extreme values (>100 and ≥1000× median magnitude). These activations remain constant across intermediate layers and bind to specific token positions (starting tokens, delimiters, weak-semantic tokens). Through attention output decomposition, these tokens produce nearly identical value updates across all positions, effectively acting as learned bias vectors embedded in the attention computation.
- Core assumption: The model has learned to repurpose certain tokens as bias carriers rather than relying on explicit architectural bias terms.
- Evidence anchors:
  - [abstract] "not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance"
  - [section 4.1] Non-GLU models (GPT-2, Phi-2, OPT-6.7B, Falcon-2-11B) show minimal perplexity degradation when massive activations are zeroed
  - [corpus] Related work "Hidden Dynamics of Massive Activations in Transformer Training" confirms massive activations emerge during training and become critical for model functionality, though dynamics vary
- Break condition: When normalization strategies differ (pre- vs post-layernorm), the activation magnitude pattern shifts from constant across layers to uptrending (observed in Gemma-2, Gemma-3).

### Mechanism 2
- Claim: Attention KV bias mitigation introduces explicit learnable bias parameters that replace the implicit bias function of massive activations, but this only transfers successfully in architectures where attention naturally concentrates on a single token.
- Mechanism: Augment attention with additional key-value embeddings k', v' that act as an extra "token" during attention computation. This transient token is concatenated to K and V, participates in softmax normalization, then its influence integrates into attention output via matrix multiplication. The model should learn to use these explicit parameters instead of repurposing input tokens.
- Core assumption: The model's attention concentration pattern is fundamentally a bias-seeking behavior that can be redirected to explicit parameters.
- Evidence anchors:
  - [abstract] "proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases"
  - [section 4.3] KV bias reduces massive activations in GPT-2 but fails completely in LLaMA-1B; attention patterns show KV bias doesn't eliminate concentration—attention shifts to the extra token or BOS token depending on architecture
  - [corpus] "Softpick: No Attention Sink, No Massive Activations" proposes alternative rectified softmax that eliminates both attention sinks and massive activations, suggesting the mechanism is tied to softmax normalization itself
- Break condition: When BOS token is present and architecturally significant (LLaMA-1B), the model preferentially attends to BOS regardless of KV bias, indicating attention concentration has multiple competing causes.

### Mechanism 3
- Claim: Hybrid mitigation strategies succeed because TVR constrains weight variance growth that amplifies activation spikes, while companion methods (KV bias, DyT) restructure how the model computes attention or normalization.
- Mechanism: TVR applies layer-index-dependent rescaling to weight initialization and updates, targeting a specific variance trajectory. This prevents extreme weight magnitudes in early/late layers where activation spikes typically emerge. Combined with DyT (which replaces normalization layers with learnable tanh) or KV bias (which provides explicit attention bias), the two mechanisms address different root causes: weight scale and architectural structure.
- Core assumption: Massive activations arise from the interaction of large weight variance with normalization dynamics, so addressing only one factor is insufficient.
- Evidence anchors:
  - [abstract] "pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance"
  - [section 4.4.2, Table 2] TVR alone reduces max activation from 1416 to 235 but doesn't eliminate massive activations; KV Bias + TVR achieves 158 max activation with 52.0% accuracy (vs 50.3% baseline); DyT + TVR achieves 45 max activation with 50.3% accuracy
  - [corpus] Limited direct corpus evidence on TVR mechanism—most related work focuses on quantization rather than training-time variance control
- Break condition: If the target standard deviation for TVR is misconfigured (too low), downstream performance degrades; if companion method hyperparameters are wrong (DyT α values, initialization scale), training may diverge.

## Foundational Learning

- Concept: **Attention sink phenomenon and its relationship to massive activations**
  - Why needed here: The paper demonstrates that massive activations and attention concentration are coupled but not identical—interventions can reduce one without the other. Understanding attention sinks helps diagnose whether a model's behavior is bias-driven or architecture-driven.
  - Quick check question: When you remove the BOS token from input, does attention concentrate on a different token or disperse across the sequence? (Answer indicates whether attention sink is token-specific or a structural property.)

- Concept: **Layer normalization variants (LayerNorm, RMSNorm) and their interaction with extreme values**
  - Why needed here: Massive activations skew the denominator in normalization layers, creating drastically different feature representations. DyT replaces these entirely, which should disrupt the implicit bias mechanism—but the paper shows attention concentration persists, suggesting the relationship is more complex.
  - Quick check question: Given a hidden state with one value of 1000 and median of 0.5, what happens to all values after RMSNorm? (They all scale by ~1/√(1000²/d) ≈ very small, effectively collapsing the representation.)

- Concept: **GLU (Gated Linear Unit) vs non-GLU feedforward architectures**
  - Why needed here: The paper finds clear separation in intervention impact: non-GLU models (GPT-2, OPT) generally show non-detrimental massive activations, while GLU-based models (LLaMA, Gemma, Mistral) are more variable. The gating mechanism appears to change how activation spikes propagate.
  - Quick check question: In a SwiGLU layer, which path (gate vs value) would amplify an activation spike, and why? (The gate applies sigmoid, capping at 1; the value path is linear and can propagate spikes.)

## Architecture Onboarding

- Component map:
```
Input tokens → Embedding → [N decoder layers] → Final norm → Output projection

Each decoder layer:
  Input → RMSNorm → Attention → + residual
       → RMSNorm → FFN (SwiGLU) → + residual

Massive activation intervention point: Hidden states after attention or FFN, before next layer
Mitigation insertion points:
  - KV bias: Inside attention, augment K/V matrices
  - TVR: Weight initialization and during-training rescaling
  - DyT: Replace all RMSNorm modules
```

- Critical path:
  1. Detect massive activations: Run inference on 100 samples, compute max/median ratio per layer, flag if max > 100 and max ≥ 1000× median
  2. Characterize pattern: Plot max activation magnitude across layers (quick rise in early layers, constant middle, decline in late layers = likely detrimental)
  3. Select mitigation: If non-GLU architecture, may not need intervention. If GLU-based with detrimental pattern, start with TVR, add KV bias or DyT if needed
  4. Validate: Monitor perplexity (WikiText, C4) AND downstream tasks (HellaSwag, PIQA, ARC)—perplexity alone is insufficient

- Design tradeoffs:
  - TVR alone: Reduces activation magnitudes 6×, preserves/improves downstream performance, but massive activations may persist by definition
  - KV bias + TVR: Eliminates massive activations (per definition), best downstream performance (+1.7% over baseline), but attention concentration persists
  - DyT + TVR: Strongest activation suppression, recovers baseline performance (DyT alone underperforms), removes normalization but attention concentration still present
  - KV bias alone: Architecture-specific, may not work at all (LLaMA-1B), underperforms even when activation reduction succeeds (GPT-2)

- Failure signatures:
  - NaN gradients during training: Activation magnitude exceeded float16 range (65,536). Observed in Gemma-3. Switch to float32 or apply TVR.
  - Perplexity explosion when massive activations zeroed: Model has detrimental massive activations acting as critical bias. Do not suppress without mitigation.
  - KV bias reduces activations but downstream performance drops: Attention concentration shifted to KV bias token without learning useful bias. Model may need TVR to stabilize weights.
  - DyT training divergence: Check initialization scale (0.02 std for decoder weights) and α values (1.0 for attention DyT, 0.5 for final projection).

- First 3 experiments:
  1. **Baseline characterization**: Run the "Summer is warm. Winter is cold" test with and without BOS token. Plot max activation magnitude per layer. Compare against patterns in Figures 1, 7-10. This reveals whether massive activations are present and their layer-wise dynamics.
  2. **Intervention test**: On models with massive activations, run "set to zero" and "set to mean" interventions on WikiText/C4. If perplexity explodes (10×+ increase), activations are detrimental. If stable, no mitigation needed. This takes ~30 minutes on a single GPU.
  3. **TVR pilot**: Train a small model (LLaMA-360M or similar) with TVR (target std 0.01) vs baseline. Track max activation magnitude during training and final downstream performance. This validates TVR impact before committing to full-scale experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms distinguish detrimental massive activations from non-detrimental ones across different LLM architectures?
- Basis in paper: [explicit] Section 5 states the need for "investigating the underlying mechanisms that differentiate detrimental from non-detrimental massive activations."
- Why unresolved: The authors found that suppressing activations destroys performance in LLaMA-2 but not in GPT-2 or Falcon-2-11B, and they could not identify a clear distinguishing indicator other than a weak pattern regarding activation magnitude curves.
- What evidence would resolve it: A fine-grained analysis of activation distribution and persistence across layers that successfully predicts the perplexity impact of suppression for various model families.

### Open Question 2
- Question: Can hybrid mitigation strategies like KV Bias + TVR and DyT + TVR effectively suppress massive activations while maintaining performance in architectures other than LLaMA-1B?
- Basis in paper: [explicit] Section 5 calls for "extending the evaluation of hybrid strategies... to a broader range of architectures and tasks to validate their generalizability."
- Why unresolved: The hybrid experiments were primarily focused on LLaMA-1B, while Section 4.3 demonstrated that single strategies like Attention KV bias fail in specific architectures like GPT-2.
- What evidence would resolve it: Successful application of these hybrid strategies during the pre-training or fine-tuning of diverse architectures (e.g., Gemma, Falcon, Mistral) with preserved downstream accuracy.

### Open Question 3
- Question: Why does the intrinsic propensity for attention concentration persist even when massive activations are eliminated?
- Basis in paper: [explicit] Section 5 suggests "analyzing in more depth how the attention concentration phenomenon exhibits under multiple different mitigation techniques."
- Why unresolved: Section 4.4 notes that even when Dynamic Tanh (DyT) successfully eliminates massive activations and removes normalization layers, the attention concentration pattern persists rather than being disrupted.
- What evidence would resolve it: Mechanistic interpretability studies or attention head ablations that decouple the formation of attention concentration from the existence of massive activation values.

## Limitations
- Architecture-specificity of findings: Results vary significantly between GLU and non-GLU architectures, limiting generalizability across all LLM types
- Quantification of "detrimental": Lack of clear operational thresholds for what constitutes acceptable performance degradation versus true detriment
- KV bias effectiveness gap: Fundamental limitations in the proposed mitigation strategy, with failures in LLaMA-1B revealing incomplete understanding of underlying mechanisms

## Confidence

**High Confidence**: Claims about massive activations existing across LLM architectures and their basic characterization (max/median ratio thresholds, layer-wise patterns) are well-supported by direct evidence.

**Medium Confidence**: The claim that massive activations function as implicit bias terms has theoretical support and some empirical evidence but relies on architectural assumptions that may not hold universally across all models.

**Low Confidence**: The assertion that hybrid strategies "successfully balance" mitigation with performance preservation is based on limited downstream task evaluations and doesn't account for potential long-term effects or different training regimes.

## Next Checks

1. **Architecture transfer validation**: Apply the massive activation detection and intervention protocol (zeroing activations, KV bias, TVR) to a diverse set of modern LLM architectures including Mistral, Qwen, and Gemma variants. Compare intervention effectiveness across GLU and non-GLU models to test the claimed architecture-dependence.

2. **Long-term stability evaluation**: Train LLaMA-1B with TVR and KV bias for extended periods (10B+ tokens beyond the 100B training described) while monitoring: (a) attention concentration patterns, (b) downstream task performance drift, and (c) activation magnitude evolution. This validates whether initial mitigation success persists through full training.

3. **Alternative normalization ablation**: Replace RMSNorm in the LLaMA-1B baseline with LayerNorm, BatchNorm, and no normalization, then repeat the massive activation detection and intervention experiments. This tests whether normalization choice fundamentally alters the emergence and impact of massive activations, addressing the mechanism uncertainty around DyT's success.