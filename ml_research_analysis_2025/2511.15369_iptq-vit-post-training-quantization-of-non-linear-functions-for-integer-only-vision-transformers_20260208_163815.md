---
ver: rpa2
title: 'IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only
  Vision Transformers'
arxiv_id: '2511.15369'
source_url: https://arxiv.org/abs/2511.15369
tags:
- quantization
- vision
- approximation
- accuracy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "IPTQ-ViT introduces a post-training quantization framework for\
  \ integer-only vision transformers without retraining. The method uses tailored\
  \ approximation functions\u2014Data-aware Poly-GELU for vision-specific GELU approximation\
  \ and Efficient Bit-Softmax for improved softmax accuracy\u2014and a unified metric\
  \ integrating quantization sensitivity, perturbation, and computational cost to\
  \ select optimal functions per activation layer."
---

# IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers

## Quick Facts
- arXiv ID: 2511.15369
- Source URL: https://arxiv.org/abs/2511.15369
- Reference count: 36
- Key outcome: Up to 6.44% accuracy improvement over PTQ baselines on ImageNet-1k and COCO, enabling fully integer-only inference without retraining

## Executive Summary
IPTQ-ViT introduces a post-training quantization framework that enables integer-only inference for vision transformers without requiring retraining. The method addresses the accuracy degradation common in PTQ by introducing two novel approximation functions: a data-aware polynomial GELU and an efficient bit-shifting softmax, along with a unified metric for layer-wise function selection. Experimental results show significant accuracy improvements over prior PTQ methods while maintaining competitive latency compared to integer-only QAT approaches.

## Method Summary
IPTQ-ViT employs a three-stage pipeline for integer-only vision transformer quantization. First, it analyzes each non-linear layer (GELU, Softmax, LayerNorm) by evaluating multiple candidate approximation functions using a unified metric that balances quantization sensitivity, perturbation error, and computational cost. Second, it assigns the optimal function to each layer based on the highest metric score. Third, it calibrates the mixed-quantized model by determining scale factors and zero-points. The framework introduces two key innovations: a data-aware polynomial approximation for GELU that adapts to vision-specific activation distributions, and a bit-shifting-based softmax that improves accuracy while maintaining integer-only operations.

## Key Results
- Achieves up to 6.44% accuracy improvement over prior PTQ methods on ImageNet-1k and COCO benchmarks
- Maintains comparable accuracy and latency to integer-only QAT approaches
- Enables fully integer-only inference for vision transformers without requiring retraining data
- Demonstrates effectiveness across classification (ViT, DeiT, Swin) and detection (Cascade Mask R-CNN) tasks

## Why This Works (Mechanism)

### Mechanism 1: Data-Range Aware Polynomial Approximation
The method addresses the failure of fixed approximations in PTQ by computing min/max ranges from actual vision calibration data and fitting a quartic polynomial to the error function within this specific range. This minimizes approximation errors relative to the actual data distribution encountered during inference.

### Mechanism 2: Refined Bit-Shift Exponential for Softmax
Instead of simple linear approximations that cause excessive error in PTQ, the method uses a first-degree Taylor expansion of the base-2 exponential with binary shift approximation of ln(2). This reduces L2 approximation error from 0.1717 to 0.1126 while maintaining integer-only operations.

### Mechanism 3: Unified Metric for Layer-wise Function Search
The framework defines a harmonic mean metric combining quantization sensitivity (SQNR), absolute perturbation error, and computational cost to select optimal approximation functions per layer. This addresses the limitation of SQNR alone in capturing error accumulation across deep layers.

## Foundational Learning

- **Post-Training Quantization vs. Quantization-Aware Training**: PTQ uses small calibration sets without retraining, while QAT requires full dataset retraining. Understanding this distinction is critical since IPTQ-ViT specifically solves PTQ's accuracy drop problem caused by lack of retraining. Quick check: Does the method require gradient descent or backpropagation on the full dataset? (Answer: No, only forward passes on calibration data).

- **Non-Linear Function Quantization**: Linear layers are easy to quantize, but non-linear functions like GELU and Softmax require floating-point math. To achieve "integer-only," these functions must be replaced with approximations, not just cast to int8. Quick check: Why can't we simply cast the output of a standard Softmax to int8? (Answer: The internal exp() calculation causes overflow/precision issues; the function itself must be replaced).

- **Signal-to-Quantization-Noise Ratio (SQNR)**: This standard metric for quantization quality is used as a component of the unified metric. The paper critiques SQNR alone but relies on it for layer sensitivity measurement. Quick check: Does a higher SQNR always mean better task accuracy? (Answer: Generally yes, but the paper argues it's not sufficient on its own for layer-wise selection).

## Architecture Onboarding

- **Component map**: Pre-trained FP32 Vision Transformer + Calibration Dataset -> Analysis Module (candidate evaluation) -> Assignment Module (function selection) -> Calibration Engine (scale/zero-point determination) -> Mixed-Quantized Integer-Only Model

- **Critical path**: 1) Hooking into forward pass to intercept activations at non-linear layers, 2) Computing min/max ranges online for Data-aware Poly-GELU, 3) Calculating Unified Metric efficiently without exploding memory usage

- **Design tradeoffs**: Search space size (159 computations for ViT-B), polynomial degree selection (quartic chosen over degree 2 for accuracy), metric weight tuning for hardware-specific optimization

- **Failure signatures**: Collapse to random guessing (~0.1% accuracy) when using QAT-designed approximations directly in PTQ, high latency if Unified Metric ignores cost or calibration fails

- **First 3 experiments**: 1) Baseline validation by applying I-BERT's i-GELU to confirm motivation, 2) Component ablation by swapping only GELU function, 3) Search space efficiency comparison between Unified Metric and naive Best SQNR selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IPTQ-ViT perform on resource-constrained edge hardware (e.g., mobile NPUs or CPUs) compared to standard GPU benchmarks?
- Basis: The paper emphasizes "resource-constrained devices" but latency evaluations are exclusively on RTX 3090 GPU using TVM
- Why unresolved: GPU architectures differ significantly from edge devices in handling integer arithmetic; latency gains on server-grade GPU may not translate to edge efficiency
- Evidence needed: End-to-end latency and energy measurements on edge hardware (ARM CPUs, Qualcomm NPUs)

### Open Question 2
- Question: Can the Data-aware Poly-GELU be adapted for data-free scenarios where access to training set is restricted?
- Basis: The method relies on "randomly sampled training images" contrasting with data-free quantization works like PSAQ-ViT or CLAMP-ViT
- Why unresolved: The "Data-aware" component requires real data statistics; it's unclear if synthetic data can substitute without severe accuracy loss
- Evidence needed: Experiments applying IPTQ-ViT using synthetic calibration data compared against real-data baseline

### Open Question 3
- Question: Do the approximation functions generalize effectively to Large Language Models (LLMs)?
- Basis: The paper notes LLM quantization methods fail on vision tasks but doesn't test if proposed vision-optimized functions improve LLM quantization
- Why unresolved: The "Data-aware" approximation is tuned for vision distributions; LLMs exhibit different statistical properties that may render current approximations suboptimal
- Evidence needed: Application of IPTQ-ViT's functions to NLP benchmarks on models like BERT or Llama, comparing accuracy against I-BERT or I-LLM

## Limitations
- Calibration set representativeness is critical for accuracy gains but lacks robustness analysis for small or biased calibration sets
- No explicit handling for out-of-range values during inference, which could cause stability issues with Data-aware Poly-GELU
- Unified Metric design is heuristic with unvalidated relative weighting that may be hardware-dependent
- No cross-hardware analysis provided to verify integer-only claim translates to real-world deployment efficiency

## Confidence
- **High Confidence**: Core mechanisms of data-range aware polynomial approximation and first-degree Taylor expansion are well-supported by error analysis and ablation studies
- **Medium Confidence**: Claim that Unified Metric provides superior selection over SQNR alone is supported by ablation but metric sensitivity to weight tuning is unexplored
- **Low Confidence**: "Comparable accuracy and latency to integer-only QAT" claim lacks QAT baseline comparisons for all models and tasks

## Next Checks
1. **Calibration Robustness Test**: Evaluate accuracy sensitivity to calibration set size and diversity (100 vs. 1000 images) to quantify collapse risk
2. **Out-of-Range Stability**: Instrument Data-aware Poly-GELU to log frequency and magnitude of out-of-range inputs during inference
3. **Cross-Hardware Latency**: Measure quantized model latency on mobile CPU (e.g., ARM) to verify integer-only claim translates to deployment efficiency