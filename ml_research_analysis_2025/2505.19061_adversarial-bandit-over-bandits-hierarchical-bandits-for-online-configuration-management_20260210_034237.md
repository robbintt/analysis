---
ver: rpa2
title: 'Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration
  Management'
arxiv_id: '2505.19061'
source_url: https://arxiv.org/abs/2505.19061
tags:
- regret
- clusters
- arms
- algorithm
- abob
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing system configurations
  in dynamic environments with large action spaces. The proposed solution, ABoB (Adversarial
  Bandit over Bandits), introduces a hierarchical approach that clusters similar configurations
  and applies adversarial bandit algorithms at multiple levels.
---

# Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management

## Quick Facts
- arXiv ID: 2505.19061
- Source URL: https://arxiv.org/abs/2505.19061
- Reference count: 40
- Key result: Hierarchical bandit framework achieves O(k^(1/4)T^(1/2)) regret under Lipschitz conditions versus O(k^(1/2)T^(1/2)) for flat methods

## Executive Summary
This paper addresses the challenge of optimizing system configurations in dynamic environments with large action spaces. The proposed solution, ABoB (Adversarial Bandit over Bandits), introduces a hierarchical approach that clusters similar configurations and applies adversarial bandit algorithms at multiple levels. The method demonstrates robust worst-case performance matching traditional flat approaches with O(k^(1/2)T^(1/2)) regret, while achieving significant improvements (up to O(k^(1/4)T^(1/2)) under favorable Lipschitz conditions. Empirical results show up to 91% improvement in regret compared to flat methods across various scenarios, including a real storage system optimization task where ABoB achieved 49% better performance.

## Method Summary
The ABoB framework organizes configuration actions into a two-level hierarchy where each parent cluster contains multiple child arms. At each time step, the algorithm first selects a cluster using a parent bandit algorithm (like EXP3), then selects an arm within that cluster using a child bandit algorithm. This hierarchical structure enables more efficient exploration-exploitation trade-offs by leveraging the metric structure of the action space. The method can work with either a pre-specified partition of actions or learn an initial partition based on the metric structure. The regret analysis shows that under favorable Lipschitz conditions on the reward function, the hierarchical approach can achieve significantly better regret bounds than flat methods.

## Key Results
- Achieves worst-case regret of O(k^(1/2)T^(1/2)) matching flat adversarial bandit methods
- Under Lipschitz conditions, achieves improved regret of O(k^(1/4)T^(1/2)) representing a √k improvement
- Empirical results show up to 91% improvement in regret compared to flat methods across various scenarios
- Real storage system optimization demonstrates 49% better performance than flat EXP3

## Why This Works (Mechanism)
The hierarchical approach works by exploiting the metric structure of the configuration space. By clustering similar configurations together, the algorithm can explore clusters more broadly while exploiting within clusters more efficiently. This reduces the effective exploration burden compared to treating all configurations independently. The two-level structure allows the parent bandit to focus on cluster selection while child bandits optimize within their respective clusters, creating a natural decomposition of the exploration-exploitation problem.

## Foundational Learning
- Adversarial bandit algorithms (why needed: core framework for worst-case guarantees; quick check: regret bounds scale as √T)
- Metric space structure (why needed: enables meaningful clustering of similar configurations; quick check: distance function satisfies triangle inequality)
- Hierarchical decision making (why needed: decomposes large action space into manageable subproblems; quick check: regret bounds combine additively across levels)
- Lipschitz continuity (why needed: guarantees that nearby configurations have similar rewards; quick check: bounded gradient or similar metric property)
- Clustering under metric constraints (why needed: partitions must respect the underlying geometry; quick check: clusters are internally similar but externally distinct)

## Architecture Onboarding

**Component Map:** Parent bandit (EXP3) -> Cluster selection -> Child bandit (EXP3) -> Arm selection -> Reward observation -> Weight updates

**Critical Path:** Time step → Parent bandit selects cluster → Child bandit selects arm → Execute configuration → Observe reward → Update both bandit algorithms

**Design Tradeoffs:** The two-level hierarchy balances between coarse exploration (at cluster level) and fine exploitation (within clusters). More levels could theoretically improve performance but increase complexity. The method trades computational overhead for improved regret bounds under favorable conditions.

**Failure Signatures:** Poor clustering leads to suboptimal regret; if the metric space doesn't reflect true reward similarity, the hierarchy provides no advantage. Non-stationary environments where optimal configurations shift rapidly may degrade performance.

**First Experiments:** 1) Test on synthetic problems with known Lipschitz properties to verify theoretical bounds. 2) Compare ABoB against flat methods on benchmark configuration optimization problems. 3) Evaluate performance sensitivity to clustering quality and number of clusters.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the ABoB framework be extended to adaptively modify the clustering structure during runtime if the underlying metric space or Lipschitz properties change?
- Basis in paper: [explicit] The conclusion explicitly lists "exploring adaptive clustering techniques" as a direction for future work.
- Why unresolved: The current ABoB algorithm assumes a fixed partition $P$ (either given or initially computed) which remains static throughout the time horizon $T$.
- What evidence would resolve it: An algorithmic variant where the number of clusters $p$ and the assignment of arms to clusters can update based on observed reward history, along with corresponding regret bounds.

### Open Question 2
- Question: Can a specific adversarial bandit algorithm be constructed that formally satisfies the "ALB+" property (Property 1) to achieve the improved regret bounds theoretically?
- Basis in paper: [inferred] Section 4.2 states, "We are not aware of an algorithm that formally fulfills Property 1 for the Lipschitz condition," noting that the paper uses EXP3 instead, which satisfies only a weaker condition.
- Why unresolved: The theoretical improvement to $O(k^{1/4}T^{1/2})$ relies on the existence of an algorithm with regret bounded by $O(\ell \sqrt{kT})$, but no such algorithm is currently known or implemented in the study.
- What evidence would resolve it: The derivation of a specific algorithm for adversarial Lipschitz bandits that meets the required regret bound relative to the Lipschitz constant $\ell$.

### Open Question 3
- Question: What are the theoretical and practical implications of extending the ABoB hierarchy to more than two levels (multilevel hierarchies)?
- Basis in paper: [explicit] The conclusion identifies "multilevel hierarchies of clusters" as a subject for future study.
- Why unresolved: The current analysis and algorithm design are restricted to a two-level hierarchy (parent clusters and child arms), and it is unclear how regret terms compound or cancel out with additional layers.
- What evidence would resolve it: A generalization of Theorem 4.2 and Theorem 4.4 for $L$ levels, showing the relationship between depth, cluster size, and regret.

### Open Question 4
- Question: How does parallel sampling (pulling multiple arms simultaneously) affect the regret bounds and convergence speed of the hierarchical approach?
- Basis in paper: [explicit] The conclusion suggests "studying the setting where multiple arms can be sampled in parallel" as future work.
- Why unresolved: The problem formulation assumes a single agent selecting a single arm $a_t$ per round, updating weights sequentially.
- What evidence would resolve it: Empirical simulations or theoretical bounds demonstrating ABoB's performance when a batch of arms is pulled and evaluated concurrently.

## Limitations
- Theoretical improvement to O(k^(1/4)T^(1/2)) relies on strong Lipschitz assumptions that may not hold in all real-world scenarios
- Empirical validation limited to synthetic benchmarks and a single real-world storage system case study
- Computational overhead of maintaining and updating clusters at multiple levels not thoroughly analyzed
- Method assumes static environment; performance in non-stationary settings with shifting optimal configurations is unclear

## Confidence
- Theoretical regret bounds: High
- Empirical performance claims: Medium
- Scalability and computational efficiency: Low

## Next Checks
1. Test ABoB on diverse real-world configuration optimization problems across different domains (e.g., networking, database tuning, cloud resource allocation) to assess generalizability.
2. Conduct a comprehensive analysis of computational overhead and runtime performance compared to flat methods, including memory usage and update frequency.
3. Evaluate the method's robustness in non-stationary environments where optimal configurations change over time, measuring performance degradation and adaptation speed.