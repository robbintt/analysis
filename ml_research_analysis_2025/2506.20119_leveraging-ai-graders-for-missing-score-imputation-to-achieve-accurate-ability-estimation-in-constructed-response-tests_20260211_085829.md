---
ver: rpa2
title: Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability
  Estimation in Constructed-Response Tests
arxiv_id: '2506.20119'
source_url: https://arxiv.org/abs/2506.20119
tags:
- missing
- scoring
- data
- ability
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study tackles the challenge of accurate ability estimation\
  \ in constructed-response tests where human graders score only a subset of answers,\
  \ leading to missing scores and reduced estimation accuracy. It proposes a novel\
  \ method that leverages automated scoring models\u2014either fine-tuned BERT variants\
  \ or zero-shot large language models\u2014to impute missing scores, thereby creating\
  \ a complete dataset for more accurate item response theory (IRT)-based ability\
  \ estimation."
---

# Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests

## Quick Facts
- **arXiv ID:** 2506.20119
- **Source URL:** https://arxiv.org/abs/2506.20119
- **Reference count:** 40
- **Primary result:** AI-imputed scores significantly improve IRT-based ability estimation accuracy over conventional imputation methods.

## Executive Summary
This study addresses the challenge of accurate ability estimation in constructed-response tests where human graders score only a subset of answers, leading to missing scores and reduced estimation accuracy. It proposes a novel method that leverages automated scoring models—either fine-tuned BERT variants or zero-shot large language models—to impute missing scores, thereby creating a complete dataset for more accurate item response theory (IRT)-based ability estimation. Experiments using real-world datasets show the proposed method significantly outperforms conventional imputation approaches like k-NN and random forest, achieving lower root-mean-squared error and higher correlation with true ability estimates. This approach effectively reduces the manual grading workload while maintaining high estimation accuracy, even under high missing ratios.

## Method Summary
The proposed method involves two key steps: (1) impute missing scores using an AI grader—either a fine-tuned BERT model on Japanese short-answer/essay datasets (SAG, ES) or a zero-shot GPT-4o model on a public writing task dataset (ELYZA-tasks-100)—and (2) estimate learner ability using the Generalized Partial Credit Model (GPCM) from the now-complete dataset. The AI graders are trained or prompted to predict scores (1-5) for each item based on the rubric and reference answer. The systematic missingness patterns (10-80% ratios) are generated via Algorithm 1 to ensure some observed scores per learner/item. The method is evaluated by comparing the imputed-ability estimates against gold-standard estimates from the complete dataset using RMSE and Pearson correlation.

## Key Results
- AI-imputed scores significantly reduce RMSE in ability estimation compared to k-NN and random forest imputation baselines across all missing ratios.
- GPT-4o zero-shot scoring achieves strong performance on the ELYZA public dataset, validating the approach beyond fine-tuned models.
- The proposed method maintains high correlation (>0.9) with gold-standard ability estimates even at 50% missing ratios.

## Why This Works (Mechanism)
The mechanism behind the method's effectiveness lies in the combination of high-quality AI scoring and the statistical rigor of IRT. By using a fine-tuned or zero-shot LLM as an automated grader, the method produces imputed scores that closely approximate human-assigned scores, preserving the true ability-score relationship. When these imputed scores are used in GPCM-based ability estimation, the model can accurately infer latent ability from the complete dataset. The AI grader's ability to generalize across missing patterns and maintain score consistency ensures that the imputed matrix retains the underlying difficulty and discrimination parameters, allowing IRT to produce reliable estimates even with high missing ratios.

## Foundational Learning
- **Item Response Theory (IRT):** A family of models linking latent ability to item responses; needed to formally estimate ability from scores. Quick check: verify GPCM parameters match expected item difficulty and discrimination patterns.
- **Generalized Partial Credit Model (GPCM):** An IRT model for polytomous (multi-point) scoring; needed for constructed-response items with partial credit. Quick check: ensure category thresholds are ordered and the model converges.
- **Score Imputation:** Filling missing values in a dataset; needed to create a complete matrix for IRT estimation. Quick check: compare imputed values against a held-out validation set if available.
- **Zero-shot LLM Scoring:** Using a pre-trained LLM to score without fine-tuning; needed for flexible, rapid deployment. Quick check: parse LLM outputs consistently to extract integer scores.

## Architecture Onboarding
- **Component Map:** (Observed Scores + AI Grader) -> Imputed Complete Matrix -> GPCM Fitting -> Ability Estimates
- **Critical Path:** The sequence of imputing missing scores and then applying GPCM is critical; errors in imputation directly degrade ability estimation.
- **Design Tradeoffs:** Fine-tuning BERT vs. zero-shot GPT-4o balances accuracy and deployment speed; higher missing ratios reduce the number of observed scores, challenging both AI grader accuracy and IRT convergence.
- **Failure Signatures:** GPT-4o may output "Score: 4" instead of just "4" (parsing issue); GPCM may fail to converge with too many missing values (solver issue).
- **First Experiments:**
    1. Run the zero-shot GPT-4o pipeline on a small subset of ELYZA-tasks-100 and verify integer score extraction.
    2. Fit GPCM on the full ELYZA dataset to establish a gold-standard ability estimate.
    3. Apply the systematic missingness Algorithm 1 (e.g., 30% missing) and verify that every learner and item retains at least one observed score.

## Open Questions the Paper Calls Out
- How does the performance of the proposed method scale with increasingly complex response types beyond short answers and essays?
- Can the AI grader maintain accuracy when trained on one language and applied to another without fine-tuning?
- What is the impact of using different LLM architectures or prompting strategies on the imputation quality and subsequent ability estimation?

## Limitations
- The study relies on proprietary datasets (SAG, ES) for the primary Japanese language experiments, limiting reproducibility without data access agreements.
- Evaluation is constrained to short-answer and essay formats; the method's effectiveness on other response types (e.g., coding, diagrams) is untested.
- The zero-shot LLM approach, while effective, may not generalize to less capable models or different languages without fine-tuning.

## Confidence
- **Core claim (AI imputation improves accuracy):** High
- **Reproducibility on ELYZA dataset:** Medium (public data, but proprietary datasets central to results)
- **Generalizability to other response types:** Low

## Next Checks
1. Reproduce core results on the public ELYZA-tasks-100 dataset using the provided LLM prompt and imputation pipeline, confirming RMSE and correlation improvements over k-NN/RF baselines.
2. Stress-test LLM parsing by running GPT-4o on a sample of outputs to verify robust extraction of integer scores and handling of edge cases.
3. Test IRT solver robustness by intentionally creating extreme missingness patterns and checking for convergence failures or unstable estimates.