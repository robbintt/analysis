---
ver: rpa2
title: Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution
  Perspective
arxiv_id: '2501.03562'
source_url: https://arxiv.org/abs/2501.03562
tags:
- arxiv
- learning
- adversarial
- policy
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adversarial attacks in deep
  reinforcement learning (DRL), particularly in continuous action spaces where existing
  methods have limited impact on overall policy distribution. The authors propose
  Distribution-Aware Projected Gradient Descent (DAPGD), which leverages the entire
  policy distribution rather than individual sampled actions for generating adversarial
  perturbations.
---

# Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective

## Quick Facts
- arXiv ID: 2501.03562
- Source URL: https://arxiv.org/abs/2501.03562
- Reference count: 40
- Key outcome: DAPGD achieves 22.03% higher reward drop than best baseline on continuous control tasks

## Executive Summary
This paper addresses the challenge of adversarial attacks in deep reinforcement learning (DRL), particularly in continuous action spaces where existing methods have limited impact on overall policy distribution. The authors propose Distribution-Aware Projected Gradient Descent (DAPGD), which leverages the entire policy distribution rather than individual sampled actions for generating adversarial perturbations. The method uses Bhattacharyya distance to measure policy similarity, enabling sensitive detection of subtle but critical differences between probability distributions. Experiments conducted on three robot navigation tasks demonstrate that DAPGD achieves state-of-the-art results, achieving an average 22.03% higher reward drop compared to the best baseline.

## Method Summary
DAPGD is an adversarial attack method that targets the full policy distribution rather than individual sampled actions. It computes perturbations by measuring the Bhattacharyya distance between the policy distributions of the original and perturbed states. The attack iteratively updates the observation using the sign of the gradient of this distance, then projects the perturbation to stay within an ε-ball constraint. The method is evaluated on Safety Gym environments with TRPO-trained agents, comparing against baseline PGD attacks using various distribution similarity metrics (Bhattacharyya, KL, JS, Wasserstein).

## Key Results
- DAPGD achieves 22.03% higher reward drop compared to best baseline on average across tested environments
- Bhattacharyya distance outperforms KL, JS, and Wasserstein distances in attack effectiveness
- DAPGD shows consistent performance across different iteration counts (N=50 vs N=100)
- The attack remains effective against both benign and adversarially trained robust agents

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Level Attack Surface Expansion
Existing PGD-based methods compute gradients from a single sampled action per iteration. In continuous action spaces, agents can select adjacent actions to counteract perturbations targeting specific actions. DAPGD replaces the per-sample loss with a distribution similarity loss (Bhattacharyya distance), forcing the perturbation to shift the entire policy distribution rather than nudging one sample.

### Mechanism 2: Bhattacharyya Distance as High-Sensitivity Distribution Metric
The Bhattacharyya distance measures distribution overlap via the geometric mean of probabilities. The paper's ablation shows BD achieves lower rewards than KL, JS, and Wasserstein across iteration counts, suggesting it induces broader distribution shifts that affect both high-probability actions and their neighbors.

### Mechanism 3: Transferable Attack Consistency Across Observation Space
By optimizing for policy distribution shift rather than single-action deviation, the gradient direction reflects aggregate vulnerability. The paper argues this produces "adversarial samples that more closely resemble real-world disturbances," though this claim lacks quantitative validation.

## Foundational Learning

- **Concept: Stochastic Policy Gradient**
  - Why needed here: DAPGD requires differentiable access to the policy distribution π(a|s), not just sampled actions
  - Quick check question: Can you derive the gradient of E[a∼π][f(a)] with respect to policy parameters when π is Gaussian?

- **Concept: Bhattacharyya Distance and Distribution Overlap**
  - Why needed here: The core innovation replaces MSE/cross-entropy loss with BD-based loss
  - Quick check question: For two 1D Gaussians N(μ₁, σ₁²) and N(μ₂, σ₂²), write the closed-form Bhattacharyya distance

- **Concept: PGD Attack Framework in RL**
  - Why needed here: DAPGD inherits the iterative perturbation structure from PGD
  - Quick check question: How does the projection step in PGD ensure the perturbation remains within ε-ball constraints?

## Architecture Onboarding

- **Component map:**
  Observation s → Policy Network π_θ → Action Distribution π(a|s)
         ↓                                    ↓
  Perturbation Generator              Bhattacharyya Distance
  (DAPGD iterative loop)              J(π[s*], π[s])
         ↓                                    ↓
  Adversarial State s*  ←────────  Gradient ∇_s* J

- **Critical path:**
  1. Extract policy distribution parameters (mean, variance for Gaussian policies)
  2. Compute BD between π[s*] and π[s] (original, unperturbed policy)
  3. Backpropagate through BD to input space
  4. Update s* iteratively with sign gradient and projection

- **Design tradeoffs:**
  - BD vs. KL/JS/Wasserstein: BD shows better empirical results, but closed-form computation depends on policy family
  - Iteration count N: N=50 vs. N=100 yields marginal differences for DAPGD
  - Perturbation budget ε: Paper uses ε=0.1 for L∞; real-world transfer may require smaller budgets

- **Failure signatures:**
  - Reward drops plateau early → BD gradient vanishes (distributions too dissimilar)
  - Attack fails on discrete action spaces → BD integral discretization errors
  - Robust models resist attack → Agent has adversarial training against distribution shifts

- **First 3 experiments:**
  1. Replicate the Button task ablation: Compare BD, KL, JS, Wasserstein losses on a pretrained TRPO agent
  2. Test transferability: Train DAPGD perturbations on one policy, apply to a differently-initialized policy in the same environment
  3. Budget sensitivity: Sweep ε ∈ {0.01, 0.05, 0.1, 0.2} and plot reward degradation curves for DAPGD vs. PGD baseline

## Open Questions the Paper Calls Out
The paper identifies extending DAPGD to applications such as large language models as a future direction, though the discrete nature of token generation presents significant challenges for applying distribution-aware attack approaches.

## Limitations
- Scalability of Bhattacharyya distance computation to non-Gaussian policy distributions is unproven
- Claim that DAPGD perturbations "more closely resemble real-world disturbances" lacks quantitative validation
- Robustness evaluation only tests against PGD-based adversarial training, not more sophisticated defense mechanisms

## Confidence
- **High confidence**: DAPGD achieves superior reward degradation compared to baselines in tested environments
- **Medium confidence**: Bhattacharyya distance is more effective than other distribution metrics for attack optimization
- **Low confidence**: DAPGD perturbations "more closely resemble real-world disturbances"

## Next Checks
1. Implement DAPGD on a non-Gaussian policy (e.g., categorical or mixture model) and verify Bhattacharyya distance computation remains tractable and effective
2. Conduct cross-environment transferability tests: train DAPGD on one Safety Gym task, evaluate on untrained tasks to measure generalization
3. Compare DAPGD against state-of-the-art robust DRL defenses beyond PGD (e.g., adversarial training with diverse perturbation types, certified robustness methods)