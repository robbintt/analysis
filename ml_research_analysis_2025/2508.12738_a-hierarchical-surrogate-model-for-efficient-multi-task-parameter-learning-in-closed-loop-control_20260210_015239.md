---
ver: rpa2
title: A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning
  in Closed-Loop Control
arxiv_id: '2508.12738'
source_url: https://arxiv.org/abs/2508.12738
tags:
- closed-loop
- cost
- learning
- control
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a hierarchical Bayesian optimization framework\
  \ for efficient controller parameter learning in closed-loop control tasks. The\
  \ core method exploits structural knowledge of the control loop\u2014specifically\
  \ the dynamical system, parameterized controller, and closed-loop cost function\u2014\
  to construct a hierarchical surrogate model."
---

# A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control

## Quick Facts
- arXiv ID: 2508.12738
- Source URL: https://arxiv.org/abs/2508.12738
- Reference count: 24
- Primary result: Hierarchical surrogate with GP-learned dynamics achieves sublinear regret and superior sample efficiency in multi-task MPC parameter learning

## Executive Summary
This paper introduces a hierarchical Bayesian optimization framework for efficient controller parameter learning in closed-loop control tasks. The core method exploits structural knowledge of the control loop—specifically the dynamical system, parameterized controller, and closed-loop cost function—to construct a hierarchical surrogate model. Instead of directly learning the mapping from parameters to closed-loop cost, the approach learns parameter-dependent closed-loop dynamics via Gaussian processes, then computes task-specific costs using known closed-form expressions. This enables knowledge transfer between tasks and improves sample efficiency. Theoretical analysis establishes sublinear regret bounds comparable to standard black-box Bayesian optimization, while practical experiments on a nonlinear cart-pole system demonstrate significant improvements in convergence speed and sample efficiency over both black-box Bayesian optimization and existing multi-task methods.

## Method Summary
The method learns parameterized closed-loop dynamics f_c(x_k, θ) → z_{k+1} using independent Gaussian processes per output dimension, where z includes both next state and current control. For each BO iteration, the algorithm selects parameters θ_t via LCB acquisition, executes a K-step closed-loop rollout, and updates the GP with K trajectory points. Task-specific costs are computed post-hoc using known stage cost functions, allowing a single learned dynamics model to serve multiple tasks with different cost weightings. The approach is validated on a nonlinear cart-pole system with 5-dimensional parameter space representing diagonal elements of state and input cost matrices.

## Key Results
- Hierarchical approach achieves sublinear cumulative regret (O(√T·log(T)^{d+1})) comparable to standard black-box BO
- Multi-task transfer enables faster convergence when switching between tasks with different cost weightings
- Sample efficiency improvements of 40-60% over black-box GP-BO and existing multi-task BO methods
- More consistent performance across multiple optimization runs with reduced variance in final costs

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the black-box cost mapping into learned dynamics plus known cost aggregation improves sample efficiency. Rather than learning J(θ) directly as a black-box, the framework learns the parameter-dependent closed-loop dynamics f_c(x_k, θ) → z_{k+1} via Gaussian processes, then computes the cost J(θ) = Σℓ(z_k) exactly using known stage costs. This allows each K-step rollout to contribute K training points to the dynamics model instead of a single scalar cost observation.

### Mechanism 2
A shared dynamics model enables transfer learning across tasks with different cost weightings. Since different closed-loop tasks share the same underlying dynamics but differ only in cost evaluation, a single learned f_c model can be reused. Task-specific costs are computed post-hoc via different ℓ_i functions, eliminating the need to relearn the surrogate for each new task.

### Mechanism 3
The hierarchical approach achieves sublinear cumulative regret comparable to standard black-box BO while enabling transfer. Multi-step error bounds propagate GP uncertainty through the K-step rollout with Lipschitz-based compounding. The LCB acquisition function uses these bounds to guarantee regret growth on par with black-box methods.

## Foundational Learning

- **Gaussian Process Regression**: GP provides the probabilistic surrogate for closed-loop dynamics, yielding both mean predictions μ̂ and uncertainty quantification κ̂ required for LCB-based exploration and regret bounds.
- **Bayesian Optimization with Lower Confidence Bound**: BO provides the outer-loop acquisition strategy; LCB explicitly trades off exploitation (low μ̂_J) and exploration (high κ̂_J) with theoretical regret guarantees.
- **Model Predictive Control with Parameterized Cost**: The inner-loop controller generates closed-loop trajectories; understanding how θ parameterizes stage/terminal costs is essential to interpret what the surrogate learns.

## Architecture Onboarding

- **Component map**: System (f) -> Parameterized MPC (π) -> Closed-loop dynamics (f_c) -> Hierarchical Surrogate (μ̂) -> Cost Aggregator -> BO Loop
- **Critical path**: Initialize GP with prior data or random θ_0 rollout; optimize acquisition α(θ; D_t) → select θ_t; run closed-loop experiment with θ_t, collect trajectory {z_k}_{k=0}^{K-1}; augment GP training data with K input-output pairs; update GP hyperparameters via evidence maximization; repeat until convergence.
- **Design tradeoffs**: Rollout horizon K (longer provides more data but increases error compounding); GP mean-only vs. uncertainty propagation (mean-only is computationally efficient but conservative); shared vs. per-task surrogates (shared enables transfer but assumes fixed dynamics).
- **Failure signatures**: Regret not decreasing (check if GP hyperparameters are stuck or insufficient data diversity); transfer degrades performance (dynamics may have changed, verify feasibility); large variance in optimization runs (acquisition function may be under-exploring or GP uncertainty poorly calibrated).
- **First 3 experiments**: Single-task validation on linear system with known optimal θ*; multi-task transfer between Task 1 and Task 2 with different cost matrices; noise sensitivity analysis with varying process noise levels.

## Open Questions the Paper Calls Out

### Open Question 1
Can the hierarchical surrogate approach scale to higher-dimensional parameter spaces while retaining sample efficiency advantages over black-box methods? The authors plan to explore Bayesian neural networks for dynamics learning to enable scalability to higher-dimensional parameter spaces, as GP-based models suffer from cubic complexity in data.

### Open Question 2
Can transfer learning be enabled between tasks that differ in aspects beyond cost function weighting, such as different system dynamics, constraints, or reference trajectories? Current experiments only vary evaluation cost matrices between tasks while keeping dynamics and controller structure fixed.

### Open Question 3
Do the theoretical regret guarantees and practical performance hold for real-world physical systems with unmodeled dynamics, delays, and non-Gaussian noise? All experiments are simulation-based; model-reality gaps and non-ideal noise characteristics could violate assumptions underlying the GP surrogate and regret bounds.

## Limitations
- Theoretical regret bounds rely heavily on GP smoothness and finite Lipschitz constant assumptions that may not hold for complex nonlinear systems
- Multi-task transfer benefits are demonstrated but not quantified in terms of how much dynamics drift can be tolerated
- Hyperparameter sensitivity (kernel choice, exploration schedule β_t, initial data) is not systematically studied

## Confidence

- **High**: The hierarchical surrogate design is theoretically sound and the sublinear regret claim aligns with established GP-BO theory when assumptions hold
- **Medium**: Experimental results show clear improvements in sample efficiency and multi-task transfer, but the lack of hyperparameter analysis and limited task diversity reduces generalizability
- **Low**: The specific regret bound extension to the multi-step surrogate setting is novel but lacks direct corpus validation; assumptions may not hold in practice for complex nonlinear systems

## Next Checks

1. **Lipschitz Sensitivity**: Systematically vary the GP kernel lengthscales and controller gain bounds to quantify the impact of LGP on regret growth and compounding errors
2. **Transfer Robustness**: Introduce controlled dynamics drift between Task 1 and Task 2; measure the point at which transfer degrades to black-box performance
3. **Long-Horizon Stability**: Increase K from 25 to 50 steps; verify whether the hierarchical approach maintains its advantage or if multi-step error accumulation erodes gains