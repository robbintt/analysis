---
ver: rpa2
title: Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing
  Modalities
arxiv_id: '2601.21950'
source_url: https://arxiv.org/abs/2601.21950
tags:
- uncertainty
- missing
- uni0000018f
- data
- uni0000013a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AUM, a multimodal learning framework that addresses
  the challenge of missing modalities in medical data by explicitly quantifying unimodal
  aleatoric uncertainty. The method represents each modality as a multivariate Gaussian
  distribution, where the variance captures inherent data uncertainty.
---

# Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities

## Quick Facts
- arXiv ID: 2601.21950
- Source URL: https://arxiv.org/abs/2601.21950
- Reference count: 0
- AUM improves mortality prediction AUC-ROC by 2.26% on MIMIC-IV and 2.17% on eICU compared to state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of missing modalities in medical multimodal learning by explicitly quantifying unimodal aleatoric uncertainty. The proposed AUM framework models each modality as a multivariate Gaussian distribution where variance captures inherent data uncertainty. A dynamic message-passing mechanism with uncertainty-aware attention emphasizes reliable information from available modalities. The method demonstrates robust performance under varying missing ratios and noisy conditions on MIMIC-IV and eICU datasets.

## Method Summary
AUM represents each medical modality as a multivariate Gaussian distribution with separate MLPs learning mean and variance vectors. The framework employs a bipartite patient-modality graph with uncertainty-aware attention that uses inverse-variance weighting to emphasize reliable information. A variational information bottleneck regularizes uncertainty learning. The model is trained in two stages: first freezing variance layers for 20 epochs, then jointly optimizing all parameters for 80 epochs. Experiments show consistent improvements in mortality prediction under MCAR and MNAR missing patterns.

## Key Results
- Achieves 2.26% AUC-ROC improvement for mortality prediction on MIMIC-IV dataset
- Demonstrates 2.17% AUC-ROC improvement on eICU dataset
- Maintains robust performance across missing ratios 0.1-0.7 under both MCAR and MNAR conditions

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Representation for Reliability Quantification
- Claim: Representing each modality as a multivariate Gaussian distribution enables principled quantification of modality-specific reliability.
- Mechanism: Two separate MLPs learn mean vectors (μ) and variance vectors (σ) for each modality. The variance explicitly captures aleatoric uncertainty—inherent noise in the data acquisition process. High variance signals low reliability, allowing downstream aggregation to discount noisy or ambiguous inputs.
- Core assumption: Aleatoric uncertainty in medical data is locally estimable from unimodal features and correlates with predictive reliability.
- Evidence anchors:
  - [abstract] "AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification."
  - [section 2.2] Equation (1)-(2) formalize the Gaussian parameterization with variance quantifying aleatoric uncertainty.
  - [corpus] Related work on reducing uncertainty through multi-modal acquisition (arXiv:2501.18268) supports the premise that uncertainty decomposition is valuable, though does not validate the specific Gaussian parameterization.
- Break condition: If learned variances collapse to near-zero or become uniform across modalities regardless of input quality, the mechanism is not functioning as intended.

### Mechanism 2: Uncertainty-Aware Attention for Dynamic Aggregation
- Claim: Inverse-uncertainty weighting in message passing emphasizes reliable modalities while suppressing noisy or missing ones.
- Mechanism: Attention weights α are computed as softmax of inverse variance (scaled by temperature θ). Messages from low-uncertainty nodes receive higher weights. Missing modalities are initialized with high uncertainty, causing them to contribute minimally without explicit imputation. Gaussian aggregation separately combines means (weighted concatenation) and variances (independent Gaussian composition rule).
- Core assumption: Lower variance correlates with higher informational value for the prediction task.
- Evidence anchors:
  - [abstract] "uncertainty-aware aggregation mechanism...more reliable information from available modalities is dynamically emphasized."
  - [section 2.3] Equation (3)-(5) define the attention mechanism and variance propagation rules.
  - [corpus] Buffer replay methods (arXiv:2511.23070) and prompt-based approaches address missing modalities through different mechanisms; corpus does not provide direct validation of uncertainty-weighted attention specifically.
- Break condition: If attention weights do not meaningfully vary with learned uncertainty, or if performance degrades when uncertainty-aware attention is replaced with uniform weighting, the mechanism may not be active.

### Mechanism 3: Variational Information Bottleneck for Regularization
- Claim: KL-divergence regularization prevents overfitting and encourages compact, generalizable uncertainty estimates.
- Mechanism: The VIB term adds KL divergence between the learned Gaussian representation and a standard normal prior to the loss. This constrains the model from encoding task-irrelevant information into the representation and prevents variance estimates from growing arbitrarily to "explain away" prediction errors.
- Core assumption: A standard normal prior is a reasonable regularizer for patient representations.
- Evidence anchors:
  - [section 2.4] Equation (6)-(7) define the VIB loss term.
  - [section 3.5, Table 2] Ablation A1.1 shows performance drop from 0.9230 to 0.9218 on MIMIC-IV mortality when VIB is removed, indicating contribution.
  - [corpus] No direct corpus validation of VIB in missing modality contexts; related work focuses on imputation or prompt-based approaches.
- Break condition: If removing VIB improves performance, or if KL term dominates the loss causing underfitting, the regularization strength may be misconfigured.

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper explicitly models aleatoric uncertainty (inherent data noise) rather than epistemic uncertainty (model uncertainty). Conflating these leads to incorrect architectural choices.
  - Quick check question: If you add more training data, which uncertainty type should decrease?

- Concept: **Variational Information Bottleneck**
  - Why needed here: The VIB loss term is critical for preventing the model from learning degenerate uncertainty estimates. Understanding the information-theoretic intuition helps debug training dynamics.
  - Quick check question: What happens to the KL term if the learned variance grows without bound?

- Concept: **Message Passing in Bipartite Graphs**
  - Why needed here: The patient-modality graph structure is the foundation for aggregation. Understanding how nodes exchange messages and update representations is prerequisite to debugging the uncertainty-aware attention.
  - Quick check question: In a bipartite graph with patient nodes and modality nodes, can two patient nodes be directly connected?

## Architecture Onboarding

- Component map: Modality-specific encoders → Mean/variance MLPs → Gaussian edge embeddings → Uncertainty-aware attention → Gaussian aggregation (mean + variance paths) → Patient representation → Classification head + VIB regularization

- Critical path:
  1. Input features → Encoder → μ, σ estimation (Equation 1)
  2. Edge embedding initialization with distributional parameters
  3. Attention computation via inverse variance (Equation 3)
  4. Separate mean aggregation (Equation 4) and variance aggregation (Equation 5)
  5. Loss computation: contrastive + classification + VIB (Equation 6-7)

- Design tradeoffs:
  - Two-stage training (20 epochs frozen variance, 80 epochs joint) stabilizes learning but requires careful scheduling
  - Gaussian assumption enables analytical variance propagation but may not match true distribution
  - Temperature parameter θ in attention controls sensitivity to uncertainty differences—requires tuning

- Failure signatures:
  - Variance collapse: all σ → 0 indicates VIB may be too weak or MLP initialization problematic
  - Attention uniformity: α values near-uniform suggests temperature θ is poorly scaled
  - Performance degradation at high missing ratios (>0.5 MCAR, >0.3 MNAR) indicates uncertainty estimates may not generalize

- First 3 experiments:
  1. **Sanity check**: Replace learned variance with constant values; verify attention mechanism responds as expected (performance should degrade).
  2. **Ablation**: Remove VIB term and compare variance distributions—are they more dispersed or collapsed?
  3. **Missing ratio sweep**: Evaluate on held-out test sets with controlled missing rates (0.1–0.7) under both MCAR and MNAR to characterize robustness boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the aggregation rule for uncertainty propagation be derived from stricter probabilistic principles rather than the current heuristic combination of Gaussian variables?
- Basis in paper: [explicit] The conclusion states future work will focus on "the mathematical modeling of uncertainty propagation."
- Why unresolved: The current method (Eq. 5) approximates the composition of independent Gaussian messages under attention using a heuristic variance aggregation strategy, which may not be theoretically optimal.
- What evidence would resolve it: A derivation of a closed-form or variational update rule for graph message passing that improves AUC-ROC or uncertainty calibration scores.

### Open Question 2
- Question: How can the quantified aleatoric uncertainty be transformed into clinically actionable explanations or interpretable diagnostics?
- Basis in paper: [explicit] The conclusion lists "applications in interpretable diagnostics" as a specific direction for future research.
- Why unresolved: While the paper demonstrates that high uncertainty correlates with missing data and noise, it does not validate if the uncertainty maps provide human-understandable insights for clinical decision support.
- What evidence would resolve it: A qualitative evaluation or user study demonstrating that the learned uncertainty weights align with known clinical ambiguities or diagnostic criteria.

### Open Question 3
- Question: Is the assumption that unimodal representations follow a multivariate Gaussian distribution sufficient to capture the complexity of inherent data uncertainty in medical modalities?
- Basis in paper: [inferred] The method models each modality as a Gaussian (Eq. 2) to capture aleatoric uncertainty, a simplification that may not reflect multi-modal or heavy-tailed data distributions found in complex clinical settings.
- Why unresolved: The paper evaluates predictive performance (AUC) but does not assess the calibration or "correctness" of the assumed distributional form against the true data likelihood.
- What evidence would resolve it: Experiments utilizing mixture models or normalizing flows to compare the fit and predictive robustness against the single-Gaussian constraint.

## Limitations
- The Gaussian assumption for uncertainty propagation may not capture complex multi-modal distributions in medical data
- Key architectural details (modality encoder specifications, exact hyperparameters) are not fully specified, limiting exact reproduction
- The two-stage training procedure introduces additional complexity that requires careful hyperparameter tuning

## Confidence
- **Medium** confidence in reported performance improvements
  - Ablation study provides evidence for individual components
  - Consistent gains across both MIMIC-IV and eICU datasets
  - Key architectural details not fully specified
  - Gaussian assumption may not match true data distributions
  - Two-stage training introduces additional complexity

## Next Checks
1. Verify the attention mechanism's responsiveness by systematically varying learned variances and measuring corresponding attention weight changes
2. Test robustness to initialization by training multiple seeds and measuring variance stability across runs
3. Evaluate performance when replacing the Gaussian aggregation with uniform weighting to isolate the contribution of uncertainty-aware attention