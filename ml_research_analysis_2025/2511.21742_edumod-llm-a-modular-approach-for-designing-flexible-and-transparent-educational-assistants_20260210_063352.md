---
ver: rpa2
title: 'EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational
  Assistants'
arxiv_id: '2511.21742'
source_url: https://arxiv.org/abs/2511.21742
tags:
- retrieval
- response
- student
- evaluation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EduMod-LLM, a modular framework for LLM-based
  educational QA systems. It enables fine-grained evaluation of function calling,
  retrieval, and response generation components using real student questions.
---

# EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants

## Quick Facts
- **arXiv ID**: 2511.21742
- **Source URL**: https://arxiv.org/abs/2511.21742
- **Reference count**: 29
- **Primary result**: EduMod-LLM achieves superior factuality (4.648/5), relevance (4.955/5), and retrieval recall (0.784@1) using modular LLM-based components for educational QA systems.

## Executive Summary
EduMod-LLM introduces a modular framework for LLM-based educational QA systems that enables fine-grained evaluation of function calling, retrieval, and response generation components using real student questions. The system employs structure-aware retrieval that leverages hierarchical course content organization and uses an LLM-as-a-Judge module aligned with TA grading standards. Key results demonstrate GPT-4.1 achieves best response quality, the structure-aware retrieval method significantly outperforms vector-based baselines, and the multihop function-calling approach matches or exceeds hand-designed rule-based methods without requiring course metadata.

## Method Summary
The framework implements three modular components: function calling (selecting appropriate retrieval sources via OpenAI Function-calling API), structure-aware retrieval (using hierarchical document trees built with GPT-4o-based chunking and beam search), and response generation (using GPT-4.1). The system employs a multihop function-calling approach that decomposes selection and argument generation into sequential LLM calls. An LLM-as-a-Judge module evaluates responses across factuality, relevance, and style dimensions using rubrics defined by expert TAs, with DeepSeek-V3 showing highest alignment with human judgments.

## Key Results
- GPT-4.1 achieves highest response quality scores: factuality 4.648/5, relevance 4.955/5, style 2.999/3
- Structure-aware retrieval achieves 0.784 Recall@1 vs 0.441-0.460 for embedding baselines
- Multihop function calling matches TA-designed rule-based methods without requiring course metadata
- LLM-as-a-Judge (DeepSeek-V3) achieves >70% exact match with expert TA evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structure-aware retrieval outperforms vector-based methods for educational content by ~30% on Recall@k metrics.
- **Mechanism:** Documents are chunked along natural structural boundaries using GPT-4o to detect headers. Chunks are recursively summarized into a k-ary tree where parent nodes summarize children. At inference, the LLM first selects relevant top-level summaries from a table of contents, then performs beam-k search through the tree to locate specific chunks.
- **Core assumption:** Educational documents possess meaningful hierarchical structure that semantic embeddings alone fail to capture.
- **Evidence anchors:** [Section 3.3] describes hierarchical tree construction; [Table 1] shows hier_gen achieves 0.784 Recall@1 vs. 0.441-0.460 for baselines.

### Mechanism 2
- **Claim:** Multihop function calling matches TA-designed rule-based function selection while eliminating hand-engineering requirements.
- **Mechanism:** Function selection and argument generation are decomposed into two sequential LLM calls rather than a single call. The first call identifies which retrieval function(s) to invoke, the second generates query arguments.
- **Core assumption:** Decomposing the decision process reduces error propagation compared to joint selection-and-argument generation.
- **Evidence anchors:** [Section 3.2] describes the multihop approach; [Table 2] shows fc_multihop achieves 4.689 factuality / 4.951 relevance vs. Edison's 4.541 / 4.918.

### Mechanism 3
- **Claim:** Non-reasoning models (DeepSeek-V3) can reliably approximate TA evaluation standards with >70% exact match.
- **Mechanism:** Expert TAs define rubrics for three dimensions (factuality 1-5, relevance 1-5, style 1-3). These criteria are encoded into few-shot prompts. LLMs score responses against these rubrics with reference to TA-written ground truth.
- **Core assumption:** TA evaluation preferences can be adequately captured by explicit rubrics.
- **Evidence anchors:** [Section 3.4] details rubric development; [Section 4.1] reports DeepSeek-v3 achieves >70% exact match across all categories.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: EduMod-LLM is fundamentally a RAG pipeline; understanding how retrieval and generation couple is prerequisite to understanding modular evaluation.
  - Quick check question: If retrieval fails to surface relevant course materials, can the generator compensate? Why or why not?

- **Concept: Function Calling in LLMs**
  - Why needed here: The pipeline reformulates retrieval strategies as functions (qa_retrieval, textbook_retrieval, etc.) that the LLM dynamically selects; this is the first modular component evaluated.
  - Quick check question: What information must the LLM have about available functions to make correct selection decisions?

- **Concept: Evaluation Metrics (Recall@k, F1, Likert Scales)**
  - Why needed here: The paper reports Recall@k for retrieval, F1 for function selection, and multi-dimensional Likert scores for response quality; understanding what each measures is essential for interpreting results.
  - Quick check question: If Recall@5 is 0.946 but Recall@1 is 0.784, what does this suggest about the retrieval ranking quality?

## Architecture Onboarding

- **Component map:**
  Student Question → [Function Calling Module] → Selected Retrieval Function(s) → [Structure-Aware Retrieval] → Retrieved Context + Question → [Response Generation LLM] → Draft Response → [LLM-as-a-Judge] → Quality Scores (F/R/S)

- **Critical path:** Function calling accuracy → retrieval relevance → generation quality. Retrieval is the primary driver (30% gain from structure-aware method), with function calling as secondary, and LLM selection as tertiary.

- **Design tradeoffs:**
  - fc_multihop vs. rule-based: Multihop requires no course-specific metadata but adds one LLM call latency; rule-based (Edison) is deterministic but requires TA engineering per course.
  - Hierarchical vs. vector retrieval: Hierarchical requires upfront tree construction and assumes structured materials; vector retrieval is zero-setup but yields lower recall.
  - LLM-as-a-Judge vs. human evaluation: Scales to 1000+ questions but relies on rubric coverage and judge model selection.

- **Failure signatures:**
  - Low FC F1 but high response quality: Model may be retrieving correct content via wrong function.
  - High Recall@5 but low response quality: Retrieved chunks not ranked well; generator receiving noise.
  - Judge disagrees with TAs on style: Rubric may not capture course-specific tone expectations.
  - Structure-aware retrieval fails on unstructured documents: Assumption of hierarchical organization violated.

- **First 3 experiments:**
  1. Validate retrieval on your course materials: Build hierarchical tree from syllabus, assignments, textbook. Compute Recall@1/3/5 on 50-100 human-annotated questions. Compare against text-embedding-ada-002 baseline.
  2. Ablate function calling strategies: Implement fc, fc_forced, fc_iterative, fc_multihop with GPT-4o on 100 questions. Measure FC F1 and downstream response quality.
  3. Calibrate LLM-as-a-Judge: Have 2 TAs label 50 responses on factuality/relevance/style. Test DeepSeek-V3, GPT-4o, Claude as judges. Report exact match and MAE.

## Open Questions the Paper Calls Out

- **Question:** Does high performance on automated QA metrics translate to measurable improvements in student learning outcomes?
  - **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that their work focuses on the quality of automated responses rather than direct impacts on learning outcomes.
  - **Why unresolved:** The current study relies on offline evaluation using historical data and proxy metrics rather than measuring knowledge acquisition or retention in live students.
  - **What evidence would resolve it:** A controlled, longitudinal deployment comparing assessment performance and concept retention between student groups using the AI assistant versus standard resources.

- **Question:** How robust is the structure-aware retrieval method when applied to educational materials that lack clear hierarchical organization?
  - **Basis in paper:** [explicit] The paper notes that the retrieval approach assumes well-organized course materials with clear structure, a condition that may not exist in all educational settings.
  - **Why unresolved:** The method was evaluated on a specific data science course with structured textbooks and assignments; its effectiveness on unstructured content remains untested.
  - **What evidence would resolve it:** Benchmarking the hierarchical retrieval module against vector baselines using datasets comprised of heterogeneous or poorly structured educational content.

- **Question:** Do the optimal component configurations transfer effectively to non-STEM or discussion-based courses?
  - **Basis in paper:** [explicit] The authors acknowledge the evaluation scope is limited to a single computing course, which may not fully generalize to other disciplines.
  - **Why unresolved:** Pedagogical norms in humanities or social sciences differ significantly from the technical correctness prioritized in data science.
  - **What evidence would resolve it:** Cross-domain experiments deploying the pipeline in diverse subjects to evaluate if component rankings remain consistent.

## Limitations
- Dataset limited to single UC Berkeley course (1,000 questions) restricting generalizability across institutions and disciplines
- Structure-aware retrieval assumes hierarchical course materials, potentially failing on unstructured content like discussion threads
- Privacy constraints prevent public release of human annotations, requiring new data collection for replication
- Computational overhead of hierarchical tree construction and beam search not characterized for scalability

## Confidence

- **High:** Hierarchical retrieval outperforming vector baselines (supported by Recall@k metrics), LLM-as-a-Judge alignment with TA scores (validated on 180 samples), GPT-4.1 response quality (factuality 4.648/5.0)
- **Medium:** fc_multihop matching rule-based approaches (limited to single course comparison), DeepSeek-V3 as optimal judge (based on one rubric type and single course domain)
- **Low:** Computational cost estimates, cross-institutional generalization, performance on unstructured or multimodal educational content

## Next Checks

1. **Dataset Diversity Test:** Apply EduMod-LLM to three courses from different departments (e.g., humanities, STEM, social sciences) with varying material structures. Measure if hierarchical retrieval maintains >0.70 Recall@1 across all domains.

2. **Latency-Cost Analysis:** Profile end-to-end latency and API costs for fc_multihop + hierarchical retrieval + GPT-4.1 generation on 100 questions. Compare against rule-based + vector retrieval baseline to quantify trade-offs.

3. **Judge Cross-Validation:** Have three independent TAs evaluate 50 responses using EduMod-LLM's rubrics. Compute inter-rater reliability (Krippendorff's alpha) and compare LLM judge scores against each TA. Verify DeepSeek-V3 maintains >65% exact match across all evaluators.