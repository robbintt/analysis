---
ver: rpa2
title: Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections
arxiv_id: '2512.14895'
source_url: https://arxiv.org/abs/2512.14895
tags:
- expert
- testbed
- agent
- file
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: On-policy expert corrections (OECs) address covariate shift in
  multi-turn LM agent training by partially rolling out trajectories with a student
  model and then switching to an expert model mid-trajectory. Experiments on SWE-bench
  show OECs improve resolution rates by 14% (7B) and 13% (32B) over traditional imitation
  learning, while combining OEC and behavioral cloning achieves SOTA performance.
---

# Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections

## Quick Facts
- arXiv ID: 2512.14895
- Source URL: https://arxiv.org/abs/2512.14895
- Reference count: 40
- Key outcome: OECs improve SWE-bench resolution rates by 14% (7B) and 13% (32B) over traditional imitation learning

## Executive Summary
On-policy expert corrections (OECs) address covariate shift in multi-turn LM agent training by partially rolling out trajectories with a student model and then switching to an expert model mid-trajectory. The method generates expert trajectories that contain student-induced states, teaching the model what to do when it reaches these states during inference. Experiments on SWE-bench show OECs improve resolution rates by 14% (7B) and 13% (32B) over traditional imitation learning, while combining OEC and behavioral cloning achieves SOTA performance.

## Method Summary
The method involves rolling out student models to generate on-policy states, randomly switching to expert models mid-trajectory, applying rejection sampling and repetition filtering, then training with on-policy masking to focus only on expert-generated tokens. The approach combines the benefits of on-policy data coverage with expert demonstrations, addressing the fundamental limitation of covariate shift in sequential decision-making.

## Key Results
- 14% absolute improvement in SWE-bench lite resolution rate (41.5% → 55.5%) for 7B models
- 13% absolute improvement in SWE-bench verified resolution rate (27% → 40%) for 32B models
- Combining OEC with behavioral cloning data achieves SOTA performance on SWE-bench
- Ablations show on-policy masking and repetitive trajectory filtering are critical for success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OECs reduce covariate shift by exposing the student to expert actions on states it actually encounters during rollout, rather than states only the expert visits.
- Mechanism: The student model begins trajectory rollouts, generating on-policy states. At a randomly sampled switch point, the expert takes over and completes the trajectory. Training only on the expert portion (via on-policy masking) teaches the model what to do when it reaches these student-induced states.
- Core assumption: The distribution of states reached by the student during training is similar to what it will encounter at inference time.
- Evidence anchors: [abstract] "...off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift"; [section 4.3] Figure 4 shows Fréchet Distance between student and expert embeddings increasing throughout trajectories; [corpus] "Visuomotor policies trained via behavior cloning are vulnerable to covariate shift" (Latent Policy Barrier, arXiv:2508.05941)

### Mechanism 2
- Claim: OECs enable rejection sampling with verifiable rewards while maintaining on-policy state coverage.
- Mechanism: Unlike traditional DAgger (which queries expert for single actions), OECs roll out full trajectories to completion. This allows environment feedback (e.g., unit tests passing) to filter successful trajectories for training, combining on-policy state coverage with quality guarantees.
- Core assumption: Tasks have verifiable reward signals (e.g., unit tests) that correlate with trajectory quality.
- Evidence anchors: [section 3.1] "OECs allow the trajectory to be rolled out until completion, thereby allowing us to incorporate verifier reward"; [section 3.2] "we evaluate the resulting patches generated from the trajectories on each problem's associated unit tests...filter out all trajectories that fail"

### Mechanism 3
- Claim: Repetition filtering and on-policy masking prevent gradient corruption from low-quality student actions.
- Mechanism: Open-source models can enter repetitive loops (e.g., reading the same file 20+ times). These trajectories pass unit tests but dominate gradients due to repeated identical actions. Filtering removes them; masking ensures only expert actions contribute to loss.
- Core assumption: Repetitive trajectories that pass tests exist and harm learning more than they help.
- Evidence anchors: [section 3.3] "8.46% (167 from repetitive file reading and 93 from identical actions) samples needed to be filtered" in 7B setting; [section 4.2/Table 2] Removing repetition filtering causes 6% absolute drop (17% → 11%) in 7B setting

## Foundational Learning

- Concept: **Covariate Shift in Sequential Decision-Making**
  - Why needed here: OECs are designed specifically to address this problem; understanding it is prerequisite to understanding why the method works.
  - Quick check question: Can you explain why a model trained on expert trajectories fails when it makes a small mistake early in a multi-turn trajectory?

- Concept: **DAgger (Dataset Aggregation) Algorithm**
  - Why needed here: OECs are explicitly inspired by DAgger; understanding the original algorithm clarifies what OECs preserve vs. modify.
  - Quick check question: How does DAgger differ from behavioral cloning, and why can't DAgger easily incorporate verifiable rewards?

- Concept: **Rejection Sampling Fine-Tuning**
  - Why needed here: OECs combine with rejection sampling; understanding this baseline clarifies what OECs add.
  - Quick check question: Why might a trajectory that passes all unit tests still be harmful for training?

## Architecture Onboarding

- Component map: Student Model (M_S) → Rollout starts → Random switch point (U[0,30]) → History reformatting → Expert Model (M_E) takes over → Trajectory completion → Unit test evaluation → Rejection sampling filter → Repetition filter → On-policy masking (mask student portion) → SFT on expert tokens only

- Critical path:
  1. Implement switch logic in SWE-agent scaffold (or your agent framework)
  2. Implement history reformatting when switching models (system prompts, tool call formats)
  3. Apply both filters in sequence: rejection sampling → repetition filter → on-policy masking
  4. Train with standard SFT, masking student-generated tokens

- Design tradeoffs:
  - Earlier switch → more expert data but less on-policy coverage; later switch → more on-policy coverage but higher discard rate if student fails early
  - 7B student with 32B expert vs. 32B student with closed-source expert: weaker students need more repetition filtering
  - Mixing OEC with BC data helps; pure OEC underperforms mixture (Figure 3)

- Failure signatures:
  - High discard rate (>50%) before expert switch: student model too weak or switch distribution too late
  - Performance degradation despite passing tests: check for repetitive trajectories
  - No improvement from OEC over BC: likely missing on-policy masking in larger models

- First 3 experiments:
  1. **Covariate shift baseline**: Generate 50 rollouts each with student and expert on same 10 problems; compute embedding Fréchet Distance across turns to confirm shift exists (replicate Figure 4).
  2. **Ablation sweep**: Train with OEC data (1) without repetition filtering, (2) without on-policy masking, (3) with both. Compare on held-out validation set to quantify each component's contribution.
  3. **Switch point sensitivity**: Partition OEC trajectories by switch index (0-6, 7-15, 16-30) and train separate models to find optimal switch distribution for your student-expert pair.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do on-policy expert corrections generalize to other multi-turn LM agent domains beyond software engineering tasks?
- Basis in paper: [explicit] "It will be important for future work to test our findings in other multi-turn LM agent domains, especially as LM agents are used for more complex and long-horizon tasks."
- Why unresolved: Experiments were limited to SWE-bench; the method's effectiveness in other agentic settings (web navigation, multi-tool orchestration, long-horizon reasoning) remains untested.
- What evidence would resolve it: Applying OECs to benchmarks like WebArena, ToolBench, or OSWorld with similar comparative analysis against BC and on-policy baselines.

### Open Question 2
- Question: Can OECs be extended to provide theoretical no-regret learning guarantees similar to traditional DAgger?
- Basis in paper: [explicit] "OECs are not known to benefit from the same no-regret learning guarantees as traditional DAgger."
- Why unresolved: The partial on-policy nature of OEC trajectories differs fundamentally from DAgger's fully on-policy state distribution, and no theoretical analysis was provided.
- What evidence would resolve it: Formal analysis bounding regret, or empirical demonstration that iterative OEC rounds converge to expert performance with provable rates.

### Open Question 3
- Question: Does iterative/online OEC generation mitigate the problem of on-policy portions becoming increasingly off-policy during fine-tuning?
- Basis in paper: [explicit] "As fine-tuning is performed on a set of OEC trajectories, the on-policy portions of the trajectories become increasingly off-policy, potentially limiting their benefit."
- Why unresolved: The paper only used a single round of OEC generation; the suggested mitigation of multiple intermediate rounds was not tested.
- What evidence would resolve it: Experiments comparing single-round OEC against iteratively regenerated OEC trajectories across multiple training epochs.

### Open Question 4
- Question: Why does on-policy masking critically impact 32B models but not 7B models when fine-tuning with OECs?
- Basis in paper: [inferred] Table 2 shows on-policy masking makes no difference for Student-7b (17% both ways) but is crucial for Student-32b (40% with masking vs. 31.8% without).
- Why unresolved: The paper observes this phenomenon but does not investigate the underlying cause related to model scale and training stability.
- What evidence would resolve it: Controlled experiments across model scales analyzing gradient distributions, loss landscapes, and trajectory quality when training with vs. without on-policy masking.

## Limitations
- Covariate shift quantification shows correlation but not direct causation with performance improvements
- Filter effectiveness validation lacks ablations showing individual component contributions
- Hyperparameter sensitivity not explored systematically for different student-expert pairs

## Confidence
- **High confidence**: Core mechanism of on-policy masking and performance improvements over BC baselines
- **Medium confidence**: Combination with BC data achieving SOTA, but relative contribution unclear
- **Low confidence**: Claims about verifiable reward incorporation need more direct empirical validation

## Next Checks
1. **Covariate shift ablation**: Generate trajectories where student and expert are identical (zero covariate shift) and compare learning curves to standard OEC training.
2. **Filter component isolation**: Run ablations with (a) rejection sampling only, (b) repetition filtering only, (c) both filters, and (d) no filters on the same dataset.
3. **Switch point optimization**: Systematically vary the switch distribution (U[0,10], U[0,20], U[0,30], U[10,30]) and measure both success rate and filtering rate.