---
ver: rpa2
title: Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer
  Networks
arxiv_id: '2511.02258'
source_url: https://arxiv.org/abs/2511.02258
tags:
- stochastic
- learning
- information
- gradient
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes functional central limit theorems (FCLTs)
  for stochastic gradient descent (SGD) in high-dimensional single-layer neural networks.
  The key contribution is identifying a critical scaling regime where the step size
  equals 1/N, revealing that the deterministic mean-field approximation (dynamical
  mean-field theory) breaks down and stochastic fluctuations become significant.
---

# Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks

## Quick Facts
- **arXiv ID**: 2511.02258
- **Source URL**: https://arxiv.org/abs/2511.02258
- **Reference count**: 32
- **Primary result**: Establishes functional central limit theorems for SGD in high-dimensional single-layer networks, revealing critical scaling regime where deterministic mean-field theory breaks down

## Executive Summary
This paper analyzes stochastic gradient descent in high-dimensional single-layer neural networks, identifying a critical scaling regime where the step size equals 1/N. In this regime, the deterministic mean-field approximation breaks down and stochastic fluctuations become significant, requiring higher-order corrections for accurate modeling. The analysis focuses on activation functions with information exponent greater than two, where sample complexity scales quasi-linearly or polynomially with dimension, and proves that near fixed points, the rescaled effective dynamics converge to Ornstein-Uhlenbeck processes rather than following deterministic ODEs.

## Method Summary
The paper studies single-layer teacher-student networks with online SGD, using summary statistics (correlation m and radius r²⊥) to characterize the dynamics. It establishes functional central limit theorems showing that at critical step-size scaling δ_N = 1/N, SGD trajectories deviate from deterministic ODEs and converge to stochastic differential equations. The analysis leverages Hermite expansions of activation functions and Stein's lemma to derive drift and volatility terms, proving that random initialization at saddle points leads to Ornstein-Uhlenbeck dynamics for the correlation component while the radius follows deterministic evolution.

## Key Results
- SGD dynamics transition from ballistic (ODE) limits to diffusive (SDE) limits at critical scaling δ_N = 1/N
- Near fixed points with information exponent > 2, rescaled correlation follows Ornstein-Uhlenbeck process while radius follows deterministic dynamics
- Deterministic mean-field theory fails to capture stochastic fluctuations in this parameter regime
- Sample complexity scaling (quasi-linear/polynomial vs linear) determines whether deterministic or stochastic modeling is appropriate

## Why This Works (Mechanism)

### Mechanism 1: Critical Step-Size Scaling Transitions Between Deterministic and Stochastic Regimes
- Claim: At δ_N = 1/N, SGD dynamics transition from ballistic (ODE) limits to diffusive (SDE) limits, introducing correction terms absent in classical mean-field theory
- Mechanism: Step-size-to-dimension ratio controls gradient noise accumulation. At critical scale, noise fluctuations persist at same order as drift terms, generating effective volatility
- Core assumption: Triplet satisfies δ_N-localizability and asymptotic closability (Definitions 2.1-2.2)
- Evidence anchors: [abstract] identifies critical scaling regime; [section 2.2] focuses on δ_N = 1/N; related work extends to momentum variants
- Break condition: δ_N >> 1/N (super-critical) or δ_N << 1/N (sub-critical)

### Mechanism 2: Information Exponent Controls Phase Diagram and Sample Complexity
- Claim: Information exponent k determines whether search phase consumes nearly all samples, forcing analysis into stochastic regime
- Mechanism: k ≥ 2 means sample complexity scales as N^(k-1) for search phase. When k > 2, polynomial scaling dominates linear descent phase, leaving insufficient data for deterministic averaging
- Core assumption: Information exponent strictly greater than 2 (a_1(f) = a_2(f) = 0)
- Evidence anchors: [abstract] focuses on k > 2; [section 2.1] defines information exponent via Hermite coefficients
- Break condition: k = 1 (trivial search phase) or k = 2 (marginal behavior)

### Mechanism 3: Ornstein-Uhlenbeck Emergence at Saddle Points Under Random Initialization
- Claim: Near fixed points with random Gaussian initialization, rescaled correlation √N·m converges to OU process while radius follows deterministic dynamics
- Mechanism: Random initialization places system at saddle set m = 0. Rescaling amplifies fluctuations to macroscopic scale. Linearized drift produces mean-reversion, gradient noise generates diffusion
- Core assumption: X_0 ~ N(0, σ²I_N) with fixed σ², activation bounded with k ≥ 2
- Evidence anchors: [abstract] describes saddle point OU dynamics; [section 2.2, Corollary 2.7] gives explicit SDE
- Break condition: Non-zero correlation initialization or unbounded activation

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) and Ornstein-Uhlenbeck Processes
  - Why needed here: Main result characterizes SGD limits as SDEs, specifically OU processes near fixed points
  - Quick check question: Can you explain why an OU process is called "mean-reverting" and identify the mean-reversion rate in Corollary 2.7's SDE?

- Concept: Hermite Polynomials and Information Exponent
  - Why needed here: Information exponent defined via Hermite coefficients; drift/corrector terms are expectations over Hermite expansions
  - Quick check question: Given f(x) = x³, what is its information exponent? (Expand in Hermite polynomials)

- Concept: Functional Central Limit Theorems (FCLTs)
  - Why needed here: Paper establishes FCLTs describing convergence of entire trajectories to continuous-time limits
  - Quick check question: How does FCLT differ from standard CLT? What additional information does it provide?

## Architecture Onboarding

- Component map:
  - Input layer: Data stream (a_k, y_k) with a_k ~ N(0, I_N), y_k = f(⟨a_k, X*⟩) + ε_k
  - Summary statistics extractor: u_N(X) = (m, r²⊥) where m = ⟨X, X*⟩, r²⊥ = ||X||² - m²
  - Dynamics engine: ODE system (Eqs. 2-3) for ballistic regime, SDE system (Eqs. 5-6) for diffusive regime
  - Limit classifier: Check step size δ_N relative to 1/N threshold
  - Output: Convergence predictions, sample complexity bounds, trajectory statistics

- Critical path:
  1. Verify δ_N-localizability conditions (bounded gradients, moment estimates)
  2. Identify information exponent from Hermite expansion of activation
  3. Determine regime: δ_N << 1/N (ballistic), δ_N ≈ 1/N (diffusive), δ_N >> 1/N (breakdown)
  4. If diffusive + near fixed point → apply OU approximation
  5. Extract drift (F - G) and volatility (Σ) from expectations in Theorems 2.4-2.6

- Design tradeoffs:
  - Deterministic vs Stochastic modeling: DMFT (ODE) simpler but fails at critical scaling; SDE captures fluctuations but requires computing volatility matrix
  - Summary statistic dimension: Two statistics (m, r²⊥) suffice for single-index models; multi-index models require larger order parameter matrices
  - Activation function choice: Bounded activations guarantee ODE existence; unbounded activations can cause divergence

- Failure signatures:
  - DMFT prediction error: Empirical SGD trajectory shows persistent variance around fixed point while DMFT predicts convergence
  - Correlation stagnation: m remains near zero longer than predicted by ODE
  - Sample complexity mismatch: Quasi-linear/polynomial scaling suggests information exponent ≥ 2; linear scaling suggests exponent = 1

- First 3 experiments:
  1. Step-size sweep: Fix N=1000, activation with known information exponent. Vary δ_N ∈ {0.1/N, 0.5/N, 1/N, 2/N}. Measure trajectory variance near fixed points
  2. Information exponent validation: Compare activations with k=1,2,3. Track sample complexity to reach correlation m > 0.5
  3. OU parameter estimation: At critical scaling with k≥2 activation, initialize at fixed point r*⊥. Collect m̃ = √N·m trajectories. Fit OU process to estimate drift coefficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do FCLTs and OU approximations hold for multi-layer networks or multi-index models?
- Basis in paper: Analysis restricted to single-layer networks using specific order parameters that don't account for hierarchical interactions
- Why unresolved: Localizability and asymptotic closability conditions proven only for single hidden layer summary statistics
- What evidence would resolve it: Extension of Theorem 2.6 to two-layer teacher-student setup proving convergence of inter-layer correlations to coupled SDE system

### Open Question 2
- Question: Can diffusive limits be extended globally to describe SGD trajectory outside microscopic neighborhoods of fixed points?
- Basis in paper: Theorems 2.6-2.7 prove OU convergence specifically when initialized near saddle set or fixed point
- Why unresolved: Proofs rely on Taylor expansions and rescaling arguments that are local to these regions
- What evidence would resolve it: Uniform convergence result demonstrating SDE dynamics during ballistic/search phase, not just near equilibrium

### Open Question 3
- Question: How does data distribution affect stochastic fluctuations and breakdown of deterministic mean-field theory?
- Basis in paper: Assumes i.i.d. standard Gaussian features and uses Stein's lemma extensively
- Why unresolved: Unclear if correction terms and OU limit depend critically on Gaussianity or generalize to non-Gaussian data
- What evidence would resolve it: Deriving limiting SDE for non-Gaussian input distributions and verifying if volatility matrix retains same form

## Limitations
- Analysis critically depends on information exponent being strictly greater than two, requiring specific Hermite coefficient structure
- Boundedness requirement for activation functions introduces ambiguity about practical function choices
- Critical step-size scaling δ_N = 1/N represents narrow regime sensitive to implementation details
- OU characterization assumes initialization at saddle points, limiting generality of results

## Confidence
**High Confidence**: Critical scaling mechanism connecting step-size to stochastic corrections is well-supported by theory and literature
**Medium Confidence**: OU process emergence near saddle points relies on delicate technical conditions requiring careful verification
**Low Confidence**: Boundedness requirements and their practical implications for activation function choice introduce significant uncertainty

## Next Checks
1. Implement systematic sweep of step sizes δ_N ∈ {0.1/N, 0.5/N, 1/N, 2/N} for fixed N and bounded activation to empirically identify transition from deterministic to stochastic regimes
2. For activations f(x) = x, f(x) = x² - 1, f(x) = (x³ - 3x)/√6, verify Hermite coefficient structure a_1 = a_2 = 0 for k > 2 cases and measure sample complexity to confirm predicted scaling
3. Initialize at fixed point radius r*⊥ with critical step size δ_N = 1/N, collect rescaled correlation m̃ = √N·m trajectories, fit OU process parameters, and compare against theoretical predictions