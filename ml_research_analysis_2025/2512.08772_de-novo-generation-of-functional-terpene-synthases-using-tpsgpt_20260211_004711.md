---
ver: rpa2
title: De novo generation of functional terpene synthases using TpsGPT
arxiv_id: '2512.08772'
source_url: https://arxiv.org/abs/2512.08772
tags:
- sequences
- protein
- sequence
- training
- protgpt2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TpsGPT, a generative model for designing terpene
  synthase (TPS) enzymes using fine-tuned protein language models. By training on
  79k TPS sequences mined from UniProt, TpsGPT generated 28k candidate sequences.
---

# De novo generation of functional terpene synthases using TpsGPT

## Quick Facts
- arXiv ID: 2512.08772
- Source URL: https://arxiv.org/abs/2512.08772
- Authors: Hamsini Ramanathan; Roman Bushuiev; Matouš Soldát; Jirí Kohout; Téo Hebra; Joshua David Smith; Josef Sivic; Tomáš Pluskal
- Reference count: 40
- Primary result: Fine-tuned protein language model generated functional terpene synthases validated by yeast expression

## Executive Summary
This work introduces TpsGPT, a generative model for designing terpene synthase (TPS) enzymes using fine-tuned protein language models. By training on 79k TPS sequences mined from UniProt, TpsGPT generated 28k candidate sequences. After rigorous filtering using multiple validation metrics—including structural confidence (pLDDT), enzymatic function prediction (EnzymeExplorer, CLEAN), domain detection (InterPro), and structural similarity (Foldseek)—seven putative TPS enzymes were identified. Two of these sequences were experimentally validated, showing enzymatic activity in yeast. The results demonstrate that fine-tuning protein language models on enzyme-specific datasets can successfully generate functional, evolutionarily distant enzymes, offering a scalable alternative to costly directed evolution approaches.

## Method Summary
The approach fine-tunes the ProtGPT2 tiny model (38.9M parameters) on 79k terpene synthase sequences mined from UniProt using HMMER with Pfam/SUPERFAMILY databases. The model generates candidate sequences which are filtered through a multi-stage pipeline: perplexity ranking, structural confidence assessment via ESMFold (pLDDT ≥70), enzymatic function prediction via EnzymeExplorer (≥0.7 TPS score) and CLEAN, sequence identity filtering (maxID ≤60%), structural similarity via Foldseek (TM-score 0.6-0.9), and domain detection via InterPro. The top candidates are expressed in yeast for experimental validation using LC-MS analysis.

## Key Results
- TpsGPT fine-tuned on 79k TPS sequences generated 28k candidate enzymes
- Seven putative TPS enzymes passed all computational validation criteria
- Two experimentally validated sequences showed enzymatic activity in yeast
- Generated sequences maintained TM-scores of 0.6-0.9 with known TPS structures while having <60% sequence identity

## Why This Works (Mechanism)

### Mechanism 1: Distributional Adaptation via Specialized Fine-Tuning
Fine-tuning a general pre-trained protein language model on a curated enzyme dataset restricts the sampleable sequence space to functional local minima. The base model encodes general protein grammar, but fine-tuning on 79k TPS sequences shifts its probability distribution to prioritize TPS-specific constraints like motif positioning and domain arrangement without requiring explicit hard-coding of rules.

### Mechanism 2: In Silico Selection Pressure via Multi-Objective Filtering
Functional sequences are isolated from a large pool of candidates by applying orthogonal structural and functional constraints as a proxy for experimental fitness. A funnel of external oracles (ESMFold, CLEAN, InterPro) rejects sequences violating structural stability or lacking specific functional signatures, effectively simulating natural selection computationally.

### Mechanism 3: Structural Plasticity in Low-Identity Space
The system exploits protein folding code degeneracy to generate evolutionarily distant sequences maintaining functional TPS fold. By enforcing bounds on structural similarity (Foldseek TM-score 0.6-0.9) while minimizing sequence identity (maxID ≤60%), the pipeline forces exploration of sequence space "dark matter"—finding novel paths to the same structural solution.

## Foundational Learning

- **Concept: Autoregressive Protein Language Models (PLMs)**
  - Why needed here: TpsGPT is based on ProtGPT2, which treats amino acid sequences as text and predicts next tokens based on previous context
  - Quick check question: How does the "next-token prediction" objective differ from the "masked-language modeling" objective used in models like ESM?

- **Concept: Perplexity as a Validity Metric**
  - Why needed here: The paper uses perplexity to rank generated sequences; low perplexity indicates the model "recognizes" the sequence as valid protein syntax
  - Quick check question: Does a low perplexity score guarantee biological function, or just structural plausibility?

- **Concept: Structural Confidence Metrics (pLDDT & TM-score)**
  - Why needed here: The pipeline relies on pLDDT for local backbone confidence and TM-score for global fold similarity
  - Quick check question: Why might a sequence have a high pLDDT (stable local structure) but a low TM-score (different global fold) compared to a target?

## Architecture Onboarding

- **Component map:** HMMER/BFD search → 79k TPS sequences → ProtGPT2-Tiny fine-tuning → 28k generated sequences → Perplexity & Max Identity filtering → pLDDT via ESMFold → EnzymeExplorer/CLEAN → MaxID via alignment → Foldseek TM-score → InterPro domain detection → Yeast expression vectors

- **Critical path:** Data curation is the highest leverage component. If the 79k training sequences contain non-TPS noise, the model learns a diffuse distribution. The transition from 28k raw candidates to 7 final candidates highlights strict dependency on filtering thresholds.

- **Design tradeoffs:**
  - ProtGPT2-Tiny vs. Large Models: Using the "tiny" model (38.9M params) reduces GPU cost to <$200 but may limit ability to capture subtle long-range dependencies
  - Novelty vs. Activity: The strict MaxID ≤60% filter prioritizes novelty ("de novo"), likely lowering absolute hit rate compared to homology-based design

- **Failure signatures:**
  - Mode Collapse: Generated sequences are identical to training set (MaxID ≈100%)
  - Structural Hallucination: High perplexity sequences often yield low pLDDT (<50) or fragmented structures in ESMFold
  - Catalytic Inactivity: Valid fold (High TM-score) but loss of catalytic motifs leading to no LC-MS signal

- **First 3 experiments:**
  1. Ablation on Training Size: Fine-tune on only 1,125 experimental seeds vs. full 79k mined set to measure value of synthetic data expansion
  2. Threshold Sensitivity Analysis: Systematically relax MaxID filter (e.g., to 80%) to determine if hit rate improves at cost of novelty
  3. Negative Control Validation: Run validation pipeline on random sequences or non-TPS proteins to confirm specificity of EnzymeExplorer and CLEAN filters

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the remaining five computationally predicted candidates (TpsGPT3–TpsGPT7) exhibit enzymatic activity? The authors state that "ongoing experiments aim to... validate other generated TPSs." Only two of seven generated sequences have undergone wet-lab validation.

- **Open Question 2:** Are the two validated enzymes (TpsGPT1 and TpsGPT2) canonical terpene synthases, and what are their specific products? The authors note that "the presence of oxygen in the product chemical formula suggests they cannot yet be confirmed as canonical TPS enzymes." While activity is confirmed, exact catalytic mechanisms and product structures remain undetermined.

- **Open Question 3:** Can TpsGPT be conditioned to generate synthases for specific terpene subclasses? The paper lists "conditioning TPS generation on terpene subclasses via curated datasets" as a future direction. The current model generates functional sequences without specific control over resulting terpene scaffold.

- **Open Question 4:** Is the fine-tuning and filtering pipeline effective for other protein families with limited datasets? The authors propose "generalizing the methodology to other protein families, such as... lysozymes." The study only demonstrates success on the terpene synthase family.

## Limitations

- Dataset Dependency: The approach relies heavily on quality and representativeness of the 79k TPS sequences mined from UniProt. The full dataset and detailed mining parameters are not publicly available, limiting reproducibility.

- Validation Pipeline Specificity: While multi-stage filtering successfully identified functional sequences, it remains unclear how generalizable these thresholds are to other enzyme families. Validation tools may have inherent biases that could systematically exclude novel but functional architectures.

- Experimental Validation Scope: Only two sequences were experimentally tested, both showing activity. This small sample size limits confidence in overall success rate. LC-MS quantification methods and specific product profiles are not fully detailed.

## Confidence

- **High Confidence**: The core methodology of fine-tuning ProtGPT2 on enzyme-specific datasets and using multi-objective filtering is well-established and successfully demonstrated
- **Medium Confidence**: The claim that TpsGPT can generate functionally novel TPS enzymes is supported by experimental validation of two sequences, but small sample size warrants caution
- **Low Confidence**: The assertion that the approach offers a "scalable alternative to costly directed evolution" is based on computational efficiency claims but lacks direct comparison to experimental directed evolution outcomes

## Next Checks

1. **Dataset Transparency**: Obtain or recreate the complete 79k TPS dataset with detailed mining parameters to verify the foundation of model training
2. **Threshold Sensitivity Analysis**: Systematically vary filtering thresholds (e.g., MaxID from 60% to 80%, TM-score from 0.6 to 0.8) to determine their impact on hit rate and novelty of generated sequences
3. **Broader Experimental Validation**: Test a larger and more diverse set of generated sequences (e.g., 10-20 candidates) across multiple yeast strains or expression systems to establish robustness and generalizability of the approach