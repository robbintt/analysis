---
ver: rpa2
title: Evaluating LLM Metrics Through Real-World Capabilities
arxiv_id: '2505.08253'
source_url: https://arxiv.org/abs/2505.08253
tags:
- technical
- tasks
- arxiv
- generation
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies six real-world AI capabilities\u2014Summarization,\
  \ Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information\
  \ Retrieval\u2014based on analysis of survey data and usage logs. Existing benchmarks\
  \ only cover four of these capabilities, with significant gaps in Evaluating tasks\
  \ like Reviewing Work and Data Structuring."
---

# Evaluating LLM Metrics Through Real-World Capabilities

## Quick Facts
- **arXiv ID:** 2505.08253
- **Source URL:** https://arxiv.org/abs/2505.08253
- **Reference count:** 40
- **Primary result:** Real-world AI usage reveals six capabilities; only four are benchmarked, with major gaps in Reviewing Work and Data Structuring.

## Executive Summary
This paper analyzes 4 million Claude.ai prompts and Danish worker surveys to identify six core AI capabilities—Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information Retrieval. Existing benchmarks only cover four of these capabilities, with Reviewing Work and Data Structuring entirely unmeasured. Using five human-centered criteria (coherence, accuracy, clarity, relevance, and efficiency), the study evaluates leading models and finds that Google Gemini 2.5 outperforms others on capability-aligned metrics, while most benchmarks prioritize abstract problem-solving over practical utility. The authors propose creating new, human-in-the-loop evaluation suites to better reflect real-world usage.

## Method Summary
The study uses qualitative thematic analysis of 32 high-exposure occupational tasks from Danish surveys, validated against the top 100 Claude.ai prompts. Tasks are mapped to six capabilities using O*NET taxonomy. Each benchmark is scored against five criteria (coherence, accuracy, clarity, relevance, efficiency) via manual assessment. Model scores are collected from technical reports for capability-aligned benchmarks, with 20+ benchmarks evaluated across 5 leading models.

## Key Results
- Six capabilities identified: Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, Information Retrieval.
- Only four capabilities (Technical Assistance, Information Retrieval, Summarization, Generation) are covered by existing benchmarks.
- Reviewing Work and Data Structuring lack dedicated benchmarks despite high usage frequency.
- Google Gemini 2.5 outperforms other models on capability-aligned metrics when evaluated with human-centered criteria.

## Why This Works (Mechanism)

### Mechanism 1: Usage-Driven Taxonomy Generation
- **Claim:** LLM evaluation grounded in empirical usage logs reveals distinct capability gaps compared to theoretical concepts.
- **Evidence:** [abstract], [section 3.1], [corpus: Agent Benchmarks Fail Public Sector Requirements]
- **Break condition:** If user intent differs significantly between logs and professional requirements.

### Mechanism 2: Conversational Criteria Filtering
- **Claim:** Human-centered linguistic criteria reveal that many high-scoring benchmarks fail to predict real-world conversational success.
- **Evidence:** [section 1.3], [section 3.2.1], [corpus: ShoppingComp]
- **Break condition:** If automated evaluation of clarity or efficiency becomes feasible.

### Mechanism 3: Recency and Selection Bias in Leaderboards
- **Claim:** Model rankings on public leaderboards are inflated by recency advantage and selective reporting.
- **Evidence:** [section 4.6], [corpus: SmartBench]
- **Break condition:** If evaluation suites become dynamic or hidden to prevent overfitting.

## Foundational Learning

- **Concept: Grice’s Cooperative Principle**
  - **Why needed:** Derives five evaluation criteria from Grice’s maxims.
  - **Quick check:** Can you explain why MMLU (multiple-choice) might violate the maxim of Manner compared to open-ended generation?

- **Concept: RLHF (Reinforcement Learning from Human Feedback)**
  - **Why needed:** Explains divergence between model optimization (chat) and testing (exams).
  - **Quick check:** Does RLHF optimize for the correct answer or the preferred answer?

- **Concept: O*NET Taxonomy**
  - **Why needed:** Critical for understanding how "real-world" utility is defined.
  - **Quick check:** If an LLM prompt is "debug my code," which O*NET capability does this map to?

## Architecture Onboarding

- **Component map:** Usage Logs + Survey Data -> Thematic Classifier -> Benchmark Router -> Evaluator
- **Critical path:** Mapping 4 million prompts to O*NET tasks and subsequently to six capabilities.
- **Design tradeoffs:** Static vs. Dynamic (scalability vs. validity), Coverage vs. Depth
- **Failure signatures:** Hallucinated Efficiency (high accuracy but low clarity), Benchmark Gaming (high leaderboard rank but fails Reviewing Work)
- **First 3 experiments:**
  1. Take your organization's internal chat logs and classify 100 random prompts into the six capabilities.
  2. Construct a small "Reviewing Work" dataset and test your current model against human baseline.
  3. Run a time-trial study measuring time to verify model-formatted dataset output.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can valid, human-in-the-loop benchmarks be constructed for Reviewing Work and Data Structuring that correlate with expert judgment?
- **Basis:** [explicit] Section 4.7 calls for new benchmarks aligned with six capabilities, noting complete absence for Reviewing Work and Data Structuring.
- **What evidence would resolve it:** Publication of benchmark suite with high inter-annotator agreement ranking models on proofreading or citation formatting.

### Open Question 2
- **Question:** Does dominance of technical assistance tasks stem from higher prompt frequency among developers vs. non-technical users?
- **Basis:** [explicit] Section 4.1 hypothesizes overrepresentation due to developer volume differences.
- **What evidence would resolve it:** Comparative log analysis isolating prompt counts per user cohort.

### Open Question 3
- **Question:** How can efficiency be operationalized as a quantitative metric in standard LLM benchmarks?
- **Basis:** [explicit] Authors identify efficiency as critical missing criterion across nearly all reviewed benchmarks.
- **What evidence would resolve it:** Development of metric quantifying time-to-solution or interaction turns.

## Limitations
- **Unknown 1:** Inter-annotator agreement protocol and coding instructions for task-to-capability mapping are not specified.
- **Unknown 2:** Model inference configurations (temperature, sampling, prompts) for collected benchmark scores are not standardized.
- **Unknown 3:** Full example prompts table referenced but only partial excerpt provided in appendix.

## Confidence
- **High:** Identification of six capabilities from empirical usage data is well-supported.
- **Medium:** Five evaluation criteria based on Grice's maxims is theoretically sound but may not fully capture utility.
- **Low:** Model ranking conclusions depend on self-reported benchmark scores without standardized configurations.

## Next Checks
1. Verify capability mapping by independently classifying a sample of 50 Claude.ai prompts into the six capabilities.
2. Test the five evaluation criteria on a new benchmark not included in the original study to assess generalizability.
3. Compare the six capabilities against internal organizational usage data to identify potential domain-specific gaps.