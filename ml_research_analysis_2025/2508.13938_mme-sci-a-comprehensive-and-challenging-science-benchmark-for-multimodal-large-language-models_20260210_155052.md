---
ver: rpa2
title: 'MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal
  Large Language Models'
arxiv_id: '2508.13938'
source_url: https://arxiv.org/abs/2508.13938
tags:
- arxiv
- mllms
- reasoning
- wang
- mme-sci
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MME-SCI, a comprehensive and challenging
  multimodal scientific benchmark for evaluating large language models. It addresses
  the lack of multilingual support, comprehensive modality coverage, and fine-grained
  knowledge annotation in existing benchmarks.
---

# MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2508.13938
- Source URL: https://arxiv.org/abs/2508.13938
- Authors: Jiacheng Ruan; Dan Jiang; Xian Gao; Ting Liu; Yuzhuo Fu; Yangyang Kang
- Reference count: 9
- Primary result: MME-SCI benchmark shows top models achieving only 19.43-52.11% accuracy in image-only mode, demonstrating significant difficulty compared to existing benchmarks

## Executive Summary
MME-SCI addresses critical gaps in existing multimodal scientific benchmarks by introducing comprehensive multilingual support (5 languages), fine-grained knowledge point annotation (63 concepts), and temporal novelty filtering (83.3% questions from 2025). The benchmark covers 1,019 high-quality question-answer pairs across four scientific subjects with three evaluation modes (text-only, image-only, image-text hybrid). Experiments with 20 models reveal MME-SCI is significantly more challenging than existing benchmarks, with top models achieving accuracy as low as 19.43% in image-only mode. The benchmark's design enables detailed diagnostic analysis of model weaknesses across languages, modalities, and specific knowledge domains.

## Method Summary
MME-SCI evaluates multimodal large language models using 1,019 manually curated QA pairs across math, physics, chemistry, and biology subjects. Questions are sourced from recent 2025 and 2024 mock exams, translated into five languages (Chinese, English, French, Spanish, Japanese), and annotated with 63 fine-grained knowledge points. The benchmark supports three evaluation modes: text-only (D_zh^text), image-only (D_zh^img), and image-text hybrid (D_en, D_fr, D_es, D_ja). Evaluation uses LLM-as-a-Judge paradigm with language-specific templates, temperature=0, and max_tokens=8192. The benchmark measures accuracy per subject and modality, cross-lingual consistency, and fine-grained knowledge point performance.

## Key Results
- Top models achieve only 19.43-52.11% accuracy in image-only mode, significantly lower than existing benchmarks (MDK12-Bench, MAC)
- Cross-lingual consistency analysis shows even top models (Doubao-Seed-1.6) achieve only 13.84% linguistically consistent correct responses
- Fine-grained knowledge point analysis reveals large performance gaps within subjects (e.g., o4-mini: 15.15% on "Magnetic Field" vs. 64.29% on "Trigonometric Functions")
- Open-source models drop 2.79-4.36% in image-only mode compared to text-only, indicating visual processing weaknesses
- Error analysis identifies five failure modes: visual perception, text comprehension, knowledge gaps, calculation errors, and reasoning failures

## Why This Works (Mechanism)

### Mechanism 1: Temporal Novelty Filtering
- Claim: Temporal novelty filtering creates benchmark saturation resistance by using recent questions less likely in training data
- Core assumption: Training data contamination is the primary cause of benchmark saturation in existing evaluations
- Evidence: 83.3% of questions from 2025, 16.2% from 2024, top models achieve only 19.43-52.11% accuracy in image-only mode

### Mechanism 2: Fine-Grained Knowledge Point Annotation
- Claim: Fine-grained knowledge point annotation enables targeted diagnostic feedback by correlating failures with specific concepts
- Core assumption: Knowledge points are mutually exclusive and correctly attributed; poor performance reflects conceptual weakness
- Evidence: 63 knowledge points enable per-concept accuracy analysis, revealing domain-specific vulnerabilities (e.g., o4-mini: 15.15% vs 64.29% across different physics concepts)

### Mechanism 3: Cross-Lingual Consistency Evaluation
- Claim: Cross-lingual consistency evaluation distinguishes reasoning transfer from language-specific pattern matching
- Core assumption: Translation preserves problem structure and difficulty; linguistic variation is the only manipulated variable
- Evidence: 5-language evaluation shows consistency improves with model capability but remains low (top model: 13.84% linguistically consistent correct responses)

## Foundational Learning

- Concept: **Benchmark Saturation**
  - Why needed here: MME-SCI addresses saturation in existing benchmarks (MMMU: 72.2%, AI2D: 85.1%) that no longer distinguish model capabilities
  - Quick check question: If a benchmark shows 85% accuracy across multiple models, what does this indicate about its discriminative power?

- Concept: **Cross-Lingual Transfer in MLLMs**
  - Why needed here: The paper uses multilingual evaluation to test whether models "truly mastered reasoning abilities, rather than relying on specific linguistic contexts"
  - Quick check question: If a model answers correctly in English but incorrectly in Spanish for the same question, what hypothesis does this support?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: The paper uses this paradigm for automated scoring across 1,019 samples
  - Quick check question: What are two failure modes when using an LLM to evaluate another LLM's responses?

## Architecture Onboarding

- Component map: Data Layer (1,019 questions, 4 subjects, 5 languages, 3 modalities) -> Annotation Layer (63 fine-grained knowledge points) -> Evaluation Layer (LLM-as-a-Judge with language templates) -> Analysis Layer (cross-lingual consistency, knowledge-point accuracy, error categorization)

- Critical path: Question selection → OCR/digitization → Screenshot capture for image-only mode → Translation to 4 additional languages → Human post-audit → Model inference → Judge evaluation → Aggregated metrics

- Design tradeoffs: Only single/multiple-choice and fill-in-blank used for automated scoring; translation assumes uniform quality; evaluation depends on judge model capability

- Failure signatures: Low cross-lingual consistency (<15%) suggests language-specific adaptation; high image-only degradation indicates visual processing weakness; knowledge-point variance >50% reveals uneven concept coverage

- First 3 experiments: 1) Run Qwen2.5VL-7B on all 6 language/modality splits to establish weakness map; 2) Manually compare 50-question subset responses across 5 languages to validate consistency metric; 3) Sample 30 incorrect responses from top model to verify automated error distribution claims

## Open Questions the Paper Calls Out
None

## Limitations

- LLM-as-a-Judge reliability depends on unspecified prompt templates and answer extraction logic, introducing scoring consistency uncertainty
- Translation quality assumption lacks quantitative validation, potentially conflating translation artifacts with reasoning transfer capability
- Knowledge point granularity depends on manual annotation without reported inter-annotator agreement or mutual exclusivity validation

## Confidence

- High Confidence: Temporal novelty filtering and observed difficulty are directly measurable; multi-modality design and multilingual coverage are verifiable
- Medium Confidence: Cross-lingual consistency metric and fine-grained knowledge point analysis depend on unverified assumptions about translation quality and annotation accuracy
- Low Confidence: Comparative claims about superiority over existing benchmarks don't account for different evaluation methodologies; temporal novelty as primary saturation cause is plausible but not isolated

## Next Checks

1. **Translation Equivalence Validation**: Have bilingual experts independently verify that translated versions preserve original problem structure, difficulty, and solution paths for a 50-question subset

2. **Judge Reliability Assessment**: Implement inter-judge agreement testing by running same responses through multiple judge configurations and calculate Cohen's kappa or similar agreement metrics

3. **Knowledge Point Annotation Validation**: Conduct blind re-annotation of 100 randomly selected questions by independent annotators and compute inter-annotator agreement to identify problematic knowledge point definitions