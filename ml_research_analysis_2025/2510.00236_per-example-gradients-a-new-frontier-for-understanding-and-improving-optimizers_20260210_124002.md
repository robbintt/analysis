---
ver: rpa2
title: 'Per-example gradients: a new frontier for understanding and improving optimizers'
arxiv_id: '2510.00236'
source_url: https://arxiv.org/abs/2510.00236
tags:
- gradient
- learning
- gradients
- batch
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to efficiently compute per-example
  gradient statistics in deep learning models, which enables new analyses and possibilities
  for algorithm design. The key idea is to perform a computational graph surgery to
  inject gradient transformations before the final averaging step, which incurs almost
  no overhead compared to standard gradient computation in some cases.
---

# Per-example gradients: a new frontier for understanding and improving optimizers

## Quick Facts
- arXiv ID: 2510.00236
- Source URL: https://arxiv.org/abs/2510.00236
- Reference count: 40
- Primary result: Introduces an efficient method to compute per-example gradient statistics via computational graph surgery, enabling new analyses of optimizer behavior and design of variants like MicroAdam and MicroSignSGD.

## Executive Summary
This paper presents a novel method to efficiently compute per-example gradient statistics in deep learning models without materializing the full gradient matrix. The key innovation is performing computational graph surgery to inject gradient transformations before the final averaging step, incurring minimal overhead compared to standard gradient computation. The approach enables studying nonlinear operations in optimization, revealing that for SignSGD the optimal sign placement is after averaging, and for Adam, stability relies on mean-squared terms rather than variance alone.

## Method Summary
The method exploits the rank-one structure of per-example gradients for linear layers to compute statistics efficiently. For dense layers, gradients decompose as outer products that can be factorized before reduction. The authors implement this through two approaches: JAX's vmap transformation for easy prototyping, and a more sophisticated Jaxpr interpreter that performs graph surgery to inject nonlinear operations (like squaring) before the final sum reduction. This enables computing statistics like per-example gradient variances and means without the O(B) memory cost of materializing all gradients.

## Key Results
- Applying the sign function after averaging gradients (SignEMA) outperforms applying it before (MicroSignSGD) due to better signal-to-noise ratio preservation
- Adam's stability depends on mean-squared gradient terms (ÂµÂ²) dominating the preconditioner, contrary to conventional wisdom about variance normalization
- MicroAdam variants show universal batch scaling (Î·âˆB) versus standard Adam's square root scaling (Î·âˆâˆšB)
- For transformers, activation memory dominates parameter memory, making per-example gradient computation feasible

## Why This Works (Mechanism)

### Mechanism 1: Factorable Gradient Aggregation via Graph Surgery
Standard reverse-mode AD computes parameter gradients as sums of per-example outer products. By injecting a function Ï† before the final sum reduction, one can compute statistics like âˆ‘Ï†(sáµ¢ráµ¢áµ€). For factorable operations like squaring, this becomes âˆ‘(sáµ¢)Â²(ráµ¢)Â², preserving computational complexity rather than scaling linearly with batch size.

### Mechanism 2: Signal-to-Noise Ratio (SNR) Preservation in SignSGD
The sign function reduces SNR for low-SNR distributions (typical for stochastic gradients) but increases it for high-SNR distributions. Averaging reduces variance first, then signing maximizes SNR. The chain "Average â†’ Sign" preserves more information than "Sign â†’ Average" which destroys magnitude information early.

### Mechanism 3: Mean-Dominated Preconditioning in Adam
Standard Adam uses Î½â‚ð’¹â‚â‚˜ â‰ˆ ÂµÂ² + ÏƒÂ²/B. Variants isolating variance (MicroAdamVar) remove the ÂµÂ² term, causing instability. The mean-squared term acts as a stabilizing "mean" prior, contradicting conventional wisdom that Adam primarily acts as variance normalization.

## Foundational Learning

- **Reverse-Mode Automatic Differentiation & Vector-Jacobian Products**: Understanding that gradients are computed via adjoints and that the "sum reduction" is the final step of the VJP for shared weights. Quick check: In a standard dense layer y = Wx, why does the gradient âˆ‡_W L involve a sum over the batch dimension?

- **Tensor Contractions & Rank-1 Decomposition**: The efficiency of graph surgery depends on exploiting the structure of linear layers. Understanding that a gradient for a single example is a rank-1 matrix (sráµ€) is essential to see why squaring it is cheap (SÂ²RÂ²). Quick check: If a gradient for a batch of size B forms a matrix G, is G typically full rank or rank-deficient relative to B?

- **Estimators of Moments (Mean vs. Variance)**: Distinguishing between the square of the mean (ÂµÂ²) and the mean of the squares (E[XÂ²]). The relationship E[XÂ²] = ÂµÂ² + ÏƒÂ² is used to dissect optimizer behavior. Quick check: How does increasing batch size B affect the gap between Î½â‚ð’¹â‚â‚˜ = (1/Bâˆ‘gáµ¢)Â² and Î½â‚˜áµ¢ð’¸áµ£â‚’ = 1/Bâˆ‘gáµ¢Â²?

## Architecture Onboarding

- **Component map**: JAX vmap (frontend) -> Jaxpr interpreter (core engine) -> Modified Adam/SignSGD implementations (optimizers)

- **Critical path**: 1) Trace: Generate the Jaxpr of the gradient computation. 2) Detect: Identify reduction operations (e.g., dot_general with batch contraction). 3) Inject: Replace standard reduction with a "reinterpreter" that applies Ï† (e.g., square) to inputs before contraction.

- **Design tradeoffs**: vmap adds ~17% compute overhead but is easy to implement; graph surgery is complex but offers near-zero overhead and better scaling with batch size. For transformers (L â‰¥ D), activation memory dominates, making per-example gradient computation feasible; for MLPs (L < D), this relationship inverts.

- **Failure signatures**: Instability in MicroAdam causes loss spikes; fix with gradient clipping threshold 10â»Â². Negative preconditioners in MicroAdamMSQ from negative ÂµÂ² estimates; fix with ReLU(Î½) + Îµ filtering. Non-universal scaling curves at large batch sizes Bâ‰¥128; this is expected and breaks universal behavior.

- **First 3 experiments**: 1) Memory Baseline: Profile peak memory usage of standard Adam vs. vmap-based MicroAdam on 151M param Transformer to verify activation memory dominance. 2) SignSGD Ablation: Compare SignEMA vs. MicroSignSGD to validate SNR hypothesis on small MLP. 3) Batch Scaling: Train Adam vs. MicroAdam across batch sizes {8,16,32,64,128,256} to observe divergence in universal scaling behavior.

## Open Questions the Paper Calls Out

- What is the causal explanation for the persistence of the square root learning rate scaling rule in Adam, given that the ÂµÂ² term dominates the preconditioner rather than the variance? The authors observe the phenomenon empirically but the theoretical mechanism remains undefined.

- Can mitigation strategies, such as averaging Î½â‚˜áµ¢ð’¸áµ£â‚’ and Î½â‚ð’¹â‚â‚˜ across blocks of parameters, stabilize the ÂµÂ²-dominated MicroAdamMSQ optimizer? Current implementation destabilizes easily, and only ReLU filtering was tested.

- How can the efficient computational graph surgery approach be adapted for operations where per-sample gradients are not rank-one matrices or are not "factorable"? Appendix B.5 highlights that for dense layers on sequences, "per-sample gradients are not rank-one vectors," suggesting architectural limits.

## Limitations
- The graph surgery mechanism requires verification on architectures with complex parameter sharing beyond standard dense layers
- The SNR-based argument for SignSGD ordering assumes near-Gaussian gradient distributions that may not hold in all training regimes
- The theoretical bounds on ÂµÂ²/ÏƒÂ² ratios could be tighter to better predict when variance-dominated methods might fail

## Confidence
- **High**: The computational graph surgery approach is technically sound and validated through JAX implementation examples
- **Medium**: The SNR-based explanation for SignSGD ordering is supported by empirical evidence but relies on distributional assumptions
- **Medium**: The Adam preconditioner analysis is empirically validated but the theoretical bounds on ÂµÂ²/ÏƒÂ² ratios could be tighter

## Next Checks
1. Test the graph surgery mechanism on architectures with complex parameter sharing (e.g., depthwise separable convolutions) to verify the rank-1 factorization assumption
2. Analyze gradient distributions at different training stages to quantify the SNR behavior that motivates the SignSGD ordering
3. Systematically vary batch sizes beyond the studied range to identify when variance-dominated Adam variants might become viable