---
ver: rpa2
title: 'From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social
  Bias in Video Diffusion Models'
arxiv_id: '2510.17247'
source_url: https://arxiv.org/abs/2510.17247
tags:
- bias
- reward
- gender
- ethnicity
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoBiasEval, a comprehensive framework
  for evaluating social biases in video diffusion models. The framework employs event-based
  prompting to systematically vary actor attributes (gender and ethnicity) while controlling
  for semantic content, and introduces multi-granular metrics to assess ethnic representation,
  gender bias conditioned on ethnicity, and temporal stability of social attributes.
---

# From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models

## Quick Facts
- arXiv ID: 2510.17247
- Source URL: https://arxiv.org/abs/2510.17247
- Authors: Zefan Cai; Haoyi Qiu; Haozhe Zhao; Ke Wan; Jiachen Li; Jiuxiang Gu; Wen Xiao; Nanyun Peng; Junjie Hu
- Reference count: 40
- Key outcome: Introduces VideoBiasEval framework and shows alignment tuning amplifies and stabilizes social biases in video diffusion models

## Executive Summary
This paper presents the first end-to-end analysis of social bias propagation in video diffusion models, tracing biases from human preference datasets through reward models to alignment-tuned video outputs. The authors introduce VideoBiasEval, a comprehensive framework for evaluating gender and ethnic biases using event-based prompting and multi-granular metrics. Their findings reveal that alignment tuning not only amplifies existing representational biases but also makes them temporally stable, producing smoother yet more stereotyped portrayals. Additionally, they demonstrate that controllable preference modeling can effectively steer video generation toward more equitable representations by adjusting the composition of training data.

## Method Summary
The authors construct VideoBiasEval using event-based prompts (Actor + Action + Context) with 42 actions, 4 gender categories, and 7 ethnic groups. They generate videos using base models like VideoCrafter-2 and VCM-VC2, then extract social attributes from 16 frames per video using an ensemble of three VLMs (Qwen2-VL-7B, Qwen2.5-VL-7B, InternVL2.5-8B). Videos are classified via majority voting and analyzed using metrics including Proportion Bias Score for Gender (PBSG), Representation Deviation Score for ethnicity (RDSe), Simpson's Diversity Index (SDI), and Temporal Attribute Stability (TAS). For alignment tuning, they fine-tune VCM-VC2 using reward feedback from various reward models over 200 steps with Latent Consistency Distillation.

## Key Results
- Alignment tuning amplifies existing biases from human preference datasets through reward models to video outputs
- Temporal stability increases from 40-80% in base models to 99.67% in aligned models, entrenching biased representations
- Controllable preference modeling can effectively reduce biases by constructing woman-preferred datasets that shift model outputs
- Ethnicity representation improves slightly after alignment, but gender bias becomes more pronounced and stable

## Why This Works (Mechanism)

### Mechanism 1: Reward-Mediated Bias Amplification
If a reward model is trained on human preference datasets containing demographic skews, alignment tuning using that reward model will likely amplify those skews in the video output. The alignment process optimizes the video diffusion model to maximize a scalar reward signal. If the reward model assigns higher scores to specific demographics (e.g., "White" or "Man") due to training data imbalances, the video model learns to prioritize generating these high-reward traits to minimize loss. This mechanism breaks if the reward model is debiased or if the alignment process includes regularization terms that penalize demographic homogenization.

### Mechanism 2: Temporal Entrenchment via Stability Optimization
Alignment tuning improves temporal consistency (reducing "flickering"), which simultaneously makes biased representations more persistent throughout a video clip. Alignment methods (like Latent Consistency Distillation) smooth frame-to-frame transitions to improve visual fidelity. When the model is biased toward a specific demographic for a given action, this smoothing mechanism ensures that the demographic label remains constant (stable) across frames, locking in the bias. This mechanism breaks if the definition of "quality" includes diversity or if the generator is forced to produce inconsistent attributes.

### Mechanism 3: Counter-Biased Steering via Data Composition
Constructing reward datasets with intentional demographic skews (e.g., "woman-preferred") can steer a video model toward more equitable representations. By reversing the preference label in the training data (labeling the underrepresented group as "preferred"), the reward model learns to assign higher scores to the minority group. The video model, optimizing for this reward, shifts its output distribution to match this new preference signal. This mechanism fails if the counter-bias is too extreme, potentially causing mode collapse or degradation in semantic alignment.

## Foundational Learning

- **Concept: Reward-Weighted Regression / RLHF (Reinforcement Learning from Human Feedback)**
  - Why needed here: The paper relies on the premise that video models are tuned to maximize a "reward." Without understanding that the model is fine-tuned to seek high scores from a judge, the amplification of bias is unexplainable.
  - Quick check question: Does the video model learn to generate bias because it wants to be "prejudiced," or because the "reward model" gave high scores to prejudiced outputs? (Answer: The latter)

- **Concept: Event-Centric Evaluation**
  - Why needed here: Standard video metrics (like FVD) measure distributional similarity but fail to detect social bias. Understanding that this paper uses structured prompts (Actor + Action + Context) is necessary to replicate the evaluation.
  - Quick check question: Why does the prompt "A person is baking" reveal more bias than "A man is baking"? (Answer: The former reveals the model's *default* prior; the latter tests instruction following)

- **Concept: Simpsonâ€™s Diversity Index (SDI) & Proportion Bias Score (PBSG)**
  - Why needed here: These are the specific mathematical tools used to quantify "fairness." PBSG measures the male/female balance; SDI measures the spread across ethnicities.
  - Quick check question: If a model generates 100 videos of "White" people and 100 videos of "Asian" people, is the SDI high or low? (Answer: High, because diversity is balanced)

## Architecture Onboarding

- **Component map:** Event Prompts -> Video Generation (VCM-VC2) -> Frame Sampling (16 frames) -> VLM Classification (Qwen2-VL-7B, Qwen2.5-VL-7B, InternVL2.5-8B) -> Majority Voting -> Metric Calculation (PBSG, RDSe, SDI, TAS)

- **Critical path:**
  1. Prompting: Generate prompts with varying demographic conditions
  2. Generation: Produce video frames
  3. Classification: Use VLMs to label gender/ethnicity per frame
  4. Aggregation: Compute video-level labels via majority voting
  5. Quantification: Calculate PBSG (Gender Bias) and RDS/SDI (Ethnicity Bias)

- **Design tradeoffs:**
  - Stability vs. Fairness: High temporal consistency (TAS) is desirable for visual quality but risks "locking in" biased representations
  - VLM vs. Human Eval: VLMs are scalable but may carry their own biases; Human eval is the ground truth but expensive

- **Failure signatures:**
  - Flickering: Low TAS scores indicate the model struggles to maintain identity, often seen in unaligned base models
  - Entrenched Bias: High TAS combined with extreme PBSG scores (>0.9) indicates alignment has successfully created a high-quality but biased model
  - Reward Hacking: If the video model generates nonsense that maximizes the reward score

- **First 3 experiments:**
  1. Baseline Audit: Run VideoBiasEval on a base model (e.g., VideoCrafter-2) to establish PBSG/RDS baseline
  2. Reward Probing: Evaluate an existing reward model (e.g., HPSv2.0) using the protocol to confirm it exhibits demographic preference
  3. Steering Test: Construct a small-scale "counter-biased" preference dataset and align the model to observe the shift in PBSG scores

## Open Questions the Paper Calls Out

- How do intersectional identity representations (e.g., combinations of gender, ethnicity, age, disability) interact with alignment tuning in video diffusion models?
- Do alternative alignment training protocols (e.g., RL-based tuning, multi-turn video instruction alignment) exhibit different bias amplification dynamics compared to single-step latent consistency distillation?
- How effectively does controllable preference modeling generalize beyond synthetic gender manipulations to naturally occurring, demographically diverse preference data?
- Do VLM-based evaluators systematically underperform or exhibit distinct bias patterns when classifying social attributes in stylized, low-light, or motion-blurred video frames?

## Limitations

- The analysis critically depends on VLM classifiers for attribute extraction, introducing potential bias propagation from the classifiers themselves
- The study focuses primarily on gender and ethnicity dimensions, potentially missing other important social attributes
- Temporal stability metrics may conflate genuine identity consistency with stereotypical representation persistence

## Confidence

- **High Confidence:** The amplification of biases through reward-mediated alignment is well-supported by direct comparisons between base and aligned models
- **Medium Confidence:** The mechanism of reward hacking through demographic preference assignment is plausible but relies on indirect evidence
- **Low Confidence:** The generalizability of findings to other social attributes beyond gender/ethnicity remains untested

## Next Checks

1. **Classifier Validation:** Conduct human evaluation studies comparing VLM classifications against ground truth annotations across diverse visual styles and lighting conditions to quantify classification bias
2. **Multi-Attribute Analysis:** Extend the evaluation framework to include intersectional attributes and other social dimensions like age or disability representation
3. **Regularization Experiments:** Test whether adding diversity regularization terms during alignment tuning can maintain temporal consistency while reducing demographic bias amplification