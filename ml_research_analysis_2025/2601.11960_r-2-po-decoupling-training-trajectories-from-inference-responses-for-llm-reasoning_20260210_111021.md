---
ver: rpa2
title: 'R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM
  Reasoning'
arxiv_id: '2601.11960'
source_url: https://arxiv.org/abs/2601.11960
tags:
- training
- policy
- reward
- r2po
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning

## Quick Facts
- arXiv ID: 2601.11960
- Source URL: https://arxiv.org/abs/2601.11960
- Authors: Jingchu Wang; Bingbing Xu; Yige Yuan; Bin Xie; Xiaoqian Sun; Huawei Shen
- Reference count: 18
- Primary result: Improves reasoning performance by 5.5% on GSM8K and 6.7% on MATH over GRPO while maintaining stable inference generation

## Executive Summary
R$^2$PO addresses the instability of reasoning-focused LLM fine-tuning by architecturally decoupling training trajectory generation from inference responses. It introduces a lightweight residual Rollout-Head that generates diverse exploration trajectories while the main policy remains shielded from destructive gradient noise. This enables controlled exploration during training without compromising the stability of the final inference policy.

## Method Summary
R$^2$PO modifies GRPO by adding a zero-initialized two-layer MLP Rollout-Head that produces a residual logit offset atop the transformer backbone. Training alternates between two stages: Stage 1 freezes the backbone and optimizes only the Rollout-Head using Group Inverse-Frequency rewards to encourage diverse trajectories; Stage 2 freezes the Rollout-Head and updates the backbone using standard accuracy/format rewards while sampling from the frozen exploration policy. At inference, only the base policy is used, discarding the Rollout-Head.

## Key Results
- Improves GSM8K Pass@1 accuracy by 5.5% over GRPO baseline
- Achieves 6.7% gain on MATH-500 out-of-distribution test
- Maintains 0% format error rate under perturbations versus 87.5% for baseline

## Why This Works (Mechanism)

### Mechanism 1: Structural Decoupling of Exploration and Exploitation
Separating training trajectory generation from inference responses prevents gradient interference, allowing exploration of diverse reasoning paths without destabilizing the primary policy. A residual Rollout-Head creates an exploration policy that offsets the base policy logits, with alternating optimization stages isolating high-variance gradients.

### Mechanism 2: Gradient Noise Shielding via Iterative Freezing
The two-stage training schedule shields the primary policy from destructive gradient noise by freezing the backbone during exploration. This prevents overfitting to spurious correlations and format exploits, demonstrated by maintaining 0% error rate under perturbations versus 87.5% for baseline.

### Mechanism 3: Group Inverse-Frequency (GIF) Rewarding
Prioritizing statistically rare trajectories within batches combats mode collapse inherent in standard correctness-based RL. GIF rewards assign higher values to less frequent responses, forcing exploration toward low-probability tokens and uncovering reasoning paths that accuracy-maximizing policies would suppress.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: R2PO builds on GRPO's group-based advantage estimation without a separate value model. Understanding GRPO's advantage calculation is essential since R2PO keeps this objective while changing the sampler.
  - Quick check: How does GRPO calculate advantage $\hat{A}_{i,t}$ for a token, and why does R2PO keep this objective while changing the sampler?

- **Exploration vs. Exploitation in LLMs**: The paper addresses the objective conflict between these phases, where single-head policies tend toward self-repetition when optimized purely for verifiable rewards.
  - Quick check: Why does a single-head policy tend toward "self-repetition" when optimized purely for verifiable rewards?

- **Residual Connections in Neural Networks**: The Rollout-Head is implemented as a residual logit offset ($f_{RO}(H) + f_{LM}(H)$). Understanding residual learning explains how the model maintains stable inference while adding controlled noise.
  - Quick check: What is the effect of zero-initialization on the exploration policy $\pi_\phi$ at the very start of training?

## Architecture Onboarding

- **Component map:** Input tokens → Transformer backbone → LM-Head + Rollout-Head → Logits → $\pi_\theta$ (inference) and $\pi_\phi$ (training)
- **Critical path:** 1) Initialize zero Rollout-Head, 2) Stage 1: Sample from $\pi_\phi$, update Rollout-Head with GIF reward, 3) Stage 2: Sample from frozen $\pi_\phi$, update backbone/LM-Head with main reward, 4) Inference: Use only $\pi_\theta$
- **Design tradeoffs:** Adds 7.8%-10.2% parameters (discarded at inference), GIF reward ablation shows Main Reward performs competitively suggesting decoupling is primary driver, Stage 1 reduces memory by ~55% but Stage 2 returns to baseline
- **Failure signatures:** Length explosion if base policy isn't shielded, format collapse without proper reward specification, insufficient exploration if GIF reward scaling is unstable
- **First 3 experiments:** 1) Verify zero-init produces identical distributions at step 0, 2) Replicate finding that Main Reward works as well as GIF on GSM8K subset, 3) Test perturbation robustness by injecting formatting errors after 2000 steps

## Open Questions the Paper Calls Out

- **Open Question 1:** Does Rollout-Head capacity limit effectiveness on larger models (70B+)? The authors note uncertainty about whether a small residual head can meaningfully perturb complex solution spaces of larger models.
- **Open Question 2:** Can the framework adapt to multi-turn dialogue or long-horizon agent tasks? The current architecture is designed for single-turn reasoning, with unknown interactions in evolving context states.
- **Open Question 3:** Does parameter overhead negate benefits on smaller-scale models? The 10% parameter increase may constitute a non-negligible capacity tax for models under 1B parameters.

## Limitations

- Uncertainty about Rollout-Head effectiveness on larger models due to limited parameter capacity
- Focus on single-turn reasoning with unknown applicability to multi-turn or long-horizon tasks
- Potential parameter overhead concerns for smaller models where capacity is constrained

## Confidence

- **Method validity:** High - The decoupled architecture directly addresses identified GRPO instability issues with clear mechanism
- **Empirical results:** Medium - Strong GSM8K and MATH results but limited ablation on exploration reward necessity
- **Generalizability:** Low - Only validated on 3B and 8B models, single-turn reasoning tasks, with open questions about scaling

## Next Checks

1. Replicate the finding that Main Reward performs competitively with GIF Reward on a smaller dataset to verify exploration reward necessity
2. Conduct perturbation test by injecting formatting errors into training data after 2000 steps to validate shielding hypothesis
3. Verify zero-initialization produces identical output distributions between $\pi_\theta$ and $\pi_\phi$ at training start