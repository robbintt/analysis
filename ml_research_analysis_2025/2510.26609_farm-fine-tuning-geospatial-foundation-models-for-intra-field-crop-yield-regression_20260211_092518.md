---
ver: rpa2
title: 'FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield
  Regression'
arxiv_id: '2510.26609'
source_url: https://arxiv.org/abs/2510.26609
tags:
- yield
- data
- crop
- farm
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FARM, a fine-tuning framework for geospatial\
  \ foundation models to perform high-resolution, intra-field canola yield regression.\
  \ It adapts the Prithvi-EO-2.0-600M foundation model for continuous, pixel-level\
  \ yield prediction from multi-temporal Sentinel-2 imagery, achieving an R\xB2 of\
  \ 0.81 and RMSE of 0.44 on validation data."
---

# FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression

## Quick Facts
- **arXiv ID**: 2510.26609
- **Source URL**: https://arxiv.org/abs/2510.26609
- **Reference count**: 34
- **Primary result**: R² = 0.81 and RMSE = 0.44 on validation data for intra-field canola yield regression

## Executive Summary
This paper introduces FARM, a fine-tuning framework for geospatial foundation models to perform high-resolution, intra-field canola yield regression. It adapts the Prithvi-EO-2.0-600M foundation model for continuous, pixel-level yield prediction from multi-temporal Sentinel-2 imagery, achieving an R² of 0.81 and RMSE of 0.44 on validation data. FARM outperforms traditional 3D-CNN and DeepYield baselines, demonstrating the advantage of leveraging pre-trained Earth observation knowledge. Further experiments confirm that fine-tuning FARM on limited high-resolution ground-truth labels yields better performance than training from scratch, highlighting the value of transfer learning from large, up-sampled datasets. Interpretability analysis reveals the model focuses on mid-season months and spectral bands (NIR, SWIR) aligned with crop physiology, supporting trust and transparency. This approach enables precise, actionable yield maps for precision agriculture.

## Method Summary
FARM adapts the Prithvi-EO-2.0-600M Vision Transformer encoder, pre-trained via masked autoencoding on global satellite archives, for pixel-level canola yield regression. The model processes 5 monthly Sentinel-2 composites (May-September) at 224×224 resolution, with 6 spectral bands (Blue, Green, Red, NIR Narrow, SWIR 1, SWIR 2). A UperNet decoder with feature pyramid network extracts multi-scale features from transformer layers 8, 16, 24, and 32, which are fused and passed through a 3-layer convolutional regression head to produce continuous yield predictions per pixel. The model is trained using MSE loss with an auxiliary deep supervision branch (weight 0.2), AdamW optimizer, cosine learning rate schedule, and mixed-precision training. Fine-tuning on limited high-resolution ground-truth labels outperforms training equivalent architectures from scratch, confirming the benefit of pre-trained Earth observation representations.

## Key Results
- FARM achieves R² = 0.81 and RMSE = 0.44 on validation data, outperforming 3D-CNN (R² = 0.62) and DeepYield baselines
- Fine-tuning FARM on limited ground-truth labels yields better performance (R² = 0.768) than training the same architecture from scratch (R² = 0.675)
- Attention analysis reveals July as the most influential time step, aligning with canola flowering and pod-filling stages
- The model focuses on NIR and SWIR spectral bands, consistent with vegetation indices used in agronomy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning a geospatial foundation model outperforms training equivalent architectures from scratch for specialized agricultural tasks.
- **Mechanism**: The Prithvi-EO-2.0-600M encoder was pre-trained via masked autoencoding on global satellite archives, embedding generalizable representations of vegetation dynamics, phenology, and land surface patterns. Fine-tuning transfers this Earth-system knowledge to the target domain rather than learning from limited task-specific data.
- **Core assumption**: The pre-trained representations capture transferable spectral-temporal patterns that apply to canola yield prediction in the Canadian Prairies.
- **Evidence anchors**:
  - [abstract] "fine-tuning FARM on limited ground-truth labels outperforms training the same architecture from scratch, confirming the benefit of pre-training"
  - [Section 4.4, Table 4] Experiment 2 (fine-tuned): R² = 0.768 vs. Experiment 3 (from scratch): R² = 0.675 on high-resolution ground truth
  - [corpus] Neighbor paper "Harvesting AlphaEarth" benchmarks GFMs for agricultural tasks, supporting the broader validity of this transfer approach
- **Break condition**: If the target domain has spectral-temporal patterns fundamentally dissimilar from pre-training data (e.g., different sensor modalities, extreme climates not represented), transfer benefits may diminish or reverse.

### Mechanism 2
- **Claim**: Vision Transformer self-attention captures long-range spatio-temporal dependencies critical for yield prediction that CNNs' local receptive fields may miss.
- **Mechanism**: The ViT encoder processes patch tokens through multi-head self-attention, allowing each spatial-temporal location to weigh relevance across all other locations. This enables modeling of field-wide contextual factors rather than localized patterns alone.
- **Core assumption**: Yield-relevant patterns require global context (e.g., field drainage patterns, management zone boundaries) not fully captured by local convolutions.
- **Evidence anchors**:
  - [Section 1, Page 2] "ViTs use self-attention to capture long-range dependencies within imagery, addressing CNNs' tendency to focus on local patterns"
  - [Section 4.3, Table 3] FARM (R² = 0.81) substantially outperforms 3D-CNN baseline (R² = 0.62)
  - [corpus] Weak direct evidence; neighbor papers do not isolate attention vs. convolution mechanisms experimentally
- **Break condition**: If intra-field yield variability is predominantly driven by hyper-local factors (sub-pixel) without field-scale structure, CNNs may match or exceed ViT performance with lower compute.

### Mechanism 3
- **Claim**: The model learns agronomically meaningful temporal attention patterns, prioritizing mid-season months corresponding to flowering and pod-filling stages.
- **Mechanism**: Attention matrices from transformer layers show July receiving highest aggregate attention scores. Deeper layers (Layer 16) exhibit long-range temporal dependencies, while earlier layers (Layer 8) show localized attention.
- **Core assumption**: Canola yield is disproportionately determined by plant status during flowering/pod-filling (July-August), which the model should discover from spectral-temporal data alone.
- **Evidence anchors**:
  - [Section 4.5, Page 18] "July emerging as the most influential time step... aligns perfectly with established crop physiology; for canola, the flowering and early pod-filling stages occurring in July are paramount"
  - [Section 4.5, Figure 9] Quantitative attention heatmaps showing July's dominance in receiving attention
  - [corpus] No corpus validation; interpretability claim is internal to this study
- **Break condition**: If phenological timing shifts substantially (e.g., different climate zones, planting dates), the learned temporal attention may not generalize without retraining.

## Foundational Learning

- **Vision Transformers (ViT) and Self-Attention**
  - **Why needed here**: FARM's encoder is a ViT; understanding patch embedding, positional encoding, and multi-head self-attention is prerequisite to modifying the architecture or debugging attention patterns.
  - **Quick check question**: Can you explain how a 14×14 patch becomes a token and how self-attention allows tokens to exchange information across the image?

- **Transfer Learning and Fine-Tuning Strategies**
  - **Why needed here**: The core contribution is fine-tuning a pre-trained foundation model. Understanding layer freezing, learning rate scaling, and catastrophic forgetting risks is essential.
  - **Quick check question**: What happens if you use a learning rate 100× larger than the paper's 5×10⁻⁶ during fine-tuning?

- **Encoder-Decoder Architectures for Dense Prediction**
  - **Why needed here**: FARM uses a ViT encoder + UperNet decoder + convolutional regression head. Understanding feature pyramid networks and multi-scale feature fusion is required to modify the decoder.
  - **Quick check question**: Why does the decoder extract features from layers 8, 16, 24, and 32 rather than just the final layer?

## Architecture Onboarding

- **Component map**: Input normalization → Patch embedding → Transformer encoding → Multi-scale feature extraction → FPN fusion → PSP context aggregation → Regression head → Per-pixel yield prediction

- **Critical path**: Input normalization (channel-wise mean/std) → Patch embedding → Transformer encoding → Multi-scale feature extraction → FPN fusion → PSP context aggregation → Regression head → Per-pixel yield prediction

- **Design tradeoffs**:
  - Monthly compositing vs. finer temporal resolution: Chosen to mitigate cloud cover; may miss sub-monthly phenological transitions
  - Unfrozen encoder (full fine-tuning) vs. frozen backbone: Higher capacity but requires more GPU memory and careful LR tuning
  - MSE loss vs. Huber loss: Huber slightly more robust to outliers (Table 2); MSE+Aux best overall

- **Failure signatures**:
  - Systematic over/under-prediction: Check normalization mismatch between training and inference data
  - Blurry yield maps with lost spatial detail: Decoder may not be receiving adequate multi-scale features; verify layer extraction indices
  - Poor generalization to new regions: Pre-training may lack representative climates; consider domain adaptation or additional fine-tuning data

- **First 3 experiments**:
  1. **Baseline reproduction**: Train 3D-CNN and DeepYield baselines on the same dataset with identical preprocessing to verify Table 3 performance gaps
  2. **Ablation on encoder freezing**: Compare (a) fully frozen encoder, (b) partial unfreezing (last N layers), (c) full fine-tuning to quantify transfer contribution
  3. **Temporal resolution sensitivity**: Test whether adding semi-monthly composites (if cloud-free data permits) improves R² or shifts attention patterns from monthly aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the FARM framework effectively transfer its learned representations to crops with distinct phenological cycles, such as wheat, using multi-crop fine-tuning strategies?
- **Basis in paper**: [explicit] The Discussion section states, "future work will focus on extending the model through multi-crop fine-tuning strategies. Specifically, we aim to adapt the architecture for wheat yield prediction."
- **Why unresolved**: The current study validates the framework exclusively on canola within the Canadian Prairies; distinct crop types exhibit different spectral and temporal growth patterns.
- **Evidence**: Comparative performance metrics (RMSE, $R^2$) of a FARM model fine-tuned on wheat datasets versus baselines trained from scratch.

### Open Question 2
- **Question**: To what extent does integrating meteorological and soil data improve the predictive accuracy of the FARM model beyond spectral-temporal satellite imagery alone?
- **Basis in paper**: [explicit] The Discussion notes "significant opportunity to enhance its predictive power by integrating meteorological and soil data, thereby providing a more holistic view."
- **Why unresolved**: The current architecture relies solely on Sentinel-2 spectral bands and temporal embeddings, potentially missing critical environmental drivers like precipitation or soil moisture.
- **Evidence**: Ablation studies showing error reduction (RMSE/MAE) when weather/soil covariates are included as input channels.

### Open Question 3
- **Question**: How can uncertainty quantification be effectively incorporated into the FARM architecture to provide confidence intervals for automated decision-making?
- **Basis in paper**: [explicit] The authors state that for commercial deployment, "future iterations must incorporate uncertainty quantification, providing users with a confidence interval."
- **Why unresolved**: The current model outputs deterministic point estimates (yield values) without indicating prediction confidence or variance.
- **Evidence**: Implementation of Bayesian deep learning methods or ensemble approaches that generate calibrated prediction intervals alongside yield maps.

### Open Question 4
- **Question**: Does the use of monthly composite images mask the detection of short-term stress events (e.g., flash droughts or heatwaves) that are critical for final yield?
- **Basis in paper**: [inferred] The methodology utilizes monthly compositing to mitigate cloud cover, which inherently smooths temporal variability and may dilute high-frequency signals.
- **Why unresolved**: Yield-limiting events often occur over days; monthly averages might average out these critical spectral responses, limiting the model's sensitivity to acute stress.
- **Evidence**: Experiments comparing model performance using monthly composites versus higher-frequency, cloud-filtered time-steps or event-based sampling.

## Limitations
- **Data Access**: The exact Sentinel-2 HLS image chips and county-level yield datasets are not publicly linked, limiting exact reproduction
- **Generalizability**: The study focuses solely on canola in the Canadian Prairies; performance on other crops or regions remains untested
- **Interpretability Depth**: Attention-based interpretability lacks ablation studies isolating individual transformer layers or band importance

## Confidence
- **High Confidence**: Fine-tuning outperforms training from scratch on limited ground-truth labels (supported by controlled ablation: R² 0.768 vs. 0.675)
- **Medium Confidence**: ViT + UperNet architecture substantially outperforms 3D-CNN and DeepYield baselines (R² 0.81 vs. 0.62), though no ablation isolates ViT vs. UperNet contributions
- **Medium Confidence**: Attention patterns align with known canola physiology (July dominance), but lacks external validation or crop-specific literature comparison

## Next Checks
1. **Baseline Ablation**: Re-implement and train the 3D-CNN and DeepYield baselines on the same dataset with identical preprocessing to verify the claimed R² gap (0.81 vs. 0.62) and isolate architecture contributions
2. **Encoder Freezing Sensitivity**: Compare (a) fully frozen encoder, (b) partial unfreezing (last N layers), and (c) full fine-tuning to quantify how much transfer learning contributes versus model capacity
3. **Cross-Crop Generalization**: Test FARM on at least one other crop (e.g., wheat) in the same region to evaluate whether attention patterns and yield prediction performance transfer without retraining