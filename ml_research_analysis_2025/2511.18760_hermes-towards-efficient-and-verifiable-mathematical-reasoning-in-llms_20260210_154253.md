---
ver: rpa2
title: 'HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs'
arxiv_id: '2511.18760'
source_url: https://arxiv.org/abs/2511.18760
tags:
- reasoning
- prover
- proof
- lean
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hermes, the first tool-assisted agent that
  explicitly interleaves informal mathematical reasoning with formally verified proof
  steps in Lean4. The framework uses intermediate formal checking to prevent reasoning
  drift and a memory module that maintains proof continuity across long, multi-step
  reasoning chains, enabling both exploration and verification within a single workflow.
---

# HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2511.18760
- **Source URL**: https://arxiv.org/abs/2511.18760
- **Reference count**: 40
- **Primary result**: Achieves up to 67% accuracy improvement on AIME'25 while using 80% fewer total inference FLOPs

## Executive Summary
HERMES is the first tool-assisted agent that explicitly interleaves informal mathematical reasoning with formally verified proof steps in Lean4. The framework uses intermediate formal checking to prevent reasoning drift and a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. Experiments on four challenging mathematical reasoning benchmarks with LLMs of varying parameter scales show that Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches.

## Method Summary
HERMES operates as a four-module inference agent: a reasoning LLM generates informal steps, an autoformalizer translates them to Lean4 with backtranslation verification, a prover attempts formal proof/disproof in parallel, and a memory block maintains proof continuity. The system interleaves natural language reasoning with formal verification, using Lean4's compiler as an oracle to provide definitive CORRECT/INCORRECT/VERIFICATION FAILURE signals. This replaces reward-based sampling with verifiable feedback, achieving higher accuracy with significantly lower computational cost.

## Key Results
- Hermes achieves up to 67% accuracy improvement on AIME'25 benchmark
- Uses 80% fewer total inference FLOPs compared to reward-based Best-of-N approaches
- On MATH500, Hermes achieves 97.4% accuracy with DeepSeek-V3.1 compared to 89.3% for the base model

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Formal Verification
The Hermes agent uses a tool-calling workflow where the LLM verifies "every critical mathematical step contributing to the final proof." The feedback module returns definitive "CORRECT" or "INCORRECT" signals from the Lean4 compiler, forcing the LLM to revise reasoning upon detecting hallucinations rather than continuing down flawed paths. This prevents accumulation of logical errors in multi-step reasoning chains.

### Mechanism 2: Memory Module for Proof Continuity
The memory block stores validated proof steps in a structured database with vector representations. For each new step, it retrieves the top-k most relevant memories and incorporates them as "preliminary hypotheses" into the Lean statement for the prover, preventing the model from contradicting earlier logic. This maintains logical consistency across long reasoning chains.

### Mechanism 3: Verifiable Feedback vs. Reward Models
Instead of generating N candidate reasoning traces and using a separate model to score them, Hermes uses the Lean4 compiler to generate a definitive boolean signal. This single verified trace replaces the need for sampling and scoring multiple traces, reducing total inference FLOPs by up to 80%.

## Foundational Learning

- **Lean4 Proof Assistant & Mathlib**: The entire verification capability depends on the Lean4 compiler and Mathlib library. Understanding how to import Mathlib and define theorems is required to set up the formal environment. *Quick check: Can you write a Lean4 statement importing Mathlib and defining a simple theorem, for example, that 2 + 2 = 4?*

- **Autoformalization**: This is the core task of the "Translator Module." You must understand the challenge of converting ambiguous natural language into precise, type-checked formal logic and why backtranslation is used for verification. *Quick check: What is the key difference between a natural language proof sketch and a formal Lean4 theorem statement?*

- **Tool-Augmented LLMs (ReAct/Tool Use)**: Hermes operates as an agent that interleaves reasoning with tool calls. Understanding how an LLM decides when to call a function and how to process its structured output is essential for the agent loop. *Quick check: In a tool-calling agent framework, what happens when the external tool returns an error or an unexpected format?*

## Architecture Onboarding

- **Component map**: Reasoning LLM -> Autoformalizer -> Backtranslation Verification -> Prover (parallel goal/¬goal) -> Feedback Module -> Memory Block -> Lean4 REPL
- **Critical path**: Reasoning LLM generates a candidate step → Autoformalizer translates step to Lean goal (with retry budget Kt) → Backtranslation verifies semantic equivalence → Prover attempts proof or counter-proof (with retry budget Kp) → Lean4 REPL returns verification signal → Memory Block is updated → Feedback is passed back to the LLM for the next step
- **Design tradeoffs**: The key levers are `Kt` (translation budget) and `Kp` (prover budget). Increasing them improves accuracy but adds latency and cost. The system defaults (Kt=4, Kp=4) are a starting point. The paper notes that the prover budget is more critical for performance than the translation budget.
- **Failure signatures**:
  - **Autoformalization Failure**: Translator cannot produce compilable Lean code or fails backtranslation equivalence check. Result: "VERIFICATION FAILURE". The LLM proceeds with caution but without verification.
  - **Prover Timeout/Inconclusiveness**: Prover cannot prove or disprove the goal within the sampling budget (`Kp`) or time limit (60s). Result: "VERIFICATION FAILURE".
  - **Reasoning Drift**: Memory block fails to retrieve a relevant prior step, leading the LLM to contradict a previously established fact.
- **First 3 experiments**:
  1. **Sanity Check (End-to-End)**: Run the full Hermes pipeline on simple algebra problems from MATH500. Verify each module produces expected output and the agent solves problems correctly.
  2. **Translator Sensitivity Test**: On precalculus problems, vary translation budget `Kt` (1, 4, 8) while keeping `Kp` fixed. Measure "VERIFICATION FAILURE" rates due to translation errors.
  3. **Prover Budget Impact**: Using default `Kt`, vary prover budget `Kp` (1, 4, 8) on AIME'25. Plot accuracy vs. `Kp` to understand marginal performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
Can Hermes close the performance gap in geometry reasoning as Mathlib's coverage of geometry-related theorems expands? The authors note geometry shows lower gains due to "limited availability of geometry-related theorems in Mathlib," but current evaluation is limited by the existing state of the library.

### Open Question 2
To what extent does increasing the prover sampling budget ($K_p$) beyond 16 improve verification success rates in large-scale settings? The authors hypothesize it may further reduce prover failure rates but limited experiments to $K_p=16$ for efficiency.

### Open Question 3
At what scale of inference budget do reward-based Best-of-N methods become more effective than Hermes for smaller language models? The paper observes reward-based models can outperform Hermes for smaller models like Qwen3-8B when given sufficiently large token budgets ($N \geq 20$), but the exact trade-off boundary remains undefined.

## Limitations
- Geometry problems underperform due to limited Mathlib coverage of geometry-related theorems
- Translation verification reliability not quantified - success rates and failure modes of backtranslation are unknown
- Prover parallelization overhead not explicitly addressed - computational cost of maintaining parallel proving processes is unclear

## Confidence

**High Confidence Claims**:
- Hermes achieves significant accuracy improvements on mathematical reasoning benchmarks compared to base models
- The intermediate formal verification mechanism provides more efficient guidance than reward-based approaches
- Token and FLOP savings are substantial and well-documented through comparative experiments

**Medium Confidence Claims**:
- The memory module effectively maintains proof continuity across long reasoning chains (supported by ablation but lacking quantitative memory retrieval analysis)
- The backtranslation verification reliably catches formalization errors (mechanism described but success rates not reported)
- Parallel proving substantially improves efficiency (claimed but overhead not quantified)

**Low Confidence Claims**:
- Geometry problems are "less suitable" due to Mathlib coverage (stated without detailed analysis of specific coverage gaps)
- The system generalizes well to problems requiring specialized libraries beyond standard Mathlib (no empirical validation on such problems)

## Next Checks

1. **Backtranslation Success Rate Analysis**: Run Hermes on 100 diverse mathematical problems and measure the percentage of steps that pass backtranslation verification versus fail. Correlate failure rates with problem complexity and domain.

2. **Memory Retrieval Precision**: Instrument the memory module to log which prior steps are retrieved for each new step. Analyze whether the top-3 retrieved memories are actually relevant to the current reasoning task, and measure how often irrelevant retrievals lead to logical inconsistencies.

3. **Prover Budget Sensitivity Beyond Default**: Conduct a comprehensive sweep of Kp values (1, 2, 4, 8, 16) on a subset of AIME problems, measuring both accuracy and wall-clock time. Determine whether the claimed efficiency gains hold at higher prover budgets or if the 80% FLOP reduction comes from suboptimal default settings.