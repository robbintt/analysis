---
ver: rpa2
title: 'Stochastic Rounding for LLM Training: Theory and Practice'
arxiv_id: '2502.20566'
source_url: https://arxiv.org/abs/2502.20566
tags:
- training
- bf16
- precision
- learning
- rounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Stochastic rounding (SR) is an unbiased alternative to nearest\
  \ rounding for low-precision training, preventing stagnation in gradient updates.\
  \ The authors provide convergence analysis for Adam under SR, showing that SR error\
  \ can be absorbed within Adam\u2019s natural tolerance, especially with higher learning\
  \ rates."
---

# Stochastic Rounding for LLM Training: Theory and Practice

## Quick Facts
- **arXiv ID**: 2502.20566
- **Source URL**: https://arxiv.org/abs/2502.20566
- **Reference count**: 40
- **Primary result**: BF16+SR training achieves up to 1.54× higher throughput, 30% less memory, and better validation perplexity than mixed-precision training for models up to 6.7B parameters.

## Executive Summary
This paper presents a practical stochastic rounding (SR) strategy for LLM training that achieves competitive or better validation perplexity than mixed-precision training while using only BF16 precision. The key innovation is applying SR only to the weight update step via dithering, avoiding forward pass overhead. The authors provide theoretical analysis showing SR error can be absorbed within Adam's natural tolerance when using 2-4× higher learning rates. Empirically, their approach achieves 1.54× higher throughput and 30% memory reduction compared to mixed precision training across multiple GPT model scales.

## Method Summary
The method applies stochastic rounding only to the AdamW update step, where weights are temporarily upcast to FP32, dithering noise is added to the mantissa, and fractional bits are shifted out before casting back to BF16. All tensors (weights, optimizer states, gradients) remain in BF16 throughout training. Critical to distributed training is the requirement for shared randomness across all ranks—random states must be forked before updates and restored after. The approach requires 2-4× higher learning rates than mixed precision to overcome quantization granularity, with specific values tuned per model scale. The strategy achieves convergence without auxiliary FP32 tensors, reducing memory by 30% compared to O1 mixed precision.

## Key Results
- BF16+SR achieves up to 1.54× higher throughput than mixed-precision training
- Memory consumption reduced by 30% compared to O1 mixed precision
- Better validation perplexity than mixed-precision across GPT models from 350M to 6.7B parameters
- Distributed training requires shared randomness to prevent model drift

## Why This Works (Mechanism)

### Mechanism 1
Stochastic rounding prevents weight update stagnation that occurs with nearest rounding when gradients are small relative to weight magnitudes. Nearest rounding is biased and deterministically rounds to the nearest quantization level—when update `u ≪ x_t`, `Q(x_t + u) = x_t` (no change). SR is unbiased: `E[Q_SR(x)] = x`, so over many steps, small updates accumulate correctly in expectation rather than being lost. The core assumption is that gradient estimates are sufficiently accurate that unbiased accumulation over many iterations yields convergence. Break condition: If learning rate is too small relative to quantization resolution (Δx), SR updates become a random walk requiring excessive iterations.

### Mechanism 2
SR training requires 2-4× higher learning rates than mixed precision to achieve competitive performance. Theorem 1 shows SR implicitly adds a quantization penalty term `(α/4)E[||ξ_α(x)||²]` to the loss. With small α, this penalty dominates, causing stagnation. Corollary 1 proves quantization error becomes negligible relative to Adam's natural gap when α is sufficiently large. The core assumption is that the model can tolerate higher learning rates; SR's additive noise decorrelates gradients, improving stability. Break condition: If learning rate exceeds model's divergence threshold (though SR provides some robustness).

### Mechanism 3
In distributed training, all devices must use identical random states for SR updates or model replicas drift apart. SR is stochastic—identical inputs can round to different values. Without synchronized randomness, gradients aggregated via all-reduce become inconsistent with local weight states, corrupting optimization. The core assumption is that gradient reduction (all-reduce) produces identical gradients across devices before the update step. Break condition: If tensor parallelism is used, effect is nuanced since most weights are already sharded.

## Foundational Learning

- **Stochastic Rounding as Unbiased Estimator**: Understanding why SR prevents error accumulation while nearest rounding doesn't. Quick check: Given a value 2.3 and quantization levels {2, 3}, what is E[Q_SR(2.3)] vs Q(2.3)?
- **Adam's Non-Vanishing Convergence Gap**: The paper's theoretical contribution is showing SR's quantization error can be absorbed into Adam's inherent gap. Quick check: What term in Adam's convergence bound does not vanish as T→∞?
- **Implicit Regularization in Gradient Descent**: The paper extends backward error analysis to show SR adds a quantization-aware penalty. Quick check: In standard GD, what term does gradient flow implicitly minimize besides the loss?

## Architecture Onboarding

- **Component map**: Forward/backward pass -> All BF16 tensors -> Optimizer states (BF16) -> Update step (FP32 dithering -> SR -> BF16)
- **Critical path**: 1. All-reduce gradients (BF16), 2. Update Adam moments (BF16), 3. Synchronize random state across ranks, 4. Compute update: `x_t - α·m̂_t/(√v̂_t + ε) + α·λ·x_t`, 5. Apply SR to update (FP32 dithering → BF16), 6. Restore per-rank random states
- **Design tradeoffs**: SR at update step only (minimal throughput overhead) vs. hardware SR everywhere; BF16 optimizer states (30% memory savings vs O1/O2) vs. numerical precision; Shared vs. independent randomness (shared required for convergence but adds synchronization)
- **Failure signatures**: Divergence with default LR (SR underperforms MP → LR likely too small, increase 2-4×); Training instability at high LR for MP but not SR (expected behavior; SR provides gradient decorrelation); Replicas diverging in distributed training (check random state synchronization); Stagnation early in training (quantization penalty dominating; verify LR scaling)
- **First 3 experiments**: 1. LR sweep: Train GPT-2 350M with SR at LRs {3×, 5×, 7×} default; validate that ~7× default yields best perplexity, 2. Ablate shared randomness: Run distributed training with vs. without synchronized random state; measure validation loss divergence, 3. Memory profiling: Compare peak memory of BF16+SR vs. O1 mixed precision on 1.3B model; target ~10-20% reduction

## Open Questions the Paper Calls Out

### Open Question 1
Can the empirical benefits of BF16-SR be preserved when generalizing to lower precision formats such as FP8 or FP4? The authors note their method generalizes in a straightforward manner to even lower precision such as FP8/4, which they leave as future works due to limited hardware support. What evidence would resolve it: Empirical results from pre-training LLMs using FP8/FP4 with stochastic rounding, showing convergence and throughput comparable to or better than current baselines.

### Open Question 2
Is the observed robustness to high learning rates in SR training strictly caused by the decorrelation of gradients? Section 2.3 notes this is only conjectured and proven in a simplified toy setting. What evidence would resolve it: A theoretical analysis of gradient autocorrelation in realistic training scenarios, or ablation studies isolating the decorrelation effect on stability.

### Open Question 3
How does the shared randomness mechanism impact convergence when combined with aggressive model sharding strategies like Tensor Parallelism? The paper acknowledges the interaction is complex but provides limited analysis. What evidence would resolve it: Ablation studies on multi-node setups comparing synchronized vs. unsynchronized random seeds across varying degrees of model sharding.

## Limitations

- Theoretical analysis assumes infinite-precision gradients, but gradients themselves are subject to BF16 quantization errors before SR application
- Learning rate scaling factor (2-4× higher than MP) is empirically determined rather than theoretically derived
- Long-term statistical properties of shared randomness synchronization across very large-scale training remain unverified

## Confidence

**High Confidence:**
- SR prevents update stagnation compared to nearest rounding for small gradients
- SR training achieves better validation perplexity than mixed precision on tested model scales
- Distributed training requires shared randomness for model consistency

**Medium Confidence:**
- SR implicitly regularizes training via quantization error penalty
- The 2-4× learning rate scaling is sufficient for convergence across different model sizes
- SR's gradient decorrelation provides stability at high learning rates

**Low Confidence:**
- Long-term effects of shared randomness synchronization at extreme scale
- Interaction between gradient quantization and SR dithering in practice
- Generalizability of optimal learning rate scaling beyond tested architectures

## Next Checks

1. **Gradient quantization interaction study**: Instrument training to measure and compare the spectral properties of gradients before and after SR application. Verify that SR's dithering doesn't amplify or distort gradient statistics in ways that could accumulate over long training runs.

2. **Learning rate scaling validation**: Systematically vary the learning rate scaling factor (1.5× to 6× default MP LR) across different model sizes and architectures to establish a more precise theoretical relationship between model capacity and optimal SR learning rates.

3. **Distributed scaling experiment**: Conduct multi-node training at increasing scale (8→32→128 ranks) with and without shared randomness synchronization to measure the statistical drift in model replicas and validate the practical limits of the shared randomness requirement.