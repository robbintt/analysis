---
ver: rpa2
title: 'CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for
  Language Learning'
arxiv_id: '2510.18466'
source_url: https://arxiv.org/abs/2510.18466
tags:
- cefr
- wordnet
- levels
- language
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method to annotate WordNet senses with
  CEFR proficiency levels using an LLM-based approach. The method measures semantic
  similarity between WordNet glosses and English Vocabulary Profile entries to assign
  proficiency levels.
---

# CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning

## Quick Facts
- arXiv ID: 2510.18466
- Source URL: https://arxiv.org/abs/2510.18466
- Reference count: 0
- Method annotates WordNet senses with CEFR levels via LLM-based semantic similarity, validated via contextual lexical classification.

## Executive Summary
This study introduces an automatic method to annotate WordNet senses with CEFR proficiency levels using LLM-based semantic similarity. By comparing WordNet glosses with EVP Online entries via GPT-4o, the method assigns CEFR levels to senses and builds a large-scale SemCor-CEFR corpus. Experiments show classifiers trained on this annotated corpus achieve a Macro-F1 score of 0.81, performing comparably to those trained on gold-standard EVP data. The approach addresses the challenge of limited CEFR-annotated lexical resources for language learning applications.

## Method Summary
The method involves three main steps: (1) Extracting gloss sets from both EVP Online and WordNet for each word-PoS pair, (2) Using GPT-4o to compute semantic similarity scores between each EVP-WordNet gloss pair, and (3) Assigning the EVP's CEFR level to the WordNet sense when the similarity score is 1 or 2. This creates a CEFR-annotated WordNet, which is then applied to SemCor 3.0 to generate SemCor-CEFR. Contextual lexical CEFR-level classifiers are trained on this data and evaluated against gold-standard EVP test sets, achieving competitive performance.

## Key Results
- Macro-F1 score of 0.81 for contextual CEFR classifiers trained on SemCor-CEFR
- Classifiers trained on annotated corpus perform comparably to those trained on gold-standard EVP data
- High Spearman correlation (0.84-0.88) with CompLex 2.0 complexity annotations

## Why This Works (Mechanism)
The approach leverages semantic similarity between expert-curated glosses from both resources to bridge the gap between WordNet's sense inventory and EVP's proficiency levels. By using an LLM to assess fine-grained semantic relationships, the method can map detailed sense descriptions to appropriate proficiency levels even when direct word-level matching fails. The large-scale SemCor-CEFR corpus then provides rich contextual training data that captures how words are actually used across different proficiency levels.

## Foundational Learning
- **CEFR proficiency levels (A1-C2)**: Framework for describing language ability; needed to structure vocabulary difficulty classification; quick check: can identify what distinguishes A1 from B2 vocabulary.
- **WordNet sense inventory**: Detailed lexical database with fine-grained word meanings; needed as target for CEFR annotation; quick check: understand difference between word and sense.
- **EVP Online**: Resource mapping vocabulary to CEFR levels with definitions and examples; needed as source of proficiency-annotated vocabulary; quick check: know EVP provides single-word entries with levels.
- **Semantic similarity scoring**: LLM-based comparison of textual meaning; needed to align glosses across resources; quick check: understand how 7-point scale works.
- **Contextual lexical classification**: Predicting word proficiency from surrounding context; needed to validate annotations and enable practical applications; quick check: know difference between word-level and context-level classification.

## Architecture Onboarding

**Component Map**
EVP Online + WordNet glosses -> GPT-4o similarity scoring -> CEFR-annotated WordNet -> SemCor-CEFR corpus -> Contextual classifiers (BERT+SVC, GPT-4.1 mini)

**Critical Path**
Gloss extraction → LLM similarity computation → CEFR assignment → SemCor mapping → Classifier training → Evaluation

**Design Tradeoffs**
- Fine-grained vs. coarse-grained annotation: WordNet senses vs. word-level EVP
- Coverage vs. accuracy: Lower similarity thresholds increase coverage but may reduce precision
- Context richness vs. annotation effort: SemCor provides real usage contexts vs. dictionary examples

**Failure Signatures**
- Low annotation coverage (<5% of WordNet senses)
- Classifier performance gap between SemCor-CEFR and EVP-trained models
- Poor correlation with external complexity measures

**First Experiments**
1. Measure annotation coverage: percentage of WordNet senses receiving CEFR levels
2. Compare classifier performance: Macro-F1 scores for models trained on EVP vs. SemCor-CEFR vs. mixture
3. Validate annotation quality: manual review of 100 randomly sampled annotated senses

## Open Questions the Paper Calls Out
**Open Question 1**: What are the practical benefits of the CEFR-annotated WordNet for second-language learners in terms of vocabulary acquisition efficiency and cognitive load reduction?
- Basis in paper: [explicit] The Discussion section states that "the practical benefits of this approach require empirical validation through classroom or longitudinal studies."
- Why unresolved: The study validated the resource using NLP metrics (Macro-F1 scores) and correlation with complexity datasets, but did not conduct user studies to measure actual learning outcomes or cognitive effects on students.
- What evidence would resolve it: Results from controlled classroom experiments or longitudinal studies comparing the vocabulary acquisition rates of learners using the CEFR-annotated WordNet versus standard WordNet.

**Open Question 2**: What is the acceptable error threshold for automatic CEFR annotations in educational tools before they negatively impact the learner experience?
- Basis in paper: [explicit] The Limitations section notes the necessity to "quantify the impact of annotation errors on L2 learners and establish acceptable error thresholds for educational use."
- Why unresolved: While the paper demonstrates that its classifiers perform well statistically (Macro-F1 of 0.81), it does not define the point at which misclassification (e.g., labeling a B2 sense as A1) becomes detrimental to a learner's progress or trust.
- What evidence would resolve it: User studies that correlate varying error rates in annotation with metrics of learner confusion, frustration, or assessment performance to identify a tolerance limit.

**Open Question 3**: How can the coverage of CEFR-level annotations be expanded to the vast majority of WordNet senses that lack corresponding entries in the English Vocabulary Profile (EVP)?
- Basis in paper: [explicit] The Conclusion states that future work will "focus on expanding the CEFR-level coverage," and the Limitations section highlights that the method currently only covers "approximately 5% of WordNet's total" due to reliance on the EVP.
- Why unresolved: The current method relies on direct matching with EVP glosses; expanding beyond this requires developing classifiers capable of accurately inferring levels for senses without existing gold-standard definitions.
- What evidence would resolve it: The successful development and evaluation of a contextual classifier that can accurately assign CEFR levels to the currently unannotated 95% of WordNet senses.

## Limitations
- Coverage limited to senses with corresponding EVP entries (~5% of WordNet)
- Annotation accuracy depends on LLM judgment reliability and gloss alignment
- Practical utility for learners requires empirical validation through user studies
- May not capture all pedagogical nuances of CEFR level distinctions

## Confidence

**High confidence**: The experimental methodology for training and evaluating contextual CEFR classifiers is sound and well-documented. The comparative results showing competitive performance between models trained on SemCor-CEFR versus gold EVP data are reliable.

**Medium confidence**: The automatic CEFR annotation accuracy itself, while validated indirectly through classifier performance, has not been directly evaluated through human expert review of a sample of annotated senses. The assumption that high semantic similarity between glosses implies matching CEFR levels is reasonable but not definitively proven.

**Medium confidence**: The practical utility of the annotated corpus for real-world applications depends on the coverage achieved and whether the training data adequately represents the contexts learners encounter.

## Next Checks
1. **Direct human validation**: Have language assessment experts manually review and validate a statistically significant sample (e.g., 100-200) of the automatically annotated WordNet senses to measure precision and identify systematic error patterns.
2. **Cross-domain generalization test**: Evaluate the trained classifiers on an additional out-of-domain corpus of learner writing or authentic language use (beyond CompLex 2.0) to assess how well the CEFR predictions transfer to real learner contexts.
3. **Threshold sensitivity analysis**: Systematically test different similarity score thresholds (not just 1-2) for assigning CEFR levels and measure the trade-off between annotation coverage and classifier performance to optimize the annotation process.