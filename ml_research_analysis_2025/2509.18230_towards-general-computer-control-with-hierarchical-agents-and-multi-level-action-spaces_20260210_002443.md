---
ver: rpa2
title: Towards General Computer Control with Hierarchical Agents and Multi-Level Action
  Spaces
arxiv_id: '2509.18230'
source_url: https://arxiv.org/abs/2509.18230
tags:
- tasks
- learning
- control
- action
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of controlling desktop applications
  through GUI interactions without relying on heavy, cloud-based multimodal language
  models. They propose a lightweight hierarchical reinforcement learning framework,
  ComputerAgent, that decomposes long-horizon tasks into high-level macro-action selection
  and low-level fine-grained execution, using a triple-modal state encoder (vision,
  task ID, numeric state) and a hybrid action space.
---

# Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces

## Quick Facts
- arXiv ID: 2509.18230
- Source URL: https://arxiv.org/abs/2509.18230
- Reference count: 40
- Authors: Zihan Dong; Xinyu Fan; Zixiang Tang; Yunqing Li
- Key outcome: Achieves 92.1% success on simple GUI tasks and 58.8% on hard tasks using a lightweight hierarchical RL framework, matching or exceeding 200B-parameter MLLM baselines while reducing model size by over four orders of magnitude.

## Executive Summary
This paper addresses the challenge of controlling desktop applications through GUI interactions without relying on heavy, cloud-based multimodal language models. The authors propose a lightweight hierarchical reinforcement learning framework, ComputerAgent, that decomposes long-horizon tasks into high-level macro-action selection and low-level fine-grained execution. Using a triple-modal state encoder and a hybrid action space, the system achieves strong performance on a suite of 135 real-world desktop tasks, demonstrating practical on-device feasibility.

## Method Summary
The method employs a hierarchical DQN architecture with a Manager (selecting macro-actions: single_key, hotkey, meta, mouse) and a Subpolicy (executing fine-grained actions). The state is encoded using a triple-modal fusion of visual (screenshot), task ID (description + embedding), and numeric state (pointer position, step count). Training uses a curriculum strategy that prioritizes simpler tasks initially, decaying toward harder tasks. The system is evaluated on 135 GUI tasks across Windows, Ubuntu, and macOS, with performance measured via precision, recall, F1, and normalized reward.

## Key Results
- Achieves 92.1% success rate on simple tasks (<8 steps) and 58.8% on hard tasks (≥8 steps).
- Matches or exceeds 200B-parameter MLLM baselines while reducing model size by over four orders of magnitude.
- Hierarchical DQN with curriculum learning shows +50.4% normalized reward gain over flat DQN baselines.
- Demonstrates on-device feasibility with halved inference time compared to large model alternatives.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Policy Decomposition
**Claim:** A two-level hierarchy (Manager/Worker) mitigates credit assignment issues in long-horizon tasks by isolating high-level logic from low-level execution.

**Core assumption:** UI tasks can be decomposed into semantically meaningful "options" reusable across different tasks.

**Evidence anchors:**
- [Section 3.1] Defines policy composition $\pi(a_t|s_t) = \pi^H \cdot \pi^L$ explicitly splitting decision horizons.
- [Section 4.2] Reports hierarchical DQN yields +50.4% normalized reward gain over flat DQN.
- [Corpus] *OpenHA* (arXiv:2509.13347) supports utility of abstracted action spaces in hierarchical models.

**Break condition:** If macro-actions require frequent mid-execution correction by the manager, the hierarchy may introduce dangerous latency.

### Mechanism 2: Triple-Modal State Fusion
**Claim:** Fusing visual, numeric, and semantic embeddings provides necessary disambiguation beyond raw pixels.

**Core assumption:** Task ID/description embeddings capture sufficient semantic variance to guide policy without live LLM planner.

**Evidence anchors:**
- [Section 3.2] Details fusion $s_t = [v_t; e_t; s'_t]$ used as input for Q-networks.
- [Table 1] Shows "no-id" ablation causes performance drops (e.g., DQN reward 89.5% → 82.6%).
- [Corpus] *UI-Vision* (arXiv:2503.15661) highlights difficulty of visual perception in desktop environments.

**Break condition:** If task embedding is too generic, visual backbone must be significantly more capable to resolve ambiguity.

### Mechanism 3: Curriculum-Guided Reward Shaping
**Claim:** Smooth curriculum that biases sampling toward simpler tasks initially stabilizes exploration policy.

**Core assumption:** Skills learned in simple tasks (e.g., single clicks) transfer directly to sub-components of complex tasks.

**Evidence anchors:**
- [Section 4.1] Defines sampling probabilities $p_{simple}(t)$ decaying via parameter $\alpha$.
- [Section 4.2] States curriculum learning "accelerates early reward accumulation" and reduces variance.
- [Corpus] General desktop control (e.g., *UFO2*) often struggles with sparse rewards; curriculum learning is effective counter-measure.

**Break condition:** If hard tasks require skills completely orthogonal to simple tasks, curriculum may lead to catastrophic forgetting.

## Foundational Learning

**Concept: Options Framework / Temporal Abstraction**
- **Why needed here:** ComputerAgent is explicitly built as a "two-level option process" [Abstract].
- **Quick check question:** Can you explain why a "macro-action" is treated as a semi-Markov decision process rather than a single timestep?

**Concept: Hybrid Action Spaces**
- **Why needed here:** Action space defined as $A = A_{type} \times A_{content}$ [Section 3.3], mixing discrete inputs with parameterized continuous spaces.
- **Quick check question:** How does the agent select parameters for mouse movement (region $R$, subregion $S$) after selecting "Mouse" action type?

**Concept: Curriculum Learning**
- **Why needed here:** Training pipeline relies on "smooth-decay curriculum strategy" [Section 4.1] to manage difficulty of 135 tasks.
- **Quick check question:** What happens to sampling probability of "hard" tasks as normalized training progress $t$ approaches 1?

## Architecture Onboarding

**Component map:**
Screenshot → Vision Backbone (WideResNet-style) → 1024-dim vector
Task Description → Ada-002 + PCA → Task Embedding + Numeric State (pointer, steps)
Fusion: Concatenate Visual + Task + State → State Vector $s_t$
Control: State Vector → Manager Policy (DQN) → Macro-Action
Execution: Macro-Action + State → Subpolicy (DQN) → Micro-Action (Mouse/Key)

**Critical path:** The fusion of task embedding with visual feature is critical for generalization. If Task ID is omitted [Table 1, "no-id"], agent loses semantic grounding, causing precision/recall drops.

**Design tradeoffs:**
- **DQN vs. PPO/A2C:** DQN offered "most stable learning dynamics," while PPO/A2C showed greater variance [Section 4.3].
- **Embedding Size:** 1536 dimensions yielded peak performance, but compact 512-dim context-aware embedding used in ablations to balance capacity vs. overfitting [Table 2].

**Failure signatures:**
- **"Pointer Stagnation":** Agent repeats actions or fails to move mouse, triggering $P_{pointer}$ [Appendix D].
- **"Early Termination":** Agent triggers STOP meta-action before task complete to avoid penalties, triggering $P_{early}$.

**First 3 experiments:**
1. **Baseline Validation:** Train flat DQN agent on 135 tasks without hierarchy to replicate "59.5% reward" baseline and confirm hierarchical gain magnitude.
2. **Embedding Ablation:** Remove Task ID embedding (set to zeros) to verify performance drop reported in Table 1 (precision drop from ~80% to ~63% for DQN).
3. **Curriculum Ablation:** Switch from smooth-decay curriculum to uniform random task sampling to observe increase in training variance and potential drop in normalized reward.

## Open Questions the Paper Calls Out

**Open Question 1:** Can replacing the current "dummy vision module" with a fine-tuned, lightweight UI-focused encoder maintain 15M parameter efficiency while significantly improving generalization to unseen visual layouts?

**Open Question 2:** Does implementing continuous mouse control significantly enhance task success rates for precision operations compared to current discrete region/subregion selection method?

**Open Question 3:** Can meta-learning or self-supervised objectives effectively bridge gap between curated training suite and novel zero-shot tasks without extensive manual reward engineering?

## Limitations
- Task definition granularity: Exact 135 task definitions and ground-truth action sequences are not fully specified, requiring recreation of representative GUI tasks.
- Simulated environment fidelity: Specific desktop environment implementation details (screenshot resolution, region/subregion grid mapping, OS-specific GUI elements) are not provided.
- Embedding API dependency: Reliance on OpenAI's Ada-002 requires API access or substitution with open-source alternatives, potentially affecting embedding quality.

## Confidence

**High Confidence:** The hierarchical decomposition mechanism showing +50.4% normalized reward improvement for DQN is well-supported by ablation study in Section 4.2.

**Medium Confidence:** The curriculum learning strategy's effectiveness (accelerating early reward accumulation) is demonstrated but relies on assumption that simple task skills transfer to complex tasks.

**Medium Confidence:** The triple-modal state fusion's contribution is supported by "no-id" ablation showing performance drops, but exact impact of each embedding component remains partially unclear.

## Next Checks

1. **Flat vs. Hierarchical Baseline:** Train a flat DQN agent on the 135 tasks without hierarchy to verify the magnitude of the reported +50.4% normalized reward gain and confirm the mechanism's effectiveness.

2. **Embedding Ablation Study:** Systematically remove or modify each embedding component (task ID, visual, numeric state) to quantify their individual contributions and validate the reported performance drops.

3. **Curriculum Transferability:** Test the agent's ability to transfer skills learned on simple tasks to completely novel complex tasks not seen during curriculum training to assess generalization claims.