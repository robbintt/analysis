---
ver: rpa2
title: 'Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models'
arxiv_id: '2512.21815'
source_url: https://arxiv.org/abs/2512.21815
tags:
- harmful
- tokens
- clean
- entropy
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the vulnerability of vision-language models\
  \ (VLMs) to adversarial attacks, specifically focusing on how entropy-guided perturbations\
  \ can induce harmful content. The key finding is that a small fraction (around 20%)\
  \ of high-entropy tokens\u2014critical decision points in autoregressive generation\u2014\
  disproportionately influence output trajectories."
---

# Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models

## Quick Facts
- arXiv ID: 2512.21815
- Source URL: https://arxiv.org/abs/2512.21815
- Authors: Mengqi He; Xinyu Tian; Xin Shen; Jinhong Ni; Shu Zou; Zhaoyuan Yang; Jing Zhang
- Reference count: 40
- Key outcome: A small fraction (~20%) of high-entropy tokens in vision-language models disproportionately influence output trajectories, enabling targeted adversarial attacks that achieve 93-95% success rates and convert 35-49% of benign outputs into harmful content.

## Executive Summary
This paper reveals a critical vulnerability in vision-language models (VLMs) where a small subset of high-entropy tokens in the autoregressive generation process can be targeted to manipulate model outputs. The authors demonstrate that by identifying and perturbing these decision-critical tokens, attackers can successfully convert benign image descriptions into harmful content with high probability. The attack method, called Entropy-bank Guided Adversarial (EGA), exploits the observation that these high-entropy positions are shared across different VLM architectures, enabling transferable attacks. This work exposes significant safety risks in current VLM deployments and provides insights into the generation dynamics that make these models vulnerable to targeted manipulation.

## Method Summary
The Entropy-bank Guided Adversarial (EGA) method identifies high-entropy tokens in the VLM's autoregressive generation process, which represent critical decision points where the model's uncertainty is highest. The attack applies small adversarial perturbations (3 pixels) to the input image specifically targeting these high-entropy positions to maximize output manipulation. The method first generates candidate tokens by running the VLM, then computes entropy for each token position to identify the most critical ones. Adversarial perturbations are then crafted using gradient-based optimization to maximize the likelihood of generating harmful content while minimizing perceptible changes to the image. The attack focuses on a small fraction (approximately 20%) of tokens that have the most significant impact on the output trajectory.

## Key Results
- EGA achieves attack success rates of 93-95% on Llama-3-V when targeting high-entropy tokens
- The method converts 35-49% of benign outputs into harmful content with only 3-pixel perturbations
- High-entropy tokens are shared across architecturally diverse VLMs (Llama-3-V, LLaVA, MM1, InternVL), enabling transferability with 17-26% harmful rates on unseen targets

## Why This Works (Mechanism)
The attack exploits the autoregressive nature of VLMs where each generated token conditions the next. High-entropy tokens represent decision boundaries where the model is most uncertain about which token to generate next. By perturbing the input image to influence these critical positions, the attacker can steer the entire generation trajectory toward harmful content. The shared high-entropy positions across different architectures suggest a common vulnerability pattern in how VLMs process visual information and generate language, making the attack transferable despite architectural differences.

## Foundational Learning
**Autoregressive Generation**: Sequential token-by-token text generation where each token conditions the next (why needed: understanding the generation process being attacked; quick check: verify the model uses next-token prediction)
**Token Entropy**: Measure of uncertainty in token prediction, with high entropy indicating ambiguous or critical decision points (why needed: identifies attack targets; quick check: confirm entropy peaks at decision boundaries)
**Vision-Language Alignment**: The process of mapping visual features to linguistic representations in VLMs (why needed: understanding how image perturbations affect text generation; quick check: verify cross-modal attention mechanisms)
**Adversarial Perturbation**: Small, often imperceptible modifications to inputs designed to manipulate model outputs (why needed: the attack mechanism; quick check: measure perturbation magnitude in pixel space)
**Transferability**: The ability of attacks to generalize across different model architectures (why needed: demonstrates attack robustness; quick check: verify success rates on target models)

## Architecture Onboarding
**Component Map**: Input Image -> Vision Encoder -> Cross-Modal Attention -> Autoregressive Language Model -> Output Text
**Critical Path**: Perturbed image features → High-attention visual tokens → High-entropy language positions → Generation trajectory
**Design Tradeoffs**: Model accuracy vs. robustness to input perturbations; generation diversity vs. susceptibility to manipulation
**Failure Signatures**: Sudden shifts to harmful content following high-entropy tokens; generation instability at critical positions
**First 3 Experiments**: 1) Identify high-entropy positions in benign generation; 2) Apply targeted perturbations to critical tokens; 3) Measure harmful content generation rate

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to one VLM architecture (Llama-3-V) and one dataset (CelebA-HQ), with only four additional VLMs tested for transferability
- Human evaluation involves only 20 raters rating 50 benign and 50 harmful outputs, potentially insufficient for capturing variability in harm perception
- Attack success rate definition conflates "harmful" and "non-informative" outputs, potentially inflating effectiveness metrics
- Fixed perturbation budget of 3 pixels without exploring trade-offs between attack success and perceptibility
- Limited harmful concept set of 20 targets may create systematic biases in attack design and outcome measurement

## Confidence
**High confidence**: Reproducibility of entropy-based token identification and effectiveness within tested configuration (Llama-3-V + "Describe the image" + CelebA-HQ)
**Medium confidence**: Transferability claims across four additional VLMs, though specific mechanisms remain under-specified
**Medium confidence**: Safety risk implications given limited harmful concept set and potential measurement artifacts
**Low confidence**: Generalizability to other VLM architectures, prompts, datasets, or perturbation budgets without further validation

## Next Checks
1. Conduct systematic ablation studies varying the perturbation budget (1-10 pixels) and prompt templates to quantify their impact on attack success rate, harmful rate, and perceptibility
2. Benchmark EGA against alternative targeting strategies, including attention-weighted token selection and random high-entropy positions, to isolate the specific contribution of the entropy-guided approach
3. Perform cross-dataset evaluation on diverse image domains (e.g., COCO, ImageNet, medical imaging) and expand the harmful concept set to include broader categories of safety risks beyond the current 20 targets