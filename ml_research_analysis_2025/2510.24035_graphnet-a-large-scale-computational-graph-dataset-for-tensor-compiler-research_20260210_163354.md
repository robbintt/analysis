---
ver: rpa2
title: 'GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research'
arxiv_id: '2510.24035'
source_url: https://arxiv.org/abs/2510.24035
tags:
- compiler
- speedup
- samples
- graph
- graphnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphNet is a large-scale dataset of 2.7K real-world deep learning
  computational graphs from multiple frameworks and task categories, designed to enable
  systematic evaluation of tensor compilers. The authors propose Speedup Score (St)
  and Error-aware Speedup Score (ESt) metrics that jointly consider runtime speedup,
  correctness, and failure penalties under tunable tolerance levels.
---

# GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research

## Quick Facts
- arXiv ID: 2510.24035
- Source URL: https://arxiv.org/abs/2510.24035
- Reference count: 40
- 2.7K real-world deep learning computational graphs benchmark tensor compilers with unified metrics

## Executive Summary
GraphNet addresses the critical need for large-scale, diverse benchmarks in tensor compiler research by providing 2.7K real-world computational graphs from multiple deep learning frameworks and task categories. The dataset enables systematic evaluation of tensor compilers through unified metrics that capture both performance improvements and correctness guarantees. Experiments on NVIDIA H20 GPU reveal that CINN and TorchInductor achieve varying peak speedups across different model categories, with the proposed metrics effectively exposing specific failure modes and performance bottlenecks.

## Method Summary
The GraphNet methodology involves automated extraction of computational graphs from PaddlePaddle and PyTorch models using symbolic tracing decorators, followed by validation and deduplication. The extracted graphs are stored in a unified format compatible with multiple compiler backends. Evaluation uses baseline eager execution versus compiled execution with warmup phases, computing Speedup Score (St) and Error-aware Speedup Score (ESt) metrics that balance performance gains against correctness and failure penalties. The framework supports various precision levels and provides tools for reproducible evaluation across different hardware platforms.

## Key Results
- GraphNet contains 2.7K computational graphs spanning CV (47.8%), NLP (39.5%), and other task categories
- CINN achieves peak St ≈ 1.2 for NLP models while TorchInductor shows St ≈ 0.8-1.1 for CV models on NVIDIA H20
- ESt metrics reveal that PyTorch-CV has 11.3% compilation failure rate and 47% negative optimization rate
- The dataset construction pipeline successfully handles models from thousands to 10B parameters across multiple frameworks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standardizing heterogeneous framework graphs into a unified IR via symbolic tracing likely enables consistent cross-compiler comparison, provided dynamic control flow is correctly captured.
- **Mechanism:** The system wraps user models in decorators (`graph_net.extract`) that perform symbolic tracing to capture operator invocations and tensor dependencies. This graph is serialized into standalone `model.py` and metadata files, decoupling computation definition from original framework runtime execution.
- **Core assumption:** Symbolic tracing correctly captures all operator semantics and dynamic shapes without information loss, which is a known challenge in dynamic deep learning models.
- **Evidence anchors:**
  - [Section 4.2.1]: "The extractor employs symbolic tracing and dynamic graph tracking mechanisms... generating a complete dynamic computational graph."
  - [Corpus - Tempo]: Related work highlights that dynamic dependencies and temporal relationships are difficult to express, implying the success depends on the robustness of the tracing backend against dynamic shapes.
- **Break condition:** The mechanism fails if the model contains control flow or operators opaque to the tracer (e.g., certain custom C++ extensions not accessible via Python).

### Mechanism 2
- **Claim:** The Speedup Score ($S_t$) functions as a robust aggregate metric by penalizing geometric mean speedup with execution failures, preventing outliers from masking instability.
- **Mechanism:** The metric calculates the geometric mean of "rectified speedups." Instead of averaging raw speedups, it multiplies the geometric mean of correct speedups ($\alpha^\lambda$) by a failure penalty term ($b^{1-\lambda}$, where $b \approx 0.1$). This causes the score to plummet exponentially if even a small fraction of samples fail to compile or run.
- **Core assumption:** The penalty factor $b=0.1$ appropriately reflects the cost of a failure relative to a speedup gain in the target evaluation context.
- **Evidence anchors:**
  - [Section 3.2.1]: "St can be viewed as the product of three components: Correct executions... Performance degradation... Failures..."
  - [Appendix B]: Proposition B.1 formally proves the equivalence between the macro formulation and the geometric mean of per-sample rectified speedups.
- **Break condition:** The metric becomes uninformative if the tolerance $t$ is set so loose that incorrect numerical results are counted as "correct," or if $b$ is set too high (forgiving failures).

### Mechanism 3
- **Claim:** The Error-aware Speedup Score ($ES_t$) isolates specific compiler failure modes (accuracy vs. crash vs. compile error) by mapping tolerance levels to error categories.
- **Mechanism:** This extends $S_t$ by reinterpreting positive tolerance values $t \in (0, +\infty)$ not as numerical precision, but as discrete error codes ($c=1,2,3$). It applies a tolerance-dependent penalty $\gamma_t$ that "forgives" specific error types as $t$ increases. By observing jumps in $ES_t$ at $t=1, 2, 3$, developers can identify if bottlenecks are due to numerical divergence or compilation crashes.
- **Core assumption:** The discrete categorization of errors into three classes captures the majority of relevant compiler bugs without significant overlap or ambiguity.
- **Evidence anchors:**
  - [Section 3.2.2]: "We assign error codes c∈{1,2,3} to accuracy errors, runtime crashes, and compilation failures... ESt reaches its maximum [at t≥3], representing the theoretical upper bound."
  - [Section 3.2.2/Results]: "PyTorch-CV shows a sharp rise at t=3, suggesting that a large number of its samples do not pass the compilation stage."
- **Break condition:** The mechanism fails to differentiate if a single sample exhibits multiple error types simultaneously (e.g., a crash caused by numerical overflow).

## Foundational Learning

- **Concept:** **Symbolic Tracing vs. Graph Capture**
  - **Why needed here:** GraphNet relies on extracting static graphs from dynamic frameworks (PyTorch/Paddle). Understanding the difference between eager execution (dynamic) and symbolic tracing (static) is prerequisite to diagnosing why certain models might fail extraction.
  - **Quick check question:** Can you explain why a Python `if x > 0` statement inside a model forward pass might cause a symbolic tracer to fail or capture a static branch?

- **Concept:** **Tensor Compiler Lowering**
  - **Why needed here:** The dataset is designed to evaluate compilers (CINN, TorchInductor). One must understand that these tools lower high-level ops (e.g., `torch.matmul`) into hardware-specific kernels (CUDA/graph-level optimizations) to interpret the "speedup" and "compilation failure" metrics.
  - **Quick check question:** What is the primary role of a "lowering" pass in a tensor compiler like TVM or CINN when processing a computational graph?

- **Concept:** **Geometric vs. Arithmetic Means in Benchmarking**
  - **Why needed here:** The paper explicitly uses geometric mean for $S_t$ to penalize failures. Standard benchmarks often use arithmetic means, which can hide tail latencies or failures behind high averages.
  - **Quick check question:** If a compiler speeds up 90% of models by 2x but crashes on 10%, why might an arithmetic mean speedup look "okay" while a geometric mean with a penalty term ($b < 1$) would look "terrible"?

## Architecture Onboarding

- **Component map:** Extractor (decorator-based symbolic tracing) -> Validator (re-execution consistency check) -> Evaluator (baseline vs compiled execution)
- **Critical path:** 1. Select model -> Wrap with Extractor -> Run forward pass -> Serialized Sample; 2. Load Sample -> Validate (Re-extract & Re-run) -> Clean GraphNet Sample; 3. Run Evaluator (Baseline vs. Compiled) -> Compute St/ESt Scores
- **Design tradeoffs:** Authenticity vs. Compatibility: dataset prioritizes real-world models but enforces constraints like "Static Analyzability" which may exclude highly dynamic models difficult for current compilers to handle anyway. Metric Sensitivity: choice of $b=0.1$ makes metric very sensitive to failures, strict for production readiness but might appear "harsh" for research-stage compilers.
- **Failure signatures:** Extraction Failure: occurs if model uses non-standard ops or complex control flow that tracer cannot follow. Compilation Failure (Error Code 3): sharp jump in $ES_t$ at $t=3$, indicating compiler backend rejected graph IR. Silent Corruption: model runs but output violates `atol`/`rtol` (detected by $S_t$ sensitivity to tolerance $t$).
- **First 3 experiments:** 1. Extraction Dry Run: Take ResNet model from torchvision, wrap with `graph_net.torch.extract`, verify generated `model.py` and `weight_meta.py` files, check for operator loss. 2. Validation Stress Test: Intentionally modify extracted `model.py` (e.g., change attribute) and run Validator to observe if consistency check catches divergence. 3. Metric Sensitivity Check: Run Evaluator on small subset of NLP models using CINN, manually calculate $S_t$ at $t=-5$ and $t=0$ to see how numerical precision affects "pass rate" ($\lambda$) and final score.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance ranking of tensor compilers on GraphNet change when evaluated on non-NVIDIA hardware (TPUs/NPUs) or frameworks outside PaddlePaddle and PyTorch?
- **Basis in paper:** [explicit] Section 6.1 states plans to "expand the dataset by adding more samples from additional frameworks such as TensorFlow, JAX... and extend evaluation to a wider range of hardware platforms."
- **Why unresolved:** Current experiments are strictly limited to NVIDIA H20 GPUs and the PaddlePaddle/PyTorch ecosystems.
- **What evidence would resolve it:** Benchmark results using proposed metrics ($S_t$, $ES_t$) on diverse hardware accelerators (e.g., TPUs) using XLA or TensorFlow.

### Open Question 2
- **Question:** Can GraphNet serve effectively as a ground-truth dataset for training Large Language Models (LLMs) to generate optimized GPU kernels or compiler passes?
- **Basis in paper:** [explicit] Section 1 notes the trend of using LLMs for operators, and Section 6.2 proposes GraphNet as "training and evaluation data for AI-generated compiler passes."
- **Why unresolved:** The paper demonstrates GraphNet's utility for evaluating *existing* compilers (CINN, TorchInductor) but does not validate its efficacy as a training corpus for AI-driven compilation methods.
- **What evidence would resolve it:** A study measuring the convergence rate and performance of LLMs fine-tuned on GraphNet samples compared to those trained on synthetic datasets.

### Open Question 3
- **Question:** Does the current lack of strict validation for "Decomposable" and "Custom Operator Accessible" constraints bias the benchmark against compilers relying on subgraph fusion or external code?
- **Basis in paper:** [inferred] Section 4.1 Remark notes that while "Runnable" and "Serializable" constraints are enforced, "The remaining two (Decomposable and Custom Operator Accessible) are actively under development."
- **Why unresolved:** Samples may currently pass validation but fail during compilation by backends that require strict subgraph decomposition or custom source code, potentially punishing those compilers unfairly via the failure penalty $b$.
- **What evidence would resolve it:** A manual audit of "compilation failure" cases in the $ES_t$ metric to determine if they stem from compiler bugs or dataset constraint violations.

## Limitations
- The dataset construction relies heavily on symbolic tracing capabilities that may fail on models with complex control flow or non-standard operators, though the paper reports successful extraction from 2.7K real-world models.
- The evaluation metrics assume penalty parameters (b=0.1, p=0.1) appropriately reflect real-world costs, but sensitivity to these values is not extensively explored.
- Benchmark results are based on a single GPU architecture (NVIDIA H20), limiting generalizability to other hardware platforms.

## Confidence

- **High Confidence:** Dataset construction methodology, unified graph format, and overall benchmark framework design. The paper provides sufficient implementation details and code availability.
- **Medium Confidence:** Metric definitions and theoretical properties. While the mathematical formulation is rigorous, empirical validation across diverse hardware and compiler configurations is limited.
- **Low Confidence:** Cross-compiler comparison fairness. The paper acknowledges different frameworks may favor their native compilers, but does not systematically control for framework-specific optimizations or implementation differences.

## Next Checks
1. **Extraction Robustness Test:** Attempt to extract graphs from models containing dynamic control flow (if/else based on input tensor values) and custom operators to validate the symbolic tracing mechanism's limits.
2. **Metric Sensitivity Analysis:** Systematically vary the penalty parameters (b and p) in St and observe how the relative rankings of compilers change across different model categories.
3. **Hardware Generalization Test:** Reproduce the benchmark on a different GPU architecture (e.g., NVIDIA A100 or AMD Instinct) to validate the performance patterns observed on H20.