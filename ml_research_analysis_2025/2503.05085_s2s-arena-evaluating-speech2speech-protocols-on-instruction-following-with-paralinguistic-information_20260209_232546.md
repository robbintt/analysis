---
ver: rpa2
title: S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with
  Paralinguistic Information
arxiv_id: '2503.05085'
source_url: https://arxiv.org/abs/2503.05085
tags:
- speech
- arxiv
- information
- paralinguistic
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S2S-Arena, a novel benchmark for evaluating
  speech-to-speech (S2S) models' instruction-following capabilities with paralinguistic
  information. The benchmark addresses limitations in existing evaluations that rely
  on text-based metrics and overlook paralinguistic cues in both speech understanding
  and generation.
---

# S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Information

## Quick Facts
- arXiv ID: 2503.05085
- Source URL: https://arxiv.org/abs/2503.05085
- Authors: Feng Jiang; Zhiyu Lin; Fan Bu; Yuhao Du; Benyou Wang; Haizhou Li
- Reference count: 14
- Introduces S2S-Arena benchmark for evaluating speech-to-speech models' instruction-following with paralinguistic information

## Executive Summary
This paper introduces S2S-Arena, a novel benchmark designed to evaluate speech-to-speech (S2S) models' instruction-following capabilities while incorporating paralinguistic information. The benchmark addresses limitations in existing evaluations that rely on text-based metrics and overlook paralinguistic cues in both speech understanding and generation. The authors create 154 samples across four domains with 21 tasks at four difficulty levels, using manual arena-style pairwise comparisons with ELO ranking to overcome unreliable automatic speech evaluation methods.

The study reveals that while speech models can understand paralinguistic information, they struggle to generate appropriate speech with such features. GPT-4o-realtime emerges as the top performer, and cascaded ASR-LLM-TTS systems generally outperform jointly trained models. The research provides valuable insights for developing future speech models that better handle multimodal and multilingual scenarios while considering paralinguistic information in speech generation.

## Method Summary
The authors developed S2S-Arena by designing 154 test samples across four domains (education, social companionship, entertainment, and medical consultation) with 21 tasks at four difficulty levels. They incorporated both TTS and human recordings to create diverse paralinguistic contexts. To address the unreliability of automatic speech evaluation metrics, they employed manual arena-style pairwise comparisons where human evaluators directly compared model outputs. The ELO ranking system was used to aggregate these pairwise comparisons into overall model performance scores. This methodology allows for more nuanced evaluation of speech generation quality, particularly regarding paralinguistic features that are difficult to quantify with traditional metrics.

## Key Results
- GPT-4o-realtime outperforms other models in instruction-following tasks with paralinguistic information
- Cascaded ASR-LLM-TTS systems generally outperform jointly trained models in S2S tasks
- Speech models demonstrate understanding of paralinguistic information but struggle to generate appropriate speech with such features
- Multilingual support limitations appear to be speech-module-dependent rather than LLM-dependent

## Why This Works (Mechanism)
The success of cascaded systems over jointly trained models stems from their modular architecture, where each component (ASR, LLM, TTS) can be optimized independently for its specific task. This separation allows each module to focus on its core competency without being constrained by the requirements of other components. The manual ELO-based evaluation methodology works effectively because it captures subjective quality aspects of speech generation that automated metrics miss, particularly paralinguistic features like emotion, emphasis, and tone that significantly impact communication effectiveness.

## Foundational Learning
- **Speech-to-Speech Protocols**: Converting spoken input to spoken output while maintaining paralinguistic features - needed for human-like conversational AI; quick check: can the system preserve speaker emotion across conversion
- **Paralinguistic Information**: Non-lexical speech elements like tone, pitch, and emotion - needed for natural communication; quick check: does the system capture speaker intent beyond literal words
- **ELO Ranking System**: Pairwise comparison methodology for ranking model performance - needed for reliable manual evaluation; quick check: do rankings converge across multiple evaluator sessions
- **Cascaded vs Joint Models**: Modular ASR-LLM-TTS versus end-to-end architectures - needed to understand architectural trade-offs; quick check: which architecture better preserves speech quality
- **Multilingual Speech Processing**: Handling multiple languages in speech models - needed for global deployment; quick check: does performance degrade consistently across language families
- **Instruction Following in Speech**: Understanding and executing verbal instructions - needed for practical applications; quick check: can the system handle complex multi-step instructions

## Architecture Onboarding
- **Component Map**: Input Speech -> ASR Module -> LLM Processing -> TTS Module -> Output Speech
- **Critical Path**: Speech input → Speech recognition → Language understanding → Response generation → Speech synthesis
- **Design Tradeoffs**: Cascaded systems offer better modularity and optimization but introduce latency; joint models are more efficient but harder to train and optimize
- **Failure Signatures**: Joint models often produce garbled output when handling complex paralinguistic features; cascaded systems may lose emotional content during module transitions
- **First Experiments**: 1) Test baseline ASR accuracy on paralinguistic-rich speech samples, 2) Evaluate LLM understanding of emotional context in instructions, 3) Measure TTS capability to reproduce paralinguistic features from text

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 154 test instances may limit statistical power and generalizability
- Manual evaluation methodology introduces subjectivity and potential evaluator bias
- Results may not fully generalize to real-world deployment scenarios due to controlled benchmark environment

## Confidence
- **High Confidence**: Cascaded ASR-LLM-TTS systems outperform jointly trained models; speech models understand but struggle to generate paralinguistic features
- **Medium Confidence**: Multilingual limitations are speech-module-dependent; domain-specific performance variations
- **Low Confidence**: Generalizability to real-world applications; specific impact of paralinguistic features on downstream task performance

## Next Checks
1. Conduct formal statistical analysis on ELO rankings across multiple evaluator groups to establish confidence intervals and determine statistical significance of model performance differences
2. Replicate evaluation with additional languages beyond current test set to verify multilingual limitations are consistent across diverse linguistic families
3. Implement time-series evaluation where same models are tested at multiple time points to assess evolution of automatic speech evaluation metrics versus manual comparisons