---
ver: rpa2
title: Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual
  Data Augmentation
arxiv_id: '2504.08792'
source_url: https://arxiv.org/abs/2504.08792
tags:
- augmentation
- data
- entity
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of Named Entity Recognition (NER)
  for low-resource Pakistani languages, including Urdu, Shahmukhi, Sindhi, and Pashto,
  which lack annotated datasets and limited representation in Pre-trained Language
  Models (PLMs). The authors propose a cross-lingual data augmentation technique that
  uses cluster-based methods to generate culturally and linguistically plausible sentences,
  improving the quality of augmented text.
---

# Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual Data Augmentation

## Quick Facts
- arXiv ID: 2504.08792
- Source URL: https://arxiv.org/abs/2504.08792
- Reference count: 31
- Primary result: Cluster-based cross-lingual augmentation with multilingual fine-tuning achieves F1 scores of 88.06 (Shahmukhi) and 88.29 (Pashto) for low-resource Pakistani NER.

## Executive Summary
This study addresses the challenge of Named Entity Recognition (NER) for low-resource Pakistani languages (Urdu, Shahmukhi, Sindhi, Pashto) that lack annotated datasets and limited representation in Pre-trained Language Models. The authors propose a cross-lingual data augmentation technique using cluster-based entity replacement to generate culturally and linguistically plausible sentences. By fine-tuning multilingual masked Large Language Models (Glot500-base and XLM-RoBERTa-large) on augmented data, they achieve significant performance improvements, particularly for Shahmukhi and Pashto, while also exploring the limitations of causal LLMs for these languages.

## Method Summary
The approach uses K-Means clustering on static word embeddings to group entities by semantic categories (e.g., masculine/feminine names, country/city locations). Entity replacements in augmentation are constrained to the same cluster to preserve grammatical and cultural plausibility. Five candidate augmentations are generated per sentence and ranked using a fine-tuned NER model's F1 score, with only top-scoring candidates retained. The augmented datasets are then used to fine-tune multilingual masked LLMs (Glot500-base, XLM-RoBERTa-large) with a token classification head for NER.

## Key Results
- Cluster-constrained augmentation improves NER F1 scores by 2-5 points for low-resource languages
- Shahmukhi achieves F1 of 88.06 and Pashto achieves 88.29 using multilingual fine-tuning
- Multilingual training provides positive transfer for topologically related and culturally similar languages
- Causal LLMs (LLaMA3, Mistral) fail for low-resource languages with F1 <50%

## Why This Works (Mechanism)

### Mechanism 1: Cluster-Constrained Entity Replacement
Constraining entity replacement to semantically coherent clusters reduces grammatical and cultural implausibility in augmented data. K-Means clustering on static word embeddings groups entities by implicit semantic categories (e.g., masculine/feminine names, city/country locations). Replacement candidates are drawn only from the same cluster as the source entity, preserving verbal agreement patterns and avoiding culturally sensitive substitutions.

### Mechanism 2: Cross-Lingual Representation Sharing
Training on combined multilingual data from typologically related languages improves NER via shared entity representations. Multilingual PLMs (Glot500, XLM-RoBERTa) encode cross-lingual semantic similarity. Combining training samples from Urdu, Shahmukhi, Sindhi, and Pashto increases effective training data and exposes the model to shared entity types, improving generalization.

### Mechanism 3: Self-Validation Ranking for Augmentation Quality
Scoring augmented sentences with a pre-trained NER model and selecting top-F1 candidates filters implausible augmentations. Five candidate augmentations are generated per sentence; each is scored by a fine-tuned Glot500-base model. Sentences where the model correctly predicts entity labels are retained, implicitly validating grammatical and contextual plausibility.

## Foundational Learning

- **Named Entity Recognition (NER) with BIO tagging**: The task uses BIO annotation scheme; understanding entity span labeling is essential for interpreting augmentation and evaluation.
  - Quick check: Given "B-PER I-PER O B-LOC O," can you identify which tokens belong to which entity type?

- **Word Embedding Similarity and Clustering**: The core augmentation relies on clustering entities by embedding similarity; understanding cosine similarity and K-Means is prerequisite.
  - Quick check: If two entity embeddings have cosine similarity of 0.95, would K-Means likely assign them to the same cluster?

- **Cross-Lingual Transfer with Multilingual PLMs**: Fine-tuning Glot500 and XLM-RoBERTa on combined language data is the primary NER approach; understanding multilingual representations is required.
  - Quick check: Why might a multilingual model trained on 100 languages transfer better to an unseen language than a monolingual model?

## Architecture Onboarding

- **Component map**: Entity extraction → clustering → alignment → candidate generation → ranking → augmented dataset construction → multilingual fine-tuning
- **Critical path**: Entity extraction → clustering (offline, once per dataset) → alignment → candidate generation → ranking → augmented dataset construction → multilingual fine-tuning
- **Design tradeoffs**: Static vs. contextual embeddings for clustering; single vs. multiple augmentation iterations; monolingual vs. multilingual training
- **Failure signatures**: Gender/agreement errors in augmented sentences; culturally offensive replacements; performance degradation with >1 augmentation iteration; causal LLMs failing on low-resource languages
- **First 3 experiments**: 1) Cluster purity validation: Manually evaluate 200 entities per type in target language to verify cluster assignments match grammatical/cultural categories (target: >80% accuracy). 2) Ablation on augmentation volume: Train with X1, X2, and "all correct" augmentation levels on held-out validation set; expect X1 to perform best. 3) Cross-lingual vs. monolingual comparison: Fine-tune on monolingual data only vs. combined 4-language data; measure delta on Shahmukhi and Pashto test sets (expect +2-5 F1 for multilingual).

## Open Questions the Paper Calls Out
None

## Limitations

- Cluster-based augmentation scalability: Effectiveness for languages with complex grammatical agreement systems beyond gender (e.g., honorifics) remains untested
- Cross-lingual transfer assumptions: Potential negative transfer effects not fully characterized, particularly for Sindhi's unique script features
- Self-validation quality filter reliability: Without direct human evaluation, unclear whether method consistently removes implausible augmentations or introduces selection bias

## Confidence

**High Confidence**: The overall finding that multilingual fine-tuning with cluster-constrained augmentation improves NER performance for low-resource Pakistani languages (Shahmukhi and Pashto showing 88.06 and 88.29 F1 scores respectively).

**Medium Confidence**: The specific mechanism that cluster-based entity replacement preserves cultural and grammatical plausibility. While supported by manual validation for Urdu, generalizability to other low-resource languages requires further testing.

**Low Confidence**: The assertion that causal LLMs are fundamentally unsuitable for low-resource NER tasks. The study's experiments may not have explored optimal prompting strategies or fine-tuning approaches that could improve causal LLM performance.

## Next Checks

1. **Cluster robustness testing**: Systematically evaluate clustering quality across all four target languages (Urdu, Shahmukhi, Sindhi, Pashto) using manual annotation of 500 entities per language. Measure precision, recall, and F1 for cluster assignments against human-annotated semantic categories, and assess correlation between cluster purity and downstream NER performance.

2. **Ablation of multilingual contributions**: Conduct controlled experiments isolating the contribution of each language to the multilingual training set. Train separate models on: Urdu-only, Shahmukhi-only, Sindhi-only, Pashto-only, all pairwise combinations, and the full multilingual set. Measure performance deltas to identify which language pairs provide positive versus negative transfer.

3. **Human evaluation of augmented data quality**: Recruit native speakers of each target language to rate 200 augmented sentences per language on grammaticality, cultural appropriateness, and plausibility using a 5-point Likert scale. Compare ratings between cluster-constrained augmentations and random replacements to quantify the practical impact of the clustering mechanism on real-world usability.