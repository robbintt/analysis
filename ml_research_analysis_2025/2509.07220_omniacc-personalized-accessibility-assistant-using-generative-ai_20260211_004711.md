---
ver: rpa2
title: 'OmniAcc: Personalized Accessibility Assistant Using Generative AI'
arxiv_id: '2509.07220'
source_url: https://arxiv.org/abs/2509.07220
tags:
- crosswalk
- dataset
- data
- crosswalks
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OmniAcc, an AI-powered navigation system for
  wheelchair users that uses GPT-4o with zero-shot learning to detect crosswalks in
  satellite imagery. The system combines high-resolution GeoTIFF images with OpenStreetMap
  data and employs prompt engineering with visual overlays to guide detection.
---

# OmniAcc: Personalized Accessibility Assistant Using Generative AI

## Quick Facts
- arXiv ID: 2509.07220
- Source URL: https://arxiv.org/abs/2509.07220
- Reference count: 20
- Primary result: 97.5% accuracy in zero-shot crosswalk detection from satellite imagery

## Executive Summary
OmniAcc is an AI-powered navigation system that enables wheelchair users to identify crosswalks using satellite imagery and GPT-4o with zero-shot learning. The system processes high-resolution GeoTIFF images combined with OpenStreetMap data through specialized prompt engineering to detect accessibility features without requiring labeled training data. Experimental results demonstrate 97.5% accuracy, 96.1% precision, and 99.0% recall using a blurred dataset configuration, showing that multimodal large language models can effectively perform specialized accessibility feature detection tasks.

## Method Summary
The system uses GPT-4o Vision API with zero-shot learning to detect crosswalks in 256×256 image patches from 5000×5000px NAIP GeoTIFF imagery. Four preprocessing configurations were tested: Plain, Separated, Overlaid (OSM vectors on raster), and Blurred (Gaussian blur σ=5 on non-relevant regions). Images are base64-encoded and processed with a chain-of-thought system prompt that provides step-by-step criteria for crosswalk identification. The model analyzes visual patterns, OSM overlays, and cross-reference features to produce structured binary classification outputs.

## Key Results
- Blurred configuration achieved 97.5% accuracy, 96.1% precision, 99.0% recall, and 97.53% F1-score
- Separated configuration performed worst at 35% accuracy, highlighting the importance of integrated raster-vector visualization
- Gaussian blur preprocessing significantly improved detection performance by reducing visual noise in non-relevant regions

## Why This Works (Mechanism)
The approach leverages GPT-4o's multimodal capabilities to interpret visual patterns in satellite imagery through carefully engineered prompts that break down the crosswalk detection task into sequential steps. The Blurred configuration works by reducing visual complexity in non-road areas, helping the model focus on relevant features while minimizing false positives from shadows or road wear patterns.

## Foundational Learning
- **Zero-shot learning**: Model performs tasks without training on labeled examples - needed for rapid deployment without extensive dataset collection, check by verifying model outputs without any parameter updates
- **GeoTIFF image processing**: High-resolution satellite imagery format at 1m/pixel resolution - needed for capturing fine crosswalk details, check by examining patch resolution quality
- **OpenStreetMap integration**: Vector road network data overlaid on raster imagery - needed to provide context for crosswalk positioning, check by verifying coordinate alignment accuracy
- **Gaussian blur preprocessing**: σ=5 blur applied to non-relevant regions - needed to reduce visual noise, check by comparing edge detection in blurred vs. non-blurred regions
- **Chain-of-thought prompting**: Step-by-step reasoning instructions for model - needed to guide systematic feature detection, check by analyzing model's response structure
- **Base64 encoding**: Image format conversion for API transmission - needed for GPT-4o Vision API compatibility, check by validating successful API response

## Architecture Onboarding

**Component map**: GeoTIFF images -> Patch extraction (256×256) -> Gaussian blur (σ=5) -> Base64 encoding -> GPT-4o Vision API -> Structured output parsing -> Classification

**Critical path**: Image preprocessing (blur) → API encoding → Prompt execution → Response parsing → Accuracy calculation

**Design tradeoffs**: Zero-shot learning avoids training costs but depends on prompt engineering quality; preprocessing choices (blur σ=5) significantly impact performance; API-based approach enables rapid deployment but incurs computational costs

**Failure signatures**: 
- False positives from shadows or road wear patterns detected as crosswalks
- False negatives when multiple road orientations confuse the model
- Low recall in Separated configuration due to missing integrated visualization

**First 3 experiments to run**:
1. Replicate Blurred configuration with different σ values (3, 7, 10) to optimize noise reduction
2. Test prompt variations with explicit negative examples for shadow patterns
3. Compare performance across different geographic regions with varying crosswalk designs

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset (200 patches from single city) limits generalizability across diverse urban environments
- Computational costs of API-based inference not addressed for large-scale deployment
- Zero-shot methodology untested on novel scenarios and different geographic contexts

## Confidence
High: Reported metrics are transparent and use established evaluation methods
Medium: Broader claim about LLMs replacing traditional CV models needs cross-validation on independent datasets
High: Blurred configuration's superiority demonstrated within this specific task

## Next Checks
1. Replicate experiment using satellite imagery from a different city or country to assess geographic generalizability
2. Test prompt engineering approach with alternative multimodal models (Claude 3, Gemini) for model-agnostic validation
3. Conduct cost-benefit analysis comparing API-based inference versus fine-tuned traditional computer vision models for urban accessibility mapping scalability