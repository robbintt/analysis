---
ver: rpa2
title: 'Information Loss in LLMs'' Multilingual Translation: The Role of Training
  Data, Language Proximity, and Language Family'
arxiv_id: '2506.23340'
source_url: https://arxiv.org/abs/2506.23340
tags:
- translation
- language
- languages
- distance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates how training data, language
  proximity, and language family affect information loss in multilingual translation
  by large language models. Using round-trip translations with GPT-4 and Llama 2,
  we evaluated translation quality through BLEU scores and BERT similarity across
  88 languages.
---

# Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family

## Quick Facts
- **arXiv ID:** 2506.23340
- **Source URL:** https://arxiv.org/abs/2506.23340
- **Reference count:** 3
- **Primary result:** Significant interactions between training data size and language distance, with structural similarity crucial for low-resource languages and language family independently influencing translation quality.

## Executive Summary
This study systematically investigates how training data, language proximity, and language family affect information loss in multilingual translation by large language models. Using round-trip translations with GPT-4 and Llama 2, we evaluated translation quality through BLEU scores and BERT similarity across 88 languages. Our results reveal significant interactions between training data size and language distance, showing that while abundant data can mitigate linguistic divergence, structurally closer languages consistently yield higher quality in low-resource conditions. Language family also independently influenced performance, with Indo-European-Romance and Germanic languages showing superior translation quality. These findings emphasize that translation quality depends not only on data volume but also on structural and typological relationships between languages.

## Method Summary
The study uses round-trip translation (English → target → English) on the FLORES-200 dataset with 1,000 English sentences to measure information loss. GPT-4 translated 88 languages and Llama 2 translated 26 languages. Translation quality was evaluated using BLEU scores (n-gram overlap) and BERT similarity (semantic similarity). Language distances were quantified using URIEL metrics (phylogenetic, orthographic, syntactic, geographic). Linear mixed-effects regression with random effects for sentences and random forest analysis were used to identify key predictors. Data collection involved API calls with temperature=0.3 and max_tokens=500, with 5 repetitions per sentence.

## Key Results
- Robust interaction between training data size and language distance, with abundant data partially compensating for linguistic differences
- Orthographic distance particularly predictive of translation quality due to subword tokenization efficiency
- Language family significantly influences performance, with Indo-European-Romance and Germanic languages showing superior translation quality
- Syntactic similarity facilitates more efficient cross-lingual transfer by enabling overlapping internal representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training data volume and language distance interact such that sufficient data can compensate for linguistic divergence, but structural similarity is critical when data is scarce.
- **Mechanism:** In high-resource settings, extensive exposure allows the model to learn distinct cross-lingual mappings despite structural differences. In low-resource settings, the model relies on transfer learning; structural similarity provides a "scaffold" for generalizing patterns from English.
- **Core assumption:** The model's internal representations of linguistically similar languages overlap sufficiently to facilitate transfer learning.
- **Evidence anchors:** "robust interaction between training data size and language distance"; "abundant training data can partially compensate for the disadvantages posed by linguistic differences"; scaling/computation mitigates but doesn't eliminate multilingual interference.
- **Break condition:** If a language pair has both low training data and high phylogenetic/syntactic distance, translation quality collapses.

### Mechanism 2
- **Claim:** Orthographic distance predicts translation quality because subword tokenization efficiency varies by script similarity.
- **Mechanism:** Languages using the Latin script share efficient token mappings with English. Non-Latin scripts force fragmented tokenization, increasing computational load and diluting semantic coherence.
- **Core assumption:** Token fragmentation directly degrades the model's ability to maintain semantic alignment during encoding/decoding.
- **Evidence anchors:** "Orthographic distance was particularly predictive... due to its direct interaction with core architectural mechanisms such as subword tokenization"; "languages with high orthographic divergence tend to have lower BLEU... as tokenization issues propagate".
- **Break condition:** A language with a non-Latin script but extremely high training data might overcome tokenization inefficiencies.

### Mechanism 3
- **Claim:** Syntactic distance influences translation quality by modulating the complexity of structural transformations required by the self-attention mechanism.
- **Mechanism:** When source and target languages share syntactic structures, the model activates overlapping internal representations. Divergent syntax requires complex reordering, increasing information loss or hallucination risk.
- **Core assumption:** LLMs possess "language-specialized attention heads" that function best when structural patterns align with pre-trained expectations.
- **Evidence anchors:** "syntactic similarity facilitates more efficient cross-lingual transfer by enabling the activation of overlapping internal representations"; "structurally closer languages consistently yield higher quality in low-resource conditions".
- **Break condition:** If the model is fine-tuned on a pivot language sharing syntax with the target, this "English-centric" syntactic penalty should decrease.

## Foundational Learning

- **Concept: Round-trip Translation**
  - **Why needed here:** The study uses this methodology (Eng → Target → Eng) to quantify "information loss" without needing gold-standard parallel corpora for 88 languages.
  - **Quick check question:** Does a high BLEU score in a round-trip translation guarantee the intermediate translation was grammatically correct, or only that the semantic gist survived?

- **Concept: URIEL Language Distance Metrics**
  - **Why needed here:** The paper relies on these vectors (phylogenetic, syntactic, geographic, etc.) to quantify "closeness."
  - **Quick check question:** If two languages are geographically close but phylogenetically distant, which metric does the paper suggest is a stronger predictor of translation quality?

- **Concept: BLEU vs. BERT Similarity**
  - **Why needed here:** The paper distinguishes between surface-form matching (BLEU) and semantic similarity (BERT).
  - **Quick check question:** Why might a model score high on BERT similarity but lower on BLEU when translating between related languages like English and Romance languages?

## Architecture Onboarding

- **Component map:** Input (FLORES-200) → Encoder/Decoder (GPT-4/Llama 2) → Forward Translation (Eng → Target) → Backward Translation (Target → Eng) → Evaluator (BLEU & BERT) → Predictors (Training Data % + Language Distance)
- **Critical path:** The interaction between Training Data Size and Language Distance determines if the system performs "transfer learning" (relying on similarity) or "direct mapping" (relying on data volume).
- **Design tradeoffs:**
  - Round-trip translation allows massive scale but introduces error propagation and may obscure unidirectional grammar errors
  - BLEU penalizes valid paraphrasing while BERT captures semantics but may miss subtle negation errors
- **Failure signatures:**
  - Hallucination/Literal translation in low-resource, high-distance pairs
  - Script-induced fragmentation causing low BLEU for Sino-Tibetan/Niger-Congo families
  - Semantic Drift with GPT-4 showing lower BERT scores for Germanic languages due to adaptive generation
- **First 3 experiments:**
  1. **Pivot Language Analysis:** Repeat using French or Chinese as pivot to test if syntactic/phylogenetic distances relative to the new pivot predict performance better than distance to English
  2. **Tokenizer Stress Test:** Compare token fragmentation rates against BLEU scores for non-Latin scripts with high data (e.g., Japanese) vs. Latin scripts with low data
  3. **Semantic vs. Surface Ablation:** Introduce targeted semantic checks (negation, numerical preservation) to detect if high BERT scores mask critical information loss

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive tokenization mechanisms tailored to specific language families mitigate the negative effects of orthographic distance on translation quality?
- **Basis in paper:** [explicit] The Discussion section states that "Future advancements in adaptive tokenization mechanisms tailored to specific language families may mitigate these effects."
- **Why unresolved:** Current subword tokenization fragments non-Latin scripts, causing semantic dilution and lower BLEU scores, but the paper does not test alternative tokenization strategies.
- **What evidence would resolve it:** A comparative study evaluating translation fidelity in LLMs using custom tokenizers for orthographically distant languages versus standard subword tokenization.

### Open Question 2
- **Question:** How do LLMs adapt their internal translation strategies, such as shifting between literal translation and paraphrasing, based on the target language family?
- **Basis in paper:** [explicit] The Conclusion proposes that future efforts "explore... how LLMs tailor their translation strategies to different language types."
- **Why unresolved:** The study observes that GPT-4 produces lower BERT similarity for structurally close languages (suggesting paraphrasing) but does not analyze the internal mechanisms driving this context-sensitive adaptation.
- **What evidence would resolve it:** Probing classifier analysis of internal model states to detect activation patterns associated with literal vs. free translation strategies across different language families.

### Open Question 3
- **Question:** Do the interactions between training data and language proximity persist in non-English-centric translation directions?
- **Basis in paper:** [inferred] The study relies exclusively on English-centric round-trip translation, limiting generalizability to direct translation between non-English pairs.
- **Why unresolved:** It remains unclear if data volume compensates for linguistic divergence when English is not the pivot, as the "English-centric" bias of LLMs might be the primary driver of the observed proximity effects.
- **What evidence would resolve it:** Evaluating direct translation performance between typologically diverse non-English language pairs relative to their specific training data and linguistic distances.

## Limitations
- Round-trip translation introduces compounding error propagation that may obscure true translation quality
- Analysis focuses primarily on English-centric translation pairs, limiting generalizability to other language families
- Reliance on automatically computed language distance metrics without validating their correlation with actual translation difficulty

## Confidence
- **High Confidence:** The finding that abundant training data can partially compensate for linguistic differences is strongly supported by robust interaction effects
- **Medium Confidence:** The specific quantitative thresholds where training data compensates for linguistic distance are less certain due to correlational nature
- **Low Confidence:** The exact relative importance of each language distance dimension varies significantly between models and may be influenced by unaccounted confounding factors

## Next Checks
1. **Pivot Language Analysis:** Repeat the entire experiment using French or Chinese as the pivot language instead of English to test whether syntactic and phylogenetic distances relative to the pivot better predict translation quality.

2. **Tokenization Efficiency Test:** For languages with non-Latin scripts but high training data (e.g., Japanese) versus Latin scripts with low data, measure token fragmentation rates and correlate with BLEU scores to directly test whether orthographic distance effects operate through tokenization efficiency.

3. **Semantic Preservation Analysis:** Implement targeted semantic checks focusing on specific information types (negation, numerical values, named entities) to determine whether high BERT similarity scores mask critical semantic losses, particularly for language pairs where paraphrasing might preserve general meaning while losing specific details.