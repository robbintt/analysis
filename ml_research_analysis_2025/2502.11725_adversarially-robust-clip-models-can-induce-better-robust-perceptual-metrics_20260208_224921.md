---
ver: rpa2
title: Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics
arxiv_id: '2502.11725'
source_url: https://arxiv.org/abs/2502.11725
tags:
- clip
- robust
- perceptual
- vit-b
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarially robust CLIP models (R-CLIP F) can serve as perceptual
  metrics that outperform both standard CLIP and existing robust perceptual metrics
  in terms of clean and adversarial accuracy on 2AFC tasks, as well as on image-to-image
  retrieval tasks including NSFW detection and dataset filtering. R-CLIP F also enables
  interpretable similarity assessment via feature and text inversion, allowing visualization
  of the rich visual concepts learned by CLIP.
---

# Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics

## Quick Facts
- **arXiv ID:** 2502.11725
- **Source URL:** https://arxiv.org/abs/2502.11725
- **Reference count:** 40
- **Primary result:** Adversarially robust CLIP models (R-CLIP F) outperform standard CLIP and existing robust perceptual metrics on 2AFC tasks and image retrieval while enabling interpretable feature inversion.

## Executive Summary
This paper demonstrates that adversarially robust CLIP models can serve as perceptual metrics that outperform both standard CLIP and existing robust perceptual metrics in terms of clean and adversarial accuracy on 2AFC tasks, as well as on image-to-image retrieval tasks including NSFW detection and dataset filtering. R-CLIP F also enables interpretable similarity assessment via feature and text inversion, allowing visualization of the rich visual concepts learned by CLIP. This demonstrates that adversarial robustness can improve perceptual alignment and utility without sacrificing clean performance.

## Method Summary
The authors create adversarially robust CLIP models (R-CLIP) through adversarial fine-tuning on ImageNet using two approaches: FARE (unsupervised) and TeCoA (supervised). FARE minimizes the ℓ2 distance between the original encoder's embedding of clean images and the fine-tuned encoder's embedding of perturbed images, while TeCoA uses ImageNet labels for supervised adversarial training. Both methods use ℓ∞-bounded perturbations with ε=4/255 and run for 2 epochs. The resulting R-CLIP models are evaluated on 2AFC tasks (NIGHTS, BAPPS), image retrieval (ROxford/RParis), and NSFW detection, demonstrating superior clean and robust performance compared to standard CLIP and existing robust perceptual metrics.

## Key Results
- R-CLIP models achieve 3-7% higher clean accuracy than their original clean counterparts across all architectures on 2AFC tasks
- R-CLIPF outperforms R-CLIPT on NSFW detection (88.6% vs 74.2% clean accuracy for safe queries)
- Feature inversion with R-CLIP produces semantically meaningful reconstructions while clean CLIP yields adversarial noise
- Robust accuracy remains stable with increased attack iterations, while standard CLIP accuracy continues to drop

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Training Selects Semantically Aligned Features
Adversarial training improves zero-shot perceptual alignment with human judgment by suppressing non-robust features (which lack semantic content) while emphasizing robust features correlated with higher-order semantic concepts. This aligns model representations more closely with human shape-biased perception.

### Mechanism 2: Unsupervised FARE Preserves Embedding Space Geometry
FARE preserves the original CLIP embedding space better than TeCoA by minimizing the ℓ2 distance between original and fine-tuned encoder outputs. This preserves compatibility with text embeddings and enables better generalization to downstream tasks beyond ImageNet.

### Mechanism 3: Robust Gradients Enable Interpretable Optimization
Adversarially robust CLIP models have gradients aligned with semantic features rather than adversarial noise. When optimizing an image to match a target embedding or text prompt, the gradient direction leads to coherent images rather than adversarial patterns.

## Foundational Learning

- **Two-Alternative Forced Choice (2AFC) Tasks**: Used to evaluate perceptual metrics where models predict which of two images is more similar to a reference. Quick check: Given embeddings for reference image r and two candidates c₁, c₂, how would you predict which candidate is more similar using cosine similarity?
- **Adversarial Training (PGD-based)**: Both TeCoA and FARE use adversarial training with ℓ∞-bounded perturbations. Quick check: What is the difference between the loss functions in Eq. (1) and Eq. (2), and why does TeCoA require labels while FARE does not?
- **Cosine Similarity as Perceptual Metric**: The paper uses cosine similarity in normalized embedding space as the core perceptual metric. Quick check: Why does the paper use cosine similarity rather than ℓ2 distance directly? What is the relationship between the two for normalized vectors?

## Architecture Onboarding

- **Component map:** Vision Encoder ϕ → Image embeddings; Text Encoder ψ → Text embeddings; Perceptual Metric: sim(x₁, x₂) = cosine(ϕ(x₁), ϕ(x₂))
- **Critical path:** Load pre-trained CLIP → Apply adversarial fine-tuning (FARE or TeCoA) on ImageNet → Evaluate zero-shot on NIGHTS/BAPPS → Optionally fine-tune with LoRA on NIGHTS → Compute cosine similarity for retrieval tasks
- **Design tradeoffs:** TeCoA gives higher robust accuracy on 2AFC (87.2% vs 85.6% ℓ∞ robust after LoRA), but FARE generalizes better to retrieval and NSFW tasks. ConvNeXt-B generally outperforms ViT-B variants.
- **Failure signatures:** Near-zero robust accuracy indicates insufficient adversarial training; feature inversion producing noise suggests using non-robust encoder; poor NSFW detection with R-CLIPT indicates overfitting to ImageNet.
- **First 3 experiments:**
  1. Zero-shot 2AFC evaluation: Load R-CLIPF (ConvNeXt-B), compute cosine similarities on NIGHTS test triplets, compare accuracy to Table I values (~90.6% clean, ~74.3% ℓ∞-robust)
  2. Robustness sanity check: Run APGD attack on NIGHTS with increasing attack iterations, verify robust accuracy stabilizes rather than continuing to drop
  3. Feature inversion visualization: Optimize random image to maximize cosine similarity to a reference image embedding using APGD, expect semantic reconstruction with R-CLIPF

## Open Questions the Paper Calls Out

1. **Perceptual similarity mechanism:** Why does adversarial training improve clean performance on perceptual similarity tasks (2AFC) while typically degrading it in classification tasks? The authors hypothesize robustness suppresses non-robust features lacking semantic information, but this needs direct testing.

2. **Types of perceptual similarity captured:** What specific perceptual attributes (shape, texture, color) do robust encoders emphasize compared to non-robust encoders? While feature inversion shows semantic content, systematic analysis across different perceptual attributes is needed.

3. **Applications to generative guidance:** Can robust perceptual metrics improve image-text alignment for guiding generative models or image quality assessment? The paper evaluates only 2AFC, retrieval, and NSFW detection, leaving generative guidance unexplored.

4. **Transfer learning limitations:** Why does fine-tuning on NIGHTS improve 2AFC performance but degrade performance on image retrieval tasks and the THINGS dataset? The authors suggest overfitting but don't investigate the underlying cause.

## Limitations

- The mechanism explanation relies heavily on the assumption that non-robust features lack semantic content, which is supported by related work but not directly tested within this paper's experiments.
- FARE's effectiveness depends on the assumption that preserving original embedding geometry is beneficial, but this could backfire if the original space contains problematic directions.
- The text inversion results use a relatively small 640-dimensional embedding space, raising questions about scalability to larger CLIP models.

## Confidence

**High Confidence:**
- R-CLIP models achieve better clean and robust accuracy on 2AFC tasks compared to standard CLIP and existing robust perceptual metrics (supported by Table I and II)
- R-CLIPF outperforms R-CLIPT on out-of-distribution retrieval tasks including NSFW detection (supported by Table III showing 14.4% absolute improvement)
- Feature inversion with robust models produces semantically meaningful reconstructions (visually demonstrated in Figure 4)

**Medium Confidence:**
- The mechanism that adversarial training selects semantically aligned features (plausible but not directly tested)
- FARE's advantage in preserving embedding space geometry for better generalization (supported by retrieval results but not explicitly tested through ablation)
- Robust gradients enable interpretable optimization (inferred from inversion results but could have alternative explanations)

**Low Confidence:**
- The claim that R-CLIP enables "interpretable similarity assessment" is somewhat overstated
- The specific epsilon value (4/255) is presented as optimal without systematic comparison to other values

## Next Checks

1. **Direct mechanism validation:** Design an experiment to test whether non-robust features in standard CLIP contain less semantic information than robust features in R-CLIP through ablating robust vs non-robust components.

2. **FARE vs TeCoA generalization study:** Systematically evaluate R-CLIPF and R-CLIPT on additional out-of-distribution datasets beyond NSFW detection to quantify the generalization advantage more broadly.

3. **Adversarial training sensitivity:** Conduct a hyperparameter sweep over ε values (e.g., 2/255, 4/255, 8/255) to determine the optimal trade-off between robustness and perceptual alignment, and test whether the mechanism claims hold across this range.