---
ver: rpa2
title: Centroid Decision Forest
arxiv_id: '2503.19306'
source_url: https://arxiv.org/abs/2503.19306
tags:
- class
- feature
- centroid
- features
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the centroid decision forest (CDF), an ensemble
  learning framework designed for high-dimensional classification. The key innovation
  is a centroid-based splitting strategy, where the class separability score (CSS)
  selects the most discriminative features at each node, and class centroids are computed
  using mean feature values.
---

# Centroid Decision Forest

## Quick Facts
- arXiv ID: 2503.19306
- Source URL: https://arxiv.org/abs/2503.19306
- Reference count: 40
- CDF achieves highest accuracy on 18/23 high-dimensional datasets, outperforming RF, XGBoost, and SVM

## Executive Summary
This paper introduces the centroid decision forest (CDF), an ensemble learning framework designed for high-dimensional classification. The key innovation is a centroid-based splitting strategy, where the class separability score (CSS) selects the most discriminative features at each node, and class centroids are computed using mean feature values. Data points are assigned to partitions based on Euclidean distance to these centroids, improving class separation and interpretability. The CDF was evaluated on 23 high-dimensional datasets against state-of-the-art classifiers, including CART, Random Forest, XGBoost, kNN, and SVM. Results show that CDF achieves the highest classification accuracy on 18 out of 23 datasets, with an average accuracy of 0.871 and Cohen's kappa of 0.734. It also outperformed competitors in robustness and stability, as indicated by boxplots of accuracy and kappa distributions. The method effectively addresses high-dimensional classification challenges, providing a scalable and interpretable solution for complex pattern recognition tasks.

## Method Summary
CDF constructs decision trees where splits are determined by class separability scores (CSS) that rank features based on their discriminative power between classes. At each node, the top m features are selected from a random subset, class centroids are computed using mean feature values, and samples are routed to child nodes based on Euclidean distance to the nearest centroid. The ensemble consists of B such trees built on bootstrap samples, with final predictions made by majority voting. The method uses fixed hyperparameters (B=500, d_max=3, mtry=0.2p, m=⌊2×log(p)⌋) and evaluates performance using accuracy and Cohen's kappa across 500 repetitions of 70/30 train-test splits on 23 benchmark datasets.

## Key Results
- CDF achieved highest accuracy on 18 out of 23 high-dimensional datasets tested
- Average accuracy of 0.871 and Cohen's kappa of 0.734 across all datasets
- Outperformed state-of-the-art classifiers including Random Forest, XGBoost, kNN, and SVM
- Demonstrated superior robustness and stability in accuracy and kappa distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Class Separability Score (CSS) identifies features with maximal discriminative power by quantifying mean differences relative to within-class variance across all class pairs.
- Mechanism: CSS computes a ratio of between-class mean differences to pooled standard deviations for each feature, summed across all $\binom{K}{2}$ class pairs. Features with large inter-class mean separations and small intra-class variance receive higher scores, making them candidates for splitting.
- Core assumption: Assumes that discriminative features exhibit consistent mean shifts between classes with low within-class spread, which holds when class-conditional distributions are approximately Gaussian with shared covariance structure.
- Evidence anchors:
  - [abstract] "class separability score (CSS) determines the selection of the most discriminative features at each node"
  - [section] Definition 1 and Lemma 1 establish CSS formulation and its relationship to discriminability
  - [corpus] Weak direct evidence; related work on oblique forests addresses similar splitting challenges but via hyperplane combinations rather than score-based filtering
- Break condition: CSS becomes unreliable when within-class distributions are multi-modal, heavily skewed, or when discriminative information resides in higher-order moments rather than means.

### Mechanism 2
- Claim: Centroid-based partitioning creates decision boundaries by assigning samples to their nearest class centroid in Euclidean space, which minimizes within-class sum of squares (WCSS).
- Mechanism: At each node, centroids $C_c = \frac{1}{n_c}\sum_{i=1}^{n_c} X_i$ are computed per class using selected features. Samples are routed to child nodes based on $\text{arg-min}_c d(X_i, C_c)$. This implicitly creates Voronoi-like partitions around class representatives.
- Core assumption: Assumes classes are roughly spherical or convex in the selected feature subspace, such that centroid proximity correlates with class membership.
- Evidence anchors:
  - [abstract] "Centroids are constructed by computing the mean feature values... ensuring a class-representative division of the feature space"
  - [section] Theorem 2 proves WCSS optimality under Euclidean assignment; Lemma 2 establishes bounded centroid perturbation stability
  - [corpus] Limited corpus support; "Quantum SMOTE" mentions centroid-based synthesis but for augmentation, not partitioning
- Break condition: Fails when class distributions have significant overlap in centroid space, unequal covariance structures, or when Euclidean distance is inappropriate for the data modality (e.g., categorical features without encoding).

### Mechanism 3
- Claim: Ensemble diversity via bootstrap sampling and random feature subsetting reduces overfitting and variance, improving generalization over single trees.
- Mechanism: B bootstrap samples create diverse training sets; at each node, only `mtry` features are considered, with top m selected via CSS. Majority voting across B trees averages out individual tree errors.
- Core assumption: Assumes errors across trees are sufficiently uncorrelated that voting cancels noise while preserving signal.
- Evidence anchors:
  - [abstract] "ensemble learning framework... redefines the splitting strategy"
  - [section] Algorithm 4 describes bootstrapping and aggregation; Section 2.2 explicitly credits diversity and robustness properties
  - [corpus] Strong indirect evidence from random forest literature (Breiman 2001, cited as [26]) establishes bootstrap aggregation benefits
- Break condition: Diversity collapses if CSS consistently selects the same features across bootstrap samples, or if dataset has insufficient samples for effective bootstrapping.

## Foundational Learning

- Concept: **Decision tree splitting criteria (Gini, entropy, variance reduction)**
  - Why needed here: CDF replaces traditional impurity-based thresholding with centroid-based distance routing; understanding baseline methods clarifies what's being modified.
  - Quick check question: Given a binary classification node with 60/40 class mix, what's the Gini impurity, and how would a standard CART split differ from assigning points to centroids?

- Concept: **Euclidean distance geometry and Voronoi tessellations**
  - Why needed here: Centroid-based assignment implicitly constructs Voronoi cells; understanding this geometry helps predict decision boundary shapes.
  - Quick check question: For two centroids at (0,0) and (4,0), sketch the decision boundary and describe what happens to a point at (2, 3).

- Concept: **Ensemble variance-bias tradeoffs and bootstrap aggregation**
  - Why needed here: CDF's performance gains derive from ensemble properties; distinguishing variance reduction from bias reduction is critical for diagnosing failure modes.
  - Quick check question: If adding trees beyond 300 stabilizes accuracy at ~0.97 (Figure 4), what does this suggest about the bias-variance decomposition of CDF's error?

## Architecture Onboarding

- Component map:
  - CSS Scorer -> Feature Selector -> Centroid Builder -> Partition Router -> Tree Aggregator

- Critical path: CSS computation → feature ranking → centroid calculation → distance-based routing → recursive tree growth → ensemble voting

- Design tradeoffs:
  - **mtry vs. m**: Larger `mtry` increases chance of finding discriminative features but raises compute; larger `m` improves centroid representativeness but may include noise
  - **Tree depth (d_max=3)**: Shallow trees reduce overfitting in high-dimensional, low-sample regimes but may underfit complex boundaries
  - **Fixed vs. tuned hyperparameters**: Paper uses fixed settings for simplicity; Section 4.3 suggests tuning can improve performance further

- Failure signatures:
  - Accuracy plateaus or degrades as `m` increases (Figure 6): indicates noisy features diluting centroid quality
  - Low kappa despite moderate accuracy: suggests class imbalance issues not fully addressed
  - High variance across bootstrap iterations: insufficient diversity, possibly due to dominant features consistently selected

- First 3 experiments:
  1. **Baseline replication**: Implement CDF on D8 (Colon) with paper-specified parameters (B=500, d_max=3, mtry=0.2p, m=2×log(p)); verify accuracy ~0.84 per Table 2
  2. **Ablation study**: Replace CSS with random feature selection; compare accuracy and kappa to quantify CSS contribution
  3. **Sensitivity analysis**: Vary m ∈ {1, 3, 5, 10} on D13 (DLBCL) and plot accuracy vs. m; confirm Figure 6-style degradation pattern and identify optimal m

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CDF framework be effectively adapted for mixed-type or categorical data by replacing Euclidean distance with flexible distance metrics?
- Basis in paper: [explicit] The conclusion explicitly identifies a limitation: "Its reliance on Euclidean distance makes it less suitable for categorical or mixed-type data," and calls for future work on "extending CDF to more flexible distance measures."
- Why unresolved: The current mathematical formulation and experiments rely exclusively on Euclidean distance and continuous numerical centroids.
- What evidence would resolve it: A modified CDF implementation utilizing metrics like Gower distance successfully classifying datasets containing categorical variables without one-hot encoding.

### Open Question 2
- Question: How does the computational efficiency of CDF scale in parallel or distributed environments compared to standard ensemble methods on massive datasets?
- Basis in paper: [explicit] The authors state that future work will focus on "improving scalability for very large datasets through parallel and distributed implementations."
- Why unresolved: The current evaluation is limited to datasets with small sample sizes ($n \leq 250$) and does not analyze runtime performance or scaling behavior on big data.
- What evidence would resolve it: Complexity analysis and runtime benchmarks of a parallelized CDF implementation against distributed Random Forest or XGBoost on datasets with millions of samples.

### Open Question 3
- Question: Can hybrid feature selection strategies outperform the standard Class Separability Score (CSS) in scenarios with highly correlated or complex features?
- Basis in paper: [explicit] The conclusion notes that "the current CSS-based feature selection is only one possible choice and may be refined" and suggests "exploring alternative or hybrid feature selection strategies."
- Why unresolved: The study only validates the CSS metric; it does not compare it against information-theoretic measures or wrapper methods within the CDF architecture.
- What evidence would resolve it: Ablation studies comparing CDF accuracy when CSS is swapped for mutual information or recursive feature elimination on datasets with known multicollinearity.

## Limitations

- Generalization beyond high-dimensional, low-sample settings is untested
- Feature scaling necessity is unclear despite negative centroid values suggesting preprocessing
- Fixed hyperparameters may not be optimal for all datasets

## Confidence

- **High**: Classification accuracy superiority (18/23 datasets), mathematical formulation of CSS and centroid partitioning, ensemble diversity benefits
- **Medium**: Generalizability to other domains, necessity of feature scaling, impact of fixed hyperparameters
- **Low**: Handling of categorical features, performance on large-sample datasets, robustness to severe class imbalance beyond reported cases

## Next Checks

1. **Feature scaling ablation**: Run CDF on D8 (Colon) with and without z-score normalization; if accuracy gap exceeds 5%, implement mandatory scaling in reproduction pipeline
2. **Depth sensitivity test**: Increase d_max from 3 to ⌈log₂(K)⌉+1 on D21 (multi-class); monitor per-class recall to detect underfitting of minority classes
3. **Large-sample evaluation**: Apply CDF to a balanced dataset with n > 1000 (e.g., MNIST subset); compare accuracy/kappa to RF to assess scalability limits