---
ver: rpa2
title: Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large
  Language Models
arxiv_id: '2505.02858'
source_url: https://arxiv.org/abs/2505.02858
tags:
- data
- synthetic
- dataset
- posts
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) to generate
  synthetic multi-platform social media datasets that match the quality of real data.
  The authors propose a Multi-Platform Topic Model (MPTM) prompting approach and compare
  it with per-platform prompting across two diverse datasets (US election discussions
  and Dutch influencer content) spanning six platforms.
---

# Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models

## Quick Facts
- **arXiv ID:** 2505.02858
- **Source URL:** https://arxiv.org/abs/2505.02858
- **Reference count:** 40
- **Primary result:** Synthetic datasets generally preserve key social media characteristics with high fidelity, though no prompting strategy consistently outperforms the other.

## Executive Summary
This paper investigates using large language models (LLMs) to generate synthetic multi-platform social media datasets that match the quality of real data. The authors propose a Multi-Platform Topic Model (MPTM) prompting approach and compare it with per-platform prompting across two diverse datasets (US election discussions and Dutch influencer content) spanning six platforms. Results show that synthetic datasets generally preserve key social media characteristics with high fidelity, though no prompting strategy consistently outperforms the other.

## Method Summary
The study uses two real datasets - US 2022 midterm elections posts from Twitter/Facebook/Reddit and Dutch influencer posts from TikTok/Instagram/YouTube. The MPTM approach combines posts across platforms, applies BERTopic with UMAP dimensionality reduction and HDBSCAN clustering to extract topics, creates sample pools, and uses few-shot prompting with 9 examples to generate synthetic posts. The method is compared against per-platform prompting, evaluating fidelity through lexical features, sentiment analysis, topic overlap, embedding similarity, and named entity distributions.

## Key Results
- GPT-4o tends to underestimate hashtag usage but aligns well with real data for mentions and emojis
- Claude-3.5 overestimates negative sentiment while GPT-4o underestimates it
- Topic overlap is stronger in the influencers dataset than the election dataset, with Gemini-2.0 performing best for TikTok and YouTube
- Embedding similarity is higher for election-related topics than influencer topics
- All LLMs better replicate infrequent named entities than frequent ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating synthetic cross-platform social media data may be improved by constraining the latent space of the LLM via topic-aligned sample pools (MPTM), rather than random platform-specific sampling.
- **Mechanism:** The Multi-Platform Topic Model (MPTM) uses BERTopic to cluster real posts by semantic content rather than platform. By populating the few-shot prompt with examples that share a topic but span different platforms, the LLM receives a "semantic anchor." This theoretically guides the model to reproduce the cross-platform narrative coherence often seen in real disinformation or marketing campaigns, rather than generating isolated platform gibberish.
- **Core assumption:** The LLM possesses sufficient cross-domain knowledge to adapt a single semantic topic into distinct platform dialects when explicitly prompted with mixed examples.
- **Evidence anchors:**
  - [abstract]: "We propose a topic-based multi-platform prompting approach... to generate synthetic data from two real datasets... spanning multiple platforms."
  - [section 4.1]: "This approach ensures that we obtain a diverse yet topically coherent sample of posts... [outputs] represent a blend of content from different platforms."
- **Break condition:** If the real datasets contain high topical noise or lack distinct clusters, UMAP/HDBSCAN may force disparate posts into incoherent clusters, leading to sample pools that confuse the LLM and degrade semantic fidelity.

### Mechanism 2
- **Claim:** LLMs likely replicate platform-specific stylistic traits (lexical features) through probabilistic associations learned during pre-training, but struggle with functional elements (URLs) that require grounded external knowledge.
- **Mechanism:** The models associate platform names or contexts with specific token probability distributions. However, this mechanism is surface-level; while the model can predict that a "YouTube" description *should* have links, it cannot generate valid, historical URLs (grounded entities), resulting in hallucinations or omissions.
- **Core assumption:** The pre-training data included a statistically significant representation of each platform's "dialect," allowing the model to toggle styles without fine-tuning.
- **Evidence anchors:**
  - [abstract]: "Results show synthetic datasets preserve key social media features... though none consistently outperform others."
  - [section 5.1]: "...no LLM matches the volume of URLs found in real social media data... [LLMs] underrepresent this characteristic feature."
- **Break condition:** When generating for platforms with highly idiosyncratic constraints not captured in generic pre-training, the LLM may revert to a "generic social media" average style.

### Mechanism 3
- **Claim:** Synthetic data better preserves infrequent named entities compared to frequent ones because the former are generated via in-context adherence to the prompt, while the latter are dominated by the model's parametric memory.
- **Mechanism:** When a rare entity appears in the few-shot prompt, the LLM's attention mechanism focuses on reproducing this specific token to satisfy the prompt's pattern. In contrast, frequent entities activate broad parametric weights, leading the model to generate them based on general probability rather than the specific distribution of the source dataset.
- **Core assumption:** In-context learning exerts stronger control over "rare" tokens than "common" tokens which are deeply embedded in the model's weights.
- **Evidence anchors:**
  - [abstract]: "Named entities in synthetic data better match infrequent real entities."
  - [section 5.5]: "Brighter colors in the bottom-20 [least frequent]... struggle when the common named entities are more frequent."
- **Break condition:** If the prompt context window is too small to include enough rare entities, the model will revert to hallucinating common entities, distorting the fidelity of the long-tail distribution.

## Foundational Learning

- **Concept:** **BERTopic Pipeline (UMAP + HDBSCAN)**
  - **Why needed here:** The paper's core contribution (MPTM) relies on clustering posts to feed the LLM. You must understand how dimensionality reduction (UMAP) and density-based clustering (HDBSCAN) isolate "topics" from raw text to troubleshoot why certain prompts are generated.
  - **Quick check question:** How does HDBSCAN handle noise points in the dataset, and does the MPTM algorithm filter them out before creating the sample pool?

- **Concept:** **Embedding Similarity (Cosine Similarity)**
  - **Why needed here:** The paper evaluates fidelity using OpenAI's `text-embedding-3-large` and cosine similarity. Understanding vector space is required to interpret the "Recall Count" and t-SNE plots used to validate the synthetic data.
  - **Quick check question:** Why might the paper use a threshold of 0.6â€“0.8 for "fairly similar" posts rather than a stricter 0.9 threshold?

- **Concept:** **Few-Shot In-Context Learning (ICL)**
  - **Why needed here:** The generation mechanism relies entirely on feeding 9 examples to the LLM to infer style and content. Understanding the limitations of ICL (recency bias, limited context) is vital for debugging poor generation quality.
  - **Quick check question:** Based on the results, does increasing the number of examples in the prompt seem to help with *factual* accuracy (URLs) or just *stylistic* accuracy?

## Architecture Onboarding

- **Component map:** Raw datasets -> BERTopic (Sentence Transformers -> UMAP -> HDBSCAN) -> Sample Pools -> LLM API (GPT-4o, Gemini 2.0, Claude 3.5) -> Synthetic posts -> Fidelity evaluation
- **Critical path:** The **Topic Modeler -> Sample Pool Selection** is the bottleneck. If the sample pool is not topically coherent, the LLM output will fail the "Topic Overlap" fidelity check regardless of the model used.
- **Design tradeoffs:**
  - **GPT-4o vs. Gemini 2.0:** Gemini-2.0 shows better topic overlap for video platforms (TikTok/YouTube), while GPT-4o aligns better with sentiment and general embedding similarity. Claude-3.5 tends to overestimate negativity and hashtags.
  - **MPTM vs. Per-Platform:** MPTM creates interconnected datasets (better for "coordinated operation" research) but dilutes platform-specific lexical features (averaging behavior). Per-Platform preserves platform quirks but lacks cross-platform semantic links.
- **Failure signatures:**
  - **URL Hallucination:** All models fail to generate valid URLs for YouTube descriptions.
  - **Emoji Divergence:** GPT-4o over-generates emojis in political contexts where they are naturally rare.
  - **Topic Collapse:** Synthetic datasets contain fewer unique topics than real data, particularly in high-diversity datasets like Influencers.
- **First 3 experiments:**
  1. **Topic Sensitivity Analysis:** Run MPTM on the Elections dataset while varying the `min_cluster_size` in HDBSCAN to see if stricter topic definitions improve the semantic similarity scores of the generated synthetic posts.
  2. **Lexical Constraint Prompting:** Modify the prompt to explicitly forbid URLs and cap emoji counts to correct GPT-4o's over-estimation issues.
  3. **Named Entity Weighting:** Create a hybrid dataset where the sample pool is artificially enriched with "Bottom-20" (rare) entities to test if the "Entity Adherence" mechanism can be forced to improve fidelity for mid-frequency entities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can post-processing techniques effectively correct the lexical discrepancies found in LLM-generated data, such as the severe underrepresentation of URLs and inconsistent emoji usage?
- **Basis in paper:** [explicit] The abstract states a "post-processing approach might be needed," and the Conclusion suggests "Improved prompting techniques and post-processing could enhance dataset fidelity" to address failures like URL generation.
- **Why unresolved:** The current study focuses only on "first steps" generation via prompting, identifying fidelity gaps without implementing or testing correction layers.
- **What evidence would resolve it:** A study implementing a post-processing pipeline that aligns lexical feature counts in synthetic data with real data distributions without degrading semantic content.

### Open Question 2
- **Question:** How does the utility of synthetic multi-platform datasets compare to real data when training models for downstream tasks like disinformation or hate speech detection?
- **Basis in paper:** [explicit] Section 6 explicitly states that "measuring task-specific utility of synthetic data is a necessary complementary metric on the path towards reliable adoption," noting it was out of scope for the current work.
- **Why unresolved:** The paper evaluates fidelity (lexical/semantic similarity) but does not validate the data's efficacy in practical applications, such as training classifiers to detect coordinated influence operations.
- **What evidence would resolve it:** Comparative experiments showing classification performance of models trained on synthetic data versus real data for specific social media tasks.

### Open Question 3
- **Question:** What privacy risks persist in LLM-generated synthetic datasets derived from real social media posts, and do they suffice for legal compliance?
- **Basis in paper:** [explicit] Section 6 identifies as a key limitation: "in this work we do not address privacy and legal concerns related to releasing synthetic datasets."
- **Why unresolved:** While the goal is to facilitate sharing without contravening regulations, the paper does not analyze if the LLMs memorize or leak sensitive information from the input prompts.
- **What evidence would resolve it:** An analysis of the synthetic outputs for memorization of unique personally identifiable information (PII) or reconstruction of the original real posts.

## Limitations
- The study lacks hyperparameter details for the core MPTM pipeline, making exact reproduction challenging
- Synthetic datasets show systematic biases: all models underestimate YouTube URLs, GPT-4o overproduces emojis in political contexts
- The mechanism explaining why rare entities are better preserved lacks direct evidence

## Confidence
- **High Confidence:** Platform-specific lexical features are preserved; topic overlap and embedding similarity metrics are robust; named entity fidelity for infrequent entities is well-demonstrated
- **Medium Confidence:** The superiority of MPTM vs. per-platform prompting is context-dependent and not consistently demonstrated; sentiment distribution findings show LLM-specific biases but limited understanding of why
- **Low Confidence:** The mechanism explaining why rare entities are better preserved (in-context learning vs. parametric memory) lacks direct evidence; the claim about MPTM creating "interconnected datasets" for coordinated operation research is not validated with network analysis

## Next Checks
1. **Hyperparameter Sensitivity Test:** Systematically vary BERTopic/UMAP/HDBSCAN parameters and LLM generation settings to determine which components most affect fidelity scores, particularly for topic overlap and URL generation
2. **Cross-Platform Network Analysis:** Apply network centrality measures to the synthetic datasets to quantify "interconnectedness" and test whether MPTM-generated data actually shows stronger cross-platform coordination signals than per-platform approaches
3. **Entity Distribution Stress Test:** Create synthetic datasets with artificially manipulated rare entity frequencies in the sample pool to test whether the "rare entity preservation" mechanism can be extended to mid-frequency entities, or if it's fundamentally limited to the long-tail