---
ver: rpa2
title: 'Erasing with Precision: Evaluating Specific Concept Erasure from Text-to-Image
  Generative Models'
arxiv_id: '2502.13989'
source_url: https://arxiv.org/abs/2502.13989
tags:
- concept
- erasure
- image
- prompt
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of evaluating concept erasure
  methods in text-to-image generative models, which currently lack comprehensive and
  standardized evaluation frameworks. To solve this, the authors propose EraseEval,
  a fundamental evaluation method that assesses concept erasure performance across
  three key criteria: (1) how well prompts containing the target concept are reflected,
  (2) the ability to reduce the impact of erased concepts when related concepts are
  used, and (3) preservation of other unrelated concepts.'
---

# Erasing with Precision: Evaluating Specific Concept Erasure from Text-to-Image Generative Models

## Quick Facts
- arXiv ID: 2502.13989
- Source URL: https://arxiv.org/abs/2502.13989
- Reference count: 40
- This paper proposes EraseEval, a standardized evaluation framework for assessing concept erasure methods in text-to-image generative models across four key metrics.

## Executive Summary
This paper addresses the critical gap in evaluating concept erasure methods for text-to-image generative models, which currently lack standardized assessment frameworks. The authors introduce EraseEval, a comprehensive evaluation method that measures erasure performance across three key criteria: how well erased concepts are suppressed when explicitly prompted, resistance to related or implicitly described prompts, and preservation of unrelated concepts. Using large language models and image encoders, the framework computes four metrics integrated into a single geometric mean score, providing a robust assessment of erasure effectiveness. Experimental results reveal that while most methods successfully preserve unrelated concepts, they often fail to fully erase targets when related concepts are used, highlighting the need for more rigorous evaluation on difficult concepts and improved robustness against prompt rephrasing.

## Method Summary
The paper proposes EraseEval, a standardized evaluation framework for concept erasure methods in text-to-image models. The framework evaluates erasure performance across three criteria: (1) how well prompts containing the target concept are reflected in outputs, (2) ability to reduce impact of erased concepts when related concepts are used, and (3) preservation of other unrelated concepts. The method leverages large language models and image encoders to compute four metrics - caption similarity (M1), image similarity under implicit prompts (M2), CLIP Score preservation (M3), and CMMD preservation (M4) - integrated into a single geometric mean score. The evaluation was applied to 11 erasure methods across 18 concepts spanning objects, artistic styles, copyrighted content, and celebrities using Stable Diffusion 1.4 as the base model.

## Key Results
- Most concept erasure methods successfully preserve unrelated concepts (high M3/M4 scores) but fail to fully erase targets when related or implicitly described prompts are used (low M2 scores)
- Erasure difficulty varies significantly by concept type, with objects like "cat" and "dog" proving more challenging than proper nouns like "Pikachu" or "Donald Trump"
- SDD method shows significantly degraded performance on unrelated concepts (low M3/M4), while other methods maintain general capabilities
- Geometric mean aggregation reveals that partial success in one area cannot compensate for failure in another - any single metric failure severely penalizes overall score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Caption similarity between original and erased model outputs indicates whether the erased concept was replaced semantically versus generating irrelevant content.
- Mechanism: When a prompt containing target concept C is fed to both models, successful erasure should produce images differing only in C-related elements. MLLM-generated captions are embedded via text encoder; cosine similarity measures semantic preservation. A detection model (PaliGemma3) acts as a binary check—if both MLLM and detector confirm C's presence, score is forced to zero.
- Core assumption: MLLMs can reliably describe images and detect concept presence; caption embeddings capture semantic similarity meaningfully.
- Evidence anchors:
  - [abstract] "These criteria are evaluated and integrated into a single metric, such that a lower score is given if any of the evaluations are low"
  - [section 4.1] "M1 = λ cos(TE(cap), TE(cap_C)), where cap = MLLM(f(p)), cap_C = MLLM(f_C(p))"
  - [corpus] Limited corpus validation; neighbor papers focus on erasure methods, not evaluation protocols specifically.

### Mechanism 2
- Claim: Adversarial prompt generation via LLM exposes robustness failures where erased concepts reappear through implicit descriptions.
- Mechanism: An LLM generates prompts that evoke C without naming it (e.g., "swirling starry night" for Van Gogh style). Images from original and erased models are compared via image encoder cosine similarity. Low similarity suggests successful erasure; high similarity indicates concept recovery. This functions as discrete-space black-box adversarial testing.
- Core assumption: LLMs encode sufficient world knowledge to generate semantically related prompts; image embeddings capture style/content similarity perceptually.
- Evidence anchors:
  - [section 4.2] "This p can be interpreted as a discrete-space adversarial attack against black-box text-to-image generative models"
  - [section 3.3] "While the text-to-image model may successfully erase it when explicitly prompted...a closely related concept such as 'Starry Night' could still trigger its reappearance"
  - [corpus] ImplicitBench (Yang et al., 2024) provides pre-generated implicit prompts; EraseEval extends this to arbitrary concepts.

### Mechanism 3
- Claim: Geometric mean enforces strict performance requirements across all criteria—any single failure severely penalizes the composite score.
- Mechanism: Four metrics (M1-M4) are combined via geometric mean: M = (M1 · M2 · M3 · M4)^(1/4). Unlike arithmetic mean, geometric mean is sensitive to any component approaching zero. This reflects the task requirement: successful erasure must satisfy ALL criteria simultaneously.
- Core assumption: All four criteria are equally necessary; partial success in one area cannot compensate for failure in another.
- Evidence anchors:
  - [section 4.4] "The content provided in Sections 4.1-4.3 forms the essential criteria for proper concept erasure, and omitting any of them would hinder its effectiveness"
  - [table 4] SDD scores M = 0.000 because M4 = 0.000, despite reasonable M1/M2 values
  - [corpus] No corpus validation found for this specific aggregation choice.

## Foundational Learning

- Concept: **CLIP embeddings and cosine similarity**
  - Why needed here: Core to all three protocols—comparing text embeddings (M1), image embeddings (M2), and text-image alignment (M3).
  - Quick check question: Given two image embeddings with cosine similarity 0.95, can you predict whether they depict the same object in different styles or different objects entirely?

- Concept: **Black-box vs white-box model access**
  - Why needed here: EraseEval operates in black-box setting (no gradient/model access), constraining evaluation to input-output observations only.
  - Quick check question: If you had white-box access, what additional attack vectors could you use that LLM-generated prompts cannot capture?

- Concept: **Geometric vs arithmetic mean for composite metrics**
  - Why needed here: The paper's choice of geometric mean enforces that all criteria must pass—this has non-obvious implications for score interpretation.
  - Quick check question: Method A scores [0.9, 0.9, 0.9, 0.1]; Method B scores [0.5, 0.5, 0.5, 0.5]. Which has higher geometric mean? Which has higher arithmetic mean?

## Architecture Onboarding

- Component map:
  - Protocol 1 Pipeline: GPT-4o (prompt generation) → Stable Diffusion 1.4 (original + erased) → GPT-4o (captioning) → PaliGemma3 (detection) → ModernBERT-Large (text encoding) → cosine similarity
  - Protocol 2 Pipeline: GPT-4o (implicit prompt generation) → Stable Diffusion 1.4 (original + erased) → EVA02-CLIP (image encoding) → cosine similarity
  - Protocol 3 Pipeline: MSCOCO-1k (random sample) → Stable Diffusion 1.4 (original + erased) → CLIP Score + CMMD calculation
  - Aggregation: Geometric mean of M1, M2, M3, M4

- Critical path: Protocol 2 is the bottleneck—it requires iterative LLM prompt refinement (up to 5 attempts) until the concept triggers in the original model. Failed iterations require regeneration with feedback.

- Design tradeoffs:
  - Sampling 1k from MSCOCO-30k reduces compute but introduces variance; paper shows standard deviation stabilizes after ~50 samples
  - Using GPT-4o for captioning costs ~$30 per full evaluation run; open-weight alternatives (LLaVA, BLIP-3) showed inferior caption quality in Appendix E
  - Binary detection (PaliGemma3) prevents MLLM hallucination from inflating M1 scores, but adds API dependency

- Failure signatures:
  - M1 near 0, M2 high: Concept successfully erased when named, but model generates irrelevant/unrelated content instead of semantically similar alternatives
  - M1 high, M2 near 0: Concept erased when named, but easily recovered via paraphrasing (common for art styles—see Table 5 M2 scores 0.17-0.58)
  - M3/M4 low: Erasure damaged general model capabilities (only SDD showed this pattern significantly)
  - High variance across concepts: Method generalizes poorly; check if results cluster by concept type (common nouns vs proper nouns)

- First 3 experiments:
  1. **Baseline validation:** Run EraseEval on unmodified Stable Diffusion 1.4 erasing "cat" (challenging) vs "Pikachu" (easy) to confirm expected performance ceiling and validate that metrics capture known difficulty differences.
  2. **Protocol 2 ablation:** Manually craft implicit prompts for a known failure case (e.g., Van Gogh → Starry Night) and verify that LLM-generated prompts achieve comparable M2 scores, validating the automated adversarial generation.
  3. **Metric sensitivity test:** Apply random noise to individual metrics (±0.1) and measure impact on final geometric mean score; identify which metric has highest leverage on overall ranking.

## Open Questions the Paper Calls Out

- **How can concept erasure methods be improved to maintain robustness against implicit or rephrased prompts that indirectly reference the erased concept?**
  - Basis in paper: The authors state that "concept erasure can be easily circumvented using simple prompts crafted by humans, making it significantly more vulnerable to attacks than previously assumed" and that "most methods are vulnerable to such rephrasing" as shown by low M2 scores across erasure methods.
  - Why unresolved: Current methods successfully erase concepts when explicitly prompted (high M1 scores) but fail when related concepts are used; the semantic associations between concepts remain embedded in the model.
  - What evidence would resolve it: Development of an erasure method that achieves consistently high M2 scores (e.g., >0.8) across artistic style and object categories, demonstrating resistance to paraphrasing attacks.

- **What underlying factors determine why certain concepts (e.g., "cat," "dog") are more difficult to erase than others (e.g., "Pikachu," proper nouns)?**
  - Basis in paper: The paper explicitly notes that "the difficulty of concept erasure varies by concept, with objects like 'cat' and 'dog' proving more challenging than proper nouns like 'Pikachu' or 'Donald Trump'" and that "high performance on Imagenette does not necessarily indicate strong erasure capabilities."
  - Why unresolved: The paper observes this phenomenon but does not investigate whether it stems from training data frequency, concept specificity, semantic entanglement with related concepts, or architectural factors.
  - What evidence would resolve it: Systematic analysis correlating erasure difficulty with factors such as concept frequency in training data, number of semantic neighbors, and concept abstraction level.

- **How should evaluation frameworks account for multi-concept erasure scenarios where trade-offs between erasing multiple targets may arise?**
  - Basis in paper: The conclusion explicitly states: "Future work will include expanding to multiple concept erasures and adding further evaluation metrics."
  - Why unresolved: Current EraseEval framework evaluates single-concept erasure; real-world applications may require erasing multiple concepts simultaneously (e.g., multiple copyrighted characters or artistic styles), and interference between erasure targets is unexplored.
  - What evidence would resolve it: Extension of EraseEval with protocols for multi-concept scenarios, plus empirical evaluation showing whether current methods maintain performance when applied to multiple concepts.

## Limitations

- Reliance on GPT-4o for both caption generation and implicit prompt creation introduces a single point of failure that may bias evaluation results
- Geometric mean aggregation assumes all four criteria are equally important despite their different operational ranges (M3/M4 near 1.0 vs more variable M1/M2)
- Limited evaluation scope with only 18 concepts raises questions about generalizability to the full space of possible concepts
- Protocol 2's iterative LLM prompt generation depends on the LLM's ability to create effective adversarial prompts, which may not always succeed

## Confidence

**High Confidence:** The finding that most erasure methods preserve unrelated concepts (M3/M4) while struggling with robustness to related prompts (M2) is well-supported by the data and consistent across multiple methods. The experimental methodology for Protocol 1 and 3 is straightforward and replicable.

**Medium Confidence:** The claim that concept erasure difficulty varies systematically by concept type (objects vs proper nouns vs artistic styles) has some support from the results, but the small sample size (18 concepts) limits generalizability. The ranking of specific methods is less reliable due to potential implementation variations and the geometric mean's sensitivity to individual metric failures.

**Low Confidence:** The assertion that EraseEval represents a "fundamental" evaluation method is not well-supported - the paper doesn't compare against alternative evaluation frameworks or demonstrate superiority over existing approaches. The choice of geometric mean over other aggregation methods lacks empirical justification.

## Next Checks

1. **Metric Sensitivity Analysis:** Apply random noise (±0.1) to individual metrics and measure the impact on final geometric mean scores. This will reveal whether the aggregation method is overly sensitive to metrics that naturally operate near 1.0 (M3/M4) versus those with more variable ranges (M1/M2).

2. **Open-Weight LLM Ablation:** Replace GPT-4o with a high-quality open-weight captioning model (e.g., LLaVA-NeXT) for Protocol 1 and compare results. This will determine whether the evaluation is measuring true model capabilities or artifacts of the specific LLM used.

3. **Cross-Concept Generalization Test:** Extend the evaluation to a broader set of 50+ concepts spanning diverse categories (not just the current 18). Analyze whether the observed patterns (difficulty by concept type, method rankings) hold across a more representative sample of the concept space.