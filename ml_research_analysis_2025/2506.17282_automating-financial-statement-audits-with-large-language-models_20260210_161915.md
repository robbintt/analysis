---
ver: rpa2
title: Automating Financial Statement Audits with Large Language Models
arxiv_id: '2506.17282'
source_url: https://arxiv.org/abs/2506.17282
tags:
- financial
- assets
- liabilities
- total
- current
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the inefficiency and inaccuracy of manual financial
  statement auditing by developing a comprehensive benchmark and evaluation framework
  to assess the capabilities of large language models (LLMs) in automating the auditing
  process. The authors create a dataset combining real-world financial tables from
  S&P 500 companies with synthetic transaction data, injecting controlled errors such
  as missing rows, numerical mistakes, redundant entries, and misclassifications.
---

# Automating Financial Statement Audits with Large Language Models

## Quick Facts
- **arXiv ID:** 2506.17282
- **Source URL:** https://arxiv.org/abs/2506.17282
- **Reference count:** 21
- **Primary result:** While state-of-the-art LLMs can reliably identify financial statement errors when given transaction data, they struggle with explaining errors, citing accounting standards, and making accurate revisions—especially for multiple-error cases.

## Executive Summary
This paper introduces a comprehensive benchmark and evaluation framework for assessing LLMs' capabilities in automating financial statement audits. The authors construct a dataset combining real S&P 500 financial tables with synthetic transaction data, injecting controlled errors such as missing rows, numerical mistakes, redundant entries, and misclassifications. A five-stage evaluation framework systematically assesses LLMs on general judgment, error identification, error resolution, standards citation, and financial statement revision. Experimental results show that while LLMs like GPT-3.5-turbo and GPT-4 can effectively identify misaligned entries, they struggle with explaining errors, citing accounting standards, and making accurate revisions—especially in tables with multiple errors. The overall success rate for complete audits remains low (below 5% for GPT-3.5-turbo, under 3% for multiple-error cases), highlighting a critical gap in domain-specific accounting knowledge.

## Method Summary
The authors developed a five-stage evaluation framework to assess LLMs' auditing capabilities, requiring them to analyze financial statements against historical transaction data. The benchmark combines 371 structured text tables extracted from S&P 500 company filings with synthetic transaction data generated via GPT-4. Controlled errors are injected into the tables, creating 1,484 single-error and 371 multiple-error cases across four error types: missing rows, numerical errors, redundant rows, and misclassifications. The evaluation measures performance at each stage using metrics including Exact Match (EM), BertScore, and BLEU scores, with overall success requiring strict thresholds across all stages. The framework also incorporates a curated knowledge base of accounting standards for the citation stage.

## Key Results
- LLMs achieve 100% EM score on general judgment for single-error statements but only 48-75% for identifying error types and entry locations
- Standards citation performance remains poor with top-1 EM scores of 13.7% (GPT-3.5-turbo) and 26.2% (GPT-4)
- Overall audit success rate drops below 5% for single errors and under 3% for multiple-error cases
- Multiple concurrent errors significantly degrade performance, suggesting error interference or state confusion

## Why This Works (Mechanism)

### Mechanism 1: Transaction-to-Statement Alignment Detection
- Claim: LLMs can reliably detect misalignments between historical transaction records and financial statements when both are provided as structured inputs.
- Mechanism: The model cross-references line items in the financial statement against supporting transaction narratives, flagging discrepancies through pattern matching and numerical comparison. This leverages the model's pre-trained capacity for comparing structured and semi-structured text.
- Core assumption: Transaction data accurately reflects ground truth and the model can parse both formats into comparable representations.
- Evidence anchors:
  - [abstract] "current state-of-the-art LLMs successfully identify financial statement errors when given historical transaction data"
  - [Page 4, Table 2] GPT-3.5-Turbo and GPT-4 both achieve 1.000 EM Score on General Judgment for single-error statements
  - [corpus] FinMaster benchmark confirms LLM effectiveness on structured financial workflow tasks with similar cross-referencing requirements
- Break condition: Performance degrades with multiple concurrent errors (EM Score drops from 1.000 to 0.482-0.752 for error type identification) or when transaction volume exceeds context window capacity.

### Mechanism 2: Domain-Specific Knowledge Grounding via External Standards Retrieval
- Claim: Providing LLMs with curated accounting standards as external memory enables citation of relevant regulatory guidance, though current retrieval-to-generation pipelines remain unreliable.
- Mechanism: A retriever extracts relevant FASB standards from a pre-built knowledge base based on identified error types; the LLM then incorporates these citations into its audit output. The paper uses professionally summarized standards rather than raw regulatory text to improve comprehension.
- Core assumption: Error type identification is accurate enough to retrieve relevant standards, and the model can map abstract standards to concrete errors.
- Evidence anchors:
  - [Page 2] "we also provide a curated knowledge base of accounting standards, serving as the agent's external memory. We have professional accounting practitioners manually summarize the accounting standards"
  - [Page 4, Table 2] Standards Citation Top-1 EM Score: 0.137 (GPT-3.5-Turbo), 0.262 (GPT-4)—indicating mechanism failure
  - [corpus] FinAuditing benchmark similarly notes LLM struggles with hierarchical XBRL/GAAP structures requiring domain grounding
- Break condition: If error type identification fails (particularly for misclassification vs. missing row distinctions), retrieved standards will be irrelevant. The 26% top-1 citation accuracy suggests this pipeline is currently broken.

### Mechanism 3: Cascaded Multi-Stage Audit Reasoning
- Claim: The five-stage framework (judgment → identification → resolution → citation → revision) creates dependencies where early-stage errors compound, making end-to-end success rare.
- Mechanism: Each stage produces outputs consumed by subsequent stages. Error type and entry identification feed into resolution explanations, which inform standards retrieval, which guides table revision. The model must maintain coherent state across all stages.
- Core assumption: Stage outputs are sufficiently accurate that errors do not cascade catastrophically.
- Evidence anchors:
  - [Page 2] "we developed a rigorous five-stage evaluation framework to assess LLMs' auditing capabilities"
  - [Page 3] "We apply strict requirements to the output. We require all the EM-Score to be 1, the BertScore to be over 0.85, and the BLEU score to be over 0.99 to obtain a complete successful auditing"
  - [Page 4, Table 2] Overall success rate: 0.025 (GPT-3.5-Turbo), 0.041 (GPT-4) for single errors; drops to 0.012-0.030 for multiple errors
- Break condition: Any upstream error in identification (0.418-0.737 EM for entry location) propagates through all downstream stages, making the strict success criteria nearly impossible to satisfy.

## Foundational Learning

- Concept: **Financial Statement Structure and Interdependencies**
  - Why needed here: Balance sheets, income statements, and cash flow statements have internal arithmetic constraints (Assets = Liabilities + Equity). Correcting a misclassification requires recalculating multiple dependent totals.
  - Quick check question: If "Accounts Receivable" is moved from current assets to non-current assets, which three line items must be recalculated?

- Concept: **Error Taxonomy for Audit Testing**
  - Why needed here: The benchmark defines four specific error types (missing row, numerical error, redundant row, misclassification). Accurate type classification is a prerequisite for correct resolution and citation.
  - Quick check question: What distinguishes a "redundant row" error from a "misclassification" error in the balance sheet context?

- Concept: **FASB Standards Hierarchy and Citation Format**
  - Why needed here: Standards citation requires mapping errors to specific ASC references (e.g., "FASB ASC 210-10-45-1"). Models must navigate this hierarchy accurately.
  - Quick check question: What FASB ASC topic would govern the classification of inventory as a current asset?

## Architecture Onboarding

- Component map:
Input: [Transaction Data] + [Financial Statement] + [Standards Knowledge Base]
        ↓
Stage 1: General Judgment (binary: correct/incorrect)
        ↓
Stage 2: Error Identification (type classification + row location)
        ↓
Stage 3: Error Resolution (natural language explanation)
        ↓
Stage 4: Standards Citation (retrieve from KB, match to error)
        ↓
Stage 5: Financial Statement Revision (corrected table output)
        ↓
Evaluation: EM Score (stages 1-2, 4), BertScore (stage 3), BLEU (stage 5), Success Rate (all combined)

- Critical path: Stage 2 (Error Identification) is the bottleneck—errors in type classification or row location invalidate all downstream stages. Current EM Scores of 0.418-0.737 for entry identification make the pipeline unreliable.

- Design tradeoffs:
  - Synthetic vs. real transaction data: Authors use synthetic due to confidentiality constraints, limiting realism and scalability
  - Summarized vs. raw standards: Summarized improves comprehension but may lose regulatory precision
  - Strict vs. relaxed success criteria: Authors apply strict thresholds (BLEU > 0.99, BertScore > 0.85), yielding <5% success but clearer diagnostic signal

- Failure signatures:
  - High judgment accuracy (100%) but low revision accuracy (<80% BLEU): Indicates detection works but correction fails
  - Standards citation EM Score of ~0.15-0.26: Model retrieves wrong standards despite error identification
  - Multiple-error performance drop (success rate halves): Suggests error interference or state confusion

- First 3 experiments:
  1. **Ablate by error type**: Run the pipeline separately for each error type (missing row, numerical, redundant, misclassification) to identify which error categories cause the most downstream failures. This isolates the weak links in error identification.
  2. **Test retrieval quality independently**: Evaluate the standards retriever in isolation by providing ground-truth error types and measuring top-k retrieval accuracy. If retrieval is already poor, the problem is in the KB/index; if retrieval is good but citation fails, the problem is in the LLM's mapping ability.
  3. **Single-stage vs. end-to-end evaluation**: Provide ground-truth outputs for stages 1-4 and evaluate only stage 5 (revision) to determine whether table revision is inherently difficult or merely appears difficult due to upstream error propagation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced prompting frameworks, such as multi-interaction or code-based agents, improve the auditing success rates of LLMs?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "It is worth exploring whether there prompting frameworks will bring extra benefit to the task."
- Why unresolved: The experiments relied on standard prompting techniques; agentic or code-execution workflows were not evaluated.
- What evidence would resolve it: A comparative study measuring the Success Rate (SR) of standard GPT-4 against agent-based implementations on the same benchmark.

### Open Question 2
- Question: Does continuing pre-training or fine-tuning LLMs on unstructured accounting text bridge the gap in domain-specific knowledge?
- Basis in paper: [explicit] The authors suggest, "We would further consider updating or continuing pretraining LLMs... to enhance its domain knowledge in accounting and finance."
- Why unresolved: The study found current LLMs struggle with citing standards (low Top-1 EM scores) due to a lack of domain grounding, but no domain-specific models were tested.
- What evidence would resolve it: Evaluating a finance-adapted LLM on the "Standards Citation" stage to see if retrieval accuracy exceeds the baseline 26.2% (GPT-4).

### Open Question 3
- Question: Can LLMs generalize their auditing capabilities to complex report types beyond primary financial statements?
- Basis in paper: [explicit] The authors note a narrow focus and ask: "Future research should investigate the capability of LLMs to handle these more diverse and complex auditing challenges [e.g., budget variance, segment disclosures]."
- Why unresolved: The current benchmark is limited to balance sheets, income statements, and cash flows; inter-company adjustments and managerial estimates were excluded.
- What evidence would resolve it: Extending the dataset to include complex disclosures and measuring model performance on identifying errors in these nuanced reports.

## Limitations

- The benchmark relies on synthetic transaction data due to confidentiality constraints, limiting realism and scalability to real-world audit scenarios
- Extremely strict success criteria (BLEU > 0.99, BertScore > 0.85) may be unrealistically stringent for financial table revision tasks
- Current performance gaps are attributed to LLM limitations rather than benchmark design choices, though this attribution is uncertain

## Confidence

- **High confidence**: The framework's systematic approach and the observation that LLMs can reliably detect misalignments between statements and transactions when both are provided.
- **Medium confidence**: The quantitative performance gaps reported, though the absolute numbers may be inflated by synthetic data properties and evaluation strictness.
- **Low confidence**: The attribution of failure modes to specific LLM limitations rather than benchmark design choices, particularly for the extremely low success rates.

## Next Checks

1. Test whether relaxing the success criteria to allow BLEU scores > 0.95 (rather than > 0.99) substantially increases measured success rates, distinguishing between genuine capability gaps and evaluation stringency.
2. Compare performance on synthetic vs. a small set of manually curated real transaction data to quantify the impact of data distribution differences on audit accuracy.
3. Conduct ablation studies where ground-truth error types are provided to the model to isolate whether failures in downstream stages are caused by error identification errors or inherent difficulties in resolution and revision.