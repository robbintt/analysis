---
ver: rpa2
title: The impact of fine tuning in LLaMA on hallucinations for named entity extraction
  in legal documentation
arxiv_id: '2506.08827'
source_url: https://arxiv.org/abs/2506.08827
tags:
- extraction
- entities
- text
- entity
- disability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of extracting information about
  traffic accidents from legal documents, focusing on identifying disability percentages
  and compensation amounts. A two-step method is proposed: first, segmenting documents
  using either regular expressions or vector-based semantic search; second, applying
  large language models (LLaMA-2, LLaMA-3, and GPT-4 Turbo) to extract entities from
  relevant segments.'
---

# The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation

## Quick Facts
- arXiv ID: 2506.08827
- Source URL: https://arxiv.org/abs/2506.08827
- Reference count: 20
- Primary result: Fine-tuning reduces hallucinations by 47.78% in LLaMA-2 7B; vector-based segmentation achieves 83.33% accuracy vs. 39.5% for regex

## Executive Summary
This paper addresses named entity extraction from Spanish legal documents, focusing on disability percentages and compensation amounts in traffic accident rulings. The proposed two-stage method combines semantic segmentation with LLM-based extraction, achieving 79.4% accuracy with fine-tuned LLaMA-2 70B. The study demonstrates that fine-tuning substantially reduces hallucinations, particularly in smaller models, and that dataset quality (removing samples without entities in segments) is more important than dataset size.

## Method Summary
The method uses a two-stage pipeline: first, documents are segmented into 120-token blocks and vectorized using MiniLM-L12-v2 embeddings with FAISS indexing, then expanded to 360-token context windows. Second, LLaMA-2/3 or GPT-4 Turbo models extract disability percentages and compensation amounts. Fine-tuning uses QLoRA with 8-bit quantization, rank-8 LoRA adapters, and attention-layer training. Dataset 1 contains 1120 samples; Dataset 2 (cleaned) has 861 samples with verified entity presence. Evaluation uses 30 documents (120 samples) with manual verification.

## Key Results
- Semantic segmentation achieves 83.33% accuracy vs. 39.5% for regex-based extraction
- LLaMA-2 70B fine-tuned achieves 79.4% extraction accuracy
- Fine-tuning reduces hallucinations by 47.78% in LLaMA-2 7B
- Base LLaMA-3 8B (76.6%) performs comparably to fine-tuned LLaMA-2 70B (79.4%)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with domain-specific data substantially reduces hallucinations in smaller LLMs during named entity extraction. LoRA-based fine-tuning on attention layers aligns generation patterns with the extraction task, reducing the tendency to generate plausible-but-incorrect entities when information is absent from context. Core assumption: Hallucinations stem from task misalignment rather than fundamental model architecture limitations. Evidence: LLaMA-2 7b shows 47.78% improvement after fine-tuning; related work on hallucination reduction via fine-tuning supports this pattern.

### Mechanism 2
Vector-based semantic segmentation retrieves more relevant document contexts than keyword-based regex for long legal documents. Embedding models create dense representations of text blocks; cosine similarity search against task-specific queries retrieves segments based on semantic relevance rather than exact keyword matches, capturing context that regex misses. Core assumption: The embedding model captures task-relevant semantics across languages (Spanish legal text). Evidence: Segmentation QA shows 83.33% accuracy with MiniLM-L12-v2 vs. regex-based extraction at 39.5%; RAG-based NER approaches show similar improvements.

### Mechanism 3
Dataset quality (noise reduction) has greater impact on fine-tuning effectiveness than dataset size alone. Removing samples where entities are not present in retrieved segments reduces contradictory training signals, leading to better generalization despite fewer samples. Core assumption: Label noise from imperfect segment-entity alignment actively harms learning. Evidence: 9.4% accuracy gain from Dataset 1 to Dataset 2 despite 23% fewer samples; Dataset 2 significantly outperforms Dataset 1.

## Foundational Learning

- **LoRA/QLoRA fine-tuning**: Why needed: Full fine-tuning of 70B models requires >96GB VRAM; QLoRA with 8-bit quantization enables training within hardware constraints. Quick check: Can you explain why LoRA reduces memory requirements while preserving model quality?

- **RAG (Retrieval-Augmented Generation)**: Why needed: Legal documents exceed model context windows; segmentation + retrieval provides relevant context for extraction. Quick check: How does chunk size affect retrieval quality vs. context completeness?

- **Hallucination detection via token probabilities**: Why needed: Paper tested minimum probability threshold approach but found it insufficient for entity extraction tasks. Quick check: Why might high-confidence tokens still correspond to incorrect extractions?

## Architecture Onboarding

- **Component map**: PDF extraction → text cleaning → header filtering → token blocking (120 tokens) → embedding (MiniLM-L12-v2 or ada-002) → FAISS indexing → query-based retrieval → context expansion (360 tokens) → extraction LLM → structured output parsing

- **Critical path**: Segmentation quality determines extraction ceiling (83% segmentation accuracy limits overall pipeline); fine-tuning dataset quality > quantity (Dataset 2 outperformed Dataset 1 with 23% fewer samples); model scale vs. generation trade-off (LLaMA-3 8B base ≈ LLaMA-2 70B fine-tuned)

- **Design tradeoffs**: Open-source vs. proprietary (LLaMA-2 70B fine-tuned 79.4% vs. GPT-4 Turbo 86.1%—7% accuracy gap for data sovereignty); segment length (120 tokens for embedding vs. 360 after expansion—longer context improves disambiguation but increases compute); base vs. fine-tuned (for LLaMA-3, base model may be sufficient; for LLaMA-2 7B, fine-tuning is essential)

- **Failure signatures**: Low recall with high accuracy (model being conservative, refusing to extract when uncertain); hallucinations spike on out-of-distribution segments (base LLaMA-2 7B hallucinates 47% more than fine-tuned version); segmentation misses relevant sections (regex approach misses 60% of entities vs. semantic search)

- **First 3 experiments**: Baseline segmentation comparison (implement both regex and semantic search on 30 documents; measure segment retrieval accuracy before any LLM extraction); hallucination quantification (run base LLaMA-2 7B on segments known to lack target entities; count false extractions at temperature=0); dataset cleaning ablation (fine-tune on Dataset 1 vs. Dataset 2; isolate the impact of sample quality vs. quantity on validation accuracy)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can hallucinations be reliably detected in legal named entity extraction tasks when token probability thresholds prove insufficient? Basis: Section 3.3 concludes that using minimum token probability thresholds was "insufficient to detect incorrect extractions" because models can be confident but wrong. Why unresolved: The authors evaluated a specific mathematical approach (minimum probability threshold) based on prior literature, but found no pattern distinguishing correct extractions from hallucinations in this specific domain. What evidence would resolve it: An alternative detection metric or decoding strategy that demonstrates a statistically significant correlation with factual errors in the extraction output.

- **Open Question 2**: Can the proposed extraction methodology effectively scale to detect jurisdictional anomalies or individual judge-level biases in ruling patterns? Basis: The Conclusion states the procedure "could be extended to the detection of jurisdictional anomalies or at the level of individual judges" regarding point values and disability percentages. Why unresolved: The current study focused on aggregate accuracy and macro-level statistics rather than granular anomaly detection across different legal jurisdictions. What evidence would resolve it: A follow-up study applying the model to a multi-jurisdictional dataset to validate detected outliers against known judicial biases or procedural anomalies.

- **Open Question 3**: How does the performance of fine-tuned generative LLMs compare to classical discriminative models like BERT for legal named entity recognition? Basis: The Conclusion notes the analysis "did not include the performance difference between large language models (LLMs) and more classical language models like BERT." Why unresolved: The study focused entirely on generative models (LLaMA, GPT) versus regular expressions, leaving the comparative efficiency and accuracy of encoder-based architectures unexplored. What evidence would resolve it: A benchmark comparison on the same legal corpus between the LLaMA/GPT pipeline and a fine-tuned BERT-based NER model.

## Limitations

- Dataset size is relatively small (1120 training samples, 120 test samples), potentially limiting generalizability to diverse legal document structures
- Spanish language focus restricts applicability to other jurisdictions and legal systems
- Manual verification process for entity presence in segments introduces potential subjectivity despite reviewer checks

## Confidence

- **High Confidence**: Semantic segmentation effectiveness (39.5% vs. 83.33% segmentation accuracy); hallucination reduction through fine-tuning (47.78% improvement for LLaMA-2 7B)
- **Medium Confidence**: Dataset quality vs. quantity claim (9.4% accuracy improvement with 23% fewer samples, but lacks controlled experiments)
- **Low Confidence**: Base LLaMA-3 8B vs. fine-tuned LLaMA-2 70B performance comparison may be model-specific and task-dependent

## Next Checks

1. **Dataset Quality vs. Quantity Experiment**: Create two additional datasets—one with Dataset 2's quality but Dataset 1's quantity, and vice versa—to isolate whether sample quality or quantity drives the observed improvements.

2. **Cross-Domain Generalization Test**: Apply the fine-tuned LLaMA-2 70B model to a different legal domain (e.g., contract analysis or criminal rulings) to assess whether hallucination reduction generalizes beyond traffic accident documentation.

3. **Human Evaluation of Hallucinations**: Conduct a blind human evaluation where annotators classify extractions as correct, hallucinated, or ambiguous, comparing base vs. fine-tuned models on segments known to lack entities to quantify the actual hallucination reduction.