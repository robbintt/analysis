---
ver: rpa2
title: 'MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating
  LLMs and VLMs'
arxiv_id: '2505.24423'
source_url: https://arxiv.org/abs/2505.24423
tags:
- emotion
- text
- video
- image
- affective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMAFFBen introduces the first comprehensive multilingual and multimodal
  affective analysis benchmark for evaluating large language and vision-language models
  across 35 languages and three modalities (text, image, video). The benchmark includes
  four key affective analysis tasks: sentiment polarity, sentiment intensity, emotion
  classification, and emotion intensity, covering 14 diverse datasets.'
---

# MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs

## Quick Facts
- arXiv ID: 2505.24423
- Source URL: https://arxiv.org/abs/2505.24423
- Reference count: 40
- Primary result: First comprehensive multilingual and multimodal affective analysis benchmark covering 35 languages and three modalities (text, image, video)

## Executive Summary
MMAFFBen introduces a novel benchmark for evaluating large language and vision-language models on multilingual multimodal affective analysis tasks. The benchmark covers 35 languages and three modalities (text, image, video) across four key tasks: sentiment polarity, sentiment intensity, emotion classification, and emotion intensity. The authors construct MMAFFIn, an instruction-tuning dataset, and develop specialized models (MMAFFLM-3B and MMAFFLM-7B) that demonstrate the effectiveness of task-specific fine-tuning for affective analysis. Systematic evaluation of 20 representative models reveals that GPT-4o-mini achieves the best overall performance without fine-tuning, while MMAFFLM-7B excels among fine-tuned models.

## Method Summary
The benchmark comprises 14 diverse datasets covering sentiment and emotion analysis tasks across 35 languages and three modalities. The authors construct MMAFFIn by reformatting and translating existing datasets into a unified instruction format. Two specialized models (MMAFFLM-3B and MMAFFLM-7B) are developed using Qwen2.5-VL as the base model and fine-tuned using the LLaMA-Factory framework. The instruction-tuning process involves streaming data processing, batch size of 256, learning rate of 5e-6, and one training epoch. Evaluation metrics include accuracy, F1-score, and Pearson/Spearman correlation coefficients depending on the task type.

## Key Results
- GPT-4o-mini achieves the best overall performance without fine-tuning, demonstrating strong zero-shot capabilities
- MMAFFLM-7B outperforms all fine-tuned models, validating the effectiveness of instruction-tuning for affective analysis
- LM-TIV models (supporting text, image, and video) outperform LM-TI models on image-based tasks, indicating transfer benefits from video training
- Model size significantly impacts performance, with 7B models consistently outperforming 3B counterparts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training vision-language models on video data enhances performance on static image-based affective tasks.
- **Mechanism:** Video data provides temporal dynamics and context that regularize the visual encoder, improving the model's ability to extract affective features from static frames.
- **Core assumption:** Visual features learned from temporal video streams transfer positively to spatial-only image analysis tasks.
- **Evidence anchors:**
  - [section 5.2] "LM-TI models underperform compared to LM-TIV models, indicating that the integration of video data enhances sentiment analysis performance even on static image tasks."
  - [table 5] Shows LM-TIV models (e.g., InternVL2.5-8B) generally outperforming LM-TI models (e.g., Llama3.2-11b) on EMOTIC and FER2013 image datasets.

### Mechanism 2
- **Claim:** Task-specific instruction tuning effectively aligns generalist Large Models to the nuanced output formats required for affective analysis.
- **Mechanism:** By constructing MMAFFIn with specific templates, the model maps latent affective features to precise numerical ranges or discrete sets, overriding general conversational priors.
- **Core assumption:** The base model possesses sufficient latent capability to understand emotion, requiring only alignment rather than capability injection.
- **Evidence anchors:**
  - [abstract] "instruction-tuning remains an effective strategy for improving affective analysis performance."
  - [table 4 & 5] MMAFFLM-7B consistently outperforms its base model and other larger non-fine-tuned models across SemEval and video tasks.

### Mechanism 3
- **Claim:** Performance in multilingual affective analysis is significantly constrained by the English-centric bias of pre-training corpora.
- **Mechanism:** Models fine-tuned only on English data fail on Arabic/Spanish tasks due to poor mapping between multilingual tokens and emotion concepts.
- **Core assumption:** The "meaning" of emotions is relatively universal, but the "lexical keys" to access it differ by language.
- **Evidence anchors:**
  - [section 5.1] "EmoLlama-chat-7B... maintains relatively strong performance in Spanish, its accuracy drops significantly in Arabic... likely due to being fine-tuned... only [on] English data."
  - [appendix] Tables 6, 7, 8 show MMAFFLM models outperforming base models on diverse languages.

## Foundational Learning

- **Concept:** **Affective Dimensions (Valence, Arousal, Dominance) vs. Discrete Categories**
  - **Why needed here:** MMAFFBen evaluates both continuous dimensions and discrete categories. Confusing these leads to comparing apples to oranges (Classification Accuracy vs. Pearson Correlation).
  - **Quick check question:** Does the task require a Yes/No/Maybe (Discrete) or a 0.0-1.0 slider (Continuous/Intensity)?

- **Concept:** **Zero-Shot Evaluation vs. Fine-Tuning**
  - **Why needed here:** The paper distinguishes between "general capability" (GPT-4o-mini zero-shot) and "specialized capability" (MMAFFLM fine-tuned).
  - **Quick check question:** Is the model applying its pre-existing world knowledge, or has it memorized the specific answer pattern of this dataset?

- **Concept:** **Modality Gap**
  - **Why needed here:** Supporting Video (LM-TV or LM-TIV) adds complexity. A learner needs to know that a model supporting 3 modalities isn't just "better"â€”it requires a complex architecture.
  - **Quick check question:** Can this model read a silent movie, or does it need the transcript (Text) alongside the frames?

## Architecture Onboarding

- **Component map:** Tokenizer for text + Visual Encoder (ViT) for images/frames -> Projector -> LLM Backbone (Qwen2.5-VL-7B) -> Output: Regression head (for intensity) or Vocabulary Head (for classification labels)

- **Critical path:**
  1. **Data Normalization:** Standardize intensity ranges to [-1, 1] or [0, 1] (See Section 2.2/2.3).
  2. **Prompt Construction:** Format inputs strictly using the provided templates (Table 9).
  3. **Training:** Use streaming strategy, batch size 256, LR 5e-6, 1 epoch (Section 3).

- **Design tradeoffs:**
  - **Generalist (GPT-4o-mini) vs. Specialist (MMAFFLM):** The Specialist wins on benchmarks but loses general chat capability. The Generalist wins on convenience but struggles with fine-grained intensity.
  - **Model Size (3B vs 7B):** Section 5.2 notes "increasing model size leads to significant performance gains." 3B is cheaper but hallucinates more on intensity tasks.

- **Failure signatures:**
  - **Intensity Drift:** The model outputs "0.5" for everything (mean regression) because the loss is dominated by neutral examples.
  - **Language Collapse:** The model generates English responses for Arabic inputs if the instruction tuning didn't balance language ratios.
  - **Modality Ignorance:** The model ignores the image/video and answers based solely on the text transcript (Text bias).

- **First 3 experiments:**
  1. **Baseline Validation:** Run GPT-4o-mini (zero-shot) vs. Qwen2.5-VL (zero-shot) on the MMAFFBen validation set to establish the "capability gap."
  2. **Overfit Test:** Fine-tune MMAFFLM-7B on *only* the SemEval text data and evaluate on Video tasks to verify if the mechanism claims hold.
  3. **Intensity Sanity Check:** Visualize the distribution of predicted intensity scores vs. ground truth for the CFAPS dataset to ensure the model isn't just classifying binary sentiment.

## Open Questions the Paper Calls Out
None

## Limitations

- **Multimodal Modality Coverage**: The benchmark evaluates models on text, image, and video tasks separately, but does not provide truly integrated multimodal samples where all three modalities coexist simultaneously.
- **Language Representation Bias**: Despite covering 35 languages, the distribution is heavily skewed toward high-resource languages (English, Spanish, Arabic, Chinese).
- **Cultural Context Limitation**: While the benchmark covers multiple languages, it does not systematically account for cultural differences in emotional expression and interpretation.

## Confidence

- **High Confidence**: Mechanism 1 - Video-to-Image Transfer (robust empirical evidence with consistent patterns across multiple datasets)
- **Medium Confidence**: Mechanism 2 - Instruction Tuning Effectiveness (performance improvements clear but specific contribution not isolated)
- **Medium Confidence**: Mechanism 3 - Multilingual Performance Constraints (strong evidence for specific language pairs but broader claims need more systematic analysis)
- **Low Confidence**: Generalization Claims (claims about "unseen" languages extend beyond what the benchmark can validate)

## Next Checks

1. **Cross-Modal Transfer Experiment**: Evaluate whether models fine-tuned on video data show systematic transfer benefits to image tasks beyond what is observed in the current results through controlled ablation studies.

2. **Cultural Context Validation**: Conduct a systematic study where the same affective content is presented to native speakers from different cultural backgrounds to establish ground truth affective labels that account for cultural variation.

3. **Low-Resource Language Performance Analysis**: Select 3-4 low-resource languages from the benchmark and conduct detailed error analysis to understand whether performance limitations stem from data quantity, cultural mismatch, or fundamental architectural constraints.