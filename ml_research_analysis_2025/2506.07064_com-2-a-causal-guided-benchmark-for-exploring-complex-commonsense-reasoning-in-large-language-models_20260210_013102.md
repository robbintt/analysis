---
ver: rpa2
title: 'Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning
  in Large Language Models'
arxiv_id: '2506.07064'
source_url: https://arxiv.org/abs/2506.07064
tags:
- causal
- question
- reasoning
- llms
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Com2, a benchmark designed to evaluate complex
  commonsense reasoning in large language models (LLMs). Unlike existing benchmarks
  focused on math and code, Com2 uses causal event graphs and causal theory to generate
  diverse reasoning scenarios, including direct, decision, transition, intervention,
  and counterfactual tasks.
---

# Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2506.07064
- **Source URL**: https://arxiv.org/abs/2506.07064
- **Reference count**: 40
- **Primary result**: Introduces Com² benchmark with 5 task types to evaluate complex commonsense reasoning in LLMs, revealing models struggle with reasoning depth despite strong performance on simpler tasks

## Executive Summary
This paper introduces Com², a novel benchmark designed to evaluate complex commonsense reasoning in large language models. Unlike existing benchmarks focused on math and code, Com² uses causal event graphs and causal theory to generate diverse reasoning scenarios including direct, decision, transition, intervention, and counterfactual tasks. The benchmark includes both a main set of 2,500 examples and a challenging subset based on detective stories. Experiments show that while LLMs perform reasonably on simpler tasks, they struggle with reasoning depth and breadth, particularly in handling sudden events or uncommon scenarios. Human evaluation confirms the quality of the benchmark, with high agreement scores.

## Method Summary
The Com² benchmark is constructed using causal event graphs (CEGs) to generate reasoning scenarios across five task types: Direct (simple inference), Decision (choice between options), Transition (reasoning through unexpected events), Intervention (identifying causes and effects of interventions), and Counterfactual (reasoning about alternative scenarios). The benchmark uses gpt-4o-mini API to synthesize examples from seed events in existing causal event graphs, with 2,500 examples in the main set and 1,254 in a detective-story-based hard subset. The paper evaluates various models including general LLMs (Qwen2, LLaMA-3.1, Gemma2) and reasoning-focused models (Open-O1, Marco-o1) using zero-shot chain-of-thought prompting. Training experiments fine-tune models on a 8,386-example dataset using full parameter fine-tuning with batch size 32, learning rate 1e-5, and 3 epochs.

## Key Results
- LLMs show significant performance gaps on complex reasoning tasks, particularly in Transition and Intervention categories requiring long causal dependencies
- Reasoning models (Open-O1, Marco-o1) outperform general LLMs on Com²-hard but underperform on Com²-main, suggesting overthinking
- Test-time scaling laws effective for math/code fail for commonsense reasoning - simply increasing token output doesn't improve performance
- Post-training and slow thinking techniques can alleviate some limitations but don't fully close the reasoning gap

## Why This Works (Mechanism)
The Com² benchmark leverages causal event graphs to systematically generate complex reasoning scenarios that require models to understand not just surface-level patterns but genuine causal relationships. By structuring questions around five distinct reasoning types (Direct, Decision, Transition, Intervention, Counterfactual), the benchmark captures different aspects of commonsense reasoning complexity. The use of both general scenarios and detective story-based hard examples provides a spectrum of difficulty levels. The causal theory foundation ensures that reasoning requirements are grounded in realistic cause-effect relationships, making the benchmark more representative of real-world commonsense challenges.

## Foundational Learning
- **Causal Event Graphs (CEGs)**: Directed graphs representing cause-effect relationships between events. Why needed: Provide structured knowledge for generating diverse reasoning scenarios. Quick check: Verify CEG examples cover common real-world cause-effect patterns.
- **Zero-shot Chain-of-Thought (CoT)**: Prompting technique where models explain reasoning step-by-step. Why needed: Encourages explicit reasoning rather than pattern matching. Quick check: Compare CoT vs direct answering accuracy on simple tasks.
- **Test-time Scaling**: Increasing computational resources during inference. Why needed: Tests whether more compute improves reasoning quality. Quick check: Plot token usage vs accuracy to identify scaling breakpoints.
- **Full Parameter Fine-tuning**: Updating all model weights during training. Why needed: Adapts pre-trained models to new task distributions. Quick check: Compare fine-tuned vs zero-shot performance on held-out validation set.

## Architecture Onboarding
- **Component Map**: gpt-4o-mini API -> Causal Chain Synthesis -> Question Generation -> Benchmark Construction -> Model Evaluation -> Fine-tuning
- **Critical Path**: Benchmark generation (CEG processing, synthesis parameters) → Evaluation (model selection, CoT prompting) → Fine-tuning (data preparation, hyperparameter tuning)
- **Design Tradeoffs**: Multiple-choice vs open-ended questions - current format enables clear evaluation but may not capture full reasoning complexity
- **Failure Signatures**: Reasoning models underperforming on Com²-main (overthinking), flat token usage vs accuracy curves for commonsense tasks, low scores on Transition/Intervention tasks requiring long causal dependencies
- **First Experiments**: 1) Run zero-shot CoT inference on all five task types using provided prompts; 2) Fine-tune LLaMA-3.1-8B-Instruct with specified hyperparameters and evaluate on both Com² splits; 3) Generate 100 additional examples using specified CEGs and compare human evaluation scores

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can test-time scaling laws, which improve performance in math and code reasoning, be effectively adapted to complex commonsense reasoning tasks?
- Basis in paper: [explicit] Section 5.2 states: "The test-time scaling law in math and code may not be effective for commonsense reasoning, as they can still fall into common sense shortcuts." and "The overall token usage is still much smaller compared to math. It is worth looking forward to the test-time scaling in commonsense reasoning."
- Why unresolved: The paper shows that simply increasing token output does not consistently improve commonsense reasoning performance; current models may fall into reasoning shortcuts rather than engaging in deeper analysis.
- What evidence would resolve it: Developing and testing specialized prompting strategies or model architectures designed for commonsense reasoning that show consistent performance gains as test-time compute increases.

### Open Question 2
- Question: How can the dataset synthesis process for complex commonsense benchmarks be improved through finer-grained guidance from causal event graphs?
- Basis in paper: [explicit] The Limitations section states: "the dataset synthesis process could benefit from finer-grained and step-by-step guidance of causal event graphs."
- Why unresolved: The current synthesis process uses CEGs at a relatively coarse level; the granularity of causal guidance during example generation may limit the diversity and complexity of synthesized questions.
- What evidence would resolve it: A systematic comparison of benchmarks generated under different levels of causal graph granularity, measuring quality through human evaluation and downstream model performance.

### Open Question 3
- Question: What types of training data beyond the current five task categories are needed to achieve robust improvement on complex commonsense reasoning tasks?
- Basis in paper: [inferred] Section 5.1 notes: "On Com2-hard, the trained LLMs perform worse than reasoning LLMs (Open-O1 or Marco-o1). This reveals that more types of training data are required for improvement."
- Why unresolved: Training on Direct, Decision, Transition, Intervention, and Counterfactual tasks provides improvement but still underperforms compared to reasoning-tuned models, suggesting gaps in training data coverage.
- What evidence would resolve it: An ablation study systematically adding new task types to training data and measuring transfer performance on held-out complex commonsense scenarios.

### Open Question 4
- Question: Can LLMs effectively evaluate open-ended commonsense reasoning questions, and would such questions better capture real-world reasoning complexity than current multiple-choice formats?
- Basis in paper: [explicit] The Limitations section states: "the questions can be open-ended, which can be evaluated via LLMs-based evaluators."
- Why unresolved: The current benchmark relies on multiple-choice and multi-select formats with fixed ground truths, which may not fully reflect the ambiguous nature of real-world commonsense reasoning.
- What evidence would resolve it: Constructing an open-ended version of Com2, developing reliable LLM-based evaluation protocols, and comparing correlation with human judgments against the current format.

## Limitations
- Benchmark construction relies on gpt-4o-mini with unspecified generation parameters (temperature, top_p, max_tokens), affecting example diversity
- Training data formatting and prompt templates not provided in appendix, making exact reproduction challenging
- Current multiple-choice format may not fully capture the ambiguous nature of real-world commonsense reasoning
- Performance gains from post-training and slow thinking techniques don't fully close the reasoning gap

## Confidence
- **High Confidence**: The benchmark's overall design and task categorization are clearly specified. The evaluation methodology (zero-shot CoT, accuracy metrics) is reproducible.
- **Medium Confidence**: The core findings about LLMs' struggles with reasoning depth and sudden events are supported, but exact reproduction requires clarifying unknown generation parameters.
- **Low Confidence**: The effectiveness of post-training and slow thinking techniques cannot be fully validated without the complete training setup details.

## Next Checks
1. Reconstruct the benchmark using gpt-4o-mini with specified generation parameters (temperature=0.7, top_p=0.9, max_tokens=100) and verify example quality through human evaluation.
2. Implement the exact training data formatting and prompt templates to reproduce the 8,386-example training set, then fine-tune LLaMA-3.1-8B-Instruct with the specified hyperparameters.
3. Conduct a systematic ablation study varying k-shot exemplar count (k=2, 5, 10) from Causenet to measure impact on benchmark difficulty and LLM performance.