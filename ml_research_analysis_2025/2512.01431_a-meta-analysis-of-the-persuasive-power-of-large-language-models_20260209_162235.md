---
ver: rpa2
title: A Meta-Analysis of the Persuasive Power of Large Language Models
arxiv_id: '2512.01431'
source_url: https://arxiv.org/abs/2512.01431
tags:
- studies
- persuasive
- llms
- persuasion
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This meta-analysis systematically reviewed the persuasive effectiveness
  of large language models (LLMs) compared to human communicators. Seven studies with
  17,422 participants were identified, yielding 12 effect size estimates.
---

# A Meta-Analysis of the Persuasive Power of Large Language Models

## Quick Facts
- arXiv ID: 2512.01431
- Source URL: https://arxiv.org/abs/2512.01431
- Reference count: 40
- Primary result: LLMs match human persuasion when appropriately implemented, but effectiveness depends strongly on context.

## Executive Summary
This meta-analysis systematically reviewed the persuasive effectiveness of large language models (LLMs) compared to human communicators. Seven studies with 17,422 participants were identified, yielding 12 effect size estimates. The analysis found no significant overall difference in persuasive performance between LLMs and humans (Hedges' g = 0.02, p = .530). However, substantial heterogeneity was observed (I² = 75.97%), suggesting contextual factors influence effectiveness. Exploratory moderator analyses showed that differences in LLM model, conversation design, and domain jointly explained 81.93% of between-study variance, with residual heterogeneity dropping to I² = 35.51%.

## Method Summary
The authors conducted a meta-analysis using random-effects models with REML estimation in R 4.5.0 via the metafor 4.8-0 package. They extracted effect sizes from 7 eligible studies (12 independent estimates) comparing LLM versus human persuasion, converting various study statistics to standardized mean differences (Hedges' g). Moderator analyses examined LLM model type, conversation design (one-shot vs. interactive), and domain (health vs. politics) using meta-regression. Egger's test and trim-and-fill procedures were used to assess publication bias.

## Key Results
- No significant overall difference in persuasive performance between LLMs and humans (Hedges' g = 0.02, p = .530)
- Substantial heterogeneity observed (I² = 75.97%) indicating contextual factors influence effectiveness
- Combined moderator model explained 81.93% of between-study variance, with residual heterogeneity dropping to I² = 35.51%

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Persuasion Effectiveness
- Claim: LLM persuasiveness is not inherently superior or inferior to humans, but rather highly dependent on contextual deployment factors.
- Mechanism: Variance in persuasive outcomes is explained by the interaction of model capabilities, conversation design, and domain, rather than a main effect of "LLM vs. human."
- Core assumption: The observed heterogeneity (I² = 75.97%) reflects meaningful differences in contextual factors rather than random noise.
- Evidence anchors:
  - "LLMs can match human persuasion when appropriately implemented, but effectiveness depends strongly on context."
  - "When considered jointly in a combined model, these factors explained a large proportion of the between-study variance (R² = 81.93%)...residual heterogeneity is low (I² = 35.51%)."
- Break condition: If future studies with larger N fail to replicate the combined moderator effects or show high residual heterogeneity even with these factors controlled.

### Mechanism 2: Interactive Dialogue Advantage
- Claim: Multi-turn interactive formats enable LLMs to adapt responses dynamically, increasing persuasive effectiveness compared to static one-shot messages.
- Mechanism: Interactivity allows for personalization, clarification, and argument refinement based on user feedback, leveraging the LLM's reasoning and generation capabilities more fully.
- Core assumption: The advantage stems from the conversational structure itself, not just from the model's base capabilities.
- Evidence anchors:
  - "Exploratory moderator analyses showed that differences in LLM model, conversation design, and domain jointly explained 81.93% of between-study variance."
  - Table 3 shows conversation design (one-shot) has a significant negative coefficient (-0.494, p < .001) in the combined model.
- Break condition: If well-powered studies show no difference between one-shot and interactive formats, or if the advantage disappears when controlling for message length/complexity.

### Mechanism 3: Domain-Specific Processing Routes
- Claim: LLMs may be more effective in domains that benefit from analytical, fact-based persuasion (health) versus those requiring emotional resonance (politics).
- Mechanism: LLMs excel at generating coherent, evidence-based arguments (central route processing in ELM terms), while humans retain advantages in emotional authenticity and narrative engagement (peripheral route).
- Core assumption: The different persuasive strengths of LLMs and humans map onto different cognitive processing routes.
- Evidence anchors:
  - "LLMs can match human persuasion when appropriately implemented."
  - "LLMs may thus be particularly effective in domains where fact-based reasoning and logical elaboration are central to persuasion."
- Break condition: If studies find LLMs equally or more effective in emotionally-driven domains, or if the domain effect disappears when controlling for other factors.

## Foundational Learning

- Concept: Meta-analysis and Effect Sizes (Hedges' g, I²)
  - Why needed here: The paper's conclusions rely on understanding pooled effect sizes and heterogeneity metrics.
  - Quick check question: What does an I² of 75.97% indicate about the consistency of results across studies?

- Concept: Elaboration Likelihood Model (ELM)
  - Why needed here: The paper uses ELM to explain why LLMs might perform differently across domains.
  - Quick check question: According to ELM, what are the two routes to persuasion and which might LLMs leverage more effectively?

- Concept: Moderator Analysis vs. Main Effects
  - Why needed here: The key finding is that context (moderators) matters more than the main effect of LLM vs. human.
  - Quick check question: Why did individual moderator analyses fail to reach significance while the combined model was explanatory?

## Architecture Onboarding

- Component map: Persuasion Engine -> Conversation Controller -> Context Adapter -> Measurement Layer
- Critical path: Start with interactive format in a health domain → Optimize argument structure for central route processing → Measure attitude/behavioral intention change → A/B test against human baseline.
- Design tradeoffs:
  1. One-shot vs. Interactive: One-shot is scalable and lower cost, but interactive has higher persuasive potential.
  2. Model selection: Newer models (GPT-4.x) may not be universally better; GPT-3.x showed higher effectiveness in some contexts.
  3. Outcome measurement: Behavioral intentions are more tangible than perceived message effectiveness (PME), which may overestimate success.
- Failure signatures:
  1. High variance in persuasive outcomes across similar tasks → Likely domain or audience mismatch.
  2. One-shot messages underperforming human baseline → Consider switching to interactive format or enhancing personalization.
  3. Inconsistent results across A/B tests → Check for uncontrolled contextual factors.
- First 3 experiments:
  1. Replicate the interactive advantage: Run an A/B test comparing one-shot vs. multi-turn persuasive dialogue in a health domain, measuring attitude change.
  2. Test domain boundary: Compare LLM persuasiveness in a highly emotional political topic vs. a fact-based health topic, using the same model and format.
  3. Isolate personalization effect: Use the same LLM and domain, but vary whether the message is generic or tailored to user demographics/values.

## Open Questions the Paper Calls Out

- **Personalization Impact**: Does tailoring LLM-generated messages to recipient traits improve persuasive effectiveness compared to generic messages, and how does this interact with domain?
- **Longitudinal Effects**: Do persuasive effects of LLMs persist, decay, or strengthen over time in repeated-exposure settings?
- **Psychological Mechanisms**: What specific psychological processes (e.g., deeper processing, credibility) mediate the persuasive success of different LLM implementations?
- **Cultural Generalizability**: Does LLM persuasive effectiveness generalize to non-WEIRD populations beyond Western, Educated, Industrialized, Rich, Democratic contexts?

## Limitations
- Limited number of primary studies (n=7) and substantial heterogeneity (I² = 75.97%) prevent definitive conclusions about contextual factors.
- No individual moderator reached statistical significance despite combined model explaining 81.93% of variance, suggesting potential underpowered analyses.
- Overwhelming reliance on WEIRD and U.S.-centric samples limits generalizability to other cultural contexts.

## Confidence

- **High Confidence**: LLMs match human persuasion overall when appropriately implemented (supported by non-significant pooled effect g = 0.02, p = .530).
- **Medium Confidence**: Interactive formats may offer advantages over one-shot messages (supported by negative coefficient in combined model but requires replication).
- **Low Confidence**: Domain-specific processing route advantages (theoretical inference not yet empirically validated).

## Next Checks
1. Conduct a replication study with at least 10 new primary studies focusing on domain comparisons (health vs. politics) to empirically test the central vs. peripheral route hypothesis.
2. Run a sensitivity analysis excluding studies with high I² contribution (>20% of heterogeneity) to test robustness of combined moderator effects.
3. Pre-register a multi-site experiment comparing one-shot vs. interactive formats across three domains (health, politics, consumer behavior) with n ≥ 1,000 total participants to detect moderator effects with adequate power.