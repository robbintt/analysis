---
ver: rpa2
title: Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation
arxiv_id: '2506.21384'
source_url: https://arxiv.org/abs/2506.21384
tags:
- query
- arxiv
- https
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Omni-RAG introduces an LLM-assisted query understanding framework
  to address the challenges of noisy, ambiguous, and multi-intent queries in live
  retrieval-augmented generation systems. The approach preprocesses user queries through
  three key modules: query denoising and decomposition using tailored LLM prompts,
  intent-aware knowledge retrieval from FineWeb via OpenSearch, and reranking with
  BGE followed by generation using Falcon-10B.'
---

# Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2506.21384
- Source URL: https://arxiv.org/abs/2506.21384
- Reference count: 40
- Primary result: 2nd place in SIGIR 2025 LiveRAG Challenge with 0.9693 Correctness, 0.3878 Faithfulness

## Executive Summary
Omni-RAG introduces an LLM-assisted query understanding framework designed to handle noisy, ambiguous, and multi-intent queries in live retrieval-augmented generation systems. The approach preprocesses user queries through three key modules: query denoising and decomposition using tailored LLM prompts, intent-aware knowledge retrieval from FineWeb via OpenSearch, and reranking with BGE followed by generation using Falcon-10B. Experimental results demonstrate that Omni-RAG achieved 2nd place in Session 1 of the SIGIR 2025 LiveRAG Challenge, with a Correctness score of 0.9693 (4% higher than 3rd place) and a Faithfulness score of 0.3878 (34% higher than 3rd place). The framework effectively improves robustness in handling complex real-world queries compared to existing RAG systems.

## Method Summary
Omni-RAG processes queries through a multi-stage pipeline: an LLM rewrites and decomposes noisy or multi-intent queries into structured sub-queries, OpenSearch retrieves top-K documents per sub-query from FineWeb, BGE reranker selects top-N relevant documents, and Falcon-10B generates the final response using a chain-of-thought prompt. The system handles query ambiguity through decomposition into multiple retrieval targets and improves answer quality through reranking and context-aware generation.

## Key Results
- Achieved 2nd place in SIGIR 2025 LiveRAG Challenge Session 1
- Correctness score of 0.9693, 4% higher than 3rd place
- Faithfulness score of 0.3878, 34% higher than 3rd place
- Self-consistency with 4 reasoning paths improved relevance but reduced faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-assisted query denoising and decomposition may improve retrieval relevance for noisy or multi-intent queries by reducing semantic mismatch between user intent and retriever.
- Mechanism: An LLM applies tailored prompts to (1) rewrite queries (correcting spelling errors, clarifying ambiguity) and (2) decompose multi-intent queries into structured sub-queries (typically JSON-formatted), enabling finer-grained retrieval targets per intent.
- Core assumption: The LLM correctly identifies distinct intents and produces sub-queries that better align with retriever vocabulary/index than the raw input.
- Evidence anchors:
  - [abstract] "Deep Query Understanding and Decomposition, which utilizes LLMs with tailored prompts to denoise queries (e.g., correcting spelling errors) and decompose multi-intent queries into structured sub-queries"
  - [Section 3.2] Equations (2–3) formalize rewriting and decomposition; sub-queries are "typically generated in a structured format, such as JSON"
  - [corpus] Weak direct evidence—neighbor papers discuss query rewriting/decomposition broadly but do not validate this specific prompt-based approach
- Break condition: If original query is already atomic and well-formed, decomposition overhead may introduce unnecessary fragmentation without retrieval gain.

### Mechanism 2
- Claim: Intent-aware retrieval over sub-queries can increase coverage of relevant evidence by aggregating documents across multiple retrieval passes before reranking.
- Mechanism: For each sub-query q'_s, retrieve top-K documents via OpenSearch from FineWeb; union the results (D_retrieved = ∪ D_s), then rerank using BGE against the (rewritten) query and select top-N for generation.
- Core assumption: Sub-queries are sufficiently distinct that their retrieval pools are complementary; reranker can effectively resolve redundancy and prioritize relevance.
- Evidence anchors:
  - [Section 3.3] "This ensures a broad coverage of information related to the different facets of the original query"
  - [Section 3.4] Reranker scores documents against q or q' and selects top-N; "After reranking, we obtained high-quality documents relevant to the query"
  - [corpus] Neighbor papers (e.g., DH-RAG, MAO-ARAG) employ multi-stage or multi-intent retrieval but use different aggregation strategies; not direct validation
- Break condition: If sub-queries are too similar, union adds redundant documents; reranker may still miss relevant passages if query-document vocabulary mismatch persists post-rewrite.

### Mechanism 3
- Claim: Chain-of-thought prompting with reranked context may improve faithfulness by encouraging stepwise attribution to retrieved documents.
- Mechanism: Top-N reranked documents are integrated with the rewritten query into a chain-of-thought prompt; Falcon-10B generates the final response conditioned on this structured context.
- Core assumption: The generator follows the chain-of-thought structure and grounds each reasoning step in provided documents rather than parametric knowledge.
- Evidence anchors:
  - [abstract] "a final response is generated by an LLM (i.e., Falcon-10B) using a chain-of-thought prompt"
  - [Section 3.4] "This step aims to synthesize the information from the retrieved documents into a coherent, accurate, and contextually appropriate answer"
  - [corpus] No direct corpus validation for CoT + Falcon-10B in this specific setup
- Break condition: If retrieved documents are noisy or contradictory, CoT may amplify confusion; long context with many documents may exceed effective attention window.

## Foundational Learning

- Concept: Query understanding (rewriting, disambiguation, decomposition)
  - Why needed here: Omni-RAG's first module relies on prompt-based query preprocessing; without understanding these sub-techniques, you cannot debug when decomposition produces overlapping or incoherent sub-queries.
  - Quick check question: Given "best budget phones 2024 with good camera," can you identify (a) noise/ambiguity and (b) at least two plausible sub-intents?

- Concept: Dense vs. sparse retrieval fundamentals
  - Why needed here: OpenSearch over FineWeb is used; understanding hybrid retrieval helps diagnose when reranker compensates for first-stage misses.
  - Quick check question: For a misspelled query, which retrieval method (dense vs. sparse) is more likely to fail at first-stage, and why?

- Concept: Reranking and relevance scoring
  - Why needed here: Omni-RAG depends on BGE reranker to filter unioned documents; you need to interpret relevance scores and tune top-N selection.
  - Quick check question: If reranker scores are near-uniform across documents, what does this suggest about query-document alignment or retriever quality?

## Architecture Onboarding

- Component map: Query Understanding Module (LLM with tailored prompts) → Retrieval Module (OpenSearch over FineWeb) → Aggregation (union of retrieved documents) → Reranking Module (BGE-reranker-large) → Generation Module (Falcon-10B + CoT prompt)

- Critical path: Query input → rewriting → decomposition → parallel sub-query retrieval → aggregation → reranking → top-N selection → CoT prompt construction → Falcon-10B generation. Latency is dominated by multiple retrieval calls and LLM inference at both preprocessing and generation stages.

- Design tradeoffs:
  - More sub-queries → broader coverage but higher retrieval latency and potential redundancy
  - Higher top-K per sub-query → more candidate documents but larger reranking compute and context length
  - Self-consistency (sc4/sc8) → improves relevance in some settings but may reduce faithfulness (observed tradeoff in Table 2)

- Failure signatures:
  - Decomposition returns overlapping sub-queries → redundant retrieval, wasted compute
  - Reranker assigns flat scores → retriever quality issue or query-document mismatch
  - Generated response ignores retrieved documents → prompt formatting issue or model grounding failure
  - Faithfulness drops when document count increases → possible context confusion or attention dilution

- First 3 experiments:
  1. Ablate query decomposition (compare: raw query vs. rewritten only vs. rewritten + decomposed) on a held-out set of multi-intent queries; measure retrieval recall and final answer correctness.
  2. Vary top-K (e.g., 3, 5, 10) and top-N (e.g., 5, 10) to quantify the latency-quality tradeoff; track reranker score distributions.
  3. Test self-consistency with 4 vs. 8 reasoning paths on the dry-test subset; report both relevance and faithfulness to replicate the observed tradeoff from Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal configuration for self-consistency reasoning paths in Omni-RAG, and why does increasing path count (sc8) fail to improve performance while sc4 shows mixed results?
- Basis in paper: [explicit] "The 5 (sc8) configuration did not yield the expected performance gains, suggesting that more reasoning paths do not necessarily lead to better results. Interestingly, the 5 (sc4) setting improved relevance but led to a moderate decline in faithfulness, highlighting the need to balance path quantity with generation quality."
- Why unresolved: The paper reports empirical findings but provides no theoretical or systematic analysis of why this trade-off occurs or how to optimize it.
- What evidence would resolve it: Ablation studies varying path counts systematically, analysis of reasoning path diversity/quality, and correlation with query complexity types.

### Open Question 2
- Question: How does Omni-RAG's latency scale with query complexity (number of decomposed sub-queries), and can the framework meet real-time constraints beyond the two-hour competition window?
- Basis in paper: [inferred] The paper notes the "two-hour time limit" constraint but provides no latency analysis. The pipeline involves sequential LLM calls (rewrite → decompose → retrieve per sub-query → rerank → generate), which could become prohibitive for queries decomposing into many sub-intents.
- Why unresolved: No computational cost analysis or latency measurements are reported despite the live RAG context.
- What evidence would resolve it: End-to-end latency measurements across varying query complexity levels, breakdown of time per module, and comparison against simpler baselines.

### Open Question 3
- Question: To what extent do the pseudo-labels generated by Qwen2.5-7B-Instruct correlate with human judgments for relevance and faithfulness evaluation?
- Basis in paper: [inferred] The paper uses Qwen2.5-7B-Instruct to generate pseudo-labels for dry-test evaluation, acknowledging this is a proxy. The validity of these labels as a reliable evaluation mechanism is not assessed.
- Why unresolved: No human evaluation or comparison to ground-truth labels is provided to validate the pseudo-labeling approach.
- What evidence would resolve it: Correlation analysis between pseudo-labels and human annotations on a shared evaluation set.

## Limitations
- Evaluation limited to single SIGIR 2025 LiveRAG Challenge with only final leaderboard scores reported
- Critical implementation details missing: exact LLM for query understanding, prompt templates, and self-consistency aggregation logic
- Benefits of query decomposition and chain-of-thought generation lack controlled ablation validation

## Confidence
- Correctness gains: Medium (rank 2nd with large margin over 3rd)
- Faithfulness attribution: Low (only final score given, no breakdown by component)
- Key assumptions include that OpenSearch+FineWeb setup matches competition and LLM preprocessing is robust to full distribution of noisy/multi-intent queries

## Next Checks
1. Implement controlled ablation: compare raw query retrieval vs. rewritten-only vs. rewritten+decomposed pipelines on held-out multi-intent queries; measure retrieval recall and final answer correctness.
2. Vary top-K and top-N to quantify retrieval-aggregation-reranking latency-quality tradeoff; record reranker score distributions to detect flat or biased scoring patterns.
3. Test self-consistency with 4 vs. 8 reasoning paths on dry-test subset; report both relevance and faithfulness to replicate observed tradeoff from Table 2.