---
ver: rpa2
title: 'Autotune: fast, accurate, and automatic tuning parameter selection for Lasso'
arxiv_id: '2512.11139'
source_url: https://arxiv.org/abs/2512.11139
tags:
- autotune
- lasso
- error
- relative
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Autotune is a fast and accurate tuning parameter selection strategy
  for Lasso regression. It iteratively estimates regression coefficients and noise
  standard deviation by optimizing a penalized Gaussian log-likelihood, using partial
  residuals to correct shrinkage bias.
---

# Autotune: fast, accurate, and automatic tuning parameter selection for Lasso

## Quick Facts
- arXiv ID: 2512.11139
- Source URL: https://arxiv.org/abs/2512.11139
- Authors: Tathagata Sadhukhan; Ines Wilms; Stephan Smeekes; Sumanta Basu
- Reference count: 40
- Autotune is a fast and accurate tuning parameter selection strategy for Lasso regression that outperforms cross-validation and Scaled Lasso while being 15-200 times faster.

## Executive Summary
Autotune is an automatic tuning parameter selection strategy for Lasso regression that eliminates the need for cross-validation or pre-defined penalty parameter grids. The method iteratively estimates regression coefficients and noise standard deviation by optimizing a penalized Gaussian log-likelihood, using partial residuals to correct shrinkage bias. Extensive simulations show Autotune outperforms existing methods in estimation, prediction, and variable selection accuracy—particularly in low signal-to-noise regimes—while achieving significant computational speedups. The method also provides accurate noise variance estimates useful for high-dimensional inference and offers a visual diagnostic for sparsity via partial residual analysis.

## Method Summary
Autotune addresses the challenge of selecting the Lasso penalty parameter without cross-validation by implementing an iterative procedure that alternates between estimating regression coefficients and noise variance. The method solves a penalized Gaussian log-likelihood, where for fixed noise variance the coefficient update is equivalent to standard Lasso with parameter λ = λ₀σ². By iterating between updating β (via Lasso) and updating σ², Autotune generates a data-adaptive sequence of λ values. The key innovation is using partial residuals to estimate σ², which corrects the shrinkage bias found in standard residual-based estimators. The method monitors support set stability for early stopping of Coordinate Descent iterations, significantly reducing runtime with minimal accuracy loss.

## Key Results
- Autotune outperforms cross-validation and Scaled Lasso in estimation, prediction, and variable selection accuracy—especially in low signal-to-noise regimes
- Autotune is 15-200 times faster than cross-validation and Scaled Lasso across various problem sizes
- For high-dimensional VAR models, Autotune is 45-190 times faster than existing methods while providing superior estimation and model selection
- The method yields accurate noise variance estimates useful for high-dimensional inference
- Autotune offers a new visual diagnostic for sparsity via partial residual analysis

## Why This Works (Mechanism)

### Mechanism 1
Autotune eliminates the need for cross-validation by iteratively estimating regression coefficients and noise variance through a penalized Gaussian log-likelihood framework. The method alternates between updating β via Lasso and updating σ², generating a data-adaptive sequence of λ values rather than searching a fixed grid. This works under the assumption that λ ∝ σ√(log p/n) holds, allowing the iterative process to converge to a stable σ estimate that yields a generalizable λ. The relationship is validated through the paper's optimization framework, though specific convergence properties remain to be characterized.

### Mechanism 2
The method corrects shrinkage bias in Lasso by using partial residuals to estimate noise variance σ². Standard Lasso residuals are biased because coefficients are shrunk toward zero, leading to under-estimation of σ. Autotune calculates the standard deviation of partial residuals for each predictor during Coordinate Descent and identifies "outlier" predictors (assumed to be true signals) via their high PR SD. It then fits nested linear models using these predictors and uses the MSE of the final model as the variance estimate. This approach assumes the true model is sparse and that PR SDs of true signal predictors are visibly larger than those of null predictors.

### Mechanism 3
Autotune achieves significant runtime improvements through early stopping of Coordinate Descent iterations based on support set stability. The method monitors the active predictors and stops CD updates for a given λ when the new support set is a subset of the previous one, indicating σ has converged. Full convergence is only computed at the final λ. This assumes σ converges faster than the high-dimensional β, and the support set trajectory provides a sufficient proxy for convergence.

## Foundational Learning

- **Shrinkage Bias in Lasso**
  - Why needed here: Standard Lasso residuals are biased (too small), leading to under-estimation of noise variance σ in methods like Scaled Lasso
  - Quick check question: Why can't we just use the Mean Squared Error (MSE) of the Lasso fit to estimate the noise variance σ²?

- **Partial Residuals in Coordinate Descent**
  - Why needed here: This is the core signal processing unit of Autotune; understanding how r^(k) = y_i - Σ_{j≠k} x_ijβ̂_j behaves is essential to understanding how the method separates signal from noise
  - Quick check question: In the context of Autotune, what statistical property of the partial residual vector for predictor k determines if k is added to the "active" set for variance estimation?

- **Sequential F-tests**
  - Why needed here: This classical statistical test is repurposed as a stopping criterion for building the variance-estimation model, determining how many "outlier" predictors to include
  - Quick check question: What is the null hypothesis H_{0,k} in the sequential F-test used in Algorithm 2?

## Architecture Onboarding

- **Component map**: Input Layer (standardized X, Y; α) -> Solver Core (CD loop computing β and PR) -> Variance Estimator (Sigma Updater using PR SDs, Gram-Schmidt, F-tests) -> Controller (updates λ and checks support stability)

- **Critical path**: Initialize σ² = Var(Y) -> Run one pass of CD to get β̂ and Partial Residuals -> Feed PR to Sigma Updater to get new σ² -> Check support set stability; if yes, stop σ updates and run CD to convergence; if no, update λ and repeat

- **Design tradeoffs**: The method tries to correct bias using PR but assumes uncorrelated or weakly correlated design for best "outlier" detection. High correlation may degrade the variance estimate. The paper sets α=0.01; larger α might include more predictors in the variance model, potentially reducing bias further but increasing variance of the estimate.

- **Failure signatures**: Correlated predictors can make PR SD ranking unstable, though the paper claims σ estimation remains reasonably accurate. Non-sparsity fails the required assumption (e.g., s=350 in Figure 12 shows no "elbow" in diagnostic plots).

- **First 3 experiments**:
  1. Replicate Figure 1c: Plot the solution path to verify Autotune visits far fewer λ values than standard Lasso
  2. Low SNR stress test: Generate data with SNR < 1 and compare Autotune's RTE against CV-Lasso to verify superiority in low signal-to-noise regimes
  3. Correlation check: Generate data with ρ=0.7 and visualize the histogram of Partial Residual SDs (Figure 3) to see if signals separate cleanly from the noise cluster

## Open Questions the Paper Calls Out

- What are the formal convergence properties of the iterative Autotune algorithm, and what are the statistical properties of its limit point?
- Can active set selection strategies be effectively adapted for Autotune in the presence of highly correlated predictors without sacrificing accuracy?
- Can the visual diagnostic for checking the sparsity assumption be formalized into a rigorous statistical test?
- Under what conditions does Autotune underperform Cross-Validation in estimation accuracy, specifically in high signal-to-noise ratio regimes?

## Limitations

- Performance heavily depends on sparsity assumption and that true signals have larger partial residual standard deviations than noise predictors
- Specific behavior in highly correlated designs and extremely dense settings represents a limitation
- Claims of being "fast, accurate, and automatic" are relative to cross-validation and Scaled Lasso benchmarks, not all possible tuning parameter selection strategies

## Confidence

- **High Confidence**: Runtime improvement claims (45-190x faster than existing methods), availability of R package implementation
- **Medium Confidence**: Superior estimation and prediction accuracy in low SNR regimes (based on simulation evidence)
- **Medium Confidence**: Validity of the partial residual diagnostic for visual sparsity assessment (based on empirical demonstration)
- **Medium Confidence**: Generalizability to high-dimensional VAR models (based on sp500 dataset results)

## Next Checks

1. **Replication of Runtime Claims**: Measure actual runtime of Autotune versus cross-validation on a standard Lasso problem (e.g., n=100, p=500) to verify the claimed 15-200x speedup

2. **Stress Test Correlated Predictors**: Generate data with high correlation structure (e.g., ρ > 0.7) and evaluate Autotune's variable selection accuracy versus its performance on independent predictors

3. **Robustness to Sparsity Assumption**: Systematically vary the true sparsity level s from very sparse (e.g., s=2) to dense (e.g., s=100) in a fixed-dimensional problem and evaluate how Autotune's accuracy degrades compared to benchmarks