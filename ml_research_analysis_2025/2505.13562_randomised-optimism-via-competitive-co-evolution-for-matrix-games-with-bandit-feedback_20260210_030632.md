---
ver: rpa2
title: Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit
  Feedback
arxiv_id: '2505.13562'
source_url: https://arxiv.org/abs/2505.13562
tags:
- coebl
- regret
- matrix
- games
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses learning in two-player zero-sum matrix games
  with bandit feedback, where the payoff matrix is unknown and players only observe
  noisy rewards. The core method, Competitive Co-evolutionary Bandit Learning (COEBL),
  integrates evolutionary algorithms with bandit learning to implement randomised
  optimism via variation operators.
---

# Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback

## Quick Facts
- arXiv ID: 2505.13562
- Source URL: https://arxiv.org/abs/2505.13562
- Authors: Shishen Lin
- Reference count: 40
- Primary result: Competitive Co-evolutionary Bandit Learning (COEBL) achieves sublinear regret in two-player zero-sum matrix games with bandit feedback, outperforming classical bandit algorithms through randomized optimism.

## Executive Summary
This paper introduces Competitive Co-evolutionary Bandit Learning (COEBL), a novel algorithm that combines evolutionary algorithms with bandit learning for two-player zero-sum matrix games with bandit feedback. The key innovation is implementing "randomized optimism" through evolutionary variation operators, generating diverse estimated payoff matrices and using selection mechanisms to guide policy updates. The algorithm provably achieves sublinear regret matching deterministic optimism-based methods while demonstrating superior empirical performance on standard benchmarks like Rock-Paper-Scissors and diagonal games.

## Method Summary
COEBL operates by maintaining an empirical estimate of the payoff matrix and using Gaussian mutation operators to create optimistic estimates. At each iteration, the algorithm samples an action from the current policy, receives bandit feedback (observed reward plus noise), and updates the empirical mean and visit count for the selected action pair. The mutation operator adds noise scaled by inverse visit count to construct an optimistic estimated payoff matrix. A linear programming solver computes the maximin policy for this optimistic matrix, and the algorithm updates its policy only if the new policy improves worst-case expected payoff. This creates a balance between exploration (via mutation) and exploitation (via selection).

## Key Results
- COEBL achieves sublinear regret in two-player zero-sum matrix games, matching the performance of deterministic optimism-based methods like UCB
- Empirical results show COEBL consistently outperforms classical bandit algorithms (EXP3, EXP3-IX, UCB) in terms of regret and convergence to Nash equilibrium
- On Rock-Paper-Scissors, COEBL with c=2 achieves regret of approximately 0.1√(K²T) at T=3000
- In head-to-head experiments, COEBL shows significantly lower regret than EXP3-IX (87 vs 2550 regret after T=3000 in DIAGONAL n=3)

## Why This Works (Mechanism)

### Mechanism 1: Randomized Optimism via Gaussian Mutation
Stochastic perturbation of payoff estimates enables diverse exploration while maintaining high-probability optimistic upper bounds on true payoffs. The mutation operator constructs Ãᵗᵢⱼ = Āᵗᵢⱼ + N(√(c log(2T²m²)/(1∨nᵗᵢⱼ+1)), 1/(1∨nᵗᵢⱼ)²), adding mean-shifted Gaussian noise scaled by inverse visit count. This creates randomized upper confidence bounds rather than deterministic ones. The theoretical assumption requires sub-Gaussian noise (ηᵗ is 1-sub-Gaussian), payoff matrix entries bounded in [0,1], and mutation rate c ≥ 8. If these conditions fail, the regret analysis becomes invalid.

### Mechanism 2: Fitness-Based Selection for Policy Stability
Elitist selection prevents premature policy degradation while allowing beneficial mutations to propagate. The candidate policy x' from LP solve is accepted only if Fitness(x', Ãᵗ) > Fitness(xᵗ⁻¹, Ãᵗ), where Fitness(x, B) = min_{y∈Δm} y^T Bx. This ensures monotonic improvement in worst-case expected payoff against optimistic opponent responses. The core assumption is that linear programming exactly solves maximin and the fitness landscape is sufficiently smooth for incremental improvements. If mutation variance is too high relative to fitness improvements, selection may reject all candidates causing stagnation.

### Mechanism 3: High-Probability Optimistic Event Decomposition
Regret analysis decomposes into "optimistic case" (Ãᵗ ≥ A entrywise) and failure case, with failure probability controlled by union bound. Define event Eᵗ = {∃i,j : Ãᵗᵢⱼ < Aᵢⱼ}. Lemma 1 bounds Pr(Eᵗ) ≤ δ = (1/2T²m²)^(c/8). Regret under ¬Eᵗ is bounded by LP solution gap; regret under Eᵗ is trivially bounded by T. Total regret: O(√(m²T log(T²m²))). The theoretical analysis assumes union bound over m² entries per iteration, T iterations, and sub-Gaussian tail behavior. If matrix dimension m grows faster than √T, union bound overhead may dominate.

## Foundational Learning

- **Concept: Nash Regret vs. External Regret**
  - Why needed here: Paper uses Nash regret R(A, ALG, T) = Σₜ(V*ₐ - rᵗ), measuring gap from equilibrium payoff V*ₐ, not from best fixed action in hindsight. This captures game-theoretic adaptation, not just bandit learning.
  - Quick check question: In a 3×3 game where Nash equilibrium payoff is 0 but best single action averages 0.5, would an algorithm with zero external regret necessarily have zero Nash regret?

- **Concept: Optimism in the Face of Uncertainty (OFU)**
  - Why needed here: COEBL implements OFU stochastically. Classical OFU (UCB) uses deterministic confidence bounds; understanding this distinction clarifies why randomized optimism may be more robust to exploitation.
  - Quick check question: If an opponent can observe your algorithm's confidence bounds, would deterministic UCB or randomized COEBL be more vulnerable to strategic exploitation?

- **Concept: Maximin Optimization via Linear Programming**
  - Why needed here: Line 6 of Algorithm 1 requires solving max_{x∈Δm} min_{y∈Δm} y^T Ãᵗ x, which reformulates as LP. This is the computational bottleneck—understanding this clarifies scalability limits.
  - Quick check question: Why does min_{y∈Δm} y^T Bx equal min_{j∈[m]} (Bx)ⱼ for any fixed x, and how does this enable LP reformulation?

## Architecture Onboarding

- Component map: [Reward Observer] → [Empirical Mean Updater] → [Mutation Operator] → [Optimistic Matrix Ãᵗ] → [LP Solver: maxₓ min_y y^TÃᵗx] → [Candidate Policy x'] → [Fitness Comparator] → [Policy Selector xᵗ]

- Critical path:
  1. **Observation**: Receive (iᵗ, jᵗ, rᵗ) from environment; update Āᵗᵢⱼ and nᵗᵢⱼ
  2. **Mutation**: For all (i,j), sample Ãᵗᵢⱼ = Āᵗᵢⱼ + N(μᵢⱼ, σᵢⱼ²) with visit-dependent parameters
  3. **Optimization**: Solve LP: max_{x∈Δm} z s.t. z ≤ (Ãᵗx)ⱼ ∀j (computes maximin)
  4. **Selection**: If min_j(Ãᵗx')ⱼ > min_j(Ãᵗxᵗ⁻¹)ⱼ, set xᵗ = x'; else xᵗ = xᵗ⁻¹
  5. **Action**: Sample iᵗ⁺¹ ~ xᵗ

- Design tradeoffs:
  - **Mutation rate c**: Paper requires c ≥ 8 theoretically but uses c = 2 for RPS empirically. Higher c = tighter bounds but slower exploration; lower c = faster adaptation but riskier.
  - **Selection threshold**: Strict inequality (>) provides stability but may reject marginally beneficial updates. Non-strict (≥) increases update frequency but risks oscillation.
  - **LP solver choice**: Exact simplex/polynomial-time methods vs. approximate first-order methods. Exact required for theoretical guarantees; approximate may suffice empirically.
  - **Assumption**: Single-population (current policy) vs. multi-population coevolution. Paper uses (1+1)-style elitism for tractability.

- Failure signatures:
  - **Policy stagnation**: Regret plateaus but remains linear → selection threshold too strict; try c < 8 or non-strict comparison
  - **Explosive regret**: Curves diverge upward → mutation variance too high; check nᵗᵢⱼ update correctness
  - **No convergence to Nash**: TV-distance stays > 1.5 on DIAGONAL n ≥ 4 → expected per Section 4.2; exponential action spaces exceed sample complexity
  - **Negative regret (player winning)**: In ALG1-vs-ALG2, positive regret means minimizer winning → if COEBL vs UCB shows small positive regret, COEBL is row player losing slightly

- First 3 experiments:
  1. **Baseline replication (RPS, c=2, T=3000)**: Run COEBL self-play; verify regret ~O(√T) per Figure 1; confirm KL-divergence to (1/3,1/3,1/3) converges faster than EXP3-IX. Success criterion: regret at T=3000 below 0.1√(K²T) bound.
  2. **Head-to-head validation (DIAGONAL n=3, COEBL vs EXP3)**: Replicate Figure 4b; expect COEBL regret ~87, EXP3 regret ~2550 after T=3000. This tests randomized optimism's robustness to opponent exploitation in structured games.
  3. **Mutation rate ablation (BIGGER NUMBER n=4, c ∈ {2,4,8,16})**: Test theoretical assumption c ≥ 8. If c=2,4 also achieve sublinear regret, theoretical constraint may be loose. Monitor both regret and TV-distance to Nash (1,0,...,0).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical regret bounds for COEBL be improved or maintained for mutation rates $c < 8$?
- Basis in paper: The paper states that the current analysis assumes $c \geq 8$ due to technical constraints but conjectures that the bound holds for smaller values.
- Why unresolved: The current mathematical proof relies on the higher constant to satisfy specific probability bounds in the concentration inequalities.
- What evidence would resolve it: A formal proof deriving sublinear regret for COEBL with a mutation constant $c < 8$.

### Open Question 2
- Question: Can the COEBL framework be effectively generalized to general-sum games or Markov games?
- Basis in paper: The conclusion identifies extending the algorithm to general-sum games with more players or Markov games as a primary avenue for future research.
- Why unresolved: The current theoretical analysis and algorithmic design are specialized for two-player zero-sum matrix games with bandit feedback.
- What evidence would resolve it: Formulation of COEBL for general-sum environments accompanied by corresponding regret guarantees or convergence proofs.

### Open Question 3
- Question: Does incorporating crossover operators or non-elitist selection mechanisms improve COEBL's performance?
- Basis in paper: The conclusion suggests that adding sophisticated mutation, crossover operators, or non-elitist selection could enhance performance in complex settings.
- Why unresolved: The proposed algorithm currently relies solely on mutation variation operators and a specific selection mechanism.
- What evidence would resolve it: Empirical benchmarks or runtime analysis comparing the current COEBL against variants utilizing crossover and non-elitist selection.

## Limitations

- The theoretical regret analysis requires mutation rate c ≥ 8, yet empirical results successfully use c = 2, suggesting potential looseness in the analysis
- The fitness-based selection mechanism, while novel, lacks comparison to standard evolutionary selection methods to isolate its specific contribution
- The paper only considers two-player zero-sum matrix games, leaving generalization to general-sum games or Markov games as open questions

## Confidence

- Theoretical regret bounds: High
- Empirical performance claims: Medium
- Mechanism explanations: Medium
- Fitness selection contribution: Low

## Next Checks

1. **Test c ≥ 8 assumption empirically**: Run COEBL on RPS with c ∈ {2, 4, 8, 16} and verify whether sublinear regret requires c ≥ 8 as theory suggests, or if lower values suffice.

2. **Compare selection mechanisms**: Implement a variant of COEBL using standard evolutionary selection (e.g., tournament selection) instead of fitness-based selection to isolate the contribution of the specific selection mechanism.

3. **Opponent adaptation stress test**: Create an adaptive opponent that can exploit deterministic confidence bounds, then compare COEBL vs deterministic UCB in head-to-head play to validate the robustness claims.