---
ver: rpa2
title: 'Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large
  Language Models'
arxiv_id: '2504.02273'
source_url: https://arxiv.org/abs/2504.02273
tags:
- reward
- reasoning
- training
- memory
- memory-r
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of applying reinforcement learning\
  \ (RL) to tiny language models (\u22641 billion parameters) for chain-of-thought\
  \ reasoning, where sparse rewards and poor exploration hinder learning. The proposed\
  \ solution, Memory-R+, introduces an episodic memory-based intrinsic reward mechanism\
  \ that balances exploration and exploitation by leveraging successful and failed\
  \ reasoning patterns from past experiences."
---

# Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models

## Quick Facts
- arXiv ID: 2504.02273
- Source URL: https://arxiv.org/abs/2504.02273
- Authors: Hung Le; Dai Do; Dung Nguyen; Svetha Venkatesh
- Reference count: 37
- Tiny LLMs (≤1B params) with memory-augmented RL achieve 2-14% accuracy gains on math reasoning

## Executive Summary
This paper tackles the fundamental challenge of applying reinforcement learning to tiny language models for chain-of-thought reasoning, where sparse rewards and poor exploration typically prevent effective learning. The authors introduce Memory-R+, a memory-augmented RL approach that uses episodic memory to provide intrinsic rewards based on successful and failed reasoning patterns from past experiences. By balancing exploration and exploitation through this memory mechanism, the method achieves significant performance improvements on mathematical reasoning benchmarks without the training collapse commonly observed in small-scale RL fine-tuning.

## Method Summary
Memory-R+ augments standard RL with an episodic memory system that stores successful and failed reasoning trajectories, using them to generate intrinsic rewards that guide exploration. The approach maintains a memory bank of past episodes, extracting patterns that indicate promising reasoning paths versus dead ends. During training, the agent receives additional rewards based on memory similarity, encouraging exploration of novel but potentially successful reasoning patterns while avoiding known failure modes. This creates a feedback loop where successful experiences reinforce themselves while the memory continuously evolves with new training data.

## Key Results
- Memory-R+ achieves 2-14% accuracy improvements across different tiny models on mathematical reasoning tasks
- Outperforms baseline RL methods on GSM8K, MATH-500, and AIME24 benchmarks
- Successfully avoids training collapse issues that plague small-scale RL fine-tuning

## Why This Works (Mechanism)
The episodic memory mechanism addresses the core challenge of sparse rewards in chain-of-thought reasoning by providing dense, informative feedback signals. When an agent encounters a reasoning pattern similar to a past successful episode, it receives positive intrinsic reward, encouraging exploration of that reasoning style. Conversely, patterns resembling past failures receive negative intrinsic rewards, steering the agent away from unproductive paths. This memory-based guidance effectively transforms the sparse reward landscape into a more navigable space, enabling efficient learning even with tiny models that lack the representational capacity of larger LLMs.

## Foundational Learning

**Reinforcement Learning** - Learning through interaction with environment using reward signals. *Why needed:* Provides the framework for training reasoning agents. *Quick check:* Can the agent improve reasoning through trial-and-error?

**Chain-of-Thought Reasoning** - Step-by-step problem-solving approach in language models. *Why needed:* Enables complex mathematical reasoning through intermediate steps. *Quick check:* Does the model generate intermediate reasoning steps?

**Intrinsic Rewards** - Additional rewards beyond environment feedback. *Why needed:* Addresses sparse reward problem in reasoning tasks. *Quick check:* Are rewards dense enough to guide learning effectively?

**Episodic Memory** - Storage and retrieval of past experiences. *Why needed:* Enables learning from both successes and failures. *Quick check:* Does memory contain diverse, high-quality samples?

**Exploration vs Exploitation** - Balancing novel vs known successful actions. *Why needed:* Prevents premature convergence to suboptimal strategies. *Quick check:* Does the agent discover new reasoning patterns?

## Architecture Onboarding

**Component Map:** Input Problem -> Chain-of-Thought Generator -> Environment -> Reward Module -> Memory Bank -> Intrinsic Reward Calculator -> Policy Update

**Critical Path:** Problem → Reasoning Steps → Environment Feedback → Reward Calculation → Memory Storage → Policy Update

**Design Tradeoffs:** Memory size vs computation overhead; exploration rate vs exploitation efficiency; positive vs negative sample balance in memory

**Failure Signatures:** Training collapse from poor exploration; memory contamination from low-quality samples; catastrophic forgetting of learned patterns

**First Experiments:**
1. Compare baseline RL vs Memory-R+ on simple reasoning tasks to verify memory contribution
2. Test memory decay effects by varying sample age and reward distribution
3. Validate memory quality control by injecting low-quality samples and measuring impact

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to small language models (≤1B parameters) and mathematical reasoning tasks
- Memory mechanism relies on positive/negative reward pairs without addressing potential contamination from low-quality samples
- 2-14% accuracy improvements come from comparison against relatively weak baseline RL methods

## Confidence

**High:** The core empirical finding that Memory-R+ outperforms standard RL baselines on GSM8K, MATH-500, and AIME24 across multiple tiny models is well-supported by the reported results and comparison methodology.

**Medium:** The claim that the episodic memory mechanism specifically prevents training collapse in small-scale RL fine-tuning is plausible but not conclusively proven, as the paper lacks direct comparison to collapse rates with and without memory components.

**Low:** The assertion that Memory-R+ enables successful RL application to 1B-parameter models where previous approaches failed is difficult to verify without access to the exact baseline implementations and training configurations used in prior work.

## Next Checks

1. Ablation study removing the memory component to quantify its exact contribution versus other RL hyperparameters
2. Memory decay and quality control experiments showing how sample age and reward distribution affect long-term performance
3. Transfer learning experiments testing whether Memory-R+ fine-tuned models maintain reasoning capabilities when scaled up to medium-sized models (3-10B parameters)