---
ver: rpa2
title: 'FACTors: A New Dataset for Studying the Fact-checking Ecosystem'
arxiv_id: '2505.09414'
source_url: https://arxiv.org/abs/2505.09414
tags:
- fact-checking
- dataset
- organisations
- claims
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACTors, the first comprehensive fact-checking
  dataset at the ecosystem level, containing 118,112 claims from 117,993 reports published
  between 1995-2025 by 39 IFCN/EFCSN signatory organizations. The dataset addresses
  limitations of existing fact-checking datasets by providing long-term coverage,
  including real-world claims without aggregation of overlapping claims, and capturing
  metadata about fact-checkers.
---

# FACTors: A New Dataset for Studying the Fact-checking Ecosystem

## Quick Facts
- arXiv ID: 2505.09414
- Source URL: https://arxiv.org/abs/2505.09414
- Reference count: 40
- 118,112 claims from 117,993 reports published 1995-2025 by 39 IFCN/EFCSN signatory organizations

## Executive Summary
FACTors is the first comprehensive fact-checking dataset at the ecosystem level, containing 118,112 claims from 117,993 reports published between 1995-2025 by 39 IFCN/EFCSN signatory organizations. The dataset addresses limitations of existing fact-checking datasets by providing long-term coverage, including real-world claims without aggregation of overlapping claims, and capturing metadata about fact-checkers. Key features include 7,327 overlapping claims corresponding to 2,977 unique claims, a Lucene index for efficient searching, and normalized verdicts mapped to six categories. The authors demonstrate the dataset's utility through three applications: ecosystem statistical analysis showing dramatic growth after 2010, political bias detection using a pre-trained BERT model revealing left-leaning tendencies, and credibility assessment assigning scores to organizations based on multiple factors.

## Method Summary
The FACTors dataset was constructed by web-scraping fact-checking reports from 39 IFCN/EFCSN signatory organizations publishing in English between January 16 and February 8, 2025. The collection pipeline used Scrapy with scrapy-playwright for dynamic pages, prioritizing ClaimReview schema extraction. Data preprocessing included language detection, deduplication (SBERT cosine >0.95 threshold), and verdict normalization to six-point scale via manual mapping for 68 short verdicts, RoBERTa fine-tuning, and manual correction for low-confidence predictions (<0.5). Overlapping claims were identified using SBERT embeddings with 0.88 cosine similarity threshold calibrated for 95% precision on 1,000 annotated pairs. The final dataset is distributed as CSV files and a Lucene 8.11.0 index.

## Key Results
- Dataset contains 118,112 fact-checks from 117,993 reports spanning 1995-2025
- 7,327 overlapping claims correspond to 2,977 unique claims investigated by multiple organizations
- Verdict normalization achieved 0.849 accuracy using fine-tuned RoBERTa model
- Political bias detection revealed left-leaning tendencies across fact-checking organizations
- Credibility assessment scores assigned to organizations based on experience, fact-checking rate, political bias, and word count

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving individual organizational verdicts for overlapping claims enables more robust aggregation methods than simple majority voting.
- Mechanism: The dataset stores each fact-checker's verdict separately for semantically identical claims (7,327 overlapping claims across 2,977 unique), allowing weighted voting schemes that can account for fact-checker credibility, expertise, and potential bias.
- Core assumption: Fact-checkers vary in credibility and political bias in ways that systematically affect their verdicts.
- Evidence anchors:
  - [abstract] "A key innovation is preserving individual organizational verdicts for overlapping claims rather than aggregating them, enabling more sophisticated analysis of verdict consistency."
  - [Section 3.4] "We added a separate column, claim_id, to our dataset where the overlapping claims share the same claim ID."
  - [corpus] Weak direct corpus evidence; neighbor papers focus on claim retrieval and fact-checker workflows rather than verdict aggregation.
- Break condition: When overlapping claim detection produces false positives (claims that appear semantically similar but differ in critical details), preserved verdicts may represent different underlying claims.

### Mechanism 2
- Claim: Sentence embeddings with cosine similarity can reliably identify semantically identical claims across fact-checking organizations.
- Mechanism: SBERT generates sentence embeddings; claim pairs with cosine similarity â‰¥0.88 are classified as overlapping (tuned for 95% precision on 1,000 annotated samples).
- Core assumption: Semantic similarity threshold generalizes from the sampled annotation to the full dataset without significant domain shift.
- Evidence anchors:
  - [Section 3.4] "We randomly sampled 1,000 claim pairs with a uniform distribution of the cosine similarity and annotated each pair as overlapping or not. Then, we looked at the cosine similarity threshold value that would have resulted in 95% precision in our sample, which appeared as 0.88."
  - [Section 3.4] "We did not check the overlapping claims manually, so there are likely false positives among them, as well as false negatives."
  - [corpus] Gangopadhyay et al. (2024) used SBERT with cosine similarity for similar claim identification; Quelle et al. (2025) used LaBSE with 0.875 threshold.
- Break condition: When claims share vocabulary but differ in critical modifiers (e.g., "X caused Y" vs. "X may have contributed to Y"), embedding similarity may exceed threshold without semantic equivalence.

### Mechanism 3
- Claim: Political bias detection using pre-trained BERT models can differentiate fact-checking organizations along a left-right spectrum.
- Mechanism: politicalBiasBERT processes fact-check report content and outputs probability distributions over [left, center, right]; organizational bias scores are computed as mean predictions.
- Core assumption: The pre-trained bias model, trained on general political text, transfers validly to fact-checking domain language.
- Evidence anchors:
  - [Section 4.2] "The distribution of scores suggests that fact-checking organisations are more likely to lean towards political left, yet there are a significant number of organisations exhibiting bias towards the right."
  - [Section 4.2] "While the predicted bias scores are not decisive and limited by the accuracy of the model, it implies that fact-checking organisations can also exhibit political bias."
  - [corpus] "Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals' Misbeliefs" (2025) suggests partisan fact-checkers can still correct misperceptions, implying bias detection is complex.
- Break condition: When fact-check reports discuss political claims without endorsing political viewpoints, the model may conflate claim content with organizational stance.

## Foundational Learning

- Concept: **Sentence embeddings and cosine similarity for semantic matching**
  - Why needed here: Core technique for identifying overlapping claims; understanding the threshold-precision tradeoff is essential for evaluating dataset quality.
  - Quick check question: Why does a higher cosine similarity threshold reduce false positives but increase false negatives?

- Concept: **Lucene inverted index structure**
  - Why needed here: Dataset is distributed as a Lucene index; understanding query capabilities (exact match, fuzzy search, boolean queries) is necessary for effective data access.
  - Quick check question: What advantage does an inverted index provide over a simple CSV file for searching claim text?

- Concept: **Verdict normalization via classification**
  - Why needed here: Raw verdicts use organization-specific scales; the three-step normalization (manual mapping â†’ RoBERTa fine-tuning â†’ low-confidence correction) enables cross-organization analysis.
  - Quick check question: Why might a 0.849 classification accuracy be acceptable for this application, and what risks does it introduce?

## Architecture Onboarding

- Component map:
  - CSV files + Lucene 8.11.0 index containing claims, verdicts, metadata (author, date, organization, URL)
  - Collection pipeline: Scrapy spiders with scrapy-playwright for dynamic pages; ClaimReview schema extraction prioritized; 39 organization-specific spiders
  - Preprocessing layer: Language detection (langdetect), deduplication (SBERT, 0.95 threshold), verdict normalization (RoBERTa), overlapping claim detection (SBERT, 0.88 threshold)
  - Query interface: Pyserini for Python access to Lucene index; supports exact/fuzzy matching, boolean queries, relevance ranking

- Critical path:
  1. Load Lucene index via Pyserini
  2. Query by organization, date range, or claim text
  3. Retrieve claim_id to identify overlapping claims
  4. Access normalized_rating field for cross-organization comparison
  5. Apply credibility scoring formula (Eq. 1) if weighted aggregation needed

- Design tradeoffs:
  - Coverage vs. copyright: Full report text excluded from public release; only metadata and claims available
  - Precision vs. recall in overlap detection: 0.88 threshold tuned for 95% precision; authors acknowledge false negatives exist
  - Automation vs. accuracy in verdict normalization: Low-confidence predictions (<0.5) manually corrected; 1,564 labels reviewed

- Failure signatures:
  - High cosine similarity but different claims: False positive overlaps when claims share entities but differ in predicates
  - Verdict misclassification: Non-standard verdict phrasing may map incorrectly to six-point scale
  - Missing author data: 19.13% of reports lack author information, limiting individual-level credibility analysis

- First 3 experiments:
  1. Validate overlapping claim quality: Sample 100 claim pairs at threshold boundary (0.86-0.90) and manually verify semantic equivalence to estimate false positive rate.
  2. Cross-validate verdict normalization: Compare RoBERTa predictions against held-out manual annotations for organizations with non-standard rating scales.
  3. Test verdict consistency on overlapping claims: Compute agreement rate (Cohen's kappa) across fact-checkers for the 2,977 claims investigated by multiple organizations; correlate disagreement with predicted political bias scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should different credibility assessment factors be optimally weighted when calculating fact-checker credibility scores?
- Basis in paper: [explicit] "Since determining the weights for each factor requires further research beyond the scope of this paper, we assign ð‘ ð‘— = 1 for all ð‘—, assuming that all the factors have the same impact on credibility."
- Why unresolved: The paper used equal weights as a simplification, but different factors (experience, fact-checking rate, political bias, word count) likely have varying relevance to actual credibility.
- What evidence would resolve it: Empirical validation comparing different weighting schemes against ground-truth measures of fact-checker reliability, or expert surveys on factor importance.

### Open Question 2
- Question: Can more accurate methods be developed for identifying semantically identical claims across fact-checking organizations?
- Basis in paper: [explicit] "We did not check the overlapping claims manually, so there are likely false positives among them, as well as false negatives that we were unable to identify... developing a more accurate overlapping claim detection method can be useful."
- Why unresolved: The current approach using SBERT embeddings with a 0.88 cosine similarity threshold is imperfect and was not manually validated.
- What evidence would resolve it: Manual annotation of overlapping claims to establish ground truth, enabling evaluation of improved detection algorithms.

### Open Question 3
- Question: How can weighted voting or game-theoretic approaches improve verdict aggregation for overlapping claims compared to simple majority voting?
- Basis in paper: [explicit] "The assignment of credibility scores, therefore, addresses the limitations of the commonly used majority voting approach for aggregating multiple verdicts reached for the same claim. And, it enables the use of more advanced solutions, such as weighted voting games."
- Why unresolved: The paper provides credibility scores but does not implement or validate any advanced aggregation method.
- What evidence would resolve it: Comparative experiments showing whether weighted aggregation produces more accurate consensus verdicts than simple majority voting on held-out claims.

### Open Question 4
- Question: How would including multilingual data change the observed patterns in the fact-checking ecosystem?
- Basis in paper: [explicit] "our dataset only covers reports written in English. Therefore, enhancing FACTors with multilingual data can enable to present a worldwide picture of the fact-checking ecosystem."
- Why unresolved: Geographic and linguistic scope is limited to English-language reports from primarily Western organizations.
- What evidence would resolve it: Expanding the dataset to include non-English sources and comparing ecosystem statistics, bias distributions, and verdict consistency patterns.

## Limitations
- Overlapping claim detection threshold (0.88 cosine similarity) was tuned for 95% precision but not manually validated on the full dataset
- Verdict normalization achieved 0.849 accuracy, which may be insufficient for high-stakes credibility assessment
- Report text excluded from public release due to copyright restrictions, limiting downstream analysis

## Confidence

- **High confidence**: Dataset construction methodology, claim counts (118,112 fact-checks), and coverage period (1995-2025) are well-documented and verifiable.
- **Medium confidence**: Overlapping claim detection threshold and verdict normalization accuracy are supported by specific metrics but rely on assumptions about generalizability.
- **Low confidence**: Political bias detection results depend on transfer learning from a general political text model to the fact-checking domain, with no reported domain adaptation evaluation.

## Next Checks
1. Sample and manually verify 100 claim pairs near the 0.88 similarity threshold (0.86-0.90 range) to estimate actual false positive rate for overlapping claims.
2. Replicate verdict normalization on held-out test set from organizations with non-standard rating scales to verify the 0.849 accuracy claim.
3. Compute inter-rater agreement (Cohen's kappa) across fact-checkers for the 2,977 claims investigated by multiple organizations, then correlate disagreement rates with predicted political bias scores to validate bias detection utility.