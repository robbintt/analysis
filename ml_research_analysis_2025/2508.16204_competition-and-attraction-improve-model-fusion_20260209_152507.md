---
ver: rpa2
title: Competition and Attraction Improve Model Fusion
arxiv_id: '2508.16204'
source_url: https://arxiv.org/abs/2508.16204
tags:
- merging
- diversity
- training
- m2n2
- evolutionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'M2N2 introduces a novel evolutionary approach to model merging
  that addresses key limitations in existing methods. The core innovation is three-fold:
  (1) evolving merging boundaries through dynamic split points rather than fixed parameter
  groups, (2) diversity preservation via resource competition inspired by natural
  niches, and (3) attraction-based parent selection that pairs models based on complementary
  strengths.'
---

# Competition and Attraction Improve Model Fusion

## Quick Facts
- arXiv ID: 2508.16204
- Source URL: https://arxiv.org/abs/2508.16204
- Reference count: 36
- M2N2 achieves state-of-the-art performance in evolving model merging strategies across MNIST, LLM, and diffusion model domains

## Executive Summary
M2N2 introduces a novel evolutionary approach to model merging that addresses key limitations in existing methods. The core innovation is three-fold: (1) evolving merging boundaries through dynamic split points rather than fixed parameter groups, (2) diversity preservation via resource competition inspired by natural niches, and (3) attraction-based parent selection that pairs models based on complementary strengths. The method demonstrates breakthrough capabilities by successfully evolving MNIST classifiers from scratch, achieving performance comparable to CMA-ES while being more computationally efficient. It also scales to complex tasks, merging specialized LLMs and diffusion models to achieve state-of-the-art performance. Notably, the approach preserves crucial model capabilities beyond explicitly optimized objectives, demonstrating robustness and versatility in gradient-free optimization.

## Method Summary
M2N2 is an evolutionary algorithm for model merging that operates through three key mechanisms. First, it uses dynamic split points during model fusion, allowing the algorithm to evolve merging boundaries rather than relying on fixed parameter groups. Second, it implements resource competition where each data sample acts as a limited resource that models must compete for, naturally preserving diversity by preventing any single solution from dominating. Third, it employs an attraction-based parent selection mechanism where Parent A is chosen by fitness and Parent B is chosen based on complementarity—specifically, how well it solves data points that Parent A struggles with. Models are merged using SLERP interpolation with random mixing ratios and split points, creating a continuous exploration of the model space.

## Key Results
- Successfully evolves MNIST classifiers from scratch, achieving accuracy comparable to CMA-ES with greater computational efficiency
- Merges specialized LLMs (WizardMath-7B for math, AgentEvol-7B for agentic tasks) achieving state-of-the-art performance on GSM8k and WebShop
- Combines diffusion models (JSDXL, SDXL 1.0, etc.) achieving superior results on COCO image generation benchmarks

## Why This Works (Mechanism)
The method works by creating an evolutionary pressure that simultaneously optimizes for performance and diversity. Resource competition ensures that models must specialize to different data samples, preventing premature convergence to local optima. The attraction mechanism ensures that successful merges combine complementary strengths rather than similar capabilities, accelerating the discovery of well-rounded solutions. Dynamic split points allow the algorithm to discover optimal merging strategies rather than being constrained by human-designed parameter groupings.

## Foundational Learning
- **SLERP interpolation**: Spherical linear interpolation for blending model parameters smoothly; needed for creating intermediate models between parents
- **Resource competition**: Each data point acts as a limited resource that models compete for; needed to maintain diversity and prevent single solutions from dominating
- **Attraction-based selection**: Choosing Parent B based on complementarity to Parent A's weaknesses; needed to accelerate the discovery of well-rounded solutions
- **Dynamic split points**: Randomly selecting where to split model parameters for merging; needed to discover optimal merging boundaries rather than using fixed groupings
- **Evolutionary archive**: Maintaining a population of diverse models; needed to preserve multiple solutions and enable recombination

## Architecture Onboarding

**Component Map:**
Archive (20-15 models) -> Parent Selection (Fitness + Attraction) -> SLERP Merge (Dynamic split, mixing ratio) -> Fitness Evaluation (Resource competition) -> Archive Update

**Critical Path:**
1. Archive initialization with random/specialist models
2. Parent A selection by fitness
3. Parent B selection by attraction score
4. SLERP merge with random parameters
5. Fitness evaluation with resource competition
6. Archive replacement if candidate is better

**Design Tradeoffs:**
- Fixed archive size vs. unlimited population (limits memory but maintains diversity pressure)
- Random vs. learned split points (simpler but may miss optimal boundaries)
- Resource competition vs. pure fitness selection (preserves diversity but adds complexity)

**Failure Signatures:**
- Training coverage plateaus early (<50% data solved) → premature convergence
- All models converge to similar solutions → insufficient diversity pressure
- Merged models perform worse than parents → SLERP implementation issues or incompatible model states

**First Experiments:**
1. Verify basic SLERP implementation by merging identical models with different mixing ratios
2. Test parent selection mechanism by running with attraction disabled and comparing coverage
3. Validate resource competition by checking if models specialize to different data samples

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can a standardized metric for model compatibility be defined and utilized as a regularization term during pre-processing (e.g., fine-tuning) to ensure successful merging?
- **Basis in paper:** The authors state in the Limitations section that "a standardized metric for model compatibility has yet to be established" and suggest that defining one could allow for better control over model compatibility.
- **Why unresolved:** The paper identifies that merging becomes impractical when models deviate significantly from their base models (divergent state representations), but currently lacks a quantitative measure to predict or constrain this incompatibility.
- **What evidence would resolve it:** The proposal of a specific metric that correlates with merge success rates, coupled with a demonstration that using this metric as a regularization loss during fine-tuning results in models that merge more effectively than non-regularized baselines.

### Open Question 2
- **Question:** Does strong evolutionary pressure exist for co-evolving models to remain compatible for merging, such that incompatible models face extinction?
- **Basis in paper:** The paper hypothesizes: "We believe there is a strong evolutionary pressure for models that are co-evolving together to remain compatible for merging... Testing this hypothesis through further research would provide valuable insights."
- **Why unresolved:** While the authors propose this dynamic, they do not perform the longitudinal analysis required to confirm that divergence leads to an inability to produce viable offspring and subsequent removal from the population.
- **What evidence would resolve it:** An analysis of evolutionary runs tracking the "compatibility trajectory" of lineages, showing that branches which drift towards incompatible state representations fail to produce high-fitness offspring and are subsequently pruned from the archive.

### Open Question 3
- **Question:** Can incorporating a compatibility metric into the attraction heuristic facilitate the formation and co-evolution of distinct "species" of models?
- **Basis in paper:** The authors suggest that "incorporating a compatibility metric into the attraction heuristic could facilitate the co-evolution of distinct species of models, defined as groups that merge with one another but not with others."
- **Why unresolved:** The current M2N2 method pairs models based on complementary strengths (attraction) but does not explicitly filter pairings based on structural or representational compatibility, nor does it attempt to evolve distinct species clusters.
- **What evidence would resolve it:** A modified version of M2N2 that includes a compatibility check in the mating pool, resulting in distinct clusters of models that merge successfully intra-cluster but not inter-cluster, potentially optimizing for diverse global capabilities.

## Limitations
- Computational efficiency claims relative to CMA-ES are based on single comparisons rather than systematic benchmarking
- Resource competition mechanism's sensitivity to hyperparameters (α and capacity limits c_j) is not thoroughly explored
- The method requires careful tuning of multiple hyperparameters including mutation rates and sampling distributions

## Confidence
- **High Confidence:** Core algorithm mechanics (SLERP merging with dynamic split-points, fitness calculation with resource competition)
- **Medium Confidence:** Cross-domain generalization claims and efficiency comparisons
- **Medium Confidence:** Capability preservation claims, though these are empirically demonstrated

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary the competition parameter α, resource capacity limits, and mutation rates to establish robustness bounds and identify failure modes
2. **Scaling Study:** Test M2N2 on increasingly complex model architectures and task types to determine practical limitations and scaling behavior
3. **Comparison Framework:** Conduct controlled experiments comparing M2N2 against multiple gradient-free optimization baselines across identical problem sets to validate efficiency and effectiveness claims