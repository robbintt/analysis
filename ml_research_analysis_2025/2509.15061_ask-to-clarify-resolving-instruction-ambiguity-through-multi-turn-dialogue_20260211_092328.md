---
ver: rpa2
title: 'Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue'
arxiv_id: '2509.15061'
source_url: https://arxiv.org/abs/2509.15061
tags:
- framework
- embodied
- action
- training
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Ask-to-Clarify, a framework for embodied
  agents that can resolve ambiguous instructions through multi-turn dialogue and then
  execute low-level actions. The key innovation is a two-stage knowledge-insulation
  training strategy: first fine-tuning a VLM to ask clarifying questions, then integrating
  a diffusion model for action generation while preserving the dialogue ability.'
---

# Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue

## Quick Facts
- **arXiv ID**: 2509.15061
- **Source URL**: https://arxiv.org/abs/2509.15061
- **Reference count**: 34
- **Primary result**: Framework achieves 95.0%-98.3% success rates on 8 real-world tasks, outperforming baselines with 0% success

## Executive Summary
Ask-to-Clarify introduces a framework for embodied agents that resolve ambiguous instructions through multi-turn dialogue before executing actions. The system combines a vision-language model for asking clarifying questions with a diffusion model for action generation, using a knowledge-insulation training strategy to preserve dialogue capabilities while learning actions. Evaluated on real-world household tasks, the framework significantly outperforms state-of-the-art visual-language agents, demonstrating that collaborative dialogue can enhance embodied task execution in ambiguous environments.

## Method Summary
The framework employs a two-stage training strategy: first fine-tuning a vision-language model (VLM) to ask clarifying questions about ambiguous instructions, then integrating a diffusion model for action generation while preserving the dialogue ability through knowledge-insulation. A connection module using Feature-wise Linear Modulation (FiLM) adjusts visual observations based on instructions to create reliable conditions for the action component. During inference, a signal detector switches between asking questions and taking actions based on extracted tokens from the VLM's output. The system was evaluated on 8 real-world household tasks requiring object identification and manipulation.

## Key Results
- Achieved average success rates of 95.0%, 98.3%, and 90.0% across three task types
- Outperformed state-of-the-art VLAs with most baselines achieving 0% success
- Demonstrated effective multi-turn dialogue capability for resolving ambiguous instructions

## Why This Works (Mechanism)
The framework works by separating the ambiguity resolution process from action execution through a two-stage training pipeline. The VLM first handles dialogue to clarify ambiguous instructions, while the diffusion model focuses purely on action generation. The knowledge-insulation approach prevents the diffusion model from degrading the VLM's dialogue capabilities, maintaining the agent's ability to ask questions even after learning actions. The connection module ensures visual observations are properly conditioned on language instructions, creating a reliable bridge between perception and action.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Needed for understanding and generating language about visual inputs; quick check: verify the VLM can correctly identify objects in images
- **Diffusion Models for Actions**: Required for generating continuous action trajectories; quick check: test if the diffusion model can generate smooth motion sequences
- **Feature-wise Linear Modulation (FiLM)**: Used to condition visual features on language instructions; quick check: confirm FiLM layers properly scale and shift visual features
- **Signal Detection for Mode Switching**: Critical for routing between dialogue and action modes; quick check: test the signal detector's accuracy in identifying when to ask questions versus act
- **Knowledge-Insulation Training**: Technique to preserve pre-trained capabilities while adding new skills; quick check: measure dialogue performance before and after action training
- **Multi-turn Dialogue Systems**: Essential for resolving ambiguity through back-and-forth interaction; quick check: verify the system can maintain context across multiple clarification turns

## Architecture Onboarding

**Component Map**: Camera/Odometry -> VLM -> Signal Detector -> (FiLM Connection) -> Diffusion Expert -> Robot Actions

**Critical Path**: Visual Input → VLM Clarification → Signal Detection → Action Generation → Robot Execution

**Design Tradeoffs**: The framework trades off pure end-to-end training for a staged approach that preserves dialogue ability but requires more complex inference routing. The FiLM connection module prioritizes simplicity over potentially more powerful fusion methods like cross-attention.

**Failure Signatures**: 
- VLM fails to generate proper signal tokens → incorrect mode switching
- Diffusion model hallucinates actions not aligned with clarified instructions
- FiLM module inadequately conditions visual features → poor action generation
- Signal detector misclassifies clear instructions as ambiguous

**Three First Experiments**:
1. Test signal detector robustness by providing instructions at varying ambiguity levels
2. Evaluate FiLM conditioning effectiveness by comparing action success with and without proper instruction conditioning
3. Measure knowledge-insulation effectiveness by testing dialogue ability after full training

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can alternative connection module designs, such as CLIP-style contrastive loss or cross-attention mechanisms, outperform the currently used Feature-wise Linear Modulation (FiLM) in aligning visual observations with language instructions?
- Basis in paper: [explicit] The "Future Work" section explicitly identifies exploring alternative connection module designs, specifically proposing CLIP-style contrastive loss or cross-attention, as a promising research direction.
- Why unresolved: The current implementation relies on FiLM for its simplicity, but the authors suggest other methods might enforce stronger semantic conditions or offer more fine-grained fusion for the diffusion expert.
- What evidence would resolve it: Comparative experiments replacing the FiLM connection module with contrastive or cross-attention mechanisms, measuring success rates on the same ambiguity-resolution tasks.

### Open Question 2
- Question: How robust is the ambiguity-solving ability when trained on LLM-generated dialogue data compared to data derived from real human conversational patterns?
- Basis in paper: [inferred] Section IV-A notes that the stage 1 training data is generated by Qwen3-235B. While effective, synthetic ambiguity patterns may diverge from the nuances of real human ambiguity.
- Why unresolved: The paper validates the framework on synthetic dialogue logic but does not evaluate the domain gap between LLM-generated questions and the diverse, unpredictable nature of human clarification requests.
- What evidence would resolve it: A user study where human participants interact with the agent, comparing performance metrics between agents trained on synthetic data versus those trained on human-annotated dialogue data.

### Open Question 3
- Question: How sensitive is the inference pipeline to VLM token generation errors, specifically regarding the reliability of the signal detector in switching between dialogue and action modes?
- Basis in paper: [inferred] Section III-C describes a "training-free signal detector" that routes behavior based on the extraction of specific tokens (`<AMBG>`, `<ACT>`).
- Why unresolved: The paper assumes the VLM generates these signal tokens reliably. If the VLM hallucinates or fails to generate the exact token string, the routing logic could fail, but this robustness is not analyzed.
- What evidence would resolve it: An analysis of failure rates specifically attributed to signal detection errors, or stress-testing the system with noisy inputs that might prompt the VLM to deviate from the required formatting.

## Limitations
- Evaluation limited to 8 real-world tasks in a single household environment, raising generalization concerns
- Performance metrics lack precision about what constitutes "success" and task complexity variations
- Knowledge-insulation approach effectiveness not empirically validated beyond reported success rates
- Signal detector robustness not thoroughly tested against false positives/negatives in ambiguous scenarios

## Confidence
- **High Confidence**: The core architectural approach of combining dialogue-based ambiguity resolution with action generation is technically sound and represents a meaningful contribution to embodied AI
- **Medium Confidence**: The reported performance improvements over baselines are significant, but the evaluation methodology and baseline selection raise questions about external validity
- **Low Confidence**: The claim of preserving "dialogue ability" during diffusion model integration lacks empirical validation, and the robustness of the signal detector mechanism is not thoroughly tested

## Next Checks
1. **Cross-Environment Generalization**: Evaluate the framework on tasks from multiple household environments (different layouts, lighting conditions, object distributions) to assess true generalization beyond the initial test set

2. **Baseline Fairness Assessment**: Re-evaluate using more competitive baselines, particularly those with established dialogue capabilities, and provide detailed ablation studies showing the individual contributions of the knowledge-insulation training strategy and connection module

3. **Robustness Testing of Signal Detection**: Systematically test the signal detector's performance under varying levels of instruction ambiguity, including cases where instructions are borderline ambiguous versus clearly ambiguous, to quantify false positive and false negative rates in the ask/act switching mechanism