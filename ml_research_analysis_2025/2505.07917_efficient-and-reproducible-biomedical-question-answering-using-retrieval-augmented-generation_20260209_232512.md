---
ver: rpa2
title: Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented
  Generation
arxiv_id: '2505.07917'
source_url: https://arxiv.org/abs/2505.07917
tags:
- retrieval
- biomedical
- documents
- system
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a scalable and reproducible Retrieval-Augmented\
  \ Generation (RAG) system for biomedical question-answering, addressing challenges\
  \ in accuracy, efficiency, and scalability. A hybrid retrieval approach combining\
  \ BM25 (lexical retrieval) with MedCPT\u2019s cross-encoder (semantic reranking)\
  \ is implemented and evaluated on PubMed datasets."
---

# Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2505.07917
- Source URL: https://arxiv.org/abs/2505.07917
- Reference count: 17
- Key outcome: Hybrid retrieval (BM25 + MedCPT) achieves 0.90 accuracy at 50 documents, with 82ms BM25 retrieval time

## Executive Summary
This paper presents a scalable and reproducible Retrieval-Augmented Generation (RAG) system for biomedical question-answering, addressing challenges in accuracy, efficiency, and scalability. A hybrid retrieval approach combining BM25 (lexical retrieval) with MedCPT's cross-encoder (semantic reranking) is implemented and evaluated on PubMed datasets. Results show that retrieving 50 documents with BM25 before reranking with MedCPT optimally balances accuracy (0.90), recall (0.90), and response time (1.91s), with BM25 retrieval time remaining stable at 82ms. The system outperforms standalone retrieval methods in end-to-end performance and is fully reproducible with open-source code. Limitations include reliance on OpenAI's GPT-3.5 and limited retriever/database evaluations. Future work will explore additional retrievers, open-source LLMs, and real-time biomedical applications.

## Method Summary
The system implements a two-stage hybrid retrieval pipeline: BM25 performs initial lexical retrieval from a 24M PubMed document corpus, retrieving k candidate documents (typically 50), followed by MedCPT's cross-encoder that reranks these candidates based on semantic relevance. The top-10 reranked documents are then passed to GPT-3.5-turbo for answer generation, with explicit prompting to cite PMIDs and ground responses in the provided context. The pipeline is evaluated on the BioASQ Task-B benchmark using standard QA metrics (accuracy, recall, precision, F1) and retrieval efficiency metrics (indexing speed, response time). Optimal configuration balances accuracy (0.90 F1) with efficiency (82ms BM25 retrieval, 1.91s total response time).

## Key Results
- Hybrid retrieval (BM25 + MedCPT) achieves 0.90 accuracy and 0.90 recall with 50 retrieved documents
- BM25 retrieval time remains stable at 82ms regardless of document count
- System response time averages 1.91s with 50-document configuration
- Accuracy drops to 0.87 and latency increases with 100 documents, indicating diminishing returns

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Hybrid Retrieval Filtering
Combining lexical (BM25) and semantic (MedCPT) retrieval maximizes recall while maintaining manageable latency. A probabilistic retriever (BM25) first performs a broad, efficient filter to reduce the search space from 24M documents to a candidate set (e.g., 50). A neural cross-encoder (MedCPT) then reranks this subset with higher semantic precision. This offloads the heavy computational cost of deep neural scoring to a small subset of data.

### Mechanism 2: Retrieval Depth Optimization
Increasing the number of retrieved documents (k) has diminishing returns and can negatively impact accuracy and latency beyond a specific threshold. A larger context window introduces more noise. While recall might theoretically improve, the LLM (GPT-3.5) may struggle to disregard irrelevant documents, and the reranking latency increases linearly.

### Mechanism 3: Citation-Grounded Prompting
Explicit instruction to cite PubMed IDs (PMIDs) reduces hallucinations and enforces verifiability. The prompt forces the model to act as a synthesizer rather than a generator. By demanding a specific output format (JSON with used PMIDs), the model is constrained to extract information strictly from the provided context window.

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval**
  - Why needed: The paper relies on distinct trade-offs between BM25 (sparse/lexical) and MedCPT (dense/semantic). You cannot debug the hybrid system without understanding that BM25 looks for keyword matches while dense retrievers look for conceptual similarity.
  - Quick check: If a user searches for "heart attack" but the documents say "myocardial infarction," which retriever type handles this better natively? (Answer: Dense/Semantic).

- **Concept: Cross-Encoders vs. Bi-Encoders**
  - Why needed: The system uses MedCPT as a cross-encoder for reranking, not initial retrieval. You must understand that cross-encoders are computationally expensive (comparing Query-Doc pairs deeply) and thus cannot run on 24M documents in real-time.
  - Quick check: Why do we separate the "retrieval" (BM25) from the "reranking" (MedCPT) steps? (Answer: Computational efficiency/Cost).

- **Concept: Context Window and Latency**
  - Why needed: The paper identifies a specific "sweet spot" (50 docs) based on latency. This requires understanding that LLM inference time increases with input token count and that neural reranking scales with the number of documents passed to it.
  - Quick check: Why didn't the authors rerank the entire 24M PubMed corpus? (Answer: Latency would be prohibitive).

## Architecture Onboarding

- **Component map:** User Query -> Elasticsearch BM25 (Retrieve 50 docs, ~82ms) -> MedCPT Reranker (Score & Select Top 10, ~800ms-1s) -> Prompt Construction -> GPT-3.5 (Generation, ~1s) -> JSON Response

- **Critical path:** User Query → Elasticsearch BM25 (Retrieve 50 docs, ~82ms) → MedCPT Reranker (Score & Select Top 10, ~800ms-1s) → Prompt Construction → GPT-3.5 (Generation, ~1s) → JSON Response

- **Design tradeoffs:**
  - Latency vs. Accuracy: Increasing `k` (retrieval depth) from 20 to 50 improves F1 from 0.88 to 0.90 but doubles retrieval time. Increasing to 100 degrades accuracy.
  - Reproducibility vs. Performance: The system relies on GPT-3.5 (proprietary) for generation, which contradicts the goal of full reproducibility/open-source, acknowledged by the authors as a limitation.

- **Failure signatures:**
  - High Latency (>3s): Likely bottleneck in the MedCPT reranking step or OpenAI API. Check the number of documents passed to the cross-encoder.
  - Hallucinated Citations: Model inventing PMIDs. Check if the prompt strictly forbids external knowledge or if retrieved context is empty.
  - Low Recall (Missing relevant docs): BM25 failing to surface relevant terms in the top 50. Check if synonyms are handled or if `k` needs to be expanded slightly despite latency costs.

- **First 3 experiments:**
  1. Baseline Reproduction: Index the 2.4M subset into Elasticsearch and verify BM25 retrieval latency matches the reported ~82ms.
  2. Retrieval Depth Ablation: Run the pipeline with k=20, 50, 100 on a sample of BioASQ questions and plot the Accuracy vs. Latency curve to verify the "diminishing returns" inflection point at 50.
  3. Reranker Impact: Compare the end-to-end answer quality (F1 score) of "BM25 only" vs. "BM25 + MedCPT" to quantify the value added by the expensive reranking step.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on proprietary GPT-3.5 for generation creates reproducibility gap despite open-source retrieval components
- Optimal retrieval depth (k=50) is empirically derived but lacks theoretical justification for generalization across domains
- Hybrid approach assumes BM25 can surface relevant documents in top 50, which may fail for synonym-heavy queries with minimal lexical overlap
- System evaluated only on BioASQ benchmark, limiting generalizability to real-world clinical applications

## Confidence
- **High confidence:** Retrieval efficiency metrics (82ms BM25 baseline), hybrid retrieval mechanism's effectiveness (F1 improvement from 0.88 to 0.90 at k=50), and reproducibility of retrieval pipeline codebase
- **Medium confidence:** Generalizability of k=50 sweet spot across different biomedical question types, specific contribution of MedCPT reranking versus BM25 alone
- **Low confidence:** Clinical applicability claims without real-world testing, performance on questions requiring deep semantic understanding beyond lexical/semantic hybrid retrieval

## Next Checks
1. Cross-domain validation: Test k=50 optimal configuration on PubMedQA dataset to verify retrieval depth sweet spot generalizes beyond BioASQ
2. Synonym sensitivity analysis: Create test queries with medical synonyms (e.g., "heart attack" vs "myocardial infarction") to measure BM25's failure rate and quantify hybrid approach's robustness
3. Open-source LLM substitution: Replace GPT-3.5 with Llama 3 in the pipeline to assess practical impact of proprietary dependency on reproducibility and performance