---
ver: rpa2
title: 'Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration
  for Long Document Summarization'
arxiv_id: '2509.20900'
source_url: https://arxiv.org/abs/2509.20900
tags:
- summary
- summq
- quiz
- summarization
- combo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUMMQ, an adversarial multi-agent framework
  for long document summarization that integrates summarization and quizzing tasks
  through collaborative intelligence between specialized agents. The framework employs
  summary generators and reviewers for creating comprehensive summaries, while quiz
  generators and reviewers create comprehension questions that serve as continuous
  quality checks, with an examinee agent validating whether summaries contain information
  needed to answer quiz questions.
---

# Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization

## Quick Facts
- **arXiv ID:** 2509.20900
- **Source URL:** https://arxiv.org/abs/2509.20900
- **Reference count:** 40
- **Primary result:** SUMMQ achieves ROUGE-L 18.24 on MENSA vs 17.11 for GPT-5, demonstrating significant gains through adversarial multi-agent quiz-based summarization

## Executive Summary
SUMMQ introduces an adversarial multi-agent framework that integrates summarization and quizzing tasks through collaborative intelligence between specialized agents. The framework employs summary generators and reviewers for creating comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks. An examinee agent validates whether summaries contain information needed to answer quiz questions. Extensive experiments on three long document summarization benchmarks demonstrate that SUMMQ significantly outperforms existing state-of-the-art methods while excelling in LLM-as-a-Judge and human evaluations.

## Method Summary
SUMMQ is a multi-agent framework for long document summarization that creates adversarial collaboration between summarization and quizzing domains. The system uses parallel pipelines with specialized agents: Summary Generators create drafts that are aggregated and voted on, Quiz Generators create comprehension questions, and Summary/Quiz Reviewers evaluate outputs and flag issues. A critical Examinee agent tests whether summaries can answer quiz questions, generating feedback for refinement. The framework operates in multiple iterations (default T_iter=3) where feedback from all agents is consolidated and used to improve summaries. Two configurations are tested: SUMMQ_SOLO (single agent per component) and SUMMQ_COMBO (3 agents per component), with the latter achieving significantly better results at higher computational cost.

## Key Results
- SUMMQ achieves ROUGE-L 18.24 on MENSA (vs 17.11 for GPT-5) and 20.38 on BookSum (vs 12.38 for GPT-5)
- Multi-agent collaboration shows substantial quality gains: SUMMQ_COMBO outperforms SUMMQ_SOLO across all metrics
- Quizzing mechanism proves crucial: removing it causes R-1 to drop by 5.00 and BERTScore-F1 by 6.40
- Performance peaks at 3 iterations; additional iterations show diminishing or negative returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The quizzing mechanism improves summary coverage by creating an adversarial signal that forces summaries to retain answerable information.
- Mechanism: Quiz generators create comprehension questions from the source document; an examinee agent attempts to answer these questions using only the generated summary. If the summary lacks information needed to answer correctly, this generates targeted feedback that guides refinement. This creates a verification loop where quiz questions serve as proxy tests for information retention.
- Core assumption: Quiz questions that cannot be answered from a summary indicate genuine information gaps rather than poorly constructed questions.
- Evidence anchors:
  - [abstract] "adversarial dynamic... enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions"
  - [section 5, Table 6] Removing the quizzing mechanism causes R-1 to drop by 5.00 and BERTScore-F1 by 6.40 for SUMMQ_SOLO
  - [corpus] Limited direct corpus evidence; related work focuses on hallucination detection (Discourse-Driven Evaluation) and multi-agent content creation (SlideBot) rather than quiz-based summarization verification
- Break condition: If quiz questions are poorly calibrated (too specific, ambiguous, or trivial), feedback may not correlate with actual summary quality improvements.

### Mechanism 2
- Claim: Multi-agent collaboration with structured voting reduces information loss compared to single-agent summarization.
- Mechanism: Multiple generator agents independently draft summaries, emphasizing different aspects of the document. An aggregator combines drafts; a ranker selects the best individual draft; all generators vote between the aggregated and best draft. This captures diverse perspectives while preventing any single agent's blind spots from dominating.
- Core assumption: Independent agents will produce meaningfully different drafts, and collective voting correlates with summary quality.
- Evidence anchors:
  - [section 5, Table 2] All components benefit from multi-agent collaboration; Summary Generators and Reviewers show the largest gains
  - [section 5, Table 4] ROUGE-L improves from 17.12 (one agent) to 18.02 (two agents), with diminishing returns beyond three agents
  - [corpus] SlideGen (arXiv:2512.04529) demonstrates collaborative multimodal agents for slide generation, suggesting multi-agent benefits extend across content creation tasks
- Break condition: If agents converge on similar drafts due to shared training data or prompts, diversity gains disappear while costs increase.

### Mechanism 3
- Claim: Structured debate among reviewers filters spurious feedback and improves review accuracy.
- Mechanism: Reviewers independently annotate issues; issues flagged by multiple reviewers are accepted automatically; contested issues undergo structured debate rounds where reviewers present evidence before majority voting. This prevents individual reviewer biases from generating false positive feedback.
- Core assumption: Reviewers can distinguish valid from invalid issues when presented with evidence during debate.
- Evidence anchors:
  - [section 3.3, Algorithm 3] Contested issues undergo T_debate rounds with evidence-based argumentation before majority vote
  - [section 5] Quiz coverage becomes more balanced across document segments by iteration 3, suggesting review feedback effectively identifies middle-segment omissions
  - [corpus] Discourse-Driven Evaluation (arXiv:2502.06185) finds factual inconsistency errors are more common in complex sentences, suggesting debate may help surface such issues
- Break condition: If debate rounds fail to surface decisive evidence, contested issues may be incorrectly accepted or rejected based on majority bias.

## Foundational Learning

- Concept: Multi-agent collaboration patterns (debate, voting, aggregation)
  - Why needed here: The entire framework depends on coordinating multiple LLM agents with specialized roles. Understanding how to structure agent interactions—including when to use independent generation vs. consensus mechanisms—is essential.
  - Quick check question: Given three reviewers who disagree on whether a summary contains a hallucination, should you use debate, voting, or aggregation to resolve it?

- Concept: Adversarial signal generation for quality assurance
  - Why needed here: The quizzing mechanism creates an adversarial pressure that improves summaries. Understanding how to design task pairs where one agent's output quality constrains another's is the core insight.
  - Quick check question: If quiz questions are too easy, what happens to the quality signal they provide for summary refinement?

- Concept: Iterative refinement with feedback consolidation
  - Why needed here: The framework uses multi-source feedback (summary reviewers, quiz reviewers, examinee) merged across iterations. Understanding how to consolidate and prioritize feedback streams prevents contradictory signals from degrading outputs.
  - Quick check question: If summary reviewers and examinee feedback disagree on whether a specific detail should be included, which takes priority and why?

## Architecture Onboarding

- Component map: Document → Summary Generators → (candidate summary) → Examinee answers Quiz → Feedback merged → Summary refinement. The quiz-summarization adversarial loop is the primary driver of quality gains.
- Critical path: Document → Summary Generators → (candidate summary) → Examinee answers Quiz → Feedback merged → Summary refinement. The quiz-summarization adversarial loop is the primary driver of quality gains.
- Design tradeoffs:
  - Number of agents per component: More agents improve quality but increase cost ($0.18 → $14.45 per instance in Table 8)
  - Number of iterations: Performance peaks at 3 iterations (Table 3); further iterations show diminishing or negative returns
  - Agent backbone selection: Stronger models (GPT-4.1, O3, GPT-5) outperform weaker ones; mixing model types can leverage complementary strengths (Table 7)
- Failure signatures:
  - Quiz questions cluster at document start/end (Figure 4, iteration 1): indicates shallow document processing
  - Feedback does not converge to empty after 3 iterations: suggests agent configuration or prompt issues
  - Contested issues remain unresolved after debate: may indicate unclear rubric or document ambiguity
- First 3 experiments:
  1. Reproduce the ablation in Table 6 (remove quizzing mechanism) on a subset of MENSA to validate the adversarial signal contribution before full implementation
  2. Run SUMMQ_SOLO (single agent per component) vs. SUMMQ_COMBO (3 agents) on 10 documents to measure quality vs. cost tradeoff for your use case
  3. Test different iteration limits (1, 2, 3, 4) on documents of varying lengths to find the optimal stopping criterion; if performance degrades at iteration 4, your documents may be over-refining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are optimal stopping criteria for the iterative refinement process beyond the current empty-feedback check, particularly to prevent oscillation or over-refinement scenarios?
- Basis in paper: [explicit] The authors note in Table 3 that "more iterations of refinement does not always lead to better summaries" and that "too many can introduce noise or over-refinement, indicating an optimal balance is needed."
- Why unresolved: The current system uses a simple termination condition (empty feedback) but provides no mechanism to detect diminishing returns, oscillation between states, or degradation in quality during later iterations.
- What evidence would resolve it: Systematic study of iteration dynamics with quality metrics tracked at each step, analysis of convergence patterns across document types, and development of adaptive stopping criteria based on marginal improvement thresholds.

### Open Question 2
- Question: How can the substantial computational cost of SUMMQ COMBO ($14.45 per instance vs. $0.18 for prompting) be reduced while maintaining summary quality gains?
- Basis in paper: [explicit] The cost analysis in Table 8 and Appendix D shows SUMMQ COMBO consumes 3.30M input tokens and 24.96K output tokens per instance, with the authors acknowledging "trade-off between summarization quality and managing computational cost."
- Why unresolved: While the authors justify the cost through quality improvements, no strategies are explored for cost reduction such as selective agent activation, early termination heuristics, or caching intermediate results across iterations.
- What evidence would resolve it: Ablation studies testing cost reduction strategies (e.g., reducing agent count for low-difficulty documents, sharing computation across quiz and summary generation), Pareto frontier analysis of cost vs. quality trade-offs.

### Open Question 3
- Question: To what extent does the quiz generation mechanism capture all important document information, and how sensitive is final summary quality to quiz coverage and quality?
- Basis in paper: [inferred] The paper shows removing quizzing causes performance drops (Table 6), but quiz quality is validated only by reviewer agents with no ground truth reference, and the fixed 10-10-10 distribution of question types is arbitrary.
- Why unresolved: There is no independent verification that generated quizzes comprehensively cover all salient document information, nor analysis of whether different quiz distributions or question types would yield different summary improvements.
- What evidence would resolve it: Human annotation of quiz coverage relative to gold-standard document key points, experiments varying question type distributions and counts, correlation analysis between quiz coverage metrics and downstream summary quality.

### Open Question 4
- Question: Why does SUMMQ show weaker relative improvement on GovReport compared to MENSA and BookSum, and what does this reveal about domain-specific applicability?
- Basis in paper: [explicit] On GovReport (Table 1), supervised methods like U-FORMER and SLED achieve competitive ROUGE-L scores (27.60, 27.40) compared to SUMMQ COMBO (21.76), while SUMMQ dramatically outperforms baselines on MENSA and BookSum.
- Why unresolved: The paper does not analyze whether this is due to GovReport's characteristics (government reports vs. narratives), the nature of reference summaries, or fundamental limitations of the quiz-based approach for certain document types.
- What evidence would resolve it: Comparative analysis of document characteristics across benchmarks, study of quiz effectiveness by document genre, investigation of whether task-specific training data gives supervised methods advantages that multi-agent prompting cannot replicate.

## Limitations

- The quizzing mechanism's effectiveness depends heavily on question quality—the paper assumes that unanswerable quiz questions indicate summary information gaps, but poorly constructed questions could generate spurious feedback.
- High computational costs (up to $14.45 per instance for SUMMQ_COMBO) limit practical deployment. The scaling experiments suggest diminishing returns beyond 3 agents per component, but the optimal balance point depends heavily on document characteristics and available resources.
- The framework's performance gains are benchmark-specific. While ROUGE scores improve significantly on MENSA (18.24 vs 17.11) and BookSum (20.38 vs 12.38), these benchmarks may not generalize to all long document domains, particularly those with different structural properties than books and government reports.

## Confidence

- High confidence: The multi-agent collaboration framework is technically sound and the ablation studies (Table 6) clearly demonstrate that removing components degrades performance.
- Medium confidence: The adversarial quiz-summarization loop genuinely improves coverage rather than just adding complexity. While the mechanism is plausible and the evidence supports it, the causal relationship between quiz feedback and summary quality could be confounded by other factors like iteration count.
- Low confidence: The framework's robustness across document types and lengths. The paper focuses on three specific benchmarks without exploring edge cases like highly technical documents, narrative vs. expository texts, or documents with varying structures.

## Next Checks

1. Test SUMMQ on documents from domains not represented in the original benchmarks (e.g., medical research papers, legal contracts) to assess domain generalization and identify whether the quiz mechanism remains effective when document structures differ significantly from books and reports.
2. Conduct a controlled ablation where quiz questions are intentionally made either too easy or too difficult, then measure whether the framework's quality gains disappear or become spurious, validating that the adversarial signal genuinely reflects information coverage rather than question difficulty.
3. Run SUMMQ_COMBO with varying agent counts (1, 2, 3, 4 per component) on a fixed set of documents to empirically determine the optimal agent count for different document lengths, verifying whether the diminishing returns observed in the paper hold across document types.