---
ver: rpa2
title: Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards
arxiv_id: '2512.21625'
source_url: https://arxiv.org/abs/2512.21625
tags:
- step
- entropy
- training
- length
- aime25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic investigation into how positive
  and negative samples affect RLVR training dynamics and behaviors. We find that positive
  samples sharpen existing correct reasoning patterns, while negative samples encourage
  exploration of new reasoning paths.
---

# Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards

## Quick Facts
- **arXiv ID**: 2512.21625
- **Source URL**: https://arxiv.org/abs/2512.21625
- **Reference count**: 40
- **Primary result**: Adaptive asymmetric token-level advantage shaping improves RLVR reasoning accuracy across 5 benchmarks.

## Executive Summary
This paper investigates how positive and negative samples affect RLVR training dynamics, revealing that positive samples sharpen existing reasoning patterns while negative samples drive exploration of new reasoning paths. The authors propose A3PO, an adaptive and asymmetric token-level advantage shaping method that allocates advantages more precisely across different polarities. Experiments on five reasoning benchmarks demonstrate significant improvements in reasoning accuracy.

## Method Summary
A3PO builds on DAPO/GRPO by introducing asymmetric token-level advantage shaping. The method scales advantages for low-probability tokens in positive samples and high-probability tokens in negative samples, with adaptive decay. Training uses 16×H200 GPUs, batch size 512, and runs for 300 steps with AdamW (lr=1e-6). The approach is evaluated on Qwen2.5-7B-Math, Qwen3-8B-Base, and DeepSeek-R1-Distill-Qwen-7B across AIME24, AIME25, MATH500, GPQA, and LiveCodeBench benchmarks.

## Key Results
- A3PO achieves significant accuracy improvements across all five reasoning benchmarks compared to baseline methods.
- Positive samples consistently reduce token entropy, sharpening correct reasoning patterns.
- Negative samples increase token entropy, encouraging exploration of new reasoning paths.
- The optimal positive-to-negative advantage ratio identified is 0.5 for the tested base models.

## Why This Works (Mechanism)
The effectiveness of A3PO stems from recognizing that positive and negative samples serve distinct roles in RLVR training. Positive samples reinforce and sharpen existing correct reasoning patterns, which manifests as reduced token entropy during generation. Negative samples, conversely, encourage exploration of alternative reasoning paths, increasing entropy and enabling discovery of new solution strategies. By applying asymmetric advantage shaping at the token level—emphasizing low-probability correct tokens in positives and high-probability incorrect tokens in negatives—A3PO more precisely targets the learning signals that matter most for reasoning improvement.

## Foundational Learning
- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Training framework where LLM policy is optimized using binary rewards based on final answer correctness. Why needed: Provides the foundation for understanding how reward signals propagate through the model during training.
- **Advantage Function**: Measures how much better a particular action is compared to the average action at that state. Why needed: Central to policy gradient methods and determines the magnitude of parameter updates.
- **Policy Gradient Methods**: Optimization techniques that directly adjust model parameters based on reward signals. Why needed: The underlying mechanism through which A3PO shapes model behavior.
- **Token-Level Advantage Shaping**: Technique of applying different advantage scales to individual tokens based on their predicted probabilities. Why needed: Enables fine-grained control over which tokens receive stronger learning signals.
- **Adaptive Decay**: Gradual reduction in scaling factors over training steps. Why needed: Prevents overfitting to initial training dynamics while maintaining exploration throughout training.

## Architecture Onboarding

**Component Map**: Base Model -> Policy Network -> Advantage Computation -> A3PO Shaping -> Policy Loss -> Parameter Update

**Critical Path**: The critical training path involves computing token probabilities from the base model, calculating advantages using the reward signal, applying A3PO's asymmetric shaping based on token probability rankings within each rollout, computing the policy loss, and updating parameters via AdamW.

**Design Tradeoffs**: The paper trades computational overhead (additional sorting and scaling operations per token) for more precise gradient signals. The asymmetric approach assumes positive and negative samples have fundamentally different roles, which may not hold for all reasoning tasks or reward definitions.

**Failure Signatures**: Performance collapse typically manifests as either entropy collapse (overfitting to positive samples) or reward hacking (excessive exploration without convergence). Training-inference mismatch can cause degraded performance when token probability distributions differ significantly between training and inference engines.

**3 First Experiments**:
1. Train with only positive samples to observe sharpening effects on reasoning patterns.
2. Train with only negative samples to measure exploration dynamics and entropy changes.
3. Apply symmetric token-level advantage shaping (same scaling for both polarities) to quantify the benefit of asymmetry.

## Open Questions the Paper Calls Out
- Whether the distinct roles of positive and negative samples generalize to multimodal or vision-language reasoning tasks.
- How sample polarity influences RLVR training dynamics in agent-based scenarios involving multi-turn interactions and tool usage.
- Whether the optimal positive-to-negative advantage ratio of 0.5 is robust across diverse base model architectures and pre-training distributions.
- If positive and negative samples maintain their respective "sharpening" and "discovery" roles when using process-based rewards instead of outcome-based rewards.

## Limitations
- The study is restricted to text-based reasoning tasks, with no analysis of RLVR behavior in open-ended generation or multi-turn dialogue.
- Exact reward signal definitions (process vs. outcome) are cursorily described, leaving open the possibility of uncontrolled confounders in reward shaping.
- Results are limited to a narrow set of benchmarks and training protocols, without extensive ablation of hyperparameters.

## Confidence
- **High confidence**: Claims about the qualitative differences between positive and negative samples in sharpening vs. exploring reasoning (supported by well-controlled in-distribution ablations).
- **Medium confidence**: Claims about overall accuracy improvements from A3PO (reliant on a narrow set of benchmarks and training protocols, without extensive ablation of hyperparameters).
- **Low confidence**: Claims about the robustness of A3PO's asymmetric token-level shaping to different reward definitions or model families (not empirically tested).

## Next Checks
1. **Ablate the adaptive decay schedule**: Re-run A3PO training with fixed ρ and with alternative decay rates to verify that gains are not specific to the chosen α=0.005.
2. **Vary reward signal definitions**: Train A3PO using alternative definitions of positive/negative samples (e.g., based on process reward only) to test sensitivity to reward shaping assumptions.
3. **Extend evaluation to open-ended tasks**: Apply A3PO to benchmarks requiring creative or conversational reasoning (e.g., AlpacaEval or MT-Bench) to assess whether the approach generalizes beyond structured reasoning.