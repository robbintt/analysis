---
ver: rpa2
title: 'HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems'
arxiv_id: '2506.08426'
source_url: https://arxiv.org/abs/2506.08426
tags:
- edge
- training
- devices
- convergence
- device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of straggler effects in Split Federated
  Learning (SFL) due to heterogeneous edge device capabilities. The authors propose
  HASFL, a framework that adaptively controls batch sizes and model splitting to optimize
  performance.
---

# HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems

## Quick Facts
- arXiv ID: 2506.08426
- Source URL: https://arxiv.org/abs/2506.08426
- Reference count: 40
- Key outcome: HASFL framework achieves 4× faster convergence and 1% higher accuracy by adaptively optimizing batch sizes and model splitting in heterogeneous edge computing environments

## Executive Summary
This paper addresses the straggler problem in Split Federated Learning (SFL) where heterogeneous edge devices with varying computational capabilities create bottlenecks during distributed training. The authors propose HASFL, a framework that adaptively controls batch sizes and model splitting to optimize performance while minimizing training latency. The approach derives a theoretical convergence bound that quantifies how batch sizes and model splitting affect learning performance, then formulates and solves an optimization problem to find the optimal parameters. Extensive experiments demonstrate that HASFL significantly outperforms state-of-the-art benchmarks, achieving at least 4× faster convergence and 1% higher accuracy across various datasets and network conditions.

## Method Summary
HASFL tackles the straggler effect in SFL by developing a convergence bound that quantifies the impact of batch sizes and model splitting on learning performance. The authors formulate an optimization problem to minimize training latency by jointly optimizing these parameters, then decompose it into tractable subproblems. An efficient iterative algorithm is developed to solve these subproblems and find the optimal batch sizes and split points. The framework adapts to heterogeneous edge device capabilities by dynamically adjusting these parameters during training, balancing computational load across devices while maintaining convergence guarantees. The approach is validated through extensive experiments showing significant improvements over existing methods.

## Key Results
- Achieves at least 4× faster convergence compared to state-of-the-art benchmarks
- Demonstrates 1% higher accuracy across various datasets
- Shows robustness to varying network resources and scalability with increasing edge device numbers
- Effective across different heterogeneity levels in edge computing environments

## Why This Works (Mechanism)
HASFL works by adaptively optimizing both batch sizes and model splitting points based on device heterogeneity. The mechanism leverages a theoretical convergence bound that captures how these parameters affect learning performance, allowing the framework to make informed decisions about resource allocation. By dynamically adjusting batch sizes, HASFL ensures faster devices process more data while slower devices receive smaller batches, preventing stragglers from holding back the entire training process. The model splitting optimization determines the optimal layer to split the neural network, balancing computational load between edge devices and the server. This dual optimization approach addresses both computational and communication bottlenecks in heterogeneous edge environments.

## Foundational Learning

**Split Federated Learning (SFL)**: A distributed learning paradigm where neural networks are split between edge devices and a central server. Why needed: Enables training on large models across resource-constrained devices. Quick check: Verify understanding of forward/backward pass splitting mechanics.

**Convergence Bounds**: Mathematical guarantees on how close the learned model is to the optimal solution. Why needed: Provides theoretical foundation for optimization decisions. Quick check: Confirm understanding of how convergence rate relates to learning rate and data heterogeneity.

**Lagrange Multipliers**: Mathematical technique for solving constrained optimization problems. Why needed: Enables efficient solution of the latency minimization problem with constraints. Quick check: Verify ability to formulate dual problems from primal constraints.

## Architecture Onboarding

**Component map**: Edge devices -> Batch size optimizer -> Model splitter -> Training coordinator -> Edge server

**Critical path**: Data acquisition → Batch size adaptation → Model split determination → Forward/backward computation → Parameter aggregation → Model update

**Design tradeoffs**: 
- Batch size vs. convergence stability
- Split layer position vs. communication overhead
- Adaptation frequency vs. optimization overhead
- Theoretical guarantees vs. practical implementation complexity

**Failure signatures**: 
- Convergence stalling indicates inappropriate batch size selection
- Increasing latency suggests suboptimal model splitting
- Accuracy degradation signals excessive adaptation frequency
- Resource starvation occurs when batch sizes are too large for slower devices

**3 first experiments**:
1. Test convergence behavior with fixed vs. adaptive batch sizes on heterogeneous device sets
2. Evaluate communication overhead for different model splitting points
3. Measure training latency under varying network conditions with HASFL vs. static approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HASFL convergence bound and iterative optimization algorithm scale effectively to Large Language Models (LLMs) with Transformer architectures?
- Basis: The introduction motivates the work by citing the prohibitive costs of training "large-sized models" like Gemini Nano-2, yet the evaluation is restricted to VGG-16 and ResNet-18 CNNs.
- Why unresolved: The layer-wise parameter distribution and activation sizes in Transformers differ significantly from CNNs, potentially altering the convergence-latency trade-off derived for the HASFL framework.
- What evidence would resolve it: Experimental results applying HASFL to a standard LLM (e.g., GPT-2 or BERT variants) on a text dataset, demonstrating convergence and latency improvements.

### Open Question 2
- Question: How does HASFL perform in highly dynamic wireless environments where channel conditions fluctuate rapidly within a single aggregation interval $I$?
- Basis: The latency model assumes fixed uplink/downlink data rates ($r^U_i, r^D_i$) for optimization, and Algorithm 2 updates parameters only every $I$ rounds.
- Why unresolved: If network conditions degrade significantly between update intervals, the "optimal" batch size and split point decisions may become stale, potentially worsening the straggler effect rather than mitigating it.
- What evidence would resolve it: Simulations modeling high mobility or volatile channel fading (e.g., Rayleigh fading) to test the robustness of the fixed-interval optimization strategy.

### Open Question 3
- Question: Does the adaptive model splitting mechanism introduce varying degrees of vulnerability to data reconstruction or gradient inversion attacks?
- Basis: The optimization problem focuses solely on minimizing latency, ignoring privacy constraints.
- Why unresolved: Dynamically changing the cut layer alters the information content of the activations sent to the edge server; it is unclear if specific split points selected by HASFL make the system more susceptible to leakage than static splits.
- What evidence would resolve it: A security analysis quantifying the reconstruction error or mutual information leakage for different optimal split points determined by the HASFL algorithm.

## Limitations

- Theoretical convergence bound relies on idealized assumptions that may not fully capture real-world edge computing dynamics
- Problem formulation assumes static device capabilities during training, which may not hold in practical scenarios
- Extensive experiments focus primarily on specific datasets and model architectures, limiting generalizability
- Fixed optimization interval may not adapt quickly enough to rapidly changing network conditions

## Confidence

**Experimental results and performance claims**: High - The empirical validation is comprehensive and well-documented
**Theoretical convergence analysis**: Medium - While mathematically rigorous, the assumptions may not fully translate to practical settings
**Scalability claims**: Medium - Results show promising scaling trends but limited testing at very large scales

## Next Checks

1. Test HASFL's performance with dynamic device capabilities where computational resources fluctuate during training
2. Evaluate the framework's effectiveness with different model architectures beyond the ones tested (e.g., transformers, recurrent networks)
3. Conduct long-duration experiments to assess the algorithm's stability and performance over extended training periods with varying network conditions