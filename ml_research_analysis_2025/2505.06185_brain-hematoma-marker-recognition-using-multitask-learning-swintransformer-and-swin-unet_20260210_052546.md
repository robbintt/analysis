---
ver: rpa2
title: 'Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer
  and Swin-Unet'
arxiv_id: '2505.06185'
source_url: https://arxiv.org/abs/2505.06185
tags:
- task
- learning
- data
- swin
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of spurious correlations in brain
  hematoma marker recognition by proposing MTL-Swin-Unet, a multi-task learning method
  using transformers for classification and semantic segmentation. The core idea is
  to enhance image representation by jointly learning from classification, segmentation,
  and image reconstruction tasks, which helps mitigate spurious correlations.
---

# Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet

## Quick Facts
- arXiv ID: 2505.06185
- Source URL: https://arxiv.org/abs/2505.06185
- Reference count: 3
- Primary result: Multi-task learning with Swin-Unet improves robustness to spurious correlations in brain hematoma classification

## Executive Summary
This paper addresses spurious correlation problems in brain hematoma marker recognition by proposing MTL-Swin-Unet, a multi-task learning method that jointly optimizes classification, semantic segmentation, and image reconstruction tasks. The shared Swin Transformer encoder learns robust image representations by leveraging spatial supervision from segmentation and comprehensive reconstruction objectives. Experiments demonstrate that MTL-Swin-Unet outperforms single-task baselines on both standard test data (F1-score 0.903, AUC 0.967) and data with covariate shift (AUC 0.799), showing improved generalization across different hospital populations.

## Method Summary
The proposed MTL-Swin-Unet architecture uses a shared Swin Transformer encoder with three task-specific heads: classification (binary hypodensity detection), segmentation (hemorrhage localization), and reconstruction (image synthesis). The model is trained end-to-end with a weighted combination of losses: 0.3·L_cls + 0.4·L_seg + 0.6·L_dice + 0.4·L_rec. The method leverages 11,780 CT images from 11 institutions, with segmentation masks available for facilities 1-4. Training uses SGD with momentum 0.9, weight decay 0.0001, and learning rate decay over 600 epochs. Data augmentation includes random rotations and flips.

## Key Results
- MTL-Swin-Unet (cls + seg + rec) achieved F1-score of 0.903 and AUC of 0.967 on test data without covariate shift
- Under covariate shift (hospitals 5-11), MTL-Swin-Unet achieved highest AUC of 0.799
- Grad-CAM visualizations showed model attention shifted from spurious features to actual hematoma regions when using multi-task learning
- MTL-Swin-Unet (cls + seg) surpassed Swin Transformer in AUC on test data (hospitals 1-4) by 0.004 and on test data (hospitals 5-11) by 0.016

## Why This Works (Mechanism)

### Mechanism 1: Spurious Correlation Mitigation via Multi-Task Representation Learning
- Claim: Multi-task learning with segmentation and reconstruction reduces the model's reliance on confounding features by forcing the encoder to produce representations useful for multiple objectives.
- Mechanism: The shared Swin Transformer encoder must generate features that simultaneously support classification, segmentation, and reconstruction. This constraint discourages shortcuts where classification relies on easy-but-spurious correlates.
- Core assumption: The segmentation task provides spatial supervision that aligns with the true classification-relevant region (the hematoma itself).
- Evidence anchors:
  - "For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction."
  - "By visualizing the areas the model focuses on with Grad-CAM, we found that the model strongly focuses on the hematomas."
- Break condition: If segmentation masks are noisy, misaligned, or inconsistent across annotators, the auxiliary task may introduce conflicting gradients that degrade classification.

### Mechanism 2: Segmentation Task Provides Stronger Auxiliary Signal Than Reconstruction
- Claim: The segmentation task contributes more to classification robustness than the reconstruction task alone.
- Mechanism: Segmentation directly supervises the encoder to localize the target lesion, which is precisely the information needed for accurate classification. Reconstruction, by contrast, forces the encoder to retain all image information—including background—which may re-introduce spurious features.
- Core assumption: Pixel-level segmentation masks are available and accurately annotated by specialists.
- Evidence anchors:
  - "MTL-Swin-Unet (cls + seg) surpassed Swin Transformer in AUC on test data (hospitals 1-4) by 0.004 and on test data (hospitals 5-11) by 0.016. Conversely, MTL-Swin-Unet (cls + rec) fell behind Swin Transformer in AUC."
  - "The image reconstruction task alone does not share advantageous factors with the classification task and may even contain conflicting information."
- Break condition: If segmentation labels are unavailable for a substantial portion of training data, the loss is computed only on labeled images, potentially reducing the regularization effect.

### Mechanism 3: Hierarchical Feature Sharing Through Skip Connections and U-Shaped Decoder
- Claim: The U-shaped Swin-Unet architecture with skip connections enables multi-scale feature reuse across tasks.
- Mechanism: The encoder progressively downsamples to capture semantic context; the decoder upsamples with skip connections to recover spatial detail. Skip connections preserve fine-grained localization information critical for segmentation, which also benefits classification by maintaining precise lesion boundaries.
- Core assumption: The input resolution (224×224) is sufficient to capture hematoma details despite downsampling through patch merging.
- Evidence anchors:
  - "The decoder reconstructs the input image resolution using representations extracted by the encoder and representations from each hierarchical level sent via skip connections."
  - "Before the SwinTransformer block, skip connections are applied similarly to Unet. The representations from the same layer in the encoder are preserved and combined along the channel dimension."
- Break condition: If the lesion is extremely small relative to input resolution, hierarchical downsampling may lose critical spatial information before skip connections can recover it.

## Foundational Learning

- Concept: **Spurious Correlation in Medical Imaging**
  - Why needed here: Understanding that models can learn to associate non-target features (skull, background) with labels is essential to appreciating why multi-task learning helps.
  - Quick check question: Can you explain why a classifier trained on waterfowl images might learn to detect water instead of birds?

- Concept: **Multi-Task Learning with Shared Representations**
  - Why needed here: The proposed method relies on a shared encoder producing features useful for three distinct tasks; understanding gradient aggregation from multiple loss functions is critical.
  - Quick check question: When combining losses from classification, segmentation, and reconstruction, what determines the relative influence of each task on the shared encoder?

- Concept: **Swin Transformer Basics (Patch Merging, Shifted Windows, Hierarchical Stages)**
  - Why needed here: The architecture builds on Swin Transformer's hierarchical structure; understanding patch merging/expansion is necessary to trace feature dimensions through encoder and decoder.
  - Quick check question: How does patch merging reduce spatial resolution while increasing channel dimension in the Swin Transformer encoder?

## Architecture Onboarding

- Component map:
  - Input CT image (224×224, HU-adjusted) -> Patch partition -> Swin Transformer encoder (4 stages with patch merging)
  - Encoder output feeds: (a) classification head via global average pooling, (b) both decoders via skip connections
  - Segmentation decoder: Symmetric Swin-Unet decoder with patch expansion and skip connections from encoder
  - Reconstruction decoder: Shared decoder structure (separate from segmentation decoder)
  - Output: segmentation mask, reconstructed image, binary classification (hypodensity vs. other)

- Critical path:
  1. Input CT image -> patch partition -> Swin Transformer encoder
  2. Encoder output feeds: (a) classification head via global average pooling, (b) both decoders via skip connections
  3. Segmentation decoder produces hematoma mask; reconstruction decoder produces reconstructed image
  4. Losses computed and aggregated; backpropagation updates shared encoder and task-specific heads simultaneously

- Design tradeoffs:
  - **cls+seg+rec vs. cls+seg**: Adding reconstruction increases computational cost and may introduce conflicting gradients, but the paper suggests combining all three yields better covariate-shift robustness (AUC 0.799 vs. 0.788 for cls+seg on hospitals 5-11)
  - **MTL-Swin-Unet vs. Joint-SwinTransformer**: MTL updates encoder jointly; Joint trains segmentation first, freezes encoder, then trains classifier with concatenated features. MTL achieves higher F1; Joint achieves slightly better AUC on some splits
  - **Model size (tiny vs. default)**: Increasing blocks from 2 to 6 in stage 3 and channels to 96 yields mixed results; not consistently beneficial

- Failure signatures:
  - **Overfitting to training hospitals**: If test AUC on hospitals 5-11 is significantly lower than on hospitals 1-4, the model has not generalized across patient populations (covariate shift detected)
  - **Spurious correlation detected via Grad-CAM**: If attention focuses on skull or non-lesion regions, the multi-task regularization has failed
  - **Segmentation loss not decreasing**: If segmentation masks are unavailable for non-hemorrhage images, verify that loss is computed only on labeled samples and averaged correctly per batch

- First 3 experiments:
  1. **Baseline classification**: Train Swin Transformer classifier (cls only) on the hematoma dataset. Record F1 and AUC on both hospital splits. Use Grad-CAM to identify spurious attention regions.
  2. **Add segmentation auxiliary task**: Train MTL-Swin-Unet (cls+seg) with λ_cls=0.4, λ_seg=0.6. Compare F1/AUC to baseline; verify Grad-CAM shifts toward hematoma regions.
  3. **Test covariate shift robustness**: Train MTL-Swin-Unet (cls+seg+rec) and evaluate on hospitals 5-11 (unseen patients). Compare AUC to cls+seg configuration to quantify reconstruction's contribution to generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MTL-Swin-Unet architecture generalize to other datasets with spurious correlations?
- Basis in paper: The conclusion states, "In future work, we plan to explore the effectiveness of the proposed method on other datasets and tasks."
- Why unresolved: The current study validates the method solely on a specific brain hematoma CT dataset, leaving its efficacy on natural images or other medical modalities unknown.
- What evidence would resolve it: Experimental results showing the method's performance on benchmark spurious correlation datasets (e.g., Waterbirds, CelebA) or other medical imaging tasks.

### Open Question 2
- Question: Why does the image reconstruction task hinder classification performance when used alone, but improve it when combined with the segmentation task?
- Basis in paper: The results section notes that MTL-Swin-Unet (cls+rec) underperformed the baseline, yet adding reconstruction to (cls+seg) improved AUC under covariate shift.
- Why unresolved: The paper observes this synergistic effect empirically but does not isolate the feature representation mechanisms causing reconstruction to be detrimental in isolation but beneficial in combination.
- What evidence would resolve it: An ablation study analyzing the learned feature spaces (e.g., using Centered Kernel Alignment) to quantify the representation overlap between tasks.

### Open Question 3
- Question: How sensitive is the model's classification accuracy to the specific manual weighting of the loss functions (λ)?
- Basis in paper: The method section specifies fixed weights (λ_cls=0.3, λ_seg=0.4, λ_rec=0.4) without providing a sensitivity analysis or rationale for these exact values.
- Why unresolved: Multi-task learning performance is often highly dependent on loss balancing; the current results may represent a local optimum or be unstable across different data splits.
- What evidence would resolve it: A hyperparameter search varying λ values or the implementation of an adaptive loss weighting strategy to compare against the fixed weights.

## Limitations
- Dataset access and preprocessing details (HU windowing parameters) are not publicly available, preventing independent verification
- The claim that reconstruction helps with covariate shift robustness is counterintuitive and needs more rigorous ablation studies
- Segmentation masks are only available for a subset of facilities (1-4), potentially limiting the multi-task learning effect

## Confidence

- **High Confidence**: The mechanism by which segmentation task reduces spurious correlation through spatial supervision is well-supported by Grad-CAM evidence showing shifted attention to hematoma regions.
- **Medium Confidence**: The overall multi-task learning framework improves performance compared to single-task baselines, though the specific contribution of each auxiliary task needs clearer delineation.
- **Low Confidence**: The claim that adding reconstruction improves covariate shift robustness is not well-justified and contradicts the observation that reconstruction alone can degrade performance.

## Next Checks

1. **Ablation study on auxiliary tasks**: Systematically evaluate MTL-Swin-Unet configurations (cls+seg, cls+rec, cls+seg+rec) on both test settings to quantify each task's contribution to spurious correlation mitigation and covariate shift robustness.

2. **Spurious correlation detection**: Generate Grad-CAM visualizations for the baseline Swin Transformer and MTL variants, then compute quantitative metrics (e.g., intersection over attention maps) to measure reduction in non-lesion attention.

3. **Cross-institutional generalization test**: Train on hospitals 1-4 and evaluate on hospitals 5-11 using different model variants to confirm that multi-task learning genuinely improves generalization across patient populations and imaging protocols.