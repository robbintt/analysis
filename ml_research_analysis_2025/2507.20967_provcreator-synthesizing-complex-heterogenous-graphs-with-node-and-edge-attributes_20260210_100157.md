---
ver: rpa2
title: 'PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes'
arxiv_id: '2507.20967'
source_url: https://arxiv.org/abs/2507.20967
tags:
- graph
- graphs
- provcreator
- generation
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProvCreator, a synthetic graph generation
  framework that addresses the challenge of generating complex heterogeneous graphs
  with high-dimensional node and edge attributes. The core idea is to cast graph synthesis
  as a sequence generation task, leveraging transformer-based large language models
  (LLMs) through a graph-to-sequence encoder-decoder architecture.
---

# PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes

## Quick Facts
- arXiv ID: 2507.20967
- Source URL: https://arxiv.org/abs/2507.20967
- Authors: Tianhao Wang; Simon Klancher; Kunal Mukherjee; Josh Wiedemeier; Feng Chen; Murat Kantarcioglu; Kangkook Jee
- Reference count: 18
- Primary result: Synthesizes complex heterogeneous graphs with high-dimensional attributes via sequence generation, achieving superior structural fidelity and retaining 65-70% of real-data F1 score for downstream classification.

## Executive Summary
This paper introduces ProvCreator, a synthetic graph generation framework that addresses the challenge of generating complex heterogeneous graphs with high-dimensional node and edge attributes. The core idea is to cast graph synthesis as a sequence generation task, leveraging transformer-based large language models (LLMs) through a graph-to-sequence encoder-decoder architecture. ProvCreator's key contributions include a unified framework for synthesizing complex heterogeneous graphs, joint modeling of structure and semantics, and a versatile graph-to-sequence encoder that preserves structural and semantic information compactly.

Evaluated on system provenance graphs in cybersecurity and knowledge graphs from the IntelliGraph Benchmark Dataset, ProvCreator demonstrates superior structural fidelity compared to prior works, achieving consistently lower Maximum Mean Discrepancy (MMD) scores across various graph metrics. Specifically, for firefox.exe and powershell.exe datasets, ProvCreator attains MMD scores of 0.277 and 1.109 respectively, outperforming the GDSS baseline. In downstream tasks, ProvCreator retains 65-70% of the real-data F1 score for program classification, indicating that synthetic graphs preserve class signal and support cross-domain generalization. The framework also achieves high attribute validity rates, with 93.77-98.70% valid process names and 96.43-98.92% valid IP addresses and ports for different datasets.

## Method Summary
ProvCreator formulates graph synthesis as a sequence generation task, where each graph is serialized into a token sequence using custom special tokens to represent nodes, edges, and their attributes. This token sequence is fed into a LLaMA3-3.2-3B backbone with LoRA fine-tuning to learn the joint distribution of graph structure and attributes. During generation, a constrained decoding process with token filtering and a state machine parser ensures the output conforms to the graph schema, producing valid synthetic heterogeneous graphs.

## Key Results
- Achieves MMD scores of 0.277 and 1.109 on degree distribution for firefox.exe and powershell.exe datasets respectively, outperforming GDSS baseline.
- Retains 65-70% of real-data F1 score for program classification on synthetic data, demonstrating preservation of class signal.
- Achieves 93.77-98.70% valid process names and 96.43-98.92% valid IP addresses and ports across datasets, indicating high attribute validity.

## Why This Works (Mechanism)

### Mechanism 1: Sequence Formulation of Graph Topology
- Claim: Reformulating graph synthesis as a sequence generation task enables the handling of complex, interdependent node and edge attributes that traditional methods struggle with.
- Mechanism: By serializing the graph structure and high-dimensional attributes into a linear token sequence, the framework leverages the autoregressive capabilities of Large Language Models (LLMs). The model predicts the next token (structural or semantic) based on the cumulative context of previous nodes, edges, and attributes.
- Core assumption: There exists a well-defined ordering of edges (e.g., temporal for provenance graphs) that, when flattened, preserves the causal and semantic relationships necessary for valid generation.
- Evidence anchors:
  - [abstract] "ProvCreator formulates graph synthesis as a sequence generation task, enabling the use of transformer-based large language models."
  - [section 3] "ProvCreator models graph generation as a sequence generation task, where each graph is represented as a sequence of tokens."
  - [corpus] Weak direct evidence; related papers like *Variational Bayesian Flow Network* explore generation via diffusion/flow, suggesting sequence formulation is a specific design choice rather than a universal standard.
- Break condition: If graphs require strict global simultaneous constraints that cannot be decomposed into a linear sequence without information loss, the autoregressive generation may produce locally consistent but globally invalid structures.

### Mechanism 2: Joint Structure-Semantics Tokenization
- Claim: Interleaving structural tokens with attribute tokens in a compact format forces the model to learn dependencies between topology and semantics.
- Mechanism: The architecture uses a specific vocabulary of special tokens (e.g., `<boe>`, `<bon>`) to wrap attributes immediately following their respective nodes or edges. This preserves the "state" of the entity in the sequence, allowing the LLM to condition attribute generation on the specific local topology.
- Core assumption: Compressing graph information into special tokens preserves enough information for the LLM to reconstruct complex attribute types (like IP addresses) without relying on the LLM's pre-trained natural language knowledge alone.
- Evidence anchors:
  - [section 3.1] "We argue and empirically demonstrate that node attributes and graph structure should be generated jointly... ProvCreator formulates both as a unified optimization problem."
  - [section 3.1] "Our tokenization schema only uses 35% and 22% tokens on provenance data... comparing to directly representing the graph in a plain text format."
- Break condition: If the attribute schema is too high-dimensional or unstructured (e.g., raw pixel data within nodes), the token context window or the capacity of the special token embeddings may be exceeded, degrading fidelity.

### Mechanism 3: Constrained Decoding via State Machines
- Claim: LLMs can generate valid graph structures if their output is filtered through a rule-based state machine during decoding.
- Mechanism: The decoding process is not passive; it actively filters invalid tokens and uses "anchor tokens" to recover the parser state. This acts as a grammar constraint, ensuring that even if the LLM hallucinates, the final output conforms to the graph schema.
- Core assumption: The probability of the LLM generating an "unrecoverable" sequence of errors is low enough that the salvage operation (skipping tokens) does not destroy the graph's integrity.
- Evidence anchors:
  - [section 3.2] "We employ two methods to ensure validity... a token filtering mechanism... [and] a parser with a state machine."
  - [section 3.2] "When an invalid token is encountered, the parser attempts to recover by skipping the invalid token... allowing the parser to recover to a valid state."
- Break condition: If the LLM drifts too far from the expected grammar (e.g., generating infinite loops of attributes), the parser might produce disconnected subgraphs or empty graphs.

## Foundational Learning

- Concept: **Heterogeneous Graphs (HINs) & Provenance**
  - Why needed here: The target data is not a simple adjacency matrix but a complex schema where nodes are processes/files/sockets and edges are syscalls. Understanding that "node type" dictates "valid attributes" is crucial for the tokenization logic.
  - Quick check question: Can you explain why a homogeneous graph generator (like a standard GAN for molecules) would fail to model a system trace where a "Process" node must have a "PID" but a "File" node must not?

- Concept: **Maximum Mean Discrepancy (MMD)**
  - Why needed here: This is the primary evaluation metric (Table 2). You must understand that MMD measures the distance between the distribution of real graphs and synthetic graphs; lower is better.
  - Quick check question: If ProvCreator achieves an MMD of 0.277 on Degree distribution vs. 0.955 for a baseline, does this prove the synthetic graphs are "realistic" or just structurally similar?

- Concept: **Autoregressive Sequence Modeling**
  - Why needed here: The core engine is a standard LLM (Llama 3). You need to understand that the model generates the graph one token at a time, conditioned on all previous tokens ($P(x_t | x_{<t})$).
  - Quick check question: In a sequence representing a graph edge $(u, v)$, why is the order of appearance (temporal or topological) critical for the model to learn causality?

## Architecture Onboarding

- Component map: Input Processor -> Graph-to-Sequence Encoder -> LLM Backbone (LLaMA3-3.2-3B with LoRA) -> Constrained Decoder -> State Machine Parser
- Critical path: The Tokenization Schema (Algorithm 1) and the Ordering of Edges. If the serialization is inefficient (too long) or semantically disordered, the LLM cannot learn dependencies, and the context window overflows.
- Design tradeoffs:
  - Custom Tokens vs. JSON/Text: The paper trades off the "ease" of using raw text (which LLMs understand natively) for the "efficiency" of custom tokens (reducing token count by ~65-78%) to fit larger graphs.
  - Generation Speed: The paper notes generation times of ~106-145 seconds per graph (Table 5), which is significantly slower than simple sampling methods; this is the cost of using large autoregressive models for structure.
- Failure signatures:
  - Empty Graph Generation: If the LLM fails to generate valid open/close tags, the parser returns empty.
  - Hallucinated Attributes: High "syntactic" validity (regex match) but low "semantic" validity (e.g., valid IP address format, but impossible network routing).
  - Token Overflow: Graphs larger than the training distribution may truncate, resulting in disconnected components.
- First 3 experiments:
  1. **Tokenizer Efficiency Test:** Run the Graph-to-Sequence encoder on a sample dataset (e.g., `powershell.exe` logs) and verify the claimed token reduction ratio (~35% of plain JSON).
  2. **Structural Ablation:** Train a version using standard text serialization (no custom tokens) and compare the MMD scores against the custom-token version to validate the efficiency claim.
  3. **Downstream Utility Check:** Train a simple GNN classifier on synthetic data and test on real data (Sec 4.5). If the accuracy drops to random guessing, the semantic link is broken even if MMD is low.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProvCreator handle graphs with complex cyclic structures that lack the natural temporal or topological ordering required by the current tokenization scheme?
- Basis in paper: [inferred] Page 5 states the method relies on a "well-defined order to visit every edge" (timestamps or topological sort), which may not exist for general cyclic graphs.
- Why unresolved: Evaluation is limited to timestamped provenance graphs and specific knowledge graphs; performance on cyclic structures is untested.
- What evidence would resolve it: Generation results on benchmark cyclic graphs lacking canonical orderings.

### Open Question 2
- Question: What is the trade-off between the high fidelity of LLM-based generation and the risk of memorizing sensitive training data?
- Basis in paper: [inferred] The abstract promises "privacy-aware" data, but the introduction notes the need for post-hoc privacy techniques, suggesting the model itself may leak data.
- Why unresolved: The paper measures utility and fidelity but does not quantify privacy risks like membership inference.
- What evidence would resolve it: A quantitative analysis of memorization rates or success of membership inference attacks on the synthetic outputs.

### Open Question 3
- Question: Can the framework be adapted to synthesize massive graphs that exceed the context window limits of the underlying LLM architecture?
- Basis in paper: [inferred] Page 2 lists "efficiently compress[ing] large graphs" as a goal, and Page 3 restricts analysis to "tractable subgraphs" due to the size of full graphs.
- Why unresolved: The evaluation only covers subgraphs fitting within standard context windows (avg. <6000 tokens).
- What evidence would resolve it: Successful generation of graph structures exceeding standard context lengths (e.g., >128k tokens) without losing global structural consistency.

### Open Question 4
- Question: Does ProvCreator preserve complex semantic interdependencies between attributes (e.g., consistency between command arguments and accessed resources) that are not captured by simple syntactic validity checks?
- Basis in paper: [inferred] The paper validates attributes using regex rules (Table 6) but asserts that node attributes are "intrinsically tied to the graph structure."
- Why unresolved: The reported validity rates (93-99%) only confirm syntax, not whether the generated attributes make logical sense within the graph's context.
- What evidence would resolve it: Domain expert evaluation or functional consistency checks (e.g., running the synthetic traces in a sandbox) rather than regex matching.

## Limitations
- Hyperparameter sensitivity: Critical training hyperparameters (LoRA rank, learning rate, batch size, epochs) are not specified, making performance reproducibility uncertain.
- Semantic validity vs. syntactic validity: High attribute validity rates only confirm correct format, not meaningful content or logical consistency.
- Graph size constraints: Performance on very large graphs exceeding context window limits is not discussed or quantified.

## Confidence

**High Confidence**:
- ProvCreator's ability to generate structurally similar graphs to real data, as evidenced by consistently lower MMD scores across multiple graph metrics compared to baselines.
- The framework's token efficiency, reducing token count by 65-78% compared to plain text serialization, as validated by the reported tokenization schema.

**Medium Confidence**:
- The claim that joint structure-semantics tokenization forces the model to learn dependencies between topology and attributes, supported by the MMD scores but not directly validated through ablation studies.
- The assertion that constrained decoding via state machines ensures valid graph generation, though the exact recovery rate and impact on graph integrity are not quantified.

**Low Confidence**:
- The assertion that synthetic graphs retain 65-70% of real-data F1 score for program classification, as this is based on a single downstream task and lacks comparison to alternative synthetic data methods.
- The claim that the framework is "versatile" for complex heterogeneous graphs, as the evaluation is limited to provenance graphs and knowledge graphs from specific datasets.

## Next Checks
1. **Ablation Study on Tokenization Efficiency**: Train a variant of ProvCreator using plain text serialization (no custom tokens) and compare MMD scores and generation speed to the custom-token version. This will validate whether the reported 65-78% token reduction directly contributes to performance gains.

2. **Semantic Validity Analysis**: Beyond syntactic regex checks, analyze the semantic plausibility of generated attributes (e.g., network paths, IP addresses) by comparing their distribution to real data. This will reveal whether the model generates meaningful content or just valid-looking noise.

3. **Downstream Task Robustness**: Evaluate synthetic graphs on multiple downstream tasks (e.g., link prediction, node classification) beyond program classification. This will test whether the structural fidelity translates to broader utility across graph ML applications.