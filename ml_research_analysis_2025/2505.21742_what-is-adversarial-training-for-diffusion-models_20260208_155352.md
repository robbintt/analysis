---
ver: rpa2
title: What is Adversarial Training for Diffusion Models?
arxiv_id: '2505.21742'
source_url: https://arxiv.org/abs/2505.21742
tags:
- noise
- data
- adversarial
- training
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of adversarial training (AT)
  for diffusion models (DMs), highlighting that AT for DMs differs fundamentally from
  classifiers: while classifiers require output invariance, DMs require equivariance
  to maintain alignment with the data distribution. The authors propose a method to
  inject either random or adversarial noise into the diffusion flow, enforcing smoothness
  in the trajectory space to improve robustness to outliers, corrupted data, and prevent
  memorization.'
---

# What is Adversarial Training for Diffusion Models?

## Quick Facts
- arXiv ID: 2505.21742
- Source URL: https://arxiv.org/abs/2505.21742
- Reference count: 40
- The paper proposes Robustadv, an adversarial training method for diffusion models that achieves significantly lower FID scores on corrupted CIFAR-10 data (24.70 vs 58.05 baseline at 90% corruption with σ=0.1).

## Executive Summary
This paper addresses the challenge of applying adversarial training to diffusion models, arguing that standard classifier-based approaches fail because diffusion models require output sensitivity (equivariance) rather than output invariance. The authors propose injecting random or adversarial noise into the diffusion process during training, enforcing smoothness in the trajectory space. This approach improves robustness to outliers, corrupted data, and prevents memorization. Experiments show state-of-the-art performance on corrupted CIFAR-10, CelebA, and LSUN Bedroom datasets, with FID scores of 24.70 (CIFAR-10, σ=0.1, 90% corruption) compared to 58.05 for the baseline DDPM.

## Method Summary
The method modifies standard diffusion model training by adding an equivariant regularization term to the loss function. During training, perturbations (either random or adversarial) are injected into the noisy input, and the model is trained to predict noise in a way that maintains alignment with the perturbed input. This creates a smoothness prior in the trajectory space, enabling the model to "unlearn" noise present in the training data. The perturbation magnitude is controlled by a time-dependent scheduler that decays as the diffusion process approaches the data (low t), preventing over-smoothing and mode collapse. The approach integrates seamlessly into diffusion training without assumptions on the noise model.

## Key Results
- Robustadv achieves FID of 24.70 on CIFAR-10 with 90% corruption (σ=0.1) vs 58.05 for DDPM baseline
- For σ=0.2 and 90% corruption, Robustadv achieves FID of 24.81 vs 102.68 for DDPM
- The method demonstrates faster sampling and improved robustness to iterative adversarial attacks
- Works across multiple datasets: CIFAR-10, CelebA, LSUN Bedroom

## Why This Works (Mechanism)

### Mechanism 1: Equivariant Regularization for Trajectory Alignment
Adversarial training for diffusion models requires enforcing equivariance rather than invariance. DMs solve a regression task to reverse a noise trajectory. If a perturbation δ shifts the noisy input x_t, the predicted noise must shift by δ to ensure the denoised point lands on the correct predecessor x_{t-1}. Enforcing invariance causes the model to learn a different distribution, whereas the equivariant objective recovers the correct manifold.

### Mechanism 2: Inductive Bias for Distributional Denoising
Injecting perturbations during training imposes a smoothness prior on the score field, enabling the model to "unlearn" noise present in the training data. The smoothness constraint prevents the model from fitting deviations caused by outliers or corruption, effectively pushing the learned distribution back toward the underlying clean manifold.

### Mechanism 3: Time-Adaptive Perturbation Scheduling
The perturbation radius r(t) must shrink as the diffusion process approaches the data (low t) to prevent over-smoothing or mode collapse. In early timesteps (t → T, "noise phase"), large perturbations are tolerated. In later timesteps (t → 0, "content phase"), the perturbation scheduler decays to zero to preserve content fidelity while maintaining a small bias γ.

## Foundational Learning

- **Score-Based Generative Modeling (DDPMs)**: Understanding the forward (adding noise) and reverse (denoising) processes is required to grasp where the perturbation δ is injected. *Quick check: Can you explain why predicting the noise ε in a diffusion model is mathematically equivalent to learning the score function (gradient of the data density)?*

- **Adversarial Training (AT) & Randomized Smoothing**: The paper positions its method as an adaptation of AT. You need to know standard AT (FGSM/PGD) for classifiers to understand why "invariance" is the default baseline the authors argue against. *Quick check: In a standard classifier, does adversarial training aim to make the gradient of the loss with respect to the input larger or smaller?*

- **Equivariance vs. Invariance**: This is the central theoretical pivot of the paper. *Quick check: If you apply a rotation δ to an input image, would a rotation-invariant model output the same embedding, or a rotated embedding?*

## Architecture Onboarding

- **Component map**: Input -> Noise Injection -> Perturbation Module -> Ray Scheduler -> Loss Aggregator -> Model Update
- **Critical path**: The calculation of δ_adv in Algorithm 1 (Line 5-6) is the critical divergence from standard DDPM. The implementation must support a backward pass to compute ∇_{x_t} J_θ to craft the attack before the main model update.
- **Design tradeoffs**:
  - Random (δ_ran) vs. Adversarial (δ_adv): δ_ran is faster (no backward pass for attack generation) but δ_adv provides significantly stronger denoising (lower FID) on corrupted data
  - Strength λ: High λ removes noise effectively but risks over-smoothing (loss of detail); low λ preserves detail but may retain training noise
- **Failure signatures**:
  - Diverging Trajectories: Using invariance loss instead of equivariance causes generations to be off-manifold
  - Mode Collapse: If r(t) doesn't decay sufficiently in content phase, distinct classes blur together
  - Slow Convergence: If λ is too low on noisy data, the model may simply fit the noise
- **First 3 experiments**:
  1. "In Vitro" Sanity Check: Train on "oblique-plane" 3D dataset with uniform outliers to confirm the model ignores outliers
  2. Robustness to Corruption: Train on CIFAR-10 with 90% of data corrupted by Gaussian noise (σ=0.1) and compare FID against DDPM baseline
  3. Equivariance vs. Invariance Ablation: Implement the invariance loss on synthetic dataset and observe the distribution drift

## Open Questions the Paper Calls Out

- **Adaptive Perturbation Scheduling**: Can the perturbation ray parameter ω be learned in an input-dependent manner rather than being fixed? The authors believe the encoding functions can be improved by letting the network learn an input-dependent ω.

- **Fully Corrupted Datasets**: Can the method be extended to handle fully corrupted datasets (p = 100%)? The authors identify this as a limitation, as current experiments only test up to 90% corruption.

- **Framework Transferability**: How does the equivariance-based AT formulation transfer to other diffusion frameworks like EDM or large-scale latent diffusion models? The authors need to port their approach to EDM framework to scale to larger datasets.

- **Optimal Trade-off Parameter**: What is the optimal trade-off between denoising strength (controlled by λ) and preservation of fine-grained details? The paper notes that λ = 0.5 causes over-smoothing while lower values prevent sufficient denoising, but no principled method for selecting λ is provided.

## Limitations
- The method struggles when corruption rates approach 100%, as there is no clean signal to anchor the trajectory
- The mathematical formalization of why invariance "breaks" the Markov Chain assumption could be more rigorous
- The approach relies on a fixed perturbation scheduler (γ, ω) without adaptation to data complexity

## Confidence
- **High**: The empirical superiority of the method on corrupted benchmarks (CIFAR-10 FID 24.70 vs 58.05 baseline at σ=0.1)
- **Medium**: The theoretical argument for equivariance vs. invariance, supported by synthetic trajectory visualizations but lacking a formal proof
- **Medium**: The claim that this is a "general" solution for denoising outliers, as performance degrades significantly at p=100% corruption

## Next Checks
1. Test the model against iterative adversarial attacks (e.g., PGD) during inference to verify if the learned smoothness generalizes beyond Gaussian corruption
2. Evaluate the method on higher-resolution datasets (e.g., 512×512 images) to assess whether the perturbation scheduler remains effective
3. Derive a formal proof or stronger theoretical argument showing why the invariance loss violates the Markov Chain assumption of diffusion models