---
ver: rpa2
title: Evaluating Actionability in Explainable AI
arxiv_id: '2601.20086'
source_url: https://arxiv.org/abs/2601.20086
tags:
- system
- information
- actions
- users
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in evaluating actionability of explainable
  AI (XAI) systems by articulating how users take actions in response to explanations.
  Through 14 interviews with doctors and teachers, the authors create a catalog of
  60 user-centered action types across three dimensions (AI Interactions, External
  Actions, Mental State Actions) and 12 information categories.
---

# Evaluating Actionability in Explainable AI

## Quick Facts
- arXiv ID: 2601.20086
- Source URL: https://arxiv.org/abs/2601.20086
- Reference count: 40
- Key outcome: Catalog of 60 user-centered action types across 3 dimensions and 12 information categories helps AI creators design measurable XAI evaluations

## Executive Summary
This paper addresses the gap in evaluating actionability of explainable AI (XAI) systems by articulating how users take actions in response to explanations. Through 14 interviews with doctors and teachers, the authors create a catalog mapping 12 information categories to 60 action types across three dimensions (AI Interactions, External Actions, Mental State Actions). The catalog helps AI creators articulate expected user actions from explanations and design measurable evaluations. A key finding is that Mental State Actions are most frequent (2190 vs 200 AI Interactions and 270 External Actions) but often overlooked in XAI research.

## Method Summary
The study conducted 14 semi-structured interviews with 9 doctors (MD/residency status) and 5 CS teachers (grades 6-12 with 1+ years experience). Participants were shown domain-specific visual mockups (EHR for medicine, student placement system for education) and asked to think aloud while engaging with scenario-based explanations. Researchers performed iterative thematic qualitative analysis to code transcripts for information categories and action types, achieving 83.5% inter-rater reliability on 22% of quotes.

## Key Results
- Mental State Actions (2190 occurrences) are significantly more frequent than AI Interactions (200) and External Actions (270)
- Cross References was the most frequently mentioned information category
- The catalog enables AI creators to map specific information types to expected user actions for measurable evaluation

## Why This Works (Mechanism)

### Mechanism 1: The Articulation-Alignment Mechanism
Explicitly mapping information types to expected user actions reduces the gap between system design and user utility. The catalog forces AI creators to move from vague goals to specific, testable hypotheses, ensuring evaluation metrics reflect actual user needs rather than algorithmic performance alone.

### Mechanism 2: The Mental State Mediation Mechanism
Recognizing "Mental State Actions" as valid, measurable outcomes mediates the gap between receiving information and taking physical action. Users often process explanations internally before engaging in visible behavior, and by validating these internal states as "actions," the system captures the primary way users currently leverage explanations.

### Mechanism 3: The Nested Information-Seeking Mechanism
Actionability relies on "nested" information seeking, where system outputs are compared against external, trusted references. Users rarely act on AI output in isolation but use system information to query trusted external sources to validate the AI.

## Foundational Learning

- **Concept: User-Centered Terminology**
  - Why needed: Technical distinctions between "model," "algorithm," and "UI" are often lost on end-users who conflate these into a single "System"
  - Quick check: When a user complains about "the AI," are they referring to the model weights or the lack of a "help" button?

- **Concept: The Actionability Dimensions**
  - Why needed: To avoid designing for the wrong outcome; most XAI focuses on AI Interactions, but Mental State Actions and External Actions are critical distinct modes
  - Quick check: Does your evaluation metric capture if the user feels confident (Mental) or just if they modified the input (AI Interaction)?

- **Concept: Information Categories**
  - Why needed: Explanations are not monolithic; users seek specific types of info (e.g., Alternative Outcomes, Consequences for User)
  - Quick check: Can you name three specific Information Categories your current interface provides?

## Architecture Onboarding

- **Component map:** User-Centered Action Types (demand) -> Information Categories (supply) -> Mapping Layer (connection logic)
- **Critical path:** 1) Audit which of 12 Information Categories your system delivers, 2) Map these categories to 60 Action Types (focus on Mental State Actions first), 3) Create tasks testing if information enables mapped action
- **Design tradeoffs:** Completeness vs. Overload (providing all 12 categories may overwhelm UI); Mental vs. Physical (designing for Mental State Actions is harder to measure but likely more valuable)
- **Failure signatures:** The "Accuracy Trap" (high accuracy but low trust), The "Isolation Effect" (users rely on system only when forced), Terminology Mismatch (users say "system broken" when explanation unclear)
- **First 3 experiments:** 1) Mental State Frequency Audit (tag user feedback for Mental State Action verbs), 2) Cross-Reference Integration Test (add one Cross Reference feature and measure External Actions), 3) Vocabulary Swap (rewrite explanation text using User-Centered Terminology and test for improved Recall of potential choices)

## Open Questions the Paper Calls Out

1. Does the prominence of Mental State Actions over AI Interactions and External Actions persist in domains beyond education and medicine?
2. How must the catalog of action types be adapted to support non-textual explanation modalities, such as visual explanations in radiology?
3. Does designing XAI systems to specifically support Mental State Actions measurably improve the quality or safety of subsequent External Actions?

## Limitations
- Study based on 14 interviews with doctors and teachers in specific domains, may not generalize to other contexts
- Catalog's 60 action types and 12 information categories may not capture all possible user actions across diverse XAI applications
- Methodology for counting action frequencies is not fully transparent

## Confidence

- **High Confidence:** Gap in evaluating XAI actionability exists; distinction between Mental State Actions, AI Interactions, and External Actions is well-supported
- **Medium Confidence:** Specific frequencies of action types and importance of Cross References within study context
- **Low Confidence:** Generalizability of catalog to other domains beyond healthcare and education

## Next Checks
1. Conduct replication study with larger, more diverse participant pool across different professional domains
2. Develop quantitative survey instrument based on catalog to measure actionability across wider range of XAI systems
3. Perform longitudinal study to assess catalog's relevance as XAI technologies and user expectations evolve