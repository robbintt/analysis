---
ver: rpa2
title: Effective Skill Unlearning through Intervention and Abstention
arxiv_id: '2503.21730'
source_url: https://arxiv.org/abs/2503.21730
tags:
- unlearning
- skill
- neuron
- space
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two training-free methods for unlearning
  specific skills from large language models while preserving overall capabilities.
  The first method, Neuron Adjust, identifies neurons with different pre-activation
  distributions between the skill to forget and skill to retain, then probabilistically
  shifts their values during inference.
---

# Effective Skill Unlearning through Intervention and Abstention

## Quick Facts
- **arXiv ID**: 2503.21730
- **Source URL**: https://arxiv.org/abs/2503.21730
- **Reference count**: 6
- **Primary result**: Two training-free methods achieve >80% skill forgetting while maintaining <10% MMLU degradation

## Executive Summary
This paper introduces two training-free approaches for selectively removing specific capabilities from large language models while preserving overall functionality. The first method, Neuron Adjust, probabilistically shifts neuron pre-activations based on skill-specific distributions. The second method, Key Space Detection, identifies and blocks queries that cluster in specific regions of feed-forward layer key spaces. Both methods demonstrate strong unlearning performance on math, coding, and language skills across multiple model architectures, with Key Space Detection showing particular advantages when skills share neural representations.

## Method Summary
The paper proposes two inference-time unlearning techniques. Neuron Adjust identifies neurons with different pre-activation distributions between skills to forget and retain, then probabilistically shifts their values during inference. Key Space Detection finds that skill-triggering queries cluster in separable regions within feed-forward layer key spaces, allowing it to block access to these regions. Both methods require only probing the model with skill-specific datasets, making them training-free. Neuron Adjust operates on individual neuron pre-activations, while KSD operates on joint key vectors to preserve correlated representations.

## Key Results
- Neuron Adjust achieves >80% performance drop on targeted skills across math, coding, and language tasks
- Key Space Detection achieves near-perfect unlearning with minimal impact on overall model capabilities
- KSD outperforms Neuron Adjust when forgetting skills that share neural substrates (e.g., languages)
- Both methods maintain less than 10% degradation on MMLU and other retained skills

## Why This Works (Mechanism)

### Mechanism 1: Neuron Pre-Activation Distribution Shift (Neuron Adjust)
- **Claim**: Neurons exhibit skill-specific pre-activation distributions; shifting values from the forget distribution toward the retain distribution suppresses the unwanted skill.
- **Mechanism**: For each neuron, approximate forget-skill and retain-skill pre-activation distributions as Gaussians N(μ_f, σ_f) and N(μ_r, σ_r). At inference, if pre-activation v is more likely under forget distribution (p_f > p_r), probabilistically reflect v toward μ_r with probability α = p_f/(p_r + p_f).
- **Core assumption**: Neuron pre-activations are approximately Gaussian and sufficiently separable across skills.
- **Evidence anchors**:
  - [abstract]: "the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills"
  - [Section 3.1, Figure 3]: Case study shows neurons at layer 17/693 and layer 0/13366 activate differently for math vs. coding tokens
  - [corpus]: No direct corpus evidence on pre-activation distributions; related unlearning work (BLUR, Leak@k) focuses on evaluation benchmarks, not mechanisms
- **Break condition**: When forget and retain distributions heavily overlap, probabilistic shifting degrades retained capabilities.

### Mechanism 2: Skill Query Clustering in Key Space Hypercubes (Key Space Detection)
- **Claim**: Queries evoking a specific skill cluster in separable axis-aligned hypercubes within the FFL key space; blocking access to these hypercubes disables the skill.
- **Mechanism**: Probe the model with D_forget to collect key vectors v_key = σ(W_gate z) ⊙ W_up z for the last token of each query. Define forget hypercube as {μ_D ± ασ_D} where α controls tightness. At inference, if v_key ∈ hypercube, output "Your query is not valid" and halt.
- **Core assumption**: Skill-triggering queries form convex clusters capturable by axis-aligned hypercubes.
- **Evidence anchors**:
  - [abstract]: "queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube"
  - [Section 4.1, Figure 5]: At α=15, ~100% of GSM8K queries are inside the hypercube while 0% of MBPP queries are; gap persists until α≈20
  - [Section 4.1, Figure 6]: Hypercube volume shrinks and cluster separation increases in deeper layers, suggesting skill specialization
  - [corpus]: Corpus evidence weak; related papers don't address key space geometry
- **Break condition**: If skill clusters are non-convex or interleaved, hypercube blocking either over-blocks (harming retention) or under-blocks (failing to forget).

### Mechanism 3: Joint Key Space Preservation (Why KSD Outperforms Neuron Adjust)
- **Claim**: Neuron Adjust treats dimensions independently, corrupting correlated representations; KSD preserves correlations by operating on the joint key vector.
- **Mechanism**: Adjusting neurons independently can shift an unrelated query's key vector into an unfavorable region (Figure 9 left). KSD blocks only vectors within the forget hypercube, guaranteeing no modification to out-of-hypercube queries (Figure 9 right).
- **Core assumption**: Skills are encoded jointly across neurons; preserving correlation structure maintains unrelated capabilities.
- **Evidence anchors**:
  - [Section 4.1]: "One limitation of Neuron Adjust is that it treats each neuron individually, without considering their correlations"
  - [Section 5.2, Figure 8]: Neuron Adjust causes >40% drop on some languages when forgetting German/Chinese/Vietnamese/Arabic; KSD keeps non-target drops <20%
  - [corpus]: No corpus evidence on neuron correlations for unlearning
- **Break condition**: When forget/retain skills share significant key space regions (e.g., math and code both use numerical tokens), even joint blocking cannot separate them cleanly.

## Foundational Learning

- **Concept: Feed-Forward Layers as Key-Value Memories**
  - Why needed here: The paper treats FFLs as associative memories where activation vectors serve as keys and W_down projections serve as values. Understanding this decomposition is prerequisite to both methods.
  - Quick check question: In `FFL(z) = W_down(σ(W_gate z) ⊙ W_up z)`, which term is the key vector and which is the value lookup?

- **Concept: Pre-Activation vs. Activation**
  - Why needed here: Neuron Adjust operates on pre-activations (before σ); KSD operates on post-activation key vectors. Confusing these breaks both implementations.
  - Quick check question: Given a GLU-FFL, is `W_gate z` a pre-activation or an activation? What about `σ(W_gate z) ⊙ W_up z`?

- **Concept: Axis-Aligned Hypercubes in High Dimensions**
  - Why needed here: KSD bounds skill clusters using per-dimension mean ± α×std. Understanding how volume scales with dimension and α is critical for tuning.
  - Quick check question: In a 10,000-dimensional key space, if each dimension is approximately N(0,1), what fraction of vectors fall within μ ± 2σ on all dimensions simultaneously?

## Architecture Onboarding

- **Component map**:
  ```
  Input token → Residual stream → LayerNorm → FFL
                                         ↓
                               W_gate·z (pre-activation)
                                         ↓
                   σ(W_gate·z) ⊙ (W_up·z) = v_key (key vector)
                                         ↓
                               W_down·v_key → Add to residual
  ```
  - **Neuron Adjust**: Intercept pre-activations, apply distribution-based shift
  - **KSD**: Intercept final-layer v_key, check hypercube membership, block if inside

- **Critical path**:
  1. **Probe phase**: Forward-pass D_forget (and D_retain for Neuron Adjust) through model; collect pre-activations (NA) or key vectors (KSD) at each FFL
  2. **Statistics phase**: Compute per-neuron μ, σ (NA) or mean/std vectors (KSD)
  3. **Inference phase**: For each token, check thresholds and apply adjustment (NA) or block (KSD)

- **Design tradeoffs**:
  | Aspect | Neuron Adjust | Key Space Detection |
  |--------|---------------|---------------------|
  | Complexity | O(KL) per token | O(L) per token |
  | Selectivity | Per-neuron; may harm retention | Joint; guarantees out-of-cube safety |
  | Tuning | β (neuron ratio): 0.5–3% | α (cube size): skill-dependent |
  | Failure mode | Gradual capability erosion | Binary over/under-blocking |

- **Failure signatures**:
  - Selective Pruning baseline produces nonsensical output when forgetting coding (even at 0.01% pruning)—shared neurons between math/code cause collapse (Section 5.1)
  - Neuron Adjust at 3% degrades MBPP retention by ~15% when forgetting GSM8K (Figure 7)
  - KSD with too-small α leaves forget-skill performance >20%; too-large α harms retention
  - Language unlearning: Neuron Adjust causes cross-language interference for German/Chinese/Vietnamese/Arabic (Figure 8), suggesting shared neuron subspaces

- **First 3 experiments**:
  1. **Validate clustering (KSD pre-check)**: Replicate Figure 5—probe with GSM8K and MBPP, plot % queries inside {μ_gsm8k ± ασ_gsm8k} vs α. Confirm an α range where GSM8K queries are 100% inside and MBPP queries are 0% inside.
  2. **Neuron Adjust ablation**: On Llama-3-8B, forget GSM8K while retaining MBPP. Test β ∈ {0.5%, 1.5%, 3.0%}. Measure: (a) GSM8K accuracy drop, (b) MBPP accuracy retention, (c) 5-shot MMLU. Plot tradeoff curve.
  3. **KSD vs Neuron Adjust head-to-head**: For the same forget/retain task, tune α to match Neuron Adjust's forgetting quality. Compare MMLU retention and inference latency. Verify KSD's out-of-cube guarantee by probing with MMLU queries and checking v_key membership.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an efficient, automatic method be developed to determine optimal values for the adjusting ratio (β) in Neuron Adjust and the size hyperparameter (α) in Key Space Detection?
- Basis in paper: [explicit] The authors state: "we have not yet identified an efficient and automatic way to determine the optimal values for the adjusting ratio and the size hyperparameter α in both methods."
- Why unresolved: Current approach requires manual selection to balance forgetting quality vs. capability retention; no systematic method exists.
- What evidence would resolve it: An automated procedure that selects hyperparameters achieving comparable or better performance than manually-tuned values across diverse unlearning tasks.

### Open Question 2
- Question: How can skill unlearning be achieved without access to a controlled dataset capturing the capability to remove?
- Basis in paper: [explicit] The limitations section notes: "our approach can only remove capabilities that can be captured by a dataset. However, in real-world applications, a specific dataset may not always be available for every capability we wish to remove."
- Why unresolved: Both proposed methods require probing with forgetting datasets to estimate distributions and hypercubes; dataset-free unlearning remains unexplored.
- What evidence would resolve it: A method that successfully unlearning skills using only textual descriptions, few examples, or implicit signals without curated datasets.

### Open Question 3
- Question: How can unlearning methods handle skills that share significant neural substrates with skills to be retained?
- Basis in paper: [explicit] The authors note: "unlearning one skill while retaining a highly dependent skill requires a more fine-grained analysis" and observations show neurons are "multi-functional" and "polysemantic."
- Why unresolved: Current methods treat skill regions as separable; dependent skills may share neurons or key space regions, causing unintended degradation.
- What evidence would resolve it: Demonstrations of selective unlearning on skill pairs with high neural overlap (e.g., math and reasoning) without harming the retained skill.

### Open Question 4
- Question: How does non-convex clustering of queries in key space affect Key Space Detection's ability to preserve unknown capabilities?
- Basis in paper: [explicit] The limitations state: "queries may cluster in a non-convex shape within the key space," and KSD's hypercube approach may degrade unknown capabilities.
- Why unresolved: Hypercubes are convex and may over-approximate non-convex skill regions, blocking benign queries.
- What evidence would resolve it: Analysis of actual query cluster geometries in key space, and methods using non-convex decision boundaries that better preserve unrelated capabilities.

## Limitations
- **Gaussian approximation uncertainty**: The paper relies on Gaussian approximations for neuron pre-activation distributions that may not hold across all skill types or model architectures.
- **Dataset dependency**: Both methods require controlled datasets to capture the capability to remove, limiting applicability to real-world scenarios where such datasets may not exist.
- **Shared neural substrate challenge**: When forget/retain skills share significant neural representations, current methods cannot achieve selective unlearning without degrading the retained skill.

## Confidence
- **High confidence**: The core mechanism of skill-specific neuron activation differences and key space clustering is well-supported by the experimental results. The >80% forgetting performance with <10% MMLU degradation is consistently demonstrated across multiple models and tasks.
- **Medium confidence**: The superiority of KSD over Neuron Adjust for correlated skills (like languages) is demonstrated, but the explanation of why joint key space preservation is superior lacks rigorous mathematical justification beyond the case study in Figure 9.
- **Low confidence**: The generalizability of these methods to non-mathematical/coding skills and to different model families (particularly non-LLaMA architectures) is not established.

## Next Checks
1. **Distribution validation**: Replicate the pre-activation distribution analysis for at least two additional skill pairs (e.g., creative writing vs. factual Q&A) to test the Gaussian approximation assumption across diverse capabilities.
2. **Cross-architecture transferability**: Apply both methods to a non-LLaMA model (e.g., Mistral or Mixtral) to verify the key space clustering phenomenon is architecture-agnostic.
3. **Temporal stability test**: Measure how pre-activation distributions and key space clusters evolve over extended inference sessions with mixed-skill prompts to assess long-term reliability of the unlearnable subspaces.