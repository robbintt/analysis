---
ver: rpa2
title: Large Language Models Do NOT Really Know What They Don't Know
arxiv_id: '2510.09033'
source_url: https://arxiv.org/abs/2510.09033
tags:
- subject
- factual
- states
- llms
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) truly
  "know what they don''t know" by analyzing their internal hidden states during knowledge
  recall. The authors categorize factual errors into two types: associated hallucinations
  (AHs), which rely on subject information, and unassociated hallucinations (UHs),
  which do not.'
---

# Large Language Models Do NOT Really Know What They Don't Know

## Quick Facts
- arXiv ID: 2510.09033
- Source URL: https://arxiv.org/abs/2510.09033
- Authors: Chi Seng Cheang; Hou Pong Chan; Wenxuan Zhang; Yang Deng
- Reference count: 40
- Large language models encode knowledge-recall patterns rather than truthfulness, making hallucination detection via internal state probing ineffective for certain error types.

## Executive Summary
This paper challenges the assumption that large language models "know what they don't know" by demonstrating that they encode patterns of knowledge recall rather than factual truthfulness. Through mechanistic analysis of hidden states during knowledge recall, the authors identify two distinct types of factual errors: associated hallucinations (AHs) that rely on subject information and unassociated hallucinations (UHs) that do not. The key finding is that AHs and correct factual associations share indistinguishable internal computation pathways, while UHs produce distinct geometric signatures that can be detected. This reveals fundamental limitations in current hallucination detection methods based on probing internal states.

## Method Summary
The authors construct a dataset from Wikidata triples with person subjects and four relations (father, mother, spouse, date of birth). They generate model outputs using greedy decoding and categorize responses into Factual Associations (FAs), Associated Hallucinations (AHs), and Unassociated Hallucinations (UHs) based on factual correctness and subject representation reliance. The mechanistic analysis involves causal interventions on subject tokens, attention flow, and last-token representations across layers, measuring JS divergence between original and perturbed outputs. They train linear probes on hidden states for hallucination detection and implement refusal tuning via QLoRA to assess generalization patterns.

## Key Results
- Associated hallucinations and factual associations share identical internal recall processes, producing overlapping hidden-state geometries that cannot be distinguished
- Unassociated hallucinations produce distinct, clustered representations with high entropy outputs that are linearly separable from factual associations
- Hallucination detection methods succeed only on UHs (AUROC ~0.9) but fail on AHs (AUROC ~0.65), while refusal tuning generalizes to UHs (82%) but poorly to AHs (28%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Associated hallucinations and factual associations share indistinguishable internal computation pathways.**
- Mechanism: Information flows from subject token representations through early-layer MLPs, propagates via mid-layer attention to the last token, where it produces low-entropy output distributions. This recall process activates identically regardless of factual correctness.
- Core assumption: The model encodes co-occurrence patterns rather than verified truth during pretraining.
- Evidence anchors:
  - [abstract] "when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries"
  - [section 4.1] "Obs2: Associated hallucinations follow the same information flow as factual associations... This indicates that, although outputs are factually wrong, the model still relies on encoded subject information."
  - [corpus] Related work (Kang and Choi, 2023) confirms models favor tokens that frequently co-occur in training over factually correct answers.

### Mechanism 2
- Claim: **Subject representation norms and parametric alignment predict recall strength, not truthfulness.**
- Mechanism: Early-layer MLP weight subspaces ($W^{down}_\ell$) amplify inputs aligned with frequently-seen subjects. Popular subjects (high Wikipedia views) yield larger activation norms and stronger propagation; unpopular subjects yield weaker signals that fail to trigger the recall pathway.
- Core assumption: Training frequency determines representation strength, which determines output confidence.
- Evidence anchors:
  - [section 4.2.2] "UHs have significantly lower overlap ratio $r(x^\ell_s)$ than AHs... early-layer parametric weights are more aligned with FA and AH subject representations than with UH subjects"
  - [section 4.2.3] "UHs dominate among the least popular subjects (94%)... AHs are rare (1%). As subject popularity rises, UH frequency falls"
  - [corpus] Corpus evidence is weak on direct parametric alignment metrics; no related papers replicate this specific subspace analysis.

### Mechanism 3
- Claim: **Last-token hidden state geometry determines hallucination detectability.**
- Mechanism: When subject-to-last attention flow is strong (FA/AH), last-token representations diverge into subject-specific subspaces with low cosine similarity (~0.2). When flow is weak (UH), representations remain clustered (similarity ~0.5), producing high-entropy outputs that are linearly separable from FA.
- Core assumption: Detectability requires distinct geometric signatures; indistinguishable geometries imply indistinguishable truth values.
- Evidence anchors:
  - [section 4.4] "UHs retain clustered last-token representations and high-entropy outputs, while FAs and AHs diverge into subject-specific subspaces"
  - [section 5] "detection methods succeed only when outputs are detached from the input but fail when hallucinations arise from the same knowledge-recall process"
  - [corpus] Related work (Gottesman & Geva, 2024; Yüksekgönül et al., 2024) claims hidden states reveal factuality; this paper challenges that claim with mechanism-level evidence.

## Foundational Learning

- **Concept: Causal Mediation Analysis**
  - Why needed here: To isolate which hidden states causally influence factual predictions vs. hallucinations.
  - Quick check question: If you corrupt subject token representations at layer 5 and output distribution shifts dramatically, what does that tell you about the causal role of that layer?

- **Concept: Attention Contribution Norms**
  - Why needed here: To quantify how much subject information propagates to the final token position.
  - Quick check question: Why would high attention contribution norms appear in both correct predictions and hallucinations?

- **Concept: Subspace Overlap Ratio**
  - Why needed here: To measure alignment between input representations and parametric knowledge stored in MLP weights.
  - Quick check question: If a subject's representation has low overlap with MLP weight subspace, would you expect high or low output confidence?

## Architecture Onboarding

- **Component map:** Subject embedding → early MLP (norm amplification if aligned) → mid-layer attention (subject-to-last transfer) → late-layer last token (subspace divergence) → output logits

- **Critical path:** Subject representations formed in early layers, propagate through attention to last token, where geometry determines output type

- **Design tradeoffs:**
  - Detection probes trained on UHs generalize to UHs but fail on AHs (AUROC ~0.9 vs ~0.6)
  - Refusal tuning on UHs generalizes within-category (82%) but not to AHs (28%)
  - Subject-popularity heuristics flag UHs but miss AHs on popular entities

- **Failure signatures:**
  - High attention flow + low output entropy + factual error = Associated Hallucination (undetectable via internal probes)
  - Low attention flow + high output entropy + factual error = Unassociated Hallucination (detectable)
  - Refusal tuning over-refuses on FAs when trained on UHs (29.5% false refusal rate)

- **First 3 experiments:**
  1. **Replicate JS divergence intervention analysis**: Corrupt subject/attention/last-token representations at each layer; plot divergence curves for FA vs AH vs UH samples to confirm overlapping causal pathways.
  2. **Train linear probes on last-token hidden states**: Evaluate AUROC on held-out AH-only, UH-only, and mixed test sets; expect ~0.65 on AH vs ~0.93 on UH.
  3. **Measure refusal tuning generalization**: Fine-tune on UH refusal pairs, evaluate refusal rates on unseen UH, AH, and FA samples; confirm asymmetric generalization gap.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the overlapping hidden-state geometries between factual associations and associated hallucinations persist in long-form, open-ended text generation tasks?
  - Basis in paper: [explicit] The authors state in the Limitations section that the study is "primarily limited to factual completion prompts" and "does not extend to long-form or open-ended text generation tasks."
  - Why unresolved: The current causal analysis relies on single-token prediction trajectories. Long-form generation involves complex multi-token dependencies and planning, potentially altering how subject representations propagate.
  - What evidence would resolve it: Applying the paper's causal intervention and t-SNE analysis methods to open-ended generation datasets to determine if associated hallucinations remain geometrically indistinguishable from factual outputs.

- **Open Question 2:** How do large reasoning models (LRMs) with multi-step chain-of-thought capabilities process associated hallucinations differently than standard LLMs?
  - Basis in paper: [explicit] The authors identify the extension of this methodology to "large reasoning models" as a "promising direction," noting that recent studies have begun examining their internal states.
  - Why unresolved: Reasoning models introduce intermediate steps (hidden states for reasoning tokens) before the final answer. It is unknown if the "recall vs. truthfulness" conflict occurs during the reasoning phase or the final generation phase.
  - What evidence would resolve it: A layer-wise causal mediation analysis on reasoning traces to see if subject-driven hallucinations originate in the reasoning steps or if the reasoning layers provide a disentangling mechanism.

- **Open Question 3:** Can verbalization-based strategies (prompting a model to express confidence) effectively distinguish associated hallucinations from factual associations?
  - Basis in paper: [explicit] The Limitations section notes the study does not include "verbalization-based strategies" and suggests that "exploring such approaches may offer complementary insights."
  - Why unresolved: The paper demonstrates that internal hidden states fail to separate AHs from FAs. Verbalized uncertainty relies on the output distribution/logits rather than intermediate dense vectors, which might behave differently.
  - What evidence would resolve it: Evaluating the calibration of verbalized confidence scores specifically on the dataset of associated hallucinations (AHs) constructed in this paper to see if they outperform the linear probes.

## Limitations

- The study focuses on a single model (LLaMA-3-8B) and narrow task domain (Wikidata factual questions about people), limiting generalizability to other architectures and knowledge domains
- The AH/UH classification relies on a JS divergence threshold that is only loosely defined as "based on average JS divergence across correct answers," introducing potential subjectivity
- The analysis assumes overlapping hidden-state geometries directly imply indistinguishable truth values without exploring whether more sophisticated, non-linear detection methods could overcome this limitation

## Confidence

- **High Confidence (80-100%)**: The core finding that FA and AH share indistinguishable internal geometries is strongly supported by multiple lines of evidence including JS divergence analysis, attention flow patterns, and last-token representation clustering. The inability of linear probes to distinguish AH from FA is reproducible.
- **Medium Confidence (50-80%)**: The UH vs FA detectability results and refusal tuning generalization patterns are well-documented but may vary across models and datasets. The specific numerical thresholds (AUROC values, refusal ratios) could shift with different evaluation protocols.
- **Low Confidence (0-50%)**: The precise JS divergence threshold for AH/UH classification and the exact prompt templates for refusal tuning remain underspecified, potentially affecting replication.

## Next Checks

1. **Replicate the JS divergence intervention analysis**: Apply subject token masking at each layer for FA, AH, and UH samples; compute output distribution divergence. Confirm overlapping divergence curves for FA and AH while UH shows distinct patterns.

2. **Test cross-model generalization**: Apply the same mechanistic analysis pipeline to different model architectures (e.g., Mistral, Qwen) and domains (e.g., scientific facts, historical events). Measure whether FA/AH geometry overlap persists across models.

3. **Evaluate non-linear detection methods**: Train transformer-based or MLP classifiers on last-token hidden states instead of linear probes. Assess whether non-linear boundaries can distinguish AH from FA despite geometric overlap, and whether this improves refusal tuning generalization to AHs.