---
ver: rpa2
title: 'EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading'
arxiv_id: '2512.20817'
source_url: https://arxiv.org/abs/2512.20817
tags:
- essay
- concept
- essaycbm
- grade
- grading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EssayCBM is a rubric-aligned framework that makes automated essay
  grading transparent by evaluating essays on eight writing concepts before computing
  the final grade. Instead of mapping text directly to grades, it uses concept scores
  as an interpretable bottleneck, enabling students and educators to understand how
  scores are determined.
---

# EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading

## Quick Facts
- arXiv ID: 2512.20817
- Source URL: https://arxiv.org/abs/2512.20817
- Reference count: 9
- Primary result: BERT-based EssayCBM achieved 81.14% accuracy and 62.38 F1-score on the AES dataset.

## Executive Summary
EssayCBM is a transparent automated essay grading system that evaluates essays on eight rubric-aligned writing concepts before computing a final grade. Unlike black-box models that map text directly to scores, EssayCBM uses concept scores as an interpretable bottleneck, enabling students and educators to understand how grades are determined. Built as a web application with real-time grading and human-in-the-loop correction, the system maintains competitive accuracy with black-box models while providing actionable, concept-level feedback. The framework was evaluated on the AES dataset using multiple transformer architectures including BERT, RoBERTa, GPT-2, and LSTM.

## Method Summary
EssayCBM uses a Concept Bottleneck Model (CBM) architecture where a pre-trained encoder (BERT, RoBERTa, GPT-2, or LSTM) processes essay text into an embedding, which is then passed to eight independent concept prediction heads that score rubric-aligned dimensions like Thesis Clarity and Use of Evidence. A lightweight feed-forward network then computes the final grade using only these concept scores, not the original text or embedding. The model is trained end-to-end with a combined loss function (L_grade + λ * L_concept) to optimize both concept prediction accuracy and final grade prediction. Eight concept annotations per essay (0-4 scale) were generated using Gemini 2.5 Flash, and the system was evaluated on the AES dataset using accuracy and macro-F1 metrics.

## Key Results
- BERT-based EssayCBM achieved 81.14% accuracy and 62.38 F1-score on the AES dataset
- BERT-based EssayCBM slightly outperformed direct black-box BERT baseline
- RoBERTa showed slight performance degradation with the bottleneck approach
- LSTM variant offered fastest inference speed suitable for real-time applications

## Why This Works (Mechanism)

### Mechanism 1: Interpretable Bottleneck Constraint
Forcing the final grade to be computed solely from human-aligned concept scores makes the model's decision-making process transparent. An encoder generates an essay embedding, which is passed to eight independent concept prediction heads, and a separate network computes the final grade using only these concept scores. The core assumption is that eight rubric-aligned concepts are sufficient to capture essay quality determinants. Evidence shows the architecture design ensures complete traceability by preventing the grade predictor from accessing essay text or embeddings directly. Break condition: if concepts are insufficient or the aggregation function is too weak, accuracy would drop below black-box levels.

### Mechanism 2: Joint Training for Alignment
Training concept prediction heads and final grade predictor jointly helps the model learn concept representations that are both predictive of the final grade and aligned with outcomes. The composite loss function (L_total = L_grade + λ * L_concept) optimizes the encoder and prediction heads for both tasks simultaneously. The core assumption is that aligning training on human-provided concept annotations will guide internal representations toward meaningful, rubric-aligned features. Evidence shows end-to-end training with combined loss. Break condition: if concept annotations are noisy or tasks conflict, joint training could degrade performance compared to pure black-box models.

### Mechanism 3: Human-in-the-Loop Correction for Trust & Refinement
Allowing instructors to manually adjust predicted concept scores creates a system supporting expert oversight with immediate links between rubric items and final grades. The architecture separates concept prediction from grade aggregation, enabling instant grade recalculation when experts correct concept scores without retraining. The core assumption is that domain experts have the knowledge and time to review and correct concept-level predictions, leading to more accurate or trusted assessments. Evidence shows the interface supports manual concept score adjustments with instant grade recalculations. Break condition: if base concept predictions are consistently poor, requiring constant correction, the system will fail to reduce educator workload and will not be trusted or adopted.

## Foundational Learning

- **Concept: Concept Bottleneck Model (CBM)**
  - Why needed: This is the core architectural innovation; the entire grading process is built on this intermediate layer of interpretable concepts, making it different from standard black-box BERT classifiers.
  - Quick check: Can you explain why a CBM is considered "inherently interpretable" compared to a standard deep learning model?

- **Concept: Encoder (Transformer/LSTM) Text Representation**
  - Why needed: The model first needs to understand raw essay text; BERT, RoBERTa, GPT-2, and LSTM are used as encoders to convert text into fixed-size numerical vectors, which are input to concept prediction heads.
  - Quick check: What is the output of an encoder like BERT in this architecture (a single vector, a sequence of vectors, etc.), and how is it used?

- **Concept: Multi-task and Joint Loss Optimization**
  - Why needed: The model is trained to predict concept scores and the final grade; understanding how these tasks are combined into a single objective function is critical to understanding training.
  - Quick check: What are the two components of the loss function L_total, and what is the role of the hyperparameter λ (lambda)?

## Architecture Onboarding

- **Component map:**
  Frontend (Streamlit) -> Backend (FastAPI) -> Model (Encoder -> Concept Heads -> Final Grade Predictor)

- **Critical path:**
  1. User submits essay via Streamlit frontend
  2. Text is tokenized and passed to selected encoder (e.g., BERT)
  3. Encoder output is fed into eight independent concept prediction heads
  4. These eight concept scores alone are passed to feed-forward network to predict grade
  5. Concept scores and final grade are displayed; expert can adjust concept score, triggering recalculation

- **Design tradeoffs:**
  - BERT: Highest accuracy (81.14%) and F1-score, recommended for accuracy-focused deployment
  - LSTM: Much faster inference speed, suitable for real-time use cases, though with lower accuracy
  - Transparency vs. Accuracy: BERT-based EssayCBM slightly outperforms BERT baseline, suggesting bottleneck doesn't degrade performance; RoBERTa showed slight degradation, indicating tradeoff exists and depends on encoder

- **Failure signatures:**
  - Inaccurate Concept Predictions: If concept heads are poorly trained, final grade will be based on incorrect inputs; human-in-the-loop feature mitigates this but requires expert effort
  - Lack of Nuance: If eight concepts aren't granular enough to capture subtle distinctions, model may struggle with complex essay profiles not well-represented by rubric
  - Dependency on Annotations: Entire concept head training relies on Gemini 2.5 Flash annotation quality; systematic bias will be learned by model

- **First 3 experiments:**
  1. Establish baseline: Run provided baseline black-box models (BERT, RoBERTa, etc.) on AES dataset to get baseline accuracy and F1 scores for direct grade prediction
  2. Ablate the bottleneck: Create version bypassing concept heads, predicting grade directly from encoder output; compare performance to full EssayCBM to quantify bottleneck impact
  3. Test lambda hyperparameter: Retrain model with different λ values (e.g., 0.1, 0.5, 1.0, 2.0); plot both concept and grade accuracies to see how they trade off

## Open Questions the Paper Calls Out
- Can EssayCBM be effectively extended to structured domains like code or math problems? The authors state future work includes extending to short answers, code, and math problems, but current eight concepts are specific to natural language, making it unclear what interpretable bottleneck concepts would be for programming or mathematical logic.
- How does model performance change when trained on human-annotated concept labels compared to the LLM-generated labels used in this study? The methodology relies entirely on Gemini 2.5 Flash to generate ground-truth concept annotations rather than human expert labels, raising questions about potential LLM hallucinations or biases distinct from human pedagogical standards.
- Why does the concept bottleneck improve performance for BERT but degrade it for RoBERTa? Table 1 shows RoBERTa accuracy dropped from 80.70% to 79.39% while BERT improved, but the paper doesn't analyze why the intermediate concept constraint causes negative transfer for RoBERTa, suggesting the bottleneck may be lossy for certain encoder representations.

## Limitations
- Dataset Specificity: The AES dataset identity and preprocessing steps are not explicitly specified, and different essay grading datasets may have varying prompt types, grade distributions, and concept alignments affecting model performance and generalizability.
- Concept Annotation Quality: The 8 concept annotations were generated via Gemini 2.5 Flash without detailed prompting specifications, introducing potential variability and bias in annotations that directly impacts concept heads' training quality.
- Architectural Details: The exact configuration of concept classifier heads and final aggregator h are described as "2-3 layers" without precise hyperparameters, creating ambiguity that could lead to differences in model capacity and performance during reproduction.

## Confidence
- **High Confidence**: The core CBM architecture (encoder → concept heads → final aggregator) is well-defined and logically sound; the claim that forcing final grade computation from intermediate concept scores makes the model interpretable is strongly supported by design.
- **Medium Confidence**: The claim that joint training aligns concept representations with final grade outcomes is plausible but lacks strong empirical evidence; effectiveness depends on quality and alignment of concept annotations.
- **Low Confidence**: The claim that human-in-the-loop correction significantly improves trust and reduces workload is supported by design rationale but lacks quantitative evidence of its impact on model accuracy or educator effort.

## Next Checks
1. **Baseline Ablation Study**: Implement and train a direct black-box baseline (encoder → grade) and compare its performance to EssayCBM to quantify the impact of the bottleneck constraint on accuracy and provide a clearer picture of the tradeoff between interpretability and performance.
2. **Concept Annotation Analysis**: Compute inter-annotator agreement (e.g., using Cohen's Kappa) on a held-out sample of essays annotated by Gemini 2.5 Flash to diagnose the quality and consistency of concept annotations, a critical input for concept heads.
3. **Lambda Sensitivity Sweep**: Retrain EssayCBM with a range of λ values (e.g., 0.1, 0.5, 1.0, 2.0) and plot both concept prediction accuracy and final grade accuracy to identify the optimal balance and test the robustness of the joint training approach.