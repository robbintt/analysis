---
ver: rpa2
title: 'Episodic Memory in Agentic Frameworks: Suggesting Next Tasks'
arxiv_id: '2511.17775'
source_url: https://arxiv.org/abs/2511.17775
tags:
- workflow
- workflows
- agent
- steps
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recommending next steps in
  scientific workflow creation using agentic frameworks powered by Large Language
  Models (LLMs). The proposed solution is an episodic memory architecture that stores
  and retrieves past workflows to guide agents in suggesting plausible next tasks,
  avoiding the need for fine-tuning LLMs with scarce proprietary data.
---

# Episodic Memory in Agentic Frameworks: Suggesting Next Tasks

## Quick Facts
- arXiv ID: 2511.17775
- Source URL: https://arxiv.org/abs/2511.17775
- Reference count: 11
- One-line primary result: Episodic memory architecture suggests next workflow steps by retrieving and matching past workflows using text embeddings and tool comparison, avoiding LLM fine-tuning.

## Executive Summary
This paper introduces an episodic memory architecture for agentic frameworks that suggests next steps in scientific workflow creation by retrieving structurally similar past workflows. The system stores execution histories in a provenance database and uses a hybrid similarity algorithm combining text embeddings for instructions and exact function-name matching for tools. In a Materials Science PFAS hazard assessment application, the architecture successfully suggested contextually appropriate next steps based on historical workflow patterns, with a similarity threshold of 0.65. When no similar workflows could be found, the system fell back to suggesting steps based on the crew's capabilities, ensuring continuous suggestion quality.

## Method Summary
The method employs an Episodic Memory Agent that intercepts user instructions, forwards them to a domain crew of specialized agents, and captures execution trajectories. These trajectories are compiled into formalized workflows using a Workflow Compiler that filters out reasoning steps and failed tool calls, producing structured descriptions of ordered steps with inputs, outputs, and nested sub-steps. The compiled workflows are stored in a provenance database using W3C PROV schemas. For retrieval, a hybrid similarity algorithm computes cosine similarity on text embeddings for user-instruction steps and exact function-name matching for tool calls, retrieving workflows when the weighted average exceeds threshold T=0.65. If no matches are found, the system prompts the LLM with crew capability descriptions instead of retrieved workflows.

## Key Results
- Successfully suggested next steps based on historical workflow patterns in PFAS hazard assessment
- Achieved reliable retrieval with similarity threshold T=0.65
- Maintained suggestion quality through fallback to crew capability descriptions when memory retrieval failed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving structurally similar past workflows enables contextually appropriate next-step suggestions without domain-specific fine-tuning.
- Mechanism: The system computes similarity between the current workflow's leaf-step sequence and subsequences of stored workflows using a hybrid scoring function: cosine similarity on text embeddings for user-instruction steps, and exact function-name matching for tool calls. A weighted average above threshold T triggers retrieval.
- Core assumption: Past workflow patterns are predictive of useful future steps; users working on similar task sequences will likely follow similar subsequent actions.
- Evidence anchors:
  - [abstract] "Retrieval based on a workflow similarity algorithm using text embeddings and tool comparison."
  - [section 2] "We retrieve Wi if sim(Sc, S′i) > T for a subsequence S′i of Si, such that sim(Sc, S′i) is the mean of the similarities of each step in both sequences."
  - [corpus] "Agentic Episodic Control" (FMR=0.58) supports episodic memory for improving RL agent generalizability, suggesting cross-domain validity of memory-based action selection.
- Break condition: If the workflow database lacks coverage for the task domain, or if the similarity threshold (T=0.65) is too high, retrieval fails and falls back to capability-based suggestion.

### Mechanism 2
- Claim: Compiling execution trajectories into formalized workflows abstracts noisy agent reasoning into reusable procedural knowledge.
- Mechanism: The Workflow Compiler filters trajectory logs—removing reasoning steps, failed tool calls, and non-essential metadata—to produce a recursive workflow structure of ordered steps (function calls and user-instructions) with inputs, outputs, and nested sub-steps.
- Core assumption: Task-essential structure can be distillable from execution traces; reasoning intermediates are not needed for workflow matching.
- Evidence anchors:
  - [section 2] "The process of compiling trajectory into a workflow generates a formalized description of the tasks in the trajectory, ignoring information that is not essential to reproduce the workflow, such as agent reasoning steps and failed tool calls."
  - [corpus] "From Knowledge to Noise: CTIM-Rover" warns that episodic memory can introduce noise, reinforcing the need for selective compilation.
- Break condition: If trajectory logs are incomplete, or if reasoning steps contain critical task logic, the compiled workflow will be lossy and retrieval quality degrades.

### Mechanism 3
- Claim: A dual-prompt strategy with fallback to crew capability descriptions maintains suggestion quality when episodic memory has no relevant precedents.
- Mechanism: When retrieval returns no workflows above threshold, the EM Agent prompts the LLM with a crew capability description (roles, tools, descriptions) instead of retrieved workflows. This prevents empty suggestions and leverages the LLM's pretrained knowledge.
- Core assumption: Crew capability descriptions are sufficient for the LLM to generate plausible, tool-aligned suggestions when no historical data exists.
- Evidence anchors:
  - [section 2] "If no similar workflow can be found, the agent provides the LLM with an alternative prompt with the description of the domain crew and ask the LLM to suggest possible next steps based on the crew's capabilities."
  - [section 3, Figure 4] Demonstrates fallback behavior when no workflows match a query about perfluorooctanoic acid.
  - [corpus] Evidence is limited; related work focuses on retrieval-augmented memory rather than fallback strategies.
- Break condition: If crew capability descriptions are incomplete, outdated, or misaligned with actual tool behavior, suggestions will be hallucinated or infeasible.

## Foundational Learning

- Concept: **Episodic Memory in Cognitive Architectures**
  - Why needed here: The architecture draws directly from cognitive science models where agents store and retrieve experience-contextualized memories to inform decisions. Understanding this helps clarify why workflow instances—not just rules—are stored.
  - Quick check question: Can you explain the difference between episodic memory (event-based, contextual) and semantic memory (fact-based, decontextualized) in the context of workflow storage?

- Concept: **Provenance Data Models (W3C PROV)**
  - Why needed here: Workflows are stored using ProvLake/W3C PROV schemas. Engineers must understand provenance entities, activities, and agents to correctly serialize and query workflow histories.
  - Quick check question: Given a workflow with nested sub-steps, how would you represent it using PROV entities and relationships?

- Concept: **Embedding-Based Similarity with Threshold Tuning**
  - Why needed here: The retrieval mechanism depends on cosine similarity thresholds (T=0.65). Understanding embedding spaces and threshold tradeoffs is critical for calibrating precision vs. recall in workflow matching.
  - Quick check question: If retrieval returns too many irrelevant workflows, should you raise or lower the similarity threshold, and what is the tradeoff?

## Architecture Onboarding

- Component map:
  - Agentic UI/Platform -> User instructions input
  - Domain Crew (e.g., Chemist Agent + sub-agents) -> Executes tasks, returns result + trajectory
  - EM Agent (interceptor layer) -> Compiles, retrieves, suggests, saves
  - Workflow Compiler -> Converts trajectory -> formalized workflow
  - Episodic Memory Retrieval Tool -> Queries Workflow DB with similarity algorithm
  - Workflow DB (ProvLake/W3C PROV) -> Stores memory workflows
  - Episodic Memory Saving Tool -> Persists current workflow on /save command

- Critical path:
  1. User sends instruction via UI
  2. EM Agent forwards instruction to Domain Crew
  3. Domain Crew executes, returns result + trajectory
  4. Workflow Compiler processes trajectory -> current workflow
  5. Retrieval Tool queries Workflow DB with current workflow
  6. If matches found (sim > 0.65): LLM prompted with similar workflows -> suggestions
  7. If no matches: LLM prompted with crew capabilities -> suggestions
  8. Suggestions concatenated to result -> returned to UI

- Design tradeoffs:
  - **Concatenation vs. Response Generation**: Authors chose direct concatenation to avoid LLM "solving" suggested tasks instead of just listing them. Tradeoff: less natural output formatting.
  - **Exact function-name matching vs. semantic tool matching**: Current implementation uses strict name equality for tool calls, which is precise but brittle if tool names evolve.
  - **Threshold T=0.65**: Empirically set; higher values improve precision but reduce recall, potentially missing useful suggestions.

- Failure signatures:
  - **Empty retrieval + weak crew description**: Suggestions become generic or hallucinated.
  - **Overly permissive threshold**: Too many low-relevance workflows retrieved, diluting suggestion quality.
  - **Incomplete trajectory logging**: Workflow compilation misses critical steps, breaking similarity matching.

- First 3 experiments:
  1. **Threshold sweep**: Vary T (e.g., 0.5, 0.65, 0.8) and measure precision@k of suggestions against human-labeled ground truth. Determine optimal operating point for your domain.
  2. **Fallback quality audit**: Force retrieval failure (empty DB) and compare crew-capability suggestions vs. human expert recommendations. Identify gaps in capability descriptions.
  3. **Workflow compiler fidelity test**: Execute known workflows, compile trajectories, and compare compiled structure against hand-authored ground truth. Measure recall of critical steps and false-positive inclusion of noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the workflow matching algorithm be evolved to maintain accuracy when agent tool structures undergo significant changes or refactoring?
- Basis in paper: [explicit] The authors state that "bigger changes might require more complex workflow matching algorithms" in the Final Remarks.
- Why unresolved: The current implementation uses a simple tree matching algorithm that may lack resiliency against structural drift in tools or agents.
- What evidence would resolve it: Retrieval accuracy metrics benchmarked against datasets where tool definitions have been intentionally altered or deprecated.

### Open Question 2
- Question: Can the architecture be extended to effectively suggest the reuse of specific intermediate results rather than just recommending abstract next tasks?
- Basis in paper: [explicit] The paper lists the "addition of suggestions for reuse of previous results" as a specific avenue for future work.
- Why unresolved: The current system focuses on recommending subsequent steps based on patterns, lacking the capability to identify and recommend identical data states to skip redundant work.
- What evidence would resolve it: Successful implementation of a result-caching module and a demonstration of reduced computational steps in repeated workflow scenarios.

### Open Question 3
- Question: To what extent does the synthetic bootstrapping of workflow memories using LLMs degrade recommendation quality compared to human-curated provenance data?
- Basis in paper: [inferred] The paper notes performance is "highly dependent on the quality of the curated workflows database" while simultaneously admitting the test database was bootstrapped via LLM (Llama 4).
- Why unresolved: It is unclear if the "synthetic" memories possess the necessary nuance to guide real scientific inquiry effectively compared to genuine user logs.
- What evidence would resolve it: An ablation study comparing task suggestion relevance scores between agents using synthetic memory versus those using a database of verified human workflows.

## Limitations
- System effectiveness depends heavily on having a sufficiently populated and representative memory database
- Hybrid similarity algorithm's reliance on exact tool name matching introduces brittleness when tool interfaces evolve
- Fallback mechanism using crew capability descriptions lacks empirical validation and comprehensive specification

## Confidence
- **High confidence**: The core mechanism of storing and retrieving historical workflows using provenance schemas is well-established. The workflow compilation process and similarity-based retrieval algorithm are clearly specified and implementable.
- **Medium confidence**: The threshold value (T=0.65) and hybrid scoring approach are empirically chosen but not rigorously validated across diverse scenarios. The fallback to crew capabilities represents a reasonable engineering decision but lacks empirical validation.
- **Low confidence**: The quality and completeness of bootstrapped memory workflows, the exact prompt templates used, and the embedding models employed are not specified, making faithful reproduction challenging.

## Next Checks
1. **Threshold sensitivity analysis**: Systematically vary the similarity threshold (0.5, 0.65, 0.8) and measure precision/recall of suggestions against human-annotated ground truth workflows to identify optimal operating points.
2. **Fallback mechanism audit**: Create controlled experiments with empty memory databases and compare crew-capability suggestions against expert-generated recommendations to quantify quality gaps.
3. **Compiler fidelity validation**: Execute known workflows end-to-end, compile the resulting trajectories, and measure structural similarity between compiled and ground-truth workflows to identify information loss during compilation.