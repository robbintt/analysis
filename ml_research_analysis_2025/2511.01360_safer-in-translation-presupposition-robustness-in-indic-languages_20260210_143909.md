---
ver: rpa2
title: Safer in Translation? Presupposition Robustness in Indic Languages
arxiv_id: '2511.01360'
source_url: https://arxiv.org/abs/2511.01360
tags:
- language
- languages
- turbo
- cancer-myth
- indic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates large language models (LLMs) on presupposition
  robustness in Indic languages, addressing the gap in multilingual LLM evaluation
  for healthcare queries. Cancer-Myth-Indic, a benchmark of 2,500 items (500 per language)
  in Hindi, Malayalam, Marathi, Tamil, and Telugu, was created by translating and
  preserving implicit presuppositions in false cancer myths.
---

# Safer in Translation? Presupposition Robustness in Indic Languages

## Quick Facts
- arXiv ID: 2511.01360
- Source URL: https://arxiv.org/abs/2511.01360
- Reference count: 0
- Models correct cancer myths better in Indo-Aryan than Dravidian languages; GPT-4o achieves >80% PCR across all languages while GPT-3.5 Turbo shows negative scores on Dravidian languages.

## Executive Summary
This work evaluates large language models (LLMs) on presupposition robustness in Indic languages, addressing the gap in multilingual LLM evaluation for healthcare queries. Cancer-Myth-Indic, a benchmark of 2,500 items (500 per language) in Hindi, Malayalam, Marathi, Tamil, and Telugu, was created by translating and preserving implicit presuppositions in false cancer myths. Evaluations show GPT-4o achieves high correction rates (>80% PCR) across all languages, while GPT-3.5 Turbo shows negative scores on Dravidian languages, confirming myths rather than correcting them. The study highlights language-conditioned safety asymmetries and recommends language-aware deployment policies for health applications.

## Method Summary
The Cancer-Myth-Indic benchmark was constructed by translating 500 items from the English Cancer-Myth benchmark into five Indic languages using native-speaker translators who followed a style guide to preserve implicit presuppositions. Each model was evaluated using deterministic decoding with a fixed three-point rubric (-1, 0, +1) scored by GPT-4o judge. Presupposition Correctness Score (PCS) and Presupposition Correction Rate (PCR) were computed per language and aggregated by language family (Indo-Aryan vs. Dravidian).

## Key Results
- GPT-4o achieves PCR >80% across all five Indic languages
- GPT-3.5 Turbo shows negative PCS on Dravidian languages (-61.1 PCS on Malayalam/Tamil)
- Large performance gap between language families: GPT-3.5 Turbo PCR 34.8% (Indo-Aryan) vs 13.2% (Dravidian)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model capability scaling differentially improves presupposition correction across language families, with larger gains for languages where older models fail catastrophically.
- Mechanism: Stronger models (GPT-4o vs GPT-3.5 Turbo) appear better equipped to handle morpho-pragmatic variation and domain priors in underrepresented languages, reducing sycophantic myth confirmation.
- Core assumption: The observed improvement stems from model capability rather than evaluation artifacts or translation drift.
- Evidence anchors:
  - [abstract] "GPT-4o achieves high correction rates (>80% PCR) across all languages, while GPT-3.5 Turbo shows negative scores on Dravidian languages"
  - [Table 2] GPT-3.5 Turbo: -61.1 PCS on Dravidian vs GPT-4o: 81.5 PCS—a 142.6 point swing
  - [corpus] Related work confirms multilingual safety degrades for older models in low-resource settings (neighbor FMR avg=0.43, but limited direct citation evidence)
- Break condition: If scoring rubric is miscalibrated across languages, family-level asymmetries may reflect evaluation bias rather than genuine capability gaps.

### Mechanism 2
- Claim: Morpho-pragmatic structures in Indic languages create language-family-conditioned affordances for presupposition correction.
- Mechanism: Dravidian languages (Malayalam, Tamil) employ rich negation systems and clause-typing that support concise denials; Indo-Aryan languages (Hindi, Marathi) use clause-level particles shaping stance. These grammatical features may interact with model training distributions.
- Core assumption: The performance gap between language families reflects linguistic structure and training coverage, not translation quality.
- Evidence anchors:
  - [Section 5] "Negation scope and periphrasis. Malayalam and Tamil allow auxiliary-based and suffixal negation that supports concise denials while preserving the question frame"
  - [Section 5] "Interrogative particles and stance. Hindi and Marathi use clause-level particles such as kya and ka that shape the distribution of neutral and assertive answers"
  - [corpus] Limited direct corpus evidence; related work focuses on Indic benchmarks but not specifically on presupposition morpho-pragmatics
- Break condition: If translation inadvertently lexicalizes claims or adds negation markers differently across languages, observed gaps could be translation artifacts.

### Mechanism 3
- Claim: Domain-specific corpus priors improve presupposition correction for health topics with stronger public health coverage in target languages.
- Mechanism: Models may leverage prior exposure to fact-checking portals and public health communication in Indic languages (e.g., diet myths, screening recommendations) to correct presuppositions without explicit lexical cues.
- Core assumption: Category-level variation in correction rates reflects differential training coverage, not prompt difficulty.
- Evidence anchors:
  - [Section 5] "We observe higher correction rates for myths that are well covered by public health communication in Indic languages, for example diet or herbal miracle cures"
  - [Section 5] References ICMR and National Health Portal resources as corpus signals
  - [corpus] No direct corpus quantification of health content coverage per language
- Break condition: If certain myth categories are inherently easier to correct regardless of language coverage, corpus prior mechanism is confounded.

## Foundational Learning

- Concept: **Presupposition robustness**
  - Why needed here: The paper evaluates whether LLMs can identify and correct false embedded assumptions in user prompts, not just answer questions accurately.
  - Quick check question: If a user asks "What's the best herbal cure for cancer?", does the model (a) recommend herbs, (b) say there's no best cure, or (c) reject the premise that herbal cures exist for cancer?

- Concept: **Presupposition Correctness Score (PCS) and Presupposition Correction Rate (PCR)**
  - Why needed here: These are the paper's core metrics for measuring model behavior on false-presupposition prompts.
  - Quick check question: A model with PCR=15% and PCS=-60 on a language is doing what? (Answer: Correcting only 15% of myths, and on average confirming more than correcting.)

- Concept: **Curse of multilinguality**
  - Why needed here: The paper situates its findings in the broader challenge that multilingual LLMs face performance degradation in low-resource languages.
  - Quick check question: Why might adding more languages to an LLM not monotonically improve performance across all languages?

## Architecture Onboarding

- Component map:
  - Cancer-Myth benchmark -> Translation pipeline -> Cancer-Myth-Indic (2,500 items) -> Evaluation pipeline -> PCS/PCR scores per language

- Critical path:
  1. Preserve implicit presuppositions in translation (no lexicalization, no added negation)
  2. Maintain adversarial nature of prompts across languages
  3. Use fixed judge rubric for cross-language comparability
  4. Spot-check with bilingual human review using implicitness checklist

- Design tradeoffs:
  - One translator per language (no inter-annotator agreement) vs resource constraints
  - Monolingual judge prompt vs cross-language recalibration
  - Fixed protocol for comparability vs potential language-specific calibration needs

- Failure signatures:
  - Negative PCS: Model confirms myths more often than corrects (GPT-3.5 Turbo on Dravidian: −61.1)
  - Low PCR (<20%): Model fails to recognize false presuppositions
  - Large family-level gap: Safety asymmetry between Indo-Aryan and Dravidian (GPT-3.5 Turbo: 34.8% vs 13.2% PCR)

- First 3 experiments:
  1. Reproduce family-level PCS/PCR gap on a held-out 100-item sample per language to confirm robustness.
  2. Run ablation with explicit vs implicit presupposition phrasing to quantify adversariality preservation.
  3. Test whether English-to-Indic backtranslation recovers the original presupposition structure as a translation quality check.

## Open Questions the Paper Calls Out

- Question: Is the automated GPT-4o judge equally calibrated for evaluating presupposition correction across morphologically distinct Indic languages?
  - Basis in paper: [explicit] Section 7 (Limitations) states the analysis relies on a fixed scoring pipeline that "may not be calibrated across languages" and that the judge prompt was not recalibrated per language.
  - Why unresolved: The validity of the reported performance gaps (e.g., GPT-3.5's negative scores in Dravidian languages) depends on the judge interpreting non-English responses consistently, which remains unverified.
  - What evidence would resolve it: A human-in-the-loop evaluation of the judge's scoring on a stratified multilingual sample to assess cross-linguistic consistency.

- Question: Can specific prompt interventions, such as enforced response templates, mitigate the myth-confirmation behavior observed in weaker models?
  - Basis in paper: [explicit] Section 6 (Deployment guidance) suggests that for health advice, systems should "enforce response templates that first paraphrase and check the presupposition."
  - Why unresolved: The paper evaluates raw model capabilities but does not experimentally validate whether the proposed mitigation strategies actually improve Presupposition Correction Rates (PCR).
  - What evidence would resolve it: Benchmarking GPT-3.5 Turbo and GPT-4 Turbo using the suggested constrained decoding or prompt engineering to measure PCR improvement.

- Question: Does single-translator sourcing introduce variance that affects the preservation of implicit presuppositions in the dataset?
  - Basis in paper: [explicit] Section 7 notes the study relies on "one translator per language" and "no formal inter-annotator agreement," while Section 4 mentions translators followed a style guide to ban lexicalization.
  - Why unresolved: Without measuring agreement, it is unclear if the "implicitness" of the false presuppositions was preserved uniformly across languages, which could confound the difficulty of the task.
  - What evidence would resolve it: Re-translating a subset of items with multiple translators and measuring inter-annotator agreement on the implicitness checklist.

## Limitations

- Translation fidelity uncertainty without access to style guide or inter-annotator agreement scores
- Monolingual judge approach assumes consistent rubric interpretation across languages without cross-language recalibration
- Domain-specific focus on cancer myths without testing generalization to other presupposition types

## Confidence

- **High confidence:** The observed performance gap between GPT-4o and GPT-3.5 Turbo on Dravidian languages (PCR 81.5 vs -61.1) is robust to measurement error.
- **Medium confidence:** Language-family performance differences reflect genuine capability gaps rather than translation artifacts.
- **Low confidence:** The claim that domain-specific corpus priors drive category-level variation.

## Next Checks

1. Conduct inter-annotator agreement testing on 50 randomly selected items across all five languages to validate translation fidelity.
2. Run parallel human evaluation on 100 stratified items to measure judge score consistency against human judgments.
3. Evaluate the same models on a non-health benchmark with implicit presuppositions to test domain generalization.