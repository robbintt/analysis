---
ver: rpa2
title: Do LLMs Surpass Encoders for Biomedical NER?
arxiv_id: '2504.00664'
source_url: https://arxiv.org/abs/2504.00664
tags:
- llms
- entities
- entity
- dataset
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether decoder-based large language models
  (LLMs) can outperform encoder-based models for biomedical named entity recognition
  (NER), a key information extraction task. While encoder models like BERT are the
  current standard for NER due to their bidirectional context processing and per-token
  tagging capability, recent LLMs (Mistral-7B and Llama-3.1-8B) offer potential improvements
  through instruction finetuning, despite being computationally expensive.
---

# Do LLMs Surpass Encoders for Biomedical NER?

## Quick Facts
- arXiv ID: 2504.00664
- Source URL: https://arxiv.org/abs/2504.00664
- Reference count: 39
- Key outcome: Decoder-based LLMs outperform encoder models by 2-8% F1 for biomedical NER, especially on longer entities, but are 10-220× slower at inference

## Executive Summary
This study systematically compares decoder-based large language models (LLMs) against traditional encoder models for biomedical named entity recognition (NER). While encoder models like BERT have dominated NER due to their bidirectional context processing, the researchers investigate whether instruction-finetuned LLMs (Mistral-7B and Llama-3.1-8B) can provide superior performance. The study evaluates five biomedical datasets using identical BIO tagging schemes across both model families, revealing that LLMs achieve 2-8% higher F1 scores in most cases, with the advantage being most pronounced for entities of three or more tokens.

However, this performance gain comes at substantial computational cost. The LLMs are 10-220 times slower at inference and require more expensive hardware configurations. The researchers conclude that while LLMs demonstrate clear advantages for long-entity recognition, encoder models remain preferable when real-time feedback is needed or when performance differences are marginal. The study also identifies promising directions for future work, including hybrid architectures and knowledge distillation techniques that could combine the strengths of both approaches.

## Method Summary
The study evaluates five biomedical NER datasets (BC5CDR-disease, BC5CDR-chem, NCBI-disease, JNLPBA, and MedMentions) using three encoder models (BERT-large-uncased/cased, BiomedBERT, and DeBERTa-v3) and two decoder LLMs (Mistral-7B and Llama-3.1-8B). All models employ identical BIO tagging schemes for fair comparison. The researchers use two instruction-finetuning strategies: in-context learning with few-shot demonstrations and supervised finetuning on NER datasets. They analyze performance across entity lengths, finding that LLM advantages increase for longer entities (≥3 tokens). The study also quantifies inference speed differences, reporting that LLMs are 10-220× slower than encoder models depending on batch size and hardware configuration.

## Key Results
- LLMs achieved 2-8% higher F1 scores than encoder models across most biomedical NER datasets
- Performance advantage was most pronounced for longer entities (≥3 tokens), where LLMs showed greater robustness
- LLMs were 10-220 times slower at inference compared to encoder models, requiring more expensive hardware
- For shorter entities, encoder models maintained competitive performance, suggesting context-dependent model selection

## Why This Works (Mechanism)
Decoder-based LLMs leverage their autoregressive generation capabilities to better handle longer, more complex entity spans that require contextual understanding across extended text regions. The instruction finetuning enables these models to better interpret NER-specific prompts and generate coherent entity sequences. Encoder models, while bidirectional, struggle with longer entities due to their fixed-length context windows and token-level tagging approach. The study demonstrates that LLMs' generative nature provides superior handling of multi-token entities that require cross-token dependency resolution.

## Foundational Learning

### NER Tagging Schemes
- **Why needed**: Understanding how entities are labeled in sequence tagging (BIO format)
- **Quick check**: Can you explain the difference between B, I, and O tags in NER?

### Encoder vs Decoder Architectures
- **Why needed**: Knowing the fundamental architectural differences between bidirectional encoders and autoregressive decoders
- **Quick check**: Can you describe how BERT's attention mechanism differs from Mistral's?

### Instruction Finetuning
- **Why needed**: Understanding how LLMs are adapted for specific tasks through prompt-based learning
- **Quick check**: Can you explain few-shot vs supervised finetuning approaches?

### Biomedical Domain Adaptation
- **Why needed**: Recognizing how domain-specific pretraining affects model performance
- **Quick check**: Can you identify which models were biomedical-specific vs general-purpose?

### Entity Length Analysis
- **Why needed**: Understanding how entity complexity affects model performance
- **Quick check**: Can you explain why longer entities pose greater challenges for NER?

## Architecture Onboarding

### Component Map
- Input Text -> Tokenizer -> Encoder/Decoder Model -> BIO Tagging Output -> Evaluation Metrics

### Critical Path
The study focuses on comparing two model families: encoder models (BERT variants and DeBERTa-v3) that process text bidirectionally and output per-token classifications, versus decoder LLMs (Mistral-7B and Llama-3.1-8B) that generate sequences autoregressively using instruction finetuning.

### Design Tradeoffs
- Encoder models: Faster inference, lower computational cost, better for short entities
- Decoder LLMs: Superior long-entity handling, better contextual understanding, significantly slower
- Instruction finetuning: Improves LLM performance but adds training complexity
- Hardware requirements: Encoders work on standard GPUs, LLMs need high-end GPUs/TPUs

### Failure Signatures
- Encoder models fail on longer entities due to context window limitations
- LLMs fail on real-time applications due to slow inference speeds
- Both may struggle with out-of-distribution biomedical text not seen during training

### First Experiments
1. Replicate the encoder vs LLM comparison on a single dataset to verify performance differences
2. Test inference speed comparison on identical hardware configurations
3. Evaluate entity length breakdown to confirm LLM advantages on multi-token entities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can hybrid architectures (e.g., UL2 or GPT-BERT) combining encoder and decoder strengths outperform standalone models for biomedical NER?
- **Basis in paper**: The conclusion suggests "there could be a way to combine encoder models and LLMs in an ensemble setup for further gains."
- **Why unresolved**: The current study only compared isolated encoder and decoder architectures.
- **What evidence would resolve it**: Implementing and evaluating hybrid models on the five datasets to see if they exceed the 2-8% F1 gains of standalone LLMs.

### Open Question 2
- **Question**: Can knowledge distillation techniques like UniversalNER offer a computationally efficient alternative that retains LLM-level performance?
- **Basis in paper**: The "Caveats" section states future work should evaluate UniversalNER to see if it provides an efficient alternative to the tested LLMs.
- **Why unresolved**: The study did not assess distillation methods, focusing only on standard encoders and generative LLMs.
- **What evidence would resolve it**: Benchmarking distilled models against the reported encoders and LLMs on inference speed and F1 scores.

### Open Question 3
- **Question**: Do the performance trends regarding longer entities and inference trade-offs generalize to non-English biomedical datasets?
- **Basis in paper**: The authors explicitly state findings are limited to English and require validation for other languages.
- **Why unresolved**: No non-English datasets were included in the methodology.
- **What evidence would resolve it**: Replicating the study on non-English biomedical corpora to verify if LLMs still dominate on long entities.

## Limitations
- Study focused on only two LLM families (Mistral-7B and Llama-3.1-8B) without testing other competitive architectures
- Computational cost analysis lacked granularity regarding different hardware setups and batch sizes
- Limited scope of encoder models tested, excluding other competitive architectures like BioClinical ModernBERT
- No evaluation of out-of-distribution performance or robustness to domain shifts

## Confidence

| Claim | Confidence |
|-------|------------|
| LLMs outperform encoders by 2-8% F1 for biomedical NER | High |
| Performance advantage increases for longer entities | High |
| LLMs are 10-220× slower at inference | Medium |
| Findings generalize beyond tested datasets | Medium-Low |

## Next Checks
1. Replicate the experiments using BioClinical ModernBERT and GLiNER-BioMed encoder models to establish whether the performance gap persists across a broader range of state-of-the-art architectures
2. Conduct a more granular efficiency analysis testing inference speeds across different GPU/CPU configurations and batch sizes to better understand the practical deployment trade-offs
3. Evaluate model performance on out-of-distribution biomedical datasets and cross-domain medical text to assess robustness beyond the curated test sets used in this study