---
ver: rpa2
title: Fine-tuning Large Language Models for Improving Factuality in Legal Question
  Answering
arxiv_id: '2501.06521'
source_url: https://arxiv.org/abs/2501.06521
tags:
- legal
- statute
- llms
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LegalHalBench, a benchmark and evaluation
  framework for measuring hallucination in legal question answering (QA) by large
  language models (LLMs). It defines five hallucination types (e.g., incorrect law
  names, fabricated legal provisions) and proposes three automated metrics: Non-Hallucinated
  Statute Rate, Statute Relevance Rate, and Legal Claim Truthfulness.'
---

# Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering

## Quick Facts
- arXiv ID: 2501.06521
- Source URL: https://arxiv.org/abs/2501.06521
- Reference count: 26
- Primary result: HIPO fine-tuning achieved 38.353% non-hallucinated statute rate vs 6.541% baseline on GLM4-Chat-9B

## Executive Summary
This paper addresses hallucination in legal question answering by introducing LegalHalBench, a benchmark specifically designed to measure factual accuracy in legal contexts. The authors identify five types of legal hallucinations including incorrect law names and fabricated provisions, then propose a two-stage fine-tuning approach combining Supervised Fine-tuning (SFT) and Hard Sample-aware Iterative Direct Preference Optimization (HIPO) to reduce these errors. Experimental results demonstrate significant improvements in factuality metrics, with HIPO achieving 38.353% non-hallucinated statute rate compared to 6.541% for the baseline model.

## Method Summary
The authors developed a two-stage fine-tuning methodology for reducing hallucinations in legal question answering. First, they created LegalHalBench, a benchmark with five defined hallucination types and three automated evaluation metrics. They then constructed a high-quality dataset of 500 legal QA pairs annotated by law students. The fine-tuning approach begins with SFT to establish baseline legal reasoning capabilities, followed by HIPO which iteratively refines the model by focusing on hard samples that exhibit hallucination tendencies. This combined approach aims to improve both the accuracy and helpfulness of legal Q&A responses.

## Key Results
- HIPO achieved 38.353% non-hallucinated statute rate on GLM4-Chat-9B vs 6.541% baseline
- Significant improvements in helpfulness metrics across multiple evaluation dimensions
- Outperformed existing legal and general-purpose LLMs on LegalHalBench benchmark
- Ablation studies confirmed effectiveness of both SFT and HIPO components

## Why This Works (Mechanism)
The approach works by combining two complementary fine-tuning strategies. SFT provides foundational legal knowledge and reasoning patterns from high-quality annotated examples. HIPO then iteratively refines the model by specifically targeting hallucination-prone responses through direct preference optimization on hard samples. This dual approach addresses both the knowledge acquisition and the reasoning quality aspects of legal Q&A, with HIPO's iterative nature allowing the model to progressively reduce hallucination tendencies while maintaining helpfulness.

## Foundational Learning
- Legal domain knowledge requirements - Essential for understanding complex legal citations and provisions; quick check: can the model correctly identify statute names and relevant legal articles
- Hallucination types identification - Critical for targeted fine-tuning; quick check: model can distinguish between different hallucination categories in outputs
- Preference optimization techniques - Needed for iterative refinement; quick check: model shows improved performance on previously challenging samples

## Architecture Onboarding

**Component Map:**
LegalHalBench Benchmark -> SFT Fine-tuning -> HIPO Fine-tuning -> Improved Model

**Critical Path:**
Data collection/annotation -> Benchmark creation -> SFT training -> HIPO iterative refinement -> Evaluation

**Design Tradeoffs:**
- Dataset size (500 pairs) vs. quality trade-off in data curation
- Generalizability vs. benchmark-specific optimization
- Computational cost of iterative HIPO vs. performance gains

**Failure Signatures:**
- Overfitting to benchmark-specific patterns
- Loss of general reasoning capabilities
- Degradation in helpfulness metrics

**First Experiments:**
1. Evaluate model on out-of-distribution legal questions
2. Test cross-jurisdiction legal knowledge transfer
3. Conduct human evaluation of automated metric correlation

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark limited to Chinese legal texts, constraining cross-jurisdiction generalizability
- Automated metrics rely on rule-based matching that may miss semantic errors
- Small fine-tuning dataset (500 pairs) raises scalability concerns
- Performance improvements may be benchmark-specific rather than general factuality gains

## Confidence
- High confidence in methodology for creating LegalHalBench and general fine-tuning approach
- Medium confidence in reported performance improvements given limited dataset size
- Medium confidence in generalizability beyond Chinese legal domain

## Next Checks
1. Test LegalHalBench framework and fine-tuning approach on legal corpora from different jurisdictions (US, EU)
2. Conduct human evaluation studies to validate correlation between automated metrics and actual legal accuracy
3. Evaluate model performance on out-of-distribution legal questions not represented in fine-tuning dataset