---
ver: rpa2
title: 'LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic
  Writing Systems for NLP'
arxiv_id: '2408.04628'
source_url: https://arxiv.org/abs/2408.04628
tags:
- ancient
- languages
- translation
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of applying NLP to ancient
  logographic languages, where the majority of data is in visual forms and lacks transcription.
  The authors propose LogogramNLP, a benchmark with four ancient logographic writing
  systems and three tasks: machine translation, dependency parsing, and attribute
  classification.'
---

# LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP

## Quick Facts
- arXiv ID: 2408.04628
- Source URL: https://arxiv.org/abs/2408.04628
- Authors: Danlu Chen; Freda Shi; Aditi Agarwal; Jacobo Myerston; Taylor Berg-Kirkpatrick
- Reference count: 40
- Primary result: Visual representations outperform textual ones for machine translation tasks on ancient logographic languages, with pixel-based models achieving highest BLEU scores across all tested language pairs.

## Executive Summary
This paper introduces LogogramNLP, a benchmark for evaluating NLP on ancient logographic writing systems where most data exists in visual form without transcription. The authors compare visual encoding strategies (pixel-based models) against traditional textual encodings across three tasks: machine translation, dependency parsing, and attribute classification. Their key finding is that visual representations significantly outperform textual ones for machine translation, with a pixel-based model pre-trained on paired modern languages achieving the highest BLEU scores across Ancient Egyptian, Akkadian, and Old Chinese to English translations. This suggests visual processing pipelines may unlock large amounts of untranscribed cultural heritage data for NLP-based analysis.

## Method Summary
The authors created LogogramNLP with four ancient logographic writing systems and three tasks. For machine translation, they use PIXEL-MT (a ViT encoder with 6-layer Transformer decoder) pre-trained on TED59 parallel data, compared against textual models like BPE-MT, BERT-MT, and others. Classification uses PIXEL encoder with 2-layer MLP head, while dependency parsing uses PIXEL with deep bi-affine parser. Training uses Adam optimizer with early stopping after 10 intervals of no validation improvement. Hyperparameters vary by task: translation (bsz=64, 30k steps, lr=5e-4), classification (bsz=256, 30k steps, lr=5e-4), parsing (bsz=256, 1k steps, lr=8e-5).

## Key Results
- PIXEL-MT (pixel-based with parallel pre-training) achieves highest BLEU scores for all three translation tasks (Egyptian, Akkadian, Old Chinese to English)
- Visual representations outperform textual ones for machine translation, with textual models trained from scratch outperforming those with pre-training due to vocabulary mismatch
- PIXEL models converge faster than textual models for dependency parsing tasks
- Visual features work well for some classification tasks where visual cues are relevant (e.g., time/location classification), but textual latinized features sometimes outperform visual ones for other classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Disjoint Character Set Bypass via Visual Encoding
Visual representations outperform textual ones because they circumvent vocabulary mismatch between ancient scripts and pre-trained models. Pixel-based models encode text as images rather than discrete tokens, eliminating dependence on character-level vocabulary overlap with pre-training corpora. The transfer learning benefit from large-scale pre-training outweighs potential information loss from rendering text to pixels.

### Mechanism 2: Parallel Pre-training Signal Transfer
PIXEL-MT's superior performance stems from leveraging paired modern language pre-training (TED59), not just visual encoding alone. Cross-lingual translation knowledge transfers more effectively through shared visual-textual alignment space than through disjoint textual vocabularies. Visual representations create a modality-agnostic embedding space where translation patterns from modern languages can transfer to ancient scripts.

### Mechanism 3: Task-Specific Modality Advantages
Visual representations show stronger advantages for translation than for classification tasks due to different information requirements. Translation relies heavily on cross-lingual pattern transfer where visual encoding excels; classification tasks can leverage surface visual features (e.g., script style for dating) that textual encodings may not capture as directly.

## Foundational Learning

- **Logographic writing systems**: These systems (e.g., cuneiform, hieroglyphs) use symbols representing morphemes/words with large inventories, creating fundamental challenges for token-based NLP approaches designed for alphabetic scripts. Quick check: Can you explain why extending a BERT vocabulary for an ancient logographic script might fail even if you add all missing characters?

- **Cross-lingual transfer learning**: Ancient languages have extremely limited annotated data, making pre-training on high-resource languages essential but problematic due to symbol inventory mismatches. Quick check: How does mBERT's pre-training on Latin script languages affect its ability to process Cuneiform Unicode directly?

- **Pixel-based language modeling**: Understanding how PIXEL and similar models encode text as images using vision transformers enables comprehension of why they bypass vocabulary issues while potentially introducing new challenges. Quick check: What information might be lost when converting a cuneiform tablet's digital Unicode text into a rendered image for PIXEL processing?

## Architecture Onboarding

- **Component map**: Raw photographs -> Hand-copied lineart -> Digitally rendered text -> Latin transliteration -> Unicode text; PIXEL (ViT-MAE), mBERT, CANINE, ByT5, ResNet-50 (full-document); GPT-2 decoder (MT), MLP classifier, Biaffine parser (dependencies); Common Crawl (MLM), TED59 parallel data (MT)

- **Critical path**: Start with lineart images -> PIXEL encoder -> task-specific head; avoid Unicode->Latin transliteration pipeline which introduces noise and information loss.

- **Design tradeoffs**: Visual vs. textual (visual bypasses vocabulary issues but requires more compute; textual enables interpretability but struggles with disjoint character sets); Pre-training strategy (parallel text pre-training outperforms MLM pre-training for translation but requires aligned data); Input quality (hand-copied lineart dramatically outperforms raw photographs for Chinese translationâ€”visual quality matters).

- **Failure signatures**: Textual models with pre-training underperform training from scratch -> indicates vocabulary mismatch problem; Repeated formatting patterns in predictions (e.g., "Confucius said:" repetition) -> suggests model not capturing semantics; Classification F1=0 with extended vocabulary -> character set completely disjoint from pre-training corpus.

- **First 3 experiments**: 1) Baseline comparison: Run PIXEL-MT vs. BPE-MT on Akkadian-English translation with identical hyperparameters to quantify visual advantage magnitude. 2) Ablation study on visual quality: Compare hand-copied vs. photograph vs. digital rendering on Old Chinese translation to measure preprocessing impact. 3) Pre-training domain transfer: Test if PIXEL-MT fine-tuned on Akkadian can transfer to Egyptian hieroglyphs with minimal data, probing cross-script generalization.

## Open Questions the Paper Calls Out

- The authors explicitly state that Mayan hieroglyphs and Oracle Bone script were omitted from the current benchmark due to collection processing status and copyright issues, respectively, raising questions about generalizability to other complex logographic writing systems.

- The paper notes they used digitally rendered text as a surrogate for Akkadian visual features and wish to conduct "apples-to-apples comparisons" once line segmentation annotations are available, leaving open whether visual NLP models can effectively process full-document artifact photographs without pre-segmentation.

- While the paper demonstrates visual advantages for translation and parsing, the classification results are mixed (visual sometimes better, sometimes worse), and the paper doesn't provide a unified theoretical framework for when visual representations should be preferred over textual ones.

## Limitations

- The stark performance differences between visual input quality (hand-copied lineart achieving BLEU 5.45 vs. raw photographs at 2.09 for Old Chinese) suggest preprocessing methodology critically affects results, which the paper doesn't fully characterize.

- While the paper mentions extending textual model vocabularies with all characters from respective scripts, it doesn't specify the exact implementation or whether this was successful across all tasks, with classification F1=0 suggesting complete vocabulary mismatch in some cases.

- The PIXEL-MT model's superior performance relies on TED59 parallel data pre-training, but the paper doesn't analyze whether modern language transfer is truly optimal for ancient scripts or if domain-specific pre-training might yield better results.

## Confidence

- **High confidence**: The fundamental finding that visual representations outperform textual ones for machine translation tasks is well-supported by systematic experiments across three language pairs, with the mechanism of vocabulary mismatch causing textual model failures clearly demonstrated.

- **Medium confidence**: The PIXEL-MT model's superiority over other approaches is demonstrated but the specific contribution of parallel pre-training vs. visual encoding alone is not fully isolated, as the paper shows PIXEL-MT > PIXEL+GPT2 without comparing to PIXEL with task-specific pre-training.

- **Low confidence**: Cross-task generalization claims are weakly supported, as while the paper shows visual advantages for translation and parsing, the classification results are mixed and the paper doesn't provide a unified theoretical framework for when visual representations should be preferred.

## Next Checks

1. **Visual quality ablation**: Systematically compare hand-copied lineart, digitally rendered Unicode, and raw photograph inputs across all three tasks and language pairs to quantify preprocessing impact on the visual vs. textual comparison.

2. **Cross-script transfer analysis**: Test whether PIXEL-MT fine-tuned on Akkadian can transfer to Egyptian hieroglyphs with minimal data, and whether this transfer is better or worse than using a PIXEL model pre-trained on Egyptian-specific data.

3. **Domain-specific pre-training**: Compare PIXEL-MT against a PIXEL model pre-trained on ancient-language-adjacent corpora (e.g., classical texts, philosophical works) to determine if modern parallel text pre-training is optimal for ancient logographic translation.