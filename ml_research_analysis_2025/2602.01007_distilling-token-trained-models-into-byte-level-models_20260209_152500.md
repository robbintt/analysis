---
ver: rpa2
title: Distilling Token-Trained Models into Byte-Level Models
arxiv_id: '2602.01007'
source_url: https://arxiv.org/abs/2602.01007
tags:
- teacher
- distillation
- byte-level
- stage
- byte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage distillation framework that converts
  token-trained large language models into byte-level models while preserving most
  of their capabilities. The approach addresses the challenge of mismatched boundaries
  between token-based and byte-based representations through progressive knowledge
  distillation and byte-level supervised fine-tuning.
---

# Distilling Token-Trained Models into Byte-Level Models

## Quick Facts
- arXiv ID: 2602.01007
- Source URL: https://arxiv.org/abs/2602.01007
- Reference count: 29
- Primary result: Converts token-trained LLMs to byte-level models while retaining over 92% of original performance using only 125B training bytes

## Executive Summary
This paper presents a two-stage distillation framework that converts token-trained large language models into byte-level models while preserving most of their capabilities. The approach addresses the challenge of mismatched boundaries between token-based and byte-based representations through progressive knowledge distillation and byte-level supervised fine-tuning. Across multiple model families including Llama, Qwen, and OLMo, the distilled byte-level models retain over 92% of the original performance using only approximately 125 billion training bytes, significantly reducing resource requirements compared to training byte-level models from scratch.

## Method Summary
The framework uses an H-Net architecture with a Mamba2 encoder to compress raw byte sequences into latent states, which are then processed by a core transformer initialized from teacher weights. Stage 1 progressively distills capabilities through three sequential steps: (1) embedding alignment between byte-patch representations and teacher token embeddings using MSE loss, (2) joint distillation using KL divergence on output distributions, and (3) boundary learning with frozen backbone using binary cross-entropy. Stage 2 adapts the model for byte-level generation through head adaptation and end-to-end fine-tuning on next-byte prediction. The entire process uses 125B bytes of training data across both stages.

## Key Results
- Distilled byte-level models retain over 92% of teacher model performance
- Framework achieves high fidelity (70.2 MMLU) with Mamba2 encoder, while transformer variants show performance decline (31.3 MMLU)
- Joint Boundary Prediction (JBP) decoding outperforms Multi-Byte Prediction (MBP) on reasoning tasks by ~16 points (GSM8K: 53.5% vs 37.4%)
- Byte-level models demonstrate improved robustness to whitespace and spelling variations compared to token-level models

## Why This Works (Mechanism)

### Mechanism 1: Embedding Alignment as a Semantic Bridge
Aligning byte-patch representations to teacher token embeddings before capability transfer prevents representational collapse. The encoder learns to map byte spans to the teacher's static embedding space via MSE loss, establishing a shared semantic foundation that allows the core transformer to interpret byte-derived representations as if they were token embeddings.

### Mechanism 2: Sequential Curriculum Avoids Objective Competition
Decomposing distillation into sequential stages (alignment → joint distillation → boundary learning) outperforms joint multi-objective optimization. Each stage trains on stable representations from the previous stage, preventing gradient interference between routing and backbone modules.

### Mechanism 3: Mamba2 Encoder Captures Long-Range Byte Dependencies
State Space Models (Mamba2) outperform attention-based encoders for byte-sequence compression. Raw byte sequences are 4-5× longer than token sequences, and Mamba2's recurrent structure compresses local byte patterns into meaningful latent states without quadratic complexity.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)** - Why needed: The entire framework transfers capabilities from teacher to student. Quick check: Can you explain why matching output distributions (KL divergence) transfers more information than matching hard labels?
- **Concept: Subword Tokenization and Its Biases** - Why needed: Understanding BPE/WordPiece helps contextualize why byte-level modeling is desirable. Quick check: What happens to a standard tokenizer when encountering "AntSpeak" text (e.g., "t h e _ c a t")?
- **Concept: Hierarchical Sequence Modeling** - Why needed: H-Net uses an encoder-compressor-decoder structure. Understanding how representations flow across resolutions is essential for debugging alignment issues. Quick check: Why does naive broadcasting of chunk representations to all byte positions violate autoregressive causality?

## Architecture Onboarding

- **Component map:** Raw Bytes → [Encoder (Mamba2 + FFN)] → Dynamic Chunking via Router → [Core Transformer (from teacher)] → [Dechunk Module] → [Local Decoder (Mamba2 + FFN)] → Byte Predictions
- **Critical path:** 1. Stage 1.1: Train encoder with $L_{align}$ only. 2. Stage 1.2: Add $L_{distill}$ with frozen router. 3. Stage 1.3: Freeze backbone, train router with $L_{boundary}$. 4. Stage 2.1: Replace token head with Dechunk+Decoder, train new modules only. 5. Stage 2.2: Unfreeze all, end-to-end next-byte prediction.
- **Design tradeoffs:** JBP vs MBP decoding: JBP is more robust for reasoning chains (GSM8K: 53.5% vs 37.4%). Integrated vs Split Router: Decoupled routing loses 19+ points on LMB. Teacher scale: Larger teachers do not help; similar-scale distillation aligns latent spaces better.
- **Failure signatures:** MMLU collapse to ~30%: Missing embedding alignment. Catastrophic forgetting on clean text: Boundary predictor overfitted to whitespace patterns. AntSpeak accuracy stagnation at ~46%: Using attention-based encoder instead of Mamba2.
- **First 3 experiments:** 1. Train Stage 1.1 (embedding alignment only) on 5B bytes; verify PIQA reaches ~75%. 2. Skip Stage 1.1, go directly to joint distillation on 20B bytes; confirm catastrophic drop (LMB ~31%). 3. After Stage 2, evaluate JBP vs MBP on GSM8K; expect ~16-point gap favoring JBP.

## Open Questions the Paper Calls Out

### Open Question 1
Can the "Space Bias" in the boundary predictor be mitigated without compromising performance on clean text? The authors identify that optimizing for noise invariance disrupts standard linguistic boundary detection, but do not propose a solution that satisfies both constraints simultaneously.

### Open Question 2
Why does the intrinsic byte-level resilience degrade during Stage 2 fine-tuning, and can this behavior be preserved? The paper quantifies the degradation but does not isolate the specific mechanism in the SFT or Dechunking process that erodes the robustness established in Stage 1.

### Open Question 3
Does the negative impact of larger teacher models on distillation performance persist across heterogeneous model families? The authors restrict analysis to specific families (Llama, Qwen) rather than testing cross-architecture distillation to determine if this is a universal property.

## Limitations

- **Limited architectural generality:** Evaluated only on encoder-decoder architectures with Mamba2 encoders; performance on decoder-only architectures or alternative encoder designs remains unknown.
- **Dataset specificity concerns:** Byte-level alignment patterns may be dataset-dependent; framework not tested on specialized domains like code or multilingual text.
- **Boundary learning brittleness:** Sequential curriculum may indicate fundamental incompatibility rather than a solvable design choice.

## Confidence

- **High Confidence:** Sequential curriculum improves stability over joint optimization; Mamba2 encoder outperforms attention-based encoders; JBP decoding outperforms MBP on reasoning tasks.
- **Medium Confidence:** Byte-level models retain >92% of teacher performance; 125B bytes sufficient for effective distillation; similar-scale distillation performs better.
- **Low Confidence:** Framework generalizes across all model families; computational efficiency claims without FLOPs comparisons.

## Next Checks

**Check 1: Ablation of Curriculum Order** - Reverse the sequential curriculum or test joint training of all objectives to validate the curriculum's importance.

**Check 2: Encoder Architecture Scaling** - Replace Mamba2 with a 4-layer Transformer encoder to confirm the encoder choice is fundamental to byte-level representation learning.

**Check 3: Cross-Dataset Generalization** - Train on FineWeb for Stage 1, then continue on CodeNet for Stage 2 to test whether byte-level representations generalize across domains.