---
ver: rpa2
title: 'Heads or Tails: A Simple Example of Causal Abstractive Simulation'
arxiv_id: '2509.01136'
source_url: https://arxiv.org/abs/2509.01136
tags:
- causal
- coin
- language
- observer
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This note introduces causal abstractive simulation, a framework\
  \ that formalizes language model simulation by combining causal abstraction with\
  \ observer-relative simulation. The method defines simulation as a commutativity\
  \ condition between an observer\u2019s causal model of a referent and a simulator\u2019\
  s behavior under observer-defined state mappings."
---

# Heads or Tails: A Simple Example of Causal Abstractive Simulation

## Quick Facts
- **arXiv ID**: 2509.01136
- **Source URL**: https://arxiv.org/abs/2509.01136
- **Reference count**: 9
- **Primary result**: Language models can successfully simulate referents when their probability distributions, sampling strategies, and output mappings align with observer expectations.

## Executive Summary
This note introduces causal abstractive simulation, a framework that formalizes language model simulation by combining causal abstraction with observer-relative simulation. The method defines simulation as a commutativity condition between an observer's causal model of a referent and a simulator's behavior under observer-defined state mappings. Applied to simulating a fair coin toss, the framework shows how simulation can fail due to sampling strategy, probability distribution bias, or mismatched observer expectations, and succeeds when these elements align. A successful case demonstrates that a language model with unbiased probabilities and top-2 sampling satisfies the causal abstractive simulation criterion for a given observer. The approach provides a formal basis for assessing language model simulation quality and connects ad-hoc benchmarking to causal theory.

## Method Summary
The framework defines simulation through a commutativity condition: an observer's causal model of a referent must produce the same distribution as the simulator's output distribution mapped through the observer's interpretation function. The method involves three components: (1) defining the observer's causal model with probability distributions over contexts, (2) characterizing the simulator's probabilistic behavior, and (3) computing and comparing both sides of the commutativity equation. The paper applies this to a fair coin toss, demonstrating failure modes with greedy sampling and biased distributions, and success with top-2 sampling and unbiased probabilities.

## Key Results
- Causal abstractive simulation provides a formal definition of when a language model successfully simulates a referent system
- Sampling strategy critically determines whether a language model's probability distribution is faithfully expressed (greedy sampling fails, top-2 succeeds)
- Simulation failure can arise from vocabulary mismatches between simulator outputs and observer interpretation mappings
- The framework successfully characterizes a simple simulation case where a language model with unbiased probabilities and top-2 sampling simulates a fair coin toss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulation succeeds when the observer's causal model of the referent commutes with the simulator's behavior under observer-defined state mappings.
- Mechanism: The formal condition ğ‘€_ğ‘‚(Pr_ğ‘‚) = Ï„_ğ‘‚(ğ‘€_ğ¿(Pr_ğ¿)) requires that (1) the observer's causal model produces a distribution over outcomes, (2) the simulator produces a distribution over outputs given inputs, and (3) the observer's mapping function Ï„ translates simulator outputs to referent states. When these compose identically to the observer's direct model, simulation holds.
- Core assumption: Observers represent referents using causal models with well-defined structural equations and probability distributions over contexts.
- Evidence anchors:
  - [abstract]: "simulation as a commutativity condition between an observer's causal model of a referent and a simulator's behavior under observer-defined state mappings"
  - [Definition 6, Page 5]: Formalizes the equality condition between mapped distributions
  - [corpus]: "What is causal about causal models and representations?" discusses intervention-to-action correspondence, relevant but not directly validating the commutativity criterion
- Break condition: If the observer's Ï„ mapping cannot interpret simulator outputs, or if simulator probability distributions diverge from observer expectations, commutativity fails.

### Mechanism 2
- Claim: Sampling strategy determines whether a language model's underlying probability distribution is faithfully expressed in outputs.
- Mechanism: Greedy sampling (argmax) collapses probability distributions to single tokens, amplifying small biases. Top-k sampling preserves relative probabilities among top candidates, allowing unbiased distributions to produce unbiased outputs.
- Core assumption: The language model's conditional probability distribution accurately reflects the target behavior when sampled appropriately.
- Evidence anchors:
  - [Example 1, Pages 7-8]: P_1 with 0.51/0.49 bias produces "Heads" 100% of the time under greedy sampling
  - [footnote 2, Page 8]: Empirical confirmation showing total variation distance of 0.500 (greedy) vs 0.0106 (top-2)
  - [corpus]: Weak direct evidence; corpus papers focus on causal structure rather than sampling
- Break condition: If sampling strategy is deterministic or overly constraining (e.g., greedy, low temperature), even well-calibrated probability distributions fail to simulate stochastic referents.

### Mechanism 3
- Claim: Simulation failure can originate from misalignment between observer expectations and simulator output vocabulary.
- Mechanism: The observer's Ï„ function maps specific tokens to referent states. If the simulator produces tokens outside this mapping (e.g., "H"/"T" instead of "Heads"/"Tails"), simulation fails regardless of correct probabilities.
- Core assumption: Observers have fixed interpretive schemes that may not adapt to simulator output variations.
- Evidence anchors:
  - [Example 3, Pages 8-9]: LM3.2 produces "H"/"T" with correct 0.5/0.5 distribution but fails because Ï„ only maps "Heads"/"Tails"
  - [Equation 20, Page 9]: Alternative Ï„â€² mapping would resolve the failure
  - [corpus]: No direct corpus support for observer-token alignment failures
- Break condition: If simulator outputs are not in the pre-image of Ï„, or if observer cannot/will not update Ï„, simulation fails.

## Foundational Learning

- Concept: **Causal Models (Structural Causal Models)**
  - Why needed here: The entire framework represents referents, observers, and simulators as causal models with exogenous variables (inputs), endogenous variables (outputs), structural equations (deterministic relationships), and interventions.
  - Quick check question: Given a causal model with exogenous variable S âˆˆ {A, B} and structural equation F = {Aâ†’X, Bâ†’Y}, what is the distribution over endogenous variables if Pr(S=A) = 0.3?

- Concept: **Top-k Sampling**
  - Why needed here: The paper demonstrates that sampling strategy is criticalâ€”greedy sampling fails while top-2 sampling succeeds for stochastic simulation.
  - Quick check question: In top-2 sampling with probabilities pâ‚=0.6 and pâ‚‚=0.4, what is the probability of selecting the second token after normalization?

- Concept: **Probabilistic Abstraction/Commutativity**
  - Why needed here: The core definition requires that two pathways (direct model vs. simulated-then-mapped) produce identical distributionsâ€”this is a commutativity condition from category theory/causal abstraction literature.
  - Quick check question: If M_ğ‘‚(Pr_ğ‘‚) = {H: 0.5, T: 0.5} and Ï„ maps simulator outputs {Heads, Tails} to {H, T}, what must M_ğ¿(Pr_ğ¿) equal for simulation to hold?

## Architecture Onboarding

- Component map:
  - Referent (R) -> Observer (O) -> Simulator (L)
  - Observer's causal model M_O -> Observer's probability Pr_O -> Observer's intervention distribution -> Observer's input mapping Pr_{ğ’°ğ¿|ğ’°ğ‘‚,â„ğ‘‚} -> Observer's output mapping Ï„_O
  - Simulator's probabilistic causal model M_L with partitioned exogenous variables (ğ’°_ğ‘‚^L: observer-settable, ğ’°_ğ¼^L: internal randomness) -> Autoregressive token generation

- Critical path:
  1. Define observer's causal model M_ğ‘‚ (variables, structural equations, probability over contexts)
  2. Define observer's mappings: how they prompt the simulator (Pr_{ğ’°ğ¿|ğ’°ğ‘‚,â„ğ‘‚}) and interpret outputs (Ï„_ğ‘‚)
  3. Characterize simulator's probability distribution over relevant outputs
  4. Compute both sides of the commutativity equation; measure distance between distributions

- Design tradeoffs:
  - Exact vs. approximate simulation: Definition 7 allows Îµ-tolerance for practical applications
  - Observer simplicity vs. coverage: A limited Ï„ mapping (e.g., only "Heads"/"Tails") restricts which simulators can succeed
  - Single-turn vs. multi-turn: Appendix 10.1 extends to multi-turn but requires decomposition into single-turn assessments

- Failure signatures:
  - **Sampling failure**: Output distribution is degenerate (e.g., always same token) despite reasonable underlying probabilities â†’ check sampling strategy
  - **Probability bias**: Outputs systematically favor one outcome â†’ examine P(w_{l+1}|prompt) conditionals
  - **Vocabulary mismatch**: Valid outputs not mapped by Ï„ â†’ expand Ï„ or constrain simulator vocabulary
  - **Prompt distribution mismatch**: Observer's prompts don't adequately cover referent states â†’ expand Pr_{ğ’°ğ¿|ğ’°ğ‘‚,â„ğ‘‚}

- First 3 experiments:
  1. **Validate sampling strategy**: Take a language model with known near-uniform distribution over two tokens (Pâ‰ˆ0.5/0.5); compare greedy vs. top-2 sampling outputs; measure total variation distance from target {0.5, 0.5} distribution.
  2. **Test vocabulary alignment**: Prompt a language model to simulate a coin; measure distribution over all generated tokens; identify whether tokens match observer's Ï„ mapping; compute simulation success rate under different Ï„ definitions.
  3. **Probe prompt sensitivity**: For the same referent (fair coin), define multiple observer prompt distributions; measure how Pr_{ğ’°ğ¿|ğ’°ğ‘‚,â„ğ‘‚} affects the marginal Pr_{ğ’°_ğ‘‚^L} and resulting output distributions; identify prompt strategies that minimize distribution distance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Causal Abstractive Simulation Hypothesis (CASH) provide a valid characterization of simulation, and what are its formal connections to established causal abstraction theory?
- Basis in paper: [explicit] "The point of this note is not to argue for this hypothesis, but to illustrate how it can be operationalized. Arguments for the CASH and its connections to the causal abstraction literature will appear in future work."
- Why unresolved: The author presents CASH as a proposition without defending its validity or establishing formal relationships to prior causal abstraction work.
- What evidence would resolve it: Formal proofs connecting CASH to Beckers & Halpern's causal abstraction framework, plus empirical validation across diverse simulation tasks.

### Open Question 2
- Question: How should the framework be extended to handle interventions, which the paper explicitly defers?
- Basis in paper: [explicit] "This note is long enough without considering interventions in detail. For the rest of this note, we will assume that the observer chooses not to intervene."
- Why unresolved: The framework defines intervention distributions Pr_{I_O|U_O} but only considers the null intervention case, leaving active intervention assessment unexplored.
- What evidence would resolve it: Worked examples where observers perform do-interventions on the referent model, with corresponding mappings to simulator inputs and formal analysis of commutativity conditions.

### Open Question 3
- Question: Can the proposed decomposition approach for multi-turn interaction preserve valid simulation assessment across dialogue trajectories?
- Basis in paper: [explicit] The appendix proposes decomposing multi-turn interactions into sequential single-turn assessments but notes this is "seemingly reasonable" without validation.
- Why unresolved: The decomposition is sketched but not formalized or tested; it remains unclear whether single-turn quality metrics aggregate meaningfully.
- What evidence would resolve it: Formal extension of Definition 6 to multi-turn settings, with empirical tests showing whether aggregated single-turn assessments predict overall simulation quality.

## Limitations

- The framework assumes observers possess complete and correct causal models of referents, but in practice, observer knowledge is typically partial and subject to revision.
- The framework focuses on single-turn simulations but extends to multi-turn only through decomposition, potentially missing emergent dynamics in extended interactions.
- The choice of simulation success threshold Îµ remains arbitrary without principled guidance from the theory.

## Confidence

- **High Confidence**: The mechanism linking sampling strategy to output distribution fidelity is well-supported by the empirical comparison between greedy (TVD = 0.500) and top-2 (TVD = 0.0106) sampling in Example 1. The mathematical formalism for causal abstractive simulation is internally consistent and builds appropriately on established causal abstraction theory.
- **Medium Confidence**: The vocabulary alignment failure mode is clearly demonstrated in Example 3, but real-world applications would likely involve more complex token mapping scenarios. The assumption that observers can perfectly specify their causal models of referents is reasonable for simple systems like coin flips but becomes problematic for complex referents.
- **Low Confidence**: The paper's empirical validation is limited to theoretical examples without testing on actual language models beyond the cited TVD measurements. The framework's applicability to high-stakes domains (e.g., medical diagnosis simulation) remains unexplored, and the consequences of partial simulation success are not characterized.

## Next Checks

1. **Empirical validation with real language models**: Select multiple language models (GPT-4, Claude, Llama) and systematically test their simulation performance across the three mechanisms: sampling strategy variation, vocabulary alignment, and prompt sensitivity. Measure TVD distributions across models and identify which factors most strongly predict simulation success.

2. **Stress test observer model specification**: For increasingly complex referents (from coin flip to multi-state systems), measure how uncertainties in observer causal models affect simulation commutativity. Quantify the relationship between observer model fidelity and simulation accuracy, identifying failure thresholds.

3. **Multi-turn simulation dynamics**: Extend the single-turn validation to sequences of interactions, measuring whether decomposition into single-turn assessments captures emergent phenomena. Test scenarios where multi-turn dependencies (e.g., state persistence) are critical to successful simulation.