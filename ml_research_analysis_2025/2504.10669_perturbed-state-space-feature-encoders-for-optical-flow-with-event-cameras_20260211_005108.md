---
ver: rpa2
title: Perturbed State Space Feature Encoders for Optical Flow with Event Cameras
arxiv_id: '2504.10669'
source_url: https://arxiv.org/abs/2504.10669
tags:
- flow
- optical
- state
- event
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optical flow estimation for event cameras,
  which offer high temporal resolution but suffer from temporal and spatial reasoning
  limitations in existing neural networks. The proposed Perturbed State Space Feature
  Encoders (P-SSE) adaptively process spatiotemporal features with a large receptive
  field while maintaining linear computational complexity.
---

# Perturbed State Space Feature Encoders for Optical Flow with Event Cameras

## Quick Facts
- **arXiv ID**: 2504.10669
- **Source URL**: https://arxiv.org/abs/2504.10669
- **Reference count**: 40
- **Primary result**: P-SSE improves optical flow estimation on event camera data by 8.48% (DSEC-Flow) and 11.86% (MVSEC) in EPE performance

## Executive Summary
This paper introduces Perturbed State Space Feature Encoders (P-SSE) to address the challenge of optical flow estimation from event camera data. Event cameras offer high temporal resolution but pose difficulties for neural networks in reasoning about long-range temporal and spatial relationships. The proposed method leverages State Space Models (SSMs) with a novel perturbation technique applied to the state dynamics matrix, enabling adaptive processing of spatiotemporal features while maintaining linear computational complexity. When integrated into an optical flow framework with bi-directional correlation volumes and temporal propagation, P-SSE significantly outperforms existing methods on standard benchmarks.

## Method Summary
The method processes event camera streams by first converting raw asynchronous events into image-like representations (ERGO-12 or time surfaces). These representations are then encoded using P-SSE, which consists of ViT-like SSM blocks with a novel Perturb-Then-Diagonalize (PTD) initialization technique. The PTD pre-trains the state matrix on ImageNet with a perturbation matrix to ensure training stability. The encoded features are processed through an E-TROF module that builds bidirectional correlation volumes and iteratively refines flow estimates. An E-MOP module connects multiple E-TROF modules in sequence, propagating motion state features across five frames to expand temporal context. The system is trained end-to-end with L1 loss for 250k iterations using AdamW optimizer.

## Key Results
- Achieves 8.48% improvement in EPE performance on DSEC-Flow dataset compared to previous state-of-the-art
- Demonstrates 11.86% improvement in EPE performance on MVSEC dataset in zero-shot evaluation
- Shows superior performance across multiple metrics including Angular Error and N-pixel Error
- Validates the effectiveness of PTD initialization through ablation studies

## Why This Works (Mechanism)
The core innovation lies in stabilizing the State Space Model through perturbation of the state dynamics matrix. By pre-training the state matrix A with a perturbation matrix E on ImageNet, the PTD technique creates a more robust state representation that can handle the noise and sparsity inherent in event camera data. This perturbed initialization ensures stable training while maintaining the SSM's ability to capture long-range temporal dependencies with linear computational complexity. The perturbation technique allows the model to maintain global receptive fields comparable to Transformers while avoiding their quadratic computational cost.

## Foundational Learning
- **Concept**: State Space Models (SSMs) and Discretization
  - Why needed here: P-SSE is built on a discretized SSM. Understanding how a continuous-time linear system is converted to a discrete one via Zero-Order Hold (ZOH) is fundamental to understanding the encoder's operation.
  - Quick check question: Given a continuous state equation $h'(t) = Ah(t) + Bx(t)$, how does the discretized form $h_k = \bar{A}h_{k-1} + \bar{B}x_k$ approximate the continuous evolution using a step size $\Delta$?

- **Concept**: Event Camera Data Representation
  - Why needed here: Raw events must be converted into a format suitable for neural networks. The paper uses representations like ERGO-12 and time surfaces. Understanding that these are "image-like" tensors aggregating events over a time window is crucial for the model's input stage.
  - Quick check question: If you have a stream of asynchronous events $(x, y, t, p)$, how would you create a voxel grid representation with $C$ bins over a fixed time duration?

- **Concept**: Optical Flow Estimation and Correlation Volumes
  - Why needed here: The architecture predicts flow by iteratively refining estimates based on correlation volumes. Grasping that a correlation volume encodes pixel-wise similarity between feature maps of different time steps is key to understanding the iterative refinement process.
  - Quick check question: In an optical flow model like RAFT, what does a 4D correlation volume represent, and how is it used to look up matching costs for a current flow hypothesis?

## Architecture Onboarding

- **Component map**: Raw Events -> Voxel Representation -> P-SSE Encoder -> Feature Embeddings -> Correlation Volume -> Iterative Refinement (E-TROF) -> Temporal Propagation (E-MOP) -> Final Flow
- **Critical path**: The most critical and novel component is the **P-SSE Encoder** with its PTD initialization, which directly impacts feature quality
- **Design tradeoffs**:
  - Temporal Context (E-MOP) vs. Computational Cost: Extending from 3 to 5 frames with motion propagation increases accuracy but adds computational overhead
  - SSM vs. Transformer Encoder: The paper claims P-SSE offers a global receptive field like Transformers but with O(L) complexity. The tradeoff is potential representational power versus significant efficiency gains
  - Perturbation Magnitude: The perturbation matrix E is constrained to ~10% of A's magnitude. This is a hyperparameter; larger values could destabilize, smaller ones might be ineffective
- **Failure signatures**:
  - Instability/Non-convergence during training: If the PTD pre-training is skipped or the perturbation is not applied correctly
  - Poor performance on sparse event data: The PTD technique is said to make the state matrix more robust to noise
  - High memory usage for long sequences: Although SSMs are O(L), long sequences of high-resolution events can still be demanding
- **First 3 experiments**:
  1. Ablation on PTD: Train the model with PTD pre-training and with standard HiPPO initialization. Compare final EPE and training stability to isolate the contribution of the proposed perturbation technique
  2. Encoder Architecture Comparison: Replace the P-SSE encoder with a standard CNN or Transformer encoder of comparable parameter count while keeping the rest of the architecture fixed. Compare accuracy and inference time to validate the P-SSE encoder's efficiency-accuracy tradeoff
  3. Temporal Context Length Analysis: Vary the number of frames processed by the E-MOP module (e.g., 3, 5, 7 frames) and measure performance on the validation set to determine the optimal temporal window

## Open Questions the Paper Calls Out
- **Open Question 1**: Is the pre-training of the perturbation matrix on ImageNet strictly necessary for P-SSE stability, or can the model achieve comparable performance when trained directly on event data?
- **Open Question 2**: Does the constraint of setting the perturbation magnitude to approximately 10% of the state matrix magnitude generalize well to different state dimensions or varying event densities?
- **Open Question 3**: Can P-SSE maintain its performance advantage and stability when scaling the temporal context beyond the current implementation of five consecutive event representations?
- **Open Question 4**: Is the performance of P-SSE dependent on the specific grid-based structure of ERGO-12, or does the encoder generalize effectively to other sparse or continuous event representations?

## Limitations
- The ImageNet pre-training procedure for the state matrix lacks critical hyperparameters (epochs, Î» value)
- The perturbation magnitude constraint (~10%) appears empirically derived rather than theoretically justified
- Computational complexity claims depend on implementation details not fully specified in the paper

## Confidence
- **High Confidence**: The core SSM discretization framework and PTD initialization technique are well-grounded in prior work and mathematically rigorous
- **Medium Confidence**: The claimed efficiency advantage over Transformers depends on implementation details not fully specified
- **Low Confidence**: Generalization of PTD beyond optical flow applications remains untested

## Next Checks
1. **Architecture Ablation**: Systematically vary P-SSE depth and width while measuring both accuracy and runtime to establish the true efficiency-accuracy Pareto frontier
2. **Cross-Dataset Robustness**: Evaluate the model's performance when trained on MVSEC and tested on DSEC-Flow to assess domain shift robustness
3. **Perturbation Sensitivity Analysis**: Conduct a grid search over perturbation magnitudes (0%, 5%, 10%, 15% of A) to determine the optimal trade-off between stability and representational capacity