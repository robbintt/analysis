---
ver: rpa2
title: 'ElectriQ: A Benchmark for Assessing the Response Capability of Large Language
  Models in Power Marketing'
arxiv_id: '2507.22911'
source_url: https://arxiv.org/abs/2507.22911
tags:
- power
- electriq
- dialogue
- energy
- seek-rag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ElectriQ is a large-scale benchmark and evaluation framework for
  large language models (LLMs) in electric power marketing (EPM). The dataset contains
  over 550,000 multi-turn dialogues across six service domains and 24 sub-scenarios,
  including sustainability-critical areas like time-of-use (TOU) tariffs, demand response
  (DR), and distributed energy resource (DER) interconnection.
---

# ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing

## Quick Facts
- **arXiv ID:** 2507.22911
- **Source URL:** https://arxiv.org/abs/2507.22911
- **Reference count:** 6
- **Key outcome:** ElectriQ is a large-scale benchmark and evaluation framework for large language models (LLMs) in electric power marketing (EPM).

## Executive Summary
ElectriQ is a large-scale benchmark and evaluation framework for assessing large language models in electric power marketing. The dataset contains over 550,000 multi-turn dialogues across six service domains and 24 sub-scenarios, including critical areas like time-of-use tariffs and demand response. A unified protocol combines human ratings on four dimensions with automatic metrics and two compliance stress tests. Experiments show that domain-aligned 7B models with the proposed SEEK-RAG method match or surpass much larger models in EPM-specific performance while reducing computational cost.

## Method Summary
The ElectriQ framework involves constructing a comprehensive EPM dialogue corpus through transcription of call recordings, knowledge base ingestion, and synthetic dialogue generation. Models are trained using a two-stage process: first with domain knowledge infusion via QLoRA fine-tuning on real and synthetic dialogues, then with human preference alignment using Direct Preference Optimization on a high-quality subset. The SEEK-RAG method employs scene classification to filter retrieval by service scenario before querying the knowledge base. Evaluation uses human ratings across four dimensions plus automatic metrics and stress tests for statutory citation correctness and long-dialogue consistency.

## Key Results
- Domain-aligned 7B models with SEEK-RAG match or exceed 100B+ parameter models in EPM-specific performance
- SEEK-RAG reduces computational cost while improving regulatory compliance and long-turn dialogue stability
- The dual-stage alignment process enables small models to achieve competitive results in specialized domains
- Stress tests reveal degradation in standard metrics for long conversations that the proposed framework addresses

## Why This Works (Mechanism)

### Mechanism 1: Scene-Filtered Retrieval for Statutory Compliance
If a retrieval-augmented generation (RAG) system filters knowledge by recognized service scenarios before retrieval, it likely reduces hallucination rates in regulated domains compared to generic vector search. SEEK-RAG uses a lightweight classifier to map user queries to specific sub-libraries before performing BM25/Vector retrieval, constraining the search space and reducing the probability of retrieving semantically similar but regulatory distinct clauses.

### Mechanism 2: Dual-Stage Alignment for Compute Efficiency
Small models (7B parameters) can match the performance of frontier models (>100B parameters) in specialized domains if subjected to a two-stage alignment process: domain knowledge infusion followed by preference optimization. Stage 1 saturates the model with domain-specific logic, while Stage 2 uses Direct Preference Optimization on a high-quality subset to enforce empathy and citation behavior.

### Mechanism 3: Stress-Testing for Cross-Turn Consistency
Standard metrics fail to capture regulatory drift in long conversations; dedicated stress tests like Long-Dialogue Consistency are required to validate state tracking. The LDC metric enforces a Finite State Machine check on dialogues, verifying that slot values remain consistent or are updated validly, forcing the model to maintain "state" explicitly via retrieval or context.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** LLMs are prone to hallucinations regarding specific, mutable regulatory clauses. RAG anchors responses in a verified document corpus.
  - **Quick check question:** Can you explain why generic vector search might fail if the knowledge base contains conflicting tariff policies from different years?

- **Concept:** Parameter-Efficient Fine-Tuning (QLoRA)
  - **Why needed here:** Utility companies often lack the infrastructure to train 70B+ models. QLoRA allows adapting 7B models on consumer-grade GPUs while retaining domain knowledge.
  - **Quick check question:** How does freezing the base model weights and only training adapter layers affect the risk of "catastrophic forgetting" of general language capabilities?

- **Concept:** Direct Preference Optimization (DPO)
  - **Why needed here:** Standard SFT teaches the model what to say, but DPO is used here to align how it says itâ€”specifically prioritizing "actionability" and "empathy" over raw information density.
  - **Quick check question:** In the context of customer service, why might maximizing ROUGE scores lead to worse user satisfaction?

## Architecture Onboarding

- **Component map:** ASR (Voice to Text) -> PII Masking -> QA-De-ID -> Knowledge Base (Regulatory Docs + 95598 Tickets) -> Vector Index (partitioned by Scenario) -> User Query -> Scene Classifier -> Retriever (BM25 + Keywords) -> LLM (7B SFT + DPO) -> Response -> SCC/LDC Stress Tests + Human/Model Review Loop

- **Critical path:** The Scene Classifier and Keyword Extractor are the gating factors. If the query is misclassified, the Retriever queries the wrong partition, and the LLM generates an answer based on incorrect context.

- **Design tradeoffs:** The paper trades raw reasoning power of 100B models for the auditability and lower compute cost of 7B+RAG. The assumption is that the knowledge base covers 100% of necessary reasoning logic. Reliance on synthetic dialogues risks propagating GPT-4o's own hallucinations into the training set.

- **Failure signatures:**
  - Low SCC Score: Model cites "Regulation (2023)" when the user asks about current "2025" rules (Staleness)
  - Low LDC Score: In turn 8, the model forgets the user is a "commercial" client and offers "residential" advice (State Drift)

- **First 3 experiments:**
  1. Zero-Shot vs. SEEK-RAG: Run 100 queries with changing tariff dates. Verify if the Base model hallucinates dates while the RAG model pulls the correct date from the index.
  2. Ablate the Scene Filter: Disable the TinyBERT classifier and search the entire vector store. Measure the drop in SCC and increase in retrieval latency.
  3. LDC Stress Test: Create a synthetic conversation that switches regions mid-dialogue. Check if the model updates the tariff citation in the next turn.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can models maximize regulatory compliance without sacrificing user-friendliness?
**Basis in paper:** The authors observe that "at very high professionalism levels... the curve flattens and clarity gains shrink," noting that "simply adding detail can hurt readability."
**Why unresolved:** The paper identifies this trade-off but does not propose specific mechanisms to resolve the tension between dense technical accuracy and readability.
**What evidence would resolve it:** A modified evaluation comparing standard responses against those using explicit clarity-enhancing interventions while maintaining high SCC scores.

### Open Question 2
**Question:** Can the ElectriQ framework be extended to multimodal power service interactions involving images or audio?
**Basis in paper:** The dataset is constructed from "transcribed" audio and text tickets, yet real-world EPM scenarios often require analyzing visual evidence not supported by the text-only benchmark.
**Why unresolved:** The current evaluation protocol and SEEK-RAG pipeline are restricted to text, leaving the handling of non-textual inputs in safety-critical grid operations unaddressed.
**What evidence would resolve it:** A multimodal benchmark extension testing LLMs on tasks requiring interpretation of meter images or infrastructure photos alongside textual queries.

### Open Question 3
**Question:** How does SEEK-RAG performance degrade under dynamic, non-stationary regulatory environments?
**Basis in paper:** The retrieval index was frozen before testing, failing to simulate the "asynchronously updated knowledge bases" and frequent policy shifts common in real-world utility operations.
**Why unresolved:** It is unclear if the "domain-aligned 7B models" can maintain Statutory Citation Correctness in real-time when regulations change without costly re-indexing or retraining.
**What evidence would resolve it:** Temporal stress tests evaluating model latency and accuracy when the knowledge base is updated mid-evaluation or when queries reference regulations enacted after the training cutoff.

### Open Question 4
**Question:** Does domain alignment for EPM generalize efficiently to other power-system domains without extensive retraining?
**Basis in paper:** The authors note "professionalism varies more" in substation and hydropower subdomains, requiring "compact, domain-specific knowledge bases" to close the gap, questioning the zero-shot transferability.
**Why unresolved:** While the paper demonstrates transfer is possible, it does not quantify the minimal data threshold required to recover "professionalism" scores to EPM-equivalent levels.
**What evidence would resolve it:** A data-ablation study measuring the performance trajectory of ElectriQ-aligned models as they are fine-tuned on incrementally smaller datasets for target subdomains.

## Limitations

- Reliance on synthetic dialogues (361.4K generated by GPT-4o) introduces potential distributional drift
- The 2.4K sample size for preference alignment is notably small for fine-tuning specialized behavior
- Stress tests (SCC/LDC) may not capture all failure modes in real deployment where customers introduce off-script emotional or non-logical transitions

## Confidence

- **High Confidence:** Benchmark construction methodology, two-stage training pipeline
- **Medium Confidence:** Performance claims comparing 7B models + SEEK-RAG to larger models
- **Low Confidence:** Generalizability of LDC stress test beyond defined FSM transitions, robustness of SEEK-RAG with ambiguous multi-scenario queries

## Next Checks

1. **Distribution Shift Analysis:** Compare statistical distribution of entities, intents, and dialogue flows between synthetic and real dialogue subsets to quantify distributional differences.

2. **Scene Classifier Robustness Test:** Construct adversarial queries spanning multiple scenarios and measure classification accuracy and retrieval effectiveness when true answers require knowledge from multiple partitions.

3. **Long-Term Consistency Validation:** Extend LDC test beyond 12 turns with dialogues including deliberate state changes (user moves cities, tariff version updates mid-conversation) to verify correct updates to all subsequent citations and slot values.