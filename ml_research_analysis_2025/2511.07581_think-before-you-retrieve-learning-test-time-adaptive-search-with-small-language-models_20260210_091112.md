---
ver: rpa2
title: 'Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language
  Models'
arxiv_id: '2511.07581'
source_url: https://arxiv.org/abs/2511.07581
tags:
- search
- retrieval
- query
- turn
- orion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Orion is a training framework that enables compact models (350M-1.2B
  parameters) to perform adaptive retrieval through learned search strategies. The
  framework combines synthetic trajectory generation for supervised fine-tuning, reinforcement
  learning with turn-level rewards, and beam search inference algorithms.
---

# Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models

## Quick Facts
- arXiv ID: 2511.07581
- Source URL: https://arxiv.org/abs/2511.07581
- Reference count: 40
- Key outcome: Orion achieves 25.2% nDCG@10 on BRIGHT and 77.6% success on SciFact, outperforming retrievers up to 200-400× larger on five of six datasets

## Executive Summary
Orion is a training framework that enables compact models (350M-1.2B parameters) to perform adaptive retrieval through learned search strategies. The framework combines synthetic trajectory generation for supervised fine-tuning, reinforcement learning with turn-level rewards, and beam search inference algorithms. Orion trains models to reason over partial evidence, refine queries iteratively, and backtrack from failed search directions. On six retrieval benchmarks, Orion achieves 25.2% nDCG@10 on BRIGHT and 77.6% success on SciFact, outperforming retrievers up to 200-400× larger on five of six datasets. The results demonstrate that retrieval performance can emerge from learned strategies rather than model scale alone, suggesting that compact models can achieve strong retrieval performance through targeted training on adaptive search behaviors.

## Method Summary
Orion employs a two-stage training approach on LFM2 base models. First, synthetic trajectories generated by 8 LLMs covering 10 behavioral archetypes (breadth-first, depth-first, hill climbing, etc.) are used for supervised fine-tuning with model souping to merge specialist behaviors. Second, GRPO with turn-level rewards (combining normalized cosine similarity and rank) refines the model's ability to select productive search actions. At inference, perplexity-based beam search exploits the self-reflection capabilities learned during RL to prune unpromising trajectories. The framework uses a deliberately weak dense retriever (MiniLM-L6-v2) to demonstrate that learned strategies can compensate for limited embedding quality.

## Key Results
- Orion achieves 25.2% nDCG@10 on BRIGHT and 77.6% success on SciFact
- Outperforms retrievers up to 200-400× larger on five of six datasets
- Model souping outperforms curriculum learning by 3-6% nDCG on BRIGHT
- GRPO variants show higher backtracking rates and lower rank stagnation than SFT-only

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Diversity Injection via Synthetic Trajectories
Training on diverse search archetypes (exploration, exploitation, recovery) enables models to adapt strategies to task structure rather than committing to a fixed search pattern. Synthetic trajectories encode 10 behavioral archetypes, and during SFT models internalize these patterns while GRPO selectively amplifies productive strategies for each domain. The core assumption is that task-appropriate search behaviors are transferable from synthetic demonstrations to real queries, and diversity matters more than scale for generalization.

### Mechanism 2: Turn-Level Reward Dense Feedback
Rewarding each retrieval step (not just final outcome) provides credit assignment that teaches models when to backtrack and when to commit. The reward structure combines normalized cosine similarity and normalized rank, with GRPO computing group-relative advantages across candidates per turn. The core assumption is that intermediate retrieval quality correlates with final success, and dense rewards avoid sparse outcome-only gradients that fail to teach recovery.

### Mechanism 3: Inference-Time Beam Search with Learned Metacognition
Perplexity-based self-assessment at inference time exploits the reflection capabilities learned during GRPO to prune unpromising trajectories. Each beam maintains structured history, and perplexity on the model's relevance judgment serves as confidence signal for beam ranking. The core assumption is that models trained with explicit reasoning spans develop metacognitive assessment capabilities that transfer to perplexity-based confidence estimation.

## Foundational Learning

- **Reinforcement Learning Policy Optimization (GRPO variant)**
  - Why needed here: Core training mechanism after SFT scaffolding. Models must learn to select actions (queries) that maximize cumulative retrieval rewards.
  - Quick check question: Can you explain why group-relative advantages (comparing candidates within a group rather than against a baseline) might stabilize training compared to standard policy gradients?

- **Information Retrieval Metrics (nDCG, MRR, Recall, Success@k)**
  - Why needed here: Reward function design and evaluation both depend on understanding what these metrics capture. The paper uses nDCG@10 as primary, with turn-level rewards combining similarity and rank.
  - Quick check question: Why might nDCG be preferred over raw precision for evaluating multi-turn retrieval where ranking quality matters?

- **Beam Search with Structured Decoding**
  - Why needed here: Inference algorithm maintains B parallel hypotheses, each with structured `思索`/`<search_query>`/`<top_k_response>` cycles. Understanding beam management is essential for debugging retrieval failures.
  - Quick check question: What happens to retrieval diversity if beam size B is too small relative to expansion width M?

## Architecture Onboarding

- **Component map:**
  LFM2 base model -> SFT on synthetic trajectories -> GRPO with turn-level rewards -> deployment with beam search

- **Critical path:**
  1. Base model (LFM2-1.2B/700M/350M) → SFT on synthetic trajectories → GRPO with turn-level rewards → deployment with beam search
  2. Reward computation at each turn: retrieve → compute sim + rank → normalize → combine → update policy
  3. Inference: expand beams → score with perplexity → prune to top-B → check success → repeat or terminate

- **Design tradeoffs:**
  - Model souping vs. curriculum learning: Tables 7-8 show souping outperforms curriculum by 3-6% nDCG on BRIGHT. Souping preserves specialist behaviors; curriculum may cause interference.
  - Structural tokens: Ablation shows tokens add ~0.4% nDCG gain but improve interpretability. Not strictly necessary for performance.
  - Data scale: 40K samples strike balance; 80K adds only marginal gains (~1-2%) per Tables 11-12.
  - Retriever quality: MiniLM-L6-v2 (22.7M) is deliberately weak—learned strategies must compensate. Stronger retrievers may reduce need for adaptive search.

- **Failure signatures:**
  - Rank stagnation: Repeated queries with unchanged rankings indicates model stuck in loop. GRPO variants show lower stagnation than SFT-only.
  - Insufficient backtracking: If model never pivots from failing direction, it's not using recovery behaviors. GRPO increases backtracking rates per Figure 6.
  - Verbose queries: Llama-3.1-405B produces highly variable query lengths; Orion variants should maintain compact distributions.
  - Late-turn failures: Success concentrated at turn 1-2 suggests poor recovery. Distributed success across turns indicates adaptive behavior.

- **First 3 experiments:**
  1. SFT-only baseline: Train on synthetic data without GRPO. Compare nDCG@10 on BRIGHT to full Orion. Expect ~10% drop based on Table 4 (0.207 vs 0.212).
  2. Reward ablation: Replace turn-level reward with outcome-only (final success only). Expect reduced backtracking and lower recall.
  3. Beam size sweep: Test B ∈ {1, 2, 4, 8} with fixed M=4. Plot nDCG vs latency. Identify saturation point where additional beams provide diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reinforcement learning provide benefits beyond supervised fine-tuning for multi-turn retrieval?
- Basis in paper: Section 5 directly asks "Does RL help beyond SFT?" and notes GRPO yields modest topline gains but induces qualitatively different backtracking behavior.
- Why unresolved: The paper shows only +1–2% nDCG gains from GRPO over SFT, leaving unclear whether RL's primary value is behavioral (backtracking capability) or metric-driven, and what reward designs would maximize benefit.
- What evidence would resolve it: Ablations varying reward sparsity, turn-level vs. outcome-only rewards, and alternative RL algorithms (PPO, DPO) on the same SFT initialization.

### Open Question 2
- Question: How should models balance recovery capabilities (backtracking) against efficiency (avoiding excessive course-correction)?
- Basis in paper: Section 5 asks "Should models learn to fail better in retrieval?" and notes the tension where "recovery that comes too late, or occurs too often, distorts the distribution of successful turns."
- Why unresolved: Higher backtracking correlates with resilience but also with diminishing returns at later turns. No reward formulation currently penalizes shallow repetition while rewarding decisive recovery.
- What evidence would resolve it: Introducing an efficiency penalty (e.g., per-turn cost or repetition penalty) into GRPO rewards and measuring both recovery success and total turns needed.

### Open Question 3
- Question: How dependent is Orion's performance on the specific dense retriever backend (MiniLM-L6-v2)?
- Basis in paper: The paper deliberately uses MiniLM-L6-v2 (22.7M parameters) to create "challenging conditions where learned strategies must compensate for weaker embeddings."
- Why unresolved: It remains unclear whether Orion's learned strategies provide diminishing returns when paired with stronger retrievers like BGE-M3, or if the approach is specifically suited to weak-backbone settings.
- What evidence would resolve it: Evaluating Orion with progressively stronger dense retrievers (BGE-base, BGE-M3, E5-large) to measure strategy value as embedding quality varies.

### Open Question 4
- Question: Is the equal 50/50 weighting between similarity and rank normalization optimal for reward computation?
- Basis in paper: Section 3.3 states "Each signal contributes equally to the reward" without ablation, and Appendix D.5 notes z-score normalization did not help but does not test alternative weightings.
- Why unresolved: Cosine similarity and normalized rank capture different aspects of retrieval quality; their relative importance may vary by task type (reasoning-intensive vs. fact-verification).
- What evidence would resolve it: Grid search over reward weight combinations (e.g., 0.25/0.75, 0.75/0.25) across BRIGHT (reasoning-heavy) and SciFact (fact-verification) to identify task-specific optima.

## Limitations
- Behavioral diversity transfer from synthetic to real data remains unproven, with performance gains potentially reflecting overfitting to synthetic patterns
- Perplexity-based beam pruning effectiveness is uncertain, as correlation with actual retrieval quality hasn't been established
- The framework may be compensating for retriever weakness rather than demonstrating genuine adaptive search capabilities, as gains with stronger retrievers are unknown

## Confidence
- **High confidence**: Model souping outperforms curriculum learning (Tables 7-8 show 3-6% nDCG gains). The synthetic trajectory generation process and GRPO hyperparameters are well-specified.
- **Medium confidence**: Turn-level rewards improve backtracking behavior (Figure 6 shows GRPO variants have higher recovery rates). The structural tokens provide modest gains (~0.4% nDCG) but improve interpretability.
- **Low confidence**: Behavioral diversity transfer from synthetic to real data. Perplexity-based beam pruning effectiveness. Whether gains persist with stronger retrievers.

## Next Checks
1. **Synthetic-to-Real Transfer Validation**: Hold out 20% of real queries during synthetic generation. Measure behavioral archetype coverage and test whether synthetic-trained models generalize to held-out real queries. If performance drops >15% on held-out data, diversity hypothesis is weakened.
2. **Perplexity Correlation Study**: Compute perplexity vs actual nDCG for beam search outputs across B=1,2,4,8. If correlation coefficient <0.3, perplexity-based pruning is unreliable and computational efficiency claims are suspect.
3. **Retriever Strength Ablation**: Repeat main experiments with E5 (state-of-the-art dense retriever) instead of MiniLM-L6-v2. If Orion's relative gains shrink by >30%, the framework may be compensating for retriever weakness rather than demonstrating genuine adaptive search capabilities.