---
ver: rpa2
title: 'BALI: Enhancing Biomedical Language Representations through Knowledge Graph
  and Language Model Alignment'
arxiv_id: '2509.07588'
source_url: https://arxiv.org/abs/2509.07588
tags:
- language
- graph
- biomedical
- bali
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving biomedical language
  models' understanding of domain-specific concept structures and factual knowledge
  encoded in biomedical Knowledge Graphs (KGs). The proposed method, BALI, is a joint
  LM and KG pre-training approach that enhances language models with external knowledge
  by simultaneously learning a dedicated KG encoder and aligning the representations
  of both the LM and the graph.
---

# BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment

## Quick Facts
- arXiv ID: 2509.07588
- Source URL: https://arxiv.org/abs/2509.07588
- Reference count: 40
- Primary result: BALI improves PubMedBERT accuracy by 2.1%, 1.7%, and 6.2% on PubMedQA, MedQA, and BioASQ benchmarks respectively

## Executive Summary
BALI addresses the challenge of integrating structured biomedical knowledge into language models by aligning their representations with the Unified Medical Language System (UMLS) Knowledge Graph. The method jointly trains a language model and a graph neural network to learn a shared embedding space where textual and graph-based entity representations are geometrically consistent. This approach enhances biomedical LMs' understanding of domain-specific concepts and improves performance on downstream QA and entity-related tasks without requiring retrieval at inference time.

## Method Summary
BALI implements a dual-encoder architecture that processes biomedical text through a transformer-based LM while simultaneously encoding 1-hop UMLS subgraphs via a Graph Attention Network (GAT). The method uses masked language modeling (MLM) as a primary objective alongside a contrastive alignment loss that pulls text-based entity embeddings closer to their corresponding graph node embeddings using InfoNCE. Entities are linked to UMLS concepts using the BERN2 tool, and training proceeds with joint optimization of both encoders. The resulting LM retains its linguistic capabilities while gaining structured knowledge from the KG, improving performance on biomedical understanding tasks.

## Key Results
- PubMedBERT shows mean accuracy improvements of 2.1%, 1.7%, and 6.2% on PubMedQA, MedQA, and BioASQ benchmarks respectively
- BALI significantly enhances the ability of LMs to generate distinguishable and informative representations of biomedical concepts
- The method demonstrates consistent improvements across multiple biomedical LM architectures (PubMedBERT and BioLinkBERT)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning unimodal embedding spaces via contrastive loss enables the LM to assimilate structured factual knowledge from the KG without requiring inference-time retrieval.
- **Mechanism:** The model uses an InfoNCE loss to minimize the cosine distance between a textual entity representation and its corresponding graph node representation. By treating matched text-graph pairs as positives and unmatched pairs as negatives, the LM is forced to adjust its internal weights to produce embeddings that are geometrically consistent with the graph's semantic structure.
- **Core assumption:** Textual context and graph topology provide complementary representations of the same concept that can be mapped to a shared latent space.
- **Evidence anchors:**
  - [abstract] "...aligning the representations of both the LM and the graph."
  - [section 4.2] "...pull two representations of a concept $v$ closer in the aligned embedding space."
  - [corpus] "Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs" supports the efficacy of this alignment strategy in biomedical domains.

### Mechanism 2
- **Claim:** Joint optimization with MLM prevents catastrophic forgetting of linguistic syntax while injecting knowledge.
- **Mechanism:** The total loss is a sum of MLM and alignment objectives. While alignment shapes global entity representations, MLM maintains token-level prediction capabilities. This dual-objective forces the model to solve the alignment task by inferring missing information from the context of the masked sequence, rather than memorizing static entity vectors.
- **Core assumption:** The linguistic competence of the pre-trained LM is a necessary foundation for understanding the context in which biomedical entities appear.
- **Evidence anchors:**
  - [section 4.2] "Since the entity representation $\bar{e}_i$ is pooled over a textual sequence masked for MLM objective, alignment loss further enforces LM to infer relevant information from the whole sequence."
  - [section 5.5.1] Ablation shows removing MLM drops PubMedQA accuracy from 65.2% to 60.54%.

### Mechanism 3
- **Claim:** Explicit GNN encoding captures structural hierarchy that sequential processing (linearization) misses, particularly for smaller models.
- **Mechanism:** A Graph Attention Network (GAT) aggregates features from 1-hop neighbors, weighting them by importance. This explicitly encodes the "local subgraph" context into the node embedding before alignment, providing a stronger structural signal than simply flattening the graph into a text string.
- **Core assumption:** The relationships between a concept and its neighbors (1-hop subgraph) contain critical disambiguating information not always present in the immediate text.
- **Evidence anchors:**
  - [section 4.1] "GNN is utilized to capture and encode the graph knowledge... while LM is used to obtain textual entity representations."
  - [section 5.4] Table 5 shows GAT (65.2%) outperforms Linearized Graph (65.0%) and mean-pooling GraphSAGE (58.8%) on PubMedQA for base models.

## Foundational Learning

- **Concept:** Contrastive Learning (InfoNCE)
  - **Why needed here:** This is the mathematical engine of BALI's alignment. You must understand how the loss function pulls positive pairs together while pushing negative pairs apart to grasp how the "alignment" actually occurs.
  - **Quick check question:** In the context of BALI, what constitutes a "positive" pair versus a "negative" pair during a training step?

- **Concept:** Graph Attention Networks (GAT)
  - **Why needed here:** BALI uses a GAT to encode the KG. You need to understand how attention mechanisms apply to irregular graph structures to see how the model decides which related concepts are important for defining the central entity.
  - **Quick check question:** How does the GAT encoder in BALI determine the weight of a neighboring node when aggregating information for the target concept?

- **Concept:** Entity Linking (EL)
  - **Why needed here:** The entire pipeline depends on the "Pretraining Data" step where biomedical entities in text are linked to UMLS concepts. If this linking fails, the model aligns text to the wrong graph nodes.
  - **Quick check question:** What tool does BALI use to link mentions in PubMed abstracts to the UMLS Knowledge Graph, and what is the risk if this tool misidentifies a concept?

## Architecture Onboarding

- **Component map:** PubMed Abstract -> BERN2 Entity Linking -> LM Encoder + GNN Encoder -> Alignment Head -> Joint Loss
- **Critical path:** The data preprocessing (Entity Linking) is the critical dependency. The model cannot learn alignment if the pairs are incorrect. During training, the critical path is the synchronization of gradients from both the LM and GNN branches during joint loss backpropagation.
- **Design tradeoffs:**
  - **GNN vs. Linearization:** Using a GNN requires maintaining a separate encoder and graph data loader (higher complexity), whereas linearization uses the existing LM (lower complexity). The paper suggests larger LMs can handle linearization well, but smaller LMs benefit from the explicit structure of the GNN.
  - **Subgraph Size:** The paper limits neighbors to 3 to manage memory footprint. Increasing neighbors might capture more context but drastically increases compute time.
- **Failure signatures:**
  - **Performance Drop on General NLP:** If alignment loss overpowers MLM, the model may lose syntactic fluency.
  - **Stagnant Loss:** If entity linking quality is poor, the contrastive loss will struggle to converge as it tries to align mismatched concepts.
  - **Memory OOM:** GNNs require holding neighbor indices in memory; increasing the max neighbor count or batch size triggers Out-Of-Memory errors faster than standard LMs.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the BALI pre-training with alignment loss only (no MLM) to confirm the catastrophic forgetting of language skills mentioned in Section 5.5.1.
  2. **Neighbor Sensitivity:** Retrain the GNN branch with "Max # of node neighbors" set to 1 vs. 5 to observe the performance saturation point on the BioASQ benchmark.
  3. **Encoder Substitution:** Replace the GAT encoder with a simple GraphSAGE (mean-pooling) to empirically verify the importance of attention in aggregating biomedical relationships on your specific validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the BALI alignment strategy be effectively adapted for decoder-only and encoder-decoder Large Language Model architectures?
- Basis in paper: [Explicit] The conclusion explicitly lists applying the method to "decoder-only and encoder-decoder models" as the primary direction for future work.
- Why unresolved: The current study exclusively validates BERT-style encoders (PubMedBERT, BioLinkBERT). These architectures differ fundamentally from generative decoders in how they process context and represent entities, making the transferability of the cross-modal alignment mechanism uncertain.
- What evidence would resolve it: Successful implementation and evaluation of BALI on generative models (e.g., Llama, T5) to determine if the alignment benefits persist without a dedicated pooled entity representation step.

### Open Question 2
- Question: Why does BALI pre-training degrade the zero-shot entity linking accuracy of models already specialized for concept clusterization (e.g., SapBERT)?
- Basis in paper: [Inferred] Section 5.3.2 and Table 3 show that BALI reduces linking quality by approximately 10% for specialized models (SapBERT, GEBERT) on social media data, a phenomenon the authors hypothesize is due to affecting the "clustered structure."
- Why unresolved: The paper identifies the regression but provides only a hypothesis regarding the disruption of the representation space structure, leaving the precise mechanism of this negative transfer unverified.
- What evidence would resolve it: A geometric analysis of the embedding space (e.g., cluster tightness, isotropy) before and after alignment, potentially leading to a modified loss function that preserves the specialized clustering.

### Open Question 3
- Question: Does the BALI method maintain its efficacy when applied to general domain corpora and open Knowledge Graphs?
- Basis in paper: [Explicit] The authors state the intent to expand the pre-training method to "general domains" in the conclusion.
- Why unresolved: The experiments are confined to the biomedical domain using UMLS, a highly curated and dense meta-thesaurus. It is unclear if the method is robust enough to handle the noise, ambiguity, and sparsity typical of general-purpose Knowledge Graphs like Wikidata.
- What evidence would resolve it: Benchmark results on general-domain datasets (e.g., WebQSP or T-REx) using BALI-enhanced general-domain encoders.

## Limitations

- The quality of entity linking via BERN2 is critical but unverified in the paper; systematic errors in mapping text mentions to UMLS concepts would degrade the alignment signal.
- The GNN subgraph size is capped at 1-hop with max 3 neighbors, which may omit relevant contextual relationships in the KG for complex or sparsely connected concepts.
- The ablation study shows removing MLM degrades performance, but the effect of removing only the alignment loss is not reported, leaving the true contribution of the KG component partially obscured.
- The pretraining corpus is a 1.67M-sentence PubMed sample, but the exact selection criteria are not specified, making exact reproduction difficult.

## Confidence

- **High confidence:** The contrastive alignment mechanism (InfoNCE loss) and its integration with MLM are clearly defined and experimentally validated across multiple biomedical benchmarks.
- **Medium confidence:** The architectural choice of GAT over simpler GNN variants is supported by ablation on base models, but the benefit for large models is less clear due to convergence issues noted in the paper.
- **Low confidence:** The long-term generalization of BALI-enhanced LMs beyond the reported benchmarks and the robustness of the approach to noisy or incomplete KGs are not explored.

## Next Checks

1. **Entity Linking Quality Audit:** Re-run the BERN2 linking step on a held-out set of PubMed abstracts and manually verify the accuracy of UMLS CUI assignments, especially for ambiguous biomedical terms.
2. **GNN Depth and Neighborhood Sensitivity:** Train BALI with varying subgraph depths (1-hop vs. 2-hop) and neighbor counts (1, 3, 5) to quantify the impact of graph context on downstream QA performance.
3. **Alignment-Only Ablation:** Train a variant of BALI with only the alignment loss (no MLM) to directly measure the degradation in language understanding tasks and isolate the contribution of the KG component.