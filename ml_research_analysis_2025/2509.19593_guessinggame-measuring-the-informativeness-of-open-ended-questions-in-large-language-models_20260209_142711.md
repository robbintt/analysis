---
ver: rpa2
title: 'GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large
  Language Models'
arxiv_id: '2509.19593'
source_url: https://arxiv.org/abs/2509.19593
tags:
- questions
- question
- object
- guesser
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GuessingGame, a protocol to evaluate large
  language models (LLMs) as strategic question-askers in open-ended, open-domain settings.
  A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle
  without predefined choices.
---

# GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.19593
- **Source URL:** https://arxiv.org/abs/2509.19593
- **Reference count:** 33
- **Primary result:** Higher information gain (IG) predicts 43% reduction in expected game length; prompting constraints guided by IG improve weak models' success rates from 39% to 80%.

## Executive Summary
This paper introduces GuessingGame, a protocol to evaluate large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices. Two information gain (IG) metrics are proposed: a Bayesian belief-tracking method using LLM-scored relevance over semantic concepts, and an entropy-based method using ConceptNet. Across 858 games, higher IG strongly predicts efficiency—a one-standard-deviation IG increase reduces expected game length by 43%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.

## Method Summary
The GuessingGame protocol involves four LLM agents: Guesser (asks questions), Oracle (answers about hidden objects), Checker (validates constraints), and Interpreter (scores answer-relevant concepts for IG). The game uses 858 physical objects from Jiang and Riloff (2021). Two IG metrics are computed: Bayesian IG via KL divergence over semantic concepts (using Gemini as Interpreter) and entropy-based IG via ConceptNet candidate filtering. Games proceed with Guesser asking questions, Checker validating them, Oracle responding, and IG being calculated from the interaction. The primary evaluation metrics are Success Rate (SR) and Average Number of Questions (ANQ).

## Key Results
- Higher Bayesian IG strongly predicts efficiency: one-SD increase reduces expected game length by 43% (β=-0.57, p=1.77×10⁻⁷)
- LLaMA-3.3 70B success rate more than doubles (39.4% to 80.0%) with repeat-type constraint preventing consecutive same-type questions
- Open-ended questions show +0.12σ higher Bayesian IG compared to closed-ended questions (-0.12σ)
- Strong negative correlation (ρ=-0.95) between average IG and game length across models and object categories

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Belief Shift → Task Efficiency
- **Claim:** Higher per-turn information gain predicts faster convergence to the correct object
- **Mechanism:** Interpreter LLM scores answer-relevant concepts; scores update probability distribution via log-linear Jeffrey conditioning; strong belief shifts increase KL divergence
- **Core assumption:** Interpreter relevance scores approximate true semantic entailment from answers
- **Evidence anchors:** Strong negative effect (β=-0.57, p=1.77×10⁻⁷) corresponds to 43% reduction in game length
- **Break condition:** Miscalibrated Interpreter scores may mislead rather than clarify

### Mechanism 2: Question-Type Diversity → Avoidance of Enumerative Failure
- **Claim:** Preventing consecutive same-type questions reduces enumerative questioning and improves success rates
- **Mechanism:** Repeat-type constraint forces diversity, maintaining higher per-turn IG
- **Core assumption:** Diminishing IG from repeated types reflects true redundancy
- **Evidence anchors:** Success rate increases from 39.4% to 80.0% with repeat-type constraint
- **Break condition:** Forced diversity may produce irrelevant queries in domains with few valid question types

### Mechanism 3: Open-Ended Questions → Guaranteed Information Yield
- **Claim:** Open-ended questions yield higher average IG than binary yes/no questions
- **Mechanism:** Open-ended questions elicit new information regardless of response content
- **Core assumption:** Oracle responses to open-ended questions are truthful and specific
- **Evidence anchors:** Open-ended questions show +0.12σ Bayesian IG vs. -0.12σ for closed-ended
- **Break condition:** Vague or misleading open-ended responses may not translate to task success

## Foundational Learning

- **Concept: KL Divergence as Belief Shift**
  - Why needed here: Core IG metric quantifies belief distribution changes; essential for interpreting β coefficients
  - Quick check question: If belief becomes more concentrated after an answer, does KL divergence increase or decrease?

- **Concept: Jeffrey Conditioning / Soft Evidence Updates**
  - Why needed here: Bayesian update uses continuous relevance scores rather than hard likelihoods
  - Quick check question: Why might soft evidence be preferable when evidence is uncertain?

- **Concept: Accelerated Failure Time (AFT) Models**
  - Why needed here: AFT quantifies how IG scales expected game duration
  - Quick check question: What does a negative AFT coefficient imply about the relationship between IG and game length?

## Architecture Onboarding

- **Component map:** Guesser -> Checker (validation) -> Oracle (response) -> Interpreter (concept scoring) -> Belief update -> IG computation
- **Critical path:** Guesser asks → Checker validates → Oracle responds → Interpreter scores → Belief update → IG computation
- **Design tradeoffs:** Bayesian IG is flexible but depends on Interpreter calibration; ConceptNet IG is model-free but limited by graph coverage
- **Failure signatures:** (1) Enumerative questioning—repeated similar queries with near-zero IG; (2) Hierarchy mismatch—Guesser fixates on wrong abstraction level; (3) Oracle misleading responses
- **First 3 experiments:**
  1. Run baseline with LLaMA-3.3 70B on 50-object subset to verify SR ~39% and establish IG distribution
  2. Enforce Attribute-only, Function-only, and Location-only conditions to confirm Attribute questions yield highest IG
  3. Apply forced-open constraint to verify success rate approaches ~97% and compare ANQ reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do externally modeled Bayesian belief updates align with LLMs' internal representations of uncertainty?
- Basis in paper: [explicit] "Future work should investigate whether these externally modeled belief updates align with a model's internal representations, potentially leveraging adequacy criteria proposed by Herrmann and Levinstein (2024)."
- Why unresolved: Bayesian IG uses external Interpreter rather than accessing internal model states
- What evidence would resolve it: Probing internal model activations during gameplay and comparing to externally computed belief distributions

### Open Question 2
- Question: Does the Bayesian IG metric generalize as a domain-general measure of question informativeness?
- Basis in paper: [explicit] "Further work is needed to test whether this metric aligns with human judgments of informativeness across diverse tasks, question formats, and domains."
- Why unresolved: Metric was only tested on concrete physical objects despite strong human performance correlations
- What evidence would resolve it: Testing metric in diagnostic domains, legal reasoning, scientific discovery tasks, and comparing against human informativeness judgments

### Open Question 3
- Question: How robust is the Bayesian IG metric to different Interpreter LLM choices?
- Basis in paper: [explicit] "Future work should validate alternative interpreters, explore ensemble methods, or benchmark against human-labeled relevance scores."
- Why unresolved: Only Gemini was used; no comparison or human benchmark conducted
- What evidence would resolve it: Systematic comparison of different LLMs as Interpreters, ensemble methods, correlation with human-labeled concept relevance scores

### Open Question 4
- Question: Can question-asking strategies guided by IG metrics be improved through fine-tuning rather than prompting?
- Basis in paper: [inferred] All interventions were prompting-based; training-based improvements remain unexplored
- Why unresolved: Paper shows prompting works but doesn't test whether models can learn to optimize IG directly through training
- What evidence would resolve it: Fine-tuning models using IG as reward signal and comparing against prompting-based approaches

## Limitations
- Bayesian IG metric introduces second-order model dependency through the Interpreter LLM's relevance scoring
- ConceptNet coverage limitations affect entropy-based IG, particularly for multi-functional or niche objects
- Forced-diversity constraint effectiveness may not generalize to domains with naturally limited question types

## Confidence

- **High Confidence:** Core finding that higher IG predicts 43% reduction in game length (β=-0.57, p=1.77×10⁻⁷)
- **Medium Confidence:** Effectiveness of prompting constraints demonstrated through ablation studies
- **Low Confidence:** Comparative advantage of open-ended questions lacks corpus-level validation; Bayesian IG metric calibration across domains unverified

## Next Checks

1. **External Calibration Study:** Test Bayesian IG metric across multiple object domains to verify Interpreter relevance scoring consistency and predictive validity

2. **Constraint Generalization Test:** Apply repeat-type and forced-open constraints to smaller models (e.g., LLaMA-3B, GPT-3.5) to determine if performance improvements transfer across model families

3. **Oracle Reliability Assessment:** Implement ground-truth verification system to quantify Oracle hallucination frequency and test whether knowledge-grounded Oracles improve IG-calibrated performance compared to pure generative Oracles