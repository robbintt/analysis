---
ver: rpa2
title: A Systematic Review of Poisoning Attacks Against Large Language Models
arxiv_id: '2506.06518'
source_url: https://arxiv.org/abs/2506.06518
tags:
- poisoning
- attacks
- poison
- data
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This systematic review clarifies the landscape of large language\
  \ model poisoning attacks by introducing a unified threat model that categorizes\
  \ attacks across four dimensions: concept poisons, persistence, stealthiness, and\
  \ unique tasks. It defines six key performance metrics\u2014including attack success\
  \ rate, clean performance, efficiency, persistence, clean label, and stealthiness\u2014\
  and outlines four attack specifications covering poison set, trigger function, poison\
  \ behavior, and deployment."
---

# A Systematic Review of Poisoning Attacks Against Large Language Models

## Quick Facts
- arXiv ID: 2506.06518
- Source URL: https://arxiv.org/abs/2506.06518
- Reference count: 14
- Primary result: Introduces unified threat model categorizing LLM poisoning attacks across four dimensions (concept poisons, persistence, stealthiness, unique tasks) and six key metrics.

## Executive Summary
This systematic review clarifies the landscape of large language model poisoning attacks by introducing a unified threat model that categorizes attacks across four dimensions: concept poisons, persistence, stealthiness, and unique tasks. It defines six key performance metrics—including attack success rate, clean performance, efficiency, persistence, clean label, and stealthiness—and outlines four attack specifications covering poison set, trigger function, poison behavior, and deployment. The review identifies 65 relevant papers, highlighting that stealthiness and persistence are the most studied areas, while clean label attacks remain less explored. Concept-based poisons, especially in instruction tuning, are noted as particularly subtle and impactful. Persistence to defenses, fine-tuning, and across tasks is analyzed, revealing mixed effectiveness of current defenses. Unique tasks such as code generation, image generation, and RLHF are discussed, with code generation attacks posing significant security risks. The review also identifies poisoning via deletion as an understudied but potentially important attack vector.

## Method Summary
The paper conducted a systematic literature review using Semantic Scholar API to identify papers tagged as "JournalArticle" (including conferences/preprints) in Computer Science or Linguistics published from 2018 onward. Papers were filtered using boolean queries requiring at least one keyword from each of three sets: poisoning-related terms (trojan, backdoor), LLM-related terms (LLaMA*, BERT*), and training-related terms (pretrain*, finetun*). After screening title/abstracts to exclude non-training attacks, full-text review was performed on remaining papers. Data extraction involved classifying 65 papers using a schema capturing 34 specific traits related to the threat model dimensions (Metrics and Attack Specifications).

## Key Results
- Introduced unified threat model categorizing poisoning attacks across four dimensions: concept poisons, persistence, stealthiness, and unique tasks
- Identified 65 relevant papers showing stealthiness and persistence are most studied areas, while clean label attacks remain less explored
- Concept-based poisons, especially in instruction tuning, are particularly subtle and impactful compared to concrete token-based triggers
- Persistence analysis reveals mixed effectiveness of current defenses against fine-tuning and cross-task transfer
- Code generation attacks pose significant security risks, while poisoning via deletion remains an understudied attack vector

## Why This Works (Mechanism)

### Mechanism 1: Concept-Based Trigger Generalization
Meta-function triggers (syntax, style, semantic patterns) achieve higher stealthiness and persistence than concrete token-based triggers because they distribute the backdoor signal across model representations rather than isolated weight regions. Instead of associating a specific token with malicious output, concept poisons modify abstract linguistic properties (e.g., subordinate clause structure, writing style) that the model learns as contextual features, making detection via keyword-based defenses ineffective. The core assumption is that LLMs encode linguistic concepts as distributed representations that generalize across phrasing variations.

### Mechanism 2: Persistence Through Representation Entanglement
Poisons that embed malicious behavior in early-layer representations or through normalized gradient updates persist through fine-tuning because subsequent training on clean data cannot fully overwrite deeply entrenched feature-activation patterns. Model poisoning approaches that modify training procedures (e.g., poisoned loss functions, gradient normalization) create stronger weight-level associations that survive partial reinitialization and continue to activate on trigger presence. The core assumption is that early-layer representations in LLMs learn generalizable features that are preserved across downstream task adaptation.

### Mechanism 3: Clean Label Exploitation of Annotation Trust
Clean label attacks evade detection by human annotators and automated label-consistency checks because the input-label pairing appears semantically correct, limiting defense mechanisms that rely on identifying label-input mismatches. Attackers make subtle input modifications (synonym substitution, sentence rewriting) that preserve semantic meaning while introducing a trigger pattern, exploiting the assumption that correct labels imply clean data. The core assumption is that human annotators and automated quality checks verify label correctness but not subtle trigger patterns in input text.

## Foundational Learning

- **Data Poisoning vs. Model Poisoning**: Understanding this distinction is essential for determining threat model scope and defense applicability. If you receive a fine-tuned LLaMA adapter from a third party, which poisoning type applies?

- **Concrete vs. Meta-Triggers**: This distinction is crucial for assessing attack stealthiness and selecting appropriate defenses. A trigger that activates whenever input contains passive voice—is this concrete or meta?

- **Attack Success Rate (ASR) vs. Clean Performance Metric (CPM)**: Poisoning attacks involve a tradeoff: higher poison rates increase ASR but degrade CPM. Evaluating attack effectiveness requires measuring both dimensions. If ASR is 95% but CACC drops from 92% to 70%, is this a successful attack in practice?

## Architecture Onboarding

- **Component map**:
```
Threat Model
├── Metrics (6)
│   ├── Attack Success Rate (ASR)
│   ├── Clean Performance (CPM)
│   ├── Efficiency (poison rate vs. ASR/CPM)
│   ├── Persistence (defense, fine-tuning, task transfer)
│   ├── Clean Label (human label disagreement)
│   └── Stealthiness (input + model)
└── Attack Specifications (4)
    ├── Poison Set (concrete keyword / meta-function)
    ├── Trigger Function (concrete string op / meta-concept)
    ├── Poison Behavior (concrete task / meta-task)
    └── Deployment (data poisoning / model poisoning, identity trigger)
```

- **Critical path**:
  1. Define poison set selection criteria (who/what to target)
  2. Design trigger function (concrete token vs. meta-concept)
  3. Specify poison behavior (targeted label, meta-task like bias injection)
  4. Choose deployment method (poison dataset vs. poisoned model release)
  5. Evaluate across all 6 metrics, emphasizing persistence and stealthiness for real-world threat assessment

- **Design tradeoffs**:
  - **Stealthiness vs. Efficiency**: Lower poison rates reduce detectability but may not achieve >90% ASR; meta-triggers improve stealth but require more sophisticated design
  - **Persistence vs. Clean Performance**: Strongly embedded poisons (normalized gradients, early-layer targeting) resist fine-tuning but risk CPM degradation
  - **Clean Label vs. Attack Effectiveness**: Clean label attacks are harder to detect but often require higher poison rates to match dirty-label ASR

- **Failure signatures**:
  - Low persistence: ASR drops sharply after 1-2 epochs of clean fine-tuning
  - Poor stealthiness: ONION defense removes trigger tokens with high fluency gain
  - Efficiency ceiling: ASR plateaus at <80% even with 10%+ poison rate (suggests trigger-behavior association not learned)

- **First 3 experiments**:
  1. **Baseline concrete trigger attack**: Insert fixed token (e.g., "cf") into sentiment classification dataset at 5% poison rate, measure ASR and CACC, test ONION defense. Establishes lower bound for stealthiness.
  2. **Meta-trigger comparison**: Implement syntactic trigger (subordinate clause restructuring) at same poison rate, compare ASR, persistence through fine-tuning, and ONION/Back-Translate defense effectiveness.
  3. **Persistence stress test**: Take best-performing attack from experiments 1-2, apply fine-pruning (remove 10-30% smallest weights) and fine-mixing (blend with clean pre-trained weights), measure ASR retention. Identifies defense-resilient attack configurations.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How effective are poisoning attacks that rely exclusively on deleting data points or content within data points?
**Basis in paper**: Section 2.2.2 states that "poisoning via deletion" is an understudied area, noting that almost all literature focuses on insertion or substitution.
**Why unresolved**: The review authors explicitly identify this as a gap, noting that while LLM practitioners frequently curate datasets by deleting data, the adversarial implications of this method are not well understood.
**What evidence would resolve it**: Empirical studies measuring Attack Success Rate (ASR) and stealthiness for deletion-based attacks compared to standard insertion-based methods.

### Open Question 2
**Question**: To what extent does Reinforcement Learning from Human Feedback (RLHF) function as a universal defense against instruction tuning poisoning?
**Basis in paper**: Section 4.4.5 notes that while RLHF was found to reduce attack efficacy, "Future work can aim to elucidate how RLHF is able to stay robust to poisoning and how universal a defense it may be."
**Why unresolved**: While RLHF shows promise in specific contexts (e.g., requiring high poison rates to succeed), its robustness across diverse attack vectors and lower poison rates requires further characterization.
**What evidence would resolve it**: Systematic evaluation of ASR on poisoned instruction-tuned models before and after RLHF alignment across multiple attack types and poison rates.

### Open Question 3
**Question**: What is the precise trade-off between re-initializing specific model layers to disrupt triggers and the resulting degradation in clean performance?
**Basis in paper**: Section 4.2.1 observes that while Re-Init is a defense, "re-initializing earlier layers can better protect against poisoning, but disrupt clean learning. This should be studied in more detail in the future."
**Why unresolved**: Determining which layers to re-initialize to disrupt trigger specificity without catastrophically forgetting pre-trained representations remains an open optimization problem.
**What evidence would resolve it**: A quantitative analysis plotting Clean Performance Metrics (CPM) against Attack Success Rates (ASR) when re-initializing layers at varying depths.

## Limitations

- The systematic review methodology depends on keyword-based search in Semantic Scholar, which may have missed relevant papers not tagged with specified terms or published in venues outside Computer Science/Linguistics.
- The paper does not provide a machine-readable schema for the 34 extracted traits, requiring manual coding that introduces potential subjectivity in categorizing attack characteristics.
- Concept-based poison mechanisms are described theoretically but lack comprehensive empirical validation across diverse model architectures and datasets.

## Confidence

- **High**: The framework's core dimensions (metrics, attack specifications) are well-defined and supported by clear examples from cited literature. The distinction between data poisoning and model poisoning, and the six-metric evaluation framework, are methodologically sound.
- **Medium**: The characterization of concept-based triggers as more stealthy and persistent is logically compelling but based on a limited number of studies (primarily Qi et al. 2021b). The generalization to all meta-triggers needs broader validation.
- **Medium**: The analysis of persistence through representation entanglement is supported by specific citations (Gu et al. 2023) but requires more diverse experimental conditions to confirm across different poisoning techniques and model sizes.

## Next Checks

1. **Empirical Validation of Concept Triggers**: Implement and test Qi et al.'s syntactic trigger approach on a modern LLM (e.g., LLaMA-2) with varying poison rates (1%, 5%, 10%) and evaluate ASR, persistence through fine-tuning, and detection by ONION/Back-Translate defenses. Compare against concrete token triggers to quantify stealthiness and persistence differences.

2. **Defense Effectiveness Survey**: Conduct a comprehensive evaluation of existing defenses (ONION, DeBITE, back-translation, fine-pruning) against clean label attacks across multiple datasets and model architectures. Measure false positive rates and ASR degradation to identify defense gaps highlighted in the review.

3. **Poisoning via Deletion Study**: Design and execute experiments testing poisoning through data deletion (removing training examples that contradict desired malicious behavior) on sentiment classification and code generation tasks. Evaluate whether deletion-based poisoning achieves comparable ASR to traditional addition-based poisoning while maintaining stealthiness.