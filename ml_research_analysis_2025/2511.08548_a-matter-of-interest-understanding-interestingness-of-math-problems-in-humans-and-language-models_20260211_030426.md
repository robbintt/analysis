---
ver: rpa2
title: 'A Matter of Interest: Understanding Interestingness of Math Problems in Humans
  and Language Models'
arxiv_id: '2511.08548'
source_url: https://arxiv.org/abs/2511.08548
tags:
- interestingness
- human
- problems
- problem
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how well large language models (LLMs)
  align with human judgments of mathematical problem interestingness and difficulty.
  The authors conducted two empirical studies: one with crowdsourced participants
  rating AMC problems, and another with IMO competitors rating IMO problems.'
---

# A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models

## Quick Facts
- arXiv ID: 2511.08548
- Source URL: https://arxiv.org/abs/2511.08548
- Reference count: 40
- Primary result: LLMs moderately align with human interestingness rankings (R² up to 0.78) but fail to capture full distribution of human judgments

## Executive Summary
This paper investigates how well large language models align with human judgments of mathematical problem interestingness and difficulty. Through two empirical studies with crowdsourced participants rating AMC problems and IMO competitors rating IMO problems, the authors collected 822 human judgments. They evaluated 12 LLMs across five families on their ability to rate problem interestingness and difficulty. Results show that while many LLMs correlate with human judgments of interestingness, they generally fail to capture the full distribution of human ratings. Only Mistral 7B closely matched human distributions. The study also found that reasoning models tend to make faster judgments about uninteresting problems compared to interesting ones, though this pattern weakens for harder problems.

## Method Summary
The study evaluated 12 LLMs across five families on AMC and IMO problems using two datasets: Prolific crowdsourcing (63 participants, 18 AMC problems + variants) and IMO survey (48 participants, 4 problems each). Models were prompted to rate interestingness and difficulty on 0-100 scales, with responses sampled at temperatures 0.3 and 1.0 (5 samples for reasoning models on IMO). Human participants provided ratings plus 1-3 sentence rationales. Analysis included R² correlation matrices, Wasserstein Distance for distributional alignment, and comparison of rationale selection distributions across 11 interestingness criteria.

## Key Results
- LLMs show moderate correlation with human interestingness judgments (R² ranging from 0.48 to 0.78) but mostly fail to capture human rating distributions
- Only Mistral 7B achieved Wasserstein Distance within the human-human split-half baseline (WD = 12.4 vs human baseline WD = 9.5)
- For IMO problems, human participants most frequently cited elegance as a key factor in interestingness
- Most LLMs showed poor alignment with human-selected interestingness rationales, selecting only 1-2 importance scores per criterion
- Reasoning models allocated fewer tokens to uninteresting problems, but this pattern disappeared for harder IMO-level problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs capture mean-level interestingness rankings moderately well but fail to match the full distribution of human judgments.
- Mechanism: Correlation-based evaluation (R²) captures ranking alignment but Wasserstein Distance (WD) reveals distributional mismatch. Mistral 7B achieved mean WD = 12.4 with 95% CI overlapping human split-half baseline (WD = 9.5, CI = [7.8, 11.5]), while other models showed higher WD (16–22).
- Core assumption: Human interestingness is a meaningful latent construct that can be measured via rating scales and that split-half correlation (~0.71 R²) represents a reasonable noise ceiling.
- Evidence anchors:
  - [abstract]: "while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments"
  - [section 4/Table 1]: Mistral 7B is the only model with WD CI overlapping human-human baseline
  - [corpus]: Limited direct corpus support; related work on engagement modeling (IntrEx) exists but does not validate this specific mechanism.
- Break condition: If humans exhibit multi-modal interestingness distributions (e.g., distinct expert vs. novice patterns) that LLMs cannot capture even with calibration, distributional alignment approaches may fail.

### Mechanism 2
- Claim: Large Reasoning Models (LRMs) allocate fewer reasoning tokens to problems they judge as uninteresting, but this pattern attenuates for harder problems.
- Mechanism: On Prolific (AMC-level) problems, all four LRMs showed "flash" judgments (shorter reasoning chains) for low-interest problems vs. longer chains for high-interest problems. This effect disappeared on IMO-level problems where parsing difficulty may dominate resource allocation.
- Core assumption: Reasoning token count proxies cognitive/resource investment; interestingness influences reasoning effort beyond problem complexity.
- Evidence anchors:
  - [abstract]: "reasoning models tended to make faster judgments about uninteresting problems compared to interesting ones"
  - [section 4/Figure 3]: Judgment speed distributions show faster responses for low-interest Prolific problems; effect disappears for IMO (Appendix Figure 6)
  - [corpus]: No direct corpus evidence on LRM resource allocation by interestingness.
- Break condition: If reasoning token length is driven primarily by problem surface features (length, notation density) rather than perceived interestingness, the mechanism confounds difficulty with interest.

### Mechanism 3
- Claim: LLMs poorly capture the rationale structure humans use to justify interestingness judgments.
- Mechanism: Humans spanned the full range of importance ratings across 11 interestingness criteria (elegance, simplicity, etc.), but most LLMs—even at temperature 1.0 with 50 samples—selected only 1–2 importance scores per criterion. Only Mistral 7B and Mistral 24B reflected human rationale distributions.
- Core assumption: Interestingness judgments are grounded in identifiable factors (elegance, surprise, utility) that generalize across problems and people.
- Evidence anchors:
  - [abstract]: "most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales"
  - [section 4]: "most LLMs only selected one to two importance scores for each reason" despite sampling
  - [corpus]: "Learning Interestingness in Automated Mathematical Theory Formation" (FERMAT) attempts to model interestingness via RL but uses symbolic heuristics, not human-aligned rationale distributions.
- Break condition: If rationale selection is post-hoc rationalization rather than causal driver of interestingness, aligning LLMs to stated reasons may not improve actual judgment alignment.

## Foundational Learning

- Concept: **Wasserstein Distance (Earth Mover's Distance)**
  - Why needed here: Quantifies distributional mismatch between human and LLM rating distributions; R² alone masks distributional differences.
  - Quick check question: If Model A and Model B both have R² = 0.7 with human ratings, but Model A has WD = 12 and Model B has WD = 20, which better captures human judgment diversity?

- Concept: **Split-half reliability (noise ceiling)**
  - Why needed here: Establishes the maximum explainable variance in human data; LLM-human R² should be interpreted relative to human-human agreement (~0.71 R²).
  - Quick check question: If human split-half R² is 0.71 and LLM-human R² is 0.78, is the LLM outperforming human consistency or is measurement error inflating the estimate?

- Concept: **Reasoning tokens (inference-time compute)**
  - Why needed here: Proxy for LRM "thinking time"; used to analyze whether models invest more resources in interesting problems.
  - Quick check question: An LRM produces 500 reasoning tokens for Problem A and 2000 tokens for Problem B. Can you conclude Problem B is more interesting? What confounds apply?

## Architecture Onboarding

- Component map: Prolific study -> 63 participants -> 18 AMC problems (8 original + variants + 2 controls) -> 822 human judgments; IMO study -> 48 participants -> 4 problems each -> aggregate analysis
- Critical path: 1) Curate problem set with variants (systematic modifications to probe interestingness factors); 2) Collect human ratings with attention checks (e.g., filter participants rating "28+13" as >90 interesting); 3) Prompt models identically; sample multiple responses per temperature; 4) Compute per-problem mean ratings → R² correlation matrix; 5) Compute full distribution comparisons → Wasserstein Distance with bootstrapped CIs; 6) Compare rationale selection distributions (humans vs. models)
- Design tradeoffs: Temperature 1.0 increases response diversity but may reduce point-estimate reliability; the paper reports both 0.3 and 1.0; IMO sample size (1–2 ratings/problem) limits per-problem analysis; aggregate rationale analysis used instead; Variant design (number scaling, step addition/removal, ambiguity) enables controlled comparison but may not generalize to research mathematics
- Failure signatures: Distributional collapse (Model produces narrow rating range despite human spread → high WD); Rationale rigidity (Model always selects same importance scores regardless of criterion → fails rationale alignment); Temperature-insensitivity (Model shows same WD at temp 0.3 and 1.0 → may indicate lack of judgment uncertainty representation)
- First 3 experiments: 1) Replicate with calibration (Take top-performing model, apply distributional calibration to human training split, measure WD reduction on held-out problems); 2) Rationale intervention (Explicitly prompt models with human-selected interestingness criteria before rating; test whether rationale awareness improves rating alignment); 3) Cross-domain transfer (Evaluate same models on non-competition math; test whether correlation patterns generalize or are overfit to AMC/IMO style)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do LLM interestingness judgments generalize to non-competition mathematical problems?
  - Basis in paper: [explicit] The authors state that their datasets use "competition math problems, which are a narrow subset of the problems educators and mathematicians encounter daily."
  - Why unresolved: The current study is restricted to AMC and IMO problems; it does not evaluate problems found in pedagogical settings or open research.
  - What evidence would resolve it: Evaluating the same models on datasets of textbook exercises, open conjectures, or research-level lemmas to see if correlations hold.

- **Open Question 2**: Which human demographic should LLMs align with, and how does expertise level impact alignment?
  - Basis in paper: [explicit] The authors ask, "Which humans should those responses align to, and at what level of mathematical experience?" noting that their populations overlook true beginners and research experts.
  - Why unresolved: The study aggregates crowdsourced (Prolific) and elite (IMO) participants but does not model the distinct interestingness criteria of different skill levels (e.g., students vs. researchers).
  - What evidence would resolve it: A comparative study where models are fine-tuned or evaluated against distinct groups (novices, educators, researchers) to identify which group yields the highest alignment.

- **Open Question 3**: What specific factors enable smaller models like Mistral 7B to outperform larger models in distributional alignment?
  - Basis in paper: [explicit] The authors note that Mistral 7B was the only model to match human distribution and state, "Future work can better understand the drivers for such differences across model families."
  - Why unresolved: The paper identifies the anomaly (high Wasserstein Distance for most models, low for Mistral 7B) but does not isolate whether this is due to architecture, training data, or safety alignment.
  - What evidence would resolve it: Ablation studies or probing tasks to determine if Mistral's training data distribution mimics the variance of human subjectivity more closely than the RLHF used in larger models.

## Limitations

- The study's human data collection for IMO problems yielded only 1-2 ratings per problem, limiting per-problem analysis
- Correlation between LLM and human judgments (R² up to 0.78) approaches but does not exceed the human-human split-half reliability (~0.71 R²), raising questions about measurement ceiling effects
- The study focuses on competition mathematics problems, which represent a narrow subset of problems encountered by educators and mathematicians

## Confidence

- **High confidence**: LLMs capture mean-level interestingness rankings but fail to match human rating distributions
- **Medium confidence**: Reasoning models allocate fewer tokens to uninteresting problems
- **Medium confidence**: LLMs poorly capture human rationale structure

## Next Checks

1. **Calibration experiment**: Apply quantile mapping to top-performing model (Mistral 7B) using human training data, then measure WD reduction on held-out problems to test whether distributional misalignment is correctable through calibration.

2. **Rationale intervention test**: Prompt models with explicit human-selected interestingness criteria before rating problems, then compare whether this improves both rating alignment (R²) and rationale alignment (importance score distributions).

3. **Cross-domain generalization**: Evaluate the same models on non-competition mathematics (textbook exercises, research conjectures) to determine whether observed correlation patterns are specific to AMC/IMO problem styles or generalize to broader mathematical domains.