---
ver: rpa2
title: Rethinking the Stability-Plasticity Trade-off in Continual Learning from an
  Architectural Perspective
arxiv_id: '2506.03951'
source_url: https://arxiv.org/abs/2506.03951
tags:
- learning
- dual-arch
- task
- plasticity
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the stability-plasticity trade-off in continual
  learning from an architectural perspective, revealing that deeper networks exhibit
  better plasticity while wider networks demonstrate superior stability under equal
  parameter constraints. The authors propose Dual-Arch, a plug-in framework that leverages
  two distinct architectures - one optimized for plasticity and another for stability
  - to balance these competing objectives.
---

# Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective

## Quick Facts
- arXiv ID: 2506.03951
- Source URL: https://arxiv.org/abs/2506.03951
- Authors: Aojun Lu; Hangjie Yuan; Tao Feng; Yanan Sun
- Reference count: 26
- Key outcome: Dual-Arch framework leverages separate deep/wide architectures for plasticity/stability, improving continual learning performance while reducing parameters by up to 87%

## Executive Summary
This paper challenges conventional approaches to the stability-plasticity trade-off in continual learning by demonstrating that architectural choices—specifically depth and width—directly influence these competing objectives. Through systematic analysis of ResNet-18 variants, the authors reveal that deeper networks exhibit superior plasticity (faster new task learning) while wider networks demonstrate enhanced stability (better retention of old tasks). Building on this insight, they propose Dual-Arch, a plug-in framework that employs two distinct architectures: a deep, narrow network optimized for plasticity and a shallow, wide network optimized for stability. Knowledge distillation transfers information from the plastic learner to the stable learner, enabling significant performance gains while dramatically reducing parameter counts.

## Method Summary
Dual-Arch implements a two-learner architecture where Pla-Net (plastic learner) uses full ResNet-18 depth with reduced width (42 channels) and trains only on current task data with cross-entropy loss. Sta-Net (stable learner) uses half the ResNet-18 blocks (10 layers) with full width (64 channels) and incorporates knowledge distillation from Pla-Net alongside continual learning regularization. The stable learner receives distilled knowledge via KL divergence, preserving soft label information that encodes inter-class relationships. Both networks share the same ResNet-18 backbone structure but differ in depth/width configurations to optimize their respective objectives.

## Key Results
- Dual-Arch consistently outperforms single-architecture baselines across multiple continual learning methods and datasets
- Achieves competitive performance on both new task accuracy and low forgetting simultaneously
- Reduces parameter counts by up to 87% compared to traditional single-network approaches
- Demonstrates reduced task-recency bias through improved task boundary discrimination in confusion matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under fixed parameter budgets, deeper networks acquire new knowledge faster while wider networks retain old knowledge better.
- Mechanism: Depth increases representation capacity for novel patterns (plasticity), whereas width distributes stored features more redundantly, reducing interference from gradient updates (stability).
- Core assumption: The depth/width trade-off observed in ResNet-18 variants generalizes to other architectures.
- Evidence anchors:
  - [abstract]: "deeper networks exhibit better plasticity, while wider networks are characterized by superior stability"
  - [section 3.2, Table 1]: ResNet-18-64 shows higher AAN (86.41) but higher FAF (35.76); ResNet-10-96 shows lower AAN (83.44) but lower FAF (33.16)
  - [corpus]: Weak—neighboring papers discuss stability-plasticity trade-offs but do not directly validate architectural depth/width effects

### Mechanism 2
- Claim: Separating plasticity and stability into independent learners allows each to optimize its objective without interference.
- Mechanism: The plastic learner (Pla-Net) trains only on current data with cross-entropy loss; the stable learner (Sta-Net) receives distilled knowledge via KL divergence, preserving soft label information that encodes inter-class relationships from the plastic learner while applying continual learning regularization.
- Core assumption: Knowledge distillation effectively transfers task-relevant information across architectures with different depth/width ratios.
- Evidence anchors:
  - [section 4.4, Eq. 3-4]: L_stable = αL_CE + (1-α)L_KD + L_CL; L_KD uses KL divergence between teacher (plastic) and student (stable) soft outputs
  - [section 5.6, Figure 4]: Dual-Arch achieves competitive performance on both new task accuracy and low forgetting, unlike single-architecture baselines
  - [corpus]: Flashback Learning (arXiv:2506.00477) uses explicit trade-off balancing but through rehearsal, not architectural separation—suggests orthogonality

### Mechanism 3
- Claim: Dual-Arch mitigates task-recency bias by improving task boundary discrimination.
- Mechanism: The stable learner's wider architecture maintains more robust feature separators for old classes, while distilled knowledge from the plastic learner provides calibrated logits that reduce overconfidence on new classes.
- Core assumption: Task-recency bias is a primary driver of catastrophic forgetting in Class-IL.
- Evidence anchors:
  - [section 5.7, Figure 5]: Confusion matrices show Dual-Arch reduces misclassification of early-task samples as late-task classes (e.g., Task 1 → Task 10 errors decrease)
  - [abstract]: "significantly enhances CL performance while reducing parameter counts by up to 87%"
  - [corpus]: No direct validation—neighboring papers do not address bias correction specifically

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: The paper's entire framework is designed to mitigate this phenomenon where learning new tasks overwrites old task representations.
  - Quick check question: Can you explain why gradient updates on new data damage representations learned for previous tasks?

- Concept: **Knowledge Distillation**
  - Why needed here: Dual-Arch uses distillation (L_KD) to transfer knowledge from plastic to stable learner; understanding soft labels and temperature scaling is essential.
  - Quick check question: Why does distillation use soft labels (probability distributions) rather than hard labels?

- Concept: **Stability-Plasticity Trade-off**
  - Why needed here: The core insight is that this trade-off exists at the architectural level, not just the parameter level.
  - Quick check question: If you increase a network's plasticity, what typically happens to its stability, and why?

## Architecture Onboarding

- Component map:
  Pla-Net (ResNet-18 full depth, width 42) -> Knowledge Distillation -> Sta-Net (ResNet-18 half depth, width 64)

- Critical path:
  1. Train Pla-Net on current task with L_CE only (no CL regularization)
  2. Freeze Pla-Net, treat as teacher for distillation
  3. Train Sta-Net with L_stable = αL_CE + (1-α)L_KD + L_CL (L_CL from your chosen CL method)
  4. Inference uses only Sta-Net

- Design tradeoffs:
  - Parameter efficiency vs. training time: 87% parameter reduction but 1.39-1.77× longer training (two sequential training phases)
  - Depth vs. width for each network: Paper provides specific configurations; deviating may break stability/plasticity specialization
  - Distillation temperature: t=4 for CIFAR100, t=3 for ImageNet100—incorrect values cause soft label degradation

- Failure signatures:
  - Stable learner shows high forgetting despite L_CL → Check distillation is actually running (L_KD > 0)
  - Plastic learner underperforms on new tasks → Pla-Net may be too narrow; increase width within parameter budget
  - Training time exceeds 2× baseline → Pla-Net and Sta-Net may be training simultaneously instead of sequentially

- First 3 experiments:
  1. Baseline replication: Run iCaRL with ResNet-18 on CIFAR100/10; record AAN, FAF, LA, AIA as reference
  2. Ablation on architecture specialization: Replace Pla-Net with Sta-Net (both learners use stable architecture); expect AIA drop of ~0.65-1.74% per Table 3
  3. Parameter sweep: Reduce both networks' width by 25% and 50%; plot AIA vs. parameter count to verify efficiency gains persist (target: 81-87% reduction with ≥0.9% AIA improvement per Figure 3)

## Open Questions the Paper Calls Out
- Can the training process of the plastic and stable learners be parallelized to mitigate the observed 1.39x to 1.77x increase in wall-clock training time?
- Can Neural Architecture Search (NAS) be utilized to automatically discover the optimal architectural topologies for the plastic and stable learners rather than relying on manual design heuristics?
- What are the theoretical mechanisms that specifically link network depth to enhanced plasticity and width to enhanced stability in the continual learning loss landscape?

## Limitations
- The sequential training paradigm may introduce practical deployment challenges not addressed in the evaluation
- Architectural depth/width specialization hypothesis lacks direct ablation studies across diverse backbone architectures
- Temperature scaling is empirically chosen without sensitivity analysis

## Confidence
- **High confidence**: Dual-Arch improves Class-IL performance over single-architecture baselines when properly tuned
- **Medium confidence**: The specific depth/width configurations generalize beyond the tested datasets
- **Low confidence**: The claimed 87% parameter reduction is achievable across diverse continual learning methods without architectural modifications

## Next Checks
1. Cross-architecture validation: Test Dual-Arch with ViT and ConvNext backbones to verify depth/width specialization principle extends beyond ResNets
2. Hyperparameter sensitivity: Conduct ablation studies on distillation temperature and α weighting to establish robust configuration guidelines
3. Training efficiency optimization: Evaluate parallel training strategies for Pla-Net and Sta-Net to reduce the 1.39-1.77× training time overhead while maintaining performance gains