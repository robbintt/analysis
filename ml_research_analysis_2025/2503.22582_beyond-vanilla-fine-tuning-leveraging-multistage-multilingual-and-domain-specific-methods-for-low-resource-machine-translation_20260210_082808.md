---
ver: rpa2
title: 'Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific
  Methods for Low-Resource Machine Translation'
arxiv_id: '2503.22582'
source_url: https://arxiv.org/abs/2503.22582
tags:
- data
- fine-tuning
- translation
- b-ft
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of continual pre-training
  (CPT) and intermediate task transfer learning (ITTL) to improve neural machine translation
  (NMT) performance for low-resource languages (LRLs). The authors propose a multistage
  fine-tuning approach that includes domain-specific CPT using monolingual data, followed
  by ITTL fine-tuning with in-domain and out-of-domain parallel corpora.
---

# Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation

## Quick Facts
- **arXiv ID:** 2503.22582
- **Source URL:** https://arxiv.org/abs/2503.22582
- **Reference count:** 40
- **Primary result:** CPT and ITTL improve translation quality by an average of +1.47 BLEU score compared to single-stage fine-tuning baselines.

## Executive Summary
This study investigates continual pre-training (CPT) and intermediate task transfer learning (ITTL) to improve neural machine translation (NMT) performance for low-resource languages (LRLs). The authors propose a multistage fine-tuning approach that includes domain-specific CPT using monolingual data, followed by ITTL fine-tuning with in-domain and out-of-domain parallel corpora. The methods are evaluated on Sinhala, Tamil, and English language pairs in extremely low-resource settings. Results show that CPT and ITTL improve translation quality by an average of +1.47 BLEU score compared to single-stage fine-tuning baselines. Multi-model ensemble further enhances performance by an average of +2.13 BLEU score across all translation directions.

## Method Summary
The authors propose a multistage fine-tuning approach for low-resource machine translation. The method begins with continual pre-training (CPT) using monolingual data to compensate for under-representation of LRLs in the base multilingual LLM. This is followed by intermediate task transfer learning (ITTL) where the model is fine-tuned using auxiliary parallel corpora (both in-domain and out-of-domain) before final fine-tuning on the limited in-domain parallel data. The approach leverages mBART50 as the base model and is evaluated on extremely low-resource language pairs (Sinhala, Tamil, and English) with less than 100k parallel samples.

## Key Results
- CPT and ITTL improve translation quality by an average of +1.47 BLEU score compared to single-stage fine-tuning baselines
- Multi-model ensemble further enhances performance by an average of +2.13 BLEU score across all translation directions
- In-domain pre-training (Case A) plays a pivotal role in enhancing translation quality, particularly when having little in-domain data

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Representation Grounding via CPT
- **Claim:** Continual Pre-Training (CPT) with in-domain monolingual data may mitigate the "under-representation" of low-resource languages in the base msLLM vocabulary.
- **Mechanism:** By exposing the pre-trained model (mBART50) to specific monolingual text using a denoising objective (span masking, sentence permutation), the model updates its weights to better accommodate the unique script and domain-specific terminology of the target language *before* any parallel translation training begins.
- **Core assumption:** The base msLLM has existing, albeit weak, representations of the target language that can be rapidly refined without catastrophic forgetting of other languages.
- **Evidence anchors:**
  - [abstract] The paper proposes CPT "to compensate for the under-representation of LRLs."
  - [section 4.2] "In-domain pre-training (Case A) plays a pivotal role... having little in-domain data would have more impact than out-domain or mixed-domain data."
- **Break condition:** If the base model has zero representation of the target language (e.g., completely unseen script), CPT on monolingual data alone might fail to converge or align the encoder-decoder space effectively without seed parallel data.

### Mechanism 2: Curriculum-Based Regularization via ITTL
- **Claim:** Multi-stage Intermediate Task Transfer Learning (ITTL) functions as a curriculum strategy, stabilizing learning in extremely low-data regimes (<100k samples).
- **Mechanism:** Instead of directly overfitting to the small in-domain parallel set, the model first learns general translation dynamics from larger auxiliary data (out-domain or multilingual), then progressively specializes. Up-sampling the limited in-domain data during intermediate "mixed" stages prevents the model from drifting too far from the target domain while benefiting from the auxiliary volume.
- **Core assumption:** The "out-domain" auxiliary data shares sufficient syntactic or lexical overlap with the "in-domain" data to enable positive transfer.
- **Evidence anchors:**
  - [abstract] ITTL is used "to enhance its translation capabilities across various domains and tasks."
  - [section 4.4] Results show that three-stage fine-tuning "significantly improves BLEU score... utilizing the available in-domain monolingual and out-domain parallel data."
- **Break condition:** If the auxiliary data is too divergent (e.g., distinct topical domains like medical vs. legal), negative transfer may occur, degrading performance compared to the baseline.

### Mechanism 3: Variance Reduction via Ensembling
- **Claim:** Ensembling diverse fine-tuned models reduces the variance of predictions, particularly for morphologically rich languages.
- **Mechanism:** Averaging the probability distributions of models trained with different curricula (e.g., one via 3-B-FT, one via M-FT) or checkpoints cancels out independent errors, reinforcing the model's confidence in correct tokens.
- **Core assumption:** The errors made by individual models are uncorrelated or weakly correlated.
- **Evidence anchors:**
  - [abstract] "Additionally, a multi-model ensemble further improves performance by an additional BLEU score."
  - [section 4.5] "The findings highlight the general superiority of multi-model ensembles in boosting translation performance."
- **Break condition:** If the component models are highly correlated (e.g., ensembling checkpoints from the very end of a single run with minimal variance), the BLEU gains may be negligible compared to the computational cost of inference.

## Foundational Learning

- **Concept: Multilingual Sequence-to-Sequence LLMs (msLLMs)**
  - **Why needed here:** The paper relies on mBART50 as the "substrate" model. Understanding that mBART is an encoder-decoder Transformer pre-trained on denoising (filling in masked text) across 50 languages is crucial to grasping why CPT (more denoising) works.
  - **Quick check question:** Why can't we use mBART for translation immediately out of the box without fine-tuning? (Answer: It was trained to reconstruct text, not translate between languages).

- **Concept: The Low-Resource "Curse"**
  - **Why needed here:** The paper defines "extremely low-resource" as <100k samples. Standard deep learning models overfit heavily on datasets of this size; understanding this explains the motivation for ITTL and data augmentation.
  - **Quick check question:** What happens to validation loss if you train a large Transformer on a very small parallel corpus without regularization? (Answer: It drops rapidly but validation loss increases due to overfitting).

- **Concept: BLEU Score**
  - **Why needed here:** This is the sole evaluation metric used. Understanding that it measures n-gram overlap with reference translations (and penalizes length mismatch) is necessary to interpret the results tables.
  - **Quick check question:** If a model translates meaning perfectly but uses different synonyms than the reference, how might this affect BLEU? (Answer: BLEU score might be artificially low).

## Architecture Onboarding

- **Component map:**
  - **Base Model:** mBART50 (Transformer Encoder-Decoder)
  - **Input Pipeline:** SentencePiece Tokenizer (requires ZWJ fix for Sinhala/Tamil)
  - **Training Stages:**
    1. **CPT Stage:** Monolingual Data -> Denoising Autoencoder
    2. **ITTL Stage:** Auxiliary Parallel Data -> Translation Task
    3. **Fine-Tuning Stage:** In-Domain Parallel Data -> Translation Task
  - **Inference:** Ensemble Decoding (averaging beam search probabilities)

- **Critical path:**
  1. **Data Prep:** Clean monolingual/parallel data; Apply ZWJ fix
  2. **CPT:** Run `train.py` with denoising objective on monolingual data (Case A/ii recommended)
  3. **ITTL:** Initialize from CPT weights; Train on mixed auxiliary parallel data
  4. **Final FT:** Initialize from ITTL weights; Train on in-domain data
  5. **Ensemble:** Average logits of top-3 checkpoints/models during beam search

- **Design tradeoffs:**
  - **Precision:** The paper found Mixed-Precision (FP16) training significantly reduced training time without BLEU degradation compared to Full Precision (FP32)
  - **CPT Data:** In-domain monolingual data is preferred over larger out-domain data, suggesting quality/domain-match beats quantity for CPT
  - **ITTL Strategy:** "Three-stage Bilingual" generally outperformed "Multilingual-then-Bilingual" in the results for non-English pairs

- **Failure signatures:**
  - **Zero Width Joiner (ZWJ) Loss:** Outputting corrupted Sinhala text (e.g., "yansaya" rendered incorrectly). This indicates the tokenizer post-processing step was missed
  - **Overfitting on ITTL:** High BLEU on auxiliary validation but poor BLEU on in-domain test. Suggests the intermediate task was too long or learning rate was too high
  - **Tamil Translation Drift:** Poor performance translating *into* Tamil specifically. The paper notes this is due to Tamil's morphological complexity and flexible word order

- **First 3 experiments:**
  1. **Baseline Replication:** Implement standard Bilingual Fine-Tuning (B-FT) using the in-domain government data to establish a BLEU baseline
  2. **CPT Ablation:** Run CPT using only the small in-domain monolingual set (Case A) followed by B-FT. Compare against the baseline to verify the "representation grounding" hypothesis
  3. **ITTL Pipeline:** Implement the [biCPT, 3-B-FT] pipeline (Out-domain -> Mixed -> In-domain) and compare against the baseline to validate the curriculum learning benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed Continual Pre-Training (CPT) and Intermediate Task Transfer Learning (ITTL) methodologies be effectively transferred to modern generative decoder-only Large Language Models (LLMs) to achieve similar gains in low-resource machine translation?
- **Basis in paper:** [explicit] The authors state in the Conclusion that their approaches "can be applied to generative LLMs as well, leaving that for future work."
- **Why unresolved:** The current study exclusively utilizes encoder-decoder msLLMs (specifically mBART50) and does not test the applicability of these multistage fine-tuning strategies on decoder-only architectures.
- **What evidence would resolve it:** Comparative experiments applying the CPT and ITTL pipelines to generative LLMs (e.g., Llama, mGPT) using the same Sinhala-Tamil-English datasets.

### Open Question 2
- **Question:** What specific refinements to the Multilingual Fine-Tuning (M-FT) process are necessary to consistently outperform Bilingual Fine-Tuning (B-FT) and facilitate the creation of a unified model?
- **Basis in paper:** [explicit] The Conclusion notes that the "adaptation of M-FT models has not consistently surpassed the performance of B-FT models" and identifies refining this process to create a "singular, comprehensive MNMT model" as a goal for future work.
- **Why unresolved:** In the results (Table 5), Many-to-Many (M2M) fine-tuning generally underperformed compared to the bilingual baseline, often resulting in negative BLEU score changes, likely due to data insufficiency.
- **What evidence would resolve it:** A modified M-FT training regime (e.g., different sampling rates or auxiliary tasks) that yields higher BLEU scores than the B-FT baseline across all six translation directions simultaneously.

### Open Question 3
- **Question:** To what extent do the intrinsic linguistic features of target languages, such as Tamil's flexible word order and complex inflection, limit the efficacy of the proposed methods?
- **Basis in paper:** [inferred] The Discussion section attributes "diminished performance" when translating into Tamil to its linguistic complexity and flexible word order, despite the application of CPT and ITTL.
- **Why unresolved:** While the paper identifies the linguistic cause for the performance gap, it does not propose or test a method to specifically address these morphological and syntactic challenges within the multistage training framework.
- **What evidence would resolve it:** Experiments incorporating morphology-aware data augmentation or constraints into the ITTL stage, resulting in a statistically significant performance increase specifically for Tamil-target translations.

## Limitations
- The study focuses exclusively on three Indo-Aryan languages with shared script features, limiting generalizability to typologically diverse languages
- The paper does not systematically analyze computational costs or trade-offs between multistage approaches and simpler fine-tuning methods
- Results rely solely on BLEU score without incorporating human evaluation or other quality measures like COMET

## Confidence
- **High Confidence:** The observation that continual pre-training with in-domain monolingual data improves translation performance is well-supported by the experimental results
- **Medium Confidence:** The claim that multi-stage intermediate task transfer learning provides consistent benefits across all translation directions has medium confidence due to variability in effectiveness for non-English pairs
- **Low Confidence:** The assertion that multi-model ensembling provides reliable, substantial improvements (+2.13 BLEU) has low confidence due to limited analysis of ensemble composition and correlation between members

## Next Checks
1. **Typological Generalization Test:** Replicate the multistage fine-tuning approach on a set of low-resource languages from different language families (e.g., Turkish, Vietnamese, Swahili) to evaluate whether the observed improvements transfer beyond Indo-Aryan languages and similar writing systems.

2. **Ablation Study with Computational Metrics:** Conduct a comprehensive ablation study measuring not only BLEU scores but also training time, memory consumption, and inference latency for each stage of the proposed pipeline, comparing these costs against performance gains to establish practical deployment guidelines.

3. **Qualitative Error Analysis:** Perform detailed error analysis comparing translations from baseline and multistage fine-tuned models using human evaluation to identify specific error types that are reduced or introduced by each training stage, particularly focusing on morphological accuracy in languages like Tamil.