---
ver: rpa2
title: 'GRAMA: Adaptive Graph Autoregressive Moving Average Models'
arxiv_id: '2501.12732'
source_url: https://arxiv.org/abs/2501.12732
tags:
- grama
- graph
- arma
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRAMA, an adaptive graph autoregressive moving
  average model that transforms static graphs into sequences while preserving permutation
  equivariance. The key innovation is combining learnable ARMA coefficients with a
  selective attention mechanism to capture long-range dependencies in graphs, addressing
  the oversquashing problem in graph neural networks.
---

# GRAMA: Adaptive Graph Autoregressive Moving Average Models

## Quick Facts
- **arXiv ID**: 2501.12732
- **Source URL**: https://arxiv.org/abs/2501.12732
- **Reference count**: 40
- **Primary result**: GRAMA improves long-range graph modeling by transforming static graphs into sequences with selective ARMA coefficients, outperforming state-of-the-art methods on synthetic and real-world datasets.

## Executive Summary
This paper introduces GRAMA, an adaptive graph autoregressive moving average model that transforms static graphs into sequences while preserving permutation equivariance. The key innovation is combining learnable ARMA coefficients with a selective attention mechanism to capture long-range dependencies in graphs, addressing the oversquashing problem in graph neural networks. GRAMA theoretically connects to state space models and demonstrates stability through eigenvalue analysis. The method consistently outperforms backbone models across 14 synthetic and real-world datasets, including long-range graph benchmarks, molecular property prediction, and heterophilic node classification tasks.

## Method Summary
GRAMA transforms static graph data into sequential data by stacking $L$ copies of input features and applying separate MLPs to create an initial sequence. The model then uses a selective attention mechanism to dynamically learn ARMA coefficients based on the input graph structure and features. These coefficients control an AR/MA recurrence that operates across the sequence dimension, while a GNN backbone handles spatial aggregation. The system maintains permutation equivariance by avoiding node ordering and random walks. Stability is ensured through coefficient normalization that constrains the spectral radius of the state matrix.

## Key Results
- GRAMA improves performance on the LRGB benchmark by over 1.2 points compared to state-of-the-art methods
- GRAMA enhances simple GNN backbones like GCN by more than 11 AP points on peptide classification tasks
- The method consistently outperforms backbone models across 14 synthetic and real-world datasets
- Selective attention coefficients provide significant gains over naive (non-selective) coefficient learning

## Why This Works (Mechanism)

### Mechanism 1
Transforming a static graph into a sequence of graph states allows ARMA filters to capture long-range dependencies while preserving permutation equivariance. Instead of node ordering or random walks, the method stacks $L$ copies of input features and applies separate MLPs to create an initial sequence. The ARMA recurrence operates across this temporal dimension while the GNN backbone handles spatial aggregation.

### Mechanism 2
Selective (input-dependent) ARMA coefficients enable adaptive control over information propagation, addressing oversquashing more effectively than static coefficients. An attention mechanism computes scores between sequence elements without softmax normalization. The last row of the attention matrix becomes the AR/MA coefficients after tanh and normalization, allowing coefficients to vary per graph based on its structure and features.

### Mechanism 3
Stability and long-range propagation are controlled by constraining the spectral radius of the state matrix, governed by the roots of the autoregressive polynomial. If $\sum_{j=1}^p |\phi_j| \leq 1$, the system is stable. Roots closer to the unit circle enable longer-range propagation, and the coefficient normalization scheme is designed to satisfy these conditions.

## Foundational Learning

- **Autoregressive Moving Average (ARMA) Models**: Understanding how AR(p) captures dependencies on past values and MA(q) captures residual dependencies is essential to grasp why combining them provides a powerful sequential processing framework. *Quick check: Given a sequence, can you explain what the AR coefficients $\phi_i$ and MA coefficients $\theta_j$ control?*

- **Permutation Equivariance in Graph Neural Networks**: The paper explicitly contrasts its approach with prior graph SSM methods that sacrifice permutation equivariance through node ordering. Understanding why equivariance matters (output changes consistently when node order changes) clarifies why the graph-to-sequence transformation is designed as it is. *Quick check: If you shuffle the node ordering of an input graph, would you expect a permutation-equivariant GNN to produce a correspondingly shuffled output?*

- **State Space Models (SSMs) and Spectral Stability**: The paper establishes theoretical equivalence between ARMA and linear SSMs, and uses eigenvalue analysis to prove stability and characterize long-range propagation. Basic familiarity with how the state matrix $A$ governs system dynamics is necessary. *Quick check: If the eigenvalues of state matrix $A$ all have magnitude greater than 1, would you expect the system to remain stable over many time steps?*

## Architecture Onboarding

- **Component map**: Input features → Initial embedding (L MLPs) → GRAMA block (selective attention for coefficients → AR/MA computation → GNN residual) repeated R times → Nonlinearity → Repeat block S times → Final readout

- **Critical path**: Input features → Initial embedding ($L$ MLPs) → ARMA recurrence (selective attention for coefficients → AR/MA computation → GNN residual) repeated $R$ times → Nonlinearity → Repeat block $S$ times → Final readout

- **Design tradeoffs**: Larger $L$ provides longer effective receptive field but higher memory and attention complexity. Selective coefficients provide input-adaptive behavior but add attention overhead. Any MPNN or Graph Transformer can be used as backbone, with GPS often performing best but GCN still achieving strong gains.

- **Failure signatures**: Training instability or loss explosion indicates coefficient normalization issues. No improvement over backbone suggests $L$ too small or attention not learning meaningful patterns. Memory OOM requires reducing $L$ or hidden dimension. Performance degradation with depth may indicate incorrect nonlinearity placement.

- **First 3 experiments**:
  1. Verify basic functionality: Implement GRAMA with GCN backbone on MUTAG dataset, comparing GCN baseline vs. GRAMA-GCN with $L=2$, $S=1$.
  2. Ablate selective vs. naive coefficients: On Peptides-func dataset, compare GRAMA-GCN with selective attention vs. fixed learned coefficients.
  3. Probe long-range capability: On synthetic graph transfer task, vary source-target distance and plot performance vs. distance for GCN, GRAMA-GCN, and GPS.

## Open Questions the Paper Calls Out
The paper identifies several interesting future directions: extending GRAMA to spatio-temporal datasets where the temporal dimension is intrinsic rather than artificially constructed, reducing the quadratic $O(L^2)$ complexity of the selective attention mechanism for very long sequences, and investigating the theoretical relationship between optimal sequence length $L$ and graph topological properties like diameter.

## Limitations
The theoretical stability analysis is provided without rigorous proof in the main text, making it difficult to verify the conditions under which the model remains stable during training. The selective attention mechanism for learning ARMA coefficients lacks ablation studies showing that attention actually learns meaningful patterns versus degenerating to uniform weights. Performance improvements are primarily demonstrated against simpler baselines rather than direct comparisons with the most recent long-range graph methods.

## Confidence
**High Confidence**: Basic mechanism of transforming static graphs to sequences and empirical improvements over backbone models. Consistent gains across diverse tasks and datasets.
**Medium Confidence**: Theoretical connections to state space models and claimed stability guarantees. Framework is sound but proofs are incomplete and practical implications are not fully explored.
**Low Confidence**: Assertion that selective attention is the primary driver of performance improvements, as the paper lacks direct comparisons between selective and naive coefficient learning across all tasks.

## Next Checks
1. **Stability Constraint Verification**: Implement monitoring of coefficient spectral radii during training on a synthetic dataset to verify the stability constraint $\sum |\phi_j| \leq 1$ is maintained throughout training.
2. **Attention Pattern Analysis**: For LRGB Peptides-func dataset, visualize learned attention weights for ARMA coefficients across different graphs and quantify their diversity.
3. **Long-Range Propagation Test**: Create controlled synthetic graph transfer task with varying source-target distances (3, 5, 10, 50 hops) to compare GRAMA's performance degradation curve against GCN and GPS baselines.