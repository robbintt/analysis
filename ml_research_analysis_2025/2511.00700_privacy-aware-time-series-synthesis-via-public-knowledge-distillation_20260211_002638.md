---
ver: rpa2
title: Privacy-Aware Time Series Synthesis via Public Knowledge Distillation
arxiv_id: '2511.00700'
source_url: https://arxiv.org/abs/2511.00700
tags:
- data
- public
- time
- private
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pub2Priv, a novel framework for privacy-aware
  time series synthesis that leverages heterogeneous public metadata to improve the
  privacy-utility trade-off in synthetic data generation. The method uses a self-attention
  transformer to encode public metadata into temporal and feature embeddings, which
  are then used to condition a diffusion model trained with differential privacy (DP-SGD).
---

# Privacy-Aware Time Series Synthesis via Public Knowledge Distillation

## Quick Facts
- arXiv ID: 2511.00700
- Source URL: https://arxiv.org/abs/2511.00700
- Reference count: 30
- Primary result: Pub2Priv achieves superior privacy-utility trade-off by conditioning DP-diffusion models on public metadata, outperforming baselines on finance, energy, and commodity datasets.

## Executive Summary
This paper introduces Pub2Priv, a novel framework for privacy-aware time series synthesis that leverages heterogeneous public metadata to improve the privacy-utility trade-off in synthetic data generation. The method uses a self-attention transformer to encode public metadata into temporal and feature embeddings, which are then used to condition a diffusion model trained with differential privacy (DP-SGD). Experimental results across finance, energy, and commodity trading domains show that Pub2Priv consistently outperforms state-of-the-art baselines, achieving better preservation of return distributions, autocorrelation structures, and private-public correlations.

## Method Summary
Pub2Priv is a two-stage framework that conditions a differentially private diffusion model on embeddings from a pre-trained knowledge transformer. First, a self-attention transformer θT encodes heterogeneous public metadata into temporal and feature embeddings. Second, a conditional diffusion model θDM generates private time series conditioned on these embeddings, trained with DP-SGD to provide formal (ε, δ)-DP guarantees. The approach requires paired private time series and public metadata datasets, with public metadata exhibiting meaningful correlations to the private data. The knowledge transformer is frozen during private training, ensuring zero privacy loss contribution while providing structured signal to improve generation quality under DP constraints.

## Key Results
- Pub2Priv achieves superior return distribution preservation (KSR: 0.28 vs. 0.29-0.46) compared to state-of-the-art baselines
- Better autocorrelation structure preservation (KSAR: 0.17 vs. 0.18-0.44) across finance, energy, and commodity datasets
- Stronger downstream utility in both discriminative (TSTR: 0.003 vs. 0.006-0.054) and predictive tasks (TSTR: 0.007 vs. 0.008-0.063)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Public metadata conditioning improves utility without additional privacy cost.
- Mechanism: A pre-trained transformer encodes heterogeneous public metadata into embeddings that condition the diffusion model. Since θT never accesses private data, it contributes zero privacy loss by the post-processing property of differential privacy. The diffusion model receives structured signal about private-public correlations through these embeddings, improving generation quality under DP-SGD noise.
- Core assumption: Public metadata exhibits meaningful correlation with private time series (e.g., weather ↔ electricity consumption, market indices ↔ portfolios).
- Evidence anchors:
  - [abstract] "Our model employs a self-attention mechanism to encode public data into temporal and feature embeddings, which serve as conditional inputs for a diffusion model"
  - [section 5.1] "Since θT is a pre-trained model that have no access the private data, it does not bring any privacy loss to the model"
- Break condition: If public-private correlations are weak or spurious, conditioning may inject noise without utility gain.

### Mechanism 2
- Claim: Dual-axis self-attention captures temporal dynamics and cross-channel dependencies in heterogeneous metadata.
- Mechanism: The knowledge transformer applies alternating attention—first along the temporal axis (self-attention over t=1,...,L at fixed feature), then along the feature axis (self-attention over i=1,...,k at fixed time). This factorization efficiently models long-range temporal structure and inter-signal correlations without quadratic memory cost in both dimensions simultaneously.
- Core assumption: Metadata channels exhibit structured dependencies (e.g., temperature and electricity price jointly influence consumption).
- Evidence anchors:
  - [section 5.1] "we adopt a two-dimensional self-attention architecture that alternates attention along the temporal and feature axes to jointly model dynamics over time and dependencies across channels"
  - [table 1] Ablation (Pub2Priv w/o θT) shows degraded |Corrmeta| (0.48 vs 0.18 on Portfolio), confirming θT's role in preserving private-public correlations.
- Break condition: If metadata is low-dimensional or uncorrelated across channels, θT provides marginal benefit.

### Mechanism 3
- Claim: DP-SGD with per-sample gradient clipping bounds individual influence, enabling formal (ε, δ)-DP guarantees.
- Mechanism: Gradients are clipped to norm C, then Gaussian noise N(0, σ²C²I) is added. Privacy accounting via Rényi Differential Privacy tracks cumulative privacy loss across iterations. The pre-trained θT is frozen during DP training, so only θDM gradients require sanitization.
- Core assumption: Gradient clipping threshold C appropriately bounds sensitivity; noise scale σ provides sufficient privacy for the given ε.
- Evidence anchors:
  - [section 5.2] "DP-SGD works by applying gradient clipping and adding noise to the gradients during training, ensuring that the privacy of individual data points is preserved"
  - [section B] Theorem B.2 proves (α, α/2σ²)-RDP for the noisy gradient release mechanism.
- Break condition: If C is set too low, gradients lose directional information; if σ is too high, utility degrades.

## Foundational Learning

- Concept: Differential Privacy (ε, δ)-DP
  - Why needed here: Core guarantee that synthetic data doesn't reveal individual training samples.
  - Quick check question: Given ε=1, δ=10⁻⁵, can you explain why smaller ε means stronger privacy?

- Concept: Diffusion Models (Forward/Reverse Process)
  - Why needed here: Pub2Priv uses conditional diffusion as the generative backbone; understanding denoising is essential.
  - Quick check question: What does the denoiser ϵθ(xt, t) predict in Eq. (5)?

- Concept: Self-Attention and Positional Encodings
  - Why needed here: The knowledge transformer uses 2D self-attention with temporal/feature positional encodings.
  - Quick check question: Why does the model need separate positional encodings for time vs. feature dimensions?

## Architecture Onboarding

- Component map:
  - θT (Knowledge Transformer) -> θDM (Diffusion Model) -> Synthetic time series generation
  - Public metadata c -> θT(c) = z (embeddings) -> θDM(z, noise) = x'

- Critical path:
  1. Identify public metadata correlated with private data (domain expertise required).
  2. Pre-train θT on public time series corpus with masked reconstruction objective.
  3. Train θDM with DP-SGD on private (x, c) pairs, conditioning on z = θT(c).
  4. Generate synthetic x' ~ p(x|c) via reverse diffusion, using frozen θT.

- Design tradeoffs:
  - Public metadata richness vs. availability: Portfolio dataset benefits from high-dimensional market indices; Electricity dataset has limited public signals.
  - Privacy budget (ε) vs. utility: Fig. 3 shows TSTR improves as ε increases from 0.1 to 1.0.
  - Clipping threshold (C): Over-clipping hurts convergence; under-clipping inflates sensitivity, requiring more noise.

- Failure signatures:
  - High identifiability I(D, D'): Synthetic samples cluster near real data (Fig. 5). Check if ε is too large or gradient noise is insufficient.
  - Poor TSTR with good KSR/|Corrmeta|: Marginal statistics preserved but temporal dynamics lost—verify diffusion steps T and denoiser capacity.
  - θT ablation shows no degradation: Public metadata may be uninformative; consider richer sources or domain-specific features.

- First 3 experiments:
  1. Sanity check on public-private correlation: Compute Pearson correlation between private x and public c (Table 2 shows Portfolio: 0.573, Electricity: 0.386, Comtrade: 0.761). If <0.3, Pub2Priv may not help.
  2. Ablate θT: Run Pub2Priv w/o θT (feed raw c to θDM). If performance similar to full model, public metadata is underutilized.
  3. Privacy sweep: Train at ε ∈ {0.1, 0.5, 1.0, 5.0} with δ=10⁻⁵. Plot TSTR vs. identifiability I(D, D') to characterize privacy-utility frontier (Fig. 3, Fig. 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Large Language Models (LLMs) be effectively utilized to automate the identification and retrieval of heterogeneous public metadata, reducing the reliance on manual domain expertise?
- Basis in paper: [explicit] The authors state in the Limitations section that "identifying relevant public information still requires domain expertise" and suggest that "future work could harness the empirical knowledge embedded in large language models (LLMs) to automatically identify useful public datasets."
- Why unresolved: The current framework relies on manual selection of metadata (e.g., mapping weather to electricity), which limits the scalability and adaptability of the model to new domains where expert knowledge is scarce or expensive.
- What evidence would resolve it: A successful integration of an LLM-based agent that can autonomously propose high-utility public datasets for a given private domain, achieving comparable or superior TSTR scores to the manually curated baselines in the paper.

### Open Question 2
- Question: How can the Pub2Priv framework be extended to safely incorporate sensitive metadata as a conditional input without compromising the differential privacy guarantees?
- Basis in paper: [explicit] In the Discussion, the authors note that they exclusively consider non-sensitive contextual information to ensure safety, but acknowledge that "future research could explore additional mechanisms for incorporating sensitive metadata in a privacy-preserving manner."
- Why unresolved: The current architecture explicitly excludes sensitive features from the metadata c to avoid privacy leaks. There is no existing mechanism within Pub2Priv to sanitize or differentially privatize the metadata itself before it is processed by the knowledge transformer θT.
- What evidence would resolve it: A modified training objective or noise injection mechanism for the metadata encoder that allows for the inclusion of sensitive contextual features while providing a rigorous proof of the resulting privacy budget.

### Open Question 3
- Question: Can the proposed empirical identifiability metric I(D, D') be theoretically linked to the differential privacy parameters (ε, δ) to prevent "center-bias" privacy leaks?
- Basis in paper: [inferred] Section 6.4 observes that a lower ε does not guarantee low identifiability, citing cases where models generate synthetic data concentrated around the geometric center. The authors highlight the importance of evaluating empirical leakage "in addition to enforcing theoretical DP guarantees," implying a disconnect between the two.
- Why unresolved: While the paper introduces identifiability as a practical metric, it does not provide a theoretical bound or formal relationship explaining why strong DP guarantees sometimes fail to prevent high identifiability in generative models.
- What evidence would resolve it: A theoretical analysis establishing bounds on the expected identifiability E[I(D, D')] given specific diffusion model architectures and ε values, or a new regularization term that minimizes I(D, D') during training.

### Open Question 4
- Question: How does the required volume and dimensionality of public metadata scale with the complexity of the private data distribution to maintain utility?
- Basis in paper: [explicit] The authors note in Section 7 that "if the private data distribution becomes more complex (larger variety of strategies...), a larger amount of public metadata may be necessary," and observe that the model benefits more from richer metadata compared to lower-dimensional sources.
- Why unresolved: The paper demonstrates performance degradation as private data complexity increases (Figure 6), but does not characterize the precise trade-off or scaling laws regarding how much additional public data is required to offset this utility loss.
- What evidence would resolve it: A systematic study plotting TSTR utility against the "complexity" of the private data (e.g., number of distinct trading strategies) while varying the dimensionality of the public metadata to establish the saturation points.

## Limitations

- The framework requires domain expertise to identify relevant public metadata, limiting scalability to new domains where expert knowledge is scarce.
- Performance degrades as private data distribution complexity increases, requiring richer public metadata that may not always be available.
- The method excludes sensitive metadata features entirely to preserve privacy, potentially missing valuable conditioning signals.

## Confidence

- High: DP-SGD implementation correctness, basic diffusion model architecture, identifiability metric computation
- Medium: Public metadata conditioning effectiveness across domains, privacy-utility trade-off characterization, generalization to other time series domains
- Low: Exact architectural details for z-embedding injection, pre-training corpus specifications for θT, handling of metadata missingness or domain shift

## Next Checks

1. **Public-Private Correlation Sensitivity**: Systematically vary the correlation strength between private time series and public metadata (e.g., via controlled synthetic experiments) and measure the impact on KSR, KSAR, and TSTR metrics to isolate conditioning benefits from spurious correlations.

2. **Privacy Budget Sweep with Identifiability**: Train Pub2Priv across ε ∈ {0.1, 0.5, 1.0, 5.0} and plot both TSTR and identifiability I(D,D′) on the same graph to characterize the full privacy-utility frontier, confirming that I(D,D′) decreases monotonically with ε as claimed.

3. **θT Architecture Ablation**: Replace the dual-axis self-attention with alternative designs (e.g., standard 1D temporal attention, or no attention with simple concatenation) and measure degradation in |Corrmeta| and TSTR to quantify the specific contribution of the alternating attention mechanism versus simpler conditioning approaches.