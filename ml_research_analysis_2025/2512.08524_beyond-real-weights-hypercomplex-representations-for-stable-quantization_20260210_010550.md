---
ver: rpa2
title: 'Beyond Real Weights: Hypercomplex Representations for Stable Quantization'
arxiv_id: '2512.08524'
source_url: https://arxiv.org/abs/2512.08524
tags:
- dense
- multimodal
- while
- parameter
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a progressive reparameterization strategy
  that compresses multimodal language models by replacing dense feed-forward network
  blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual
  interpolation schedule, along with lightweight reconstruction and knowledge distillation
  losses, ensures that PHM modules inherit the functional behavior of their dense
  counterparts during training.
---

# Beyond Real Weights: Hypercomplex Representations for Stable Quantization

## Quick Facts
- arXiv ID: 2512.08524
- Source URL: https://arxiv.org/abs/2512.08524
- Reference count: 40
- Replaces dense feed-forward layers with compact Parameterized Hypercomplex Multiplication (PHM) layers to compress multimodal language models.

## Executive Summary
This work introduces a progressive reparameterization strategy that compresses multimodal language models by replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, along with lightweight reconstruction and knowledge distillation losses, ensures that PHM modules inherit the functional behavior of their dense counterparts during training. The approach maintains strong multimodal alignment while delivering substantial parameter and FLOP reductions, enabling faster inference without degrading output quality. Evaluated on multiple vision-language models, the method preserves performance comparable to base models while reducing parameters by 20–30% and improving latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning.

## Method Summary
The approach compresses multimodal models by progressively replacing dense feed-forward (FFN) layers with Parameterized Hypercomplex Multiplication (PHM) layers. During Stage A, a residual interpolation schedule blends dense and PHM weights while applying knowledge distillation and reconstruction losses to preserve teacher behavior. Stage B drops the dense branches and fine-tunes the PHM-only model. Selective capacity is used: B=3 for top-K language MLP layers, B=2 elsewhere, with LoRA applied to text-side Q/K/V projections. Initialization uses least-squares projection from dense weights. The method maintains strong multimodal alignment while achieving substantial parameter and FLOP reductions.

## Key Results
- Preserves performance comparable to base models while reducing parameters by 20–30%
- Improves inference latency without degrading output quality
- Maintains strong multimodal alignment across vision-language tasks
- Achieves efficient compression compatible with existing architectures

## Why This Works (Mechanism)
PHM layers compress dense weight matrices by representing them in hypercomplex algebras (quaternions, octonions), reducing storage while preserving representational power. The progressive reparameterization schedule allows smooth transition from dense to compact representations, preventing catastrophic forgetting. Knowledge distillation from the dense teacher ensures behavioral consistency during compression. Selective capacity allocation (B=3 for top layers, B=2 elsewhere) balances compression with task-critical performance. The two-stage training protocol first learns PHM behavior alongside dense weights, then refines PHM-only performance.

## Foundational Learning

**Hypercomplex multiplication (B parameter)**: Represents weight matrices using algebras like quaternions/octonions with B>1 divisions. Why needed: Enables substantial parameter compression while preserving expressive power. Quick check: Verify B correctly partitions dense weight matrices during initialization.

**Progressive reparameterization schedule**: Gradually transitions from dense to PHM weights using α(t) interpolation. Why needed: Prevents performance collapse during compression by allowing smooth adaptation. Quick check: Monitor validation loss during α ramp for stability.

**Knowledge distillation with reconstruction**: Uses teacher network outputs plus reconstruction loss during Stage A. Why needed: Preserves functional behavior while compressing weights. Quick check: Ensure KD loss decreases as α→1.

## Architecture Onboarding

**Component map**: Dense FFN layers → PHM layers (progressive) -> LoRA on Q/K/V -> Output

**Critical path**: W_dense → W_PHM initialization → α-schedule blending → KD/reconstruction training → Stage B fine-tuning

**Design tradeoffs**: Higher B values give more compression but risk accuracy loss; selective capacity allocation prioritizes critical layers. Failure signatures include sudden performance drops during α ramp or PHM-only under-performance indicating initialization issues.

**First experiments**:
1. Initialize a test layer and verify ||W_dense−W_PHM||_F is small
2. Run short α-fade schedule and check smooth CE/CIDEr curves
3. Profile FLOPs/latency on target hardware for compression validation

## Open Questions the Paper Calls Out
None

## Limitations
- Sensitive to hyperparameter choices (fade schedule, reconstruction weight, LoRA configuration)
- Two-stage training protocol requires careful schedule design
- Performance preservation depends on initialization fidelity
- Compression efficacy varies with model architecture and task

## Confidence
- Compression efficacy claim: Medium (depends on unknown hyperparameters)
- PHM substitution feasibility: High (methodologically sound)
- Performance preservation: Credible with proper tuning, not guaranteed

## Next Checks
1. Replicate initialization error (||W_dense - W_PHM||_F) for a small test layer
2. Run short α-fade schedule and verify smooth CE/CIDEr curves
3. Profile FLOPs and latency on same hardware to confirm efficiency gains