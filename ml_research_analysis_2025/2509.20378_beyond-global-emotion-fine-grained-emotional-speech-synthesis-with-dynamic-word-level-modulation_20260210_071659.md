---
ver: rpa2
title: 'Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic
  Word-Level Modulation'
arxiv_id: '2509.20378'
source_url: https://arxiv.org/abs/2509.20378
tags:
- emotion
- speech
- emotional
- fine-grained
- emo-film
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Emo-FiLM addresses the problem of global-only emotion control in
  TTS by introducing fine-grained word-level emotion modulation using FiLM-based affine
  transformation. The approach aligns frame-level emotion features to words via emotion2vec
  and modulates text embeddings with learned scale/shift parameters, enabling dynamic
  emotional transitions within sentences.
---

# Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation

## Quick Facts
- **arXiv ID:** 2509.20378
- **Source URL:** https://arxiv.org/abs/2509.20378
- **Reference count:** 0
- **Primary result:** Emo-FiLM achieves 98.78% Emo SIM and 23.98 DTW on ESD, outperforming baselines through fine-grained word-level emotion modulation.

## Executive Summary
Emo-FiLM addresses the limitation of global-only emotion control in TTS by introducing fine-grained word-level emotion modulation. The approach uses emotion2vec to extract frame-level features, aligns them to words via MFA, and aggregates them into word-level emotion labels. These labels are then used to modulate text embeddings through a FiLM-based affine transformation, enabling dynamic emotional transitions within sentences. On ESD, Emo-FiLM achieves 98.78% Emo SIM and 23.98 DTW, outperforming baselines. On FEDD, it improves to 99.32% Emo SIM and 49.62 DTW, with higher subjective ratings for emotion similarity and naturalness. Ablation studies confirm the FiLM layer and word-level supervision are critical components.

## Method Summary
Emo-FiLM introduces a word-level emotion annotation pipeline using emotion2vec and MFA to create fine-grained training data, then builds on a frozen LLM backbone (CosyVoice2) by adding an E-FiLM module that modulates text embeddings with emotion-dependent affine parameters. Training uses a multi-task objective combining TTS loss with auxiliary word-level emotion classification. The method generates pseudo-labels on training corpora and fine-tunes only the E-FiLM and decoding head while freezing the LLM.

## Key Results
- Emo-FiLM achieves 98.78% Emo SIM and 23.98 DTW on ESD, outperforming baselines
- On FEDD, improves to 99.32% Emo SIM and 49.62 DTW with higher subjective ratings
- Ablation shows FiLM layer and word-level supervision are critical, with simple addition performing significantly worse

## Why This Works (Mechanism)

### Mechanism 1: Fine-grained word-level emotion annotation
- **Core assumption:** Frame-level emotion features from emotion2vec correlate with semantic word units, enabling meaningful localization of emotional prosody
- **Evidence anchors:** Abstract mentions frame-level features aligned to words via emotion2vec; corpus papers support shift toward granular emotion control
- **Break condition:** Poor forced alignment or global prosodic features spanning phrases would create noisy word-level labels

### Mechanism 2: FiLM-based affine transformation
- **Core assumption:** Text embedding space has linear subspaces that can be re-weighted and shifted to represent different emotional prosodies
- **Evidence anchors:** Abstract states FiLM enables word-level control by modulating text embeddings; ablation shows replacing FiLM with addition severely hurts performance
- **Break condition:** Overly aggressive modulation could push embeddings outside LLM's distribution, causing gibberish output

### Mechanism 3: Auxiliary word-level emotion classification loss
- **Core assumption:** Pre-trained LLM backbone needs explicit word-level regularization to learn fine-grained emotion alignment
- **Evidence anchors:** Section 2.2 describes step-wise cross-entropy loss at each time step; ablation shows removing emotion loss causes large performance drops
- **Break condition:** Inaccurate pseudo-labels would provide conflicting training signals, potentially reducing performance

## Foundational Learning

- **Feature-wise Linear Modulation (FiLM)**
  - Why needed: Core contribution for injecting emotion signal into LLM's embedding space
  - Quick check: Given emotion embedding $e$, how derive $\gamma$ and $\beta$ to modulate text embedding $h$? Complete equation?

- **Forced Alignment (MFA)**
  - Why needed: Critical for generating fine-grained training data by aligning audio frames to text words
  - Quick check: What is MFA output and how used to aggregate frame-level emotion2vec features into word-level features?

- **Self-supervised Speech Emotion Representation (emotion2vec)**
  - Why needed: Quality of word-level emotion labels depends entirely on quality of frame-level features from this model
  - Quick check: What kind of model is emotion2vec and what features does it extract from raw audio?

## Architecture Onboarding

- **Component map:** Offline pipeline: emotion2vec -> MFA -> Masked Average Pooling -> Lightweight Transformer (Classifier/Regressor) -> Word-level labels. Online TTS: LLM Tokenizer -> Text Embeddings -> Emotion Encoder -> **FiLM Layer** -> Modulated Text Embeddings -> LLM Decoder -> Flow Matching -> HiFi-GAN Vocoder

- **Critical path:** Word-level Emotion Label -> Emotion Encoder -> FiLM Layer -> Modulated Text Embedding -> LLM Decoder

- **Design tradeoffs:**
  - Freezing LLM backbone reduces training cost but limits adaptation to emotion
  - Pseudo-labeling is scalable but introduces noise compared to human annotation
  - FiLM is more expressive than addition but introduces more parameters

- **Failure signatures:**
  - Flat Prosody: FiLM outputs near-identity parameters ($\gamma \approx 1, \beta \approx 0$)
  - Catastrophic Forgetting: Overly strong modulation increases WER significantly
  - Noisy Transitions: Poor word-level labels cause erratic emotion transitions
  - High DTW, Low WER: Intelligible speech but fails to capture emotional dynamics

- **First 3 experiments:**
  1. Reproduce FiLM ablation: Compare DTW scores between full FiLM layer and simple addition
  2. Assess label noise impact: Compare performance using automatic vs. manually corrected word-level labels
  3. Visualize FiLM parameters: Extract and cluster $\gamma$ and $\beta$ parameters to verify structured emotion space

## Open Questions the Paper Calls Out
- **Cross-lingual generalization:** How well does Emo-FiLM perform on languages beyond English given experiments were only on English ESD?
- **Pseudo-label quality:** How robust are the automatic word-level emotion labels and what is error propagation to synthesis quality?
- **Complex emotion taxonomies:** Can Emo-FiLM handle more nuanced emotion distinctions beyond 5 basic emotions?
- **Streaming capability:** Does FiLM-based approach support real-time synthesis for interactive dialogue systems?

## Limitations
- Performance relies heavily on quality of emotion2vec + MFA pseudo-labels with no quantitative assessment of label noise
- Several critical hyperparameters are unspecified, affecting reproducibility and performance
- FEDD dataset is not publicly available, limiting independent verification of fine-grained control claims
- Results are reported only on English datasets with no evidence of cross-lingual performance

## Confidence
- **High Confidence:** Core architectural claims (FiLM modulation, word-level alignment, auxiliary loss) are well-described and supported by ablation studies
- **Medium Confidence:** Quantitative results appear internally consistent but confidence limited by unknown hyperparameters and lack of alternative approach comparisons
- **Low Confidence:** Fine-grained transition claims rely heavily on constructed FEDD dataset without clear access or construction criteria

## Next Checks
1. Conduct hyperparameter sensitivity analysis varying Î» (emotion loss weight), learning rate, and label smoothing to determine impact on DTW and Emo SIM scores
2. Test label noise robustness by comparing models using automatic emotion2vec labels versus manually corrected word-level labels
3. Evaluate cross-dataset generalization by testing trained model on different emotional TTS dataset (different language or speaker demographic)