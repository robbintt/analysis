---
ver: rpa2
title: 'Attribution Explanations for Deep Neural Networks: A Theoretical Perspective'
arxiv_id: '2508.07636'
source_url: https://arxiv.org/abs/2508.07636
tags:
- attribution
- methods
- theoretical
- input
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive review of recent theoretical
  advancements in attribution explanations for deep neural networks. The authors organize
  these developments into three key dimensions: theoretical unification, which reveals
  shared structures across diverse attribution methods; theoretical rationale, which
  clarifies the mechanisms justifying these methods; and theoretical evaluation, which
  rigorously assesses whether methods satisfy faithfulness principles.'
---

# Attribution Explanations for Deep Neural Networks: A Theoretical Perspective

## Quick Facts
- **arXiv ID:** 2508.07636
- **Source URL:** https://arxiv.org/abs/2508.07636
- **Reference count:** 40
- **Primary result:** Comprehensive survey unifying and theoretically evaluating attribution methods for deep neural networks

## Executive Summary
This survey provides a systematic review of theoretical advancements in attribution explanations for deep neural networks. The authors organize recent developments across three dimensions: theoretical unification (revealing shared structures across diverse methods), theoretical rationale (clarifying mechanisms justifying methods), and theoretical evaluation (rigorously assessing faithfulness). By synthesizing fragmented theoretical efforts, the work bridges empirical evaluation challenges with the need for principled foundations. The survey offers valuable insights for understanding attribution methods, informing selection, and inspiring new techniques that satisfy theoretical guarantees.

## Method Summary
The survey method involves mathematically reformulating diverse attribution techniques into unified frameworks, then applying theoretical evaluation principles to assess their faithfulness. The approach maps methods to reformulation families (e.g., Taylor Interaction, Modified Gradient×Input) and evaluates them against axiomatic principles like Output Sensitivity and Parameter Sensitivity. No training is performed; instead, the method relies on mathematical derivation and sanity checks (e.g., parameter randomization tests) to verify theoretical claims. The survey synthesizes existing proofs and establishes connections between seemingly disparate attribution approaches.

## Key Results
- Attribution methods can be unified under shared mathematical frameworks, with differences primarily in allocation weights for independent versus interaction effects
- Nonnegative Backpropagation family methods (LRP, DTD) violate sensitivity principles, proven by failing parameter randomization sanity checks
- Shapley values provide the unique game-theoretic solution satisfying key axioms (Efficiency, Symmetry, Linearity) for feature attribution

## Why This Works (Mechanism)

### Mechanism 1: Reformulation-Based Unification
- Claim: Diverse attribution methods can be reformulated under shared mathematical frameworks, revealing they differ primarily in allocation weights rather than fundamental theoretical differences
- Mechanism: Methods like Grad×Input, Integrated Gradients, and LRP are mapped to unified Taylor expansion forms, showing they compute attributions by weighting independent effects $\phi(j)$ and interaction effects $I(S)$
- Core assumption: Multivariate Taylor expansion provides valid decomposition of model decision process
- Evidence anchors: Proven that 14 popular methods unify within Taylor interaction family; Table III shows allocation weight as key difference
- Break condition: Fails if models contain non-smooth discontinuities that invalidate Taylor expansion

### Mechanism 2: Axiomatic Falsification (Sensitivity)
- Claim: Methods generating consistent explanations under parameter randomization violate Parameter Sensitivity and are theoretically unfaithful
- Mechanism: Acts as sanity check requiring attributions to change when model changes; Nonnegative BP methods fail as attribution flow converges to fixed direction
- Core assumption: Faithful explanations must depend on specific trained model structure
- Evidence anchors: Theoretical analysis shows Nonnegative BP family violates sensitivity principles; Table VI lists methods violating Output/Parameter Sensitivity
- Break condition: May incorrectly flag faithful methods if networks are regularized for invariant attributions

### Mechanism 3: Game-Theoretic Value Allocation
- Claim: Shapley values provide unique solution satisfying specific axioms, offering theoretically rationale baseline for contribution
- Mechanism: Treats model as value function where Shapley value calculates marginal contribution by averaging across all feature coalition orderings
- Core assumption: Cooperative game axioms accurately reflect semantic contribution accumulation
- Evidence anchors: Shapley value is unique method satisfying linearity, dummy, symmetry, and efficiency axioms
- Break condition: Computationally fails if ground truth requires exponential queries, forcing approximations that violate axioms

## Foundational Learning

- **Concept: Multivariate Taylor Expansion**
  - Why needed here: Mathematical bedrock for theoretical unification section
  - Quick check question: Can you explain why first-order Taylor expansion might fail to capture interaction effects between two input pixels?

- **Concept: Axiomatic Evaluation (Sanity Checks)**
  - Why needed here: Paper relies on theoretical evaluation principles to dismiss certain methods
  - Quick check question: If attribution map for "Cat" class doesn't change when replacing final layer weights with random noise, which theoretical principle is violated?

- **Concept: Cooperative Game Theory (Shapley Values)**
  - Why needed here: Gold standard for theoretical rationale, connecting attribution to well-established mathematical field
  - Quick check question: Why does Shapley value require averaging over all possible coalitions rather than just measuring effect of removing feature alone?

## Architecture Onboarding

- **Component map:** Reformulation -> Classification -> Verification (unified form → family assignment → family-specific evaluation)

- **Critical path:**
  1. Reformulate target attribution method in unified form (e.g., Eq. 10 or 16)
  2. Classify method to family (e.g., Modified Gradient or Taylor Interaction)
  3. Apply family-specific theoretical evaluation principles (e.g., check for Rank-1 convergence)

- **Design tradeoffs:**
  - Unification vs. Specificity: Grouping methods helps evaluation but may obscure practical performance details
  - Axiomatic Rigor vs. Computability: Shapley values are gold standard but intractable; approximations trade guarantees for speed
  - Faithfulness vs. Robustness: Methods satisfying faithfulness axioms may still be fragile to input perturbations

- **Failure signatures:**
  - Rank-1 Convergence: Nonnegative BP attributions look like edge-detectors regardless of class label
  - Input Reconstruction: Gradient methods produce input-like images rather than class-specific maps
  - Allocation Leakage: Taylor family methods assign interaction effects to uninvolved variables

- **First 3 experiments:**
  1. Parameter Sensitivity Stress Test: Implement randomization test, progressively randomize layers, plot attribution correlation
  2. Unification Verification: Verify Grad×Input and Integrated Gradients fit Modified GI formulation by checking derivative rules/baseline differences
  3. Interaction Allocation Audit: Manually decompose synthetic network output, check if Occ-1 incorrectly assigns full interaction term while Shapley distributes evenly

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can sufficient conditions be formulated to verify faithfulness of attribution methods, moving beyond current paradigm of falsifying unfaithful ones?
- **Basis in paper:** [explicit] Authors state current evaluations focus on "falsifiability," serving as necessary but not sufficient conditions, calling for work to advance towards verifiability
- **Why unresolved:** No existing principle guarantees satisfying axioms ensures explanation reflects true decision logic
- **What evidence would resolve it:** Derivation of rigorous, sufficient theoretical conditions that mathematically guarantee faithfulness to model's reasoning process

### Open Question 2
- **Question:** How can theoretical evaluation frameworks be expanded to support comparisons across different attribution families and cover under-evaluated paradigms?
- **Basis in paper:** [explicit] Section VI-B notes evaluations are "confined to limited number of attribution families," calling for "cross-family evaluation metrics"
- **Why unresolved:** Different families rely on distinct formulations, making unified theoretical comparison difficult
- **What evidence would resolve it:** Development of unified theoretical metric capable of assessing faithfulness properties across diverse paradigms

### Open Question 3
- **Question:** How can attribution methods be theoretically adapted to align with specific internal structures of Transformers and domain constraints of scientific applications?
- **Basis in paper:** [explicit] Section VI-D highlights need for "context-aware attribution," noting Grad-CAM fails on Transformers due to lack of spatial hierarchies
- **Why unresolved:** Standard attribution theories assume spatial continuity or instance-specific importance, failing to account for attention mechanisms or structural constraints
- **What evidence would resolve it:** Formulation of attribution principles grounded in attention mechanisms or physical constraints that provide theoretical guarantees within specific architectures

## Limitations
- Theoretical unification assumes smooth, differentiable models that may not hold for networks with non-differentiable operations
- Axiomatic evaluations rely on idealized assumptions about model behavior that may not translate to complex real-world networks
- Computational intractability of Shapley values forces approximations that may violate theoretical guarantees

## Confidence
- **High Confidence:** Theoretical unification framework and core mathematical derivations are well-established and rigorously proven
- **Medium Confidence:** Family classifications and associated theoretical properties are derived from cited works with potential interpretative nuances
- **Medium Confidence:** Practical implications for method selection and failure modes are logically sound but may require empirical validation

## Next Checks
1. Cross-Architecture Sensitivity Test: Implement Parameter Sensitivity sanity check across multiple architectures to verify theoretical predictions about family-specific failures
2. Computational Trade-off Analysis: Quantify computational cost of Shapley value approximations versus theoretical guarantees to validate Complexity-Faithfulness trade-off
3. Robustness Evaluation: Test whether methods satisfying faithfulness axioms demonstrate superior robustness to input perturbations compared to non-axiomatic methods