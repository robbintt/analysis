---
ver: rpa2
title: 'Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions'
arxiv_id: '2512.02682'
source_url: https://arxiv.org/abs/2512.02682
tags:
- agents
- safety
- risks
- interaction
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that single-agent safety mechanisms fail in
  multi-LLM ecosystems because local alignment does not prevent emergent collective
  risks. It proposes the Emergent Systemic Risk Horizon (ESRH) framework, which formalizes
  how systemic instability arises from interaction structure, not isolated misbehavior.
---

# Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions

## Quick Facts
- arXiv ID: 2512.02682
- Source URL: https://arxiv.org/abs/2512.02682
- Reference count: 12
- Key outcome: Single-agent safety mechanisms fail in multi-LLM ecosystems because local alignment does not prevent emergent collective risks

## Executive Summary
This paper addresses the critical gap in AI safety research that focuses on individual model alignment while ignoring risks that emerge when multiple LLMs interact. The authors demonstrate that existing safety measures designed for single-agent systems are insufficient for multi-agent ecosystems, where collective behaviors can produce novel risks through semantic drift, coordination failures, and potential collusion. To address this challenge, they propose the Emergent Systemic Risk Horizon (ESRH) framework, which formalizes how systemic instability arises from interaction structures rather than isolated misbehavior. The work introduces Institutional AI, a governance architecture that embeds adaptive oversight, peer evaluation, and functional differentiation directly within multi-agent systems to enhance system-level resilience.

## Method Summary
The paper presents a theoretical framework combining systems theory with AI safety research to analyze multi-agent LLM interactions. The authors develop the ESRH framework through conceptual analysis and formalize a three-tier taxonomy of risks (micro, meso, and macro levels). They propose Institutional AI as a solution architecture, drawing parallels with organizational theory and institutional design. While the work is primarily conceptual and theoretical, it provides a structured approach to understanding and mitigating emergent risks in LLM-to-LLM interactions through governance mechanisms embedded within the system itself.

## Key Results
- Single-agent safety mechanisms fail in multi-LLM ecosystems because local alignment does not prevent emergent collective risks
- The ESRH framework formalizes how systemic instability arises from interaction structure, not isolated misbehavior
- A three-tier taxonomy classifies risks as micro (semantic drift, prompt infection), meso (coordination failure, false consensus), and macro (miscoordination, collusion)
- Institutional AI, a governance architecture embedding adaptive oversight, peer evaluation, and functional differentiation, addresses these systemic risks

## Why This Works (Mechanism)
The paper argues that systemic risks emerge when LLMs interact because each model's local safety measures cannot account for the collective dynamics that arise from multiple agents exchanging information. The ESRH framework captures this by focusing on interaction structures rather than individual model behavior. When LLMs communicate, they can propagate subtle errors, biases, or unsafe behaviors that compound over time through semantic drift and prompt infection at the micro level, coordination failures at the meso level, and eventually collusion or miscoordination at the macro level. Institutional AI works by embedding governance mechanisms that monitor and regulate these interactions in real-time, preventing the accumulation of systemic risks through adaptive oversight and functional differentiation.

## Foundational Learning
**Emergent Systemic Risk Horizon (ESRH)**: A framework for analyzing how risks propagate through multi-agent systems through interaction structures rather than individual model failures. *Why needed*: Single-agent safety analysis cannot capture collective behaviors. *Quick check*: Does the framework identify risk propagation paths in your multi-agent system?

**Three-tier risk taxonomy**: Classification of risks into micro (semantic drift, prompt infection), meso (coordination failure, false consensus), and macro (miscoordination, collusion) levels. *Why needed*: Provides structured approach to understanding different scales of emergent risks. *Quick check*: Can you map observed risks in your system to these three categories?

**Institutional AI**: Governance architecture embedding oversight, peer evaluation, and functional differentiation within multi-agent systems. *Why needed*: Traditional safety mechanisms cannot address system-level risks. *Quick check*: Does your system have mechanisms for real-time monitoring and regulation of agent interactions?

## Architecture Onboarding

**Component map**: Institutional AI -> Adaptive Oversight -> Peer Evaluation -> Functional Differentiation -> System Stability

**Critical path**: Interaction monitoring → Risk detection → Governance intervention → System stabilization

**Design tradeoffs**: Centralized vs. distributed oversight (efficiency vs. resilience), strict vs. adaptive governance (predictability vs. flexibility), homogeneous vs. differentiated roles (simplicity vs. robustness)

**Failure signatures**: Cascading semantic drift across agents, coordination deadlocks, false consensus formation, emergent collusion patterns

**First experiments**: 1) Test adaptive oversight in detecting semantic drift over 10 interaction rounds; 2) Compare centralized vs. distributed governance in preventing coordination failures; 3) Measure effectiveness of functional differentiation in preventing prompt infection spread

## Open Questions the Paper Calls Out
None

## Limitations
- The ESRH framework and Institutional AI concepts are largely theoretical with limited empirical validation
- Boundaries between micro, meso, and macro risk levels could benefit from more precise operational definitions
- Claims about systemic instability would benefit from more quantitative evidence in real multi-agent systems

## Confidence

**High confidence**: The observation that single-agent safety mechanisms fail in multi-LLM ecosystems; the need for system-level safety analysis beyond individual model alignment

**Medium confidence**: The ESRH framework's conceptual validity; the three-tier taxonomy as a useful classification scheme

**Low confidence**: Specific predictions about how Institutional AI would perform in practice; the relative prevalence and impact of different risk categories without empirical data

## Next Checks

1. Design controlled experiments testing how different interaction structures (hierarchical vs. peer-to-peer vs. star topology) affect emergent risk propagation in multi-LLM systems
2. Implement a prototype Institutional AI system and measure its effectiveness at preventing semantic drift and prompt infection compared to baseline approaches
3. Conduct longitudinal studies tracking how safety violations compound over multiple interaction rounds, quantifying the "systemic instability" described in the ESRH framework