---
ver: rpa2
title: A methodological analysis of prompt perturbations and their effect on attack
  success rates
arxiv_id: '2511.10686'
source_url: https://arxiv.org/abs/2511.10686
tags:
- 'true'
- prompt
- 'false'
- attack
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how different alignment methods (SFT, DPO,
  RLHF) affect Large Language Models' susceptibility to prompt attacks, revealing
  that even minor prompt modifications can significantly alter attack success rates.
  The study uses systematic statistical analysis to evaluate model responses to various
  attack types (Best-of-N, Base64 encoding, adversarial tokens, emotional blackmail)
  with and without alignment prompts and instruction templates.
---

# A methodological analysis of prompt perturbations and their effect on attack success rates

## Quick Facts
- arXiv ID: 2511.10686
- Source URL: https://arxiv.org/abs/2511.10686
- Reference count: 12
- Key outcome: Even minor prompt modifications can significantly alter attack success rates, with SFT models being particularly vulnerable to instruction templates

## Executive Summary
This study systematically investigates how different alignment methods (SFT, DPO, RLHF) affect Large Language Models' susceptibility to prompt attacks, revealing that ASR is not invariant to prompt variations. The research uses statistical analysis to evaluate model responses to various attack types (Best-of-N, Base64 encoding, adversarial tokens, emotional blackmail) with and without alignment prompts and instruction templates. Results show statistically significant differences in attack success rates across alignment methods and prompt configurations, demonstrating that existing "attack benchmarks" alone may not fully capture model vulnerabilities.

## Method Summary
The study evaluates 7 models (Llama-2-7b, Llama-3-8B, Vicuna-7b, Zephyr-7b SFT/DPO, Mistral-7b variants) using 2^3 combinations of: 1) Alignment Prompt (On/Off), 2) Instruction Template (On/Off: "Sure, here is..."), 3) Attack Type (Base64, Adversarial Tokens, Best-of-N, Emotional Blackmail). Attack success rates are derived from MD-Judge-v0-2-internlm2-7b scores (1-5 scale, 3-5 considered unsafe) and statistical significance is measured using Mann-Whitney U test (95% CI) on AdvBench dataset (520 samples).

## Key Results
- Instruction templates that prescribe affirmative response prefixes can statistically significantly increase ASR across all tested alignment methods
- SFT-aligned models exhibit significantly higher ASR variance and absolute values compared to RLHF and DPO methods under identical prompt configurations
- ASR varies significantly across attack types depending on the presence of alignment prompts and instruction templates, indicating no universal attack ranking exists

## Why This Works (Mechanism)

### Mechanism 1: Instruction Template Priming Effect
- **Claim:** Instruction templates that prescribe affirmative response prefixes ("Sure, here is a...") can statistically significantly increase ASR across all tested alignment methods.
- **Mechanism:** The template establishes a compliance pattern that conditions the model's initial token generation toward harmful content completion, partially overriding safety training.
- **Core assumption:** Safety alignment is partially encoded in early-token refusal patterns that can be disrupted by response-formatting instructions.
- **Evidence anchors:**
  - [Table 2]: Instruction template presence/absence produced statistically significant ASR differences in 6/7 models for Base64 attacks (e.g., ZephyrSFT: 96.15% → 0%, p = 2.147e-209)
  - [Methods - Instruction template]: "This technique has been explored as a way to induce the model to provide the harmful content by first complying with a seemingly innocuous prefix"
  - [corpus]: Related work on "shallow alignment" (Xiangyu et al. 2024) suggests alignment primarily affects initial output tokens, supporting this vulnerability

### Mechanism 2: Alignment Method Vulnerability Differential
- **Claim:** SFT-aligned models exhibit significantly higher ASR variance and absolute values compared to RLHF and DPO methods under identical prompt configurations.
- **Mechanism:** SFT optimizes for pattern matching from demonstration data without explicit reward modeling for refusal behavior, creating shallower safety boundaries. RLHF/DPO incorporate explicit preference signals distinguishing harmful from safe outputs.
- **Core assumption:** Preference-based alignment (RLHF/DPO) creates more robust internal representations of safety boundaries than supervised pattern replication.
- **Evidence anchors:**
  - [Table 1]: SFT models (ZephyrSFT: 96.15%/92.88%, Mistral-CAI: 84.8%/84.03%) show consistently high ASR vs. RLHF models (Llama2: 22.11%/31.15%, Llama3: 1.53%/6.53%)
  - [Results]: "Prompt modifications on SFT-based models can reduce the safe output production by a factor greater than 2"
  - [Figure 2]: Direct comparison shows ZephyrSFT highly vulnerable while ZephyrDPO largely refuses attacks under identical configurations

### Mechanism 3: Attack-Type Specificity to Prompt Configuration
- **Claim:** ASR varies significantly across attack types (Base64, adversarial tokens, emotional blackmail, Best-of-N) depending on the presence of alignment prompts and instruction templates, indicating no universal attack ranking exists.
- **Mechanism:** Different attacks exploit different model vulnerabilities—encoding attacks bypass token-level filters, emotional appeals exploit context-sensitivity, BoN exploits stochastic sampling—each interacting differently with prompt structure elements.
- **Core assumption:** Model safety is not a single-dimensional property but a surface with attack-specific weaknesses.
- **Evidence anchors:**
  - [Results - Experiment 1]: All models showed statistically significant ASR differences when varying prompt elements across attack types
  - [Discussion]: "It shows the dangers of measuring it uni-dimensionally as much of the literature cited in the background section does"

## Foundational Learning

- **Concept: Attack Success Rate (ASR)**
  - **Why needed here:** Central metric for all experiments; measures proportion of prompts that successfully elicit harmful outputs
  - **Quick check question:** If a model has 85% ASR on 520 prompts, how many harmful responses were generated? (Answer: 442)

- **Concept: Mann-Whitney U Test**
  - **Why needed here:** Statistical test used throughout to determine if ASR differences are significant rather than due to chance
  - **Quick check question:** A p-value of 0.01 with CI=95% means what about the difference between two configurations? (Answer: The difference is statistically significant; we reject the null hypothesis that distributions are identical)

- **Concept: Alignment Method Taxonomy (SFT/DPO/RLHF)**
  - **Why needed here:** The three primary alignment approaches compared; each has distinct training objectives affecting safety robustness
  - **Quick check question:** Which alignment method directly optimizes a reward model from human preferences without reinforcement learning iterations? (Answer: DPO)

## Architecture Onboarding

- **Component map:**
AdvBench Dataset (520 harmful prompts) → Prompt Configuration Layer (Alignment Prompt On/Off, Instruction Template On/Off, Attack Type) → Target Model (7 models: SFT/DPO/RLHF variants) → Judge Model (MD-Judge-v0-2-internlm2-7b → Likert 1-5) → Statistical Analysis (Mann-Whitney, CI=95%) → ASR Calculation & Comparison

- **Critical path:**
  1. Select prompt configuration (3 binary choices × 4 attack types = 12+ configurations per model)
  2. Generate responses from target model
  3. Classify with judge model (threshold: scores 3-5 = unsafe)
  4. Compute ASR and run Mann-Whitney to compare distributions

- **Design tradeoffs:**
  - **Judge model selection:** MD-Judge (1-5 scale) vs. LlamaGuard (binary)—paper used both but prefers ordinal for granularity
  - **Model size limit:** 8B parameters (fits single A100) vs. larger models with potentially different alignment properties
  - **Single dataset (AdvBench):** Limits generalizability; authors acknowledge this as a limitation

- **Failure signatures:**
  - **High ASR variance:** ASR swinging from ~0% to >80% based on template presence indicates fragile alignment
  - **Non-significant p-values across configurations:** Suggests either robust alignment or ineffective attack variations
  - **Judge model hallucinations:** LlamaGuard produced unusable responses requiring exclusion

- **First 3 experiments:**
  1. **Baseline calibration:** Run a single model (e.g., Llama-2-7b-chat-hf) through all 12+ configurations on a 50-prompt subset to establish ASR range and identify which attack/configuration combinations produce the widest variance
  2. **Within-method stability test:** Compare one model against itself varying only instruction template presence across all attack types; verify statistical significance with Mann-Whitney (expect significant differences per paper findings)
  3. **Cross-method comparison:** Select one attack type (e.g., Base64), run identical configuration on an SFT model (ZephyrSFT) and its DPO counterpart (ZephyrDPO), compute ASR difference and significance; this directly tests whether alignment method affects vulnerability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed sensitivities of Attack Success Rate (ASR) to prompt perturbations generalize to models significantly larger than 8 billion parameters?
- **Basis:** [explicit] The authors state, "We limited our model choices to those with no more than eight billion parameters."
- **Why unresolved:** The study focuses on smaller, accessible models, leaving the safety robustness of larger frontier models against these specific statistical variances unverified.
- **What evidence would resolve it:** Replicating the statistical analysis on instruction-tuned models with 70 billion or more parameters.

### Open Question 2
- **Question:** Does the statistical variability of ASR persist when evaluated against attack datasets other than AdvBench?
- **Basis:** [explicit] "Another limitation is that we are using only one dataset for prompt attacks (AdvBench)."
- **Why unresolved:** AdvBench may possess specific linguistic properties that interact uniquely with the perturbations; it is unclear if other harm taxonomies would yield similar variance.
- **What evidence would resolve it:** Running the full experiment protocol on alternative benchmarks such as HarmBench or EasyJailbreak.

### Open Question 3
- **Question:** How can a multidimensional ASR metric be formulated to statistically account for variance across different prompt configurations?
- **Basis:** [explicit] Future work intends to "create a multidimensional ASR metric that can account for different prompt configurations."
- **Why unresolved:** The authors demonstrate that uni-dimensional ASR is unreliable, but they do not propose a concrete statistical replacement that aggregates these fluctuations.
- **What evidence would resolve it:** A formalized metric that incorporates confidence intervals or aggregate scores derived from multiple prompt configuration trials.

## Limitations

- **Dataset Representativeness:** The study uses only the AdvBench dataset, which may not capture the full diversity of harmful prompts or attack patterns.
- **Model Size Constraints:** All experiments were conducted on models with ≤8B parameters; findings may not extrapolate to frontier models with different architectural properties.
- **Judge Model Reliability:** The study relies on MD-Judge-v0-2-internlm2-7b for safety classification, and while authors report high agreement with human annotations, potential for judge model hallucinations or biases introduces uncertainty into ASR calculations.

## Confidence

**High Confidence:**
- Instruction templates significantly increase ASR across multiple attack types and models (p-values << 0.05, large effect sizes observed)
- Differential vulnerability between SFT and RLHF/DPO alignment methods (consistent patterns across multiple models)
- Overall methodological contribution that ASR is not invariant to prompt variations

**Medium Confidence:**
- Specific ranking of attack type effectiveness (e.g., Base64 vs. Adversarial Tokens vs. Emotional Blackmail)
- Quantitative magnitude of ASR changes (e.g., "factor greater than 2" reductions in safe output production)
- Generalizability of findings to other datasets or larger models

**Low Confidence:**
- Predictions about how these findings would apply to frontier models (GPT-4 class)
- Stability of results across different judge models or human evaluation
- Long-term robustness of identified vulnerabilities against future alignment improvements

## Next Checks

1. **Dataset Generalization Test:** Reproduce the analysis using a different harmful prompts dataset (e.g., WildChat, HarmBench) to verify whether the observed prompt configuration effects hold across datasets.

2. **Model Size Scaling Investigation:** Test whether the identified vulnerabilities persist in larger models (13B-70B parameters) using the same methodology, starting with a single attack type (e.g., Base64) and configuration on one SFT and one RLHF model at each size tier.

3. **Judge Model Cross-Validation:** Compare MD-Judge results against human annotations for a stratified sample of 100-200 outputs across different configurations and attack types to establish confidence bounds on the reported ASR values.