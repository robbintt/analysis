---
ver: rpa2
title: 'ALIGN-FL: Architecture-independent Learning through Invariant Generative component
  sharing in Federated Learning'
arxiv_id: '2512.13316'
source_url: https://arxiv.org/abs/2512.13316
tags:
- data
- privacy
- learning
- training
- align-fl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALIGN-FL addresses federated learning across non-overlapping data
  distributions by sharing only generative model components rather than full parameters.
  The approach enables knowledge transfer between clients with completely disjoint
  data through selective sharing of VAE decoders, while the server trains on synthetic
  data generated from these components.
---

# ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning

## Quick Facts
- arXiv ID: 2512.13316
- Source URL: https://arxiv.org/abs/2512.13316
- Authors: Mayank Gulati, Benedikt Groß, Gerhard Wunder
- Reference count: 31
- One-line result: Enables federated learning across clients with completely disjoint data distributions through decoder-only VAE transfer, achieving 74.25% accuracy vs. FedAvg's 43.75% on extreme Non-IID MNIST

## Executive Summary
ALIGN-FL addresses the fundamental challenge of federated learning across clients with non-overlapping data distributions by sharing only generative model components rather than full parameters. The approach transfers VAE decoders from clients to a central server, which generates synthetic data to train a global model without requiring parameter averaging. Two privacy mechanisms are explored: DP-SGD with adaptive clipping for formal (ε, δ)-DP guarantees, and Lipschitz regularization of VAE decoders for implicit privacy. Experimental results on MNIST and Fashion-MNIST show significant improvements over traditional FL methods, with FID scores of 81.91-98.46, accuracy of 74.25-63.25%, and F1-scores of 74.06-60.01%.

## Method Summary
ALIGN-FL operates through a novel generative component sharing mechanism where each client trains a local VAE on their non-overlapping data partition, then uploads only the decoder parameters to a central server. The server generates synthetic data samples from these decoders, aggregates them into a combined dataset, and trains a global model on this synthetic data. This architecture-independent approach preserves client model diversity while enabling knowledge transfer across completely disjoint distributions. Privacy is enforced through either full-architecture DP-SGD with adaptive clipping or Lipschitz-constrained VAE decoders with gradient penalties. The framework is specifically designed for extreme Non-IID settings where traditional parameter averaging fails catastrophically.

## Key Results
- FID scores of 81.91-98.46 compared to FedAvg's 148.70 on extreme Non-IID MNIST
- Classification accuracy of 74.25-63.25% versus FedAvg's 43.75% under same conditions
- F1-scores of 74.06-60.01% demonstrating strong semantic preservation across privacy mechanisms
- DP-SGD achieves 51.75% accuracy with ε=10, δ=10⁻⁵ while maintaining formal privacy guarantees
- LCD-VAE achieves 63.25% accuracy with better outlier mapping but lacks formal DP certification

## Why This Works (Mechanism)

### Mechanism 1: Generative Component Sharing via Decoder-Only Transfer
Transferring only VAE decoders enables knowledge transfer across completely disjoint data distributions where parameter averaging fails. Each client trains a local VAE, uploads only decoder parameters, and the server samples synthetic data from these decoders to train a global model. This works because decoders capture distributional knowledge sufficient to generate useful synthetic samples, and synthetic data training approximates learning from the union of client distributions. Evidence shows ALIGN-FL achieves 74.25% accuracy vs. FedAvg 43.75% under extreme Non-IID, validating decoder-only transfer over parameter averaging.

### Mechanism 2: Differential Privacy via DP-SGD with Adaptive Clipping
Applying DP-SGD to the full VAE architecture provides formal (ε, δ)-DP guarantees while mapping sensitive outliers to typical data points. Per-example gradients are clipped to norm C with calibrated Gaussian noise added. The paper uses adaptive clipping based on client-specific gradient norm quantiles (median), computed independently per client. This bounds each training sample's influence. Evidence shows DP-SGD on decoder-only fails; full VAE DP-SGD required due to encoder-decoder coupling. Visual confirmation shows DP-SGD maps Fashion-MNIST outliers to semantically similar MNIST digits with noise artifacts.

### Mechanism 3: Privacy via Lipschitz-Constrained Decoder (LCD-VAE)
Enforcing L-Lipschitz continuity on the VAE decoder through gradient penalties naturally limits sensitivity to individual training examples, providing privacy without explicit noise injection. A gradient penalty term is added to the ELBO objective, constraining how much the decoder output can change relative to latent space perturbations. This limits reconstruction fidelity for outliers. Evidence shows LCD-VAE produces cleaner outlier mappings and higher accuracy (63.25%) than DP-SGD (51.75%), though formal DP guarantees remain unproven.

## Foundational Learning

- **Concept: Variational Autoencoder (VAE) fundamentals**
  - Why needed here: ALIGN-FL's core architecture relies on VAE encoder-decoder pairs. Understanding ELBO optimization, latent space geometry, and the reconstruction-generation duality is essential for debugging synthetic data quality.
  - Quick check question: Can you explain why the KL divergence term in ELBO forces the latent space to be smooth and continuous, enabling decoder-only sampling?

- **Concept: Differential Privacy (DP-SGD specifically)**
  - Why needed here: One of two privacy mechanisms requires understanding gradient clipping, noise calibration, privacy budget composition across federated rounds, and the (ε, δ) semantics.
  - Quick check question: Given T=10 communication rounds with per-round ε=1, what is the approximate cumulative privacy loss under basic composition?

- **Concept: Non-IID data heterogeneity in Federated Learning**
  - Why needed here: The entire motivation for ALIGN-FL stems from extreme Non-IID settings where supp(Pi) ∩ supp(Pj) ≈ ∅. Understanding why FedAvg fails here (parameter averaging creates interference) clarifies the architectural choice.
  - Quick check question: Why does averaging model parameters from clients with disjoint class distributions (e.g., {0,1} vs. {8,9}) produce worse-than-random global models?

## Architecture Onboarding

- **Component map**: Client-side local VAE (encoder q_φ, decoder p_θ) → decoder extraction module → server-side synthetic data buffer → global model M_s → training loop on aggregated synthetic data
- **Critical path**: 1) Initialize: Each client trains local VAE on their disjoint data partition; 2) Extract: Copy decoder weights only; keep encoder local; 3) Upload: Send decoder to server (with DP protections already applied during training); 4) Aggregate: Server samples N_synthetic from each decoder (5000 total in paper); 5) Train: Server trains global model on combined synthetic dataset; 6) Repeat: Clients continue stateful training; no parameter overwriting from server
- **Design tradeoffs**: DP-SGD vs. LCD-VAE: DP-SGD provides formal guarantees (ε, δ) but lower utility; LCD-VAE offers better accuracy (63% vs. 52%) but lacks formal DP certification. Synthetic sample count: More samples improve global model but increase server compute; paper uses 5000. Latent dimensionality: 2D used for visualization; higher dimensions may capture more structure but complicate Lipschitz enforcement.
- **Failure signatures**: Disorganized latent space: If global encoder produces overlapping clusters, check if synthetic data is mode-collapsed or if client decoders have insufficient diversity. Outlier leakage: If No-DP baseline reconstructs sensitive outliers, verify privacy mechanism is actually enabled during training. Communication breakdown: If FedAvg-style averaging is accidentally applied, expect accuracy drop to ~40% (see Table III).
- **First 3 experiments**: 1) Baseline replication: Reproduce the 5-client MNIST partition ({0,1}, {2,3}, {4,5}, {6,7}, {8,9}) with No-DP ALIGN-FL; verify FID ~82 and accuracy ~74%. 2) Privacy mechanism comparison: Run DP-SGD (ε=10) vs. LCD-VAE (λ_GP=1) on same partition; confirm DP-SGD has lower accuracy but formal guarantees; LCD-VAE has cleaner reconstructions per Figure 3. 3) Ablation on synthetic sample count: Vary N_synthetic ∈ {1000, 5000, 10000} and measure global model accuracy to identify compute-utility tradeoff frontier.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can formal (ε, δ)-differential privacy guarantees be established for Lipschitz-constrained VAE decoders? The paper states "establishing formal (ε, δ)-DP guarantees for Lipschitz-constrained models remains an open challenge" and notes "LCD-VAE's privacy-preserving properties" are empirically demonstrated but theoretically incomplete. What evidence would resolve it: A theorem mapping Lipschitz constant L to (ε, δ) bounds under standard DP composition without restrictive sampling assumptions.

- **Open Question 2**: What are the formal convergence guarantees for ALIGN-FL under extreme Non-IID settings? The conclusion states "formal convergence guarantees remain to be established and are a valuable direction for future work." What evidence would resolve it: Theoretical analysis bounding convergence rate in terms of number of clients, communication rounds, and distribution divergence metrics.

- **Open Question 3**: How does ALIGN-FL scale to complex, high-dimensional data domains beyond MNIST and Fashion-MNIST? "The quality gap between privacy-preserving and non-private synthetic data widens significantly for more complex data domains beyond our experimental setup." What evidence would resolve it: Evaluation on complex datasets (e.g., CIFAR-100, medical imaging, or text) showing FID, accuracy, and privacy-utility trade-offs.

## Limitations
- DP-SGD adaptive clipping implementation details are underspecified, potentially affecting privacy-utility tradeoffs
- LCD-VAE lacks formal (ε, δ)-DP certification despite better empirical performance
- Synthetic data aggregation strategy (distribution across clients) is unspecified
- Scaling to high-dimensional complex data domains remains untested and may degrade performance
- Formal convergence guarantees for the federated learning process are not established

## Confidence
- **High confidence**: Core mechanism of decoder-only transfer and synthetic data aggregation (FID scores 81.91-98.46, accuracy 74.25-63.25% vs FedAvg 43.75%)
- **Medium confidence**: DP-SGD with adaptive clipping effectiveness (51.75% accuracy with ε=10), as adaptive clipping details are underspecified
- **Medium confidence**: LCD-VAE privacy claims, as formal DP guarantees are not established despite better empirical performance
- **High confidence**: Architecture independence and extreme Non-IID handling, as clearly demonstrated in experimental results

## Next Checks
1. **Privacy mechanism ablation**: Run the 5-client MNIST experiment with three variants - full DP-SGD on VAE, DP-SGD on decoder-only (to confirm the paper's claim that this fails), and LCD-VAE with varying λ_GP values to identify the optimal privacy-utility tradeoff.
2. **Outlier reconstruction analysis**: Systematically test whether privacy mechanisms (DP-SGD and LCD-VAE) actually map the 5% cross-domain outliers to typical data points by measuring reconstruction similarity between outliers and their nearest neighbors in the synthetic data.
3. **Synthetic sample size sweep**: Vary synthetic sample count from 1000 to 10000 in 1000 increments and measure the marginal improvement in global model accuracy to quantify the compute-utility tradeoff and identify potential overfitting to synthetic data.