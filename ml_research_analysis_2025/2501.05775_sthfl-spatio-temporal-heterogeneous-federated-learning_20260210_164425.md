---
ver: rpa2
title: 'STHFL: Spatio-Temporal Heterogeneous Federated Learning'
arxiv_id: '2501.05775'
source_url: https://arxiv.org/abs/2501.05775
tags:
- learning
- local
- data
- prototype
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel federated learning setting called Spatio-Temporal
  Heterogeneous Federated Learning (STHFL), which addresses both spatial heterogeneity
  (non-iid and long-tailed data across clients) and temporal heterogeneity (task increments
  within each client). To tackle these challenges, the authors introduce the Global-Local
  Dynamic Prototype (GLDP) framework, which employs personalized layers for each client
  and utilizes global and local prototypes to guide learning.
---

# STHFL: Spatio-Temporal Heterogeneous Federated Learning

## Quick Facts
- arXiv ID: 2501.05775
- Source URL: https://arxiv.org/abs/2501.05775
- Authors: Shunxin Guo; Hongsong Wang; Shuxia Lin; Xu Yang; Xin Geng
- Reference count: 40
- One-line primary result: Proposed GLDP framework achieves up to 19.12% accuracy improvement on various datasets by addressing spatial heterogeneity (non-IID and long-tailed data) and temporal heterogeneity (task increments) in federated learning

## Executive Summary
This paper introduces Spatio-Temporal Heterogeneous Federated Learning (STHFL), a novel federated learning setting that simultaneously addresses spatial heterogeneity (non-IID and long-tailed data distributions across clients) and temporal heterogeneity (incremental tasks within each client). The authors propose the Global-Local Dynamic Prototype (GLDP) framework, which employs personalized layers for each client and utilizes global and local prototypes to guide learning. The method demonstrates remarkable performance improvements over state-of-the-art approaches, effectively tackling the complex challenges of federated learning in heterogeneous environments.

## Method Summary
GLDP decouples the neural network into shared base layers (globally aggregated) and personalized head layers (locally updated). The framework uses three key mechanisms: 1) Base layer sharing to learn a global representation while personalization layers adapt to client-specific distributions, 2) Global prototype alignment where clients align their local class representations with global prototypes to mitigate long-tailed distribution effects, and 3) Local prototype preservation where clients maintain prototypes from previous tasks to prevent catastrophic forgetting during incremental learning. The training involves a composite loss function balancing cross-entropy, local prototype relation, and global-local prototype relation losses.

## Key Results
- Achieves up to 19.12% accuracy improvement compared to state-of-the-art federated learning methods
- Effectively handles both spatial heterogeneity (non-IID and long-tailed distributions) and temporal heterogeneity (incremental tasks)
- Demonstrates robust performance across multiple datasets including CIFAR10, CIFAR100, and TinyImageNet
- Shows superior handling of rare classes through global prototype guidance mechanism

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling the network into shared base layers and private personalized layers may reduce inter-domain concept shift caused by non-IID data distributions.
- **Mechanism:** The model splits parameters into base layers ($\mu$), updated via global aggregation, and personalized layers ($\nu$), updated strictly locally. This allows the base to learn a generalized representation while the personalization layers adapt to specific client label distributions.
- **Core assumption:** A shared representation space exists that benefits all clients despite significant distribution skew.
- **Evidence anchors:** [Section 4.1] describes how base representation layers contribute to a global shared representation space while personalization layers adapt to their respective class distribution. Similar approaches in "FedRep" and "Robust Federated Learning" support representation decoupling as a strategy for statistical heterogeneity.
- **Break condition:** If the shared representation requires contradictory features for different clients (extreme non-IID), the global aggregation step may degrade local performance.

### Mechanism 2
- **Claim:** Utilizing global prototypes as "complementary knowledge" appears to mitigate the performance degradation caused by long-tailed class distributions.
- **Mechanism:** The server aggregates class prototypes (mean embedding vectors) from clients. Clients minimize a Global-Local Prototype relation loss ($L_{GP}$), forcing local class representations to approximate the global prototype. This allows clients with few samples (tail classes) to borrow statistical strength from the global view.
- **Core assumption:** Global prototypes accurately represent the class center and can be transferred to local models without negative interference.
- **Evidence anchors:** [Abstract] states that global prototypes are served as complementary knowledge for training on classes with few samples. [Section 4.2] discusses how this constructs an unbiased local model that can reduce the influence of long-tailed distribution.
- **Break condition:** If local data is severely corrupted or domain-shifted relative to the global average, forcing alignment with the global prototype may introduce noise.

### Mechanism 3
- **Claim:** Preserving local prototype history likely prevents catastrophic forgetting in temporal (incremental) tasks without storing raw data.
- **Mechanism:** Clients store "old" local prototypes ($\hat{C}_{n,i}$) from previous tasks. During new task training, a Local Prototype relation loss ($L_{LP}$) minimizes the KL divergence between current class predictions and the preserved prototypes, effectively distilling knowledge of old classes.
- **Core assumption:** A single prototype vector per class captures sufficient information to preserve class boundaries over time.
- **Evidence anchors:** [Abstract] states that local prototypes from previous tasks guide current task learning to prevent catastrophic forgetting. [Section 4.2] explains how the knowledge of local prototypes generated in previous tasks guides training in the current task.
- **Break condition:** If intra-class variance is high, a single average prototype may fail to capture the complexity of old classes, leading to semantic drift.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** The GLDP framework modifies standard FedAvg by splitting layers. Understanding the baseline aggregation of weights ($\Theta$) is required to see how GLDP isolates personalization.
  - **Quick check question:** Do you know which layers are aggregated on the server and which are kept local in this architecture?

- **Concept: Prototype Learning**
  - **Why needed here:** The core mechanism relies on representing classes as average embedding vectors (prototypes) rather than just classifier weights.
  - **Quick check question:** Can you explain how a class prototype is computed (Eq. 6) and why it is considered privacy-preserving compared to raw data?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The "Temporal" aspect of STHFL addresses the loss of accuracy on old tasks when training on new ones.
  - **Quick check question:** How does the proposed method solve forgetting without storing a replay buffer of raw images?

## Architecture Onboarding

- **Component map:**
  - **Client:** ResNet backbone split into Base (Global $\mu$) and Head (Personal $\nu$); Local Prototype Bank ($\hat{C}$)
  - **Server:** Global Model Aggregator ($\Theta$); Global Prototype Bank ($eC$)
  - **Loss Function:** Composite of Cross-Entropy ($L_{CE}$), Local Prototype Loss ($L_{LP}$), and Global-Local Prototype Loss ($L_{GP}$)

- **Critical path:**
  1. Client receives Global Base Weights $\Theta$ and Global Prototypes $eC$
  2. Client trains on current stage data $D^{(m)}$, balancing classification accuracy with prototype alignment (Eq. 12)
  3. Client updates Local Prototype Bank via moving average (Eq. 9)
  4. Client uploads updated Base Weights $\mu$ and new Local Prototypes $C$
  5. Server aggregates weights and updates Global Prototypes (Eq. 11)

- **Design tradeoffs:**
  - **Privacy vs. Utility:** Transmitting prototypes (Eq. 6) is irreversible (claiming privacy), but averaging may lose intra-class detail
  - **Stability vs. Plasticity:** The $\lambda$ hyperparameter controls the trade-off between adhering to old knowledge (Local Prototype Loss) and learning new global knowledge (Global-Local Prototype Loss)

- **Failure signatures:**
  - **Long-tail bias persists:** If accuracy on rare classes remains low, check the weighting of the Global-Local Prototype Loss ($L_{GP}$) or verify if the global prototype for that class is dominated by a specific client's domain
  - **Catastrophic forgetting:** If accuracy drops on previous stages, check the moving average coefficient $\beta$ (Eq. 9); if too low, old knowledge is overwritten too quickly

- **First 3 experiments:**
  1. **Ablation on Loss Components:** Set $\lambda=0$ (remove temporal) and $\lambda=1$ (remove global guidance) to verify the individual contribution of spatial vs. temporal mechanisms (See Table 4)
  2. **Long-tail Robustness:** Vary the Imbalance Factor (IF) from 10 to 100 on CIFAR10 to observe how global prototype alignment handles increasing class scarcity
  3. **Incremental Stage Analysis:** Plot accuracy over stages ($M=1$ to $5$) to visualize the slope of catastrophic forgetting compared to standard FedAvg

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does GLDP performance vary when clients possess heterogeneous numbers of tasks, mimicking real-world scenarios like large versus small hospitals?
- **Basis in paper:** [explicit] The conclusion states, "we set each client device to contain the same number of tasks. However... in future work, we will set different numbers of tasks according to different types of client devices."
- **Why unresolved:** The current experimental setup uniformly assigns $M=5$ tasks to every client, which does not reflect the "large-scale hospitals" vs. "small-scale hospitals" imbalance described.
- **What evidence would resolve it:** Experimental results from a modified simulation where the number of tasks $M$ varies significantly across clients (e.g., some clients having 2 tasks, others having 20).

### Open Question 2
- **Question:** Can the transmission of class prototypes be exploited by malicious actors to reconstruct training data, despite the authors' claim of irreversibility?
- **Basis in paper:** [inferred] The paper claims privacy preservation because averaging embeddings is "irreversible," but provides no security analysis against gradient inversion or prototype reconstruction attacks.
- **Why unresolved:** While averaging prevents direct data recovery, prototypes are dense representations of class features; their vulnerability to inference attacks remains unverified.
- **What evidence would resolve it:** A robustness analysis measuring the success rate of membership inference or data reconstruction attacks specifically targeting the shared global and local prototypes.

### Open Question 3
- **Question:** How does the communication efficiency of GLDP scale when applied to datasets with significantly larger label spaces (e.g., thousands of classes)?
- **Basis in paper:** [inferred] The method requires transmitting sets of prototypes $\{C_n\}$; however, experiments were limited to datasets with only 10 to 100 classes.
- **Why unresolved:** As the number of classes grows, the size of the uploaded prototype set increases linearly, potentially creating a communication bottleneck similar to transmitting full model gradients.
- **What evidence would resolve it:** Profiling of bandwidth usage and convergence speed on high-class-count datasets (e.g., ImageNet-1K) compared to standard parameter-only federated approaches.

## Limitations

- **Layer split configuration:** The paper does not specify which ResNet layers are designated as base vs. personalization, creating ambiguity in faithful reproduction
- **Optimizer specification:** While hyperparameters are provided, the exact optimizer (SGD vs. Adam) remains unspecified
- **Extreme domain shift robustness:** The prototype alignment mechanism may fail under severe domain divergence between clients
- **Prototype representational capacity:** Using single average vectors per class may be insufficient for capturing complex class boundaries, particularly for classes with high intra-class variance

## Confidence

- **Mechanism 1 (Layer decoupling):** High - well-established approach in federated learning literature
- **Mechanism 2 (Global prototype guidance):** Medium - prototype-based methods are established, but effectiveness on long-tailed distributions needs more validation
- **Mechanism 3 (Local prototype preservation):** Medium - prototype-based continual learning is promising but single-vector representation may be limiting
- **Overall method effectiveness:** Medium - strong empirical results reported but reproduction requires resolving implementation ambiguities

## Next Checks

1. **Layer split verification:** Implement and test multiple layer split configurations (e.g., split after conv3, conv4, or just before FC layer) to identify optimal base/personalization division

2. **Prototype dimensionality analysis:** Compare model performance using different prototype representations - single average vector vs. cluster-based prototypes (k-means) to assess representational sufficiency

3. **Domain shift stress test:** Systematically vary the degree of domain shift between clients (e.g., by rotating image datasets differently) to evaluate when global prototype alignment breaks down