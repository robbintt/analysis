---
ver: rpa2
title: Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation
  under Limited Computing Resources
arxiv_id: '2512.02438'
source_url: https://arxiv.org/abs/2512.02438
tags:
- learning
- batch
- momentum
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a resource-efficient framework for pretraining
  medical vision-language models under computational constraints. The core idea leverages
  momentum self-distillation to replace exact label matching with soft similarity
  distributions, enabling effective learning even with small batch sizes.
---

# Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources

## Quick Facts
- **arXiv ID:** 2512.02438
- **Source URL:** https://arxiv.org/abs/2512.02438
- **Reference count:** 31
- **Primary result:** Achieves 83%+ zero-shot AUC, 90%+ few-shot AUC, and 2-3% retrieval gains using single GPU under limited resources

## Executive Summary
This paper addresses the challenge of pretraining medical vision-language models under computational constraints. The authors introduce momentum self-distillation, replacing exact label matching with soft similarity distributions computed via momentum encoders. This enables effective contrastive learning even with small batch sizes, which is critical for resource-limited environments. The method leverages gradient-free momentum key encoders to simulate large batch sizes without additional GPU memory, using gradient accumulation on query branches. Experiments demonstrate competitive zero-shot classification (over 83% AUC), significantly boosted few-shot performance (over 90% AUC), and improved image-text retrieval (2-3% gains) across MIMIC-CXR, VinDr-CXR, RSNA, and SIIM datasets, all achievable on a single GPU.

## Method Summary
The framework combines momentum self-distillation with resource-free batch enlargement for medical vision-language pretraining. Two momentum encoders (image and text) maintain slowly-updating representations in queues, while query encoders compute gradients. The key innovation is replacing hard one-hot labels with soft similarity distributions from key-to-key (p_k2k) and query-to-key (p_q2k) comparisons, weighted asymmetrically (α=0.3, β=0.7) via KL divergence. The gradient-free nature of momentum keys enables large effective batch sizes through gradient accumulation across sub-batches. The architecture uses Swin-Tiny for images, pretrained BERT for text, with a momentum queue size of 4096 and effective batch size of 512. The total loss combines uni-modal InfoNCE loss (ω_uni=1) and multi-modal KL divergence loss (ω_multi=10).

## Key Results
- **Zero-shot classification:** Achieves over 83% AUC across VinDr-CXR, RSNA Pneumonia, SIIM Pneumothorax, and CheXpert5x200 datasets
- **Few-shot classification:** Boosts performance to over 90% AUC when using only 10% of training labels
- **Image-text retrieval:** Improves Recall@K by 2-3% (e.g., R@1 from 10.9% to 22.7% at batch size 16)
- **Resource efficiency:** Trains effectively on single RTX 4090 with ~9GB VRAM, compared to 32GB+ for end-to-end approaches

## Why This Works (Mechanism)

### Mechanism 1
Replacing exact one-hot labels with soft similarity distributions via momentum self-distillation improves contrastive learning under small batch sizes by handling semantic ambiguity in medical text. The method computes two similarity distributions: key-to-key similarity (p_k2k) between image keys in the momentum queue, and query-to-key similarity (p_q2k) between the momentum-encoded query text and image keys. These serve as soft targets through KL divergence loss, weighted asymmetrically (α=0.3, β=0.7), allowing the model to learn from partial matches rather than forcing binary correct/incorrect signals. Medical text descriptions are inherently ambiguous—multiple images can legitimately match the same textual description (e.g., "heart size and cardiomediastinal contours are normal" applies to many normal chest X-rays). This is supported by SIIM few-shot showing 11.3% improvement (81.2% → 92.5%) with batch size 16 using MSD, while MM-MoCo shows 0% improvement at same batch size.

### Mechanism 2
The gradient-free nature of momentum key encoders enables simulating large batch sizes without additional GPU memory by decoupling key computation from gradient storage. Split large batch into sub-batches. Precompute all momentum keys first (no gradient storage required). Then process query sub-batches sequentially with gradient accumulation—gradients accumulate across sub-batches, and optimizer updates only after all sub-batches complete. Each query simultaneously interacts with the full expanded key set. The EMA-updated momentum encoder remains sufficiently aligned with the query encoder throughout training (m=0.995 ensures gradual drift). This achieves batch size 512 on single RTX 4090 with ~9GB VRAM; end-to-end requires 32GB+ across 4×A100 for similar batch. If momentum coefficient is too low (e.g., m<0.99), key encoder drifts too quickly, creating inconsistent negative representations.

### Mechanism 3
The key-to-key similarity distribution provides a stable training anchor that prevents divergence, while query-to-key distribution adds finer multimodal alignment when properly weighted. p_k2k acts as soft labels because same-instance pairs naturally achieve highest cosine similarity within the momentum queue. p_q2k provides cross-modal alignment signal but lacks inherent stability. The asymmetric weighting (β=0.7 > α=0.3) prioritizes the stable global structure from p_k2k while incorporating p_q2k's multimodal benefits. The momentum encoder's slow EMA updates provide a stable reference distribution throughout training. Training fails with α=1.0, β=0.0; stable with α=0.0, β=1.0; best performance at α=0.3, β=0.7. If momentum queue contains overly stale representations (queue too large or momentum too high), p_k2k loses relevance to current query encoder state.

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE Loss**
  - Why needed: The entire framework builds on distinguishing positive pairs (matched image-text) from negative pairs in embedding space using noise contrastive estimation.
  - Quick check question: Why does contrastive learning typically require large batch sizes? (Answer: More negatives per batch provide stronger separation signal; fewer negatives lead to weak supervision and false negatives.)

- **Concept: Exponential Moving Average (EMA) for Momentum Updates**
  - Why needed: Understanding θ_k → mθ_k + (1-m)θ_q creates a slowly-evolving encoder that provides stable representations without gradient computation.
  - Quick check question: If m=0.995, what percentage of the query encoder's change is incorporated into the key encoder per update? (Answer: 0.5% per step, creating smooth, stable evolution.)

- **Concept: Knowledge Distillation with Soft Targets**
  - Why needed: Self-distillation transfers knowledge through soft probability distributions rather than hard one-hot labels, handling label ambiguity.
  - Quick check question: Why might soft targets outperform hard labels for medical image-text pairs? (Answer: Soft targets allow partial matching across semantically similar pairs rather than forcing binary correct/incorrect, reducing false negative penalties.)

## Architecture Onboarding

- **Component map:**
  Image Input --> [Query Encoder (gradient-based)] --> q_I
  Image Input --> [Key Encoder (EMA-updated)] --> k_I --> Momentum Queue (4096)
  Text Input --> [Query Encoder (gradient-based)] --> q_T
  Text Input --> [Key Encoder (EMA-updated)] --> k_T --> Momentum Queue (4096)

- **Critical path:**
  1. Initialize query encoders; copy weights to key encoders
  2. For each training batch:
     - Split batch into sub-batches fitting GPU memory
     - **Phase 1:** Precompute all momentum keys across sub-batches (no gradients), concatenate into expanded key set
     - **Phase 2:** For each query sub-batch: forward pass, compute losses against full key set, accumulate gradients
     - **Phase 3:** After all sub-batches, single optimizer step
  3. Update key encoders via EMA: θ_k ← m·θ_k + (1-m)·θ_q
  4. Maintain momentum queues with FIFO replacement

- **Design tradeoffs:**
  - **Momentum coefficient (m):** Higher (0.999) = more stable keys but slower adaptation to query encoder improvements; Lower (0.99) = faster alignment but risk of unstable training
  - **Queue size (4096):** Larger = more diverse negatives but increasingly stale representations; Smaller = fresher but fewer negatives
  - **Loss weights (ω_multi=10):** Addresses observed optimization imbalance where uni-modal loss dominated; may need tuning for different datasets
  - **α/β ratio (0.3/0.7):** Prioritizes stable p_k2k signal; increasing α risks divergence

- **Failure signatures:**
  - **Training divergence with α=1.0:** Query-to-key signal alone is unstable—must maintain β>α
  - **No few-shot improvement over zero-shot:** Batch size too small without MSD enabled; verify MSD is active
  - **VRAM scaling unexpectedly:** Gradient accumulation not properly implemented—check optimizer.step() only called after all sub-batches
  - **Retrieval R@1 degrading over training:** Momentum queue too stale—reduce queue size or increase momentum coefficient
  - **Loss plateaus early:** Check if ω_multi is too low; uni-modal optimization may be dominating

- **First 3 experiments:**
  1. **Baseline reproduction at batch size 16:** Compare end-to-end (CXR-CLIP), MM-MoCo, and MSD on retrieval task to reproduce R@1 gap (10.9% vs 3.5% vs 22.7% from Table 8); validates MSD benefit at constrained batch sizes
  2. **α/β sensitivity analysis:** Test (0.0,1.0), (0.3,0.7), (0.5,0.5), (0.7,0.3), (1.0,0.0) on classification task to verify divergence at α=1.0 and optimal around 0.3/0.7 (Tables 7, 9)
  3. **Memory-efficiency validation:** Profile VRAM usage at batch sizes 16, 128, 256, 512 with RFBE enabled; confirm ~9GB ceiling versus end-to-end's linear scaling (Table 5)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of generative objectives, such as report generation, compromise the discriminative efficiency or performance of the momentum framework?
- Basis in paper: The authors state in the conclusion that "future research should explore... the integration of generative objectives to enhance model interpretability and clinical utility."
- Why unresolved: The current architecture is optimized for contrastive alignment using encoders; adding a decoder for generation introduces architectural complexities and computational overhead that may negate the proposed efficiency benefits.
- What evidence would resolve it: A study evaluating the trade-off between generation metrics (e.g., BLEU, ROUGE) and classification/retrieval performance when a decoding head is added to the framework.

### Open Question 2
- Question: Does the momentum self-distillation strategy generalize effectively to 3D imaging modalities (e.g., CT or MRI) which have higher memory demands?
- Basis in paper: The evaluation is restricted to 2D chest radiographs (MIMIC-CXR, VinDr, etc.), yet the work is framed as a general "medical vision-language" solution.
- Why unresolved: 3D volumes consume significantly more GPU memory than 2D images, potentially limiting the ability to maintain even the "small" batch sizes (e.g., 16) required for the momentum queue to function effectively.
- What evidence would resolve it: Reproducing the pretraining pipeline on a 3D dataset (e.g., CT volumes) and reporting the zero-shot classification performance and memory footprint.

### Open Question 3
- Question: Is the resource-free batch enlargement technique sufficient for training with larger backbone architectures (e.g., ViT-Large) where parameter storage dominates memory usage?
- Basis in paper: The efficiency experiments utilize "Swin-Tiny" to demonstrate feasibility on limited resources (RTX 2080Ti/4090).
- Why unresolved: The proposed gradient accumulation simulates large batch sizes by saving activation memory, but it does not reduce the memory required for model parameters, which becomes the bottleneck for larger models.
- What evidence would resolve it: Training the framework with larger encoders (e.g., Swin-Base or ViT-Large) on a single consumer-grade GPU and comparing the maximum achievable batch size and performance.

## Limitations

- **BioViL BERT dependency:** The framework relies on BioViL BERT for text encoding, which is not publicly available, requiring assumptions about the text encoder implementation.
- **Few-shot fine-tuning procedure:** The few-shot adaptation process lacks detail on learning rate, epochs, and whether linear probing or full fine-tuning was used.
- **Momentum queue stability:** The 0.995 momentum coefficient's stability across long training runs (50 epochs) needs empirical validation, as stale representations could degrade retrieval performance over time.

## Confidence

- **High Confidence:** The resource-efficiency mechanism (gradient-free momentum keys enabling large effective batch sizes) is well-supported by the architectural description and VRAM measurements in Table 5. The asymmetric loss weighting (α=0.3, β=0.7) preventing divergence has clear empirical validation in Tables 7 and 9.
- **Medium Confidence:** The performance improvements (83%+ zero-shot AUC, 90%+ few-shot AUC, 2-3% retrieval gains) are well-documented across multiple datasets, but depend on the BioViL BERT assumption. The few-shot improvement claims assume the unspecified fine-tuning procedure was optimal.
- **Low Confidence:** The exact computational scaling relationship between batch size and VRAM usage requires verification, as Table 5 shows dramatic efficiency gains but the underlying implementation details (sub-batch size, gradient accumulation steps) are not fully specified.

## Next Checks

1. **BioViL BERT Reproduction:** Test the full pipeline using alternative pretrained medical BERT models (e.g., microsoft/BiomedNLP-PubMedBERT) to assess sensitivity to text encoder choice and verify performance claims are not tied to a specific unavailable model.

2. **Momentum Queue Stability:** Implement logging of momentum queue representation staleness (e.g., tracking distribution drift over epochs) to empirically validate that 0.995 momentum coefficient maintains stable soft targets throughout 50-epoch training.

3. **Scaling Verification:** Systematically measure VRAM usage at batch sizes 16, 32, 64, 128, 256, 512 with RFBE enabled to confirm the sub-linear scaling relationship and validate that the reported ~9GB ceiling is reproducible.