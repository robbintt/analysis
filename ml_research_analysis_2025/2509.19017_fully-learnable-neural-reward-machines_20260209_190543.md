---
ver: rpa2
title: Fully Learnable Neural Reward Machines
arxiv_id: '2509.19017'
source_url: https://arxiv.org/abs/2509.19017
tags:
- reward
- learning
- function
- reinforcement
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fully Learnable Neural Reward Machines (FLNRM),
  a method that can learn both the automaton structure and the symbol grounding function
  end-to-end directly from raw sensory inputs, without requiring prior knowledge of
  the task automaton or symbol mapping. The method extends Neural Reward Machines
  by removing the assumption of a known Moore machine and instead learns the transition
  and output functions through neural networks with softmax-based probabilistic grounding.
---

# Fully Learnable Neural Reward Machines

## Quick Facts
- arXiv ID: 2509.19017
- Source URL: https://arxiv.org/abs/2509.19017
- Reference count: 40
- Key outcome: FLNRM learns automaton structure and symbol grounding end-to-end from raw sensory inputs without prior knowledge

## Executive Summary
This paper proposes Fully Learnable Neural Reward Machines (FLNRM), a method that can learn both the automaton structure and the symbol grounding function end-to-end directly from raw sensory inputs, without requiring prior knowledge of the task automaton or symbol mapping. The method extends Neural Reward Machines by removing the assumption of a known Moore machine and instead learns the transition and output functions through neural networks with softmax-based probabilistic grounding. Experiments in navigation environments show that FLNRM outperforms RNN-based baselines, especially in tasks with complex logical constraints like Global_Avoidance, with performance stable across different state sizes and observation modalities. The method retains the interpretability and formal grounding of automata-based approaches while achieving the general applicability of deep RL.

## Method Summary
FLNRM addresses non-Markovian RL by learning a differentiable automaton that predicts rewards from raw observations. The method consists of a symbol grounder network that maps raw states to symbol probabilities, a transition tensor and reward matrix (both learned via softmax), and a state tracking vector. The automaton is trained to predict observed rewards via cross-entropy loss, while the policy network uses the augmented state (raw state + automaton state) for action selection. The method removes the need for prior knowledge of the automaton structure or symbol mapping that was required in previous Neural Reward Machines approaches.

## Key Results
- FLNRM outperforms RNN baselines in navigation tasks with complex logical constraints like Global_Avoidance
- Performance remains stable across different state sizes (5 vs 30 states) and observation modalities (2D vectors vs 64x64 RGB images)
- The method achieves faster learning in non-Markovian tasks compared to traditional RNN approaches

## Why This Works (Mechanism)
FLNRM works by making every component differentiable through softmax-based probabilistic grounding. The symbol grounder network outputs symbol probabilities, the transition tensor defines probabilistic state transitions, and the reward matrix predicts rewards from automaton states. During training, gradients flow from the reward prediction error back through all components, allowing simultaneous learning of the automaton structure and symbol mapping. The use of softmax with temperature enables exploration in automaton space while maintaining differentiability, and the cross-entropy loss provides a strong supervisory signal for learning the automaton's logical structure.

## Foundational Learning
- **Non-Markovian Reward Decision Processes (NMRDPs)**: The core problem class addressed - reward depends on history, not just current state. Quick check: Can the agent determine optimal action by looking only at current observation?
- **Reward Machines (RMs) & Finite-State Automata (FSA)**: Formal tools that encode reward dependencies on history into discrete states and transitions. Quick check: Can you draw a state diagram where states represent task progress?
- **Symbol Grounding**: Mapping from low-level observations to high-level discrete symbols that drive the automaton. Quick check: How would a neural network take a raw image and output a probability for the symbol 'door'?
- **End-to-End Differentiable Training**: Makes all components differentiable to enable gradient-based learning. Quick check: How can you compute a gradient through a discrete state transition?

## Architecture Onboarding
- **Component map:** Raw state -> Symbol Grounder -> Symbol probabilities -> Differentiable Automaton (Transition tensor + Output matrix) -> Automaton state + Predicted reward -> Augmented state for policy
- **Critical path:** 1) Agent takes action, observes state s(t) and reward r(t) 2) State s(t) passes through symbol grounder to get symbol distribution 3) Automaton state updates using transition tensor and new symbols 4) Predicted reward computed from automaton state 5) FLNRM parameters trained via cross-entropy loss on reward prediction 6) Policy trained using augmented state (s(t), automaton state)
- **Design tradeoffs:** Number of automaton states |Q̂| and symbols |P̂| must be chosen; temperature τ controls determinism vs differentiability; hyperparameters affect learning dynamics
- **Failure signatures:** Insufficient |P̂| or |Q̂| leads to poor convergence; sparse rewards provide weak supervisory signals; temperature too low causes gradient vanishing
- **First 3 experiments:** 1) Simple Navigation with Visit formulas using 2D vectors 2) Complex Global_Avoidance constraints with negative rewards 3) Image-based observations (64x64 RGB) to validate raw pixel processing

## Open Questions the Paper Calls Out
- How does FLNRM performance degrade when the initialized number of symbols deviates from ground truth? The paper assumes knowing the ground-truth number of symbols, an unrealistic constraint.
- Does the probabilistic transition matrix converge to a discrete, human-readable automaton or remain a soft distributed representation? The method claims interpretability but uses probabilistic states.
- Can the architecture dynamically determine necessary automaton states rather than requiring fixed |Q̂| as hyperparameter? The paper relies on manual selection of state count.

## Limitations
- Lacks complete architectural specifications for symbol grounder networks and CNN architecture for image observations
- Missing training procedure details including update frequency, episode length, and discount factor
- Ground-truth reward machine structure used to generate training rewards remains unspecified
- Claims of stable performance across state sizes lack systematic hyperparameter sensitivity analysis

## Confidence
- **High Confidence**: Core mathematical framework and comparison methodology are clearly specified
- **Medium Confidence**: Experimental results showing FLNRM outperforming RNN baselines appear sound but verification is difficult
- **Low Confidence**: Claims about performance stability across state sizes and modalities lack sufficient empirical evidence

## Next Checks
1. Implement exact symbol grounder architectures (MLP for map, CNN for image) to verify reported performance gap replicates
2. Systematically test FLNRM performance across state counts (3-50) and temperature values (0.1-1.0) to verify stability claims
3. Apply FLNRM to novel environment with different reward structure to test learned symbol grounding and automaton structure transfer