---
ver: rpa2
title: 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal'
arxiv_id: '2508.05988'
source_url: https://arxiv.org/abs/2508.05988
tags:
- reasoning
- tokens
- pruning
- asap
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of Large Reasoning Models
  (LRMs) stemming from overly long Chain-of-Thought (CoT) traces, which increase training
  costs and inference latency while containing substantial redundancy. The core idea
  is to prune these traces using a two-stage coarse-to-fine framework called ASAP.
---

# Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal

## Quick Facts
- **arXiv ID:** 2508.05988
- **Source URL:** https://arxiv.org/abs/2508.05988
- **Reference count:** 40
- **Primary result:** State-of-the-art accuracy with 23.5% fewer tokens and 43.5% lower latency on LiveCodeBench v4_v5

## Executive Summary
This paper addresses the inefficiency of Large Reasoning Models (LRMs) stemming from overly long Chain-of-Thought (CoT) traces, which increase training costs and inference latency while containing substantial redundancy. The core idea is to prune these traces using a two-stage coarse-to-fine framework called ASAP. First, anchor-guided pruning removes structurally redundant branches by aligning the CoT with a concise logical backbone. Second, surprisal-based refining eliminates logically redundant steps by leveraging a novel first-token surprisal metric that identifies high-information cognitive pivots. The approach is validated across code generation and mathematical reasoning benchmarks, showing state-of-the-art accuracy while significantly reducing tokens and latency. For example, on LiveCodeBench v4_v5, ASAP achieves 36.19% Pass@1 with 23.5% fewer tokens and 43.5% lower latency compared to the strongest baseline.

## Method Summary
The method employs a two-stage coarse-to-fine framework for pruning Chain-of-Thought traces. Stage 1 uses anchor-guided pruning to remove structurally redundant branches by generating a "Direct Thought" from question-answer pairs and aligning it with the original CoT via pattern matching (Gestalt, τ=0.6). Stage 2 applies surprisal-based refining that computes first-token surprisal scores (-log P(first_token|prefix)) for each step and greedily removes lowest-surprisal steps until a token budget (4096) is met. The pruned CoTs are then used for supervised fine-tuning with standard NLL loss. The approach is validated on code generation (CodeForces-CoTs) and mathematical reasoning (OpenR1-Math) benchmarks.

## Key Results
- On LiveCodeBench v4_v5, ASAP achieves 36.19% Pass@1 with 23.5% fewer tokens and 43.5% lower latency
- State-of-the-art performance on multiple benchmarks while maintaining accuracy
- Significant reduction in both training costs and inference latency

## Why This Works (Mechanism)
The approach exploits the principle that reasoning traces contain redundant information distributed unevenly across steps. By identifying cognitive pivots—points where the reasoning shifts significantly—the method can remove low-information steps while preserving essential logic. The two-stage pruning first removes entire redundant branches (coarse pruning) then eliminates individual low-surprisal steps (fine pruning), achieving optimal compression without sacrificing reasoning quality.

## Foundational Learning
- **Chain-of-Thought (CoT) traces:** Sequential reasoning steps generated by LRMs during problem-solving; needed to understand what's being pruned and why redundancy exists
- **First-token surprisal:** -log P(x₁|C_pre) measures how unexpected the first token of a step is given its context; needed as the pruning criterion to identify high-information steps
- **Anchor-guided pruning:** Uses a concise logical backbone to identify and remove structurally redundant branches; needed to eliminate entire sections of redundant reasoning paths
- **Surprisal-based refining:** Greedily removes steps with lowest first-token surprisal; needed to fine-tune the pruning after coarse structural elimination
- **Supervised fine-tuning (SFT):** Standard training procedure on pruned CoTs; needed to adapt the model to the compressed reasoning traces
- **Gestalt pattern matching:** Validates that pruned content aligns with original CoT (τ=0.6 threshold); needed to ensure pruning doesn't introduce hallucinations

## Architecture Onboarding

**Component Map:** Question+Answer+Original_CoT -> Direct Thought Generation -> Coarse Pruning -> First-Token Surprisal Scoring -> Fine Pruning -> Pruned_CoT -> SFT -> Final Model

**Critical Path:** The most performance-critical path is the surprisal computation and greedy pruning loop, as it must process potentially thousands of steps efficiently while maintaining accuracy.

**Design Tradeoffs:** The method trades off computational overhead of surprisal scoring against significant latency gains from pruning. Using first-token surprisal (rather than full-step surprisal) reduces computation but may miss some redundant steps.

**Failure Signatures:** 
- Coarse pruning produces hallucinated content not in original CoT (Gestalt similarity <0.6)
- Surprisal-based pruning removes too many critical steps (accuracy drops significantly)
- Pruning budget too aggressive (CoT <1K tokens)
- Anchor generation fails to capture essential reasoning (low-quality Direct Thought)

**3 First Experiments:**
1. Test anchor generation prompts on a small subset of CoTs to ensure coarse pruning fidelity (Gestalt similarity >0.6)
2. Validate surprisal pruning algorithm on held-out set, ensuring pruned steps correlate with expected token budget (≤4096)
3. Reproduce SFT training curve on single epoch to confirm loss stabilizes and pruned model generalizes

## Open Questions the Paper Calls Out
- Can the First-Token Surprisal metric be effectively applied during online inference to enable real-time adaptive pruning, rather than requiring offline pre-processing of training data?
- Does the information concentration principle generalize to domains beyond mathematical and code reasoning, such as creative writing, commonsense reasoning, or multi-turn dialogue?
- How robust is the anchor-guided pruning stage to errors or hallucinations in the generated "Direct Thought," and can smaller models reliably generate anchors without quality degradation?

## Limitations
- Relies on capable LLMs for generating high-quality "Direct Thoughts" in Stage 1
- Experiments primarily focus on code generation and mathematical reasoning domains
- Current implementation requires offline pre-processing rather than online inference capability

## Confidence
- **Empirical Results:** High (detailed hyperparameters, standard benchmarks, clear methodology)
- **Reproducibility:** Medium (missing prompt templates, reliance on proprietary models for certain stages)
- **Methodological Innovation:** High (novel surprisal-based pruning approach, theoretically grounded)

## Next Checks
1. Reconstruct and test the anchor generation prompts on a small subset of CoTs to ensure coarse pruning fidelity (Gestalt similarity >0.6)
2. Validate the surprisal pruning algorithm on a held-out set, ensuring that the number of pruned steps correlates with the expected token budget (≤4096)
3. Reproduce the SFT training curve on a single epoch to confirm that the loss stabilizes and that the pruned model generalizes to unseen prompts