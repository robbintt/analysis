---
ver: rpa2
title: Measuring General Intelligence with Generated Games
arxiv_id: '2505.07215'
source_url: https://arxiv.org/abs/2505.07215
tags:
- games
- game
- language
- each
- gg-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces gg-bench, a synthetic benchmark for evaluating
  general intelligence in language models through procedurally generated two-player
  strategy games. The core idea is to use LLMs to generate game descriptions, implement
  them as Gym environments, and train RL agents via self-play, creating a scalable,
  contamination-resistant evaluation framework.
---

# Measuring General Intelligence with Generated Games

## Quick Facts
- arXiv ID: 2505.07215
- Source URL: https://arxiv.org/abs/2505.07215
- Reference count: 40
- Primary result: Novel synthetic benchmark using LLM-generated games to evaluate general intelligence, showing reasoning models achieve 31-36% winrate vs non-reasoning models at 7-9%

## Executive Summary
This paper introduces gg-bench, a synthetic benchmark for evaluating general intelligence in language models through procedurally generated two-player strategy games. The core idea is to use LLMs to generate game descriptions, implement them as Gym environments, and train RL agents via self-play, creating a scalable, contamination-resistant evaluation framework. Models are assessed by their winrate against these RL agents, with the current benchmark containing 126 diverse games after multi-stage filtering. State-of-the-art non-reasoning models like GPT-4o and Claude 3.7 Sonnet achieve only 7-9% winrates, while reasoning models like o1, o3-mini, and DeepSeek-R1 reach 31-36%, demonstrating that strategic planning and adaptability are critical. The benchmark's scalability is validated by showing that stronger models (o1 vs. GPT-4o) generate harder, more diverse games, suggesting gg-bench can evolve with model capabilities.

## Method Summary
The benchmark generates games through a three-stage pipeline: LLM-generated game descriptions in natural language, Gym environment implementation, and action space mapping. After filtering 1000 candidates to 126 valid games, PPO agents train via self-play for 10^6 timesteps. At inference, RL agents use MCTS with 100 rollouts. LLM performance is measured by winrate over 30 games against a specific "beat-able" RL checkpoint, with full game descriptions and valid moves provided each turn.

## Key Results
- Non-reasoning models (GPT-4o, Claude 3.7 Sonnet) achieve 7-9% winrates
- Reasoning models (o1, o3-mini, DeepSeek-R1) achieve 31-36% winrates
- GPT-4o generated games collapse to Tic-Tac-Toe variants (8/10 survived filtering)
- o1-generated games show significantly more diversity (1162/1000 survived filtering)

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Task Generation Creates Out-of-Distribution Evaluation
- Claim: LLMs can generate tasks they themselves cannot reliably solve, creating novel evaluation scenarios resistant to data contamination.
- Mechanism: A generator model (o1) produces game descriptions and Gym implementations; these novel combinations require reasoning beyond memorized patterns from training data.
- Core assumption: Generated games are sufficiently novel to differ from games over-represented in pre-training corpora.
- Evidence anchors:
  - [abstract] "gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment"
  - [section 1] "LLMs are capable of generating complex tasks that they themselves are incapable of solving"
  - [corpus] Related scalable benchmarks (Dynabench, SWE-bench, τ-bench) similarly use dynamic generation to avoid saturation
- Break condition: If generated games converge to known patterns (e.g., GPT-4o generated 8/10 surviving games as Tic-Tac-Toe variants; see Section 4.3)

### Mechanism 2: Self-Play RL Produces Consistent Baseline Opponents
- Claim: Self-play reinforcement learning with PPO creates stable reference agents at known skill levels for meaningful comparison.
- Mechanism: Agents train via self-play with checkpointed opponents sampled uniformly after warmup. At inference, MCTS with 100 rollouts selects the action with highest visit count from winning trajectories.
- Core assumption: Trained RL agents reach sufficient skill to differentiate LLM capabilities while remaining beatable.
- Evidence anchors:
  - [section 2.2] "We train agents for 10^6 timesteps and checkpoint every 2.5 × 10^5 timesteps"
  - [section 2.4] "the winning RL agents achieve an average winrate of 91.02% against the chosen benchmark opponents, providing an existence proof that the games are practically beatable"
  - [corpus] Code World Models paper highlights limitations of direct move prompting vs. learned game representations
- Break condition: If RL agents fail to converge, or if selected checkpoints are too weak/strong to differentiate models

### Mechanism 3: Strategic Planning Gap Detection
- Claim: The large performance gap between reasoning and non-reasoning models suggests long-horizon planning as the primary bottleneck.
- Mechanism: Games require multi-turn reasoning. Non-reasoning models (7-9% winrate) fail to evaluate long-term consequences; reasoning models (31-36%) leverage extended computation for lookahead.
- Core assumption: Winrate differences reflect strategic reasoning capability, not just instruction-following or action-space interpretation.
- Evidence anchors:
  - [abstract] "language models struggle primarily with long-term strategic planning and correctly generalizing from game descriptions to new gameplay scenarios"
  - [section 4.2] "This trajectory illustrates the LLM's inability to evaluate long-term consequences of trades and territory exposure" (Cross Over game analysis)
  - [corpus] Related work on chain-of-thought and extended reasoning supports multi-step planning advantages
- Break condition: If games can be solved with greedy local strategies without forward planning

## Foundational Learning

### Concept: OpenAI Gym/Gymnasium API
- Why needed here: All 126 games are implemented as Gym environments with action_space, observation_space, reset(), step(), render(), and valid_moves().
- Quick check question: Can you implement a simple Tic-Tac-Toe environment following the Gym API with discrete action space and Box observation space?

### Concept: Proximal Policy Optimization (PPO)
- Why needed here: RL baseline opponents are trained via PPO self-play with clipped surrogate objective and GAE advantage estimation.
- Quick check question: Can you explain why PPO's clipping (ε = 0.2) prevents destabilizing policy updates during self-play?

### Concept: Monte Carlo Tree Search (MCTS) for Inference
- Why needed here: RL agents select moves via MCTS with 100 self-play rollouts, choosing the action leading to the most simulated wins.
- Quick check question: Can you describe how MCTS uses visit counts N(s,a) to select the final action at inference time?

## Architecture Onboarding

### Component map:
Game Generation Pipeline: LLM → Game Description → Gym Implementation → Action Space Description → Filtering Layer → RL Training System → Evaluation Framework

### Critical path:
Correct game description → Faithful Gym implementation → Successful RL training convergence → Valid opponent checkpoint selection → Meaningful winrate evaluation

### Design tradeoffs:
- **Generator model strength**: o1 produces diverse games (1162/1000), GPT-4o collapses to Tic-Tac-Toe variants (only 10/1000 survive) — suggests future-proofing via stronger generators
- **Filtering aggressiveness**: Strict filtering ensures quality but limits scale; 87.4% of games rejected
- **Opponent strength selection**: Weaker checkpoints make benchmark accessible; stronger ones may saturate at 0% for current models
- **Input context**: Full game description + action mapping provided each turn increases token costs (99.95% of cost for non-reasoning models)

### Failure signatures:
1. **Invalid move outputs**: LLM returns moves not in valid_moves list (8.97% of GPT-4o losses); handled by re-prompting
2. **Implementation-description drift**: Generated code doesn't match natural language rules; manually verified 10/10 sample games as faithful
3. **Non-terminating games**: Win condition bugs; filtered via 100-move timeout wrapper
4. **Trivial game diversity**: Generator collapse to known patterns; mitigated by using stronger generator model

### First 3 experiments:
1. **Run the generation pipeline end-to-end** for 10 candidate games, verify all filtering stages pass, and inspect implementation-description faithfulness
2. **Train a PPO agent** on one generated game for 10^6 timesteps, verify checkpoint winrates increase over training, and confirm MCTS inference produces valid moves
3. **Evaluate a baseline LLM** (e.g., GPT-4o-mini) against one trained RL agent for 30 matches to validate the evaluation loop produces winrates in expected range (5-15%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance vary as a function of the "novelty" or originality of the generated game relative to the model's pre-training data?
- Basis in paper: [explicit] The authors state in Section 6: "Future work could further analyze the originality of our games and measure model performance as a function of game novelty."
- Why unresolved: While the paper demonstrates that models struggle with generated games, it does not quantify the semantic distance between these new games and existing popular games (like Chess or Go) found in training corpora.
- What evidence would resolve it: A quantitative analysis correlating model win rates against a metric of "game novelty" or distance from known benchmarks.

### Open Question 2
- Question: Can stronger models consistently generate increasingly complex games to prevent benchmark saturation?
- Basis in paper: [explicit] Section 6 predicts that "as model capabilities improve... stronger models will also be able to generate increasingly difficult games," suggesting the benchmark is "future-proof."
- Why unresolved: The paper provides only preliminary evidence by comparing GPT-4o (generating simple Tic-Tac-Toe variants) and o1, but it remains to be seen if this trend scales indefinitely.
- What evidence would resolve it: Longitudinal evaluation showing that state-of-the-art models fail on games generated by more advanced successor models.

### Open Question 3
- Question: Can the filtering pipeline be modified to preserve complex game genres (e.g., combat or spatial reasoning) that are currently disproportionately removed?
- Basis in paper: [inferred] Section 3.2 notes that "number games" surged from 20.3% to 36.7% after filtering, while "combat" games dropped, likely because complex win conditions are harder to implement and pass validity checks.
- Why unresolved: The current validity filters (execution, timeout, RL upper bound) inadvertently select for simpler, self-contained mechanics, potentially narrowing the scope of "general intelligence" tested.
- What evidence would resolve it: A revised generation pipeline that successfully retains and validates a higher percentage of complex, multi-mechanic games.

## Limitations
- Cannot assess social intelligence, language grounding, or real-world task planning
- Evaluation quality depends entirely on generator model's diversity
- Manual filtering limits scalability and introduces human bias
- Significant computational resources required for both generation and RL training

## Confidence

- **High**: Observed winrate differences between reasoning and non-reasoning models (7-9% vs 31-36%) are consistent across multiple evaluations and clearly demonstrate the importance of strategic planning.
- **Medium**: The claim that synthetic generation creates contamination-resistant evaluation is plausible but requires ongoing validation as models become stronger at generation tasks.
- **Medium**: The benchmark's scalability claim (stronger generators produce harder games) is supported by the o1 vs GPT-4o comparison but needs broader validation across different generator architectures.

## Next Checks

1. **Generator Diversity Stress Test**: Run the generation pipeline with GPT-4o and o1 using identical prompts, then compute pairwise game similarity metrics to quantify diversity differences and identify convergence patterns.
2. **Human Baseline Comparison**: Have human players attempt 10 representative games from the benchmark, recording winrates and time-to-solution to establish whether the 7-9% vs 31-36% gap reflects meaningful capability differences.
3. **Cross-Architecture Generalization**: Evaluate the same benchmark games with a non-LLM agent (e.g., a search-based planner with game-specific heuristics) to determine whether winrate differences reflect general reasoning or LLM-specific limitations.