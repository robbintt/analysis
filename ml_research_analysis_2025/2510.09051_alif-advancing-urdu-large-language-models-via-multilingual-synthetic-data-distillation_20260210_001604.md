---
ver: rpa2
title: 'Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data
  Distillation'
arxiv_id: '2510.09051'
source_url: https://arxiv.org/abs/2510.09051
tags:
- urdu
- b-instruct
- dataset
- translation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Alif-1.0-8B-Instruct, a multilingual Urdu-English
  language model developed to address the scarcity of high-quality datasets and cultural
  nuance in existing Urdu LLMs. The model is trained using a modified self-instruct
  technique on a high-quality synthetic dataset (Urdu-Instruct) that incorporates
  Urdu-native chain-of-thought reasoning, bilingual translation, and cultural sensitivity.
---

# Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation

## Quick Facts
- arXiv ID: 2510.09051
- Source URL: https://arxiv.org/abs/2510.09051
- Reference count: 26
- Alif-1.0-8B-Instruct achieves 75.5 average score on Urdu-translated benchmarks, outperforming leading multilingual models within a $100 training budget

## Executive Summary
This paper introduces Alif-1.0-8B-Instruct, a multilingual Urdu-English language model addressing the scarcity of high-quality datasets and cultural nuance in existing Urdu LLMs. The model is trained using a modified self-instruct technique on a high-quality synthetic dataset (Urdu-Instruct) that incorporates Urdu-native chain-of-thought reasoning, bilingual translation, and cultural sensitivity. Built on Llama-3.1-8B, Alif-1.0-8B-Instruct outperforms leading multilingual models on Urdu-specific tasks while maintaining strong English performance through replay datasets.

## Method Summary
Alif-1.0-8B-Instruct is developed through a two-stage training pipeline: first, continued pre-training on 200K Urdu Wikipedia articles using CLM loss; second, instruction fine-tuning on a blended dataset of 105K examples including Urdu-Instruct (51,686 synthetic examples from GPT-4o), translated data (28,910), and English replay datasets (Alpaca 10,400 + OpenOrca 10,000). The model uses LoRA adapters (rank=128, alpha=32) on QKVO, MLP, and embedding layers, covering 14.72% trainable parameters. Total training time is 39 hours on A100 80GB, costing approximately $100.

## Key Results
- Alif-1.0-8B-Instruct achieves 75.5 average score across Urdu-translated benchmarks (MGSM, Alpaca Eval, Dolly General QA)
- Outperforms Llama-3.1-8B-Instruct (74.4), Mistral-7B-Instruct-v0.3 (74.0), and Qwen-2.5-7B-Instruct (73.2) on Urdu-specific tasks
- Maintains English performance with slight improvements on arc_challenge (0.517→0.548) and hellaswag (0.591→0.614), though MMLU declines from 0.680 to 0.618

## Why This Works (Mechanism)

### Mechanism 1: Modified Self-Instruct with Cultural Calibration
Task-specific prompt engineering and cross-task deduplication improves synthetic data quality for low-resource language instruction tuning. Each task category receives unique prompts + 4 human-annotated seeds + 2 machine-generated seeds. A global task pool with ROUGE-L threshold 0.7 prevents semantic duplication. Human annotators filter grammar, factual errors, and unethical content.

### Mechanism 2: Replay-Based Catastrophic Forgetting Mitigation
Mixing English replay datasets during Urdu fine-tuning preserves baseline English capabilities while acquiring Urdu proficiency. The fine-tuning blend includes Urdu-Instruct (51,686), translated data (28,910), plus English Alpaca (10,400) and OpenOrca (10,000), with loss computed only on output tokens.

### Mechanism 3: Two-Stage Training with LoRA Adapters
Separating foundational language acquisition from task alignment with parameter-efficient adapters enables low-cost adaptation. Stage 1 uses CLM loss on 200K Urdu Wikipedia (1 epoch, 23 hrs). Stage 2 uses instruction tuning on 105K blended examples (2 epochs, 16 hrs) with LoRA applied to QKVO, MLP, and embedding layers.

## Foundational Learning

- **Self-Instruct Data Generation**: Core technique for creating Urdu-Instruct dataset when human annotation at scale is infeasible. Quick check: Can you explain why ROUGE-L filtering prevents instruction duplication but may not catch semantic equivalence?

- **LoRA (Low-Rank Adaptation)**: Enables training 8B model under $100 by freezing base weights and training only adapter matrices. Quick check: What is the relationship between LoRA rank and representational capacity for new language vocabulary?

- **Catastrophic Forgetting**: Central risk when fine-tuning multilingual models on new language; replay datasets are the mitigation. Quick check: Why does replay help, and what tradeoff does it introduce for target language learning speed?

## Architecture Onboarding

- Component map: Llama-3.1-8B (frozen base) → LoRA adapters (QKVO, MLP, embeddings) → Stage 1: Urdu Wikipedia → Stage 2: Urdu-Instruct + Translated + Replay → Alif-1.0-8B-Instruct

- Critical path: Modified self-instruct pipeline → Urdu-Instruct quality → Stage 2 fine-tuning signal → Final benchmark performance. Data quality at step 1 is the highest-leverage variable.

- Design tradeoffs: Higher replay ratio → better English retention but slower Urdu acquisition; larger LoRA rank → more capacity but higher VRAM and risk of overfitting; more synthetic data → better coverage but potential propagation of teacher model biases.

- Failure signatures: Urdu responses with mixed scripts or Arabic characters → tokenization mismatch or insufficient pre-training; degraded English on factual tasks → insufficient replay or learning rate too high; repetitive/generic Urdu outputs → ROUGE filtering too aggressive or seed diversity insufficient.

- First 3 experiments: Ablate replay ratio (10%, 20%, 30%) to measure Urdu vs English benchmark tradeoffs; compare Urdu-Instruct subsets generated by GPT-4o vs Claude vs Qwen-2.5 to quantify teacher dependency; test LoRA rank=[16, 32, 64, 128] on held-out Urdu Evaluation Set to identify saturation point.

## Open Questions the Paper Calls Out

- How does Alif-1.0-8B-Instruct's performance compare to baseline models when evaluated using objective, task-specific metrics rather than LLM-as-a-judge scoring? The paper states future work will incorporate Exact Match, F1, BLEU, COMET, and BERTScore for more rigorous quantification.

- Can the modified self-instruct technique be effectively generalized to other low-resource languages outside of Urdu? The paper claims this "scalable method improves instruction quality and can be adapted to other low-resource languages for broader NLP development."

- How can the English replay dataset strategy be optimized to prevent the observed degradation in domain-specific STEM and humanities knowledge while maintaining Urdu proficiency? The paper notes a slight decline in MMLU scores despite replay mitigation.

## Limitations

- The synthetic data quality depends entirely on GPT-4o prompts and seeds, which are not released, making exact reproduction impossible without replicating the prompt engineering.
- LLM-as-judge evaluation (GPT-4o) introduces potential circularity and bias, as the same model evaluating outputs may have influenced their generation.
- The slight MMLU decline (0.680→0.618) indicates imperfect preservation of specialized academic knowledge despite replay dataset mitigation.

## Confidence

- **High Confidence**: Two-stage training methodology with LoRA adapters is technically sound and well-documented; hardware costs are verifiable.
- **Medium Confidence**: Catastrophic forgetting mitigation through replay datasets appears effective but imperfect; replay ratio selection significantly impacts results.
- **Low Confidence**: Absolute superiority claims rely on Urdu benchmark translations and GPT-4o evaluation, making direct comparison difficult without access to same evaluation framework.

## Next Checks

1. **Synthetic Data Quality Audit**: Generate 1,000-example subset of Urdu-Instruct using publicly available models (Qwen-2.5-Coder, Claude) with described task-specific prompts and seeds. Compare linguistic quality, cultural relevance, and factual accuracy against GPT-4o-generated samples using human annotators blind to source.

2. **Replay Ratio Sensitivity Analysis**: Train three Alif variants with English replay ratios of 10%, 20%, and 30% while holding other hyperparameters constant. Evaluate Urdu benchmarks (MGSM, AlpacaEval, Dolly QA) and English benchmarks (MMLU, HellaSwag, ARC) to quantify Pareto frontier between language preservation and acquisition.

3. **Cross-Teacher Performance Comparison**: Use same Urdu-Instruct data generation pipeline but substitute GPT-4o with Claude-3-Sonnet and Qwen-2.5-Coder as teacher models. Fine-tune Alif using each synthetic dataset and evaluate on Urdu benchmarks to determine teacher model dependency and potential bias propagation.