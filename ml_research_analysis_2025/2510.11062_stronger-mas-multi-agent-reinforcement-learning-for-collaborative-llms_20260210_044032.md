---
ver: rpa2
title: 'Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs'
arxiv_id: '2510.11062'
source_url: https://arxiv.org/abs/2510.11062
tags:
- code
- agent
- output
- tool
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AT-GRPO, an agent- and turn-wise grouped
  reinforcement learning method designed for multi-agent systems. It addresses challenges
  in applying on-policy RL to MAS by adapting group-relative optimization for agent-specific
  prompts and histories, and by building a training system that supports both single-
  and multi-policy updates.
---

# Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs

## Quick Facts
- **arXiv ID:** 2510.11062
- **Source URL:** https://arxiv.org/abs/2510.11062
- **Reference count:** 40
- **Primary result:** AT-GRPO achieves 96-99.5% accuracy on long-horizon planning tasks, improving over single-agent RL baselines by 14-47 percentage points.

## Executive Summary
This paper introduces AT-GRPO, an agent- and turn-wise grouped reinforcement learning method designed for multi-agent systems. It addresses challenges in applying on-policy RL to MAS by adapting group-relative optimization for agent-specific prompts and histories, and by building a training system that supports both single- and multi-policy updates. Across game, planning, coding, and math tasks, AT-GRPO delivers consistent gains: in long-horizon planning, accuracy improves from a 14-47% single-agent RL baseline to 96-99.5%; in coding and math, average gains are 3.87-7.62% and 9.0-17.93%, respectively. The method also scales to collaborative multi-agent settings, demonstrating superior performance over single-agent baselines and other MARL frameworks. Results confirm that MAS with AT-GRPO enables stronger specialization and collaboration, with the choice of shared or role-specific policies depending on task characteristics.

## Method Summary
The paper presents AT-GRPO, a reinforcement learning method for multi-agent systems that adapts group-relative optimization to handle agent-specific prompts and turn-based interactions. The approach uses tree-structured sampling where each agent generates K candidate actions per turn, with advantages computed relative to these agent-and-turn-wise groups to ensure prompt identity. A dedicated training system with isolated GPU pools per policy enables concurrent on-policy training of multiple policies, with separate rollout and update workers. The method combines team and local rewards through a mixing parameter α, and supports both shared policies (M=1) and role-specific policies (M=N) depending on task characteristics.

## Key Results
- Long-horizon planning tasks: Accuracy improves from 14-47% single-agent RL baseline to 96-99.5% with AT-GRPO
- Coding tasks: Average gains of 3.87-7.62% over baselines on APPS and CodeContests
- Math tasks: Average improvements of 9.0-17.93% on OlympiadBench and AIME24/25
- Role specialization ablation: Training agents jointly in MAS boosts accuracy to 96% vs 16% when combining separately trained single-agent models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AT-GRPO enables stable on-policy reinforcement learning in multi-agent systems by preserving the critical assumption of shared context for advantage calculation.
- **Mechanism:** The algorithm groups candidate actions not by problem alone, but by agent identity (role) and turn number within the trajectory. It uses tree-structured sampling: at each turn, each agent generates K candidate actions. Advantages are computed relative to this specific group, ensuring all actions share an identical prompt (role instructions + history). This prevents "identical-state assumption" violation that occurs when naively comparing actions from different agents or turns. A PPO-style loss then updates the policy using these correctly grouped advantages.
- **Core assumption:** Group-based policy optimization requires comparing actions generated from a truly identical prompt and state context. If context differs (different roles/histories), rewards are not directly comparable for relative advantage calculation.
- **Evidence anchors:**
  - [abstract] "AT-GRPO...adapts group-relative optimization for agent-specific prompts and histories."
  - [Section 4.1] "Standard GRPO grouping assumptions fail in MAS because prompts differ by role and turn... we therefore adopt agent-wise and turn-wise grouping... candidates share the same role and turn position, ensuring prompt identity for valid GRPO advantage comparisons."
  - [corpus] Related work (MAPoRL, MARFT) avoids this problem through homogeneous agents or single-turn interactions. AT-GRPO generalizes GiGPO-style grouping to the MAS context.
- **Break condition:** Fails if prompt identity within groups is not maintained, or if rewards are purely global without informative individual signals for credit assignment.

### Mechanism 2
- **Claim:** Joint training of LLM agents in a multi-agent RL context induces specialized collaboration that dramatically outperforms isolated agent training.
- **Mechanism:** On-policy rollouts where agents interact via structured workflows (Coder-Tester, Tool-User-Executor) drive co-adaptation. The mixed reward signal (Eq. 3: `r_t,i = α * r_team + r_loc_t,i`) combines team objectives with individual incentives. This creates role-specific specialization—agents learn distinct, complementary skills—while learning to align outputs for collaboration. Ablation shows single-agent trained agents combined into MAS achieve only 16% vs. 96% accuracy from joint training.
- **Core assumption:** MAS roles are complementary and tasks benefit from specialized, structured processing. The reward structure can effectively disentangle individual contributions from group success.
- **Evidence anchors:**
  - [abstract] "...choice of shared or role-specific policies depending on task characteristics. Results confirm that MAS with AT-GRPO enables stronger specialization and collaboration."
  - [Section 5.4 Ablation Study] "Training agents in a single-agent (SA) setting offers limited benefits... In stark contrast, training the agents jointly within the MAS environment boosts accuracy to 96.00."
  - [corpus] CURE co-evolves agents but restricts to single-turn code workflows. AT-GRPO generalizes to diverse domains with explicit multi-turn interactions.
- **Break condition:** Fails if roles are ill-defined or redundant, leading to competition rather than collaboration. Less effective on tasks solvable by a single generalist without structured sub-task decomposition.

### Mechanism 3
- **Claim:** A dedicated training system with isolated resource pools is a prerequisite for efficiently scaling multi-agent on-policy RL.
- **Mechanism:** Each LLM policy has its own "LLM Resource Pool" (GPU group) with Rollout and Update workers. Environment execution runs in a separate CPU pool. This separation enables parallel sampling/updating for multiple policies while routing trajectory data correctly to associated update workers. This overcomes single-agent, single-buffer limitations of existing frameworks (TRL, VERL).
- **Core assumption:** Scalable on-policy MAS training requires managing multiple distinct models, data streams, and update loops concurrently. Monolithic single-model frameworks cannot support this without major architectural changes.
- **Evidence anchors:**
  - [abstract] "...a training system that supports both single- and multi-policy regimes."
  - [Section 4.2 MAS Training System] "...our system is designed to support the concurrent on-policy training of multiple policies. ... Each policy is managed within an independent resource pool."
  - [corpus] While many papers propose algorithms, this explicitly details architectural components to make MAS RL runnable at scale, addressing a gap in existing frameworks.
- **Break condition:** System bottlenecks if routing between CPU environment pools and GPU update workers cannot handle trajectory throughput, or if cluster lacks memory for independent model pools.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** AT-GRPO modifies GRPO. Understanding how GRPO computes advantages by normalizing rewards within a group of sampled outputs for a single prompt is essential to grasp why "agent- and turn-wise" grouping is the key innovation.
  - **Quick check question:** In GRPO, if you sample 4 candidates with rewards [0, 1, 0, 1], what is the advantage of the first answer? (Mean=0.5, so advantage ≈ -0.5/0.5 = -1.0)

- **Concept: On-Policy Reinforcement Learning**
  - **Why needed here:** The entire methodology is on-policy: the model learns only from data generated by its current version, requiring continuous rollout-update cycles.
  - **Quick check question:** Can you use a replay buffer of trajectories from an older model version for pure on-policy training? Why or why not?

- **Concept: Multi-Agent System (MAS) Workflows**
  - **Why needed here:** Gains are conditional on structured MAS workflows (Coder-Tester debate, Tool-User-Executor). The method optimizes these collaborative processes, not solitary agents.
  - **Quick check question:** In a "Coder-Tester" workflow, what is the termination condition for an episode during training? (Alignment: tests pass or max turns reached)

## Architecture Onboarding

- **Component Map:**
  CPU Pool (EnvWorkers) -> Router -> GPU Pools (Per Policy: RolloutWorker, UpdateWorker)

- **Critical Path:**
  1. **Initialization:** Define MAS workflow (N agents, roles), reward functions (team + local), task environments
  2. **Data Generation (Rollout):** For each parallel environment, agents generate K actions per turn, forming trajectory trees. EnvWorkers compute rewards.
  3. **Data Routing:** Router streams trajectories to UpdateWorkers of corresponding policies.
  4. **Policy Update:** UpdateWorkers compute AT-GRPO advantages and apply PPO loss. Updated parameters sync to RolloutWorkers.
  5. Repeat from step 2.

- **Design Tradeoffs:**
  - **Role-sharing vs. Role-specialized Policies:** Role-sharing (M=1) simplifies training but limits specialization. Role-specialized (M=N) maximizes specialization but increases memory/compute. Choice depends on task (specialization helps coding; may not help math with overlapping roles).
  - **Tree vs. Parallel Sampling:** Tree sampling (K actions per turn) is crucial for valid AT-GRPO groups but requires more compute than simple parallel trajectory sampling.
  - **Reward Shaping:** Dense rewards accelerate learning but may introduce bias. Method remains effective with sparse outcome-only rewards (ablation shows -0% to -4% drop vs. dense).

- **Failure Signatures:**
  - **Catastrophic Drop on Policy Swapping:** Swapping trained role-specialized policies between agents (e.g., giving Coder's policy to Tester) causes performance collapse (96% → 6% in ablation). This confirms successful specialization.
  - **Unstable Training with Naive MAS-GRPO:** Standard GRPO without agent/turn grouping yields unstable/no improvement due to incorrect advantage estimation from heterogeneous prompts.
  - **Reward Hacking:** Agents may maximize local reward at team expense if balance parameter (α) is misconfigured.

- **First 3 Experiments:**
  1. **Sanity Check (Single Agent Baseline):** Train single agent with standard GRPO on target task (e.g., Sokoban). Verify struggle with long-horizon dependencies (~14-47% baseline).
  2. **AT-GRPO Core Validation:** Train MAS with full AT-GRPO (role-sharing policies) on planning task. Verify dramatic accuracy increase (>90%).
  3. **Ablation on Specialization:** Compare role-sharing vs. role-specialized policies on coding (distinct roles) and math (overlapping roles). Verify role-specialization helps coding (+3.05 pts avg for 1.7B) but shared policy can be superior for math on some benchmarks (e.g., OlympiadBench: 39.60% shared vs. 35.20% specialized).

## Open Questions the Paper Calls Out

- **Open Question 1:** What factors determine whether a shared policy or role-specific policies are optimal for a given task in MAS-RL?
  - **Basis in paper:** [explicit] Conclusion and analysis section state that the choice "needs to be determined by the task characteristics," observing that coding benefits from specialization while math sometimes prefers shared policies.
  - **Why unresolved:** The paper does not provide a principled method to predict which regime is better a priori.
  - **What evidence would resolve it:** A systematic study across diverse tasks with controlled variables (e.g., role heterogeneity, task interdependence, feedback structure) to identify key predictive factors.

- **Open Question 2:** How well does AT-GRPO generalize to MAS with more than two agents or complex interaction graphs?
  - **Basis in paper:** [inferred] Scalability analysis is limited to up to 7 agents in a specific ensemble architecture (Reasoners + Tool-Users + Judge), with no exploration of hierarchical or dynamic structures.
  - **Why unresolved:** Real-world MAS often involve more agents, non-flat topologies, or evolving roles.
  - **What evidence would resolve it:** Evaluations on MAS benchmarks with diverse agent counts, interaction topologies (e.g., star, mesh, hierarchy), and dynamic role assignments.

- **Open Question 3:** Is AT-GRPO robust to variations in the reward-mixing coefficient α across different task domains?
  - **Basis in paper:** [inferred] The paper uses a fixed α=1 without ablation or sensitivity analysis, though the reward design mixes global and local rewards via α.
  - **Why unresolved:** Performance may degrade with suboptimal α, and adaptive α could potentially improve results.
  - **What evidence would resolve it:** Ablation studies varying α per task (e.g., 0.0, 0.5, 1.0, 2.0) and analyzing performance trends; possibly learning α adaptively.

## Limitations
- Sandbox execution environment: Exact timeout values, memory limits, and security boundaries are not specified, which could affect training dynamics and final performance.
- Router implementation details: The data routing mechanism that directs trajectories to correct per-policy UpdateWorkers is critical but underspecified in architectural details.
- Credit assignment in mixed rewards: The effectiveness of disentangling individual contributions from team success in the α·r_team + r_loc formulation is not rigorously validated across all task types.

## Confidence
- **High Confidence:** The core algorithmic contribution (AT-GRPO with agent- and turn-wise grouping) and its necessity for stable on-policy MAS RL is well-supported by ablation studies showing dramatic performance differences.
- **Medium Confidence:** The architectural contribution (dedicated training system with isolated resource pools) is conceptually sound but relies on assumptions about vLLM-style async execution that are not fully detailed.
- **Medium Confidence:** The empirical gains across diverse domains are substantial, but the exact contribution of each component (algorithm vs. system vs. reward shaping) to these gains is not fully isolated.

## Next Checks
1. **Reproduce the core ablation:** Train MAS with standard GRPO (no agent/turn grouping) on a planning task and verify the dramatic performance drop that validates AT-GRPO's necessity.
2. **Test role-swapping failure:** After training role-specialized policies, swap them between agents and verify the catastrophic performance collapse (96% → 6%) that confirms successful specialization.
3. **Validate router implementation:** Implement the trajectory routing mechanism and verify it correctly streams data to the appropriate per-policy UpdateWorkers under concurrent multi-policy training.