---
ver: rpa2
title: 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents'
arxiv_id: '2509.06477'
source_url: https://arxiv.org/abs/2509.06477
tags:
- shortcuts
- agent
- shortcut
- agents
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAS-Bench introduces the first systematic benchmark for evaluating
  GUI-shortcut hybrid mobile agents. It features 139 complex tasks across 11 real-world
  apps, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts),
  and 7 evaluation metrics.
---

# MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents
## Quick Facts
- arXiv ID: 2509.06477
- Source URL: https://arxiv.org/abs/2509.06477
- Reference count: 13
- Introduces first systematic benchmark for evaluating GUI-shortcut hybrid mobile agents

## Executive Summary
MAS-Bench introduces a comprehensive benchmark for evaluating hybrid mobile GUI agents that combine traditional GUI operations with shortcut techniques like APIs, deep links, and RPA scripts. The benchmark features 139 complex tasks across 11 real-world apps and includes 88 predefined shortcuts. Experiments demonstrate that hybrid agents achieve up to 64.1% success rate compared to 44.6% for GUI-only approaches, with over 40% greater efficiency. The benchmark also reveals a critical research gap: while predefined shortcuts achieve 100% success, agent-generated shortcuts show only 38% success, highlighting the need for improved shortcut generation capabilities.

## Method Summary
MAS-Bench provides a unified framework for evaluating mobile GUI agents through a knowledge base of 88 predefined shortcuts (APIs, deep links, RPA scripts) across 11 real-world apps. The benchmark includes 139 complex tasks designed to test both GUI operations and shortcut utilization. Agents are evaluated on seven metrics including success rate and efficiency. The framework supports both predefined shortcut usage and agent-generated shortcut creation, with a novel evaluation methodology that measures the effectiveness of different shortcut strategies.

## Key Results
- Hybrid agents achieve 64.1% success rate versus 44.6% for GUI-only agents
- Efficiency improvements exceed 40% when using shortcut-augmented approaches
- Predefined shortcuts achieve 100% success rate while agent-generated shortcuts reach only 38% success

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic evaluation of shortcut utilization in mobile GUI agents. By providing a knowledge base of 88 predefined shortcuts and testing both their usage and generation, MAS-Bench creates a comprehensive framework for measuring agent capabilities. The combination of GUI operations with shortcuts enables more efficient task completion, as shortcuts can bypass multiple GUI steps through direct API calls, deep links, or RPA scripts.

## Foundational Learning
- **GUI Operations**: Basic interaction with mobile interfaces through touch events - needed for baseline agent functionality, quick check: can agent tap, swipe, and type accurately
- **Shortcut Types**: APIs, deep links, and RPA scripts as alternative interaction methods - needed for efficiency gains, quick check: does agent correctly identify when to use each shortcut type
- **Task Complexity**: Multi-step operations across different app contexts - needed to test agent reasoning, quick check: can agent chain operations across app boundaries
- **Success Metrics**: Multiple evaluation criteria beyond binary success - needed for nuanced assessment, quick check: does agent optimize for efficiency metrics
- **Shortcut Generation**: Creating new shortcuts from interaction patterns - needed for adaptive agents, quick check: can agent derive new shortcuts from successful operation sequences

## Architecture Onboarding
- **Component Map**: Task Parser -> Agent Executor -> Shortcut Selector -> App Interface
- **Critical Path**: Task definition → Shortcut selection → Execution → Result validation
- **Design Tradeoffs**: Predefined vs generated shortcuts balance between reliability and adaptability
- **Failure Signatures**: GUI-only failures due to step inefficiency, shortcut failures due to context mismatch
- **First Experiments**: 1) Baseline GUI-only agent performance measurement, 2) Predefined shortcut integration testing, 3) Agent-generated shortcut creation evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Success rate differences depend on representativeness of 139 tasks for real-world usage patterns
- Predefined shortcuts achieving 100% success may not generalize to app version changes
- Evaluation criteria for "success" in shortcut generation need clearer definition

## Confidence
- Benchmark comprehensiveness: High confidence - Extensive task and shortcut coverage
- Efficiency improvements: Medium confidence - Reported metrics require more validation
- Shortcut generation capability: Low confidence - 38% success rate indicates significant challenges

## Next Checks
1. Conduct cross-version testing by applying benchmark to different app versions to assess shortcut robustness
2. Implement ablation studies to isolate contributions of individual shortcut types to performance
3. Perform user study validation comparing agent performance against human expert performance on same task set