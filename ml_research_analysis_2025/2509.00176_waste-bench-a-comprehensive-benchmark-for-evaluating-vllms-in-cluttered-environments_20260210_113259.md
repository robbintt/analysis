---
ver: rpa2
title: 'Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments'
arxiv_id: '2509.00176'
source_url: https://arxiv.org/abs/2509.00176
tags:
- image
- waste
- vllms
- cluttered
- waste-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in evaluating Vision Language Models
  (VLLMs) for real-world waste classification in complex, cluttered environments with
  deformed objects. The authors introduce Waste-Bench, a comprehensive benchmark dataset
  containing 9,520 high-quality question-answer pairs across 952 images, covering
  diverse scenarios like single/multiclass classification, counting, color diversity,
  shape analysis, and condition evaluation.
---

# Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments

## Quick Facts
- arXiv ID: 2509.00176
- Source URL: https://arxiv.org/abs/2509.00176
- Authors: Muhammad Ali; Salman Khan
- Reference count: 7
- One-line primary result: GPT-4o achieves 57.52% accuracy on Waste-Bench, while MiniGPT-4 scores only 36.40%, highlighting significant VLLM struggles with real-world waste classification in cluttered scenes.

## Executive Summary
This paper introduces Waste-Bench, a comprehensive benchmark designed to evaluate Vision Language Models (VLLMs) on real-world waste classification tasks in cluttered environments with deformed objects. The benchmark contains 9,520 high-quality question-answer pairs across 952 images, covering diverse scenarios including single/multiclass classification, counting, color diversity, shape analysis, and condition evaluation. Through rigorous human-in-the-loop annotation and filtering, the dataset ensures relevance and accuracy. Seven VLLMs, including both open-source and closed-source models, were evaluated on Waste-Bench, revealing significant performance drops compared to standard benchmarks. The study demonstrates that VLLMs struggle with reasoning in cluttered scenes, recognizing rare classes, and handling visual degradations, providing valuable insights for improving VLLM robustness in challenging real-world applications.

## Method Summary
The Waste-Bench benchmark was constructed through a multi-stage process: first, ZeroWaste images were captioned using Gemini-Pro with ground-truth metadata prompts; then GPT-3.5 generated 10 diverse questions per image from these captions; next, two expert annotators independently filtered ~20% of noisy QA pairs, retaining only those with substantial inter-rater reliability (Cohen's κ = 0.78); finally, pre-trained VLLMs were evaluated zero-shot on the resulting 9,520 QA pairs, with GPT-4 serving as judge comparing predictions to ground truth using a calibrated evaluation prompt, validated against human evaluations showing high consistency.

## Key Results
- GPT-4o achieved the highest accuracy at 57.52%, while MiniGPT-4 performed notably worse at 36.40%
- All models showed significant performance drops compared to standard benchmarks, with no model exceeding 60% accuracy
- VLLMs consistently struggled with color misidentification under occlusion, rare class recognition in clutter, counting deformed objects, and degradation robustness (0.5-4% drops under noise/shading)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-in-the-loop annotation with inter-rater reliability filtering improves benchmark quality by removing ~20% noisy question-answer pairs.
- Mechanism: Two expert annotators independently verify captions and QA pairs; only pairs where both agree on semantic relevance, clarity, and technical accuracy are retained, reducing irrelevant, unanswerable, or redundant questions.
- Core assumption: High inter-annotator agreement (Cohen's κ = 0.78) translates to benchmark labels that better reflect real-world task requirements.
- Evidence anchors:
  - [section 3.2]: "Two expert annotators independently reviewed each caption; only captions in which both agreed every class mention was correct were retained... Inter-rater reliability was substantial (Cohen's κ = 0.78, 95% CI 0.73–0.83)."
  - [section 3.2, Stage 3]: "~20% of the Q/A pairs (1,904 pairs) were filtered out, leaving a total of 9,520 updated Q/A pairs."
  - [corpus]: Limited direct corpus evidence; related VLLM benchmarks (e.g., safety-critical driving, table understanding) also use multi-stage annotation but report varied quality control specifics.
- Break condition: If annotator expertise does not match target domain or if κ drops below ~0.6, filtering may retain systematic errors or discard valid edge cases.

### Mechanism 2
- Claim: Multi-category question design across 11 task types exposes VLLM limitations in cluttered scenes more effectively than single-task benchmarks.
- Mechanism: Each image receives ~10 diverse questions targeting different reasoning dimensions; aggregate performance reveals consistent weaknesses (e.g., rare class recognition, counting deformed objects, color misidentification under occlusion).
- Core assumption: Error distribution across categories generalizes to real-world waste-sorting scenarios where tasks are interleaved.
- Evidence anchors:
  - [abstract]: "covering diverse scenarios like single/multiclass classification, counting, color diversity, shape analysis, and condition evaluation."
  - [section 3.1]: "Waste-Bench encompasses 11 different question categories and 9,520 high-quality open-ended question-answer pairs... average of 10 questions per image."
  - [corpus]: Related cluttered-scene datasets (e.g., GraspClutter6D) emphasize occlusion and diversity but focus on robotics grasping, not VLLM evaluation.
- Break condition: If categories are too narrow or fail to reflect operational task distribution, benchmark may over- or under-estimate real-world readiness.

### Mechanism 3
- Claim: LLM-as-judge evaluation (GPT-4) calibrated against human verification provides scalable, consistent assessment of open-ended VLLM responses.
- Mechanism: GPT-4 compares predicted answers to ground truth with binary correctness judgments and reasoning; human evaluators validate subset showing high consistency (e.g., InstructBLIP 59% human vs. 63% GPT, CogVLM 46% vs. 45%).
- Core assumption: GPT-4's judgment sufficiently proxies human evaluation for waste-domain QA; calibration holds across models and question types.
- Evidence anchors:
  - [section 3.2, Stage 4]: "GPT-4 to evaluate the correctness of VLLM predictions against ground-truth answers... To ensure accuracy, two assistants reviewed the evaluation results... high consistency between GPT-4 and human evaluations."
  - [Table 1]: Shows performance comparison across models for GPT vs. human evaluation.
  - [corpus]: Limited corpus evidence on LLM-as-judge calibration for cluttered waste scenes; broader VLLM benchmarks adopt similar strategies with varied validation rigor.
- Break condition: If systematic biases exist in GPT-4's evaluation (e.g., favoring verbose answers or specific phrasing), accuracy comparisons may be skewed.

## Foundational Learning

- Concept: Vision-Language Model (VLLM) architecture
  - Why needed here: Understanding how vision backbones (e.g., CLIP, ViT) connect to LLMs via adapters (MLP, Q-former) explains performance differences across evaluated models.
  - Quick check question: Can you explain how a Q-former adapter differs from a simple MLP projection in multimodal fusion?

- Concept: Cluttered scene understanding and occlusion handling
  - Why needed here: Waste-Bench explicitly targets deformed, overlapping objects; baseline VLLM training on clean images fails to generalize.
  - Quick check question: What visual features would help a model distinguish a crumpled plastic bag from cardboard in a cluttered pile?

- Concept: Open-ended QA evaluation with semantic equivalence
  - Why needed here: Unlike multiple-choice benchmarks, Waste-Bench uses free-form answers; evaluation must judge semantic correctness, not exact string match.
  - Quick check question: How would you design a prompt for an LLM judge to score "partially correct" predictions?

## Architecture Onboarding

- Component map: ZeroWaste images -> Gemini-Pro captioning -> human verification -> GPT-3.5 QA generation -> human filtration -> VLLM inference -> GPT-4 evaluation -> human calibration check
- Critical path: QA generation and filtering quality directly determine benchmark validity; evaluation prompt design governs score reliability
- Design tradeoffs: Open-ended questions increase realism but complicate evaluation; human-in-the-loop ensures quality but limits scale; using proprietary models (Gemini, GPT-4) for annotation and evaluation may introduce model-specific biases
- Failure signatures: Models consistently fail at (1) color misidentification under occlusion (e.g., plastic bag color confused with background), (2) rare class detection in clutter (e.g., metal cans missed), (3) counting deformed/overlapping objects, (4) performance degradation under noise/shading (Table 7 shows 0.5–2% drops for noise, 1–4% for shading across models)
- First 3 experiments:
  1. Establish baseline accuracy for each VLLM across all 11 question categories using pre-trained weights (as in Table 3)
  2. Validate GPT-4 judge against human evaluation on a 200-sample subset to confirm consistency before full benchmarking
  3. Test degradation robustness by applying fixed noise (σ=7), shading (intensity 0.7), and enhanced lighting (brightness factor 1.2) to measure accuracy drops (replicating Table 7)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain adaptation strategies or fine-tuning on Waste-Bench significantly improve VLLM performance in cluttered environments without causing catastrophic forgetting or performance degradation on standard natural image tasks?
- Basis in paper: [explicit] The authors explicitly state that techniques like domain adaptation can help VLLMs adapt, noting that "Fine-tuning models on Waste-Bench’s diverse and complex scenarios ensures that they become more robust... improving model generalization without sacrificing accuracy."
- Why unresolved: The study evaluates pre-trained models in zero-shot settings only; no experiments were conducted on models fine-tuned with the proposed benchmark to verify if the trade-off between specialization and generalization can be managed.
- What evidence would resolve it: A comparative study showing VLLM performance on both Waste-Bench and standard benchmarks (e.g., SEED-Bench) before and after fine-tuning on the waste dataset.

### Open Question 2
- Question: To what degree are the observed failures in Waste-Bench attributable to visual hallucinations (inventing objects) versus a fundamental inability to perform spatial reasoning in cluttered, occluded scenes?
- Basis in paper: [explicit] The authors mention in the experiments section that "future work will address issues like hallucination and robustness for better performance in complex tasks" and highlight struggles with "reasoning in cluttered scenes."
- Why unresolved: The current evaluation relies on GPT-4 to grade answers as correct/incorrect based on semantic similarity to ground truth, but it does not perform a granular error analysis distinguishing between reasoning errors and hallucinations.
- What evidence would resolve it: A detailed ablation study or human evaluation categorizing specific error types (e.g., "hallucinated object," "misclassified visible object," "spatial relation error") for the low-performing models.

### Open Question 3
- Question: How does the interaction between visual degradations (noise, shading) and object deformity affect VLLM accuracy, and are current models more susceptible to clutter or to signal noise?
- Basis in paper: [inferred] While the authors provide auxiliary results on degradations in Appendix A.5 and discuss robustness to noise, they focus their primary analysis on clutter and deformity, leaving the interaction between these distinct visual stressors under-explored.
- Why unresolved: The paper presents degradation results as a separate table without analyzing the correlation between a model's resilience to noise versus its ability to handle the geometric ambiguity of deformed objects.
- What evidence would resolve it: Experiments that systematically vary clutter density alongside controlled noise levels to determine which factor acts as the primary bottleneck for model performance.

## Limitations
- Dataset Availability: Waste-Bench dataset is not yet publicly available, preventing independent verification
- Evaluation Calibration: Limited corpus evidence exists on GPT-4 judge calibration specifically for cluttered waste-scene QA
- Generalization Scope: Performance drops may be specific to ZeroWaste-derived images and 11 curated categories

## Confidence
- High: (1) Dataset construction process; (2) Performance ranking of VLLMs; (3) Identified failure modes
- Medium: (1) GPT-4 as judge calibration; (2) Category-wise error distribution generalization; (3) Robustness degradation metrics
- Low: (1) Direct comparison to other VLLM benchmarks; (2) Model-specific checkpoint details and exact API parameters

## Next Checks
1. Reproduce Human-LLM Agreement: Independently evaluate 200 predictions from Waste-Bench using GPT-4 judge and human raters to confirm reported high consistency
2. Test Degradation Transfer: Apply identical noise (σ=7), shading (gradient mask 0.7), and lighting (brightness factor 1.2) degradations to other cluttered-scene datasets to verify whether observed 0.5-4% accuracy drops are dataset-specific or general VLLM limitations
3. Benchmark Category Coverage Audit: Analyze task distribution in operational waste-sorting logs to confirm Waste-Bench's 11 categories and ~10 questions per image adequately reflect real-world task diversity before drawing readiness conclusions