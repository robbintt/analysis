---
ver: rpa2
title: Handling Students Dropouts in an LLM-driven Interactive Online Course Using
  Language Models
arxiv_id: '2508.17310'
source_url: https://arxiv.org/abs/2508.17310
tags:
- dropout
- course
- dropouts
- maic
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies student dropout prediction and intervention
  in LLM-driven interactive online courses (MAIC). The authors analyze interaction
  logs from a specific MAIC course to define dropouts and identify contributing factors,
  finding strong links between dropout behaviors and textual interaction patterns.
---

# Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models

## Quick Facts
- arXiv ID: 2508.17310
- Source URL: https://arxiv.org/abs/2508.17310
- Authors: Yuanchun Wang; Yiyang Fu; Jifan Yu; Daniel Zhang-Li; Zheyuan Zhang; Joy Lim Jia Yin; Yucheng Wang; Peng Zhou; Jing Zhang; Huiqin Liu
- Reference count: 12
- Primary result: Achieves up to 95.4% accuracy predicting student dropouts in LLM-driven MAIC courses using interaction logs and personalized email interventions

## Executive Summary
This paper addresses student dropout prediction and intervention in Multi-Agent LLM-driven Interactive Courses (MAIC) where students interact with AI agents via text. The authors analyze interaction logs from a specific MAIC course to define dropouts and identify contributing factors, finding strong links between dropout behaviors and textual interaction patterns. They propose a course-progress-adaptive dropout prediction framework (CPADP) that achieves up to 95.4% accuracy using fine-tuned language models, and design a personalized email recall agent that generates unique emails for at-risk students based on their interaction records. The approach is validated in a real-world MAIC system with over 3,000 students, demonstrating effectiveness in re-engaging students across diverse backgrounds.

## Method Summary
The method employs a three-stage CPADP framework for dropout prediction: zero-shot LLM inference for new courses, few-shot prompting as data accumulates, and fine-tuned PLM with MLP classifier when sufficient examples exist. Interaction logs serve as primary features, with students' chronological conversations with AI agents encoded by PLMs and classified into dropout/retention labels. A personalized email recall agent uses LLM generation to create context-aware messages referencing specific in-class interactions for at-risk students. The system is trained on 1201 instances from 186 students, achieving 95.4% accuracy with fine-tuned PLMs and demonstrating intervention success through increased login activity.

## Key Results
- CPADP framework achieves up to 95.4% accuracy in dropout prediction using fine-tuned PLMs
- Personalized email recall agent increases login frequency from 14 to 25 per week for recalled students
- Interaction patterns (frequency, length) are strong predictors of dropout, more so than demographic factors
- Framework successfully re-engages diverse students across different backgrounds and time zones

## Why This Works (Mechanism)

### Mechanism 1: Interaction Logs as Primary Dropout Signal
Textual interaction patterns between learners and AI agents are more predictive of dropout than demographic or trait-based factors. Students who engage with higher frequency and longer message length per chapter demonstrate continued commitment; sparse or off-topic interactions signal disengagement before formal dropout. The model extracts features from chronologically-structured interaction strings rather than static learner profiles.

### Mechanism 2: Course-Progress-Adaptive Prediction (CPADP)
A staged prediction approach—zero-shot → few-shot → fine-tuning—achieves high accuracy while remaining practical for new courses with limited historical data. Early in a course, LLMs with prior knowledge perform zero-shot dropout assessment. As labeled examples accumulate, few-shot prompting improves inference. Once sufficient data exists, a smaller PLM with MLP classifier is fine-tuned, achieving higher accuracy (95.4%) at lower inference cost.

### Mechanism 3: Personalized Email Recall via LLM Agent
Generating unique emails referencing specific in-class interaction content increases re-engagement among at-risk students who would otherwise remain inactive. An LLM-based email agent receives each student's interaction history, identifies salient moments, and generates a personalized message that recalls that content, previews upcoming material, and adopts the persona of the relevant AI agent.

## Foundational Learning

- **Concept: Multi-Agent LLM Learning Environments (MAIC)**
  - Why needed here: The paper's context assumes familiarity with courses where students interact with multiple AI agents (Teacher, TA, simulated peers) via text, replacing passive video consumption. Understanding this paradigm shift is prerequisite to interpreting interaction logs as engagement data.
  - Quick check question: Can you distinguish between a MOOC (video-based) and a MAIC (text-based, multi-agent) learning environment?

- **Concept: Binary Classification on Structured Text**
  - Why needed here: Dropout prediction is framed as classifying interaction record strings into dropout/retention labels. Requires understanding how PLMs extract features from text and how MLPs map those features to probabilities.
  - Quick check question: Given a student's interaction log as input, what would the output of a dropout prediction model be?

- **Concept: Zero-Shot / Few-Shot / Fine-Tuning Spectrum**
  - Why needed here: CPADP's staged approach relies on progressively more data-dependent methods. Understanding the trade-offs (inference cost, accuracy, data requirements) is essential for deployment decisions.
  - Quick check question: At course start (no historical data), which prediction method is appropriate? After 3 semesters, which method becomes viable?

## Architecture Onboarding

- **Component map**: Interaction logs -> PLM encoder -> MLP classifier -> P(Dropout), P(Retention) -> LLM email agent -> Personalized email -> Student

- **Critical path**:
  1. Collect and log all student-AI interactions with timestamps and chapter markers
  2. At prediction checkpoint (start of chapter Ch), aggregate history ICh
  3. Route to appropriate predictor (zero-shot if new course, few-shot if <N examples, fine-tuned PLM if ≥N)
  4. For students above dropout probability threshold, trigger email agent with interaction history
  5. Send generated email; monitor login/activity metrics for intervention effect

- **Design tradeoffs**:
  - Prediction horizon (Cp−Ch): Shorter horizons yield higher accuracy but give less lead time for intervention
  - LLM vs. PLM: LLMs (GPT-4, GLM-4) require no training data but have lower accuracy and higher cost; fine-tuned PLMs achieve 95.4% accuracy but need 800+ labeled instances
  - Email personalization depth: More context improves relevance but increases token cost and generation time

- **Failure signatures**:
  - Prediction accuracy drops sharply for Cp−Ch > 2 (predicting too far ahead)
  - Emails fail to recall if interaction logs are sparse or contain only off-topic content
  - Zero-shot/few-shot models misclassify when interaction patterns deviate from prior knowledge assumptions

- **First 3 experiments**:
  1. **Baseline replication**: On existing MAIC-TAGI data, retrain PLM+MLP classifier; verify 95.4% accuracy and analyze per-chapter accuracy heatmap.
  2. **Cross-course transfer**: Train on MAIC-TAGI, test on a different MAIC course (different subject/institution); measure accuracy drop to assess generalization.
  3. **A/B email intervention**: Randomly assign at-risk students to personalized email vs. generic reminder; compare login rates and chapter progress over 2 weeks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CPADP framework and personalized recall agent maintain high performance when applied to MAIC courses with varying subjects, difficulty levels, and institutional contexts?
- Basis in paper: Section 7 (Limitations) states the study relied on data from a single course (MAIC-TAGI) and "lacks diversity," explicitly calling for future work to include "more MAIC courses across different subjects, difficulty levels, and institutions."
- Why unresolved: The identified interaction patterns and prediction accuracy (95.4%) may be overfitted to the specific domain of the "Towards Artificial General Intelligence" course or the demographics of the specific university involved.
- What evidence would resolve it: Validation of the framework's prediction accuracy (F1 score) and intervention recall rates across a dataset of diverse MAIC courses (e.g., Humanities vs. STEM) from multiple universities.

### Open Question 2
- Question: Does the LLM-generated personalized email intervention significantly outperform generic notifications in reducing dropout rates?
- Basis in paper: Section 7 notes the "evaluation of the personalized email recall system is relatively limited" and explicitly proposes "A/B testing with larger student populations" and "varying email designs" as necessary future work.
- Why unresolved: The current study observed increased logins post-intervention but did not isolate the "personalization" variable against a control group receiving a standard email to prove statistical significance.
- What evidence would resolve it: Results from a randomized controlled trial (A/B test) comparing retention rates between a group receiving personalized LLM emails and a control group receiving generic reminders.

### Open Question 3
- Question: Does the email recall agent lead to sustained engagement and course completion, or does it only generate a transient spike in login activity?
- Basis in paper: Section 7 highlights the need for "longer-term tracking of re-engagement effects," as the validation focused on immediate login frequency (Days 63–68) rather than ultimate course completion.
- Why unresolved: A short-term increase in logins (observed in the recalled group) does not guarantee that the student will persist through the remaining chapters of the course.
- What evidence would resolve it: Longitudinal data tracking the final completion rates of the "recalled" students compared to their original predicted dropout trajectories.

## Limitations
- Framework validated on single MAIC course with limited student diversity
- Email intervention tested on only 8 students, lacking statistical significance
- No analysis of model stability across semesters or changing course content
- Binary dropout definition may misclassify intentional partial completers as dropouts

## Confidence

- **High confidence**: Interaction logs as dropout predictors (strong empirical support from dataset analysis showing clear behavioral differences between completers and dropouts)
- **Medium confidence**: CPADP framework effectiveness (95.4% accuracy is impressive but from single course; staged approach is theoretically sound)
- **Low confidence**: Email intervention efficacy (based on n=8, with only directional evidence of increased logins; no control group comparison for attrition vs. re-engagement)

## Next Checks

1. **Cross-course generalization**: Train dropout prediction model on MAIC-TAGI data, test on a different MAIC course from another institution; measure accuracy drop and identify domain-specific features
2. **A/B intervention testing**: Randomly assign at-risk students to personalized email vs. generic reminder vs. no intervention; measure chapter completion rates over 30 days to establish causal impact
3. **Longitudinal stability analysis**: Retrain CPADP models on semester 1 data, evaluate on semester 2+; quantify accuracy decay and determine retraining frequency requirements