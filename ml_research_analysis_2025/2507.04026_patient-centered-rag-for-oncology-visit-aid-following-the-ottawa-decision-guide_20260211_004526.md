---
ver: rpa2
title: Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide
arxiv_id: '2507.04026'
source_url: https://arxiv.org/abs/2507.04026
tags:
- system
- cancer
- patients
- patient
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an interactive AI-assisted system that adapts
  the Ottawa Personal Decision Guide into a retrieval-augmented generation workflow
  to help cancer patients prepare for medical visits. The system retrieves tailored
  medical knowledge, supports personal reflection, and generates personalized visit
  questions.
---

# Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide

## Quick Facts
- arXiv ID: 2507.04026
- Source URL: https://arxiv.org/abs/2507.04026
- Authors: Siyang Liu; Lawrence Chin-I An; Rada Mihalcea
- Reference count: 6
- One-line primary result: Interactive AI system helps cancer patients prepare for oncology visits with high usability and clinical faithfulness scores.

## Executive Summary
This study presents an interactive AI-assisted system that adapts the Ottawa Personal Decision Guide into a retrieval-augmented generation workflow to help cancer patients prepare for medical visits. The system retrieves tailored medical knowledge, supports personal reflection, and generates personalized visit questions. In a user study with 10 prostate cancer patients, the system achieved high usability (UMUX mean = 6.0/7), strong relevance of generated content (mean = 6.7/7), minimal editing needs (20% of users edited, 16% token change on average), and high clinical faithfulness (mean = 6.82/7). Participants found 3.3 of 5 "know-them" questions helpful and 3.5 of 5 "ask-them" questions useful for doctor visits. The clinical expert confirmed high accuracy of generated content.

## Method Summary
The system uses a two-stage RAG pipeline with LlamaIndex. First, it retrieves relevant medical content from a vector-indexed Prostate Cancer Foundation guidebook based on patient-selected topics and concerns. Then, it generates an editable personal narrative and visit questions (separating factual "know-them" questions from personalized "ask-them" questions). The frontend uses Gradio for structured interviews, knowledge panel display, narrative editing, and question generation. OpenAI API handles both embeddings and LLM generation tasks. The system was evaluated through a user study with 10 prostate cancer patients measuring usability, content relevance, editing behavior, question usefulness, and clinical faithfulness.

## Key Results
- High usability with UMUX score of 6.0/7 and System Usability Scale score of 86/100
- Strong knowledge panel relevance (mean = 6.7/7) and clinical faithfulness (mean = 6.82/7)
- Low editing burden: 20% of users edited narrative with 16% average token change
- Generated questions found useful: 3.3/5 "know-them" questions helpful, 3.5/5 "ask-them" questions useful for doctor visits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured knowledge gap identification before retrieval improves relevance of delivered content.
- Mechanism: The system first elicits patient concerns through topic selection and open-ended questions, then uses these signals to query a domain-specific vector index, targeting retrieval to the patient's specific decision context.
- Core assumption: Patients can articulate topic-level concerns even without technical vocabulary.
- Evidence anchors: Abstract states system helps "bridge knowledge gaps"; section 2.2 describes identifying patient knowledge gaps based on responses.
- Break condition: If users cannot articulate concerns or select irrelevant topics, retrieval quality degrades without explicit correction mechanism.

### Mechanism 2
- Claim: Grounding generation in expert-verified source documents constrains hallucination and enables clinical faithfulness.
- Mechanism: All generated content is conditioned on retrieved segments from the Prostate Cancer Foundation Patient Guide, with explicit separation between "know-them" (source-answerable) and "ask-them" (personalized synthesis).
- Core assumption: The source guidebook comprehensively covers relevant decision factors for localized prostate cancer.
- Evidence anchors: Abstract reports high clinical faithfulness (mean = 6.82/7); section 2.4 defines "know-them" questions as retrieved from validated guidebooks.
- Break condition: If patient concerns fall outside the guidebook scope, the system cannot generate faithful content and may produce generic outputs.

### Mechanism 3
- Claim: Editable first-person narratives create feedback loops that improve downstream personalization.
- Mechanism: The system generates a personalized summary that patients can edit (only 20% edited with 16% token change), which then serves as context for generating visit questions, ensuring questions reflect authentic patient priorities.
- Core assumption: Patients will engage with and correct the narrative if it misrepresents their situation.
- Evidence anchors: Abstract notes minimal editing needs (20% edited, 16% token change); section 2.3 describes supporting narrative agency.
- Break condition: If patients accept inaccurate narratives without editing, downstream question generation will be misaligned with actual concerns.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The core architecture depends on retrieving domain-specific medical content and conditioning LLM generation on it.
  - Quick check question: Can you explain why the system retrieves guidebook segments before generating answers rather than relying on the LLM's parametric knowledge?

- Concept: The Ottawa Personal Decision Guide Framework
  - Why needed here: The entire system workflow (Steps A→B→C) is adapted from this established decision aid.
  - Quick check question: How does the system map OPDG's three steps to its interface components?

- Concept: Semantic Similarity Search with Vector Embeddings
  - Why needed here: The custom book uploader creates vector indices for semantic retrieval.
  - Quick check question: If a patient asks about "sexual side effects" but the guidebook uses "erectile dysfunction," will semantic search retrieve the relevant section? Why or why not?

## Architecture Onboarding

- Component map: Frontend (Gradio web interface) -> Backend (LlamaIndex RAG pipeline) -> Knowledge Base (PDF guidebook → chunks → vector embeddings) -> OpenAI API (generation and embeddings)

- Critical path: 1. User selects topics and answers questions → 2. System queries vector index with user input → 3. Retrieved segments populate knowledge panel → 4. User reflects on values → 5. System generates editable narrative using conversation history + retrieved context → 6. User confirms/edits narrative → 7. System generates "know-them" and "ask-them" questions conditioned on narrative and retrieval → 8. Output displayed for visit preparation

- Design tradeoffs:
  - Structured vs. open-ended input: Topic selection reduces search space but may miss nuanced concerns
  - Editable vs. fixed narratives: Editing increases patient agency but adds friction; low edit rates suggest generation quality is sufficient
  - Single vs. multiple source documents: Current design uses one guidebook for consistency but limits scope; uploader enables extensibility

- Failure signatures:
  - Low relevance scores: Retrieval not matching patient concerns—check embedding quality, chunk size, or query formulation
  - High edit rates: Narrative generation not capturing patient intent—review prompt design or context window
  - Low question usefulness: Gap between patient values and generated questions—examine reflection-to-question generation prompt
  - Clinical faithfulness drops: Generation drifting from retrieved sources—add explicit citation requirements or constrained decoding

- First 3 experiments:
  1. Retrieval ablation: Replace semantic search with keyword matching to measure contribution of embeddings to relevance scores.
  2. Narrative editing correlation: Track whether patients who edit narratives rate final questions differently than those who don't.
  3. Guidebook expansion test: Upload a second cancer type guidebook and measure whether retrieval quality degrades due to domain mixing in the vector index.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the system's effectiveness generalize to other cancer types or complex medical conditions beyond localized prostate cancer?
- Basis in paper: The authors state in the Limitations section that they "tested it only on localized prostate cancer, so the results may not apply to other conditions."
- Why unresolved: The current study utilized a specific knowledge base and a cohort specific to that disease, which has distinct decision-making characteristics.
- What evidence would resolve it: Evaluation results from deploying the system with different knowledge bases (e.g., breast or lung cancer guides) and respective patient groups, measuring usability and relevance scores comparable to the prostate cancer study.

### Open Question 2
- Question: Does the use of this AI-assisted preparation tool improve actual patient-provider communication and decisional outcomes during real clinical visits?
- Basis in paper: The study evaluates "visit readiness" and content relevance, but the outcome measures rely on user feedback and expert review rather than measuring the quality of the actual subsequent medical consultation.
- Why unresolved: The study design focused on system usability and content validity in a simulated or preparatory context, stopping short of measuring real-world behavioral changes or clinical decision quality.
- What evidence would resolve it: A study design that records or surveys actual clinical encounters to measure physician satisfaction, patient anxiety reduction, and the alignment of final decisions with patient values.

### Open Question 3
- Question: How robust is the system's personalization capability when users provide sparse, ambiguous, or low-quality input during the structured interview?
- Basis in paper: The authors note that "the system's personalization depends on the quality of user input—brief answers may lead to less useful outputs."
- Why unresolved: While the study reports high relevance on average, it does not specifically analyze the correlation between input brevity and output quality or failure rates.
- What evidence would resolve it: A component-level error analysis examining the relationship between user input length/specificity and the resulting "helpfulness" ratings of the generated questions.

## Limitations

- Small sample size (n=10) from a single cancer center limits generalizability to broader patient populations and cancer types.
- System effectiveness depends heavily on the comprehensiveness of the uploaded guidebook, potentially limiting usefulness for topics outside the Prostate Cancer Foundation guide's scope.
- No formal validation of generated content with treating oncologists or post-visit patient experiences to confirm actual utility during medical consultations.

## Confidence

- Confidence in usability metrics (UMUX, Likert ratings) and clinical faithfulness (expert assessment): High
- Confidence in knowledge relevance and question usefulness (self-reported user ratings): Medium
- Confidence in claims about downstream impact (improved patient-provider communication, decision quality): Low

## Next Checks

1. **Post-visit validation study**: Track whether patients who used the system asked their generated questions during actual oncology visits and assess whether these questions led to productive conversations or informed decision-making.

2. **Multi-center deployment**: Test the system with diverse patient populations across different cancer centers and cancer types to evaluate generalizability and identify potential demographic or disease-specific limitations.

3. **Knowledge base expansion test**: Systematically evaluate retrieval quality and faithfulness when multiple cancer type guidebooks are uploaded simultaneously, measuring whether cross-domain mixing degrades performance or requires architectural adjustments.