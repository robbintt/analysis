---
ver: rpa2
title: 'REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease
  Diagnosis'
arxiv_id: '2510.04923'
source_url: https://arxiv.org/abs/2510.04923
tags:
- expert
- gating
- anatomical
- experts
- radiomics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces REN, the first anatomically-informed Mixture-of-Experts
  (MoE) framework specifically designed for medical imaging. By training seven specialized
  experts on distinct lung lobes and bilateral lung regions, REN leverages anatomical
  priors to capture region-specific pathological variations in interstitial lung disease
  (ILD).
---

# REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis

## Quick Facts
- arXiv ID: 2510.04923
- Source URL: https://arxiv.org/abs/2510.04923
- Reference count: 33
- Primary result: 12.5% AUC improvement over SwinUNETR baseline (0.8646±0.0467 vs 0.7685, p=0.031)

## Executive Summary
This study introduces REN, the first anatomically-informed Mixture-of-Experts (MoE) framework specifically designed for medical imaging. By training seven specialized experts on distinct lung lobes and bilateral lung regions, REN leverages anatomical priors to capture region-specific pathological variations in interstitial lung disease (ILD). The framework integrates radiomics biomarkers with deep learning features through multi-modal gating mechanisms that dynamically weight expert contributions. Applied to ILD classification, REN achieved an average AUC of 0.8646±0.0467, representing a 12.5% improvement over the SwinUNETR baseline (AUC 0.7685, p=0.031). Region-specific experts showed particularly strong performance in lower lobes (AUC 0.88-0.90), aligning with known disease progression patterns and surpassing traditional DL approaches.

## Method Summary
REN employs a 4-stage pipeline: (1) automated lung segmentation and lobe partitioning into 7 regions (5 lobes + 2 bilateral lungs), (2) training 28 regional experts per fold (7 regions × 4 architectures: CNN, ViT, Mamba, XGBoost radiomics), (3) validation-based gating strategy selection using 11 different approaches including performance-based, feature-based, and learned mechanisms, and (4) integration with a SwinUNETR backbone for end-to-end MoE with weighted expert fusion. The approach uses patient-level 5-fold cross-validation on 597 patients with 1,898 CT scans, incorporating radiomics features (107 PyRadiomics features per region) to guide expert weighting. Key innovations include anatomical region specialization, radiomics-guided gating, and staged training that decouples expert learning from expert combination.

## Key Results
- REN achieved an average AUC of 0.8646±0.0467, representing a 12.5% improvement over SwinUNETR baseline (p=0.031)
- Lower-lobe models achieved AUCs of 0.88-0.90, surpassing traditional DL approaches (CNN: 0.76-0.79) and aligning with known disease progression patterns
- Radiomics-based MoE models outperformed neural network-based MoEs in most region-fold comparisons (23/25 vs Mamba, 24/25 vs ViT, 17/25 vs CNN)
- Weighted ensembles consistently outperformed end-to-end gated counterparts, validating the staged training approach

## Why This Works (Mechanism)

### Mechanism 1
Anatomically-constrained expert assignment captures disease-relevant regional patterns that global models dilute. Seven experts receive masked CT inputs for specific lung regions (five lobes + bilateral lungs). ILD preferentially affects lower lobes early; specialized experts learn region-specific texture/density signatures rather than averaging across anatomically heterogeneous tissue. Core assumption: Disease manifestation follows predictable anatomical distributions governed by physiology. Evidence anchors: [abstract] "lower-lobe models achieved AUCs of 0.88–0.90, surpassing DL counterparts (CNN: 0.76–0.79) and aligning with known disease progression patterns"; [Results] "Model performance mirrored known ILD progression patterns, with highest AUCs in the left and right lower lobes (LLL/RLL), consistently exceeding 0.85 in the radiomics-based MoE approach. This aligns with clinical evidence that ILD typically begins in the lung bases." Break condition: Diseases with uniform organ distribution or diffuse patterns without regional preference would not benefit from anatomical specialization.

### Mechanism 2
Radiomics-guided gating provides pathology-aware reliability signals that outperform purely learned deep features on limited medical data. Radiomics features (GLCM, GLRLM texture families) encode clinically interpretable biomarkers. Validation AUCs from XGBoost radiomics experts weight deep expert outputs, coupling handcrafted pathology priors with learned representations. Core assumption: Radiomics captures discriminative signals that deep networks miss in small-data regimes. Evidence anchors: [abstract] "the radiomics-guided ensemble reached an average AUC of 0.8646±0.0467, a +12.5% improvement over the SwinUNETR baseline (AUC 0.7685, p=0.031)"; [Results] "Radiomics-based MoE models outperformed neural networks based MoEs in most region-fold comparisons: 23/25 vs. Mamba, 24/25 vs. ViT, and 17/25 vs. CNN." Break condition: Large-scale pretrained medical vision models may learn equivalent features end-to-end, reducing radiomics marginal utility.

### Mechanism 3
Staged training with fixed gating preserves regional specialization that end-to-end joint optimization obscures. Experts train independently on masked regions (Stage 2); gating weights are derived and frozen from validation data (Stage 3); only then does limited end-to-end fine-tuning occur (Stage 4). Decoupling prevents gradient interference across regions. Core assumption: Independent regional learning yields more robust specialists than simultaneous optimization. Evidence anchors: [Results] "weighted ensembles consistently outperform end-to-end gated counterparts. This validates our training strategy. This staged approach—training experts independently before combination—preserves specialized anatomical knowledge that joint optimization obscures"; [Methods] "Crucially, once selected, these gating weights are fixed and are not updated during subsequent inference or training. This staged design decouples expert learning from expert combination." Break condition: Tasks requiring tight cross-region feature interactions may need joint training for optimal coupling.

## Foundational Learning

- Concept: **Mixture-of-Experts routing fundamentals**
  - Why needed here: REN extends generic MoE with anatomical constraints; understanding baseline gating (softmax, load balancing, sparse activation) clarifies what the paper modifies.
  - Quick check question: Can you explain why sparsemax normalization might encourage expert collapse versus softmax in small medical datasets?

- Concept: **Radiomics feature families (texture, shape, first-order)**
  - Why needed here: 107 PyRadiomics features drive the highest-performing gating strategy; GLCM/GLRLM specifically excel in lower lobes.
  - Quick check question: Why would texture features outperform shape features for early-stage ILD detection?

- Concept: **Lung lobar anatomy and ILD pathophysiology**
  - Why needed here: Lower-lobe predominance of fibrotic changes motivates the seven-region decomposition; clinical interpretability depends on mapping expert outputs to anatomy.
  - Quick check question: Which two lobes combine to form the left lung, and why might lower lobes show higher AUCs in ILD classification?

## Architecture Onboarding

- Component map: lungmask segmentation → 7 binary masks (LUL, LLL, RUL, RML, RLL, Left Lung, Right Lung) → 28 independent experts per fold (7 regions × 4 architectures: CNN, ViT, Mamba, XGBoost) → 11 gating strategies (performance-based, feature-based, learned) → SwinUNETR backbone + weighted expert fusion → multi-component loss (CE + entropy + load balance + diversity)

- Critical path: Preprocessing → individual expert training → validation-based gating selection → frozen ensemble inference. Do not skip Stage 3 gating extraction; it determines the 12.5% gain.

- Design tradeoffs:
  - Five-lobe vs. seven-region: Five lobes yield 1.4% higher AUC; bilateral regions introduce redundancy.
  - Staged vs. end-to-end: Staged preserves specialization but requires 5+ GPU-hours per fold vs. 0.91 for baseline.
  - Dense vs. sparse activation: REN activates all experts (clinical completeness) rather than sparse routing (computational efficiency).

- Failure signatures:
  - Expert collapse: High usage variance + high cosine similarity (>0.8) indicates regularization failure (see Table IV).
  - Upper-lobe underperformance: AUC <0.75 suggests texture features insufficient; consider additional augmentations or alternative architectures.
  - Segmentation error propagation: lungmask boundary errors corrupt masked inputs; validate masks before Stage 2.

- First 3 experiments:
  1. Replicate Stage 2 single-region radiomics expert on one lobe (LLL or RLL) with 5-fold patient-level CV; confirm AUC 0.88+ to validate preprocessing pipeline.
  2. Implement validation-AUC-weighted ensemble (Equation 1–2) with frozen weights; compare against SwinUNETR baseline to verify 12.5% improvement holds.
  3. Ablate diversity regularization (set λ_diversity=0); measure cosine similarity increase to confirm regularization mechanism per Table IV.

## Open Questions the Paper Calls Out

- How does REN's performance and gating behavior generalize across diverse scanner protocols, patient demographics, and ILD etiologies beyond the single-institution systemic sclerosis cohort? The authors state "the dataset originates from a single institution and focuses exclusively on systemic sclerosis-related ILD, which may limit generalizability to other ILD subtypes and populations" and describe "planned multi-institutional validation through collaborations with external academic centers" as ongoing work. The 597-patient cohort derives entirely from the Northwestern Scleroderma Registry (2001–2023), with no external validation performed. Publicly available ILD cohorts lack the required regional annotations or focus on different disease etiologies, constraining straightforward replication.

- Can REN be extended from binary ILD detection to multi-class classification distinguishing ILD subtypes with different regional involvement patterns? The authors acknowledge "our current binary ILD classification does not capture the heterogeneity across ILD subtypes" and list "expansion to multi-class ILD classification" as a future direction. Different ILD subtypes exhibit characteristic distributions (basal/subpleural in UIP/IPF, mid-upper lung in sarcoidosis, peribronchovascular in connective tissue disease–ILD) that may require subtype-specific expert weighting strategies or multi-task training.

- How robust is REN to errors in automated lung and lobe segmentation, particularly at anatomical boundaries where pathology may affect segmentation accuracy? The authors note "reliance on automated lungmask segmentation may propagate boundary errors throughout the pipeline" and state "Future work will evaluate sensitivity to segmentation quality through controlled perturbation experiments." Expert specialization depends entirely on accurate lobe segmentation to generate masked inputs. Boundary errors could misassign pathological regions to incorrect experts, degrading the anatomical prior assumption that underpins the framework.

## Limitations
- Private dataset access blocks full replication; performance claims depend on Northwestern Scleroderma Registry's 597-patient cohort and 1,898 CT scans.
- Radiomics feature engineering quality critically determines gating strategy performance; feature extraction pipelines (HU bin width, texture matrix settings) are not fully specified.
- Expert diversity regularization effectiveness depends on unmeasured baseline collapse risk without diversity term.
- Clinical interpretability relies on anatomical priors that may not generalize to other organ systems or diffuse disease patterns.

## Confidence
- **High confidence** in anatomically-constrained MoE framework improving AUC over global baselines (0.8646 vs. 0.7685, p=0.031) based on rigorous patient-level cross-validation.
- **Medium confidence** in radiomics gating superiority over learned deep features due to small-data regime; large pretrained models may close this gap.
- **Medium confidence** in staged training benefits; no ablation against end-to-end optimization in the same cohort.
- **Low confidence** in cross-disease generalization without testing on non-ILD or non-systemic sclerosis populations.

## Next Checks
1. Test REN on publicly available ILD datasets (e.g., LIDC-IDRI) to verify architecture generalizes beyond private cohort.
2. Compare radiomics gating against end-to-end learned attention gating on same data to quantify radiomics marginal value.
3. Ablate expert diversity regularization to measure performance drop and confirm its role in preventing expert collapse.