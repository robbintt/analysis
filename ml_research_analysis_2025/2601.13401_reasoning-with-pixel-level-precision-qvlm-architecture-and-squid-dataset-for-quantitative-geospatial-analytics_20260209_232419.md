---
ver: rpa2
title: 'Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset
  for Quantitative Geospatial Analytics'
arxiv_id: '2601.13401'
source_url: https://arxiv.org/abs/2601.13401
tags:
- image
- area
- segmentation
- qvlm
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QVLM, an architecture that overcomes the
  pixel-level precision limitations of current Vision-Language Models (VLMs) by generating
  executable code that operates directly on segmentation masks rather than compressed
  embeddings. The approach decouples language understanding from visual analysis,
  enabling accurate quantitative spatial reasoning for satellite imagery.
---

# Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics

## Quick Facts
- **arXiv ID:** 2601.13401
- **Source URL:** https://arxiv.org/abs/2601.13401
- **Reference count:** 40
- **Key outcome:** QVLM achieves 42.0% accuracy on SQuID vs 28.1% for traditional VLMs by generating executable code that operates on segmentation masks rather than compressed embeddings

## Executive Summary
This paper introduces QVLM, an architecture that overcomes pixel-level precision limitations of current Vision-Language Models (VLMs) by generating executable code that operates directly on segmentation masks rather than compressed embeddings. The approach decouples language understanding from visual analysis, enabling accurate quantitative spatial reasoning for satellite imagery. The authors also introduce SQuID, a benchmark of 2,000 satellite image question-answer pairs with human-validated answer ranges, designed to test quantitative spatial reasoning across three difficulty tiers. QVLM using GPT-5 as code generator achieves 42.0% accuracy on SQuID compared to 28.1% for traditional VLMs prompted with image-question pairs, demonstrating that architectural decoupling through code generation and segmentation enables better quantitative spatial reasoning.

## Method Summary
QVLM architecture generates executable Python code from natural language questions, which orchestrates specialized segmentation models operating directly on pixel-accurate masks. The system uses either ConvNeXt-UNet or DINOv3-Mask2Former for segmentation, then performs geometric operations (counting, distance calculation, area computation) on discrete masks rather than continuous embeddings. The approach preserves spatial precision lost in traditional VLM compression through patch embeddings. SQuID benchmark contains 2,000 question-answer pairs with human-validated answer ranges using Median Absolute Deviation from 10 annotators. QVLM achieves 42.0% accuracy compared to 28.1% for traditional VLMs on SQuID, with the largest gains in geometric operations like fragmentation (81.63% vs 26.53%) and connectivity (74.04% vs 37.5%).

## Key Results
- QVLM achieves 42.0% accuracy on SQuID vs 28.1% for traditional VLMs using GPT-5
- Largest gains in geometric operations: fragmentation (81.63% vs 26.53%), connectivity (74.04% vs 37.5%), counting (56.74% vs 36.52%)
- Switching segmentation backend from ConvNeXt-UNet to DINOv3-Mask2Former reduces accuracy from 42.0% to 30.8%
- SQuID benchmark established with 2,000 question-answer pairs across three difficulty tiers

## Why This Works (Mechanism)

### Mechanism 1: Patch Embedding Compression
Traditional VLMs compress images through patch embeddings (typically 16×16 pixels → single token), causing 256-fold compression that eliminates spatial indexing. QVLM bypasses this by having an LLM generate executable code that operates on uncompressed segmentation masks. When a 1024×1024 satellite image is processed through a vision encoder, patches are compressed into a 64×64 grid of tokens—reducing 1,048,576 pixels to 4,096 tokens. This compression architecturally discards pixel-level precision that cannot be recovered through training.

### Mechanism 2: Executable Code for Spatial Operations
The LLM generates Python code that calls segmentation APIs and performs operations (counting, distance calculation, area computation) on discrete masks rather than continuous embeddings. This preserves exact spatial relationships through explicit geometric operations that neural networks cannot reliably perform. Geometric operations show largest gains: fragmentation (81.63% vs 26.53%), connectivity (74.04% vs 37.5%), counting (56.74% vs 36.52%).

### Mechanism 3: Human-Validated Answer Ranges
SQuID uses Median Absolute Deviation (MAD) from 10 human annotators to define acceptable ranges (±1.735% for percentages, ±19% normalized for counts). This accounts for legitimate disagreement in boundary cases. Inter-rater reliability shows count questions obtaining highest agreement (α= 0.959) and proximity questions lowest (α= 0.424), indicating inherent task difficulty.

## Foundational Learning

- **Patch Embedding Compression in Vision Transformers**
  - Why needed here: Understanding why VLMs fail at counting requires knowing how ViT-style encoders fundamentally destroy spatial locality.
  - Quick check question: "If a 512×512 image is divided into 16×16 patches, how many tokens result, and what spatial resolution does each token represent?"

- **Segmentation Masks as Geometric Representations**
  - Why needed here: QVLM's core innovation is operating on binary masks rather than embeddings; understanding mask representations is essential.
  - Quick check question: "Given a binary mask for 'buildings', what operations would you need to count disconnected instances larger than 0.1 hectares?"

- **Code Generation for Tool Orchestration**
  - Why needed here: QVLM uses LLMs to generate executable Python that calls vision APIs; this paradigm differs fundamentally from end-to-end neural processing.
  - Quick check question: "What are the safety and reliability trade-offs between generating executable code versus direct neural prediction?"

## Architecture Onboarding

- **Component map:** LLM Code Generator (GPT-5, gpt-oss-120B, or Llama-3.1-8B) → Segmentation Server (ConvNeXt-UNet or DINOv3-Mask2Former) → Spatial Operations API → Execution Sandbox
- **Critical path:** Question → LLM receives developer prompt + API signatures → Code generation → Segmentation inference → Mask operations → Final answer. Accuracy bottleneck is typically segmentation quality (switching ConvNeXt→DINOv3 drops accuracy from 42.0% to 30.8%).
- **Design tradeoffs:** Slower inference than VLMs (two-stage: segmentation + execution), higher token consumption (1600 tokens for developer prompt vs 700 for VLM encoding), closed-vocabulary limitation (only 8 classes currently supported), modularity benefit: components upgrade independently without retraining
- **Failure signatures:** Over-segmentation: "Model counted 9 urban regions ≥0.1 ha vs expected 5±1" (Figure 10), incorrect magnitude estimation: VLMs guess wild values (20000 ha vs 105.55 ha) when lacking geometric tools (Figure 11), proximity questions show lowest human agreement (α=0.424)
- **First 3 experiments:** 1) Replicate baseline comparison on SQuID Tier 1 only: QVLM(GPT-5+ConvNeXt) vs GPT-5 VLM on basic counting/percentage questions to validate the 14.22 point improvement claim, 2) Ablate segmentation quality: Use progressively noisier segmentation masks (add synthetic noise) to quantify the relationship between mask quality and final accuracy, 3) Test code generator quality: Compare GPT-5 vs Llama-3.1-8B on the same segmentation backend to isolate LLM contribution (expected: 42.0% vs 29.0% from paper)

## Open Questions the Paper Calls Out
1. Can open-vocabulary segmentation models be integrated into QVLM to remove the current closed-vocabulary constraint without sacrificing mask precision? (Future work section states: "Future work should explore open-vocabulary segmentation, end-to-end training, and hybrid architectures...")
2. Can end-to-end training jointly optimize the code generation and segmentation components to improve quantitative reasoning performance? (Listed as future work direction alongside open-vocabulary segmentation)
3. Can hybrid architectures effectively route queries between quantitative code-generation and qualitative VLM reasoning based on task requirements? (Proposed as third future direction; also inferred from paper's claim that decoupling benefits quantitative tasks specifically)

## Limitations
- Segmentation model quality critically affects accuracy, with performance dropping from 42.0% to 30.8% when switching backends
- Current architecture supports only 8 land-use classes, limiting flexibility to novel land-cover categories
- Two-stage inference process (segmentation + execution) is slower than traditional VLMs

## Confidence
- **High Confidence**: The core architectural claim that decoupling language understanding from visual analysis through code generation preserves precision that neural compression destroys
- **Medium Confidence**: The specific performance numbers on SQuID, given the limited dataset size and potential domain-specific training of segmentation models
- **Low Confidence**: Long-term scalability claims, as the approach currently supports only 8 land-use classes and requires significant computational overhead

## Next Checks
1. **Geographic Generalization Test**: Evaluate QVLM on satellite imagery from different geographic regions, sensors, and resolutions not represented in the training data to assess segmentation model robustness and overall accuracy retention
2. **Ablation on Segmentation Quality**: Systematically vary segmentation mask quality through controlled noise injection or use progressively lower-quality segmentation models to quantify the relationship between mask precision and final answer accuracy
3. **Alternative Code Generation Comparison**: Compare QVLM's performance using different LLMs (including open alternatives to GPT-5) while holding segmentation backend constant, to isolate the contribution of code generation quality from other architectural components