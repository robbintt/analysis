---
ver: rpa2
title: 'MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and
  Attention Distillation'
arxiv_id: '2501.02754'
source_url: https://arxiv.org/abs/2501.02754
tags:
- backdoor
- mbtsad
- attention
- clean
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks on language
  models, where attackers can poison models during training or fine-tuning to produce
  malicious outputs when specific triggers are present. The proposed method, MBTSAD,
  mitigates backdoors without requiring access to pre-trained model weights, a significant
  limitation of existing approaches.
---

# MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation

## Quick Facts
- arXiv ID: 2501.02754
- Source URL: https://arxiv.org/abs/2501.02754
- Reference count: 40
- Primary result: Mitigates backdoors in language models without requiring access to pre-trained model weights

## Executive Summary
This paper addresses the critical challenge of defending language models against backdoor attacks when pre-trained model weights are inaccessible. The proposed MBTSAD method introduces a two-step defense mechanism that first retrains the backdoored model on an augmented dataset created through token splitting, introducing Out-of-Distribution data to disrupt backdoor patterns. It then applies attention distillation using the retrained model as a teacher and the backdoored model as a student to further eliminate backdoors while preserving clean data performance. Experimental results demonstrate that MBTSAD achieves comparable effectiveness to weight-based defense methods, reducing attack success rates to below 20% and as low as 9.65% on SST-2 dataset while maintaining clean data accuracy.

## Method Summary
MBTSAD addresses backdoor attacks through a novel two-step approach that doesn't require pre-trained model weights. The first step involves token splitting, where the original training data is augmented by splitting tokens into subword units, creating Out-of-Distribution data that disrupts the backdoor trigger patterns. The backdoored model is then retrained on this augmented dataset. The second step applies attention distillation, where the retrained model serves as a teacher and the backdoored model as a student, transferring clean attention patterns while eliminating backdoor behaviors. This approach is particularly valuable for scenarios where model weights have been modified or where access to original pre-trained weights is restricted.

## Key Results
- MBTSAD reduces attack success rates to below 20% and as low as 9.65% on SST-2 dataset
- The method achieves comparable backdoor mitigation effectiveness to approaches requiring pre-trained weights
- Maintains clean data accuracy while defending against backdoor attacks
- Successfully tested on both SST-2 and IMDb datasets

## Why This Works (Mechanism)
The MBTSAD method works by disrupting backdoor patterns through data augmentation and knowledge transfer. Token splitting creates OOD data that breaks the statistical correlation between trigger tokens and malicious outputs, while attention distillation transfers clean attention patterns from a retrained model to the backdoored model, effectively overwriting the backdoor behavior without requiring access to original weights.

## Foundational Learning
- **Token splitting**: Breaking tokens into subword units to create Out-of-Distribution data - needed to disrupt backdoor trigger patterns; quick check: verify split tokens don't recreate original trigger sequences
- **Attention distillation**: Knowledge transfer from teacher to student model through attention mechanism - needed to preserve clean data performance while eliminating backdoors; quick check: compare attention weights before and after distillation
- **OOD data injection**: Introducing out-of-distribution samples during training - needed to break statistical dependencies in backdoored models; quick check: measure distribution shift in augmented dataset
- **Fine-tuning without pre-trained weights**: Adapting modified models without original parameters - needed for practical deployment scenarios; quick check: validate performance on clean validation set
- **Backdoor trigger patterns**: Statistical correlations between specific inputs and malicious outputs - needed to understand attack vectors; quick check: analyze activation patterns for trigger presence
- **Teacher-student knowledge transfer**: Model-to-model knowledge distillation - needed to maintain performance while removing malicious behaviors; quick check: compare teacher and student attention maps

## Architecture Onboarding

**Component Map**: Token Splitting -> Retraining -> Attention Distillation -> Clean Model

**Critical Path**: The method follows a sequential pipeline where token splitting creates augmented data, retraining establishes clean baseline behavior, and attention distillation transfers clean patterns while eliminating backdoors.

**Design Tradeoffs**: The approach sacrifices some training efficiency for weight accessibility, as it requires retraining the model but gains the ability to defend against backdoors without original weights. The token splitting strategy may introduce computational overhead but provides robustness against various trigger types.

**Failure Signatures**: The method may fail when backdoor triggers are highly sophisticated or when token splitting doesn't sufficiently disrupt trigger patterns. Attention distillation might be ineffective if the teacher model retains residual backdoor behaviors or if the student model cannot adequately learn from the teacher.

**First Experiments**: 
1. Test MBTSAD on SST-2 dataset with simple token-based backdoor triggers to establish baseline effectiveness
2. Evaluate performance degradation on clean data after backdoor mitigation to ensure functionality is preserved
3. Compare attack success rates before and after MBTSAD application across different trigger intensities

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness claims are based on only two datasets (SST-2 and IMDb), limiting generalizability across diverse NLP tasks
- Token splitting augmentation may not be universally applicable to all backdoor trigger types or language model architectures
- Attention distillation assumes the retrained model serves as an effective teacher, but potential knowledge gaps between teacher and student models are not thoroughly analyzed

## Confidence

**High confidence**: The core contribution of addressing backdoor mitigation without requiring pre-trained weights is novel and technically sound.

**Medium confidence**: The two-step approach (token splitting + attention distillation) appears methodologically reasonable, but effectiveness across broader contexts remains to be validated.

**Low confidence**: The claim that MBTSAD achieves "comparable" effectiveness to weight-based methods needs more rigorous comparative analysis across multiple attack scenarios and model sizes.

## Next Checks

1. Evaluate MBTSAD across at least five diverse NLP datasets (beyond sentiment analysis) to assess generalizability to different task types and trigger mechanisms.

2. Conduct ablation studies to quantify the individual contributions of token splitting versus attention distillation to overall backdoor mitigation performance.

3. Test MBTSAD against advanced backdoor attack variants, including those designed to be resistant to fine-tuning and those with dynamic or semantic triggers rather than simple token-based triggers.