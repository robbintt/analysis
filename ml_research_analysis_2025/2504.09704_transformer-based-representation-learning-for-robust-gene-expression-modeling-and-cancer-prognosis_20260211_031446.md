---
ver: rpa2
title: Transformer-Based Representation Learning for Robust Gene Expression Modeling
  and Cancer Prognosis
arxiv_id: '2504.09704'
source_url: https://arxiv.org/abs/2504.09704
tags:
- genes
- gene
- expression
- gexbert
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GexBERT, a transformer-based autoencoder
  framework for robust representation learning of gene expression data. The model
  is pretrained on large-scale transcriptomic profiles using a masking and restoration
  objective that captures co-expression relationships among genes.
---

# Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis

## Quick Facts
- **arXiv ID**: 2504.09704
- **Source URL**: https://arxiv.org/abs/2504.09704
- **Reference count**: 40
- **Primary result**: Transformer-based autoencoder achieves state-of-the-art cancer classification (0.94 accuracy) and survival prediction (0.625 C-index) from limited gene subsets.

## Executive Summary
This paper introduces GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. The model is pretrained on large-scale transcriptomic profiles using a masking and restoration objective that captures co-expression relationships among genes. GexBERT is evaluated on three cancer research tasks: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. It achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. The attention-based interpretability reveals biologically meaningful gene patterns across cancer types.

## Method Summary
GexBERT is a transformer encoder-decoder architecture that learns gene expression representations through pretraining on TCGA data (11,014 patients, 20,311 genes). Expression values are discretized into 64 categorical bins with learnable embeddings, and gene embeddings are randomly initialized. The model samples 512 genes per patient, encodes them with self-attention, and restores masked gene expressions. Pretraining occurs in two phases: same-gene restoration (10,000 epochs) followed by different-gene restoration (1,500 epochs). Downstream tasks use the CLS token embedding for classification or restore prognostic anchor genes for survival prediction using Cox proportional hazards models.

## Key Results
- Achieves pan-cancer classification accuracy up to 0.94 using only 64-256 input genes
- Improves cancer-specific survival prediction C-index to 0.625 by restoring prognostic anchor genes
- Outperforms conventional imputation methods under high missingness (1-50% rates)
- Attention patterns reveal biologically meaningful gene interactions across cancer types
- Maintains robustness to missing data while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Masking and restoration pretraining induces co-expression relationship learning across gene subsets.
- **Mechanism**: During pretraining, the model encodes a subset of genes and restores their expression values (or a different subset). The transformer encoder learns to predict masked values from contextual genes via self-attention, forcing it to capture which genes carry information about which others.
- **Core assumption**: Genes have consistent co-expression structure across patients that can be approximated by attention over randomly sampled subsets.
- **Evidence anchors**: [abstract] "pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes"; [section 3.1] "The encoder randomly selects a set of genes... The summary embedding... is then used to decode the expression value of another set of genes"
- **Break condition**: If co-expression relationships are highly patient-specific or context-dependent beyond what sampling captures, the learned embeddings may not transfer well.

### Mechanism 2
- **Claim**: Discretizing continuous expression values into learnable value embeddings preserves quantitative information while enabling transformer-style processing.
- **Mechanism**: Expression values are binned into 64 categorical levels; each level has a learnable embedding. The final gene representation is the sum of the gene embedding and the value embedding.
- **Core assumption**: 64 bins provide sufficient resolution to capture biologically meaningful expression differences.
- **Evidence anchors**: [section 3.3] "we discretize them into N_levels categorical bins, where each level j is associated with a learnable embedding vector e_j ∈ R^d"; [section 5.4] "genes with greater variability in expression... tend to receive higher attention weights"
- **Break condition**: If prognostic signals require finer granularity than 64 bins, discretization could lose information.

### Mechanism 3
- **Claim**: Restoring prognostic anchor genes from limited input genes improves survival prediction by leveraging learned co-expression patterns.
- **Mechanism**: Anchor genes with high univariate CoxPH significance are pre-selected. During evaluation, the model encodes available input genes and restores the anchor genes' expression; restored values are fed to a CoxPH model.
- **Core assumption**: Anchor genes' expression can be reliably inferred from other co-expressed genes even when not directly measured.
- **Evidence anchors**: [abstract] "improves survival prediction by restoring expression of prognostic anchor genes"; [section 5.2] "with up to 256 input genes, GexBERT's restored expression levels outperformed the original expression levels by 1.7%, 1.4%, and 1.3%"
- **Break condition**: If input genes have weak co-expression with anchor genes, restoration adds noise rather than signal.

## Foundational Learning

- **Concept**: Self-attention and transformer encoder-decoder
  - **Why needed here**: GexBERT uses multi-head self-attention to model gene-gene interactions; the decoder attends to the cls token to restore masked genes.
  - **Quick check question**: Can you explain why self-attention complexity scales as O(n²) and why random sampling mitigates this for 20,000+ genes?

- **Concept**: Masked autoencoding (BERT-style pretraining)
  - **Why needed here**: The pretraining objective masks and restores expression values, analogous to BERT's MLM but adapted for continuous data.
  - **Quick check question**: How does masking encourage the model to learn contextual representations rather than memorizing per-gene patterns?

- **Concept**: Cox proportional hazards model
  - **Why needed here**: Survival prediction uses CoxPH on restored anchor gene expression; the concordance index (C-index) is the evaluation metric.
  - **Quick check question**: What does a C-index of 0.5 vs. 0.625 imply about model discrimination?

## Architecture Onboarding

- **Component map**: Gene embedding layer (20,311 genes × 200-dim) -> Value embedding layer (64 levels × 200-dim) -> Transformer encoder (8 layers, 8 heads) -> Transformer decoder (8 layers, 8 heads) -> Linear projection head (200-dim → scalar)

- **Critical path**:
  1. Sample 512 genes per patient (random)
  2. Discretize expression → value tokens; sum with gene embeddings
  3. Prepend cls token; pass through encoder
  4. For restoration: concatenate cls embedding with masked gene embeddings; decode
  5. For downstream tasks: extract cls embedding (classification) or restore anchor genes (survival)

- **Design tradeoffs**:
  - 512 genes per iteration reduces quadratic complexity but may miss long-range interactions
  - 64 value bins balance granularity vs. embedding vocabulary size
  - Two-phase pretraining (same-gene restoration → different-gene restoration) improves cls token quality (ablation: 0.940 vs. 0.920 accuracy at 64 genes)

- **Failure signatures**:
  - Classification accuracy plateaus at ~0.979 beyond 512 genes
  - Restored anchor genes underperform original at 512+ input genes
  - Summary embedding alone poorly predicts survival (C-index ~0.55 vs. 0.62 with anchor restoration)
  - Performance degrades gracefully with missingness but C-index drops below 0.57 at 50% missing

- **First 3 experiments**:
  1. **Sanity check pretraining**: Run 100 epochs on subset; verify restoration MSE decreases (target: ~0.16 after 10,000 epochs per paper).
  2. **Pan-cancer classification probe**: Freeze encoder, train linear classifier on cls embeddings with 64, 256, 1024 input genes; compare to PCA-200 baseline.
  3. **Missing value robustness**: Inject 10-50% MCAR missingness; compare GexBERT imputation vs. KNN vs. mean imputation on downstream survival C-index.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can GexBERT generalize effectively to independently generated datasets, specifically regarding cross-platform transfer (e.g., microarray to RNA-seq) and diverse patient populations?
- **Basis in paper**: [explicit] The Discussion states future studies should assess generalization across independently generated datasets, including cross-platform and cross-population settings.
- **Why unresolved**: The current study evaluated the model on held-out TCGA data, which shares common data acquisition and processing pipelines.
- **What evidence would resolve it**: Benchmarking GexBERT on external cohorts (e.g., GEO or ICGC) with different measurement technologies and demographic distributions.

### Open Question 2
- **Question**: How does GexBERT's imputation performance change when facing structured or batch-specific missingness compared to the simulated Missing Completely At Random (MCAR) scenarios?
- **Basis in paper**: [explicit] The Discussion notes that real-world data may exhibit structured or batch-specific missingness that could influence imputation performance differently than the MCAR simulation used.
- **Why unresolved**: The experiments only simulated random missingness; performance on Missing Not At Random (MNAR) or Missing At Random (MAR) mechanisms remains untested.
- **What evidence would resolve it**: Evaluation on datasets with known technical dropout patterns or systematic batch effects causing non-random missing values.

### Open Question 3
- **Question**: Can the GexBERT framework be extended to integrate multi-omics data (e.g., DNA methylation or mutations) to further enhance prognosis prediction?
- **Basis in paper**: [inferred] The Conclusion mentions the model's application facilitates "integrative analysis in multi-omics research," though the current implementation utilizes only transcriptomic profiles.
- **Why unresolved**: The current architecture is tailored for gene expression symbols and values; it lacks mechanisms to process disparate omics data types.
- **What evidence would resolve it**: A modified GexBERT architecture that successfully fuses transcriptomic data with other molecular layers to improve concordance indices.

## Limitations
- Discretization into 64 bins may lose information that affects downstream tasks requiring fine expression granularity.
- Anchor gene selection lacks detailed methodology and sensitivity analysis for optimal set size.
- State-of-the-art claims lack direct comparison to established baselines like scBERT or scVI on identical datasets.

## Confidence
- **Pan-cancer classification accuracy (up to 0.94)**: High confidence - Multiple experiments show consistent improvement over PCA with clear ablation demonstrating 0.940 vs 0.920 accuracy difference.
- **Survival prediction improvement via anchor gene restoration (1.3-1.7% gain)**: Medium confidence - Statistically significant improvements shown, but anchor gene selection method lacks detail and generalizability across cancer types is unclear.
- **Missing value imputation outperforming conventional methods**: Medium confidence - Downstream survival C-index improvements demonstrated, but comparison limited to KNN and mean imputation without modern imputation baselines like MICE or autoencoders.

## Next Checks
1. **Generalization test**: Apply GexBERT pretrained on TCGA to an independent cancer dataset (e.g., GEO or ICGC) to verify learned representations transfer across cohorts and platforms.
2. **Alternative discretization sensitivity**: Repeat pretraining and downstream tasks with 32, 128, and 256 expression bins to quantify information loss/gain from current 64-bin choice.
3. **Anchor gene ablation study**: Systematically vary the number of anchor genes (from 1 to 50) in survival prediction to determine optimal set size and assess robustness when prognostic genes are missing or noisy.