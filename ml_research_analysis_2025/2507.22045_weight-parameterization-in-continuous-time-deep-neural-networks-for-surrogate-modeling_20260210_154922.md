---
ver: rpa2
title: Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate
  Modeling
arxiv_id: '2507.22045'
source_url: https://arxiv.org/abs/2507.22045
tags:
- neural
- weights
- training
- time
- legendre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies weight parameterization strategies for continuous-time
  deep neural networks (CT-DNNs) in surrogate modeling tasks. The authors investigate
  both monomial and Legendre polynomial bases within neural ODE and ResNet architectures
  under discretize-then-optimize and optimize-then-discretize training paradigms.
---

# Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling

## Quick Facts
- **arXiv ID**: 2507.22045
- **Source URL**: https://arxiv.org/abs/2507.22045
- **Reference count**: 36
- **Primary result**: Legendre polynomial parameterization yields 8× fewer function evaluations and more stable training than monomial bases in continuous-time DNNs for surrogate modeling.

## Executive Summary
This paper investigates weight parameterization strategies for continuous-time deep neural networks in surrogate modeling tasks. The authors compare monomial and Legendre polynomial bases within neural ODE and ResNet architectures under two training paradigms: discretize-then-optimize and optimize-then-discretize. Their key finding is that Legendre parameterization provides superior computational efficiency (fewer function evaluations) and training stability while maintaining accuracy comparable to or better than unconstrained models. The study demonstrates these benefits across three high-dimensional benchmark problems, showing that orthogonal polynomial bases offer favorable tradeoffs between model expressivity and training efficiency in continuous-time DNNs.

## Method Summary
The method involves parameterizing time-varying weights θ(t) in continuous-time DNNs using polynomial bases, specifically monomials and Legendre polynomials. For a polynomial of degree d, weights are expressed as θ_P(t) = Σ p_i(t)θ_i where θ_i are trainable coefficients. The authors implement this within both ResNet architectures (discretize-then-optimize) using forward Euler integration and neural ODEs (optimize-then-discretize) using the DOPRI5 adaptive solver. Training employs ADAM optimization with batch size 32 and learning rate 0.001 over 1000 epochs. The key innovation is constraining weight evolution to low-dimensional polynomial subspaces, which acts as implicit regularization and improves numerical conditioning during optimization.

## Key Results
- Legendre-parameterized neural ODEs require approximately 9.17×10⁷ function evaluations versus over 7.87×10⁸ for monomial parameterizations, yielding nearly 8× improvement in computational efficiency
- Across three high-dimensional benchmark problems (ELM, CDR, DCR), Legendre parameterization consistently outperforms monomial approaches in both accuracy and convergence speed
- Legendre bases provide more stable training dynamics, reducing the risk of gradient explosion or vanishing that plagues monomial parameterizations
- The constrained parameterization improves generalization in data-scarce surrogate modeling scenarios by preventing overfitting to high-frequency temporal variations

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Polynomial Bases Stabilize Gradient Flow
Monomial bases produce ill-conditioned Vandermonde matrices where small weight perturbations cause large solution instabilities. Legendre polynomials maintain orthogonality, keeping the system well-conditioned and preventing gradient explosion or stalling during backpropagation. This numerical stability is critical for the adjoint method used in neural ODE training.

### Mechanism 2: Low-Dimensional Subspace as Implicit Regularizer
By constraining weights to a sum of basis functions, the dimensionality drops from N time steps to d basis coefficients. This restricts the network to learning smoother temporal dynamics, acting as implicit regularization that reduces overfitting risk in data-scarce surrogate modeling tasks where the underlying physical dynamics are sufficiently smooth.

### Mechanism 3: Smooth Weight Evolution Reduces Computational Burden
Adaptive ODE solvers adjust step size based on error estimates. Erratic monomial weights create jagged vector fields, forcing tiny steps. Smooth Legendre weights produce smoother trajectories, allowing larger efficient steps. The primary computational cost driver in Neural ODEs is the number of function evaluations dictated by the solver's step adjustment.

## Foundational Learning

- **Concept: Neural ODEs & ResNets as Dynamical Systems**
  - Why needed: The paper frames deep learning as solving an Initial Value Problem (IVP). You must distinguish between discretizing first (ResNet, layer-by-layer) vs. optimizing in continuous time (Neural ODE, adjoint method).
  - Quick check: Does a ResNet correspond to discretize-then-optimize or optimize-then-discretize?

- **Concept: Vandermonde Matrices & Conditioning**
  - Why needed: The core failure mode of monomials is numerical instability rooted in the Vandermonde matrix condition number. Understanding this explains why "more expressivity" (high degree monomials) leads to failure.
  - Quick check: Why does the condition number of a monomial Vandermonde matrix grow exponentially with polynomial degree?

- **Concept: The Adjoint Method**
  - Why needed: For Neural ODEs, gradients aren't computed via standard backprop but by solving a backward ODE (the adjoint). Instability here kills training.
  - Quick check: In the optimize-then-discretize paradigm, when is the adjoint ODE solved relative to the forward pass?

## Architecture Onboarding

- **Component map**: Input y → Encoder σ(K_in·y + b_in) → Dynamics Layer f(u,t,θ(t)) → Integrator (DOPRI5 or Forward Euler) → Output ŷ
- **Critical path**: 1) Initialize trainable coefficients Θ = [θ₁,...,θ_d]; 2) For each solver query at time t, compute instantaneous weights θ(t) = Θ·Basis(t); 3) Feed θ(t) into network dynamics f; 4) Solver updates state u(t)
- **Design tradeoffs**: Basis choice (monomials cheap but toxic vs. Legendre slight overhead but stable); Degree d (higher increases expressivity but risks overfitting); Architecture (ResNets easier to debug vs. Neural ODEs more memory efficient but sensitive)
- **Failure signatures**: Stalling Loss (training loss plateaus immediately - switch to Legendre); Exploding Function Evaluations (NFE >> 10⁸ - reduce polynomial degree or switch to Legendre); Non-convergence in Hamiltonian Nets (wrong integrator - use Verlet)
- **First 3 experiments**: 1) Implement shallow ResNet on ELM dataset, compare monomial vs. Legendre (d=3) loss curves; 2) Implement Neural ODE for DCR, measure NFE difference between bases; 3) Fix Hamiltonian ResNet, vary d ∈ {3,4,5,6} on DCR to observe DoF vs. Test Error tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can non-polynomial parameterizations, such as Fourier series, splines, or neural network-based time embeddings, outperform Legendre polynomials in capturing periodic or highly nonlinear temporal dynamics? The study restricted its scope to monomial and Legendre polynomial bases, leaving open whether alternative representations might offer richer temporal modeling capabilities.

### Open Question 2
How do sparse or low-rank time-varying weight structures impact the efficiency and performance of continuous-time networks in real-time control scenarios? The current experiments utilized standard dense polynomial expansions without enforcing sparsity or low-rank constraints, suggesting potential for resource-aware inference applications.

### Open Question 3
Do the efficiency and stability benefits of Legendre parameterization generalize to physics-informed machine learning and inverse problems in imaging? While the conclusion notes potential applications in these domains, the specific tradeoffs between polynomial bases and physical constraints in imaging or physics-informed contexts were not tested.

## Limitations

- The numerical analysis relies heavily on comparative performance metrics without full theoretical justification for why orthogonal bases outperform monomials beyond empirical observations of conditioning
- The study does not explore alternative basis functions (Chebyshev, Fourier) that might offer similar or superior properties to Legendre polynomials
- Computational savings depend critically on the specific adaptive solver implementation and may not generalize to all ODE integrators
- The generalization to problems outside surrogate modeling domain (e.g., image classification or natural language tasks) remains unexplored

## Confidence

**High Confidence**: The computational efficiency advantage of Legendre parameterization (8× fewer function evaluations) is well-supported by reported numerical experiments. The observation that monomial bases lead to training instability due to ill-conditioned Vandermonde matrices is grounded in established numerical analysis theory.

**Medium Confidence**: The claim that Legendre parameterization consistently improves accuracy across all three benchmark problems is supported by experimental results, but the magnitude of improvement varies significantly between datasets. The assertion that constraining weights to low-dimensional subspaces improves generalization is plausible but lacks rigorous statistical validation.

**Low Confidence**: The assertion that Legendre parameterization is universally "more stable" for training dynamics lacks precise mathematical characterization of stability conditions. The paper does not provide comprehensive ablation studies on polynomial degree sensitivity or investigate effects of regularization strength across broader parameter sweeps.

## Next Checks

1. **Theoretical Characterization**: Derive explicit bounds on the condition number of the weight parameterization matrix for both monomial and Legendre bases as a function of polynomial degree, and establish conditions under which the conditioning advantage translates to training stability.

2. **Cross-Domain Generalization**: Implement and evaluate the weight parameterization framework on standard computer vision benchmarks (e.g., CIFAR-10) using ResNet and Neural ODE architectures to assess whether observed benefits transfer beyond surrogate modeling tasks.

3. **Solver-Agnostic Analysis**: Repeat computational efficiency experiments using multiple ODE solvers (e.g., RK4, BDF methods) and discretization schemes to determine whether the Legendre advantage persists across different numerical integration approaches.