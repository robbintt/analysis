---
ver: rpa2
title: Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using
  Explainable AI
arxiv_id: '2507.08829'
source_url: https://arxiv.org/abs/2507.08829
tags:
- weights
- critical
- network
- most
- reliability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an efficient TMR approach to enhance DNN reliability
  against bit-flip faults using XAI. The method leverages LRP to compute importance
  scores for DNN weights, selectively applying TMR to the top 1% most critical weights
  to minimize overhead.
---

# Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI

## Quick Facts
- **arXiv ID**: 2507.08829
- **Source URL**: https://arxiv.org/abs/2507.08829
- **Reference count**: 0
- **Primary result**: Achieves 60%+ reliability improvement with only 1% memory overhead vs. 200% for full TMR

## Executive Summary
This paper introduces an efficient Triple Modular Redundancy (TMR) approach for enhancing Deep Neural Network (DNN) reliability against bit-flip faults. The method leverages Layer-wise Relevance Propagation (LRP) from Explainable AI to compute importance scores for DNN weights, enabling selective TMR application to only the most critical weights. By protecting just the top 1% of weights based on their importance scores, the approach achieves significant reliability improvements while maintaining minimal memory overhead. The XAI-based method outperforms both full TMR and traditional magnitude-based selection techniques.

## Method Summary
The proposed approach combines Triple Modular Redundancy with Explainable AI to create a selective fault tolerance mechanism for DNNs. The core innovation lies in using LRP to compute relevance scores for each weight in the network, identifying which weights are most critical to model performance. During training, LRP is applied to generate these importance scores, which are then used to select the top 1% of weights for TMR protection. This selective approach dramatically reduces the overhead compared to traditional full TMR while maintaining effectiveness. The method is evaluated on VGG16 and AlexNet architectures using MNIST and CIFAR-10 datasets, demonstrating superior performance compared to magnitude-based weight selection methods.

## Key Results
- Achieves over 60% reliability improvement at BER of 10⁻⁴ with only 1% memory overhead
- Outperforms full TMR (200% overhead) and magnitude-based methods (16-21% overhead)
- XAI-based selection accurately identifies critical weights, providing better fault protection efficiency

## Why This Works (Mechanism)
The approach works by leveraging the interpretability capabilities of Explainable AI to identify which weights in a DNN are most critical to its performance. Traditional TMR applies redundancy to all components, resulting in significant overhead. By using LRP to compute relevance scores, the method can prioritize protection for weights that contribute most to the model's decision-making process. This targeted approach ensures that the most important components receive fault tolerance while less critical weights remain unprotected, achieving a better trade-off between reliability and resource efficiency.

## Foundational Learning
- **Layer-wise Relevance Propagation (LRP)**: A technique for attributing a model's prediction to its input features by propagating relevance backwards through the network; needed to identify critical weights for protection, quick check: verify LRP implementation produces consistent relevance scores across similar inputs.
- **Triple Modular Redundancy (TMR)**: A fault tolerance technique where three identical components perform the same operation and majority voting determines the correct output; needed as the underlying redundancy mechanism, quick check: confirm majority voting correctly handles all bit-flip scenarios.
- **Bit-flip Fault Models**: Error patterns where individual bits in memory or computation are inverted; needed to simulate hardware reliability issues in DNNs, quick check: validate fault injection correctly targets random bit positions at specified BER.
- **Weight Importance Scoring**: The process of ranking network parameters by their contribution to model accuracy; needed to select which weights receive TMR protection, quick check: correlate importance scores with actual accuracy degradation when weights are corrupted.
- **Memory Overhead Calculation**: Measuring the additional memory required for redundancy mechanisms; needed to quantify the efficiency improvement over full TMR, quick check: verify overhead calculations account for all redundant storage requirements.

## Architecture Onboarding
- **Component Map**: Input Data -> DNN Model -> LRP Analysis -> Weight Importance Scores -> TMR Selection -> Protected Model
- **Critical Path**: The sequence from fault injection through LRP-based weight selection to final model protection determines the overall reliability improvement.
- **Design Tradeoffs**: The 1% threshold represents a balance between protection effectiveness and overhead minimization, though this may need adjustment for different architectures or fault rates.
- **Failure Signatures**: Bit-flips in highly important weights cause significant accuracy degradation, while flips in less important weights have minimal impact.
- **First Experiments**: 1) Compare LRP importance scores against magnitude-based selection on VGG16/MNIST, 2) Evaluate reliability improvement across different BER values (10⁻³ to 10⁻⁶), 3) Test the approach on ResNet architecture with CIFAR-10 to assess scalability.

## Open Questions the Paper Calls Out
None

## Limitations
- The LRP-based importance scores may not generalize perfectly across different DNN architectures or fault models beyond bit-flips
- The approach focuses on specific datasets (MNIST, CIFAR-10) and models (VGG16, AlexNet), raising scalability questions
- The 1% threshold for weight protection may not be optimal for all scenarios or fault rates

## Confidence
- **High Confidence**: The 60%+ reliability improvement claim and 1% vs 200% overhead comparison are well-supported by presented data
- **Medium Confidence**: Superiority over magnitude-based methods (16-21% overhead) is demonstrated but could benefit from testing on additional architectures
- **Medium Confidence**: The XAI-based weight selection methodology is sound but requires validation across diverse DNN types

## Next Checks
1. Test the approach on deeper networks (ResNet, EfficientNet) and more complex datasets (ImageNet) to verify scalability
2. Evaluate performance under different fault models including stuck-at faults and multi-bit errors to assess robustness
3. Characterize the runtime overhead of LRP computation during both training and inference phases to quantify the full system cost