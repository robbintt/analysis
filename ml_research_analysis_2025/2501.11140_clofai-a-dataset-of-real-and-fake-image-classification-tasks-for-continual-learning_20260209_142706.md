---
ver: rpa2
title: 'CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual
  Learning'
arxiv_id: '2501.11140'
source_url: https://arxiv.org/abs/2501.11140
tags:
- task
- learning
- continual
- tasks
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLOFAI, a novel dataset designed for continual
  learning on real and fake image classification tasks. CLOFAI simulates a scenario
  where a classifier must adapt to increasingly realistic generative models over time,
  maintaining performance on prior tasks without retraining on the entire dataset.
---

# CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual Learning

## Quick Facts
- arXiv ID: 2501.11140
- Source URL: https://arxiv.org/abs/2501.11140
- Reference count: 40
- Primary result: Introduces CLOFAI dataset for continual learning on real/fake image detection across evolving generative models

## Executive Summary
This paper introduces CLOFAI, a novel dataset designed for continual learning on real and fake image classification tasks. CLOFAI simulates a scenario where a classifier must adapt to increasingly realistic generative models over time, maintaining performance on prior tasks without retraining on the entire dataset. The dataset consists of five tasks, each containing real images from CIFAR-10 and fake images generated by different generative models (VAE, VAEBM, GAN, Flow-Based, and Denoising Diffusion). The paper establishes a baseline for continual learning methods on this dataset, comparing EWC, GEM, and Experience Replay against a Naive baseline and a fully trained Baseline. Results show that GEM and Experience Replay perform significantly better than the Naive approach, while EWC fails to prevent catastrophic forgetting effectively. Pre-training the classifier on the first task substantially improves performance.

## Method Summary
The CLOFAI dataset consists of 5 sequential tasks for domain-incremental learning, each containing 5,000 real CIFAR-10 images and 5,000 fake images generated by different generative models (VAE, VAEBM, GAN, Flow-Based, and Denoising Diffusion). The study uses an EfficientNet_b0 backbone pre-trained on ImageNet, modified with a single output node and sigmoid activation for binary classification. Training proceeds for 3 epochs per task with batch size 128 and Adam optimizer (lr=0.0001). Four continual learning strategies are evaluated: Naive (sequential training without mitigation), EWC (elastic weight consolidation), Experience Replay (episodic memory), and GEM (gradient episodic memory). The Naive baseline shows catastrophic forgetting, dropping from 99.75% to 37.45% accuracy on Task 1 after training on Task 4.

## Key Results
- GEM and Experience Replay significantly outperform Naive baseline, preventing catastrophic forgetting across tasks
- EWC fails to prevent forgetting despite high regularization strength (λ ≥ 100,000)
- Pre-training on Task 1 improves Task 5 accuracy from 54.90% to 59.05% compared to training from scratch
- GEM shows substantial improvement when increasing memory buffer from 100 to 500 samples per task
- Experience Replay shows only marginal gains from larger memory buffers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GEM's constrained optimization mitigates catastrophic forgetting more effectively than regularization when sufficient memory samples are available.
- Mechanism: GEM computes gradients from stored episodic memory samples and projects the current task's gradient onto a feasible direction that minimizes new task loss without increasing loss on prior tasks. The constraint L₁:ₜ₋₁(θ_new) ≤ L₁:ₜ₋₁(θ_old) directly bounds forgetting.
- Core assumption: Stored samples adequately represent prior task distributions; gradient constraints transfer to unseen test data.
- Evidence anchors:
  - [abstract] "GEM and Experience Replay show promise, performing significantly better than a Naive baseline"
  - [section 5.5] GEM with 500 samples shows "significantly better" results than 100 samples, with classifier "less prone to catastrophic forgetting"
  - [corpus] Neighbor papers on fake image detection focus on single-task training; no direct corpus evidence on GEM specifically for this domain.
- Break condition: If parameter importance overlaps heavily across tasks (as observed with EWC), gradient-based constraints may still allow interference when memory samples don't capture task boundaries fully.

### Mechanism 2
- Claim: Experience Replay retains prior knowledge through explicit sample rehearsal, with marginal gains from larger memory buffers in domain-incremental settings.
- Mechanism: Stores representative samples per task in a fixed buffer; interleaves these with current task data during training. This adds an implicit regularization term to the loss representing error on past distributions.
- Core assumption: The model can simultaneously optimize for current and replayed data without severe gradient interference.
- Evidence anchors:
  - [abstract] "Experience Replay show promise, performing significantly better than a Naive baseline"
  - [section 5.4] Accuracy with 500 samples is "slightly better" than 100 samples, but "differences are small"
  - [corpus] No corpus evidence comparing replay buffer sizes in continual fake image detection.
- Break condition: When tasks have conflicting feature-label mappings (e.g., a pattern indicates "fake" in Task 4 but "real" in Task 1), simple replay cannot resolve the contradiction without task-specific adaptation.

### Mechanism 3
- Claim: Pre-training on large-scale datasets provides transferable feature representations critical for detecting subtle generative artifacts in small-data regimes.
- Mechanism: ImageNet pre-training equips EfficientNet_b0 with general visual features (edges, textures, object parts). When fine-tuned on CLOFAI's 8,000 samples per task, these features accelerate learning of domain-specific discriminators.
- Core assumption: Generative model artifacts share underlying detectable patterns with natural image features learned during pre-training.
- Evidence anchors:
  - [abstract] "Pre-training the classifier on the first task substantially improves performance"
  - [section 5.1] Pre-trained baseline achieves 59.05% on Task 5 vs. 54.90% without pre-training; Task 1 accuracy is 99.85% vs. 84.85%
  - [corpus] Neighbor paper "R²BD" leverages pre-trained diffusion models for reconstruction-based detection, suggesting pre-training benefits transfer across detection approaches.
- Break condition: If novel generative architectures (e.g., transformer-based models) produce artifacts fundamentally different from ImageNet features, pre-training benefits may diminish.

## Foundational Learning

- Concept: **Domain-Incremental Learning**
  - Why needed here: CLOFAI uses fixed labels (real/fake) but shifting input distributions as generative models evolve. This differs from task-incremental settings where task identity is available at inference.
  - Quick check question: Can you explain why domain-incremental learning is harder than task-incremental learning when task boundaries are ambiguous?

- Concept: **Catastrophic Forgetting**
  - Why needed here: The Naive baseline shows accuracy on Task 1 drops from 99.75% to 37.45% after training on Task 4, demonstrating severe forgetting when no mitigation is applied.
  - Quick check question: What metric would you use to quantify forgetting rate across tasks in a continual learning experiment?

- Concept: **Stability-Plasticity Dilemma**
  - Why needed here: EWC's failure demonstrates this tradeoff—high λ preserves old tasks but blocks new learning; low λ allows learning but permits forgetting.
  - Quick check question: In GEM, which component controls stability and which controls plasticity?

## Architecture Onboarding

- Component map: EfficientNet_b0 (ImageNet pre-trained) -> Linear layer (output=1) -> Sigmoid activation
- Critical path:
  1. Initialize with pre-trained weights
  2. For each task: load task data -> if continual method, retrieve memory samples -> train 3 epochs -> store samples in memory
  3. Evaluate on all task test sets after each task
- Design tradeoffs:
  - Memory size vs. performance: GEM benefits significantly from 500 vs. 100 samples; Experience Replay shows diminishing returns
  - Pre-training vs. scratch: Pre-training essential given small per-task data (8,000 samples)
  - EWC regularization strength: No value of λ succeeds—parameter importance overlap makes regularization ineffective
- Failure signatures:
  - EWC: Important parameters overlap across tasks (Table 7 shows 43-154% changes in top-10 important parameters)
  - Naive: Accuracy collapse on early tasks after training on later tasks (Task 1: 99.75% → 37.45%)
  - Overfitting: Training beyond 3 epochs causes task-specific overfitting
- First 3 experiments:
  1. **Reproduce Naive baseline**: Train sequentially on Tasks 1-5 without mitigation; verify catastrophic forgetting pattern matches Table 4.
  2. **Ablate memory size**: Run GEM with 50, 100, 200, 500 samples per task; plot forgetting curve vs. memory budget.
  3. **Test pre-training necessity**: Train from scratch vs. ImageNet pre-trained on Task 1 only; measure gap to establish upper bound for transfer learning contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of advanced generative models, specifically those based on transformer architectures, impact the performance and adaptability of continual learning methods on the CLOFAI dataset?
- Basis in paper: [explicit] The conclusion explicitly identifies this as a potential direction for future research to provide deeper insights.
- Why unresolved: The current dataset is limited to VAE, VAEBM, GAN, Flow-Based, and Denoising Diffusion models.
- What evidence would resolve it: Benchmark results from an updated CLOFAI dataset that includes images generated by transformer-based models.

### Open Question 2
- Question: Can regularization-based continual learning methods be modified to succeed in domain-incremental tasks where the same network parameters appear critical across distinct tasks?
- Basis in paper: [inferred] The paper analyzes the failure of Elastic Weight Consolidation (EWC), noting that "the same parameters are highly important across all tasks," causing the regularization to fail.
- Why unresolved: The paper demonstrates the failure and hypothesizes the cause but does not propose or test a solution for this specific constraint.
- What evidence would resolve it: A successful application of a regularization method that accounts for shared parameter importance without inhibiting the learning of new tasks.

### Open Question 3
- Question: Why does Gradient Episodic Memory (GEM) utilize a larger memory buffer more effectively than Experience Replay in this specific domain?
- Basis in paper: [inferred] The results show GEM performance improves significantly when moving from 100 to 500 samples, whereas Experience Replay shows only small improvements.
- Why unresolved: The paper offers a brief theoretical explanation regarding optimization constraints but leaves the specific dynamics in this domain unverified.
- What evidence would resolve it: A comparative analysis of gradient updates and loss landscapes for both methods under varying memory constraints in the CLOFAI context.

## Limitations

- The exact architectural parameters and training seeds for the generative models are not specified, making exact replication difficult
- Replay buffer selection strategy (random vs. class-balanced) is unspecified beyond sample count
- The 3-epoch training limit appears critical but was not extensively validated through ablation studies

## Confidence

- **High**: Pre-training benefits and catastrophic forgetting patterns in Naive baseline (supported by clear numerical evidence in Tables 4-5)
- **Medium**: GEM and Experience Replay performance superiority (methodologically sound but dependent on generator implementation)
- **Low**: EWC failure interpretation (could reflect implementation issues rather than fundamental mechanism)

## Next Checks

1. Generate CLOFAI tasks using publicly available implementations of the specified generators with consistent seeds
2. Test multiple replay buffer selection strategies (random vs. class-balanced) to verify robustness
3. Conduct ablation studies varying training epochs (2-5) to confirm overfitting threshold