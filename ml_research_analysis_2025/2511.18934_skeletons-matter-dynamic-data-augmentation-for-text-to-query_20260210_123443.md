---
ver: rpa2
title: 'Skeletons Matter: Dynamic Data Augmentation for Text-to-Query'
arxiv_id: '2511.18934'
source_url: https://arxiv.org/abs/2511.18934
tags:
- query
- data
- skeleton
- schema
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Text-to-Query paradigm, unifying semantic
  parsing tasks across SQL, Cypher, and nGQL. It identifies query skeletons as shared
  optimization targets and proposes a dynamic data augmentation framework that diagnoses
  model weaknesses on these skeletons to synthesize targeted training data.
---

# Skeletons Matter: Dynamic Data Augmentation for Text-to-Query

## Quick Facts
- arXiv ID: 2511.18934
- Source URL: https://arxiv.org/abs/2511.18934
- Authors: Yuchen Ji; Bo Xu; Jie Shi; Jiaqing Liang; Deqing Yang; Yu Mao; Hai Chen; Yanghua Xiao
- Reference count: 40
- Primary result: State-of-the-art Text-to-Query performance using only 10K synthesized examples across SQL, Cypher, and nGQL benchmarks

## Executive Summary
This paper introduces the Text-to-Query paradigm that unifies semantic parsing across SQL, Cypher, and nGQL by identifying query skeletons as shared optimization targets. The authors propose a dynamic data augmentation framework that diagnoses model-specific weaknesses in handling these skeletons through K-fold cross-validation and structural similarity measures. By synthesizing targeted training data focused on error-prone skeletons, the approach achieves state-of-the-art performance on four benchmarks while using only 10K examples, demonstrating significant efficiency gains over static augmentation methods.

## Method Summary
The framework operates through three key stages: dynamic diagnosis identifies skeleton-level errors using AST-based or token-based structural distance with a threshold of 2; a skeleton generalizer fine-tuned on error-prone skeletons generates novel structures to prevent overfitting; and a backward-forward synthesis pipeline fills skeletons with schema elements, generates questions from queries (backward), and verifies consistency (forward) using a teacher LLM. The target model is then fine-tuned on the combined original and synthetic data. The approach leverages the relative ease of query→question translation compared to forward generation, with CoT verification ensuring quality.

## Key Results
- Achieves state-of-the-art performance on four benchmarks (Spider, BIRD, Text2Cypher-Exec, NL2GQL)
- Uses only 10K synthesized examples compared to typical 50K+ requirements
- Ablation shows 84.3→76.5 TS drop on Spider dev when skeleton generalizer removed
- Dynamic diagnosis improves over static augmentation by targeting systematic weaknesses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic diagnosis of skeleton errors enables targeted, efficient data augmentation.
- Mechanism: K-fold cross-validation identifies failure cases; structural similarity measures (AST-based edit distance or token-based edit distance) isolate skeleton-level errors from schema-linking or content errors. Only skeletons with error rate >20% enter the error-prone set, focusing synthesis on systematic weaknesses.
- Core assumption: Skeleton errors reflect transferable structural misunderstandings rather than isolated mistakes.
- Evidence anchors: [abstract] "proposes a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons"; [Section 4.1] Describes AST-based structural distance using Change Distiller algorithm and threshold-based skeleton error detection; [corpus] Related work on "Program Skeletons for Automated Program Translation" supports skeleton abstraction as a transferable representation.

### Mechanism 2
- Claim: Skeleton-guided backward-forward synthesis produces higher-quality training data than static or question-first augmentation.
- Mechanism: Error-prone skeletons → generalization via fine-tuned LLM → skeleton instantiation with schema elements → backward translation (query→question) → forward verification (question→query consistency check with CoT). This leverages the relative ease of backward translation compared to forward.
- Core assumption: Query→Question is semantically clearer than Question→Query due to formal language's unambiguous semantics.
- Evidence anchors: [abstract] "achieves state-of-the-art performance using only 10K synthesized examples"; [Section 4.3] "this backward translation is substantially easier than the forward direction"; [corpus] Weak direct evidence; corpus papers focus on query-language tasks but not backward-forward synthesis specifically.

### Mechanism 3
- Claim: Skeleton generalization prevents overfitting to observed error patterns.
- Mechanism: Fine-tune skeleton generator on error-prone set using partial instruction prefix; sample novel skeletons to expand candidate pool beyond training distribution.
- Core assumption: Error-prone skeletons contain latent patterns that can be extrapolated to structurally valid unseen skeletons.
- Evidence anchors: [Section 4.2] Describes skeleton generalizer training and prefix-guided generation; [Table 4] Ablation shows performance drop (84.3→76.5 TS on Spider dev) when Skeleton Generalizer removed; [corpus] No direct corpus support.

## Foundational Learning

- **Abstract Syntax Trees (AST) for structural code comparison**
  - Why needed here: Core to measuring query skeleton similarity and diagnosing structural errors
  - Quick check question: Can you explain why AST edit distance captures structural differences better than token-level comparison?

- **Back-translation in data augmentation**
  - Why needed here: Underpins the query→question→verification pipeline
  - Quick check question: Why might back-translation (query→question) produce higher-quality pairs than forward generation (question→query)?

- **K-fold cross-validation for error diagnosis**
  - Why needed here: Used to identify model failures across training set before augmentation
  - Quick check question: How does K-fold CV help distinguish systematic skeleton errors from random failures?

## Architecture Onboarding

- **Component map**: Dynamic Diagnosis Module → Skeleton Generalizer → Data Synthesis Pipeline → Target Model (Skeletron)
- **Critical path**: Diagnosis → error-prone skeletons → generalizer → synthesis → SFT → inference
- **Design tradeoffs**:
  - AST-based vs token-based distance: AST more precise but requires parsers; token-based fallback for less-supported languages (nGQL)
  - Threshold tuning: Low threshold increases recall but adds noise; high threshold improves precision but misses errors
  - Data volume: 10K synthesized examples sufficient; more data may yield diminishing returns
- **Failure signatures**:
  - High skeleton error rate post-training: Generalizer producing invalid skeletons or synthesis pipeline hallucinating
  - No improvement over base model: Dynamic diagnosis threshold misconfigured; error-prone set empty
  - Performance drop on simple queries: Overfitting to complex skeletons; verify forward verification not introducing bias
- **First 3 experiments**:
  1. Run dynamic diagnosis on your target dataset with threshold=2; validate error-prone skeleton set has >10 distinct patterns
  2. Ablate skeleton generalizer: train with vs without; expect 3-5% TS drop on Spider-dev
  3. Compare forward verification settings: with CoT vs without; measure synthetic data quality via manual inspection of 50 random pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dynamic data augmentation framework be extended to train a single unified model capable of processing multiple query languages (e.g., SQL, Cypher, nGQL) simultaneously?
- Basis in paper: [explicit] The authors state in the Limitations section that "The current setup does not support a unified model that can handle multiple query languages simultaneously," identifying this as a direction for future work.
- Why unresolved: The current methodology trains separate models for each language, and combining them introduces challenges regarding conflicting syntax rules and schema representations.
- What evidence would resolve it: A single model fine-tuned on multi-language augmented data that maintains performance parity with the specialized models across all benchmarks.

### Open Question 2
- Question: Is there an adaptive mechanism to determine the optimal structural distance threshold for skeleton error detection without manual tuning?
- Basis in paper: [inferred] Section 5.7 analyzes the trade-off of thresholds (set to 2), noting that values that are too low introduce noise and high values miss errors, yet no automated selection method is proposed.
- Why unresolved: The threshold appears to be a hyperparameter tuned based on observation, rather than dynamically adjusted based on the query language complexity or model state.
- What evidence would resolve it: An algorithmic approach that adjusts the threshold based on AST depth or error variance, demonstrating stability across different query languages without manual intervention.

### Open Question 3
- Question: How does the reliance on K-fold cross-validation on the training set for dynamic diagnosis impact the model's ability to generalize to entirely new skeleton structures?
- Basis in paper: [inferred] The method diagnoses "error-prone skeletons" from the training set (Section 4.1) and uses a generator to create "novel" ones, but the diagnosis is fundamentally limited by the initial training distribution.
- Why unresolved: If the test set contains skeletons structurally distant from the training set, the diagnosis step might fail to identify the relevant weaknesses, limiting the effectiveness of the targeted augmentation.
- What evidence would resolve it: A comparative analysis of performance on test subsets specifically filtered for "unseen" skeletons versus those structurally similar to the training data.

## Limitations

- The framework requires separate models for each query language rather than a unified multilingual model
- Dynamic diagnosis relies on K-fold cross-validation with an unspecified K value that could impact error detection sensitivity
- The 20% error rate threshold and structural distance threshold of 2 are fixed without sensitivity analysis or adaptive mechanisms

## Confidence

- **High confidence**: The skeleton abstraction framework and its application across SQL, Cypher, and nGQL demonstrate clear conceptual validity. The empirical results showing state-of-the-art performance on four benchmarks are reproducible given the specified models and hyperparameters.
- **Medium confidence**: The dynamic diagnosis mechanism's effectiveness depends heavily on threshold calibration. While the ablation studies support the skeleton generalizer's contribution (84.3→76.5 TS drop when removed), the exact contribution of each component remains unclear.
- **Low confidence**: The claim that query→question translation is "substantially easier" than forward generation lacks quantitative support. The assertion that backward-forward synthesis produces higher-quality data than static augmentation is plausible but not directly compared to alternatives in the experiments.

## Next Checks

1. **Threshold Sensitivity Analysis**: Run dynamic diagnosis with structural distance thresholds of 1, 2, and 3 on Spider dev. Measure resulting error-prone skeleton set size and downstream model performance to identify optimal threshold.
2. **Teacher Model Verification Reliability**: Generate 100 synthetic pairs using the full pipeline. Manually annotate for consistency errors and calculate verification success rate. Compare against pairs generated without forward verification to quantify its impact.
3. **Alternative Augmentation Comparison**: Implement a static skeleton-based augmentation baseline using the same 10K synthetic examples but without dynamic diagnosis. Train identical models and compare performance on all four benchmarks to isolate the dynamic diagnosis contribution.