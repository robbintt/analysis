---
ver: rpa2
title: A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic
  Alignment of LLMs
arxiv_id: '2512.08786'
source_url: https://arxiv.org/abs/2512.08786
tags:
- alignment
- preference
- aggregation
- reward
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning LLMs with diverse
  human preferences in federated settings, where standard methods often fail to represent
  diverse viewpoints. The authors propose a systematic evaluation framework comparing
  different reward aggregation strategies in federated RLHF.
---

# A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs

## Quick Facts
- arXiv ID: 2512.08786
- Source URL: https://arxiv.org/abs/2512.08786
- Reference count: 10
- Primary result: Adaptive alpha aggregation achieves FI≈0.99 and competitive alignment (Avg AS≈0.90-0.95) while balancing fairness and performance in federated RLHF

## Executive Summary
This paper addresses the challenge of aligning large language models with diverse human preferences in federated settings, where standard methods often fail to represent diverse viewpoints. The authors propose a systematic evaluation framework comparing different reward aggregation strategies in federated RLHF, introducing an adaptive alpha aggregation scheme that dynamically adjusts preference weights based on each group's historical alignment performance. Experiments on question-answering tasks using a PPO-based RLHF pipeline demonstrate that their adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores.

## Method Summary
The method uses federated RLHF with PPO optimization where groups maintain local preference predictors (PluralLLM modules) trained on their private data. The server aggregates rewards using an adaptive scheme that switches between uniform averaging (when FI≥0.9) and log-sum-exp weighted aggregation with dynamically computed weights based on historical alignment performance. The aggregation weights are computed as softmax((1−hᵍᵗ⁻¹)/T) where hᵍᵗ⁻¹ represents accumulated alignment rewards, giving higher influence to underrepresented groups.

## Key Results
- Adaptive alpha aggregation consistently achieves superior fairness (FI≈0.99) while maintaining competitive alignment scores (Avg AS≈0.90-0.95)
- The method successfully balances fairness and performance by preventing any group from being left behind while preserving overall utility
- Experiments on Pew Research Center's Global Attitudes Surveys dataset with Gemma-2B-it model show the approach outperforms standard aggregation methods

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Alpha Aggregation via Inverse-Performance Reweighting
Dynamically adjusting aggregation weights based on each group's historical alignment performance improves fairness without sacrificing overall utility. The scheme computes group-specific weights using softmax on historical scores, upweighting groups with lower past rewards to correct underrepresentation.

### Mechanism 2: Fairness Index-Gated Aggregation Selection
A fairness threshold (FI≥0.9) triggers switching between computationally simple averaging and adaptive weighting, preserving efficiency when groups are already well-aligned. The FI measures reward consistency across groups, indicating when corrective reweighting is needed.

### Mechanism 3: Decentralized Reward Generation via PluralLLM Modules
Lightweight local preference predictors trained in FL generate group-specific reward signals without sharing raw preference data. Each group trains a PluralLLM module on local preference data, computing scalar rewards during RLHF that preserve privacy while providing meaningful preference signals.

## Foundational Learning

- **Federated Learning (FL) fundamentals**: Understand client-server training, local model updates, and privacy preservation. Quick check: Can you explain why the server aggregates rewards (not model weights) in this architecture?

- **RLHF with PPO**: Understand KL penalties, value functions, and rollout generation in PPO-based RLHF. Quick check: What role does the reward model play in standard RLHF, and how does this paper replace/augment it?

- **Preference aggregation theory (social choice)**: Understand how min/max/avg/alpha aggregation schemes map to classical social choice concepts. Quick check: Why might "average" aggregation fail to represent minority preferences?

## Architecture Onboarding

- **Component map**: Base LLM (Gemma-2B-it) -> SFT -> Policy model π^policy -> PPO updates -> Aggregated rewards from PluralLLM modules -> Updated policy

- **Critical path**: 1. Initialize: SFT on task data → policy model 2. Per iteration: generate rollouts → broadcast to groups → receive per-group rewards → compute FI → select aggregation → run PPO step → update policy 3. Track hᵍ (historical alignment) to compute αᵍᵗ next iteration

- **Design tradeoffs**: Temperature T in softmax (currently 0.1): lower T = sharper reweighting, higher T = smoother; FI threshold (currently 0.9): higher = more aggressive adaptive mode, lower = more averaging; Reward metric choice: Wasserstein/Cosine favor distribution matching; Borda/Kendall favor ranking

- **Failure signatures**: FI stuck below 0.9 with no improvement: groups may have fundamentally incompatible preferences; Min AS degrades while Avg AS improves: aggregation is sacrificing worst-group for average; Training instability: check reward whitening, KL coefficient, or exploding αᵍᵗ values

- **First 3 experiments**: 1. Baseline replication: Run SFT-only model on Pew dataset, measure FI and alignment scores 2. Aggregation ablation: Compare MIN/AVG/MAX/ALPHA on 3-group subset using Wasserstein reward 3. Sensitivity to T and FI threshold: Vary T∈{0.05, 0.1, 0.2} and FI threshold∈{0.85, 0.90, 0.95}

## Open Questions the Paper Calls Out

### Open Question 1
How does the adaptive alpha aggregation scheme perform when applied to alternative RL optimization paradigms such as Direct Preference Optimization (DPO) or Group Robust Policy Optimization (GRPO)? The paper currently relies on PPO and suggests testing across different optimization paradigms.

### Open Question 2
How robust is adaptive alpha aggregation when groups hold fundamentally adversarial or mutually exclusive preferences, rather than the variation captured in the Pew Global Attitudes dataset? The paper notes that the Pew dataset may be relatively conducive to cross-group alignment.

### Open Question 3
Can the adaptive alpha aggregation framework generalize to open-ended generation tasks such as summarization, dialogue, or code generation that lack discrete answer choices? The paper focuses on multiple-choice Q&A tasks and suggests extending to diverse tasks.

## Limitations
- The approach critically depends on PluralLLM local preference predictors, which are referenced from external work but not fully specified
- The FI≥0.9 threshold appears empirically chosen without theoretical justification or sensitivity analysis
- Effectiveness relies on the quality and calibration of PluralLLM outputs, which are not independently verified

## Confidence
- **High confidence**: General federated RLHF framework and PPO implementation details
- **Medium confidence**: Fairness and alignment metrics (FI, Avg AS, Min AS) are well-defined
- **Low confidence**: Adaptive weighting mechanism specifically relies on unknown PluralLLM components

## Next Checks
1. Replicate the SFT baseline training with specified hyperparameters and verify baseline FI and alignment scores
2. Implement adaptive alpha aggregation with synthetic preference groups to verify correct upweighting of underrepresented groups
3. Conduct ablation studies varying temperature T and FI threshold to determine sensitivity and optimal values