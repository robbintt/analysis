---
ver: rpa2
title: Provocations from the Humanities for Generative AI Research
arxiv_id: '2502.19190'
source_url: https://arxiv.org/abs/2502.19190
tags:
- humanities
- data
- research
- generative
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that humanities research is currently underrepresented
  in generative AI development and offers eight provocations to inform its future
  applications. These provocations address language and meaning, culture, representation,
  model size, data quality, openness, compute access, and AI universalism.
---

# Provocations from the Humanities for Generative AI Research

## Quick Facts
- **arXiv ID:** 2502.19190
- **Source URL:** https://arxiv.org/abs/2502.19190
- **Reference count:** 40
- **Primary result:** Humanities research is underrepresented in generative AI; eight provocations address language, culture, representation, model size, data quality, openness, compute access, and AI universalism.

## Executive Summary
This paper argues that humanities research perspectives are currently underrepresented in generative AI development. The authors present eight provocations to guide future applications, emphasizing that language models generate words while humans create meaning, that culture must be broadly defined, and that no dataset can be truly representative. They advocate for smaller, domain-specific models with rich metadata and intentional data curation, while resisting the extraction of humanities expertise and promoting equal collaboration between technical and humanities researchers.

## Method Summary
The authors synthesize existing humanities scholarship and case studies to develop provocations for generative AI research. Rather than presenting new empirical data, they draw on theoretical frameworks from digital humanities, archival studies, and critical race theory to identify key tensions and opportunities. The approach emphasizes conceptual reframing over technical specification, using examples from historical text analysis and cultural preservation to illustrate how humanities methods can inform AI development.

## Key Results
- Models generate statistical sequences while meaning is an external human construct
- No dataset can be truly representative due to structural power dynamics and inherent absences
- Smaller, domain-specific models with expert curation can outperform larger general models for cultural specificity
- Humanities expertise must be integrated as equal partners from project inception, not extracted post-hoc

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language models generate statistical sequences (words), but meaning is an external construct assigned by human interpretation.
- **Mechanism:** The model operates on token probability and fluency, while the user operates on semantic intent and social context. When these diverge, the model produces "hallucinations" or fluent nonsense. Because humans are predisposed to seek intentionality, they often falsely attribute "knowing" or "meaning" to what is merely statistical output.
- **Core assumption:** Users cannot distinguish between a system that "understands" and a system that statistically mimics patterns of understanding without explicit training or friction.
- **Evidence anchors:**
  - [Section 4.1] "Models make words, but people make meaning" outlines how theories of meaning-making (semiotics/semantics) demonstrate that sign-systems have non-referential qualities that serve functions beyond communication.
  - [Abstract] Explicitly lists "Models make words, but people make meaning" as the first provocation.
  - [Corpus] Related work suggests provocations can restore critical thinking in AI-assisted work, implying the default state is one of over-trust or "meaning projection."
- **Break condition:** If model outputs are constrained to narrow, verifiable domains (e.g., code synthesis, formal translation) where "meaning" is strictly functional and less open to interpretation, the gap between generation and meaning shrinks.

### Mechanism 2
- **Claim:** No dataset can be truly representative because data selection is inherently a political act of power and perspective.
- **Mechanism:** A dataset is not a neutral snapshot of reality but an "archive" defined as much by what is missing (silences/absences) as by what is present. Attempting to "fix" bias purely by adding more data or de-biasing algorithms fails because it does not address the underlying structural power differentials that caused the initial omissions.
- **Core assumption:** The "silences" in data (e.g., marginalized voices, non-digital cultures) are structurally irrecoverable, meaning statistical interpolation cannot accurately recreate them.
- **Evidence anchors:**
  - [Section 4.3] "AI can never be 'representative'" argues that bias reflects deeper structural problems and unequal power dynamics that cannot be modeled away.
  - [Section 4.5] Discusses "documentation debt" and how data is treated as a homogeneous resource ("oil") rather than a collection of specific cultural objects.
  - [Corpus] The concept of "Digital Humanities" research teams emphasizes "collaborative data behaviors," supporting the view that data curation is a complex, human-driven process, not an automated extraction.
- **Break condition:** This mechanism breaks if one accepts a definition of "representative" that is strictly statistical (e.g., "matches the distribution of the internet") rather than cultural or ethical (e.g., "accurately reflects the lived experience of a group").

### Mechanism 3
- **Claim:** Increasing model scale (parameters/data) dilutes domain specificity and obscures the cultural characteristics of training data.
- **Mechanism:** "Bigger is better" approaches rely on massive, heterogeneous datasets that treat all data as equivalent tokens. This homogenization washes out the "conceptual and cultural characteristics" of specific data sources. Conversely, smaller, domain-specific models with expert curation (archival approach) retain semantic richness for specific tasks.
- **Core assumption:** General intelligence (or AGI) is a flawed goal because "complete knowledge" is philosophically impossible; precision requires boundaries and specificity.
- **Evidence anchors:**
  - [Section 4.4] "Bigger models are not always better models" cites studies showing more parameters/compute do not always yield better output and argues for small, curated models.
  - [Section 4.5] Argues that ignoring the "conceptual and cultural characteristics" of data leads to a cycle of post-hoc fixes (guardrails/RLHF).
  - [Corpus] Related papers on evaluating LLMs in social sciences suggest that generic models often require specific "bridging" or adaptation to be effective in specialized humanities contexts.
- **Break condition:** If generalized reasoning capabilities that are truly orthogonal to domain specifics are developed (e.g., perfectly efficient few-shot learning), the need for domain-specific curation might diminish.

## Foundational Learning

- **Concept: The "Epistemology of Ignorance"**
  - **Why needed here:** To understand why standard "bias mitigation" fails. One must grasp that the problem isn't just "bad data" but an active, structural avoidance of frameworks that challenge normative beliefs (Section 1).
  - **Quick check question:** Can you distinguish between "missing data" (a statistical gap) and "silences" (a structural erasure) in your current dataset?

- **Concept: Data as Archive vs. Data as Oil**
  - **Why needed here:** This reframes the architectural input. If data is oil, you just need volume. If data is an archive, you need provenance, context, and curation (Section 4.2, 4.5).
  - **Quick check question:** Before training, do you know the "provenance" (origin, intent, and authority) of your top 3 data sources, or just their file size?

- **Concept: Situated Knowledges (Standpoint Theory)**
  - **Why needed here:** To combat "AI Universalism." The idea that all knowledge is from a specific perspective prevents the creation of a "universal" model that inevitably defaults to a narrow, dominant subject (Section 4.8).
  - **Quick check question:** Does your model claim to serve "everyone," or does it document the specific "community of practice" it was trained to serve?

## Architecture Onboarding

- **Component map:**
  - **The Input (The Archive):** Curation layer (Humanities experts) -> Metadata enrichment (Context/Provenance) -> Dataset
  - **The Engine (The Model):** Scale vs. Specificity trade-off. Smaller models are preferred for domain fidelity
  - **The Output (The Meaning):** Model -> User Interpretation. *Note: The architecture does not end at generation; it extends to the human act of meaning-making*

- **Critical path:**
  1. **Define Culture:** Explicitly define "culture" for the specific application (Section 4.2)
  2. **Curate Data:** Select data not by volume but by "conceptual characteristics" (Section 4.5)
  3. **Document Perspectives:** Acknowledge that the dataset is not representative and document its specific perspective/silences (Section 4.3)

- **Design tradeoffs:**
  - **Scale vs. Accountability:** Larger models are harder to document and hold accountable; smaller models allow for finer-grained metadata and provenance tracking
  - **Openness vs. Harm:** "Open" data release can harm communities who did not consent to extraction; "Closed" data may impede research (Section 4.6)

- **Failure signatures:**
  - **Universalism Fallacy:** The system performs well for dominant groups but fails or harms minoritized groups because it assumes a "universal" human subject (Section 4.8)
  - **Extraction:** The system uses humanities expertise (theories/methods) to justify its validity but strips credit/funding from the humanities disciplines that produced it (Section 5)

- **First 3 experiments:**
  1. **Provocation Audit:** Select one model output and apply Provocation 4.1 ("Who is making meaning here?"). Trace the output back to training data to see if "meaning" was statistically inferred or explicitly present
  2. **Small Model A/B Test:** For a defined domain task, compare a generic large model vs. a small, intentionally curated model using humanities-specific metrics (e.g., nuance preservation, not just accuracy) (Section 4.4)
  3. **Silence Mapping:** Take a "representative" dataset and instead of looking for what is there, identify one category of "silence" (what is missing). Attempt to document how this absence affects model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can technical systems be designed to explicitly mark missing data or amplify sparse data in training sets, rather than attempting to "de-bias" or ignore archival gaps?
- Basis in paper: [explicit] Provocation 3 asks, "how can we mark missing data, or amplify the significance of sparse data, rather than pass over these gaps [of archival silence]?"
- Why unresolved: Current bias mitigation often focuses on balancing existing data rather than acknowledging structural absences, which requires new methods for quantifying and representing silence.
- What evidence would resolve it: Successful implementation of algorithms that weight or flag absent narratives, resulting in model outputs that acknowledge ignorance rather than hallucinating filler.

### Open Question 2
- Question: How does the inclusion of rich, context-specific metadata (e.g., author demographics, historical provenance) in pretraining datasets influence downstream task performance and cultural alignment?
- Basis in paper: [explicit] Provocation 5 suggests AI research must "consider pretraining data's conceptual and cultural characteristics in addition to its technical characteristics."
- Why unresolved: Pretraining data is frequently treated as a homogeneous resource ("oil"), lacking the deep documentation necessary to test the causal impact of cultural context on model behavior.
- What evidence would resolve it: Comparative studies of models trained on "thin" versus "thick" metadata datasets, showing measurable differences in cultural nuance or error rates.

### Open Question 3
- Question: In what specific research contexts do smaller, domain-specific models outperform large, general-purpose models in understanding cultural or historical nuance?
- Basis in paper: [explicit] Provocation 4 asserts that "bigger models are not always better models" and recommends researchers "ask whether a small model might be better for the task."
- Why unresolved: The field currently prioritizes scaling laws and general capabilities, leaving the specific utility of smaller, curated models for humanistic inquiry under-explored.
- What evidence would resolve it: Benchmarks demonstrating that smaller, expert-curated models (e.g., for historical newspapers or specific dialects) yield higher precision and fewer hallucinations than general LLMs.

## Limitations

- The paper provides no specific architectures, hyperparameters, or code for the cited projects
- Specific evaluation benchmarks for "cultural competence" are only referenced conceptually without precise measurement protocols
- Claims about humanities extraction patterns and collaboration failure modes are based on case studies without systematic measurement

## Confidence

- **High Confidence:** The basic premise that language models generate statistical sequences while meaning is externally constructed is well-established in computational linguistics and philosophy of language
- **Medium Confidence:** The claim that "bigger is not always better" has empirical support in efficiency literature, but the specific argument about cultural dilution through scale remains largely theoretical
- **Low Confidence:** Specific predictions about humanities extraction patterns and collaboration failure modes are based on case studies without systematic measurement

## Next Checks

1. **Provocation Audit Protocol:** Develop a standardized method to trace model outputs back to training data and document whether "meaning" is statistically inferred or explicitly present
2. **Small Model Cultural Benchmark:** Create evaluation metrics for cultural nuance preservation that go beyond accuracy to include interpretive fidelity
3. **Silence Mapping Framework:** Develop systematic methods to identify and document data absences, then measure their impact on model behavior across different domains