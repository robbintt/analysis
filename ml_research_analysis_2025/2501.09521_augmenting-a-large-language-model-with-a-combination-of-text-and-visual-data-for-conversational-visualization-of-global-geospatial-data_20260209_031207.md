---
ver: rpa2
title: Augmenting a Large Language Model with a Combination of Text and Visual Data
  for Conversational Visualization of Global Geospatial Data
arxiv_id: '2501.09521'
source_url: https://arxiv.org/abs/2501.09521
tags:
- information
- visualization
- data
- visual
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to augment LLMs with both visual
  and textual data for conversational visualization of scientific datasets. The key
  innovation is extracting visual features from rendered frames using vision models,
  combining them with textual descriptions, and structuring the combined information
  into a compact JSON format.
---

# Augmenting a Large Language Model with a Combination of Text and Visual Data for Conversational Visualization of Global Geospatial Data

## Quick Facts
- arXiv ID: 2501.09521
- Source URL: https://arxiv.org/abs/2501.09521
- Reference count: 40
- Authors augment LLMs with structured JSON combining visual features from frames and text descriptions, achieving significantly higher accuracy on visualization questions than text-only or text-plus-vision baselines

## Executive Summary
This paper presents a method to enable LLMs to understand and respond to questions about scientific visualizations by augmenting them with structured data that combines visual features extracted from rendered frames and textual descriptions. The system processes pre-rendered frames through a vision model, merges the extracted visual information with dataset text descriptions, and structures this combined information into a compact JSON format. This JSON is then used to augment every user query in a conversational interface, allowing the LLM to accurately interpret visualizations without requiring fine-tuning. The approach was evaluated on geospatial data from the Science On a Sphere project, showing significantly improved accuracy over baseline methods and achieving high usability scores in user studies.

## Method Summary
The method involves two main phases: pre-processing and runtime. During pre-processing, the system samples frames from visualization videos, extracts visual features using a vision model (OpenAI VISION) with a descriptive prompt, and concatenates these with textual dataset descriptions. This combined information is then fed to GPT-4o with a structured prompt that instructs it to generate a compact JSON containing categories like Title, Visual Cues, Color Encoding, and Key Points. At runtime, each user query is augmented with this JSON context and the last 20 interactions in a FIFO queue. The system uses a dual-bot architecture where GPT-4o generates conversational responses while GPT-4 extracts navigation commands, routing each query through both models to reduce confusion and improve reliability.

## Key Results
- Structured JSON augmentation achieved significantly higher accuracy than text-only (p<0.05) and text-plus-vision baselines
- Average SUS usability score of 83.75 indicates high user satisfaction
- The system supports multi-language conversational interactions across various devices including XR platforms
- LLM-agnostic approach requires no fine-tuning and works with standard API calls

## Why This Works (Mechanism)

### Mechanism 1: Structured Visual-Text Fusion via Vision Model Extraction
- **Claim:** Extracting visual features from rendered frames and merging them with textual descriptions into a structured JSON format improves LLM accuracy on visualization questions compared to providing raw text or text-plus-image inputs.
- **Mechanism:** A vision model processes sampled frames with a descriptive prompt to extract color patterns, visual encodings, and spatial features. This output is concatenated with the dataset's text description, then fed to an LLM with a prompt instructing it to generate a compact JSON structure containing categories like Title, Visual Cues, Color Encoding, Locations, and Key Points. At runtime, this JSON is injected into every prompt, providing consistent, parseable context.
- **Core assumption:** LLMs process structured, compact representations more effectively than verbose free-form text, even when the structured version contains less total information.
- **Evidence anchors:**
  - [abstract] "extracting visual features from rendered frames using vision models, combining them with textual descriptions, and structuring the combined information into a compact JSON format"
  - [section 4.1.3] "The novelty of this approach lies in the use of a structured format to insure that we get a single text file used for LLM augmentation"
  - [section 7.2] "Our PoC is significantly more accurate than pure text augmentation, supporting H1. It also outperforms the baseline method that combines text description with visual information"
  - [corpus] Related work (VizTA, VizGen) explores LLM-visualization interaction but does not specifically validate structured JSON augmentation versus raw visual input; corpus evidence for this specific mechanism is weak.
- **Break condition:** If visual elements change dynamically per frame (e.g., time-varying tsunami spread) and too few frames are sampled, the JSON will lack time-dependent visual state, causing incorrect or stale responses. The paper explicitly notes this limitation (section 6.2).

### Mechanism 2: Dual-Bot Separation for Query Interpretation and Action Execution
- **Claim:** Routing each user query through two separate LLM instances—one for conversational response (GPT-4o) and one for command extraction (GPT-4)—reduces confusion and improves output reliability.
- **Mechanism:** At runtime, the user query plus context window plus JSON augmentation is sent to both models. GPT-4o generates the informational response. GPT-4 parses the query for navigation/control commands and outputs structured commands (e.g., `[ϕ, λ]` coordinates or `[axis, angle]` rotations) or `null` if no action is needed. The system executes valid commands deterministically.
- **Core assumption:** Task separation prevents the same model from conflating explanation generation with precise command formatting; GPT-4 is assumed better at following strict output-format instructions than GPT-4o.
- **Evidence anchors:**
  - [section 4.2.2] "we found GPT-4 to be better than GPT-4o at following precise instructions to generate formal commands, hence our choice to use it for this purpose"
  - [section 4.2.2] "Having one system per task greatly reduces the risk of the LLM getting confused and failing to deliver the expected output"
  - [corpus] No direct corpus validation of dual-bot versus single-bot architectures for visualization tasks; mechanism remains paper-specific.
- **Break condition:** If command prompts are underspecified or ambiguous, GPT-4 may output malformed commands (e.g., invalid coordinates), leading to failed or incorrect scene changes.

### Mechanism 3: Context Window FIFO for Multi-Turn Dialogue Continuity
- **Claim:** Maintaining a sliding window of the last 20 user–LLM interactions in a FIFO queue enables coherent multi-turn conversations without native memory.
- **Mechanism:** Each new query is prepended with the last 20 interaction pairs and the JSON augmentation before being sent to the LLM. The queue updates dynamically: oldest entries are dropped when capacity is exceeded.
- **Core assumption:** 20 interactions balance context retention with token limits and LLM attention capacity; exceeding this causes confusion rather than improving memory.
- **Evidence anchors:**
  - [section 4.2.1] "we found this value of 20 to be sufficient for multi-turn dialogue, and low enough to avoid overwhelming the LLM with excessive information"
  - [section 4.2.1] "This synthetic memory stores interactions between the user and the LLM in a FIFO queue"
  - [corpus] Related papers (e.g., "State of the Art of LLM-Enabled Interaction with Visualization") discuss multi-turn dialogue in LLM-vis contexts but do not evaluate specific context-window sizes.
- **Break condition:** If conversations exceed 20 turns and reference early context, the system will lose thread continuity. Complex tasks requiring long-term reference will degrade.

## Foundational Learning

- **Concept: Vision-Language Model Feature Extraction**
  - **Why needed here:** The system relies on a vision model to translate rendered frames into textual descriptions of visual encodings. Without understanding how vision models respond to prompts, you cannot debug poor extraction quality.
  - **Quick check question:** Given a sample visualization frame, can you predict which visual features a vision model would likely extract versus miss?

- **Concept: Structured Prompt Augmentation for LLMs**
  - **Why needed here:** The paper's central claim is that JSON-structured augmentation outperforms verbose text. Understanding how LLMs attend to structured vs. unstructured context is essential for prompt design.
  - **Quick check question:** If you double the JSON fields but halve the content per field, would you expect accuracy to increase, decrease, or stay the same? Why?

- **Concept: Context Window Management in Stateless APIs**
  - **Why needed here:** LLM APIs are stateless; conversation history must be manually managed. Mismanaging this breaks multi-turn dialogue.
  - **Quick check question:** If a user references "the red area we discussed earlier" in turn 25 of a conversation, what will the system do given a 20-turn FIFO window?

## Architecture Onboarding

- **Component map:**
  1. **Pre-processing Pipeline** (run once per dataset):
     - Frame Sampler → selects N frames from video
     - Vision Model (OpenAI VISION) → extracts visual descriptions
     - Text Concatenator → merges extracted visuals with dataset text description
     - JSON Generator (GPT-4o) → produces structured augmentation file
  2. **Runtime System**:
     - Context Window Manager (FIFO queue, 20-turn limit)
     - Dual-Bot Router → sends query + context + JSON to both GPT-4o (info) and GPT-4 (commands)
     - Command Executor → parses and applies navigation commands
     - Response Renderer → returns text/audio to user

- **Critical path:** Frame sampling quality → Vision model extraction accuracy → JSON structure completeness → Runtime prompt construction → LLM response accuracy. Errors propagate; incomplete JSON causes the most user-visible failures.

- **Design tradeoffs:**
  - More sampled frames → higher accuracy but longer preprocessing and higher API costs
  - Larger context window → better continuity but higher token costs and potential LLM confusion
  - Dual-bot architecture → cleaner separation but doubles API calls per query

- **Failure signatures:**
  - LLM misidentifies color encodings → JSON missing or incomplete color mapping fields
  - Commands not executed → GPT-4 returning `null` for valid requests (prompt mismatch) or malformed output
  - LLM forgets earlier context → conversation exceeds 20 turns
  - Temporal questions answered incorrectly → dataset has time-varying visuals but too few frames sampled

- **First 3 experiments:**
  1. **Baseline comparison on held-out queries:** Take 20 real user queries from the study. Run three conditions (text-only, text+image, structured JSON) and blind-score outputs. Confirm JSON augmentation improves accuracy per section 7.2.
  2. **Frame count sensitivity:** For a time-varying dataset (e.g., tsunami), preprocess with 2, 5, and 10 sampled frames. Measure accuracy on temporal questions. Identify break-even point where more frames yield diminishing returns.
  3. **Context window ablation:** Run multi-turn dialogues of 10, 20, and 30 turns with cross-turn references. Measure accuracy decay beyond the 20-turn boundary to validate the FIFO window sizing assumption.

## Open Questions the Paper Calls Out
None

## Limitations
- The specific runtime prompts for the conversational agent are not provided, only the pre-processing prompts, preventing exact reproduction of conversational behavior
- The study evaluated only 10 datasets from one source (NOAA SoS), limiting generalizability to other visualization types or domains
- The temporal reasoning capability is explicitly limited to datasets with only 2 sampled frames, which may fail on highly dynamic visualizations

## Confidence
- **High confidence** in the core claim that structured JSON augmentation improves accuracy over text-only or text-plus-image baselines, supported by statistical significance (p<0.05) and direct comparison in section 7.2
- **Medium confidence** in the dual-bot architecture benefits, as the claim relies primarily on author observation rather than comparative evaluation with single-bot approaches
- **Medium confidence** in the 20-turn context window being optimal, as this was determined empirically by the authors without broader validation across different conversation types

## Next Checks
1. **Cross-dataset generalization test:** Apply the pre-processing pipeline to 5-10 additional scientific visualization datasets from different domains (e.g., medical imaging, financial charts) and measure accuracy degradation compared to SoS datasets
2. **Temporal frame sampling ablation:** Systematically vary frame sampling from 1-10 frames on datasets with known temporal variation (tsunami, hurricane paths) and measure accuracy on time-dependent questions to identify optimal sampling strategy
3. **Single-bot vs dual-bot comparison:** Implement a version using only GPT-4o for both conversational response and command extraction, then compare accuracy and command success rates on a standardized test set to quantify the claimed benefits of task separation