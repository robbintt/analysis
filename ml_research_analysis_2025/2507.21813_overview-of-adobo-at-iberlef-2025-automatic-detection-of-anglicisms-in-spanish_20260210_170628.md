---
ver: rpa2
title: 'Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish'
arxiv_id: '2507.21813'
source_url: https://arxiv.org/abs/2507.21813
tags:
- spanish
- task
- anglicisms
- were
- borrowings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ADoBo 2025 shared task focused on automatically identifying
  English lexical borrowings (anglicisms) in Spanish journalistic texts. Five teams
  participated, using methods ranging from large language models (LLMs) to rule-based
  and transformer-based systems.
---

# Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish

## Quick Facts
- arXiv ID: 2507.21813
- Source URL: https://arxiv.org/abs/2507.21813
- Reference count: 26
- Best system achieved F1=0.99 using LLM with guideline-enriched prompting

## Executive Summary
The ADoBo 2025 shared task at IberLEF focused on automatically identifying English lexical borrowings (anglicisms) in Spanish journalistic texts. Five teams participated, employing diverse approaches ranging from large language models to rule-based and transformer-based systems. The best-performing system achieved an F1 score of 0.99, with other top systems scoring between 0.92 and 0.96. While these results demonstrate that current models can achieve near-perfect performance on this task, challenges remain for ambiguous or novel borrowings. The evaluation used the newly created BLAS test set specifically designed to cover linguistic variability, though it does not thoroughly test precision in contexts with high false-positive potential.

## Method Summary
The task involved identifying English lexical borrowings (anglicisms) in Spanish journalistic text as span-based extraction. Five teams participated using different approaches: LLMs with guideline-enriched prompting, transformer-based token classification models (BETO, XLM-R, mBERT), rule-based systems using gazetteers, and combinations thereof. The evaluation used strict span-level metrics (Precision, Recall, F1) with case-insensitive matching and exact span matching (no partial credit). The BLAS test set contained 1,836 sentences with 2,076 annotated anglicism spans, while a filtered development set from prior work was provided for validation.

## Key Results
- Best system achieved F1=0.99 using OpenAI's o3 model with explicit annotation guidelines in prompts
- Rule-based gazetteer system achieved F1=0.96 using 37,000+ borrowing candidates and contextual rules
- Transformer-based systems achieved F1=0.92 using post-processing with RAE dictionary and NER filtering
- Systematic failures occurred on ambiguous words existing in both languages (e.g., "pie," "red")

## Why This Works (Mechanism)

### Mechanism 1: Guideline-Enriched LLM Prompting
Large language models can achieve near-perfect anglicism detection when prompted with explicit annotation guidelines and reminders. Providing detailed linguistic criteria in prompts reduces task ambiguity by encoding boundary conditions for what constitutes an unassimilated borrowing versus assimilated loanwords or named entities. The o3 model with guidelines scored F1=0.99 versus F1=0.45 without guidelines. Performance degrades significantly for ambiguous words that exist in both Spanish and English.

### Mechanism 2: Gazetteer-Based Rule Systems with Contextual Features
Lexicon-driven approaches can achieve competitive performance (F1=0.96) with lower computational cost than LLMs. A pre-compiled gazetteer of anglicism candidates (37,000 entries) extracted from journalistic corpora using typographic cues serves as the backbone, with contextual rules handling casing, sentence position, and punctuation variations. Performance is limited to the gazetteer's coverage, missing novel borrowings not in the lexicon.

### Mechanism 3: Transformer Sequence Labeling with Post-Processing Heuristics
Fine-tuned transformer models framed as token classification can achieve strong results when combined with precision-focused post-processing. BIO-encoded sequence labeling identifies candidate spans; post-processing modules cross-reference dictionaries (RAE) and NER systems to filter false positives from foreign named entities. Common failures include adjacent span fusion and multiword expression splitting.

## Foundational Learning

- **Sequence Labeling with BIO Encoding**: Anglicism detection is fundamentally span extraction; BIO tags (Beginning, Inside, Outside) provide token-level labels that convert to contiguous spans. Given "El prime time es popular," BIO tags would be: O, B, I, O, O, O.
- **Lexical Borrowing vs. Codeswitching vs. Named Entities**: The paper distinguishes unassimilated borrowings (retained English orthography/morphology) from assimilated loanwords and foreign named entities. "Sgt. Pepper's Lonely Hearts Club Band" in quotation marks is a named entity, not an anglicism span.
- **Precision vs. Recall in Benchmark Design**: The BLAS test set maximizes recall challenges but under-tests precision. If a system expands spans to include all quoted text containing a known anglicism, it may include non-borrowing quoted content, reducing precision.

## Architecture Onboarding

- **Component map**: Input -> Core detection (LLM/Transformer/Rule-based) -> Post-processing (span normalization, dictionary cross-reference, NER filtering) -> Output CSV
- **Critical path**: 1) Obtain BLAS test data and development set 2) Select architecture based on resource constraints 3) Implement span extraction with strict matching 4) Add heuristics for casing/quotation handling
- **Design tradeoffs**: LLM approach offers highest performance but API dependency; rule-based offers strong performance with low compute but requires gazetteer curation; transformer offers moderate performance with trainable flexibility
- **Failure signatures**: Ambiguous word confusion, adjacent span fusion, multiword splitting, named entity false positives
- **First 3 experiments**: 1) Reproduce baseline using fine-tuned BETO on COALAS dataset 2) Test zero-shot LLM detection with simple vs. guideline-enriched prompts 3) Build minimal gazetteer from development set and test recall with/without contextual rules

## Open Questions the Paper Calls Out
None

## Limitations
- The claim of task "largely solved" is qualified by untested precision on false-positive-rich contexts and systematic failures on ambiguous cases
- Exceptional LLM performance critically depends on prompt engineering with detailed guidelinesâ€”without these, performance drops dramatically
- Rule-based systems require extensive pre-compiled lexicons and cannot reliably detect novel borrowings
- No systematic evaluation of computational cost versus performance trade-offs was provided

## Confidence
- **High confidence**: Task setup, evaluation methodology, and basic performance ordering are well-documented and reproducible
- **Medium confidence**: LLM near-perfect performance claim is supported but critically depends on undocumented prompt details
- **Low confidence**: Assertion that task is "largely solved" is premature given untested precision scenarios and fundamental limitations

## Next Checks
1. Systematically test zero-shot LLM performance across multiple prompt variants on the development set to quantify impact of task specification
2. Construct a benchmark subset designed to maximize false positives and measure precision degradation across all system types
3. Evaluate each system's ability to detect anglicisms not present in any lexicon or training data by introducing controlled novel borrowings into test sentences