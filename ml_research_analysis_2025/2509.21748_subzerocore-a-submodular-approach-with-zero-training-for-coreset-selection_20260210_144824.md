---
ver: rpa2
title: 'SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection'
arxiv_id: '2509.21748'
source_url: https://arxiv.org/abs/2509.21748
tags:
- selection
- coreset
- subzerocore
- coverage
- submodular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SubZeroCore, a training-free coreset selection
  method that integrates submodular coverage and density optimization. Unlike existing
  methods requiring expensive model training, SubZeroCore uses a closed-form solution
  to balance these objectives, controlled by a single hyperparameter for desired coverage.
---

# SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection

## Quick Facts
- arXiv ID: 2509.21748
- Source URL: https://arxiv.org/abs/2509.21748
- Reference count: 40
- Introduces training-free coreset selection method using submodular optimization

## Executive Summary
SubZeroCore presents a novel training-free approach for coreset selection that integrates submodular coverage and density optimization. Unlike existing methods requiring expensive model training, SubZeroCore uses a closed-form solution to balance these objectives through a single hyperparameter controlling desired coverage. The method demonstrates comparable or superior performance to training-based baselines on CIFAR-10 and ImageNet-1K, particularly at high pruning rates, while significantly reducing computational overhead.

## Method Summary
SubZeroCore employs a greedy submodular maximization framework that weights the facility location objective by local density. The method first determines optimal neighborhood size K through numerical inversion of a coverage formula, then computes K-NN distances to estimate local density. These density scores weight similarity terms in the facility location function, which is maximized greedily to select a representative coreset. The entire process requires only a single forward pass through a pretrained model to extract embeddings, eliminating the need for iterative training.

## Key Results
- Achieves 95.4% accuracy on CIFAR-10 with only 0.1% of training data (99.9% pruning)
- Outperforms training-based baselines at high pruning rates (50-90%)
- Demonstrates superior robustness to 10% label noise compared to standard facility location
- Requires only one forward pass through pretrained model versus multiple training iterations

## Why This Works (Mechanism)

### Mechanism 1: Density-Weighted Submodular Optimization
Weighting the facility location objective by local density improves coreset quality by downweighting outliers. Density scores are computed from K-NN radii using Gaussian weighting, where samples in averagely dense regions receive weights near 1 while outliers are downweighted. This makes the method robust to label noise as mislabeled data tends to lie in sparse regions.

### Mechanism 2: Coverage-Guided K Selection via Closed-Form Inversion
The single hyperparameter γ (target coverage) determines optimal neighborhood size K through mathematical inversion of the expected coverage formula. This eliminates manual tuning by finding K that achieves desired coverage in the combinatorial model. The method shows γ≈0.6 provides best trade-off for CIFAR-10/ImageNet-1K.

### Mechanism 3: Greedy Submodular Maximization with Approximation Guarantee
The weighted facility location remains submodular, enabling greedy optimization with (1-1/e) ≈ 63% approximation guarantee. High submodular score correlates with downstream model generalization, making greedy selection effective despite suboptimality.

## Foundational Learning

- **Submodular Functions and Diminishing Returns**
  - Why needed here: Enables efficient greedy optimization with provable guarantees
  - Quick check question: Given sets A ⊆ B and element j, can you state the diminishing returns inequality that defines submodularity?

- **K-Nearest Neighbor Density Estimation**
  - Why needed here: Density scores derive from K-NN radii; understanding K's effect is crucial
  - Quick check question: If sample x has radius r = NND_K(x), what does a smaller r indicate about x's local neighborhood?

- **Facility Location Objective**
  - Why needed here: Base submodular function being weighted; understanding coverage behavior is essential
  - Quick check question: In f_FL(S) = Σ_x max_{x_S∈S} sim(x, x_S), why does this objective inherently favor coverage over density?

## Architecture Onboarding

- **Component map**: Embedding Extraction -> K Determination -> Radius Computation -> Density Scoring -> Greedy Selection
- **Critical path**: Embedding quality → K-NN distance accuracy → density score validity → submodular optimization. Errors propagate through all steps.
- **Design tradeoffs**:
  - Larger K: Smoother density estimates but higher O(N²·d/C) compute cost per class
  - Higher γ: More coverage-focused, potentially including sparse outliers; lower γ: More density-focused
  - Pretrained vs. task-specific embeddings: Paper uses generic pretrained models; specialized domains may need fine-tuned encoders
- **Failure signatures**:
  - Coreset performs worse than random sampling: Check embedding quality on your domain
  - Extreme K values (K≈1 or K≈N): Dataset too small for closed-form coverage; use direct coverage estimation
  - No improvement over vanilla facility location: Density weighting has no effect when radii distribution is uniform
- **First 3 experiments**:
  1. Baseline validation: Run SubZeroCore on CIFAR-10 with γ=0.6, α=0.9, compare test accuracy against facility location (no density weighting)
  2. K sensitivity analysis: For your dataset size |T| and target pruning α, compute K for γ∈{0.3, 0.5, 0.7, 0.9}; verify K values are reasonable
  3. Embedding ablation: Compare SubZeroCore using different pretrained encoders (ResNet-18, InceptionNetV3, CLIP) on your data

## Open Questions the Paper Calls Out

- **Open Question 1**: How can SubZeroCore be adapted for dynamic data streams where the dataset evolves over time?
  - Basis: Conclusion explicitly mentions extending framework to dynamic data streams
  - Why unresolved: Current method relies on static density calculations and fixed dataset
  - What evidence would resolve it: Algorithmic extension with online updates and streaming benchmark evaluation

- **Open Question 2**: Can introducing an exponent to the density weights improve the balance between coverage and density?
  - Basis: Conclusion suggests introducing additional power on weights to control density/coverage balance
  - Why unresolved: Current density score is a fixed function with no mechanism to tune density influence
  - What evidence would resolve it: Ablation study introducing power parameter p to demonstrate performance gains

## Limitations

- Performance relies on quality of pretrained embeddings, which may not capture task-specific structure
- Closed-form coverage formula becomes unstable for very small datasets where |T| ≈ |S| + K
- Optimal γ value (0.6) determined empirically for vision datasets; may not generalize to other modalities

## Confidence

- **High Confidence**: Submodular optimization framework with (1-1/e) approximation guarantee
- **Medium Confidence**: Density weighting improves robustness to label noise (demonstrated empirically)
- **Low Confidence**: Closed-form K determination via coverage inversion generalizes beyond tested datasets

## Next Checks

1. **Domain Transfer Validation**: Apply SubZeroCore to a domain-specific dataset (e.g., medical imaging) using task-adapted embeddings to test if density weighting maintains performance advantage

2. **Extreme Label Noise Test**: Evaluate robustness under 20-30% label corruption to determine if density weighting scales with noise level

3. **Embeddings Ablation**: Compare performance using frozen generic embeddings versus task-specific fine-tuned embeddings to quantify embedding quality impact