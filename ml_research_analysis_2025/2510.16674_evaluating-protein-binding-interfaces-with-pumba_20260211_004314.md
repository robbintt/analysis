---
ver: rpa2
title: Evaluating protein binding interfaces with PUMBA
arxiv_id: '2510.16674'
source_url: https://arxiv.org/abs/2510.16674
tags:
- protein
- piston
- pumba
- docking
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PUMBA is a deep learning scoring function for protein-protein docking
  that replaces the Vision Transformer backbone of PIsToN with a Vision Mamba architecture.
  This substitution enables more efficient modeling of long-range dependencies in
  protein interface features while maintaining interpretability.
---

# Evaluating protein binding interfaces with PUMBA

## Quick Facts
- arXiv ID: 2510.16674
- Source URL: https://arxiv.org/abs/2510.16674
- Authors: Azam Shirali; Giri Narasimhan
- Reference count: 8
- Primary result: PUMBA achieves higher AUC ROC (68.83 vs 64.77) and success rates on CAPRI v2022 Difficult benchmark compared to PIsToN

## Executive Summary
PUMBA is a deep learning scoring function for protein-protein docking that replaces the Vision Transformer backbone of PIsToN with a Vision Mamba architecture. This substitution enables more efficient modeling of long-range dependencies in protein interface features while maintaining interpretability. PUMBA outperforms PIsToN across eight benchmark datasets, with significant improvements in challenging cases such as CAPRI v2022 Difficult and Dockground.

## Method Summary
PUMBA takes refined protein-protein docking models from FireDock and converts them into multi-channel 2D surface patch images using MaSIF preprocessing. The model employs a Vision Mamba backbone with 5 parallel feature-group branches (shape, RASA, charge, H-bonds, hydropathy) that feed into a final encoder. Bidirectional state space modeling enables efficient long-range sequence processing, while hidden attention reformulation preserves interpretability. The model is trained with a combined loss of binary cross-entropy, supervised contrastive loss, and margin ranking loss.

## Key Results
- PUMBA achieves 68.83 AUC ROC on CAPRI v2022 Difficult versus 64.77 for PIsToN
- On Dockground benchmark, PUMBA reaches 25.33 AP versus 10.71 for PIsToN
- PUMBA shows 32% Top1 success rate on Dockground versus 15% for PIsToN
- The model maintains interpretability through hidden attention analysis focused on biologically relevant features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Vision Transformer attention with Vision Mamba's bidirectional state space modeling improves capture of both local and global interface patterns
- Mechanism: Vision Mamba uses forward and backward SSM passes per encoder block, enabling each token to access both past and future contexts without quadratic attention cost
- Core assumption: Protein interface features exhibit spatial correlations that benefit from bidirectional sequence modeling rather than global attention matrices
- Evidence anchors: [abstract] "This change allows us to leverage Mamba's efficient long-range sequence modeling"; [section 3.1] "ViM utilizes Bidirectional SSMs idea"

### Mechanism 2
- Claim: Linear sequence complexity enables efficient processing of high-resolution interface images without sacrificing discriminative power
- Mechanism: Mamba's selective state space model processes P patches in O(P) time versus ViT's O(P²) self-attention, reducing memory and compute while maintaining modeling capacity
- Core assumption: The efficiency gain does not come at the cost of losing critical interface discriminators that require dense attention
- Evidence anchors: [section 2.2] "Mamba... employs a selective state space model (SSM) that processes sequences with linear complexity"

### Mechanism 3
- Claim: Hidden attention matrix reformulation preserves interpretability despite lacking explicit self-attention
- Mechanism: Following Ali et al. (2024), PUMBA reformulates Mamba layers to expose implicit attention matrices, enabling finer-grained token-to-token influence tracking
- Core assumption: Domain experts can derive actionable biological insights from hidden attention maps comparable to explicit ViT attention
- Evidence anchors: [section 2.3] "SSM models like Mamba can be reformulated to expose implicit attention matrices"

## Foundational Learning

- Concept: **State Space Models (SSMs) and Selective Scan**
  - Why needed here: PUMBA's core innovation is replacing attention with SSMs; understanding selective state dynamics is prerequisite to debugging the backbone
  - Quick check question: Can you explain how a selective SSM differs from standard RNN hidden state updates in terms of input-dependent gating?

- Concept: **Vision Transformers and Patch Tokenization**
  - Why needed here: PUMBA inherits PIsToN's image preprocessing; understanding patch embedding and positional encoding is necessary to interpret the input pipeline
  - Quick check question: Given a 224×224 image with patch size 16, how many tokens does ViT produce, and where is positional information encoded?

- Concept: **Protein Interface Feature Engineering**
  - Why needed here: PUMBA relies on hand-crafted features (shape index, curvature, hydropathy, etc.); understanding these is critical for data validation and explainability mapping
  - Quick check question: Why are geodesic distances used instead of Euclidean distances when defining surface patches for protein interfaces?

## Architecture Onboarding

- Component map: PDB structure → FireDock refinement → Surface cropping/triangulation → Feature computation (6 channels + patch dist) → MDS projection to multi-channel 2D image → Patch tokenization with center CLS token → 5 parallel ViM encoders (shape, RASA, charge, H-bonds, hydropathy) + FC integration → Final ViM encoder → Binding score

- Critical path: Feature image quality → patch tokenization → bidirectional SSM state propagation → hidden attention alignment with biological features → final score calibration

- Design tradeoffs: ViM bidirectional scans add computation vs. unidirectional SSM; center CLS token placement optimizes bidirectional context but may miss corner patch information; 5-branch architecture increases parameters but enables feature-specific attention analysis

- Failure signatures: (1) AUC ROC drops below PIsToN on easy datasets → bidirectional fusion may be corrupting local patterns; (2) Hidden attention maps show random noise → reformulation not correctly implemented or normalization issue; (3) Training loss diverges → check learning rate compatibility with SSM dynamics

- First 3 experiments:
  1. **Reproduction baseline**: Run PUMBA on CAPRI v2022 Difficult subset; verify AUC ROC ~68.83 and Top1 success ~22% match reported values to validate code correctness
  2. **Ablation on CLS token position**: Move CLS token from center to start/end; measure impact on Dockground AP to confirm center placement benefit
  3. **Hidden attention sanity check**: Extract hidden attention for a known native complex (e.g., 1J3R); verify hydropathy feature receives highest attention as reported in Fig.2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does PUMBA provide empirical computational advantages (speed/memory) over PIsToN?
- Basis: The authors cite Mamba's theoretical linear complexity but report no runtime or memory benchmarks
- Why unresolved: Hardware specifications are listed, but efficiency metrics are absent from the evaluation
- What evidence would resolve it: Comparative analysis of inference time and peak GPU memory usage

### Open Question 2
- Question: Is PUMBA's performance limited by the reuse of PIsToN's specific training dataset?
- Basis: The model adopts the exact training data and loss formulation of its predecessor
- Why unresolved: It is unclear if the enhanced architecture is data-saturated or restricted by legacy dataset size
- What evidence would resolve it: Re-training PUMBA on larger, modern structural datasets (e.g., AlphaFold DB)

### Open Question 3
- Question: How does PUMBA compare to integrated end-to-end predictors like AlphaFold3?
- Basis: AlphaFold3 is discussed in related work but excluded from experimental benchmarks
- Why unresolved: The study focuses solely on the PIsToN lineage, leaving the relative utility against generative predictors unknown
- What evidence would resolve it: Head-to-head ranking accuracy against AF3's confidence metrics on a blind test set

## Limitations

- Critical hyperparameters for Vision Mamba architecture (layer count, embedding dimensions, patch size, SSM parameters) are not specified, making exact reproduction challenging
- The bidirectional SSM implementation's computational scaling on large interface images remains empirically unverified despite theoretical O(P) complexity claims
- Hidden attention reformulation, while enabling interpretability, lacks validation through ablation studies showing its contribution to performance gains

## Confidence

- **High Confidence**: Benchmark performance improvements over PIsToN (supported by multiple datasets and metrics)
- **Medium Confidence**: Architectural efficiency claims (theoretical complexity arguments lack empirical scaling validation)
- **Medium Confidence**: Interpretability preservation (qualitative feature map analysis without quantitative validation)

## Next Checks

1. **Architectural Scaling Validation**: Profile training time and memory usage on interface images of varying resolution to empirically verify the O(P) complexity advantage of bidirectional SSM over ViT attention

2. **Hidden Attention Ablation**: Implement an ablated version of PUMBA without hidden attention reformulation and compare both performance and explainability outputs on CAPRI v2022 Difficult to quantify the reformulation's contribution

3. **Cross-Dataset Generalization**: Evaluate PUMBA on protein-protein docking datasets from other structural biology communities (e.g., HADDOCK benchmarks) to test robustness beyond the eight specified benchmarks