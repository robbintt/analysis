---
ver: rpa2
title: Pruning Increases Orderedness in Recurrent Computation
arxiv_id: '2507.14747'
source_url: https://arxiv.org/abs/2507.14747
tags:
- pruning
- orderedness
- layer
- perceptron
- complete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether the directionality commonly found in
  artificial neural networks is a necessary inductive bias or can be discovered through
  training. The authors introduce the "complete perceptron layer," a weight-tied recurrent
  architecture with all-to-all connections, and formalize a metric called "orderedness"
  to measure topological ordering of information flow in the weight matrix.
---

# Pruning Increases Orderedness in Recurrent Computation

## Quick Facts
- arXiv ID: 2507.14747
- Source URL: https://arxiv.org/abs/2507.14747
- Reference count: 20
- Primary result: Pruning induces topological ordering in recurrent weight matrices without compromising performance

## Executive Summary
This paper explores whether the directionality commonly found in artificial neural networks is a necessary inductive bias or can be discovered through training. The authors introduce the "complete perceptron layer," a weight-tied recurrent architecture with all-to-all connections, and formalize a metric called "orderedness" to measure topological ordering of information flow in the weight matrix. They demonstrate that with appropriate pruning techniques—particularly Top-K and dynamic variants—orderedness can be induced without compromising performance. Experiments on XOR and Sine tasks show that pruning correlates with increased orderedness, with Top-K pruning methods achieving significant improvements (e.g., ΔO values up to 0.243 on XOR). The results suggest that directionality is not a prerequisite for learning but may be an advantageous inductive bias discoverable by gradient descent and sparsification.

## Method Summary
The complete perceptron layer is a weight-tied recurrent architecture where all neurons are connected to all others, including themselves. The hidden state evolves over T iterations via s(t+1) = σ([s(t) x]W^T + b), replacing spatial depth with temporal depth. Orderedness O(W) is defined as 1 minus the minimum ratio of lower-triangular to total weight mass across all permutations of hidden neurons. Pruning methods tested include Random (p=0.5), Top-K (k=0.5), Dynamic Top-K, and Tril-damp (f=0.8). Experiments use XOR (5 hidden units, T=3, 1000 steps) and Sine tasks (10 hidden units, T=3, 600 steps) with sigmoid activation, Adam optimizer (lr=0.01), and MSE loss.

## Key Results
- Top-K pruning achieves ΔO up to 0.243 on XOR task
- Dynamic Top-K pruning shows stable orderedness induction across tasks
- Orderedness peaks at T=2 iterations for XOR task, suggesting optimal depth varies by task
- No performance degradation despite increased orderedness from pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning induces topological ordering in recurrent weight matrices without explicit directional bias.
- Mechanism: When pruning eliminates weaker connections during gradient descent, the network preferentially retains weights that support feedforward-like information flow. Sparsity constraints force the model to allocate limited connection budget efficiently, and topologically-ordered paths emerge as solutions requiring fewer "back-flow" edges to accomplish the task.
- Core assumption: Gradient-based learning under sparsity pressure favors computationally efficient pathways, which structurally resemble feedforward organization.
- Evidence anchors:
  - [abstract] "pruning schemes successfully induce greater topological ordering in information flow between neurons without compromising performance"
  - [Section 3.4] "enforcing sparsity could encourage the model to be more economical with its high-magnitude weights, and whether that would lead to increased orderedness for efficient computation"
  - [corpus] "Information Consistent Pruning" (arXiv:2501.15592) supports iterative magnitude pruning uncovering efficient subnetworks, though doesn't directly address orderedness.

### Mechanism 2
- Claim: Temporal depth can substitute for spatial depth in all-to-all connected networks.
- Mechanism: The complete perceptron layer evolves hidden state s(t) over T iterations via s(t+1) = σ([s(t) x]W^T + b), replacing multi-layer feedforward structure with iterative recurrent application. Input is clamped; hidden units integrate information across timesteps before output extraction.
- Core assumption: T iterations provide sufficient computational capacity for tasks traditionally requiring T layers.
- Evidence anchors:
  - [Section 3.1] "we run it for multiple iterations, replacing spatial depth with temporal depth"
  - [Appendix D, Figure 5] Training loss descends smoothly on both XOR and Sine tasks, confirming trainability.
  - [corpus] "Physics-inspired Energy Transition Neural Network" (arXiv:2505.03281) discusses recurrent alternatives to layered architectures, though different formalism.

### Mechanism 3
- Claim: The orderedness metric O(W) quantifies latent topological structure independent of neuron indexing.
- Mechanism: Since hidden unit ordering is arbitrary, O(W) finds the permutation π minimizing lower-triangular weight mass: O(W) = 1 - min_π(L(π(W_abs))/S(π(W_abs))). Higher O indicates more weights flow "forward" under optimal ordering.
- Core assumption: Networks contain discoverable directional structure that permutation can reveal.
- Evidence anchors:
  - [Section 3.3] Full formalization of orderedness metric with permutation search.
  - [Figure 1] Visual comparison of block-structured MLP weights vs. upper-triangular topologically-ordered weights.
  - [corpus] Weak direct evidence; no corpus papers formalize equivalent metrics.

## Foundational Learning

- Concept: **Topological ordering in directed graphs**
  - Why needed here: Orderedness assumes understanding that acyclic information flow corresponds to upper-triangular adjacency structure; back-flow edges create cycles.
  - Quick check question: Given a 4-neuron recurrent network, which connections must be removed to ensure no information cycles back?

- Concept: **Weight-tied recurrence vs. feedforward depth**
  - Why needed here: Complete perceptron uses same W across iterations; distinguishing this from weight-untied layered computation clarifies capacity differences.
  - Quick check question: If W is applied 5 times with sigmoid activation, how does gradient flow differ from a 5-layer MLP with distinct W_1,...,W_5?

- Concept: **Magnitude-based pruning as implicit regularization**
  - Why needed here: Top-K pruning selects weights by absolute value, implicitly assuming large-magnitude connections are functionally important.
  - Quick check question: Why might pruning small weights preferentially remove "back-flow" connections in a randomly initialized recurrent network?

## Architecture Onboarding

- Component map:
  - Input units (i) clamped externally
  - Hidden units (h) evolve via s(t+1) = σ([s(t) x]W^T + b)
  - Output units (o) extracted from first o positions of s(T)
  - Weight matrix W ∈ R^(o+h)×(o+h+i) with [W_s | W_x] structure
  - Pruning mask applied during training

- Critical path:
  1. Initialize W, v, optionally b
  2. For each batch: evolve s(t) for T iterations per Equation 1
  3. Extract output y = s(T)[:,:o]
  4. Compute loss, backpropagate through all T unrolled steps
  5. Apply pruning to W before next step

- Design tradeoffs:
  - Higher T: More capacity but slower training; may reduce orderedness
  - Higher h: More expressive but orderedness relationship unclear
  - Top-K vs. Tril-damp: Top-K is architecture-agnostic; Tril-damp explicitly biases toward orderedness
  - Dynamic vs. static pruning: Dynamic avoids early-training instability

- Failure signatures:
  - Loss plateaus: T insufficient or learning rate too high
  - Orderedness decreases: Pruning too aggressive early
  - High variance (±0.08): Architecture sensitive to initialization

- First 3 experiments:
  1. Baseline trainability: Implement complete perceptron on XOR with T=3, h=5, no pruning. Confirm loss <0.1 after 1000 steps.
  2. Pruning comparison: Add Top-K (k=0.5) pruning. Measure ΔO before/after training. Expect ~0.12-0.15.
  3. Iteration-depth sweep: Fix h=5, vary T∈{2,3,5,10}. Plot ΔO vs. T to replicate Figure 2 spike pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise theoretical relationship between sparsity levels and orderedness, and why does high variance obscure this relationship?
- Basis in paper: [explicit] "More theoretical and empirical work is required to understand precisely how sparsity affects orderedness."
- Why unresolved: Figure 3 shows trends but variance is too large for conclusions.
- What evidence would resolve it: Larger-scale experiments across more seeds and tasks, plus theoretical analysis linking sparsity constraints to directed graph structure.

### Open Question 2
- Question: Why does orderedness spike at exactly 2 iterations for XOR, and what determines the optimal number of evolution iterations for different tasks?
- Basis in paper: [inferred] Authors note "the relationship between the number of hidden states, evolution iterations, and orderedness is still unclear" and observe an unexplained "spike" at 2 iterations.
- Why unresolved: Limited experiments on only two simple synthetic tasks; no theoretical account for the spike phenomenon.
- What evidence would resolve it: Systematic ablations across iteration counts on diverse tasks, with analysis of information propagation paths.

### Open Question 3
- Question: Does orderedness induction via pruning generalize to more complex, higher-dimensional tasks and larger network sizes?
- Basis in paper: [explicit] "Because of the limitations of compute and the low scalability of the complete perceptron layer, we chose to use very simple tasks for our experiments."
- Why unresolved: Only XOR and 1D Sine tasks tested; network sizes were very small (5–10 hidden units).
- What evidence would resolve it: Scaling experiments on image classification, language modeling, or control tasks with larger hidden dimensions and deeper recurrence.

## Limitations
- Orderedness metric computational complexity grows with network size due to permutation search
- High variance in orderedness measurements across seeds (±0.08) obscures precise relationships
- Experiments limited to simple synthetic tasks with small network sizes (5-10 hidden units)

## Confidence

- **High confidence**: The complete perceptron layer is trainable on both XOR and Sine tasks
- **Medium confidence**: Top-K pruning consistently induces orderedness across tasks
- **Low confidence**: Orderedness is universally beneficial for downstream performance

## Next Checks

1. Implement and benchmark multiple permutation search algorithms (brute-force, greedy swap, Hungarian) for the orderedness metric to establish computational feasibility
2. Conduct systematic ablation studies varying pruning hyperparameters (k values, f values for Tril-damp) across all three tasks to map the orderedness-performance landscape
3. Test orderedness emergence on more complex sequential tasks (e.g., n-bit parity with n > T) to probe the limits of this inductive bias discovery mechanism