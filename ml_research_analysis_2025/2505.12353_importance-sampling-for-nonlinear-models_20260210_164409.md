---
ver: rpa2
title: Importance Sampling for Nonlinear Models
arxiv_id: '2505.12353'
source_url: https://arxiv.org/abs/2505.12353
tags:
- nonlinear
- scores
- linear
- sampling
- leverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in importance sampling techniques
  for nonlinear models by introducing a systematic framework based on the adjoint
  operator of nonlinear maps. The key contribution is generalizing norm-based and
  leverage-score-based importance sampling to nonlinear settings, providing approximation
  guarantees similar to linear subspace embeddings.
---

# Importance Sampling for Nonlinear Models

## Quick Facts
- arXiv ID: 2505.12353
- Source URL: https://arxiv.org/abs/2505.12353
- Reference count: 40
- Primary result: Generalizes norm-based and leverage-score-based importance sampling to nonlinear models using adjoint operators, providing approximation guarantees similar to linear subspace embeddings.

## Executive Summary
This paper addresses the gap in importance sampling techniques for nonlinear models by introducing a systematic framework based on the adjoint operator of nonlinear maps. The key contribution is generalizing norm-based and leverage-score-based importance sampling to nonlinear settings, providing approximation guarantees similar to linear subspace embeddings. The authors demonstrate that sampling according to these generalized scores enables efficient training on large datasets while offering novel mechanisms for model explainability and outlier detection.

## Method Summary
The method introduces the adjoint operator of nonlinear maps to generalize importance sampling to nonlinear settings. For a nonlinear function f(θ), the adjoint operator f⋆(θ) = ∫₀¹ ∂f(tθ)/∂θ dt captures how f varies along the path from 0 to θ, yielding f(θ) = ⟨θ, f⋆(θ)⟩. This enables construction of a nonlinear dual matrix F⋆(θ) by stacking adjoint operators, analogous to the linear case. Leverage scores τᵢ(θ) = ⟨eᵢ, F̃⋆(θ)(F̃⋆(θ))†eᵢ⟩ / rank(F̃⋆(θ)) are then computed, and samples are drawn with probabilities proportional to these scores. The method provides theoretical guarantees: under certain assumptions, sampling O(p log(p/δ)/ε²) points achieves (1-ε)L(θ*) ≤ L_S(θ*S) ≤ (1+ε)L(θ*) with probability ≥ 1-δ.

## Key Results
- The method achieves lower training error with fewer samples compared to uniform and linear sampling strategies on both regression and classification tasks
- Nonlinear leverage scores successfully identify important samples and detect anomalies, particularly in complex datasets like FER-2013
- Under activation function constraints, nonlinear scores can be bounded by linear scores, enabling parameter-independent sampling
- Theoretical analyses show (1-ε)L(θ*) ≤ L_S(θ*S) ≤ (1+ε)L(θ*) with probability at least 1-δ under certain assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonlinear adjoint operators enable inner-product-like representations for general nonlinear maps.
- Mechanism: For a nonlinear function f(θ), the adjoint operator f⋆(θ) = ∫₀¹ ∂f(tθ)/∂θ dt captures how f varies along the path from 0 to θ. This yields f(θ) = ⟨θ, f⋆(θ)⟩ (when f(0)=0), analogous to Riesz representation. For generalized linear predictors f(θ) = ϕ(⟨θ, x⟩), the adjoint simplifies to [(ϕ(⟨θ, x⟩) - ϕ(0)) / ⟨θ, x⟩] · x.
- Core assumption: The function f is absolutely continuous on the path [0, θ], enabling differentiability almost everywhere.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: The nonlinear dual matrix enables subspace embedding guarantees analogous to linear settings.
- Mechanism: Stack adjoint operators into F⋆(θ) ∈ Rⁿˣᵖ. The loss L(θ) = ||F̃⋆(θ)θ̃||² resembles linear least-squares, allowing leverage scores τᵢ(θ) = ⟨eᵢ, F̃⋆(θ)(F̃⋆(θ))†eᵢ⟩ / rank(F̃⋆(θ)) to generalize linear definitions. For fixed θ, sampling O(p log(p/δ)/ε²) points with probabilities proportional to τᵢ(θ) yields (1-ε)L(θ) ≤ L_S(θ) ≤ (1+ε)L(θ) with probability ≥ 1-δ.
- Core assumption: Parameter θ is fixed; the dual matrix is full column rank or has well-defined pseudoinverse.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: Nonlinear scores can be bounded by linear scores under activation function constraints, enabling parameter-independent sampling.
- Mechanism: For Swish-type activations with bounds c₁ ≤ ϕ²(t)/t² ≤ c₂, the nonlinear leverage score τᵢ(θ) satisfies τᵢ(θ) ≤ (u/l) · τᵢ^linear where τᵢ^linear is computed from X^TX. For ReLU networks with bounded weights, norm scores satisfy analogous bounds with β = min{c₁l, 1}/max{c₂u, 1}. This decouples sampling from unknown θ*.
- Core assumption: The optimal parameter θ* lies within a bounded constraint set C; activation functions have bounded ratios ϕ²(t)/t².
- Evidence anchors: [abstract], [section], [corpus]

## Foundational Learning

- Concept: **Leverage Scores (Linear)**
  - Why needed here: The paper directly generalizes linear leverage scores to nonlinear settings; understanding the linear case is essential to grasp the extension.
  - Quick check question: Given a data matrix X ∈ Rⁿˣᵈ, can you compute the leverage score of row xᵢ as xᵢ^T(X^TX)⁻¹xᵢ and explain what it measures about that row's influence on the column space?

- Concept: **Subspace Embedding Property**
  - Why needed here: The theoretical guarantees depend on the sampled matrix preserving spectral properties of the original, formalized through subspace embeddings.
  - Quick check question: If you sample rows from X to form X_S, under what conditions does (1-ε)||Xv||² ≤ ||X_S v||² ≤ (1+ε)||Xv||² hold for all v?

- Concept: **ε-Net and Union Bound Arguments**
  - Why needed here: The extension from fixed-θ to all-θ guarantees uses ε-net discretization plus union bounds to control failure probability over continuous parameter spaces.
  - Quick check question: How many points are needed to cover a ball of radius R in R^p with ε-balls, and how does this scale with dimension?

## Architecture Onboarding

- Component map: Input data -> Adjoint computation -> Dual matrix assembly -> Score calculation -> Sampling engine -> Constrained optimizer
- Critical path: Adjoint computation → Dual matrix assembly → Score calculation → Sampling → Constrained optimization on sampled subset
- Design tradeoffs:
  - Leverage vs. norm scores: Leverage scores provide tighter bounds but require O(np²) for SVD; norm scores are O(np) but may need more samples
  - Exact vs. approximate scores: Closed-form adjoints enable exact scores; numerical integration introduces approximation error
  - Constrained vs. unconstrained optimization: Constraints enable theoretical guarantees but may complicate solver selection
- Failure signatures:
  - Scores all near zero or uniform: Likely numerical issues in adjoint computation or rank-deficient dual matrix
  - Sampled model performs worse than uniform sampling: Constraint set C may not contain θ*, or β bound is too loose
  - High variance across sampling runs: Sample size s insufficient for target ε, δ
- First 3 experiments:
  1. Validate on synthetic single-index model: Generate data from f(θ) = ϕ(⟨θ*, x⟩) + noise with known θ*, compute nonlinear leverage scores at θ*, verify that sampling by these scores reduces MSE vs. uniform sampling as theory predicts
  2. Compare linear vs. nonlinear scores on real regression: On California Housing dataset, train a bounded Swish model, compare training error decay curves for linear leverage, nonlinear leverage, and uniform sampling across varying sample sizes
  3. Outlier detection visualization: On FER-2013 classification, train a small ReLU network, rank samples by nonlinear leverage scores at converged θ, visually inspect top-50 and bottom-50 scoring images for interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the approximation guarantees be extended to general loss functions beyond nonlinear least-squares?
- Basis in paper: [explicit] The Conclusion states, "Although Appendix A.3 outlines preliminary steps... fully generalizing them remains an avenue for future work."
- Why unresolved: The main theoretical results (Theorem 3.1) rely specifically on the structure of squared loss.
- What evidence would resolve it: A proof of the approximation guarantee (Eq. 2) generalized for convex loss functions or non-squared losses.

### Open Question 2
- Question: Can the sample complexity dependence on the parameter dimension be reduced from quadratic to linear?
- Basis in paper: [explicit] Remark 3.3 notes, "our result has quadratic dependence on the dimension, while [Gajjar et al. (2024)] is linear."
- Why unresolved: The current proof technique relies on an ε-net covering a ball, yielding O(p²) scaling.
- What evidence would resolve it: A refined analysis or sampling algorithm achieving a sample complexity of O(p log p / ε²).

### Open Question 3
- Question: Can the approximation guarantees be adapted for unconstrained optimization without requiring a predefined bounded set for the parameters?
- Basis in paper: [inferred] Remark 3.3 contrasts the method's constrained optimization with the unconstrained approach of Gajjar et al. (2024).
- Why unresolved: The current proof requires the constraint set C to establish Lipschitz continuity and approximate scores.
- What evidence would resolve it: A theoretical guarantee holding for θ ∈ R^p without assumptions bounding the norm of θ*.

## Limitations
- The theoretical framework relies heavily on assumptions about the function space (absolutely continuous, smooth activation functions) and bounded parameter sets
- The β-bounds for parameter-independent sampling are conservative and may not hold for all activation functions or unbounded parameter spaces
- Numerical computation of adjoint operators via quadrature introduces approximation errors not quantified in the current analysis
- The experiments focus on relatively small-scale models and datasets, leaving scalability questions unanswered for deep networks

## Confidence

- **High Confidence:** The core adjoint operator construction and its application to leverage score computation (Mechanism 1 and 2). The basic theoretical framework connecting dual matrix structure to subspace embeddings is well-established.
- **Medium Confidence:** The β-bounds enabling parameter-independent sampling (Mechanism 3). While the derivations follow logically, the practical tightness of these bounds across diverse activation functions remains to be tested.
- **Medium Confidence:** The empirical demonstrations on regression and classification tasks. The experimental setup is detailed, but the sample sizes and model scales are modest compared to modern deep learning applications.

## Next Checks

1. **Activation Function Boundary Test:** Systematically evaluate the β-bounds across a spectrum of activation functions (ReLU, Swish variants, sigmoid, tanh) on synthetic data, measuring the actual vs. theoretical sampling efficiency gap when using parameter-independent scores.

2. **Deep Network Scalability:** Apply the nonlinear importance sampling framework to a multi-layer MLP or small CNN on CIFAR-10, comparing training curves and final accuracy against uniform sampling and linear score baselines.

3. **Adjoint Approximation Error Analysis:** Implement both closed-form and numerical quadrature versions of the adjoint operator, quantify the approximation error introduced by quadrature, and measure its impact on the final training performance and score quality.