---
ver: rpa2
title: 'BM-CL: Bias Mitigation through the lens of Continual Learning'
arxiv_id: '2509.01730'
source_url: https://arxiv.org/abs/2509.01730
tags:
- groups
- learning
- bias
- accuracy
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BM-CL, a novel framework that integrates
  continual learning with bias mitigation to address the leveling-down effect in machine
  learning fairness interventions. The core idea is to reinterpret bias mitigation
  as a form of task-incremental continual learning, allowing models to improve outcomes
  for disadvantaged groups while preserving performance for advantaged groups.
---

# BM-CL: Bias Mitigation through the lens of Continual Learning

## Quick Facts
- **arXiv ID:** 2509.01730
- **Source URL:** https://arxiv.org/abs/2509.01730
- **Reference count:** 40
- **Primary result:** BM-CL framework improves worst-group accuracy while preserving best-group performance by treating bias mitigation as continual learning.

## Executive Summary
BM-CL introduces a novel framework that integrates continual learning techniques with bias mitigation to address the leveling-down effect in machine learning fairness interventions. The core insight is reinterpreting bias mitigation as a form of task-incremental continual learning, where the goal is to improve outcomes for disadvantaged groups while preserving performance for advantaged groups. By incorporating Learning without Forgetting (LwF) and Elastic Weight Consolidation (EWC), BM-CL mitigates catastrophic forgetting that occurs when fairness objectives shift model priorities. Experiments on synthetic and real-world datasets demonstrate that BM-CL consistently improves worst-group accuracy while minimizing the performance trade-offs for best-group accuracy typically observed in conventional bias mitigation techniques.

## Method Summary
BM-CL is a two-stage training pipeline that treats bias mitigation as a continual learning problem. Stage 1 trains a standard ERM model to establish baseline performance and identify disadvantaged groups (G_worst) versus advantaged groups (G_best) using a validation set threshold. Stage 2 fine-tunes the Stage 1 model using a composite loss function that combines a bias mitigation loss (e.g., GroupDRO, ReSample) with a continual learning regularizer (LwF or EWC) weighted by hyperparameter λ. This approach preserves knowledge about advantaged groups while allowing the model to improve performance on disadvantaged groups, effectively mitigating the leveling-down effect where fairness interventions degrade performance on already well-performing groups.

## Key Results
- BM-CL improves worst-group accuracy by 4.6% on average across Waterbirds, CelebA, and CheXpert datasets compared to standard bias mitigation methods
- LwF-augmented methods preserved best-group accuracy to a greater extent than other approaches, effectively balancing fairness and overall accuracy
- The framework achieves consistent performance improvements across different bias mitigation techniques and dataset types

## Why This Works (Mechanism)
The framework works by reframing the fairness intervention problem as a continual learning scenario. When standard bias mitigation techniques are applied, they can cause catastrophic forgetting of knowledge about advantaged groups, leading to the leveling-down effect. By treating the shift from Stage 1 (ERM) to Stage 2 (bias mitigation) as a continual learning problem, BM-CL applies regularization techniques that preserve the model's knowledge about G_best while allowing adaptation to improve G_worst. The hyperparameter λ controls the trade-off between preserving existing knowledge and acquiring new fairness-related knowledge, with appropriate tuning enabling both objectives to be achieved simultaneously.

## Foundational Learning

- **Concept:** Catastrophic Forgetting
  - **Why needed here:** This is the central problem the paper claims to solve. It's the tendency of neural networks to completely lose previously learned information upon learning new data. The paper reinterprets the "leveling-down effect" in fairness as a form of catastrophic forgetting.
  - **Quick check question:** When you fine-tune a model on a new task, does its performance on the original task degrade?

- **Concept:** Stability-Plasticity Dilemma
  - **Why needed here:** This is the core trade-off in continual learning that the framework must navigate. It's the challenge of balancing the retention of old knowledge (stability for the advantaged groups) with the acquisition of new knowledge (plasticity to improve disadvantaged groups).
  - **Quick check question:** How can a model be flexible enough to learn from new data without overwriting what it has already learned?

- **Concept:** Knowledge Distillation
  - **Why needed here:** This is the underlying technique for the Learning without Forgetting (LwF) component of the framework. It involves training a new "student" model to mimic the output probabilities of a previously trained "teacher" model, thereby transferring its knowledge without needing the original data.
  - **Quick check question:** How can you transfer the "knowledge" of one model to another?

## Architecture Onboarding

- **Component map:** ERM Baseline -> Group Identification (G_best/G_worst) -> Fine-tuning with L_BM-CL = L_BM + λ * L_CL
- **Critical path:** The success of the entire framework hinges on the correct identification of G_best and G_worst at the end of Stage 1. If the validation set is not representative or if the initial ERM model's biases are not properly exposed, the partition will be incorrect, and the continual learning regularizer will target the wrong groups. The choice of the pretraining ratio ρ (when to stop Stage 1) is also critical.
- **Design tradeoffs:** The primary tradeoff is controlled by the hyperparameter λ.
  - High λ: Stronger regularization. Prioritizes preserving performance on G_best but may limit the model's ability to improve on G_worst.
  - Low λ: Weaker regularization. Allows the bias mitigation method to more aggressively improve G_worst, but risks a greater leveling-down effect on G_best.
- **Failure signatures:**
  - Incorrect Group Partition: If the validation set is small or biased, G_best and G_worst may be misidentified. The CL regularizer will then work to preserve a "wrong" group.
  - Over-regularization: If λ is too high, the model will be too rigid, and worst-group accuracy will not improve, leading to a model that is both unfair and not better than the baseline.
  - Under-regularization: If λ is too low, the framework behaves like the standard bias mitigation baselines, suffering from significant leveling-down on the best group.
- **First 3 experiments:**
  1. Establish a Baseline: Train a standard ERM model and evaluate its per-group accuracy. Identify G_best and G_worst on a held-out validation set. This is your starting point for the tradeoff.
  2. Hyperparameter Sweep for λ: Fix the bias mitigation method (e.g., GroupDRO) and the pretraining ratio ρ. Run a sweep for the regularization strength λ. Plot both best-group and worst-group accuracy as a function of λ to visualize the tradeoff and find the "sweet spot."
  3. Compare LwF vs. EWC: Using the best λ found, compare the performance of LwF and EWC as the regularizer. LwF may be more computationally efficient as it only requires storing model outputs, while EWC requires computing the Fisher Information Matrix. Note which method provides a better balance for your specific dataset.

## Open Questions the Paper Calls Out
- How does the BM-CL framework perform when integrated with rehearsal-based or architectural continual learning strategies, as opposed to the regularization-based methods (LwF, EWC) tested?
- Is the BM-CL framework robust to noise in the identification of best- and worst-performing groups during the initial ERM stage?
- Does the interpretation of the leveling-down effect as catastrophic forgetting generalize to non-convolutional architectures, such as Vision Transformers (ViTs)?

## Limitations
- The framework's effectiveness depends heavily on the assumption that group identification through balanced accuracy thresholding captures the true "disadvantaged" groups, which may not hold in scenarios with intersectional biases or non-binary protected attributes.
- The computational overhead introduced by the two-stage training pipeline and the need for careful hyperparameter tuning may limit practical deployment.
- The focus on worst-group accuracy as the primary fairness metric may not capture other important fairness notions like equal opportunity or demographic parity.

## Confidence
- **High Confidence:** The core technical contribution of reinterpreting bias mitigation as a continual learning problem is sound and well-supported by the theoretical framework.
- **Medium Confidence:** The empirical results showing consistent worst-group accuracy improvements across three datasets are promising but may overstate real-world applicability.
- **Low Confidence:** The claim that BM-CL "effectively balances fairness and overall accuracy" is somewhat overstated, and the paper's assertion that this represents a "novel" approach may be limited.

## Next Checks
1. Implement a Monte Carlo simulation where the group partitioning step is repeated multiple times with perturbed validation set samples to assess how sensitive the final BM-CL performance is to small changes in G_best/G_worst assignments.
2. Train a single BM-CL model on one dataset (e.g., CelebA) and evaluate it directly on another dataset (e.g., CheXpert) without fine-tuning, to assess whether the learned regularization patterns transfer across different bias structures.
3. Implement a version of BM-CL where the group partitioning is updated periodically during Stage 2 training (rather than only once at the beginning) to track how the model's fairness performance evolves as the underlying group distributions shift.