---
ver: rpa2
title: Hybridising Reinforcement Learning and Heuristics for Hierarchical Directed
  Arc Routing Problems
arxiv_id: '2501.00852'
source_url: https://arxiv.org/abs/2501.00852
tags:
- time
- arcs
- algorithm
- routing
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid algorithm combining Reinforcement
  Learning (RL) and heuristics to solve the Hierarchical Directed Capacitated Arc
  Routing Problem (HDCARP). The algorithm uses RL to guide local search operators,
  significantly reducing computation time while maintaining solution quality.
---

# Hybridising Reinforcement Learning and Heuristics for Hierarchical Directed Arc Routing Problems

## Quick Facts
- **arXiv ID:** 2501.00852
- **Source URL:** https://arxiv.org/abs/2501.00852
- **Reference count:** 40
- **Primary result:** Hybrid RL-heuristic algorithm reduces computation time while maintaining solution quality for HDCARP.

## Executive Summary
This paper introduces HRDA, a hybrid algorithm that combines Reinforcement Learning (RL) and heuristics to solve the Hierarchical Directed Capacitated Arc Routing Problem (HDCARP). The method uses an RL agent to generate initial solutions, which are then refined by local search operators. Computational experiments demonstrate that HRDA significantly reduces computation time compared to standalone RL and heuristic methods while maintaining competitive solution quality, particularly on larger-scale problems.

## Method Summary
The HRDA algorithm uses Proximal Policy Optimization (PPO) to train a policy network that generates feasible arc sequences as initial solutions. An Adjacency Matrix Attention Encoder (AMAE) processes graph features and topology to create embeddings. The initial solution is refined using local search operators (Swap/Shift) to produce an optimized solution. The reward is calculated based on the improved solution, creating a guided feedback loop that accelerates convergence and stability.

## Key Results
- HRDA achieves faster convergence and improved performance compared to standalone RL methods
- The hybrid approach maintains solution quality while significantly reducing computation time
- Performance advantages are particularly notable on larger-scale problem instances

## Why This Works (Mechanism)

### Mechanism 1: Search Space Reduction via Strategic Initialization
The RL agent generates high-quality initial solutions, restricting subsequent local search to promising regions. This warm-start approach reduces deadheading moves and iterations needed to reach local optima. The core assumption is that RL initialization produces solutions closer to optimal local optima than greedy heuristics.

### Mechanism 2: Credit Assignment via Heuristic Refinement
Reward calculation based on the refined solution (after local search) rather than raw RL output improves convergence speed and stability. This guided feedback loop ensures the RL agent learns to generate solutions that are specifically "heuristic-friendly," preventing the policy from getting stuck in poor local optima.

### Mechanism 3: Adjacency-Aware Topology Encoding (AMAE)
The AMAE incorporates the adjacency matrix directly into Multi-Head Attention, explicitly encoding travel times and connectivity. This allows the decoder to respect capacity and temporal constraints during autoregressive generation, capturing spatial relationships more effectively than standard sequence models.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**
  - *Why needed here:* PPO is the specific RL algorithm used to train the policy network. Understanding the clipping mechanism is vital to debug stability issues during training.
  - *Quick check question:* How does the PPO clipping objective prevent the policy from changing too drastically during a single update step?

- **Capacitated Arc Routing Problem (CARP)**
  - *Why needed here:* Unlike standard Vehicle Routing, demand is located on the edges/arcs, not nodes. The hierarchical variant adds priority constraints (Classes).
  - *Quick check question:* What is the specific objective difference between HDCARP-P (Precedence) and HDCARP-U (Flexible)?

- **Autoregressive Decoding**
  - *Why needed here:* The solution is generated as a sequence, with the decoder selecting one arc at a time based on previous choices.
  - *Quick check question:* How does the "Pointer Network" mechanism restrict the output vocabulary to valid graph nodes/arcs?

## Architecture Onboarding

- **Component map:** Graph features + Adjacency Matrix -> AMAE Encoder -> Pointer Network Decoder -> Initial Solution -> Local Search (Swap operators) -> Optimized Solution -> Reward Calculation -> PPO Update

- **Critical path:** Instance Encoding -> Solution Sampling -> Heuristic Refinement (The performance bottleneck) -> Reward Calculation -> Policy Gradient Update

- **Design tradeoffs:** HRDA trades guaranteed optimality bounds of exact