---
ver: rpa2
title: 'Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition'
arxiv_id: '2505.22375'
source_url: https://arxiv.org/abs/2505.22375
tags:
- data
- training
- reasoning
- pangu
- embedded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Pangu Embedded, an efficient LLM reasoner developed
  on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking
  capabilities. Pangu Embedded addresses the significant computational costs and inference
  latency challenges prevalent in existing reasoning-optimized LLMs.
---

# Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition

## Quick Facts
- arXiv ID: 2505.22375
- Source URL: https://arxiv.org/abs/2505.22375
- Reference count: 40
- 7B model outperforms Qwen3-8B and GLM4-9B on AIME 2024, GPQA, and LiveCodeBench while delivering rapid responses

## Executive Summary
Pangu Embedded is an efficient LLM reasoner developed on Ascend Neural Processing Units that addresses the computational costs and inference latency challenges of existing reasoning-optimized LLMs. The system features flexible fast and slow thinking capabilities through a two-stage training framework. Stage 1 employs iterative distillation with model-aware data complexity selection and inter-iteration model merging, followed by reinforcement learning optimized with a latency-tolerant scheduler. Stage 2 introduces a dual-system framework with manual and automatic mode switching for routine and complex queries. Experimental results demonstrate state-of-the-art reasoning quality within a unified model architecture.

## Method Summary
Pangu Embedded is constructed through a two-stage training framework on Ascend NPUs. Stage 1 uses iterative distillation with model-aware data complexity selection and inter-iteration model merging to consolidate knowledge, followed by reinforcement learning guided by a Multi-source Adaptive Reward System (MARS). Stage 2 introduces a dual-system framework that enables both manual mode switching and automatic complexity-aware selection between fast responses for routine queries and deeper slow reasoning for complex problems. The system is trained on 1024 Ascend nodes for policy training and 256 for reference, using a staleness-synchronous parallelism scheduler and distributed prioritized queues.

## Key Results
- 7B Pangu Embedded outperforms Qwen3-8B and GLM4-9B on AIME 2024, GPQA, and LiveCodeBench benchmarks
- Adaptive mode selection maintains 92.2% accuracy on MATH500 while reducing tokens by 11%
- On GSM8K, tokens drop 88% with minimal accuracy loss through automatic fast-slow mode switching

## Why This Works (Mechanism)

### Mechanism 1: Model-aware Data Complexity Selection
- Claim: Aligning training data difficulty with the model's current capability improves reasoning acquisition more than random or static selection.
- Mechanism: The model generates k responses per sample; complexity score C = 1 - (pass_rate). Samples with medium C are selected with higher probability via Gaussian sampling. As iterations progress, the model masters more samples, naturally shifting the distribution.
- Core assumption: Curricula that match the "zone of proximal development" yield better sample efficiency than uniform exposure.
- Evidence anchors: Table 1 shows balanced complexity data achieves 50.42% on AIME vs 45.42% (easy) and 48.75% (hard); Figure 12(b) confirms medium-complexity EXP1 outperforms both extremes.
- Break condition: If model performance plateaus on validation set across iterations, or if complexity scores become uniformly low with no hard samples remaining.

### Mechanism 2: Inter-iteration Model Merging
- Claim: Merging checkpoints across distillation iterations consolidates complementary knowledge and mitigates catastrophic forgetting.
- Mechanism: After each SFT iteration, compute average parametric delta from N checkpoints relative to previous merged model, then apply weighted update: Θ_merged = Θ_prev + λ × δ̄.
- Core assumption: Parameter deltas from different data distributions encode partially independent capabilities that can be linearly combined.
- Evidence anchors: Equation 4 formalizes the delta-based merging strategy; Table 6 shows merging improves AIME 57.25→54.58, LiveCodeBench 44.48→40.44, GPQA 53.53→52.53.
- Break condition: If merged model underperforms the best individual checkpoint consistently, or if validation metrics oscillate.

### Mechanism 3: Adaptive Fast-Slow Mode Selection
- Claim: A single model can learn to autonomously select reasoning depth based on query complexity, maintaining accuracy while reducing token usage.
- Mechanism: Fine-tune on a fused dataset D_fusion containing both fast-mode samples (direct answers) and slow-mode samples (CoT with ồ...ọ tags). The model implicitly learns to generate the ồ...ọ block only for "hard" queries.
- Core assumption: Query complexity can be reliably estimated by LLM annotation and correlates with optimal reasoning depth.
- Evidence anchors: Complexity threshold Cc≤2, Tc≤2 defines "easy" vs "hard"; Table 4 shows adaptive mode maintains 92.2% accuracy on MATH500 while reducing tokens 11%.
- Break condition: If accuracy degrades >2-3% on hard benchmarks, or if token savings disappear, the complexity classifier may be misaligned.

## Foundational Learning

- **Curriculum Learning / Self-Paced Learning**
  - Why needed here: Model-aware data selection requires understanding how to dynamically adjust training data difficulty based on model capability.
  - Quick check question: Can you explain why training on uniformly hard data might underperform compared to a mixed curriculum?

- **Knowledge Distillation for Reasoning**
  - Why needed here: Stage 1 transfers reasoning from a teacher model; understanding what transfers (patterns vs. surface form) is critical.
  - Quick check question: Why might shorter correct reasoning paths be preferred over longer ones during distillation?

- **Reinforcement Learning from Verifiable Rewards**
  - Why needed here: MARS uses correctness rewards (math/code) and preference rewards; understanding reward design tradeoffs is essential.
  - Quick check question: What happens to GRPO if all responses for a prompt receive identical rewards?

## Architecture Onboarding

- Component map: Data pool → Prior filtering → Diversity maintenance (ZIP) → Iterative SFT → Repetition self-repair → RL with MARS → Fusion training for dual modes
- Critical path: Data pool quality → Model-aware selection → Iterative SFT with merging → RL with MARS → Fusion training for dual modes. Errors in early stages compound; validate complexity scoring before full runs.
- Design tradeoffs:
  - μ in selection probability: Lower μ (easier bias) improves stability but may limit ceiling; paper uses μ slightly <0.5.
  - Merging weight λ: Higher λ accelerates new learning but risks forgetting; paper doesn't specify values.
  - Easy:medium:hard ratio for RL curriculum: Paper uses 1:7:2 to ensure non-zero advantages.
- Failure signatures:
  - Repetition loops in long CoT → Check self-repair activation thresholds.
  - RL reward collapse → Verify MARS routing; check for zero-advantage masking.
  - Mode confusion in dual-system → Inspect D_fusion quality; ensure clear separation of fast/slow exemplars.
- First 3 experiments:
  1. Validate complexity scoring: On a held-out subset, verify that C(x,y; G_{t-1}) correlates with human-judged difficulty. If correlation is weak, annotation pipeline needs revision.
  2. Ablate model merging: Run 3 iterations with and without merging on AIME/LiveCodeBench. Confirm gains in Table 6 are reproducible.
  3. Test adaptive switching boundary: Evaluate on datasets with known difficulty distributions. Verify that slow-mode activation rates match Figure 11 patterns (higher for harder problems).

## Open Questions the Paper Calls Out
None

## Limitations

- Generalizability across domains: MARS reward system is explicitly tailored to mathematics, coding, and general problem-solving with no evidence for non-technical domains.
- Computational overhead of dual-system training: The adaptive switching mechanism requires fine-tuning on a fused dataset with manually annotated complexity labels, but computational cost is unreported.
- Dependence on specialized hardware: All results rely on Ascend NPUs with performance on other accelerators unreported.

## Confidence

- **High confidence**: Performance claims on AIME 2024, GPQA, and LiveCodeBench benchmarks
- **Medium confidence**: Model-aware Data Complexity Selection mechanism
- **Medium confidence**: Inter-iteration Model Merging mechanism
- **Medium confidence**: Adaptive Fast-Slow Mode Selection mechanism
- **Low confidence**: Hardware-specific efficiency claims

## Next Checks

1. Validate complexity scoring stability: On a held-out subset, compute the complexity score C(x,y; G_{t-1}) across 3 consecutive model checkpoints. Verify score distribution stability and ranking consistency.

2. Ablate model merging contribution: Run 4 iterations of Stage 1 training with identical hyperparameters except for the inter-iteration merging step. Compare final AIME 2024 accuracy and LiveCodeBench scores between merged and non-merged runs.

3. Test adaptive switching boundary sensitivity: Evaluate the dual-system model on MATH500 and GSM8K with varying complexity thresholds (Cc≤1, Cc≤2, Cc≤3). Plot accuracy vs. token reduction curves to identify optimal boundaries.