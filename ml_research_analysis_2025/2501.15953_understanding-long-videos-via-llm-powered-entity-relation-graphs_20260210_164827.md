---
ver: rpa2
title: Understanding Long Videos via LLM-Powered Entity Relation Graphs
arxiv_id: '2501.15953'
source_url: https://arxiv.org/abs/2501.15953
tags:
- video
- understanding
- graph
- entity
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphVideoAgent addresses long-form video understanding by introducing
  a dynamic entity relation graph that tracks visual elements and their evolving relationships
  across time. The system integrates a large language model agent with graph-structured
  memory to iteratively reason about video content and select key frames.
---

# Understanding Long Videos via LLM-Powered Entity Relation Graphs

## Quick Facts
- arXiv ID: 2501.15953
- Source URL: https://arxiv.org/abs/2501.15953
- Authors: Meng Chu; Yicong Li; Tat-Seng Chua
- Reference count: 40
- Key outcome: GraphVideoAgent achieves 56.3% accuracy on EgoSchema and 73.3% on NExT-QA, requiring only 8.2 and 8.1 frames per video on average

## Executive Summary
GraphVideoAgent addresses long-form video understanding by introducing a dynamic entity relation graph that tracks visual elements and their evolving relationships across time. The system integrates a large language model agent with graph-structured memory to iteratively reason about video content and select key frames. By explicitly modeling entity interactions through a multi-relational graph with temporal state tracking, the approach overcomes limitations of sequential frame processing. The method achieves state-of-the-art performance on two benchmarks while maintaining remarkable efficiency.

## Method Summary
GraphVideoAgent builds a dynamic entity relation graph where nodes represent tracked entities with attributes including frame indices, visual features, captions, and state change sequences. Edges encode spatial, interaction, and action relations extracted from captions. The system processes videos through uniform sampling, builds an initial graph, then uses an LLM agent to predict answers with confidence scores. If confidence falls below threshold τ=3, the agent retrieves additional frames using a weighted scoring function combining graph relevance, visual similarity, and temporal coherence. This iterative process continues until confidence threshold is met or maximum iterations reached.

## Key Results
- Achieves 56.3% accuracy on EgoSchema benchmark (2.2% improvement over existing methods)
- Achieves 73.3% accuracy on NExT-QA benchmark (2.0% improvement over existing methods)
- Requires only 8.2 and 8.1 frames per video on average, demonstrating remarkable efficiency

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Entity Relation Graph with Temporal State Tracking
Structured graph representation enables coherent tracking of entities and their evolving relations across time, overcoming limitations of sequential memory that struggles when objects temporarily vanish and reappear. Graph G = (V, E) where entity nodes store tuples (frame indices, visual features, captions, state change sequences) and edges represent three relation types—spatial, interaction, and action. Temporal coherence computed as T(e,f) = α·S(e,f) + (1-α)·R(e,f), balancing entity state consistency with relation persistence. Break condition: If entity extraction from captions is unreliable or key video content cannot be represented as entity relations.

### Mechanism 2: Iterative LLM Agent with Confidence-Based Self-Reflection
Multi-round reasoning with explicit confidence assessment enables targeted information gathering, avoiding both premature answering and unnecessary frame processing. Three-stage pipeline—(1) uniform sampling with initial answer, (2) if confidence < τ=3, graph-guided retrieval, (3) if still insufficient, expanded context search. LLM uses chain-of-thought prompting to generate predictions and self-reflection to assess confidence on three-level scale. Break condition: If LLM self-reflection is poorly calibrated or correct answer requires information outside graph-reachable frames.

### Mechanism 3: Graph-Guided Frame Scoring for Efficient Retrieval
Weighted combination of graph relevance, visual similarity, and temporal coherence identifies answer-critical frames more efficiently than uniform sampling. Frame scoring S(f) = 0.5·s_graph + 0.3·s_visual + 0.2·s_temporal (normalized [0,1] scores). Graph score prioritizes frames containing entities/relations relevant to the question. Break condition: If optimal answer frames don't contain tracked entities or if weighting doesn't generalize across video domains.

## Foundational Learning

- Concept: Scene Graph Generation
  - Why needed here: The paper extends scene graph concepts from static images to temporal video sequences; understanding object-relation graphs is prerequisite.
  - Quick check question: Given an image caption "A person holding a cup sits on a chair near a table," can you sketch the corresponding scene graph with nodes and edge types?

- Concept: LLM Chain-of-Thought Prompting
  - Why needed here: The LLM agent uses CoT for multi-round reasoning and self-reflection; understanding prompting strategies is essential.
  - Quick check question: How does chain-of-thought prompting differ from standard few-shot prompting for multi-step reasoning tasks?

- Concept: Video Temporal Segmentation and Sampling
  - Why needed here: The paper's efficiency gains come from intelligent frame selection versus uniform sampling; understanding sampling tradeoffs contextualizes the contribution.
  - Quick check question: For a 5-minute video, what are the tradeoffs between uniform sampling (1 fps), keyframe detection, and content-based sampling?

## Architecture Onboarding

- Component map: Video → Uniform sample N frames → VLM generate captions → Extract entities/relations → Build initial graph G₀ → LLM predict answer with confidence → If confidence < 3: Score frames via S(f) → Retrieve top frames → Update graph → Re-evaluate → (loop until confidence ≥ 3 or max iterations) → Output answer

- Critical path: The agent processes videos through uniform sampling, graph construction from captions, LLM reasoning with confidence assessment, and iterative retrieval guided by the graph structure.

- Design tradeoffs:
  - Graph granularity vs. construction cost: More entity types and relation types improve accuracy but increase NLP overhead and graph complexity
  - Retrieval stages vs. latency: Paper uses max 3 stages × 3 frames = 9 additional frames; more stages improve hard questions but increase response time
  - LLM choice vs. cost/performance: GPT-4 (62.7%) substantially outperforms Llama3-70B (50.1%) but with higher API cost

- Failure signatures:
  - Sparse graphs: Captions lack entity mentions → graph has few nodes → retrieval degrades to visual-only
  - Confidence miscalibration: Agent outputs confidence=3 on incorrect answers → early stopping with wrong answer
  - Entity tracking breaks: Same entity labeled differently across frames (e.g., "person," "man," "he") → fragmented graph

- First 3 experiments:
  1. Replicate the graph component ablation: Run full model, then remove entity relations, temporal tracking, and multi-dimension structure separately to verify contribution magnitude
  2. Entity scaling analysis: Create test subsets with 2-3, 4-6, and 7+ entities to measure performance degradation and compare graceful scaling vs. baseline
  3. Confidence threshold sweep: Test τ ∈ {2, 2.5, 3, 3.5} to validate whether τ=3 is optimal or task-dependent

## Open Questions the Paper Calls Out

### Open Question 1
Can the graph structure be enhanced to capture more complex entity relations and integrate multi-modal information beyond textual captions? The conclusion states the graph structure could be enhanced to capture more complex entity relations and multi-modal information. This remains unresolved as the current implementation relies primarily on VLM-generated captions and NLP tools. Evidence would require a modified architecture that ingests audio or raw visual features directly into the graph, showing improved performance on multi-modal benchmarks.

### Open Question 2
How can the graph construction mechanism be optimized to enable real-time processing for long video streams? The authors suggest developing more efficient graph construction mechanisms that enable real-time applications as a future direction. This remains unresolved as the current system relies on an iterative LLM-powered reasoning loop which is computationally expensive and slow. Evidence would require a latency analysis showing the system processing video at a speed comparable to or faster than the video frame rate.

### Open Question 3
How can the framework's dependency on proprietary large language models (e.g., GPT-4) be reduced while maintaining high reasoning accuracy? Table 4 shows a drastic performance drop when replacing GPT-4 with open-source models like Mistral or Llama3. This remains unresolved as the complex reasoning and self-reflection capabilities required by the agent seem tightly coupled with the specific capabilities of GPT-4. Evidence would require fine-tuning open-source models or simplifying the reasoning prompts to achieve comparable results without proprietary APIs.

## Limitations

- The LLM self-reflection accuracy is not independently validated, creating uncertainty about confidence-based retrieval effectiveness
- The approach assumes entity relations extracted from captions sufficiently represent video content, potentially missing non-linguistic information
- The 50/30/20 frame scoring weights appear heuristic rather than empirically optimized across diverse video domains

## Confidence

- **High confidence**: The core mechanism of using a dynamic entity relation graph with temporal state tracking is well-specified and the 2-3% absolute accuracy improvements over baselines are statistically meaningful
- **Medium confidence**: The efficiency claims (8.2-8.1 frames vs 32-256 for baselines) are supported but depend on assumptions about baseline processing
- **Low confidence**: The LLM self-reflection calibration and the optimality of the 50/30/20 frame scoring weights are not rigorously validated

## Next Checks

1. **Self-reflection calibration test**: Run the agent on a validation set with known answers and measure the correlation between confidence scores and actual correctness to verify if τ=3 threshold is properly calibrated

2. **Frame scoring weight optimization**: Perform an ablation study varying the 0.5/0.3/0.2 weights across different video categories to determine if the current weighting is optimal or task-dependent

3. **Graph sparsity stress test**: Create synthetic videos with minimal entity presence (e.g., abstract patterns, ambient scenes) to measure performance degradation and validate the claim that entity relations are essential for the approach