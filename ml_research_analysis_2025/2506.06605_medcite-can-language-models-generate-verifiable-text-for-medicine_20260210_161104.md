---
ver: rpa2
title: 'MedCite: Can Language Models Generate Verifiable Text for Medicine?'
arxiv_id: '2506.06605'
source_url: https://arxiv.org/abs/2506.06605
tags:
- citation
- medical
- citations
- generation
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedCite, the first end-to-end framework for
  citation generation in medical question-answering systems. It addresses the lack
  of verifiability in existing LLM-based medical QA by enabling accurate, evidence-backed
  citations.
---

# MedCite: Can Language Models Generate Verifiable Text for Medicine?

## Quick Facts
- arXiv ID: 2506.06605
- Source URL: https://arxiv.org/abs/2506.06605
- Reference count: 40
- Primary result: MedCite achieves citation precision of 71.74% and recall of 47.79% on BioASQ, outperforming baselines by up to 31.61% in precision and 47.39% in recall

## Executive Summary
MedCite introduces the first end-to-end framework for citation generation in medical question-answering systems. It addresses the lack of verifiability in existing LLM-based medical QA by enabling accurate, evidence-backed citations through a multi-pass retrieval-citation method. The framework combines retrieval-augmented generation with post-generation refinement to improve both answer correctness and citation quality. By leveraging non-parametric citation from trusted corpora like PubMed, MedCite reduces hallucination and ensures verifiability, achieving state-of-the-art performance on the BioASQ dataset.

## Method Summary
MedCite employs a multi-pass retrieval-citation framework that first uses RAG to generate an initial answer with citations from retrieved PubMed documents, then re-retrieves documents for each statement to refine citations through deduplication and LLM reranking. The hierarchical retrieval combines BM25 for lexical matching with MedCPT for semantic reranking, balancing precision and recall. Citation evaluation uses statement-level recall (all information supported) and precision (each citation individually contributes), assessed by an attribution judge LLM.

## Key Results
- Citation precision of 71.74% and recall of 47.79% on BioASQ, outperforming strong baselines by up to 31.61% in precision and 47.39% in recall
- Double-pass method achieves 65.69% recall vs. 49.01% for RAG-only on BioASQ with Llama-3-8B-Instruct
- Hierarchical BM25+MedCPT retrieval achieves 80.02% precision vs. 66.78% for semantic-only in Oracle setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-pass citation generation improves both recall and precision over single-pass methods by combining generative and retrieval-based approaches.
- Mechanism: First pass uses RAG to generate an initial answer with citations from retrieved documents; second pass re-retrieves documents for each statement and deduplicates citations, mitigating hallucinations from the generative phase while preserving context-grounded claims.
- Core assumption: LLMs can produce useful initial citations when grounded in retrieved documents, even if incomplete or imprecise.
- Evidence anchors:
  - [abstract]: "proposed multi-pass retrieval-citation method combines retrieval-augmented generation with post-generation refinement to improve both answer correctness and citation quality"
  - [section]: Table 4 shows double-pass achieves 65.69% recall vs. 49.01% for RAG-only (Llama-3-8B-Instruct on BioASQ)
  - [corpus]: Neighbor paper "Generation-Time vs. Post-hoc Citation" explores the same tradeoff space, suggesting active research interest but no definitive consensus on optimal timing
- Break condition: If the initial RAG retrieval is severely mismatched to the query, second-pass re-retrieval cannot recover; also, if the LLM ignores retrieved context, first-pass citations degrade.

### Mechanism 2
- Claim: Non-parametric citation from trusted corpora (e.g., PubMed) outperforms parametric self-citation for open-source LLMs due to reduced fabrication and verifiability.
- Mechanism: Instead of prompting LLMs to generate citations from parametric memory, retrieve documents from an external, curated database and assign them post-hoc via dense retrieval (MedCPT) with LLM reranking.
- Core assumption: The external corpus contains documents that actually support the generated statements, and the retriever can surface them.
- Evidence anchors:
  - [section]: Table 1 shows Llama-3-8B-Instruct and UltraMedical fail to follow parametric citation instructions correctly; GPT-4o generates well-formatted but outdated references (pre-2018)
  - [section]: "Without API access to the content of any scientific articles generated as citations by parametric methods, it is challenging to automatically evaluate their quality"
  - [corpus]: No direct corpus evidence on parametric vs. non-parametric for medicine; this appears underexplored beyond this paper
- Break condition: If the external corpus lacks coverage for emerging treatments or niche topics, non-parametric methods will fail to find supporting citations regardless of retrieval quality.

### Mechanism 3
- Claim: Hierarchical two-stage retrieval (lexical-first, then semantic reranking) improves citation precision by prioritizing exact term matching before semantic similarity.
- Mechanism: BM25 retrieves a broad candidate pool based on keyword overlap; MedCPT cross-encoder reranks by semantic relevance, balancing precision (exact medical terminology) with recall (conceptual similarity).
- Core assumption: Medical claims often require verbatim terminology matching (drug names, genomic markers) that pure semantic similarity may miss.
- Evidence anchors:
  - [section]: Table 6 shows hierarchical BM25+MedCPT achieves 80.02% precision vs. 66.78% for semantic-only (Oracle setting)
  - [section]: "citation retrieval requires examination of precise medical terminology and quoting verbatim from the source"
  - [corpus]: Weak corpus signal; related papers focus on RAG broadly without isolating retrieval architecture effects on citation precision
- Break condition: If queries use highly paraphrased lay terminology (e.g., patient descriptions), lexical-first retrieval may return empty or irrelevant results.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: MedCite builds on RAG as the foundation for grounding answers before citation; understanding RAG's retrieve-then-generate loop is essential.
  - Quick check question: Can you explain why RAG improves answer correctness but, per the paper, can initially *degrade* citation quality compared to non-RAG baselines?

- Concept: Citation/Attribution Evaluation (Precision vs. Recall)
  - Why needed here: The paper introduces statement-level citation recall (all information supported) and citation precision (each citation individually contributes), which differ from standard retrieval metrics.
  - Quick check question: For a statement with three citations where only one partially supports the claim, what are the recall and precision scores?

- Concept: Dense vs. Sparse Retrieval (MedCPT vs. BM25)
  - Why needed here: The hierarchical retriever combines both; understanding when lexical matching outperforms semantic similarity in medicine is critical for architecture decisions.
  - Quick check question: Why might a semantic retriever return a document about "calitoxin" when the claim mentions "chlorotoxin," and how does BM25 avoid this?

## Architecture Onboarding

- Component map: Query → Pre-gen retrieval (top-32 docs) → RAG generation (answer + initial citations) → Statement segmentation → Per-statement re-retrieval (top-3) → LLM reranking + deduplication → Final cited answer → Optional: Attribution evaluation

- Critical path: Query → Pre-gen retrieval (top-32 docs) → RAG generation (answer + initial citations) → Statement segmentation → Per-statement re-retrieval (top-3) → LLM reranking + deduplication → Final cited answer → Optional: Attribution evaluation

- Design tradeoffs:
  - **RAG vs. PGC-only**: RAG improves answer correctness (82.85% vs. 71.36% accuracy) but initially lowers citation quality; double-pass recovers citation metrics
  - **LLM vs. NLI reranking**: NLI is 5.8% lower in precision but much cheaper; choose based on cost/latency constraints
  - **Retriever choice**: Lexical-only (BM25) highest citation precision; semantic-only best for paraphrased queries; hierarchical balances both

- Failure signatures:
  - **Parametric citation mode**: Open-source LLMs produce ill-formatted or fabricated references; GPT-4o produces outdated references (pre-2018)
  - **Semantic-only retrieval**: Returns conceptually similar but terminologically incorrect documents (e.g., calitoxin vs. chlorotoxin)
  - **UltraMedical with citation prompts**: Accuracy drops significantly (74.92% → 63.43%) due to context-length confusion

- First 3 experiments:
  1. **Reproduce Table 4 (double-pass vs. single-pass)** on BioASQ with Llama-3-8B-Instruct to validate citation recall/precision gains; vary top-k (3, 5, 10) to test sensitivity.
  2. **Ablate retriever architecture** by comparing lexical-only, semantic-only, and hierarchical retrievers on a subset of BioASQ; measure citation precision to confirm Table 6 findings.
  3. **Test attribution judge correlation** by running Llama-3.1-8B-Instruct and GPT-4o as judges on 50 sampled statements; compare agreement with human annotations to validate Table 7 claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a high level of consensus be achieved among medical experts regarding the extent to which a document partially supports a statement?
- Basis in paper: [explicit] Section 8 notes that "whether a document supports a statement can be subject to interpretation," making it crucial to assess if experts can agree on partial support.
- Why unresolved: Medical fact interpretation is inherently subjective, and defining "partial support" lacks a standardized rubric, leading to potential variability in ground truth data.
- What evidence would resolve it: Large-scale annotation studies measuring inter-annotator agreement (e.g., Cohen’s Kappa) specifically for "partially supported" labels across diverse medical topics.

### Open Question 2
- Question: How should citation generation systems effectively manage scenarios where retrieved citations disprove claims or where corpus updates refute prior studies?
- Basis in paper: [explicit] Section 8 states it is "worth exploring the reasons behind instances where citations disprove claims," particularly regarding corpus updates and refutations.
- Why unresolved: Current frameworks focus primarily on finding supporting evidence rather than detecting contradictions or handling the temporal validity of medical knowledge.
- What evidence would resolve it: Development and evaluation of modules capable of flagging contradictory evidence or prioritizing recent publications that invalidate older citations.

### Open Question 3
- Question: Can the MedCite framework be adapted to general domains that lack centralized, well-curated corpora like PubMed?
- Basis in paper: [explicit] Section 8 highlights that "generalization to other fields presents notable challenges" due to the unique availability of resources like PubMed in the medical field.
- Why unresolved: General domains often lack a single trusted database and uniform retrieval objectives, requiring significant modifications to the hierarchical retrieval strategy.
- What evidence would resolve it: Successful application of the multi-pass retrieval-citation method on general-domain benchmarks (e.g., open-domain web QA) without performance degradation.

## Limitations
- Framework's effectiveness depends heavily on PubMed corpus coverage, which may be insufficient for emerging treatments or niche specialties
- Attribution judge relies on general-domain LLMs rather than medical-specialized models, though the latter show poor correlation with human judgments
- Limited exploration of performance degradation when queries use highly paraphrased lay terminology

## Confidence

- **High confidence**: The multi-pass retrieval-citation method demonstrably improves citation precision and recall over single-pass approaches, with clear evidence from Table 4 showing 65.69% vs. 49.01% recall for double-pass vs. RAG-only.
- **Medium confidence**: The claim that non-parametric citation outperforms parametric self-citation for open-source LLMs is well-supported by Table 1's evidence of fabrication issues, though direct comparative corpus evidence is limited.
- **Medium confidence**: The hierarchical retrieval architecture's superiority for citation precision is shown in Table 6 (80.02% vs. 66.78%), but the paper provides limited corpus evidence on why lexical-first retrieval specifically matters for medical terminology.

## Next Checks

1. **Test corpus coverage limits**: Evaluate MedCite on a dataset of medical questions about emerging treatments or niche specialties to quantify performance degradation when PubMed lacks relevant documents.
2. **Ablate attribution judge models**: Compare the correlation of medical-specialized NLI models (SciFive-MedNLI, JSL-MedPhi2) against the recommended general-domain LLMs on 100 sampled statements to confirm the paper's findings about poor medical model performance.
3. **Test paraphrased query handling**: Evaluate hierarchical vs. semantic-only retrieval on a test set of patient-descriptions or lay terminology queries to measure the claimed advantage of lexical-first matching for medical terminology precision.