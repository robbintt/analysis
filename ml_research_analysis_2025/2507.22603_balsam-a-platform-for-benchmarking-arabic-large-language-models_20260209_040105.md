---
ver: rpa2
title: 'BALSAM: A Platform for Benchmarking Arabic Large Language Models'
arxiv_id: '2507.22603'
source_url: https://arxiv.org/abs/2507.22603
tags:
- arabic
- evaluation
- text
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BALSAM is a community-driven benchmark platform designed to evaluate
  Arabic large language models (LLMs) across 78 tasks in 14 categories, totaling 52K
  examples. It addresses key gaps in existing Arabic benchmarks by providing blind
  test sets, diverse task coverage, and a centralized evaluation platform with an
  Arabic LLM Leaderboard.
---

# BALSAM: A Platform for Benchmarking Arabic Large Language Models

## Quick Facts
- **arXiv ID**: 2507.22603
- **Source URL**: https://arxiv.org/abs/2507.22603
- **Reference count**: 40
- **Primary result**: Introduces a blind-test benchmark platform for Arabic LLMs with 78 tasks, 52K examples, and a leaderboard using LLM-as-a-judge.

## Executive Summary
BALSAM is a community-driven benchmark platform designed to evaluate Arabic large language models (LLMs) across 78 tasks in 14 categories, totaling 52K examples. It addresses key gaps in existing Arabic benchmarks by providing blind test sets, diverse task coverage, and a centralized evaluation platform with an Arabic LLM Leaderboard. The benchmark emphasizes quality assurance, mitigates data leakage risks, and uses a combination of automatic metrics and human evaluation to ensure reliable assessments. Experiments revealed that standard metrics like BLEU and ROUGE poorly correlate with human judgments, especially due to Arabic morphology and verbosity differences. In contrast, LLM-as-a-judge demonstrated strong correlation with human evaluations, becoming the preferred evaluation method. The platform reveals that large closed models like GPT-4o and DeepSeek V3 significantly outperform Arabic-centric models, though model size alone is not a predictor of success. BALSAM aims to unify the Arabic NLP community around shared standards and datasets, fostering collaborative research to advance Arabic LLM capabilities. Future work will expand task coverage, improve data quality, and explore additional evaluation dimensions.

## Method Summary
BALSAM evaluates Arabic LLMs using a centralized platform that conducts evaluations via API calls to registered models or by running open-weights models in a controlled environment. The benchmark uses blind test sets to mitigate data contamination risks. Evaluation is performed using a customized version of LM-Evaluation-Harness, with primary scoring done by an LLM-as-a-judge (Gemini 2.5 Flash) using a 0-3 rubric. The platform handles 52K examples across 78 tasks, with some sequence labeling tasks converted to text generation or MCQ formats for evaluation compatibility. Scores are computed as macro-averages across tasks and categories.

## Key Results
- Standard metrics (BLEU, ROUGE) show poor correlation with human judgments due to Arabic morphological complexity and verbosity differences.
- LLM-as-a-judge demonstrates strong correlation with human evaluations and becomes the preferred evaluation method.
- Large closed models like GPT-4o and DeepSeek V3 significantly outperform Arabic-centric models, though model size alone is not a predictor of success.
- The platform reveals that model size is not a sufficient predictor of success in Arabic evaluation, with some smaller models outperforming larger ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-as-a-judge provides a more reliable evaluation of Arabic LLM outputs than traditional n-gram metrics.
- Mechanism: A strong LLM (like Gemini 2.5 Flash) is used to extract the core answer from a model's output, removing verbosity, and then to score that extracted answer against a ground truth using a rubric (0-3 scale). This two-step process mitigates issues with verbosity and morphological complexity that plague surface-level n-gram matching.
- Core assumption: The LLM used as a judge possesses sufficient linguistic and reasoning capability to understand the prompt, output, and ground truth in Arabic, and its judgments align with human expert preferences.
- Evidence anchors:
  - [abstract] "In contrast, LLM-as-a-judge demonstrated strong correlation with human evaluations, becoming the preferred evaluation method."
  - [PAGE 6, Section: Human-to-Automatic Measure Correlation] "We can see very poor correlation between manual judgments and automatic measures."
  - [PAGE 6, Section: Beyond BLEU and ROUGE] "LLM as a judge was highly correlated with human judgments for all categories... it correlated better with the average of judges' scores than judges correlated with each other."
  - [corpus] Corpus neighbors confirm the broader trend of using LLMs for evaluation, as seen in "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP," but specific evidence for LLM-as-a-judge correlation is strong in the paper itself.
- Break condition: The correlation between the LLM judge and human evaluators drops significantly, potentially due to task types (e.g., complex reasoning) or biases (e.g., self-enhancement) not mitigated by the extraction step. The paper notes LLM-as-a-judge may struggle with assessing reasoning and math tasks.

### Mechanism 2
- Claim: A centralized, blind-test benchmark platform mitigates data contamination and fosters fairer model comparison.
- Mechanism: The BALSAM platform holds back test sets from public release. Evaluation is conducted by the platform via API calls to registered models or by running open-weights models in a controlled environment. This prevents model developers from training on the test data.
- Core assumption: Model developers cannot access the private test set, and the platform's evaluation is impartial.
- Evidence anchors:
  - [abstract] "It addresses key gaps... by providing blind test sets... and mitigates data leakage risks."
  - [PAGE 3, Section: 2.4 Challenges and Gaps] "...static, publicly available datasets... heighten the risk of test data contamination... Hence, leaderboards with rigorous contamination checks... are needed."
  - [PAGE 4, Section: 3.3 Mitigating Data Leakage] "...we restricted the access to the test sets to a small group of individuals responsible for quality assessment and platform development..."
  - [corpus] Corpus evidence related to data contamination in LLMs is present in the related work (Deng et al., 2023; Dong et al., 2024) cited within the paper.
- Break condition: The test set is leaked to the public or to model development teams, allowing them to train or fine-tune on it. The platform's security is compromised.

### Mechanism 3
- Claim: Converting sequence labeling tasks to text generation or MCQ formats enables broader evaluation with existing frameworks.
- Mechanism: Tasks like Named Entity Recognition (NER) and Part-of-Speech (POS) tagging, which are traditionally sequence labeling, are reformulated. For example, NER becomes a text generation task where the model must output the entities. This allows the use of text-generation-focused evaluation frameworks (like LM-Harness) and metrics.
- Core assumption: The conversion process doesn't degrade the core assessment of the model's capability. The resulting task is a valid proxy for the original NLP task.
- Evidence anchors:
  - [PAGE 3, Section: 3.1 Dataset Creation] "Note that we converted to MCQ or text generation some tasks, such as Part-of-Speech (POS) tagging and Named Entity Recognition (NER)... The aim was to ease evaluation as we currently cannot handle sequence labeling tasks..."
  - [PAGE 5, Section: 4.2 Evaluation Framework] "We adopted the LM-Evaluation-Harness... framework... for several reasons: (i) it supports evaluation of both open-source LLMs... as well as commercial LLMs..."
  - [corpus] No direct corpus evidence supports or refutes this specific conversion strategy in other papers.
- Break condition: The reformulated task proves to be significantly easier or harder than the original sequence labeling task, or it introduces artifacts that allow models to solve it without demonstrating the underlying linguistic understanding. Future work showing the platform's support for native sequence tagging would supersede this mechanism.

## Foundational Learning

- Concept: **Arabic Morphological Complexity.**
  - Why needed here: Arabic's rich morphology (e.g., prefixes, suffixes, templatic patterns) makes exact string matching unreliable. A model might produce a semantically correct but morphologically different word (e.g., with a definite article), causing n-gram metrics to score it as zero. Understanding this is key to seeing why BLEU/ROUGE fail and why LLM-as-a-judge is used.
  - Quick check question: Why would a model that produces a semantically correct answer score zero on a BLEU metric for Arabic?

- Concept: **LLM-as-a-Judge and its Biases.**
  - Why needed here: This is the core evaluation methodology proposed by the paper. Understanding its strengths (correlation with humans) and weaknesses (struggles with reasoning/math, potential for verbosity bias) is critical for interpreting the BALSAM leaderboard.
  - Quick check question: What are two main limitations of using an LLM as an automatic judge?

- Concept: **Data Contamination in LLM Benchmarks.**
  - Why needed here: This is the central problem BALSAM's blind test sets and platform are designed to solve. A foundational understanding of how public test sets can be compromised is necessary to appreciate the platform's design.
  - Quick check question: How does a centralized platform with a private test set directly address the problem of data contamination?

## Architecture Onboarding

- Component map: The user-facing **BALSAM Leaderboard** displays results. This is fed by the **BALSAM Evaluation Platform**, which manages evaluation jobs. The core engine is a customized version of **LM-Evaluation-Harness**, which preprocesses data (from the **BALSAM Dataset** of 52K examples) into YAML format, runs inference against a registered **Model API** (OpenAI-compatible or aiXplain), and calculates scores. **Gemini 2.5 Flash** is used as the LLM-judge for Phase 1 scoring.

- Critical path: A new model is registered via its API endpoint -> The evaluation is triggered on the platform for a set of tasks -> LM-Harness runs the model on the blind test data -> The raw output is processed by the LLM-judge (answer extraction + scoring) -> Scores are aggregated (macro-averages) -> Results are posted to the leaderboard.

- Design tradeoffs: The key tradeoff was between the reliability of human evaluation and the scalability of automatic metrics. This was resolved by adopting LLM-as-a-judge, which offers a balance. Another tradeoff was task coverage vs. evaluation simplicity, resolved by converting sequence tasks to text generation (a temporary measure).

- Failure signatures:
  - **Metric-Model Mismatch:** A model with verbose outputs scores poorly on BLEU/ROUGE but well with an LLM-judge.
  - **Task Reformulation Artifacts:** A model solves a converted NER task without true named entity understanding.
  - **Judge Bias:** The LLM-judge favors a particular response style or reasoning pattern not aligned with human judgments for a specific task.

- First 3 experiments:
  1. **Baseline your model on BALSAM:** Register your model's API and run the full evaluation to get initial scores across the 78 tasks.
  2. **Ablate the evaluation metric:** For a subset of tasks (e.g., Creative Writing), compare your model's scores using both ROUGE and the LLM-judge to quantify the metric bias.
  3. **Probe a specific failure category:** Identify a low-scoring category (e.g., Logic) for your model, manually inspect a sample of failures, and compare the model's raw output with the judge's extracted answer to diagnose the issue.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors (e.g., Arabic tokenization, training data composition) allow smaller models to outperform significantly larger ones in Arabic evaluation?
- Basis in paper: [inferred] Section 5.2 notes that model size is not a sufficient predictor of success, citing that the 132B parameter DBRX-instruct performed poorly compared to smaller models.
- Why unresolved: The paper identifies the phenomenon but does not isolate the causal variables, suggesting tokenization and Arabic-centric SFT as likely factors without testing them.
- What evidence would resolve it: Ablation studies controlling for tokenizer efficiency and the ratio of Arabic instruction-tuning data across different model scales.

### Open Question 2
- Question: How can the benchmark be expanded to effectively evaluate dynamic capabilities such as multi-turn dialogue, tool usage, and hallucination?
- Basis in paper: [explicit] The Limitations section explicitly lists these missing functions (fluency, tool usage, multi-turn chat) as necessary additions for the next iteration.
- Why unresolved: The current benchmark focuses on static tasks like classification and single-turn generation, lacking metrics for interactive or agentic behavior.
- What evidence would resolve it: The creation and integration of new test sets specifically designed to measure consistency and tool-calling accuracy in Arabic.

### Open Question 3
- Question: Does the reliance on translated or synthetic data introduce cultural or linguistic biases that distort the evaluation of Arabic LLMs?
- Basis in paper: [explicit] Section 6 states future work aims to "eliminate translations and any form of synthetic data generation" to improve quality, acknowledging current limitations.
- Why unresolved: While the platform currently uses these methods to ensure task coverage, the authors flag potential cultural misalignments (e.g., religious terminology) in the Limitations section.
- What evidence would resolve it: A comparative study of model performance on the current synthetic/translated subsets versus new, natively authored Arabic datasets.

## Limitations
- **Security Dependency:** The effectiveness of the blind-test mechanism depends entirely on the platform's ability to prevent leaks.
- **Generalization Uncertainty:** The LLM-as-a-judge methodology's performance may vary with different judges, task types, or Arabic dialect variations.
- **Task Conversion Impact:** The temporary conversion of NER and POS tasks to text generation formats may not fully capture the nuances of these sequence labeling tasks.

## Confidence

- **High Confidence:** The core methodology of using blind test sets to prevent data contamination and the general finding that traditional n-gram metrics poorly correlate with human judgments in Arabic.
- **Medium Confidence:** The specific claim that LLM-as-a-judge is the preferred evaluation method, as this is based on correlation with human judgments for the specific tasks and human evaluators in this study.
- **Medium Confidence:** The overall ranking of models on the BALSAM leaderboard, as it depends on the specific LLM-judge used and the current task coverage.

## Next Checks

1. **Longitudinal Security Audit:** Monitor the BALSAM platform over an extended period to verify that the blind test sets remain secure and that no data contamination occurs.
2. **Judge Ablation Study:** Evaluate the same BALSAM tasks using different LLM judges (e.g., GPT-4o, Claude 3.5) to assess the robustness of the evaluation methodology and identify potential judge-specific biases.
3. **Native Sequence Task Support:** Implement native support for sequence labeling tasks in the BALSAM platform and compare the results with the current text-generation conversions to quantify any impact on model evaluation.