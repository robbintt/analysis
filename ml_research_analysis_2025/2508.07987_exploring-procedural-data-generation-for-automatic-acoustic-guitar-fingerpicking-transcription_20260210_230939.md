---
ver: rpa2
title: Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking
  Transcription
arxiv_id: '2508.07987'
source_url: https://arxiv.org/abs/2508.07987
tags:
- data
- audio
- guitar
- transcription
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automatic transcription
  of acoustic guitar fingerpicking performances, which is hindered by the scarcity
  of labeled training data and legal constraints on using copyrighted recordings.
  To overcome this, the authors propose a fully procedural data generation pipeline
  that synthesizes training data through four stages: knowledge-based fingerpicking
  tablature composition, MIDI performance rendering with humanization, physical modeling
  using an extended Karplus-Strong algorithm, and audio augmentation simulating recording
  environments.'
---

# Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription

## Quick Facts
- arXiv ID: 2508.07987
- Source URL: https://arxiv.org/abs/2508.07987
- Reference count: 2
- This paper demonstrates that procedurally generated synthetic data can effectively pretrain models for acoustic guitar fingerpicking transcription, achieving 83.49% F1-Score when finetuned on just 60 real recordings.

## Executive Summary
This paper addresses the challenge of automatic transcription of acoustic guitar fingerpicking performances by developing a fully procedural data generation pipeline. The approach synthesizes training data through knowledge-based tablature composition, MIDI performance rendering with humanization, physical modeling using an extended Karplus-Strong algorithm, and audio augmentation. The generated synthetic data is used to train a CRNN-based note-tracking model, showing that models trained solely on synthetic audio achieve reasonable transcription performance, and pretraining on synthetic data followed by finetuning on small amounts of real recordings yields improved performance compared to training only on real data.

## Method Summary
The authors propose a four-stage procedural data generation pipeline: (1) knowledge-based fingerpicking tablature composition using chord progressions and picking patterns, (2) MIDI performance rendering with humanization including timing jitter and pitch perturbations, (3) physical modeling using an extended Karplus-Strong algorithm with six filters and randomized parameters, and (4) audio augmentation simulating recording environments with distortion, filtering, reverb, and noise. The synthetic data is used to train a CRNN-based note-tracking model, with pretraining on synthetic data followed by finetuning on real recordings to overcome data scarcity challenges.

## Key Results
- Models trained solely on procedurally generated audio achieve 66.23% F1-Score on the GuitarSet test split
- Pretraining on synthetic data followed by finetuning on 60 real recordings yields 83.49% F1-Score, outperforming models trained exclusively on real recordings
- Pretraining with only 60 real recordings achieves 77.45% F1-Score versus 63.32% without pretraining—a 14% absolute improvement
- Parameter modulation in synthesis is critical: static synthesis yields 27.65% F1-Score, while combined modulation achieves 64.44%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Procedural data generation enables effective pretraining for low-data regimes
- Mechanism: Synthetic data provides broad coverage of musical and acoustic patterns. Pretraining on this diverse distribution learns generalizable acoustic features that transfer to real recordings, reducing the amount of labeled real data needed for finetuning
- Core assumption: The synthetic distribution captures sufficient overlap with real-world acoustic features for transfer learning to occur
- Evidence anchors:
  - [abstract] "Finetuning with a small amount of real data further enhances transcription accuracy, improving over models trained exclusively on real recordings"
  - [section 4.5] Figure 7 shows pretraining with only 60 real recordings achieves 77.45% F1-Score versus 63.32% without pretraining—a 14% absolute improvement
- Break condition: If synthetic audio lacks critical acoustic features present in real recordings, pretraining gains will diminish significantly

### Mechanism 2
- Claim: Parameter modulation in synthesis prevents overfitting and improves generalization
- Mechanism: Randomizing synthesis parameters creates timbral variability across samples, forcing the model to learn robust note detection rather than memorizing a narrow acoustic profile
- Core assumption: Timbral diversity is more important than audio fidelity for transcription model training
- Evidence anchors:
  - [section 4.2, Table 2] Static synthesis yields 27.65% F1-Score (high precision, poor recall). Combined modulation achieves 64.44%—detune alone improves F1 from 27.65% to 61.67%
- Break condition: If parameter ranges exceed realistic acoustic bounds, generated samples may introduce artifacts that degrade rather than improve generalization

### Mechanism 3
- Claim: Audio augmentation simulates recording conditions and bridges synthetic-real domain gap
- Mechanism: Post-processing effects simulate diverse recording environments and equipment imperfections, helping models generalize beyond clean synthetic audio to real-world recordings
- Core assumption: Real guitar recordings contain environmental artifacts that models must learn to ignore
- Evidence anchors:
  - [section 4.3, Table 3] Combined augmentation improves F1-Score from 64.41% to 68.22% on full test set
  - [section 4.4, Table 5] Ablation shows removing augmentation causes 15% F1-Score drop (66.23% → 57.54%)
- Break condition: If augmentation parameters are too aggressive, the underlying note content may become unrecoverable, hurting training

## Foundational Learning

- Concept: **Karplus-Strong Algorithm**
  - Why needed here: Core synthesis method modeling plucked string physics via delay lines and filters. Essential for debugging synthesis quality and parameter effects
  - Quick check question: Can you explain why the delay line length N = fs/f0 determines pitch frequency?

- Concept: **Transfer Learning with Synthetic Pretraining**
  - Why needed here: The paper's main contribution is showing synthetic pretraining reduces real data requirements. Understanding pretraining-finetuning dynamics is critical for replication
  - Quick check question: Why would a model pretrained on synthetic data outperform one trained from scratch on the same small real dataset?

- Concept: **Onsets and Frames Architecture**
  - Why needed here: The CRNN model separates onset detection from sustain prediction. This dual-objective approach requires understanding both temporal and spectral feature extraction
  - Quick check question: Why does the architecture use separate branches for onsets and frames rather than a single output?

## Architecture Onboarding

- Component map: Tablature Composer -> MIDI Humanizer -> Karplus-Strong Synthesizer -> Audio Augmentation -> Mel spectrogram extraction -> CRNN training -> Finetuning on real data

- Critical path: Tablature sampling → MIDI humanization → Karplus-Strong synthesis → Audio augmentation → Mel spectrogram extraction → CRNN training → Finetuning on real data

- Design tradeoffs:
  - **Karplus-Strong vs. VST synthesis**: Karplus-Strong is computationally efficient and runs natively on Linux but may lack acoustic fidelity vs. sample-based VSTs (Table 1 shows VST F1=71.58% vs. KS F1=68.22% when trained without augmentation)
  - **Musical validity vs. diversity**: Knowledge-based composer ensures musical coherence but may limit stylistic diversity compared to neural generators (Table 4 shows fingerpicking composer F1=66.23% vs. MMM F1=60.21%)
  - **Parameter modulation vs. fidelity**: Aggressive modulation improves generalization but deviates from strict musical accuracy

- Failure signatures:
  - High precision, low recall (as in static synthesis): Model overfits to narrow acoustic profile; increase parameter modulation
  - Large gap between solo and accompaniment performance: Comping tracks have higher note density; consider data balancing or architecture adjustments
  - Finetuning instability: Learning rate may be too high; paper uses 6×10⁻⁵ for finetuning vs. 6×10⁻⁴ for initial training
  - Poor transfer to real recordings: Augmentation may be insufficient; verify combined augmentation pipeline is applied consistently

- First 3 experiments:
  1. Baseline replication: Train CRNN on GuitarSet training split, evaluate on test split. Target: ~81% F1-Score
  2. Synthetic-only training: Generate 10,000 procedurally synthesized samples, train CRNN from scratch, evaluate on GuitarSet. Target: ~66% F1-Score
  3. Pretraining + finetuning ablation: Pretrain on synthetic data for 10,000 steps, finetune on varying fractions of real data (1/5 to 5/5 splits). Verify Figure 7 curve: pretraining should provide ~10-15% F1 improvement at low data regimes

## Open Questions the Paper Calls Out
None

## Limitations
- The transfer learning mechanism operates on an assumption that synthetic and real audio distributions have sufficient overlap, but this is not directly validated
- Parameter modulation effects show clear improvements but optimal ranges remain unvalidated and may be arbitrary
- The augmentation pipeline shows consistent improvements but lacks component-level validation and could be sensitive to parameter choices

## Confidence
- **High Confidence**: The core finding that procedural data generation enables effective pretraining - supported by multiple experimental conditions and consistent improvement patterns across data regimes
- **Medium Confidence**: The specific parameter modulation ranges and their individual contributions - while modulation clearly helps, optimal ranges and individual parameter importance are not established
- **Medium Confidence**: The augmentation pipeline's overall effectiveness - shows consistent gains but lacks component-level validation

## Next Checks
1. **Transfer Learning Ablation**: Systematically vary the synthetic data generation parameters (tabulature diversity, parameter modulation ranges, augmentation intensity) to identify which components most strongly influence pretraining effectiveness

2. **Real-to-Synthetic Domain Gap Analysis**: Conduct a quantitative analysis of acoustic feature distributions between procedurally generated and real recordings, measuring overlap in spectral, temporal, and timbral dimensions

3. **Cross-Style Generalization Test**: Evaluate the pretrained models on fingerpicking styles not represented in the procedural generation corpus (e.g., flamenco, classical) to assess whether learned features generalize beyond the training distribution