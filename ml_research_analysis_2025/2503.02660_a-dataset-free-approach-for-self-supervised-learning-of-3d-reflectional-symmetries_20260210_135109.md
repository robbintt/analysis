---
ver: rpa2
title: A dataset-free approach for self-supervised learning of 3D reflectional symmetries
arxiv_id: '2503.02660'
source_url: https://arxiv.org/abs/2503.02660
tags:
- symmetry
- object
- features
- symmetries
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-supervised approach for detecting
  3D reflectional symmetries without requiring datasets or ground truth labels. The
  method uses a neural network trained solely on the input object, leveraging visual
  features extracted from rendered images using a foundational model.
---

# A dataset-free approach for self-supervised learning of 3D reflectional symmetries

## Quick Facts
- arXiv ID: 2503.02660
- Source URL: https://arxiv.org/abs/2503.02660
- Authors: Isaac Aguirre; Ivan Sipiran; Gabriel Montañana
- Reference count: 40
- Primary result: Self-supervised 3D reflectional symmetry detection without datasets, achieving F-score of 0.70 on ShapeNet

## Executive Summary
This paper introduces a self-supervised approach for detecting 3D reflectional symmetries without requiring datasets or ground truth labels. The method uses a neural network trained solely on the input object, leveraging visual features extracted from rendered images using a foundational model. These features are back-projected onto the point cloud to enhance symmetry detection. The network predicts symmetry planes via Householder transformations, optimized using an extended Chamfer distance loss that incorporates both geometric and feature-based information. Experiments on ShapeNet demonstrate superior performance over state-of-the-art methods, with an F-score of 0.70 and symmetry distance error of 4.72×10^-4. The approach generalizes well to out-of-distribution geometries, achieving precise symmetry detection without reliance on external training data.

## Method Summary
The method involves three key components: (1) visual feature extraction from rendered views using DINOv2, back-projected onto the point cloud and reduced to 3D via PCA; (2) a PointNet-based network that predicts reflection plane normals, converted to Householder transformations; (3) an extended Chamfer distance loss that combines geometric and feature-based congruence measures, optimized over 100 iterations with random point sampling. The approach leverages the object's intrinsic structure as supervision, eliminating the need for external datasets.

## Key Results
- F-score of 0.70 on ShapeNet test set, outperforming state-of-the-art methods
- Symmetry Distance Error (SDE) of 4.72×10^-4, compared to 6.30×10^-4 for E3Sym
- Achieves high accuracy on out-of-distribution geometries without additional training
- Visual features from DINOv2 improve performance from 0.65 to 0.70 F-score

## Why This Works (Mechanism)

### Mechanism 1: Self-Prior Optimization
Training a network on a single object's intrinsic structure can detect symmetry without external datasets. The network optimizes transformation parameters by minimizing the discrepancy between the original point cloud and its reflected version, using the symmetry definition itself (T(O) = O) as supervisory signal. This assumes an object's symmetry is determined entirely by its intrinsic geometric and visual structure.

### Mechanism 2: Visual Feature Back-Projection with Rotational Augmentation
Back-projecting DINOv2 features from rendered views onto point clouds creates symmetry-aware visual descriptors that improve detection accuracy. Images are rendered from Fibonacci-sampled viewpoints, DINOv2 extracts patch features, 90°/180°/270° rotations ensure rotational invariance, features back-project to vertices via bilinear interpolation, and aggregate across views by averaging before PCA reduction to 3D.

### Mechanism 3: Householder Transformation with Extended Chamfer Loss
Parameterizing reflection planes via Householder transformations and optimizing with feature-augmented Chamfer distance yields precise symmetry detection. The network predicts normal vectors, normalized and converted to reflection matrices via H(N) = I - 2NN^T. Extended Chamfer distance adds feature distance ‖Fx - Fy‖² to geometric distance, providing richer gradients that enforce both geometric and visual symmetry.

## Foundational Learning

- **Chamfer Distance**: Core loss function measuring point cloud congruence; must understand bidirectional nearest-neighbor matching. Quick check: Given two point sets A and B, can you explain why Chamfer distance is computed as a sum over both A→B and B→A nearest distances?
- **Householder Transformations**: Converts predicted normal vectors into valid reflection matrices; ensures the transformation is mathematically a reflection. Quick check: If N = [0, 0, 1], what reflection matrix does H(N) = I - 2NN^T produce?
- **Self-Prior / Deep Internal Learning**: The paradigm of training on a single input rather than a dataset; different from standard supervised learning intuition. Quick check: How does the network avoid overfitting to noise when trained on only one object with no ground truth?

## Architecture Onboarding

- Component map: Mesh → Fibonacci viewpoints (render images) → DINOv2 → bilinear interpolation → back-project to vertices → PCA (384→3 dims) → barycentric interpolation for sampled points → PointNet encoder → 3 linear heads → normalize normals → Householder transformation → extended Chamfer loss
- Critical path: Feature extraction quality → network optimization stability → plane filtering threshold. The ablation (Table 2) shows feature extraction method causes ~27 point F-score difference.
- Design tradeoffs: Fibonacci vs. uniform sampling (better rotational invariance vs. implementation complexity); 3D PCA features vs. raw 384-dim (computational efficiency vs. potential information loss); 100 iterations (fast convergence vs. risk of local minima).
- Failure signatures: High loss that doesn't decrease (object may lack global symmetry); multiple identical plane predictions (regularization weight may need adjustment); poor OOD performance on specific geometries (foundational model may not capture relevant visual features).
- First 3 experiments: (1) Ablate visual features - run with 3D coordinates only on 10 test objects; expect F-score drop per Table 2 (0.65 vs. 0.70). (2) Vary viewpoint count - test Fibonacci sampling with 20, 40, 80 viewpoints; measure SDE convergence. (3) Test inference filtering - vary Chamfer threshold (0.01, 0.02, 0.05) on objects with known 0, 1, 2, 3 symmetries; plot precision-recall tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed self-prior framework be extended to detect rotational and continuous symmetries in addition to planar reflectional symmetries? The current architecture is specifically designed for reflection, predicting normal vectors for reflective planes via Householder transformations, and does not output rotation axes or angles. Future work will explore extending our framework to detect rotational symmetries.

### Open Question 2
How does the method perform on partial or occluded surfaces where the geometric centroid cannot be reliably used as a fixed point for the transformation? The methodology explicitly assumes global symmetry and uses the object's centroid as the fixed point for the transformation; this assumption fails when large portions of the object are missing. Handle objects with partial or occluded surfaces is listed as future work.

### Open Question 3
To what extent does the reliance on DINOv2 visual features compromise detection accuracy on objects with symmetrical geometry but asymmetrical textures? If an object is geometrically symmetric but has asymmetric visual patterns, the extended Chamfer distance might increase the loss for correct geometric planes, potentially hindering convergence. The hypothesis that symmetric points should exhibit similar visual appearances may fail on textured surfaces.

## Limitations
- Self-prior optimization may fail on objects with weak or approximate symmetries, as the supervisory signal depends entirely on the object's intrinsic structure
- Method's performance on non-rigid or articulated objects remains untested
- Inference filtering threshold (0.02) appears empirically chosen without systematic sensitivity analysis

## Confidence
- Self-prior optimization effectiveness: High (ablation shows 0.65 F-score without visual features vs. 0.70 with features)
- Visual feature contribution: Medium (significant improvement shown, but generalization to abstract geometries lacks validation)
- Generalization to out-of-distribution geometries: Low (claims good OOD performance but provides limited evidence beyond ShapeNet variations)

## Next Checks
1. Test the method on a dataset of objects with known approximate symmetries (e.g., organic shapes) to assess robustness when symmetry is not exact.
2. Conduct a systematic ablation study varying the inference Chamfer threshold (0.01-0.05) to understand precision-recall tradeoffs and determine optimal filtering.
3. Evaluate performance on a held-out set of articulated or non-rigid objects to verify the method's generalization claims beyond rigid geometries.