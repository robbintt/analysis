---
ver: rpa2
title: The System Description of CPS Team for Track on Driving with Language of CVPR
  2024 Autonomous Grand Challenge
arxiv_id: '2509.11071'
source_url: https://arxiv.org/abs/2509.11071
tags:
- depth
- language
- information
- objects
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vision-language model system for the Driving
  with Language track of CVPR 2024 Autonomous Grand Challenge. The authors fine-tuned
  LLaVA models using the DriveLM-nuScenes dataset, incorporating depth information
  from an open-source depth estimation model and applying LoRA/DoRA parameter-efficient
  fine-tuning.
---

# The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge

## Quick Facts
- arXiv ID: 2509.11071
- Source URL: https://arxiv.org/abs/2509.11071
- Reference count: 14
- Primary result: Achieved 0.7799 validation score, ranking 1st in CVPR 2024 Autonomous Grand Challenge Driving with Language track

## Executive Summary
This paper presents a vision-language model system for the Driving with Language track of CVPR 2024 Autonomous Grand Challenge. The authors fine-tuned LLaVA models using the DriveLM-nuScenes dataset, incorporating depth information from an open-source depth estimation model and applying LoRA/DoRA parameter-efficient fine-tuning. They also employed Chain-of-Thought reasoning for multiple-choice and yes/no questions. Their approach achieved a top score of 0.7799 on the validation set leaderboard, ranking 1st. The system integrates depth information, key object descriptions, and states into prompts for enhanced reasoning capabilities.

## Method Summary
The system builds on LLaVA models, enhanced through LoRA and DoRA fine-tuning with the DriveLM-nuScenes dataset. Depth information from Depth Anything is integrated by computing the 75th percentile depth within key object bounding boxes and converting this to textual labels like "close" or "far." For inference, KeyObj Tags are extracted from questions to retrieve corresponding object descriptions and states, which are prepended to prompts along with depth labels. Chain-of-Thought reasoning is applied for multiple-choice and yes/no questions. The final answers are generated through model selection and voting across multiple systems.

## Key Results
- Achieved 0.7799 weighted score on validation set, ranking 1st place
- LoRA (rank=8) outperformed DoRA despite DoRA's theoretically more advanced weight decomposition
- Few-shot Chain-of-Thought prompting degraded performance compared to standard prompting

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning (LoRA) enables effective adaptation of VLMs to driving tasks without full model retraining. LoRA injects low-rank decomposition matrices into fully connected layers of the language model component, allowing gradient updates to modify behavior while keeping base weights frozen. This preserves pre-trained visual-linguistic knowledge while specializing for driving QA. Core assumption: The driving domain requires semantic reasoning capabilities already present in the base VLM, needing only task alignment rather than fundamental capability acquisition.

### Mechanism 2
Depth estimation integration enriches spatial reasoning by converting visual depth signals into textual prompts. The Depth Anything model estimates per-pixel depth; the system extracts the 75th percentile depth value within key object bounding boxes, translates this to discrete textual labels ("close"/"far"), and prepends this to the question prompt. The VLM then reasons over explicit spatial information. Core assumption: VLMs reason more accurately over explicit textual spatial descriptors than implicit visual depth cues.

### Mechanism 3
Structured prompt construction with object metadata guides model attention toward task-relevant scene elements. The system extracts KeyObj Tags from questions, retrieves corresponding object descriptions and states, computes depth labels, and concatenates: `{KeyObj_Desc&State}{KeyObj_Depth}{Question}`. This explicit conditioning reduces search space and focuses reasoning. Core assumption: Providing ground-truth object metadata at inference time compensates for imperfect visual grounding in the base VLM.

## Foundational Learning

- **Vision-Language Models (VLMs) with instruction tuning**
  - Why needed here: LLaVA-1.5 and LLaVA-NeXT are not general-purpose multimodal models; they require understanding of how visual encoders (CLIP) map to LLM token spaces via projection layers.
  - Quick check question: Can you explain why LLaVA uses a two-stage training process (pre-training for feature alignment, then instruction tuning)?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper uses rank=8, alpha=16 LoRA on fully connected layers. Understanding how LoRA decomposes weight updates as BA matrices is essential for debugging convergence.
  - Quick check question: If LoRA rank is increased from 8 to 32, what trade-off between adaptability and overfitting should you expect?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper reports that few-shot CoT "may have inadvertently constrained the model's inherent reasoning abilities." Understanding CoT failure modes is critical.
  - Quick check question: Why might zero-shot CoT ("Let's think step by step") outperform few-shot CoT in novel driving scenarios?

## Architecture Onboarding

- **Component map**: Images + Question → KeyObj Tag extraction → Depth computation → Prompt assembly → LLaVA with LoRA → Answer generation → Model fusion
- **Critical path**: Question parsing → KeyObj extraction → Image selection → Depth computation → Prompt assembly → VLM inference → Answer fusion
- **Design tradeoffs**: LoRA vs. DoRA: LoRA (rank=8) outperformed DoRA despite DoRA's weight decomposition advantages—likely due to dataset size (4072 frames) favoring simpler adaptation. CoT complexity: Few-shot CoT reduced performance; the paper suggests comprehensive prompt design over rigid reasoning templates. Image selection strategy: Forward-facing default for ambiguous questions trades specificity for coverage.
- **Failure signatures**: Low accuracy on questions without KeyObj Tags (image selection defaults to front camera regardless of actual scene content). Depth discretization failures when 75th percentile doesn't represent object distance (e.g., occluded objects). CoT degradation: Model generates repetitive or constrained responses when few-shot examples don't match query distribution.
- **First 3 experiments**: 1) Ablate depth information: Train/inference without depth textual labels to measure contribution. 2) Vary LoRA rank: Test rank∈{4,8,16,32} on held-out validation subset to find overfitting threshold. 3) Prompt format variants: Compare `{Desc}{Depth}{Question}` vs. `{Question}{Desc}{Depth}` ordering to assess attention patterns.

## Open Questions the Paper Calls Out

- **How can Chain-of-Thought (CoT) prompting be adapted to improve, rather than degrade, VLM performance in the complex context of autonomous driving?** The authors identify that few-shot CoT "inadvertently constrained the model's inherent reasoning abilities, limiting its capacity to generate diverse responses," leading to decreased evaluation performance, but only suggest future strategies without providing a validated solution.

- **Why does Low-Rank Adaptation (LoRA) outperform the theoretically more advanced Weight-Decomposed Low-Rank Adaptation (DoRA) in this specific fine-tuning regime?** The paper reports the counter-intuitive result where LoRA achieved a higher Final Score (0.7710) than DoRA (0.7177) despite DoRA utilizing more parameters and training time, without providing an analysis explaining this discrepancy.

- **Is the textual discretization of depth data (e.g., 'close'/'far') optimal for VLMs compared to direct numerical feature injection?** The authors describe converting continuous depth estimates into textual descriptions via a 75th-percentile rule, but do not validate if this translation results in information loss compared to preserving continuous depth features within the visual encoder.

## Limitations

- The exact depth threshold values for "close" vs. "far" discretization are unspecified, creating potential reproducibility gaps.
- The dataset size (4,072 training frames) is relatively small for VLMs, raising concerns about generalization to unseen driving scenarios.
- The KeyObj Tag extraction relies on heuristic rules that may fail on complex or ambiguous questions, with the system defaulting to forward camera selection when cues are missing.

## Confidence

- **High confidence**: The parameter-efficient fine-tuning approach (LoRA rank=8) and depth integration mechanism are well-specified and theoretically sound.
- **Medium confidence**: The overall system ranking (1st place) and validation score (0.7799) are directly measured, but generalizability to test set and real-world deployment remains unproven.
- **Low confidence**: The specific prompt engineering details (exact CoT templates, fusion rules) and depth discretization thresholds lack sufficient specification for full reproduction.

## Next Checks

1. **Depth discretization sensitivity analysis**: Systematically vary depth thresholds for "close"/"far" mapping and measure impact on accuracy across different object distance ranges. Test whether continuous depth values outperform discrete labels.

2. **Camera selection ablation**: Compare performance when always using all six camera views versus the heuristic-based single-view selection. Measure accuracy loss when KeyObj Tag extraction fails or returns ambiguous results.

3. **Dataset size scaling study**: Train on progressively larger subsets (10%, 25%, 50%, 100%) of the 4,072 frames to identify overfitting thresholds and determine minimum effective dataset size for this approach.