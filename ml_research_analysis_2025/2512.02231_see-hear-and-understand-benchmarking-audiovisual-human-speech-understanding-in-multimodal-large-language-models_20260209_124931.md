---
ver: rpa2
title: 'See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding
  in Multimodal Large Language Models'
arxiv_id: '2512.02231'
source_url: https://arxiv.org/abs/2512.02231
tags:
- video
- visual
- speech
- audio
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AV-SpeakerBench, a new benchmark designed
  to evaluate fine-grained audiovisual human speech understanding in multimodal large
  language models (MLLMs). Unlike existing video benchmarks that often focus on visually
  solvable questions or coarse audio-visual matching, AV-SpeakerBench emphasizes speaker-centric
  audiovisual reasoning.
---

# See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2512.02231
- Source URL: https://arxiv.org/abs/2512.02231
- Authors: Le Thien Phuc Nguyen; Zhuoran Yu; Samuel Low Yu Hang; Subin An; Jeongik Lee; Yohan Ban; SeungEun Chung; Thanh-Huy Nguyen; JuWan Maeng; Soochahn Lee; Yong Jae Lee
- Reference count: 40
- Key outcome: Introduces AV-SpeakerBench benchmark revealing current MLLMs achieve only ~73% accuracy on speaker-centric audiovisual reasoning versus human performance of 93.74%, with fusion weaknesses driving the gap.

## Executive Summary
This paper introduces AV-SpeakerBench, a new benchmark designed to evaluate fine-grained audiovisual human speech understanding in multimodal large language models (MLLMs). Unlike existing video benchmarks that often focus on visually solvable questions or coarse audio-visual matching, AV-SpeakerBench emphasizes speaker-centric audiovisual reasoning. It contains 3,212 multiple-choice questions across 12 task types, all requiring models to jointly interpret who speaks, what is said, and when it occurs in real-world videos. The benchmark uses a speaker-centered formulation, fusion-grounded question design, and expert-curated annotations to ensure temporal precision and cross-modal validity. Comprehensive evaluations reveal that current MLLMs, including both proprietary (e.g., Gemini 2.5 Pro) and open-source models (e.g., Qwen3-Omni-30B), still struggle with this task, with the best models achieving only around 73% accuracy compared to human performance of 93.74%. The performance gap is primarily due to weaknesses in audiovisual fusion rather than visual perception alone. AV-SpeakerBench thus provides a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.

## Method Summary
AV-SpeakerBench is an evaluation-only benchmark containing 3,212 MCQs across 12 speaker-centric audiovisual reasoning tasks. Questions require joint interpretation of speaker identity, spoken content, and temporal information from 5-30s YouTube video clips. Models are evaluated with native audiovisual interfaces using specified frame sampling rates (ranging from 1-8 frames per video). The benchmark employs a unified prompt format and measures 4-choice accuracy, with human baseline at 93.74% and best model (Gemini 2.5 Pro) achieving 73.04%.

## Key Results
- Current MLLMs achieve only ~73% accuracy on AV-SpeakerBench versus human performance of 93.74%
- Performance gap primarily due to audiovisual fusion weaknesses rather than unimodal perception
- Gemini 2.5 Pro shows consistent 10-20% accuracy gains from audio across all tasks, while Qwen3-Omni-30B exhibits limited or negative gains in some tasks
- Accuracy degrades as visual complexity increases, dropping from 74.8% (≤2 visible people) to 70.9% (≥5 visible people)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speaker-centric formulation shifts the reasoning unit from scenes to individual speakers, enabling fine-grained audiovisual evaluation.
- Mechanism: By grounding all questions in the "who speaks, when, what is said" triad, the benchmark forces models to perform joint identity, temporal, and content reasoning rather than relying on scene-level shortcuts.
- Core assumption: Models that can process audio and video separately may still fail at binding spoken utterances to specific visible individuals in time.
- Evidence anchors:
  - [abstract] "AV-SpeakerBench... focused on speaker-centric audiovisual reasoning... treats speakers—not scenes—as the core reasoning unit."
  - [section 3] "Each question in our benchmark is designed to require joint interpretation of visual and auditory cues in natural human speech scenes."
- Break condition: If models can answer questions by tracking only scene-level features (e.g., counting visible people without needing speaker identity), the speaker-centric constraint is bypassed.

### Mechanism 2
- Claim: Fusion-grounded question design embeds cross-modal dependencies directly into question semantics, ensuring true multimodal integration.
- Mechanism: Questions are constructed so that correct answers require linking auditory events (e.g., spoken phrases) with visual events (e.g., gestures, identity) or vice versa, preventing unimodal shortcuts.
- Core assumption: Models with weak audiovisual fusion will show minimal accuracy gains when audio is added to video-only inputs.
- Evidence anchors:
  - [abstract] "...fusion-grounded question design embedding audiovisual dependencies into question semantics..."
  - [section 3.1] "In our formulation, audiovisual dependency is embedded directly in the textual construction of the question and its options."
- Break condition: If models achieve high accuracy on vision-only inputs, the benchmark may not be enforcing fusion as intended (though the paper acknowledges some visual inference is natural).

### Mechanism 3
- Claim: Performance gaps between proprietary and open-source models primarily reflect differences in audiovisual fusion capability, not unimodal perception.
- Mechanism: Stronger models like Gemini 2.5 Pro show consistent accuracy gains (10–20%) when audio is added, while weaker models like Qwen3-Omni show modest or negative gains, indicating fusion failure.
- Core assumption: Audiovisual fusion is a learnable capability separable from visual or auditory perception alone.
- Evidence anchors:
  - [abstract] "...performance gap is primarily due to weaknesses in audiovisual fusion rather than visual perception alone."
  - [section 4.3] "Gemini 2.5 Pro exhibits consistent gains of roughly 10–20 percentage points across all tasks when both modalities are available... Qwen3-Omni-30B achieves much smaller gains—and in some tasks, even negative differences..."
- Break condition: If future models achieve high accuracy through improved unimodal perception alone (e.g., better lip-reading without true fusion), the diagnostic value of the benchmark for fusion may diminish.

## Foundational Learning
- Concept: Audiovisual Fusion
  - Why needed here: The benchmark is explicitly designed to test whether models can integrate auditory and visual information, not just process each modality separately.
  - Quick check question: Can the model correctly identify who spoke a specific phrase in a multi-speaker video when only audio is available vs. when both audio and video are available?
- Concept: Temporal Grounding
  - Why needed here: Many questions require reasoning about events before, after, or during specific utterances, demanding precise temporal alignment.
  - Quick check question: Given a video clip, can the model localize the moment when a specific phrase is spoken and retrieve concurrent visual information?
- Concept: Speaker Diarization
  - Why needed here: The benchmark involves multi-speaker scenarios where distinguishing who speaks when is essential.
  - Quick check question: In a video with three visible speakers, can the model attribute each utterance to the correct visible individual?

## Architecture Onboarding
- Component map: Video encoder -> Audio encoder -> Fusion module -> LLM backbone -> Answer selection
- Critical path: Video + Audio → Encoders → Fusion → LLM → Answer selection (A/B/C/D)
- Design tradeoffs:
  - Frame sampling rate: Higher rates improve temporal resolution but increase compute cost
  - Audio resolution: Detailed audio features (e.g., pitch, intensity) may be needed for paralinguistic tasks but increase model complexity
  - Fusion depth: Shallow fusion (early concatenation) may fail on complex temporal reasoning; deep fusion (cross-modal attention) may be required but is harder to train
- Failure signatures:
  - Audio perception errors: Mishearing phrases or failing to distinguish overlapping speech
  - Cross-modal attribution errors: Correctly perceiving audio and visual content separately but misattributing utterances to wrong speakers
  - Temporal grounding errors: Identifying events but applying incorrect temporal relations (e.g., before vs. after)
  - Temporal localization errors: Selecting wrong time segments, leading to misaligned reasoning
- First 3 experiments:
  1. Modality ablation: Evaluate models with video-only, audio-only, and both modalities to quantify fusion contributions (as done in paper Figure 3a)
  2. Error analysis: Manually categorize failures into perception, attribution, grounding, and localization errors to identify weak components
  3. Scale vs. fusion: Compare model performance across sizes (e.g., Qwen2.5-Omni 3B vs. 7B vs. Qwen3-Omni 30B) to test whether scaling improves fusion or just unimodal perception

## Open Questions the Paper Calls Out
- How can audiovisual fusion mechanisms be improved to ensure consistent cross-modal gains across all speaker-centric reasoning tasks?
- What specific improvements in audio perception and temporal grounding are required to close the ~20% human–model performance gap on speaker-centric reasoning?
- How does visual complexity (number of visible speakers) systematically affect the difficulty of audiovisual speaker reasoning, and can models be made robust to multi-party scenes?

## Limitations
- Benchmark relies on YouTube-sourced videos with potential licensing and reproducibility constraints
- Fixed frame sampling rates per model may not be optimal for all architectures
- Attribution of performance differences to audiovisual fusion weakness is plausible but not definitively proven

## Confidence
**High Confidence**: The benchmark's design principles (speaker-centric formulation, fusion-grounded questions, expert annotations) are clearly articulated and methodologically sound. The performance gap between humans (93.74%) and best models (73.04%) is statistically significant and robustly measured.

**Medium Confidence**: The attribution of performance differences primarily to audiovisual fusion weakness is plausible but not definitively proven, as the analysis does not fully control for unimodal perception capabilities versus true cross-modal integration.

**Low Confidence**: The generalization of these findings to other audiovisual reasoning tasks beyond the specific speaker-focused scenarios in AV-SpeakerBench remains uncertain.

## Next Checks
1. **Fusion Isolation Test**: Create synthetic videos where audio and visual content are temporally aligned but semantically independent, then measure whether models can selectively integrate only relevant cross-modal information versus being distracted by irrelevant correlations.

2. **Temporal Resolution Sweep**: Systematically vary frame sampling rates (1, 2, 4, 8 fps) across all evaluated models to determine whether temporal resolution or fusion capability is the primary bottleneck for audiovisual reasoning.

3. **Unimodal Ablation with Perceptual Control**: Compare vision-only and audio-only performance while controlling for unimodal perception quality (using human perceptual baselines for audio transcription and visual speaker identification) to isolate the contribution of fusion from basic modality comprehension.