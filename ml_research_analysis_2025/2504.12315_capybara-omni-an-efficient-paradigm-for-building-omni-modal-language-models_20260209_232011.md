---
ver: rpa2
title: 'Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models'
arxiv_id: '2504.12315'
source_url: https://arxiv.org/abs/2504.12315
tags:
- arxiv
- zhang
- data
- wang
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Capybara-OMNI, a lightweight and efficient
  approach for building omni-modal language models that support text, image, video,
  and audio understanding. The authors propose a progressive training strategy that
  builds upon a vision-language model by sequentially adding audio capabilities and
  then enhancing instruction-following skills.
---

# Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models

## Quick Facts
- arXiv ID: 2504.12315
- Source URL: https://arxiv.org/abs/2504.12315
- Authors: Xingguang Ji; Jiakang Wang; Hongzhi Zhang; Jingyuan Zhang; Haonan Zhou; Chenxi Sun; Yahui Liu; Qi Wang; Fuzheng Zhang
- Reference count: 26
- Primary result: Achieves competitive performance across text, image, video, and audio tasks using progressive training and selective freezing, with significant efficiency gains over comparable models.

## Executive Summary
Capybara-OMNI introduces a lightweight, efficient approach for building omni-modal language models that support text, image, video, and audio understanding. The authors propose a progressive training strategy that builds upon a vision-language model by sequentially adding audio capabilities and then enhancing instruction-following skills. The model uses a SigLIP visual encoder, Qwen2 LLM, and initializes the audio encoder from Qwen2-Audio to avoid catastrophic forgetting. Training data quality is improved through filtering and clustering, while cross-modal instruction tuning employs video-based dialogue data to enhance human interaction. Results show that Capybara-OMNI achieves competitive performance across multiple benchmarks while using significantly less training data than comparable models.

## Method Summary
Capybara-OMNI employs a three-stage progressive training paradigm. First, visual alignment trains the SigLIP vision encoder with a 2-layer MLP adapter to the Qwen2.5-7B LLM using image-text pairs. Second, audio alignment initializes the audio encoder from Qwen2-Audio and trains it with a 1-layer MLP adapter while freezing the LLM and visual components to prevent interference. Third, cross-modal instruction tuning unfreezes all parameters and trains on synthetic video-based dialogue data generated by GPT-4o to enhance conversational capabilities. The architecture uses dynamic resolution splitting for high-resolution images and stride-2 pooling to compress audio tokens, maintaining efficiency while supporting omni-modal understanding.

## Key Results
- Achieves competitive performance across multiple benchmarks: HallusionBench (50.6), MMVET (60.5), MMBench (81.3), MathVista (68.3), and OCRBench (84.4)
- Audio comprehension significantly improved through Qwen2-Audio initialization, reducing CER from 7.4 to 2.6 on Aishell-1
- Data quality improvements through filtering and clustering further reduce CER to 2.1
- Uses significantly less training data than comparable models while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Progressive Modality Alignment with Selective Freezing
- Claim: If visual capabilities are fully trained and frozen before introducing audio, the model mitigates catastrophic forgetting of visual features while acquiring audio understanding.
- Mechanism: The training pipeline enforces a strict dependency: Visual Alignment â†’ Audio Alignment. During the Audio Alignment stage, the LLM and visual encoder parameters are frozen, and only the audio encoder and adapter are trained. This isolates the optimization landscape, preventing gradient updates from the audio modality from degrading the established visual representations.
- Core assumption: The visual model has reached a sufficient performance plateau before audio training begins, and the LLM has enough capacity to serve as a universal reasoning core without further fine-tuning during alignment.
- Evidence anchors: [abstract] "trained progressively through three stages... to mitigate interference among modalities." [section 3.2] "...it is essential not only to enable the LLM to comprehend audio inputs but also to prevent any negative impacts on its understanding capabilities of visual modalities... the parameters of the LLM and the visual component are kept frozen."

### Mechanism 2: Warm-Starting Audio via Specialized Initialization
- Claim: Initializing the audio encoder from a specialized audio-language model significantly accelerates convergence and improves accuracy.
- Mechanism: Instead of training a raw Whisper encoder from scratch, Capybara-OMNI initializes from Qwen2-Audio. This transfers knowledge of audio-text alignment and robust feature extraction, reducing the data volume required to achieve low Word Error Rates.
- Core assumption: The feature space of the pre-trained audio encoder (Qwen2-Audio) is compatible with the target LLM (Qwen2.5-7B) and can be mapped via a simple MLP adapter.
- Evidence anchors: [abstract] "...initializing with Qwen2-Audio... significantly improves audio comprehension, with CER of 2.1..." [section 3.2] "Inspired by the remarkable performance demonstrated by Qwen2-Audio... we initialize the audio encoder from Qwen2-Audio."

### Mechanism 3: Synthetic Cross-Modal Instruction Tuning
- Claim: High-quality, synthetic dialogue data generated by stronger models enables conversational capabilities and cross-modal reasoning that strictly academic/alignment data cannot provide.
- Mechanism: The system uses prompts to guide GPT-4o to generate multi-turn dialogues based on video frames, simulating casual conversation. This moves the model from "captioning" to "interacting," aligning output format with user habits.
- Core assumption: The synthetic data generated by GPT-4o covers the distribution of real-world user queries sufficiently well, and the student model has the capacity to distill this reasoning.
- Evidence anchors: [abstract] "The cross-modal instruction tuning stage enables effective speech-based interaction... producing strong human-computer interaction capabilities." [section 5] "We use prompts to guide GPT-4o... to understand video frames, and then construct alternating dialogue data... simulating the interaction scenarios."

## Foundational Learning

- **Catastrophic Forgetting**: Why needed here: The paper claims success specifically because it solves the interference between audio and visual modalities. Understanding why neural networks forget previously learned tasks when optimized for new ones is central to Section 3.2.
  - Quick check question: Why does the paper recommend freezing the LLM and visual encoder during the Audio Alignment stage?

- **Adapter / Projector Layers (MLP)**: Why needed here: Capybara relies on lightweight MLPs to bridge different modalities (SigLIP, Whisper) to the LLM, rather than re-training the entire backbone. This defines the "efficient" nature of the paradigm.
  - Quick check question: What is the function of the 2-layer perceptron used between the vision encoder and the LLM in Section 3.1?

- **Token Compression (Bilinear Interpolation & Pooling)**: Why needed here: Visual and audio data produce massive token sequences. The paper explicitly details compression strategies to fit these into the context window of the LLM.
  - Quick check question: How many audio tokens does the encoder map per second of audio, and how does this compare to the raw feature rate?

## Architecture Onboarding

- **Component map**: Qwen2.5-7B LLM -> SigLIP vision encoder + 2-layer MLP adapter -> Audio encoder (Qwen2-Audio initialized) + 1-layer MLP adapter

- **Critical path**:
  1. Visual Alignment: Train Visual Adapter, then Unfreeze All. Load data from Table 6.
  2. Audio Alignment: Freeze LLM & Visual. Train Audio Encoder + Audio Adapter on ASR/S2TT data.
  3. Instruction Tuning: Unfreeze All. Train on synthetic video-chat data.

- **Design tradeoffs**:
  - Freezing vs. Full Fine-tuning: The paper trades off potential joint visual-audio feature refinement for stability and compute efficiency during alignment.
  - Resolution vs. Compute: Using MiniCPM scoring to split images allows 1344x1344 resolution handling but increases token count; mitigated by 2x2 compression.

- **Failure signatures**:
  - Audio degradation: If WER rises significantly compared to the baseline in Table 5, check if the LLM parameters were accidentally unfrozen during Audio Alignment.
  - Visual hallucination: If the model describes objects not present in video frames, check the "Data Metabolism" filtering logic or balance of CoT data.

- **First 3 experiments**:
  1. Verify Visual Preservation: Run inference on a standard VQA benchmark using the model immediately after Audio Alignment. Compare scores to the pre-audio checkpoint to verify that freezing prevented degradation.
  2. Audio Ablation: Train a variant using a standard Whisper encoder instead of the Qwen2-Audio initialization. Measure the delta in CER on Aishell-1 to validate the "warm-start" benefit.
  3. Chat Activation: Run a qualitative "Turing test" on the Chat version. Input a video + audio query and verify the model responds to the audio content in the context of the visual content.

## Open Questions the Paper Calls Out

- **Can the performance gap with specialized audio models be closed by updating the LLM parameters during the audio alignment phase?**
  - Basis in paper: [explicit] The authors note Capybara-OMNI underperforms Qwen2-Audio and attribute this to freezing the LLM during alignment, stating this "represents a direction worthy of future exploration."
  - Why unresolved: Updating the LLM risks catastrophic forgetting of visual capabilities, a trade-off the current freezing strategy was designed to avoid.
  - What evidence would resolve it: Ablation studies evaluating audio performance versus visual retention when the LLM is unfrozen during audio training.

## Limitations

- The progressive training methodology's reliance on strict freezing during audio alignment potentially prevents beneficial joint optimization between modalities, limiting the model's ability to dynamically adapt visual representations for audio-visual reasoning tasks.
- The data quality improvement pipeline through filtering and clustering lacks detailed specification of the clustering methodology and quality thresholds, making exact replication challenging.
- The synthetic dialogue generation using GPT-4o for cross-modal instruction tuning introduces dependence on external model outputs, with unspecified prompts and generation parameters that may affect reproducibility.

## Confidence

- **High Confidence**: The progressive training architecture and its three-stage implementation are well-documented and reproducible. The freezing mechanism during audio alignment is clearly described and validated through ablation studies.
- **Medium Confidence**: The effectiveness of Qwen2-Audio initialization for the audio encoder is supported by strong quantitative evidence (CER reduction from 7.4 to 2.6). However, the generalizability of this initialization strategy to different base models or audio tasks requires further validation.
- **Low Confidence**: The synthetic dialogue generation methodology lacks sufficient detail for exact replication. The prompts used with GPT-4o and the specific criteria for constructing conversational data are not specified.

## Next Checks

1. **Cross-Modal Joint Optimization Test**: Conduct an ablation study comparing the frozen approach against a variant with partial joint fine-tuning of visual and audio encoders during the instruction tuning stage. Measure performance on tasks requiring tight audio-visual integration to quantify the tradeoff between stability and capability enhancement.

2. **Data Quality Filtering Replication**: Implement the exact data filtering and clustering pipeline described in the paper using publicly available audio datasets. Systematically vary the filtering thresholds and clustering parameters to identify the specific conditions under which the quality improvements translate to performance gains.

3. **Synthetic Data Generalization Analysis**: Generate synthetic dialogue data using different prompting strategies and weaker language models (e.g., GPT-3.5-turbo) to test the sensitivity of the cross-modal instruction tuning to the quality of synthetic data. Compare performance metrics across different synthetic data sources to establish the robustness of the approach.