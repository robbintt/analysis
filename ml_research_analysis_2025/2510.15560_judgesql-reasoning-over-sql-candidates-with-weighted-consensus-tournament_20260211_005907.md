---
ver: rpa2
title: 'JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament'
arxiv_id: '2510.15560'
source_url: https://arxiv.org/abs/2510.15560
tags:
- reasoning
- candidate
- selection
- judgment
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JudgeSQL addresses the challenge of selecting the correct SQL query
  from a pool of candidates generated by large language models, a critical bottleneck
  in text-to-SQL systems. It introduces a reasoning-based SQL judge model trained
  with reinforcement learning guided by verifiable rewards, enabling accurate and
  interpretable judgments.
---

# JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament

## Quick Facts
- arXiv ID: 2510.15560
- Source URL: https://arxiv.org/abs/2510.15560
- Reference count: 40
- Key outcome: RJudge + WCT achieves 71.9% execution accuracy on BIRD, outperforming baselines

## Executive Summary
JudgeSQL addresses the critical bottleneck in text-to-SQL systems: selecting the correct SQL query from multiple candidates generated by large language models. The system introduces a reasoning-based SQL judge model trained with reinforcement learning guided by verifiable rewards, enabling accurate and interpretable judgments. Paired with a weighted consensus tournament mechanism that combines explicit reasoning preferences with implicit generator confidence, JudgeSQL significantly outperforms baseline selection strategies. Evaluated on the BIRD benchmark, it achieves up to 71.9% execution accuracy and demonstrates strong cross-scale generalization and robustness to generator capacity.

## Method Summary
JudgeSQL operates in two stages: a reasoning-based judge model and a weighted consensus tournament selection mechanism. The judge is trained via two-stage learning - first distilling reasoning traces from GPT-4o that explain SQL preferences, then reinforcement learning with binary verifiable rewards based on execution correctness. The weighted consensus tournament clusters candidates by execution results, runs a double round-robin tournament on proxy representatives from each cluster, and aggregates scores weighted by cluster cardinality. This approach combines explicit reasoning preferences with implicit generator confidence while reducing redundant comparisons from O(N²) to O(K²).

## Key Results
- RJudge + WCT achieves 71.9% execution accuracy on BIRD benchmark
- Outperforms self-consistency by 10.2% absolute points
- Maintains consistent gains across different generation models (7B to 32B parameters)
- Reduces judge call comparisons by 18-44× compared to exhaustive pairwise selection

## Why This Works (Mechanism)

### Mechanism 1
Structured reasoning traces improve SQL candidate discrimination beyond scalar scoring. The judge model is trained via two-stage learning - first distilling reasoning traces from GPT-4o that explain why one SQL is preferred over another, then reinforcement learning with binary verifiable rewards based on execution correctness. This produces interpretable, stepwise reasoning rather than opaque scores.

### Mechanism 2
Clustering candidates by execution results reduces redundant comparisons and amplifies correct-signal density. Instead of comparing all N candidates pairwise, JudgeSQL clusters SQLs by execution result, selects one proxy per cluster, and runs a double round-robin tournament only on proxies. Each cluster's final score is weighted by its cardinality, combining explicit reasoning preference with implicit generator confidence.

### Mechanism 3
Cross-generator generalization emerges from training on diverse preference pairs rather than generator-specific distributions. The judge is trained on preference pairs constructed from multiple generators with execution results as the only correctness signal, forcing it to learn SQL-semantic reasoning rather than generator-specific artifacts.

## Foundational Learning

- Concept: Test-time scaling (Best-of-N sampling)
  - Why needed here: JudgeSQL operates at the selection layer after N candidates are generated; understanding the tradeoff between sampling budget and selection quality is essential.
  - Quick check question: If you increase N from 8 to 64, what happens to the number of execution-consistent clusters and tournament comparisons under WCT vs. DRT?

- Concept: Reinforcement learning with verifiable rewards (RLVR)
  - Why needed here: The GRPO stage relies on binary rewards derived from execution correctness, not human preference labels.
  - Quick check question: What is the reward for a response with correct reasoning but missing the required <answer> tag?

- Concept: Self-consistency and majority voting baselines
  - Why needed here: JudgeSQL is evaluated against self-consistency; understanding SC's failure modes clarifies why WCT helps.
  - Quick check question: Why does SC fail when two incorrect SQLs produce identical wrong results but a correct SQL is unique?

## Architecture Onboarding

- Component map: Candidate Generator → Execution Engine → Clustering → Proxy Selector → Reasoning Judge → Weighted Consensus Tournament → Final SQL
- Critical path: Execution results → clustering → proxy selection → pairwise judge calls → weighted aggregation → final SQL. Latency bottleneck is the judge inference for O(K²) pairs; K is typically 5-15 for N=8-64.
- Design tradeoffs:
  - More samples (higher N) → more clusters → more judge calls but higher potential accuracy
  - Random vs. likelihood-based proxy selection: comparable; random is simpler
  - PJudge (zero-shot) vs. RJudge (GRPO-trained): RJudge costs training compute but improves SA by 6-10 points
- Failure signatures:
  - Empty execution results treated as valid cluster; judge must handle this gracefully
  - Position bias in pairwise prompting; mitigated by bidirectional training
  - Tie-breaking relies on cluster size; undefined behavior if tie persists
- First 3 experiments:
  1. Reproduce SC vs. WCT+PJudge vs. WCT+RJudge on BIRD dev with N=8 using Qwen2.5-Coder-7B
  2. Ablate cluster weighting: compare WCT (weighted) vs. CT (unweighted) vs. DRT (exhaustive pairwise)
  3. Measure judge call efficiency: log actual K and pairwise comparisons for N=16, 32, 64

## Open Questions the Paper Calls Out
The paper identifies three key open questions: whether linear weighting remains optimal as N scales beyond 64, whether the 7B judge can effectively discriminate candidates from significantly more capable generators (>32B parameters), and how performance degrades when no execution-equivalent SQL exists in the candidate pool.

## Limitations
- The execution-grounded clustering assumption may fail when execution-equivalent SQLs are semantically different in edge cases
- Performance gains may diminish with extremely high-capacity generators that the 7B judge cannot fully discriminate
- The system lacks a reject option for cases where all candidates are incorrect, potentially forcing selection when none exist

## Confidence
- High for the core claim that WCT + RJudge improves execution accuracy over SC and PJudge (supported by Table 1, Table 2, Table 4)
- Medium for the claim of cross-generator generalization (supported by Section 5.2, but limited to three generators)
- Medium for the efficiency claim (reduced comparisons) - Table 3 shows reduction, but real-world latency depends on cluster distribution

## Next Checks
1. Stress-test the clustering assumption: inject systematically incorrect but execution-equivalent SQLs and measure WCT's selection accuracy degradation
2. Evaluate RJudge on out-of-domain schemas (different domain/type distributions than BIRD) to quantify generalization limits
3. Benchmark WCT vs. SC as N increases to 64-128, measuring both accuracy gains and judge call efficiency to validate the claimed O(N²) → O(K²) scaling advantage