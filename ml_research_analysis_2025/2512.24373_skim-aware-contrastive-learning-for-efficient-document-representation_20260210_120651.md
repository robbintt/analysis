---
ver: rpa2
title: Skim-Aware Contrastive Learning for Efficient Document Representation
arxiv_id: '2512.24373'
source_url: https://arxiv.org/abs/2512.24373
tags:
- document
- chunk
- documents
- long
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-supervised contrastive learning framework
  called Chunk Prediction Encoder (CPE) to improve long document representation, particularly
  for legal and medical texts. The method randomly masks a document section and uses
  an NLI-style contrastive objective to align it with relevant parts while distancing
  it from unrelated ones, mimicking human skimming behavior.
---

# Skim-Aware Contrastive Learning for Efficient Document Representation

## Quick Facts
- **arXiv ID**: 2512.24373
- **Source URL**: https://arxiv.org/abs/2512.24373
- **Reference count**: 34
- **Primary result**: CPE with LegalBERT achieves 57.74 macro-F1 on ECHR and 62.69 macro-F1 on SCOTUS, outperforming contrastive learning baselines by 6-12% macro-F1

## Executive Summary
This paper introduces Chunk Prediction Encoder (CPE), a self-supervised contrastive learning framework that improves long document representation by mimicking human skimming behavior. CPE randomly masks document sections and uses an NLI-style contrastive objective to align masked chunks with relevant parts while distancing them from unrelated ones. Applied to both hierarchical transformers and Longformer architectures, CPE shows significant improvements over strong baselines including SimCSE and ESimCSE on legal and medical text datasets. The method achieves 57.74 macro-F1 on ECHR and 62.69 macro-F1 on SCOTUS when combined with LegalBERT, outperforming contrastive learning baselines by 6-12% macro-F1.

## Method Summary
CPE is a self-supervised contrastive learning framework that processes long documents by splitting them into fixed-size chunks (128 tokens by default). During pre-training, one chunk is randomly masked per document and the model learns to recognize whether a candidate chunk belongs to the masked document (positive) or another document (negative) using multiple negatives ranking loss. The framework operates over shared encoder outputs for each chunk, which are then aggregated via mean or max pooling. CPE can be applied to both hierarchical transformer architectures and Longformer models, with the encoder frozen during downstream classification using a simple MLP head. The method fine-tunes domain-specific pre-trained encoders like LegalBERT and ClinicalBioBERT, which are shown to be particularly effective for their respective domains.

## Key Results
- CPE with LegalBERT achieves 57.74 macro-F1 on ECHR and 62.69 macro-F1 on SCOTUS, outperforming SimCSE and ESimCSE by 6-12% macro-F1
- Chunk size ablation shows 128-token chunks are optimal, with degradation at 512 tokens (EURLEX: 29.29 vs 42.16 macro-F1)
- CPE performs well on short documents (BioASQ dataset) and produces more semantically coherent embeddings than SimCSE according to DBSCAN clustering metrics
- Domain-adapted encoders amplify CPE's effectiveness, with LegalBERT showing 5% improvement over BERT on SCOTUS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chunk-level contrastive prediction forces the encoder to learn cross-fragment semantic coherence without supervised labels.
- **Mechanism:** By randomly masking one chunk and training the model to recognize whether a candidate chunk belongs to the masked document (positive) or another document (negative), the encoder must compress both local chunk semantics and global document context into compatible representations. The multiple negatives ranking loss explicitly maximizes similarity between (document\chunk, chunk+) pairs while minimizing similarity with chunks from other documents in the same batch.
- **Core assumption:** Documents contain coherent internal structure where semantically related fragments can predict each other's presence (analogous to the human "skimming" intuition cited by authors).
- **Evidence anchors:**
  - [abstract] "randomly masks a section of the document and uses an NLI-based contrastive objective to align it with relevant parts while distancing it from unrelated ones"
  - [section 3, Learning process] "By doing so, we force the model to learn the dependencies between chunks and their relevance in representing a document"
  - [corpus] ScaleFormer (arXiv 2511.10029) similarly uses span-level representations for long-context tasks, suggesting span/chunk-level processing is a broader design pattern.
- **Break condition:** If documents are incoherent (e.g., randomly shuffled sentences) or chunks are too large (≥512 tokens), the predictive signal degrades—see ablation where 512-token chunks underperform 128-token chunks on EURLEX (29.29 vs 42.16 macro-F1).

### Mechanism 2
- **Claim:** Domain-adapted pre-trained encoders amplify CPE's effectiveness compared to general-purpose models.
- **Mechanism:** CPE fine-tunes an existing encoder's representations rather than training from scratch. When the base encoder (e.g., LegalBERT, ClinicalBioBERT) already captures domain-specific vocabulary and discourse patterns, the contrastive objective refines rather than builds these representations from scratch.
- **Core assumption:** The contrastive learning signal is sufficiently complementary to (not conflicting with) the pre-training objective.
- **Evidence anchors:**
  - [section 5, Evaluation of Hierarchical Representation] "In-domain knowledge appears to be particularly crucial for the SCOTUS dataset, resulting in a 5% improvement in macro F1 score of LegalBERT EmbeddingCPE MLP over BERT EmbeddingCPE MLP"
  - [section 6] "leveraging domain-specific pre-trained models like LegalBERT and ClinicalBioBERT within CPE further boosted performance"
  - [corpus] No direct corpus evidence on domain-adapted encoders + contrastive learning interaction; relationship is inferred from paper's experimental results only.
- **Break condition:** If domain pre-training and contrastive objectives misalign (e.g., applying ClinicalBioBERT to legal texts), gains diminish or reverse.

### Mechanism 3
- **Claim:** Chunk size governs the granularity of semantic abstraction, with 128 tokens providing an optimal balance for most datasets.
- **Mechanism:** Smaller chunks (64 tokens) may fragment semantic units, while larger chunks (256–512 tokens) dilute the contrastive signal by including irrelevant content. The 128-token chunk approximates a semantic "paragraph" level that preserves coherent meaning units while remaining discriminative.
- **Core assumption:** Semantic coherence in legal/medical texts clusters around ~128-token spans.
- **Evidence anchors:**
  - [section B, ablation] Table 6 shows 128 tokens yields best or near-best macro-F1 across ECHR (57.74), SCOTUS (62.69), EURLEX (42.16), with degradation at 512 tokens
  - [section 2, Related Work] "(Dai et al., 2022)... showed that better results were achieved by splitting the document into smaller chunks of 128 tokens"
  - [corpus] Weak evidence; no corpus papers directly address chunk size for contrastive document learning.
- **Break condition:** For datasets with very short average length (e.g., EURLEX at 1400 tokens), larger chunks may exceed meaningful content boundaries; for very long documents, too-small chunks may increase computational overhead without proportional gains.

## Foundational Learning

- **Contrastive Learning (NCE / InfoNCE family):**
  - Why needed here: CPE uses multiple negatives ranking loss, a variant of InfoNCE, to learn document representations by pulling positive pairs together and pushing negative pairs apart in embedding space.
  - Quick check question: Can you explain why increasing batch size generally improves contrastive learning with in-batch negatives?

- **Hierarchical Document Encoders:**
  - Why needed here: CPE operates over chunked documents processed by a shared encoder, then aggregated (mean/max pooling). Understanding how local representations compose into global ones is essential for debugging representation collapse.
  - Quick check question: What is the difference between mean-pooling and max-pooling over chunk embeddings, and when might one fail?

- **Natural Language Inference (NLI) as a training proxy:**
  - Why needed here: The authors frame chunk prediction as an "NLI-style" task—entailment (chunk belongs) vs. contradiction (chunk from different document)—even though labels are constructed automatically, not from human annotations.
  - Quick check question: How does using NLI-style objectives differ from using standard classification loss for document embedding learning?

## Architecture Onboarding

- **Component map:** Document → Chunker (split into 32 chunks of 128 tokens) → Shared Encoder (BERT/LegalBERT/ClinicalBioBERT) → Chunk embeddings → Aggregation (mean/max pooling) → Document representation → Contrastive head (pre-training) or MLP classifier (downstream)

- **Critical path:**
  1. Pre-processing: Truncate/pad to 4096 tokens → split into 32 chunks
  2. Pre-training: For each batch of N documents, sample one chunk per document as positive, use chunks from other documents as in-batch negatives → optimize contrastive loss (3 epochs, lr=2e-5)
  3. Freeze encoder, train MLP classifier on frozen embeddings (20 epochs)
  4. (Alternative) End-to-end fine-tuning: Replace simple pooling with 2-layer transformer encoder over chunks; train all parameters jointly

- **Design tradeoffs:**
  - **Hierarchical vs. Longformer:** Hierarchical models outperform Longformer on ECHR, SCOTUS, MIMIC; Longformer slightly better on shorter EURLEX. Use hierarchical for documents >3000 tokens.
  - **Frozen embeddings vs. end-to-end:** Frozen embeddings + MLP is faster and competitive; end-to-end fine-tuning yields ~5-8 macro-F1 additional gain but requires more compute.
  - **Chunk size:** 128 tokens is robust default; 64 tokens may help for shorter documents; avoid ≥256 for datasets with high semantic density per token.

- **Failure signatures:**
  - **Representation collapse:** All document embeddings cluster tightly regardless of class; check cosine similarity distribution across classes
  - **Overfitting to chunk position:** Model learns positional artifacts rather than semantic content; shuffle chunks during training
  - **Domain mismatch:** ClinicalBioBERT on legal texts underperforms LegalBERT; verify base encoder matches target domain

- **First 3 experiments:**
  1. **Baseline sanity check:** Train MLP classifier on frozen BERT-base embeddings (no contrastive pre-training) on ECHR to reproduce ~36 macro-F1; confirm CPE pre-training lifts this to ~54-57.
  2. **Chunk size ablation:** Run CPE with 64, 128, 256, 512 token chunks on a held-out validation split of your target dataset; plot macro-F1 vs. chunk size to confirm 128 is optimal or identify dataset-specific optimum.
  3. **Domain encoder comparison:** Compare BERT-base vs. domain-adapted encoder (LegalBERT for legal, ClinicalBioBERT for medical) with identical CPE training; quantify domain adaptation contribution (expect 3-5% macro-F1 delta per paper).

## Open Questions the Paper Calls Out
None

## Limitations
- **Hyperparameter Ambiguity**: The paper does not specify the hidden layer dimensions of the MLP classifier, which could significantly impact downstream performance.
- **Negative Sampling Strategy**: The paper mentions using in-batch negatives but does not clarify whether these are randomly sampled or if there is any mechanism for selecting "hard" negatives.
- **Domain Adaptation Generalization**: While the paper demonstrates strong performance with domain-specific encoders, it does not test the limits of this approach or explore what happens with mismatched domain encoders.

## Confidence
- **High Confidence**: The core mechanism of chunk-level contrastive learning (Mechanism 1) is well-supported by the paper's experimental results, particularly the ablation showing 128-token chunks outperform larger sizes across multiple datasets.
- **Medium Confidence**: The effectiveness of domain-adapted encoders (Mechanism 2) is demonstrated empirically, but the underlying reason (complementarity between contrastive objective and pre-training) is inferred rather than directly tested.
- **Medium Confidence**: The claim that 128 tokens is the optimal chunk size (Mechanism 3) is supported by ablation results, but the paper does not explore the theoretical basis for this choice or test it across a broader range of document types.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the MLP hidden layer dimensions and batch sizes to determine their impact on downstream macro-F1 scores. This would help identify whether the reported improvements are robust to architectural choices.

2. **Cross-Domain Encoder Evaluation**: Test CPE with mismatched domain encoders (e.g., ClinicalBioBERT on legal texts) to quantify the cost of domain mismatch and explore whether the contrastive objective can compensate for it.

3. **Chunk Size vs. Document Length Interaction**: Conduct experiments where chunk size is varied as a function of document length to determine if there is an optimal chunk size that scales with document size, rather than using a fixed 128 tokens for all documents.