---
ver: rpa2
title: 'Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking
  and LLM Content Safety'
arxiv_id: '2505.04146'
source_url: https://arxiv.org/abs/2505.04146
tags:
- prompts
- prompt
- image
- jailbreak
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Unmasking the Canvas (UTC Benchmark; UTCB),
  a dynamic benchmark dataset to evaluate LLM vulnerability in image generation tasks.
  The authors observed that existing image generation models remain vulnerable to
  prompt-based jailbreaks, with even short, natural prompts leading to the generation
  of compromising images.
---

# Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety

## Quick Facts
- arXiv ID: 2505.04146
- Source URL: https://arxiv.org/abs/2505.04146
- Reference count: 3
- Key outcome: Introduces UTCB, a dynamic benchmark showing image generation models remain vulnerable to jailbreak prompts, with obfuscation techniques and structured templates bypassing safety filters

## Executive Summary
This paper introduces Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic benchmark dataset to evaluate LLM vulnerability in image generation tasks. The authors observed that existing image generation models remain vulnerable to prompt-based jailbreaks, with even short, natural prompts leading to the generation of compromising images. To address this, they developed a scalable pipeline combining structured prompt engineering, multilingual obfuscation (Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3 models. The benchmark includes over 6,700 image-generation prompts curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. Results showed that certain prompt templates like Split_Image were particularly effective at bypassing safety filters, while Zulu and Gaelic obfuscation techniques reduced model-assigned risk scores, indicating potential bypass methods. The work highlights both the vulnerability of current models and the risk of adversarial use for large-scale attacks, while proposing responsible access controls and continuous dataset evolution.

## Method Summary
The authors developed a pipeline using Groq-hosted LLaMA-3 models to generate, evaluate, and classify image-generation jailbreak prompts. Starting from JAILBREAKHUB seed prompts, they used automated prompt generation to create variants, applied multilingual obfuscation (Zulu, Gaelic, Base64), and pre-filtered prompts using an image mimicker model. A judge model assigned risk scores (0-1) and PASS/BLOCK tags. The resulting dataset was organized into Bronze (untested), Silver (LLM-verified), and Gold (manually verified) tiers, with Gold requiring human review for highest confidence. The approach emphasizes scalability and responsible access controls while highlighting vulnerabilities in current safety mechanisms.

## Key Results
- Split_Image template most effective at bypassing safety filters, likely due to sentiment analysis mechanisms
- Zulu and Gaelic obfuscation reduced model-assigned risk scores, exposing multilingual bypass risks
- Automated prompt generation enables scalable attack surface discovery with minimal effort
- Over 6,700 prompts organized across three verification tiers for responsible disclosure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual obfuscation bypasses safety filters by creating task confusion between translation and content evaluation.
- Mechanism: When prompts are submitted in low-resource languages (Zulu, Gaelic) or encoded formats (Base64), models attempt to process both translation and content generation simultaneously. Safety training data is linguistically unequal, creating coverage gaps the model cannot adequately evaluate.
- Core assumption: Safety filters are primarily trained on English-language harmful content patterns and do not generalize cross-lingually.
- Evidence anchors:
  - [abstract]: "Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64)..."
  - [Section 4.4]: "Zulu and Gaelic prompts had a low model-assigned risk score, further exposing the risks of such obfuscation techniques to bypass model defenses."
  - [corpus]: Neighbor papers (GhostPrompt, Reason2Attack) confirm semantic-level detection remains vulnerable to non-standard encodings; no contradictory evidence found.
- Break condition: If models implement separate translation pipelines before safety evaluation, or if safety training expands to include multilingual harmful content, this mechanism degrades.

### Mechanism 2
- Claim: Split-image templates exploit sentiment analysis filters by embedding malicious requests within seemingly balanced framing.
- Mechanism: Prompts structured as "Generate an image of [X], on the right, the opposite" present a superficially neutral or educational framing. Modifying the trailing phrase to "on the right, the same being empowered" introduces positive sentiment that masks the malicious core. Models apply sentiment-based filtering that fails to detect the harmful intent.
- Core assumption: Models rely partially on sentiment signals rather than deep semantic understanding of image generation requests.
- Evidence anchors:
  - [Section 4.3]: "Split_Image seems to be the most harmful method of attack... This might be due to the fact that the models apply some sort of sentiment analysis filters to analyze user prompts and mask model output."
  - [corpus]: Metaphor-based Jailbreaking Attacks paper confirms framing-based attacks bypass semantic detection; weak direct evidence for sentiment-specific mechanism in corpus.
- Break condition: If models implement intent classification independent of surface sentiment, or explicitly flag comparative generation requests for review, this approach becomes less effective.

### Mechanism 3
- Claim: Automated prompt generation scales attack surface discovery without manual effort.
- Mechanism: An attacker LLM iteratively generates jailbreak variants from seed prompts, pruning unsuccessful candidates. This reduces the cost and expertise barrier for discovering vulnerabilities, enabling systematic probing of model defenses across categories and obfuscation types.
- Core assumption: LLMs can effectively generate syntactically valid jailbreak prompts without human intervention.
- Evidence anchors:
  - [Section 3.1]: "We used the prompts from this dataset and asked a Llama model to generate prompts and categories such as Privacy Violation, Violence, or General Maliciousness fine tuned for image-based jailbreaks."
  - [Section 4.5]: "Attackers could use LLM models to curate a huge number of prompts with no cost and minimal effort, all they have to do is some smart prompt engineering."
  - [corpus]: Tree of Attacks (TAP) paper (cited in Related Work) demonstrates automated black-box jailbreak refinement; corpus confirms this is an established attack pattern.
- Break condition: If API rate limits, prompt logging, or behavioral analysis detect automated generation patterns, scalability is constrained.

## Foundational Learning

- Concept: **Jailbreak taxonomy (prompt injection vs. template-based vs. obfuscation)**
  - Why needed here: The UTCB dataset organizes attacks by type and category. Understanding the distinction helps interpret which defenses fail and why.
  - Quick check question: What is the structural difference between an "ethical_override" prompt and a "split_image" prompt in this dataset?

- Concept: **Black-box vs. white-box adversarial testing**
  - Why needed here: The paper explicitly operates in a black-box setting (API-only access). This constrains attack strategies to input-output observation without model internals.
  - Quick check question: Why does the paper rely on automated prompt generation rather than gradient-based optimization?

- Concept: **Benchmark tiering (Bronze/Silver/Gold)**
  - Why needed here: UTCB uses verification tiers to manage quality and responsible disclosure. Silver relies on LLM-aided verification; Gold requires human review.
  - Quick check question: What metadata differentiates a Silver-tier prompt from a Gold-tier prompt?

## Architecture Onboarding

- Component map:
  - Seed prompts from JAILBREAKHub -> Prompt Generator (Llama on Groq) -> obfuscated variants
  - Variants -> Image Mimicker (Llama) -> filter STRAIGHT-DENIALS / DENIALS / JAILBREAK
  - Filtered prompts -> Judge Model (Llama) -> risk scoring
  - High-priority prompts -> Annotation Interface (access-controlled) -> Gold-tier verification
  - Storage (private Huggingface) with rich metadata

- Critical path:
  1. Seed prompts from JAILBREAKHub -> Prompt Generator -> obfuscated variants
  2. Variants -> Image Mimicker -> filter STRAIGHT-DENIALS / DENIALS / JAILBREAK
  3. Filtered prompts -> Judge Model -> risk scoring
  4. High-priority prompts -> Annotation Interface -> Gold-tier verification

- Design tradeoffs:
  - Using Llama as mimicker reduces cost but may not reflect actual image generator behavior
  - Free-tier Groq API introduces token limits and failed responses (acknowledged limitation)
  - Manual verification provides ground truth but does not scale; model used (ChatGPT vs. Grok) was not logged

- Failure signatures:
  - API token limit errors during large prompt generation
  - Mimicker producing unexpected tags when balancing translation vs. generation tasks
  - Judge model scoring inconsistent with manual annotation (e.g., Ethical_Override scored high-risk but defended well in practice)

- First 3 experiments:
  1. Replicate the Base64 obfuscation test on a target model: encode a known-harmful prompt, submit, and compare generation rate against plaintext control.
  2. Test the split-image template with modified trailing phrases ("opposite" vs. "empowered") to validate sentiment-filter hypothesis.
  3. Run the Judge Model against a held-out subset of Gold-tier prompts to measure correlation between automated risk scores and human-verified jailbreak success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the automated evaluation pipeline change when applied to non-LLaMA family models?
- Basis in paper: [explicit] The authors state in Future Work, "We would also like to test our pipeline on non-Llama family models to analyze how models behave differently."
- Why unresolved: The current experimental setup relied exclusively on the LLaMA architecture for the judge, mimicker, and generator components.
- What evidence would resolve it: Evaluation results from running the UTCB dataset and judge model against alternative architectures (e.g., GPT-4, Claude).

### Open Question 2
- Question: Does the text-based "Image Mimicker" correlate accurately with the refusal rates and behaviors of actual image generation models?
- Basis in paper: [inferred] The authors note that using LLaMA as a mimicker "may have limited the credibility of evaluations, as whether image generators follow the same patterns is something we have not tested."
- Why unresolved: The pipeline substitutes actual image generation (costly) with a text model predicting generation (cost-effective), but the equivalence of these behaviors is unverified.
- What evidence would resolve it: A comparative study measuring the refusal rate gap between the LLaMA mimicker predictions and actual outputs from models like DALL-E or Stable Diffusion.

### Open Question 3
- Question: What is the vulnerability profile of models when subjected to multi-turn in-context attacks (e.g., requiring a second "Yes" prompt)?
- Basis in paper: [explicit] The authors state in Limitations, "this paper did not consider many in-context examples, i.e., scenarios which required a second Yes/Proceed prompt... eventually leading to the generation of malicious image content."
- Why unresolved: The dataset focuses on zero-shot or immediate responses, ignoring jailbreak strategies that rely on conversational buildup.
- What evidence would resolve it: Extending the UTCB dataset to include multi-turn conversational data and measuring the delta in jailbreak success rates.

## Limitations
- Key system prompts, exact model variants, and translation service specifications are not provided, affecting reproducibility
- Use of Llama as image mimicker rather than actual image generation models may introduce performance discrepancies
- Risk scoring mechanism relies on single judge model without validation against human-verified outcomes across all tiers

## Confidence
- **High Confidence**: The vulnerability of image generation models to structured prompt templates like Split_Image and obfuscation methods like Base64 encoding
- **Medium Confidence**: The hypothesis that Zulu and Gaelic obfuscation reduce model-assigned risk scores
- **Low Confidence**: The scalability of automated prompt generation via LLM models for large-scale jailbreak discovery

## Next Checks
1. Replicate the Base64 obfuscation test on a target image generation model: encode a known-harmful prompt, submit, and compare generation success rate against plaintext control to isolate obfuscation impact.
2. Test the Split_Image template with modified trailing phrases ("opposite" vs. "empowered") to validate whether sentiment filtering is the mechanism for bypassing safety controls.
3. Run the Judge Model against a held-out subset of Gold-tier prompts to measure correlation between automated risk scores and human-verified jailbreak success, assessing reliability of the scoring system.