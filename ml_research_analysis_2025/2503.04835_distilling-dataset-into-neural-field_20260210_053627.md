---
ver: rpa2
title: Distilling Dataset into Neural Field
arxiv_id: '2503.04835'
source_url: https://arxiv.org/abs/2503.04835
tags:
- ddif
- dataset
- synthetic
- neural
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DDiF, a novel dataset distillation method
  that leverages neural fields to efficiently store and decode high-dimensional grid-based
  data. By parameterizing synthetic instances as neural fields, DDiF overcomes limitations
  of existing methods in scalability and resolution adaptiveness.
---

# Distilling Dataset into Neural Field

## Quick Facts
- **arXiv ID:** 2503.04835
- **Source URL:** https://arxiv.org/abs/2503.04835
- **Authors:** Donghyeok Shin; HeeSun Bae; Gyuwon Sim; Wanmo Kang; Il-Chul Moon
- **Reference count:** 40
- **Key outcome:** Introduces DDiF, a dataset distillation method using neural fields that overcomes scalability and resolution adaptiveness limitations of existing methods

## Executive Summary
This paper presents DDiF (Dataset Distillation with Neural Fields), a novel approach to dataset distillation that leverages coordinate-conditioned neural networks to store high-dimensional grid-based data efficiently. By parameterizing synthetic instances as neural fields, DDiF achieves resolution-agnostic storage where the network size remains constant regardless of the data resolution. The method demonstrates superior performance across various modalities including images, videos, audio, and 3D voxels, while also exhibiting strong cross-resolution generalization and robustness to data corruption.

## Method Summary
DDiF represents synthetic data instances as neural fields - coordinate-conditioned MLPs with sine activations (SIRENs) that map coordinate grids to data values. Unlike traditional methods that optimize discrete tensors, DDiF optimizes the weights of these neural networks. The approach involves a warm-up phase where fields are initialized by regressing onto random real images, followed by a distillation phase where the field weights are optimized using standard dataset distillation objectives like Trajectory Matching. During decoding, synthetic instances are generated by querying the neural field with coordinate grids, enabling resolution-agnostic generation.

## Key Results
- DDiF consistently achieves superior performance across image, video, audio, and 3D voxel modalities
- Demonstrates strong cross-resolution generalization, maintaining performance across 128x128 to 512x512 resolutions
- Shows robustness to data corruption while using fewer parameters than competing methods
- Theoretical analysis proves DDiF has greater expressiveness than prior methods under the same parameter budget

## Why This Works (Mechanism)

### Mechanism 1: Coordinate-Conditioned Storage Efficiency
- **Claim:** Neural fields decouple parameter budget from spatial resolution, improving storage efficiency for high-dimensional inputs
- **Core assumption:** Information for downstream tasks can be approximated by continuous functions over coordinate space
- **Evidence:** Theorem 3.2 shows larger feasible space under same budget; Figure 3 demonstrates lower reconstruction error

### Mechanism 2: Feasible Space Expansion via Sinusoidal Networks
- **Claim:** SIREN parameterization allows larger representable space compared to static frequency methods
- **Core assumption:** Larger feasible space correlates with lower dataset distillation loss
- **Evidence:** Theoretical proof of larger feasible space; empirical performance gains at lower budgets

### Mechanism 3: Resolution-Agnostic Decoding
- **Claim:** Continuous function storage enables cross-resolution generalization
- **Core assumption:** Neural field learns underlying continuous signal rather than overfitting to discretization
- **Evidence:** Figure 5a shows least performance decrease across resolution changes

## Foundational Learning

- **Implicit Neural Representations (Neural Fields)**
  - **Why needed:** Core building block of DDiF - understand how MLP maps coordinates to values
  - **Quick check:** For a 2D image, what are inputs and outputs of the approximating neural network?

- **Dataset Distillation Objectives (Matching)**
  - **Why needed:** DDiF is a parameterization method requiring external loss function
  - **Quick check:** Does DDiF define how to match data distribution or what format synthetic data takes?

- **Spectral Bias**
  - **Why needed:** SIRENs avoid spectral bias that prevents standard ReLU networks from capturing high frequencies
  - **Quick check:** Why might ReLU MLP fail to reconstruct sharp edges compared to Sine-activated MLP?

## Architecture Onboarding

- **Component map:** Coordinate Grid -> SIREN MLP -> Synthetic Instance -> Distillation Loss -> Field Weights Update
- **Critical path:**
  1. Warm-up: Initialize fields by regressing onto random real images
  2. Forward: Generate coordinate grid → Forward through F_ψ → Reshape to synthetic image
  3. Loss: Calculate distillation loss between real and synthetic features
  4. Backprop: Update ψ (field weights) to minimize loss

- **Design tradeoffs:** Width vs. Capacity (increasing MLP width increases expressiveness per instance but reduces IPC); Budget Allocation (DDiF uses decoder-only budget)
- **Failure signatures:** Convergence to mean (gray blocks); Jagged/noisy outputs (unnormalized coordinates or high learning rate)
- **First 3 experiments:**
  1. Overfit sanity check: Distill single class into single neural field with large budget
  2. Resolution scaling: Train at 128x128, evaluate at 512x512 to validate upscaling
  3. Ablation on activation: Replace Sine with ReLU and measure drop in test accuracy

## Open Questions the Paper Calls Out

- **Open Question 1:** How to construct theoretical framework comparing expressiveness under fixed entire storage budget rather than per-instance budget?
- **Open Question 2:** Can DDiF be extended with modulation or conditional codes to store shared intra- and inter-class information?
- **Open Question 3:** How to analyze or modify neural field structure for improved efficiency on low-dimensional datasets?

## Limitations
- Parameter budget allocation between neural field weights and expert networks lacks theoretical framework
- Practical implementation details for multi-instance representation and memory management are unclear
- Validation depth varies significantly across modalities, with sparse results for 3D voxels and audio

## Confidence

- **High Confidence:** Core mechanism of coordinate-conditioned neural fields for resolution-agnostic storage (supported by theoretical analysis and empirical evidence)
- **Medium Confidence:** Superior expressiveness under same parameter budget (supported by Figure 3 but relies on theoretical assumptions)
- **Low Confidence:** Practical scalability claims for video and 3D modalities (primarily supported by relative performance metrics)

## Next Checks

1. **Budget Allocation Sensitivity:** Systematically vary IPC and MLP width while keeping total parameters constant to determine optimal allocation strategy
2. **Cross-Resolution Fidelity Analysis:** Quantitatively measure frequency spectrum preservation when decoding at different resolutions
3. **Multi-Instance Training Efficiency:** Implement and benchmark batching strategy for multiple neural fields to measure memory overhead and convergence characteristics