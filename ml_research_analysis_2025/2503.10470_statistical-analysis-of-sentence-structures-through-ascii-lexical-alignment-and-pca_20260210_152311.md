---
ver: rpa2
title: Statistical Analysis of Sentence Structures through ASCII, Lexical Alignment
  and PCA
arxiv_id: '2503.10470'
source_url: https://arxiv.org/abs/2503.10470
tags:
- text
- test
- ascii
- normality
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel statistical approach to assess sentence
  structure balance in natural language text using ASCII codes and Principal Component
  Analysis (PCA). By mapping 17 lexical categories to ASCII, compressing them via
  PCA, and computing dot products with compressed sentence representations, the method
  evaluates lexical alignment without traditional syntactic tools.
---

# Statistical Analysis of Sentence Structures through ASCII, Lexical Alignment and PCA

## Quick Facts
- arXiv ID: 2503.10470
- Source URL: https://arxiv.org/abs/2503.10470
- Authors: Abhijeet Sahdev
- Reference count: 20
- One-line primary result: ASCII-based PCA detects lexical balance in four of eleven text corpora; LLM-generated and movie review texts fail normality tests.

## Executive Summary
This study introduces a novel statistical approach to assess sentence structure balance in natural language text using ASCII codes and Principal Component Analysis (PCA). By mapping 17 lexical categories to ASCII, compressing them via PCA, and computing dot products with compressed sentence representations, the method evaluates lexical alignment without traditional syntactic tools. Normality tests (Shapiro-Wilk and Anderson-Darling) reveal that four out of eleven diverse text corpora—including blogs and articles—exhibit balanced sentence structures (normality confirmed), while movie review datasets and LLM-generated text fail these tests. The Grok-generated story shows near-normality in histograms but fails statistical tests, suggesting limited lexical variety in sentence lengths. The findings demonstrate that ASCII-based PCA offers a resource-efficient complement to syntactic analysis for high-level text quality and style evaluation.

## Method Summary
The method converts 17 lexical category names (e.g., noun, verb) into ASCII codes and compresses them to a 1D vector via PCA. Sentences are similarly converted to ASCII and compressed to 17 dimensions via PCA. Dot products between sentence vectors and the lexical category vector quantify structural alignment. The distribution of these dot products across sentences is tested for normality using Shapiro-Wilk and Anderson-Darling tests; normality indicates "balanced" sentence structure. The approach is applied to 11 diverse text corpora including blogs, articles, movie reviews, and LLM-generated text.

## Key Results
- Four of eleven text corpora (2 blogs, 3 articles) passed normality tests, indicating balanced sentence structures
- Movie review datasets and LLM-generated text failed normality tests
- Grok-generated story showed visual bell curve but failed statistical tests
- All corpora except Grok story exhibited significant correlations in dot product distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting text to ASCII codes provides a sufficient low-dimensional numerical basis for detecting structural patterns without traditional syntactic tools.
- **Mechanism:** Text characters are mapped to their integer ASCII values. These sequences are treated as feature vectors, transforming the problem from linguistic to statistical. PCA then compresses these vectors, isolating dominant patterns of character distribution as principal components.
- **Core assumption:** The statistical distribution of character codes correlates with underlying lexical structure and style, rather than being dominated by noise.
- **Evidence anchors:**
  - [abstract] "proposes a novel statistical method that uses American Standard Code for Information Interchange (ASCII) codes to represent text... and their lexical category alignment after using their compressed versions through PCA"
  - [section III] "text corpora... were sent through a pre-processing pipeline to extract sentences in ASCII codes."
  - [corpus] Weak direct support; neighboring papers use complex embeddings, making this paper's simplified approach a distinct, unproven proposition.
- **Break condition:** Fails if ASCII variance is driven primarily by topic-specific vocabulary or noise, obscuring structural signals.

### Mechanism 2
- **Claim:** Dot products between PCA-compressed sentences and a PCA-compressed "lexical category" vector quantify structural alignment.
- **Mechanism:** Seventeen lexical category names (e.g., "noun", "verb") are converted to ASCII and PCA-reduced to 1 dimension to form a reference vector $\bar{K}$. Sentences are converted to ASCII and PCA-reduced to 17 dimensions to form vectors $\bar{J}$. The dot product $\bar{J} \cdot \bar{K}$ is computed. The hypothesis is that this measures alignment with the structural template implied by $\bar{K}$.
- **Core assumption:** The dot product between a sentence's principal components and the categories' principal components is a meaningful proxy for lexical "balance."
- **Evidence anchors:**
  - [abstract] "their lexical category alignment after using their compressed versions through PCA"
  - [section III] "Compressed values are recorded here and they act as vector K... The dot product was computed and analyzed"
  - [corpus] No external support found for this specific dot-product-on-ASCII-PCA mechanism.
- **Break condition:** Fails if the PCA reduction eliminates variance related to balance, or if the geometric relationship between $\bar{J}$ and $\bar{K}$ is mathematically spurious.

### Mechanism 3
- **Claim:** Statistical normality of the alignment-score distribution serves as a valid proxy for text quality and structural "balance."
- **Mechanism:** The distribution of dot products is tested for normality using Shapiro-Wilk and Anderson-Darling tests. A p-value > 0.05 leads to accepting the null hypothesis of normality, which is interpreted as "balanced sentence structure."
- **Core assumption:** High-quality text tends to produce a normal distribution of this specific metric, while skewed or multi-modal distributions indicate imbalance or limited lexical variety.
- **Evidence anchors:**
  - [abstract] "Normality tests... reveal that four out of eleven diverse text corpora... exhibit balanced sentence structures"
  - [section V] "Blogs and articles... clear these normality tests indicating that their structural content in terms of the usage of lexical resources is well balanced."
  - [corpus] Indirect support from 'Comparing human and LLM proofreading...', which shows LLM and human texts have distinguishable features.
- **Break condition:** Fails if normality is linked to confounding factors (e.g., sentence length uniformity) rather than quality. The paper notes LLM text failed tests despite a visual bell curve, suggesting brittleness.

## Foundational Learning

- **Concept:** Principal Component Analysis (PCA)
  - **Why needed here:** The core transformation. The paper relies on PCA to reduce ASCII vectors and create the alignment space.
  - **Quick check question:** If the cumulative explained variance for the chosen number of components was 0.5 instead of ~1.0, how would that affect confidence in the results?

- **Concept:** ASCII Encoding
  - **Why needed here:** The chosen data representation. It's a simplistic mapping compared to embeddings, and understanding its limitations is crucial.
  - **Quick check question:** How would the ASCII sequence for "Hello" vs. "hello" differ, and what impact might this case sensitivity have on a statistical model?

- **Concept:** Normality Tests (Shapiro-Wilk, Anderson-Darling)
  - **Why needed here:** The evaluation criteria. The conclusion rests entirely on passing or failing these tests.
  - **Quick check question:** A large dataset (N > 1000) fails the Shapiro-Wilk test with a tiny p-value. Does this guarantee the underlying text is "unbalanced," or could it be a property of the test itself?

## Architecture Onboarding

- **Component map:** 17 lexical categories + raw text corpora -> ASCII Encoding -> PCA Compression -> Alignment (dot product) -> Evaluation (normality tests)

- **Critical path:** The alignment step. The validity of the entire pipeline depends on whether the dot product between a 1D category vector and a 17D sentence vector is a meaningful operation.

- **Design tradeoffs:**
  - **Pros:** Computationally cheap, no heavy NLP models, highly interpretable statistical outputs
  - **Cons:** ASCII is semantically lossy; the theoretical link between "normality" and "quality" is weak; results appear sensitive to corpus properties (e.g., sentence length)

- **Failure signatures:**
  - Submodular distributions: Histograms with multiple peaks (Fig 4f-4i)
  - Uniformity in LLM text: Lacking long sentences (Fig 5e), causing "near-normality" visually but statistical failure
  - Sensitivity to sentence length variance

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the pipeline on the provided "Blog ia" and "Reviews" datasets to replicate the pass/fail normality results
  2. **Ablate Dimensionality:** Change the sentence PCA reduction dimension (e.g., from 17 to 5 or 30) and observe the effect on normality test outcomes
  3. **Control for Length:** Normalize sentences by length or filter for specific length ranges to test if the "balance" signal is independent of sentence length

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical link between ASCII variance and lexical structure is largely unproven
- Normality of dot products may reflect sentence length uniformity rather than genuine lexical harmony
- Results appear sensitive to corpus properties, particularly sentence length distribution

## Confidence
- **High:** Computational pipeline reproducibility
- **Medium:** Descriptive findings about which corpora passed/failed tests
- **Low:** Core claims that ASCII+PCA dot products meaningfully measure lexical balance or that normality equates to text quality

## Next Checks
1. **Baseline Reproduction:** Replicate normality test results on the provided "Blog ia" and "Reviews" datasets to verify the pipeline produces expected pass/fail outcomes
2. **Dimensionality Sensitivity:** Vary the sentence PCA reduction dimension (e.g., 5, 17, 30) and observe whether normality test outcomes remain consistent or shift dramatically
3. **Control for Sentence Length:** Normalize or filter sentences by length to test if the "balance" signal persists independent of sentence length distribution effects