---
ver: rpa2
title: BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and
  Reasoning
arxiv_id: '2507.15717'
source_url: https://arxiv.org/abs/2507.15717
tags:
- belo
- reasoning
- were
- ophthalmology
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BELO (BEnchmarking LLMs for Ophthalmology),
  a comprehensive benchmark dataset for evaluating large language models (LLMs) in
  ophthalmology. BELO consists of 900 high-quality, expert-reviewed multiple-choice
  questions (MCQs) aggregated from five medical datasets (BCSC, BioASQ, MedMCQA, MedQA,
  and PubMedQA).
---

# BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning

## Quick Facts
- arXiv ID: 2507.15717
- Source URL: https://arxiv.org/abs/2507.15717
- Reference count: 0
- Six LLMs evaluated on 900 expert-reviewed MCQs, with OpenAI o1 achieving highest accuracy (0.882) and macro-F1 (0.890)

## Executive Summary
This study introduces BELO (BEnchmarking LLMs for Ophthalmology), a comprehensive benchmark dataset for evaluating large language models in ophthalmology. BELO consists of 900 high-quality, expert-reviewed multiple-choice questions aggregated from five medical datasets, with all questions accompanied by gold-standard reasoning explanations. The study evaluates six LLMs using accuracy, macro-F1, and five text-generation metrics, finding that reasoning-focused models like OpenAI o1 outperform standard LLMs, though clinical reasoning quality remains suboptimal.

## Method Summary
The study creates BELO by extracting ophthalmology questions from five sources (BCSC, BioASQ, MedMCQA, MedQA, PubMedQA) using keyword matching and fine-tuned PubMedBERT, then subjecting them to multi-stage expert review by 13 ophthalmologists. Questions flagged as non-ophthalmological or duplicates (510 items) are replaced, and reasoning explanations are curated. Six LLMs are evaluated zero-shot via API using standardized prompts, with performance measured through accuracy, macro-F1 on 872 four-option questions, five text-generation metrics (ROUGE-L, BERTScore, BARTScore, METEOR, AlignScore), and qualitative expert review on a 5-point Likert scale.

## Key Results
- OpenAI o1 achieved highest accuracy (0.882) and macro-F1 (0.890) among evaluated models
- GPT-4o performed best in qualitative expert evaluations for readability despite lower accuracy
- Text-generation metrics showed suboptimal performance (scores ranging 20.40-71.80/100), indicating reasoning limitations
- Gemini 1.5 Pro performed poorly with accuracy of only 0.596

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage expert review produces higher-quality benchmark items than automated extraction alone. Keyword matching and fine-tuned PubMedBERT achieve high sensitivity (0.937) but generate false positives; human experts then filter non-ophthalmological QAs, remove duplicates, and upgrade low-quality reasoning explanations. Of 900 items, 510 were flagged and replaced. Core assumption: Expert physicians can reliably identify clinical reasoning gaps that automated methods miss. Evidence anchors: Abstract confirms rigorous multi-stage expert checking; section details 510 items flagged and replaced. Break condition: If expert reviewers introduce systematic bias or if ground-truth reasonings disagree with clinical consensus, benchmark validity degrades.

### Mechanism 2
Reasoning-focused LLMs (o1, DeepSeek-R1) outperform standard LLMs on accuracy in ophthalmology QA. Models with "thinking" capabilities generate intermediate reasoning steps before answering, improving performance on complex clinical questions. o1 achieved 0.882 accuracy vs. GPT-4o's 0.831. Core assumption: Extended inference time enables better medical reasoning rather than just superficial pattern matching. Evidence anchors: Abstract shows o1 achieved highest accuracy (0.882) and macro-F1 (0.890); section confirms o1 demonstrated best performance. Break condition: If performance gains derive from training data contamination rather than genuine reasoning improvements, benchmark validity is compromised.

### Mechanism 3
Text-generation metrics reveal reasoning limitations that accuracy alone obscures. Five metrics (ROUGE-L, BERTScore, BARTScore, METEOR, AlignScore) measure semantic similarity, factual consistency, and fluency between model outputs and gold-standard explanations. Low scores (0.20-0.72 range) indicate reasoning gaps despite high accuracy. Core assumption: Ground-truth reasonings written by ophthalmologists represent optimal clinical explanations. Evidence anchors: Abstract notes models' performance on text-generation metrics was generally suboptimal; section shows GPT-4o (0.204) emerged joint-best with o3-mini (0.203) yet both far below ceiling. Break condition: If metrics fail to correlate with clinical utility or if gold-standard explanations are incomplete, metric interpretation becomes unreliable.

## Foundational Learning

- Concept: **Macro-F1 vs. Accuracy**
  - Why needed here: Accuracy measures correct predictions; macro-F1 balances precision and recall across classes, important when answer distributions are imbalanced.
  - Quick check question: If a model gets 88% accuracy but favors one answer option disproportionately, which metric would reveal this bias?

- Concept: **Zero-shot Evaluation**
  - Why needed here: BELO evaluates models without task-specific examples, testing generalization rather than memorization.
  - Quick check question: Why does BELO remain a "hold-out, evaluation-only benchmark" rather than releasing for training?

- Concept: **Text-Generation Metrics Suite**
  - Why needed here: Each metric captures different aspectsâ€”ROUGE-L for lexical overlap, BERTScore for semantic similarity, AlignScore for factual consistency.
  - Quick check question: Why might a model score high on accuracy but low on AlignScore?

## Architecture Onboarding

- Component map:
  Data Sources (BCSC, BioASQ, MedMCQA, MedQA, PubMedQA) -> Extraction Layer (Keyword matching + fine-tuned PubMedBERT) -> Expert Review Pipeline (Manual QC -> reasoning amendment -> senior adjudication) -> Evaluation Layer (Accuracy, macro-F1, 5 text-generation metrics, qualitative expert review) -> Output (Public leaderboard at belo-dataset.vercel.app)

- Critical path:
  1. Extract ophthalmology QAs from source datasets
  2. Sample proportionally (900 items total, with oversampling for underrepresented sources)
  3. Expert review -> replace flagged items -> amend reasoning
  4. Run zero-shot evaluation via API for each model
  5. Compute metrics + statistical comparisons (Bonferroni-corrected)

- Design tradeoffs:
  - 900 questions: Balances content diversity against expert review cost
  - MCQ-only format: Excludes visual QA (planned for future phases)
  - Hold-out dataset: Ensures fair comparison but limits training utility
  - Multi-source aggregation: Broader coverage but heterogeneous question styles

- Failure signatures:
  - High accuracy + low text-generation scores -> model guessing without genuine reasoning
  - Gemini 1.5 Pro's 0.596 accuracy -> potential domain adaptation failure
  - High false-positive rate in extraction -> wasted expert review effort

- First 3 experiments:
  1. Reproduce baseline metrics on BELO using a standard model (GPT-4o) to validate evaluation pipeline
  2. Ablate individual text-generation metrics to identify which best correlates with expert qualitative ratings
  3. Test a reasoning-focused model (o1 or DeepSeek-R1) with varied prompt formats to measure prompt sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
How will multimodal vision-language models perform when BELO is expanded to include visual question-answering tasks using ophthalmological images? Basis in paper: The conclusion states "Future BELO benchmarking efforts will be expanded to include vision LLMs," and the discussion notes plans to "build upon our existing visual-language benchmark, the Large Multimodal Ophthalmology Dataset (LMOD) to incorporate visual question-answering items that evaluate multimodal reasoning." Why unresolved: The current BELO benchmark is text-only; multimodal evaluation requires integrating image data and developing appropriate metrics for visual reasoning. What evidence would resolve it: Evaluation results from vision-language models on an expanded BELO benchmark that includes retinal images, slit-lamp photographs, or OCT scans paired with clinical questions.

### Open Question 2
Can LLMs demonstrate comparable performance on real-world longitudinal clinical cases with multi-visit follow-ups and management planning compared to examination-style MCQs? Basis in paper: The discussion acknowledges that "BELO has insufficient questions from real-world clinical cases" and states that "questions reflecting real-world clinical cases with multiple follow ups and corresponding management plans will be included in our future works to enhance clinical fidelity." Why unresolved: Current BELO questions are derived primarily from board examinations and may not reflect the complexity, ambiguity, or temporal dynamics of actual patient care. What evidence would resolve it: Comparative benchmarking results using newly curated case-based questions that simulate multi-visit clinical scenarios with evolving patient data.

### Open Question 3
What accounts for the observed discrepancy between high accuracy (~88%) and suboptimal text-generation metric scores, and how can clinical reasoning quality be systematically improved? Basis in paper: The authors note that "models' performance on text-generation metrics was generally suboptimal, with scores ranging from 20.40 to 71.80 (out of 100)... indicating room for improvement in clinical reasoning." Why unresolved: The study establishes the gap but does not investigate whether it stems from evaluation metric limitations, model architecture constraints, or training data deficiencies. What evidence would resolve it: Ablation studies comparing different model architectures, fine-tuning strategies on ophthalmology corpora, or development of reasoning-specific evaluation metrics validated against expert judgments.

## Limitations

- Dataset hold-out status prevents independent verification; reproduction requires author access or proxy reconstruction
- Heterogeneous source datasets (varying styles, difficulty) may confound model performance interpretation
- Expert review process, while rigorous, introduces potential human bias in QA selection and reasoning curation
- Limited to multiple-choice format without visual components, constraining real-world clinical applicability

## Confidence

- **High Confidence**: BELO's multi-source aggregation and expert review methodology for creating clinically relevant benchmark items
- **Medium Confidence**: Claim that reasoning-focused LLMs (o1, DeepSeek-R1) outperform standard models, given potential training data contamination concerns
- **Medium Confidence**: Text-generation metrics revealing reasoning limitations, pending validation of metric-clinical utility correlation

## Next Checks

1. Replicate baseline performance using a standard model (GPT-4o) to validate the evaluation pipeline and metric computation
2. Conduct ablation study on individual text-generation metrics to identify which best correlates with expert qualitative ratings of clinical reasoning quality
3. Test model performance sensitivity to prompt variations to distinguish genuine reasoning improvements from prompt engineering effects