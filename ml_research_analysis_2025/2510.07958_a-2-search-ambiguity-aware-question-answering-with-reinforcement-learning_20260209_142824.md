---
ver: rpa2
title: 'A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning'
arxiv_id: '2510.07958'
source_url: https://arxiv.org/abs/2510.07958
tags:
- answer
- tool
- answers
- search
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A2SEARCH, a novel reinforcement learning
  framework for ambiguity-aware question answering. The core idea is to automatically
  detect ambiguous questions and generate multiple valid answers via trajectory sampling
  and evidence verification, then train the model with an answer-level F1 reward to
  naturally accommodate multiple answers.
---

# A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.07958
- Source URL: https://arxiv.org/abs/2510.07958
- Reference count: 40
- A2SEARCH-7B achieves 48.4% average AnsF1@1 across four multi-hop datasets, outperforming ReSearch-32B (46.2%)

## Executive Summary
This paper introduces A2SEARCH, a reinforcement learning framework that automatically detects ambiguous questions and generates multiple valid answers through trajectory sampling and evidence verification. The system uses a novel AnsF1 reward that naturally accommodates multiple answers during training, enabling models to learn when and how to generate alternative responses. Experiments on eight benchmarks show A2SEARCH-7B achieves state-of-the-art performance, particularly excelling at multi-answer generation where it produces 1.51 answers per question on average.

## Method Summary
A2SEARCH combines an automated 4-step pipeline (multi-model trajectory sampling → filtering → LLM verification → answer grouping) to discover alternative answers, followed by GRPO training with a carefully designed AnsF1 reward. The method trains models to generate structured outputs with tool calls, reasoning, and answers while masking tool-response tokens from policy loss. The AnsF1 reward balances precision and recall for questions with multiple valid answers, using α=0.4 as the optimal parameter. The approach requires indexing Wikipedia with E5 embeddings and sampling 16 trajectories per question from 5 different search models.

## Key Results
- A2SEARCH-7B achieves 48.4% average AnsF1@1 across four multi-hop datasets
- A2SEARCH-3B outperforms larger models like ReSearch-32B on multi-answer generation
- A2SEARCH generates 1.51 answers per question on average, compared to 1.23 for smaller variants
- The system identifies 27.6% of training examples as having multiple valid answers

## Why This Works (Mechanism)

### Mechanism 1: Evidence-Verified Alternative Answer Discovery
The automated 4-step pipeline identifies valid alternative answers without manual annotation by sampling diverse trajectories from multiple search-capable models, verifying evidence support through LLM consensus voting, and grouping semantically equivalent answers. This process creates training data where 19% of questions have multiple valid answers.

### Mechanism 2: AnsF1 Reward for Multi-Answer Optimization
The answer-level F1 reward naturally balances precision and recall when questions have multiple valid answers. The reward function R = 1 - α(1 - AnsF1) when hits > 0 rewards covering reference answers while penalizing over-generation, with α=0.4 creating the optimal precision-recall tradeoff.

### Mechanism 3: Agentic Search with Tool-Response Masking
GRPO training with masked tool-response tokens enables models to learn search strategies without backpropagating through external retrieval. Rollouts alternate between model actions and tool responses, with only model-generated tokens contributing to policy loss. Group-relative advantages normalize rewards across 16 rollouts per question.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Provides stable RL training without a separate critic network; essential for search-agent training where value estimation is complex.
  - Quick check question: Can you explain how GRPO estimates baselines from sampled rollouts rather than a learned critic?

- **Concept: Answer-level F1 (AnsF1)**
  - Why needed here: Standard exact match fails for multi-answer questions; AnsF1 rewards partial coverage appropriately.
  - Quick check question: Given 2 reference answers and predictions [A, B, C] where A and B match, what is AnsF1? (Answer: P=2/3, R=2/2, F1=0.8)

- **Concept: Trajectory Sampling with Tool Integration**
  - Why needed here: Models must learn when to search vs. reason; tool calls must be generated, not retrieved.
  - Quick check question: Why mask tool-response tokens from policy loss? (Answer: They come from external tools, not the policy model)

## Architecture Onboarding

- **Component map:** Data Pipeline: Multi-model sampling (5 models × 16 trajectories) → Filtering → Verification (4 LLMs with η=3 voting) → Grouping → GRPO Training → Inference

- **Critical path:**
  1. Verify alternative answer quality: Run pipeline on held-out questions, sample 100 positives, check human agreement (target: >95%)
  2. Monitor entropy during training: 7B should stay 0.15-0.35; 3B around 0.2
  3. Track AnsF1 and Recall separately: Both should increase steadily; divergence indicates precision-recall imbalance

- **Design tradeoffs:**
  - Higher η (verification threshold) → higher precision but fewer training examples (η=4 retains only 4.6% of data)
  - Larger rollout size → better advantage estimation but slower training (16 is sweet spot)
  - Instruct vs. Base models: Instruct better for tool-use; Base requires entropy regularization

- **Failure signatures:**
  - Models always output 1 answer: α too high (try 0.2-0.4) or data pipeline failed
  - Models output 5+ answers frequently: α too low (try 0.6-0.8)
  - Training instability with Base models: Add adaptive entropy control

- **First 3 experiments:**
  1. **Reproduce data pipeline statistics**: Run on 1000 questions from MuSiQue; verify 20-30% yield alternative answers, human agreement >90%
  2. **Ablate α values**: Train A²SEARCH-7B with α∈{0.2, 0.4, 0.6, 0.8} for 1 epoch; confirm α=0.4 achieves best precision-recall balance
  3. **Single-rollout comparison**: Evaluate A²SEARCH-7B vs. ReSearch-32B on MuSiQue with k=1 greedy decoding; confirm 48.4% vs. 46.2% AnsF1 gap reproduces

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section.

## Limitations
- Data Pipeline Reliability: The automated pipeline for discovering alternative answers relies heavily on LLM-based verification, which may introduce false positives or miss valid alternatives
- Generalizability Across Domains: The method's effectiveness for domains outside web-based QA (e.g., medical, legal, or scientific domains) remains untested
- Computational Cost: The approach requires substantial computational resources with 5 search models generating 16 trajectories each per question plus 4 LLM verifiers

## Confidence
- **High Confidence**: AnsF1 reward formulation effectiveness, single-rollout comparison results, basic GRPO implementation
- **Medium Confidence**: Alternative answer discovery pipeline quality, cross-domain generalization, computational cost scaling
- **Low Confidence**: Long-tail performance on extremely ambiguous questions, robustness to adversarial examples

## Next Checks
1. **Human Evaluation of Alternative Answers**: Sample 100 questions from the pipeline output and conduct human evaluation to verify that ≥95% of the identified alternative answers are genuinely valid and semantically distinct from reference answers.

2. **Cross-Domain Transfer Testing**: Evaluate A2SEARCH on a held-out domain (e.g., scientific papers or medical literature) to assess whether the ambiguity-aware capabilities transfer beyond web-based QA, measuring both performance degradation and any novel failure modes.

3. **Cost-Performance Scaling Analysis**: Systematically vary the number of trajectories per question (8, 16, 32) and verification threshold η (2, 3, 4) to quantify the tradeoff between computational cost and performance gains, identifying the optimal configuration for practical deployment scenarios.