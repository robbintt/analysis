---
ver: rpa2
title: A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using
  Visual and Wireless Signals
arxiv_id: '2504.20178'
source_url: https://arxiv.org/abs/2504.20178
tags:
- crowd
- counting
- attention
- information
- wifi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TransFusion, a multimodal crowd counting model
  that integrates Channel State Information (CSI) from WiFi signals with visual images
  using a Transformer-based architecture. The key innovation is combining global feature
  extraction from Transformers with local feature extraction from CNNs to overcome
  the limitations of single-modal approaches.
---

# A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals

## Quick Facts
- arXiv ID: 2504.20178
- Source URL: https://arxiv.org/abs/2504.20178
- Reference count: 29
- Key outcome: TransFusion achieves MAE of 0.2069, MSE of 0.3831, and R2 of 0.9978, representing 60.49% reduction in MAE compared to previous best multimodal fusion model (LF-LSTM)

## Executive Summary
This paper presents TransFusion, a multimodal crowd counting model that integrates Channel State Information (CSI) from WiFi signals with visual images using a Transformer-based architecture. The key innovation is combining global feature extraction from Transformers with local feature extraction from CNNs to overcome the limitations of single-modal approaches. The model employs linear attention mechanisms to reduce computational complexity while maintaining accuracy. Experimental results show TransFusion significantly outperforms baseline methods, achieving state-of-the-art performance in capturing both global contextual information and fine-grained local details essential for accurate crowd counting.

## Method Summary
TransFusion uses a dual-stream architecture where CSI signals and visual images are processed separately before fusion. CSI data undergoes Hampel filtering for denoising, while images are divided into 16×16 patches. Both modalities receive shared sinusoidal positional encoding. The core innovation is a cross-modal fusion module using linear attention to enable bidirectional feature exchange between WiFi and vision streams across multiple transformer layers. After fusion, a multi-scale CNN compensates for Transformers' local feature blindness by extracting fine-grained spatial patterns. The fused representations are then processed by self-attention transformers, concatenated, and passed through a linear regression head trained with L1 loss to predict crowd counts.

## Key Results
- Achieves MAE of 0.2069, MSE of 0.3831, and R2 of 0.9978 on multimodal fusion task
- Outperforms previous best multimodal fusion model (LF-LSTM) by 60.49% reduction in MAE
- Ablation studies show both cross-modal attention and multi-scale CNN components contribute significantly to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal attention enables bidirectional feature exchange between WiFi CSI and visual modalities, allowing each modality to enhance the other's representations.
- Mechanism: The cross-modal fusion module uses one modality as Query and the other as Key/Value pairs. WiFi-to-Vision (W→V) and Vision-to-WiFi (V→W) streams operate in parallel across N layers, with layer normalization and residual connections at each step.
- Core assumption: WiFi CSI perturbations caused by human movement correlate spatially and temporally with visual crowd presence.
- Evidence anchors:
  - [abstract] "integrates Channel State Information (CSI) with image data... enabling the capture of comprehensive global contextual information"
  - [Section II.D.1] "the sequence of one modality (source modality) serves as the Query input... while the sequences of the other modality (target modality) are utilized as the Key and Value inputs"
  - [corpus] Related work FedAPA also leverages WiFi CSI for crowd counting, validating the modality's utility, but TransFusion uniquely combines it with vision via attention.

### Mechanism 2
- Claim: Multi-scale CNN compensates for Transformer's local feature blindness by extracting fine-grained spatial patterns at multiple receptive field sizes.
- Mechanism: After cross-modal attention layers, multi-scale CNN applies kernels of varying sizes in a feed-forward manner.
- Core assumption: Local features (individual people, short-term signal spikes) contribute meaningfully to crowd count estimation beyond global context.
- Evidence anchors:
  - [abstract] "transformers... potentially fail to identify finer-grained, local details essential for precise crowd counting. To mitigate this, we incorporate Convolutional Neural Networks"
  - [Section II.D.2] "multi-scale CNN processes the input sequences using kernels of varying sizes, allowing it to effectively capture both fine-grained local features and broader contextual information"
  - [Table II] Ablation shows removing multi-scale CNN increases MAE by +0.4607 and MSE by +3.0900, demonstrating measurable contribution.

### Mechanism 3
- Claim: Linear attention reduces computational complexity from O(n²) to O(n) while preserving cross-modal correlation modeling, enabling practical deployment.
- Mechanism: Standard softmax attention computes QK^T which scales quadratically with sequence length. Linear attention reformulates this via kernel feature maps, decomposing the computation to avoid the pairwise comparison matrix.
- Core assumption: The approximation introduced by linear attention does not significantly degrade the quality of learned cross-modal correlations.
- Evidence anchors:
  - [abstract] "employs linear attention mechanisms to reduce computational complexity while maintaining accuracy"
  - [Section II.D.1] "CF^{W→V}_{mul,i} denotes the multi-head cross-modal attention at the i-th layer" (implied linear attention throughout)
  - [corpus] Weak corpus support—no direct comparison of linear vs. standard attention complexity in cited neighbors.

## Foundational Learning

- **Channel State Information (CSI) Fundamentals**
  - Why needed here: CSI amplitude captures how WiFi signals are attenuated and scattered by human bodies in the environment. Unlike RSSI (coarse signal strength), CSI provides per-subcarrier channel measurements, enabling fine-grained sensing of crowd-induced multipath changes.
  - Quick check question: Can you explain why CSI amplitude is used while phase is discarded in this paper, and what preprocessing steps are required before feeding CSI to a neural network?

- **Attention Mechanisms (Self-Attention, Cross-Attention, Linear Attention)**
  - Why needed here: TransFusion relies on three attention variants—cross-modal attention for fusion, self-attention for temporal modeling, and linear attention for efficiency. Understanding query/key/value roles and computational tradeoffs is essential.
  - Quick check question: Given sequences of length L=2000 (CSI packets) and L=196 (16×16 image patches of a 224×224 image), what is the approximate FLOP reduction when switching from standard O(L²) to linear O(L) attention?

- **Hybrid CNN-Transformer Architectures**
  - Why needed here: The model explicitly combines CNNs (local receptive fields, translation invariance) with Transformers (global context, permutation equivariance). Understanding when to use each is critical for debugging feature extraction failures.
  - Quick check question: If the model performs well on dense crowds but fails on sparse crowds, which component (CNN local features or Transformer global context) would you investigate first, and why?

## Architecture Onboarding

- **Component map:**
  Raw Inputs → Preprocessing (Hampel filter for CSI, patch embedding for images) → Positional Encoding (sinusoidal, shared formula) → Cross-Modal Fusion (N layers, bidirectional W↔V streams) → Self-Attention Transformer (per stream, temporal aggregation) → Concatenation → Linear head → Crowd count prediction

- **Critical path:**
  1. CSI denoising (Hampel filter with window 2K+1, 3σ threshold) — garbage in, garbage out
  2. Cross-modal attention weight initialization — if attention collapses to uniform, fusion fails
  3. Multi-scale CNN kernel size selection — must match spatial scale of crowd features
  4. Final linear layer regression — L1 loss training must converge without overfitting

- **Design tradeoffs:**
  - **Linear vs. standard attention:** ~10×+ speedup on long sequences, but may lose precision on subtle cross-modal correlations. Monitor validation MAE gap.
  - **Patch size p=16:** Larger patches reduce sequence length (faster) but lose fine detail. Smaller patches increase compute and may overfit.
  - **Number of N layers:** Paper doesn't specify exact N. More layers = deeper fusion but risk of over-smoothing and training instability.
  - **L1 vs. L2 loss:** L1 (MAE) chosen for outlier robustness. If crowd count variance increases significantly (large crowds), L2 might better penalize large errors.

- **Failure signatures:**
  - **Single-modality collapse:** If one stream's features dominate (e.g., vision much stronger than WiFi), cross-modal attention may ignore the weaker modality. Check attention weight distributions per stream.
  - **Temporal misalignment:** If WiFi sampling (500 Hz, 4-second windows) and video frames are not synchronized, cross-modal attention learns noise. Verify timestamp alignment.
  - **Overfitting to room layout:** Model trained in 10.8m×7m room with specific furniture may not generalize. Check performance on held-out spatial configurations.
  - **Hampel filter over-smoothing:** If filter window too large, genuine crowd-induced CSI variations are removed as "noise."

- **First 3 experiments:**
  1. **Modality ablation:** Train vision-only and WiFi-only variants. Confirm both substantially underperform fusion (paper shows MAE increases from 0.21 to 2.03 and 1.59 respectively). This validates fusion is doing useful work.
  2. **Attention visualization:** Extract and visualize cross-modal attention maps for sample inputs. Verify WiFi-to-vision attention highlights crowd regions and vision-to-WiFi attention aligns with signal perturbation patterns.
  3. **Sequence length sensitivity:** Test with shortened CSI windows (e.g., 2 seconds, 1 second) and fewer image patches. Identify minimum viable input length before MAE degrades >10%.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does TransFusion generalize to unseen indoor environments with different layouts and sizes?
- **Basis in paper:** [inferred] The experimental evaluation was restricted to a single meeting room (10.8m × 7m), and the Introduction notes that WiFi signals are "susceptible to environmental influences."
- **Why unresolved:** The model may overfit to the specific multipath effects and furniture arrangements of the single test environment, limiting its applicability to new venues without retraining.
- **What evidence would resolve it:** Cross-domain testing results where the model is trained on the current room and tested on data collected in different rooms with varying dimensions and objects.

### Open Question 2
- **Question:** Can the inclusion of CSI phase information further improve the model's accuracy?
- **Basis in paper:** [explicit] Footnote 1 states, "In our work we ignore the phase of CSI which is not reliable for the chosen WiFi NIC."
- **Why unresolved:** It remains untested whether the Transformer-based fusion could leverage valid phase data to capture finer details, as the current implementation relies solely on amplitude.
- **What evidence would resolve it:** Comparative experiments using hardware capable of extracting reliable phase information (e.g., software-defined radios) to evaluate the delta in performance.

### Open Question 3
- **Question:** How does the model's performance scale with extreme crowd densities beyond the tested maximum of 44 people?
- **Basis in paper:** [inferred] The dataset included a maximum of 44 volunteers, which may not represent high-density scenarios where severe occlusions and signal saturation occur.
- **Why unresolved:** It is unclear if the "linear attention" and local feature extraction mechanisms can handle the noise and overlapping features inherent in large crowds (e.g., >100 people).
- **What evidence would resolve it:** Evaluation of the model in a larger environment with a significantly higher number of participants to identify saturation points.

## Limitations

- **Dataset availability**: Raw CSI/image pairs require custom collection with Intel 5300 NIC and synchronized video
- **Architectural hyperparameters**: Number of layers, attention heads, and kernel sizes are unspecified
- **Single environment testing**: Model trained and tested only in one meeting room (10.8m × 7m), limiting generalization claims

## Confidence

- **High confidence**: The multimodal fusion concept and general architecture direction are well-supported by the ablation results (60.49% MAE reduction over LF-LSTM baseline)
- **Medium confidence**: The specific mechanism claims (cross-modal attention benefits, multi-scale CNN compensation) are logically consistent but lack direct experimental isolation in the paper
- **Low confidence**: Claims about linear attention computational savings are stated but not empirically validated against standard attention

## Next Checks

1. **Dataset replication validation**: Collect a small-scale synchronized CSI/image dataset (10 scenarios, 100 samples each) and verify baseline single-modality performance matches paper expectations (MAE ~2.0 for vision-only, ~1.6 for WiFi-only)
2. **Ablation reproducibility**: Systematically remove cross-modal attention and multi-scale CNN components to confirm the stated performance degradation (+0.46 MAE for multi-scale removal)
3. **Temporal alignment verification**: Implement timestamp synchronization between 500Hz CSI sampling and video frames, then measure cross-modal attention weight distributions to confirm meaningful feature exchange rather than random correlation