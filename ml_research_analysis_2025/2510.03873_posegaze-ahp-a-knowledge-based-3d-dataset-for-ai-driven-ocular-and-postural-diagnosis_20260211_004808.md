---
ver: rpa2
title: 'PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural
  Diagnosis'
arxiv_id: '2510.03873'
source_url: https://arxiv.org/abs/2510.03873
tags:
- data
- head
- extraction
- accuracy
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PoseGaze-AHP, a 3D dataset for diagnosing
  ocular-induced abnormal head posture (AHP) by synchronously capturing head pose
  and gaze movement data. It uses large language models (Claude 3.5 Sonnet) to extract
  structured clinical data from 148 medical papers with 91.92% accuracy, then transforms
  this data into 3D representations using the Neural Head Avatar framework.
---

# PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis

## Quick Facts
- **arXiv ID**: 2510.03873
- **Source URL**: https://arxiv.org/abs/2510.03873
- **Reference count**: 0
- **Primary result**: Introduces first 3D dataset for AI-driven ocular-induced abnormal head posture diagnosis using LLM-extracted clinical data and synthetic 3D generation

## Executive Summary
This paper introduces PoseGaze-AHP, a novel 3D dataset specifically designed for AI-driven diagnosis of ocular-induced abnormal head posture (AHP). The dataset is constructed by extracting structured clinical data from 148 medical papers using large language models (Claude 3.5 Sonnet), achieving 91.92% accuracy. This clinical data is then transformed into 3D representations using the Neural Head Avatar framework, generating 7,920 synthetic images across two head textures. The dataset enables development of privacy-compliant diagnostic tools that integrate head posture and ocular movement analysis, addressing the critical need for specialized datasets in ocular-AHP diagnosis.

## Method Summary
The dataset construction pipeline involves four key stages: (1) extracting clinical data from 148 medical papers using Claude 3.5 Sonnet with iterative prompt engineering and feedback loops, (2) post-processing the extracted data through standardization and hierarchical medical rule-based imputation to complete missing fields, (3) generating synthetic 3D head pose and gaze representations using the Neural Head Avatar framework based on clinical parameters, and (4) rendering multi-view images (7 camera views) to create the final dataset of 7,920 images. The approach leverages LLM extraction with three prompting strategies (steps, hierarchical, complex) and medical imputation rules derived from systematic review to ensure clinical validity.

## Key Results
- Achieved 91.92% overall extraction accuracy from medical papers, with 84.53% patient-level accuracy
- Generated 7,920 synthetic 3D images covering diverse ocular conditions using two head textures
- Created first publicly available resource specifically for AI-driven ocular-induced AHP diagnosis
- Demonstrated that structured LLM prompting with iterative feedback can effectively extract clinical data from heterogeneous medical literature

## Why This Works (Mechanism)

### Mechanism 1
Structured LLM prompting with iterative feedback can extract clinical data from heterogeneous medical literature at ~92% accuracy. Three prompting strategies (steps, hierarchical, complex) decompose extraction into phases, with feedback categorizing errors to enable self-correction. Temperature=0.0 minimizes hallucination. Core assumption: medical papers share enough structural regularity for LLMs to map unstructured text to predefined fields. Evidence: 91.92% overall accuracy achieved; steps strategy reaches 92.5% accuracy. Break condition: highly non-standard papers or ambiguous abbreviations may exceed error correction capacity.

### Mechanism 2
Hierarchical medical rule-based imputation can complete missing clinical fields while preserving diagnostic validity. Imputation follows fixed order: AHP type → direction → diagnosed eye → misalignment → PD/degree, derived from systematic review of 180 studies. Rules are flagged as imputed vs. extracted. Core assumption: ocular conditions have predictable AHP presentations across populations. Evidence: rules derived from prior systematic review by same author group. Break condition: rare conditions or atypical presentations may receive incorrect imputations.

### Mechanism 3
Neural Head Avatar framework can generate biomechanically plausible 3D head pose and gaze representations from clinical parameters. FLAME morphable model provides parametric control (yaw ±90°, pitch ±15°, articulated gaze) with geometry refinement and texture networks. Seven camera views capture each pose. Core assumption: parametric models can adequately represent abnormal head postures without real patient 3D scans. Evidence: 7,920 images generated from 495 records × 2 textures × 8 views. Break condition: extreme AHP angles or asymmetric features may exceed FLAME's expressive range.

## Foundational Learning

- **Prompt Engineering (Zero-shot, Few-shot, Chain-of-Thought)**: Why needed: extraction pipeline relies entirely on selecting and tuning prompting strategies for heterogeneous paper formats. Quick check: Can you explain why temperature=0.0 is used and what "few-shot" means in this context?
- **Data Imputation (Rule-based vs. Statistical)**: Why needed: missing fields filled using deterministic medical rules rather than ML-based imputation; understanding hierarchical dependencies is critical. Quick check: Why does imputation order matter (AHP type before direction)?
- **Parametric 3D Face Models (FLAME)**: Why needed: NHA framework extends FLAME; understanding pose parameters (yaw, pitch, roll) and gaze articulation is needed to configure generation. Quick check: What are the rotational limits of the FLAME-based NHA framework for head pose?

## Architecture Onboarding

- **Component map**: PDF/text extraction → Iterative LLM extraction (steps/hierarchical/complex prompts + feedback) → Post-processing (standardization + imputation) → JSON storage → NHA framework → FLAME parameters → Multi-view rendering → PNG + metadata
- **Critical path**: 1. Ground truth table definition (key fields) → 2. LLM extraction with steps strategy → 3. Standardization + imputation → 4. NHA rendering (clinical params → pose/gaze → images)
- **Design tradeoffs**: Synthetic data preserves privacy but may lack real-patient variability; two textures limit demographic diversity; imputation enables completeness but introduces unverified values; class imbalance reflects clinical reality but challenges ML training
- **Failure signatures**: High "Complete Error" or "Extra Patient" rates → prompt needs refinement; imputation flags on critical fields → rule coverage gaps; unnatural poses → FLAME parameter limits exceeded; direction mismatches → ipsilateral/contralateral mapping errors
- **First 3 experiments**: 1. Validate extraction on held-out papers: manually annotate 10 papers not in training set; compare LLM extraction accuracy against ground truth. 2. Assess imputation quality: identify records where both extracted and imputed values exist; measure agreement rate. 3. Visual fidelity check: generate images for known extreme AHP cases; have clinical expert rate anatomical plausibility.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but identifies several critical considerations: the dataset's notable class imbalance (Duane Syndrome and Superior Oblique Palsy dominate) requires class-balancing techniques for equitable learning; the use of only two head textures may limit feature learning independent of facial identity; and the dataset's synthetic nature raises questions about Sim2Real transfer to clinical settings.

## Limitations
- Extraction pipeline achieves 91.92% accuracy but has 24% hallucination rate (Extra Patient errors) concerning for clinical applications
- Synthetic 3D data may not capture full variability of real patient presentations, particularly for extreme or asymmetric cases
- Class imbalance (dominant conditions like Duane Syndrome) could limit model generalizability to rare ocular disorders

## Confidence

- **High Confidence**: LLM extraction methodology (prompt engineering, feedback loops) is well-established and reproducible; NHA framework for 3D generation is technically sound and validated for facial animation
- **Medium Confidence**: Medical imputation rules are grounded in systematic review but require domain expertise to verify; overall dataset construction pipeline is logical but lacks external validation of clinical accuracy
- **Low Confidence**: Clinical validity of synthetic 3D representations for actual diagnostic use cases remains unproven without physician validation studies

## Next Checks
1. **Clinical Expert Validation**: Recruit ophthalmologists to review a stratified sample of generated images against their clinical experience with actual AHP patients. Measure agreement on pose plausibility and condition representation.
2. **Downstream Model Performance**: Train and evaluate an AHP classification model using PoseGaze-AHP versus a model using real patient images (where available). Compare diagnostic accuracy and false positive rates on a held-out clinical dataset.
3. **Imputation Accuracy Assessment**: Identify papers containing complete clinical data. Compare LLM-extracted values against ground truth for fields that would have been imputed in incomplete records. Calculate imputation precision and recall for each clinical parameter.