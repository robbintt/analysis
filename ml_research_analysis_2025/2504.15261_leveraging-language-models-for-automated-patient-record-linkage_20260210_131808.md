---
ver: rpa2
title: Leveraging Language Models for Automated Patient Record Linkage
arxiv_id: '2504.15261'
source_url: https://arxiv.org/abs/2504.15261
tags:
- record
- linkage
- matching
- blocking
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Leveraging Language Models for Automated Patient Record Linkage

## Quick Facts
- arXiv ID: 2504.15261
- Source URL: https://arxiv.org/abs/2504.15261
- Authors: Mohammad Beheshti; Lovedeep Gondara; Iris Zachary
- Reference count: 39
- Primary result: Fine-tuned Mistral-7B achieved only 6 incorrect predictions on patient record linkage task

## Executive Summary
This paper investigates using large language models (LLMs) for automated patient record linkage, a critical task for healthcare data integration. The authors evaluate both blocking (candidate pair generation) and matching (classification) stages using fine-tuned LLMs compared to traditional methods. Their results show that while fine-tuned LLMs significantly outperform zero-shot approaches and encoder-only models, traditional hybrid rule-based and probabilistic methods remain more efficient for blocking. The best performance was achieved with fine-tuned Mistral-7B, which made only 6 incorrect predictions on the test dataset.

## Method Summary
The study evaluates two approaches: (1) embedding-based blocking using fine-tuned RoBERTa for candidate pair reduction, and (2) fine-tuned LLMs (Mistral-7B, Llama-3.x) for binary match classification. Blocking uses SBERT fine-tuning with cosine similarity loss and FAISS KNN retrieval, while matching employs LoRA adaptation with temperature=0 inference for deterministic "Yes"/"No" outputs. The Missouri Cancer Registry dataset provides 58,383 training pairs and 52,917 test pairs with fields including names, birth dates, sex, SSN (97% missing), and addresses (81% missing). Serialization uses Ditto format with "Unknown" for missing values.

## Key Results
- Fine-tuned blocking model achieved 92% reduction in candidate pairs while maintaining near-perfect recall
- Fine-tuned Mistral-7B achieved best performance with only 6 incorrect predictions (F1=0.999)
- LLMs remain less accurate and efficient than hybrid rule-based and probabilistic approaches for blocking
- Zero-shot LLMs without fine-tuning show substantial degradation (2,454 errors vs. 6 with fine-tuning)

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Based Semantic Blocking for Candidate Reduction
Fine-tuned sentence embeddings can reduce candidate pairs by ~92% while maintaining near-perfect recall. RoBERTa is fine-tuned using cosine similarity loss with mean pooling to position similar records closer in embedding space. K-nearest neighbors retrieval via FAISS then identifies candidate pairs above a similarity threshold. Semantic similarity in embedding space correlates with record identity despite typos and missing fields. However, subword tokenization splits typos into unrelated tokens, causing near-identical records to receive low similarity scores.

### Mechanism 2: Fine-Tuned Generative LLMs for Binary Match Classification
Fine-tuned instruction-tuned LLMs (Mistral-7B, Llama-3.x) achieve lower error rates than both zero-shot models and encoder-only classifiers like RoBERTa. Record pairs are serialized into structured prompts with special tokens. LoRA adaptation (rank=32, alpha=32) fine-tunes on labeled pairs. Instruction fine-tuning enables generative models to outperform dedicated classification architectures on binary entity matching. Few-shot prompting underperformed as models over-anchored to limited examples.

### Mechanism 3: Hybrid Rule-Based and Probabilistic Blocking as Baseline
Traditional rule-based blocking combined with probabilistic similarity scores can achieve 100% recall with greater efficiency than pure embedding-based blocking. Soundex phonetic blocking on names plus exact match on birth date/SSN, followed by Fellegi-Sunter probabilistic scoring. Domain-specific blocking rules with probabilistic thresholds capture identity better than learned embeddings alone. Requires manual rule design per dataset and is not generalizable across healthcare systems with different schemas.

## Foundational Learning

- **Record Linkage Pipeline (Blocking → Matching)**: Understanding why blocking precedes matching explains the two-stage experimental design and why blocking errors propagate to final linkage quality. Quick check: If blocking recall is 99%, can matching ever recover the missed 1%? (Answer: No—those pairs are never compared.)

- **Transformer Fine-Tuning Paradigms (Encoder-only vs. Decoder-only)**: RoBERTa (encoder) uses a classification head on CLS tokens; generative LLMs use instruction prompts. Different setups required different tokenization and training configurations. Quick check: Why does RoBERTa require a linear classification head while Mistral does not? (Answer: RoBERTa outputs embeddings; Mistral generates tokens that can be parsed as labels.)

- **LoRA (Low-Rank Adaptation) for Parameter-Efficient Fine-Tuning**: Fine-tuning 7B-70B models on a single 48GB GPU requires LoRA to reduce memory footprint while preserving adaptation quality. Quick check: What happens if LoRA rank is set too low for a complex matching task? (Answer: Model underfits; insufficient capacity to learn domain-specific patterns.)

## Architecture Onboarding

- **Component map**: Record pairs → RoBERTa-base fine-tuning → FAISS KNN index → Cosine threshold filter → Candidate pairs → Mistral-7B LoRA → Structured prompt → Temperature=0 inference → "Yes"/"No" classification → Evaluation

- **Critical path**: Preprocess records → serialize fields (handle missing values with "Unknown") → Blocking: generate embeddings → KNN search → threshold filter → Matching: serialize pairs → model inference → collect predictions → Evaluate against labeled dataset

- **Design tradeoffs**: Blocking threshold - lower threshold = higher recall, more candidates, higher compute (K=10, threshold=0.75 optimal). Fine-tuning vs. Zero-shot - fine-tuning reduces errors 10-400x but requires labeled data. Reasoning models - higher accuracy but 50x slower inference (26 hours vs. 30 minutes).

- **Failure signatures**: Blocking misses near-identical records with minor typos (subword tokenization artifact). Zero-shot models produce high false positives on ambiguous pairs. Few-shot prompting causes over-anchoring to examples. Reasoning models timeout or exceed memory on large batch inference.

- **First 3 experiments**: 1) Baseline blocking calibration - run rule-based + probabilistic linkage to establish ground truth and identify ambiguity zone. 2) Embedding blocking sweep - test K=[5, 10, 20] × threshold=[0.5, 0.6, 0.7, 0.75, 0.8]. 3) Matching model comparison - zero-shot inference on Mistral-7B, Llama-3.1-8B, then fine-tune each with LoRA.

## Open Questions the Paper Calls Out

### Open Question 1
Can character-level tokenizers significantly improve the robustness of embedding-based blocking models against minor typographical errors compared to subword tokenizers? The authors observe that subword tokenizers (e.g., RoBERTa) split identifiers with minor typos into unrelated units, lowering similarity scores. A comparative evaluation of blocking recall on datasets with synthetic typos would resolve this.

### Open Question 2
Does a hybrid pipeline combining rule-based blocking with LLM-based matching offer superior accuracy and efficiency compared to a fully LLM-based linkage approach? The authors conclude that LLMs "remain less accurate and efficient than a hybrid rule-based and probabilistic approach for blocking." End-to-end metrics comparing "Rule-based Blocking + LLM Matching" against "LLM Blocking + LLM Matching" would resolve this.

### Open Question 3
Can reasoning models (e.g., DeepSeek-R1) be optimized for record linkage to justify their high computational cost, or do they remain impractical regardless of accuracy gains? The study notes that reasoning models are "impractical for large-scale record linkage due to high computational costs" (26 hours vs. 30 minutes). Benchmarking distilled or quantized reasoning models would determine if they can achieve processing speeds comparable to standard LLMs.

### Open Question 4
To what extent do fine-tuned linkage models generalize to external healthcare datasets with different demographic profiles or data missingness patterns? The study relies on a single data source (Missouri Cancer Registry) with specific characteristics. Zero-shot evaluation on a geographically distinct cancer registry dataset would measure performance degradation.

## Limitations

- Limited domain generalization due to evaluation on only Missouri Cancer Registry data with specific missingness patterns
- Prompt template ambiguity creates uncertainty in exact reproduction due to incomplete specification
- Single dataset evaluation limits generalizability across different healthcare systems or record types

## Confidence

- **Embedding-based blocking achieving 92% reduction with near-perfect recall**: High confidence
- **Fine-tuned Mistral-7B outperforming other LLMs with only 6 errors**: High confidence
- **LLMs remaining less efficient than hybrid rule-based approaches**: Medium confidence

## Next Checks

1. **Cross-dataset generalization test**: Evaluate fine-tuned blocking and matching models on a different healthcare dataset with varying schema to assess domain transferability and measure whether the 92% blocking reduction is maintained.

2. **Missing value pattern sensitivity analysis**: Systematically vary the percentage of missing fields (SSN, address) to determine how performance degrades as missingness increases beyond the 97% and 81% levels observed in the study.

3. **Prompt template ablation study**: Test different prompt structures (varying field order, handling of missing values, instruction specificity) to identify the minimum viable prompt that maintains the 6-error performance level with Mistral-7B.