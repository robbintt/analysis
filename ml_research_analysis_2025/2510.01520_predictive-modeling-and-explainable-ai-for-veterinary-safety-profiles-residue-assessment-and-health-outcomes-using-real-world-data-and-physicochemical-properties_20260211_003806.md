---
ver: rpa2
title: Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue
  Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties
arxiv_id: '2510.01520'
source_url: https://arxiv.org/abs/2510.01520
tags:
- data
- were
- drug
- veterinary
- outcomes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed a predictive framework to classify veterinary\
  \ drug safety outcomes (Death vs. Recovery) using ~1.28 million OpenFDA adverse\
  \ event reports (1987\u20132025 Q1)."
---

# Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties

## Quick Facts
- arXiv ID: 2510.01520
- Source URL: https://arxiv.org/abs/2510.01520
- Reference count: 31
- Primary result: Framework achieved F1=0.95 for classifying Death vs. Recovery outcomes in veterinary adverse events

## Executive Summary
This study develops a predictive framework to classify veterinary drug safety outcomes (Death vs. Recovery) using ~1.28 million OpenFDA adverse event reports (1987–2025 Q1). The approach integrates rigorous data preprocessing, ontological standardization, and physicochemical drug features, followed by supervised and semi-supervised machine learning with ensemble models. CatBoost, Voting, and Stacking classifiers achieved precision, recall, and F1-scores of 0.95, with improved recall for fatal outcomes via undersampling and Average Uncertainty Margin (AUM)-based pseudo-labeling. SHAP analysis identified biologically plausible predictors, including respiratory disorders, animal demographics, and drug physicochemical properties. The framework enhances early detection of high-risk drug-event profiles, supporting FARAD's mission in residue risk assessment and regulatory decision-making.

## Method Summary
The framework processes ~1.28M OpenFDA CVM reports through relational table merging, VeDDRA and ATCvet ontology mapping, and PubChem physicochemical property integration. After cleaning to ~690K reports, the data undergoes hierarchical feature reduction, missing value imputation, and categorical encoding. Class imbalance is addressed through undersampling, oversampling, and SMOTE+ENN techniques. Models include CatBoost, XGBoost, Random Forest, ExcelFormer, MLP, and ensemble Voting/Stacking classifiers. AUM-based pseudo-labeling extends training with uncertain cases. Evaluation uses weighted F1, precision, recall, and class-specific Death Recall. SHAP provides interpretability for feature contributions.

## Key Results
- CatBoost, Voting, and Stacking classifiers achieved F1=0.95 across sampling strategies
- Undersampling improved Death recall from 0.73 to 0.87 while maintaining overall F1=0.94
- AUM-based pseudo-labeling improved minority-class detection, particularly in ExcelFormer and XGBoost
- SHAP analysis identified respiratory disorders, animal demographics, and drug physicochemical properties as key predictors
- Semi-supervised learning improved Death recall for some models while slightly reducing F1 for strong ensembles

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical ontology mapping enables effective learning from high-cardinality veterinary adverse event data. Raw adverse events (>3,000 categories) and drug names (>18,500 categories) are mapped through VeDDRA (LLT→PT→HLT→SOC) and ATCvet hierarchies to reduced, semantically coherent feature sets. This reduces noise while preserving clinically meaningful patterns that tree-based models can exploit. Assumption: Consolidating rare or synonymous terms at higher ontology levels does not eliminate critical outcome-discriminative information.

### Mechanism 2
AUM-based pseudo-labeling with confidence filtering improves minority-class (Death) recall by expanding the effective training set. A baseline model trained on labeled cases generates predictions for uncertain reports (Unknown/Ongoing outcomes). The Average Uncertainty Margin (difference between top two class probabilities across epochs) ranks pseudo-labels by stability; only high-confidence cases (top 20-80%) are merged for retraining, reducing noise propagation while increasing minority-class representation. Assumption: High AUM scores correlate with correct pseudo-labels, and the uncertainty cases contain learnable signal rather than pure noise.

### Mechanism 3
Ensemble methods with class-aware resampling capture non-linear interactions across heterogeneous clinical, demographic, and physicochemical features. CatBoost, Voting (CatBoost + Random Forest + XGBoost), and Stacking classifiers combine tree-based learners optimized for categorical data with different split strategies. Undersampling and SMOTE+ENN rebalance training data, ensuring minority-class gradients contribute meaningfully to ensemble decisions. Assumption: Base learners make partially independent errors, and resampling does not distort the true decision boundary.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations)
  - Why needed here: Critical for regulatory acceptance; SHAP quantifies feature contributions to individual Death/Recovery predictions, enabling veterinarians and regulators to validate model reasoning against biological knowledge.
  - Quick check question: Given a SHAP summary plot showing "lung disorders" with negative values for Death predictions, does this mean lung disorders decrease or increase Death probability?

- Concept: Class Imbalance Handling (Undersampling, SMOTE+ENN)
  - Why needed here: The dataset has 85% Recovered vs. 15% Death cases; without rebalancing, models optimize for majority class accuracy at the expense of fatal outcome detection—clinically unacceptable for safety applications.
  - Quick check question: Why might undersampling improve Death recall but reduce overall F1-score compared to SMOTE+ENN?

- Concept: Semi-supervised Learning with Pseudo-labeling
  - Why needed here: ~410,000 reports have uncertain outcomes (Ongoing/Unknown); discarding them wastes potential signal. Pseudo-labeling leverages this data while confidence metrics control noise.
  - Quick check question: What happens to model performance if pseudo-labels are incorporated without confidence filtering?

## Architecture Onboarding

- Component map: OpenFDA JSON files -> PostgreSQL relational tables (Main, AEs, Outcome, Drug) -> PubChem API integration -> VeDDRA/ATCvet ontology mapping -> Feature engineering -> Train/validation/test split -> Resampling (undersampling/SMOTE+ENN) -> Supervised models (CatBoost, XGBoost, Random Forest, ExcelFormer, MLP) -> AUM-based pseudo-labeling -> Ensemble methods (Voting, Stacking) -> SHAP interpretability

- Critical path: Ontology mapping correctness → Class imbalance strategy selection → Ensemble configuration → AUM threshold tuning

- Design tradeoffs:
  - Supervised-only vs. semi-supervised: SSL improves recall for some models (XGBoost, ExcelFormer) but may slightly reduce F1 for already-strong ensembles due to label noise
  - Undersampling vs. SMOTE+ENN: Undersampling yields higher Death recall but lower overall F1; SMOTE+ENN provides more balanced performance
  - LLM vs. tabular models: LLMs (Gemma, Phi) underperform without fine-tuning; tabular ensembles are currently superior for structured AE data

- Failure signatures:
  - Death recall <0.70: Indicates class imbalance handling failure or feature set missing critical clinical signals
  - Large discrepancy between validation and test performance: Suggests overfitting to resampled training distribution
  - SHAP features inconsistent with biological knowledge: Signals data leakage or ontology mapping errors

- First 3 experiments:
  1. Baseline supervised model (CatBoost, no resampling) on cleaned dataset to establish Death recall and F1 benchmarks
  2. Ablation over resampling strategies (undersampling, oversampling, SMOTE+ENN) with fixed CatBoost configuration, measuring Death recall improvement vs. F1 cost
  3. Semi-supervised AUM pipeline integration, sweeping confidence threshold (20%, 50%, 80%) and comparing XGBoost/ExcelFormer Death recall against supervised baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the integration of protein-level descriptors (e.g., drug-target interactions, binding affinities) and pharmacokinetic parameters (e.g., clearance rate, half-life) alter the predictive accuracy and mechanistic interpretability of veterinary safety outcomes? Basis: The authors explicitly state in the Future Work section that adding protein-level and PK data could transform the model into a "mechanistically informed predictive framework." Why unresolved: The current study relied on physicochemical properties from PubChem and clinical data, lacking specific biological mechanism data to explain why specific drug-event profiles occur. What evidence would resolve it: A comparative study showing improved F1-scores or more specific SHAP feature importance attribution when PK/protein features are added to the training set.

### Open Question 2
Can domain-specific fine-tuning of Large Language Models (LLMs) close the performance gap with traditional ensemble methods for classifying veterinary adverse events? Basis: The results show LLMs (Gemma, Phi) significantly underperformed (F1 ~0.80) compared to CatBoost/Ensembles (F1 0.95), which the authors attribute to the use of "general-purpose" models without "domain-specific adaptation." Why unresolved: It is unclear if the poor performance is inherent to the LLM architecture for this tabular task or simply a result of the zero-shot/5-shot approach used in the study. What evidence would resolve it: Benchmarking fine-tuned veterinary-specific LLMs against the current baseline to see if recall for fatal outcomes matches or exceeds CatBoost.

### Open Question 3
Does the simple summation of physicochemical properties for multi-ingredient formulations obscure non-linear drug-drug interactions that are critical for predicting toxicity? Basis: The methodology aggregates physicochemical values via summation to represent "total molecular exposure," but this assumes additivity rather than potential synergistic or antagonistic interactions common in toxicology. Why unresolved: The paper validates the model's overall accuracy but does not ablate the feature aggregation strategy to determine if complex interactions in combination therapies are being missed. What evidence would resolve it: A feature ablation study comparing summation-based features against interaction-term features or graph-based representations for multi-drug reports.

## Limitations
- No external validation on independent veterinary adverse event datasets; performance based solely on internal train/validation/test splits
- Pseudo-labeling quality uncertain; confidence thresholds tuned on validation data without ground truth verification for pseudo-labeled cases
- Incomplete hyperparameter documentation; critical model configurations not specified pending GitHub code release

## Confidence

- **High Confidence**: Ontology mapping effectiveness, class imbalance handling, and SHAP interpretability features
- **Medium Confidence**: Ensemble model performance claims (F1=0.95)
- **Low Confidence**: Semi-supervised learning claims

## Next Checks
1. External validation study: Apply trained models to a completely separate veterinary adverse event dataset to assess generalizability beyond OpenFDA
2. Pseudo-label quality assessment: For a random sample of 100-200 pseudo-labeled cases (Uncertainty <0.3), manually review actual outcomes from source reports to estimate true positive rate and calibrate confidence thresholds
3. Ablation study on ontology granularity: Compare model performance using different VeDDRA mapping levels (LLT vs PT vs HLT) to quantify tradeoff between feature reduction and outcome discrimination