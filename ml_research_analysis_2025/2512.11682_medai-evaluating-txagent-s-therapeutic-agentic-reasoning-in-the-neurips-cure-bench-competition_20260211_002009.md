---
ver: rpa2
title: 'MedAI: Evaluating TxAgent''s Therapeutic Agentic Reasoning in the NeurIPS
  CURE-Bench Competition'
arxiv_id: '2512.11682'
source_url: https://arxiv.org/abs/2512.11682
tags:
- txagent
- retrieval
- information
- function
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents TxAgent, an agentic AI system for therapeutic\
  \ reasoning in clinical medicine, addressing the challenge of accurate, multi-step\
  \ decision-making under strict safety constraints. TxAgent uses a fine-tuned Llama-3.1-8B\
  \ model integrated with ToolUniverse\u2014a biomedical tool suite\u2014to dynamically\
  \ retrieve and reason over current therapeutic information via iterative retrieval-augmented\
  \ generation."
---

# MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition

## Quick Facts
- arXiv ID: 2512.11682
- Source URL: https://arxiv.org/abs/2512.11682
- Authors: Tim Cofala; Christian Kalfar; Jingge Xiao; Johanna Schrader; Michelle Tang; Wolfgang Nejdl
- Reference count: 17
- Primary result: TxAgent received Excellence Award in Open Science at CURE-Bench NeurIPS 2025 Challenge

## Executive Summary
TxAgent is an agentic AI system for therapeutic reasoning in clinical medicine that addresses the challenge of accurate, multi-step decision-making under strict safety constraints. The system uses a fine-tuned Llama-3.1-8B model integrated with ToolUniverse—a biomedical tool suite—to dynamically retrieve and reason over current therapeutic information via iterative retrieval-augmented generation. Comparative experiments demonstrated that TxAgent's Qwen2-1.5B-based tool retrieval outperformed various sparse and dense retrievers, with DailyMed integration significantly improving access to up-to-date drug label information.

## Method Summary
TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls through ToolUniverse, a biomedical tool suite. The system uses iterative retrieval-augmented generation (RAG) cycles where the LLM reformulates queries to extract therapeutic intent, retrieves relevant tool descriptions via a fine-tuned Qwen2-1.5B retriever, executes function calls, and integrates results. The approach was evaluated on the CURE-Bench NeurIPS 2025 Challenge validation set (459 questions) and test sets (2,097 and 2,491 questions) across three question styles: Open-ended (OE), Multiple-choice (MC), and Open-ended multiple-choice (OE-MC).

## Key Results
- TxAgent's Qwen2-1.5B retriever outperformed BM25 and other dense retrievers in tool selection accuracy
- DailyMed integration significantly improved performance by providing complete, version-controlled clinical narratives
- The system received Excellence Award in Open Science at the CURE-Bench NeurIPS 2025 Challenge
- Smaller models (Gemma3-4B, Qwen3-4B) achieved high accuracy when provided with retrieved context, suggesting cost-effective alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Iterative tool-augmented retrieval improves therapeutic reasoning accuracy by enabling multi-step information gathering
- The system reformulates queries to extract intent → retrieves relevant tool descriptions via ToolRAG → executes function calls via ToolUniverse → integrates results → decides whether additional retrieval is needed, creating retrieval cycles until sufficient information is gathered
- Core assumption: Therapeutic questions require decomposed multi-step reasoning rather than single-pass retrieval; errors in early steps can be corrected through iteration
- Evidence: TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls... integrating FDA Drug API, OpenTargets, and Monarch resources; In each iteration, the LLM decides whether additional up-to-date information is needed to answer the question... generating ToolRAG cycles
- Break condition: When query reformulation fails to capture therapeutic intent, or when tool descriptions lack sufficient semantic context for matching (observed with BM25)

### Mechanism 2
- Domain-specific fine-tuning of a smaller retriever model (Qwen2-1.5B) outperforms general-purpose sparse and dense retrievers for biomedical tool selection
- Fine-tuning on therapeutic questions with ground-truth tool-call labels teaches the retriever to map clinical intent to appropriate function names via cosine similarity, compensating for limited function description length
- Core assumption: Tool descriptions in ToolUniverse contain sufficient semantic signal when the retriever has learned domain-specific intent-to-function mappings
- Evidence: TxAgent itself uses a dense retriever with Qwen2-1.5B, which has been further fine-tuned on therapeutic medical questions with additional ground-truth tool-call labels; Figure 1 shows Qwen2-1.5B outperforms BM25 and other dense retrievers
- Break condition: When function descriptions lack domain-specific terminology or when queries contain novel intent patterns not seen during fine-tuning

### Mechanism 3
- Integrating DailyMed's structured product labeling provides comprehensive clinical narratives that reduce multi-hop retrieval overhead
- DailyMed tools return complete, version-controlled clinical narratives in a single call, whereas openFDA tools require multiple granular queries to assemble equivalent context, reducing token overhead and retrieval iterations
- Core assumption: Comprehensive clinical narratives (rather than granular metadata) better support therapeutic reasoning with acceptable context window costs
- Evidence: DailyMed tool integration grants TxAgent direct access to authoritative Structured Product Labeling (SPL), ensuring retrieval of complete, version-controlled clinical narratives; performance of Qwen2-1.5B is only surpassed when integrating DailyMed
- Break condition: When queries require precise metadata rather than narrative context, or when context window constraints make large narrative retrieval inefficient

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) vs. Tool-Calling
  - Why needed here: TxAgent extends RAG by dynamically selecting and executing API-style tools rather than retrieving from a fixed corpus; understanding this distinction is essential for debugging retrieval failures
  - Quick check question: Can you explain why BM25 fails when function descriptions are only 1-2 sentences?

- Concept: Sparse vs. Dense Retrieval
  - Why needed here: Section 4.2 shows retriever choice significantly impacts tool selection; sparse (BM25) fails on semantic matching while dense retrievers vary in performance
  - Quick check question: What type of retrieval task would favor BM25 over dense embeddings in this system?

- Concept: Fine-tuning for Context Utilization
  - Why needed here: Section 4.3.2 shows fine-tuned Llama-3.1-8B outperforms non-fine-tuned version with identical context; fine-tuning teaches models to leverage retrieved information
  - Quick check question: In the fixed retrieval experiments, why do smaller models (Gemma3-4B, Qwen3-4B) achieve high accuracy when provided retrieved context?

## Architecture Onboarding

- Component map: User query → Llama3.1 intent extraction → Qwen2-1.5B top-k tool retrieval → Llama3.1 parameter construction → ToolUniverse execution → Context integration → Iterate or terminate → Final answer

- Critical path: User query → Llama3.1 intent extraction → Qwen2-1.5B top-k tool retrieval → Llama3.1 parameter construction → ToolUniverse execution → Context integration → Iterate or terminate → Final answer

- Design tradeoffs:
  - Granular vs. narrative retrieval: openFDA provides token-efficient specific queries but requires multiple calls; DailyMed provides comprehensive narratives at higher token cost
  - Retriever size: Smaller fine-tuned retriever (1.5B) outperforms larger general-purpose retrievers but requires labeled training data
  - Context window: Multiple ToolRAG cycles improve accuracy but increase latency and token usage

- Failure signatures:
  - Repeated function calls with incorrectly formatted parameters
  - Correct tools retrieved but wrong tools selected by LLM
  - Function calls returning unexpected/insufficient information triggering unnecessary iterations

- First 3 experiments:
  1. Reproduce retriever comparison (Figure 1): Test BM25, dense retrievers, and Qwen2-1.5B on validation set tool selection accuracy
  2. Ablate DailyMed: Run TxAgent with/without DailyMed integration on OE-MC questions to measure delta
  3. Fixed retrieval test: Provide identical retrieved context to different LLMs (Llama3.1-8B fine-tuned vs. base, Gemma3-4B, Qwen3-4B) to isolate context utilization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DailyMed integration be optimized to provide fine-grained tool selection comparable to ToolUniverse's native tools, thereby reducing context window expansion and computational overhead?
- Basis in paper: Our extension using DailyMed does not employ such fine-grained selection. Consequently, it may retrieve information that would otherwise require several ToolUniverse function calls, potentially leading to larger context windows and increased computational overhead.
- Why unresolved: The paper demonstrates DailyMed improves performance but does not address the architectural tension between comprehensive single-call retrieval and token-efficient multi-call approaches
- What evidence would resolve it: Experiments comparing context window sizes, latency, and accuracy between fine-grained DailyMed tool variants versus the current monolithic implementation

### Open Question 2
- Question: What mechanism modifications could reduce the observed failure modes in TxAgent's tool calling, specifically repeated calls from incorrectly formatted parameters and selection of suboptimal tools despite correct retrieval?
- Basis in paper: Analyzing the evaluation runs of function calls, we observe several recurring issues: functions are called repeatedly due to incorrectly formatted input parameter names; incorrect functions are selected even when better candidates are retrieved; and some function calls fail to return the expected information.
- Why unresolved: The paper identifies these issues but does not propose or evaluate solutions; the integration of DailyMed addressed data access but not the underlying tool-selection decision process
- What evidence would resolve it: Ablation studies testing parameter validation layers, re-ranking mechanisms, or self-correction loops that explicitly target each failure mode

### Open Question 3
- Question: What is the optimal trade-off between model scale and retrieval augmentation quality for therapeutic reasoning, given that smaller models (Gemma3-4B, Qwen3-4B) achieved high accuracy when provided with retrieved context?
- Basis in paper: Smaller models, such as Gemma3-4B and Qwen3-4B, achieve high accuracy in the presence of retrieved information. This indicates that even smaller models can effectively utilize their context windows and could serve as cost-effective alternatives.
- Why unresolved: The experiments use fixed retrieved context from TxAgent runs rather than controlling for retrieval quality across model sizes, leaving the interaction between model capacity and retrieval dependence unclear
- What evidence would resolve it: Controlled experiments varying model size and retrieval quality factorially, measuring accuracy, calibration, and failure rates across therapeutic reasoning tasks

## Limitations

- Fine-tuning methodology remains partially unspecified due to competition constraints - ground-truth tool-call labels were unavailable to participants
- DailyMed integration represents a novel component with limited external validation beyond TxAgent's performance
- The iterative retrieval-augmented generation framework's scalability is untested beyond the CURE-Bench validation set

## Confidence

- **High confidence**: Retrieval quality improvements from DailyMed integration (supported by controlled experiments showing performance gains when added)
- **Medium confidence**: Qwen2-1.5B fine-tuning superiority (demonstrated within TxAgent framework but relies on unspecified training data)
- **Low confidence**: Generalization of iterative RAG cycles to broader therapeutic reasoning tasks (validated only on CURE-Bench dataset)

## Next Checks

1. Ablation study on DailyMed: Run TxAgent with/without DailyMed on OE-MC questions to quantify performance delta and assess token efficiency trade-offs
2. Generalizability test: Apply TxAgent framework to external biomedical datasets (e.g., PubMedQA, MedQA-USMLE) to evaluate cross-domain performance
3. Retriever robustness analysis: Test Qwen2-1.5B against BM25 and other dense retrievers on queries with novel intent patterns not seen during fine-tuning