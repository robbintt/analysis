---
ver: rpa2
title: 'Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned
  Energy Landscapes'
arxiv_id: '2512.17846'
source_url: https://arxiv.org/abs/2512.17846
tags:
- planning
- energy
- learning
- future
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Planning as Descent (PaD), a novel framework
  for offline goal-conditioned reinforcement learning that learns to synthesize trajectories
  by minimizing a goal-conditioned energy function over entire future plans. Instead
  of learning a policy or explicit planner, PaD learns an energy landscape that assigns
  low energy to dynamically plausible, goal-consistent trajectories.
---

# Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes

## Quick Facts
- **arXiv ID**: 2512.17846
- **Source URL**: https://arxiv.org/abs/2512.17846
- **Reference count**: 10
- **Primary result**: Achieves 95% success rate on OGBench cube tasks with narrow expert data, outperforming prior methods (68% peak) and showing improved efficiency when trained on noisy data

## Executive Summary
This paper introduces Planning as Descent (PaD), a novel framework for offline goal-conditioned reinforcement learning that learns to synthesize trajectories by minimizing a goal-conditioned energy function over entire future plans. Instead of learning a policy or explicit planner, PaD learns an energy landscape that assigns low energy to dynamically plausible, goal-consistent trajectories. Planning is achieved through gradient-based refinement in this energy landscape, with identical computation during training and inference to reduce train-test mismatch. The method is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected. PaD is evaluated on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95% success, strongly outperforming prior methods that peak at 68%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. These results suggest that learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.

## Method Summary
PaD learns a goal-conditioned energy function over latent trajectories, where low energy indicates dynamical plausibility and goal consistency. The method uses a 2-layer MLP encoder to map states to 128-dim latents, a convolutional Transformer-based energy model that conditions on past states, goals, and temporal tokens, and a lightweight projector network to maintain latent-space optimization on the encoder-induced manifold. Training uses hindsight goal relabeling to convert reward-free trajectories into goal-conditioned examples, with corruption and gradient refinement steps performed identically during training and inference. At test time, PaD generates multiple trajectory candidates with different temporal hypotheses, refines them via gradient descent in the energy landscape, and selects the lowest-energy plan for execution through an inverse dynamics model. The unified training-inference computation eliminates the train-test mismatch common in decoupled model-based planning pipelines.

## Key Results
- **State-of-the-art performance**: PaD achieves 95% success rate on cube manipulation tasks with narrow expert data, compared to 68% for prior methods
- **Data efficiency**: PaD trains in 200K updates (1.66× overhead vs. 1M updates for baselines) while achieving superior performance
- **Robustness to noise**: Training on noisy, suboptimal data improves both success rate and plan efficiency, with mean episode lengths of 63 steps versus 78 for expert-only training
- **Efficiency without replanning**: PaD maintains performance with fewer replanning steps when trained on diverse data, demonstrating robustness-efficiency tradeoffs

## Why This Works (Mechanism)

### Mechanism 1: Unified Trajectory Evaluation and Synthesis via Energy-Based Models
- **Claim**: Learning to evaluate trajectories provides a robust alternative to direct policy learning or generative trajectory modeling in offline settings.
- **Mechanism**: Instead of learning a policy, dynamics model, or trajectory generator, PaD learns a goal-conditioned energy function over entire latent trajectories. Low energy indicates dynamical plausibility and goal consistency; high energy indicates incompatibility. Planning emerges as gradient descent in this energy landscape, iteratively refining candidate trajectories.
- **Core assumption**: The energy landscape can be shaped such that gradient descent produces valid plans without requiring explicit forward dynamics simulation or likelihood-based trajectory generation.
- **Evidence anchors**:
  - [abstract] "PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape."
  - [Section 4.2] "PaD performs planning by iteratively refining the latent trajectory through gradient descent on this energy landscape."
  - [corpus] Related work (Learning from Reward-Free Offline Data) supports planning in latent spaces but uses explicit dynamics models; PaD differs by avoiding forward rollouts entirely.

### Mechanism 2: Training-as-Inference Alignment Reduces Train-Test Mismatch
- **Claim**: Using identical refinement dynamics during training and inference mitigates exploitation of model errors that plague decoupled model-based planning pipelines.
- **Mechanism**: The same gradient-based refinement procedure (Eq. 1) is applied during both training and inference. The energy landscape is shaped around the exact descent dynamics used at test time, ensuring inference corresponds to optimization behavior the model has been trained to support.
- **Core assumption**: The training loss (smooth-L1 denoising) provides sufficient signal to shape the energy landscape for effective planning, without requiring explicit value backups or reward signals.
- **Evidence anchors**:
  - [abstract] "using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines"
  - [Section 4.3] "A defining property of PaD is that the refinement procedure used for planning is identical during training and inference. This 'training-as-inference' principle regularizes the energy landscape such that its gradient flow implements the desired planning behavior."

### Mechanism 3: Manifold Projector Stabilizes Latent-Space Optimization
- **Claim**: A lightweight projector network is essential for preventing gradient descent from pushing latent states off the encoder-induced manifold.
- **Mechanism**: After each gradient refinement step, the updated trajectory is passed through a shallow projector network p_θ that maps refined latents back onto the encoder-induced manifold. This prevents energy gradients from producing degenerate latents that do not correspond to valid encoded observations.
- **Core assumption**: The encoder-induced manifold captures the structure of valid latent states, and the projector can be learned to map arbitrary vectors back to this manifold.
- **Evidence anchors**:
  - [Section 4.2] "To ensure that refinement remains on or near the encoder-induced manifold, the updated trajectory is passed through a shallow learnable projector p_θ."
  - [Table 4] Removing the projector causes success rate to drop from 98% to 50% on single-cube-noisy-v0.

## Foundational Learning

- **Concept: Energy-Based Models (EBMs)**
  - **Why needed here**: PaD formulates planning entirely within an EBM framework. Understanding that EBMs assign scalar energies to configurations and perform inference via energy minimization is prerequisite.
  - **Quick check question**: Can you explain why EBMs avoid computing the partition function during training, and how inference differs from likelihood-based models?

- **Concept: Hindsight Goal Relabeling**
  - **Why needed here**: PaD is trained via self-supervised hindsight relabeling, which converts reward-free trajectories into goal-conditioned training examples by treating visited states as goals.
  - **Quick check question**: Given a trajectory of states, how would you construct a training example with hindsight relabeling? What is the role of the temporal target λ in PaD's relabeling scheme?

- **Concept: Second-Order Optimization / Hessian-Vector Products**
  - **Why needed here**: PaD's training requires backpropagating through the refinement process, which involves computing gradients of gradients with respect to model parameters.
  - **Quick check question**: Why does training through an iterative optimization process require second-order derivatives? How does PaD efficiently compute these terms?

## Architecture Onboarding

- **Component map**: State encoder f_θ (2-layer MLP + LayerNorm) → Energy model E_θ (conv encoder → 3-block decoder-only Transformer) → Projector p_θ (2-layer MLP) → Inverse dynamics g_ψ (2-layer MLP)
- **Critical path**:
  1. Encode past states: z_past = f_θ(s_past)
  2. Initialize future latents: z_future^0 ~ N(0, I) (inference) or corrupted clean latents (training)
  3. For T refinement steps:
     - Compute energy: E = E_θ(z_future, z_past, s_g, λ)
     - Gradient step: z_future ← z_future - η∇E
     - Project: z_future ← p_θ(z_future)
  4. Select lowest-energy trajectory from B candidates with different λ hypotheses
  5. Decode actions via inverse dynamics: a_t = g_ψ(z_t, z_{t+1})

- **Design tradeoffs**:
  - **Refinement steps T**: Paper uses T=2. More steps increase computation linearly but may improve plan quality.
  - **Number of hypotheses B**: Paper uses B=768. More candidates improve coverage of temporal targets but increase memory/compute.
  - **Replanning interval N**: Frequent replanning (N=1) is robust but costly (100ms/step). N=4-8 offers efficiency-robustness tradeoff, especially for models trained on diverse data.
  - **Training budget**: Paper trains for 200K updates vs. 1M for baselines. Scaling properties unexplored.

- **Failure signatures**:
  - **Training instability with rising loss**: Likely missing or misconfigured projector. Check Figure 2 for expected curve shape.
  - **Low success rate despite low training loss**: May indicate poor generalization to test-time goals; verify evaluation uses correct multi-goal protocol.
  - **Plans that reach goal but are highly inefficient**: May indicate λ conditioning not working; check temporal token encoding.
  - **Rapid performance degradation with larger replanning intervals**: Expected for models trained on narrow expert data; consider training on diverse data instead.

- **First 3 experiments**:
  1. **Validate projector necessity**: Train PaD with and without p_θ on single-cube-noisy-v0. Expect dramatic success drop (98% → 50%) without projector. Confirms manifold constraint is essential.
  2. **Ablate replanning interval**: Evaluate PaD-play and PaD-noisy across N ∈ {1, 2, 4, 8, 16}. Expect PaD-noisy to maintain performance at larger N while PaD-play degrades rapidly. Quantifies robustness-efficiency tradeoff.
  3. **Compare data regimes on plan efficiency**: Measure episode length distributions for PaD-play vs. PaD-noisy on successful rollouts. Expect PaD-noisy to produce systematically shorter plans (mean ~63 vs ~78 steps). Tests whether diverse suboptimal data improves planning efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the PaD framework be effectively extended to high-dimensional visual observations (pixel-based inputs) without losing planning stability?
- **Basis in paper**: [explicit] Section 6 states, "Extending PaD to pixel-based or multimodal settings would require integrating visual representation learning... [and] remains an open and promising challenge."
- **Why unresolved**: The current architecture and experiments are restricted to state-based inputs. Visual inputs introduce high dimensionality and representation noise that may disturb the smooth energy landscape required for gradient-based descent.
- **What evidence would resolve it**: Successful application of PaD to image-based benchmarks (e.g., OGBench visual tasks), demonstrating that the energy landscape remains navigable despite the complexity of visual latent spaces.

### Open Question 2
- **Question**: How does the performance and computational cost of PaD scale with increased model capacity and longer planning horizons?
- **Basis in paper**: [explicit] The authors note in Section 6 that the "experiments are conducted in a relatively modest computational regime" and "Understanding these scaling properties is an important direction for future work."
- **Why unresolved**: The current study utilizes lightweight models (6.5M parameters) and short horizons (~10² steps). It is untested whether the optimization stability and 1.66× training overhead remain manageable at the scale required for more complex, long-horizon tasks.
- **What evidence would resolve it**: Empirical analysis of success rates and training stability curves when scaling parameter counts and horizon lengths by orders of magnitude on more complex datasets.

### Open Question 3
- **Question**: To what extent can the inverse dynamics model be trained on reduced labeled data without compromising the execution quality of the plan?
- **Basis in paper**: [explicit] Section 4.5 claims the inverse dynamics model can be trained from a "substantially smaller labeled subset," but explicitly adds, "We leave a systematic study of the trade-off between action-label availability and execution performance to future work."
- **Why unresolved**: While the planner is action-free, execution requires an inverse dynamics model. The precise data efficiency of this separate component has not been quantified in the results.
- **What evidence would resolve it**: Ablation studies reporting success rates while varying the percentage of available action labels (e.g., 1%, 10%, 100%) used for training the inverse dynamics model $g_\psi$.

## Limitations

- **Domain restriction**: Evaluation limited to OGBench cube manipulation tasks, may not generalize to high-dimensional, real-world robotics scenarios
- **State-based inputs only**: Method relies on state observations rather than visual inputs, limiting applicability to domains requiring visual perception
- **Computational overhead**: Second-order gradient computation through iterative refinement requires substantial compute despite 1.66× training efficiency gain
- **Hyperparameter sensitivity**: Performance heavily dependent on projector network configuration and refinement step count, requiring careful tuning for deployment

## Confidence

- **High Confidence**: The core mechanism of unified trajectory evaluation and synthesis via energy-based models, and the demonstrated performance advantage over prior methods on the specified benchmarks
- **Medium Confidence**: The claims about training-as-inference alignment reducing train-test mismatch and the benefits of manifold projection for latent-space optimization, as these depend on architectural choices that may not transfer to other domains
- **Medium Confidence**: The surprising finding that training on noisy, suboptimal data improves plan efficiency, though this is observed only within the cube manipulation domain and may not generalize to all offline RL settings

## Next Checks

1. **Domain Transfer Test**: Evaluate PaD on a different task domain (e.g., locomotion or manipulation with visual inputs) to assess generalization beyond cube manipulation
2. **Sample Efficiency Analysis**: Measure learning curves with varying amounts of offline data to determine if PaD's advantage persists when data is limited or abundant
3. **Plan Robustness to Noise**: Test PaD's performance when the initial trajectory candidates are corrupted with varying levels of noise during inference, to quantify the method's robustness to suboptimal initialization