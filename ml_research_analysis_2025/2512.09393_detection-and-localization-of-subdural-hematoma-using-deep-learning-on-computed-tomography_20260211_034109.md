---
ver: rpa2
title: Detection and Localization of Subdural Hematoma Using Deep Learning on Computed
  Tomography
arxiv_id: '2512.09393'
source_url: https://arxiv.org/abs/2512.09393
tags:
- hematoma
- subdural
- detection
- segmentation
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a multimodal deep-learning framework for detecting
  and localizing subdural hematomas (SDH) in head CT scans. The approach integrates
  clinical data (demographics, comorbidities, medications) with 3D CT volumes and
  2D segmentation-derived probability maps, combining an XGBoost model, convolutional
  neural networks, and a transformer-enhanced Swin-Unet hybrid for hematoma localization.
---

# Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography

## Quick Facts
- **arXiv ID:** 2512.09393
- **Source URL:** https://arxiv.org/abs/2512.09393
- **Reference count:** 34
- **Primary result:** Multimodal ensemble achieved AUC 0.9407 for SDH detection on 25,315 CT studies

## Executive Summary
This study presents a multimodal deep-learning framework for detecting and localizing subdural hematomas (SDH) in head CT scans. The approach integrates clinical data with imaging-derived features using an ensemble of models including XGBoost, convolutional neural networks, and a transformer-enhanced segmentation network. Evaluated on over 25,000 CT studies, the framework achieved high detection accuracy (AUC 0.9407) and precise localization (Dice score 0.77), outperforming single-modality approaches. The transparent, anatomically interpretable outputs support clinical decision-making for triage and treatment planning.

## Method Summary
The framework combines tabular clinical features (demographics, comorbidities, medications) with 3D CT volumes and 2D segmentation-derived probability maps. Clinical data is processed through XGBoost, while CT volumes are analyzed by multiple 3D CNNs and a hybrid Swin-Unet segmentation model. The segmentation model outputs probability maps that are further processed by additional 3D CNNs. Models are selected via greedy forward ensemble selection based on validation AUC, with final predictions generated by averaging probabilities from 9 models. CT preprocessing includes Hounsfield Unit conversion, soft-tissue windowing (0-130 HU), skull stripping, and standardization to 15 slices.

## Key Results
- Multimodal ensemble achieved AUC 0.9407 (95% CI: 0.930–0.951) for SDH detection
- Segmentation model achieved Dice score of 0.77 for hematoma localization
- Clinical data alone provided modest performance (AUC 0.75)
- Individual CNN models on CT volumes achieved AUCs of 0.922 and 0.926

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining structured clinical variables with imaging-based predictions via probability averaging yields higher detection accuracy than either modality alone.
- **Mechanism:** Clinical tabular data captures patient-level risk factors invisible to imaging models, while imaging models detect visual hematoma signatures. Averaging predicted probabilities from independently trained models fuses these complementary information sources without requiring joint training.
- **Core assumption:** Clinical risk factors and imaging features provide non-redundant predictive signal; errors across modalities are at least partially uncorrelated.
- **Evidence anchors:** Clinical variables alone (AUC 0.75) underperform imaging models (AUCs 0.922 and 0.926), while multimodal ensemble (AUC 0.9407) exceeds best individual model by approximately 2%.
- **Break condition:** If clinical variables become systematically missing or corrupted in deployment, or if imaging models already saturate performance ceiling, tabular contribution diminishes.

### Mechanism 2
- **Claim:** 3D convolutional architectures capture inter-slice spatial continuity of SDH presentations that 2D slice-by-slice approaches miss.
- **Mechanism:** SDH is inherently a 3D pathology extending across multiple axial slices. 3D CNN kernels learn hierarchical features that encode both within-slice texture and between-slice anatomical continuity, which 2D models cannot directly access.
- **Core assumption:** The 15-slice standardization preserves sufficient spatial information for the model to learn meaningful inter-slice patterns; downsampling does not destroy critical hematoma morphology.
- **Evidence anchors:** CNN ensemble on CT volumes achieved AUC 0.922, substantially outperforming clinical-only baseline.
- **Break condition:** If slice standardization loses critical hematoma extent information, or if SDH presentations are confined to fewer slices than the 15-slice minimum, inter-slice learning degrades.

### Mechanism 3
- **Claim:** Hybrid Swin Transformer + CNN segmentation architecture enables precise localization by fusing global anatomical context with local boundary detection.
- **Mechanism:** The convolutional path captures fine-grained local hematoma boundaries, while the Swin Transformer branch models global context such as midline shift and hemispheric asymmetry. Multi-scale feature fusion during decoding combines both streams.
- **Core assumption:** Global anatomical context improves segmentation; the 335 manually annotated scans provide sufficient training signal despite limited size.
- **Evidence anchors:** Hybrid model achieved Dice 0.768 vs. vanilla Swin U-Net 0.692, demonstrating fusion benefit.
- **Break condition:** If training annotations have systematic errors or if test SDH presentations differ substantially from annotated distribution, localization accuracy drops.

## Foundational Learning

- **Concept:** Hounsfield Units (HU) and CT windowing
  - **Why needed here:** The paper uses soft-tissue windowing (0-130 HU, centered at 65 HU) to enhance subdural contrast. Understanding how HU values map to tissue densities and why windowing matters is essential for preprocessing and debugging.
  - **Quick check question:** Why would a soft-tissue window (0-130 HU) be preferred over a bone window for detecting subdural hematomas?

- **Concept:** Dice coefficient for segmentation evaluation
  - **Why needed here:** The segmentation model's primary metric is Dice score (0.77 achieved). Dice measures overlap between predicted and ground truth masks, penalizing both false positives and false negatives equally.
  - **Quick check question:** If a model predicts a mask that covers 80% of the true hematoma but also includes 50% extra area, would the Dice score be higher or lower than if it predicted exactly the true area?

- **Concept:** Ensemble diversity and greedy selection
  - **Why needed here:** The paper uses greedy forward selection to build an ensemble from 10 candidate models. Understanding why model diversity matters (uncorrelated errors improve ensemble) is critical for reproducing results.
  - **Quick check question:** Why might adding a fifth CNN trained on the same data with the same architecture not improve the ensemble, even if it has similar individual performance?

## Architecture Onboarding

- **Component map:** [Tabular Features] → XGBoost → Probability_1 → [CT Volume (15 slices)] → 3D CNN (×5) → Probability_2-6 → [CT Volume] → 2D Segmentation → Probability Maps → 3D CNN (×4) → Probability_7-10 → All 10 probabilities → Average → Final SDH Probability

- **Critical path:** CT preprocessing → slice standardization to 15 slices → soft-tissue windowing → skull stripping → model inference. Errors in skull stripping or slice standardization propagate to both detection and segmentation branches.

- **Design tradeoffs:**
  - 2D vs. 3D segmentation: Paper chose 2D for computational efficiency with "similar performance" to 3D alternatives—assumption requires validation on your hardware.
  - 15-slice standardization: Balances information retention vs. memory; may lose thin SDH confined to fewer slices.
  - Ensemble size (10 models): Adding more showed no validation improvement—suggesting diminishing returns, but may not generalize to new datasets.

- **Failure signatures:**
  - High AUC but low Dice: Detection works but localization fails—check segmentation training data quality.
  - Good performance on large hematomas, poor on small: Model may have learned volume threshold bias—examine training distribution.
  - Calibration drift in deployment: Clinical variable distributions shifted—re-calibrate on local population.

- **First 3 experiments:**
  1. **Reproduce tabular-only baseline:** Train XGBoost on provided clinical features; verify AUC ≈ 0.75. If substantially different, check feature engineering or cohort definitions.
  2. **Ablate segmentation from ensemble:** Run full ensemble vs. CNN-only ensemble; quantify segmentation contribution to detection improvement.
  3. **External slice count sensitivity:** Test model on CTs with varying slice counts (before standardization) to characterize performance degradation when original scans have fewer than 15 valid slices.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the multimodal ensemble perform when externally validated on datasets from institutions with different CT scanner manufacturers, imaging protocols, and patient demographics?
- **Basis in paper:** [explicit] "Additionally, external validation across institutions and imaging protocols will be critical for assessing real-world performance."
- **Why unresolved:** All data came from a single hospital system (Hartford HealthCare) with uniform data collection protocols; model generalizability to other clinical environments remains unknown.
- **What evidence would resolve it:** Performance metrics (AUC, Dice score) evaluated on independent datasets from geographically diverse hospitals with different scanner types and patient populations.

### Open Question 2
- **Question:** Can semi-supervised or weakly supervised learning approaches achieve comparable segmentation performance while reducing reliance on manual annotations?
- **Basis in paper:** [explicit] "Leveraging semi-supervised or weakly supervised learning could further reduce dependence on manual labeling, making future iterations of the model more scalable."
- **Why unresolved:** Only 335 manually annotated CT studies were available for segmentation training, creating a scalability and cost bottleneck for broader deployment.
- **What evidence would resolve it:** Comparative study showing Dice scores for semi-supervised models trained with varying proportions of labeled versus unlabeled CT scans.

### Open Question 3
- **Question:** Does prospective integration of this framework into clinical triage workflows reduce time-to-intervention and improve patient outcomes?
- **Basis in paper:** [inferred] The authors plan to deploy for "prospective data analysis and real-time use" and aim to "reduce the 'time to surgery' penalty" and "enhance patient outcomes."
- **Why unresolved:** All reported results are retrospective; no prospective trial has assessed whether model predictions actually change clinical decisions or improve care.
- **What evidence would resolve it:** A prospective clinical study measuring time-to-intervention, surgical timing, and neurological outcomes comparing patients triaged with versus without AI assistance.

## Limitations

- Segmentation model trained on limited annotations (335 studies), raising generalization concerns across diverse protocols
- Exact architectural specifications for 3D CNN and hybrid segmentation models require supplementary materials
- Clinical utility in real-world triage settings unproven—no evaluation of time-to-detection or workflow integration

## Confidence

- **Detection performance (AUC 0.9407):** High confidence - well-validated on large test set with clear metrics
- **Segmentation accuracy (Dice 0.77):** Medium confidence - limited training data and manual annotations
- **Multimodal fusion benefit:** High confidence - ablation studies clearly show improvement over single modalities
- **Clinical interpretability:** Medium confidence - anatomical outputs demonstrated but real-world validation pending

## Next Checks

1. **External validation on independent cohort:** Test the framework on CT scans from different institutions with varying scanner parameters to assess generalizability.
2. **Calibration assessment across risk strata:** Evaluate model calibration separately for high-risk (anticoagulated) and low-risk patients to ensure clinical reliability.
3. **Temporal stability analysis:** Measure performance degradation over time by testing on chronologically separated subsets of the data to identify potential concept drift.