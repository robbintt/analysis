---
ver: rpa2
title: Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using
  Vision Language Models
arxiv_id: '2601.22754'
source_url: https://arxiv.org/abs/2601.22754
tags:
- extraction
- visual
- relation
- knowledge
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates vision language models for extracting procedural
  knowledge from industrial troubleshooting guides, comparing standard and augmented
  prompting strategies. Results show both models struggle with entity extraction (F1:
  0.24-0.34) and severely underperform on relation extraction (F1: <0.11).'
---

# Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models

## Quick Facts
- arXiv ID: 2601.22754
- Source URL: https://arxiv.org/abs/2601.22754
- Authors: Guillermo Gil de Avalle; Laura Maruster; Christos Emmanouilidis
- Reference count: 3
- Key outcome: VLMs struggle with entity extraction (F1: 0.24-0.34) and relation extraction (F1: <0.11); Qwen2-VL-7B achieved higher peak performance (F1: 0.78) but exhibited "infinite loop collapse" failure modes, while Pixtral-12B demonstrated consistent but universally limited performance.

## Executive Summary
This paper evaluates vision language models for extracting procedural knowledge from industrial troubleshooting guides, comparing standard and augmented prompting strategies. Results show both models struggle with entity extraction (F1: 0.24-0.34) and severely underperform on relation extraction (F1: <0.11). Qwen2-VL-7B achieved higher peak performance (F1: 0.78) but exhibited "infinite loop collapse" failure modes, while Pixtral-12B demonstrated consistent but universally limited performance. Augmented prompting improved Qwen's relation extraction but degraded Pixtral's results. The primary bottleneck is spatial reasoning for diagram relationships, suggesting current VLMs are unsuitable for autonomous deployment in safety-critical settings, though they may support human-in-the-loop workflows with reliable hallucination detection.

## Method Summary
The study evaluates Qwen2-VL-7B and Pixtral-12B on extracting entities and relations from 12 proprietary Dutch troubleshooting guides. The task involves converting flowchart diagrams into structured JSON format with three entity types (Condition, Action, Decision) and one relation type (isPreceededBy). Two prompting strategies are compared: standard instruction-guided prompts versus augmented prompts with visual convention descriptions. Evaluation uses entity F1 (0.9 token-overlap threshold) and relation F1 (content-based matching). Models are run on a single Nvidia A100 GPU with 40GB VRAM without fine-tuning.

## Key Results
- Entity extraction F1: 0.24-0.34 across both models and strategies
- Relation extraction F1: <0.11 for both models with both prompting strategies
- Qwen2-VL-7B peak performance: F1 0.78 on one document, but exhibited "infinite loop collapse" after 15-25 correct extractions
- Augmented prompting improved Qwen's relation extraction but degraded Pixtral's results
- Spatial reasoning limitations prevent accurate relation extraction from diagram layouts

## Why This Works (Mechanism)
The study demonstrates that current VLMs face fundamental limitations in spatial reasoning required for extracting procedural relationships from flowchart diagrams. While entity identification shows moderate success (0.24-0.34 F1), the inability to parse connecting arrows and densely packed structures leads to severe underperformance in relation extraction (<0.11 F1). The "infinite loop collapse" failure mode in Qwen2-VL-7B reveals a critical reliability barrier where models generate hundreds of near-identical entities after initial success, exhausting token budgets.

## Foundational Learning
- **Entity extraction**: Identifying diagram components as structured data types
  - Why needed: Forms the basis for procedural knowledge representation
  - Quick check: Verify entity F1 scores exceed 0.30 threshold

- **Relation extraction**: Mapping connections between identified entities
  - Why needed: Captures procedural flow and decision logic
  - Quick check: Confirm relation F1 scores remain below 0.15

- **Spatial reasoning**: Interpreting diagram layouts and connecting elements
  - Why needed: Critical for understanding flowchart relationships
  - Quick check: Document density correlates with extraction difficulty

- **Prompt engineering**: Designing effective instructions for VLM tasks
  - Why needed: Influences model performance and reliability
  - Quick check: Compare standard vs augmented prompting outcomes

- **Hallucination detection**: Identifying generated content not grounded in input
  - Why needed: Ensures reliability in safety-critical applications
  - Quick check: Monitor for repetition patterns indicating infinite loops

- **Evaluation metrics**: Defining precise matching criteria for extracted knowledge
  - Why needed: Enables quantitative comparison of model performance
  - Quick check: Verify F1 computation matches described methodology

## Architecture Onboarding

**Component map**: VLM (Qwen2-VL-7B or Pixtral-12B) -> Prompt strategy (standard/augmented) -> Flowchart image -> JSON output -> Evaluation pipeline (lemmatization + matching)

**Critical path**: Image input -> Entity extraction -> Relation extraction -> JSON formatting -> Evaluation scoring

**Design tradeoffs**: 
- Standard prompting provides baseline performance but misses visual conventions
- Augmented prompting adds visual description context but may confuse certain models
- No fine-tuning maintains generalizability but limits task-specific optimization
- Single GPU constraints prevent testing larger architectures that might overcome limitations

**Failure signatures**:
- Entity extraction: Incomplete identification of diagram components (F1 <0.30)
- Relation extraction: Zero F1 scores despite moderate entity performance
- Hallucination: Infinite loop collapse generating hundreds of near-identical entities
- Spatial reasoning: Inability to parse overlapping arrows and dense layouts

**First experiments**:
1. Run standard prompting on Qwen2-VL-7B with single flowchart page to verify entity extraction baseline
2. Test augmented prompting with visual convention descriptions on Pixtral-12B to assess relation extraction improvement
3. Monitor token generation patterns to detect early signs of infinite loop collapse

## Open Questions the Paper Calls Out

**Open Question 1**: Can text-only LLMs achieve comparable or better performance than VLMs for procedural knowledge extraction from troubleshooting guides when relation extraction is already severely limited? The study only evaluated VLMs; text-only approaches were not tested despite spatial reasoning being the primary bottleneck.

**Open Question 2**: What triggers the "infinite loop collapse" failure mode in Qwen2-VL, and can it be reliably predicted or prevented? Documents with identical layouts produced wildly different outcomes (F1 from 0.00 to 0.78), and the authors could not identify the specific trigger.

**Open Question 3**: Can constrained decoding strategies (e.g., repetition detection validators) effectively mitigate infinite loop hallucinations without degrading extraction quality? Proposed as mitigation strategy but not implemented or tested in the study.

**Open Question 4**: Do larger VLM architectures (>12B parameters) overcome the spatial reasoning bottleneck for relation extraction in industrial diagrams? Only 7B and 12B parameter models were evaluated; it remains unknown whether scaling addresses the fundamental relation extraction failure (F1 <0.11).

## Limitations
- Proprietary Dutch troubleshooting guides limit external validation and reproducibility
- Critical details about exact prompt templates remain unspecified
- Hardware constraints (40GB VRAM) prevented evaluation of larger architectures that might overcome spatial reasoning limitations
- "Infinite loop collapse" failure mode represents a fundamental reliability barrier that cannot be mitigated through prompting alone

## Confidence

**High confidence**: Measured performance metrics (F1 scores) and identified failure modes are reliably reported

**Medium confidence**: General conclusions about VLM limitations for spatial reasoning tasks are supported by evidence

**Low confidence**: Specific recommendations for practical deployment scenarios require additional validation beyond the studied domain

## Next Checks

1. Replicate entity extraction performance using publicly available flowchart datasets to verify the 0.24-0.34 F1 range is not dataset-specific

2. Test the augmented prompting strategy on additional VLM architectures to determine if improvements generalize beyond the two studied models

3. Implement and evaluate the proposed "reliable hallucination detection" mechanisms in human-in-the-loop scenarios to assess practical utility