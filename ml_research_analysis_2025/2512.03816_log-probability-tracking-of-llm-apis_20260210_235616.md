---
ver: rpa2
title: Log Probability Tracking of LLM APIs
arxiv_id: '2512.03816'
source_url: https://arxiv.org/abs/2512.03816
tags:
- changes
- token
- change
- each
- logprobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM APIs are expected to serve consistent models over time, but
  users cannot verify this consistency in practice. Existing audit methods are too
  expensive for continuous monitoring, leaving API changes largely unmonitored.
---

# Log Probability Tracking of LLM APIs

## Quick Facts
- **arXiv ID**: 2512.03816
- **Source URL**: https://arxiv.org/abs/2512.03816
- **Reference count**: 30
- **Primary result**: Logprob tracking detects LLM API changes 1,000x cheaper than existing methods, identifying undocumented model modifications in production.

## Executive Summary
LLM APIs are expected to serve consistent models over time, but users cannot verify this consistency in practice. Existing audit methods are too expensive for continuous monitoring, leaving API changes largely unmonitored. We propose logprob tracking (LT), a cost-effective method that detects changes by analyzing the log probabilities of a single output token using a simple statistical test. LT is up to 1,000x cheaper and more sensitive than existing methods, detecting changes as small as one fine-tuning step. We introduce the TinyChange benchmark to evaluate detection sensitivity and demonstrate LT's effectiveness across multiple model modifications. Deploying LT at scale across hundreds of API endpoints, we identified dozens of unexplained changes, showing that undocumented model changes are both detectable and prevalent. LT provides a lightweight, practical first line of defense for reproducibility and integrity monitoring.

## Method Summary
The method tracks changes in LLM APIs by collecting log probability distributions for a single output token from repeated queries. It uses a permutation test comparing logprob samples from reference and current time windows to detect statistical shifts. The approach requires only one token per query and requests `top_logprobs` from the API, making it dramatically cheaper than existing methods while maintaining high sensitivity to model modifications including fine-tuning, LoRA, pruning, and noise injection.

## Key Results
- LT detects changes as small as one fine-tuning step with ROC AUC > 0.95
- Method is up to 1,000x cheaper than existing LLM change detection approaches
- Real-world deployment identified dozens of unexplained changes across hundreds of API endpoints
- Single-token prompts are sufficient for reliable detection, reducing monitoring costs significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Log probabilities provide higher-resolution signal than discrete output tokens
- **Mechanism**: Discrete tokens are samples from a distribution, collapsing continuous probability information. By accessing `top-k` logprobs, the method observes the underlying probability distribution directly. Model modifications manifest as shifts in this distribution before changing sampled tokens.
- **Core assumption**: API returns `top_logprobs` and tokenizer remains constant
- **Break condition**: If providers disable `logprobs` access or enforce output token caching

### Mechanism 2
- **Claim**: Statistical aggregation distinguishes model shifts from stochastic inference noise
- **Mechanism**: Treats individual logprob values as samples from a stationary distribution. Collects N samples and applies two-sample permutation test on mean absolute distance between token probabilities, filtering out high-variance noise.
- **Core assumption**: Unintentional non-determinism is stationary and zero-mean relative to model change magnitude
- **Break condition**: If inference noise magnitude increases significantly, swamping the signal

### Mechanism 3
- **Claim**: Minimal prompts (single tokens) suffice to detect global parameter changes
- **Mechanism**: Model modifications affect global parameters, propagating effects to output distribution of even arbitrary prompts like "x".
- **Core assumption**: Model changes are global parameter updates rather than localized prompt-specific patches
- **Break condition**: If provider swaps model only for specific system prompts while leaving base distribution untouched

## Foundational Learning

- **Concept: Permutation Testing**
  - **Why needed here**: Verifies if two logprob samples differ significantly without assuming normal distribution
  - **Quick check**: Can you explain why permutation test is robust to non-Gaussian noise in LLM inference logs?

- **Concept: Log Probabilities (Logprobs)**
  - **Why needed here**: Essential for interpreting the monitor's signal and understanding relationship between logits, softmax, and returned log probabilities
  - **Quick check**: If temperature T=1, how does logprob value relate to token probability?

- **Concept: LLM Inference Non-determinism**
  - **Why needed here**: Distinguishes between model change and background noise from GPU kernel scheduling and batch composition
  - **Quick check**: Why does batching requests together in production API cause logprobs of single request to fluctuate?

## Architecture Onboarding

- **Component map**: Probe Client -> Time-Window Buffer -> Hypothesis Engine
- **Critical path**: Availability of `logprobs` parameter in API response
- **Design tradeoffs**:
  - Sensitivity vs. Cost: Increasing samples improves detection but increases cost linearly
  - Prompt Length: Longer prompts offer marginal (<1% AUC) improvement but increase token costs significantly
- **Failure signatures**:
  - API Constraints: Providers may block requests with `max_tokens < 16`
  - False Positives: High traffic variance causing standard deviation spikes
  - Evasion: Providers caching responses for specific monitoring prompt "x"
- **First 3 experiments**:
  1. Baseline Variance: Query stable local model 1,000 times to measure natural logprob standard deviation
  2. TinyChange Reproduction: Fine-tune local model for 1 step and verify permutation test detection
  3. Live Monitoring: Deploy probe on public endpoint for 24 hours to visualize time-series drift

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can sequential analysis methods like CUSUM control charts improve detection latency or sensitivity compared to current permutation test?
- **Basis in paper**: Authors state alternative approaches like CUSUM could be explored
- **Why unresolved**: Paper uses static permutation test without evaluating online change-point detection algorithms
- **What evidence would resolve it**: Comparison of detection delay between permutation method and CUSUM on TinyChange benchmark streams

### Open Question 2
- **Question**: Can providers successfully evade logprob tracking through limited caching of short-token responses without creating detectable inconsistencies?
- **Basis in paper**: Section discusses LT evasion through caching or identifying monitoring queries
- **Why unresolved**: Paper theorizes evasion is difficult but doesn't simulate adversarial providers
- **What evidence would resolve it**: Red-team simulation measuring false negative rate from caching strategies

### Open Question 3
- **Question**: Do specific model modifications like adjusting generation-length parameters effectively evade detection by leaving first-token log-probabilities unchanged?
- **Basis in paper**: Notes theoretical blind spot for modifications leaving initial tokens unchanged
- **Why unresolved**: Observes theoretical blind spot but doesn't empirically test method's blindness
- **What evidence would resolve it**: Targeted experiment applying EOS-bias modifications to verify LT failure

## Limitations

- Method fundamentally depends on availability of logprob output from LLM API
- Statistical power trade-off requires collecting N samples per comparison window
- Real-world noise validation lacks systematic validation of false positive rates
- Generalizability to model swaps not directly tested - TinyChange variants represent perturbations rather than wholesale substitutions

## Confidence

- **High Confidence**: Core statistical mechanism is well-established and controlled experiments demonstrate reliable detection
- **Medium Confidence**: Cost comparison and sensitivity claims supported by benchmark but rely on specific conditions
- **Low Confidence**: Claim that "undetected changes are prevalent" lacks quantitative backing on false positive rates

## Next Checks

1. Deploy LT monitoring on stable, known-good API endpoint for 72 hours to establish baseline false positive rates and characterize natural variance under realistic load

2. Extend TinyChange benchmark to include direct comparisons between completely different base models to verify detection of wholesale model swaps

3. Conduct systematic survey of major LLM API providers to determine which support logprob output and whether these policies are documented or subject to change