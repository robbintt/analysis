---
ver: rpa2
title: 'RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free
  Guidance'
arxiv_id: '2509.25604'
source_url: https://arxiv.org/abs/2509.25604
tags:
- diffusion
- arxiv
- language
- reasoning
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Reward-Free Guidance (RFG), a method for guiding
  diffusion language model (dLLM) reasoning without requiring explicit process reward
  models. The key innovation is parameterizing the reward as the log-likelihood ratio
  of a policy and reference dLLM, allowing step-wise guidance to be derived without
  additional training.
---

# RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance

## Quick Facts
- arXiv ID: 2509.25604
- Source URL: https://arxiv.org/abs/2509.25604
- Reference count: 40
- Introduces training-free test-time scaling method for diffusion LLMs that improves reasoning performance by up to 9.2%

## Executive Summary
This paper introduces Reward-Free Guidance (RFG), a novel method for guiding diffusion language model (dLLM) reasoning without requiring explicit process reward models. The key innovation is parameterizing the reward as the log-likelihood ratio of a policy and reference dLLM, allowing step-wise guidance to be derived without additional training. The authors theoretically justify that RFG induces a reward-reweighted target distribution. Experiments on four benchmarks (GSM8K, MATH-500, HumanEval, MBPP) using diverse dLLMs (LLaDA, Dream) and post-training methods show consistent improvements, with accuracy gains up to 9.2% over baselines, including naive ensembles with identical compute. RFG is model-agnostic and training-free, demonstrating robust test-time scaling for dLLM reasoning.

## Method Summary
RFG derives step-wise guidance signals for diffusion LLMs by calculating the log-likelihood ratio between a post-trained policy model and a reference model. At each denoising step, the method combines logits from both models using a guidance weight, effectively steering the sampling process toward the policy distribution while suppressing reference model characteristics. The approach requires no additional training and works with any pre-existing dLLM pair, effectively doubling inference compute by requiring forward passes through both models at each step.

## Key Results
- Achieves accuracy gains up to 9.2% over baseline dLLM approaches on GSM8K, MATH-500, HumanEval, and MBPP benchmarks
- Outperforms naive ensemble methods (simple logit averaging) with identical compute requirements
- Works consistently across different dLLM architectures (LLaDA, Dream) and post-training methods (RL, SFT)
- Demonstrates effectiveness on both mathematical reasoning and code generation tasks

## Why This Works (Mechanism)

### Mechanism 1: Implicit Reward via Policy-Reference Likelihood Ratio
RFG enables test-time guidance for diffusion LLMs without training an explicit Process Reward Model by deriving the reward signal from the log-likelihood ratio of a policy model and a reference model. The method assumes that an off-the-shelf post-trained model implicitly represents a reward function relative to the base model. By calculating the log-likelihood ratio, RFG recovers a dense reward signal that decomposes mathematically into step-wise conditional rewards suitable for guiding the denoising process.

### Mechanism 2: Logit-Space Guidance Analogy to Classifier-Free Guidance (CFG)
RFG improves sample quality by applying a guidance mechanism structurally identical to Classifier-Free Guidance, replacing the "conditional/unconditional" pair with a "policy/reference" pair. At each denoising step, RFG extrapolates the sampling trajectory away from the reference distribution and toward the policy distribution by adjusting the sampling log-probability. This amplifies the features present in the enhanced model while suppressing those shared with the base model.

### Mechanism 3: Adaptive Over-Optimization Correction
RFG can improve upon the policy model itself by reversing "over-optimization" effects common in RL-tuned models via negative guidance weights. If a policy model has overfitted to a specific reward signal during training, RFG allows setting a guidance strength between -1 and 0, pushing the generation slightly back toward the reference model to find a "sweet spot" between the base model's diversity and the policy model's specificity.

## Foundational Learning

- **Discrete Masked Diffusion Models (dLLMs)**: Unlike Autoregressive models, dLLMs generate tokens in any order by iteratively unmasking a noisy sequence. Understanding this parallel, iterative refinement is necessary to grasp why standard AR Process Reward Models fail here. Quick check: Can you explain why estimating the likelihood of a "partially masked sentence" is fundamentally different from evaluating a "partial prefix" in an AR model?

- **Classifier-Free Guidance (CFG)**: RFG is mathematically derived as a variant of CFG. Without understanding how CFG combines conditional and unconditional scores to steer generation, the RFG logit combination formula will seem arbitrary. Quick check: In image generation, how does increasing the CFG scale affect image fidelity vs. diversity? Map this to how the guidance weight might affect text reasoning.

- **Reward Hypothesis & RL reparameterization**: The paper relies on the theoretical link between policy optimization and log-likelihood ratios (derived from DPO/RL literature). Quick check: Why does the term log(p_theta/p_ref) emerge as a proxy for reward in preference optimization contexts?

## Architecture Onboarding

- **Component map**: Reference Model (p_ref) -> Policy Model (p_theta) -> Guided Sampler -> Output
- **Critical path**: The inference loop modification (Algorithm 1) is the sole integration point. It requires doubling the forward pass compute (one for p_ref, one for p_theta) at every denoising step.
- **Design tradeoffs**: RFG effectively doubles inference latency/compute (2 forward passes per step). The paper notes a "wide plateau" for guidance weight, but extreme values degrade output. Negative guidance weight is a niche tool for correcting over-optimized models.
- **Failure signatures**: If performance matches a simple average of logits, the RFG implementation is likely incorrect (RFG relies on the difference in logits, not just the average). If guidance weight is too high, generated code may look valid structurally but fail logical tests, or text may become repetitive.
- **First 3 experiments**: 1) Run inference on GSM8K using only Policy Model vs. RFG with w=1.0 to verify improvement. 2) Sweep w from -0.5 to 2.0 to check for the "plateau" and identify optimal peak. 3) Swap Policy model for a standard SFT model to see if the "implicit reward" holds.

## Open Questions the Paper Calls Out

None identified in the paper.

## Limitations

- The evaluation covers four benchmarks and two model families but all experiments use English-language tasks; effectiveness for multilingual reasoning remains unverified.
- RFG doubles inference compute by requiring forward passes through both reference and policy models at each denoising step, with no absolute latency measurements provided.
- The method's performance depends critically on the quality of the policy model, but doesn't systematically compare cases where the policy model might be worse than the reference model.

## Confidence

**High Confidence**: The core mathematical framework connecting log-likelihood ratios to reward signals is well-grounded in established RL theory. The derivation and its application to dLLM guidance follows logically from existing work on preference optimization and classifier-free guidance.

**Medium Confidence**: The reported accuracy gains (up to 9.2%) are substantial but come from a limited set of experiments on specific model architectures. The consistency across different dLLMs and benchmarks supports the findings, though independent replication would strengthen confidence.

**Medium Confidence**: The negative guidance weight mechanism is theoretically sound and supported by the single experiment in Section 4.3, but the phenomenon of over-optimization in dLLMs specifically needs more systematic investigation across different post-training methods.

## Next Checks

1. Apply RFG to a multilingual reasoning benchmark (e.g., XSum or multilingual GSM8K variants) using a multilingual dLLM pair to verify if the likelihood ratio reward generalizes across languages.

2. Intentionally pair a high-quality reference model with a deliberately corrupted policy model (e.g., with injected syntax errors) to test whether RFG can still provide useful guidance or if it amplifies the policy model's weaknesses.

3. Measure wall-clock inference time for RFG versus baseline dLLMs across different guidance weights, and plot the Pareto frontier of accuracy gain versus compute overhead to identify practical deployment thresholds.