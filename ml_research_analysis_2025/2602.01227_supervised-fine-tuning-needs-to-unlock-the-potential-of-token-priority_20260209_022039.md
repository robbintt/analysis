---
ver: rpa2
title: Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority
arxiv_id: '2602.01227'
source_url: https://arxiv.org/abs/2602.01227
tags:
- arxiv
- priority
- token
- tokens
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental granularity mismatch in supervised
  fine-tuning: treating all tokens equally fails to align models with true human utility.
  It proposes Token Priority as a unifying framework to reshape the training distribution
  at the token level.'
---

# Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority

## Quick Facts
- arXiv ID: 2602.01227
- Source URL: https://arxiv.org/abs/2602.01227
- Reference count: 33
- Primary result: Token Priority framework identifies granularity mismatch in SFT and proposes token-level weighting to mitigate gradient starvation and exposure bias

## Executive Summary
This paper identifies a fundamental granularity mismatch in supervised fine-tuning: treating all tokens equally fails to align models with true human utility. It proposes Token Priority as a unifying framework to reshape the training distribution at the token level. Two regimes are introduced: Positive Priority (Φ≥0) for noise filtration and signal amplification via hard selection or soft reweighting, and Signed Priority (Φ∈R) for actively unlearning toxic modes through gradient reversal. The framework unifies recent advancements like Rho-1, T-Shirt, and Forgetting, and challenges the "Scale is All You Need" hypothesis by showing that explicit prioritization is essential to avoid gradient starvation, exposure bias, and model collapse.

## Method Summary
The method introduces a Token Priority function Φ(x) that weights the cross-entropy loss at the token level. Instead of uniform training, tokens are prioritized based on their information value or toxicity. Positive Priority regimes (Φ≥0) implement hard selection (masking low-value tokens) or soft reweighting (scaling gradients by importance), while Signed Priority regimes (Φ∈R) use negative weights to actively unlearn failure modes. The framework mathematically unifies importance sampling for exposure bias correction and gradient reversal for unlearning.

## Key Results
- Standard SFT suffers from gradient starvation where easy tokens monopolize optimization budget
- Token Priority enables selective amplification of reasoning tokens while suppressing noise
- The framework provides theoretical foundation for recent methods like Rho-1 and T-Shirt
- Signed Priority offers a unified approach to both filtering and active unlearning

## Why This Works (Mechanism)

### Mechanism 1: Gradient Redistribution via Dynamic Equalization
Standard SFT minimizes negative log-likelihood uniformly, causing syntactic anchors (e.g., "the", "is") to monopolize the optimization budget. The Token Priority function Φ(x) acts as a dynamic equalizer, down-weighting trivial tokens (Φ < 1) or masking them entirely (Φ = 0) to reserve capacity for high-information reasoning pivots. This mitigates gradient starvation where easy tokens starve the gradients needed for hard features.

### Mechanism 2: Density Ratio Estimation for Exposure Bias Correction
Teacher forcing creates a "comfort zone" where the model conditions on ground-truth history during training but its own history during inference. By treating Φ(x) as an importance sampling weight approximating the density ratio w_t = π*(x_t)/π_ref(x_t), the loss function effectively reshapes the training distribution to simulate inference conditions, emphasizing "Recovery Tokens" that steer trajectories back from errors.

### Mechanism 3: Active Unlearning via Signed Gradients
Assigning negative priority (Φ < 0) to toxic or hallucinated tokens enables "corrective" optimization that actively pushes the model away from failure modes. Unlike standard SFT which treats all data as positive signal (Φ ≥ 0), this regime uses negative weights to perform gradient ascent on specific tokens, allowing the model to erase spurious correlations or hallucinated facts while preserving the underlying knowledge base.

## Foundational Learning

- **Concept: Teacher Forcing & Exposure Bias**
  - Why needed here: Explains why "more data" doesn't fix "hallucination loops" - the structural artifact of training on perfect history vs. running on flawed history
  - Quick check question: Does the model see its own mistakes during training? (No, hence the "comfort zone")

- **Concept: Gradient Starvation (The Pezeshki et al. effect)**
  - Why needed here: Explains why equal treatment of tokens fails - easy features (syntax) learn faster and "starve" the gradients needed for hard features (reasoning)
  - Quick check question: Why does low training loss not guarantee high reasoning accuracy? (Because the loss might be dominated by easy tokens)

- **Concept: Importance Sampling (in RL/Statistics)**
  - Why needed here: The paper proposes Φ(x) as a density ratio to estimate the expectation of a distribution you can't sample from
  - Quick check question: How do you estimate the expectation of a distribution you can't sample from? (By weighting samples from a related distribution you can sample from)

## Architecture Onboarding

- **Component map:** Data Loader -> Priority Estimator (Φ-Engine) -> Weighted Loss -> Optimizer
- **Critical path:** The definition of the Priority Function Φ
  - If Φ is binary ({0,1}), you are doing Hard Selection (Robust but coarse)
  - If Φ is continuous (ℝ), you are doing Soft Reweighting (Precise but fragile)
  - If Φ includes negatives (ℝ), you are doing Unlearning
- **Design tradeoffs:**
  - Intrinsic vs. External Signals: Intrinsic (e.g., Entropy) scales infinitely but risks "blind leading the blind." External (e.g., labels) is accurate but expensive.
  - Granularity: Token-level selection risks "Semantic Fragmentation." Chunk-level (T-Shirt) preserves structure but is less precise.
- **Failure signatures:**
  - Semantic Fragmentation: Generated text becomes incoherent or grammatically broken
  - Reference Ceiling: Student cannot exceed teacher's ability to discern good tokens
  - Oscillation: Aggressive negative gradients cause flip-flopping between modes
- **First 3 experiments:**
  1. Implement "High-Pass Filter" (Hard Selection): Train on math dataset, mask out tokens where model loss < ε, verify reasoning accuracy improvement
  2. Implement "Entropy Scaling" (Soft Reweighting): Weight loss by token-level entropy, verify focus on reasoning junctions
  3. Stress Test for Fragmentation: Run Hard Selection model, check perplexity on WikiText to ensure general language capability preserved

## Open Questions the Paper Calls Out

- **Open Question 1:** How can priority functions be formulated to preserve semantic integrity rather than treating tokens as independent units?
  - Basis: Appendix A.1 asks how to mathematically formulate priority such that removing a "low-value" token respects the causal integrity of dependent high-value tokens
  - Why unresolved: Current methods risk "semantic fragmentation," severing the "connective tissue" of language
  - What evidence would resolve it: A "Topological Priority" framework based on semantic dependency graphs

- **Open Question 2:** How can priority estimation decouple model confidence from factual correctness to avoid reinforcing confident hallucinations?
  - Basis: Appendix A.2 identifies the "Epistemic Reliability Gap" where standard priority functions reinforce errors by assigning high Φ to hallucinated content
  - Why unresolved: Proxy signals like loss or entropy often conflate "Statistical Priority" with "Veridical Priority"
  - What evidence would resolve it: Scalable, reference-free verification mechanisms that ground priority in truth

- **Open Question 3:** How can we define time-dependent priority schedules to adapt to dynamic learning trajectories and prevent distributional drift?
  - Basis: Appendix A.3 posits the need for "Priority Schedules" asking when tokens should transition between "Constructive," "Neutral," and "Corrective" states
  - Why unresolved: Static priority masks fail to account for "Trajectory Mismatch" where token difficulty shifts during training
  - What evidence would resolve it: An optimal control framework for Φ(x, t) treating priority as dynamic policy

## Limitations

- Theoretical scope relies heavily on existing methods without demonstrating novel mathematical contributions beyond aggregation
- Minimal quantitative evidence and no ablation studies comparing different Φ functions
- Implementation details for Topological Priority are deferred to future work
- No standardization of hyperparameters for priority thresholds across model scales

## Confidence

**High Confidence (Theoretical Foundation):** The identification of gradient starvation as a fundamental limitation is well-supported by Pezeshki et al. literature. Mathematical formulation of Token Priority as importance sampling is sound.

**Medium Confidence (Practical Implementation):** The two-regime classification provides useful conceptual framework, but practical effectiveness depends heavily on proxy quality and hyperparameter tuning.

**Low Confidence (Novel Claims):** The assertion that signed gradients enable reliable unlearning without catastrophic forgetting is largely theoretical, with minimal experimental validation.

## Next Checks

1. **Ablation Study on Priority Proxies:** Systematically compare Rho-1, Entropy, and DFT as Φ functions on GSM8K to determine which proxy most effectively mitigates gradient starvation while preserving general language capability.

2. **Signed Priority Stability Test:** Implement gradient reversal mechanism (Φ < 0) on controlled toxic generation task, tracking model perplexity, hallucination rate, and parameter stability to detect catastrophic forgetting.

3. **Cross-Domain Generalization:** Train Token Priority models on multiple domains (math, code, general instructions) and evaluate zero-shot transfer to test whether benefits are domain-specific or represent genuine capability enhancement.