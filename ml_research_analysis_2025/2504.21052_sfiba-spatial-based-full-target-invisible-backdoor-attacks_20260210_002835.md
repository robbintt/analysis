---
ver: rpa2
title: 'SFIBA: Spatial-based Full-target Invisible Backdoor Attacks'
arxiv_id: '2504.21052'
source_url: https://arxiv.org/abs/2504.21052
tags:
- trigger
- backdoor
- poisoned
- attack
- sfiba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SFIBA, a novel spatial-based full-target
  invisible backdoor attack designed to address limitations in existing multi-target
  backdoor attacks, particularly in black-box settings. The key innovation lies in
  leveraging the sensitivity of backdoor models to trigger spatial location and morphology
  to ensure trigger specificity, while employing frequency-domain-based trigger injection
  methods to guarantee stealthiness.
---

# SFIBA: Spatial-based Full-target Invisible Backdoor Attacks

## Quick Facts
- **arXiv ID**: 2504.21052
- **Source URL**: https://arxiv.org/abs/2504.21052
- **Reference count**: 37
- **Primary result**: SFIBA achieves high attack success rates for any predefined target class while maintaining model performance on benign samples and effectively bypassing existing backdoor defenses.

## Executive Summary
SFIBA is a novel spatial-based full-target invisible backdoor attack that addresses limitations in existing multi-target backdoor attacks, particularly in black-box settings. The key innovation lies in leveraging the sensitivity of backdoor models to trigger spatial location and morphology to ensure trigger specificity, while employing frequency-domain-based trigger injection methods to guarantee stealthiness. SFIBA achieves full-target attacks by dividing images into disjoint blocks, injecting invisible, morphologically constrained triggers into specific blocks for different target classes, and dynamically optimizing injection coefficients. Experiments across multiple datasets (CIFAR10, GTSRB, ImageNet100) and models demonstrate that SFIBA achieves high attack success rates (ASR) for any predefined target class while maintaining model performance on benign samples and effectively bypassing existing backdoor defenses.

## Method Summary
SFIBA combines spatial block partitioning with frequency-domain trigger injection to create a full-target invisible backdoor attack. The method divides each image into disjoint spatial blocks, assigns each block to a target class, and injects an invisible trigger into the frequency domain (amplitude spectrum) of the assigned block. The trigger is made invisible through DWT and SVD-based fusion with the clean image, and its morphology is constrained to ensure spatial specificity. A dynamic coefficient adjustment algorithm optimizes the injection strength per-sample to meet stealthiness targets while maintaining attack effectiveness.

## Key Results
- Achieves high attack success rates (ASR) for any predefined target class while maintaining model performance on benign samples
- Effectively bypasses existing backdoor defenses through frequency-domain injection and morphological constraints
- Demonstrates stealthiness validated through visual metrics (PSNR, SSIM, LPIPS) across multiple datasets (CIFAR10, GTSRB, ImageNet100)

## Why This Works (Mechanism)
SFIBA works by exploiting the spatial sensitivity of CNN models to trigger location while ensuring trigger invisibility through frequency-domain manipulation. The method divides images into disjoint blocks, each assigned to a target class, and injects triggers into the amplitude spectrum of specific blocks using DWT and SVD. The diagonal features (HH components) of the amplitude spectrum are targeted because they contain less energy and are better for hiding. Morphological constraints ensure that triggers are only effective when placed in their designated spatial locations, enabling full-target attacks. Dynamic coefficient adjustment optimizes the trade-off between stealthiness and attack effectiveness on a per-sample basis.

## Foundational Learning
- **Concept: Fast Fourier Transform (FFT) for Image Analysis**
  - Why needed here: Understanding how to decompose an image into frequency components (amplitude and phase spectra) is critical. SFIBA injects the trigger into the amplitude spectrum, which is less sensitive to the human visual system than the phase spectrum, enabling stealth.
  - Quick check question: If you modify only the phase spectrum of an image, what happens to the visual structure of the image?

- **Concept: Discrete Wavelet Transform (DWT) and Singular Value Decomposition (SVD)**
  - Why needed here: SFIBA uses DWT to isolate diagonal features ($HH$) from the amplitude spectrum for trigger injection because they contain less energy and are better for hiding. SVD is then used for fusion, stabilizing the trigger within the image's structural properties. Grasping this multi-step process is essential.
  - Quick check question: Why might injecting a trigger into the "diagonal features" of an amplitude spectrum be stealthier than injecting it into the "approximation" features?

- **Concept: Black-Box Backdoor Attacks and Poisoning Rate**
  - Why needed here: This paper operates under a black-box threat model (attacker controls only training data). You must understand how a poisoning attack works by modifying a subset of the training set to create a model that misclassifies inputs with a specific trigger.
  - Quick check question: In a black-box backdoor attack scenario, what is the primary lever an attacker has to influence the final trained model?

## Architecture Onboarding
- **Component map**: `Clean Image` → `Block Selection (based on target class)` → `FFT` → `DWT (extract diagonal features)` → `SVD (fuse trigger)` → `Inverse Transforms` → `Morphological Filtering` → `Dynamic Coefficient Adjustment` → `Final Poisoned Sample`
- **Critical path**: The poisoned image flows through block selection, frequency-domain transformation, trigger injection, inverse transformation, morphological filtering, and dynamic coefficient adjustment before becoming the final poisoned sample.
- **Design tradeoffs**:
  - **Block Size vs. Capacity**: Smaller blocks provide more spatial regions for more target classes but constrain the trigger's size, potentially making it less effective or harder to inject stealthily. The paper finds a 4x4 pixel minimum for effectiveness.
  - **Stealth vs. Effectiveness**: A higher injection coefficient $K$ makes the trigger stronger (higher ASR) but creates more visible artifacts. The dynamic tuner is designed to find the optimal point on this tradeoff curve for each image.
- **Failure signatures**:
  - **Low ASR for all targets**: Suggests the injection coefficient $K$ is too low, the model is not learning the frequency-domain features, or data augmentation is destroying the spatial trigger location.
  - **Visible Artifacts (low PSNR)**: The injection coefficient $K$ is too high or the SVD fusion is unstable.
  - **Interference between classes**: Spatial blocks are not truly disjoint or data augmentation is causing triggers to overlap in the training batch.
- **First 3 experiments**:
  1. **Single-Target Feasibility Test**: Implement the full FFT-DWT-SVD pipeline for a *single* target class on a small dataset like CIFAR-10. Confirm that the model achieves high ASR on poisoned test data while maintaining high clean accuracy. This validates the core stealthy trigger mechanism.
  2. **Spatial Sensitivity Validation**: Train the model with a trigger in Block A for Class 1. At inference, apply the trigger to Block A (should succeed) and then to Block B (should fail). This confirms the theoretical claim of spatial sensitivity required for multi-target capability.
  3. **Multi-Target Attack Scaling**: Extend the setup to all 10 classes on CIFAR-10, assigning each a unique block/channel combination. Measure the ASR for each class individually to check for any interference as the number of concurrent backdoors increases.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The method assumes perfect spatial disentanglement of blocks across all classes, which may be compromised by data augmentation in practical deployments
- Dynamic coefficient adjustment via binary search introduces computational overhead and may not scale efficiently to larger datasets or models
- The paper focuses primarily on classification tasks and doesn't explore whether the attack generalizes to other domains like object detection or segmentation

## Confidence
- **High Confidence**: The fundamental methodology combining FFT, DWT, SVD, and morphological filtering for stealthy trigger injection is well-established and the experimental results demonstrating the stealthiness metrics (PSNR, SSIM, LPIPS) are reproducible.
- **Medium Confidence**: The claim of achieving high ASR across all target classes simultaneously is supported by experiments, but the potential interference between spatially adjacent blocks under real-world data augmentation remains unclear.
- **Medium Confidence**: The assertion that the attack is robust to existing backdoor defenses is based on the stealthiness metrics and the frequency-domain injection approach, but specific defense mechanisms are not directly evaluated in the paper.

## Next Checks
1. **Data Augmentation Robustness Test**: Implement standard data augmentation pipelines (random cropping, horizontal flipping, rotation) during training and evaluate whether the spatial specificity of triggers is maintained. Measure ASR degradation when triggers are shifted or distorted.

2. **Transferability Analysis**: Train the poisoned model on CIFAR-10 and test the trigger effectiveness on CIFAR-100 and vice versa to evaluate whether the frequency-domain features learned are transferable across similar but distinct datasets.

3. **Defense Evasion Benchmark**: Test the poisoned model against at least three state-of-the-art backdoor defense mechanisms (e.g., Neural Cleanse, STRIP, Spectral Signature) to quantify the actual effectiveness of SFIBA against real-world defense systems, beyond just stealthiness metrics.