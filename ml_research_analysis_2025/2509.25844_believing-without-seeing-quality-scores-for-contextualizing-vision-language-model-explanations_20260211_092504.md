---
ver: rpa2
title: 'Believing without Seeing: Quality Scores for Contextualizing Vision-Language
  Model Explanations'
arxiv_id: '2509.25844'
source_url: https://arxiv.org/abs/2509.25844
tags:
- explanation
- quality
- visual
- answer
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two new explanation quality scoring functions
  for vision-language models (VLMs): Visual Fidelity, which measures how faithfully
  an explanation reflects the image, and Contrastiveness, which measures how well
  it distinguishes the prediction from alternatives. The authors find these qualities
  better predict VLM correctness than existing metrics.'
---

# Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations

## Quick Facts
- arXiv ID: 2509.25844
- Source URL: https://arxiv.org/abs/2509.25844
- Reference count: 40
- Quality scores improve VLM prediction accuracy by 11.1% when users judge without seeing images

## Executive Summary
This paper introduces two novel explanation quality scoring functions for vision-language models (VLMs): Visual Fidelity and Contrastiveness. These metrics measure how faithfully explanations reflect the image content and how well they distinguish correct predictions from alternatives. The authors demonstrate that these quality scores better predict VLM correctness than existing metrics. Through user studies where participants judge VLM predictions without seeing images, the research shows that displaying these quality scores alongside explanations significantly improves accuracy and reduces false beliefs in incorrect predictions.

## Method Summary
The paper proposes two explanation quality scoring functions for VLMs. Visual Fidelity measures how well an explanation reflects the actual image content using CLIP similarity between the image and generated explanation. Contrastiveness evaluates how effectively an explanation distinguishes the correct prediction from plausible alternatives by measuring the difference in CLIP similarity between the correct and incorrect options. The authors validate these metrics through user studies where participants judge VLM predictions without seeing images, comparing performance with and without quality score presentations. They also introduce descriptive quality presentations that enhance user performance beyond numerical scores alone.

## Key Results
- Users improved prediction accuracy by 11.1% when shown quality scores alongside VLM explanations
- False beliefs in incorrect predictions decreased by 15.4% with quality score presentation
- Descriptive quality presentations further enhanced performance compared to numerical scores alone

## Why This Works (Mechanism)
The paper's mechanism works because quality scores provide users with interpretable signals about explanation reliability when they cannot directly verify predictions against images. Visual Fidelity helps users assess whether the explanation is grounded in the actual visual content, while Contrastiveness indicates whether the explanation provides discriminative reasoning that sets the correct answer apart from alternatives. By quantifying these aspects, users can make more informed judgments about whether to trust a VLM's prediction, effectively compensating for their inability to see the image directly.

## Foundational Learning

**CLIP Similarity** - A metric measuring semantic alignment between images and text using a contrastive language-image pre-training model. Needed to quantify how well explanations align with visual content. Quick check: Compare CLIP scores between ground truth explanations and random text.

**Explanation Fidelity** - The degree to which an explanation accurately reflects the reasoning behind a prediction. Critical for determining whether users can trust model outputs. Quick check: Evaluate if high-fidelity explanations correlate with correct predictions.

**Contrastive Reasoning** - The ability to differentiate between correct and incorrect options through explanation. Essential for understanding why one answer is better than alternatives. Quick check: Measure if explanations that highlight distinguishing features improve user discrimination.

**User Trust Calibration** - How users adjust their confidence in predictions based on quality signals. Fundamental for understanding the impact of quality scores on decision-making. Quick check: Track changes in user confidence before and after seeing quality scores.

**Visual-Text Alignment** - The semantic correspondence between visual content and textual descriptions. Important for evaluating explanation quality. Quick check: Test alignment scores on images with varying complexity.

## Architecture Onboarding

**Component Map**: VLM -> Explanation Generator -> Quality Score Calculator -> User Interface

**Critical Path**: Image → VLM → Prediction → Explanation → Quality Scores → User Judgment
The most time-sensitive path is generating explanations and quality scores quickly enough for real-time user interaction.

**Design Tradeoffs**: 
- Numerical vs descriptive quality presentations: Numerical scores are computationally efficient but descriptive presentations show better user performance
- Quality score granularity: More granular scores provide more information but may overwhelm users
- Real-time vs precomputed scores: Precomputing saves latency but reduces flexibility for dynamic content

**Failure Signatures**:
- Low Visual Fidelity with high user confidence indicates users may be misled by irrelevant explanations
- High Contrastiveness with incorrect predictions suggests the model may be exploiting spurious correlations
- Inconsistent quality scores across similar predictions reveal potential bias in the scoring metrics

**First Experiments**:
1. Test quality scores on a diverse set of images beyond the initial validation set
2. Compare user performance with quality scores versus expert human explanations
3. Evaluate how quality scores perform when explanations are intentionally manipulated or contain errors

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the provided text.

## Limitations
- The study relies on a single VLM (GPT-4V) and specific image set, limiting generalizability
- Visual Fidelity depends on CLIP similarity which may not capture all aspects of visual relevance
- The Contrastiveness metric assumes a fixed set of alternatives that may not reflect real-world ambiguity
- The study doesn't address how quality scores perform when explanations are incorrect but convincing

## Confidence
**Empirical Results (Medium)**: The 11.1% accuracy improvement and 15.4% reduction in false beliefs are well-supported by user studies, but rely on controlled conditions that may not generalize to real-world usage.

**Metric Design (Medium)**: The Visual Fidelity and Contrastiveness metrics show promise but depend on CLIP similarity, which has known limitations in capturing complex visual semantics.

**Generalizability (Low)**: Limited testing across different VLMs and image domains means the findings may not extend beyond the specific experimental setup.

## Next Checks
1. Test the quality scores across multiple VLMs and diverse image domains to assess generalizability
2. Conduct longitudinal studies examining how users' trust in and reliance on quality scores evolves over repeated interactions
3. Evaluate the system under realistic constraints including explanation generation time and computational resources