---
ver: rpa2
title: 'Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing
  for Efficient LLM/MLLM Reasoning'
arxiv_id: '2505.15154'
source_url: https://arxiv.org/abs/2505.15154
tags:
- reasoning
- answers
- short
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CAR, a framework that dynamically switches
  between short answers and long-form reasoning based on model confidence measured
  by perplexity. Experimental results across diverse benchmarks demonstrate that CAR
  outperforms both pure short-answer and exhaustive long-form reasoning approaches,
  achieving higher accuracy while significantly reducing token usage.
---

# Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning

## Quick Facts
- arXiv ID: 2505.15154
- Source URL: https://arxiv.org/abs/2505.15154
- Reference count: 40
- This work introduces CAR, a framework that dynamically switches between short answers and long-form reasoning based on model confidence measured by perplexity.

## Executive Summary
This paper introduces CAR (Certainty-based Adaptive Routing), a framework that dynamically routes between short answers and long-form reasoning based on model confidence measured via perplexity. The key insight is that prolonged reasoning isn't always beneficial—sometimes it harms performance on simpler tasks while consuming unnecessary tokens. CAR fine-tunes models on both response modes, estimates Gaussian distributions of perplexity for correct/incorrect answers during training, and at inference uses Bayesian routing to decide whether additional reasoning is needed. Experiments across VQA, KIE, and math reasoning benchmarks demonstrate CAR achieves up to 6.9% accuracy improvement and 39.0% token reduction compared to state-of-the-art reasoning token reduction methods.

## Method Summary
CAR operates through a four-stage pipeline: (1) fine-tune base models on blended datasets with both short-answer and long-reasoning prompts, (2) extract perplexity scores for short answers on training data, (3) fit Gaussian distributions to PPL scores for correct vs. incorrect cases and balance class priors, and (4) at inference, generate a short answer, compute its perplexity, and route to long-form reasoning only if the Bayesian posterior probability indicates low confidence. The method requires both response modes during training and uses synthetic reasoning traces (via DeepSeek-R1) where human-annotated reasoning is unavailable.

## Key Results
- CAR achieves up to 6.9% accuracy improvement and 39.0% token reduction compared to state-of-the-art reasoning token reduction methods
- On text-rich VQA/KIE tasks, CAR correctly routes to short answers where long reasoning would harm performance (e.g., 53.9% vs 41.1% on DocVQA with short vs long reasoning)
- CAR's adaptive routing is particularly effective on MathQA datasets (83.8% vs 67.1% of COD with Qwen2.5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity serves as a reliable proxy for model confidence in short-answer predictions, enabling automated routing decisions.
- Mechanism: CAR computes PPL for each generated short answer, then compares likelihood under two Gaussian distributions (correct vs. incorrect) to decide whether additional reasoning is needed.
- Core assumption: PPL distributions for correct and incorrect answers are sufficiently separable and approximately Gaussian.
- Evidence anchors:
  - [abstract]: "CAR first generates a short answer and evaluates its perplexity, triggering reasoning only when the model exhibits low confidence (i.e., high perplexity)."
  - [Section 3.3, Table 2]: Mean PPL for correct cases ranges 1.05–1.28 vs. 1.18–1.40 for incorrect across 8 datasets.
  - [corpus]: Weak direct support; neighbor paper "Increasing the Thinking Budget is Not All You Need" aligns conceptually but uses different methodology.
- Break condition: If PPL distributions for correct/incorrect significantly overlap (e.g., on noisy or ambiguous tasks), routing becomes unreliable.

### Mechanism 2
- Claim: Gaussian modeling of PPL distributions enables Bayesian routing that outperforms fixed thresholds.
- Mechanism: CAR estimates μ₁, σ₁² (correct) and μ₀, σ₀² (incorrect) from training PPLs, then applies Bayes' theorem: if P(C=1|PPL_new) > P(C=0|PPL_new), output short answer; otherwise trigger long reasoning.
- Core assumption: Gaussian distributions adequately capture PPL characteristics; priors from training generalize to inference.
- Evidence anchors:
  - [Section 4.3]: "We assume that the distribution of PPL scores for correct and incorrect short answers follows a Gaussian distribution."
  - [Section 5.3]: CAR achieves 81.1% accuracy with Qwen2.5 vs. 75.0% for pure long-form, with 45.1% token reduction.
  - [corpus]: No direct corpus validation of Gaussian assumption for PPL-based routing.
- Break condition: If deployment priors differ substantially from training, or if PPL distributions are multimodal, Bayesian estimates degrade.

### Mechanism 3
- Claim: Reasoning provides differential benefit across task types—prolonged reasoning can harm performance on simpler extractive tasks.
- Mechanism: On text-rich VQA/KIE, short answers outperform long reasoning (45.7% vs. 34.1% avg). CAR identifies such cases via low PPL and avoids unnecessary reasoning. On complex tasks (GSM8K), most queries require reasoning; CAR correctly routes to long-form.
- Core assumption: Task complexity correlates with reasoning necessity, and PPL captures this implicitly.
- Evidence anchors:
  - [Table 1]: Qwen2.5-0.5B short answers: 53.9% DocVQA, 25.3% ChartQA, 58.0% FUNSD vs. long-form: 41.1%, 16.6%, 44.5%.
  - [Section 5.4]: "CAR's adaptive routing proves particularly effective in MathQA datasets (83.8% vs. 67.1% of COD with Qwen2.5)."
  - [corpus]: "Increasing the Thinking Budget is Not All You Need" corroborates that more compute doesn't uniformly improve reasoning.
- Break condition: If deployment tasks differ in complexity distribution from training, routing calibration drifts.

## Foundational Learning

- Concept: **Perplexity computation**
  - Why needed here: CAR's core signal is PPL = exp(-1/T × Σlog p(w_t|context)). Understanding this metric is essential for debugging routing decisions.
  - Quick check question: Given token probabilities [0.8, 0.6, 0.9], compute the sequence perplexity.

- Concept: **Bayesian posterior probability**
  - Why needed here: CAR routes via P(C|PPL) using likelihoods from Gaussian PDFs and class priors. Misunderstanding Bayes' theorem leads to incorrect threshold implementation.
  - Quick check question: If P(PPL|correct)=0.7, P(PPL|incorrect)=0.2, P(correct)=0.6, compute P(correct|PPL).

- Concept: **Instruction tuning for dual response modes**
  - Why needed here: CAR requires models capable of both short and long-form outputs depending on prompt. Standard SFT on mixed data with mode-specific prompts (e.g., "Please directly output the answer" vs. "Please output the reasoning process first") enables this.
  - Quick check question: What prompt modifications would you use to train a model for both concise and reasoning-heavy responses?

## Architecture Onboarding

- Component map: Training module -> PPL extraction -> Distribution estimator -> Inference router
- Critical path:
  1. PPL extraction accuracy (correct implementation of Equation 2)
  2. Gaussian parameter estimation quality (sufficient samples per class)
  3. Prior balancing (n₀=n₁) to avoid bias toward one class
- Design tradeoffs:
  - **Training data requirement**: Needs both short and reasoning annotations; paper uses DeepSeek-R1 to synthesize reasoning where unavailable.
  - **Inference overhead**: Routing requires initial short-answer pass; if routed to long-form, total compute approaches pure-reasoning baseline.
  - **OOD sensitivity**: PPL distributions may shift on out-of-domain data; paper shows mixed OOD results (Section 3.3).
- Failure signatures:
  - **High PPL overlap**: Correct/incorrect distributions near-identical → random routing → no efficiency gain.
  - **Prior mismatch**: Training has 50/50 correct/incorrect; deployment has 90% easy → over-triggers reasoning.
  - **Reasoning degradation**: On tasks where long reasoning consistently hurts (e.g., simple extraction), CAR's fallback still produces errors.
- First 3 experiments:
  1. **PPL separability validation**: On 2-3 datasets, fine-tune base model with short/long prompts; plot PPL histograms for correct vs. incorrect. Target: visible distribution separation (mean ΔPPL > 0.1).
  2. **Routing threshold ablation**: Compare fixed-percentile threshold (pilot study approach) vs. Bayesian routing on held-out validation. Measure accuracy/token trade-off curves.
  3. **Prior sensitivity test**: Train with n₀=n₁ vs. natural class imbalance; evaluate routing bias toward short or long mode on test set.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would integrating CAR with other token reduction methods (CoD, Skeleton-of-Thought) yield similar synergistic improvements as the CAR-TALE combination?
  - Basis in paper: [explicit] Section 5.4 states: "Future work could explore tighter integration of confidence-aware routing with other reasoning token reduction methods."
  - Why unresolved: Only TALE integration was tested; other methods have different mechanisms (parallelization vs. concise drafting) that may interact differently with confidence-based routing.
  - What evidence would resolve it: Experiments combining CAR with CoD and Skeleton-of-Thought across the same benchmarks, measuring both accuracy gains and token reduction.

- **Open Question 2**: Can alternative confidence metrics (beyond perplexity) improve routing decisions, particularly for complex reasoning tasks where CAR shows modest gains?
  - Basis in paper: [inferred] Section 6 explicitly notes "Performance gains are more modest on complex reasoning tasks (e.g., MathQA, GSM8K)" as a limitation.
  - Why unresolved: PPL captures token-level uncertainty but may not reflect reasoning-chain quality or logical consistency needed for complex tasks.
  - What evidence would resolve it: Comparative experiments using entropy-based measures, self-consistency scores, or learned confidence predictors as routing signals on GSM8K/MathQA.

- **Open Question 3**: Is the Gaussian distribution assumption for modeling PPL scores optimal, or would mixture models better capture the underlying distribution?
  - Basis in paper: [inferred] Section 4.3 assumes "PPL scores for correct and incorrect short answers follows a Gaussian distribution" without ablation of this modeling choice.
  - Why unresolved: The PPL distributions in Figure 1 show some asymmetry; Gaussian may be a simplifying assumption that reduces routing precision.
  - What evidence would resolve it: Ablation comparing Gaussian vs. mixture models vs. kernel density estimation for PPL distribution modeling, measuring routing accuracy.

- **Open Question 4**: How does CAR's routing overhead scale with model size and input length, and can this overhead be amortized in batch inference settings?
  - Basis in paper: [explicit] Section 6 notes: "The adaptive routing process introduces minor computational overhead compared to pure short-answer inference."
  - Why unresolved: The overhead was only measured per-query; scalability implications for production systems with large models or high-throughput requirements remain unexplored.
  - What evidence would resolve it: Latency profiling across model scales (7B, 70B, etc.) and batch sizes, quantifying overhead as percentage of total inference time.

## Limitations

- PPL-based routing reliability: The fundamental assumption that perplexity distributions for correct and incorrect short answers are reliably separable remains unverified across diverse domains.
- Gaussian approximation validity: The Bayesian routing mechanism assumes Gaussian distributions for PPL scores without rigorous statistical validation of this distributional assumption.
- Synthetic reasoning traces dependency: CAR's training relies on DeepSeek-R1 to generate reasoning traces, introducing potential distribution mismatch between synthetic and human reasoning patterns.

## Confidence

**High confidence**: CAR's overall effectiveness in balancing accuracy and efficiency is well-supported by experimental results across 8 datasets with clear accuracy improvements (up to 6.9%) and token reductions (up to 39.0%).

**Medium confidence**: The PPL-based confidence estimation mechanism works reliably on tested VQA/KIE tasks, but confidence decreases for domains with different characteristics.

**Low confidence**: The Gaussian distributional assumption for PPL scores and the generalization of synthetic reasoning traces to human reasoning patterns lack sufficient validation.

## Next Checks

1. **PPL distribution validation**: Collect PPL scores from human-annotated short answers across diverse task types and rigorously test whether Gaussian distributions adequately model correct vs. incorrect answer PPLs using goodness-of-fit tests and evaluate routing performance with empirically estimated distributions.

2. **Cross-domain generalization study**: Evaluate CAR on domains significantly different from VQA/KIE (e.g., medical diagnosis, creative writing, open-domain QA) to assess whether PPL-based routing remains reliable when answer certainty is more subjective or when distributions overlap substantially.

3. **Synthetic vs. human reasoning comparison**: Train CAR with reasoning traces from both DeepSeek-R1 and human annotators on the same dataset subset, then compare routing accuracy and downstream performance to quantify the impact of synthetic reasoning traces on model calibration and decision quality.