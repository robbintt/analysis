---
ver: rpa2
title: 'Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study
  from an Approximation Perspective'
arxiv_id: '2504.13558'
source_url: https://arxiv.org/abs/2504.13558
tags:
- function
- neural
- feedforward
- lemma
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the approximation of H\xF6lder continuous\
  \ functions by Transformers and constructs several Transformers that can overcome\
  \ the curse of dimensionality. The core method uses the Kolmogorov-Arnold Representation\
  \ Theorem to transform the approximation problem into approximation and memorization\
  \ problems of feedforward neural networks."
---

# Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective

## Quick Facts
- arXiv ID: 2504.13558
- Source URL: https://arxiv.org/abs/2504.13558
- Reference count: 40
- Primary result: Transformers can overcome the curse of dimensionality for Hölder continuous function approximation using Kolmogorov-Arnold decomposition

## Executive Summary
This paper establishes that Transformers can approximate Hölder continuous functions without suffering from the curse of dimensionality. The key insight is to leverage the Kolmogorov-Arnold Representation Theorem to decompose multivariate approximation into univariate approximation and summation problems. By constructing specific Transformers that implement this decomposition, the authors demonstrate that depth and width requirements scale favorably with approximation accuracy ε, independent of input dimension d. The construction uses one self-attention layer and feedforward blocks with specific activation functions to achieve theoretical guarantees on approximation rates.

## Method Summary
The method transforms multivariate approximation into univariate problems via the Kolmogorov-Arnold Representation Theorem, which expresses any Hölder continuous function as a univariate function applied to a weighted sum of transformed input components. This decomposition allows implementing the approximation using a specific Transformer architecture: a feedforward block that implements the inner transformation φ_K and piecewise function f^(R-pi)_FF, followed by a single-head softmax self-attention layer that aggregates columns through summation, and a final feedforward block that memorizes and approximates the outer univariate function g. The translation technique enables shared-weight feedforward layers to implement different functions for different input columns, while specific activation choices (ReLU+floor, sine/2^x, σ_NP, σ_RC) determine the trade-off between constant width versus depth.

## Key Results
- Transformers with one self-attention layer and feedforward blocks can overcome the curse of dimensionality for Hölder continuous functions
- For ReLU+floor activations, O(log 1/ε) layers with widths O(1/ε^(2/β) log 1/ε) suffice to achieve ε-accuracy
- With special activation functions (sine/2^x, σ_NP, σ_RC), constant width suffices for ε-accuracy
- The construction achieves these results under weak smoothness assumptions (Hölder continuity)

## Why This Works (Mechanism)

### Mechanism 1: Kolmogorov-Arnold Representation Decomposition
Multivariate approximation is reduced to univariate function approximation and summation through the Kolmogorov-Arnold Representation Theorem with space-filling curves. This expresses a Hölder continuous function f as f(x) = g(3∑_p φ(x_p)), collapsing dependence on dimension d into a single summed index. The core assumption is that the target function is Hölder continuous and the KST variant holds with smoothness transfer to g.

### Mechanism 2: Translation Technique for Token-wise Feedforward Application
Token-wise feedforward layers can implement different operations per column despite shared weights by offsetting each token column into disjoint subdomains. This allows applying prior FNN approximation results inside Transformers. The core assumption is that feedforward layers are permitted to have bias matrices with different columns, enabling the translation trick.

### Mechanism 3: Single Softmax Self-Attention as Column Aggregation
One single-head softmax self-attention layer suffices to aggregate the KST-required summation across columns by computing a column-wise sum. This implements the inner sum over columns without depth scaling with sequence length. The core assumption is that softmax self-attention can realize an (approximate) uniform aggregation needed for the inner KST sum.

## Foundational Learning

- **Hölder continuity & KST basics**
  - Why needed: The main theorem targets Hölder continuous functions, and KST is the dimensionality-reduction backbone
  - Quick check: Can you write the Hölder condition for exponent β and constant Q, and paraphrase the KST representation for a 2D input?

- **Universal approximation for FNNs**
  - Why needed: The construction translates FNN approximation results into Transformers
  - Quick check: For a ReLU FNN achieving ε error on a Hölder class, how does width or depth typically scale with ε in classical results?

- **Softmax self-attention mechanics**
  - Why needed: The single-head softmax self-attention must implement column summation
  - Quick check: Given W_Q, W_K, W_V, what is the output expression for single-head softmax self-attention, and how could you encourage near-uniform attention weights?

## Architecture Onboarding

- **Component map:** Token columns → translation offsets → φ_K per entry → aggregated inner sum via self-attention → linear transform → memorization FNN → target approximation

- **Critical path:** The construction follows: token columns are translated via offsets, passed through φ_K to create the KST inner transformation, aggregated via single-head softmax self-attention to compute the inner sum, linearly transformed, and finally processed by a memorization FNN that approximates the outer function g.

- **Design tradeoffs:**
  - Activation choices trade constant width versus depth and realizability on hardware
  - Single-head softmax attention simplifies theory but may be less expressive than multi-head
  - The theoretical construction may not map cleanly to large pretrained Transformers with layer norm and residuals

- **Failure signatures:**
  - For ReLU-only, certain input regions (set Ω(flaw)) are excluded; expect higher error on measure-zero but potentially adversarial inputs
  - If translation offsets are not implemented, the method cannot differentiate per-token functions, leading to biased outputs
  - If softmax is replaced by hardmax or masked attention, the uniform aggregation may fail

- **First 3 experiments:**
  1. Toy task: Approximate a known Hölder function f(x)=∑_i sin(x_i²) on [0,1]^(d×n) using the constructed single-attention + FNN network with ReLU+floor; measure L^∞ error vs depth/width
  2. Ablate translation: Train same architecture but enforce column-identical biases; compare error and check if error no longer shrinks with ε as theory predicts
  3. Activation comparison: Implement the four variants in Theorem 1 (ReLU+floor, ReLU+floor+2^x, σ_NP, σ_RC) on the same toy function; track parameter counts vs achieved ε

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the expressive power of Transformers utilizing the Kolmogorov-Arnold Representation Theorem (KST) be extended to other function spaces, such as C^s spaces or Sobolev spaces?
- Basis: The conclusion states, "Investigating how Transformers approximate functions in other function spaces, such as C^s spaces, Sobolev spaces, etc., poses an intriguing question."
- Why unresolved: The current theoretical construction is tailored specifically for Hölder continuous function classes, and the authors have not yet derived the approximation rates or architectural requirements for smoothness spaces like Sobolev spaces using their KST-based method.
- What evidence would resolve it: New theorems establishing approximation bounds and network sizes required for Transformers to approximate functions in Sobolev or C^s spaces without the curse of dimensionality.

### Open Question 2
- Question: How can the theoretical approximation guarantees established in this work be integrated with the specific practical tasks (e.g., NLP) faced by Transformers?
- Basis: The conclusion identifies this as a direction for future work: "Integrating the approximation issue with the specific tasks faced by Transformers is also a meaningful subject to explore."
- Why unresolved: The paper focuses on the pure mathematical capability of sequence-to-sequence function approximation, isolating this from the complexities of specific real-world tasks like next-token prediction or translation where data distribution and loss functions vary.
- What evidence would resolve it: Theoretical analysis linking the approximation error bounds of the constructed Transformers to generalization error or performance metrics on specific downstream applications.

### Open Question 3
- Question: Is it possible to construct a Transformer that overcomes the curse of dimensionality with constant width using only standard activations (like ReLU), rather than requiring floor functions or specialized non-polynomial activations?
- Basis: The paper shows that constant width is achievable only when using specific activations like σ_NP or σ_RC, whereas using ReLU and floor requires widths scaling as O(1/ε^(2/β) log 1/ε).
- Why unresolved: The proof technique relies heavily on the properties of floor functions for precise discretization or special non-polynomial functions for density arguments to achieve constant width; it is unproven whether these specific tools are strictly necessary or if ReLU networks alone could suffice.
- What evidence would resolve it: A construction proof demonstrating that a ReLU-only Transformer can approximate Hölder continuous functions with constant width independent of dimension, or a proof that such constant width is impossible with ReLU alone.

## Limitations
- The construction is theoretical and has not been empirically validated on practical tasks or with standard Transformer implementations
- The theoretical architecture uses a single-head softmax self-attention without layer normalization or residual connections, differing from practical Transformers
- The approximation guarantees apply specifically to Hölder continuous functions and may not extend to other function classes without additional theoretical development

## Confidence

*High Confidence:* The mathematical framework for approximating Hölder continuous functions using Kolmogorov-Arnold decomposition is rigorously established. The depth-width trade-offs for feedforward networks with specific activation functions are well-grounded in existing literature.

*Medium Confidence:* The translation technique for implementing column-wise differentiation in shared-weight feedforward layers is theoretically novel but lacks empirical validation or precedent in the literature. The single-head softmax self-attention as column aggregator is theoretically justified but may not map cleanly to practical implementations.

*Low Confidence:* The overall construction's behavior on functions outside the strict Hölder class, or on inputs with discontinuities or heavy-tailed distributions, is not characterized. The numerical stability of the φ_K mapping and the practical implications of the excluded set Ω(flaw) are not quantified.

## Next Checks

1. **Numerical Validation of KST Decomposition**: Implement the Kolmogorov-Arnold Representation Theorem decomposition for a range of Hölder continuous functions (varying β and Q) and measure the actual error in practice. Test on functions with known KST representations versus those requiring numerical approximation.

2. **Translation Technique Implementation**: Construct a minimal Transformer with shared-weight feedforward layers but column-specific offsets, and verify through controlled experiments that the translation technique successfully enables different column-wise transformations. Compare against a baseline without translation to quantify the effect.

3. **Self-Attention Aggregation Robustness**: Implement the single-head softmax self-attention aggregation and test its robustness to different input distributions, sequence lengths, and attention maskings. Verify that the aggregation remains effective when extending beyond the theoretical single-head, no-masking setting.