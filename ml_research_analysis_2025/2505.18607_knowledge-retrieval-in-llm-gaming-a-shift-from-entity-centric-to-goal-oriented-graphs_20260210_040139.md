---
ver: rpa2
title: 'Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented
  Graphs'
arxiv_id: '2505.18607'
source_url: https://arxiv.org/abs/2505.18607
tags:
- craft
- goal
- pickaxe
- goals
- iron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Goal-Oriented Graphs (GoGs) as a structured
  alternative to GraphRAG for improving reasoning in LLM gaming tasks. Instead of
  fragmented entity-relation triples, GoGs represent goals and their dependencies
  as nodes and edges, enabling coherent reasoning chains.
---

# Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs

## Quick Facts
- arXiv ID: 2505.18607
- Source URL: https://arxiv.org/abs/2505.18607
- Reference count: 40
- The paper introduces Goal-Oriented Graphs (GoGs) as a structured alternative to GraphRAG for improving reasoning in LLM gaming tasks

## Executive Summary
This paper addresses the limitations of entity-centric GraphRAG for multi-step reasoning tasks by introducing Goal-Oriented Graphs (GoGs). Unlike traditional GraphRAG that fragments knowledge into entity-relation triples, GoGs represent goals and their dependencies as nodes and edges, preserving task hierarchy and supporting coherent reasoning chains. The framework constructs a directed graph from source documents where each node encodes a goal with preconditions and postconditions, and edges denote subgoal relationships. During inference, the system retrieves the most relevant goal and recursively obtains its subgoals to form a reasoning path that guides LLM prompting. Experiments on Minecraft tasks show GoGs outperform GraphRAG and other baselines, especially on complex tasks requiring long-horizon planning.

## Method Summary
The method constructs a directed graph where nodes represent goals (with attributes like name, description, preconditions, postconditions, required tools, and materials) and edges represent subgoal dependencies. During knowledge base construction, documents are chunked and processed by an LLM to extract goal tuples, which are then deduplicated using embedding similarity and condition matching. Subgoal relationships are derived by matching preconditions to postconditions across goals. For inference, the system retrieves top-k goals via embedding similarity, selects the best goal with an LLM, performs depth-first search to retrieve all subgoals recursively, aggregates material requirements, and generates a plan using the LLM. The framework was evaluated on 66 Minecraft tasks across 7 difficulty tiers using success rate and plan quality metrics.

## Key Results
- GoGs achieve success rate improvements of up to 57.84% on armor tasks compared to GraphRAG
- The framework maintains above 50% success rate on diamond tasks while GraphRAG degrades
- GoGs use 703 nodes and 1,653 edges versus GraphRAG's 12,388 nodes and 18,347 edges from the same source documents
- Material list inclusion significantly improves plan quality across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Goal-oriented graph structure reduces retrieval noise compared to entity-centric graphs for multi-step reasoning tasks.
- Mechanism: By encoding goals as nodes (with preconditions/postconditions) rather than fragmented entity-relation triples, the graph preserves task hierarchy and logical dependencies. This avoids the "dense 1-hop neighborhood" problem where generic entities connect to many irrelevant nodes.
- Core assumption: Domain knowledge has inherent procedural structure that can be extracted and represented as goal hierarchies.
- Evidence anchors:
  - [abstract] "GoGs preserve task hierarchy and support coherent reasoning chains"
  - [section 3.4] GoG contains 703 nodes and 1,653 edges vs GraphRAG's 12,388 nodes and 18,347 edges from the same source
  - [section 4.2] "GraphRAG returns many irrelevant but connected nodes from the 1-hop neighbourhood"
  - [corpus] Limited corpus support; neighbor papers focus on GraphRAG improvements but don't address goal-oriented restructuring directly
- Break condition: Domains where knowledge is primarily declarative/encyclopedic rather than procedural; or where goals cannot be clearly decomposed into subgoals with explicit dependencies.

### Mechanism 2
- Claim: Recursive subgoal retrieval explicitly constructs reasoning paths that guide LLM planning.
- Mechanism: Given a task query, the system (1) retrieves top-k matching goals via embedding similarity, (2) LLM selects the best goal, (3) depth-first search retrieves all subgoals recursively, forming a complete goal tree. This explicit chain replaces implicit LLM reasoning about intermediate steps.
- Core assumption: Tasks have achievable goal decompositions that terminate in atomic operations.
- Evidence anchors:
  - [abstract] "explicit retrieval of reasoning paths by first identifying high-level goals and recursively retrieving their subgoals"
  - [section 3.3] "After selecting the goal that matches the query, we retrieve all subgoals using depth-first search"
  - [corpus] No direct corpus validation of this specific retrieval mechanism
- Break condition: Cyclic subgoal relationships (paper notes these exist and are handled by tracking visited nodes); tasks where multiple valid decomposition paths exist and must be ranked by context.

### Mechanism 3
- Claim: Encoding preconditions and postconditions per goal enables material/quantity reasoning that prevents execution failures.
- Mechanism: Each goal node includes explicit `req_tools`, `req_materials`, and `postconditions` attributes. The goal tree traversal aggregates these into a complete material list with quantities, which is provided to the LLM for plan generation.
- Core assumption: LLMs struggle with implicit quantity calculations but can correctly sequence steps when given explicit material requirements.
- Evidence anchors:
  - [section 3.1] "preconditions are a list of prerequisites that are needed before the goal can be pursued, and postconditions explicitly describe the expected outcome"
  - [section 4.5] GraphRAG and vanilla baselines "generate plans that produce too few sticks, forcing the agent to replan"
  - [section 4.4] Ablation shows material list alone improves all four plan quality metrics significantly
  - [corpus] No corpus papers directly validate precondition-based planning augmentation
- Break condition: Domains where preconditions are implicit or context-dependent; real-world environments where resource availability is uncertain.

## Foundational Learning

- Concept: Hierarchical Task Network (HTN) planning
  - Why needed here: GoG is fundamentally an HTN-style decomposition where high-level goals recursively decompose into subgoals until reaching primitive actions. Understanding HTN helps you see why the graph structure works.
  - Quick check question: Can you explain why HTN planning differs from classical STRIPS-style planning?

- Concept: Graph-based retrieval vs. vector-only RAG
  - Why needed here: The paper positions GoG against GraphRAG. You need to understand how graph structure enables traversal-based retrieval vs. pure embedding similarity.
  - Quick check question: What types of queries benefit from graph traversal over semantic similarity search?

- Concept: LLM prompt engineering with structured context
  - Why needed here: The retrieved goal tree and material list are injected into prompts. Understanding how to format structured data for LLM consumption affects success.
  - Quick check question: How does providing explicit quantities vs. implicit quantity requirements affect LLM planning accuracy?

## Architecture Onboarding

- Component map:
  - Knowledge Base Construction: Text chunking → LLM goal extraction → Goal deduplication (embedding similarity + condition matching) → Subgoal relationship derivation
  - Inference Pipeline: Query → Top-k goal retrieval (embedding) → LLM goal selection → DFS subgoal retrieval → Goal tree parsing → Material list aggregation → LLM plan generation

- Critical path:
  1. Goal extraction prompt quality (Appendix B) - determines graph coverage and accuracy
  2. Deduplication threshold (θ = 0.92) - controls graph density vs. completeness
  3. Material list generation from goal tree - directly feeds planning prompt
  4. Plan generation prompt - final output quality

- Design tradeoffs:
  - k value for top-k retrieval: Paper uses k=3; higher k increases retrieval diversity but adds LLM selection overhead with diminishing returns (Table 2 shows minimal improvement beyond k=3)
  - Graph granularity: Fewer, more abstract goals reduce retrieval noise but may miss critical intermediate steps
  - Embedding model choice: nomic-text-embed-v1.5 used; different models affect goal matching accuracy

- Failure signatures:
  - "smelt diamond" hallucination (Table 5): LLM generates invalid steps when goal structure doesn't explicitly encode domain rules
  - Insufficient quantity planning (Tables 7, 8): Material list incomplete or LLM ignores explicit counts
  - GraphRAG degradation vs. vanilla (Table 1): Noisy 1-hop neighborhood retrieval introduces irrelevant context

- First 3 experiments:
  1. Replicate the plan quality ablation (Section 4.4) on your domain: test goal-tree-only vs. material-list-only vs. both to isolate contribution
  2. Measure graph construction cost: compare LLM extraction calls needed for GoG vs. GraphRAG on equivalent source documents
  3. Test retrieval precision: for a held-out set of task queries, measure whether top-k goal retrieval returns the correct goal (analogous to Table 2 analysis)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Goal-Oriented Graph (GoG) framework be adapted for domains that lack the well-defined logic and rich documentation found in Minecraft?
- Basis in paper: [explicit] The authors state, "Applying GoG to less structured or poorly documented domains may require additional adaptation or domain-specific tuning."
- Why unresolved: The current evaluation is restricted to Minecraft, which has explicit crafting recipes and wiki documentation.
- What evidence would resolve it: Successful application of GoG to domains with ambiguous procedures (e.g., creative writing, legal reasoning) or sparse documentation.

### Open Question 2
- Question: What methods can effectively mitigate construction errors caused by LLM misinterpretation during the goal and subgoal extraction phase?
- Basis in paper: [explicit] The authors note that "Errors in this extraction process... can propagate through the graph and affect downstream performance. Future work could pay more attention to reduce the construction errors."
- Why unresolved: The framework currently relies on LLMs to generate the graph structure without verification steps to catch hallucinations.
- What evidence would resolve it: A modified framework incorporating validation loops or hybrid extraction methods that reduces graph error rates compared to the baseline.

### Open Question 3
- Question: Why does the inclusion of the goal tree decrease planning performance for the Gemma 3 model, and does this indicate a limitation in handling hierarchical context?
- Basis in paper: [explicit] In Section 4.4, the authors observe performance drops with Gemma 3 when the goal tree is included, noting, "We leave further investigation of this issue to future work."
- Why unresolved: It is unclear if the failure is due to context length, information overlap, or the model's specific architecture.
- What evidence would resolve it: An ablation study isolating context length vs. hierarchical structure, or testing across a wider range of open-source models.

### Open Question 4
- Question: How does retrieval efficiency and context noise scale as the size and complexity of the Goal-Oriented Graph increase?
- Basis in paper: [explicit] The limitations section warns, "As the number of knowledge sources increases... This can lead to increasingly complex graphs, potentially introducing retrieval inefficiencies... We plan to explore the scalability... in future work."
- Why unresolved: The current experiments utilize a fixed-size knowledge base (514 Wiki pages); performance on massive corpora is unknown.
- What evidence would resolve it: Benchmarking GoG's retrieval latency and planning accuracy on datasets orders of magnitude larger than the Minecraft Wiki.

## Limitations
- The framework is evaluated only on Minecraft, limiting generalizability to domains without formal procedural knowledge
- LLM-based graph construction requires multiple calls per document, creating significant computational overhead
- The recursive subgoal retrieval assumes acyclic goal hierarchies, though cycles exist and are handled without detailed algorithmic description

## Confidence

- **High confidence**: Graph construction efficiency claims (GoG uses ~5.4% of GraphRAG nodes/edges while maintaining comparable or better performance)
- **Medium confidence**: Success rate improvements across all task types, though improvements vary significantly by task complexity
- **Low confidence**: Generalization to non-Minecraft domains with less structured procedural knowledge

## Next Checks

1. **Cross-domain generalization test**: Apply the GoG framework to a non-gaming domain with formal procedural knowledge (e.g., cooking recipes or software installation procedures) and measure whether the same success rate improvements hold when entity-centric GraphRAG would normally be used.

2. **Material quantity validation**: Design a controlled experiment where material lists are intentionally omitted from the goal tree and observe whether plan quality degrades by a measurable amount, specifically tracking whether LLMs can implicitly infer correct quantities from goal descriptions alone.

3. **Graph construction cost analysis**: Measure the exact number of LLM calls required to construct GoG versus GraphRAG from the same corpus, including both goal extraction and relationship derivation phases, to quantify the practical computational tradeoff between graph density and retrieval accuracy.