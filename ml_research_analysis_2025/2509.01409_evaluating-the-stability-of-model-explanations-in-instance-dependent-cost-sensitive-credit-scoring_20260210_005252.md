---
ver: rpa2
title: Evaluating the stability of model explanations in instance-dependent cost-sensitive
  credit scoring
arxiv_id: '2509.01409'
source_url: https://arxiv.org/abs/2509.01409
tags:
- uni00000008
- uni00000013
- idcs
- uni00000018
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the underexplored impact of instance-dependent
  cost-sensitive (IDCS) classifiers on model explanation stability in credit scoring.
  While IDCS methods improve cost-efficiency, their effect on the stability of post-hoc
  explainability techniques like SHAP and LIME remains unclear.
---

# Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring

## Quick Facts
- **arXiv ID**: 2509.01409
- **Source URL**: https://arxiv.org/abs/2509.01409
- **Reference count**: 6
- **Key outcome**: IDCS classifiers significantly reduce the stability of SHAP and LIME explanations compared to traditional models, with instability increasing as class imbalance grows

## Executive Summary
This study investigates the underexplored relationship between instance-dependent cost-sensitive (IDCS) classifiers and the stability of post-hoc explainability techniques in credit scoring. While IDCS methods are known to improve cost-efficiency in financial decision-making, their impact on explanation stability remains unclear. The research systematically evaluates this trade-off using four open-source credit scoring datasets, nested 5-fold cross-validation, and controlled resampling to create varying class imbalance scenarios. The findings reveal a significant degradation in SHAP and LIME explanation stability when using IDCS classifiers, with the effect intensifying as class imbalance increases. This work highlights a critical challenge for deploying IDCS models in regulated financial settings where both cost optimization and interpretability are essential.

## Method Summary
The study employs a comprehensive experimental framework to evaluate explanation stability in IDCS credit scoring models. Four open-source credit scoring datasets are analyzed using nested 5-fold cross-validation to ensure robust model evaluation. Class imbalance is systematically controlled through resampling techniques, creating multiple imbalance scenarios for each dataset. Both traditional and IDCS classifiers are trained and compared, with SHAP and LIME serving as the primary post-hoc explanation methods. Explanation stability is assessed through multiple metrics across different imbalance levels, allowing for a detailed characterization of how IDCS approaches affect interpretability. The methodology specifically isolates the effect of cost-sensitive learning on explanation consistency while controlling for other variables.

## Key Results
- IDCS classifiers significantly reduce SHAP and LIME explanation stability compared to traditional models
- Explanation instability increases proportionally with class imbalance severity
- A fundamental trade-off exists between cost optimization and interpretability in credit scoring applications

## Why This Works (Mechanism)
The mechanism underlying this phenomenon relates to how IDCS classifiers optimize for cost-sensitive performance metrics rather than pure classification accuracy. By weighting different types of errors based on their financial impact, these models learn decision boundaries that prioritize cost minimization over stable feature importance attribution. This optimization process creates explanations that are more sensitive to input variations and dataset characteristics, particularly under imbalanced conditions where the cost structure amplifies the impact of minority class predictions.

## Foundational Learning
- **Instance-dependent cost-sensitive learning**: Essential for understanding how financial penalties shape model behavior differently than accuracy-based objectives. Quick check: Verify that cost matrices properly reflect real financial consequences of different prediction errors.
- **Post-hoc explainability techniques**: Critical background for evaluating SHAP and LIME's behavior under different learning paradigms. Quick check: Confirm that explanation methods are properly calibrated and consistent across runs.
- **Class imbalance effects**: Necessary context for understanding how skewed data distributions interact with cost-sensitive optimization. Quick check: Measure imbalance ratios and their impact on both model performance and explanation stability.

## Architecture Onboarding
- **Component map**: Data preprocessing -> IDCS classifier training -> Explanation generation (SHAP/LIME) -> Stability evaluation -> Performance comparison
- **Critical path**: The stability evaluation phase is critical as it directly measures the research hypothesis about explanation degradation under IDCS conditions
- **Design tradeoffs**: Balancing cost optimization against interpretability requires careful consideration of regulatory requirements and business objectives in credit scoring
- **Failure signatures**: Explanation instability manifests as high variance in feature importance scores across similar instances or repeated runs
- **First experiments**: 1) Compare explanation stability on balanced vs. imbalanced datasets with traditional classifiers, 2) Test different cost matrix configurations in IDCS models, 3) Evaluate alternative explanation methods beyond SHAP and LIME

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Study focuses exclusively on two post-hoc explanation methods (SHAP and LIME), limiting generalizability
- Synthetic resampling approach may not fully capture real-world credit scoring conditions
- Limited to four open-source datasets, potentially missing domain-specific variations

## Confidence
- **Explanation stability degradation**: Medium
- **Trade-off with cost optimization**: Medium
- **Generalizability to other methods/datasets**: Low

## Next Checks
1. Replicate experiments using additional post-hoc explanation methods (Anchors, Integrated Gradients) to verify if stability degradation extends beyond SHAP and LIME

2. Conduct real-world field testing with actual imbalanced credit scoring datasets from financial institutions to validate findings under operational conditions

3. Investigate the impact of different cost-sensitive loss functions and their parameters on explanation stability to determine if certain configurations mitigate the observed trade-off