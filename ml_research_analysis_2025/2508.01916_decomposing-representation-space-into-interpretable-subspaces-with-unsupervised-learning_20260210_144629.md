---
ver: rpa2
title: Decomposing Representation Space into Interpretable Subspaces with Unsupervised
  Learning
arxiv_id: '2508.01916'
source_url: https://arxiv.org/abs/2508.01916
tags:
- subspace
- subspaces
- information
- more
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to decompose neural representation
  space into interpretable subspaces without supervision. It introduces Neighbor Distance
  Minimization (NDM), which learns an orthogonal matrix to rotate and partition the
  space so that each subspace has minimal distance to its nearest neighbor.
---

# Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning

## Quick Facts
- **arXiv ID:** 2508.01916
- **Source URL:** https://arxiv.org/abs/2508.01916
- **Reference count:** 40
- **Primary result:** Introduces Neighbor Distance Minimization (NDM) to decompose neural representations into interpretable, orthogonal subspaces without supervision, validated on GPT-2 and larger models.

## Executive Summary
This paper presents a method to automatically decompose neural representation spaces into interpretable subspaces without supervision. The approach, called Neighbor Distance Minimization (NDM), learns an orthogonal matrix to rotate and partition the space so that each subspace has minimal distance to its nearest neighbor. This encourages subspaces to be as independent as possible, making them interpretable as model variables. The method is validated in toy models, GPT-2 Small, and larger models (Qwen2.5-1.5B, Gemma-2-2B), showing that NDM finds meaningful subspaces encoding distinct aspects of inputs and knowledge. Quantitative tests based on known GPT-2 circuits show high Gini coefficients (up to 0.89), indicating strong concentration of target information in subspaces. Qualitative analysis reveals consistent, interpretable content within subspaces.

## Method Summary
NDM works by learning an orthogonal matrix $R$ that rotates the activation space to minimize nearest-neighbor distances within subspaces. The method starts with a fine-grained partition (e.g., 32-dimensional subspaces) and iteratively updates $R$ to minimize the average distance to the nearest neighbor in each subspace. It then periodically estimates mutual information between subspaces and merges those with high dependency. This process continues until subspaces are as independent as possible while maintaining interpretability. The method implicitly minimizes Total Correlation between subspaces, encouraging the decomposition into mutually exclusive feature groups. NDM scales to 2B models, identifying separate subspaces for context and parametric knowledge routing.

## Key Results
- NDM achieves Gini coefficients up to 0.89 on GPT-2 circuits, compared to 0.3-0.4 for identity/random baselines
- Identifies distinct subspaces for context vs. parametric knowledge in 2B models
- Maintains interpretability across different layers and model scales
- Successfully recovers ground-truth feature groups in toy models with mutually exclusive features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing nearest-neighbor distance within subspaces forces the partitioning of "feature groups" (sets of mutually exclusive features) into orthogonal subspaces.
- **Mechanism:** If features in a group are mutually exclusive (only one activates at a time), their representations cluster in specific directions. By rotating the space (learning an orthogonal matrix $R$) to minimize the distance to the nearest neighbor *within* a partition, the algorithm encourages configurations where points in a subspace are tightly clustered (low entropy) rather than spread out.
- **Core assumption:** The model represents information in "feature groups" where features are mutually exclusive within the group but independent across groups.
- **Evidence anchors:** [Page 3] "The correct partition will make the distance to the nearest neighbor inside subspaces smaller, whereas incorrect partitions entail large distances." [Page 17] Toy model experiments confirm $RW$ aligns with ground truth feature groups when sparsity/mutual exclusivity holds.
- **Break condition:** If the model relies heavily on distributed representations where features are **not** mutually exclusive or co-occur frequently across all conceptual groups, the "natural" orthogonal partition may not exist.

### Mechanism 2
- **Claim:** NDM functions as an unsupervised proxy for minimizing the Total Correlation (multi-variate mutual information) between subspaces.
- **Mechanism:** Nearest neighbor distance is a proxy for entropy. Minimizing the sum of distances in subspaces minimizes the sum of marginal entropies. Since the entropy of the whole space is invariant under rotation, minimizing marginal entropies is equivalent to minimizing the Total Correlation (dependency) between subspaces.
- **Core assumption:** The relationship between k-nearest neighbor distance and entropy holds for the activation distribution.
- **Evidence anchors:** [Page 4] "We look for a partition... such that the resulting subspaces are as independent as possible." [Appendix B.1] Derivation of the Total Correlation perspective. [Page 25] Fig. 14 shows MI decreases significantly after NDM training compared to the identity matrix.
- **Break condition:** If the activation space has non-Gaussian or complex topological structures where distance-to-entropy heuristics fail.

### Mechanism 3
- **Claim:** High Gini coefficients in subspace patching indicate that specific "circuit variables" (e.g., position, token identity) are concentrated in single, learned subspaces.
- **Mechanism:** If a subspace encodes a specific variable (e.g., "previous token"), patching that subspace with a counterfactual value should cause a large effect, while patching other subspaces should have minimal effect. This inequality in effect size is measured by the Gini coefficient.
- **Core assumption:** The "variables" identified by known circuits (like IOI in GPT-2) are the dominant information in that layer.
- **Evidence anchors:** [Page 5] "If the target information is mainly encoded in one subspace, the patching effect for this one would be much higher than others." [Page 6] Table 1 shows NDM achieves Gini coefficients up to 0.89, whereas Identity/Random baselines stay around 0.3.
- **Break condition:** If the model distributes a single logical variable across many non-orthogonal subspaces (anti-concentration), patching effects will be diffuse (low Gini).

## Foundational Learning

- **Concept:** Superposition & Mutual Exclusivity
  - **Why needed here:** The paper argues that *mutual exclusivity* (only one feature active per group) is a stronger driver for orthogonal subspaces than mere sparsity.
  - **Quick check question:** Can you explain why two features that always co-occur cannot be easily separated into two orthogonal subspaces?

- **Concept:** Orthogonal Matrices & Rotation
  - **Why needed here:** The method learns a matrix $R$ to rotate the basis. Understanding that rotation preserves vector norms and geometric relationships is crucial for interpreting the "preimages."
  - **Quick check question:** Why does the paper constrain $R$ to be orthogonal rather than just any linear transformation?

- **Concept:** Total Correlation (Multi-information)
  - **Why needed here:** This is the theoretical objective NDM approximates. It measures the redundancy among a set of random variables (subspaces).
  - **Quick check question:** How does minimizing Total Correlation differ from minimizing Pairwise Mutual Information?

## Architecture Onboarding

- **Component map:** Input activations $H$ -> Orthogonal matrix $R$ -> Partitioned subspaces -> Nearest neighbor search -> Distance minimization -> Mutual information estimation -> Subspace merging
- **Critical path:**
  1. **Initialization:** $R = I$ (Identity)
  2. **Optimization:** For a batch, find nearest neighbors in current subspaces; update $R$ to minimize distance
  3. **Merging:** Periodically estimate MI between subspaces; if MI > $\tau$, merge subspaces (combine dimensions)
- **Design tradeoffs:**
  - **Unit Size:** Smaller units allow fine-grained features but increase optimization difficulty. (Paper uses 32 or 64)
  - **Search Number:** Higher $N$ stabilizes estimates but drastically increases compute (vector search cost)
  - **Distance Metric:** Euclidean found to work better than Cosine similarity (Appendix C.2)
- **Failure signatures:**
  - **Subspace Collapse:** All information falls into one massive subspace (MI never drops below threshold)
  - **Local Minima:** $R$ gets stuck; Appendix C.2 notes "R usually get stuck at sub-optimal point" requiring re-initialization heuristics in toy settings
  - **Uninterpretability:** InversionView shows mixed concepts in one subspace (indicates imperfect decomposition)
- **First 3 experiments:**
  1. **Toy Model Verification:** Replicate the $d=12, z=40$ toy model experiment (Fig 9) to verify your implementation of $R$ correctly recovers the ground-truth feature groups
  2. **GPT-2 Layer 4:** Train NDM on `h4_post` of GPT-2 Small. Check the Gini coefficient on the "Previous Token" task (Test 1 in paper)
  3. **InversionView Inspection:** Pick a random activation from the "Previous Token" subspace found in Exp 2 and check if the retrieved nearest neighbors actually share the same previous token

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the NDM framework be extended to support hierarchical decomposition to identify fine-grained variables encoded in subspaces smaller than the current 32-dimension minimum?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the method currently provides a "coarse-grained partition" starting from 32 dimensions and suggest in Future Directions that "a hierarchical structure of subspaces could also be helpful."
- **Why unresolved:** The current merging algorithm starts from a fixed unit size, which may obscure intermediate variables that take values from small sets (e.g., sentiment) and are encoded in tiny subspaces.
- **What evidence would resolve it:** A recursive application of NDM that successfully isolates low-dimensional features (e.g., specific syntactic roles) and validates them with causal intervention tests.

### Open Question 2
- **Question:** Can "subspace circuits" be effectively constructed by analyzing the interaction between model weights and the learned orthogonal subspaces across layers?
- **Basis in paper:** [explicit] The Future Directions section proposes "building connections between subspaces across layers with model weights, we could construct input-independent circuits."
- **Why unresolved:** The current work focuses on interpreting subspaces within individual layers using activation patching but does not map how attention heads or MLPs read from and write to these subspaces to form a complete circuit.
- **What evidence would resolve it:** A demonstrated methodology for tracing information flow through subspaces via weight matrices, producing a circuit description that remains valid across different input sequences.

### Open Question 3
- **Question:** Does the reliance on the Linear Representation Hypothesis limit the applicability of NDM to models or layers with non-linear representations?
- **Basis in paper:** [explicit] The Limitations section notes, "we rely on the Linear Representation Hypothesis... which could be untrue in some cases," specifically citing potential non-linearities in certain architectures.
- **Why unresolved:** The method interprets subspaces by finding preimages based on cosine similarity, assuming features correspond to linear directions. If features are non-linear manifolds, this interpretability approach may fail.
- **What evidence would resolve it:** Application of NDM to models with known non-linear representations (e.g., certain recurrent networks) to test if the resulting subspaces remain interpretable or if the method requires modification.

## Limitations

- **Conceptual assumptions:** The effectiveness of NDM critically depends on the existence of mutually exclusive feature groups in the model's representation space, which may not hold across different architectures or training paradigms.
- **Scalability concerns:** The nearest-neighbor search becomes computationally expensive as dimensionality increases, with unclear scaling behavior for much larger models beyond 2B parameters.
- **Evaluation scope:** The paper primarily evaluates on GPT-2 Small and two other models using specific circuits, with generalizability to other tasks, modalities, or more complex reasoning capabilities remaining untested.

## Confidence

- **Mechanism 1 (Feature group decomposition):** Medium confidence. The theoretical argument is sound, and toy model experiments support it, but the assumption about mutual exclusivity in real models is not rigorously validated beyond showing successful outcomes.
- **Mechanism 2 (Total Correlation minimization):** Medium confidence. The mathematical connection between nearest-neighbor distance and entropy is well-established, and MI reduction is empirically observed. However, the practical relationship between this proxy objective and true independence is not fully characterized.
- **Mechanism 3 (Interpretability via Gini coefficients):** High confidence. The patching experiments directly measure the concentration of specific variables in subspaces, and the results are consistent across multiple known circuits and model sizes.

## Next Checks

1. **Alternative data distribution test:** Apply NDM to GPT-2 Small using OpenWebText (which the paper notes performs worse) and systematically characterize the degradation in Gini coefficients and subspace interpretability compared to MiniPile. This would validate the data sensitivity claims.

2. **Distributed representation challenge:** Design a synthetic dataset where features are explicitly designed to be non-mutually-exclusive (high co-occurrence across feature groups) and test whether NDM fails to find meaningful orthogonal subspaces, confirming the theoretical break condition.

3. **Cross-model generalization:** Apply NDM to a transformer architecture with different design choices (e.g., attention-only models, or models with layer normalization placement variations) and compare the resulting subspace structure and interpretability to the MLP-focused results presented.