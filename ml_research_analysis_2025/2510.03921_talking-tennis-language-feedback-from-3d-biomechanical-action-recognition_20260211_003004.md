---
ver: rpa2
title: 'Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition'
arxiv_id: '2510.03921'
source_url: https://arxiv.org/abs/2510.03921
tags:
- stroke
- feedback
- tennis
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between automated stroke classification
  and actionable coaching feedback in tennis by developing a pipeline that combines
  CNN-LSTM-based 3D motion recognition with LLM-generated language feedback. The method
  extracts biomechanical features such as joint angles, velocities, and kinetic chain
  patterns from motion capture data, then grounds LLM prompts with reference ranges
  to ensure compliance and interpretability.
---

# Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition

## Quick Facts
- arXiv ID: 2510.03921
- Source URL: https://arxiv.org/abs/2510.03921
- Reference count: 13
- Primary result: 79.17% stroke classification accuracy with LLM-generated biomechanical feedback rated as interpretable and actionable by coaches

## Executive Summary
This paper bridges the gap between automated tennis stroke classification and actionable coaching feedback by integrating 3D motion recognition with LLM-generated language guidance. The authors develop a pipeline that extracts biomechanical features from motion capture data, then grounds LLM prompts with reference ranges to produce structured, coach-aligned feedback. Results show statistically validated expert-novice differences in stroke mechanics, 100% compliance with LLM output constraints, and coach ratings indicating the feedback is interpretable, actionable, and aligned with expert strategies. The framework offers a scalable approach for translating biomechanical analysis into practical coaching guidance.

## Method Summary
The method combines a CNN-LSTM model for stroke classification with LLM-generated biomechanical feedback. EfficientNet-B0 extracts spatial features from 16-frame sequences, which are processed by a 2-layer LSTM to capture temporal dynamics, achieving 79.17% accuracy across 12 stroke classes. Biomechanical features including joint angles, velocities, kinetic chain timing, and trunk rotation are computed from 3D skeleton data. These features are compared to hand-specified reference ranges per stroke type, and the resulting deviation reports are used to construct structured prompts for GPT-4o. The LLM generates feedback constrained to a specific format: overall score out of 10, 2-3 sentence diagnosis, and exactly 3 corrections.

## Key Results
- 79.17% stroke classification accuracy, improving >5 percentage points over previous leading performances
- Statistically validated expert-novice biomechanical differences, with stroke duration showing largest group difference (Cohen's d ≈ 0.92)
- LLM feedback consistently met formatting constraints (100% compliance) and received positive coach ratings (interpretability 7.3-8.4, actionability 8.2-9.3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid CNN-LSTM architecture improves fine-grained stroke classification by explicitly modeling sequential motion patterns.
- Mechanism: EfficientNet-B0 extracts 1280-dim spatial features per frame → 2-layer LSTM (512 hidden units, dropout 0.4) captures temporal dependencies across 16-frame sequences → final hidden state maps to 12 stroke classes.
- Core assumption: Tennis strokes have discriminative temporal dynamics that pure spatiotemporal convolutions may under-model.
- Evidence anchors:
  - [abstract] "CNN-LSTM-based 3D motion recognition... 79.17% stroke classification accuracy"
  - [section 4.1] ">5 percentage point improvement over previous leading performances" (vs. SlowFast at 73.96%)
  - [corpus] Weak direct corpus support for CNN-LSTM superiority in tennis specifically; related work (Hovad et al., 2024) cited as baseline.
- Break condition: If frame-to-frame transitions carry minimal discriminative signal beyond static pose, LSTM provides negligible gain over CNN-only baselines.

### Mechanism 2
- Claim: Biomechanical features (joint angles, velocities, kinetic chain timing) differentiate expert from novice strokes and provide interpretable diagnostics.
- Mechanism: Raw 3D joint coordinates → three-point angle computation, finite-difference velocity, trunk rotation via arctan2 → statistical comparison (t-tests, Mann–Whitney U) reveals expertise-correlated features.
- Core assumption: Expert-novice differences manifest in quantifiable kinematic features relevant to coaching.
- Evidence anchors:
  - [abstract] "expert-novice biomechanical differences validated statistically"
  - [section 4.2] "Stroke duration showed the largest group difference (Cohen's d ≈ 0.92, p = 0.069)... Maximum racket velocity also showed a medium effect size (d ≈ 0.63)"
  - [corpus] STROKEVISION-BENCH and rehabilitation HAR papers suggest biomechanical features generalize across motor skill domains, but direct replication in tennis is limited.
- Break condition: If dataset-specific noise or marker proxy issues (hand marker for racket tip) dominate signal, feature-validity claims weaken.

### Mechanism 3
- Claim: Deterministic reference-range grounding constrains LLM output to biomechanically plausible, format-compliant coaching feedback.
- Mechanism: Feature dictionary + stroke-type → compare_to_reference() produces deviation reports → structured prompt enforces "Overall Score: X/10", 2–3 sentence diagnosis, exactly 3 corrections → LLM (GPT-4o, temp=0.2, max_tokens=120) generates feedback.
- Core assumption: Hand-specified reference intervals (from literature + expert consultation) are sufficiently accurate; LLM follows constraints when explicitly prompted.
- Evidence anchors:
  - [abstract] "LLM feedback consistently met formatting constraints and was rated by coaches as interpretable, actionable"
  - [section 4.3] "100% of outputs satisfied all three constraints"; coach evaluation: interpretability 7.3–8.4, actionability 8.2–9.3, coaching alignment 7.5–9.0
  - [corpus] No direct corpus evidence on reference-range grounding specifically; prompt engineering literature suggests constraint adherence varies by task complexity.
- Break condition: If reference ranges are mis-specified or LLM temperature/format constraints are loosened, hallucination risk and coherence degrade.

## Foundational Learning

- Concept: **Temporal modeling with RNNs/LSTMs**
  - Why needed here: Tennis strokes are sequential; frame-by-frame spatial features alone miss motion dynamics critical for classification.
  - Quick check question: Given a 16-frame sequence with shuffled order, would an LSTM-only model classify differently than a CNN-only model?

- Concept: **Biomechanical kinematic features (joint angles, velocities, kinetic chain)**
  - Why needed here: These are the intermediate representation between raw pose data and coaching semantics; interpretability depends on their validity.
  - Quick check question: If trunk rotation is computed via shoulder-line orientation, what happens if shoulder markers are occluded or noisy?

- Concept: **LLM grounding and constrained decoding**
  - Why needed here: Unconstrained LLMs may fabricate metrics; reference-range comparison and strict prompt templates bind output to evidence.
  - Quick check question: If a feature value is missing, should the LLM infer it or explicitly flag it as unavailable?

## Architecture Onboarding

- Component map:
  Raw video/skeleton → CNN frame features → LSTM sequence encoding → classification + feature extraction → reference comparison → LLM prompt → feedback string

- Critical path:
  Raw video/skeleton → CNN frame features → LSTM sequence encoding → classification + feature extraction → reference comparison → LLM prompt → feedback string

- Design tradeoffs:
  - EfficientNet-B0 vs. larger backbones: Faster training, lower capacity; may underfit complex motions.
  - 16-frame clips: Balances temporal coverage vs. computational cost; may miss full stroke context for long motions.
  - Hand-specified reference ranges: Rapid prototyping vs. empirical derivation; may not generalize across skill levels or body types.
  - Low temperature (0.2): Determinism vs. creativity; may produce repetitive phrasing across similar strokes.

- Failure signatures:
  - Classification confusion between visually similar strokes (e.g., slice vs. topspin forehand) → check LSTM hidden state discrimination.
  - Feature extraction errors (NaN velocities, invalid angles) → check marker presence, imputation logic.
  - LLM output malformed (missing score, >3 corrections) → verify prompt template, API key, token limit.
  - Feedback misaligned with coach judgment → inspect reference range appropriateness for stroke type.

- First 3 experiments:
  1. **Ablate LSTM**: Replace with frame-averaged CNN features → measure classification accuracy drop to quantify temporal modeling contribution.
  2. **Perturb reference ranges**: Shift bounds by ±10–20% → assess LLM feedback sensitivity and coach alignment scores.
  3. **Cross-validate on held-out stroke types**: Train on 10 classes, test on 2 → evaluate generalization to unseen stroke categories.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hand-specified reference ranges for biomechanical features be replaced with empirically derived, population-normed bounds?
- Basis in paper: [explicit] The authors state: "future work would refine these hand-specified intervals with empirically derived bounds" (Section 3.3).
- Why unresolved: Current reference ranges were manually encoded based on literature and coach discussions, not systematically extracted from large-scale motion data.
- What evidence would resolve it: Statistical analysis of feature distributions across skill levels in a larger dataset to establish data-driven optimal ranges.

### Open Question 2
- Question: Does the framework generalize to other sports with different kinetic chain patterns and equipment constraints?
- Basis in paper: [explicit] The conclusion claims the framework "serves as a general template for embodied AI" across sports, rehabilitation, and human-robot skill learning.
- Why unresolved: Tennis-specific features (racket dynamics, specific joint angles) may not transfer directly to other domains.
- What evidence would resolve it: Application of the pipeline to a different sport (e.g., golf, baseball) with modified feature extractors and evaluation of coaching feedback quality.

### Open Question 3
- Question: Can rotational biomechanical features be improved to better discriminate expert from novice performance?
- Basis in paper: [inferred] Rotation-based metrics "exhibited smaller differences" (Section 4.2) compared to temporal and velocity features in expert-novice comparisons.
- Why unresolved: The current rotation features may not capture the most relevant aspects of trunk coordination for skill differentiation.
- What evidence would resolve it: Development of alternative rotation metrics (e.g., segmental sequencing, inter-segmental phase relationships) and re-evaluation of discriminative power.

### Open Question 4
- Question: How sensitive is LLM feedback quality to the choice of temperature and decoding parameters?
- Basis in paper: [inferred] Decoding parameters (temperature=0.2, max_tokens=120) were fixed without ablation study.
- Why unresolved: Different settings may produce more natural, varied, or coach-aligned feedback; the current choice balances determinism but may sacrifice nuance.
- What evidence would resolve it: Ablation study varying temperature and token limits, evaluated by coaches on interpretability and actionability.

## Limitations
- Reference Range Grounding: Hand-specified REFERENCE_RANGES are not fully tabulated and were derived from literature and expert consultation rather than empirical data.
- Small Expert Sample: The THETIS dataset contains only 24 expert players, limiting statistical power for validating expert-novice differences.
- Cross-Validation Strategy: Train/validation/test split ratios and random seeds are not specified, affecting reproducibility.

## Confidence
- **High Confidence**: CNN-LSTM architecture achieves 79.17% stroke classification accuracy with >5 percentage point improvement over baseline.
- **Medium Confidence**: Expert-novice biomechanical differences are statistically validated (stroke duration, racket velocity).
- **Medium Confidence**: LLM feedback meets formatting constraints and receives positive coach ratings (interpretability 7.3–8.4, actionability 8.2–9.3).

## Next Checks
1. **Reference Range Sensitivity Analysis**: Systematically perturb REFERENCE_RANGES by ±10-20% and measure impact on LLM feedback quality and coach alignment scores to assess robustness to parameter specification.
2. **Cross-Validation on Held-Out Strokes**: Train on 10 stroke classes and test on 2 unseen classes to evaluate model generalization beyond the 12-class classification setting reported.
3. **LSTM Ablation Study**: Replace the 2-layer LSTM with frame-averaged CNN features and measure classification accuracy drop to quantify the contribution of temporal modeling to the 79.17% performance.