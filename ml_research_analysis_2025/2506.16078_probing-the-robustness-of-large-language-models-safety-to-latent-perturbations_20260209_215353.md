---
ver: rpa2
title: Probing the Robustness of Large Language Models Safety to Latent Perturbations
arxiv_id: '2506.16078'
source_url: https://arxiv.org/abs/2506.16078
tags:
- arxiv
- llama-3
- layer
- safety
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that current LLM safety alignment lacks robustness
  to latent perturbations, showing that minor changes to internal representations
  can bypass safety mechanisms even in well-aligned models. The authors propose a
  probing method using Negative Log-Likelihood (NLL) to detect vulnerable directions
  in latent space, leading to Activation Steering Attack (ASA) which injects perturbations
  into transformer layers to trigger unsafe responses.
---

# Probing the Robustness of Large Language Models Safety to Latent Perturbations

## Quick Facts
- arXiv ID: 2506.16078
- Source URL: https://arxiv.org/abs/2506.16078
- Reference count: 40
- This paper reveals that current LLM safety alignment lacks robustness to latent perturbations, showing that minor changes to internal representations can bypass safety mechanisms even in well-aligned models.

## Executive Summary
This work demonstrates that current LLM safety alignment is vulnerable to latent perturbations—minor changes to internal representations can bypass safety mechanisms even in well-aligned models. The authors introduce a probing method using Negative Log-Likelihood (NLL) to detect vulnerable directions in latent space, leading to Activation Steering Attack (ASA) which injects perturbations into transformer layers to trigger unsafe responses. They construct ASABench, a benchmark with 4,862 validated attack instances across 8 models, enabling standardized evaluation of latent robustness. To defend against these vulnerabilities, they introduce Layer-wise Adversarial Patch Training (LAPT), which fine-tunes models by injecting perturbations into identified fragile layers. Experiments show LAPT significantly improves safety robustness under latent perturbations while preserving general task performance, with accuracy drops under 0.05 on GSM8K and CommonsenseQA.

## Method Summary
The paper introduces three key methods: NLL probing to identify vulnerable latent directions, Activation Steering Attack (ASA) to bypass safety alignment through normalized perturbation injection at specific transformer layers, and Layer-wise Adversarial Patch Training (LAPT) as a defense mechanism that fine-tunes models on perturbed activations at identified fragile layers. ASArandom samples from Gaussian distribution, normalizes to match activation statistics, and adds to hidden states, while ASAgrad uses gradient-based optimization. LAPT combines adversarial training with model interpolation to preserve general capabilities while enhancing safety robustness.

## Key Results
- ASA successfully bypasses safety alignment in 8 different models with varying success rates across layers
- ASABench benchmark constructed with 4,862 validated attack instances enables standardized evaluation
- LAPT significantly reduces attack success rates while maintaining general capability accuracy within 0.05 of baseline on GSM8K and CommonsenseQA
- Safety robustness requires representation-level training rather than surface-level behavior supervision

## Why This Works (Mechanism)

### Mechanism 1: NLL Probing as Latent Vulnerability Detector
- Claim: Negative Log-Likelihood on original safe responses serves as a diagnostic signal for local robustness in latent space.
- Mechanism: By measuring how internal perturbations change output likelihood, NLL reveals directions where small activation shifts can degrade safety behavior. Higher NLL indicates the model's original safe response is becoming less probable under perturbation.
- Core assumption: Safety-aligned responses have an implicit binary structure (refusal vs. compliance) that makes loss-based probing meaningful, analogous to classification tasks.
- Evidence anchors:
  - [abstract] "we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions."
  - [section 2.1] "By increasing the loss (and thus the NLL) on the model's safe response, we can identify the latent directions where minor perturbations can degrade safety."
  - [corpus] Related work "Polarity-Aware Probing for Quantifying Latent Alignment" explores similar unsupervised probing for latent beliefs, suggesting probing methods can reveal hidden model states, though sensitivity varies by probe design.
- Break condition: If model outputs lack consistent structure (e.g., refusal responses vary arbitrarily without shared representation), NLL probing loses signal.

### Mechanism 2: Activation Steering Attack via Normalized Perturbation Injection
- Claim: Normalized perturbations injected at specific transformer layers can bypass safety alignment without requiring gradient optimization.
- Mechanism: ASArandom samples from Gaussian distribution, normalizes to match activation statistics (mean/std), then adds to hidden states. Effects accumulate across generation steps due to autoregressive propagation—KL divergence increases steadily with token position.
- Core assumption: Safety alignment modifies output distributions without fundamentally restructuring internal representations, leaving "shallow" constraints vulnerable to activation-space perturbations.
- Evidence anchors:
  - [abstract] "minor latent shifts can still trigger unsafe responses in aligned models... this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations."
  - [section 2.2, Fig 4] "KL divergence increases steadily with token position across all injection layers, indicating that the perturbation effects accumulate throughout the generation process."
  - [corpus] "Probing Latent Subspaces in LLM for AI Security" similarly finds adversarial states in latent space can override safety, consistent with representation-level vulnerabilities.
- Break condition: If normalization is omitted, perplexity can increase by orders of magnitude (Tab. 9 shows Llama-3.1-8B-Instruct: 701 → 623,488), causing degenerate outputs rather than targeted jailbreaks.

### Mechanism 3: Layer-wise Adversarial Patch Training for Targeted Robustification
- Claim: Fine-tuning on perturbed activations at identified fragile layers improves safety robustness while preserving general capabilities.
- Mechanism: LAPT injects controlled perturbations into hidden representations during training at layers identified via ASABench vulnerability analysis. Model interpolation with original weights (λ ∈ [0.1, 0.5]) balances robustness against capability degradation.
- Core assumption: Vulnerabilities cluster at specific "fragile layers" rather than distributing uniformly, enabling targeted intervention.
- Evidence anchors:
  - [abstract] "LAPT strengthen alignment robustness without compromising general capabilities"
  - [section 3.4, Tab 3] "LAPT achieves consistent reductions in attack success rates across pre-, peak, and post-PASR layers... general task performance remains stable, with accuracy deviations within 0.05"
  - [corpus] "Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment" similarly finds noise-robust training improves alignment stability, supporting representation-level training strategies.
- Break condition: If interpolation weight λ is too high, general capabilities degrade beyond acceptable threshold (>0.05 accuracy drop on CommonsenseQA).

## Foundational Learning

- **Concept: Autoregressive token generation and activation propagation**
  - Why needed here: ASA exploits how perturbations at token position t influence all subsequent tokens through the causal attention mechanism. Understanding this cascade is essential for predicting attack spread.
  - Quick check question: If you inject a perturbation at layer 15 for token position 5, which token positions in the output sequence will be affected?

- **Concept: NLL as model confidence vs. robustness signal**
  - Why needed here: The paper repurposes NLL from a training objective into a diagnostic tool. Distinguishing these roles prevents confusion about what NLL measures in this context.
  - Quick check question: A model has NLL = 2.1 on its original safe response under clean conditions, and NLL = 4.7 after ASA injection. What does this increase indicate about the perturbation's effect?

- **Concept: Layer-wise vulnerability heterogeneity in transformers**
  - Why needed here: Not all layers are equally vulnerable—PASR identifies "peak layers" with highest attack success. LAPT exploits this concentration for targeted defense.
  - Quick check question: Why might middle-to-late transformer layers show higher LASR than early layers for safety-related behaviors?

## Architecture Onboarding

- **Component map:**
  User Prompt → Embedding → [Transformer Layers 1...L] → LM Head → Output
                              ↑
                         ASA injection point (at layer l*, token position t)
                              |
                         Normalized perturbation δ' = μ(h) + (δ - μ(δ))/σ(δ) × σ(h)

  - ASABench: 4,862 validated attack instances with layer-wise attribution, split 60/40 train/test
  - LAPT training loop: Sample from ASABench → inject perturbation at fragile layer → compute CE loss on original response → update → interpolate with base model

- **Critical path:**
  1. Run ASA sweep across all layers on target model (100 samples × L layers)
  2. Compute MASR/PASR/LASR metrics to identify fragile layers
  3. Filter successful attacks into ASABench format with QwQ judge validation
  4. Apply LAPT at identified fragile layers with interpolation weight tuning
  5. Evaluate on ASABench test split + capability benchmarks (GSM8K, CommonsenseQA)

- **Design tradeoffs:**
  - ASArandom vs. ASAgrad: Random requires no gradient computation but lower PASR; gradient-based achieves +0.18 to +0.41 PASR improvement (Tab. 1) but needs target suffix specification
  - Generation length vs. attack effectiveness: MASR/PASR increase with length (Fig. 3), but longer generations increase compute cost
  - Interpolation weight λ: Higher λ improves robustness but risks capability loss; Tab. 11 shows optimal λ varies from 0.1 to 0.5 across models

- **Failure signatures:**
  - Degenerate outputs (repeated tokens, incoherent text): Normalization omitted or perturbation magnitude too high
  - No improvement from LAPT: Fragile layer misidentification or interpolation weight too low
  - Inconsistent results across seeds: PASR variance (±0.06 to ±0.12 in Tab. 6) is expected; use MASR for stable comparison

- **First 3 experiments:**
  1. Reproduce ASArandom on Qwen-2.5-7B-Instruct: Run 100 AdvBench samples across all 28 layers with seed=42, compute LASR heatmap to identify peak layer
  2. Ablate normalization: Compare perplexity and output quality with/without Eq. 1 normalization on 10 samples at identified peak layer
  3. Validate LAPT transfer: Train LAPT on ASABench train split for Llama-3.2-3B-Instruct, evaluate PASR reduction on test split, verify CommonsenseQA accuracy within 0.05 of baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LAPT robustness generalize to latent attack methods beyond the ASA family used in its training data?
- Basis in paper: [inferred] LAPT is trained on ASABench data derived from ASA/ASAgrad attacks and evaluated primarily against these same attack variants. The paper states LAPT "significantly enhances alignment robustness under latent perturbations" but does not test against conceptually different latent attacks (e.g., contrastive activation steering, representation engineering attacks).
- Why unresolved: Adversarial training often overfits to specific attack patterns. The paper does not include cross-method robustness evaluation against other activation-level manipulation techniques.
- What evidence would resolve it: Evaluate LAPT-trained models against diverse latent attacks (contrastive steering vectors, targeted representation editing) and report comparative robustness metrics.

### Open Question 2
- Question: What mechanistic factors determine which transformer layers emerge as "fragile layers" with heightened vulnerability?
- Basis in paper: [explicit] The paper observes "successful attacks concentrate around specific 'fragile layers'" and uses this for targeted defense, but Section 2.2 does not explain why certain layers (e.g., layer 9 for Llama-3.2-3B-Instruct vs. layer 27 for Qwen-2.5-7B-Base) exhibit higher PASR.
- Why unresolved: The layer-wise vulnerability patterns vary across models without clear architectural or functional explanations. The paper identifies the phenomenon but not its cause.
- What evidence would resolve it: Correlate fragile layer positions with architectural features (attention head density, residual stream norms) or functional properties (safety-relevant circuit localization via probing).

### Open Question 3
- Question: Can representation-level alignment methods achieve robustness without requiring model interpolation to preserve general capabilities?
- Basis in paper: [inferred] LAPT requires post-hoc model interpolation with weights tuned to keep CommonsenseQA accuracy within 0.05 of baseline. This suggests a tradeoff between safety robustness and general performance that may limit practical deployment.
- Why unresolved: The interpolation step indicates LAPT alone degrades capabilities. Whether this tradeoff is fundamental to representational interventions or can be resolved through different training objectives is unclear.
- What evidence would resolve it: Compare LAPT variants with different loss formulations or regularization strategies that directly optimize joint safety-capability preservation during training.

## Limitations
- Data dependency on AdvBench subset - study uses only first 100 samples from AdvBench, potentially limiting generalizability
- Mechanism specificity to transformer architectures - all experiments focus on decoder-only transformers
- Confounding effects of model interpolation - LAPT combines adversarial training with interpolation, making it difficult to isolate sources of robustness gains

## Confidence
- High confidence - The fundamental vulnerability that safety alignment operates primarily at the output level while internal representations remain manipulable
- Medium confidence - The NLL probing method reliably identifies vulnerable directions, though binary structure assumption requires further validation
- Low confidence - The general capability preservation claim (accuracy drops <0.05), as this threshold is tight and variance across runs is not reported

## Next Checks
1. **Cross-model robustness transfer** - Train LAPT on ASABench derived from one model family (e.g., Llama) and evaluate attack success rates on a different family (e.g., Qwen or Mistral) to reveal whether learned robustness generalizes beyond the training distribution

2. **Adversarial training ablation study** - Implement pure adversarial training (no interpolation) with varying perturbation magnitudes and compare against LAPT, measuring both attack success rate reduction and capability degradation to determine if interpolation is necessary

3. **Temporal stability analysis** - Generate ASABench test attacks using different random seeds and at different time points during LAPT training, tracking how attack success rates evolve to determine whether learned robustness degrades over time or remains stable across model checkpoints