---
ver: rpa2
title: Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning
arxiv_id: '2501.19133'
source_url: https://arxiv.org/abs/2501.19133
tags:
- decorrelation
- learning
- dsac
- network
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Decorrelated Soft Actor-Critic (DSAC), a method
  that integrates online input decorrelation into deep reinforcement learning using
  the decorrelated backpropagation (DBP) algorithm. Decorrelation matrices are added
  to each layer of the network and updated in parallel with standard RL training to
  minimize total decorrelation loss across all layers.
---

# Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.19133
- Source URL: https://arxiv.org/abs/2501.19133
- Authors: Burcu Küçükoğlu; Sander Dalm; Marcel van Gerven
- Reference count: 13
- Primary result: DSAC achieves faster training (up to 76% wall-clock reduction) and improved rewards in select Atari 2600 games by minimizing decorrelation loss across all network layers

## Executive Summary
Decorrelated Soft Actor-Critic (DSAC) integrates online input decorrelation into deep reinforcement learning by adding decorrelation matrices to each network layer and updating them in parallel with standard SAC training. The method minimizes total decorrelation loss across all layers, enabling more effective credit assignment and representation learning. When evaluated on the Atari 100k benchmark using discrete SAC, DSAC demonstrated faster training in 5 out of 7 games with up to 76% reduction in wall-clock time, and improved reward performance in 2 games (Alien: 86% gain, Seaquest: 6% gain), while maintaining performance in the others.

## Method Summary
DSAC extends the Soft Actor-Critic algorithm by incorporating the decorrelated backpropagation (DBP) technique. The key innovation is adding decorrelation matrices to each layer of the actor and critic networks, which are updated simultaneously with standard RL parameters. During training, DSAC minimizes a total decorrelation loss that spans all layers, computed as the sum of decorrelation losses at each layer. This approach enables the network to learn decorrelated representations that improve sample efficiency and credit assignment. The method was specifically evaluated using discrete SAC on the Atari 100k benchmark, a challenging setting with limited training samples.

## Key Results
- DSAC achieved 76% reduction in wall-clock training time across 5 out of 7 Atari 100k games
- Reward improvements of 86% in Alien and 6% in Seaquest, with maintained performance in other games
- Decorrelation loss successfully minimized throughout training in DSAC, while increasing steadily in baseline SAC
- Demonstrated that network-wide decorrelation enhances deep RL training efficiency through improved representation learning

## Why This Works (Mechanism)
The paper proposes that decorrelation improves credit assignment and representation learning by preventing redundant feature learning across network layers. By maintaining decorrelated representations, the network can more effectively propagate reward signals through the architecture and learn more informative features from limited samples. The decorrelation matrices act as regularizers that encourage diverse feature representations, which is particularly valuable in sample-limited scenarios like Atari 100k.

## Foundational Learning

**Decorrelated Backpropagation (DBP)**: A technique that introduces decorrelation matrices to neural network layers to prevent feature redundancy. Why needed: Standard backpropagation can lead to correlated features that reduce learning efficiency. Quick check: Monitor decorrelation loss during training to ensure it decreases as expected.

**Soft Actor-Critic (SAC)**: An off-policy actor-critic algorithm that maximizes expected return while also maximizing entropy. Why needed: Provides the base RL framework that DSAC builds upon. Quick check: Verify that DSAC maintains SAC's core stability properties.

**Atari 100k Benchmark**: A challenging RL benchmark using only 100,000 environment steps (equivalent to 2 hours of gameplay). Why needed: Tests sample efficiency, which is where DSAC aims to improve. Quick check: Ensure proper frame stacking and preprocessing as per standard Atari preprocessing.

## Architecture Onboarding

**Component map**: Input -> Convolutional layers (with decorrelation matrices) -> Fully connected layers (with decorrelation matrices) -> Actor/Critic heads -> Environment interaction -> Decorrelation loss computation -> Backpropagation

**Critical path**: State observation → Convolutional feature extraction (decorrelated) → Value/policy computation → Action selection → Reward collection → Parameter updates (including decorrelation matrices)

**Design tradeoffs**: DSAC trades additional computational overhead and memory usage for improved sample efficiency. The method requires maintaining and updating decorrelation matrices for each layer, which increases model complexity but potentially enables faster learning from limited data.

**Failure signatures**: If decorrelation matrices are not properly initialized or updated, the method may fail to improve upon baseline SAC. Additionally, if the decorrelation loss is not properly balanced with the RL objective, it could hinder rather than help learning.

**3 first experiments**:
1. Verify that decorrelation loss decreases during training while remaining stable or increasing in baseline SAC
2. Compare feature representations between DSAC and SAC to confirm decorrelated features are learned
3. Test DSAC on a simple continuous control task to verify basic functionality before Atari evaluation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but significant uncertainties remain regarding the generalizability of DSAC beyond the Atari 100k benchmark. The results show improvements in only 5 out of 7 games for training speed and 2 out of 7 for reward performance, raising questions about the method's consistency across different game types.

## Limitations
- Results are limited to the Atari 100k benchmark and may not generalize to other RL domains such as continuous control or robotics
- Performance improvements were observed in only 5 out of 7 games for training speed and 2 out of 7 for reward performance
- Computational overhead of maintaining decorrelation matrices across all layers is not quantified in terms of memory usage or per-step training time

## Confidence
- **High confidence**: The empirical observation that decorrelation loss increases in baseline SAC while being successfully minimized in DSAC during training
- **Medium confidence**: The claim that decorrelation improves credit assignment and representation learning, as the causal mechanism is not explicitly demonstrated
- **Medium confidence**: The specific performance improvements (76% wall-clock time reduction, 86% reward gain in Alien), as these are limited to 7 games and may not generalize

## Next Checks
1. Evaluate DSAC on continuous control benchmarks (e.g., OpenAI Gym MuJoCo tasks) to test generalizability beyond discrete action spaces
2. Conduct ablation studies removing decorrelation matrices from different layers to identify which layers contribute most to performance gains
3. Measure and report the additional computational overhead (memory and per-step training time) introduced by maintaining decorrelation matrices across all layers