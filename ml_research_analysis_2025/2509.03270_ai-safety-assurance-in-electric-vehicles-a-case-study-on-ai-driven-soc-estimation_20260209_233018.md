---
ver: rpa2
title: 'AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation'
arxiv_id: '2509.03270'
source_url: https://arxiv.org/abs/2509.03270
tags:
- safety
- system
- systems
- data
- faults
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating artificial intelligence
  (AI) into safety assurance for electric vehicles (EVs) under ISO 26262 standards.
  The authors propose combining ISO 26262 with the emerging ISO/PAS 8800 standard
  to evaluate AI-driven SOC estimation systems.
---

# AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation

## Quick Facts
- **arXiv ID**: 2509.03270
- **Source URL**: https://arxiv.org/abs/2509.03270
- **Reference count**: 23
- **Primary result**: Combines ISO 26262 with ISO/PAS 8800 to evaluate AI-driven SOC estimation using fault injection experiments that show voltage sensors have the largest impact on prediction errors.

## Executive Summary
This paper addresses the challenge of integrating artificial intelligence (AI) into safety assurance for electric vehicles (EVs) under ISO 26262 standards. The authors propose combining ISO 26262 with the emerging ISO/PAS 8800 standard to evaluate AI-driven SOC estimation systems. They identify safety cages as key architectural elements for demarcation between traditional safety measures and AI-specific requirements. The research employs fault injection experiments using stuck-at faults on sensor inputs (voltage, current, temperature) to assess the AI model's resilience to input variance.

## Method Summary
The study uses pre-runtime Software-Implemented Fault Injection (SWIFI) with stuck-at-0 and stuck-at-1 models on Float64 representations of voltage, current, and temperature sensor inputs. The method tests a pre-trained RNN with LSTM cells from prior work using an LG 18650HG2 Li-ion Battery dataset. Faults are injected at the start of discharge cycles, and the resulting SOC prediction deviations are measured through RMSE and absolute deviation metrics across the entire cycle.

## Key Results
- Stuck-at faults in exponent bits cause RMSE spikes greater than 0.8 in SOC predictions
- Voltage sensors have the largest impact on prediction errors compared to temperature and current
- Errors increase notably when exponent bits are affected, with voltage bit 11/12 showing specific failure modes at low charge states (<44% SOC)

## Why This Works (Mechanism)

### Mechanism 1: Demarcation via Safety Cages
The architecture partitions the system into a complex "actuator" (the AI SOC estimator) and a simpler "monitor" (conventional SW/HW). The monitor handles safety requirements (e.g., inhibiting output to prevent thermal runaway), reducing the verification burden on the opaque AI model.

### Mechanism 2: Fault Injection as Robustness Evidence
By forcing specific bits in the Float64 representation of sensor inputs to 0 or 1, the method emulates sensor hardware faults. The resulting deviation in SOC prediction (RMSE) quantifies the model's lack of robustness.

### Mechanism 3: Input Variable Sensitivity (Voltage Dominance)
The LSTM relies on the correlation between inputs. Corruption in Voltage (which varies significantly during discharge) disrupts the primary SOC correlation, whereas Temperature (relatively stable) shows lower sensitivity to bit corruption.

## Foundational Learning

- **ISO/PAS 8800 Lifecycle**: Extends the V-model to handle AI-specific issues like data quality and continuous assurance, which ISO 26262 does not cover. *Quick check: Can you identify which part of the AI lifecycle in Figure 3 handles "continuous assurance" during operation?*
- **Float64 Representation**: Understanding the IEEE 754 format (Sign, Exponent, Significand) is required to interpret why bits 3-12 (Exponent) cause massive value shifts. *Quick check: Why does a stuck-at-0 fault in an exponent bit cause a larger error than in a significand bit?*
- **Software-Implemented Fault Injection (SWIFI)**: The primary method used to verify the AI component's robustness without physical hardware intrusion. *Quick check: What is the difference between pre-runtime and runtime fault injection, and which was used in this study?*

## Architecture Onboarding

- **Component map**: Input Sensors -> Normalization -> LSTM Weights -> SOC Prediction. Monitor (Non-AI) reading Voltage/Temp -> Inhibitor Logic.
- **Critical path**: Input Sensors -> Normalization -> LSTM Weights -> SOC Prediction. (Note: The Monitor runs in parallel but only engages on deviation).
- **Design tradeoffs**: Performance vs. Safety: The AI offers better estimation capabilities but is opaque; the Monitor ensures safety but may be conservative. Bit-depth vs. Sensitivity: Using Float64 offers precision but exposes 64 bits per input to potential stuck-at faults.
- **Failure signatures**: Exponent Stuck-at-0: Sudden, massive SOC prediction error (RMSE > 0.8). Voltage Bit 11/12 Error: Specific failure mode where error spikes at low charge states (<44% SOC). Monitor Inhibit: System enters safe mode; valid signals blocked due to detected anomalies.
- **First 3 experiments**: 
  1. Bit Sensitivity Scan: Inject stuck-at-0 and stuck-at-1 faults into bits 3-12 of the Voltage input to replicate the RMSE spikes seen in Figure 7.
  2. Discharge Phase Testing: Inject faults at various SOC levels (e.g., 80% vs 40%) to verify the increased sensitivity at lower charge states shown in Figure 9.
  3. Monitor Threshold Tuning: Inject the "Voltage Bit 11" fault and adjust the Monitor's threshold to determine if it successfully inhibits the erroneous output.

## Open Questions the Paper Calls Out

1. How effective is the proposed "safety cage" monitor in detecting and mitigating the specific failure modes identified in the AI-based SOC estimator? [explicit] The authors state in the "Conclusions and future work" section: "Finally, it will evaluate the effectiveness of the safety monitor itself."

2. How do additional failure modes, such as oscillatory behavior and drift, impact the AI SOC estimation model compared to the tested stuck-at faults? [explicit] The paper lists future work to "analyse the effects of additional failure modes from AIC1, AIC2, AIC3 on AI SOC prediction."

3. Can training the AI model with fault-injected data improve the robustness of SOC estimation against hardware faults? [explicit] The authors propose to "investigate the quality and relevance of using training data with faults already applied in order to improve the robustness."

## Limitations

- Fault injection methodology relies on permanent stuck-at faults, which may not fully represent transient sensor failures common in EV environments
- Analysis focuses on a single battery dataset and pre-trained model from prior work, limiting generalizability
- Safety cage concept assumes the monitor can observe all relevant failure modes of the AI component, but this remains theoretically unproven

## Confidence

- **High Confidence**: Empirical results showing voltage input sensitivity and exponent bit impact on SOC predictions (RMSE > 0.8)
- **Medium Confidence**: Architectural separation via safety cages as a general strategy for AI safety integration
- **Low Confidence**: Claim that temperature's lower sensitivity is due to operational stability assumes test conditions match real-world variability

## Next Checks

1. **Transient Fault Testing**: Implement bit-flip fault injection patterns to assess model robustness against transient rather than permanent faults, comparing results to stuck-at fault outcomes.
2. **Monitor Coverage Analysis**: Systematically enumerate potential SOC estimation failure modes (e.g., drift, bias, oscillation) and verify whether the current monitor architecture can detect each case.
3. **Cross-Dataset Validation**: Apply the fault injection methodology to SOC estimation models trained on different battery chemistries and discharge profiles to assess generalizability of the sensitivity hierarchy findings.