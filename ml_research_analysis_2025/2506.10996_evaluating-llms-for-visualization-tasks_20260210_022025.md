---
ver: rpa2
title: Evaluating LLMs for Visualization Tasks
arxiv_id: '2506.10996'
source_url: https://arxiv.org/abs/2506.10996
tags:
- llms
- chart
- visualization
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the capabilities of large language models
  (LLMs) for visualization generation and understanding. The authors analyze four
  popular LLMs (GPT-3.5, GPT-4o, Gemini-1.5-pro, and Claude 3 Opus) using two approaches:
  generating visualization code from prompts and answering questions about charts.'
---

# Evaluating LLMs for Visualization Tasks

## Quick Facts
- **arXiv ID**: 2506.10996
- **Source URL**: https://arxiv.org/abs/2506.10996
- **Authors**: Saadiq Rauf Khan; Vinit Chandak; Sougata Mukherjea
- **Reference count**: 3
- **Primary result**: GPT-4o achieved 95% accuracy in Python visualization code generation but only 70% in Vega-Lite, with notable struggles on line charts and complex visual reasoning tasks.

## Executive Summary
This paper evaluates the capabilities of large language models for visualization generation and understanding. The authors analyze four popular LLMs (GPT-3.5, GPT-4o, Gemini-1.5-pro, and Claude 3 Opus) using two approaches: generating visualization code from prompts and answering questions about charts. For visualization generation, GPT-4o achieved 95% accuracy in Python code generation for 24 chart types, while performance dropped significantly for Vega-Lite scripts (70% accuracy). For visualization understanding, GPT-4o achieved 66% accuracy on binary questions from the FigureQA dataset, outperforming Gemini (64%) and Claude (55%). Manual analysis revealed significant performance variations across chart types, with all models struggling particularly with line charts and comparisons between close bar lengths. The study highlights that while LLMs show promise for basic visualization tasks, they have notable limitations with complex visualizations and visual reasoning tasks, suggesting opportunities for improvement in both LLMs and visualization systems.

## Method Summary
The study evaluates LLMs using zero-shot prompting for two tasks: visualization generation and understanding. For generation, 24 chart types are tested with Python and Vega-Lite prompts using custom datasets. Each prompt is repeated three times with identical outputs required for acceptance. For understanding, the FigureQA dataset is used with binary questions, plus manual analysis with non-binary questions on 20 charts per type. Four LLMs are tested: GPT-3.5, GPT-4o, Gemini-1.5-pro, and Claude 3 Opus, with system prompts tested for understanding tasks.

## Key Results
- GPT-4o achieved 95% accuracy generating Python visualization code but only 70% for Vega-Lite scripts
- For visualization understanding, GPT-4o scored 66% on binary questions, outperforming Gemini (64%) and Claude (55%)
- All models struggled significantly with line charts (20% accuracy for GPT-4o) and comparing close bar lengths
- System prompts improved visualization understanding accuracy, especially for Gemini

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate Python visualization code more reliably than Vega-Lite scripts when prompted in natural language.
- Mechanism: Python visualization libraries have larger representation in public code repositories and training corpora, enabling stronger pattern completion for common chart types. The model leverages memorized idioms and frequent co-occurrence patterns between prompt keywords and library APIs.
- Core assumption: The distribution of training data correlates with task frequency in online code repositories; Python visualization tutorials and examples significantly outnumber Vega-Lite resources.
- Evidence anchors:
  - [abstract] "GPT-4o achieved 95% accuracy in Python code generation for 24 chart types, while performance dropped significantly for Vega-Lite scripts (70% accuracy)"
  - [section 3.3, Table 2] "Gemini was only able to create roughly 40% of the total charts. The performance of GPT-4o also reduced significantly when switching from Python to Vega-lite."
  - [corpus] Neighbor paper "PlotGen-Bench" evaluates VLMs across multiple libraries, suggesting library-specific performance variation is a recognized pattern.

### Mechanism 2
- Claim: Visualization understanding performance varies substantially by chart type, with pie charts interpreted more accurately than line charts.
- Mechanism: Pie charts present salient color-region associations with clear area-based magnitude cues, whereas line charts—particularly those with dotted/non-dotted line mixes—introduce visual patterns that may be misclassified as noise or artifacts by vision encoders.
- Core assumption: The visual encoder component of multimodal LLMs treats certain line styles (e.g., dotted patterns) as low-signal or noisy regions, leading to missed features.
- Evidence anchors:
  - [section 4.5] "GPT-4o had 85% accuracy on pie charts and a mere 20% accuracy on line charts... All of the models performed very poorly on the line charts. This might be because of the presence of dotted lines, which might be treated as some kind of noise by the models."
  - [section 4.5, Figure 3] "All three models could not determine the color of the longest bar since all of the models struggle in determining the larger/smaller bars" when bar lengths are close.
  - [corpus] Limited direct corpus support; "AstroVisBench" touches on visualization but focuses on scientific computing tasks.

### Mechanism 3
- Claim: System prompts that instruct careful analysis improve visualization understanding accuracy, with larger gains for models with weaker baseline performance.
- Mechanism: A system prompt constrains the model's attention and response space, reducing off-task generation and encouraging more deliberate visual inspection. For models with less robust internal calibration, explicit instructions reduce variance and bias in the response distribution.
- Core assumption: The model's default response behavior under zero-shot conditions is under-constrained for visual reasoning tasks; prompts provide task-relevant context that narrows the hypothesis space.
- Evidence anchors:
  - [section 4.5, Table 4] "In all the cases, the use of system prompts improved the performance of models... Gemini-1.5-Pro improved significantly with the use of system prompts."
  - [section 4.5] "This shows the importance of context and guidance in improving the performance of LLMs."
  - [corpus] Weak direct evidence; neighbor papers do not report systematic prompt-ablation studies for visualization tasks.

## Foundational Learning

- Concept: **Zero-shot prompting**
  - Why needed here: The entire experimental design relies on prompting LLMs without task-specific examples to measure baseline capabilities.
  - Quick check question: Can you explain why zero-shot evaluation differs from few-shot evaluation for measuring inherent model capability?

- Concept: **Vision-language model (VLM) architecture**
  - Why needed here: Visualization understanding requires a model that can ingest images (charts) and reason over them; understanding that this involves a visual encoder + language decoder clarifies why certain visual features may fail.
  - Quick check question: What component in a VLM is responsible for converting image pixels into representations the language model can process?

- Concept: **Binary classification baseline and random guessing**
  - Why needed here: The FigureQA binary-question evaluation admits ~50% random-guess accuracy; understanding this informs why manual analysis with non-binary questions was added.
  - Quick check question: If a binary QA dataset has balanced yes/no labels, what is the expected accuracy of random guessing?

## Architecture Onboarding

- Component map:
  - Visualization generation pipeline: Natural language prompt -> LLM -> Python code OR Vega-Lite script -> External executor -> Rendered chart -> Manual correctness check
  - Visualization understanding pipeline: Chart image -> Multimodal LLM -> Textual answer -> Automated comparison (binary) or manual evaluation (non-binary)

- Critical path:
  1. Define chart type and data schema (columns, types)
  2. Construct zero-shot prompt with clear specification
  3. Execute generated code in a sandboxed environment; verify output visually and against specification
  4. For understanding: upload chart image, pose binary or non-binary questions, record model response

- Design tradeoffs:
  - **Python vs. Vega-Lite**: Python offers higher success rates (95% vs. 70%) but requires execution environment; Vega-Lite is declarative and portable but less robust
  - **Binary vs. non-binary questions**: Binary enables automated scaling but inflates accuracy via random guessing (50% baseline); non-binary requires manual grading but reveals deeper reasoning failures
  - **System prompt vs. no system prompt**: System prompts improve accuracy (e.g., Gemini improved from ~85% to ~89% on non-binary questions) but introduce a confound for capability measurement

- Failure signatures:
  - Chart type not recognized: Model generates incorrect chart type
  - Visual feature loss: Dotted lines not detected; close bar lengths miscompared
  - Hallucinated data references: Code references non-existent CSV columns or incorrect file names

- First 3 experiments:
  1. Replicate Python generation for a subset of 5 common chart types (bar, scatter, line, pie, area) with zero-shot prompts; compare success rate to paper's reported ~95% for GPT-4o
  2. Test Vega-Lite generation for the same 5 chart types; expect degradation and analyze error types (syntax, missing features, incorrect spec)
  3. Run binary QA on 20 FigureQA images (mix of chart types) with and without system prompt; measure accuracy delta and note chart-type-specific failures (especially line charts)

## Open Questions the Paper Calls Out

- Can advanced prompting techniques like Chain-of-Thought (CoT) significantly improve LLM accuracy in visualization generation and understanding tasks?
- How do LLMs perform on visualization tasks for non-tabular data structures, specifically network graphs and hierarchical trees?
- Can the integration of LLMs with visualization tools effectively generate interactive visualizations?

## Limitations

- Dataset completeness: Only 100 FigureQA images and 20 per chart type in manual analysis; results may not generalize to the full distribution of visualization complexity and domain
- Reproducibility constraints: Lack of full prompt templates, dataset definitions, and LLM parameter settings prevents exact replication
- Unmeasured confounds: No ablation of chart data complexity (e.g., number of categories, value ranges) or visual encoding variations that could affect model performance

## Confidence

- **High confidence**: Python generation accuracy (95%) and Vega-Lite degradation (70%) for GPT-4o—clear, consistent results across repeated trials
- **Medium confidence**: Visualization understanding accuracy for binary questions—binary nature inflates performance; manual analysis provides more nuanced insight but is less scalable
- **Low confidence**: Generalizability of chart-type performance differences (e.g., line charts at 20%)—limited sample size per type and lack of cross-dataset validation

## Next Checks

1. Test zero-shot vs. few-shot prompts for each chart type; measure whether inclusion of one successful example increases Python generation accuracy above 95%
2. Isolate whether Vega-Lite failures are due to missing library support, JSON syntax errors, or incorrect encoding mappings; provide gold-standard specs for comparison
3. Evaluate visualization understanding on a second dataset (e.g., DVQA or PlotQA) to confirm whether line-chart failures and pie-chart success are consistent across domains