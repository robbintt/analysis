---
ver: rpa2
title: Automating SPARQL Query Translations between DBpedia and Wikidata
arxiv_id: '2507.10045'
source_url: https://arxiv.org/abs/2507.10045
tags:
- wikidata
- dbpedia
- query
- sparql
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether Large Language Models (LLMs) can\
  \ automatically translate SPARQL queries between Knowledge Graphs (KGs), focusing\
  \ on DBpedia \u2194 Wikidata and DBLP \u2192 OpenAlex. Two benchmarks of 100 queries\
  \ each were constructed, and three open LLMs (Llama-3-8B, DeepSeek-R70B, Mistral-Large-2407)\
  \ were tested with zero-shot, few-shot, and chain-of-thought prompting, with explicit\
  \ entity-relation mapping provided."
---

# Automating SPARQL Query Translations between DBpedia and Wikidata

## Quick Facts
- arXiv ID: 2507.10045
- Source URL: https://arxiv.org/abs/2507.10045
- Reference count: 40
- Primary result: Large Language Models can automatically translate SPARQL queries between Knowledge Graphs with up to 86% accuracy when provided with schema mappings and few-shot prompting

## Executive Summary
This study investigates whether Large Language Models (LLMs) can automatically translate SPARQL queries between Knowledge Graphs (KGs), focusing on DBpedia ↔ Wikidata and DBLP → OpenAlex. Two benchmarks of 100 queries each were constructed, and three open LLMs (Llama-3-8B, DeepSeek-R70B, Mistral-Large-2407) were tested with zero-shot, few-shot, and chain-of-thought prompting, with explicit entity-relation mapping provided. Translation accuracy varied by direction and method, with Wikidata → DBpedia yielding better results (up to 86% accuracy with Mistral-Large-Instruct-2407 using few-shot prompting). The largest model, Mistral-Large-Instruct-2407, performed best overall. Few-shot prompting with schema mapping proved critical, especially for specialized KGs. The approach generalizes to non-encyclopedic KGs, suggesting LLMs with structured prompting and schema mappings can significantly advance KG interoperability.

## Method Summary
The researchers constructed two benchmark datasets of 100 SPARQL queries each for DBpedia ↔ Wikidata translation, and another benchmark for DBLP → OpenAlex. Three open-source LLMs (Llama-3-8B, DeepSeek-R70B, Mistral-Large-2407) were evaluated using zero-shot, few-shot, and chain-of-thought prompting strategies. All experiments provided explicit entity-relation mappings between the KGs. Translation accuracy was measured by comparing generated queries against ground truth, with results aggregated across multiple runs. The study tested both encyclopedic (DBpedia/Wikidata) and specialized (DBLP/OpenAlex) KGs to assess generalizability.

## Key Results
- Wikidata → DBpedia translation achieved up to 86% accuracy with Mistral-Large-Instruct-2407 using few-shot prompting
- Few-shot prompting with schema mapping was critical for high accuracy, especially for specialized KGs
- Mistral-Large-Instruct-2407 outperformed other models overall, though differences with DeepSeek-R70B were not always statistically significant
- Structural errors were the most common issue, often co-occurring with semantic errors

## Why This Works (Mechanism)
The approach works because LLMs can leverage provided schema mappings to understand structural differences between knowledge graphs while using prompting strategies to improve reasoning about query semantics. Few-shot examples help the model learn translation patterns, and explicit entity-relation mappings reduce ambiguity in the translation process.

## Foundational Learning
- SPARQL query structure: why needed to understand query translation targets; quick check: identify SELECT, WHERE, and FILTER clauses in sample queries
- Knowledge Graph schema differences: why needed to understand mapping requirements; quick check: compare DBpedia and Wikidata class hierarchies
- Chain-of-thought prompting: why needed to understand reasoning enhancement; quick check: implement basic CoT on simple query transformation
- Entity-relation mapping: why needed to bridge schema differences; quick check: create mapping table between equivalent predicates in different KGs
- Few-shot learning: why needed to provide translation patterns; quick check: design 3-5 example query pairs for model training

## Architecture Onboarding

**Component map:**
Query input -> LLM with schema mapping -> Translated query output -> Accuracy evaluation

**Critical path:**
Query input → Schema mapping integration → Prompt construction → LLM inference → Query validation → Accuracy scoring

**Design tradeoffs:**
- Model size vs. computational cost (larger models perform better but require more resources)
- Schema mapping completeness vs. translation accuracy (more complete mappings improve results)
- Prompt strategy selection (few-shot vs. zero-shot impacts accuracy differently per direction)

**Failure signatures:**
- Structural errors: queries syntactically correct but use wrong predicates or patterns
- Semantic errors: queries semantically equivalent but return incorrect results due to KG structure differences
- Combined errors: both structural and semantic issues present

**First experiments:**
1. Test zero-shot translation on 10 simple queries to establish baseline performance
2. Add schema mapping to same queries to measure improvement impact
3. Implement few-shot prompting with 5 examples and re-test same queries

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Results based on only 100 queries per direction, potentially missing real-world variability
- Focus on two specific KG pairs limits generalizability to other combinations
- Combined structural and semantic error scoring makes it difficult to assess which error type is more problematic

## Confidence
- High: Explicit schema mapping improves translation accuracy across all tested LLMs
- Medium: Relative performance ranking of Mistral-Large-Instruct-2407 vs DeepSeek-R70B not statistically significant in all cases
- Medium: Generalization to non-encyclopedic KGs supported by DBLP→OpenAlex but remains somewhat speculative

## Next Checks
1. Test translation approach on larger and more diverse query corpus spanning multiple KG pairs to assess scalability and generalizability
2. Conduct ablation studies to isolate impact of schema mapping versus prompting strategies on translation accuracy
3. Evaluate translated queries on their target KGs to measure actual performance differences between semantically correct but structurally different queries