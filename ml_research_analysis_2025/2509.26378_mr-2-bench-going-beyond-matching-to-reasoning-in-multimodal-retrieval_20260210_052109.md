---
ver: rpa2
title: 'MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval'
arxiv_id: '2509.26378'
source_url: https://arxiv.org/abs/2509.26378
tags:
- multimodal
- retrieval
- visual
- reasoning
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MR2-Bench, a benchmark for evaluating multimodal
  retrieval systems on reasoning-intensive tasks. Unlike existing benchmarks that
  focus on surface-level semantic matching, MR2-Bench requires models to perform logical,
  spatial, and causal inference over complex multimodal data including natural images,
  diagrams, and visual puzzles.
---

# MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval

## Quick Facts
- arXiv ID: 2509.26378
- Source URL: https://arxiv.org/abs/2509.26378
- Reference count: 39
- Primary result: Seed1.6-Embedding achieves 77.78 Recall@1 on MMEB but only 9.91 Recall@1 on MR$^2$-Bench

## Executive Summary
This paper introduces MR$^2$-Bench, a benchmark for evaluating multimodal retrieval systems on reasoning-intensive tasks. Unlike existing benchmarks that focus on surface-level semantic matching, MR$^2$-Bench requires models to perform logical, spatial, and causal inference over complex multimodal data including natural images, diagrams, and visual puzzles. The benchmark contains 1,309 queries across 12 sub-tasks spanning three meta-tasks: multimodal knowledge retrieval, visual illustration search, and visual relation reasoning. Despite strong performance on existing benchmarks like MMEB (Recall@1 of 77.78), the leading Seed1.6-Embedding model achieves only 9.91 Recall@1 on MR$^2$-Bench, highlighting the increased difficulty. The authors demonstrate that reasoning-oriented text retrievers and multimodal retrievers perform better than matching-centric models, and that techniques like query rewriting and reranking can significantly improve performance. The benchmark aims to guide development of more capable multimodal retrievers for real-world applications requiring deep reasoning.

## Method Summary
MR$^2$-Bench evaluates multimodal retrieval through 1,309 curated queries across 12 sub-tasks with corpus sizes ranging from 944 to 7,572 per sub-task. The benchmark tests three meta-tasks: multimodal knowledge retrieval (retrieving relevant information from multimodal documents), visual illustration search (finding diagrams and illustrations), and visual relation reasoning (abstract visual logic puzzles). Evaluation uses nDCG@10 as the primary metric, with Recall@1, Recall@5, Recall@10, nDCG@5, and nDCG@20 also reported. The authors test 11 embedding models including 6 text-only and 5 multimodal retrievers, with two text model configurations: text-only and caption-augmented using Qwen2.5-VL-7B with a specified prompt for image captioning.

## Key Results
- Seed1.6-Embedding achieves 77.78 Recall@1 on MMEB but only 9.91 Recall@1 on MR$^2$-Bench
- Reasoning-oriented text retrievers significantly outperform traditional matching-based retrievers
- Reranking with GPT-5 boosts performance from 30.68 to 45.90 nDCG@10 (15.22 absolute gain)
- Visual relation reasoning tasks (Analogies, Puzzles) remain extremely challenging with near-zero performance on some tasks

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Bridged Semantic Alignment
- **Claim:** If queries and documents share no surface-level semantics, retrieval success appears conditional on generating an intermediate "reasoning bridge" that explicitly states latent constraints or causal links.
- **Mechanism:** The system decomposes complex queries into sub-questions and required evidence types, expanding semantic footprint to match on implications rather than tokens.
- **Core assumption:** The underlying embedding model has sufficient latent capacity to encode increased semantic density of rewritten queries.
- **Evidence anchors:** Table 4 shows query rewriting with GPT-5 improves Seed-1.6 performance from 30.68 to 33.87 nDCG@10; CrossCheck-Bench supports necessity of resolving "real-world inconsistencies."

### Mechanism 2: Generative Reranking with Visual Verification
- **Claim:** Offloading final ranking to a Large Multimodal Model acts as a "verification mechanism" that can rectify retrieval errors caused by visual compression in bi-encoders.
- **Mechanism:** Initial retriever casts wide net, reranker ingests interleaved image-text candidates, performs chain-of-thought reasoning to verify visual evidence satisfies query, then re-scores.
- **Core assumption:** The LMM's context window can accommodate resolution and complexity of retrieved image.
- **Evidence anchors:** Figure 2 demonstrates GPT-5 reranking boosts performance to 45.90 nDCG@10; UNIDOC-BENCH reinforces difficulty of "document-centric" retrieval where embeddings fail to capture visual nuance.

### Mechanism 3: Visual-Relational Pattern Abstraction
- **Claim:** Success on "Visual Relation Reasoning" tasks relies on the model's ability to abstract transformation rules from visual structures rather than matching object embeddings.
- **Mechanism:** Model processes image pair (A, A') to infer transformation function, then applies this function to candidate B to predict B'.
- **Core assumption:** Vision encoder has not discarded spatial relationship data during pre-training.
- **Evidence anchors:** Table 3 shows even advanced models like Seed-1.6 score near zero (0.93 nDCG@10) on Visual Puzzles; IDMR discusses "Instance-Driven Precise Visual Correspondence."

## Foundational Learning

- **Concept:** Bi-Encoder vs. Cross-Encoder Architectures
  - **Why needed here:** Paper distinguishes between fast but lower fidelity embedding models (Bi-Encoders like Seed-1.6) and reasoning-capable but slow rerankers (Cross-Encoders/LMMs).
  - **Quick check question:** Can you explain why Seed-1.6 achieves 30.68 nDCG initially but GPT-5 reranking jumps to 45.90?

- **Concept:** Interleaved Multimodal Representation
  - **Why needed here:** MR$^2$-Bench explicitly supports queries and documents with "interleaved image-text layouts."
  - **Quick check question:** How does a model handle a document with 3 images and text segments between them differently than a single image-text pair?

- **Concept:** Chain-of-Thought (CoT) in Retrieval
  - **Why needed here:** Paper leverages CoT for retrieval enhancement through query rewriting and reranking prompts.
  - **Quick check question:** In the context of this paper, does CoT happen before the search (Query Rewriting) or after the search (Reranking), or both?

## Architecture Onboarding

- **Component map:** Input Processor -> First-Stage Retriever -> Reasoning Augmenter (Optional) -> Second-Stage Reranker
- **Critical path:** Implementation of the Reranker Prompt (Figure 4) where "reasoning" capability is actually injected
- **Design tradeoffs:**
  - Captioning vs. Native Multimodal: Text retrievers + captions are strong baselines, but native multimodal models ultimately win on average
  - Latency vs. Accuracy: 15-point boost from GPT-5 reranking comes at high inference cost
  - Assumption: System prioritizes accuracy over real-time latency
- **Failure signatures:**
  - Zero-shot on Puzzles: Random scores on "Visual Puzzle" tasks indicate vision encoder lacks spatial reasoning depth
  - Keyword Hallucination: Query Rewriting introduces terms not in corpus, causing retrieval failures
- **First 3 experiments:**
  1. Establish Baseline Gap: Run BGE-M3 vs. Seed-1.6 on MR$^2$-Bench to reproduce 9.91 Recall@1 "shock"
  2. Ablate Visual Input: Run best multimodal retriever with images replaced by captions vs. raw images on "Visual Illustration Search" tasks
  3. Reranker Integration: Implement "Reason-then-Rank" prompt using Qwen2.5-VL-7B to measure reasoning lift

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the explicit reasoning capabilities demonstrated by large multimodal rerankers be effectively distilled into efficient, first-stage embedding models to reduce reliance on external inference steps?
- **Basis in paper:** Authors note reranking with GPT-5 yields nDCG@10 of 45.90 (15.22 gain over base retriever), indicating "substantial headroom for improvement unlocked by reranking" that current embedding models cannot capture independently.
- **Why unresolved:** Current state-of-the-art embedding models achieve only 30.68 nDCG@10, failing to internalize reasoning logic that rerankers apply post-hoc.
- **What evidence would resolve it:** Development of standalone multimodal embedding model achieving parity with "Retriever + GPT-5 Reranker" pipeline without increasing inference latency.

### Open Question 2
- **Question:** What specific training data compositions or objectives are required to resolve the "consistent failures" in abstract visual reasoning tasks where current models score near zero?
- **Basis in paper:** Results show even strongest models struggle with "capturing complex visual relationships and abstract concepts," specifically citing Analogy and Visual Puzzle sub-tasks where Seed-1.6 achieves less than 1% Recall@1.
- **Why unresolved:** Authors hypothesize current embedding models struggle to comprehend "inherently visual-centric nature" of these tasks but do not propose method to overcome lack of visual logical grounding.
- **What evidence would resolve it:** Model achieving significantly higher than random baseline performance on Visual Puzzle sub-task, potentially through training on synthetic relational data.

### Open Question 3
- **Question:** To what extent does training text retrievers on reasoning-intensive data transfer to multimodal reasoning versus merely improving handling of textual components in interleaved inputs?
- **Basis in paper:** Authors observe "reasoning-oriented text retrievers significantly outperform traditional matching-based retrievers" and suggest capabilities "effectively transfer to multimodal retrieval tasks," but exact mechanism remains unanalyzed.
- **Why unresolved:** While performance improves, unclear if model is reasoning across modalities or simply leveraging stronger text understanding to compensate for visual gaps.
- **What evidence would resolve it:** Ablation studies isolating performance on purely image-based queries for text-reasoning retrievers versus native multimodal retrievers to quantify cross-modal transfer gap.

## Limitations
- Benchmark relies heavily on closed-source models (Seed1.6-Embedding, GPT-5) for both evaluation and enhancement techniques, limiting reproducibility
- Dataset remains unreleased, preventing independent validation of query difficulty calibration and negative sampling quality
- Paper does not report statistical significance testing for performance differences between models

## Confidence
- High confidence: Benchmark's construction methodology and task definitions are well-specified
- Medium confidence: Claim that reasoning capability explains performance gap (requires access to models for validation)
- Low confidence: Generalizability of results beyond specific corpus and model versions used

## Next Checks
1. Replicate Seed-1.6 vs. MMEB performance comparison using available open-source models once dataset is released
2. Conduct ablation studies comparing pure visual reasoning tasks against text-only substitutions to quantify visual reasoning requirements
3. Test query rewriting and reranking enhancements with open-source alternatives (Qwen2.5-VL instead of GPT-5) to verify "reasoning lift" is not model-specific