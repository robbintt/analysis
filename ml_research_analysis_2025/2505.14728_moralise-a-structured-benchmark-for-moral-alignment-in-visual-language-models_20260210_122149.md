---
ver: rpa2
title: 'MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models'
arxiv_id: '2505.14728'
source_url: https://arxiv.org/abs/2505.14728
tags:
- moral
- internvl3
- open-source
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MORALISE, a structured benchmark for evaluating
  the moral alignment of vision-language models (VLMs). It addresses the gap in existing
  benchmarks that focus only on text or use AI-generated images, which can lead to
  distributional biases and reduced realism.
---

# MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models

## Quick Facts
- arXiv ID: 2505.14728
- Source URL: https://arxiv.org/abs/2505.14728
- Reference count: 40
- This paper introduces MORALISE, a structured benchmark for evaluating the moral alignment of vision-language models (VLMs).

## Executive Summary
This paper introduces MORALISE, a structured benchmark for evaluating the moral alignment of vision-language models (VLMs). It addresses the gap in existing benchmarks that focus only on text or use AI-generated images, which can lead to distributional biases and reduced realism. The benchmark proposes a taxonomy of 13 moral topics grounded in Turiel’s Domain Theory, covering personal, interpersonal, and societal domains. It includes 2,481 expert-verified real-world image-text pairs, annotated for both moral topics and modality-specific violations. Two evaluation tasks—moral judgment and moral norm attribution—assess models' ability to detect moral violations and identify specific violated norms. Experiments on 19 popular VLMs reveal persistent moral limitations, with models performing significantly worse on norm attribution than on binary moral judgment. Results also show that closed-source models generally outperform open-source ones, but scaling alone is insufficient for moral alignment. Visual moral reasoning lags behind text-based reasoning, and models from the same family exhibit similar moral behavior. The findings highlight the need for improved training and evaluation methods to enhance the moral alignment of VLMs.

## Method Summary
The MORALISE benchmark evaluates VLMs on moral alignment using a curated dataset of 2,481 real-world image-text pairs annotated with 13 moral topics and modality tags (text-centric vs. image-centric violations). The evaluation consists of two tasks: binary moral judgment (A/B) and multi-class moral norm attribution (A-N). The dataset was constructed through human-in-the-loop curation, filtering AI-generated content, and expert verification of topic and modality labels. Nineteen popular VLMs, including both closed-source (GPT-4o) and open-source models (Gemma, InternVL, Qwen), were evaluated under zero-temperature greedy decoding to assess their moral reasoning capabilities.

## Key Results
- VLMs perform significantly worse on moral norm attribution (identifying specific violated norms) than on binary moral judgment
- Visual moral reasoning lags behind text-based reasoning, with models relying heavily on textual semantic priors
- Scaling model parameters improves moral judgment only up to a threshold (~10B parameters), after which performance plateaus without targeted alignment data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vision-Language Models (VLMs) exhibit a modality asymmetry, relying primarily on textual semantic priors while failing to extract moral salience from visual features alone.
- **Mechanism:** The visual encoder aligns image features to the token embedding space, but the language model backbone appears to drive the final moral decision. When moral cues are absent from the text (image-centric violations), the model lacks the grounding to interpret visual scenes as moral transgressions, effectively defaulting to "safe" or "uncertain" behaviors derived from language patterns rather than visual reasoning.
- **Core assumption:** The alignment layer between vision and language does not currently preserve sufficient semantic depth for abstract concepts like "sanctity" or "justice" without explicit textual grounding.
- **Evidence anchors:**
  - [abstract] "Modality annotation, indicating whether the violation arises from the image or the text."
  - [section 4.4] "Textual cues consistently lead to higher accuracy... visual moral reasoning lags behind text-based reasoning."
  - [corpus] "Do VLMs Have a Moral Backbone?" (Neighbor paper) suggests VLM moral judgments are fragile under perturbation, supporting the reliance on superficial text cues.
- **Break condition:** If visual encoders are trained or fine-tuned with explicit moral supervision (e.g., contrastive learning on moral vs. neutral image pairs), this performance gap should narrow.

### Mechanism 2
- **Claim:** Scaling model parameters improves moral judgment competence only up to a threshold (approx. 10B parameters), after which performance plateaus without targeted alignment data.
- **Mechanism:** Scaling improves general multimodal understanding (object detection, OCR, captioning), which are prerequisites for moral reasoning (the "basic capability" lift). However, once the model can adequately "see" and "read," further parameter increases do not inherently instill the normative frameworks required for distinguishing complex moral topics (e.g., Fairness vs. Justice).
- **Core assumption:** Moral reasoning is a higher-order cognitive function that relies on, but is distinct from, basic perception and language fluency.
- **Evidence anchors:**
  - [section 4.3] "Scaling from small to medium... significantly improves... However, the benefit plateaus beyond the medium size."
  - [section 4.3] "Scaling alone is insufficient for moral alignment."
  - [corpus] "Structured Moral Reasoning in Language Models" (Neighbor paper) argues LLM reasoning is often shallow, aligning with the idea that raw scale doesn't guarantee deep ethical integration.
- **Break condition:** If general pre-training data were to heavily emphasize moral philosophy or annotated social dilemmas, the scaling curve might extend further, but current web-scale data appears insufficient.

### Mechanism 3
- **Claim:** VLMs likely rely on family-specific data distributions rather than universal moral reasoning logic, leading to high correlation within model families but inconsistency across them.
- **Mechanism:** Models from the same "family" (e.g., Qwen variants) share pre-training corpora and alignment fine-tuning datasets. Their moral "intuition" is thus a statistical reflection of that specific training mix rather than a generalized reasoning engine. This explains why sibling models correlate highly even across different sizes, while similar-sized models from different families diverge.
- **Core assumption:** Moral outputs are predominantly driven by the statistical likelihood of tokens in the training data rather than emergent reasoning capabilities.
- **Evidence anchors:**
  - [section 4.4] "Responses from VLMs of the same series... tend to exhibit high similarity... models within the same family but trained on different corpora... do not exhibit strong correlation."
  - [corpus] "Moral Outrage Shapes Commitments" (Neighbor paper) touches on how rhetoric/media framing shapes engagement, paralleling how training data "framing" shapes model output.
- **Break condition:** If models were trained on identical, standardized moral corpora (like MORALISE itself) during fine-tuning, inter-family correlations would likely increase.

## Foundational Learning

- **Concept:** **Turiel’s Domain Theory**
  - **Why needed here:** This theory is the structural backbone of the MORALISE dataset, dividing the 12,000+ examples into Personal, Interpersonal, and Societal domains. Understanding this distinction is necessary to interpret why "Liberty" (Personal) and "Authority" (Societal) are treated as distinct failure modes.
  - **Quick check question:** Can you explain why a dress code violation (conventional) is categorized differently from a physical harm violation (moral) within this framework?

- **Concept:** **Modality Centricity**
  - **Why needed here:** The benchmark uniquely distinguishes whether the "clue" to the moral violation is in the image or the text. Evaluators must understand that a "text-centric" violation implies the image might be neutral, testing if the model hallucinates moral content based on a benign visual paired with toxic text.
  - **Quick check question:** If a model scores high on text-centric violations but low on image-centric ones, which component of the VLM (Vision Encoder or LLM) is likely the bottleneck?

- **Concept:** **Norm Attribution vs. Binary Judgment**
  - **Why needed here:** This paper moves beyond "Is this bad?" (Binary) to "Why is this bad?" (Attribution). Understanding this shift is key to recognizing that current models are barely above chance on specific norm identification (e.g., confusing "Fairness" with "Justice").
  - **Quick check question:** Why does the paper argue that "Moral Norm Attribution" is a more robust proxy for alignment than simple binary safety classification?

## Architecture Onboarding

- **Component map:**
  Input: Real-world Image + Text Prompt → 19 VLMs (Proprietary: GPT-4o; Open-source: Gemma, InternVL, Qwen) → Evaluation Interface: Two-stage prompting (Judgment: A/B; Attribution: A-N multi-choice) → Ground Truth: Expert-verified labels for 13 topics + Modality tag (Image/Text)

- **Critical path:**
  1.  **Data Curation:** Scraping real images (Reddit, Pinterest) while filtering AI-generated content.
  2.  **Annotation:** Labeling for one of 13 topics + Modality (Text vs. Image cue).
  3.  **Inference:** Running VLMs with 0 temperature (greedy decoding).
  4.  **Metric Calculation:** Accuracy for Judgment; Hit-Rate/F1 for Attribution.

- **Design tradeoffs:**
  - **Real vs. AI Images:** Authors chose real images for ecological validity, sacrificing the infinite scalability of synthetic data (like M3oralBench).
  - **Breadth vs. Depth:** Using 13 topics covers a vast moral landscape but increases the classification difficulty (multi-class) compared to binary safety benchmarks.

- **Failure signatures:**
  - **The "Text-Bias" Failure:** High performance on text-centric pairs but random-guess performance on image-centric pairs.
  - **The "Norm Collapse":** Model predicts "Harm" for almost all negative examples because it is the most over-represented concept in safety training (low precision on "Liberty" or "Sanctity").
  - **The "Scale Plateau":** A 32B model performing identically to a 7B model from the same family on specific norms.

- **First 3 experiments:**
  1.  **Modality Ablation:** Mask the text input and force the model to judge purely on the image to quantify the "visual moral blindness" gap.
  2.  **Confusion Matrix Analysis:** specifically between "Justice" and "Fairness" to see if models distinguish procedural justice from distributive fairness.
  3.  **Family Correlation Check:** Run embeddings for the moral prompts through different model families (Gemma vs. Qwen) to visualize the clustering of "moral concepts" in latent space.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed moral alignment limitations and scaling plateaus generalize to ultra-large VLMs exceeding 50 billion parameters?
- Basis in paper: [explicit] The authors state in Section E (Limitations) that the evaluation is limited to models under 50B parameters, and findings may not generalize to emerging ultra-large models.
- Why unresolved: The computational and accessibility constraints prevented the authors from testing models larger than 50B (e.g., GPT-4o full size vs mini, or open models >70B).
- What evidence would resolve it: Evaluation results on the MORALISE benchmark for models significantly larger than 50B parameters (e.g., 70B or 100B+) to see if the correlation between scale and moral alignment remains flat.

### Open Question 2
- Question: Can targeted training objectives or the incorporation of diverse multi-modal moral alignment data during pre-training effectively break the performance plateau observed in medium-sized VLMs?
- Basis in paper: [explicit] The authors conclude (Takeaway #5) that "scaling alone is insufficient for moral alignment" and suggest in Section 4.4 that incorporating diverse moral data during pretraining could be effective.
- Why unresolved: The experiments analyzed existing models trained on general corpora; no models were specifically trained or fine-tuned using the proposed methodology to validate the hypothesis.
- What evidence would resolve it: Training ablations where VLMs are fine-tuned or pre-trained with curated moral alignment data, demonstrating statistically significant improvements over non-aligned baselines of the same size.

### Open Question 3
- Question: What specific architectural or training modifications are required to elevate visual moral reasoning capabilities to the level of text-based reasoning in VLMs?
- Basis in paper: [explicit] The authors conclude (Takeaway #6) that "visual moral reasoning lags behind text-based reasoning" and identify a need to enhance moral understanding derived specifically from images.
- Why unresolved: The paper diagnoses the performance gap (text-centric vs. image-centric accuracy) but does not investigate the underlying causes within the visual encoders or cross-modal attention mechanisms.
- What evidence would resolve it: A mechanistic interpretability study or architectural ablation that identifies why visual features fail to trigger moral norms as effectively as text tokens, followed by a fix that closes the performance gap.

### Open Question 4
- Question: How can the human-in-the-loop curation and verification pipeline be adapted to scale to substantially larger datasets or broader moral domains without compromising annotation quality?
- Basis in paper: [explicit] In Section E (Limitations), the authors note that their human-in-the-loop pipeline is "inherently labor-intensive and lacks scalability."
- Why unresolved: The current dataset relies entirely on expert graduate students for verification, a method that is resource-intensive and difficult to replicate for larger datasets.
- What evidence would resolve it: The development and validation of a semi-automated annotation pipeline (e.g., using high-performing models to pre-filter or annotate) that matches the quality of expert human annotation while significantly reducing manual labor.

## Limitations
- Sampling bias in real-world data from specific platforms may limit generalizability to global moral norms
- Modality annotation scheme oversimplifies complex integration of visual and textual moral salience
- Expert verification focused on factual correctness rather than philosophical consistency for abstract concepts

## Confidence
- **High Confidence Claims:**
  - VLMs show significantly better performance on text-centric versus image-centric moral violations
  - Scaling alone does not ensure moral alignment
  - Models within the same family exhibit high correlation in moral judgments

- **Medium Confidence Claims:**
  - Visual moral reasoning fundamentally lags behind text-based reasoning due to modality asymmetry
  - The 13-topic taxonomy comprehensively covers moral domains
  - Real images provide more ecologically valid evaluation than AI-generated images

- **Low Confidence Claims:**
  - Specific failure patterns reflect deep conceptual misunderstanding rather than statistical correlations
  - MORALISE will meaningfully guide future VLM alignment efforts
  - The benchmark captures the full complexity of human moral reasoning

## Next Checks
1. **Modality Ablation Study:** Systematically mask either visual or textual components of test samples to quantify the exact contribution of each modality to moral judgments.
2. **Cross-Cultural Validation:** Evaluate MORALISE on culturally diverse datasets and with annotators from different backgrounds to assess generalizability beyond WEIRD populations.
3. **Temporal Robustness Test:** Re-evaluate the same VLMs on MORALISE after additional fine-tuning with moral reasoning datasets to determine whether targeted alignment training can overcome identified limitations.