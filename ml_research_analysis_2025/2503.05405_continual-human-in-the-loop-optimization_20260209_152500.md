---
ver: rpa2
title: Continual Human-in-the-Loop Optimization
arxiv_id: '2503.05405'
source_url: https://arxiv.org/abs/2503.05405
tags:
- user
- optimization
- conbo
- users
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual Human-in-the-Loop Optimization
  (CHiLO), a framework enabling optimizers to continuously improve efficiency by leveraging
  data from prior users. The authors identify key challenges including scalability,
  catastrophic forgetting, and the stability-plasticity dilemma, and propose Population-Informed
  Continual Bayesian Optimization (ConBO) to address them.
---

# Continual Human-in-the-Loop Optimization

## Quick Facts
- arXiv ID: 2503.05405
- Source URL: https://arxiv.org/abs/2503.05405
- Reference count: 40
- Primary result: Introduces ConBO, a scalable continual optimization framework that leverages data from prior users to improve efficiency and personalization.

## Executive Summary
This paper introduces Continual Human-in-the-Loop Optimization (CHiLO), a framework enabling optimizers to continuously improve efficiency by leveraging data from prior users. The authors identify key challenges including scalability, catastrophic forgetting, and the stability-plasticity dilemma, and propose Population-Informed Continual Bayesian Optimization (ConBO) to address them. ConBO uses a Bayesian Neural Network (BNN) trained on synthesized data from previous users' models, combined with a generative replay strategy to retain knowledge and mitigate forgetting. Evaluations on synthetic benchmark functions and a real-world VR mid-air keyboard personalization task demonstrate that ConBO significantly outperforms standard Bayesian optimization and manual adjustment in adaptation efficiency and typing performance. Across 12 users, ConBO achieved up to 2.17 net WPM higher average typing speeds and lower regret values, with progressive improvements as more users contributed data. The method shows strong potential for scalable, efficient, and personalized interaction systems that improve with accumulated user experience.

## Method Summary
ConBO implements continual learning in human-in-the-loop optimization by maintaining two models: a population model (BNN) that captures aggregated knowledge from all prior users, and a user-specific model (GP) that adapts to the current user's preferences. During meta-training, the system synthesizes training data by querying stored GP models from previous users across a design space grid, filtering out high-variance (unexplored) regions. The population BNN is then trained on this filtered data to approximate the collective user landscape. During online adaptation, the system calculates a weighted acquisition function combining the population model's general trends with the user-specific model's individual preferences, dynamically adjusting weights based on observed data. After each user session, the user's GP model is stored, and the population BNN is retrained using generative replay from all stored models, ensuring knowledge retention while avoiding catastrophic forgetting.

## Key Results
- ConBO achieved up to 2.17 net WPM higher average typing speeds compared to manual adjustment in VR keyboard personalization
- Across 12 users, ConBO demonstrated lower cumulative regret values than standard Bayesian optimization
- The system showed progressive improvement as more users contributed data, validating its scalability claims
- ConBO maintained efficient performance while standard GP-based methods experienced cubic scaling with user count

## Why This Works (Mechanism)

### Mechanism 1
The system achieves scalable continual learning by compressing historical user data into a "Population Model" via generative replay, rather than accumulating raw observations. Standard Gaussian Processes (GPs) scale cubically ($O(n^3)$), becoming computationally intractable as user data grows. ConBO addresses this by storing only the parameters of previous users' GP models (lightweight storage). When updating the "Population Model" (a Bayesian Neural Network, or BNN), the system synthesizes training data by querying these stored GPs across the design space. This transfers knowledge from thousands of past interactions into a fixed-size neural network. Core assumption: The BNN is sufficiently expressive to approximate the aggregate distribution of user performance landscapes. Evidence anchors: Section 4.1 and 4.2 describe the population model construction and generative replay process. Break condition: If the BNN architecture is too shallow, it will fail to capture the complex, multi-modal distributions of diverse user preferences, leading to underfitting.

### Mechanism 2
The system balances general population trends with individual idiosyncrasies using a dynamic weighted acquisition function. New users benefit from "priors" (population trends) but may have unique needs. ConBO calculates the next best design by weighting the acquisition values (Expected Improvement) from two sources: the Population Model (general) and the User-Specific Model (personal). Initially, the Population Model dominates (high weight $w_m$) to avoid random exploration. As the current user provides more data, the weight shifts to the User-Specific Model to fine-tune parameters. Core assumption: The initial population priors are sufficiently relevant to the new user to offer a better starting point than random search. Evidence anchors: Section 4.3 details the weighted acquisition function, and Section 5.4 demonstrates adaptation improvements over iterations. Break condition: If the user is an extreme outlier, the initial high weight of the Population Model may guide the system into a local optimum, requiring the user to endure suboptimal settings until the User-Specific Model gains sufficient weight.

### Mechanism 3
"Catastrophic forgetting" is mitigated by filtering synthesized training data based on uncertainty, ensuring the population model only learns reliable patterns. Previous users do not explore the entire design space; they leave "holes" of unexplored territory. If the system trains the population BNN on these holes, it learns noise. ConBO uses a variance threshold ($\lambda$); when synthesizing data from old models, predictions with high variance (unexplored regions) are discarded. This ensures the BNN is only trained on regions where previous users actually generated confident performance data. Core assumption: High variance in a surrogate model correlates with a lack of observational data in that region of the design space. Evidence anchors: Section 4.3 and Appendix A.5 describe the variance filtering mechanism. Break condition: If the variance threshold is set too aggressively, the BNN may be trained on too little data to generalize; if set too loosely, the BNN may hallucinate performance metrics in unexplored zones.

## Foundational Learning

### Concept: Bayesian Optimization (BO)
Why needed here: This is the engine of the system. You must understand the loop of *Surrogate Model $\to$ Acquisition Function $\to$ Evaluation* to understand how ConBO modifies the "Surrogate" part to be continual. Quick check question: Can you explain why an Acquisition Function (like Expected Improvement) is necessary, rather than just optimizing the Surrogate Model directly?

### Concept: Catastrophic Forgetting
Why needed here: This is the primary failure mode ConBO is designed to solve. Without understanding that neural networks tend to overwrite old tasks when learning new ones, the complexity of the "Generative Replay" mechanism seems arbitrary. Quick check question: If you fine-tuned a generic image classifier on a dataset of only cats, what would likely happen to its ability to recognize dogs?

### Concept: Bayesian Neural Networks (BNNs) vs. Gaussian Processes (GPs)
Why needed here: The paper deliberately swaps the standard GP for a BNN for the population model. Understanding the scalability differences (BNNs scale linearly/quadratically vs GP's cubic scaling) is crucial for the architecture. Quick check question: Why does a standard Gaussian Process become computationally prohibitive when dataset size $N$ exceeds $\approx 10,000$ points?

## Architecture Onboarding
- **Component map:** Population Model (BNN) -> User-Specific Model (GP) -> Model Library -> Variance Filter
- **Critical path:** 1. Inference: User requests design $\to$ Calculate Acquisition (Weighted sum of BNN + current GP) $\to$ User performs task. 2. Online Update: Observe $(x, y)$ $\to$ Update User-Specific GP $\to$ Update Population BNN (partial fine-tune). 3. Offline/Post-Session Update: Train User GP fully $\to$ Store in Library $\to$ Retrain Population BNN using Generative Replay from Library (Grid sampling + Variance Filtering).
- **Design tradeoffs:** Scalability vs. Precision (BNN enables scaling to infinite users but introduces stochasticity compared to exact GPs); Privacy vs. Replay (stores models not raw data, providing implicit data minimization).
- **Failure signatures:** Mode Collapse (BNN overfits, suggests same design to every user); Regressing Efficiency (regret increases after User 10, indicating forgetting); Latency Spikes (inference time grows linearly with user count, code likely ensembling all past GPs).
- **First 3 experiments:** 1. Compute Scaling Benchmark: Run ConBO vs. Standard GP-ensemble (TAF) on synthetic benchmark (Branin) for 50 users. Plot wall-clock time per iteration. 2. Forgetting Test (Re-optimization): Optimize for User A $\to$ Users B through Z $\to$ Re-optimize User A. Check if system remembers User A. 3. Ablation on Variance Filter: Run VR Keyboard task with variance filter disabled ($\lambda = \infty$). Inspect BNN loss curve and prediction plots for hallucinated performance in unexplored regions.

## Open Questions the Paper Calls Out

### Open Question 1
How does the sequence in which users are encountered (e.g., outliers vs. average users appearing first) affect the convergence and stability of the ConBO population model? Basis: Section 6.1 states the order can influence performance and future research should test effects of user order. Why unresolved: Current user study used fixed participant order. What evidence would resolve it: Ablation study varying presentation order of synthetic and real users to measure regret and adaptation speed.

### Open Question 2
How can CHiLO be adapted to handle non-stationary user characteristics, such as changes in performance due to fatigue or motor learning during a session? Basis: Section 6.1 notes current implementation assumes constant user characteristics and future work should focus on real-time detection of changes. Why unresolved: Method assumes static objective function for each user. What evidence would resolve it: Experiments with induced fatigue or extended practice periods to evaluate real-time detection and re-adaptation mechanisms.

### Open Question 3
Can the CHiLO framework be effectively extended to multi-objective optimization tasks, such as balancing typing speed against error rates? Basis: Section 6.1 suggests exploring extension to multi-objective tasks by adapting BNN and updating acquisition function to Expected Hypervolume Improvement (EHVI). Why unresolved: Current ConBO implementation is restricted to single-objective functions. What evidence would resolve it: Implementation of multi-output BNN and EHVI acquisition function tested on benchmark functions and multi-criteria interaction tasks.

### Open Question 4
Can ConBO transfer knowledge across different applications (e.g., from mid-air typing to gesture-based input) rather than just across users within the same task? Basis: Section 6.1 posits potential for ConBO to transfer knowledge across different applications and future work should explore generalizability across various input methods. Why unresolved: Population model is trained specifically on data from the same optimization task. What evidence would resolve it: Cross-domain user studies evaluating optimization speed when transferring pre-trained population model to novel interaction task.

## Limitations
- The generative replay step requires querying all stored models at every meta-update, which could become computationally prohibitive as the library grows (10,000 users x 400 grid points x GP evaluation cost)
- The choice of BNN architecture (3-layer, 100 nodes) was not justified against alternatives, raising questions about whether the model is over- or under-parameterized
- The claim that ConBO learns "personalized" settings is only weakly supported; individual user curves show significant variance, suggesting personalization may be limited to first few iterations

## Confidence
- **High:** The catastrophic forgetting mitigation mechanism (variance-filtered generative replay) is well-grounded in established continual learning literature and experimental ablation supports it
- **Medium:** Efficiency gains over manual adjustment and standard BO are demonstrated, but paper does not benchmark against most closely related continual BO methods (e.g., TAF)
- **Low:** The claim that ConBO learns "personalized" settings is only weakly supported; aggregate improvements shown but individual user curves reveal significant variance

## Next Checks
1. **Scalability Profiling:** Measure wall-clock time for generative replay step as function of stored user models (10, 50, 100, 500 users). Verify time per meta-update remains practical.
2. **Architecture Sensitivity:** Run synthetic benchmark with simpler BNN (2 layers, 50 nodes) and deeper one (5 layers, 200 nodes). Check if performance is robust to architecture changes.
3. **Personalization Granularity:** For VR keyboard task, plot cumulative regret for each of 12 individual users, not just average. Identify if subset of users show little to no improvement, indicating failure of personalization.