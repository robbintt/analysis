---
ver: rpa2
title: Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement
  Learning
arxiv_id: '2510.12096'
source_url: https://arxiv.org/abs/2510.12096
tags:
- training
- learning
- dynamic
- type
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Dynamic sparse training strategies provide module-specific benefits\
  \ that complement architectural improvements in scalable deep reinforcement learning.\
  \ While strong architectural foundations remain essential for effective scaling,\
  \ targeted training approaches for each network component\u2014dense training for\
  \ encoders, dynamic sparse training for critics, and static sparse training for\
  \ actors\u2014unlock further scalability potential."
---

# Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.12096
- Source URL: https://arxiv.org/abs/2510.12096
- Authors: Guozheng Ma; Lu Li; Zilin Wang; Haoyu Wang; Shengchao Hu; Leszek Rutkowski; Dacheng Tao
- Reference count: 40
- Primary result: Targeted sparse training strategies (dense encoders, dynamic sparse critics, static sparse actors) unlock further scalability potential for massive RL models (362M parameters) without algorithmic changes.

## Executive Summary
This work investigates whether dynamic sparse training can enhance the scalability of deep reinforcement learning beyond what architectural improvements alone can achieve. Through systematic ablation studies on scaled MR.Q agents (up to 362M parameters), the authors discover that training strategy choices have module-specific impacts: architectural design dominates for encoders, dynamic sparse training is essential for critics to combat TD-induced plasticity loss, and static sparse training provides optimal stability for actors. The resulting Module-Specific Training framework demonstrates superior sample efficiency and stable learning dynamics across diverse RL algorithms without requiring algorithmic modifications.

## Method Summary
The study employs MR.Q, a modular RL architecture with three components: encoder (self-supervised dynamics prediction), critic (TD learning), and actor (policy gradient). MST applies different training strategies per module: dense training for encoders, dynamic sparse training (RigL) for critics, and static sparse training (ER pruning) for actors. The framework was evaluated at massive scale (362M parameters) on Dog Run and Humanoid Run tasks from DeepMind Control Suite, measuring performance via Episode Return and sample efficiency while monitoring optimization health through Stable Rank and Dormant Ratio metrics.

## Key Results
- Dense training for encoders prevents representational collapse and enables stable scaling when combined with strong architectural foundations (Type 6 blocks).
- Dynamic sparse training for critics maintains low dormant ratios and high representational rank at scale, mitigating TD-induced plasticity loss.
- Static sparse training for actors provides optimal stability during policy optimization, avoiding the stochasticity introduced by dynamic topology changes.

## Why This Works (Mechanism)

### Mechanism 1: Architecture Dominates Training Strategy for Representation Learning
Strong architectural design (layer normalization, residual connections, activation functions) determines scalability more than training regime choice for self-supervised encoders. Layer normalization prevents catastrophic representational collapse during dynamics prediction, creating inherently better optimization landscapes that sparse training cannot replicate.

### Mechanism 2: Dynamic Topology Adaptation Mitigates TD-Induced Plasticity Loss
Critics trained via TD bootstrapping require dynamic sparse training to maintain plasticity at scale. The periodic drop-and-grow cycle of DST actively removes dormant connections and reallocates capacity to useful pathways, maintaining both low dormant ratio and high representational rank despite non-stationary TD targets.

### Mechanism 3: Static Sparsity Preserves Policy Stability During Scaling
Actor networks benefit from static sparse training rather than dynamic approaches because policy gradient optimization requires stable gradient pathways. SST provides sparsity benefits without disrupting established gradient paths, while ReLU activation induces beneficial feature sparsity that better supports policy gradient optimization.

## Foundational Learning

- **Temporal Difference (TD) Learning and Bootstrapping**
  - Why needed here: Understanding why critics suffer unique pathologies requires grasping how TD targets are non-stationary (they depend on the current value function estimate).
  - Quick check question: Can you explain why TD learning creates a moving target that doesn't occur in supervised learning?

- **Plasticity Loss and Dormant Neurons**
  - Why needed here: The paper's core intervention targets plasticity loss; you need to recognize symptoms (high dormant ratio, declining SRank, parameter norm growth).
  - Quick check question: If a network's dormant ratio increases from 5% to 40% during training, what does this indicate about its learning capacity?

- **Dynamic Sparse Training (RigL specifically)**
  - Why needed here: MST relies on RigL's drop-and-grow mechanism; understanding gradient-based growth vs. magnitude-based pruning is essential for implementation.
  - Quick check question: In RigL, why would you grow connections based on gradient magnitude rather than random selection?

## Architecture Onboarding

**Component map:**
```
MST Framework
Encoder (self-supervised dynamics prediction)
├── Architecture: Type 6 (ELU + LayerNorm + Residual)
└── Training: Dense (no sparsity)
Critic (TD learning, TD3-style)
├── Architecture: Type 4+ (ELU + LayerNorm minimum)
└── Training: DST via RigL at 0.6 sparsity
Actor (deterministic policy gradient)
├── Architecture: Type 5+ (ReLU + LayerNorm + Residual)
└── Training: SST via ER initialization at 0.6 sparsity
```

**Critical path:**
1. Establish strong encoder architecture first (encoder collapse cascades to critic/actor failures)
2. Apply DST to critic for scalability gains
3. Apply SST to actor last (least sensitive to training regime at base scale)

**Design tradeoffs:**
- Sparsity level (0.6): Balance between parameter efficiency and expressivity. Higher sparsity (>0.8) causes instability; lower (<0.4) loses plasticity benefits.
- ER vs. uniform sparsity: Erdős-Rényi initialization outperforms uniform at high sparsity (corroborated by multiple works).
- ReLU vs. ELU: ReLU benefits actors (feature sparsity); ELU benefits encoders/critics (plasticity maintenance).

**Failure signatures:**
- Encoder SRank declining rapidly → missing layer normalization
- Critic dormant ratio >20% at scale → dense training instead of DST
- Actor performance high variance at scale → dynamic training instead of SST
- Catastrophic collapse at 5× scale → missing residual connections

**First 3 experiments:**
1. **Baseline sanity check:** Implement Type 4 encoder + Type 4 critic + Type 3 actor with dense training at default scale. Verify performance matches paper baseline before scaling.
2. **Architecture-only ablation:** Scale to 3× with dense training only. Confirm degradation pattern (critic collapse, actor instability) to establish need for MST.
3. **Critic DST validation:** Apply DST to critic only at 3× scale. Measure dormant ratio and SRank against dense baseline to verify plasticity improvement mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Module-Specific Training (MST) preserve its scaling benefits in real-world RL scenarios involving larger action spaces, multi-task conflicts, and continual adaptation requirements?
- Basis in paper: Lesson 3 states it's time to advance toward more challenging real-world scenarios and explicitly identifies these as "demanding optimization challenges" for future research.
- Why unresolved: MST was evaluated only on standard continuous control benchmarks with single-task, fixed-action-space settings.
- What evidence would resolve it: Experiments applying MST to multi-task RL benchmarks, continual learning setups, and environments with high-dimensional action spaces demonstrating whether module-specific benefits persist.

### Open Question 2
- Question: How does MST interact with complementary scaling mechanisms such as Mixtures of Experts (MoE) or alternative optimizer designs?
- Basis in paper: The paper cites MoE-based scaling and specialized optimizers as related approaches but doesn't investigate their combination with MST.
- Why unresolved: The investigation isolates architecture-training interactions but doesn't examine whether MST is compatible with or redundant relative to other scaling paradigms.
- What evidence would resolve it: Ablation studies combining MST with MoE layers or alternative optimizers, measuring whether benefits are additive, synergistic, or conflicting.

### Open Question 3
- Question: Are the optimal sparsity levels module-specific and scale-dependent, or does the fixed 0.6 sparsity used in experiments generalize across configurations?
- Basis in paper: The paper adopts a fixed 0.6 sparsity level across all experiments based on preliminary analysis, while Figure 12 shows sparsity trends affect performance.
- Why unresolved: No systematic sweep of sparsity levels across different modules, architectures, and scales was conducted.
- What evidence would resolve it: Comprehensive sparsity ablations for each module at multiple scales, identifying whether critics benefit from higher sparsity than actors or whether optimal levels shift as models grow larger.

## Limitations
- MST's effectiveness hinges on precise sparsity levels (0.6) and training schedules, with unclear robustness to hyperparameter variation.
- The underlying reasons for activation function preferences (ELU for encoders/critics, ReLU for actors) aren't fully characterized.
- Analysis assumes smooth scaling from baseline to 5× scale, potentially missing phase transitions or emergent failures.

## Confidence

- **High confidence**: The architectural dominance finding - supported by direct ablation showing catastrophic collapse without layer normalization regardless of training strategy.
- **Medium confidence**: The dynamic training benefits for critics - while supported by SRank and dormant ratio metrics, the causal relationship between TD bootstrapping and plasticity loss could benefit from additional controls.
- **Medium confidence**: The static training benefits for actors - supported by stability metrics, but the claim about gradient pathway disruption could be more rigorously quantified.

## Next Checks

1. **Hyperparameter robustness test**: Systematically vary sparsity levels (0.4, 0.6, 0.8) and RigL update frequencies to identify performance boundaries and determine if MST benefits persist across reasonable parameter ranges.

2. **Architecture transfer validation**: Apply MST framework to a fundamentally different RL algorithm (e.g., Soft Actor-Critic or Proximal Policy Optimization) to verify that module-specific training benefits generalize beyond MR.Q.

3. **Failure mode characterization**: Intentionally induce specific architectural failures (remove layer normalization, increase network depth beyond tested limits) to map the boundary conditions where MST can and cannot rescue learning performance.