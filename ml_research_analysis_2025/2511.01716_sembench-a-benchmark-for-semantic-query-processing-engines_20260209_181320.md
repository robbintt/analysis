---
ver: rpa2
title: 'SemBench: A Benchmark for Semantic Query Processing Engines'
arxiv_id: '2511.01716'
source_url: https://arxiv.org/abs/2511.01716
tags:
- semantic
- data
- queries
- systems
- operators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemBench, a benchmark designed for semantic
  query processing engines (SQPEs) that leverage large language models (LLMs) to process
  multimodal data. The benchmark introduces diversity across scenarios (ranging from
  movie review analysis to medical question-answering), modalities (including text,
  images, and audio), and operators (semantic filters, joins, mappings, ranking, and
  classification).
---

# SemBench: A Benchmark for Semantic Query Processing Engines

## Quick Facts
- **arXiv ID:** 2511.01716
- **Source URL:** https://arxiv.org/abs/2511.01716
- **Reference count:** 40
- **Primary result:** SemBench benchmark reveals up to 100× cost variations across semantic query processing engines for identical queries

## Executive Summary
This paper introduces SemBench, a benchmark designed to evaluate semantic query processing engines (SQPEs) that leverage large language models for multimodal data processing. The benchmark covers five diverse scenarios with 55 queries, testing combinations of semantic operators (filters, joins, mappings, ranking, classification) across text, image, and audio data. Evaluation on three academic systems (LOTUS, Palimpzest, ThalamusDB) and Google BigQuery reveals significant performance differences, with cost variations up to 100× for certain queries. The study highlights how operator implementation choices and prompt design critically impact performance, providing insights into strengths and weaknesses of current SQPEs.

## Method Summary
The benchmark uses datasets from Kaggle sources with specified scale factors, stored as relational tables with file paths for multimodal data. Evaluation runs on AWS g4dn.2xlarge instances using Gemini 2.5 Flash (temperature=0, reasoning disabled) with parallelism=20. Five systems are tested: LOTUS v1.1.3, Palimpzest v0.8.2, ThalamusDB v0.1.15, and BigQuery. Queries combine traditional SQL operators with semantic operators (AI.IF, AI_CLASSIFY, etc.). Performance is measured across quality metrics (F1, Spearman, Adjusted Rand Index), monetary cost, and latency, with five runs per query to average results.

## Key Results
- Cost variations of up to 100× across systems for identical queries, primarily due to different implementations of LIMIT clauses and semantic joins
- Early termination strategies for semantic operators can reduce costs by orders of magnitude compared to post-processing filtering
- Embedding-based approximations for semantic joins trade result quality for significant cost reduction
- Prompt design complexity creates direct trade-offs between token costs and result accuracy
- Systems show varying strengths: LOTUS excels at text queries, Palimpzest at classification tasks, while multimodal queries reveal system limitations

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Early termination strategies for semantic operators (specifically `LIMIT`) can reduce monetary costs by orders of magnitude compared to post-processing filtering.

**Mechanism:** Systems that integrate `LIMIT` logic directly into semantic operator execution avoid invoking the LLM on the entire dataset. Instead of evaluating every row via expensive LLM calls first, they stop as soon as enough matching rows are found.

**Core assumption:** The cost of LLM invocation is the dominant factor in query processing, significantly outweighing relational overhead.

**Evidence anchors:**
- [abstract] Mentions "cost variations of up to 100× across systems for certain queries."
- [section 6.3] "For Queries Q1, Q5, and Q6, processing costs vary by more than a factor of 100×... SPQEs that terminate evaluation via semantic operators early... achieve significantly better performance."
- [corpus] Related work on *Implementing Semantic Join Operators Efficiently* supports the focus on operator-level optimization for cost reduction.

**Break condition:** If the selectivity of the semantic filter is extremely low (very few matching rows), early termination might still require scanning most of the dataset, reducing the relative advantage.

### Mechanism 2
**Claim:** Utilizing embedding-based approximations or batching strategies for semantic joins reduces cost and latency, often at the expense of result quality.

**Mechanism:** Instead of calling a generative LLM for every possible row pair (quadratic complexity), systems use embedding similarity (e.g., `e5-base-v2`) to prune candidates or batching to process multiple items in a single prompt. This trades the high latency and cost of individual LLM calls for the cheaper computation of vector distances or efficient context packing.

**Core assumption:** Embedding vectors capture sufficient semantic similarity to serve as a proxy for the specific natural language join condition, or LLMs can handle multiple inputs in a batch without significant performance degradation.

**Evidence anchors:**
- [section 5.1] Describes LOTUS using "embedding similarity (e.g., applying a pre-trained transformer model... to compute embeddings)" to approximate joins.
- [section 6.3] "ThalamusDB uses batching to include multiple items... into the prompt... [while] LOTUS implements relational joins by matching items first based on their embedding vectors."
- [corpus] Neighbor paper *Implementing Semantic Join Operators Efficiently* highlights the importance of these optimizations.

**Break condition:** If the join condition requires complex reasoning not captured by vector similarity (e.g., "Does image A match the negation of text B?"), embedding approximations may fail, yielding zero recall.

### Mechanism 3
**Claim:** Prompt design complexity creates a direct trade-off between token costs (input) and result quality (accuracy).

**Mechanism:** Systems using detailed prompts with few-shot examples (Palimpzest in Wildlife scenario) provide better context to the LLM, potentially improving accuracy. However, these longer prompts significantly increase the number of input tokens processed per row. Conversely, concise prompts reduce token count but may fail to guide the model effectively for complex tasks.

**Core assumption:** LLM performance is highly sensitive to instruction formatting and context length directly correlates with monetary cost.

**Evidence anchors:**
- [section 6.3] "Palimpzest uses relatively long prompt templates, containing examples... This increases quality while increasing costs... ThalamusDB uses concise prompt templates that reduce costs at the expense of quality."
- [section 7] "Prompt design matters... different SQPEs seem to aim at different cost-quality tradeoffs."
- [corpus] *Pairwise Judgment Formulation for Semantic Embedding Model* discusses semantic relevance modeling, indirectly supporting the difficulty of capturing intent without sufficient context.

**Break condition:** If the underlying model is incapable of the task regardless of prompting (e.g., domain limitations), increased prompt complexity yields diminishing returns or higher cost with no quality gain.

## Foundational Learning

**Concept:** **Semantic Operators vs. Relational Operators**
- **Why needed here:** The paper defines a new class of engines (SQPEs) that extend SQL with AI-driven operators (`AI.IF`, `AI_CLASSIFY`). Understanding that these operators are stochastic and expensive (unlike deterministic `WHERE` clauses) is crucial for interpreting the benchmark results.
- **Quick check question:** How does the execution cost of a semantic filter (`AI.IF`) differ fundamentally from a standard SQL filter (`WHERE age > 25`)?

**Concept:** **Token Economy (Input vs. Output)**
- **Why needed here:** The benchmark evaluates cost based on monetary fees derived from token usage. The distinction between input tokens (prompt size) and output tokens (generated data) explains why prompt engineering is a critical optimization vector in Section 6.3.
- **Quick check question:** Why might a system with a complex few-shot prompt be more expensive than one with a zero-shot prompt, even if they use the same underlying model?

**Concept:** **Multimodal Capabilities of LLMs**
- **Why needed here:** The benchmark includes image and audio data. One must understand that modern LLMs (like Gemini) can ingest non-text modalities directly, which allows SQL engines to query "unstructured" columns like images or audio files.
- **Quick check question:** In the Wildlife scenario, how does the engine process an 'audio' column to detect an elephant sound?

## Architecture Onboarding

**Component map:**
Data Layer (Kaggle datasets) -> SQPE Layer (LOTUS, Palimpzest, ThalamusDB, BigQuery) -> Model Layer (Gemini 2.5 Flash via API) -> Evaluation Layer (SemBench framework)

**Critical path:**
1. **Query Parsing:** System accepts a query with semantic operators (e.g., `SELECT * FROM T WHERE AI.IF(img, 'cat')`)
2. **Plan Generation:** Optimizer decides whether to use exact LLM calls, embeddings, or batching
3. **Execution:** The system invokes the LLM API for specific rows/modalities
4. **Aggregation:** Results are filtered/aggregated based on LLM responses

**Design tradeoffs:**
- **Accuracy vs. Cost:** High accuracy requires expensive models and detailed prompts; low cost requires aggressive approximation (embeddings) or smaller models
- **Latency vs. Throughput:** Batching improves throughput (cost efficiency) but may increase latency for individual items waiting in a batch
- **Modality Support:** Supporting audio/images increases system complexity and processing overhead compared to text-only systems

**Failure signatures:**
- **High Cost / Low Latency:** Likely using a powerful model (GPT-4 class) without caching or approximation
- **Zero Quality / Low Cost:** Likely using an embedding approximation for a task that requires complex reasoning (approximation breakdown) or a prompt that is too concise/constrained
- **Refusal / Errors:** LLM refusing to answer sensitive queries (e.g., Medical skin mole mapping without disclaimers) or output schema mismatches

**First 3 experiments:**
1. **Establish Baseline (Movies Scenario):** Run a simple semantic filter (`AI.IF`) on text data to measure the "best-case" latency and quality for each system
2. **Stress Test Join (E-Commerce Q7):** Execute a semantic join to observe the "worst-case" cost explosion (quadratic LLM calls) and verify if systems utilize optimizations (embeddings/batching)
3. **Modality Check (Wildlife Scenario):** Run a filter on audio/image data to confirm multimodal ingestion pipelines are working and compare token costs against text-only queries

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** To what extent can automated prompt design strategies dynamically optimize the cost-quality trade-off in semantic query processing engines?
- **Basis in paper:** [explicit] The conclusion explicitly identifies "automated prompt design strategies, optimizing cost-quality tradeoffs" as an interesting extension currently missing from evaluated systems.
- **Why unresolved:** Current systems rely on static prompt templates or manual tuning, which cannot adapt to varying data domains (e.g., medical vs. general text) where model refusals or accuracy requirements differ.
- **What evidence would resolve it:** A system demonstration showing an optimizer that automatically rewrites prompts to prevent LLM refusals or improve accuracy while reducing token usage compared to static baselines.

**Open Question 2**
- **Question:** Does fusing multiple semantic operators (e.g., filter and join) into a single LLM call yield significant performance improvements over sequential execution?
- **Basis in paper:** [explicit] The conclusion states that "fusing multiple semantic operators into one single LLM call offers potential for performance improvements" in complex scenarios.
- **Why unresolved:** Current SQPEs typically evaluate operators sequentially, leading to redundant data processing and token generation.
- **What evidence would resolve it:** Benchmark results from a system implementing operator fusion showing reduced latency and monetary cost on the E-Commerce scenario queries (Q10–Q14) without loss in result quality.

**Open Question 3**
- **Question:** How can query optimizers effectively implement early termination for LIMIT clauses to minimize LLM invocations?
- **Basis in paper:** [inferred] Section 6.3 highlights that cost variations of up to 100× occur because some systems apply LIMIT as a post-processing step rather than terminating semantic evaluation early.
- **Why unresolved:** Integrating early termination requires balancing the probabilistic nature of LLM outputs with the deterministic requirements of query limits, which current systems handle inconsistently.
- **What evidence would resolve it:** An implementation where queries with LIMIT clauses (e.g., Movies Q1, Q5) consistently invoke LLMs only until the limit is satisfied, matching the lower cost bounds observed in the study.

## Limitations
- Evaluation focuses on a specific configuration (Gemini 2.5 Flash, temperature 0) that may not generalize to other model families or temperature settings
- Cost measurements depend on real-time API pricing, which can fluctuate significantly
- The benchmark's five scenarios, while diverse, may not capture all practical use cases for semantic query processing

## Confidence
- **High Confidence:** The observation that semantic operators introduce significant cost variability (up to 100×) across systems is well-supported by the experimental data across multiple queries and scenarios
- **Medium Confidence:** The effectiveness of embedding-based approximations for semantic joins is demonstrated but may vary significantly with join complexity and domain specificity
- **Medium Confidence:** The tradeoff between prompt complexity and cost-quality tradeoff is supported by the data, though the optimal balance likely depends heavily on specific task requirements

## Next Checks
1. **Cross-model validation:** Replicate key experiments using alternative LLM providers (e.g., GPT-4, Claude) to assess the generalizability of performance patterns beyond Gemini
2. **Edge case testing:** Design queries that specifically stress the boundaries of approximation techniques (e.g., complex negation in joins, rare class classification) to validate when embedding approximations break down
3. **Scale sensitivity analysis:** Systematically vary scale factors beyond those tested to identify inflection points where approximation strategies become necessary or where systems hit practical limits