---
ver: rpa2
title: 'Exploring a Large Language Model for Transforming Taxonomic Data into OWL:
  Lessons Learned and Implications for Ontology Development'
arxiv_id: '2504.18651'
source_url: https://arxiv.org/abs/2504.18651
tags:
- data
- species
- gbif
- https
- taxonomic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explored using ChatGPT-4 to automate the development
  of the Organism module in the Agricultural Product Types Ontology (APTO) for species
  classification. Two approaches were tested: using ChatGPT with the BrowserOP plugin
  to query the GBIF Backbone API directly, and directing ChatGPT to design a Python
  algorithm for analogous tasks.'
---

# Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development

## Quick Facts
- arXiv ID: 2504.18651
- Source URL: https://arxiv.org/abs/2504.18651
- Reference count: 40
- The study demonstrated that ChatGPT-4 can effectively generate Python scripts to automate the creation of taxonomic OWL hierarchies from species lists, overcoming scalability limitations of direct LLM-based approaches.

## Executive Summary
This paper investigates the use of Large Language Models, specifically ChatGPT-4, to automate the development of the Organism module in the Agricultural Product Types Ontology (APTO) by transforming scientific species names into structured OWL taxonomic hierarchies. Two distinct approaches were tested: using ChatGPT with a Browser plugin to directly query the GBIF Backbone API, and directing ChatGPT to design a Python algorithm for the task. While the direct approach showed severe scalability limitations and was prone to hallucinations, the Python-based approach proved superior, successfully handling 74 plant names in 4 minutes. The study highlights the potential of LLMs as tool builders for ontology development while underscoring the importance of robust data preprocessing and validation mechanisms.

## Method Summary
The study employed ChatGPT-4 to automate the transformation of a list of scientific species names into OWL taxonomic hierarchies. Two approaches were explored: (1) direct interaction with the GBIF Backbone API via a Browser plugin, and (2) generating a Python script ("Taxonomy OWLizer") to perform the same task. The Python script was iteratively refined through debugging sessions with ChatGPT. The final script queries the GBIF API, accumulates unique taxa, and generates OWL/XML files with classes as GBIF URIs and hierarchical relationships. Input data included 14 animal and 74 plant scientific names, with specific preprocessing applied to correct typographical errors such as capitalizing specific epithets.

## Key Results
- ChatGPT-4 successfully generated a Python script that processed 74 plant species names into a valid OWL hierarchy in 4 minutes.
- The direct ChatGPT-Browser approach was deprecated due to scalability issues, failing beyond 5 names and producing hallucinations.
- The generated OWL files correctly represented hierarchical relationships (e.g., Aves as a subclass of Animalia) and used GBIF URIs as class identifiers.
- Input data preprocessing (e.g., lowercasing specific epithets) was critical to prevent API mismatches and script failures.

## Why This Works (Mechanism)
The study leverages ChatGPT-4's ability to understand complex instructions and generate executable code, transforming it from a direct agent into a tool developer. By generating a Python script, the LLM offloads the iterative, data-intensive task of querying the GBIF API and constructing OWL hierarchies to a deterministic program. This approach bypasses the context limitations and hallucination risks of direct LLM interaction, enabling scalable processing of large species lists. The GBIF Backbone API serves as the authoritative source of taxonomic data, ensuring accurate synonym resolution and class uniqueness in the final OWL output.

## Foundational Learning
- **Web Ontology Language (OWL) & Taxonomies**
  - Why needed here: The task requires converting species names into a structured OWL hierarchy with `subClassOf` relationships to represent taxonomic levels (kingdom, phylum, etc.).
  - Quick check question: What does a `subClassOf` relationship between two taxa (e.g., `Aves` and `Animalia`) signify in the final OWL file?
- **APIs (Application Programming Interfaces)**
  - Why needed here: The entire workflow depends on querying the GBIF Backbone API for taxonomic data and synonym resolution.
  - Quick check question: If the GBIF API changes its URL structure or JSON keys, which part of the workflow would break first?
- **LLM as a Tool-Builder vs. a Tool-User**
  - Why needed here: The paper's key finding is that generating a Python script (LLM as developer) is more effective than direct LLM processing (LLM as agent) for scalability.
  - Quick check question: Why is an LLM-generated script better for processing 1,000 items than asking an LLM to process them one by one in a chat session?

## Architecture Onboarding
- **Component map:** User Prompt → ChatGPT-4 → Python Script (Taxonomy OWLizer) → GBIF Backbone API → OWL/XML Output
- **Critical path:** The GBIF Backbone Taxonomy API (`https://api.gbif.org/v1/species/match?name=...`) is the single source of truth. The core logic maps API JSON fields (`kingdomKey`, `phylumKey`, etc.) into a nested OWL class hierarchy with unique classes and `subClassOf` relationships.
- **Design tradeoffs:**
  - Iterative refinement vs. scalability: Approach 1 allows real-time debugging but fails at scale; Approach 2 sacrifices interactivity for speed and scalability.
  - Flexibility vs. robustness: Direct LLM interaction is flexible but prone to hallucinations; the generated script is deterministic but requires code changes for new requirements.
- **Failure signatures:**
  - LLM hallucination: Fabricated API endpoints or non-existent taxonomic keys.
  - Context overflow: LLM forgets earlier instructions as the session grows.
  - Data format errors: Typos in input species lists cause API failures or incorrect matches.
- **First 3 experiments:**
  1. Reproduce the API interaction: Query the GBIF species match API with a few species names and inspect the JSON structure.
  2. Reproduce Approach 1 (Single Species): Use an LLM to generate OWL for one species with explicit GBIF API instructions.
  3. Develop a mini-taxonomy OWLizer: Prompt an LLM to write a Python script for a small list (3-5 species) and print a basic hierarchical structure.

## Open Questions the Paper Calls Out
- How can an automated algorithm be developed to periodically verify and update species names within an ontology based on the latest GBIF Backbone Taxonomy? The authors note that future work should address the long-term maintenance of ontologies as taxonomies evolve.
- Can fully automated solutions for error detection and validation be effectively integrated to reduce manual intervention in LLM-driven ontology development? The current workflow still relies on human oversight for tasks like correcting typos and verifying OWL structures.
- How can hybrid species relationships be accurately modeled when input taxonomic databases exhibit inconsistent formatting for hybrid nomenclature? The study found that inconsistent use of the hybrid symbol ('x') in GBIF caused failures, requiring manual modeling of hybrid relationships.

## Limitations
- The direct ChatGPT-Browser approach is non-reproducible and deprecated due to scalability issues and hallucinations.
- The exact iterative prompts used to debug the Python script are summarized but not fully provided, limiting reproducibility of the specific prompt engineering process.
- The paper does not provide a comprehensive set of preprocessing rules to handle all edge cases (e.g., hybrid nomenclature) in advance.

## Confidence
- **Claim: "LLMs can automate ontology development for taxonomic data."** Confidence: High. The successful generation and execution of the "Taxonomy OWLizer" script provides strong evidence.
- **Claim: "Approach 2 (LLM as Developer) is superior to Approach 1 (LLM as Agent) for scalability."** Confidence: High. The paper demonstrates that the Python script approach handled 74 names in 4 minutes, while the direct approach failed at 5 names.
- **Claim: "The system correctly handles synonym resolution and class uniqueness."** Confidence: Medium. The validation method (checking in Protégé) is structural, but semantic correctness would require deeper validation against a gold standard.

## Next Checks
1. Validate the "Taxonomy OWLizer" script with a new dataset: Download the script from Zenodo and run it on a new list of 10-20 species names not used in the original paper. Verify the output OWL file in Protégé for correct hierarchical structure and absence of duplicate classes or fabricated URIs.
2. Stress-test the GBIF API integration: Design API queries to test the script's robustness against edge cases like hybrid names (e.g., *Triticum x Secale*), special characters, and multiple possible matches. Document behavior and failures.
3. Perform a side-by-side comparison with a traditional tool: Use a well-established ontology tool (e.g., ROBOT) to process the same original species list. Compare output OWL files for structural differences, processing time, and human intervention points.