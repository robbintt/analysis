---
ver: rpa2
title: Task Prototype-Based Knowledge Retrieval for Multi-Task Learning from Partially
  Annotated Data
arxiv_id: '2601.07474'
source_url: https://arxiv.org/abs/2601.07474
tags:
- task
- tasks
- knowledge
- learning
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-task learning (MTL)
  with partially annotated data, where obtaining labels for all tasks is impractical.
  The authors propose a prototype-based knowledge retrieval framework that captures
  task-specific characteristics and quantifies task associations through a task prototype,
  instead of relying on predictions from unlabeled tasks.
---

# Task Prototype-Based Knowledge Retrieval for Multi-Task Learning from Partially Annotated Data

## Quick Facts
- arXiv ID: 2601.07474
- Source URL: https://arxiv.org/abs/2601.07474
- Reference count: 7
- Multi-task learning with partial annotations, achieves 59.78% mIoU on PASCAL-Context and 45.95% mIoU on NYUD-v2 under one-label settings

## Executive Summary
This paper addresses multi-task learning with partially annotated data by proposing a prototype-based knowledge retrieval framework. Instead of relying on predictions from unlabeled tasks, the method captures task-specific characteristics through a learnable task prototype and quantifies task associations. A knowledge retrieval transformer then adaptively refines feature representations using these associations. Experiments demonstrate state-of-the-art performance on PASCAL-Context and NYUD-v2 datasets under various annotation scenarios.

## Method Summary
The proposed framework maintains a learnable task prototype with T slots (one per task) and uses task-specific features to compute affinity scores via cosine similarity. The Task Knowledge Embedding (TKE) loss trains the prototype so that features from task t yield highest affinity for slot v_t. Cross-attention with task-affinity features enables adaptive feature refinement, while Task Consistency (TC) loss ensures prototype slots maintain distinct task characteristics. The method integrates vector quantization to expand the shared feature space and handles partially labeled data through task-specific loss computation.

## Key Results
- Achieves 59.78% mIoU on PASCAL-Context and 45.95% mIoU on NYUD-v2 under one-label settings
- Outperforms existing methods in multi-task partially supervised learning scenarios
- Demonstrates effectiveness across semantic segmentation, depth estimation, surface normal prediction, and other dense prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Task prototypes enable reliable cross-task knowledge transfer without requiring predictions from unlabeled tasks. **Mechanism:** The framework maintains a learnable task prototype V with T slots. Task-specific features are projected and compared to prototype slots via cosine similarity, producing task-affinity scores. The TKE loss trains the prototype so features from task t yield highest affinity for slot v_t. **Core assumption:** Task-specific features contain discriminative patterns that can be consistently mapped to prototype slots even when labels are unavailable. **Evidence anchors:** Abstract statement about capturing task characteristics without pseudo-labels; Eq. 4 showing TKE loss formulation; limited direct validation in corpus. **Break condition:** If task-specific features become too noisy, prototype affinity scores may become unreliable.

### Mechanism 2
**Claim:** Cross-attention with task-affinity features enables adaptive, task-aware feature refinement. **Mechanism:** Task-affinity scores are combined with the prototype via matrix multiplication to create task-affinity features, which serve as key/value in cross-attention. This retrieves and integrates prototype knowledge weighted by task relevance. **Core assumption:** Task-affinity features encode meaningful task relationships that generalize across samples. **Evidence anchors:** Abstract reference to knowledge retrieval transformer; Eq. 7-9 showing cross-attention formulation; partial support from neighbor work on attention-based feature refinement. **Break condition:** If affinity scores are miscalibrated, cross-attention receives uninformative guidance.

### Mechanism 3
**Claim:** Task Consistency (TC) loss ensures prototype slots maintain distinct, stable task characteristics. **Mechanism:** TC loss aggregates task-specific features across a batch and enforces that features from task t have higher similarity to the aggregate than to aggregates from other tasks. **Core assumption:** Task characteristics are intrinsically distinguishable and should remain separated in prototype space. **Evidence anchors:** Eq. 5 showing TC loss formulation; Figure 4 visualizing attraction-repulsion dynamics; no direct corpus validation. **Break condition:** If tasks share substantial characteristics, forced separation may hinder beneficial knowledge sharing.

## Foundational Learning

- **Concept: Vector Quantization (VQ) for Feature Discretization**
  - **Why needed here:** VQ maps encoded features to a learnable codebook, expanding the shared feature space to hold diverse task cues when labels are sparse.
  - **Quick check question:** Can you explain how VQ maps continuous features to discrete codebook entries and why this helps with partially labeled data?

- **Concept: Cross-Attention in Transformers**
  - **Why needed here:** The knowledge retrieval transformer uses cross-attention to integrate task-affinity features (key/value) with task-specific features (query), enabling selective knowledge retrieval.
  - **Quick check question:** How does cross-attention differ from self-attention, and what role do Q, K, V play in determining which information is retrieved?

- **Concept: Multi-Task Partially Supervised Learning (MTPSL)**
  - **Why needed here:** This is the core problem setting—training on datasets where each sample has labels for only a subset of tasks.
  - **Quick check question:** Why does relying on pseudo-labels for unlabeled tasks risk negative transfer, and how does the prototype approach mitigate this?

## Architecture Onboarding

- **Component map:** Backbone (ResNet-18) -> Encoded feature f_e -> Vector Quantization -> Codebook Z -> Quantized feature f_q -> Integrated feature f_i -> Task-Specific Decoder -> Task-specific features f_t for each task -> Task Prototype V -> Task-affinity scores A(f̂_t, V) -> Knowledge Retrieval Transformer -> Refined features f_tr -> Task Heads -> Final predictions

- **Critical path:** 1) Image -> Backbone -> f_e 2) f_e -> VQ (TAE loss for reconstruction) -> f_i 3) f_i -> Task decoder -> f_t 4) f_t -> Task prototype -> Affinity scores -> f_ta 5) f_t + f_ta -> Cross-attention -> f_tr -> Task head -> Prediction 6) Only labeled tasks contribute to supervised L_MTL

- **Design tradeoffs:** Prototype slot dimension: 1024 is optimal; smaller dims underfit, larger dims struggle with information utilization. Codebook size K: Larger K expands shared representation capacity but increases memory/compute. Number of tasks T: Must match dataset; prototype doesn't generalize to unseen tasks.

- **Failure signatures:** Uniform affinity scores indicate prototype not learning discriminative task characteristics; check TKE loss convergence. Degraded performance on specific tasks may indicate TC loss coefficient α too aggressive. No improvement over MTL baseline suggests vector quantization not functioning.

- **First 3 experiments:** 1) Ablation on AKG loss components: Train with L_tke only vs. L_tke + L_tc to verify TC loss contribution. 2) Prototype dimensionality sweep: Test slot dimensions {256, 512, 1024, 2048} to find dataset-specific optimum. 3) Label regime comparison: Evaluate under one-label vs. random-label settings to confirm robustness to annotation sparsity.

## Open Questions the Paper Calls Out
- How can the fixed-size task prototype framework be extended to accommodate novel tasks unseen during training (e.g., in a zero-shot or meta-learning setting)? [explicit] The authors explicitly state in the "Limitations" section: "Extending this framework to unseen tasks in a zero-shot or meta-learning remains an open challenge."
- Does the Task Consistency (TC) loss, which enforces separation between task-specific characteristics, negatively impact performance on tasks that rely heavily on shared features? [inferred] The TC loss explicitly maximizes the distance between target task features and other task prototypes, but this may inhibit positive transfer from shared features.
- How does the dimensionality of the task prototype slots scale with the complexity and quantity of tasks, and is there a theoretical upper bound? [inferred] The authors ablate the slot dimension finding 1024 optimal for NYUD-v2 (3 tasks), but provide no theoretical justification and test only on datasets with 3-5 tasks.

## Limitations
- Loss hyperparameters (λ₁, λ₂, α) are unspecified, requiring tuning for reproduction
- The method assumes task prototypes can be learned effectively from limited labeled samples per task
- Zero-shot extension to unseen tasks is not addressed, limiting practical deployment flexibility

## Confidence

**High confidence** in prototype-based affinity scoring mechanism and its theoretical soundness
**Medium confidence** in cross-attention refinement, supported by neighbor work on attention-based feature integration
**Medium confidence** in TC loss effectiveness, though direct validation is limited

## Next Checks

1. Perform ablation study on prototype dimensionality to identify optimal slot size for each dataset
2. Evaluate under extreme label scarcity (e.g., 10% task coverage) to stress-test robustness
3. Compare against distillation-based MTL approaches to quantify relative gains from prototype-based retrieval