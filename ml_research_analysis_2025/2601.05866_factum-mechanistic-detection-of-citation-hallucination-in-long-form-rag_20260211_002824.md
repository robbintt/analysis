---
ver: rpa2
title: 'FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG'
arxiv_id: '2601.05866'
source_url: https://arxiv.org/abs/2601.05866
tags:
- citation
- https
- hallucination
- factum
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical problem of citation hallucination
  in retrieval-augmented generation (RAG) systems, where models misattribute information
  to incorrect or fabricated sources. The authors introduce FACTUM, a mechanistic
  framework that analyzes the internal states of transformer models to detect such
  hallucinations by examining the coordination between the Attention (reading) and
  Feed-Forward Network (recalling) pathways.
---

# FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG

## Quick Facts
- arXiv ID: 2601.05866
- Source URL: https://arxiv.org/abs/2601.05866
- Reference count: 40
- Primary result: Up to 37.5% improvement in AUC for citation hallucination detection compared to state-of-the-art baselines

## Executive Summary
This paper addresses citation hallucination in retrieval-augmented generation (RAG) systems, where models misattribute information to incorrect or fabricated sources. The authors introduce FACTUM, a mechanistic framework that analyzes transformer internal states to detect such hallucinations by examining coordination between attention (reading) and FFN (recalling) pathways. The framework achieves significant performance improvements over existing methods while providing interpretable insights into how different model scales handle citation generation differently.

## Method Summary
FACTUM analyzes the residual stream of transformer models to quantify the independent contributions of attention and FFN pathways during citation generation. The method computes four mechanistic scores at citation tokens: Contextual Alignment (CAS) measures attention-weighted context similarity to citation token hidden state; Beginning-of-Sentence Attention Usage (BAS) tracks attention to the `<s>` token as a synthesis indicator; Parametric Force (PFS) measures FFN update magnitude as a knowledge application indicator; and Pathway Alignment (PAS) measures cosine similarity between attention and FFN update vectors. These scores are pruned by correlation with labels, aggregated across heads, and fed to classifiers (Logistic Regression, LightGBM, or EBM) with 10-fold stratified cross-validation at the report level.

## Key Results
- FACTUM achieves up to 37.5% improvement in AUC for citation hallucination detection compared to state-of-the-art baselines
- On NeuCLIR 2024 dataset, FACTUM achieves AUC of 0.737 with Logistic Regression versus 0.613 for best baseline (ECS+PKS)
- Precision improves from 0.201 to 0.334 for 8B model, representing 66% relative improvement over baseline
- The signature of correctness evolves with model scale, shifting from pathway agreement in smaller models to specialized orthogonal contributions in larger models

## Why This Works (Mechanism)

### Mechanism 1: Pathway Decoupling via Residual Stream Analysis
Citation hallucination can be detected by measuring the independent contributions of attention (contextual reading) and FFN (parametric recall) pathways to the residual stream. The transformer's residual stream acts as a linear communication channel where each sub-layer performs additive updates: `x_i^(l) = x_i^(l-1) + v_attn,i^(l) + v_ffn,i^(l)`. By isolating these update vectors, FACTUM quantifies pathway-specific activity at citation tokens without relying on noisy proxies like LogitLens. The core assumption is that residual stream update vectors faithfully represent semantic contributions of each pathway.

### Mechanism 2: Parametric Force as Constructive Corroboration
High FFN pathway activity (parametric force) during citation generation correlates with correct attribution, contradicting prior theories that parametric knowledge drives hallucination. The Parametric Force Score (PFS) measures the L2 norm of the FFN update vector. A larger norm indicates the model is confidently applying stored knowledge. Correct citations exhibit higher PFS, suggesting the model cross-validates contextual evidence against parametric memory. The core assumption is that FFN updates during citation generation primarily reflect factual knowledge retrieval rather than syntactic or structural processing.

### Mechanism 3: Attention Sink Usage for Synthesis Stability
Higher attention to the beginning-of-sentence token (BAS) during citation generation indicates constructive information synthesis and correlates with correct attribution. The `<s>` token functions as an attention sink that manages information flow in long contexts, preventing representational collapse. BAS measures the proportion of attention allocated to this sink. Higher BAS suggests the model is integrating retrieved information before committing to a citation. The core assumption is that attention sink usage during citation generation reflects synthesis behavior rather than a default attention pattern unrelated to citation quality.

## Foundational Learning

- **Concept: Residual Stream Semantics**
  - **Why needed here:** FACTUM's core measurements depend on interpreting additive updates to the residual stream as semantically meaningful pathway contributions. Without this foundation, PFS and PAS are uninterpretable metrics.
  - **Quick check question:** If you doubled all FFN weights in a layer, would PFS increase? Would citation detection performance change? (Answer: PFS would increase, but if the semantic function is preserved via learned normalization, detection might not change—revealing a potential confound.)

- **Concept: Attention Sink Function in Long Contexts**
  - **Why needed here:** BAS assumes the `<s>` token serves a functional role in information synthesis. Understanding why transformers attend to the first token is essential to interpreting BAS correctly.
  - **Quick check question:** In a 100k-token context, what happens to attention patterns if you remove the first token? (Answer: Attention sink theory predicts representational degradation or redistributed attention—empirical verification is ongoing.)

- **Concept: Scale-Dependent Mechanistic Signatures**
  - **Why needed here:** FACTUM's best detector for the 8B model uses only 25% of components and identifies orthogonal pathway contributions as correct, while the 3B model uses all components and favors pathway alignment. Deploying FACTUM requires recognizing that optimal detection strategies are not portable across scales.
  - **Quick check question:** If you train a FACTUM classifier on a 3B model and apply it to an 8B model without retraining, what performance change would you expect? (Answer: Degraded performance due to signature shift from alignment to orthogonality.)

## Architecture Onboarding

- **Component map:**
  - Input documents and query → Llama model inference → citation token generation → intermediate states extraction → mechanistic scores computation (CAS, BAS, PFS, PAS) → component pruning → feature aggregation → classifier → hallucination prediction

- **Critical path:** The most sensitive step is component pruning with hyperparameter `k`. The paper reports optimal `k=100%` for 3B and `k=25%` for 8B. Incorrect `k` values will either include noise (low precision) or discard signal (low recall). This must be tuned per model scale using held-out training data.

- **Design tradeoffs:**
  - Token-level vs. span-level detection: FACTUM operates on citation digit tokens (e.g., "1" in "[Source: 1]"). This is precise but may miss contextual signals from surrounding tokens. Aggregation strategies for multi-source citations are underspecified.
  - Direct pathway scores vs. LogitLens proxies: FACTUM avoids LogitLens due to unreliable vocabulary-space projection, but this requires access to intermediate residual stream states, which may not be available in all inference APIs.
  - Classifier choice: Logistic Regression achieved highest AUC (0.737) with minimal features (4), while EBM provides interpretability. Production deployment must balance performance against explainability requirements.

- **Failure signatures:**
  - Low CAS + High PFS: Model is applying parametric knowledge without contextual grounding—classic attributional drift
  - High PAS in 8B model: May indicate redundant pathway contributions rather than specialized processing—potential false positive for correctness in larger models
  - Unstable BAS across layers: If attention sink usage is inconsistent, synthesis may be incomplete, increasing hallucination risk

- **First 3 experiments:**
  1. Baseline replication on new model family: Re-implement CAS, BAS, PFS, PAS for a different architecture (e.g., Mistral or Qwen) to test whether the scale-dependent signature shift generalizes beyond Llama
  2. Ablation of individual scores: Train classifiers with each score independently (CAS-only, PFS-only, etc.) to quantify marginal contribution
  3. Cross-domain validation: Apply FACTUM to a dataset with different citation patterns (e.g., scientific papers with author-year citations) to test robustness to citation format changes

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the shift from pathway agreement (high PAS) to specialized orthogonal contributions (low PAS) as a signature of correct citations continue monotonically with model scale beyond 8B parameters? The study only evaluates 3B and 8B Llama models, leaving the trajectory at larger scales unexplored.

- **Open Question 2:** Are the mechanistic signatures of citation hallucination generalizable across transformer architectures beyond the Llama family? Different architectures may organize attention and FFN pathways differently, and the four mechanistic scores may capture different signals in models with distinct training procedures.

- **Open Question 3:** What determines the optimal component pruning percentage (k) for different model scales, and can it be predicted a priori? The paper treats k as a hyperparameter and finds optimal k=100% for 3B but k=25% for 8B, but the mechanism driving this shift remains unexplained.

## Limitations

- The method requires access to intermediate transformer states (residual streams, hidden states), which may not be available in standard API deployments
- Reported improvements are measured on a single dataset (NeuCLIR 2024) with specific model architectures, raising questions about generalizability
- The mechanistic explanations, while theoretically grounded, lack direct experimental validation for why specific scores predict hallucination

## Confidence

- **High Confidence:** The overall experimental methodology (10-fold stratified CV, LLM-as-judge labeling, comparative baseline evaluation) is sound and well-documented
- **Medium Confidence:** The scale-dependent signature shift (orthogonal pathways in 8B vs aligned pathways in 3B) is well-observed in the data, but the mechanistic interpretation requires additional validation
- **Low Confidence:** The specific mechanistic explanations for why each score works (e.g., that BAS specifically reflects information synthesis rather than generic attention patterns) remain speculative without targeted ablation studies

## Next Checks

1. **Cross-architecture generalization test:** Apply FACTUM to a different model family (e.g., Mistral or Qwen) with similar parameter counts to verify whether the scale-dependent signature shift generalizes beyond Llama models

2. **Ablation of FFN functionality:** Conduct targeted experiments to verify what FFN updates during citation generation actually represent (factual knowledge vs. syntactic processing vs. token statistics) by modifying FFN behavior and observing score changes

3. **API deployment simulation:** Test FACTUM's practical utility by implementing it with only accessible model outputs (e.g., using LogitLens or other proxy methods) to understand the real-world performance degradation when intermediate states are unavailable