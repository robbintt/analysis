---
ver: rpa2
title: LLM-driven Constrained Copy Generation through Iterative Refinement
arxiv_id: '2504.10391'
source_url: https://arxiv.org/abs/2504.10391
tags:
- copy
- generation
- copies
- constraints
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating marketing copy
  for e-commerce banners that must satisfy multiple intricate constraints simultaneously.
  The proposed solution is an LLM-based framework that employs iterative refinement,
  consisting of a generator, an evaluator block, and a refiner.
---

# LLM-driven Constrained Copy Generation through Iterative Refinement

## Quick Facts
- arXiv ID: 2504.10391
- Source URL: https://arxiv.org/abs/2504.10391
- Reference count: 29
- Primary result: Iterative refinement increases success rates by 16.25–35.91% compared to base LLM for constrained marketing copy generation

## Executive Summary
This work addresses the challenge of generating marketing copy for e-commerce banners that must satisfy multiple intricate constraints simultaneously. The proposed solution is an LLM-based framework that employs iterative refinement, consisting of a generator, an evaluator block, and a refiner. The generator creates initial copies, evaluators assess them against various constraints (e.g., length, tone, keywords, topics, coherence, lexical ordering), and the refiner iteratively revises copies that fail evaluations based on specific feedback. Experiments on three use cases of varying complexity show that iterative refinement increases success rates by 16.25–35.91% compared to the base LLM. Pilot studies demonstrate that generated copies outperform manual content, achieving 38.5–45.21% higher click-through rates.

## Method Summary
The framework employs an iterative refinement approach with three main components: a generator that creates initial copies using LLM with context-specific prompts and few-shot examples, an evaluator block that assesses copies against deterministic and LLM-based constraints (length, tone, coherence, topic inclusion), and a refiner that iteratively revises failed copies based on specific evaluation feedback. The system operates in batches (m=10-20 copies) with up to J=1-2 refinement attempts per copy. A formatter component applies rule-based cleanup before evaluation. The approach is tested across three use cases with varying constraint complexity, demonstrating significant improvements in constraint satisfaction compared to baseline generation.

## Key Results
- Iterative refinement increases success rates by 16.25–35.91% compared to base LLM generation
- Generated copies achieve 38.5–45.21% higher click-through rates than manual content in pilot studies
- Length constraint violations represent the dominant failure mode (72.16% for Campaign-B)

## Why This Works (Mechanism)
The framework works by breaking down the complex multi-constraint generation problem into manageable sub-tasks through iterative refinement. Instead of requiring the generator to satisfy all constraints in a single pass, the system allows for progressive improvement through targeted feedback. The evaluator block provides structured, constraint-specific feedback that the refiner uses to make focused adjustments, avoiding the combinatorial explosion of trying to satisfy all constraints simultaneously. This decomposition makes the problem tractable for LLMs while maintaining copy quality through the iterative process.

## Foundational Learning
- **Iterative refinement architecture**: Generator → Evaluator → Refiner loop with structured feedback; needed to handle multiple complex constraints systematically; quick check: trace single copy through all three stages
- **Constraint-specific evaluation**: Separate evaluators for length, tone, coherence, and topic inclusion; needed because different constraint types require different assessment strategies; quick check: verify evaluator outputs for a failing copy
- **Chain-of-thought prompting**: Evaluators use "Think step-by-step" reasoning; needed for reliable multi-faceted constraint assessment; quick check: compare evaluator outputs with and without CoT prompting
- **Batch processing with m=10-20**: Multiple copies generated simultaneously to increase probability of success; needed to amortize generation cost across refinement attempts; quick check: measure success rate improvement with batch size
- **Formatter rule-based cleanup**: "and"→"&", remove serial commas, bracket fixes; needed to standardize output before evaluation; quick check: verify formatter handles edge cases correctly
- **Few-shot in-context learning**: Generator uses examples in prompt; needed to align output with brand guidelines and use case requirements; quick check: test generator with different example sets

## Architecture Onboarding

**Component map**: Generator → Formatter → Evaluator Block → Refiner (back to Generator)

**Critical path**: Generator batch → Formatter cleanup → Evaluator block assessment → If fail → Refiner with feedback → Reformat → Re-evaluate (loop until pass or J attempts exhausted)

**Design tradeoffs**: Batch size m=10-20 balances generation cost against success probability; max J=1-2 refinement attempts balances quality improvement against diminishing returns and cost; separate deterministic and LLM evaluators optimizes for speed vs. nuance

**Failure signatures**: Length violations (72.16% for short headers), tone violations (hyperbolic language), coherence failures (illogical connections), topic exclusion failures; each produces specific evaluator feedback for refiner targeting

**First experiments**:
1. Generate 20 copies with base LLM, measure baseline success rate without refinement
2. Run single refinement iteration on failing copies, measure delta in success rate
3. Test refiner with synthetic feedback on known-bad copies to verify feedback interpretation

## Open Questions the Paper Calls Out

**Open Question 1**: How can the framework automatically adapt to "criteria drift," where evaluation standards evolve as human reviewers identify new types of errors in generated outputs? The paper relies on human intervention to update evaluators but does not propose a mechanism for dynamic constraint integration from rejection logs.

**Open Question 2**: Does the iterative refinement framework remain effective when deployed on smaller, open-source language models, or is it dependent on high-capacity models? The framework's reliance on LLMs for nuanced discrimination is not tested across model sizes.

**Open Question 3**: Is there an upper bound to the number of conflicting constraints the refiner can handle before the success rate plateaus or degrades? The paper tests three use cases but does not systematically increase constraint density to find the failure point.

## Limitations

- Exact prompt templates and few-shot examples are not specified, making faithful reproduction challenging
- 38.5-45.21% CTR improvement claim lacks detailed experimental methodology and statistical significance testing
- Cost-benefit analysis of multiple LLM calls per copy versus manual copywriting is not addressed
- Performance on constraints beyond the three tested use cases is unknown

## Confidence

**High confidence**: The core iterative refinement architecture is technically sound and the 16.25-35.91% improvement in success rate is internally consistent with reported failure modes.

**Medium confidence**: The CTR improvement claim lacks sufficient methodological detail for independent verification, though the direction of improvement is plausible.

**Low confidence**: The generalizability of results to other e-commerce domains or constraint types is unclear without testing on additional use cases.

## Next Checks

1. **Ablation study on refinement iterations**: Systematically measure success rate and copy quality degradation when varying J from 0 to 3 refinement attempts to identify optimal trade-off.

2. **Evaluator reliability testing**: Conduct inter-rater reliability analysis comparing LLM-based evaluator judgments against human annotators on 100+ copy samples to quantify agreement rates.

3. **Cost-performance benchmarking**: Calculate cost per accepted copy (including all generator and refiner calls) and compare against manual copywriting time and cost for equivalent constraint satisfaction rates.