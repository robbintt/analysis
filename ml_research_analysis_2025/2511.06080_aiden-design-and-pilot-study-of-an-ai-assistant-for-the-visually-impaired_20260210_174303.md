---
ver: rpa2
title: 'AIDEN: Design and Pilot Study of an AI Assistant for the Visually Impaired'
arxiv_id: '2511.06080'
source_url: https://arxiv.org/abs/2511.06080
tags:
- system
- user
- object
- aiden
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AIDEN is an AI-based assistant designed to enhance the autonomy
  of visually impaired individuals by addressing limitations of existing audio-centric
  solutions such as auditory overload and privacy concerns. The system integrates
  real-time object detection (YOLO) with a large language and vision assistant (LLaVA)
  for scene description and OCR, and employs a continuous haptic guidance mechanism
  based on a Geiger-counter metaphor to avoid auditory masking.
---

# AIDEN: Design and Pilot Study of an AI Assistant for the Visually Impaired

## Quick Facts
- **arXiv ID:** 2511.06080
- **Source URL:** https://arxiv.org/abs/2511.06080
- **Reference count:** 35
- **One-line primary result:** AIDEN is an AI-based assistant designed to enhance the autonomy of visually impaired individuals by addressing limitations of existing audio-centric solutions such as auditory overload and privacy concerns.

## Executive Summary
AIDEN is an AI-based assistant designed to enhance the autonomy of visually impaired individuals by addressing limitations of existing audio-centric solutions such as auditory overload and privacy concerns. The system integrates real-time object detection (YOLO) with a large language and vision assistant (LLaVA) for scene description and OCR, and employs a continuous haptic guidance mechanism based on a Geiger-counter metaphor to avoid auditory masking. A pilot study with seven visually impaired participants demonstrated high user satisfaction, particularly regarding intuitiveness and perceived autonomy, with a strong correlation between Attitude Towards Using and Behavioral Intention to adopt the system. Technical evaluation showed effective real-time performance, with "Find an Object" achieving 1.96 frames per second.

## Method Summary
The AIDEN system uses a client-server architecture with an Ionic/Vue mobile frontend and a Python backend. The server runs YOLOv8 (extended with a custom "door" class) for object detection and LLaVA for visual question answering and OCR. A Geiger-counter metaphor guides haptic feedback, with vibration frequency increasing as objects approach the screen center. The pilot study used a pre-test/post-test design with TAM questionnaires, comparing AIDEN against a standard screen reader.

## Key Results
- High user satisfaction with AIDEN, particularly for intuitiveness and perceived autonomy
- Strong correlation between Attitude Towards Using (ATT) and Behavioral Intention (BI) to adopt the system
- Technical performance of 1.96 FPS for object detection in "Find an Object" mode

## Why This Works (Mechanism)
The system addresses auditory overload and privacy concerns common in existing audio-centric solutions by integrating haptic feedback for navigation guidance. The Geiger-counter metaphor provides intuitive distance feedback without masking environmental sounds. The hybrid approach combining YOLO for detection with LLaVA for VQA/OCR leverages specialized models for different tasks while maintaining a unified interface.

## Foundational Learning
- **Geiger-counter metaphor for haptic guidance:** Uses vibration frequency to indicate distance to objects, avoiding auditory masking. Quick check: Test vibration patterns at different distances from screen center.
- **YOLOv8 object detection:** Real-time bounding box detection for "Find an Object" functionality. Quick check: Verify frame rate and accuracy on test images.
- **LLaVA for VQA/OCR:** Large language-vision model for scene description and text recognition. Quick check: Test transcription accuracy on varied text samples.
- **TAM questionnaire structure:** Technology Acceptance Model used to measure user acceptance through perceived usefulness, ease of use, and behavioral intention. Quick check: Validate questionnaire scoring against pilot data.
- **Client-server architecture with REST API:** Enables offloading computation to powerful servers while maintaining mobile accessibility. Quick check: Test API response times with different image resolutions.
- **Ionic/Vue mobile framework:** Cross-platform mobile development with accessibility features. Quick check: Verify screen reader compatibility and gesture navigation.

## Architecture Onboarding

**Component Map:** Mobile Client -> REST API -> YOLOv8 Server -> LLaVA Server

**Critical Path:** Image capture → Server inference → Haptic feedback loop for "Find an Object" mode

**Design Tradeoffs:** The system prioritizes real-time performance and user autonomy over simultaneous multi-modal feedback, as "Find an Object" and VQA cannot currently run concurrently. The Geiger-counter metaphor trades precise distance information for intuitive, non-intrusive guidance.

**Failure Signatures:** Latency exceeding 1 FPS breaks the haptic feedback illusion. Network disruptions halt all functionality. VLLM hallucinations produce potentially dangerous misinformation without confidence scoring.

**First Experiments:**
1. Deploy the REST API locally and test YOLOv8 inference times on sample images
2. Implement the distance-to-frequency mapping in the mobile app and verify vibration patterns
3. Run the complete "Find an Object" pipeline with a single test image and measure end-to-end latency

## Open Questions the Paper Calls Out
1. Does user acceptance and system reliability remain robust during longitudinal "beta testing" in uncontrolled, real-world environments?
2. Can confidence scoring layers effectively mitigate the safety risks of VLLM hallucinations without negatively impacting the user experience?
3. Can Edge AI capabilities be integrated to maintain the system's real-time performance without relying on remote servers?
4. Does the strong correlation between Attitude Towards Using (ATT) and Behavioral Intention (BI) persist in a larger, more demographically diverse cohort?

## Limitations
- Limited generalizability from small pilot study (N=7) to broader populations of visually impaired users
- Current architecture prevents simultaneous operation of "Find an Object" and VQA functionalities
- System requires active internet connection and relies on specific server hardware for real-time performance

## Confidence

**High Confidence:** The core technical architecture (client-server model with YOLOv8 and LLaVA) and basic functionality of individual components (object detection, OCR, haptic feedback) are well-specified and reproducible.

**Medium Confidence:** The reported performance metrics (1.96 FPS, 0.51s total latency) and user satisfaction scores from the pilot study, given the limited sample size and specific testing conditions.

**Low Confidence:** The system's effectiveness in diverse real-world environments and the long-term usability and adoption rates among visually impaired users.

## Next Checks
1. **Performance Scaling Test:** Reproduce the system on different hardware configurations (e.g., mobile GPUs, cloud inference) to verify the 1.96 FPS claim and identify performance bottlenecks across various devices.
2. **Expanded User Study:** Conduct a larger-scale user study (n≥30) across diverse environments and visual impairment types to validate TAM scores and assess real-world usability of the haptic guidance system.
3. **Simultaneous Functionality Test:** Implement and evaluate the proposed parallel processing architecture to enable concurrent "Find an Object" and VQA operations, measuring the impact on latency and user experience.