---
ver: rpa2
title: 'Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary
  Search Approach'
arxiv_id: '2506.18756'
source_url: https://arxiv.org/abs/2506.18756
tags:
- attack
- search
- agbs
- adversarial
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Adaptive Greedy Binary Search (AGBS) method
  for generating semantic-preserving adversarial attacks on large language models
  (LLMs). AGBS dynamically adjusts keyword replacement candidates using semantic similarity
  thresholds to maintain input intent while inducing model misbehavior.
---

# Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach

## Quick Facts
- arXiv ID: 2506.18756
- Source URL: https://arxiv.org/abs/2506.18756
- Reference count: 40
- Primary result: AGBS achieves up to 94% attack success rate on numerical QA datasets while maintaining BERTScore 0.80 semantic preservation

## Executive Summary
This paper introduces Adaptive Greedy Binary Search (AGBS), a method for generating semantic-preserving adversarial attacks on large language models. AGBS dynamically adjusts keyword replacement candidates using semantic similarity thresholds to maintain input intent while inducing model misbehavior. Tested across 2400 cases on multiple open and closed-source LLMs including GPT-4, Llama, Qwen, and Gemma, AGBS achieved attack success rates up to 94% on numerical QA datasets like GSM8K while maintaining average BERTScore of 0.80 for semantic preservation.

## Method Summary
AGBS identifies keywords via POS tagging, masks them, and replaces them with candidates from a BERT-predicted distribution. The algorithm starts from the middle-ranked candidate and uses semantic similarity (BERTScore) to guide a binary search adjustment - if similarity is below threshold, it moves to more similar candidates; if above, it moves to less similar ones. This dynamic adjustment targets a similarity "sweet spot" that preserves meaning while maximizing perturbation effectiveness. The method uses bert-large-uncased for candidate generation and similarity computation, with a default threshold of 0.80 and search scope of 13,000 candidates.

## Key Results
- Achieved up to 94% attack success rate on GSM8K numerical reasoning dataset
- Maintained average BERTScore of 0.80 across all attacks, demonstrating semantic preservation
- Outperformed classical adversarial attack baselines (OET, GCG, Fun-tuning) across multiple model families
- Showed optimal performance at 13,000 candidate search scope with dynamic strategy achieving 90.91% ASR on GPT-4-turbo vs. 36.36% for static strategy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing keywords with semantically similar alternatives can preserve human-perceived meaning while altering LLM outputs.
- **Mechanism:** AGBS identifies keywords via POS tagging, masks them, and replaces them with BERT-predicted candidates maintaining sufficient semantic overlap (BERTScore ≥0.80) to avoid detection while exploiting LLM sensitivity to lexical choices.
- **Core assumption:** LLMs exhibit disproportionate sensitivity to certain keywords in reasoning tasks.
- **Break condition:** If target LLM's decision boundary is robust to synonym substitution, attack success rates would drop significantly.

### Mechanism 2
- **Claim:** Dynamic candidate selection via similarity-guided binary search improves attack success rates over static beam search.
- **Mechanism:** Starting from middle-ranked candidate, the algorithm computes semantic similarity and adjusts candidate position (±s steps) based on similarity threshold, targeting an optimal similarity "sweet spot."
- **Core assumption:** An optimal similarity threshold exists where semantic drift is minimized but attack effectiveness is maximized.
- **Break condition:** If similarity scores are noisy or uncorrelated with attack effectiveness, the binary search would oscillate without converging.

### Mechanism 3
- **Claim:** Numerical reasoning tasks are more vulnerable to semantic-preserving attacks than text-based QA tasks.
- **Mechanism:** Numerical QA requires precise reasoning chains; subtle word substitutions may disrupt semantic parsing that feeds into arithmetic reasoning without changing surface meaning.
- **Core assumption:** LLM reasoning on numerical tasks involves fragile intermediate representations sensitive to specific lexical choices.
- **Break condition:** If numerical reasoning models are explicitly trained on paraphrased examples, the vulnerability gap would narrow.

## Foundational Learning

- **Concept:** Adversarial attacks in NLP
  - **Why needed here:** AGBS is fundamentally an adversarial attack method; understanding the threat model (black-box, semantic constraints) is prerequisite to understanding why this approach differs from gradient-based white-box attacks.
  - **Quick check question:** Why can't gradient-based attacks like GCG be applied directly to closed-source models like GPT-4?

- **Concept:** Semantic similarity metrics (BERTScore, cosine similarity)
  - **Why needed here:** The entire AGBS algorithm relies on computing similarity between original and perturbed text to maintain semantic preservation.
  - **Quick check question:** If BERTScore is 0.80, what does this quantitatively mean about token-level overlap between two sentences?

- **Concept:** Beam search and its variants
  - **Why needed here:** AGBS modifies standard beam search by dynamically adjusting which candidate in the Top-k list is selected based on similarity feedback.
  - **Quick check question:** In standard beam search with k=5, how are candidates ranked and pruned at each step?

## Architecture Onboarding

- **Component map:** POS Extractor → BERT Masked Language Model → Similarity Computer → Adaptive Selector → Attack Executor
- **Critical path:** Original question → POS tagging → Keyword masking → BERT candidate generation → Initial middle candidate selection → For each sub-clause: compute similarity → adjust candidate rank → replace keyword → submit to target LLM → check if answer changed
- **Design tradeoffs:**
  - Search scope (k=13,000): Larger scope increases candidate diversity and attack success but raises computational cost
  - Similarity threshold (σ=0.80): Higher threshold preserves meaning better but may limit attack success
  - Step size (s=50): Larger steps accelerate search but may overshoot optimal candidate
- **Failure signatures:**
  1. Low ASR on large models (>70B parameters) - expected per paper
  2. High semantic drift - if BERTScore falls below 0.70
  3. Static strategy underperformance - ASR drops by 40-60% if dynamic adjustment is disabled
- **First 3 experiments:**
  1. Reproduce dynamic vs. static ablation on Llama-3.2-1B with GSM8K subset to validate adaptive selection gains
  2. Hyperparameter sweep on σ (0.6, 0.7, 0.8, 0.9) with fixed search scope to understand sensitivity
  3. Cross-model transferability test: generate on Llama-3.1-8B and test on GPT-4o without regeneration

## Open Questions the Paper Calls Out

- **Multi-modal and multi-turn adaptation:** The current study is restricted to numerical and text response QA and does not cover VQA or multi-round QA scenarios.
- **Large model robustness:** Models scaled to >20B parameters show "obvious resistance," causing ASR to drop sharply.
- **Non-beam-search compatibility:** The methodology is currently tailored to beam search and does not involve other prompt engineering methods.

## Limitations

- Attack success rates vary substantially across model sizes and dataset types, with numerical QA achieving up to 94% ASR while text-based QA shows much lower success rates (20-40%)
- Critical implementation details like exact BERTScore computation method and OpenAI API configuration parameters are not fully specified
- The evaluation focuses on closed-book QA tasks without exploring whether semantic-preserving attacks generalize to other LLM capabilities

## Confidence

**High Confidence:** The core AGBS mechanism is technically sound and the ablation study comparing dynamic vs. static strategies shows statistically significant performance difference.

**Medium Confidence:** Comparative analysis against baselines demonstrates superior performance, but experimental setup may not fully account for implementation differences in baseline methods.

**Low Confidence:** The assertion that numerical QA tasks are inherently more vulnerable lacks mechanistic explanation beyond surface-level correlation.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Conduct systematic sweep of similarity threshold σ (0.6, 0.7, 0.8, 0.9) and search scope (1,000 to 20,000) across all tested models and datasets to identify truly optimal values.

2. **Cross-Model Transferability Experiment:** Generate adversarial samples using AGBS on Llama-3.1-8B for GSM8K and test them on GPT-4o, Claude-3-5-Sonnet, and Gemini-Pro without regeneration to determine if perturbations transfer across model architectures.

3. **Robustness to Input Perturbation:** Apply AGBS to adversarially perturbed versions of the original inputs (e.g., adding random synonym substitutions) to test whether the method remains effective when the target model has already been exposed to lexical variations.