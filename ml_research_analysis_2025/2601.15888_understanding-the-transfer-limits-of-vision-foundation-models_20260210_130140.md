---
ver: rpa2
title: Understanding the Transfer Limits of Vision Foundation Models
arxiv_id: '2601.15888'
source_url: https://arxiv.org/abs/2601.15888
tags:
- tasks
- pretraining
- downstream
- task
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why vision foundation models (VFMs) show
  uneven transfer performance across downstream tasks. The authors hypothesize that
  a mismatch between pretraining objectives and downstream task demands limits transfer
  gains.
---

# Understanding the Transfer Limits of Vision Foundation Models

## Quick Facts
- arXiv ID: 2601.15888
- Source URL: https://arxiv.org/abs/2601.15888
- Authors: Shiqi Huang; Yipei Wang; Natasha Thorley; Alexander Ng; Shaheer Saeed; Mark Emberton; Shonit Punwani; Veeru Kasivisvanathan; Dean Barratt; Daniel Alexander; Yipeng Hu
- Reference count: 25
- Key outcome: This paper investigates why vision foundation models (VFMs) show uneven transfer performance across downstream tasks. The authors hypothesize that a mismatch between pretraining objectives and downstream task demands limits transfer gains. To test this, they evaluate two VFMs—ProFound (MAE-based reconstruction) and ProViCNet (contrastive-learning-based)—on five prostate multiparametric MRI tasks: classification, segmentation, super-resolution, distortion correction, and modality translation. They quantify task alignment using distributional similarity metrics (MMD) between pretraining and downstream representations, and measure relative performance gain (RPG) over baseline models. Results show that better alignment between pretraining and downstream tasks correlates with larger performance gains and faster convergence. ProFound excels in reconstruction-like tasks (e.g., distortion correction, RPG = 20.72%), while ProViCNet performs better in semantic tasks (e.g., segmentation, RPG = 21.23%). The findings highlight the importance of designing pretraining objectives aligned with downstream applications and suggest potential for task-agnostic or adaptive pretraining strategies to improve transfer.

## Executive Summary
This paper investigates why vision foundation models (VFMs) show uneven transfer performance across downstream tasks. The authors hypothesize that a mismatch between pretraining objectives and downstream task demands limits transfer gains. To test this, they evaluate two VFMs—ProFound (MAE-based reconstruction) and ProViCNet (contrastive-learning-based)—on five prostate multiparametric MRI tasks: classification, segmentation, super-resolution, distortion correction, and modality translation. They quantify task alignment using distributional similarity metrics (MMD) between pretraining and downstream representations, and measure relative performance gain (RPG) over baseline models. Results show that better alignment between pretraining and downstream tasks correlates with larger performance gains and faster convergence. ProFound excels in reconstruction-like tasks (e.g., distortion correction, RPG = 20.72%), while ProViCNet performs better in semantic tasks (e.g., segmentation, RPG = 21.23%). The findings highlight the importance of designing pretraining objectives aligned with downstream applications and suggest potential for task-agnostic or adaptive pretraining strategies to improve transfer.

## Method Summary
The authors evaluate two VFMs—ProFound (MAE-based reconstruction) and ProViCNet (contrastive-learning-based)—across five prostate multiparametric MRI tasks. They quantify task alignment using Maximum Mean Discrepancy (MMD) between pretraining and downstream representations, and measure relative performance gain (RPG) over baseline models. The study uses the PROMIS dataset with 740 cases, evaluating tasks including classification, segmentation, super-resolution, distortion correction, and modality translation. ProFound freezes the encoder during fine-tuning, while ProViCNet unfreezes the last two encoder blocks and normalization layers. Training uses AdamW optimizer with learning rate 1e-4 and cosine annealing for 100 epochs.

## Key Results
- Better alignment between pretraining and downstream tasks, measured by MMD between representations, correlates with greater performance improvements and faster convergence
- ProFound excels in reconstruction-like tasks (e.g., distortion correction, RPG = 20.72%), while ProViCNet performs better in semantic tasks (e.g., segmentation, RPG = 21.23%)
- The study identifies a strong negative correlation (Pearson's r < -0.8) between representational distance to pretraining (D2P) and relative performance gain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller representational distance between pretrained and fine-tuned states (D2P) correlates with larger relative performance gains (RPG).
- Mechanism: When pretraining objectives structurally resemble downstream task demands, fine-tuning requires less representational shift. Pretrained features remain more stable, enabling the optimizer to exploit existing structure rather than relearn from near-random initialization.
- Core assumption: MMD meaningfully captures task-relevant alignment (not just distributional overlap on irrelevant dimensions); RPG is comparable across tasks with different metrics.
- Evidence anchors:
  - [abstract]: "better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence"
  - [section 4.1]: "RPG with specialized task-specific model exhibits a strong negative correlation with the representational D2P (Pearson's r<-0.8, p-value<0.05)"
  - [corpus]: Limited direct support. SpaRRTa notes VFMs "exhibit limited spatial reasoning capabilities" but does not address MMD-based alignment measurement.
- Break condition: When downstream task requires representations outside the capacity of the pretraining objective (e.g., semantic reasoning from pure reconstruction pretraining with insufficient fine-tuning data). When fine-tuning learning rate is too high, destroying pretrained structure regardless of alignment.

### Mechanism 2
- Claim: MAE-based pretraining favors reconstruction-like tasks; contrastive-learning pretraining favors semantic discrimination tasks.
- Mechanism: Pretraining loss functions selectively strengthen certain feature pathways. MAE optimizes for local patch prediction, encoding structural continuity and texture patterns. DINOv2-style contrastive learning with semantic guidance optimizes for discriminative embeddings that separate semantic categories, encoding global shape and contextual relationships.
- Core assumption: Observed task preferences stem primarily from pretraining objectives rather than backbone architecture differences (ConvNeXt vs ViT) or dataset characteristics.
- Evidence anchors:
  - [abstract]: "Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures"
  - [section 4.2]: "For the MAE-based FM (ProFound), the distortion-correction task shows the highest alignment... D2P = 0.0057... for the DINOv2-based FM (ProViCNet)... segmentation task achieves the lowest D2P distance (0.0049)"
  - [corpus]: VER paper notes "individual VFMs typically excel only in specific domains, limiting generality across tasks"—consistent with objective-dependent specialization, though mechanism differs.
- Break condition: When pretraining and downstream data domains diverge substantially (e.g., natural images to medical imaging without domain adaptation). When semantic supervision in pretraining is weak or noisy, reducing contrastive benefits.

### Mechanism 3
- Claim: Better alignment reduces compute requirements (GPU-hours) for fine-tuning relative to training from scratch.
- Mechanism: Aligned pretraining initializes the model in a region of parameter space closer to the downstream optimum. Fewer gradient updates are needed to reach convergence, reducing both training time and risk of overfitting to limited downstream data.
- Core assumption: GPU-hours fairly represent computational efficiency; observed speedups generalize across hardware and batch sizes.
- Evidence anchors:
  - [abstract]: "better alignment... correlates with... faster convergence"
  - [section 4.3, Table 1]: Distortion correction with ProFound (lowest D2P = 0.0057) shows FT/Scratch GPU-hour ratio of 0.31; PI-RADS classification with ProFound (highest D2P = 0.8034) shows ratio of 0.72.
  - [corpus]: No direct corpus evidence on compute-alignment relationship.
- Break condition: When fine-tuning requires architectural modifications (new heads, different output dimensions) that add overhead. When pretrained weights require aggressive learning rate warmup, offsetting convergence benefits.

## Foundational Learning

- **Maximum Mean Discrepancy (MMD)**
  - Why needed here: Core metric for quantifying pretraining-downstream alignment; used to compute D2P, D2R, D2S.
  - Quick check question: Given two feature sets from different models, can you sketch the MMD computation using a Gaussian kernel?

- **Masked Autoencoding (MAE)**
  - Why needed here: ProFound's pretraining objective; determines which downstream tasks will align well.
  - Quick check question: Why does predicting randomly masked patches encourage learning of local structural patterns over global semantics?

- **Contrastive Learning with Semantic Guidance**
  - Why needed here: ProViCNet's pretraining approach; explains its preference for segmentation/classification.
  - Quick check question: How does patch-level contrastive learning differ from global instance discrimination, and why might semantic annotations change what features are learned?

## Architecture Onboarding

- **Component map**:
  - ProFound: ConvNeXt V2 Tiny encoder (frozen during FT) -> task-specific decoder/head
  - ProViCNet: 3D-enhanced ViT encoder (last 2 blocks unfrozen) -> task-specific head
  - Fine-tuning heads: MLP (classification), softmax decoder (segmentation/distortion/translation), subpixel conv (super-resolution)
  - MMD computation: Extract φ(x) from encoder -> compute pairwise kernel distances -> aggregate

- **Critical path**:
  1. Identify downstream task type (reconstruction vs semantic)
  2. Select VFM: ProFound for reconstruction, ProViCNet for semantic
  3. Configure freeze strategy per model protocol
  4. Train baselines: random-weight, scratch-trained, task-specific
  5. Extract features at pretraining, random, fine-tuned, scratch states
  6. Compute D2P, D2R, D2S; correlate with RPG

- **Design tradeoffs**:
  - Freeze encoder (ProFound) vs unfreeze blocks (ProViCNet): stability vs adaptability
  - D2P vs D2S: pretraining alignment vs functional equivalence
  - RPG baseline choice: task-specific model (practical value) vs scratch (learning contribution)

- **Failure signatures**:
  - High D2P (>0.7) + low RPG (<5%): fundamental misalignment—try different VFM
  - Low D2R after training: model not learning—check loss, data, LR
  - Fine-tuned < scratch: negative transfer—pretrained features harmful
  - Early plateau in fine-tuning but scratch continues: stuck in pretrained local minimum

- **First 3 experiments**:
  1. Reproduce one reconstruction task (distortion correction) and one semantic task (segmentation) with both VFMs; compare FT vs scratch vs task-specific to validate alignment hypothesis on your data.
  2. Compute D2P at epochs 1, 10, 50, 100 for both models on both task types; verify D2P decreases with training and correlates with metric improvement.
  3. Run all five downstream tasks with both VFMs; plot D2P vs RPG and D2S vs RPG; quantify Pearson correlation to confirm paper's r < -0.8 finding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive or task-agnostic pretraining strategies be developed to minimize the specific pretraining-downstream discrepancies identified in VFMs?
- **Basis in paper:** [explicit] The Conclusion states that "Future research may explore adaptive or task-agnostic pretraining strategies to enhance generalization and enable wider, clinically meaningful applications."
- **Why unresolved:** This work focuses on quantifying *existing* limits and alignment using fixed pretraining paradigms (MAE vs. Contrastive), but does not propose or validate a new pretraining method that dynamically aligns with diverse downstream objectives.
- **What evidence would resolve it:** A new VFM pretraining strategy tested across the same five tasks that achieves high Relative Performance Gain (RPG) and low Distance to Pretraining (D2P) regardless of whether the downstream task is reconstruction-based or semantic.

### Open Question 2
- **Question:** Can fine-tuning methodologies be engineered to be invariant to the objective misalignment between pretraining and downstream tasks?
- **Basis in paper:** [explicit] The Conclusion suggests that "developing approaches that are invariant to misalignment, offers promising directions."
- **Why unresolved:** The results currently show a negative correlation between task alignment (D2P) and performance gains; the authors identify this as a limit but do not investigate transfer learning techniques that might succeed despite high misalignment.
- **What evidence would resolve it:** The development of a fine-tuning protocol (e.g., specialized adapters or regularization) that decouples performance metrics (RPG) from the divergence metrics (MMD), maintaining high performance even when D2P is high.

### Open Question 3
- **Question:** Does the correlation between distributional alignment (MMD) and transfer performance generalize to non-medical domains or alternative VFM architectures?
- **Basis in paper:** [inferred] The study is restricted to prostate MRI and two specific architectures (ProFound/MAE and ProViCNet/DINOv2).
- **Why unresolved:** While the hypothesis is general, the evidence is derived entirely from a specific clinical context. It remains unconfirmed if "task alignment" measured by MMD is a universal predictor of transfer success in natural images or other medical modalities.
- **What evidence would resolve it:** Reproducing the D2P/RPG correlation experiments on standard computer vision benchmarks (e.g., ImageNet) or other foundation models (e.g., SAM) to validate the universality of the alignment metric.

## Limitations
- Analysis restricted to prostate multiparametric MRI, limiting generalizability to other medical imaging domains
- Limited ablation studies prevent ruling out architecture differences versus objective-driven specialization
- Compute-efficiency claims lack sufficient empirical validation beyond GPU-hour ratios
- Does not explore task-agnostic pretraining approaches that might outperform specialized strategies

## Confidence
- **High**: Core alignment hypothesis (MMD correlating with RPG) is well-supported by clear quantitative patterns across diverse tasks
- **Medium**: Mechanism explanations are plausible but lack direct ablation evidence to isolate pretraining objective effects
- **Low**: Computational efficiency claims are weakly supported with limited empirical validation

## Next Checks
1. Test whether MMD-based alignment predicts transfer gains on a non-medical dataset (e.g., ImageNet fine-tuning tasks) to assess domain generality
2. Conduct an ablation isolating pretraining objective effects by training both architectures with each objective (MAE on ViT, contrastive on ConvNeXt)
3. Measure actual wall-clock training time and memory usage across fine-tuning runs to validate computational efficiency claims beyond GPU-hour ratios