---
ver: rpa2
title: 'No-Human in the Loop: Agentic Evaluation at Scale for Recommendation'
arxiv_id: '2511.03051'
source_url: https://arxiv.org/abs/2511.03051
tags:
- evaluation
- sports
- electronics
- product
- anchor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScalingEval presents a large-scale benchmarking framework for evaluating
  LLMs as judges in complementary-item recommendation. The study systematically compares
  36 LLMs across multiple product categories using a consensus-driven multi-agent
  evaluation protocol that generates ground-truth labels through scalable majority
  voting without human annotation.
---

# No-Human in the Loop: Agentic Evaluation at Scale for Recommendation

## Quick Facts
- arXiv ID: 2511.03051
- Source URL: https://arxiv.org/abs/2511.03051
- Reference count: 40
- Primary result: ScalingEval benchmark identifies Gemini-1.5-Pro as best overall LLM judge for recommendation evaluation with 87% coverage and 88% confidence

## Executive Summary
ScalingEval presents a large-scale benchmarking framework for evaluating LLMs as judges in complementary-item recommendation without human annotation. The study systematically compares 36 LLMs across seven product categories using a consensus-driven multi-agent evaluation protocol that generates ground-truth labels through scalable majority voting. Applied to Walmart-scale recommendation data, the benchmark identifies Gemini-1.5-Pro as the best overall performer in accuracy, coverage, and latency, Claude-3.5-Sonnet as having the highest decision confidence (~99%), and GPT-4o as offering the most favorable latency-accuracy-cost tradeoff. The framework reveals strong cross-model agreement in structured domains like Electronics and Sports, but persistent disagreement in lifestyle categories like Clothing and Food.

## Method Summary
The framework implements a three-agent pipeline: CI Pattern Audit (8 patterns), Issue Audit (8 issue codes), and Report Generation (conflict resolution). Each of 36 LLMs independently evaluates anchor-recommendation pairs, with judgments aggregated via 60% majority voting among the top 25 quality-scored models. A strict conflict resolution hierarchy (Reject > Major > Minor > Good) ensures conservative decisions. The method eliminates human annotation by treating models as crowd workers, applying classical aggregation theory to reduce systematic biases. Latency bottleneck is the 36-model parallel evaluation, with chunking for scale.

## Key Results
- Gemini-1.5-Pro achieves best overall performance: 87% coverage, 88% confidence, and balanced latency
- Claude-3.5-Sonnet achieves highest confidence (~99%) but lowest coverage (31%)
- GPT-4o offers most favorable latency-accuracy-cost tradeoff (60% accuracy at 1× latency)
- Category agreement rates: Sports & Outdoors 93.8%, Electronics 91.3%, Clothing & Shoes 84.2%, Food & Beverages 85.4%

## Why This Works (Mechanism)

### Mechanism 1: Specialized Audit Agent Decomposition
Decomposing evaluation into specialized pattern-detection and issue-flagging agents improves detection accuracy over single-model evaluation. The framework separates complementary-item evaluation into two distinct audit passes: CI Pattern Audit Agent (8 patterns) and Recommendation Issue Audit Agent (8 issue codes). Results are aggregated via chunked processing. Core assumption: pattern-matching and issue-detection require different reasoning modes and are more reliable when separated rather than combined in a single prompt.

### Mechanism 2: Cross-Family Majority Voting with Conservative Conflict Resolution
Aggregating judgments across diverse model families reduces individual model biases and produces stable pseudo-ground-truth without human annotation. Each of 36 LLMs independently evaluates all item pairs. A 60% agreement threshold among the top 25 quality-scored models determines consensus labels. When agreement falls below threshold, a strict hierarchy (Reject > Major > Minor > Good) ensures conservative decisions. Core assumption: Systematic biases are uncorrelated across model families, so collective reasoning cancels individual errors.

### Mechanism 3: Domain Structure Determines Agreement Consensus
Structured domains with well-defined functional taxonomies achieve higher cross-model agreement than lifestyle domains with subjective or contextual relationships. In categories like Electronics and Sports, complementary relationships map cleanly to the 8 CI patterns (e.g., "phone + case" → Accessory). In lifestyle categories (Clothing, Food), relationships depend on aesthetic preference or cultural context that the pattern taxonomy underspecifies, producing model divergence. Core assumption: The 8-pattern taxonomy adequately covers functional complementarity but not contextual/aesthetic complementarity.

## Foundational Learning

- **Majority Voting / Ensemble Methods**
  - Why needed here: The entire ScalingEval framework relies on aggregating 36 independent model judgments; understanding why ensemble methods reduce variance is essential.
  - Quick check question: If 3 models vote 2-1 for "Good," what happens to confidence if you add 2 more models that also vote "Good"?

- **LLM-as-a-Judge Paradigm**
  - Why needed here: This paper positions LLMs as evaluators rather than generators; understanding the shift from generation to evaluation is prerequisite.
  - Quick check question: Why might an LLM be better at judging whether "tuna + mayonnaise" is complementary than generating a mayonnaise recipe?

- **Complementary vs. Substitute Recommendation**
  - Why needed here: The core task distinguishes complements (add-ons) from substitutes (replacements); confusing these causes false positives.
  - Quick check question: Is "Pepsi recommended with Coca-Cola anchor" a complement or substitute? Which CI pattern or issue code applies?

## Architecture Onboarding

- **Component map:** User Query → Multi-Agent Planner → CI Pattern Audit (8 patterns) → Issue Audit Agent (8 issue codes) → Report Generation Agent (chunked aggregation) → Majority-Vote Synthesis (36 models × N pairs) → Consensus Ground Truth + Agreement Metrics

- **Critical path:** Pattern audit → Issue audit → Report generation → Majority voting (60% threshold, top 25 models). Latency bottleneck is the 36-model parallel evaluation.

- **Design tradeoffs:**
  - Confidence vs. Coverage: Claude-3.5-sonnet achieves 98–99% confidence but 31% coverage; Gemini-1.5-pro achieves 88% confidence but 87% coverage.
  - Latency vs. Accuracy: GPT-o1 highest accuracy (67–82% by category) at 4.9× latency; GPT-4o balanced (60% accuracy) at 1× latency.
  - Cost vs. Control: Closed-source models provide better performance; GPT-OSS-20B offers mid-tier performance at zero marginal cost but requires A100 infrastructure.

- **Failure signatures:**
  - High conflict rates (>15%) in Clothing & Shoes and Food & Beverages categories.
  - Temperature sensitivity in some open-source models (Llama-3-3B drops from 93% to 86% confidence as temperature increases).
  - Coverage gaps when models return "Unknown" judgments (lower coverage in Sports for some model variants).

- **First 3 experiments:**
  1. Validate pattern coverage: Run CI Pattern Audit on 50 Electronics pairs; manually verify that ≥90% map cleanly to one of the 8 patterns without triggering "Other."
  2. Measure voting convergence: Compare agreement rates using 5, 15, and 25 models on a held-out subset; identify the minimum ensemble size for stable consensus.
  3. Stress-test conflict resolution: Inject 20 known problematic pairs (substitutes, category-distant items) and verify the Reject > Major > Minor > Good hierarchy correctly flags ≥85% as rejected.

## Open Questions the Paper Calls Out

### Open Question 1
Does the consensus-driven ground truth generated by the multi-agent majority vote align with actual human expert judgments? The methodology creates ground truth via "Scalable Majority-vote" specifically to avoid human annotation, but provides no external validation that this synthetic consensus matches human preferences. This leaves the validity of the "no-human" labels unproven. What evidence would resolve it: A correlation analysis comparing ScalingEval's consensus labels against a human-annotated dataset for the same item pairs.

### Open Question 2
What specific semantic or attribute-level factors drive the persistent disagreement in lifestyle categories like Clothing and Food? While the paper quantifies the agreement gap, it does not mechanistically explain whether the divergence stems from ambiguous product data or limitations in the models' contextual reasoning. What evidence would resolve it: A qualitative error analysis of "conflicted pairs" in lifestyle categories to determine if the ambiguity is linguistic, contextual, or related to subjective complementarity definitions.

### Open Question 3
Do the reliability rankings of LLM judges (e.g., Gemini-1.5-pro vs. GPT-4o) hold steady when the framework is applied to non-complementary recommendation tasks? The current benchmark is specialized for CIR using specific audit patterns (e.g., "Functional Synergy"); it is unclear if the top-performing models generalize to tasks like substitute recommendation. What evidence would resolve it: Replicating the ScalingEval protocol on distinct tasks (e.g., substitute finding) to see if Gemini-1.5-pro remains the top performer.

## Limitations

- Circular validation problem: The framework depends on LLM-generated pseudo-ground truth without external human validation, leaving systematic blind spots in the pattern taxonomy potentially consistently replicated across models.
- Underspecified selection criterion: The "top 25 models based on quality scores" selection remains unclear, potentially introducing selection bias in the consensus labels.
- Taxonomy inadequacy: The current 8-pattern taxonomy may inadequately capture contextual or aesthetic complementarity in lifestyle categories, as evidenced by persistent disagreement in Clothing and Food.

## Confidence

- High Confidence: Category-level agreement patterns (Electronics/Sports vs Clothing/Food) are empirically observed with clear distributional differences and reproducible across model families.
- Medium Confidence: The mechanism by which specialized audit decomposition improves accuracy over single-model evaluation lacks direct ablation studies comparing decomposed vs unified approaches on identical datasets.
- Medium Confidence: Cross-family majority voting reduces individual model biases, but the assumption of uncorrelated systematic biases across families requires validation through correlation analysis of model errors.

## Next Checks

1. Apply CI Pattern Audit to 100 randomly selected Electronics pairs and manually verify that ≥95% map cleanly to the 8 patterns without triggering "Other," establishing whether the taxonomy adequately covers structured domains.

2. Compute pairwise Pearson correlation coefficients of individual model errors on a held-out validation set; if correlations exceed 0.3 within any model family, the voting ensemble may be amplifying rather than canceling systematic biases.

3. Implement a unified evaluation model that performs pattern matching and issue detection in a single prompt; compare accuracy, coverage, and confidence metrics against the decomposed three-agent pipeline on identical 500-pair subsets across all categories.