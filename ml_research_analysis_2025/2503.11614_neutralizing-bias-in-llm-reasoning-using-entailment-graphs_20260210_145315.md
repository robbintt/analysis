---
ver: rpa2
title: Neutralizing Bias in LLM Reasoning using Entailment Graphs
arxiv_id: '2503.11614'
source_url: https://arxiv.org/abs/2503.11614
tags:
- llms
- bias
- attestation
- inference
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses attestation bias in large language models
  (LLMs) during natural language inference (NLI), where models rely on memorized knowledge
  rather than logical reasoning from premises, leading to false positive entailment
  judgments. The core method uses an unsupervised approach to construct counterfactual
  reasoning datasets by extracting entailment graphs (EGs) from open-domain corpora
  and instantiating them with random entities.
---

# Neutralizing Bias in LLM Reasoning using Entailment Graphs

## Quick Facts
- arXiv ID: 2503.11614
- Source URL: https://arxiv.org/abs/2503.11614
- Authors: Liang Cheng; Tianyi Li; Zhaowei Wang; Tianyang Liu; Mark Steedman
- Reference count: 35
- Primary result: Counterfactual entailment graphs reduce attestation bias from 19-33 to 5-14 across four LLMs

## Executive Summary
This paper addresses attestation bias in large language models during natural language inference, where models rely on memorized knowledge rather than logical reasoning from premises. The authors propose an unsupervised framework that constructs counterfactual reasoning datasets by extracting entailment graphs from open-domain corpora and instantiating them with random entities. Fine-tuning LLMs on this data significantly reduces attestation bias while improving performance on both original and bias-neutralized NLI datasets.

## Method Summary
The approach extracts entailment graphs using CCG parsing and distributional similarity from the NewsSpike corpus, then instantiates typed predicate relationships with random entities to create counterfactual examples. These examples are used to fine-tune LLMs with LoRA adapters (rank=8, dropout=0.05, lr=1e-4, 12 epochs). The method specifically targets attestation bias by forcing models to learn predicate-level entailment relationships without relying on memorized entity-specific knowledge.

## Key Results
- Attestation bias reduced from 19-33 to 5-14 on bias-adversarial tasks across four LLMs
- Models show consistent improvements on both original NLI datasets and bias-neutralized versions
- 8B models achieve near-zero attestation bias with 11k training samples; 70B models need more data
- Fine-tuned models maintain or improve performance on original NLI tasks while reducing bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual reasoning datasets constructed from entailment graphs reduce attestation bias by forcing models to learn predicate-level entailment relationships without relying on memorized entity-specific knowledge.
- **Mechanism:** Extract entailment rules between predicates from large-scale corpora using CCG parsing and distributional similarity (Weeds score), strip specific entities, keep typed predicate relationships, then instantiate with random entities of matching types—creating logically valid but counterfactual examples.
- **Core assumption:** Models that learn predicate entailment patterns independently of specific entities will generalize better and rely less on memorized facts.
- **Evidence anchors:**
  - [abstract]: "unsupervised framework to construct counterfactual reasoning data and fine-tune LLMs to reduce attestation bias"
  - [section 3.1]: "The counterfactual nature of our EG-based training corpus... LLMs learn entailment between predicates while incorporating counterfactual knowledge"
  - [corpus]: Weak direct verification—no related papers specifically address this mechanism
- **Break condition:** If entailment graphs are too sparse or extracted predicates don't capture meaningful semantic relationships, counterfactual data won't generalize.

### Mechanism 2
- **Claim:** Bias-adversarial training via Random Premise Inference forces models to distinguish between logical inference and memorization by decoupling hypothesis attestedness from premise validity.
- **Mechanism:** Create adversarial examples by replacing premise predicates randomly while keeping hypotheses fixed (all labeled No-Entail). Models predicting Entail exhibit attestation bias—responding to attested hypotheses rather than actual premise-hypothesis relationships.
- **Core assumption:** Attestation bias is primarily driven by models recognizing hypotheses as "true" facts and ignoring premise content.
- **Evidence anchors:**
  - [abstract]: "build bias-adversarial variants of NLI datasets with randomly replaced predicates in premises"
  - [section 5]: "RPI task effectively tests the model's reliance on propositional memory... while maintaining attestedness of conclusions"
  - [corpus]: "LLMs are Frequency Pattern Learners in NLI" supports frequency/memorization bias hypothesis
- **Break condition:** If random predicates accidentally form true entailments (paper notes ~9.6% rate), this introduces noise into the bias reduction signal.

### Mechanism 3
- **Claim:** Parameter-efficient fine-tuning via LoRA on counterfactual entailment data enhances logical reasoning without catastrophic forgetting or full retraining.
- **Mechanism:** Apply LoRA (rank=8, dropout=0.05) within PEFT framework, train 12 epochs at 1e-4 LR on 11k counterfactual examples. Smaller models show more pronounced improvements; larger models need more data.
- **Core assumption:** Counterfactual reasoning capabilities can be enhanced through efficient adaptation without architectural changes.
- **Evidence anchors:**
  - [abstract]: "fine-tune LLMs to reduce attestation bias" with significant improvements
  - [section 3.2 + Table 7]: Detailed fine-tuning parameters and scaling results
  - [corpus]: No direct corpus evidence for LoRA effectiveness specifically for bias reduction
- **Break condition:** If LoRA rank is too low or training data insufficient, larger models won't benefit (LLaMA-70B shows this pattern).

## Foundational Learning

- **Attestation Bias**:
  - Why needed here: Core problem—LLMs conflate "this hypothesis matches training data" with "this hypothesis follows from the premise"
  - Quick check question: Can you explain why a model might correctly identify "Paris is in France" as entailed, but for the wrong reason (memorization vs. inference)?

- **Entailment Graphs**:
  - Why needed here: Solution relies on extracting predicate-level relationships; understanding EG structure is essential for implementing counterfactual data generation
  - Quick check question: What's the difference between sentence-level and predicate-level entailment rules?

- **Distributional Semantics for Entailment**:
  - Why needed here: Paper uses Weeds similarity scores based on co-occurrence patterns; requires understanding how distributional similarity relates to semantic relationships
  - Quick check question: Why might predicates co-occurring with same entities in similar contexts have an entailment relationship?

## Architecture Onboarding

- **Component map**: GraphParser (CCG parser) -> Aidalight (entity linker) -> Weeds similarity scorer -> Entailment Graph Extractor -> Entity Type Mapper (FIGER) -> Counterfactual Instantiator -> Training Data Formatter -> LoRA Adapter -> Bias Evaluator

- **Critical path**: Extract EGs from NewsSpike (220G CPU, 20h) → Map entities to types → Generate 11k counterfactual examples → Fine-tune with LoRA (4x A6000, 21h for 70B) → Evaluate on original + bias-neutralized tests

- **Design tradeoffs**:
  - NewsSpike corpus provides parallel event coverage but limits domain generality
  - 48 FIGER types balance coverage vs. granularity
  - 11k samples work for 7-8B models; 70B needs more data (scaling experiments in Appendix D)

- **Failure signatures**:
  - Persistent high bias: Check EG extraction quality, increase training data
  - Performance drop on original NLI: Possible overfit to counterfactuals, reduce epochs
  - Random performance on shuffled-EG test: Expected—confirms learning entailment, not templates

- **First 3 experiments**:
  1. Baseline AttBias measurement on vanilla LLM using RPI task
  2. Ablation on EG quality: train with shuffled EGs to verify pattern learning
  3. Scaling study: vary EG data size across model sizes to find optimal ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning with counterfactual Entailment Graphs (EGs) generalize to improve performance on downstream reasoning tasks beyond Natural Language Inference (NLI)?
- Basis in paper: [explicit] Section 8 states that it remains "uncertain whether our approach will generalize effectively to other tasks," noting that the method was only tested on NLI.
- Why unresolved: The experiments were restricted to Levy/Holt and SNLI datasets; reasoning capabilities in areas like mathematical logic or commonsense QA were not evaluated.
- What evidence would resolve it: Evaluation of EG-enhanced models on reasoning benchmarks such as GSM8K (math) or CommonsenseQA to see if the reduction in attestation bias transfers.

### Open Question 2
- Question: Can counterfactual EG data be integrated into general instruction-tuning frameworks without degrading the model's ability to follow other types of instructions?
- Basis in paper: [explicit] The authors propose in Section 8: "In future work, we plan to integrate counterfactual reasoning data into instruction-tuning frameworks."
- Why unresolved: The current method uses task-specific fine-tuning; it is unknown if mixing this data with broad instruction datasets dilutes the bias-reduction signal.
- What evidence would resolve it: A comparison of standard instruction-tuned models versus those trained with mixed EG data on a diverse benchmark like MT-Bench.

### Open Question 3
- Question: What is the optimal ratio of generated EG data required to saturate bias reduction in extremely large models (e.g., 70B+ parameters)?
- Basis in paper: [inferred] Appendix D shows that LLaMA-3-70B requires significantly more data (18k samples) to reach lower bias scores compared to smaller models, and the trend suggests it may need even more.
- Why unresolved: The paper provides a snapshot of performance at specific data volumes, but does not establish a ceiling or the scaling laws between data volume and bias reduction for the largest models.
- What evidence would resolve it: A scaling curve analysis plotting EG dataset size against AttBias scores for 70B+ models to identify the point of diminishing returns.

## Limitations
- Counterfactual reasoning may not generalize across domains beyond news-style language
- Random predicate replacement introduces ~9.6% noise in bias evaluation
- Results only demonstrated for 8B and 70B parameter models
- Does not conclusively prove models learn genuine reasoning versus new patterns

## Confidence
- **High confidence**: The core attestation bias definition and measurement methodology are well-grounded in prior work. The significant bias reduction from 19-33 to 5-14 is clearly demonstrated.
- **Medium confidence**: The counterfactual data generation mechanism is theoretically sound, but actual EG quality and coverage from NewsSpike is not extensively validated.
- **Low confidence**: The claim that predicate entailment learning transfers to genuine reasoning improvements is only partially supported by bias-neutralized dataset performance.

## Next Checks
1. **Cross-domain generalization test**: Evaluate fine-tuned models on NLI datasets from different domains (e.g., biomedical or legal texts) to verify counterfactual reasoning capabilities transfer beyond news-style language.
2. **Human evaluation of reasoning quality**: Conduct human assessments to distinguish between models that have learned genuine entailment reasoning versus those that have memorized new statistical patterns from the counterfactual training data.
3. **EG quality and coverage analysis**: Perform ablation studies varying the quality and quantity of extracted entailment graphs to determine minimum viable corpus requirements and identify failure modes when EG extraction quality degrades.