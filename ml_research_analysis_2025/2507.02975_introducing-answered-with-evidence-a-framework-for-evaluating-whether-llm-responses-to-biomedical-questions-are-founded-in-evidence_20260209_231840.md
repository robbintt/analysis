---
ver: rpa2
title: Introducing Answered with Evidence -- a framework for evaluating whether LLM
  responses to biomedical questions are founded in evidence
arxiv_id: '2507.02975'
source_url: https://arxiv.org/abs/2507.02975
tags:
- evidence
- green
- yellow
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces "Answered with Evidence," a framework for
  evaluating whether LLM-generated responses to biomedical questions are grounded
  in scientific literature. The authors analyzed thousands of physician-submitted
  questions using a comparative pipeline that included three evidence sources: Alexandria
  (Atropos Evidence Library), System (PubMed-based), and Perplexity (PubMed-based).'
---

# Introducing Answered with Evidence -- a framework for evaluating whether LLM responses to biomedical questions are founded in evidence

## Quick Facts
- arXiv ID: 2507.02975
- Source URL: https://arxiv.org/abs/2507.02975
- Authors: Julian D Baldwin; Christina Dinh; Arjun Mukerji; Neil Sanghavi; Saurabh Gombar
- Reference count: 21
- Primary result: Combined evidence sources enabled reliable answers to over 70% of biomedical queries vs. ~50% individually

## Executive Summary
This paper introduces "Answered with Evidence," a framework for evaluating whether LLM-generated responses to biomedical questions are grounded in scientific literature. The authors analyzed thousands of physician-submitted questions using a comparative pipeline that included three evidence sources: Alexandria (Atropos Evidence Library), System (PubMed-based), and Perplexity (PubMed-based). They found that PubMed-based systems provided evidence-supported answers for approximately 44% of questions, while the novel evidence source did so for about 50%. Combined, these sources enabled reliable answers to over 70% of biomedical queries. The study highlights the value of combining published literature with real-world evidence to improve the reliability of LLM-generated answers in biomedical contexts.

## Method Summary
The framework processes physician-submitted biomedical questions through parallel RAG systems using three distinct evidence sources: Alexandria (proprietary real-world evidence library), System (PubMed-based), and Perplexity (PubMed-based academic-only). LLM-generated answers are evaluated using a structured rubric with three binary criteria: direct answer, relevance, and grounding in context. A "traffic light" badge system (Green/Yellow/Red) classifies responses based on these criteria, with Green indicating fully evidence-supported answers, Yellow indicating related but non-answering context, and Red indicating hallucinations or irrelevance.

## Key Results
- PubMed-based systems provided evidence-supported answers for approximately 44% of questions
- The novel evidence source (Alexandria) provided evidence-supported answers for about 50% of questions
- Combined sources enabled reliable answers to over 70% of biomedical queries
- Novelty rates were 17.9% for Alexandria and 17.8% for System, indicating distinct contributions
- Green badge agreement between sources was approximately 22%, demonstrating complementary coverage

## Why This Works (Mechanism)

### Mechanism 1: Complementary Evidence Coverage
Combining published literature (PubMed) with real-world evidence (RWE) increases the proportion of answerable biomedical questions beyond what either source can achieve alone. PubMed-based RAG systems excel at established clinical knowledge but hit a performance "ceiling" (~55%) due to generalizability gaps in clinical trials. RWE sources (Alexandria) address complex, comorbid, or underrepresented scenarios using EHR and claims data. The two sources exhibit low overlap in "Green" badge agreement (approx. 22%), meaning they fail and succeed on different queries.

### Mechanism 2: Atomic Evaluation via LLM-as-a-Judge
Decomposing "answer quality" into three binary criteria (Direct Answer, Relevance, Grounding) allows an LLM to reliably classify response trustworthiness. Instead of asking an LLM to "rate" a complex answer, the framework forces boolean decisions against a specific context. This constrains the evaluation space, reducing model ambiguity and aligning the output with the "Green/Yellow/Red" badge logic.

### Mechanism 3: Traffic-Light Signaling of Evidence Gaps
The "Yellow" badge serves as a distinct signal for "actionable ignorance," differentiating between "system failure" and "evidence gap." By requiring the context to be "related" but not "directly answering," the Yellow badge identifies valid research questions where literature exists but is insufficient. This prevents the system from hallucinating an answer and signals the need for a new study.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** The entire framework relies on the premise that LLMs must cite external sources (PubMed/Alexandria) rather than relying on internal weights to answer biomedical queries.
  - **Quick check question:** Can you explain why standard LLM generation is insufficient for medical safety without a retrieval step?

- **Concept:** Real-World Evidence (RWE) vs. Clinical Trials
  - **Why needed here:** To understand why "Alexandria" complements "PubMed." One provides controlled experimental data; the other provides data on real-world practice, comorbidities, and rare events.
  - **Quick check question:** What is the primary limitation of RCTs that RWE aims to solve in this context? (Hint: Generalizability).

- **Concept:** Hallucination Detection
  - **Why needed here:** The core evaluation rubric is designed to detect when an LLM adds information "not present in the context." Understanding what constitutes a hallucination vs. a valid inference is critical.
  - **Quick check question:** If an LLM summarizes a study and adds a statistic that isn't in the retrieved text, which evaluation criterion fails?

## Architecture Onboarding

- **Component map:** Input: Physician Question -> Retrieval Layer (3 parallel paths: System/PubMed, Perplexity/PubMed, Alexandria/RWE) -> Generation Layer (LLMs produce answers) -> Evaluation Layer (LLM-as-a-Judge applies 3-criterion rubric) -> Interface (Badge assignment Green/Yellow/Red)

- **Critical path:** The **Evaluation Prompt** (Supplemental Fig S1). If this prompt is poorly designed or the judge model is weak, the entire trust mechanism fails, and badges become noise.

- **Design tradeoffs:**
  - Efficiency vs. Robustness: Using a single LLM judge is fast and cheap but prone to "model-specific bias" (Limitations). An "LLM Jury" would be more robust but slower/costlier.
  - Recall vs. Precision: Strictly enforcing "Answer grounded in context" (Green) minimizes hallucinations but increases the Yellow/Red rate, potentially frustrating users who want a quick answer.

- **Failure signatures:**
  - High Red Rate in PubMed: Suggests retrieval is failing to find relevant papers, or the query is too obscure.
  - High Yellow Rate: Indicates a widespread "evidence gap" or that the retrieval is finding generic background info rather than specific answers.
  - Green Badge with Wrong Facts: Implies the "Judge" LLM failed to detect hallucinations (evaluator blind spot).

- **First 3 experiments:**
  1. Inter-Rater Reliability: Run 100 questions through the pipeline and have human experts assign badges to validate the "LLM-as-a-Judge" accuracy.
  2. Source Ablation: Measure the Green badge rate for "Alexandria only" vs. "System only" vs. "Combined" on a held-out set of 100 rare disease queries to quantify synergy.
  3. Judge Sensitivity: Swap the "Judge" LLM (e.g., GPT-4 vs. Claude) to see if badge distributions shift significantly, testing for evaluation bias.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does utilizing an "LLM-as-a-jury" approach (ensemble evaluation) significantly reduce the bias and inconsistency observed when using a single LLM-as-a-judge for evidence evaluation? The current study relied on a single LLM to assess response quality, which introduces the risk of model-specific bias and evaluation inconsistency, but the effectiveness of a jury approach in this specific biomedical context remains untested.

- **Open Question 2:** Can the accuracy of the "Answered with Evidence" badge assignment be improved by tailoring evaluation prompts to the specific response styles (e.g., hedging language) of different LLMs? The current framework applies a uniform rubric to diverse systems, potentially misclassifying answers due to stylistic variations rather than factual content, and it is unknown if customization yields better results.

- **Open Question 3:** How do other emerging LLM-based literature summarization platforms perform on the "Answered with Evidence" benchmark relative to the systems tested (System, Perplexity, and Alexandria)? The current findings are limited to three specific systems, leaving the generalizability of the 44-50% evidence-grounding rate unknown across the broader landscape of biomedical AI tools.

## Limitations

- The evaluation framework relies heavily on LLM-as-a-judge without reported inter-rater reliability between human experts and the LLM judge.
- The Alexandria evidence source is proprietary, making independent validation impossible and limiting reproducibility.
- The paper does not explore whether different LLM judges produce significantly different badge distributions, which could indicate evaluation bias.

## Confidence

**High confidence** in the core finding that combining PubMed-based sources with RWE increases the proportion of answerable biomedical questions from ~50% to >70%.

**Medium confidence** in the mechanism by which the three-criteria LLM evaluation reliably detects hallucinations and evidence gaps.

**Low confidence** in the Yellow badge as a reliable signal for "actionable ignorance" since this interpretation is largely derived from the methodology itself rather than empirical validation.

## Next Checks

1. **Human validation study**: Have clinical experts independently badge-assign 100 randomly selected questions to measure LLM judge accuracy and identify systematic biases.

2. **Judge model comparison**: Run the same 100-question evaluation using different LLM judges (GPT-4, Claude, Gemini) to quantify sensitivity to evaluation model choice and assess consistency of badge distributions.

3. **Source complementarity analysis**: Perform ablation testing on a held-out set of 200 rare disease queries to quantify the marginal contribution of Alexandria versus PubMed-only systems and validate the claimed synergy.