---
ver: rpa2
title: Hallucination Detection with Small Language Models
arxiv_id: '2506.22486'
source_url: https://arxiv.org/abs/2506.22486
tags:
- responses
- language
- arxiv
- correct
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting hallucinations
  in responses generated by large language models (LLMs). Hallucinations are a common
  problem in LLM outputs, where the model generates inaccurate or fabricated information,
  which can be difficult to detect without ground truth, especially in question-and-answer
  scenarios.
---

# Hallucination Detection with Small Language Models

## Quick Facts
- arXiv ID: 2506.22486
- Source URL: https://arxiv.org/abs/2506.22486
- Authors: Ming Cheung
- Reference count: 39
- Primary result: 10% F1 improvement in hallucination detection using multiple small language models

## Executive Summary
This paper addresses the challenge of detecting hallucinations in LLM-generated responses by proposing a framework that uses multiple small language models (SLMs) to verify responses against retrieved context. The framework breaks responses into individual sentences and uses the probability of generating "Yes" tokens from SLMs to determine if each sentence is supported by the context. Experiments with a real HR handbook dataset demonstrate that this approach improves F1 scores by 10% compared to baselines, showing that SLMs can effectively verify LLM responses without requiring ground truth answers.

## Method Summary
The framework processes each question-response pair by first retrieving relevant context from a vectorized database. The LLM response is split into individual sentences using SpaCy. For each sentence, multiple SLMs (Qwen2-1.5B and MiniCPM-2B) are prompted to answer whether the sentence is supported by the context with a "Yes" or "No" response. The probability of the first token being "Yes" is extracted from each SLM's output, normalized using z-scores, and averaged across models. Finally, a harmonic mean is computed across all sentence scores to produce the final hallucination score for the response.

## Key Results
- Achieves 10% F1 improvement in detecting correct vs. hallucinated responses
- Best F1 scores: 0.99 (correct vs. wrong) and 0.81 (correct vs. partial)
- Harmonic mean aggregation outperforms arithmetic mean for partial response detection
- Multiple SLMs provide better performance than single-model approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level verification isolates hallucinated claims within partially correct responses
- Core assumption: Hallucinations are localized to specific sentences rather than distributed across response structure
- Evidence anchors: Abstract states "breaking down responses into individual sentences... hallucinations can be detected"; Section IV.A notes that evaluating whole sentences with mixed information confuses the checker
- Break condition: If hallucinations manifest as subtle misframings within grammatically correct sentences rather than false claims

### Mechanism 2
- Claim: First-token "Yes" probability from SLMs serves as a calibrated faithfulness signal when context is provided
- Core assumption: SLMs can perform contextual entailment even if they cannot generate high-quality free-text responses
- Evidence anchors: Abstract mentions "utilizing the probability of generating 'Yes' tokens"; Section IV.B describes asking SLMs to generate responses starting with YES or NO
- Break condition: If SLMs systematically miscalibrate on domain-specific terminology not well-represented in training

### Mechanism 3
- Claim: Multi-SLM aggregation with harmonic mean improves robustness against individual model biases
- Core assumption: Different SLMs have independent failure modes; their errors don't fully correlate
- Evidence anchors: Abstract states "integrates multiple small language models"; Section V.D shows multiple SLMs improve performance; Section V.E finds harmonic mean yields best outcomes for partial response detection
- Break condition: If SLMs share systematic biases (e.g., common training data artifacts), aggregation provides limited benefit

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Framework assumes context is retrieved from vector database to ground verification
  - Quick check question: Can you explain why providing retrieved context to an LLM changes its vulnerability to hallucinations?

- Concept: Token probability extraction from language models
  - Why needed here: Mechanism depends on accessing P(token₁ = yes) directly from model outputs
  - Quick check question: How would you obtain the probability of a specific token from a HuggingFace model's forward pass?

- Concept: Ensemble score aggregation (normalization + harmonic mean)
  - Why needed here: Combining scores from multiple SLMs requires per-model normalization and appropriate aggregation
  - Quick check question: Why does harmonic mean penalize outliers more severely than arithmetic mean when combining verification scores?

## Architecture Onboarding

- Component map: Question → Vector DB → Context → LLM → Response → Splitter → Sentences → SLMs → P(Yes) → Normalize → Aggregate → Final score
- Critical path:
  1. Question qᵢ → Vector DB → Context cᵢ
  2. (qᵢ, cᵢ) → LLM → Response rᵢ
  3. rᵢ → Splitter → Sentences {rᵢ,ⱼ}
  4. For each rᵢ,ⱼ and each SLM m: Compute s⁽ᵐ⁾ᵢ,ⱼ = P(yes | qᵢ, cᵢ, rᵢ,ⱼ)
  5. Normalize s̃⁽ᵐ⁾ᵢ,ⱼ using historical μₘ, σₘ
  6. Aggregate: sᵢ,ⱼ = (1/M) Σₘ s̃⁽ᵐ⁾ᵢ,ⱼ
  7. Final score: sᵢ = |S(rᵢ)| / Σⱼ 1/sᵢ,ⱼ (harmonic mean)
  8. Threshold comparison: If sᵢ > τ, label as "correct"; else "hallucination"

- Design tradeoffs:
  - Harmonic vs. arithmetic mean: Harmonic better for partial correctness detection (F1=0.81 vs. 0.75)
  - Number of SLMs: Paper uses 2; more models may improve robustness but increase latency
  - SLM size: 1.5B-2.4B parameters used; smaller models may lack contextual reasoning
  - Normalization: Required because different SLMs have different score distributions

- Failure signatures:
  - High false negatives on partial responses: Check if harmonic mean is used
  - Inconsistent scores across runs: Verify normalization statistics are computed from sufficient data
  - Poor performance on domain-specific content: SLMs may lack domain knowledge
  - Threshold sensitivity: F1 scores vary significantly with threshold

- First 3 experiments:
  1. Baseline comparison: Replicate Table III results comparing Proposed vs. ChatGPT vs. P(yes) vs. single-SLM approaches
  2. Mean aggregation ablation: Test arithmetic, geometric, harmonic, max, min aggregation methods
  3. Score distribution analysis: Generate histograms of sᵢ for correct/wrong/partial responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of gating mechanisms into the SLM framework improve verification accuracy or efficiency?
- Basis in paper: Conclusion states future research could focus on "better integration of SLMs, such as adding gating mechanisms"
- Why unresolved: Current framework uses normalization and averaging without dynamic weighting or gating
- What evidence would resolve it: Experimental results comparing current averaging method against Mixture-of-Experts or gating-based approach

### Open Question 2
- Question: How does the framework perform when verifying responses in domains other than HR policies or when using different data types?
- Basis in paper: Conclusion suggests optimizing "the framework's performance on different types of data"
- Why unresolved: Experiments restricted to specific HR handbook dataset
- What evidence would resolve it: Benchmarks on datasets from distinct domains showing consistent F1 improvements

### Open Question 3
- Question: Can the proposed method be effectively integrated with verification frameworks that extract information online to check general context?
- Basis in paper: Conclusion proposes integrating with verification frameworks to "extract additional information online for checking general context"
- Why unresolved: Current system relies solely on static, pre-vectorized database
- What evidence would resolve it: System architecture and performance metrics demonstrating successful hallucination detection using live web search context

### Open Question 4
- Question: Does the framework exhibit varying detection performance across the distinct hallucination types (Logical, Prompt, Factual) defined in the introduction?
- Basis in paper: Table I defines three specific hallucination types, but experimental results aggregate performance without analyzing error rates by type
- Why unresolved: Unclear if "Yes" token probability method is equally effective for logical contradictions as factual errors
- What evidence would resolve it: Breakdown of F1 scores specific to Logical, Prompt, and Factual contradiction categories

## Limitations

- Dataset Specificity: Experiments limited to HR handbook domain with limited size (>100 samples), limiting generalizability
- SLM Calibration: Framework relies on stable token probability calibration but doesn't address how normalization parameters are computed or maintained
- Aggregation Sensitivity: Effectiveness of harmonic mean for partial response detection demonstrated but may not generalize across different types of partial correctness

## Confidence

- High Confidence: Framework's core architecture is clearly specified and reproducible; claim that sentence-level verification prevents averaging effects and multiple SLMs improve performance is well-supported
- Medium Confidence: 10% F1 improvement claim is statistically valid for specific dataset tested but generalizability to other domains remains uncertain
- Low Confidence: Claims about scalability and efficiency benefits lack empirical validation; assumption that SLMs have independent failure modes sufficient for robust aggregation is plausible but not rigorously tested

## Next Checks

- Check 1: Test framework generalization on diverse domains - Apply methodology to at least two additional datasets from different domains and verify if 10% improvement holds
- Check 2: Validate normalization parameter stability - Train on Lane Crawford dataset, compute μₘ and σₘ on training set only, and assess whether score distributions remain consistent
- Check 3: Conduct ablation study on aggregation methods - Systematically compare harmonic mean, arithmetic mean, geometric mean, maximum, and minimum aggregation across multiple datasets and response types