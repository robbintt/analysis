---
ver: rpa2
title: 'Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video
  Generation'
arxiv_id: '2512.03040'
source_url: https://arxiv.org/abs/2512.03040
tags:
- video
- context
- frames
- arxiv
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VIDEO4SPATIAL, a framework that enables video
  generative models to perform visuospatial reasoning tasks using only video-based
  scene context, without auxiliary modalities like depth or poses. The core idea is
  to condition a standard video diffusion model on video context and instructions,
  leveraging design choices such as joint classifier-free guidance, auxiliary bounding
  boxes for object grounding, and non-contiguous context sampling with RoPE.
---

# Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation

## Quick Facts
- arXiv ID: 2512.03040
- Source URL: https://arxiv.org/abs/2512.03040
- Authors: Zeqi Xiao; Yiwei Zhao; Lingxiao Li; Yushi Lan; Ning Yu; Rahul Garg; Roshni Cooper; Mohammad H. Taghavi; Xingang Pan
- Reference count: 40
- Primary result: Video generative model achieves strong spatial reasoning using only video context, without auxiliary depth/pose modalities

## Executive Summary
VIDEO4SPATIAL introduces a framework that enables video generative models to perform visuospatial reasoning tasks using only video-based scene context. The method conditions a standard video diffusion model on video context and instructions, leveraging design choices such as joint classifier-free guidance, auxiliary bounding boxes for object grounding, and non-contiguous context sampling with RoPE. The framework is evaluated on video-based object grounding and scene navigation tasks, demonstrating strong spatial understanding and competitive performance in grounding accuracy and geometric consistency.

## Method Summary
VIDEO4SPATIAL builds on Wan2.2 video diffusion transformer, conditioning it on video context frames and instructions. The model uses joint classifier-free guidance over context and instruction, auxiliary bounding boxes as explicit reasoning patterns, and non-contiguous context sampling with non-continuous RoPE. Training data consists of video pairs from ScanNet++/ARKitScenes with VLM-generated instructions and bbox annotations. The model generates videos with optional bounding box frames to improve grounding accuracy.

## Key Results
- Achieves 10× improvement in joint grounding-consistency (SD from 0.8294 to 0.1099) with joint CFG
- 20% relative improvement in grounding accuracy (IF from 0.5401 to 0.6486) with auxiliary bbox
- Successfully generalizes to longer inference contexts (337 frames) beyond training context (169 frames)

## Why This Works (Mechanism)

### Mechanism 1: Joint Classifier-Free Guidance Over Context and Instruction
- Claim: Extending CFG to jointly condition on both video context and instruction substantially improves spatial consistency and grounding accuracy compared to text-only CFG.
- Mechanism: The model learns to denoise with randomly dropped conditions during training. At inference, the guidance amplifies the signal from both the visual context and the instruction simultaneously.
- Core assumption: The video context contains sufficient geometric and semantic information for the model to infer 3D structure without explicit depth or pose signals.
- Evidence anchors:
  - Table 1: Without context CFG (SD=0.8294, IF=0.0373) vs. with full method (SD=0.1099, IF=0.6486)
  - Section 3.4: "Joint CFG significantly improves the quality and consistency of the output."
- Break condition: If context is extremely short (1 frame), CFG provides limited benefit because the model lacks sufficient evidence for grounding.

### Mechanism 2: Auxiliary Bounding Box as an Explicit Reasoning Pattern
- Claim: Training the model to generate visual bounding boxes around target objects in the final frames improves grounding accuracy by enforcing an explicit localization objective.
- Mechanism: The video output is augmented with red pixel-value bounding boxes centered on the target object.
- Core assumption: The model can learn to correlate the text instruction, navigation path, and final localization through the shared diffusion objective.
- Evidence anchors:
  - Table 1: With auxiliary bbox (IF=0.6486) vs. without (IF=0.5401)
  - Section 3.4: "Injecting explicit reasoning patterns have been shown to improve language models' capabilities."
- Break condition: If the bbox is poorly annotated or the target object is occluded/ambiguous in the final frames.

### Mechanism 3: Non-Contiguous Context Sampling with Non-Continuous RoPE
- Claim: Sampling sparse, non-contiguous context frames while retaining their original temporal indices in RoPE enables efficient long-context understanding and improves extrapolation to longer inference contexts.
- Mechanism: Redundant adjacent frames are subsampled during training. RoPE positional embeddings use the original source indices.
- Core assumption: The model can interpolate spatial structure between sampled frames without requiring dense temporal coverage.
- Evidence anchors:
  - Table 3: Non-continuous sampling (SD=0.1099, IF=0.6486) vs. continuous (SD=0.3246, IF=0.4600)
  - Figure 8: Models trained with 169 frames generalize to 337 inference frames with improved SD and IF.
- Break condition: If the sampling gap exceeds the model's interpolation capacity (e.g., >10 frame intervals).

## Foundational Learning

- **Video Diffusion Models (DiT + Latent Diffusion)**
  - Why needed here: VIDEO4SPATIAL builds on Wan2.1/Wan2.2, a transformer-based latent video diffusion model. Understanding how DiT processes temporal/spatial attention and how latent compression works is essential for modifying the architecture.
  - Quick check question: Can you explain how a 3D video latent is factorized across spatial and temporal axes in a video DiT?

- **Classifier-Free Guidance (CFG)**
  - Why needed here: Joint CFG over context and instruction is a core contribution. Understanding the standard CFG formulation and how guidance scale affects output fidelity vs. diversity is prerequisite to implementing the joint extension.
  - Quick check question: What happens to output diversity as CFG scale ω increases from 1 to 10?

- **Rotary Positional Embeddings (RoPE)**
  - Why needed here: Non-continuous RoPE indexing is critical for sparse context handling. Understanding how RoPE encodes relative positions and why it generalizes better than absolute embeddings for extrapolation is necessary.
  - Quick check question: How does RoPE handle attention between tokens at positions 0 and 100 vs. positions 0 and 10?

## Architecture Onboarding

- **Component map:** Context frames (337) -> VAE latents -> concatenated with target latents -> DiT (context at t=0, targets at t>0) -> VAE decode -> RGB video (161+20 frames with optional bbox)

- **Critical path:**
  1. Curate video pairs (context + target) from ScanNet++/ARKitScenes with VLM-generated instructions and bbox annotations.
  2. Pre-encode all frames to latents; sample non-contiguous context groups.
  3. Train with flow-matching objective, joint CFG dropout (context + instruction), and bbox-supervised final frames.
  4. Inference: provide 337 context frames + instruction -> generate 161 frames + 20 bbox frames with CFG scale 3-7.

- **Design tradeoffs:**
  - Resolution vs. efficiency: 416×256 is computationally tractable but limits visual fidelity; context compression could enable higher resolutions.
  - Training context length: 169 frames balances spatial consistency and instruction following; 337 frames slightly degrades IF due to over-constraint.
  - Bbox frames: 20 auxiliary frames improve grounding but add generation overhead.

- **Failure signatures:**
  - Temporal discontinuities: Sudden jumps between frames; indicates insufficient temporal modeling or excessive sampling gaps.
  - Hallucinated objects: Target object appears but is not present in context; indicates weak context CFG or insufficient context frames.
  - Incorrect grounding: Model drifts to wrong object; indicates bbox supervision failure or ambiguous instruction.

- **First 3 experiments:**
  1. Ablate CFG components: Run inference with no CFG, text-only CFG, and joint CFG on 20 validation scenes; report SD and IF(SD<0.2).
  2. Vary context length at inference: Train with 169 frames; test with 1, 45, 169, 337 context frames; plot SD and IF curves.
  3. Test OOD generalization: Run grounding on 5 outdoor scenes with unseen object categories; report SD and IF.

## Open Questions the Paper Calls Out

- Can the framework be extended to complex, dynamic environments and a broader range of tasks beyond static scene navigation?
- What context compression techniques can be integrated to support higher resolutions without sacrificing spatial consistency?
- How can grounding objectives and data augmentation be improved to mitigate temporal discontinuities and incorrect grounding on long-tail object categories?

## Limitations
- Claim that video context alone suffices for spatial reasoning remains unproven for extremely complex scenes or instructions requiring deep 3D reasoning
- Auxiliary bounding box mechanism relies on VLM-generated annotations that may introduce systematic errors
- Non-continuous context sampling assumes the model can interpolate between sparse frames, which may fail when critical spatial information exists only in skipped frames

## Confidence
- **High Confidence**: Core architectural framework is sound and quantitative improvements over baseline methods are statistically significant
- **Medium Confidence**: Claim that video context alone suffices for spatial reasoning is plausible but requires validation on more diverse scenarios
- **Medium Confidence**: Auxiliary bounding box mechanism improves grounding, but reliance on VLM-generated annotations introduces potential biases

## Next Checks
1. Test the model with inference contexts of 500+ frames on both indoor and outdoor scenes to rigorously evaluate the RoPE extrapolation claims
2. Create a test set with manually verified vs. VLM-generated bounding boxes to measure grounding accuracy degradation
3. Design multi-step navigation tasks requiring object permanence across view changes to test true 3D spatial understanding