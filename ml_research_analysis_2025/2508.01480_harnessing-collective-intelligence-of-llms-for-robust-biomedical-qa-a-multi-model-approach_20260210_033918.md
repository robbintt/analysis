---
ver: rpa2
title: 'Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model
  Approach'
arxiv_id: '2508.01480'
source_url: https://arxiv.org/abs/2508.01480
tags:
- llms
- question
- questions
- list
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a multi-model LLM approach for biomedical
  question answering in the BioASQ challenge, addressing the problem of generating
  accurate and reliable answers from the growing biomedical literature. The method
  uses retrieval-augmented generation with 13 open-source LLMs, applying different
  combination strategies per question type: majority voting for Yes/No questions,
  union of answers for List and Factoid questions, and optimized model subsets for
  each category.'
---

# Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach

## Quick Facts
- arXiv ID: 2508.01480
- Source URL: https://arxiv.org/abs/2508.01480
- Authors: Dimitra Panou; Alexandros C. Dimopoulos; Manolis Koubarakis; Martin Reczko
- Reference count: 19
- Key outcome: Multi-model LLM ensemble achieved first place for ideal answers and shared first places for exact answers in BioASQ Synergy task 2025

## Executive Summary
This paper presents a multi-model LLM approach for biomedical question answering in the BioASQ challenge, addressing the problem of generating accurate and reliable answers from the growing biomedical literature. The method uses retrieval-augmented generation with 13 open-source LLMs, applying different combination strategies per question type: majority voting for Yes/No questions, union of answers for List and Factoid questions, and optimized model subsets for each category. For Factoid questions, the best results came from combining six LLMs, while List and Yes/No questions benefited from smaller combinations (3-4 models). The approach achieved strong performance in the 2025 BioASQ challenge, securing first place for ideal answers and multiple shared first places for exact answers in the Synergy task. This demonstrates that combining complementary LLMs significantly improves biomedical QA accuracy.

## Method Summary
The method combines retrieval-augmented generation with ensemble aggregation of 13 open-source LLMs. For each question, a BM25+RM3 retrieval pipeline fetches top 50 PubMed documents, which are re-ranked to 10 based on snippet relevance. The optimal LLM subsets are then selected per question type: six models for Factoid questions, three for List questions, and three for Yes/No questions. Answers are combined using majority voting for Yes/No, union with deduplication for List questions (cosine similarity threshold 0.7), and union with relevance score aggregation for Factoid questions. This approach leverages complementary strengths of different models while minimizing individual weaknesses.

## Key Results
- Achieved first place for ideal answers and shared first places for exact answers in BioASQ Synergy task 2025
- Factoid questions: MRR optimal with union of 6 LLMs (DeepSeek-R1-Distill-Llama-70B, LLaMA 3.3, Qwen3-14B, Qwen3-32B, Reflection, Smaug)
- List questions: F1 optimal with union of 3 LLMs (DeepSeek-R1-Distill-Llama-70B, LLaMA 3.3, Qwen3-14B)
- Yes/No questions: Accuracy improved with majority voting of 3-4 models (Aya, Qwen3-32B, Smaug)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Union aggregation of multiple LLM outputs captures complementary answer coverage for factoid and list questions.
- Mechanism: Different LLMs retrieve different relevant entities due to varied training data, tokenization, and attention patterns. The union operation preserves all unique predictions. For factoids, relevance scores from each LLM weight the merged ranking. For lists, simple union followed by deduplication aggregates diverse answers.
- Core assumption: Individual LLMs have non-overlapping failure modes—what one misses, another captures.
- Evidence anchors:
  - [abstract] "union of their answers is used" for list and factoid questions
  - [section 2.3.1] "larger unions give better results is very likely due to the complementarity of the answers of the different LLMs"
  - [corpus] Related work on LLM ensembles (arxiv 2509.08596) explores similar zero-shot ensemble strategies for BioASQ

### Mechanism 2
- Claim: Majority voting reduces classification errors for binary Yes/No questions by canceling out individual model biases.
- Mechanism: Each LLM independently classifies; the final answer follows the majority count. This exploits the statistical principle that independent errors tend to cancel when aggregated, assuming models err in different directions.
- Core assumption: Model errors are not systematically biased in the same direction.
- Evidence anchors:
  - [abstract] "A majority voting system combines their output to determine the final answer for Yes/No questions"
  - [section 2.3.3] "The final answer will be 'Yes' if there are a higher or equal number of 'Yes' outcomes than 'No' outcomes"
  - [corpus] Limited direct corpus evidence on majority voting for biomedical QA specifically

### Mechanism 3
- Claim: Task-specific ensemble sizing optimizes the precision-recall tradeoff differently for each question type.
- Mechanism: For factoids, larger ensembles (6 models) improve MRR because missing a relevant entity is costlier than including lower-ranked ones. For lists, smaller ensembles (3 models) optimize F1 because false positives directly harm precision. For Yes/No, small juries (3-4 models) suffice since output space is binary.
- Core assumption: The cost function implicit in each evaluation metric (MRR, F1, accuracy) reflects real-world utility.
- Evidence anchors:
  - [section 2.3.1] "highest performances are obtained with unions of 6 LLMs" for factoids
  - [section 2.3.2] "large subsets with more than 7 LLMs have significantly lower performance" for list questions
  - [section 2.3.3] "combinations of a few LLMs like [Aya, Qwen32, Smaug] that outperform any of the individual LLMs alone"
  - [corpus] Related work on evidence-grounded QA (arxiv 2507.02975) suggests ensemble benefits depend on task structure

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: All LLMs operate on retrieved PubMed abstracts/snippets rather than parametric knowledge alone, reducing hallucination.
  - Quick check question: Can you explain why grounding LLM outputs in retrieved documents reduces factual errors compared to pure generation?

- Concept: **Mean Reciprocal Rank (MRR)**
  - Why needed here: Factoid evaluation uses MRR, which rewards placing the correct answer early in the ranked list.
  - Quick check question: If the correct answer appears at positions 1, 3, and 7 across three questions, what is the MRR?

- Concept: **Ensemble Diversity vs. Correlation**
  - Why needed here: The paper's "farming" strategy assumes diverse models provide complementary coverage.
  - Quick check question: Why would three models trained on identical data with different random seeds provide less ensemble benefit than three architecturally distinct models?

## Architecture Onboarding

- Component map:
  - Question -> Retrieval (BM25+RM3) -> Documents/Snippets -> Prompt Construction -> LLM Farm (13 models) -> Aggregation Layer -> Final Answer

- Critical path:
  1. Question → Retriever → Documents/Snippets
  2. Prompt construction (question + snippets OR abstracts)
  3. Parallel inference across all models in the optimal subset
  4. Type-specific aggregation → Final answer

- Design tradeoffs:
  - Snippets vs. full abstracts: Snippets are faster; abstracts provide broader context and outperformed snippets in evaluation (Section 2.3)
  - Ensemble size: Larger for factoids (coverage), smaller for lists (precision), smallest for Yes/No (sufficiency)
  - Deduplication threshold: Higher threshold = more aggressive merging; paper found 0.76 optimal for lists but didn't improve factoids

- Failure signatures:
  - List explosion: Too many models → false positives dominate → F1 drops (Figure 6)
  - Voting ties: Even split on Yes/No questions → paper defaults to "Yes" which may introduce bias
  - Retrieval bottleneck: Low MAP in Phase A (0.01-0.04) limits ceiling for generation quality regardless of ensemble

- First 3 experiments:
  1. Baseline single-model: Run each of the 13 LLMs individually on BioASQ12 test set; record MRR/F1/accuracy per question type to establish individual model rankings.
  2. Ablation by ensemble size: For factoid questions, measure MRR for subsets of size 1 through 6 using the best-performing models; plot the curve to validate the 6-model optimum.
  3. Deduplication sweep: For list questions, test cosine similarity thresholds from 0.5 to 0.9 in 0.05 increments; identify the F1-optimal threshold and compare to paper's 0.7.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can confidence-weighted aggregation mechanisms further improve performance over simple majority voting and union strategies for biomedical QA?
- Basis in paper: [explicit] Authors state they plan to "incorporate more confidence scoring mechanisms across model outputs to better weigh and reconcile conflicting answers" in future work.
- Why unresolved: The current approach uses simple combination strategies (majority voting for Yes/No, union for List/Factoid) without weighting individual model outputs by confidence.
- What evidence would resolve it: Experiments comparing current aggregation methods against confidence-weighted approaches on BioASQ test sets, showing whether weighted combinations outperform the optimal fixed subsets identified in this work.

### Open Question 2
- Question: What underlying factors cause optimal ensemble sizes to differ across question types (6 models for factoid vs. 3-4 for list and yes/no)?
- Basis in paper: [inferred] The paper demonstrates that different question types require different optimal model subset sizes but does not investigate why factoid questions benefit from larger ensembles while list and yes/no questions degrade with too many models.
- Why unresolved: The authors observe the pattern across BioASQ11 and BioASQ12 datasets but attribute it only generally to "complementarity" without analyzing the specific error modes or answer distributions that drive these differences.
- What evidence would resolve it: Analysis of individual model error patterns by question type, measuring answer overlap, unique correct predictions per model, and false positive rates as ensemble size increases.

### Open Question 3
- Question: Why does semantic deduplication improve F1 for list questions but fails to improve MRR for factoid questions?
- Basis in paper: [inferred] The paper shows deduplication at threshold 0.7 improves list question performance (optimal at 0.76), but Figure 4 shows deduplication does not improve factoid MRR, consistent with the observation that larger subsets generally perform better for factoids.
- Why unresolved: This contrasting behavior suggests fundamental differences in how redundancy manifests across answer types, but no explanation is provided for why removing similar answers helps one type but not the other.
- What evidence would resolve it: Comparative analysis of duplicate vs. unique factoid and list answers across models, examining whether semantically similar factoids tend to be correct (preserving them helps) while similar list items introduce noise (removing them helps).

## Limitations

- Opaque ensemble aggregation: The exact mechanism for extracting and combining relevance scores from individual LLMs remains unspecified, creating a reproducibility barrier.
- Potential overfitting: Optimal ensemble sizes are tuned to BioASQ's specific evaluation metrics and may not generalize to other biomedical QA tasks or real-world deployment scenarios.
- Retrieval bottleneck: With Phase A MAP scores of only 0.01-0.04, generation quality is fundamentally constrained by upstream retrieval performance.

## Confidence

- **High confidence** in the core insight that LLM ensembles outperform individual models for biomedical QA, supported by both quantitative results and qualitative evidence of complementarity
- **Medium confidence** in the optimal ensemble sizing per question type, given the strong competition results but limited ablation analysis across diverse datasets
- **Low confidence** in the reproducibility of the exact aggregation mechanism, due to underspecified details about score extraction and combination

## Next Checks

1. **Correlation Analysis:** Measure pairwise correlation coefficients between the top 6 factoid models on a held-out set to empirically quantify complementarity. Calculate how much unique coverage each model provides versus redundancy.

2. **Ensemble Robustness Test:** Replicate the factoid ensemble results on a different biomedical QA dataset (e.g., PubMedQA) using the same 6-model combination to assess generalizability beyond BioASQ's specific distribution.

3. **Ablation on Score Aggregation:** Implement alternative methods for combining LLM confidence scores (e.g., max, min, geometric mean) and measure impact on MRR to determine whether the reported gains are robust to the specific aggregation choice.