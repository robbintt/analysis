---
ver: rpa2
title: 'Understanding Large Language Models in Your Pockets: Performance Study on
  COTS Mobile Devices'
arxiv_id: '2410.03613'
source_url: https://arxiv.org/abs/2410.03613
tags:
- uni00000014
- uni00000011
- uni00000013
- uni00000003
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive measurement study of on-device
  large language model (LLM) inference across commercial-off-the-shelf (COTS) mobile
  devices. The authors deploy six lightweight LLM models (Llama2/3.2, Qwen3, Gemma3)
  on seven devices from major SoC vendors (Qualcomm, HiSilicon, MediaTek, Apple) using
  llama.cpp and MLC LLM inference engines.
---

# Understanding Large Language Models in Your Pockets: Performance Study on COTS Mobile Devices

## Quick Facts
- **arXiv ID:** 2410.03613
- **Source URL:** https://arxiv.org/abs/2410.03613
- **Authors:** Jie Xiao; Qianyi Huang; Xu Chen; Chen Tian
- **Reference count:** 40
- **Primary result:** 4-bit quantization provides optimal balance between accuracy and throughput for mobile LLM inference

## Executive Summary
This paper presents a comprehensive measurement study of on-device large language model (LLM) inference across commercial-off-the-shelf (COTS) mobile devices. The authors deploy six lightweight LLM models (Llama2/3.2, Qwen3, Gemma3) on seven devices from major SoC vendors (Qualcomm, HiSilicon, MediaTek, Apple) using llama.cpp and MLC LLM inference engines. Through systematic evaluation of user-centric metrics (token throughput, latency, response quality) and developer-critical factors (resource utilization, OS strategies, battery consumption, launch time), they identify key bottlenecks in mobile LLM deployment. The study reveals that 4-bit quantization provides optimal balance between accuracy and throughput, CPU performance is limited by memory-bound decoding stages, and GPU utilization remains suboptimal due to inefficient memory access patterns and lack of device-specific optimization. The findings highlight the need for hardware-aware quantization strategies, customized operator implementations, and dynamic resource allocation to improve mobile LLM performance.

## Method Summary
The study deploys llama.cpp and MLC LLM inference engines on seven mobile devices spanning major SoC vendors (Snapdragon, Dimensity, Kirin, Apple M1). Six models are tested: Llama2-7B, Llama3.2-1B/3B, Qwen3-1.7B/4B, and Gemma3-1B in various quantization formats (Q4_0, Q4_K_M, Q8_0, F16). Performance metrics include prefill/decode throughput, accuracy on MMLU/ARC/TruthfulQA, CPU/GPU utilization, power consumption, memory bandwidth, and DVFS behavior. The methodology uses Perfetto and Snapdragon Profiler for tracing, with thermal management studied through repeated inference runs. Testing is conducted across different thread counts, quantization schemes, and execution engines to isolate bottlenecks.

## Key Results
- 4-bit quantization (Q4_0, Q4_K_M) provides optimal balance between accuracy and throughput, with 3-bit models suffering accuracy loss and 8-bit models showing no significant speed advantage
- CPU decode performance is memory-bound rather than compute-bound, with diminishing returns when adding efficiency cores beyond big core count
- GPU utilization remains suboptimal (5-20%) due to memory access patterns and lack of device-specific optimization, despite higher peak throughput potential in prefill stage

## Why This Works (Mechanism)

### Mechanism 1: Specialized Instruction Acceleration for CPU Prefill
- **Claim:** Enabling specialized CPU instructions significantly increases throughput in the prefill stage if accompanied by memory layout optimization.
- **Mechanism:** Armv9-A CPUs (e.g., Cortex-X4) support `smmla` and `sdot` instructions (SVE/NEON) for INT8 matrix multiplication. However, standard weight layouts process single columns sequentially. By rearranging weights into 32×16 blocks (as in Q4_0 quantization), data is distributed across multiple vector lanes, allowing parallel processing and reducing memory access overhead.
- **Core assumption:** The overhead of rearranging weights is amortized by the speedup in matrix multiplication.
- **Evidence anchors:**
  - [abstract] Mentions Armv9-A CPUs with specialized instructions deliver gains comparable to iOS.
  - [Section V-A2] Shows `smmla` yields up to 6× speed improvement for Q4_0 vs. Q4_K_M due to weight rearrangement.
  - [corpus] Evidence is weak or missing; related papers focus on LLM reasoning quality rather than hardware instruction sets.
- **Break condition:** If the CPU lacks `smmla`/`sdot` support (older Armv8-A cores) or if the quantization scheme requires excessive dequantization logic per element (e.g., Q3_K_M), the vectorized instruction benefit is nullified.

### Mechanism 2: Memory-Bound Decoding Saturation
- **Claim:** Increasing thread count or clock speed yields diminishing returns during the decode stage because the bottleneck shifts from compute to memory bandwidth.
- **Mechanism:** The decode stage is autoregressive (generating one token at a time). It requires fetching the entire model's weights from RAM to compute a single token. Unlike prefill (compute-bound), this process saturates memory bandwidth quickly. Adding "little" cores (efficiency cores) often degrades performance because they compete for the same limited memory bus without contributing significant compute power.
- **Core assumption:** The memory bus cannot service requests from all cores simultaneously at peak throughput.
- **Evidence anchors:**
  - [abstract] States CPU performance is limited by memory-bound decoding.
  - [Section V-A1] Shows that adding efficiency cores (little cores) drops decode throughput on devices like Snapdragon 8 Gen3.
  - [corpus] Evidence is weak or missing; corpus papers do not analyze memory hierarchy bottlenecks in autoregressive decoding.
- **Break condition:** If memory bandwidth scales perfectly with core count (unlikely in current SoCs) or if model weights are fully resident in a massive L4 cache (not present in tested devices), this mechanism would not hold.

### Mechanism 3: GPU Utilization via Memory Coalescing
- **Claim:** Low GPU utilization (5-20%) in general-purpose engines is caused primarily by non-contiguous memory access patterns rather than a lack of raw compute units.
- **Mechanism:** General LLM implementations (e.g., MLC LLM) use generic kernel parameters. On GPUs like Adreno/Mali, accessing non-contiguous weight matrices causes partial-width reads and L2 cache misses. This stalls the Arithmetic Logic Units (ALUs) as they wait for data. Optimizing data layout into contiguous tiles improves memory coalescing, directly feeding the ALUs and raising utilization.
- **Core assumption:** The GPU has sufficient ALU capacity but insufficient cache/memory efficiency for naive access patterns.
- **Evidence anchors:**
  - [abstract] Notes GPU utilization remains suboptimal (5-20%) due to inefficient memory access.
  - [Section V-B1] Correlates 60% L2 cache miss rates and partial-width reads with low ALU usage on Adreno 750.
  - [corpus] Evidence is weak or missing; corpus does not discuss GPU cache miss rates or OpenCL tuning.
- **Break condition:** If the model size fits entirely within the GPU's high-bandwidth internal cache, or if the operators are already optimized for the specific GPU architecture, this bottleneck vanishes.

## Foundational Learning
- **Concept: big.LITTLE CPU Architecture**
  - **Why needed here:** The paper emphasizes that simply using "all cores" degrades performance. You must understand that "big" (performance) cores handle the heavy lifting, while "little" (efficiency) cores can interfere with memory bandwidth during LLM decoding.
  - **Quick check question:** On an 8-core CPU (1 Prime, 3 Performance, 4 Efficiency), which configuration likely yields the highest *decode* speed: 4 threads or 8 threads? (Answer based on paper: Likely 4 threads).

- **Concept: Prefill vs. Decode Stages**
  - **Why needed here:** Optimization strategies are stage-dependent. Prefill is parallelizable (compute-bound), favoring GPUs and high core counts. Decode is sequential (memory-bound), favoring memory bandwidth and specialized instructions over raw core count.
  - **Quick check question:** Why does increasing prompt length significantly affect GPU prefill speed but has a negligible effect on CPU decode speed relative to model size? (Answer: Prefill is compute-heavy on attention; Decode is memory-heavy on weight loading).

- **Concept: Quantization (INT4 vs FP16)**
  - **Why needed here:** The paper identifies 4-bit quantization as the "sweet spot." You need to understand that quantization reduces memory footprint (allowing models to fit in RAM) and enables the use of faster integer math instructions (`smmla`), but it introduces a dequantization cost.
  - **Quick check question:** Why might a 3-bit model run slower than a 4-bit model on an 8-bit byte-aligned CPU architecture? (Answer: 3-bit requires bitwise operations to unpack bits from bytes, creating overhead; 4-bit aligns more naturally with byte structures for vectorization).

## Architecture Onboarding
- **Component map:** SoC -> CPU/GPU/NPU -> RAM (Bandwidth limit) -> Inference Engine (llama.cpp for CPU, MLC LLM for GPU)
- **Critical path:**
  1. **Model Loading:** `mmap` loads weights (slow cold start, fast warm start)
  2. **Prefill:** Processes prompt in parallel (Compute-bound). Best on GPU or multi-core CPU with AVX/SVE instructions
  3. **Decode:** Generates tokens sequentially (Memory-bound). Best on fewer high-frequency big cores
- **Design tradeoffs:**
  - **Accuracy vs. Speed:** 4-bit is generally optimal; 3-bit degrades accuracy; 8-bit is often unnecessary for speed
  - **GPU vs. CPU:** GPU offers higher peak throughput in prefill but requires massive tuning for memory access; CPU is more stable and easier to deploy but slower for prefill
  - **Thermal throttling:** High-frequency sustained inference (e.g., on iPad M1) triggers DVFS throttling faster than GPU offloading
- **Failure signatures:**
  - **Throughput Collapse:** If decode speed drops when moving from 4 to 8 threads, you are hitting memory bandwidth saturation with "little" cores
  - **Stalled GPU:** If GPU utilization is <5% on Mali/Adreno, the issue is likely L2 cache misses due to memory layout, not GPU power
  - **App Crash:** On devices with <4GB available RAM or strict OS limits (iOS), loading 7B models will cause OOM (Out of Memory) crashes
- **First 3 experiments:**
  1. **Thread Affinity Sweep:** Run inference with thread counts matching the number of *big cores* (e.g., 4 threads) vs. *all cores* (e.g., 8 threads) to identify the memory-bound decode limit on your target device
  2. **Quantization Benchmark:** Compare Q4_0 (block-based) vs. Q4_K_M (k-quant) on an Armv9 device to measure the impact of weight rearrangement on `smmla` instruction utilization
  3. **Hybrid Execution Test:** Measure the latency of performing prefill on the GPU and decoding on the CPU to verify if the data transfer overhead is amortized by the GPU's compute advantage

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on 7B parameter models, leaving uncertainty about how findings scale to larger models (13B, 70B) that would be more relevant for production applications but may exceed mobile device memory constraints
- Thermal management analysis is limited to short-duration benchmarks, potentially missing longer-term performance degradation patterns that would affect real-world usage
- The lack of detail on GPU kernel tuning parameters makes exact reproduction challenging, as performance depends heavily on specific tile sizes and prefill chunk configurations

## Confidence
- **High Confidence:** 4-bit quantization providing optimal accuracy-throughput balance; Memory-bound decoding bottleneck
- **Medium Confidence:** Specialized instruction acceleration benefits; GPU utilization being primarily memory-access limited
- **Low Confidence:** OS resource allocation strategy recommendations

## Next Checks
1. **GPU Kernel Tuning Validation:** Replicate the performance measurements using different tile size and prefill chunk configurations on Adreno and Mali GPUs to quantify the impact of kernel tuning on the observed 5-20% utilization rates.

2. **Cross-Model Scaling Study:** Test the same methodology with 13B parameter models to verify whether the 4-bit quantization sweet spot, memory-bound decoding characteristics, and GPU utilization patterns hold at larger model scales.

3. **Long-Duration Thermal Analysis:** Extend the thermal profiling to continuous 30+ minute inference sessions to capture DVFS behavior and performance degradation patterns that emerge over extended usage periods.