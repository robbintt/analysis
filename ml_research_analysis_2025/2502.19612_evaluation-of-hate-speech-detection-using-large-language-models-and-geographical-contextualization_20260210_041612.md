---
ver: rpa2
title: Evaluation of Hate Speech Detection Using Large Language Models and Geographical
  Contextualization
arxiv_id: '2502.19612'
source_url: https://arxiv.org/abs/2502.19612
tags:
- hate
- speech
- comment
- llms
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates the effectiveness of large language models
  (LLMs) in detecting hate speech across multilingual and geographically diverse contexts.
  A dataset of 1,000 comments from five regions was used, translated into English
  for analysis.
---

# Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization

## Quick Facts
- arXiv ID: 2502.19612
- Source URL: https://arxiv.org/abs/2502.19612
- Reference count: 24
- Primary result: Codellama achieved 70.6% recall and 52.18% F1-score for hate speech detection across multilingual, geographically diverse data.

## Executive Summary
This study evaluates three large language models (LLMs)—Llama2, Codellama, and DeepSeekCoder—for hate speech detection across multilingual and geographically diverse contexts. Using 1,000 translated comments from five regions, the researchers tested binary classification, geographic sensitivity, and robustness against adversarial manipulation. Codellama showed the best classification performance (70.6% recall, 52.18% F1), while DeepSeekCoder excelled at geographic identification (63/265 locations). However, Llama2 demonstrated significant vulnerability to adversarial samples, misclassifying 62.5% of manipulated texts. The findings highlight trade-offs between accuracy, contextual understanding, and robustness in current LLMs, emphasizing the need for contextually aware and resilient systems in hate speech detection.

## Method Summary
The study used 1,000 comments from five regions (Arab, Bangladesh, India, China, Russia/Ukraine) in five languages, translated to English via Google Translate API. Three LLMs (Llama2-13b, Codellama-7b, DeepSeekCoder-6.7b) performed zero-shot classification using structured prompts with three in-context examples. Outputs were constrained using [ANSWER] and [LOCATION] tags for automated parsing. Performance was evaluated using accuracy, precision, recall, F1-score, geographic accuracy, and adversarial misclassification rate. Adversarial samples were generated by GPT-4 paraphrasing original comments.

## Key Results
- Codellama achieved the highest recall (70.6%) and F1-score (52.18%) for binary hate speech classification
- DeepSeekCoder correctly identified 63 out of 265 geographic locations, performing best in geographic detection
- Llama2 misclassified 62.5% of adversarial samples, showing significant vulnerability to manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompts with in-context examples enable LLMs to perform hate speech classification without task-specific fine-tuning
- Mechanism: Prompt architecture combines task description with 3 in-context examples showing labeled comments, using structured tags ([ANSWER][/ANSWER], [LOCATION][/LOCATION]) for programmatic parsing. Leverages LLM's pre-trained pattern-matching
- Core assumption: In-context examples sufficiently represent semantic and cultural diversity of hate speech patterns
- Evidence anchors: Codellama's 70.6% recall with structured prompts; 62.5% adversarial misclassification for Llama2 suggests pattern-matching failure
- Break condition: Novel slang, cultural references, or adversarial obfuscation not represented in examples causes pattern-matching failure

### Mechanism 2
- Claim: Geographic context detection emerges from LLMs' pre-trained associations between named entities, cultural markers, and regional patterns
- Mechanism: LLM identifies location-relevant signals from translated comments and maps them to five geographic categories. DeepSeekCoder's superior location detection (63/265) suggests differences in pre-training
- Core assumption: Translation preserves enough geographic and cultural markers for location inference
- Evidence anchors: DeepSeekCoder's 63/265 location accuracy; Codellama's poor location detection (10.5%) despite good classification
- Break condition: Translation strips location-specific references or comments lack explicit geographic markers

### Mechanism 3
- Claim: Adversarial robustness varies across LLM architectures based on sensitivity to surface-level lexical perturbations versus semantic understanding
- Mechanism: GPT-4 generates adversarial samples by paraphrasing original comments. Model robustness depends on whether classification relies on superficial token patterns or deeper semantic understanding
- Core assumption: Synthetic adversarial samples approximate real-world adversarial manipulation strategies
- Evidence anchors: Llama2's 62.5% flip rate vs. DeepSeekCoder and Codellama's 1-2 misclassifications out of 22-25 correct predictions
- Break condition: Adversarial perturbations too aggressive (destroying semantic content) or too subtle (not changing model behavior)

## Foundational Learning

- Concept: **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: Entire methodology depends on prompting pre-trained LLMs with examples rather than fine-tuning
  - Quick check question: Can you explain why providing 3 in-context examples might improve hate speech detection compared to zero-shot prompting?

- Concept: **Precision-Recall Trade-off in Binary Classification**
  - Why needed here: Codellama's 70.6% recall but 35.3% accuracy suggests class imbalance and bias toward hate speech class
  - Quick check question: If a model has high recall (70%) but low accuracy (35%), what does this suggest about its prediction bias?

- Concept: **Adversarial Robustness in NLP**
  - Why needed here: Third evaluation dimension tests whether paraphrasing can flip labels
  - Quick check question: Why might paraphrasing "retard liberals" to "clueless thinkers" cause an LLM to flip its hate speech classification?

## Architecture Onboarding

- Component map: Data sampling -> Translation -> Prompt construction -> LLM inference -> Output parsing -> Metric computation
- Critical path: Data sampling → Translation → Prompt construction → LLM inference → Output parsing → Metric computation. Translation quality directly affects downstream classification; adversarial generation is parallel evaluation branch
- Design tradeoffs:
  1. Translation-first vs. Native Multilingual: Chose translation for compatibility with non-multilingual LLMs, but introduces sentiment preservation risk
  2. Pre-trained vs. Fine-tuned: Using pre-trained models enables rapid evaluation but sacrifices task-specific performance
  3. Structured vs. Open Output: Constrained tag output ([ANSWER]) enables automated parsing but may limit model reasoning expression
- Failure signatures:
  1. High recall, low accuracy: Codellama's pattern suggests over-prediction of hate speech class
  2. Adversarial vulnerability spike: Llama2's 62.5% flip rate indicates reliance on surface features
  3. Geographic detection dissociation: Strong hate speech detection but poor location detection (Codellama: 353 correct hate speech, only 37 correct locations)
- First 3 experiments:
  1. Baseline replication: Run same 1,000-comment evaluation on Llama3 or GPT-4 using provided prompts
  2. Translation quality ablation: Compare detection performance using human translation vs. Google Translate vs. LLM translation on 200-comment subset
  3. Adversarial robustness scaling: Expand testing from 50 to 200+ samples with multiple perturbation types

## Open Questions the Paper Calls Out

- Can targeted fine-tuning on larger curated multilingual datasets simultaneously improve classification accuracy, geographic contextualization, and adversarial robustness?
- How does sentiment-preserving translation quality from LLMs compare to Google Translate when optimized for hate speech detection?
- Do synthetic adversarial samples generated by LLMs accurately represent real-world adversarial manipulation patterns?
- What architectural characteristics explain why certain LLMs excel at geographic contextualization while others excel at hate speech classification?

## Limitations
- Translation dependency introduces uncertainty about semantic and contextual nuance preservation
- Dataset transparency issues with unavailable original 10,000-comment dataset and selection methodology
- Adversarial sample representativeness limited to GPT-4-generated paraphrasing on small sample size
- Architecture-specific focus without exploring generalizability across models or sizes

## Confidence

- **High Confidence**: Codellama's 70.6% recall and 52.18% F1-score for classification; Llama2's 62.5% adversarial misclassification rate
- **Medium Confidence**: DeepSeekCoder's 63/265 location accuracy; dissociation between hate speech and geographic detection capabilities
- **Low Confidence**: Translation-first methodology optimality; vague claim that LLMs "still need improvement" without specific benchmarks

## Next Checks

1. **Translation quality ablation study**: Compare hate speech detection performance across three translation methods (Google Translate API, professional human translation, LLM-based translation) on 200-comment subset

2. **Adversarial attack surface expansion**: Generate and test 200+ adversarial samples using multiple perturbation types across all three LLM architectures

3. **Multilingual native vs. translation comparison**: Evaluate same 1,000-comment dataset using multilingual LLM (mBERT/XLM-R) that processes non-English text directly, comparing to translation-based approach