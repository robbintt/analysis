---
ver: rpa2
title: 'Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR
  to Knowledge Reasoning'
arxiv_id: '2509.09731'
source_url: https://arxiv.org/abs/2509.09731
tags:
- qwen2
- ancient
- chinese
- gpt-4o
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning

## Quick Facts
- **arXiv ID:** 2509.09731
- **Source URL:** https://arxiv.org/abs/2509.09731
- **Reference count:** 9
- **Primary result:** AncientDoc dataset benchmark for Chinese ancient documents

## Executive Summary
This paper introduces AncientDoc, the first comprehensive dataset for evaluating Vision-Language Models on Chinese ancient documents spanning from the Tang dynasty to the Republican period. The dataset encompasses six document types including calendars, poems, and medicinal recipes, enabling systematic assessment of OCR and knowledge reasoning capabilities. The authors evaluate 10 leading VLMs across six tasks, revealing significant performance gaps particularly in complex reasoning scenarios involving cultural knowledge integration.

The benchmark fills a critical gap in document processing research by providing standardized evaluation for historical Chinese texts, which present unique challenges due to complex layouts, classical language, and cultural context requirements. The study demonstrates that while modern VLMs achieve strong OCR performance on ancient texts, their reasoning capabilities remain limited, with top models achieving only around 44% accuracy on knowledge reasoning tasks.

## Method Summary
The authors constructed AncientDoc by collecting high-resolution images of Chinese ancient documents from various historical periods, focusing on six representative document types. Each document was carefully annotated with ground truth text transcriptions and knowledge-based questions requiring cultural understanding. The evaluation framework consists of six tasks: two OCR tasks (vertical and horizontal text extraction), two document understanding tasks (layout analysis and content comprehension), and two knowledge reasoning tasks (temporal and cultural inference). Ten state-of-the-art VLMs including GPT-4V, Qwen-VL-Chat, and Gemini-Pro were systematically evaluated using standardized metrics.

## Key Results
- GPT-4V achieved 82.9% accuracy on vertical text OCR and 79.8% on horizontal text OCR
- Qwen-VL-Chat reached 94.3% accuracy on document understanding tasks
- Knowledge reasoning performance remained challenging with GPT-4V at 41.4% and Qwen-VL-Chat at 44.6% accuracy
- Significant performance variation across document types, with poems and letters showing better results than calendars and medicinal recipes

## Why This Works (Mechanism)
The benchmark succeeds by providing a standardized evaluation framework that captures the full spectrum of challenges in Chinese ancient document processing, from basic text extraction to complex cultural reasoning. The systematic approach reveals that current VLMs excel at pattern recognition for OCR but struggle with the cultural and temporal context required for knowledge reasoning. The diverse document types ensure that models cannot rely on single-task optimization strategies.

## Foundational Learning
- **Classical Chinese Language Processing** - Understanding classical Chinese grammar and vocabulary is essential for accurate text extraction and interpretation. Quick check: Model correctly parses classical Chinese sentence structures without modern language interference.
- **Historical Document Layout Analysis** - Ancient documents feature non-standard layouts requiring specialized spatial reasoning. Quick check: Model identifies text blocks and their relationships regardless of document age or format.
- **Cultural Knowledge Integration** - Many documents contain implicit cultural references requiring external knowledge. Quick check: Model correctly interprets culturally-specific terminology and historical references.
- **Multi-modal Feature Fusion** - Combining visual document features with linguistic understanding is critical for reasoning tasks. Quick check: Model maintains consistent performance across OCR and reasoning tasks on same document.
- **Temporal Reasoning** - Documents span multiple dynasties requiring historical context understanding. Quick check: Model correctly identifies temporal references and their implications.
- **Layout-to-Content Mapping** - Translating spatial arrangements into semantic meaning is crucial for document understanding. Quick check: Model accurately extracts content from complex multi-column layouts.

## Architecture Onboarding
- **Component Map:** Input Document Images -> VLM Encoder -> OCR Module / Document Understanding Module / Knowledge Reasoning Module -> Output Results
- **Critical Path:** Document Image → Visual Feature Extraction → Text Recognition → Language Understanding → Reasoning → Final Answer
- **Design Tradeoffs:** Accuracy vs. inference speed, general-purpose vs. domain-specific models, single-pass vs. multi-step reasoning approaches
- **Failure Signatures:** OCR failures manifest as garbled text or missing characters; reasoning failures appear as culturally inappropriate interpretations; layout failures result in incorrect content extraction
- **First Experiment:** Evaluate basic OCR accuracy on simple single-column documents
- **Second Experiment:** Test document understanding on clearly structured documents with explicit content
- **Third Experiment:** Assess reasoning capabilities on documents requiring only one-step inference

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of their findings across broader document types and historical periods. The authors question whether current evaluation metrics adequately capture the nuanced understanding required for ancient documents, particularly for cultural knowledge that may be implicit rather than explicitly stated. They also raise concerns about the potential need for specialized fine-tuning approaches for different document categories.

## Limitations
- **Dataset Scope:** Limited to six document types from specific historical periods, potentially missing broader document diversity
- **Evaluation Consistency:** Different tasks use varying evaluation protocols, complicating cross-task performance comparisons
- **Cultural Bias:** Annotation process may reflect specific cultural knowledge assumptions that don't represent all historical contexts
- **Methodological Heterogeneity:** Inconsistent tokenization and normalization procedures across tasks affect fair model comparisons

## Confidence
- **Generalizability Claims:** Low confidence due to restricted dataset scope and methodological inconsistencies
- **Performance Comparisons:** Medium confidence given varying evaluation protocols across tasks
- **Reasoning Limitations:** High confidence that current VLMs struggle with complex cultural reasoning in ancient documents

## Next Checks
1. Conduct cross-validation using documents from historical periods not represented in the current dataset to test temporal generalization capabilities
2. Implement standardized evaluation protocols across all six tasks, including consistent tokenization and normalization procedures, to enable fair model comparisons
3. Perform ablation studies isolating OCR accuracy from reasoning performance to determine whether current models fail at text extraction or knowledge integration stages