---
ver: rpa2
title: Estimating Dimensionality of Neural Representations from Finite Samples
arxiv_id: '2509.26560'
source_url: https://arxiv.org/abs/2509.26560
tags:
- dimensionality
- both
- estimator
- local
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating the global dimensionality
  of neural representations from finite samples, which is challenging because existing
  methods are sensitive to the number of samples and noise. The authors propose a
  bias-corrected estimator of the participation ratio (PR), a widely used measure
  of dimensionality, by deriving unbiased estimators for the numerator and denominator
  of PR.
---

# Estimating Dimensionality of Neural Representations from Finite Samples

## Quick Facts
- arXiv ID: 2509.26560
- Source URL: https://arxiv.org/abs/2509.26560
- Reference count: 40
- Key outcome: The paper proposes a bias-corrected estimator for the participation ratio (PR) to accurately estimate global and local dimensionality of neural representations from finite samples, showing invariance to sample size and improved accuracy over existing methods.

## Executive Summary
This paper addresses the challenge of accurately estimating the dimensionality of neural representations from finite samples, which is critical for understanding neural coding efficiency and complexity. Existing methods like the participation ratio (PR) are sensitive to sample size and noise, leading to biased estimates. The authors derive unbiased estimators for the numerator and denominator of PR by summing over distinct indices only, then correct for the bias introduced by their ratio. They also extend their method to handle noise and measure local dimensionality, demonstrating superior performance on synthetic data, real neural recordings, and artificial neural networks.

## Method Summary
The method derives unbiased estimators for the participation ratio (PR) by computing expanded einsum expressions for the numerator (A) and denominator (B) that only sum over distinct indices (α≠β), correcting the bias from naive summation. For global dimensionality, the full bias-corrected estimator γ_both is computed using five expanded terms (t₁^both through t₅^both) involving non-matching indices, then A_both = t₁ - 2t₂ + t₅ and B_both = t₃ - 2t₄ + t₅. For noise handling, a two-trial approach estimates noise variance and applies a correction term. For local dimensionality, the method computes PR within local neighborhoods defined by Mahalanobis distance, sweeping across ball radii to observe convergence in the small-radius limit.

## Key Results
- The bias-corrected γ_both estimator recovers the true dimensionality (d=50) across varying sample sizes (P,Q ∈ [100,1000]) on synthetic linear data with Gaussian noise.
- On real neural datasets (Stringer mouse V1, Steinmetz mouse electrophysiology, HCP human fMRI), γ_both shows invariance to sample size while naive methods exhibit strong dependence.
- For local dimensionality estimation, the proposed method outperforms TwoNN especially in the presence of noise, though it requires manual inspection of radius sweep convergence.

## Why This Works (Mechanism)
The method works by correcting the systematic bias in dimensionality estimation that arises from finite sampling. In the naive approach, the participation ratio is computed by summing over all pairs of samples, which overcounts correlations when the same sample appears multiple times. By explicitly summing only over distinct indices (α≠β), the unbiased estimators eliminate this finite-sample bias. The ratio of these unbiased estimators still introduces a small residual bias due to the non-linearity of division, but this is significantly smaller than the original bias. For local dimensionality, the method leverages the intuition that dimensionality within a small neighborhood should approximate the local intrinsic dimension, and the Mahalanobis metric accounts for the covariance structure of the data.

## Foundational Learning
- **Participation Ratio (PR)**: A measure of dimensionality defined as (Σλᵢ)² / Σλᵢ², where λᵢ are eigenvalues of the covariance matrix. Why needed: PR is widely used in neuroscience to quantify neural representation dimensionality but suffers from finite-sample bias.
- **Unbiased Estimation**: Deriving estimators whose expected value equals the true population parameter. Why needed: Naive estimators of PR are biased downward when computed from finite samples, systematically underestimating dimensionality.
- **Einsum Expansion**: Expanding tensor contractions into explicit algebraic forms to enable unbiased summation over distinct indices. Why needed: Direct implementation of the unbiased estimators would be computationally infeasible (O(n⁴)), requiring algebraic simplification.
- **Mahalanobis Distance**: A distance metric that accounts for the covariance structure of the data. Why needed: For local dimensionality estimation, it ensures neighborhoods are defined in the intrinsic geometry of the representation space rather than Euclidean space.
- **Two-NN Method**: A popular local dimensionality estimation method based on the ratio of distances to the first and second nearest neighbors. Why needed: Serves as a baseline for comparison in local dimensionality estimation experiments.

## Architecture Onboarding
- **Component Map**: Synthetic data generator -> γ_both implementation -> Real neural data preprocessing -> Dimensionality estimation -> Comparison with baselines
- **Critical Path**: Generate synthetic data with known dimensionality → Implement bias-corrected PR estimator → Validate recovery of true dimensionality → Apply to real neural datasets → Compare with existing methods
- **Design Tradeoffs**: The full einsum implementation is mathematically exact but computationally expensive (O(n⁴) in naive form), requiring algebraic expansion for tractability. The local dimensionality method requires manual inspection of radius sweep convergence, trading automation for accuracy.
- **Failure Signatures**: Implementation hangs for P,Q > 100 (indicates missing algebraic expansion); γ_both underestimates for tiny samples even on synthetic data (indicates residual ratio bias); single-trial noise correction yields biased estimates (indicates need for two trials).
- **First Experiments**:
  1. Implement γ_both using expanded formulas from Section A.3 and verify it recovers d=50 on synthetic linear data across sample sizes.
  2. Apply γ_both to Stringer mouse V1 data subsampled at different sizes to confirm invariance.
  3. Compare γ_both against naive PR and TwoNN on synthetic data with varying noise levels.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the bias-correction technique be extended to more general matrix-based entropy measures, such as the logarithm of the dimensionality or Renyi entropy? The current derivation relies on the specific ratio definition of PR, and it's unclear if unbiased estimators translate to logarithmic or non-linear functional forms.
- **Open Question 2**: Is there a principled heuristic to select the local ball radius r for local dimensionality estimation without manual inspection? The current workflow requires visual inspection of convergence, which becomes a bottleneck for large datasets or automated pipelines.
- **Open Question 3**: Can the small residual bias introduced by the ratio of unbiased estimators be eliminated through higher-order corrections? The paper notes this residual bias is proportional to the covariance and variance of the estimators, and it's unclear if higher-order Taylor expansion terms could correct it.

## Limitations
- The method assumes a linear generative model with i.i.d. Gaussian noise, which may not hold in all neural datasets, potentially limiting bias-correction effectiveness for non-linear or heteroscedastic noise structures.
- The computational complexity of the full einsum implementation may limit applicability to very large-scale datasets without approximation schemes.
- The local dimensionality method requires manual inspection of radius sweep convergence, trading automation for accuracy and creating a bottleneck for large-scale applications.

## Confidence
- **High confidence**: The bias-correction for participation ratio (PR) under the linear Gaussian model is mathematically rigorous and validated on synthetic data.
- **Medium confidence**: Performance on real neural datasets is strong, but the exact preprocessing pipeline is not fully specified, making exact replication uncertain.
- **Medium confidence**: Local dimensionality estimates show promise but depend on hyperparameter choices that are not fully specified.

## Next Checks
1. **Synthetic stress test**: Generate data with varying levels of non-linear mixing and heteroscedastic noise to test robustness beyond the assumed generative model.
2. **Hyperparameter sensitivity**: Systematically vary the radius for local dimensionality estimation and k for Mahalanobis metric to determine optimal settings for different data regimes.
3. **Computational scaling**: Profile runtime and memory usage for P,Q > 1000 to identify bottlenecks and test the feasibility of the full einsum implementation on large datasets.