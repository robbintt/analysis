---
ver: rpa2
title: Local Intrinsic Dimension of Representations Predicts Alignment and Generalization
  in AI Models and Human Brain
arxiv_id: '2601.22722'
source_url: https://arxiv.org/abs/2601.22722
tags:
- alignment
- across
- generalization
- convnext
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the convergence of neural representations
  between artificial vision models and the human brain. The authors demonstrate that
  models with stronger generalization performance exhibit higher alignment with both
  other models and human neural activity, particularly in visual cortex.
---

# Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain

## Quick Facts
- **arXiv ID**: 2601.22722
- **Source URL**: https://arxiv.org/abs/2601.22722
- **Reference count**: 30
- **Primary result**: Models with lower local intrinsic dimension show stronger alignment with both other models and human brain activity, as well as better generalization performance

## Executive Summary
This paper investigates the convergence of neural representations between artificial vision models and the human brain, demonstrating that models with stronger generalization performance exhibit higher alignment with both other models and human neural activity. The authors identify local intrinsic dimension (LID) as a unifying geometric property that predicts both cross-system alignment and generalization performance. Models with lower LID show stronger alignment with both other models and brain regions, particularly visual cortex. The relationship is most pronounced at local scales rather than global dimensions. The authors further show that increasing model capacity and training data systematically reduces local intrinsic dimension, providing a geometric explanation for the benefits of scaling. These findings establish local intrinsic dimension as a unifying descriptor of representational convergence across artificial and biological systems.

## Method Summary
The study analyzes 51 ConvNeXt models of varying scales and training datasets, extracting embeddings from their penultimate layers for images from the Natural Scenes Dataset (NSD). Embeddings are projected to 300 principal components via PCA. AI-Brain alignment is quantified by fitting ridge regression models to predict voxel-wise fMRI responses, with performance measured via R². AI-AI alignment is computed similarly by predicting one model's embeddings from another's. Local intrinsic dimension is estimated using the maximum likelihood estimator (Levina & Bickel, 2004) with K-nearest neighbors, computed at multiple neighborhood sizes (K=50 for AI-AI alignment and generalization, K=1000 for AI-Brain alignment). Generalization is measured via ImageNet-1K accuracy. The analysis correlates LID values with alignment scores and performance metrics across models.

## Key Results
- Lower local intrinsic dimension consistently predicts stronger model-model alignment and model-brain alignment
- Local intrinsic dimension is negatively correlated with ImageNet-1K accuracy (better generalization)
- The relationship between LID and alignment is strongest at local scales (small K values)
- Increasing model scale and training data systematically reduces local intrinsic dimension
- Alignment is strongly shaped by architectural family, with cross-architecture alignment being reduced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local intrinsic dimension (LID) acts as a unified geometric predictor for generalization and cross-system alignment.
- **Mechanism:** High-performing models converge onto regions of the loss landscape characterized by "flat minima," which correspond to locally compressed representations (lower LID). This geometric compression forces models to discard noise and retain only robust, task-relevant features—structures statistically similar to those used by the human brain.
- **Core assumption:** The "Platonic Representation Hypothesis" holds, implying optimal solutions for natural tasks share similar low-dimensional geometric structure regardless of substrate.
- **Evidence anchors:** Lower LID is consistently associated with stronger alignment and better generalization. Flat minima are characterized by many distinct parameter configurations giving rise to mutually alignable representations. Token embeddings often share similar relative orientations and local structures.

### Mechanism 2
- **Claim:** Scaling model capacity and data systematically reduces LID, providing a geometric explanation for scaling laws.
- **Mechanism:** Larger models use their excess capacity to "flatten" the local geometry of the embedding space, collapsing representations of similar concepts into lower-dimensional sub-manifolds rather than memorizing high-dimensional noise.
- **Core assumption:** The benefit of scale is primarily representational (geometric) rather than purely functional (memorization capacity).
- **Evidence anchors:** Increasing model scale or dataset size promotes locally lower-dimensional representations, providing a geometric account for enhanced neural alignment and generalization. Explicit negative correlation between training data amount/parameters and local intrinsic dimension. Similar dynamics observed in LLMs where fine-tuning affects behavior via geometric properties.

### Mechanism 3
- **Claim:** Predictive alignment is a property of local neighborhoods, not global structure.
- **Mechanism:** Global dimensionality measures average structure across diverse concepts, washing out the signal. Local dimensionality (small k in nearest-neighbor estimation) isolates the geometry of individual concepts/categories where alignment with brain regions occurs.
- **Core assumption:** The brain and high-performing AI models process information by localizing distinct concepts in compact, low-dimensional zones.
- **Evidence anchors:** Alignment- and performance-relevant structure is more closely associated with local geometry, with correlations weakening as neighborhood size increases. Predictive power of dimensionality arises specifically from local embedding structure. Intrinsic dimension varies locally by concept/complexity, reinforcing that global measures miss specific local dynamics.

## Foundational Learning

- **Concept: Intrinsic Dimensionality vs. Ambient Dimensionality**
  - **Why needed here:** You must distinguish between the size of the vector (e.g., 1024 floats) and the actual "degrees of freedom" or manifold size the data occupies. The paper claims the *intrinsic* size (often much smaller) is what matters.
  - **Quick check question:** If a 1000-dimensional embedding lies perfectly on a flat 2D plane, what is its intrinsic dimension? (Answer: 2).

- **Concept: Maximum Likelihood Estimation (MLE) for Dimension**
  - **Why needed here:** The paper uses a specific algorithm (Levina & Bickel, 2004) to estimate dimension. Understanding that this relies on neighbor distances ($T_j$) is crucial for debugging the scale parameter $k$.
  - **Quick check question:** In the equation $\hat{m}_K(z)$, does a larger neighborhood size $K$ typically result in a higher or lower estimated dimension in this paper's findings? (Answer: Higher/Less predictive, as it moves from local to global).

- **Concept: Representational Alignment via Linear Regression**
  - **Why needed here:** The paper quantifies "alignment" by seeing how well a linear map (Ridge Regression) can translate Model A's embeddings to Model B's (or fMRI data).
  - **Quick check question:** Why is $R^2$ (coefficient of determination) used instead of simple accuracy to measure alignment between a model and brain activity?

## Architecture Onboarding

- **Component map:** Images from NSD -> Vision models (ConvNeXt, ResNet, ViT) -> Feature Extractors -> Local Intrinsic Dimension Estimator (MLE) -> Ridge Regression models -> fMRI voxel responses

- **Critical path:**
  1. Embedding Extraction: Pass images through models to get vector outputs
  2. PCA Projection: Reduce vectors to top 300 components (critical normalization step)
  3. LID Calculation: Compute local intrinsic dimension using MLE estimator on embeddings
  4. Alignment Scoring: Train Ridge Regression to predict fMRI responses from embeddings; record R²

- **Design tradeoffs:**
  - Scale (K) Selection: Small K (e.g., 50-100) captures local geometry (high signal) but is sensitive to noise/outliers. Large K is robust but loses the specific "local" predictive power.
  - Estimator Choice: MLE (Levina & Bickel) is standard but assumes data density doesn't vary too wildly. Alternatives like MOM or MADA exist but show consistent trends.
  - Reference Model: For AI-AI alignment, the paper uses the highest-performing model as a reference. Choosing a poor reference model might obscure convergence trends.

- **Failure signatures:**
  - High LID / High Performance: If you observe a model with high LID that still performs well, check the K value; it might be estimated at a global scale where the correlation breaks down.
  - Low Alignment despite Low LID: Check the architectural family. Alignment is "strongly shaped by architectural family" (e.g., ConvNeXt vs. ResNet). Low LID is necessary but not always sufficient for cross-architecture alignment.
  - Negative Correlations: If LID correlates *positively* with performance, verify the dimensionality estimator implementation; this contradicts the core finding.

- **First 3 experiments:**
  1. Scale Sensitivity Check: Calculate LID for a single model (e.g., ConvNeXt-Tiny) across a range of K (10 to 1000). Plot LID vs. K to visualize the decay curve and identify the "local" plateau.
  2. Cross-Model Correlation: Extract embeddings for 5 diverse models. Calculate LID (fixed low K) and ImageNet Top-1 accuracy. Verify the negative correlation (lower LID → higher accuracy).
  3. Regress one layer: Pick one intermediate layer of a model (not just final logits). Compute LID and try to predict the alignment score for a specific brain region (e.g., EBA) to see if the relationship holds in hidden states.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the relationship between local intrinsic dimension, generalization, and alignment hold in non-visual modalities such as language or audition?
- **Basis in paper:** The authors explicitly state in the Limitations section that extending the framework to other modalities is necessary to assess the generality of representational convergence.
- **Why unresolved:** The current study relies exclusively on vision models and the Natural Scenes Dataset (fMRI responses to visual stimuli).
- **What evidence would resolve it:** Replicating the correlation analysis between local intrinsic dimension and neural alignment using Large Language Models (LLMs) paired with neuroimaging data from language processing tasks.

### Open Question 2
- **Question:** What are the theoretical mechanisms that causally link low local intrinsic dimensionality to improved generalization and brain alignment?
- **Basis in paper:** The authors note that while the predictive power of local intrinsic dimensionality is strong, "the theoretical mechanisms underlying its relevance remain unclear."
- **Why unresolved:** The paper establishes robust correlations and offers a "flat minima" interpretation, but does not derive a formal theoretical proof or causal model.
- **What evidence would resolve it:** A formal theoretical framework deriving local dimensionality as a necessary condition for generalization, or interventional experiments where dimensionality is constrained during training to observe causal effects on alignment.

### Open Question 3
- **Question:** Can explicit regularization of local intrinsic dimension during training induce higher model-brain alignment without increasing model scale?
- **Basis in paper:** The paper demonstrates that scaling reduces dimensionality and improves alignment, but it does not test if dimensionality reduction alone is sufficient to cause alignment independent of scale.
- **Why unresolved:** It is currently ambiguous whether low dimensionality is a causal driver of alignment or merely a correlated byproduct of training on larger datasets with larger models.
- **What evidence would resolve it:** Training small-scale models with a loss term specifically penalizing high local intrinsic dimension to determine if this manipulation artificially increases alignment with neural data.

### Open Question 4
- **Question:** What geometric properties, distinct from intrinsic dimensionality, explain the "reduced cross-architecture alignment" observed between different model families?
- **Basis in paper:** Section 4.4 notes that while dimensionality predicts alignment, "average intrinsic dimensionality does not differ systematically across architectures," implying other geometric or structural factors create barriers to alignment between families (e.g., ResNet vs. ViT).
- **Why unresolved:** The paper identifies the gap in cross-architecture alignment but relies on a single geometric descriptor (dimensionality) which fails to explain it.
- **What evidence would resolve it:** Multi-scale geometric analysis (e.g., curvature, topological data analysis) identifying structural invariants that differ across architectures but converge within high-performing models of the same family.

## Limitations

- The study relies on linear alignment (ridge regression) which may miss nonlinear representational correspondences between AI models and brain activity
- Findings are limited to vision models and may not generalize to other domains like language or audition
- Local intrinsic dimension estimates depend heavily on the choice of neighborhood size K, though the paper demonstrates robustness across different values

## Confidence

- **High confidence**: The relationship between lower local intrinsic dimension and better generalization performance across multiple vision models
- **Medium confidence**: The relationship between local intrinsic dimension and AI-brain alignment, particularly given the complexity of neural data
- **Medium confidence**: The mechanistic explanation linking flat minima to representational alignment

## Next Checks

1. Test the LID-alignment-generalization relationship across non-vision domains (language, reinforcement learning) to assess domain generality
2. Validate findings using alternative alignment metrics beyond linear regression (e.g., centered kernel alignment, nonlinear manifold alignment)
3. Perform ablation studies varying the neighborhood size K systematically to identify the precise scale at which LID becomes most predictive of alignment