---
ver: rpa2
title: Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation
arxiv_id: '2511.21989'
source_url: https://arxiv.org/abs/2511.21989
tags:
- user
- item
- cold-start
- users
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the cold-start item recommendation problem,
  where new items lack interaction data, by leveraging Large Language Models (LLMs)
  as data augmenters. The authors propose an RL-based framework that trains a policy
  model to select optimal users for data augmentation, rather than relying on random
  selection or partial user histories.
---

# Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation

## Quick Facts
- arXiv ID: 2511.21989
- Source URL: https://arxiv.org/abs/2511.21989
- Reference count: 40
- Primary result: RL-based user selection improves cold-start item recall by up to 21.56% over random and 9.96% over feature-based selection.

## Executive Summary
This paper addresses the cold-start item recommendation problem by leveraging Large Language Models (LLMs) to generate synthetic user interactions for new items. The key innovation is an RL-based framework that trains a policy model to select optimal users for data augmentation, rather than relying on random selection or partial user histories. The policy model uses user features like popularity, rating variance, and category diversity to identify users whose interactions, when augmented with LLM-generated data, most improve cold-start item performance. Experiments on Amazon Beauty and Sports datasets demonstrate substantial gains in cold-start item recall.

## Method Summary
The method trains a policy model to select users whose interaction histories, when used to prompt LLMs for pairwise cold-start item preferences, generate optimal augmented training data. User features (MP, AP, RV, CKLD, CSD, EE, V, AR) are computed from historical item metadata and scaled. A linear policy outputs selection probabilities; selected users trigger LLM-based pairwise preference generation. The resulting augmented data trains a two-tower model with In-Batch Softmax + LogQ and auxiliary BPR loss. Cold-start recall@K provides the reward for REINFORCE policy updates with EMA baseline. The policy is trained over 10-15 iterations, each with M=20-24 two-tower model seeds.

## Key Results
- Up to 21.56% improvement in cold-start item recall over random user selection
- 9.96% improvement over feature-based selection methods
- Feature importance varies by domain: Beauty benefits from MP, AP, CSD, AR, V; Sports benefits from RV, CKLD, V, AP, EE
- Full user histories yield more faithful LLM-generated preferences than partial histories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing the LLM with complete user histories yields more faithful preference predictions than partial histories.
- Mechanism: Prior approaches prompt the LLM using only pre-interaction items (or only top-rated items), which omits signal. By sampling users rather than interactions and including the full history, the LLM can better emulate the user when judging pairwise cold-start item preferences.
- Core assumption: Full-history prompts map to more accurate preference simulation; the LLM generalizes reasonably from past behavior to unseen cold-start items.
- Evidence anchors:
  - [abstract] "they use partial user histories, which does not allow the LLM to fully emulate the user"
  - [section 4.1] "we prompt the LLM with the user's entire training history... This enables the LLM to generate interactions in a manner that is more faithful to the user's persona."
  - [corpus] Related work (ItemRAG) incorporates retrieval-augmented generation for LLM recommendation but does not specifically address the partial-vs-full history ablation studied here.
- Break condition: If user histories are extremely long or highly noisy, truncation or denoising may be required; full-history benefit may diminish for users with sparse or highly inconsistent behavior.

### Mechanism 2
- Claim: A learned selection policy (contextual bandit via policy gradient) selects users whose augmentation yields higher cold-start recall than random or heuristic selection.
- Mechanism: The policy takes user features and outputs selection probability via a linear layer and sigmoid. Selected users trigger LLM-based pairwise preference generation; the resulting augmented data trains a two-tower model. Cold-start recall on held-out items provides the reward; REINFORCE updates increase selection probability for high-reward users.
- Core assumption: Cold-start recall improvement from augmentation generalizes across policy iterations and is not dominated by variance in LLM outputs or recommender training.
- Evidence anchors:
  - [abstract] "Experiments on Amazon Beauty and Sports datasets show substantial gains in cold-start item recall—up to 21.56% improvement over random selection and 9.96% over feature-based selection"
  - [section 4.2] "we iterate through the users... in descending order of their output logits... we select each user with probability π_θ(a=1|f_u)"
  - [corpus] Next-User Retrieval (neighbor paper) uses generative next-user modeling for cold-start but does not use RL-based user selection for augmentation.
- Break condition: If the reward signal (cold-start recall) is too noisy or the policy overfits to a small user subset, learned selection may converge to suboptimal policies; regularization or critic baselines may help.

### Mechanism 3
- Claim: Handcrafted behavioral features—such as popularity (MP/AP), rating variance (RV), and category diversity (CKLD/EE)—carry signal about which users' LLM-generated preferences improve cold-start performance when used for augmentation.
- Mechanism: Features are computed from historical item metadata. The policy receives min-max scaled features; initialization and gradient updates learn a weighted combination that favors users whose augmentation yields higher cold recall.
- Core assumption: The selected features capture stable user traits relevant to cold-start item preference prediction; feature effectiveness may vary by domain.
- Evidence anchors:
  - [section 5.3] "For Amazon Beauty, these features are MP, AP, CSD, AR, and V. The top five Amazon Sports features are RV, CKLD, V, AP, and EE."
  - [table 1] Defines MP, AP, RV, CKLD, EE, and other features with clear behavioral interpretations.
  - [corpus] Corpus papers address cold-start but do not analyze the same feature set for user selection in LLM-based augmentation.
- Break condition: If domains differ significantly (e.g., cross-domain), the feature set may need reselection; features derived from sparse histories may be unreliable.

## Foundational Learning

- Concept: Policy Gradient / REINFORCE
  - Why needed here: The method frames user selection as a contextual bandit and uses REINFORCE to increase selection probability for users yielding high cold-start recall rewards.
  - Quick check question: Can you explain why a baseline (e.g., EMA of past rewards) is subtracted from the raw return to reduce variance in the gradient estimate?

- Concept: Cold-Start Item Problem in Collaborative Filtering
  - Why needed here: New items lack interaction data, so ID embeddings cannot be trained effectively; data augmentation aims to inject synthetic collaborative signal.
  - Quick check question: Why do random hash buckets or content features help cold-start items in standard CF, and how does augmentation differ?

- Concept: Two-Tower Retrieval with BPR Auxiliary Loss
  - Why needed here: The backbone recommender is a two-tower model; augmentation data is incorporated via a Bayesian Personalized Ranking (BPR) loss to pull positive cold-start items closer to user embeddings.
  - Quick check question: How does the BPR loss for augmentation differ from the main In-Batch Softmax loss, and why keep them separate?

## Architecture Onboarding

- Component map:
  - User features (MP, AP, RV, CKLD, CSD, EE, V, AR) -> Policy model (Linear + Sigmoid) -> User selection probability -> LLM inference module -> Two-tower recommender (User tower, Item tower) -> Cold-start recall@K -> Policy update (REINFORCE)

- Critical path:
  1. Initialize policy and user baselines (non-augmented and feature-averaged cold recall).
  2. For each policy iteration:
     - Score users with policy; iterate in descending logit order; select users stochastically.
     - For each selected user, prompt LLM for pairwise cold-start preference.
     - Train two-tower model on original + augmented data.
     - Compute cold recall@K; subtract per-user baseline to get reward.
     - Update policy parameters via policy gradient.

- Design tradeoffs:
  - Linear vs MLP policy: Linear is simpler and faster; MLP can capture non-linear feature interactions but may overfit.
  - Full-history vs truncated prompts: Full history improves fidelity but increases token count and cost.
  - Proxy rewards (fine-tuning or early-stopped two-tower) reduce training time but may introduce bias.

- Failure signatures:
  - Policy collapses to selecting a small, repetitive user subset -> check reward variance, temperature, and exploration.
  - Augmentation improves cold recall but severely degrades warm recall -> balance BPR weight, reduce augmentation ratio.
  - Cold recall improvements not statistically significant -> increase number of two-tower seeds, reduce reward noise.

- First 3 experiments:
  1. Ablate prompt design: compare LLM-as-user (full history) vs interaction-history prompting on a small user subset; measure cold recall@10/50.
  2. Feature importance sweep: select top-k users per individual feature; identify top-5 features per dataset as policy inputs.
  3. Policy vs baselines: train policy with top-5 features; compare cold recall@50 against random selection and best-feature selection; report mean ± SE over 20 seeds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be effectively extended to cross-domain recommendation scenarios where user histories and cold-start items belong to different semantic categories?
- Basis in paper: [explicit] The conclusion states, "Our evaluation only considered single-domain datasets. We suggest that future works run experiments on cross-domain datasets."
- Why unresolved: The current method assumes a shared embedding space for user history and target items; cross-domain scenarios require the policy to leverage transferable user features to bridge distinct semantic gaps.
- What evidence would resolve it: Experiments on standard cross-domain datasets (e.g., Amazon "Clothing" to "Home") demonstrating that the learned policy generalizes or adapts to selecting users whose preferences transfer across domains.

### Open Question 2
- Question: Do advanced RL algorithms like Actor-Critic or PPO offer improved training stability and convergence over the REINFORCE method used in this study?
- Basis in paper: [explicit] The authors note, "Another future work could entail Actor-Critic or PPO policy implementations to improve stabilization."
- Why unresolved: The current implementation relies on a basic policy gradient approach (REINFORCE) with a moving average baseline, which is susceptible to high variance gradients.
- What evidence would resolve it: A comparative analysis of training variance and cold-start recall performance between the current REINFORCE setup and alternative actor-critic implementations.

### Open Question 3
- Question: Can a universal set of user features be identified that consistently predicts augmentation success, or are optimal features strictly domain-dependent?
- Basis in paper: [inferred] Results in Section 5.3 show conflicting optimal features: "Median Popularity" worked best for Beauty, whereas "Category Diversity" and "Rating Variance" were superior for Sports.
- Why unresolved: The paper observes these differences but does not determine if they stem from specific data distributions or fundamental differences in how users interact with different product types.
- What evidence would resolve it: A meta-analysis across additional datasets to correlate dataset characteristics (e.g., density, item type) with the efficacy of specific user features.

## Limitations
- Single-domain evaluation: Results are limited to Amazon Beauty and Sports datasets; cross-domain performance is unknown.
- Feature dependence: Optimal features vary by domain, suggesting limited generalizability without feature reselection.
- REINFORCE limitations: Basic policy gradient with EMA baseline may suffer from high variance and suboptimal convergence.

## Confidence
- **High** for cold-start recall improvements (21.56% over random, 9.96% over feature-based)
- **Medium** for feature generalizability across domains (only Beauty and Sports tested)
- **Medium** for policy learning stability (REINFORCE with noisy rewards)

## Next Checks
1. **Ablation of prompt design**: Compare LLM-as-user (full history) prompting against interaction-history prompting on a held-out user subset to isolate the effect of full-history inclusion on cold recall@K.

2. **Cross-domain feature transferability**: Apply the identified top-5 features (MP, AP, RV, CKLD, etc.) to a different cold-start recommendation dataset (e.g., MovieLens or Yelp) and evaluate whether the policy selection still yields significant cold-start recall improvements.

3. **Policy robustness under reward noise**: Introduce controlled variance in the cold recall reward (e.g., by varying the number of two-tower seeds or LLM inference runs) and measure the stability of the learned policy across multiple training runs.