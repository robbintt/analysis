---
ver: rpa2
title: 'DAP: A Discrete-token Autoregressive Planner for Autonomous Driving'
arxiv_id: '2511.13306'
source_url: https://arxiv.org/abs/2511.13306
tags:
- trajectory
- driving
- tokens
- planning
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAP, a discrete-token autoregressive planner
  for autonomous driving that jointly forecasts future BEV semantics and ego trajectories.
  The method uses a decoder-only Transformer with sparse MoE layers to predict discrete
  BEV and trajectory tokens in an interleaved fashion, providing dense spatio-temporal
  supervision.
---

# DAP: A Discrete-token Autoregressive Planner for Autonomous Driving

## Quick Facts
- arXiv ID: 2511.13306
- Source URL: https://arxiv.org/abs/2511.13306
- Authors: Bowen Ye; Bin Zhang; Hang Zhao
- Reference count: 40
- Key outcome: DAP achieves 1.202m ADE, 91.68% OLS on NuPlan Val4k and 87.2 PDMS on NavSim with 160M parameters

## Executive Summary
DAP introduces a discrete-token autoregressive planner that jointly forecasts future BEV semantics and ego trajectories. The method uses a decoder-only Transformer with sparse MoE layers to predict discrete BEV and trajectory tokens in an interleaved fashion, providing dense spatio-temporal supervision. SAC-BC fine-tuning enhances the coupling between predicted environment states and generated ego trajectories while preserving stability. Despite a compact 160M parameter budget, DAP achieves state-of-the-art open-loop performance on NuPlan and competitive closed-loop results on NavSim.

## Method Summary
DAP is a two-stage training pipeline for autonomous driving planning. First, a decoder-only Transformer learns to predict future BEV semantic tokens and trajectory tokens autoregressively from past observations and high-level commands. The model uses fixed-bin discretization in curvature-acceleration (κ-a) space for trajectories and VQ-VAE for BEV tokenization. Second, SAC-BC fine-tuning injects reward-guided improvements while preserving behavior cloning priors. The architecture is compact at ~160M parameters and achieves dense spatio-temporal supervision through joint BEV-trajectory forecasting.

## Key Results
- Open-loop NuPlan Val4k: 1.202m ADE, 91.68% OLS (state-of-the-art)
- Closed-loop NavSim: 87.2 PDMS with 97.4 NC, 97.4 DAC, 87.8 EP
- κ-a discretization achieves 0.40/0.68/0.55/1.02/0.33/0.68 ADE/FDE across datasets with 10x smaller codebook than xy-space
- SAC-BC improves safety metrics (TTC: 94.6→97.4) while maintaining planning quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint BEV-trajectory forecasting provides dense spatio-temporal supervision that strengthens coupling between scene understanding and motion generation.
- Mechanism: By predicting BEV semantic tokens at each time step alongside trajectory tokens, the model receives per-cell supervision at every step rather than sparse waypoint labels only. This constrains how scene evolution should shape ego motion through shared latent representations.
- Core assumption: BEV semantics provide sufficient signal to capture scene dynamics that matter for planning; the interleaved generation order (BEV first, then trajectory) allows motion prediction to condition on the newly decoded scene representation.
- Evidence anchors:
  - [abstract] "jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion"
  - [section 3.1] "jointly predicts future semantic BEV states tokens that summarize near-future scene evolution and κ–a trajectory tokens that govern ego motion, thereby tightly coupling scene understanding with motion generation under dense, spatio-temporally aligned supervision"
  - [Table 5] Trajectory-only baseline achieves PDMS=82.7; adding joint BEV-trajectory supervision under pure BC improves to PDMS=84.6 (EP=90.2 but safety suboptimal)
  - [corpus] Limited direct comparison; Int2Planner mentions integrated prediction-planning benefits but doesn't isolate the dense supervision mechanism
- Break condition: If BEV prediction quality degrades significantly (e.g., small obstacles/distant agents poorly reconstructed as shown in Appendix A.2), the supervision signal may misguide trajectory generation; if κ-a discretization loses critical displacement information, the coupling may not transfer to continuous control.

### Mechanism 2
- Claim: Curvature-acceleration (κ-a) discretization with fixed-bin quantization provides a compact, behavior-aligned action space that improves sampling efficiency compared to xy-position or learned codebooks.
- Mechanism: Transforming (x, y, yaw) trajectories into (κ, a) space operates in a lower-dimensional, kinematically meaningful manifold. Uniform grid discretization avoids codebook collapse issues observed with learned quantizers like IBQ on sparse trajectory data, while maintaining vocabulary size ~10x smaller than xy discretization for comparable accuracy.
- Core assumption: The trajectory manifold is intrinsically low-dimensional and information-sparse; behavior-aligned spaces (steering curvature, longitudinal acceleration) are sufficient for planning.
- Evidence anchors:
  - [section 3.1] Eqs. 2a-2d define the transformation from (x, y, yaw) to (κ, a); Eq. 3 defines trajectory token packing
  - [Table 4] κ-a FB achieves 0.40/0.68/0.55/1.02/0.33/0.68 ADE/FDE across datasets; xy-IBQ shows 1.18/3.90 on Navsim (worse); ka-IBQ shows 2.84/6.38 on Nuscenes (worse)
  - [Appendix B] "IBQ performs worse here because the trajectory space is intrinsically low-dimensional and information-sparse, so the learned codebook tends to collapse toward frequent patterns and fails to faithfully represent rare, sharp maneuvers"
  - [Table 7] FB-ka achieves ADE=0.27-0.43 with codebook sizes 1146-14592; FB-xy requires 45056-2230272 codebook size for comparable performance
  - [corpus] No direct comparison found; related work doesn't systematically compare discretization schemes
- Break condition: If sharp/rare maneuvers exceed the fixed-bin range (e.g., κ > 0.48 rad/m in config D), reconstruction bias may cause systematic planning errors; Table 6 shows 99% CI for κ is [-0.48, 0.47], suggesting edge cases may exceed range.

### Mechanism 3
- Claim: SAC-BC fine-tuning breaks loss symmetry in imitation learning by using explicit reward signals to distinguish between trajectory modes with similar surrogate loss but different risk profiles.
- Mechanism: Pure IL treats multiple feasible continuations as loss-equivalent (e.g., "drift left" vs "drift right" both match expert labels). SAC-BC adds reward terms for safety (distance to obstacles), progress (distance to centerline), and comfort (acceleration smoothness), enabling the policy to prefer lower-risk modes while regularizing toward BC prior via value-aware cloning.
- Core assumption: The reward formulation captures critical safety/comfort factors; the BC regularization prevents deviation into OOD regions where rewards may be unreliable.
- Evidence anchors:
  - [abstract] "SAC-BC fine-tuning... preserves supervised behavior cloning priors while injecting reward-guided improvements"
  - [Figure 3] Visualization showing trajectories 2 and 3 have nearly identical BC loss but trajectory 3 leads to collision
  - [section 3.2] Eqs. 7-9 define reward components; Eqs. 10-15 define SAC-BC objectives with CQL conservative regularizer and AWAC-style value-aware BC
  - [Table 5] SAC-BC improves NC from 94.6→97.4, TTC from 94.6→97.4, PDMS from 84.6→87.0 (with EP drop from 90.2→87.8)
  - [corpus] Related work (ReCogDrive, AutoVLA) uses IL+RL combinations but doesn't isolate the symmetry-breaking mechanism
- Break condition: If reward signals are noisy or mis-specified (e.g., d_ctr or d_clr computed inaccurately from predicted BEV), SAC may optimize wrong objectives; if λ_awac temperature is poorly tuned, value-aware BC may over/under-weight high-advantage expert actions.

## Foundational Learning

- Concept: **VQ-VAE (Vector Quantized Variational Autoencoder)**
  - Why needed here: Core component for discretizing BEV features; understanding encoder-decoder structure, codebook learning, and straight-through estimator is essential for debugging BEV token prediction quality.
  - Quick check question: Given a BEV feature map F∈R^{H×W×D}, can you trace how it becomes discrete tokens and back? What happens if a latent vector z_i,j is equidistant to multiple codebook entries?

- Concept: **Decoder-only Transformer with Causal Masking**
  - Why needed here: The autoregressive backbone requires understanding causal self-attention, next-token prediction, and how scheduled sampling bridges train-inference distribution gap.
  - Quick check question: In Eq. 5a-5b, what does p=0 vs p=1 mean for teacher forcing vs autoregressive rollout? Why does the paper use scheduled sampling (gradually increasing p)?

- Concept: **Soft Actor-Critic with Conservative Q-Learning (CQL)**
  - Why needed here: SAC-BC fine-tuning stage requires understanding entropy-regularized RL, twin critics, and CQL's conservative penalty for OOD actions.
  - Quick check question: In Eq. 11, what does the CQL term α_cql * [log Σ_a exp(Q) - Q(ctx, A_t)] accomplish? Why is this important for offline RL?

## Architecture Onboarding

- Component map:
  Camera/LiDAR → TransFuser Encoder → BEV Features → VQ-VAE Tokenizer → Token Sequence
  High-level Command → Tokenizer → Past Trajectory → κ-a Discretizer → Token Sequence
  → Decoder-only Transformer + MoE → Future BEV Tokens + Trajectory Tokens → SAC-BC Fine-tuning → Trajectory Post-tuning

- Critical path:
  1. BEV token quality (VQ-VAE reconstruction) → scene representation fidelity
  2. κ-a discretization range/resolution → action space coverage
  3. SAC reward formulation → safety-comfort-progress tradeoff
  4. Scheduled sampling ratio p → train-inference distribution alignment

- Design tradeoffs:
  - **BEV vs trajectory token loss weighting (λ_bev vs λ_traj)**: Paper uses larger λ_traj to prioritize motion accuracy; smaller λ_bev stabilizes scene representation without dominating. Ablation not shown for different ratios.
  - **Codebook size vs reconstruction error**: Table 7 shows FB-ka configs A/B/C/D with codebooks 1146→14592 achieve ADE 0.43→0.27; but Table 8 shows open-loop performance plateaus (FB-ka-B/C best at 0.42/0.63 ADE/FDE; FB-ka-D degrades to 0.61/0.97).
  - **SAC-BC λ weighting**: Controls BC regularization strength; too low → OOD drift, too high → insufficient reward-driven improvement.
  - **Post-tuning smoothing weights (w_ℓ,1, w_ℓ,2, w_s,1, w_s,2)**: Trade comfort/compliance (DAC=97.4, C=1.00) vs progress (EP drops 87.8→82.2 per Table 5).

- Failure signatures:
  - **Lateral zigzag/jitter**: Discrete κ-a token switches causing abrupt steering changes; mitigated by post-tuning (Eq. 16-17).
  - **Collision in dense traffic**: BEV prediction missing small obstacles/distant pedestrians (Appendix A.2); SAC reward may not penalize sufficiently.
  - **Mode collapse in trajectory prediction**: IBQ-style learned codebook collapses to frequent patterns (Table 4); use fixed-bin κ-a discretization instead.
  - **Excessive progress drop**: Over-aggressive post-tuning (high w_ℓ,2, w_s,2) or conservative SAC rewards (high w_clr penalty near obstacles).
  - **OOD action explosion**: Insufficient CQL penalty (α_cql too low) causing Q-overestimation on unseen actions during SAC-BC.

- First 3 experiments:
  1. **BEV tokenization quality baseline**: Freeze TransFuser encoder, train VQ-VAE on BEV features, measure reconstruction error (pixel-wise accuracy, IoU for semantic classes). Target: >80% reconstruction fidelity before proceeding to joint training.
  2. **Trajectory discretization ablation**: Train with κ-a-FB, xy-FB, and xy-DCT on NuPlan-mini (BC only, 4 epochs). Compare 4s ADE/FDE and codebook size. Expected: κ-a-FB achieves <0.5m ADE with <15k codebook; xy-DCT may achieve lower error but poor autoregressive rollout alignment.
  3. **SAC-BC reward sensitivity**: Starting from BC checkpoint, fine-tune with SAC-BC varying w_ctr, w_clr, w_comf (±50%). Measure PDMS decomposition (NC, DAC, TTC, C, EP). Expected: Higher w_clr improves TTC but may reduce EP; find balance near reported weights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reconstruction bias between $\kappa$-$\alpha$ (curvature-acceleration) and $xy$ trajectory representations be minimized to improve planning precision?
- Basis: [explicit] The authors state in the ablation study: "A remaining caveat is the reconstruction bias between ka and xy representations, which is small in practice but still a target for future refinement."
- Why unresolved: The chosen fixed-bin discretization operates in $\kappa$-$\alpha$ space for compactness, but this introduces quantization errors when converting back to $xy$ coordinates for control.
- Evidence: A hybrid tokenization scheme or differentiable decoder that closes the reconstruction error gap without increasing codebook size.

### Open Question 2
- Question: Does the discrete autoregressive planning paradigm maintain compute-optimal scaling laws when applied to significantly larger parameter budgets (e.g., >1B parameters)?
- Basis: [explicit] The abstract states: "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving."
- Why unresolved: The authors demonstrate success with a compact 160M model, but it is unclear if the favorable scaling laws of decoder-only Transformers persist in driving domains without saturating or overfitting to IL distributions.
- Evidence: A scaling curve analysis plotting training FLOPs against open/closed-loop metrics for models up to 7B parameters.

### Open Question 3
- Question: Can the sequential autoregressive decoding be accelerated to meet strict real-time latency constraints as horizon length and model capacity increase?
- Basis: [inferred] The architecture relies on decoder-only Transformers, which suffer from sequential inference bottlenecks compared to non-autoregressive single-pass methods.
- Why unresolved: The paper achieves strong results with a compact model but does not profile inference speed, which typically degrades linearly with horizon length in AR models.
- Evidence: Demonstration of real-time (e.g., <50ms) planning using parallel decoding or speculative decoding techniques without loss of planning fidelity.

## Limitations

- Discrete action space limitations: Fixed-bin κ-a discretization may struggle with rare maneuvers exceeding the specified range (κ > 0.48 rad/m), potentially causing systematic planning errors in aggressive driving scenarios.
- Offline RL stability concerns: SAC-BC fine-tuning shows promising closed-loop improvements but remains sensitive to hyperparameter tuning and reward mis-specification, with limited analysis of Q-value overestimation under distribution shift.
- BEV prediction error propagation: The VQ-VAE-based BEV tokenizer achieves "adequate" reconstruction quality but doesn't fully quantify how BEV prediction errors (missing obstacles, incorrect semantics) propagate to safety-critical planning decisions.

## Confidence

**High Confidence Claims**:
- The decoder-only Transformer architecture with sparse MoE layers is technically sound and achieves state-of-the-art open-loop performance on NuPlan (1.202m ADE, 91.68% OLS)
- The fixed-bin κ-a discretization provides more stable action space representation than learned codebooks (IBQ) for sparse trajectory data
- The joint BEV-trajectory forecasting approach enables dense spatio-temporal supervision that improves coupling between scene understanding and motion generation

**Medium Confidence Claims**:
- The SAC-BC fine-tuning stage meaningfully improves closed-loop safety metrics (TTC from 94.6→97.4) while preserving BC priors
- The 160M parameter budget represents an efficient design choice compared to larger planning systems
- The post-tuning stage effectively smooths discrete trajectory artifacts without significant progress degradation

**Low Confidence Claims**:
- The method's robustness to BEV prediction failures in complex urban scenarios
- Generalization performance beyond the specific datasets and driving conditions tested
- The long-term stability of the SAC-BC fine-tuning under distribution shift

## Next Checks

**Validation Check 1: Stress Test κ-a Discretization Limits**
- Systematically evaluate trajectory reconstruction accuracy as a function of curvature magnitude and acceleration rate
- Generate synthetic trajectories spanning the full κ-a range and measure reconstruction bias/saturation
- Identify the exact thresholds where the fixed-bin discretization fails and quantify impact on planning performance

**Validation Check 2: Offline RL Sensitivity Analysis**
- Perform ablation studies on SAC-BC hyperparameters (α_cql, λ_awac, reward weights) across multiple random seeds
- Measure Q-value overestimation and OOD action probability during fine-tuning
- Compare SAC-BC performance against pure IL baselines on held-out test scenarios with varying complexity

**Validation Check 3: BEV Prediction Error Propagation**
- Quantify the correlation between BEV reconstruction error (pixel-wise accuracy, semantic IoU) and trajectory planning performance
- Ablate VQ-VAE codebook size and measure impact on both BEV quality and downstream planning metrics
- Test closed-loop performance when BEV predictions contain systematic errors (missing obstacles, incorrect semantics)