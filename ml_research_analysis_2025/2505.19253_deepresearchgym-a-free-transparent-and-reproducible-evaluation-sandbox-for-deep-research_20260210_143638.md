---
ver: rpa2
title: 'DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox
  for Deep Research'
arxiv_id: '2505.19253'
source_url: https://arxiv.org/abs/2505.19253
tags:
- search
- systems
- used
- research
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEEPRESEARCHGYM provides a free, reproducible search API and evaluation
  protocol for benchmarking deep research systems. Built on public web corpora ClueWeb22
  and FineWeb with dense retrieval and DiskANN indexing, the API achieves lower latency
  than commercial alternatives while ensuring stable rankings.
---

# DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research

## Quick Facts
- arXiv ID: 2505.19253
- Source URL: https://arxiv.org/abs/2505.19253
- Reference count: 40
- Provides free API for benchmarking deep research systems with stable rankings and sub-second latency

## Executive Summary
DEEPRESEARCHGYM addresses the reproducibility crisis in deep research evaluation by providing a free, transparent API built on static web corpora. The framework uses DiskANN-based dense retrieval with pre-computed embeddings to achieve sub-second latency while maintaining stable rankings across runs. It extends the Researchy Questions dataset with LLM-as-a-judge evaluation covering relevance (KPR), factual grounding (citation precision/recall), and report quality. A key contribution is demonstrating that agents trained within the sandbox generalize to commercial search APIs, enabling cost-effective training without sacrificing real-world performance.

## Method Summary
The sandbox uses DiskANN approximate nearest neighbor search on pre-computed document embeddings from MiniCPM-Embedding-Light, partitioning ClueWeb22 and FineWeb into 25M-document shards for parallel retrieval. The API provides deterministic /search and /fetch endpoints with configurable candidate list size (L) to balance recall against latency. Evaluation uses LLM judges to assess answer relevance, citation accuracy, and overall report quality on the Researchy Questions test set. The framework enables reproducible benchmarking by eliminating variance from live web indexing and ranking changes.

## Key Results
- Achieves sub-second latency (0.5s for K=100) with 94%+ ANN recall at R@100
- Maintains stable rankings across runs on static corpora (ClueWeb22 2022, FineWeb 2024)
- Top systems achieve KPR scores above 70% on the benchmark
- Sandbox-trained agents match performance of commercially-trained agents on GAIA (20.3% vs 18.4%)

## Why This Works (Mechanism)

### Mechanism 1
The DiskANN-based dense retrieval architecture enables sub-second latency while maintaining retrieval quality through parallel shard querying and configurable candidate lists. This trades slight recall for dramatic speed improvements.

### Mechanism 2
Stable document rankings across runs enable reproducible benchmarking by operating on fixed snapshots rather than live web content, eliminating variance from continuous re-indexing.

### Mechanism 3
Agents trained on the sandbox generalize to commercial search because reinforcement learning teaches robust search strategies that transfer across retrieval backends, not corpus-specific mappings.

## Foundational Learning

- **Dense retrieval with approximate nearest neighbor search**
  - Why needed: The entire search infrastructure depends on understanding embedding-based similarity search and DiskANN's graph-based approximation
  - Quick check: Can you explain why increasing the L parameter in DiskANN improves recall at the cost of latency?

- **LLM-as-a-judge evaluation methodology**
  - Why needed: The evaluation protocol relies entirely on LLM judges for KPR, citation precision/recall, and quality scores
  - Quick check: Why does the paper use structured JSON output formats and explicit scoring rubrics in judge prompts?

- **Agentic search with reinforcement learning (GRPO)**
  - Why needed: The training case study uses GRPO to teach search agents when to issue queries
  - Quick check: What three actions does the trained search agent have access to, and how does LLM-as-a-judge provide the reward signal?

## Architecture Onboarding

- **Component map**: /search endpoint → DiskANN index → ranked documents → /fetch endpoint → archived document content

- **Critical path**:
  1. Obtain corpus access (FineWeb immediate, ClueWeb22 requires license)
  2. Register for API credentials at deepresearchgym.ai
  3. Integrate /search calls into your deep research system's retrieval loop
  4. Run evaluation protocol on Researchy Questions test set using provided judge prompts

- **Design tradeoffs**:
  - Static vs. live corpora: Reproducibility gained, temporal coverage lost
  - L=K×5 default: Balances latency (~0.5s for K=100) against 95%+ ANN recall
  - Single LLM judge (gpt-4.1-mini): Cost-efficient, but Appendix F shows 0.4-0.8 correlation with alternative judges

- **Failure signatures**:
  - KPR scores plateau despite improved retrieval → likely multi-facet coverage gaps (78.5% of failures per Appendix G)
  - Low citation precision with high recall → citations reference broad sections rather than specific claims
  - Query-level correlation drops between APIs → retrieval faithfulness is sensitive to document distribution shifts

- **First 3 experiments**:
  1. Reproduce Table 2 baseline: Run GPT-Researcher with DEEPRESEARCHGYM API on 100 Researchy Questions, verify KPR ~64% and citation precision ~85%
  2. Ablate L parameter: Compare retrieval quality (nDCG@10) and latency with L=100 vs. L=500 on held-out queries
  3. Generalization test: Train a simple search agent on ClueWeb22-B, evaluate on both DEEPRESEARCHGYM and a commercial API to replicate Table 3 transfer results

## Open Questions the Paper Calls Out
None

## Limitations

- Static corpora cannot capture temporally evolving information needs (18.4% of queries are time-sensitive)
- Single LLM judge approach shows only moderate correlation (0.4-0.8) with alternative judges
- 25M document shard size may create retrieval bottlenecks for very broad queries

## Confidence

- **High confidence**: Latency claims and stable ranking reproducibility are well-supported by empirical measurements
- **Medium confidence**: Sandbox-trained agents generalizing to commercial APIs is promising but limited to one case study
- **Low confidence**: Claims about strategy transfer "regardless of which API" overstate evidence from single-provider testing

## Next Checks

1. **Temporal robustness test**: Evaluate the same 100 Researchy Questions on both 2022 ClueWeb22 and 2024 FineWeb corpora to quantify degradation for time-sensitive queries

2. **Multi-judge reliability assessment**: Run full evaluation protocol using three different LLM judges on a stratified sample of 50 questions to quantify variance in KPR scores

3. **Cross-API generalization study**: Train search agents using DEEPRESEARCHGYM with one commercial API, then test transfer performance across three additional commercial APIs