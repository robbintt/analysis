---
ver: rpa2
title: GNN Explanations that do not Explain and How to find Them
arxiv_id: '2601.20815'
source_url: https://arxiv.org/abs/2601.20815
tags:
- explanations
- explanation
- section
- class
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Self-explainable Graph Neural Networks (SE-GNNs) provide explanations
  during inference by coupling an explanation extractor that identifies explanatory
  subgraphs with a classifier that uses these subgraphs to generate predictions. However,
  this work identifies a critical failure: explanations can be completely unrelated
  to how SE-GNNs infer labels.'
---

# GNN Explanations that do not Explain and How to find Them

## Quick Facts
- arXiv ID: 2601.20815
- Source URL: https://arxiv.org/abs/2601.20815
- Authors: Steve Azzolin; Stefano Teso; Bruno Lepri; Andrea Passerini; Sagar Malhotra
- Reference count: 40
- One-line primary result: Existing faithfulness metrics fail to detect degenerate explanations in SE-GNNs, but the proposed EST metric reliably identifies them

## Executive Summary
Self-explainable Graph Neural Networks (SE-GNNs) provide explanations during inference by coupling an explanation extractor with a classifier. However, this work identifies a critical failure: explanations can be completely unrelated to how SE-GNNs infer labels. The study demonstrates that SE-GNNs can achieve optimal accuracy while producing degenerate explanations that encode the predicted label without revealing the actual decision-making process. This failure can be maliciously exploited to conceal sensitive attributes and can emerge naturally during training. The authors introduce a novel faithfulness metric, EST (Extension Sufficiency Test), which reliably identifies unfaithful explanations by evaluating all possible supergraphs of the explanation within the input graph.

## Method Summary
The study introduces a malicious training procedure that forces SE-GNNs to output task-irrelevant explanations (degenerate explanations) while maintaining high predictive accuracy. The attack optimizes a standard classification loss plus a binary cross-entropy loss that forces the model to assign high relevance scores to designated anchor nodes present in all graphs. The proposed Extension Sufficiency Test (EST) metric evaluates explanation faithfulness by sampling all possible supergraphs of the explanation and measuring the maximum change in prediction. Experiments compare EST against popular faithfulness metrics on attacked SE-GNN models and show that EST consistently rejects degenerate explanations while other metrics fail.

## Key Results
- SE-GNNs can achieve optimal accuracy while producing explanations that are completely unrelated to the actual decision-making process
- Existing faithfulness metrics fail to detect degenerate explanations, with rejection ratios of zero for known-unfaithful explanations
- EST consistently achieves the highest rejection ratios across all tested configurations, reliably identifying degenerate explanations
- The attack is successful on both synthetic (RBGV) and real-world (SST2P) datasets with multiple SE-GNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SE-GNNs can achieve optimal accuracy while producing explanations that are unrelated to how the model makes predictions.
- Mechanism: The explanation extractor can encode the predicted label within a "degenerate" subgraph (an anchor set) present in every graph, and the classifier then maps this subgraph to the label. This allows the explanation to be task-irrelevant while still serving as a perfect label-encoding mechanism for the classifier.
- Core assumption: The SE-GNN consists of a hard explanation extractor and a separate classifier, and there exists a set of subgraphs that appear in every graph and have no class-discriminative power.
- Evidence anchors:
  - [abstract] "We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations..."
  - [section 3] "Theorem 1... shows that the explanation extractor can use any recurrent set of nodes as label-encoding explanations for the classifier. These explanations are unambiguously unfaithful."
  - [corpus] The paper "Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective" provides a formal definition of Minimal Explanations and analyzes their properties.

### Mechanism 2
- Claim: Existing faithfulness metrics can fail to identify degenerate explanations because each metric only probes a restricted set of perturbations.
- Mechanism: Metrics like sufficiency (perturbing the complement) or necessity (perturbing the explanation) test specific hypotheses. A degenerate explanation can be constructed to be robust to one type of perturbation while being unfaithful, thereby evading detection.
- Core assumption: Faithfulness is measured by specific perturbation-based metrics rather than an exhaustive search over all possible explanations.
- Evidence anchors:
  - [section 5.2] "existing metrics can fail because each of them focuses on a restricted set of perturbations... for each of them, it is possible to construct unfaithful explanations evading detection under the chosen perturbations."
  - [table 3] Shows rejection ratios of zero for many metrics on known-unfaithful explanations.

### Mechanism 3
- Claim: The proposed Extension Sufficiency Test (EST) metric reliably identifies degenerate explanations by probing all supergraphs of the explanation.
- Mechanism: EST defines sufficiency by testing every subgraph of the input graph that contains the explanation. It computes the maximum change in prediction over this set. If the explanation is degenerate, a supergraph exists where the prediction changes, leading to a high EST score and rejection of the explanation.
- Core assumption: The true prediction depends on features that can be part of the supergraphs tested. Testing all supergraphs makes it impossible to hide task-relevant information in the complement without it being found.
- Evidence anchors:
  - [definition 1] "EST is a simple but robust faithfulness metric that avoids the previously underlined issues by holistically considering all supergraphs G' of R contained in G..."
  - [section 5.2 results] "EST is the only metric consistently achieving the highest – or close to thereof – rejection ratios across every configuration..."

## Foundational Learning

**Concept: Self-Explainable Graph Neural Networks (SE-GNNs)**
- Why needed here: This is the core subject of the paper. Understanding that SE-GNNs have an *explanation extractor* and a *classifier* is essential for grasping how a misalignment between the two can occur.
- Quick check question: What are the two main components of an SE-GNN?

**Concept: Graph Classification and Subgraphs**
- Why needed here: The paper operates on graph data where a subgraph is selected as an explanation. Understanding that a graph $G$ contains a subgraph $R$ is crucial for following the logic of sufficiency and necessity metrics.
- Quick check question: In the paper, what denotes the explanatory subgraph?

**Concept: Sufficiency and Necessity in Explainability**
- Why needed here: The paper critiques existing metrics based on these concepts. Knowing that *sufficiency* means the explanation alone is enough for the prediction, and *necessity* means the prediction fails without it, is key to understanding the proposed solution (EST).
- Quick check question: What does a sufficiency metric like Fid- measure?

## Architecture Onboarding

**Component map:**
Input Graph $\rightarrow$ Explanation Extractor $e(G) \rightarrow$ Explanatory Subgraph $R \rightarrow$ Classifier $g(R) \rightarrow$ Prediction

**Critical path:**
1. An input graph $G$ is fed to the SE-GNN
2. The explanation extractor $e$ identifies a subgraph $R$
3. The classifier $g$ uses $R$ to produce the final prediction
4. (Auditing with EST) The metric generates multiple $G'$ by randomly sampling nodes and edges from the complement of $R$ in $G$, while ensuring $R$ is always included
5. Each $G'$ is fed back to the classifier to check if the prediction changes

**Design tradeoffs:**
- EST Metric Complexity vs. Robustness: EST is more computationally expensive than simpler metrics because it samples many supergraphs, but it is theoretically guaranteed to catch more unfaithful explanations
- Conservativeness: EST prioritizes minimizing false negatives at the risk of higher false positives

**Failure signatures:**
- Degenerate Explanations: An SE-GNN provides explanations that are consistently task-irrelevant but the model still achieves high accuracy
- Metric Failure: An existing faithfulness metric gives a high score to an explanation that is known to be unfaithful

**First 3 experiments:**
1. Implement the malicious training procedure to force a model like GSAT to output a pre-defined, task-irrelevant explanation while maintaining high accuracy
2. Benchmark existing faithfulness metrics by computing rejection ratios for the attacked model
3. Apply and verify EST by implementing the metric and running it on the attacked model's explanations

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on assumptions about SE-GNN architecture that may not hold for all variants
- EST's conservativeness means it may reject some faithful explanations that are not Prime Implicant explanations
- The study focuses on graph data; the failure modes may not generalize to other data types like images or tabular data

## Confidence

**High confidence:** SE-GNNs can achieve optimal accuracy while producing degenerate explanations (Section 3 theoretical analysis)

**High confidence:** Existing faithfulness metrics fail to detect known-unfaithful explanations (Section 5.2 experimental results)

**Medium confidence:** EST reliably identifies degenerate explanations in all tested scenarios (results show consistent performance, but theoretical guarantees are conservative)

**Medium confidence:** The proposed malicious attack is practical and effective (demonstrated on two datasets with multiple models)

## Next Checks
1. Test EST on non-prime-implicant faithful explanations to quantify false positive rates in practice
2. Evaluate whether the degenerate explanation failure mode extends to post-hoc explanation methods for GNNs
3. Assess EST's performance on real-world graph datasets (e.g., social networks, molecular graphs) where anchor sets may not exist