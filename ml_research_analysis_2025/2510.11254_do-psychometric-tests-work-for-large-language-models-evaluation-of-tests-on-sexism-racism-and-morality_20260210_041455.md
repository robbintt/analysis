---
ver: rpa2
title: Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on
  Sexism, Racism, and Morality
arxiv_id: '2510.11254'
source_url: https://arxiv.org/abs/2510.11254
tags:
- llms
- tests
- test
- answer
- psychometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Psychometric tests from psychology are increasingly applied to
  evaluate LLMs, but it remains unclear if they yield meaningful results when applied
  to LLMs. This study systematically evaluates reliability and validity of human psychometric
  tests on 17 LLMs for three constructs: sexism, racism, and morality.'
---

# Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality

## Quick Facts
- **arXiv ID**: 2510.11254
- **Source URL**: https://arxiv.org/abs/2510.11254
- **Reference count**: 40
- **Primary result**: Psychometric test scores show moderate reliability but fail to predict LLM behavior in downstream tasks, with negative correlations indicating low ecological validity.

## Executive Summary
This study systematically evaluates whether human psychometric tests can reliably and validly measure constructs like sexism, racism, and morality in Large Language Models. The authors find that while test scores show moderate reliability across multiple item and prompt variations, they fail to align with model behavior in downstream tasks. Crucially, psychometric test scores sometimes negatively correlate with model behavior, suggesting that LLMs can "say the right thing" on tests while still exhibiting biased behavior in practical applications. These findings highlight the need for LLM-specific test development and validation before interpreting psychometric scores for AI systems.

## Method Summary
The researchers evaluated 17 LLMs using three established psychometric tests: the Ambivalent Sexism Inventory, Symbolic Racism 2000 Scale, and Moral Foundations Questionnaire. They assessed reliability through alternate forms, reversed answer options, and punctuation changes, then measured validity by correlating test scores with three behavioral tasks: reference letter generation (sexism), housing recommendations (racism), and moral advice (morality). Each test was run with 5 seeds, and downstream tasks were evaluated using both dictionary-based scoring and GPT-4o assessment.

## Key Results
- Psychometric tests show moderate reliability across multiple item variations but exhibit significant order-sensitivity when answer options are reversed
- Test scores fail to align with downstream behavioral task performance, with some showing negative correlations
- Larger models within families tend to show more pronounced problematic behavior despite lower psychometric test scores
- LLMs demonstrate ability to score well on bias tests while still generating biased content in practical applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Psychometric test scores misalign with downstream behavior because direct opinion queries trigger RLHF guardrails while behavioral tasks bypass them
- **Mechanism**: LLMs are trained to refuse or correct biased responses to explicit bias questions, but this surface-level alignment does not extend to contextual generation tasks where biases emerge implicitly
- **Core assumption**: RLHF optimization targets explicit harm statements but fails to address subtle behavioral manifestations
- **Evidence anchors**:
  - [abstract] "test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks"
  - [section 6] "psychometric items often ask for direct opinions on sensitive topics... which can trigger guardrails that may not activate in subtler tasks"
- **Break condition**: If models with no RLHF alignment still show negative test-behavior correlations, guardrails are not the cause

### Mechanism 2
- **Claim**: Likert-scale answer formats introduce order-sensitivity artifacts that undermine reliability
- **Mechanism**: LLMs exhibit positional bias when selecting from ordered options, producing inconsistent scores when option order is reversedâ€”even when semantic content is identical
- **Core assumption**: LLMs process option lists with attention mechanisms that favor certain positions, unlike humans who comprehend scales semantically
- **Evidence anchors**:
  - [section 5.1] "reliability drops significantly when altering the order of answer options, with models like Llama 3.1 8B and Qwen 2.5 7B showing particularly inconsistent behavior"
- **Break condition**: If alternative response formats (open-ended, pairwise comparison) eliminate the order-sensitivity effect, format is causal

### Mechanism 3
- **Claim**: Larger or more capable models exhibit stronger negative correlations between test scores and biased behavior
- **Mechanism**: Advanced instruction-following improves surface compliance with anti-bias prompts while generation capabilities amplify subtle biases; better models become more skilled at "saying the right thing" while acting differently
- **Core assumption**: Capability gains in instruction-following and generation are decoupled, allowing divergence
- **Evidence anchors**:
  - [section 5.2] "models with larger parameter sizes within their family tend to exhibit more pronounced problematic behavior despite scoring lower on the psychometric tests"
- **Break condition**: If smaller models show the same negative correlation magnitude, capability is not the driver

## Foundational Learning

- **Concept: Ecological Validity**
  - **Why needed here**: Distinguishes whether a test measures what it claims in real-world contexts vs. artificial settings; the paper's central critique is that human tests lack ecological validity for LLMs
  - **Quick check question**: Does the test score predict behavior in the actual deployment context?

- **Concept: Construct Validity (Convergent)**
  - **Why needed here**: Determines whether test scores relate to other measures as theory predicts; the paper shows convergent validity exists but doesn't rescue ecological validity
  - **Quick check question**: Do scores correlate with theoretically related constructs?

- **Concept: RLHF Alignment Surface**
  - **Why needed here**: Understanding that reinforcement learning from human feedback optimizes for explicit refusal patterns without addressing implicit behavioral biases
  - **Quick check question**: Does alignment on explicit queries transfer to contextual generation?

## Architecture Onboarding

- **Component map**: Behavioral probe layer -> downstream task design (reference letters, housing recommendations, advice) -> Validation pipeline -> reliability testing (alternate forms, option order, EOS changes) + validity testing (convergent, ecological) -> Comparison framework -> human baseline for reliability thresholds

- **Critical path**:
  1. Select construct and theory
  2. Design behavioral task grounded in theory (not Likert-scale self-report)
  3. Validate task against human behavioral patterns if possible
  4. Run on model with multiple seeds
  5. Compare to psychometric test scores for same model

- **Design tradeoffs**:
  - Open-ended generation tasks better reflect real use but harder to score objectively
  - Closed-form tasks easier to score but may not capture actual behavior
  - Multiple behavioral tasks per construct increase coverage but multiply cost

- **Failure signatures**:
  - High test reliability + low ecological validity = test misaligned with behavior
  - Negative test-behavior correlation = surface compliance without behavioral change
  - Order-sensitivity on reversed options > 20% inconsistency = positional bias artifact

- **First 3 experiments**:
  1. Replicate the paper's reliability tests on your target model with reversed answer options to detect positional bias
  2. Run sexism reference letter task with male/female name pairs and score for agentic vs. communal language frequency
  3. Correlate psychometric test scores with behavioral task scores; expect near-zero or negative correlation if the paper's findings generalize

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can a valid "sample" of LLMs be constructed to support standard psychometric validation techniques like factor analysis?
- **Basis in paper**: [explicit] The authors explicitly ask, "How exactly would a sample of LLMs look like?" noting that current validation relies on inter-individual variability absent in single models.
- **Why unresolved**: Standard factor analysis requires a population of individuals with meaningful variance, but it is theoretically undefined whether "individual differences" in LLMs should be represented by different personas, random seeds, or model families.
- **What evidence would resolve it**: A theoretical framework defining the dimensions of variation for an LLM population and the successful application of factor analysis on such a sample.

### Open Question 2
- **Question**: Does using open-ended answer formats instead of Likert scales improve the ecological validity of psychometric tests for LLMs?
- **Basis in paper**: [explicit] The authors state, "Future work could further examine the impact of factors such as answer formats... to improve ecological validity."
- **Why unresolved**: The study hypothesizes that closed-answer formats may confound evaluations and that open-ended formats might align better with generative behavior, but this was not tested.
- **What evidence would resolve it**: A comparative study administering the same tests in open-ended and Likert formats, followed by a correlation analysis with downstream behavioral tasks.

### Open Question 3
- **Question**: Do the findings of low ecological validity generalize to other psychological constructs such as personality, political leaning, or values?
- **Basis in paper**: [explicit] The authors explicitly note in the limitations: "Further research should assess if similar findings hold for others, e.g., personality, political leaning, or values."
- **Why unresolved**: The current study was restricted to sexism, racism, and morality; constructs that are less sensitive or have different theoretical grounding might exhibit different alignment between test scores and behavior.
- **What evidence would resolve it**: Replication of the paper's validation framework (reliability and ecological validity checks) applied to personality or political psychometric tests.

## Limitations

- Unclear causal mechanism behind negative correlation between test scores and downstream behavior (guardrail effects vs. contamination vs. other factors)
- Ecological validity analysis relies on proxy tasks that may not fully capture real-world behavior
- Findings may not generalize to all LLMs and constructs beyond the three tested

## Confidence

- **High confidence**: Reliability findings showing order-sensitivity artifacts when answer options are reversed
- **Medium confidence**: Ecological validity conclusions showing negative correlations between test scores and behavioral task performance
- **Low confidence**: Generalizability of findings across all LLMs and constructs

## Next Checks

1. Run reliability tests with reversed answer options on your target model using the exact prompt templates and regex extraction patterns to verify positional bias effect

2. Test the guardrail hypothesis by running the same construct on models with and without RLHF alignment (base vs. instruction-tuned versions) to isolate causal mechanism

3. Implement an alternative response format (open-ended generation or pairwise comparison instead of Likert scales) for one construct and measure whether order-sensitivity artifact disappears