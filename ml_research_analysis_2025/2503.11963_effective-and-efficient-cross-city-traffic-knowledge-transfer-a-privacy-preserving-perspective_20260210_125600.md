---
ver: rpa2
title: 'Effective and Efficient Cross-City Traffic Knowledge Transfer: A Privacy-Preserving
  Perspective'
arxiv_id: '2503.11963'
source_url: https://arxiv.org/abs/2503.11963
tags:
- traffic
- data
- training
- transfer
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Effective and Efficient Cross-City Traffic Knowledge Transfer: A Privacy-Preserving Perspective

## Quick Facts
- **arXiv ID**: 2503.11963
- **Source URL**: https://arxiv.org/abs/2503.11963
- **Authors**: Zhihao Zeng; Ziquan Fang; Yuting Huang; Lu Chen; Yunjun Gao
- **Reference count**: 40
- **Primary result**: FedTT achieves state-of-the-art MAE on cross-city traffic prediction while reducing communication by ~90% vs. standard FedAvg

## Executive Summary
This paper proposes FedTT, a federated learning framework for cross-city traffic knowledge transfer that addresses three key challenges: missing traffic data, domain distribution discrepancies, and privacy preservation. The framework integrates Traffic View Imputation (TVI) for data completion, Traffic Domain Adapter (TDA) for distribution alignment, and Traffic Secret Transmission (TST) for lightweight secure aggregation. Experiments on four real-world traffic datasets demonstrate FedTT's effectiveness (lower MAE/RMSE) and efficiency (reduced communication and runtime) compared to existing methods.

## Method Summary
FedTT is a federated learning framework that transfers traffic knowledge from multiple source cities to a target city while preserving privacy. It consists of three main modules: TVI for imputing missing traffic data using spatio-temporal graph neural networks, TDA for aligning traffic data distributions between source and target cities using a GAN-based approach, and TST for secure data aggregation that minimizes communication overhead by using masking instead of homomorphic encryption for all but the first round. The framework uses split learning with frozen data updated every 5 rounds to enable parallel training while maintaining data locality.

## Key Results
- FedTT achieves state-of-the-art MAE on cross-city traffic prediction tasks across four real-world datasets
- Communication overhead is reduced by approximately 90% compared to standard FedAvg through the TST masking protocol
- Privacy is preserved through differential masking while maintaining model effectiveness
- The framework demonstrates robustness to missing data through the TVI module's spatio-temporal imputation

## Why This Works (Mechanism)

### Mechanism 1: Traffic View Imputation (TVI) for Missing Data Completion
Spatio-temporal dependency modeling enables robust traffic data imputation, improving downstream transfer quality when sensor readings are incomplete. TVI operates in two stages—spatial view extension computes shortest-path distances between sensors via Dijkstra's algorithm, then applies Graph Attention Networks (GAT) to aggregate features from available sensors and predict missing values; temporal view enhancement uses the DyHSL model to predict next/previous traffic subviews from historical sequences, averaging bidirectional predictions. The final imputed traffic view combines actual readings with predicted values for unavailable sensors.

### Mechanism 2: Traffic Domain Adapter (TDA) for Distribution Alignment
Explicit domain transformation from source to target traffic distributions improves knowledge transfer effectiveness compared to direct model parameter sharing. TDA uses a GAN-based architecture with three operations: domain transformation computes two matrices—A_G (road network transformation via gradient descent on adjacency alignment) and A_P (prototype transformation via gradient descent on average traffic features); the generator θ_Gen applies these via MLP layers to map source data to target domain; domain alignment trains the generator to minimize distance from transformed data to target prototype P_S; domain classification trains discriminator θ_Dis to distinguish transformed vs. genuine target data, providing adversarial signal to the generator.

### Mechanism 3: Traffic Secret Transmission (TST) for Lightweight Privacy
Masking individual transformed data with aggregated data from previous rounds prevents inference of individual city contributions while avoiding homomorphic encryption overhead. TST uses a differential masking scheme where client c_i computes masked data X^(R→S,R_i) = X^(R→S)_(r-1) + X^(R_i→S)_r - X^(R_i→S)_(r-1), transmitting only the mask. The server sums all masks and reconstructs current aggregated data via X^(R→S)_r = ΣX^(R→S,R_i)_r - (n-1)×X^(R→S)_(r-1). Only the first round requires homomorphic encryption for initial aggregation; subsequent rounds use plaintext masking since only aggregated data is exposed.

## Foundational Learning

- **Federated Learning (FL) Fundamentals**: Why needed: FedTT modifies the standard FL paradigm—instead of aggregating model parameters (FedAvg), it aggregates transformed data while keeping the model training on the server side. Quick check: Can you explain why sharing gradients/parameters (standard FL) still poses privacy risks via gradient inversion attacks?

- **Generative Adversarial Networks (GANs)**: Why needed: TDA employs a GAN framework where the generator transforms source domain data and the discriminator distinguishes transformed vs. target data. Quick check: In the TDA module, what happens to the generator if λ_1 is set too high (discriminator loss dominates)?

- **Spatio-Temporal Graph Neural Networks**: Why needed: TVI uses GAT for spatial dependencies and DyHSL for temporal dependencies; the core traffic prediction model θ_TP defaults to DyHSL. Quick check: Why does the paper use shortest-path distance (Dijkstra) rather than direct adjacency for computing spatial features in the GAT layer?

## Architecture Onboarding

- **Component map**: Client-side (Source Cities R_i): TVI module (θ_SV + θ_TV, trained before federation) → TDA Generator θ^(R_i)_Gen → TDA Local Discriminator θ^(R_i)_Dis → TST Masking; Server-side (Target City S): TDA Global Discriminator θ_Dis → Traffic Prediction Model θ_TP → Aggregated Data X^(R→S)

- **Critical path**: 1. Pre-training: Each client trains TVI models (θ_SV, θ_TV) locally on their incomplete data (Eq. 35); 2. Round 1: Clients transform data via TDA, encrypt with HE, send to server; 3. Server aggregates encrypted data, broadcasts X^(R→S)_1; 4. Rounds 2+: Clients compute masks, send plaintext to server; server reconstructs aggregated data; 5. Server trains θ_Dis and θ_TP on mixed data (aggregated + target local); 6. Clients train θ^(R_i)_Gen and θ^(R_i)_Dis using frozen server discriminator outputs (Eq. 29, split learning)

- **Design tradeoffs**: Data-level vs. Model-level Aggregation: FedTT aggregates transformed data rather than model parameters—this enables domain adaptation at the data level but increases communication for large datasets; HE-only vs. Masking: Full HE every round would guarantee stronger privacy but add ~10-100x overhead (paper cites [47, 52]); masking reduces to HE-only in round 1, trading theoretical strength for efficiency; Frozen vs. Real-time Server Feedback: FPT freezes server discriminator outputs for 5 rounds (Eq. 29, Fr function) to enable parallel training, introducing staleness in adversarial signal

- **Failure signatures**: MAE does not decrease after round 10: Check if λ_1/λ_2 are misconfigured (Fig. 10 shows sensitivity); verify TDA generator is not collapsing (discriminator accuracy should stay near 0.5); Communication size unexpectedly high: Verify first-round HE is not repeated every round (check Algorithm 1 lines 6-8 condition); Missing data imputation produces NaN: Check if GAT receives empty neighbor sets (all sensors unavailable at time t); TVI should fall back to temporal-only prediction; Aggregated data reconstruction fails: Verify client-server counter synchronization; masking requires consistent r-1 values

- **First 3 experiments**: 1. TVI Validation on Single City: Train θ_SV and θ_TV on HK-Traffic with 20% sensors masked; report imputation MAE vs. baseline (zero-filling, LATC). This validates the imputation module in isolation before federation; 2. TDA Domain Transfer Test: Using (P4, FT, HK)→P8 setting, visualize t-SNE plots of: (a) raw source data, (b) TDA-transformed source data, (c) target data. Quantify distribution alignment via Maximum Mean Discrepancy (MMD); 3. End-to-End Ablation: Run FedTT on (P8, FT, HK)→P4 with all modules, then remove TST (use standard FedAvg aggregation on transformed data); compare MAE, communication size (GB), and running time (minutes) to verify TST efficiency claims (Fig. 6 shows ~90% communication reduction)

## Open Questions the Paper Calls Out
The paper identifies several areas for future work but does not explicitly call out open questions in the text provided.

## Limitations
- Learning rates, batch sizes, and number of training rounds for all modules are unspecified, making exact reproduction difficult
- The homomorphic encryption method and data normalization strategy are not specified in the paper
- The Frozen function Fr(·) implementation details are missing, affecting reproducibility of the split learning component

## Confidence
- **Mechanism 1 (TVI)**: Medium - Well-grounded in spatio-temporal dependency modeling with clear evidence in equations and module descriptions
- **Mechanism 2 (TDA)**: Medium - Plausible GAN-based approach but effectiveness depends heavily on hyperparameter tuning
- **Mechanism 3 (TST)**: Medium - Sound in principle but security guarantees rely on assumptions about aggregated data that require further validation

## Next Checks
1. **TVI Isolation Test**: Validate the TVI module independently on HK-Traffic with 20% sensors masked, comparing imputation MAE against zero-filling and LATC baselines
2. **TDA Domain Alignment Visualization**: Using the (P4, FT, HK)→P8 setting, visualize t-SNE plots of raw source data, TDA-transformed source data, and target data to quantify distribution alignment via MMD
3. **TST Privacy Leakage Analysis**: Analyze the masking protocol for potential privacy leakage, particularly in scenarios with small n (e.g., n=2) or when clients collude with the server