---
ver: rpa2
title: 'From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL'
arxiv_id: '2510.25997'
source_url: https://arxiv.org/abs/2510.25997
tags:
- queries
- agent
- schema
- naive
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an agentic LLM pipeline for natural-language-to-SQL
  (NL-to-SQL) queries over spatio-temporal check-in datasets. The key idea is to extend
  a naive text-to-SQL baseline with a Mistral-based ReAct agent that can plan, decompose,
  and adapt queries using schema inspection, SQL generation, execution, and visualization
  tools.
---

# From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL

## Quick Facts
- arXiv ID: 2510.25997
- Source URL: https://arxiv.org/abs/2510.25997
- Authors: Manu Redd; Tao Zhe; Dongjie Wang
- Reference count: 8
- Key outcome: Agentic LLM pipeline achieves 91.4% accuracy vs 28.6% baseline on NL-to-SQL over spatio-temporal check-in data

## Executive Summary
This paper introduces an agentic LLM pipeline that extends a naive text-to-SQL baseline with a Mistral-based ReAct agent capable of planning, decomposing, and adapting queries through schema inspection and tool use. Evaluated on 35 queries over NYC and Tokyo check-in datasets, the agentic pipeline achieved 91.4% accuracy versus 28.6% for the baseline, with particularly large gains on spatial, external knowledge, and multi-table reasoning tasks. The agent automatically produces maps, plots, and structured summaries to enhance usability.

## Method Summary
The approach compares a naive single-pass SQL generation baseline against an agentic pipeline built with Mistral Large and LangGraph. The agentic system uses six tools: schema inspection, SQL generation (via llama-3-sqlcoder-8b), execution, file reading, plotting, and mapping. It implements a ReAct-style plan-act-observe loop that reformulates queries based on schema inspection, retries on errors, and decomposes complex multi-constraint queries into sub-queries. The pipeline operates on NYC and Tokyo check-in datasets (227K and 574K records respectively) without PostGIS extensions, using bounding boxes for spatial operations.

## Key Results
- Agentic pipeline: 91.4% accuracy (32/35 queries correct) vs baseline: 28.6% (10/35)
- Particularly strong on spatial/geographic (S: 14.3%→100%), external knowledge (E: 0%→100%), and multi-table reasoning (M: 14.3%→100%) categories
- Agent averaged 1.51 SQL generation calls per query versus 1.0 for baseline
- Automatic generation of maps, plots, and structured summaries for all successful queries

## Why This Works (Mechanism)

### Mechanism 1: Schema-Aware Reformulation
The agentic pipeline improves accuracy by actively inspecting database schema and reformulating vague natural language queries into schema-aligned predicates before SQL generation. User query → schema inspection → category/value discovery → query reformulation with concrete schema terms → SQL generation. This bridges the semantic gap between colloquial phrasing ("nightlife") and database vocabulary ("Bar", "Nightclub", "Music Venue").

### Mechanism 2: Error Recovery Through Plan-Act-Observe Loop
The ReAct-style orchestration enables iterative query refinement based on execution feedback, allowing recovery from syntax errors, empty results, and semantic mismatches. Plan → Execute → Observe results/errors → Adapt strategy → Retry. The agent detects failed executions or empty results and triggers refinement strategies (rephrasing, broadening categories, substituting bounding boxes).

### Mechanism 3: Task Decomposition for Multi-Constraint Queries
Complex spatio-temporal queries requiring multiple reasoning steps are handled by decomposing into sub-queries, executing independently, and synthesizing results. Multi-constraint query → identify sub-tasks → execute sub-queries sequentially → aggregate/compare results → generate summary and visualization. This avoids brittle single-query formulations.

## Foundational Learning

- Concept: ReAct (Reasoning + Acting) paradigm
  - Why needed here: Core orchestration pattern for the agentic pipeline—understanding plan-act-observe loops is essential for grasping how the agent iteratively refines queries
  - Quick check question: Can you explain how a ReAct agent differs from a single-pass LLM call in terms of error handling?

- Concept: Schema grounding in NL-to-SQL
  - Why needed here: Central mechanism for bridging semantic mismatch—requires understanding how natural language maps to database schema elements
  - Quick check question: Given a user query "Find late-night dining spots" and a schema with category_name values, how would you approach schema grounding?

- Concept: Spatio-temporal query semantics
  - Why needed here: Domain-specific challenges (temporal reasoning, spatial predicates, external geographic knowledge) are the primary evaluation focus
  - Quick check question: What database capabilities are needed to handle "check-ins within 2km of JFK Airport" versus "check-ins in Brooklyn"?

- Concept: LangGraph orchestration framework
  - Why needed here: Implementation framework used in the paper—understanding tool-calling, state management, and control flow is necessary for reproduction
  - Quick check question: How does LangGraph differ from direct LLM API calls when building multi-tool agents?

## Architecture Onboarding

- Component map: Mistral Large (LLM API) with LangGraph for control flow -> llama-3-sqlcoder-8b (SQL generator) -> PostgreSQL database -> 6 tools (schema_inspector, sql_generator, sql_executor, file_reader, plotter, mapper)

- Critical path:
  1. Receive natural language query
  2. Call schema inspector tool to retrieve table/column metadata + sample rows
  3. Reconstruct query with schema-aligned predicates
  4. Call SQL generator tool (wraps sqlcoder-8b)
  5. Execute SQL and observe results/errors
  6. If failure: analyze error type and retry with adaptation (rephrase/broaden/substitute)
  7. If success: optionally call visualization tools (plot/map)
  8. Generate structured natural language summary

- Design tradeoffs:
  - Latency vs. accuracy: Agentic pipeline averages 1.51 SQL generation calls vs. 1.0 baseline (~50% overhead)
  - Complexity vs. robustness: 6-tool system more complex than single-pass but enables error recovery
  - Geographic precision vs. compatibility: PostgreSQL without PostGIS limits spatial operations to bounding boxes; PostGIS would enable radius queries but add deployment complexity
  - Visualization overhead: Auto-generation improves usability but adds latency; could be made optional

- Failure signatures:
  - Q17 pattern: Agent attempts geodesic functions (ST_Distance, PostGIS) on database without extension → SQL execution error. Fix: pre-validate function availability, auto-substitute bounding boxes
  - Q21 pattern: Agent overfits to literal user phrasing without exploring schema for synonyms → empty results or wrong categories. Fix: enforce exploratory DISTINCT + fuzzy matching when direct match fails
  - Q35 pattern: Cross-dataset synthesis produces logically incorrect query despite schema access → semantic error not caught by execution. Fix: constrain multi-table patterns, add query linting for join validity

- First 3 experiments:
  1. Reproduce baseline vs. agentic comparison on 10 queries from Table 1 (mix of B, A, T categories) to validate orchestration effect—expect ~30% vs. ~90% accuracy gap
  2. Ablate schema inspection tool: run agentic pipeline without schema grounding on queries with semantic mismatch (e.g., Q15 laundromats, Q29 nightlife) to quantify contribution of reformulation mechanism—expect accuracy drop on category-misalignment queries
  3. Test error recovery: inject deliberate SQL syntax errors and empty-result scenarios to verify retry behavior—agent should adapt predicates or broaden filters within 2-3 iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the marginal accuracy contribution of specific tools (e.g., schema inspection, visualization, retries) within the agentic pipeline?
- Basis in paper: [explicit] The authors list "Ablations and tool attribution" as a future direction, noting the need to "quantify the marginal contribution of each tool."
- Why unresolved: The study compares a holistic agentic pipeline against a baseline but does not isolate gains derived from individual tools like visualization or error recovery loops.
- What evidence would resolve it: Ablation studies measuring accuracy drops when specific tools (e.g., `plot_results_tool` or `get_database_schema_tool`) are removed from the agent's toolset.

### Open Question 2
- Question: How does the pipeline generalize to heterogeneous spatio-temporal datasets without manually specified geographic priors?
- Basis in paper: [explicit] Section 8.2 calls for "Scale and generalization" to "unseen schemas," and Section 7 notes that "Geographic priors were manually specified."
- Why unresolved: The current success relies on manually injected bounding boxes (e.g., for Brooklyn), and it is unclear if the agent can autonomously derive such spatial context or handle different data structures.
- What evidence would resolve it: Evaluation results on diverse, unseen spatio-temporal datasets where the agent must retrieve or infer spatial boundaries without manual hard-coding.

### Open Question 3
- Question: Can constrained join patterns or validation guardrails effectively mitigate planning failures in complex cross-dataset synthesis?
- Basis in paper: [explicit] The authors identify "planning instability with complex cross-dataset synthesis" (Q35 failure) and suggest "constrained multi-table joins" and "systematic guardrails" as remedies.
- Why unresolved: The current agent hallucinated incorrect logic when comparing datasets, suggesting that naive ReAct planning is insufficient for reliable multi-table reasoning.
- What evidence would resolve it: A comparison of success rates on multi-table (Class X) queries between a baseline ReAct agent and an agent augmented with query linting or join constraints.

## Limitations

- Manual specification of geographic priors (bounding boxes for neighborhoods, landmarks) represents significant engineering burden that may not scale to arbitrary domains
- Single 8B SQLCoder model used for both baseline and agentic SQL generation doesn't isolate whether improvements come from orchestration versus prompting differences
- 35-query evaluation set remains relatively small for establishing robust performance differences, particularly for the X (external knowledge) category where only 2 queries exist

## Confidence

- High confidence (90%+): The agentic pipeline outperforms the baseline in overall accuracy (91.4% vs 28.6%) and on B/A/T/M reasoning categories where the improvement is substantial and consistent across multiple queries
- Medium confidence (70-89%): The mechanism explanations for schema grounding and error recovery are well-supported by specific query examples, though the general applicability to other domains remains uncertain without broader evaluation
- Low confidence (below 70%): Claims about the agent's usability improvements (automatic maps/plots/summaries) lack quantitative user studies or systematic evaluation of visualization quality and relevance

## Next Checks

1. Ablation study on schema inspection: Run the agentic pipeline without schema grounding on 10 queries with known semantic mismatches (e.g., Q15, Q29, Q21) to quantify the specific contribution of reformulation to accuracy gains

2. Error recovery stress test: Systematically inject SQL syntax errors and empty-result scenarios into 15 diverse queries to verify the agent consistently attempts 2-3 refinement iterations with meaningful adaptations rather than premature failure

3. Geographic precision comparison: Implement a PostGIS-enabled version of the pipeline and test 5 spatial queries (Q17, Q23, Q24, Q25, Q27) to measure accuracy differences between bounding box approximations versus geodesic distance calculations