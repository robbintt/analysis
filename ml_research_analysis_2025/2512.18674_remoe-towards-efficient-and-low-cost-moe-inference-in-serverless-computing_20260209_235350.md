---
ver: rpa2
title: 'Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing'
arxiv_id: '2512.18674'
source_url: https://arxiv.org/abs/2512.18674
tags:
- expert
- experts
- inference
- remote
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Remoe, a system designed to minimize the
  cost of serverless inference for Mixture-of-Experts (MoE) models. MoE models have
  a large number of experts, most of which are not activated during inference, leading
  to high memory usage and costs in serverless environments where billing is based
  on allocated resources.
---

# Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing

## Quick Facts
- arXiv ID: 2512.18674
- Source URL: https://arxiv.org/abs/2512.18674
- Reference count: 39
- Primary result: Reduces MoE inference cost by up to 57% and cold start latency by 47% in serverless environments

## Executive Summary
Remoe addresses the high memory costs of serverless inference for Mixture-of-Experts (MoE) models by introducing a heterogeneous architecture that splits non-expert and expert modules across GPUs and CPUs. The system offloads infrequently activated experts to separate serverless functions, enabling parallel execution while minimizing memory allocation. Three key techniques—semantic similarity-based activation prediction, worst-case memory pre-allocation, and Lagrangian duality optimization—work together to reduce cost by up to 57% while meeting strict service-level objectives.

## Method Summary
Remoe implements a heterogeneous serverless architecture where non-expert modules run on GPUs and experts run on CPUs, with low-frequency experts offloaded to separate functions. The system uses a Similar Prompts Searching (SPS) algorithm that builds a multi-fork clustering tree of historical prompts based on semantic similarity to predict expert activation patterns. A Main Model Pre-allocation (MMP) algorithm ensures service-level objectives through worst-case memory estimation. The remote expert allocation is optimized using Lagrangian duality with an exponential decay model of inference time versus memory allocation. The system is implemented on Kubernetes and evaluated across multiple LLM benchmarks.

## Key Results
- Reduces inference cost by up to 57% compared to state-of-the-art baselines
- Cuts cold start latency by 47% through optimized memory pre-allocation
- Maintains service-level objectives (TTFT, TPOT) while significantly reducing GB-s billing

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Function Partitioning
The system splits MoE models across GPU and CPU resources, placing attention layers on GPUs for speed while routing expert layers to CPUs. Low-frequency experts are isolated into separate serverless functions that are only invoked when needed. This works because expert computation is lightweight enough for CPUs and the network overhead of remote invocations is outweighed by memory cost savings. The architecture breaks down if most experts are needed per request, causing latency spikes.

### Mechanism 2: Semantic Similarity for Activation Prediction (SPS)
Remoe builds a multi-fork clustering tree using Soft Cosine Similarity to predict which experts will activate for new prompts. When a request arrives, the system retrieves top-α similar historical prompts and aggregates their activation distributions to determine local versus remote expert placement. This assumes stable semantic-activation relationships that aren't disrupted by model changes or fine-tuning. The approach fails if gating networks become chaotic or semantics no longer predict activation patterns.

### Mechanism 3: Lagrangian Relaxation for Memory Optimization
The paper models the relationship between CPU memory allocation and inference time as an exponential decay curve, formulating it as a convex optimization problem solvable via Lagrangian duality. This allows mathematical derivation of optimal memory allocation and replica counts to minimize cost under TTFT constraints. The method assumes the fitted exponential curve accurately reflects hardware performance and that convexity holds across different resource configurations.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Sparsity**
  - Why needed: The entire architecture relies on MoE models not using all parameters simultaneously. Understanding that tokens route to only K experts explains why keeping all experts in memory is wasteful.
  - Quick check: If a model has 8 experts per layer but routes each token to only 2, what percentage of expert parameters are "inactive" for a single forward pass?

- **Concept: Serverless Billing Model**
  - Why needed: The problem is defined by pay-per-use billing where Cost = Allocated Memory × Duration. Reducing memory footprint directly translates to linear cost savings.
  - Quick check: Why does "Expert Offloading" (moving experts from GPU to CPU memory) fail to reduce costs in a naive serverless setup?

- **Concept: Cold Starts vs. Warm Starts**
  - Why needed: Remoe optimizes for cold starts by predicting activation before loading, rather than predicting during inference which would incur cold start penalties for remote functions.
  - Quick check: Why is online token-by-token prediction incompatible with the serverless requirement to pre-allocate resources?

## Architecture Onboarding

- **Component map:**
  Main Model Function (GPU+CPU) -> Non-Expert layers (GPU) and Local Experts (CPU - High Frequency)
  Remote Expert Functions (CPU) -> Remote Experts (Low Frequency)
  SPS Engine -> Predicts activation matrix
  MMP Allocator -> Determines Main Model memory config

- **Critical path:**
  1. Input → SPS (Predict activation matrix)
  2. MMP (Determine Main Model memory config and start cold start)
  3. Selection (Label experts Local vs. Remote based on utility score)
  4. Optimization (Solve for Remote memory/replicas via Lagrangian)
  5. Execution (Parallel run of Local and Remote experts)

- **Design tradeoffs:**
  - Utility Score Selection: Setting remote expert ratio too high saves memory but risks high latency; too low negates cost savings
  - Fitting Accuracy: Simpler models fit faster but may be less accurate than complex profiling

- **Failure signatures:**
  - High Latency Variance: If serverless invocation overhead fluctuates, mathematical optimization will miss SLO targets
  - Payload Overflow: Long prompts may exceed 6MB payload limit, causing crashes

- **First 3 experiments:**
  1. Validate SPS Correlation: Scatter plot semantic similarity vs. JS Divergence of expert activation on subset data
  2. Profile CPU Expert Performance: Run benchmarks to verify exponential relationship between memory and latency
  3. Cost Benchmarking: Compare "Remoe" split against "MIX" (all local) baseline on Kubernetes

## Open Questions the Paper Calls Out

- **How can a serverless MoE inference system maintain performance and cost-efficiency when faced with unpredictable cold start times and network latency fluctuations?**
  - Basis: The Conclusion states the current approach relies on "idealized assumptions" and future work must address "real-world operational complexities"
  - Why unresolved: Remoe's optimization algorithms assume stable execution environments to pre-allocate resources and guarantee SLOs
  - What evidence would resolve it: A fault-tolerant mechanism or adaptive re-configuration strategy tested under injected network latency and cold start variability

- **Does the Remoe optimization framework remain valid on serverless platforms where vCPU allocation is not strictly proportional to memory?**
  - Basis: Section III-A assumes "1 GB of memory corresponds to 1 vCPU" but this linear relationship doesn't hold for all commercial serverless offerings
  - Why unresolved: The convexity analysis and Lagrangian duality method rely on continuous, monotonic resource relationships
  - What evidence would resolve it: Sensitivity analysis of cost model and SLO satisfaction when deployed on platforms with step-function vCPU allocation

- **How does the system overhead scale when managing expert partitioning and multi-replica inference for models with significantly more experts?**
  - Basis: The Introduction identifies models like DeepSeek-V3 (thousands of experts) as challenging, but Evaluation only tests models with 8-64 experts
  - Why unresolved: Complexity of LPT scheduling and gRPC communication overhead may become bottlenecks as remote expert functions increase
  - What evidence would resolve it: Benchmarking on MoE models with expert counts exceeding 128 per layer, measuring scheduling latency and network overhead

## Limitations

- Real-world deployment overhead from variable serverless invocation latency and cold start times could violate SLOs despite optimization
- The exponential decay relationship between memory and latency is only validated on specific hardware, raising generalizability concerns
- The 6MB payload constraint for remote function invocations isn't addressed with mitigation strategies for long prompts

## Confidence

- **High Confidence**: The heterogeneous architecture and mathematical framework for Lagrangian optimization are well-grounded in prior work and strongly supported
- **Medium Confidence**: SPS algorithm effectiveness depends on semantic-activation correlation holding universally across different models and domains
- **Low Confidence**: Generalizability of exponential latency model and 6MB payload handling are weakly validated, representing potential failure points

## Next Checks

1. **Correlation Validation**: Run controlled experiment on target MoE model and dataset to measure correlation between semantic similarity (SCS) and expert activation divergence (JS Divergence). Plot relationship to verify monotonic trend.

2. **Hardware Performance Profiling**: Profile expert inference latency across range of CPU memory allocations on target hardware. Fit exponential decay curve and verify Theorem 2's convexity condition holds.

3. **Constraint Stress Test**: Generate synthetic long prompts approaching 6MB and measure actual payload sizes when passing tokens to remote functions. Verify system handles oversized requests gracefully without crashing.