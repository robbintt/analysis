---
ver: rpa2
title: 'OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic
  Speech Recognition with Dynamic Vocabulary'
arxiv_id: '2506.09448'
source_url: https://arxiv.org/abs/2506.09448
tags:
- owsm
- biasing
- speech
- words
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving automatic speech
  recognition (ASR) accuracy for rare and unseen words using speech foundation models
  (SFMs) like Open Whisper-Style Speech Models (OWSM). The authors propose OWSM-Biasing,
  which integrates a dynamic vocabulary-based contextual biasing method with OWSM
  v3.1 while freezing the pre-trained parameters.
---

# OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary

## Quick Facts
- arXiv ID: 2506.09448
- Source URL: https://arxiv.org/abs/2506.09448
- Authors: Yui Sudo; Yusuke Fujita; Atsushi Kojima; Tomoya Mizumoto; Lianbo Liu
- Reference count: 0
- Primary result: 11.6-point improvement in biasing word error rate (B-WER) and 0.9-point improvement in overall word error rate (WER)

## Executive Summary
This paper addresses the problem of improving automatic speech recognition (ASR) accuracy for rare and unseen words using speech foundation models (SFMs) like Open Whisper-Style Speech Models (OWSM). The authors propose OWSM-Biasing, which integrates a dynamic vocabulary-based contextual biasing method with OWSM v3.1 while freezing the pre-trained parameters. The approach leverages the knowledge embedded in SFMs to improve recognition of rare and unseen words without requiring large-scale training data. Experimental results on the LibriSpeech 100 test-clean set show that OWSM-Biasing improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9 point improvement in overall word error rate (WER) while reducing the real-time factor by 7.5% compared to the non-biasing baseline.

## Method Summary
OWSM-Biasing freezes the pre-trained OWSM v3.1 encoder and decoder while adding trainable biasing modules: a biasing encoder that creates word-level embeddings from a biasing list, an extended embedding layer that handles both static vocabulary tokens and dynamic vocabulary words via linear projection, and an extended output layer that computes probabilities over both static (K tokens) and dynamic (N biasing words) vocabularies using concatenated softmax. The model is trained with biasing weight μ=0.3 and beam size 3 with joint CTC/attention decoding. The dynamic vocabulary method reduces decoding iterations by representing biasing words as single tokens rather than multiple subword tokens.

## Key Results
- B-WER improves by 11.6 points on LibriSpeech test-clean set
- Overall WER improves by 0.9 points
- Real-time factor reduces by 7.5% compared to non-biasing baseline
- B-WER degradation observed as biasing list size N increases beyond 2000 due to phonetic similarity

## Why This Works (Mechanism)

### Mechanism 1
Freezing SFM parameters while training only lightweight biasing modules enables effective contextual biasing without catastrophic forgetting or requiring large-scale retraining data. The pre-trained OWSM v3.1 encoder and decoder remain frozen. Trainable modules consist of a biasing encoder (Transformer layers), extended embedding layer, and extended output layer. During training, only these ~3-26% of total parameters (depending on model size) are updated.

### Mechanism 2
Dynamic vocabulary expansion reduces decoding iterations by representing biasing words as single tokens rather than multiple subword tokens. Instead of generating ["all", "ig", "at", "or"] for "alligator", the model outputs a single dynamic token <alligator>. The extended output layer computes similarity scores for both static vocabulary (K tokens) and dynamic vocabulary (N biasing words) via concatenated softmax.

### Mechanism 3
Biasing encoder creates word-level embeddings that allow the decoder to attend to contextual words without modifying the core attention mechanism. A Transformer-based biasing encoder processes the biasing list B to produce embeddings V ∈ R^(N×d'). These embeddings are used in the extended embedding layer (via linear projection) and extended output layer (via dot-product similarity scoring).

## Foundational Learning

- **Contextual Biasing (CB) in ASR**: CB methods explicitly guide ASR toward recognizing specific words from an editable list, distinguishing this from prompt-based approaches. Why needed here: Understanding that CB methods explicitly guide ASR toward recognizing specific words from an editable list, distinguishing this from prompt-based approaches. Quick check question: Can you explain why CB is preferable to prompt-based methods for large vocabulary scenarios?

- **Encoder-Decoder Architecture with Attention**: The method extends both embedding and output layers while preserving cross-attention between encoder hidden states and decoder embeddings. Why needed here: The method extends both embedding and output layers while preserving cross-attention between encoder hidden states and decoder embeddings. Quick check question: How does the extended output layer compute probabilities over both static and dynamic vocabularies?

- **Subword Tokenization and Vocabulary Expansion**: The dynamic vocabulary method adds whole-word tokens that bypass subword tokenization, directly affecting decoding length. Why needed here: The dynamic vocabulary method adds whole-word tokens that bypass subword tokenization, directly affecting decoding length. Quick check question: What happens to the vocabulary size during inference when N=500 biasing words are added to a K=5000 static vocabulary?

## Architecture Onboarding

- **Component map**:
  - Frozen: OWSM Encoder (E-Branchformer) → Hidden representation H
  - Frozen: OWSM Decoder (Transformer) → Hidden state u
  - Trainable: Biasing Encoder (Transformer) → Biasing embeddings V
  - Trainable: Extended Embedding Layer → Handles static tokens via Embedding(), dynamic tokens via Linear(Select(V, token))
  - Trainable: Extended Output Layer → Concatenates Linear(u) for static and dot-product similarity for dynamic tokens

- **Critical path**:
  1. Audio → OWSM Encoder → H
  2. Biasing List → Biasing Encoder → V (pre-computed, reused unless list updates)
  3. Previous tokens → Extended Embedding → E'
  4. H + E' → OWSM Decoder → u'
  5. u' → Extended Output Layer → z' over (K+N) vocabulary
  6. Apply biasing weight μ during decoding to mitigate over/under-biasing

- **Design tradeoffs**:
  - Biasing list size N: Larger N improves coverage but increases B-WER due to phonetic confusion (Table 2 shows N=100→N=2000 increases B-WER from 3.9% to 6.9%)
  - Model size: Medium model has lower trainable parameter ratio (3.1%) vs base (26.3%), potentially more stable but requires more compute
  - Freezing vs fine-tuning: Freezing preserves generalization; fine-tuning all parameters risks catastrophic forgetting (A3 vs A4 in Table 2)

- **Failure signatures**:
  - B-WER plateaus or degrades as N increases: Phonetic similarity causing confusion among biasing words
  - RTF increases unexpectedly: Biasing encoder being re-computed per utterance instead of cached
  - Overall WER degrades: Over-biasing toward contextual words; adjust biasing weight μ

- **First 3 experiments**:
  1. **Baseline validation**: Run OWSM v3.1 base without biasing on your target domain to establish WER and identify rare word error patterns.
  2. **Small biasing list (N=100)**: Create a biasing list from domain-specific terms and measure B-WER improvement; this validates the integration is working correctly.
  3. **Scaling test (N=500, 1000, 2000)**: Gradually increase biasing list size to identify the saturation point where B-WER improvement diminishes for your specific vocabulary distribution.

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance degradation caused by phonetically similar words prevent the scaling of this method to biasing lists significantly larger than 2,000 words? Basis in paper: [inferred] The authors note that B-WER degrades as the biasing list size $N$ increases due to the inclusion of phonetically similar words, but experiments only cover up to $N=2000$. Why unresolved: It is unclear if the linear projection approach for dynamic tokens remains effective or becomes computationally burdensome with massive real-world vocabularies (e.g., enterprise contact lists). Evidence: Evaluation of B-WER and RTF using biasing lists of 5,000 to 20,000 words.

### Open Question 2
Can the proposed architecture generalize to other pre-trained Speech Foundation Models (SFMs), such as OpenAI's Whisper or Google USM, without fine-tuning the core encoder? Basis in paper: [explicit] The paper claims the design "can be applied to various end-to-end ASR architectures... such as [1, 5, 7]" but validates it exclusively on OWSM v3.1. Why unresolved: While the architecture is compatible, the efficacy of training lightweight extensions while freezing parameters may vary depending on the specific knowledge embedding and decoder structure of other SFMs. Evidence: Experimental results applying the identical frozen-biasing protocol to the official Whisper large-v3 model.

### Open Question 3
How does the fixed biasing weight $\mu$ affect performance in acoustically challenging or domain-shifted environments outside of LibriSpeech? Basis in paper: [inferred] The biasing weight $\mu$ is manually set to 0.3 for the experiments, and the evaluation is limited to the LibriSpeech corpus. Why unresolved: A fixed weight may lead to over-biasing in noisy conditions (test-other) or under-biasing in domain-specific contexts (e.g., medical terms), requiring an adaptive mechanism. Evidence: Analysis of WER and B-WER across varying SNR levels or out-of-domain datasets (e.g., AMI meetings) using the current fixed weight.

## Limitations
- Experimental validation is limited to LibriSpeech, a relatively clean read speech corpus, with unverified effectiveness on far-field, noisy, or accented speech
- Vocabulary coverage gap analysis is missing for when biasing words fall outside OWSM vocabulary or when phonetic confusion causes degradation
- Computational overhead uncertainty exists as absolute overhead of biasing modules versus benefits is not independently quantified

## Confidence
**High Confidence**: The core mechanism of freezing OWSM parameters while training only biasing modules is well-specified and the WER/B-WER improvements on LibriSpeech are clearly demonstrated with proper experimental controls.

**Medium Confidence**: The claim about reduced decoding iterations and improved RTF relies on a single comparison without ablation studies. The dynamic vocabulary consolidation mechanism is theoretically sound but lacks extensive empirical validation.

**Low Confidence**: The generalizability claims to other domains, languages, and SFM architectures are not supported by experimental evidence. The paper assumes the biasing encoder can effectively capture phonetic information from subword sequences without validating this assumption through qualitative analysis.

## Next Checks
**Validation Check 1**: Implement the same OWSM-Biasing pipeline on a noisy speech corpus (e.g., CHiME-6 or AMI) to verify that B-WER improvements persist in realistic acoustic conditions. Measure whether the freezing strategy still prevents catastrophic forgetting when domain characteristics differ significantly from LibriSpeech.

**Validation Check 2**: Conduct an ablation study comparing OWSM-Biasing against prompt-based contextual biasing approaches on the same dataset. Specifically measure performance when the biasing vocabulary exceeds 2000 words to characterize the saturation point where phonetic confusion dominates.

**Validation Check 3**: Profile the absolute computational overhead of the biasing modules (biasing encoder, extended embedding, extended output) versus the WER improvement across different N values. Determine the breakeven point where the additional computation no longer justifies the marginal accuracy gains for production deployment.