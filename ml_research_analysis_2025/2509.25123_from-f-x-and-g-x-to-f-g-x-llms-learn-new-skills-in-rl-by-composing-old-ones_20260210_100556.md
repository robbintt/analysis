---
ver: rpa2
title: 'From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing
  Old Ones'
arxiv_id: '2509.25123'
source_url: https://arxiv.org/abs/2509.25123
tags:
- level
- func
- string
- skills
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reinforcement learning (RL) can
  teach large language models (LLMs) genuinely new skills by composing existing ones.
  The authors construct a synthetic framework using string transformation prediction
  tasks, where atomic skills are defined as single transformations and compositional
  skills as their nested combinations.
---

# From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones

## Quick Facts
- arXiv ID: 2509.25123
- Source URL: https://arxiv.org/abs/2509.25123
- Authors: Lifan Yuan; Weize Chen; Yuchen Zhang; Ganqu Cui; Hanbin Wang; Ziming You; Ning Ding; Zhiyuan Liu; Maosong Sun; Hao Peng
- Reference count: 40
- Primary result: RL on compositional training data enables models to learn new compositional skills that generalize to unseen compositions and higher difficulty levels

## Executive Summary
This paper investigates whether reinforcement learning (RL) can teach large language models (LLMs) genuinely new skills by composing existing ones. Using a synthetic string transformation framework, the authors demonstrate that RL with compositional training data enables models to learn compositional skills that generalize to unseen compositions and higher difficulty levels. Specifically, RL improves accuracy on Level-3 tasks from near-zero to 30% and Level-4 to 15%, while RL on atomic tasks alone fails. The compositional skills learned through RL transfer to a different task (Countdown) when the model has the necessary atomic skills. Behavioral analysis reveals RL fundamentally changes reasoning behavior, shifting failure modes from compositional misunderstandings to atomic prediction errors.

## Method Summary
The paper employs a two-stage training protocol using a synthetic string transformation task. Stage 1 uses RFT to teach atomic skills (single transformations) on Level-1 problems. Stage 2 applies RL (DAPO/GRPO) to teach compositional skills on Level-2 problems (nested compositions). The model learns to predict output strings given input strings and function compositions, with function definitions hidden during both training stages. Evaluation tests generalization to held-out functions and higher difficulty levels (3-8). The authors compare RL against RFT baselines and test transfer to a different task (Countdown) requiring the same atomic skills.

## Key Results
- RL on Level-2 compositional problems substantially improves generalization to unseen Level-3 tasks (30% accuracy vs near-zero) and Level-4 (15% vs near-zero)
- RFT on compositional data only achieves 2.6% accuracy on Level-3, showing RL is necessary for compositional skill acquisition
- RL on atomic-only data (Level-1) shows near-zero accuracy on Level-3+ compositions
- Compositional RL skills transfer to Countdown task when atomic prerequisites are met (35% accuracy), but fail without them (0% accuracy)
- RL fundamentally changes failure modes from compositional misunderstandings to atomic prediction errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RL training on simple compositional problems induces a *generalizable compositional skill* that extends to unseen function combinations at higher difficulty levels.
- **Mechanism:** RL optimizes for outcome correctness, which forces the model to discover internal representations that support systematic function composition. Unlike NTP, which only learns local pattern statistics, RL receives a sparse reward signal that can only be maximized by correctly chaining operations—creating pressure for the model to learn a compositional "meta-skill" rather than memorizing specific compositions.
- **Core assumption:** The model has already internalized all atomic skills through prior training (Stage 1 RFT).
- **Evidence anchors:**
  - [abstract] "RL on Level-2 problems... substantially improves generalization on more difficult problems: performance on unseen Level-3 tasks improves from near-zero to 30%, and Level-4 to 15%."
  - [Section 4.1] "RL Level 2 and RL Level 1+2 models demonstrate strong performance to generalize to problems with nesting depths exceeding their training data."
  - [corpus] Weak direct corpus support; PolySkill (arXiv 2510.15863) addresses skill abstraction but not RL-specific composition.
- **Break condition:** If the base model lacks atomic skill proficiency (>90% on Level-1), RL cannot bootstrap composition.

### Mechanism 2
- **Claim:** Both RL *and* compositional training data are *jointly necessary; neither alone produces generalizable compositional skill.
- **Mechanism:** RFT on compositional data only teaches the model to imitate correct traces without learning *why* compositions work, while RL on atomic data never creates compositional pressure. The combination provides both: compositional data defines *what* to compose, and RL's reward signal teaches *how* to systematically chain operations.
- **Core assumption:** The compositional training examples are sufficiently diverse to prevent rote memorization.
- **Evidence anchors:**
  - [Section 4.2] "RFT model's accuracy is significantly worse than RL across all compositional levels... on Level 3 it never surpasses 2.6%."
  - [Section 4.1] RL Level 1 (atomic-only) shows "near-zero" accuracy on Level 3+.
  - [corpus] DeCo (arXiv 2505.00527) supports task decomposition for generalization, but in IL, not RL.
- **Break condition:** Training data distribution collapse (all rollouts correct/incorrect) prevents learning; authors filter these.

### Mechanism 3
- **Claim:** Compositional skill learned on one task transfers to a different task when atomic prerequisites are met.
- **Mechanism:** RL induces a task-agnostic "composition operator" that can bind any available atomic skill representations. The meta-skill is learning *how* to chain, not *what* to chain.
- **Core assumption:** The target task's atomic skills are already acquired and share a compatible representational format.
- **Evidence anchors:**
  - [Section 4.3] "Compositional RL on string task boosts accuracy on unseen Level-3 Countdown problems to 35%."
  - [Figure 4] String-Base + RL L1+2 (no Countdown atomic skills) fails completely; Multi-Base + RL L1+2 succeeds.
  - [corpus] RELIC (arXiv 2506.05205) evaluates compositional instruction following but doesn't study RL-induced transfer.
- **Break condition:** Target task lacks required atomic skills → transfer collapses (String-Base condition shows 0% accuracy).

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR):**
  - **Why needed here:** The paper uses outcome-based binary rewards (correct/incorrect output) without process supervision. Understanding how sparse rewards drive skill acquisition is essential.
  - **Quick check question:** Can you explain why pass@k gaps shrinking with k does *not* prove RL only reranks?

- **Compositionality:**
  - **Why needed here:** The core hypothesis hinges on compositional generalization—applying learned composition rules to unseen function combinations.
  - **Quick check question:** What's the difference between learning f(g(x)) as a pattern vs. learning a compositional meta-skill?

- **Two-Stage Training Protocol:**
  - **Why needed here:** Separates atomic skill acquisition (Stage 1 RFT) from compositional skill learning (Stage 2 RL). This mimics real post-training where base models have pre-existing capabilities.
  - **Quick check question:** Why must function definitions be hidden during Stage 2 training?

## Architecture Onboarding

- **Component map:**
  1. Synthetic task framework (25 string transformation functions with obfuscated identifiers, difficulty by nesting depth)
  2. Stage 1 RFT (atomic skill acquisition from correct trajectories, function definitions hidden during training)
  3. Stage 2 RL (GRPO with binary correctness rewards on Level-2 compositions, definitions never shown)
  4. Evaluation (held-out functions, higher difficulty levels 3-8, cross-task transfer to Countdown)

- **Critical path:**
  1. Verify base model achieves >90% on Level-1 atomic tasks (else compositional RL cannot succeed)
  2. Train Stage 2 RL on Level-2 compositional data only
  3. Evaluate on held-out Level-3+ to confirm generalization
  4. If transfer is goal, ensure target task atomic skills are pre-acquired

- **Design tradeoffs:**
  - **Synthetic vs. natural tasks:** Synthetic enables causal attribution but may not reflect real-world skill boundaries
  - **GRPO vs. other RL algorithms:** Authors use GRPO for stability; DAPO variant employed
  - **Level-2 only vs. mixed Level-1+2 training:** Both work; Level-2 alone is more efficient for compositional skill isolation

- **Failure signatures:**
  - RL Level 1 model: High Level-1 accuracy (>90%) but <25% on Level-2, near-zero on Level-3+
  - RFT model: Stagnates after first iteration; fails to generalize even to same-difficulty held-out problems
  - Missing atomic skills: Transfer task accuracy remains near-zero regardless of compositional training

- **First 3 experiments:**
  1. **Replicate the RL vs. RFT comparison** on Level-2 compositional data; confirm RFT plateaus while RL continues improving on held-out Level-3
  2. **Ablate compositional data** by training RL on Level-1 only; verify Level-3+ remains at near-zero
  3. **Test transfer failure mode** by withholding Countdown atomic skills; confirm compositional RL provides no benefit without prerequisites

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** Can RL enable LLMs to acquire both atomic and compositional skills simultaneously without supervised scaffolding?
  - **Basis in paper:** [explicit] The conclusion states: "Future work may investigate the open question of whether RL can be scaled to acquire both atomic and compositional skills simultaneously without supervised scaffolding."
  - **Why unresolved:** The current study relies on a two-stage protocol where atomic skills are strictly learned via supervised finetuning (RFT) before RL is applied for composition.
  - **What evidence would resolve it:** Successful training runs where a base model achieves high accuracy on compositional tasks using RL alone, without any prior supervised demonstration of the atomic skills.

- **Open Question 2**
  - **Question:** Do the compositional skill acquisition mechanisms identified in synthetic tasks transfer to complex, natural reasoning domains?
  - **Basis in paper:** [explicit] The Limitations section notes that synthetic tasks "may not fully capture the complexity and nuance of real-world reasoning," identifying real-world application as a "valuable open challenge."
  - **Why unresolved:** While the paper demonstrates transfer from string tasks to Countdown, it relies on synthetic environments to ensure decontamination and controlled difficulty, which abstracts away the ambiguity of natural language.
  - **What evidence would resolve it:** Replicating the specific finding—that RL on simple compositions generalizes to harder ones better than RFT—in domains like mathematical theorem proving or code generation.

- **Open Question 3**
  - **Question:** Does the RL mechanism specifically induce a structural representation of composition that RFT fails to learn?
  - **Basis in paper:** [inferred] The paper observes that RL changes failure modes from "compositional misunderstandings" to "atomic errors," suggesting a fundamental behavioral shift, but does not analyze the internal representations.
  - **Why unresolved:** It is unclear if the improvement stems from better structural reasoning or merely better credit assignment over long contexts.
  - **What evidence would resolve it:** Mechanistic interpretability analysis (e.g., probing classifiers) comparing how RL and RFT models represent nested function structures internally.

## Limitations

- Synthetic task framework may not reflect real-world reasoning complexity and ambiguity
- Two-stage protocol assumes clean separation between atomic and compositional skills, which may not hold for real models
- GRPO implementation details (particularly DAPO variant) could affect reproducibility
- External validity concerns due to handcrafted 25 functions that may not represent natural skill boundaries

## Confidence

- **High confidence**: RL with compositional data enables generalization to higher difficulty levels (Level 3 accuracy improvement from near-zero to 30% is robust across experiments)
- **Medium confidence**: Compositional skills transfer to different tasks when atomic prerequisites are met (Countdown transfer shows 35% accuracy, but this depends on specific task setup)
- **Medium confidence**: Both RL and compositional data are jointly necessary (ablations show neither alone succeeds, but synthetic nature limits generalizability)

## Next Checks

1. **Cross-task generalization**: Test whether compositional RL benefits transfer to natural language tasks (e.g., nested reasoning chains) rather than just synthetic Countdown
2. **Scaling analysis**: Verify if compositional skill emergence scales with model size beyond Llama-3.1-8B, particularly for larger models that may already possess implicit compositional abilities
3. **Real-world application**: Apply the two-stage protocol to a practical task (e.g., code generation with nested API calls) to assess practical utility beyond controlled experiments