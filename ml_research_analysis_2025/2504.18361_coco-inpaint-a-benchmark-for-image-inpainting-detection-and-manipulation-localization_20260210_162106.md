---
ver: rpa2
title: 'COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation
  Localization'
arxiv_id: '2504.18361'
source_url: https://arxiv.org/abs/2504.18361
tags:
- image
- inpainting
- mask
- imdl
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COCOInpaint, a large-scale benchmark for
  image inpainting detection and manipulation localization. Unlike existing datasets
  focused on splicing and copy-move forgeries, COCOInpaint addresses the growing threat
  of inpainting-based image manipulations.
---

# COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization

## Quick Facts
- arXiv ID: 2504.18361
- Source URL: https://arxiv.org/abs/2504.18361
- Reference count: 40
- A large-scale benchmark with 258,266 inpainted images and 117,266 authentic images for advancing inpainting forensics research

## Executive Summary
COCOInpaint introduces a comprehensive benchmark for image inpainting detection and manipulation localization, addressing the growing threat of inpainting-based image manipulations. Unlike existing datasets focused on splicing and copy-move forgeries, this benchmark specifically targets the detection of inpainting artifacts across diverse mask types and six state-of-the-art inpainting models. The dataset enables systematic evaluation of model generalization across different inpainting techniques, mask characteristics, and manipulation ratios.

The benchmark demonstrates that Vision Transformer-based models significantly outperform CNN-based models for inpainting detection tasks. Multi-level evaluation reveals both the capabilities and limitations of current detection approaches, particularly highlighting challenges in cross-model generalization where models trained on one inpainting technique struggle to detect artifacts from others. The controlled generation process and large scale make COCOInpaint a valuable resource for advancing research in multimedia integrity and digital forensics.

## Method Summary
The COCOInpaint benchmark was constructed by generating inpainted images using six state-of-the-art inpainting models applied to authentic images from the COCO dataset. The generation process systematically varied mask types (free-form, random, square), mask ratios, and text guidance conditions to create a diverse test bed. The benchmark includes 258,266 inpainted images and 117,266 authentic images, enabling comprehensive evaluation of detection and localization performance. Evaluation metrics include AUC, accuracy, precision, recall, and F1-score, with particular emphasis on cross-model generalization where models are trained on one inpainting technique and tested on others.

## Key Results
- Vision Transformer-based models consistently outperform CNN-based models for inpainting detection across all evaluation metrics
- Cross-model generalization remains a significant challenge, with detection performance dropping substantially when models are trained on one inpainting technique and tested on another
- Random masks demonstrate superior robustness compared to other mask types, suggesting they provide more generalizable training signals for detection models

## Why This Works (Mechanism)
COCOInpaint works by providing a controlled, systematic evaluation framework that isolates the forensic traces left by different inpainting models. The benchmark's strength lies in its comprehensive coverage of inpainting techniques and mask characteristics, enabling researchers to identify which model features and mask properties most strongly influence detection performance. By including both within-model and cross-model evaluations, COCOInpaint reveals fundamental limitations in current detection approaches while providing the data necessary to develop more robust solutions.

## Foundational Learning
- **Inpainting forensics fundamentals**: Understanding how different inpainting models leave unique artifact patterns that can be detected - essential for developing effective forensic tools
- **Mask type influence**: Free-form masks create irregular boundaries while random masks provide better generalization training signals - critical for understanding detection robustness
- **Cross-model generalization challenges**: Models trained on one inpainting technique often fail on others due to different artifact characteristics - key limitation in current detection approaches
- **Vision Transformer advantages**: Transformer architectures capture long-range dependencies better than CNNs for detecting inpainting artifacts - explains superior performance
- **Multi-level evaluation framework**: Systematic testing across different inpainting models, mask types, and ratios enables comprehensive benchmarking - necessary for thorough assessment
- **Artifact localization techniques**: Identifying manipulation boundaries requires different approaches than binary classification - important for practical forensic applications

## Architecture Onboarding

**Component Map**: Image Dataset → Inpainting Models (6 variants) → Mask Generators (free-form, random, square) → Generated Images → Detection Models (CNN/ViT) → Evaluation Metrics

**Critical Path**: Authentic images → Inpainting model application → Mask application → Artifact generation → Detection model training → Performance evaluation → Generalization testing

**Design Tradeoffs**: Large-scale synthetic dataset provides controlled evaluation but may not capture all real-world manipulation scenarios; comprehensive model coverage versus computational cost of generation; balance between mask diversity and realistic manipulation patterns

**Failure Signatures**: Significant performance drops in cross-model testing indicate overfitting to specific artifact patterns; poor random mask performance suggests inadequate generalization; localization failures reveal limitations in boundary detection capabilities

**First Experiments**:
1. Train ViT model on images inpainted with Model A, test on same model images to establish baseline performance
2. Train CNN model on random mask images, evaluate on free-form mask images to assess mask generalization
3. Compare detection performance across different mask ratios (10%, 30%, 50%) to identify optimal training conditions

## Open Questions the Paper Calls Out
The paper acknowledges that while COCOInpaint provides a comprehensive benchmark for inpainting detection, it does not address the full spectrum of real-world forensic challenges. The synthetic nature of the dataset, while enabling controlled evaluation, may not fully capture the complexity of authentic manipulation scenarios that include post-processing, compression, and adversarial conditions. Additionally, the benchmark focuses specifically on inpainting detection and does not extend to other manipulation types that forensics systems must handle in practice.

## Limitations
- Synthetic generation process may not capture all realistic manipulation scenarios including post-processing and compression artifacts
- Benchmark focuses exclusively on inpainting detection, excluding other manipulation types like splicing and copy-move forgeries
- Does not address temporal evolution of inpainting models or techniques that could evade current detection approaches
- Limited exploration of underlying causes for cross-model generalization failures and potential mitigation strategies

## Confidence
- **Cross-model generalization claims**: Medium - experimental results support challenges but underlying causes need further investigation
- **Vision Transformer superiority**: High - consistent experimental results across multiple evaluation metrics and conditions
- **Random mask robustness**: Medium - demonstrated effectiveness but limited exploration of why this occurs
- **Benchmark representativeness**: Medium - comprehensive within-distribution evaluation but ecological validity concerns remain

## Next Checks
1. Test model performance on compressed versions of COCOInpaint images to assess robustness to common real-world conditions
2. Evaluate detection models on images containing multiple manipulation types (inpainting combined with splicing or copy-move) to assess practical applicability
3. Conduct ablation studies on mask type combinations to identify which mask characteristics most strongly influence detection performance across different inpainting models