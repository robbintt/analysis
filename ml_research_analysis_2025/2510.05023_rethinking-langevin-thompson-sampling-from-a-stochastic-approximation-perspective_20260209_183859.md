---
ver: rpa2
title: Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective
arxiv_id: '2510.05023'
source_url: https://arxiv.org/abs/2510.05023
tags:
- posterior
- algorithm
- ts-sa
- reward
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thompson Sampling with Stochastic Approximation
  (TS-SA), a novel algorithm for multi-armed bandit problems that addresses key limitations
  of existing approximate Thompson sampling methods. While current approaches like
  TS-SGLD use Langevin dynamics to sample from a posterior that changes every round,
  requiring per-round hyperparameter tuning and complicating theoretical analysis,
  TS-SA maintains a stationary target posterior by averaging Langevin proposals over
  time.
---

# Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective

## Quick Facts
- **arXiv ID**: 2510.05023
- **Source URL**: https://arxiv.org/abs/2510.05023
- **Reference count**: 12
- **Primary result**: TS-SA achieves near-optimal O(√KT) regret with simpler hyperparameter tuning and theoretical analysis compared to TS-SGLD

## Executive Summary
This paper introduces Thompson Sampling with Stochastic Approximation (TS-SA), a novel algorithm for multi-armed bandit problems that addresses key limitations of existing approximate Thompson sampling methods. While current approaches like TS-SGLD use Langevin dynamics to sample from a posterior that changes every round, requiring per-round hyperparameter tuning and complicating theoretical analysis, TS-SA maintains a stationary target posterior by averaging Langevin proposals over time. The algorithm constructs gradient estimates using only recent rewards, performs Langevin Monte Carlo updates, and applies stochastic approximation to average iterates, enabling a fixed step-size and simplified convergence analysis. Theoretical guarantees establish near-optimal regret bounds of O(√KT), matching existing methods while offering a more intuitive analysis framework. Empirical results demonstrate that even simplified implementations, such as single-step Langevin updates with warm-up, substantially outperform existing MAB algorithms.

## Method Summary
TS-SA addresses the challenge of approximate Thompson sampling in multi-armed bandits by maintaining a stationary target posterior through stochastic approximation. The algorithm uses a sliding window of recent rewards to estimate gradients, performs Langevin Monte Carlo updates with fixed step-size, and applies SA averaging to reduce variance. Key design choices include: (1) using only the most recent B rewards for gradient estimation to eliminate temporal bias, (2) applying SA averaging to smooth the trajectory and reduce decision variance, and (3) maintaining a stationary target posterior throughout the algorithm rather than tracking a dynamic posterior. The method requires warm-up exploration (pulling each arm Ω times initially) and uses a constant step-size h that can be fixed rather than tuned per round. The theoretical framework establishes O(√KT) regret bounds through a unified analysis that treats the entire algorithm as a single Markov chain rather than separate chains per round.

## Key Results
- TS-SA achieves O(√KT) regret bounds, matching theoretical guarantees of TS-SGLD while offering simpler analysis
- Fixed step-size h and stationary target posterior eliminate per-round hyperparameter tuning required by TS-SGLD
- Even simplified implementations (single-step Langevin updates) outperform existing MAB algorithms in empirical evaluation
- The algorithm demonstrates robustness to hyperparameter choices compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1: Stationary Target Approximation
- **Claim**: TS-SA simplifies theoretical convergence and enables fixed hyperparameters by simulating a single stationary sampling process rather than tracking a dynamic posterior that changes every round.
- **Mechanism**: The algorithm treats the entire sequence of bandit rounds as a single Stochastic Gradient Langevin Dynamics (SGLD) chain. Instead of re-initializing a chain to sample from $p(\theta|X_1, \dots, X_n)$ at round $n$, it targets a stationary distribution $\mu_{SA} \propto \exp(T F_T(\theta))$ using a constant step-size $h$. Stochastic Approximation (SA) averages the iterates to maintain this trajectory.
- **Core assumption**: The total time horizon $T$ is effectively fixed or bounded, allowing the definition of a stationary potential function $F_T$.
- **Evidence anchors**:
  - [Abstract]: "TS-SA maintains a stationary target posterior by averaging Langevin proposals over time... enabling a fixed step-size."
  - [Section 4.2]: "This can be interpreted as approximating a stationary target posterior throughout the entire algorithm... the iterates across rounds form a single Markov chain."
  - [Corpus]: Evidence is weak; while "Langevin Soft Actor-Critic" uses Langevin for RL, it does not validate the specific stationary target mechanism for MABs described here.
- **Break condition**: If the reward distribution is non-stationary (change-point detection required), this fixed-target assumption fails, potentially locking the algorithm onto an obsolete posterior.

### Mechanism 2: Temporal Bias Reduction via Windowed Gradients
- **Claim**: Restricting gradient estimates to the most recent $B$ rewards reduces the "temporal bias" inherent in uniform sampling from a growing history.
- **Mechanism**: In prior methods (TS-SGLD), uniform sampling from history causes early observations to be over-represented in the gradient pool. TS-SA uses a fixed window of recent rewards, ensuring each observation contributes equally to the gradient estimate during its active window, preventing early outliers from permanently skewing the trajectory.
- **Core assumption**: The statistics of recent data are sufficiently representative of the optimal policy to guide exploration.
- **Evidence anchors**:
  - [Section 4.3]: "In TS-SA, we use the most recent reward(s) to estimate the gradient... [which] eliminates temporal bias and stabilizes learning."
  - [Table 1]: Highlights "Gradient estimate" difference: Uniform minibatch vs. Most recent rewards (constant memory).
  - [Corpus]: Missing specific validation; related papers focus on Langevin stability generally, not this specific windowing heuristic.
- **Break condition**: If the batch size $B$ is too small, gradient variance explodes; if too large, memory benefits vanish and bias re-enters.

### Mechanism 3: Variance Reduction via Iterate Averaging
- **Claim**: The Stochastic Approximation (SA) step reduces decision variance and stabilizes early-stage robustness compared to using the last sample of a Markov chain.
- **Mechanism**: The update rule $\theta^{(j+1)} = (1-\gamma_n)\theta^{(j)} + \gamma_n \omega^{(j)}$ filters the noisy Langevin proposals ($\omega$). This acts as a low-pass filter, smoothing out the stochastic noise of the gradient steps while tracking the drift of the posterior mean.
- **Core assumption**: The mixing rate of the Langevin dynamics is sufficient for the averaged iterate to remain within the high-probability region of the target.
- **Evidence anchors**:
  - [Abstract]: "Applies an SA step to average noisy proposals over time... [improving] posterior estimates through temporal averaging."
  - [Section 5.1]: "Time-averaging reduces decision variance and allows a constant step-size h without per-round retuning."
  - [Corpus]: "A Broader View of Thompson Sampling" discusses general TS mechanisms but does not specifically confirm this averaging technique.
- **Break condition**: If the SA step-size $\gamma_n$ decays too slowly or is too large, the averaging may fail to track the shifting mean in time for the decision.

## Foundational Learning

- **Concept: Langevin Monte Carlo (LMC)**
  - **Why needed here**: This is the fundamental inference engine. Understanding that LMC transforms optimization (gradient descent) into sampling (gradient descent + Gaussian noise) is required to grasp how TS-SA explores.
  - **Quick check question**: How does the injected noise term $\sqrt{2h} \mathcal{N}(0,I)$ in LMC prevent the algorithm from collapsing to the Maximum A Posteriori (MAP) estimate?

- **Concept: Stochastic Approximation (SA)**
  - **Why needed here**: The "SA" in TS-SA. It is the recursive averaging technique used to stabilize the sampling process.
  - **Quick check question**: In the update $\theta_{t+1} = (1-\gamma_t)\theta_t + \gamma_t (\text{data})$, what is the effect of the sequence $\gamma_t \to 0$ versus a constant $\gamma_t$?

- **Concept: Regret in Multi-Armed Bandits**
  - **Why needed here**: The paper claims "near-optimal regret." You must understand that regret measures the opportunity cost of not playing the optimal arm to contextualize the theoretical guarantees.
  - **Quick check question**: Why is an $O(\sqrt{KT})$ regret bound considered "near-optimal" for stochastic bandits?

## Architecture Onboarding

- **Component map**:
  1. **Buffer (Size $B$)**: Stores the most recent rewards (Sliding Window)
  2. **Sampler**: Generates decision parameters $\theta_{a,t}$ by adding scaled noise to the current mean estimate
  3. **Actor**: Selects arm $A_t = \text{argmax}_a \langle \phi_a, \theta_{a,t} \rangle$
  4. **LMC Updater**: Computes gradient $\nabla \log p$ using the Buffer
  5. **SA Integrator**: Averages the LMC proposal into the running state $\theta_a(n_a)$

- **Critical path**: The **SA Integrator** (Algorithm 2, Line 9) is the most sensitive addition. If implemented as a simple assignment instead of a weighted average, the algorithm reverts to high-variance TS-SGLD behavior.

- **Design tradeoffs**:
  - **Fixed Step-size ($h$)**: Simplifies tuning and theory but assumes the stationary approximation holds
  - **Warm-up ($\Omega$)**: Essential for stability (ablation shows linear regret without it), but costs initial exploration efficiency

- **Failure signatures**:
  - **Linear Regret**: Occurs if Batch size $B < 20$ or Warm-up $\Omega < 20$ in high-variance tasks (Section 6, Failure Modes)
  - **Stagnation**: Occurs if temperature $\tau$ is too large, smoothing the posterior into a uniform distribution

- **First 3 experiments**:
  1. **Sanity Check (MAB)**: Implement TS-SA on a Gaussian MAB with $K=10, T=1000$. Plot cumulative regret against TS-SGLD to verify the "lower variance" claim
  2. **Ablation on Window Size ($B$)**: Run with $B=1$ vs $B=20$ to observe the transition from linear to sublinear regret described in Section 6
  3. **Hyperparameter Sensitivity**: Test the SA step-size $\gamma$. Validate if the theoretically suggested $\gamma = 1/T$ is necessary or if a fixed small $\gamma$ works empirically (Remark 5.6)

## Open Questions the Paper Calls Out
1. **Can the TS-SA framework be effectively extended to contextual bandits or reinforcement learning settings?**
   - Basis in paper: [explicit] Section 7 states, "Future work may extend TS-SA to contextual bandits or reinforcement learning settings..."
   - Why unresolved: The current theoretical framework relies on a stationary target posterior derived from fixed feature vectors; changing contexts in these domains introduce non-stationarity that the current proof does not address
   - What evidence would resolve it: Theoretical regret bounds and empirical validation in contextual bandit or RL benchmarks demonstrating maintained performance

2. **Can the theoretical analysis be adapted for adaptive SA step sizes (e.g., $\gamma_{na} = 1/n_a^\alpha$) rather than the fixed $1/T$ used for tractability?**
   - Basis in paper: [inferred] Remark 5.6 notes that proving results for adaptive $\gamma$ is challenging due to state-dependent noise, despite practical benefits
   - Why unresolved: The paper relies on a time-homogeneous diffusion term for convergence analysis; adaptive steps introduce complex, time-inhomogeneous noise
   - What evidence would resolve it: A convergence proof utilizing techniques such as Poisson equations to handle the state-dependent noise of adaptive schedules

3. **Does defining the stationary target posterior using the arm-specific pull count $T_a(T)$ instead of the global horizon $T$ yield tighter bounds?**
   - Basis in paper: [inferred] Remark 4.1 explains the authors conservatively use global $T$ to avoid "challenging dependence issues" caused by the randomness of $T_a(T)$
   - Why unresolved: $T_a(T)$ is a random variable dependent on the algorithm's trajectory, which complicates the stochastic differential equation analysis
   - What evidence would resolve it: A theoretical framework that successfully decouples the trajectory dependence to analyze the intuitive target $T_a(T)$

## Limitations
- The stationary target approximation assumes bounded horizon $T$ without rigorous validation for dynamic or infinite-horizon settings
- Temporal bias reduction mechanism lacks direct experimental validation beyond ablation studies showing performance degradation
- Theoretical analysis relies on strong log-concavity assumptions (Assumptions 5.1-5.3) that may not hold in all practical scenarios

## Confidence

- Stationary target approximation: Medium-Low (assumes bounded $T$ without rigorous validation)
- Temporal bias reduction: Medium (empirical ablation supports but mechanism not independently verified)  
- Variance reduction via averaging: Medium-High (supported by empirical results and SA theory)

## Next Checks
1. **Stationary Target Validation**: Implement TS-SA with increasing $T$ values and measure whether the fixed-target approximation breaks down. Compare regret scaling to theoretical predictions.
2. **Temporal Bias Quantification**: In TS-SGLD, measure the effective sample size of early observations in gradient estimates. Compare to TS-SA's windowed approach to quantify bias reduction.
3. **SA Step-size Sensitivity**: Test the claim that TS-SA enables fixed step-sizes by running experiments with constant $\gamma$ values versus the theoretically suggested decaying schedule. Measure regret and mixing behavior.