---
ver: rpa2
title: 'Large Language Models and Forensic Linguistics: Navigating Opportunities and
  Threats in the Age of Generative AI'
arxiv_id: '2512.06922'
source_url: https://arxiv.org/abs/2512.06922
tags:
- forensic
- detection
- llms
- authorship
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This article critically examines how large language models (LLMs)\
  \ both challenge and extend forensic linguistics, focusing on authorship attribution\
  \ and text detection. LLMs can enhance forensic analysis through scalable, embedding-based\
  \ attribution and multilingual, multitask detection, yet they threaten traditional\
  \ idiolect-based methods by enabling style mimicry, producing synthetic text, and\
  \ introducing bias in detection tools\u2014particularly against non-native English\
  \ writers."
---

# Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI

## Quick Facts
- **arXiv ID**: 2512.06922
- **Source URL**: https://arxiv.org/abs/2512.06922
- **Reference count**: 10
- **Primary result**: LLM-based stylometry and detection can improve forensic analysis but introduce new biases and legal challenges, requiring hybrid human-AI workflows and rigorous validation.

## Executive Summary
This article critically examines the dual role of large language models (LLMs) in forensic linguistics, highlighting both opportunities and threats to traditional authorship attribution and text detection methods. LLMs offer scalable, multilingual analysis and advanced detection capabilities, but also enable style mimicry, synthetic text generation, and introduce biases—especially against non-native English writers. AI-text detectors are vulnerable to adversarial attacks and exhibit high false-positive rates, undermining their reliability in legal contexts. The article advocates for hybrid human-AI workflows, explainable detection paradigms, and rigorous validation across diverse populations to maintain scientific credibility and legal admissibility in the era of generative AI.

## Method Summary
The article synthesizes findings from recent empirical studies on LLM-based stylometry, authorship attribution, and AI-text detection, focusing on Burrows' Delta distances, perplexity-based classifiers, and adversarial attacks such as homoglyph substitution. It reviews LLM "literary imitations" and bias in AI detectors against non-native English writers, recommending hybrid workflows and explainable AI (e.g., SHAP features) for forensic applications. The proposed reproduction plan involves stylometric verification using the `Stylo` R package, bias testing with GPTZero, and adversarial attack simulation.

## Key Results
- LLM-based stylometry can achieve ~85% attribution accuracy using embedding-based Bayesian frameworks.
- AI-text detectors exhibit high false-positive rates against non-native English writers.
- Adversarial attacks (e.g., homoglyph substitution) degrade detector performance and undermine legal admissibility under Daubert standards.

## Why This Works (Mechanism)
LLMs transform forensic linguistics by enabling scalable, embedding-based authorship attribution and multilingual detection, but also by introducing new attack vectors (e.g., style mimicry, synthetic text) and biases (e.g., against non-native speakers). The mechanism relies on comparing stylistic fingerprints (via Burrows' Delta, perplexity) and exploiting the distributional properties of LLM-generated text, which differ from human-authored corpora.

## Foundational Learning
- **Burrows' Delta**: A distance metric for stylometric clustering; needed to compare authorial styles.
  - *Quick check*: Calculate Delta for two texts; higher values indicate stylistic divergence.
- **Perplexity-based detection**: Measures how "predictable" a text is under a language model; needed for AI-text classification.
  - *Quick check*: Low perplexity for human texts, high for synthetic; test on sample corpus.
- **SHAP (SHapley Additive exPlanations)**: A method for explaining ML predictions; needed for interpretable forensic attribution.
  - *Quick check*: Run SHAP on a trained classifier; inspect feature importance plots.

## Architecture Onboarding
- **Component map**: Human texts & LLM imitations → Stylometric analysis (Burrows' Delta) → Attribution accuracy → Bias detection (GPTZero) → Adversarial attack (homoglyph) → Legal admissibility check.
- **Critical path**: Stylometric analysis → Attribution accuracy; Bias detection → Legal admissibility.
- **Design tradeoffs**: Scalability vs. explainability; accuracy vs. bias; robustness vs. adversarial vulnerability.
- **Failure signatures**: High FPR on non-native texts; low Delta separability between human and LLM texts; degraded detection after homoglyph substitution.
- **First experiments**:
  1. Run Burrows' Delta on human vs. LLM imitation corpus to verify clustering.
  2. Test GPTZero on non-native TOEFL essays to confirm high FPR.
  3. Perform homoglyph substitution on texts and reassess detection performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical claims depend on representativeness of cited studies (Mikros 2025, O'Sullivan 2025, Liang et al. 2023), which are not directly reproduced.
- Practical efficacy of proposed hybrid workflows and SHAP-based explainability is not validated in diverse forensic contexts.
- Rapid evolution of LLM capabilities may quickly render current stylistic mimicry detection methods obsolete.

## Confidence
- **High confidence**: AI-text detectors exhibit high false-positive rates against non-native English writers (supported by Liang et al. 2023).
- **Medium confidence**: LLM-based stylometry can reach ~85% accuracy, but depends on dataset and prompt specifics.
- **Low confidence**: Practical efficacy of hybrid workflows and SHAP-based explainability without implementation details or validation.

## Next Checks
1. Replicate Burrows' Delta analysis using `Stylo` on human-LLM paired corpus to confirm detectable stylistic divergence.
2. Test GPTZero (or similar) on non-native TOEFL essays and human-authored texts to empirically verify reported high false-positive rates.
3. Perform homoglyph substitution on synthetic and human texts to assess degradation in detector performance, replicating adversarial attack scenario.