---
ver: rpa2
title: Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation
arxiv_id: '2602.00899'
source_url: https://arxiv.org/abs/2602.00899
tags:
- retrieval
- search
- training
- latency
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the vocabulary mismatch problem in e-commerce
  recommendation by proposing a dense retrieval system that semantically matches user
  intent (e.g., reviews or queries) to product metadata. The core approach uses a
  two-tower bi-encoder fine-tuned via supervised contrastive learning on Amazon Fashion
  reviews, treating reviews as query proxies and item metadata as positive documents.
---

# Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation

## Quick Facts
- arXiv ID: 2602.00899
- Source URL: https://arxiv.org/abs/2602.00899
- Authors: Mritunjay Pandey
- Reference count: 4
- One-line primary result: Dense retrieval with contrastive fine-tuning achieves 0.66 Recall@10 on Amazon Fashion review-to-title matching, a 2.5× improvement over BM25 baseline.

## Executive Summary
This paper tackles the vocabulary mismatch problem in e-commerce recommendation by proposing a dense retrieval system that semantically matches user intent (e.g., reviews or queries) to product metadata. The core approach uses a two-tower bi-encoder fine-tuned via supervised contrastive learning on Amazon Fashion reviews, treating reviews as query proxies and item metadata as positive documents. For scalability, the system employs ONNX Runtime with INT8 quantization for low-latency CPU inference and FAISS HNSW for efficient approximate nearest neighbor search. The method improves Recall@10 from 0.26 (BM25) to 0.66 on a review-to-title benchmark over 826,402 catalog items, achieving 6.1 ms median CPU latency and a 4× reduction in model size, enabling practical, high-performance content-based recommendation at scale.

## Method Summary
The method employs a Siamese bi-encoder architecture initialized from all-mpnet-base-v2, where both query (review text) and document (product metadata) towers share weights and project inputs into a 768-dimensional space. Training uses supervised contrastive learning with Multiple Negatives Ranking Loss (MNRL), treating each review-item pair as positive and in-batch items as negatives. The system processes Amazon Fashion reviews and metadata, filtering for high-quality interactions (rating ≥4, English, ≥5 tokens) to create 50,000 training pairs from 826,402 items. For deployment, the model is exported to ONNX with INT8 quantization and indexed using FAISS HNSW for efficient approximate nearest neighbor search, enabling sub-10ms CPU inference at scale.

## Key Results
- Recall@10 improves from 0.26 (BM25) to 0.66 on review-to-title benchmark
- 6.1 ms median CPU inference latency with batch size 1
- 4× model size reduction through INT8 quantization
- HNSW search achieves <1ms latency with <1% recall loss versus flat index

## Why This Works (Mechanism)

### Mechanism 1: Domain-Adaptive Contrastive Alignment
Fine-tuning a bi-encoder on (review, item) pairs via MNRL bridges vocabulary mismatch by aligning embedding space so semantically related intent and metadata cluster together. For each batch, MNRL maximizes similarity between a review and its purchased item while pushing apart unrelated products using other batch items as negatives.

### Mechanism 2: Siamese Bi-Encoder with Mean Pooling
A shared-weight two-tower architecture projects both queries and documents into unified 768-dim vector space using MPNet encoders with mean pooling and attention masking. Weight sharing forces a unified semantic function across heterogeneous inputs, regularizing the model while enabling document pre-indexing.

### Mechanism 3: Quantized Inference + Graph-Based ANN
Post-training INT8 quantization reduces memory bandwidth and model size by 4× while HNSW indexing enables sub-linear search (O(log N)) over 826K items. The combination achieves <10ms CPU latency with negligible recall loss, making large-scale deployment practical.

## Foundational Learning

- **Contrastive Learning (InfoNCE/MNRL)**: Core training objective using in-batch negatives for efficient supervision without hard-negative mining. *Quick check*: For batch size 8, how many negative pairs does each query implicitly train against?

- **Bi-Encoder vs. Cross-Encoder Architectures**: Bi-encoders allow document embeddings to be pre-computed and indexed for sub-second retrieval, while cross-encoders require full recomputation per query. *Quick check*: Why can't you pre-compute document representations with a cross-encoder?

- **Approximate Nearest Neighbor (HNSW)**: Enables sub-linear search over large catalogs with configurable recall-latency trade-off via parameters M and efSearch. *Quick check*: What happens to recall if efSearch=2 vs. efSearch=16?

## Architecture Onboarding

- **Component map**: Data filtering → (review, item) pair construction → MNRL fine-tuning (PyTorch, T4 GPU) → ONNX export + INT8 quantization → batch vectorization → FAISS HNSW index build + metadata cache (pickle hashmap) → Online query encoding → HNSW search → O(1) metadata lookup

- **Critical path**: Query encoding (~6.1ms p50) dominates; HNSW search (<1ms) and metadata lookup are negligible.

- **Design tradeoffs**: INT8 vs. FP32: 4× smaller model, 81% latency reduction, ~0.2% recall loss. HNSW vs. Flat index: <1ms vs. 92ms search, ~0.3% recall loss, ~2.7GB RAM overhead. Batch size 8 + gradient accumulation 4: Fits T4 memory while maintaining effective batch 32.

- **Failure signatures**: Semantic drift (Nike queries retrieving Adidas), attribute hallucination (fine-grained features ignored), cold-start latency (2.7GB index + ~1GB cache startup).

- **First 3 experiments**: 1) Reproduce baseline: Zero-shot MPNet vs. BM25 Recall@10 (expected: 0.42 vs. 0.26). 2) Quantization ablation: INT8 vs. FP32 recall/latency (expected: <0.5% recall drop). 3) Index scaling: Profile HNSW search latency at efSearch ∈ {8, 16, 32} over full 826K catalog (target: <2ms p99).

## Open Questions the Paper Calls Out

1. Does hybrid score fusion (combining BM25 with dense vectors) effectively mitigate entity confusion while preserving semantic recall in production deployments? The paper identifies entity confusion as a limitation and proposes hybrid search via score fusion but doesn't implement or evaluate it.

2. Would cross-encoder re-ranking of HNSW top-K candidates yield meaningful MRR improvements with acceptable latency overhead for interactive systems? Future work explicitly calls for cross-encoder re-ranking to improve precision at top ranks with minimal latency overhead.

3. How well does training on review-to-item pairs generalize to actual short search queries in e-commerce? The paper uses reviews as "query proxies" but doesn't validate this transfer assumption against real user search queries.

## Limitations
- Review proxy assumption unproven: Semantic alignment between review text and actual user search intent remains unverified
- Single-domain evaluation: Amazon Fashion results may not generalize to other e-commerce verticals
- Training efficiency concerns: Batch size 8 × gradient accumulation 4 and 1 epoch may limit negative sampling diversity

## Confidence

**High Confidence**: Dense retrieval architecture and training pipeline (bi-encoder + MNRL + INT8 + HNSW) are well-specified and reproducible. Latency measurements (6.1 ms median) and model size reduction (4×) are concrete and verifiable.

**Medium Confidence**: Recall improvements (0.26→0.66 BM25→Dense) are measurable but depend on review proxy assumption. Semantic matching quality requires validation beyond raw recall metrics.

**Low Confidence**: Generalization to other domains and robustness of review-based query proxies across different product categories remain uncertain without cross-domain validation.

## Next Checks

1. **Review Proxy Validation**: Construct test set where actual user search queries (from query logs) are paired with clicked items, then measure Recall@10 using trained model. Compare performance between review-based queries vs. real search queries to quantify proxy quality degradation.

2. **Cross-Domain Transferability**: Apply trained Fashion model to different Amazon category (Electronics or Home) without fine-tuning. Measure performance drop and identify which product attribute types cause most degradation.

3. **Hard Negative Mining Impact**: Implement semi-hard or hard negative mining during training using faiss index to find nearest negatives. Measure Recall@10 improvement versus in-batch negatives baseline to quantify gains from more sophisticated negative sampling.