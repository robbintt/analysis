---
ver: rpa2
title: 'LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement
  Learning'
arxiv_id: '2506.04821'
source_url: https://arxiv.org/abs/2506.04821
tags:
- reasoning
- arxiv
- logic
- puzzles
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LogicPuzzleRL, a reinforcement learning framework
  that fine-tunes LLMs on seven custom logic puzzles to improve mathematical reasoning.
  Each puzzle targets a distinct reasoning skill (e.g., constraint propagation, spatial
  consistency) and provides verifiable rewards that encourage iterative, hypothesis-driven
  problem solving.
---

# LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.04821
- **Source URL:** https://arxiv.org/abs/2506.04821
- **Reference count:** 6
- **Primary result:** Fine-tuning LLMs via reinforcement learning on custom logic puzzles improves mathematical reasoning, achieving 48.17% average accuracy on diverse math benchmarks (+8.22% relative gain over zero-shot).

## Executive Summary
This paper introduces LogicPuzzleRL, a reinforcement learning framework that fine-tunes LLMs on seven custom logic puzzles to improve mathematical reasoning. Each puzzle targets a distinct reasoning skill (e.g., constraint propagation, spatial consistency) and provides verifiable rewards that encourage iterative, hypothesis-driven problem solving. The model is trained with RLVR, using a composite reward function that evaluates format compliance, intermediate reasoning steps, and final correctness, with curriculum-based difficulty progression. Evaluations on diverse math benchmarks (AIME24, GSM8K, MATH, AMC23, OlympiadBench, Gaokao2024, Minerva-MATH) show that LogicPuzzleRL significantly improves performance, especially on mid-difficulty problems requiring multi-step reasoning. The All-Game RL model achieves an average accuracy of 48.17% (+8.22% relative gain over zero-shot), with the largest gains in geometry (from Nonogram training) and combinatorics. Analysis reveals that puzzle-based RL strengthens compositional reasoning routines, leading to better performance on structured math tasks, though gains are limited for easy or highly specialized problems.

## Method Summary
LogicPuzzleRL uses reinforcement learning with verifiable rewards (RLVR) to fine-tune LLMs on seven custom logic puzzles (Sudoku, Nonogram, Cryptarithm, Magic Square, Zebra Puzzle, Graph Connectivity, Knights and Knaves). Each puzzle is generated with controlled difficulty and verified for logical consistency and unique solutions. The training employs a composite reward function R = Σ[rfmt(st, at) + rint(st, at)] + rfinal(sT+1), where rfmt evaluates format compliance, rint rewards intermediate reasoning steps, and rfinal provides binary correctness feedback. The GRPO algorithm optimizes the policy with entropy regularization for exploration. A curriculum manager dynamically adjusts puzzle difficulty based on validation accuracy thresholds. The All-Game training combines all puzzles with per-game reward components, while single-game variants focus on individual puzzle types.

## Key Results
- LogicPuzzleRL improves mathematical reasoning across diverse benchmarks (AIME24, GSM8K, MATH, AMC23, OlympiadBench, Gaokao2024, Minerva-MATH).
- All-Game RL model achieves 48.17% average accuracy (+8.22% relative gain over zero-shot).
- Largest gains observed in geometry (from Nonogram training) and combinatorics, with improvements concentrated on mid-difficulty problems requiring multi-step reasoning.
- Compositional reasoning routines learned from puzzles transfer to structured math tasks, though gains are limited for easy or highly specialized problems.

## Why This Works (Mechanism)
LogicPuzzleRL works by cultivating general-purpose reasoning capabilities through reinforcement learning on verifiable logic puzzles. The framework leverages the fact that puzzles like Sudoku and Cryptarithm are constraint satisfaction problems, teaching the model to propagate constraints and eliminate options systematically. The composite reward function with intermediate step validation encourages the model to develop coherent reasoning chains rather than relying on pattern matching. The curriculum-based difficulty progression ensures the model builds skills incrementally, while the compositional nature of puzzle-solving teaches the model to combine learned reasoning primitives in novel sequences for new problems.

## Foundational Learning
- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** This is the core training paradigm. Unlike standard supervised fine-tuning (SFT) which mimics expert outputs, RLVR uses an objective, programmable reward signal (puzzle correctness) to optimize model policy, allowing the model to discover novel reasoning paths.
  - **Quick check question:** Can you explain why binary reward signals from a deterministic puzzle environment are easier to scale than learned reward models based on human preferences?

- **Concept: Constraint Propagation & Prune-and-Search**
  - **Why needed here:** This is the primary reasoning subroutine the model is intended to learn. Understanding that puzzles like Sudoku and Cryptarithm are constraint satisfaction problems is key to interpreting why this training transfers to algebraic manipulation.
  - **Quick check question:** In a Cryptarithm (SEND + MORE = MONEY), does learning to propagate the carry from the ones place to the tens place count as domain-specific heuristics or a general reasoning strategy? Explain based on the paper's claims.

- **Concept: Compositional Generalization**
  - **Why needed here:** The paper claims its success is due to "compositional reasoning routines." This means the model isn't just pattern-matching entire solutions but is combining learned reasoning primitives (e.g., "check constraint," "eliminate option") in novel sequences for new problems.
  - **Quick check question:** According to the paper's analysis, does this compositional training improve performance uniformly across all difficulty levels, and if not, where does it fail?

## Architecture Onboarding
- **Component map:**
  - Puzzle Engine -> Validator -> Reward Aggregator -> Curriculum Manager -> RL Loop (GRPO)
- **Critical path:** The Data Construction phase is the primary bottleneck. Ensuring each puzzle has a unique solution and annotated intermediate steps for rint is non-trivial and essential for the reward signal.
- **Design tradeoffs:**
  - Single-Game vs. All-Game Training: Single-game RL optimizes for domain-specific heuristics; All-Game RL promotes broader generalization. Engineers must decide between a specialist model or a generalist.
  - Intermediate vs. Final Rewards: Designing rint requires more engineering effort (partial solution labels) but yields more stable training than sparse, outcome-only rfinal.
- **Failure signatures:**
  - Negative Transfer: Zebra-RL showed performance drop on target math domains. Monitor for this.
  - Overfitting to Easy Patterns: Model plateaus and fails to advance in curriculum. Suggests rfmt/rint are too permissive or difficulty jump is too steep.
  - Incoherent Reasoning Chains: Model achieves correct rfinal with illogical intermediate steps. Suggests rint is not being enforced correctly.
- **First 3 experiments:**
  1. Ablate Intermediate Rewards: Train a model using only rfinal to quantify the impact of process supervision on reasoning coherence.
  2. Cross-Domain Transfer Test: Train on Nonogram-RL and evaluate only on geometry subsets of MATH benchmark to validate the spatial reasoning hypothesis.
  3. Curriculum Stress Test: Fix curriculum progression to "hard" from step 1 and compare convergence speed and final performance against the dynamic curriculum to test its efficiency.

## Open Questions the Paper Calls Out
- **Open Question 1:** Why does Zebra puzzle training produce negative transfer (−1.26%) to mathematical reasoning benchmarks when other individual puzzle types yield positive gains?
- **Open Question 2:** Can puzzle-based RL training be extended to improve performance on the highest-difficulty problems (ratings 8–9) that currently show negligible or negative gains?
- **Open Question 3:** Does puzzle-based RL training improve reasoning capabilities in non-mathematical domains such as natural language inference, scientific reasoning, or code comprehension?
- **Open Question 4:** What explains the consistent performance decline in Applied and Computational Mathematics across all puzzle types, and can this negative transfer be mitigated?

## Limitations
- The framework shows limited gains on easy or highly specialized problems, suggesting the approach may not be universally effective.
- Specific architectural details and hyperparameters are not provided, making exact reproduction challenging.
- The mechanism for computing intermediate rewards (rint) is not fully specified for all puzzle types, requiring assumptions that could impact training dynamics.
- The strong transfer observed from Nonogram to geometry problems, while notable, requires deeper analysis to confirm whether this is a robust pattern or specific to the evaluation setup.

## Confidence
- **High Confidence:** The core RLVR framework using verifiable puzzle rewards and the composite reward function (rfmt, rint, rfinal) is well-defined and technically sound. The general trend of improvement across math benchmarks is supported.
- **Medium Confidence:** The claim that puzzle-based RL strengthens "compositional reasoning routines" is plausible based on the analysis of improvement on structured math tasks, but the specific nature of these learned primitives is not fully characterized. The strong transfer from Nonogram to geometry is observed but requires further validation.
- **Low Confidence:** The specific numerical results (e.g., 48.17% average accuracy, +8.22% relative gain) are tied to the unspecified model and hyperparameters, making exact reproduction uncertain.

## Next Checks
1. **Ablation of Intermediate Rewards:** Train a control model using only the final reward (rfinal) and compare its reasoning coherence and final performance against the full LogicPuzzleRL model to quantify the impact of process supervision.
2. **Cross-Domain Transfer Specificity:** Conduct a focused evaluation where a model trained on Nonogram-RL is tested exclusively on the geometry subsets of the MATH benchmark. This will provide a more direct test of the claimed spatial reasoning transfer.
3. **Curriculum Efficiency Analysis:** Implement a "hard from the start" curriculum and compare its convergence speed and final performance against the dynamic curriculum. This will help determine if the curriculum manager is efficiently optimizing the learning trajectory or if a simpler approach could suffice.