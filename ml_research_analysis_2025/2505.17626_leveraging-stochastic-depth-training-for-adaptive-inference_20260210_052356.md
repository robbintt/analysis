---
ver: rpa2
title: Leveraging Stochastic Depth Training for Adaptive Inference
arxiv_id: '2505.17626'
source_url: https://arxiv.org/abs/2505.17626
tags:
- skipping
- inference
- blocks
- accuracy
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for adaptive inference using models
  trained with Stochastic Depth and selective layer-skipping configurations. The key
  insight is that Stochastic Depth training makes models more resilient to skipping
  layers at inference time.
---

# Leveraging Stochastic Depth Training for Adaptive Inference

## Quick Facts
- arXiv ID: 2505.17626
- Source URL: https://arxiv.org/abs/2505.17626
- Reference count: 22
- Key outcome: Up to 2× power efficiency improvement with minimal accuracy loss using Stochastic Depth-trained models and zero-overhead skipping gates

## Executive Summary
This paper introduces a framework for adaptive inference on edge devices using models trained with Stochastic Depth. The key insight is that Stochastic Depth training makes models more resilient to layer skipping at inference time. By inserting simple zero-overhead gates and using sensitivity analysis to identify optimal skipping configurations, the approach achieves significant energy savings while maintaining accuracy. The framework is evaluated on CIFAR-10/100 with ResNet architectures, demonstrating up to 2× power efficiency improvements and up to 1.97× more processed inferences compared to baseline models.

## Method Summary
The method trains ResNet models with Stochastic Depth for 500 epochs, then inserts weightless skip gates before all blocks except the first in each segment. Sensitivity analysis evaluates each block individually to rank their importance, and Pareto-optimal skipping configurations are generated by evaluating accuracy versus inference time for configurations that skip the N least sensitive blocks. Runtime adaptation selects from these configurations based on device load, achieving predictable inference without the overhead of learned decision gates. The approach is implemented using IREE compilation and evaluated on an Odroid XU4 edge platform.

## Key Results
- Up to 2× improvement in power efficiency compared to original models
- Up to 1.97× more processed inferences while maintaining accuracy
- Minimal accuracy drops as low as 0.71% when skipping layers
- More predictable inference compared to SkipNet (13× variance vs. controlled adaptation)
- Achieves 3.33× speedup compared to SkipNet's 0.96×

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Depth Training Induces Skipping Resilience
During Stochastic Depth training, residual blocks are randomly deactivated with layer-dependent probability, forcing the network to learn representations that function correctly across many different effective depths. This creates an implicit ensemble-like behavior that transfers to deliberate, structured skipping at deployment. Models trained this way maintain substantially higher accuracy across all skipping levels compared to traditionally-trained models.

### Mechanism 2: Sensitivity Analysis Prunes the Combinatorial Design Space
Individual block sensitivity analysis provides a tractable approximation to the intractable combinatorial problem of selecting optimal skipping configurations. Each skippable block is independently removed once, and accuracy impact is measured. Blocks are ranked from least to most sensitive, and near-Pareto configurations are generated by iteratively skipping the N least-sensitive blocks, reducing search from exponential to linear evaluations.

### Mechanism 3: Zero-Overhead Gates Enable Predictable Runtime Adaptation
Weightless, compile-time-configurable gates avoid the memory, compute, and unpredictability overheads of learned decision gates while enabling runtime adaptation. Skip gates are implemented as simple conditional execution paths compiled via IREE's MLIR-based flow. The skipping configuration is a binary array passed as input alongside the image, allowing configuration selection based on system load rather than input content.

## Foundational Learning

- **Residual Connections (Skip Connections)**: Why needed: The entire approach relies on ResNet architecture where identity shortcuts allow information to bypass layers. Without these, "skipping" a layer would destroy the network's ability to propagate information. Quick check: If you remove a residual block but keep its skip connection active, what happens to the tensor dimensions at that point in the network?

- **Stochastic Depth Training Procedure**: Why needed: This is the core enabler—understanding that p_L controls the survival probability of the final layer, and how the linear decay schedule affects which layers are more frequently trained. Quick check: With p_L = 0.5 in a 100-layer ResNet, approximately what fraction of the network is active during each training mini-batch on average?

- **Pareto Frontier in Multi-Objective Optimization**: Why needed: The method produces multiple (accuracy, latency/energy) operating points. Understanding non-dominated solutions helps interpret why 32 configurations survive pruning. Quick check: If configuration A has 90% accuracy at 10ms and configuration B has 89% accuracy at 8ms, is either dominated? What about 89% at 12ms?

## Architecture Onboarding

- Component map: [Input CNN + Dataset] -> [Stochastic Depth Training] -> [Gate Insertion] -> [Sensitivity Analysis] -> [Pareto Front Generation] -> [Runtime Adaptation] -> [Edge Deployment]

- Critical path:
  1. Stochastic Depth training (500 epochs in experiments)—if this is skipped or poorly tuned, no resilience property emerges.
  2. Sensitivity analysis accuracy—must be run on held-out validation set representative of deployment distribution.
  3. Pareto front generation—requires actual timing measurements on target hardware, not FLOP estimates.

- Design tradeoffs:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Higher p_L (more layers kept during training) | Better final accuracy | Less skipping resilience |
  | Lower min_acc threshold | More adaptation flexibility | Risk of unusable low-accuracy outputs |
  | Aggressive Δ_req (fast return to high-accuracy config) | Better accuracy recovery | More configuration thrashing |
  | Skip only non-first blocks per segment | Ensures channel dimension changes handled correctly | Reduces B_s, limiting configuration diversity |

- Failure signatures:
  - Catastrophic accuracy cliff at certain skip counts: Block interaction effects violate sensitivity analysis assumptions. Investigate which block combinations cause failure.
  - No significant speedup despite skipping: Compiler not eliminating skipped computation; verify IREE conditional lowering.
  - Runtime thrashing between configs: Δ_req too short or workload variance too high. Increase Δ_req or add hysteresis.
  - Power efficiency gains smaller than expected: Baseline already compute-bound; measure memory access patterns.

- First 3 experiments:
  1. Reproduce Figure 1 on your target architecture: Train ResNet-20 on CIFAR-10 with traditional vs. Stochastic Depth (p_L=0.5), then sweep skip counts. Verify resilience gap exists. This validates the core mechanism before investing in full pipeline.
  2. Sensitivity list sanity check: For ResNet-20 (7 skippable blocks), compute individual sensitivities. Then exhaustively evaluate all 2^7 = 128 configurations. Compare Pareto front from sensitivity-based filtering vs. exhaustive search. Quantify approximation error.
  3. End-to-end latency validation on target hardware: Deploy full pipeline on your edge platform. Measure: (a) configuration switch overhead, (b) actual inference time variance for fixed configuration, (c) adaptation algorithm overhead. Compare against reported 2× efficiency gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Core mechanism validation limited to CIFAR-10/100 with ResNet-20/110, lacking broader dataset and architecture generalization
- Sensitivity analysis approximation quality versus exhaustive search not quantified
- Runtime adaptation assumes predictable workload patterns that may not hold in all edge deployment scenarios

## Confidence

- **High Confidence**: The basic observation that Stochastic Depth training provides some resilience to layer skipping, supported by Figure 1's accuracy curves showing traditional training's vulnerability to skipping.
- **Medium Confidence**: The sensitivity analysis method effectively identifies useful skipping configurations, based on the empirical Pareto front containing 32 out of 51 possible configurations, though the approximation quality versus exhaustive search is not quantified.
- **Medium Confidence**: The zero-overhead gate implementation achieves predictable, controllable inference compared to SkipNet, supported by the 0.96× versus up to 3.33× speedup comparison, though the claim relies on specific compiler optimization capabilities.

## Next Checks

1. **Exhaustive Configuration Validation**: For the smaller ResNet-20 (7 skippable blocks), perform exhaustive evaluation of all 128 configurations and compare the Pareto front from sensitivity analysis versus ground truth. Quantify the approximation error and identify failure modes where sensitivity analysis mispredicts.

2. **Generalization Test**: Apply the framework to a different dataset (e.g., SVHN or Tiny ImageNet) and a non-image classification task (e.g., semantic segmentation) to verify Stochastic Depth resilience transfers beyond CIFAR classification with ResNet.

3. **Cross-Architecture Verification**: Implement the approach on a non-ResNet architecture with residual connections (e.g., DenseNet or Wide ResNet) to test whether the Stochastic Depth→skipping resilience mechanism depends on ResNet's specific block structure.