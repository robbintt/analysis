---
ver: rpa2
title: On the Fundamental Limitations of Decentralized Learnable Reward Shaping in
  Cooperative Multi-Agent Reinforcement Learning
arxiv_id: '2511.00034'
source_url: https://arxiv.org/abs/2511.00034
tags:
- reward
- decentralized
- coordination
- learning
- shaping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic empirical study of decentralized
  learnable reward shaping in cooperative multi-agent reinforcement learning. The
  authors propose DMARL-RSA, a fully decentralized system where each agent learns
  individual reward shaping functions, and evaluate it on cooperative navigation tasks
  in the simplespreadv3 environment.
---

# On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.00034
- Source URL: https://arxiv.org/abs/2511.00034
- Reference count: 22
- Key result: DMARL-RSA achieves only -24.20 ± 0.09 average reward, a 26.12-point gap behind MAPPO's 1.92 ± 0.87

## Executive Summary
This paper presents a systematic empirical study demonstrating that decentralized learnable reward shaping cannot overcome fundamental coordination barriers in cooperative multi-agent reinforcement learning. Despite sophisticated reward learning mechanisms, the proposed DMARL-RSA system achieves performance comparable to simple independent learning, falling far short of centralized training methods. The study identifies three critical barriers—non-stationarity from concurrent policy updates, exponential credit assignment complexity, and misalignment between individual and global objectives—that limit decentralized coordination even with advanced reward shaping.

## Method Summary
The study evaluates three approaches on cooperative navigation tasks in simple_spread_v3: MAPPO (centralized training with decentralized execution), DMARL-RSA (decentralized reward shaping), and IPPO (independent learning). All methods use PPO with similar hyperparameters but differ in information access during training. MAPPO uses a centralized critic with global state information, while DMARL-RSA and IPPO use independent actor-critics with only local observations. DMARL-RSA adds learnable reward shaping networks per agent to modify environmental rewards. The study runs 5,000-episode training sessions with 25-step episodes across three random seeds.

## Key Results
- DMARL-RSA achieves -24.20 ± 0.09 average reward, performing similarly to IPPO (-23.19 ± 0.96) but 26.12 points worse than MAPPO (1.92 ± 0.87)
- Despite achieving >0.88/3 landmark coverage, DMARL-RSA fails to maximize joint rewards due to myopic optimization
- Reward shaping networks (64→32→16→1) provide no statistically significant improvement over independent learning
- The performance gap is consistent across different random seeds and demonstrates the fundamental limitations of decentralized coordination

## Why This Works (Mechanism)

### Mechanism 1: Centralized Critic for Global Credit Assignment
Access to global state information during training enables coordinated policy updates that decentralized methods cannot replicate. MAPPO's centralized critic computes V(s_global) using complete state information, allowing each agent's policy gradient to implicitly account for teammates' contributions to joint rewards. This provides counterfactual information about alternative agent actions without explicit communication. When global state is unavailable or high-dimensional, or when training must be fully distributed, this advantage disappears.

### Mechanism 2: Non-Stationarity from Concurrent Policy Updates
Decentralized multi-agent learning violates Markov assumptions because each agent perceives a non-stationary transition function as teammates' policies evolve. Agent i observes P_i(s'|s, a_i) = Σ_{a_{-i}} P(s'|s, a_i, a_{-i}) Π_{j≠i} π_j(a_j|s_j). Since {π_j} change during training, the effective transition function shifts, biasing policy gradients and preventing convergence to optimal policies. This barrier persists when agents cannot observe or model teammates' policy changes in real-time, but can be mitigated when agents have access to teammates' policies or can model policy shifts.

### Mechanism 3: Individual-Global Objective Misalignment
Even optimal individual reward shaping cannot recover globally optimal team behavior because local shaping functions lack joint state-action information. Each agent optimizes L_i = E[r_env + α·R_i(s_i, a_i, s'_i)], but the environmental reward r_env = f(s, a_1, ..., a_n) depends on all agents. Without access to (s_{-i}, a_{-i}), individual gradients cannot capture coordination terms needed for global optimum. This misalignment persists when reward shaping networks are restricted to local observations and no communication protocol exists, but tasks that are fully decomposable or have communication channels providing joint state approximations can overcome this barrier.

## Foundational Learning

- Concept: **Potential-Based Reward Shaping (PBRS)**
  - Why needed here: Ng et al.'s guarantee that F(s,a,s') = γΦ(s') - Φ(s) preserves optimal policies underpins DMARL-RSA's design, but stationarity assumptions break in multi-agent settings.
  - Quick check question: Can you explain why potential-based shaping preserves policies in single-agent RL but may fail when multiple agents learn simultaneously?

- Concept: **Credit Assignment in RL**
  - Why needed here: The exponential complexity (2^{|A|(n-1)} counterfactual combinations) makes decentralized credit assignment intractable without centralized critics or counterfactual reasoning.
  - Quick check question: Given a joint reward of +10 for 3 agents, how would you determine each agent's individual contribution without centralized information?

- Concept: **CTDE (Centralized Training with Decentralized Execution)**
  - Why needed here: MAPPO's success relies on this paradigm; understanding the training-time vs. execution-time information asymmetry is essential for interpreting results.
  - Quick check question: What information is available to MAPPO's critic during training that becomes unavailable during execution?

## Architecture Onboarding

- Component map:
  MAPPO: Shared critic (global state) → individual actors (local obs) → coordinated policy updates
  DMARL-RSA: Independent actor-critic per agent + reward shaping network (64→32→16→1) → local-only updates
  IPPO: Independent actor-critic per agent → treats teammates as environment dynamics

- Critical path:
  1. Environment generates (obs_i, r_env) for each agent
  2. MAPPO: critic sees full state; DMARL-RSA/IPPO: local obs only
  3. DMARL-RSA: shaping network modifies r_total = r_heuristic + α·R_i(s,a,s')
  4. PPO update with GAE (λ=0.95), clip ratio 0.2
  5. Convergence: MAPPO → positive reward; DMARL-RSA/IPPO → negative plateau

- Design tradeoffs:
  - MAPPO: Superior coordination but requires global observability; less scalable
  - DMARL-RSA: Adds 1,617 parameters/agent for shaping networks but achieves no statistically significant improvement over IPPO
  - All methods: 25-step episodes limit long-term coordination dynamics

- Failure signatures:
  - DMARL-RSA converges to tight negative range (-24.20 ± 0.09) indicating consistent suboptimal local optimum
  - High landmark coverage (>0.88/3) with poor reward indicates myopic optimization
  - Initialization sensitivity causes early-training variance (MAPPO starting returns: -20 to +19)

- First 3 experiments:
  1. **Reproduce baseline gap**: Run MAPPO vs IPPO vs DMARL-RSA on simple_spread_v3 with seeds {42, 123, 999}; verify 26-point gap and statistical significance (Welch's t-test).
  2. **Ablate shaping coefficient**: Test DMARL-RSA with α ∈ {0.0, 0.5, 1.0, 2.0} to confirm shaping provides no benefit (expect α=0.0 to match α=1.0 within error bars).
  3. **Scale agent count**: Run 2-agent and 5-agent variants to test whether non-stationarity and credit assignment barriers worsen with more agents (hypothesis: gap widens).

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid approaches combining minimal communication protocols with decentralized learning bridge the performance gap between purely decentralized and centralized methods? The paper suggests future work on hybrid approaches that provide coordination information while maintaining scalability benefits, but only evaluates fully decentralized and fully centralized paradigms.

### Open Question 2
How do the identified coordination barriers scale with increasing numbers of agents beyond the 3-agent scenarios tested? The paper notes that scalability patterns may differ significantly with larger numbers of agents, as credit assignment complexity grows exponentially with agent count, but empirical scaling behavior remains untested.

### Open Question 3
Can formal frameworks predict when decentralized reward learning will succeed versus when centralized coordination is necessary? While the paper's three-barrier framework (non-stationarity, credit assignment, misalignment) provides a starting point, it lacks formal predictive criteria that map task properties to expected decentralized performance.

### Open Question 4
Does the coordination paradox (high local coverage, poor global reward) generalize to real-world distributed systems with different reward structures? The paper speculates this pattern likely generalizes to practical distributed systems but only tests sparse-reward navigation, leaving generalization to diverse cooperative tasks unexamined.

## Limitations
- Experiments focus on three-agent coordination; scalability patterns with larger agent populations remain untested
- Specific architectural choices (network depth, optimizer settings) and their impact on results are not systematically varied
- The exact heuristic shaping formula and reward shaping network training details are underspecified, limiting precise reproduction

## Confidence
- **High**: Performance gap (26.12 points), non-stationarity as critical barrier, credit assignment complexity as fundamental limitation
- **Medium**: Reward shaping network's ineffectiveness, heuristic shaping formula, initialization sensitivity impact
- **Low**: Generalizability of findings to all cooperative MARL tasks, impact of specific architectural choices (network depth, optimizer)

## Next Checks
1. Vary the heuristic shaping formula: Systematically test different proximity bonus definitions and landmark coverage rewards to isolate their impact on performance and address underspecification.
2. Analyze reward shaping network gradients: Log and visualize the learned shaping rewards and their gradients relative to environmental rewards to confirm the networks are not learning useful coordination signals.
3. Test with partial observability: Modify the simple_spread_v3 environment to introduce partial observability during training to assess whether the performance gap persists under more realistic conditions and test CTDE limits.