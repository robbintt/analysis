---
ver: rpa2
title: Policy Teaching via Data Poisoning in Learning from Human Preferences
arxiv_id: '2503.10228'
source_url: https://arxiv.org/abs/2503.10228
tags:
- rlhf
- problem
- attack
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a framework for understanding data poisoning\
  \ attacks on preference-based learning systems, specifically targeting reinforcement\
  \ learning from human feedback (RLHF) and direct preference optimization (DPO).\
  \ The authors develop theoretical bounds on the number of poisoned samples required\
  \ to enforce a target policy \u03C0\u2020 by analyzing the susceptibility of these\
  \ paradigms under different attack scenarios (data augmentation vs."
---

# Policy Teaching via Data Poisoning in Learning from Human Preferences

## Quick Facts
- arXiv ID: 2503.10228
- Source URL: https://arxiv.org/abs/2503.10228
- Reference count: 40
- Primary result: Provides theoretical bounds on poisoning sample complexity for RLHF and DPO, showing DPO is more robust when target policy is distant from reference

## Executive Summary
This paper presents a theoretical framework for analyzing data poisoning attacks on preference-based learning systems, specifically targeting reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). The authors develop upper and lower bounds on the number of poisoned preference pairs required to enforce a specific target policy. They identify a fundamental difference in attack susceptibility: DPO exhibits higher robustness than RLHF when the target policy is far from the reference policy, due to the implicit KL-regularization relative to the reference. The analysis distinguishes between data augmentation and synthesis attack scenarios, providing different bounds for each.

## Method Summary
The paper analyzes data poisoning attacks by deriving theoretical bounds on the sample complexity required to enforce a target policy. For RLHF, the attack constructs preference pairs that shift the learned reward parameter into a feasible polytope where the target policy is optimal. For DPO, the attack directly optimizes the policy parameter while accounting for the regularization relative to the reference policy. The analysis covers both unregularized and regularized settings, with the latter requiring KL-divergence constraints. Bounds are derived based on the distance between current and target parameters, the geometry of the constraint sets, and the attacker's ability to synthesize arbitrary trajectory features.

## Key Results
- RLHF attacks require sample complexity proportional to the distance from current reward parameter to target polytope
- DPO exhibits higher robustness than RLHF when target policy is distant from reference policy
- Attack complexity scales inversely with the distance between target and reference policies for DPO
- Regularized RLHF attacks must satisfy KL-divergence constraints rather than hard polytope constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unregularized RLHF is susceptible to poisoning because the attacker can manipulate the reward model parameters to ensure a specific deterministic target policy becomes the unique optimal solution.
- **Mechanism:** The attack constructs preference pairs that shift the learned reward parameter $\omega$ into a feasible "polytope" defined by $M_{\pi^\dagger}^\top \omega \ge \epsilon'$. Within this region, the target policy $\pi^\dagger$ is $\epsilon'$-robust optimal, meaning it achieves strictly higher value than any neighbor. The number of required samples scales with the distance between the initial reward parameter and this polytope.
- **Core assumption:** The target policy $\pi^\dagger$ is deterministic and the feature mapping $\Phi$ is full rank.
- **Evidence anchors:**
  - [Section 4.1]: Describes the attack constraints as a polytope $M_{\pi^\dagger}^\top \omega \ge \epsilon'$ where columns represent feature differences between $\pi^\dagger$ and its neighbors.
  - [Theorem 4.1]: Provides the upper bound on sample complexity depending on projection onto this polytope.
  - [Corpus]: Evidence is weak; related papers discuss different attack vectors like label-flipping or retrieval poisoning, but not the polytope mechanism for RLHF.
- **Break condition:** The mechanism fails if the covariance matrix of the pre-existing data is rank-deficient or if the distance to the target polytope is infinite.

### Mechanism 2
- **Claim:** DPO exhibits higher robustness to poisoning than RLHF when the target policy is distant from the reference policy because the attack must overcome the implicit KL-regularization relative to the reference $\mu$.
- **Mechanism:** DPO attacks optimize the policy parameter $\theta$ directly. The reference policy parameter $\theta_\mu$ acts as an anchor. Theoretical bounds show the required sample complexity scales inversely with the distance $||\theta^\dagger - \theta_\mu||$ only when this distance is small. As the target deviates significantly from the reference, the regularization loss $\lambda ||\theta - \theta_\mu||^2$ dominates, requiring exponentially more poisoned samples to shift the policy.
- **Core assumption:** Policies are log-linear and the feature mapping is bounded.
- **Evidence anchors:**
  - [Section 6]: "The value of $\kappa_1$ is proportional to the distance between $\theta^\dagger$ and $\theta_\mu$... the greater the distance... the less susceptible DPO becomes relative to RLHF."
  - [Theorem 5.2]: Lower bounds on sample complexity show dependence on $||\theta^\dagger - \theta_\mu||$.
  - [Corpus]: No direct evidence found in the provided corpus neighbors regarding the specific DPO robustness mechanism.
- **Break condition:** The robustness claim weakens if the target policy is close to the reference policy, in which case the lower bound becomes vacuous.

### Mechanism 3
- **Claim:** Regularized RLHF attacks function by enforcing a constraint on the KL-divergence between the target policy and the optimal regularized policy, rather than a hard polytope constraint.
- **Mechanism:** For stochastic policies (regularized RLHF), the "optimality" constraint is replaced by a KL-divergence constraint: $D_{KL}(\pi^\dagger || \pi^{reg}_{r_\omega}) \le \epsilon'$. The attacker synthesizes data to minimize the gradient of the loss w.r.t. pre-existing data while satisfying this divergence limit.
- **Core assumption:** The attacker can synthesize trajectories with specific feature differences $\phi(\tau) - \phi(\tau')$.
- **Evidence anchors:**
  - [Section 4.2]: Defines the surrogate attack problem with the constraint $D_{KL}(\pi^\dagger || \pi^{reg}_{r_\omega}) \le \epsilon'$.
  - [Theorem 4.2]: Bounds depend on the KL gap $\beta^2 (D_{KL}(\pi^\dagger || \mu) - \epsilon')^2$.
  - [Corpus]: Evidence is weak; no corpus papers address KL-constrained poisoning of RLHF.
- **Break condition:** If the solution $\omega^\dagger$ is such that the feature difference term $\Gamma_\phi$ is zero, the bounds simplify, but generally, the mechanism requires the ability to solve a fixed-point equation for $\omega$.

## Foundational Learning

- **Concept:** **Bradley-Terry Preference Model**
  - **Why needed here:** This model connects trajectory preferences to rewards. Understanding that $P(o=1|\tau, \tau') = \sigma(r(\tau) - r(\tau'))$ is essential to see how manipulating preference data $D$ alters the reward landscape in RLHF.
  - **Quick check question:** How does the Bradley-Terry model map a reward difference of zero to a preference probability?

- **Concept:** **KL-Regularized Optimization**
  - **Why needed here:** Both RLHF (policy phase) and DPO rely on maximizing reward while staying close to a reference policy $\mu$ via a KL-divergence penalty. This is the structural feature that DPO exploits for robustness.
  - **Quick check question:** If the KL-regularization coefficient $\beta$ increases, does the policy become more or less sensitive to poisoned preference data indicating a large deviation from the reference?

- **Concept:** **Logistic Regression Teaching (Machine Teaching)**
  - **Why needed here:** The attack on the RLHF reward model (a logistic regression problem) leverages teaching set theory. The attacker constructs "teaching samples" to force a specific parameter vector.
  - **Quick check question:** In machine teaching, what is the minimal set of samples required to force a logistic regression model to adopt a specific parameter vector $\omega^\dagger$?

## Architecture Onboarding

- **Component map:**
  - Target Policy ($\pi^\dagger$) -> Synthesis Module -> Learner (RLHF/DPO) -> Policy Output

- **Critical path:**
  1. Define the target policy $\pi^\dagger$ and compute its parameter representation (reward $\omega^\dagger$ or policy $\theta^\dagger$)
  2. Compute the "gap" between the current learner state (or empty dataset state) and the target state (e.g., distance to polytope or $\ell_2$ ball)
  3. Generate the poisoned dataset $\tilde{D}$ containing identical samples with feature vectors scaled to bridge this gap

- **Design tradeoffs:**
  - **Attack Granularity ($\epsilon$) vs. Sample Complexity:** A tighter tolerance $\epsilon$ (closer match to target) requires significantly more samples, as bounds scale with $\epsilon^2$ or $\epsilon^{-1}$
  - **RLHF vs. DPO Attacks:** RLHF attacks are computationally simpler (polytope constraints) but potentially more brittle in regularized settings. DPO attacks are robustly resisted by the reference policy anchor

- **Failure signatures:**
  - **Empty Gradient:** If the gradient of the loss w.r.t. the target parameter is zero (e.g., pre-existing data is already perfectly aligned), the attack sample count drops to zero (success), but if the geometry prohibits the projection, the bounds become infinite (failure)
  - **Vacuous Bounds:** If the lower bound calculation for DPO yields a negative number (due to large $n$ or small distance to $\mu$), the theoretical guarantee of DPO robustness is not applicable

- **First 3 experiments:**
  1. **Linear Reward Verification:** Implement the unregularized RLHF attack in a tabular environment with linear rewards to verify the sample count scales as $O(SA/\sigma_{min}(M)^2)$ as predicted by Theorem 4.1
  2. **DPO Distance Scaling:** Train DPO on a synthetic dataset with a reference policy $\mu$, varying the distance $||\theta^\dagger - \theta_\mu||$ of the target policy to confirm that attack cost increases with distance (Theorem 6.1)
  3. **Robustness Comparison:** Fix a target policy $\pi^\dagger$ and a budget of poisoned samples $k$. Compare the final policy divergence $||\pi_{final} - \pi^\dagger||$ for both RLHF and DPO learners to validate the theoretical susceptibility comparison

## Open Questions the Paper Calls Out

- **Open Question 1:** Can theoretical bounds on poisoning sample complexity be derived for RLHF and DPO under general, non-linear function approximation (e.g., deep neural networks) rather than linear rewards and log-linear policies?
  - **Basis in paper:** [explicit] The concluding discussion states, "It is currently not clear whether RLHF and DPO attacks are feasible for general formulations" and suggests relaxing the (log)linearity assumptions as a direction for future work.
  - **Why unresolved:** The current theoretical guarantees rely on the specific geometric properties of reward polytopes and the strong convexity of the loss landscape in linear settings; non-linear settings introduce non-convexity and complex decision boundaries that break these proofs.
  - **What evidence would resolve it:** A theoretical analysis deriving bounds for general function classes or empirical verification showing whether the sample complexity scales similarly for over-parameterized neural networks.

- **Open Question 2:** What are the sample complexity bounds for preference poisoning attacks if the KL-divergence constraints in regularized RLHF are replaced by alternative constraints, such as total variation distance?
  - **Basis in paper:** [explicit] Section 8 states that because it is "not possible to obtain closed-form solutions to our problems whenever KL constraints are present," it would be useful to "study this attack framework with alternate constraints, such as total variation distance."
  - **Why unresolved:** The KL-divergence term results in non-linear constraints that complicate the optimization, preventing the derivation of closed-form solutions for the attack sample size in the regularized setting.
  - **What evidence would resolve it:** A derivation of closed-form upper and lower bounds on the number of poisoned samples required when the policy regularization is defined via total variation distance instead of KL divergence.

- **Open Question 3:** How susceptible are RLHF and DPO to label-flipping attacks, where the attacker modifies existing preference labels rather than synthesizing new data?
  - **Basis in paper:** [explicit] The authors identify label-flipping attacks as an important direction for future work, distinct from the data synthesis/augmentation scenarios analyzed in the paper.
  - **Why unresolved:** The current analysis assumes the attacker has "synthesis capability" to generate arbitrary trajectory pairs with specific feature vectors to optimize the gradient; label-flipping restricts the attacker to the manifold of the existing dataset, changing the optimization problem from continuous feature synthesis to discrete label selection.
  - **What evidence would resolve it:** Theoretical bounds on the fraction of labels an attacker must flip to enforce a target policy, or an analysis of the feasibility of such attacks given the covariance structure of the existing dataset.

- **Open Question 4:** How does the sample complexity of attacks change when the learner employs robust variants of RLHF or DPO designed to handle data corruption?
  - **Basis in paper:** [explicit] The concluding discussion notes it would be "interesting to... understand the effectiveness of these attacks when a learner uses robust variants of these methods."
  - **Why unresolved:** The derived bounds (e.g., in Theorems 4.1 and 5.1) assume a standard learner maximizing likelihood; robust learners typically employ different optimization objectives or aggregation rules that may dampen the effect of poisoned gradients.
  - **What evidence would resolve it:** A comparative theoretical analysis deriving new bounds for attacking a robust learner (e.g., one using median-of-means or trimmed estimators) to quantify the increase in the number of samples required for a successful attack.

## Limitations

- The analysis assumes idealized conditions including perfect optimization convergence and full-rank feature mappings
- Bounds rely heavily on the attacker's ability to generate exact feature vectors, which may be infeasible in complex real-world environments
- Comparison between RLHF and DPO focuses on specific parameter distances and may not capture all practical deployment scenarios

## Confidence

- **High Confidence:** The theoretical framework and mathematical derivations are sound, with clear proofs for the bounds under stated assumptions. The mechanism by which RLHF reward models can be manipulated through preference pairs is well-established.
- **Medium Confidence:** The robustness claims for DPO relative to RLHF depend on specific parameter regimes and the assumption that feature synthesis is feasible. The empirical validation in the paper is limited.
- **Low Confidence:** The practical attack success rates given real-world constraints (noisy preferences, partial observability, non-linear reward models) are not quantified and may differ significantly from theoretical bounds.

## Next Checks

1. Implement a concrete gridworld environment to verify that the RLHF attack sample complexity scales as predicted by Theorem 4.1, particularly testing the dependence on the minimum singular value of the constraint matrix.
2. Conduct experiments comparing RLHF and DPO under identical poisoning conditions across multiple target policies with varying distances from the reference policy to empirically validate the robustness claims.
3. Test the attack framework under more realistic conditions where feature synthesis is approximate rather than exact, measuring the degradation in attack effectiveness as synthesis capability is constrained.