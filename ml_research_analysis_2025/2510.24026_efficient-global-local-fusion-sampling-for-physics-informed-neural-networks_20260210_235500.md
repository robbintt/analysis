---
ver: rpa2
title: Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks
arxiv_id: '2510.24026'
source_url: https://arxiv.org/abs/2510.24026
tags:
- points
- sampling
- global
- distribution
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Global-Local Fusion (GLF) Sampling Strategy
  for Physics-Informed Neural Networks (PINNs) that combines the strengths of global
  and local sampling methods. The proposed method addresses the trade-off between
  global exploration (which ensures stability but is computationally expensive) and
  local refinement (which is efficient but may compromise robustness).
---

# Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2510.24026
- Source URL: https://arxiv.org/abs/2510.24026
- Authors: Jiaqi Luo; Shixin Xu; Zhouwang Yang
- Reference count: 40
- This paper introduces a Global-Local Fusion (GLF) Sampling Strategy for Physics-Informed Neural Networks (PINNs) that combines the strengths of global and local sampling methods.

## Executive Summary
This paper addresses the fundamental trade-off in PINN training between global exploration (which ensures stability but is computationally expensive) and local refinement (which is efficient but may compromise robustness). The proposed GLF method perturbs training points with Gaussian noise scaled inversely to their residuals, concentrating samples in high-residual regions while maintaining exploration in well-learned areas. A lightweight linear surrogate approximates the global residual-based distribution to reduce computational overhead. Extensive experiments on five benchmark PDEs (including 10D and 20D problems) demonstrate that GLF consistently outperforms both global methods like RAD and local methods like RSmote in terms of accuracy and efficiency.

## Method Summary
GLF introduces an anchor-based sampling framework where each training point generates multiple perturbed candidates. The perturbation scale h(x_i) = α/(r(x_i)+ε) is inversely proportional to the residual magnitude, creating tight clusters around high-residual points and broad exploration around low-residual points. To avoid expensive global residual computation, each candidate inherits its anchor's residual value, creating a piecewise-linear approximation of the true residual landscape. Candidates are then weighted by their residuals and resampled to form the next training set. The method balances the stability of global sampling with the efficiency of local refinement while maintaining computational tractability through the lightweight surrogate.

## Key Results
- GLF achieves lower relative errors than both RAD and RSmote across 5 benchmark PDEs including 10D and 20D problems
- Memory efficiency: GLF uses significantly less GPU memory than RAD while maintaining better accuracy
- Ablation studies confirm the effectiveness of the approximation strategy and neighborhood sampling design
- GLF-D memory cost: 2380MB vs RAD: 10230MB on 20D Reaction-Diffusion equation
- L² relative errors: GLF 0.0036 vs RAD 0.0101 on Allen-Cahn 2D problem

## Why This Works (Mechanism)

### Mechanism 1: Residual-Inverse Gaussian Perturbation
Scaling perturbation noise inversely with residual magnitude creates adaptive local-global exploration. High-residual points yield small perturbations (local refinement), while low-residual points yield large perturbations (global exploration). This assumes residuals are meaningful proxies for local solution difficulty.

### Mechanism 2: Anchor-Based Residual Inheritance
Assigning anchor residuals to neighborhood candidates approximates global residual distribution at fraction of the cost. This piecewise-linear approximation avoids expensive higher-order derivative evaluations by assuming nearby points share similar residual magnitudes.

### Mechanism 3: Probability-Weighted Resampling
Sampling from normalized residual-based weights balances exploitation of difficult regions with coverage. The method computes weights w(x) = r(x)^k/(E[r(x)^k]+c) and draws samples from this distribution, assuming k-exponent and normalization preserve meaningful relative differences.

## Foundational Learning

- Concept: Monte Carlo integration for PDE residuals
  - Why needed here: PINN loss is an intractable integral over domain; sampling quality directly determines approximation quality
  - Quick check question: Can you explain why collocation point placement affects the bias-variance tradeoff in residual approximation?

- Concept: Residual-based adaptivity in scientific ML
  - Why needed here: The core innovation relies on using PDE residuals as a signal for where to concentrate computational effort
  - Quick check question: What physical or mathematical conditions might cause residuals to be a poor proxy for actual solution error?

- Concept: Computational cost of automatic differentiation
  - Why needed here: The lightweight surrogate exists specifically because computing higher-order derivatives for residuals on large candidate sets is expensive
  - Quick check question: Why does the 10D third-order Dispersive equation have similar memory cost to the 20D second-order Reaction-Diffusion equation in GLF-D?

## Architecture Onboarding

- Component map: Residual evaluator -> Scaling module -> Candidate generator -> Weight approximator -> Resampler
- Critical path: Residual evaluation → scaling computation → candidate generation → weight normalization → resampling
- Design tradeoffs:
  - M (candidates per anchor): Paper uses M=3. Ablation GLF-M shows more candidates don't improve results
  - α (scale control): Set to 1e-4 for low-dim, 1e-3 for high-dim. Controls exploration vs. exploitation balance
  - k, c (weight hyperparameters): From RAD framework; control sensitivity to residual magnitude
- Failure signatures:
  - Loss oscillation during training (seen in RSmote): Indicates local-only refinement neglecting other regions
  - Memory blowup: Indicates accidental use of exact residual computation (GLF-D) instead of surrogate
  - Collapsed sampling distribution: Check if weight normalization produces near-deterministic selection
- First 3 experiments:
  1. Reproduce Allen-Cahn 2D baseline: N=2000 points, 3-layer network width 64. Compare RAD, RSmote, GLF on error and memory
  2. Ablation on M: Test M∈{1,3,5,10} on Burgers equation. Verify paper's finding that M=3 is sufficient
  3. High-dimensional scaling test: Run 10D Dispersive with GLF vs RAD. Measure GPU memory ratio

## Open Questions the Paper Calls Out

### Open Question 1
Can the GLF sampling strategy be effectively integrated with other PINN frameworks (e.g., DeepONet, Fourier feature networks) or extended to inverse problems and operator learning? The conclusion states the principles may extend beyond PDEs to other deep learning-based methods where sampling efficiency is a bottleneck.

### Open Question 2
Does GLF maintain its advantages on PDEs with discontinuous solutions, shocks, or singularities (e.g., hyperbolic conservation laws)? All five benchmark PDEs have smooth analytical solutions; no experiments test discontinuous features common in fluid dynamics or wave propagation problems.

### Open Question 3
Can the scaling coefficient α and neighborhood size M be adaptively adjusted during training rather than manually tuned per problem? The paper sets α differently for low-dimensional (1×10⁻⁴) versus high-dimensional (1×10⁻³) problems, with no principled selection criterion or sensitivity analysis provided.

### Open Question 4
What theoretical approximation guarantees exist for the piecewise-linear residual surrogate relative to the true residual distribution? Figure 3 empirically shows the approximation improves with denser training points, but no error bounds, convergence rates, or conditions under which the approximation degrades are provided.

## Limitations

- Hyperparameters k and c for weighting function not explicitly specified, affecting reproducibility
- High-dimensional results (10D, 20D) rely on single PDE each, limiting generalizability
- Claim that local-only methods suffer from "significant oscillations" needs more rigorous quantification

## Confidence

- **High**: GLF's improved accuracy over RAD and RSmote on 2D problems (Allen-Cahn, Burgers, Laplace)
- **Medium**: Computational efficiency claims (memory reduction), as they depend on specific hardware implementation details
- **Medium**: High-dimensional scaling behavior, given limited problem diversity in these regimes

## Next Checks

1. Perform systematic hyperparameter sweep on k and c values to verify robustness of GLF performance
2. Test GLF on additional high-dimensional PDEs (e.g., 15D Allen-Cahn) to confirm scaling patterns
3. Quantify oscillation severity in RSmote through loss variance metrics rather than qualitative assessment