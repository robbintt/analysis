---
ver: rpa2
title: 'LongReasonArena: A Long Reasoning Benchmark for Large Language Models'
arxiv_id: '2508.19363'
source_url: https://arxiv.org/abs/2508.19363
tags:
- reasoning
- long
- length
- input
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LongReasonArena, a benchmark designed to evaluate
  long reasoning capabilities of Large Language Models (LLMs) by requiring algorithmic
  execution with controllable reasoning steps. Unlike existing long-context benchmarks
  that focus on comprehension of long inputs, LongReasonArena emphasizes active generation,
  structuring, and self-correction in long reasoning processes.
---

# LongReasonArena: A Long Reasoning Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2508.19363
- Source URL: https://arxiv.org/abs/2508.19363
- Reference count: 18
- Primary result: Introduces a benchmark evaluating long reasoning via algorithmic execution, showing performance drops with reasoning complexity and highlighting retrieval/backtracking weaknesses.

## Executive Summary
This paper introduces LongReasonArena, a benchmark designed to evaluate long reasoning capabilities of Large Language Models (LLMs) by requiring algorithmic execution with controllable reasoning steps. Unlike existing long-context benchmarks that focus on comprehension of long inputs, LongReasonArena emphasizes active generation, structuring, and self-correction in long reasoning processes. Tasks are categorized into three difficulty levels (1K, 100K, and 1M tokens of reasoning) based on the number of execution steps in solution code. Evaluation of 12 models, including reasoning and non-reasoning models, shows significant performance drops with increasing reasoning complexity. State-of-the-art models like DeepSeek-R1 achieve only 7.5% accuracy on the most challenging tasks. Analysis reveals a linear decline in accuracy with respect to the logarithm of expected reasoning steps and highlights persistent difficulties in retrieval and backtracking operations. The benchmark provides a scalable framework for advancing research in long reasoning.

## Method Summary
The benchmark uses LeetCode algorithmic problems with auto-generated inputs to create verifiable long reasoning tasks. Difficulty is controlled by measuring execution lines of reference code, creating three levels based on reasoning steps (10²–10⁴, 10⁴–10⁵, 10⁵–10⁶). Models are evaluated on their ability to compute final answers through multi-step reasoning, not code generation. The evaluation uses inference-only testing with specific sampling parameters (T=0.6, top_p=0.95, top_k=40, min_p=0) and extracts answers from a specific output format. The methodology isolates reasoning difficulty by capping input length at 32K tokens.

## Key Results
- DeepSeek-R1 achieves only 7.5% accuracy on the most challenging 1M-token reasoning tasks
- Accuracy exhibits linear decline with respect to the logarithm of expected reasoning steps (R² > 0.9 for all reasoning models)
- Retrieval failures transition from index errors (misalignment) to full errors (complete miss) as reasoning chains grow longer
- Models struggle with backtracking operations, often getting stuck in repetitive loops during search tasks

## Why This Works (Mechanism)

### Mechanism 1: Algorithmic Execution Complexity Control
Controlling the scale of generated inputs allows for precise control over the required number of reasoning steps, creating a scalable proxy for long reasoning difficulty. A generated input is fed into known solution code, and the number of execution lines serves as a proxy for reasoning steps. By generating inputs that trigger more or fewer execution lines, the benchmark creates tasks of controllable difficulty.

### Mechanism 2: Retrieval Failure in Active Reasoning Chains
Models fail at retrieval not due to inability to access information but due to failure to manage and query their own dynamically generated state over many steps. In tasks like Two Sum, models must repeatedly query their context for values, functioning as working memory that must be reliably queried.

### Mechanism 3: Log-Linear Performance Degradation
Model accuracy on long reasoning tasks degrades linearly with the logarithm of required reasoning steps. This suggests probabilistic error accumulation where constant per-step failure probability leads to exponential decay in success rate, appearing linear on a log scale.

## Foundational Learning

- **Backtracking in Search**: Understanding DFS and how algorithms explore and abandon paths is crucial for interpreting Word Search failure cases. Quick check: Can you explain why a DFS algorithm might fail to find a solution in a graph even if one exists?

- **Log-Linear Relationship**: Understanding the difference between linear and log-linear decay is essential for grasping the severity of the scaling challenge. Quick check: If a model has a 99% success rate per step, would its overall accuracy decay linearly or exponentially with the number of steps?

- **Code as a Specification for Reasoning**: The benchmark uses algorithmic problems as a specification for verifiable long reasoning processes. The model must execute algorithms mentally, not generate code. Quick check: How does using a known algorithm as the basis for a task ensure that the required reasoning process is both verifiable and controllable in length?

## Architecture Onboarding

- **Component map**: LeetCode problems -> Qwen2.5-Coder input generator -> Difficulty classifier (execution tracer) -> Sample filters -> Evaluation harness
- **Critical path**: The Input Generator is most critical for data quality. If it produces low-coverage or invalid inputs, the difficulty scaling will be noisy or fail.
- **Design tradeoffs**: Algorithmic vs. Open-Ended (verifiability vs. scope), Execution Lines vs. Model Steps (controllable but indirect proxy), Verifiability vs. Contamination (LeetCode problems are public)
- **Failure signatures**: Index Errors (hallucinated positions), Full Errors (complete failure to find seen values), Repetitive Loops (backtracking failures revisiting same paths)
- **First 3 experiments**: 
  1. Run strong reasoning model on all three levels to confirm log-linear relationship (R² > 0.9)
  2. Isolate retrieval vs. backtracking by comparing pure retrieval and pure backtracking tasks
  3. Analyze reasoning length for correct vs. incorrect samples to confirm inefficient compute usage

## Open Questions the Paper Calls Out

### Open Question 1
Can models' performance on retrieval and backtracking operations during long reasoning be improved through targeted training or architectural modifications? The paper notes models struggle with these basic operations despite excelling at simple long-input retrieval tasks.

### Open Question 2
Why does extended reasoning length not correlate with improved problem-solving accuracy, and how can this relationship be corrected? The authors found incorrect samples have significantly longer reasoning length than correct ones.

### Open Question 3
How can we extend long reasoning evaluation to cover inductive and analogical reasoning beyond the deductive reasoning tested in algorithmic execution? The benchmark primarily focuses on deductive reasoning through algorithmic execution.

### Open Question 4
What explains the linear relationship between accuracy decline and the logarithm of reasoning steps across all tested reasoning models? The paper reveals this consistent pattern but doesn't explain the underlying mechanism.

## Limitations

- The benchmark assumes execution lines are a reliable proxy for reasoning complexity, which may not hold for all model architectures or problem types
- Focus on LeetCode-style problems may not represent the full diversity of long reasoning tasks encountered in practice
- The specific output format requirement could advantage or disadvantage certain model architectures

## Confidence

- **High Confidence**: The log-linear relationship between accuracy and reasoning complexity is well-supported by experimental data (R² > 0.9 for all reasoning models)
- **Medium Confidence**: The conclusion that current models struggle with retrieval and backtracking is supported, but the mechanism may be more complex than described
- **Low Confidence**: The claim that execution line count is a perfect proxy for reasoning difficulty assumes step-by-step reasoning like reference code

## Next Checks

1. **Proxy Validation Test**: Run the same benchmark tasks with models that have known different reasoning strategies and verify whether execution line counts consistently predict performance across architectures

2. **Cross-Architecture Error Analysis**: Conduct deeper error analysis comparing retrieval failures across different model families to determine if failure modes are universal or architecture-specific

3. **Real-World Transfer Test**: Evaluate the same models on tasks that combine long input comprehension with reasoning to assess how well isolated long reasoning results generalize to integrated scenarios