---
ver: rpa2
title: 'AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation'
arxiv_id: '2508.17926'
source_url: https://arxiv.org/abs/2508.17926
tags:
- argument
- tasks
- task
- mining
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of performing multiple argument
  mining tasks using a single large language model (LLM). The authors create a unified
  multi-task dataset by converting 19 existing argument mining datasets into a standardized
  format, covering eight tasks such as argument component classification, claim detection,
  evidence detection, argument relation classification, evidence type classification,
  stance detection, fallacies detection, and argument quality assessment.
---

# AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation

## Quick Facts
- **arXiv ID:** 2508.17926
- **Source URL:** https://arxiv.org/abs/2508.17926
- **Reference count:** 40
- **Primary result:** Task-specific fine-tuning of Llama-3.1-8B-Instruct achieves up to 90.74% F1 on argument mining tasks, with multi-task fine-tuning maintaining strong performance without degradation

## Executive Summary
This paper addresses the challenge of performing multiple argument mining tasks using a single large language model (LLM). The authors create a unified multi-task dataset by converting 19 existing argument mining datasets into a standardized format, covering eight tasks such as argument component classification, claim detection, evidence detection, argument relation classification, evidence type classification, stance detection, fallacies detection, and argument quality assessment. They fine-tune Meta AI's Llama-3.1-8B-Instruct model using three strategies: task-specific fine-tuning, multi-task fine-tuning, and model merging. Results show that task-specific fine-tuning significantly improves performance across all tasks, with some tasks reaching up to 90% F1 score. Multi-task fine-tuning maintains strong performance without degradation, demonstrating effective transfer learning. Model merging offers a competitive compromise, achieving high performance while reducing computational costs.

## Method Summary
The authors fine-tune Llama-3.1-8B-Instruct using LoRA (rank=16) for 2 epochs with batch size 32. They convert 19 argument mining datasets into a unified JSONL format and create balanced per-task splits (4000/800/800 for train/validation/test). Three strategies are compared: task-specific fine-tuning of separate models, multi-task fine-tuning on all tasks simultaneously, and model merging using the DELLA algorithm via mergekit library. The merging approach combines task-specific models with task-difficulty weighted parameters to balance performance across different task complexities.

## Key Results
- Task-specific fine-tuning achieves up to 90.74% F1 on Argument Component Classification
- Multi-task fine-tuning reaches 86.59% F1 on Argument Component Classification without performance degradation
- DELLA II merged model achieves 78.72% F1 on Argument Component Classification while reducing computational costs
- Multi-task models show consistent improvements across all eight argument mining tasks

## Why This Works (Mechanism)
The approach works by leveraging transfer learning through fine-tuning strategies that preserve task-specific capabilities while enabling multi-task functionality. Task-specific fine-tuning establishes strong baselines by adapting the LLM to each task's unique characteristics. Multi-task training allows the model to learn shared representations across related argument mining tasks, while the DELLA merging algorithm effectively combines task-specific knowledge by selectively preserving important parameters. The unified dataset format with consistent prompts ensures the model learns to recognize and respond to argument mining tasks regardless of their source.

## Foundational Learning
- **Concept: Argument Mining (AM) Tasks & their Formalization**
  - Why needed here: The entire paper is built around eight specific AM tasks. To understand or replicate this work, one must grasp what each task entails (its input and output) and how they are formalized as classification problems for the LLM.
  - Quick check question: For the "Argument Relation Classification (AR)" task, what are the inputs, and what are the possible output classes? (Answer in your head, then check Section 3.2).

- **Concept: Fine-Tuning Strategies: Task-Specific, Multi-Task, and Model Merging**
  - Why needed here: The paper's core contribution is comparing these three strategies. Understanding the conceptual difference between training one model per task, training one model on all tasks at once, and combining separate models is essential to interpreting the results.
  - Quick check question: What is the fundamental difference in the training process between "multi-task fine-tuning" and the "model merging" approach used to create a multi-task model?

- **Concept: Evaluation Metrics: F1 Score (Micro/Macro) and Mean F1**
  - Why needed here: Performance is the primary measure of success. The paper uses F1 scores to report results on individual tasks (e.g., 90.74% F1 for ACC) and a "mean F1" score to compare overall multi-task performance (e.g., 63.64% for the best merged model). Understanding that a higher F1 score is better and how the mean aggregates performance is critical.
  - Quick check question: A model has a high F1 score on "Stance Detection" but a low F1 score on "Fallacies Detection." Would you say this is a successful multi-task model? How might the "mean F1" score obscure this detail?

## Architecture Onboarding

**Component map:**
Base Model -> Data Pipeline -> Training Modules (Task-Specific SFT, Multi-Task SFT) -> Merging Module -> Evaluation

**Critical path:**
1. Data Unification & Sampling: Convert source datasets to JSONL and create the task-specific splits. This is the foundation.
2. Task-Specific Training: Fine-tune 8 separate models. This is required for both the task-specific baseline and as a source for the merging experiments.
3. Evaluation: Run inference on the test sets for all models (zero-shot, few-shot, fine-tuned, merged) using the standardized prompts.

**Design tradeoffs:**
- Multi-task training vs. Merging: Multi-task training yields higher peak performance (90.74% vs 78.72% F1 on ACC) but requires access to all training data and a more complex training run. Merging is a post-hoc, modular approach that is computationally cheaper and allows incremental addition of new tasks but sacrifices some performance.
- DELLA vs. DARE Merging: The paper finds DELLA (magnitude-based pruning) is more effective than DARE (random pruning), justifying a slightly more complex merging algorithm for better results.
- LoRA vs. Full Fine-Tuning: The authors choose LoRA (rank=16) for all experiments to reduce computational costs and memory usage, trading off potential performance gains from full parameter updates for feasibility and efficiency.

**Failure signatures:**
- Poor performance on "Hard" tasks: A merged model significantly underperforming on Fallacy Detection (FD) or Argument Quality Assessment (AQ) indicates that the merging process failed to preserve the critical task-specific knowledge for these more complex tasks.
- Catastrophic Forgetting: In multi-task training, if performance on a previously learned task (e.g., Stance Detection) crashes as training progresses on other tasks, it's a sign of negative interference.
- Format Hallucination: A fine-tuned model failing to follow the prescribed `<|ANSWER|> <answer> <|ANSWER|>` format, especially in zero/few-shot settings, indicates a failure to learn the output constraint.

**First 3 experiments:**
1. Baseline Establishment: Evaluate the base `Llama-3.1-8B-Instruct` model in zero-shot and few-shot settings on all eight AM test sets to quantify the performance gains from fine-tuning.
2. Task-Specific Fine-Tuning: Train a LoRA adapter for one of the "easy" tasks (e.g., Stance Detection) and evaluate its performance on the test set. Compare this to the baseline.
3. Merge Two Models: Train LoRA adapters for two related tasks (e.g., Stance Detection and Evidence Detection). Merge their weights using a simple DARE configuration and evaluate the resulting model on both tasks' test sets to observe interference and capability retention.

## Open Questions the Paper Calls Out
- Can the AMELIA framework be effectively extended to generative argument mining tasks, such as automatic argument graph construction or debate summarization? The authors note that extending to generative tasks "would substantially broaden its scope" but acknowledge the current restriction to classification tasks.
- Can formal argumentation semantics be integrated into LLM-based systems to improve interpretability without compromising predictive performance? The paper identifies "incorporating explainability mechanisms and formal argumentation semantics" as critical for deployment in sensitive domains like law and healthcare.
- Do the benefits of multi-task fine-tuning and model merging observed in Llama-3.1-8B generalize to other LLM architectures or scale sizes? The authors state they "intend to extend our approach to other LLM architectures" to better characterize model-specific strengths and limitations.

## Limitations
- The merging approach shows performance degradation on harder tasks like fallacy detection and argument quality assessment, suggesting the method may not fully preserve task-specific capabilities for complex tasks.
- Evaluation focuses exclusively on F1 scores without examining per-class performance or error analysis, potentially masking systematic biases in the models' predictions.
- The study relies on LoRA with unspecified hyperparameters (learning rate, dropout, alpha), introducing potential variability in reproducibility across different experimental setups.

## Confidence

- **Task-Specific Fine-Tuning Performance Claims (High Confidence):** The paper demonstrates clear and consistent performance improvements across all eight tasks with task-specific fine-tuning, with the ACC task achieving 90.74% F1. The experimental design and results are straightforward and reproducible.

- **Multi-Task vs. Model Merging Claims (Medium Confidence):** While the paper shows multi-task training outperforms merging (90.74% vs 78.72% F1 on ACC), the merging approach still achieves reasonable performance. However, the specific DELLA algorithm parameters and task difficulty weighting scheme may not generalize across different model families or task combinations.

- **Unified Dataset Construction Claims (Medium Confidence):** The methodology for converting 19 datasets into a unified format appears sound, but the sampling strategy's impact on class balance and task difficulty distribution is not fully explored.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary LoRA rank, learning rate, and dropout parameters to determine their impact on both individual task performance and merged model quality.

2. **Per-Class Performance Evaluation:** Analyze F1 scores for each class within tasks, particularly for harder tasks like fallacy detection, to identify whether performance degradation affects specific argument types more than others.

3. **Cross-Dataset Generalization Test:** Evaluate the best-performing models on held-out samples from the original 19 datasets to measure distribution shift and real-world applicability beyond the unified test set.