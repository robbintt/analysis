---
ver: rpa2
title: Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent
  Framework
arxiv_id: '2601.09342'
source_url: https://arxiv.org/abs/2601.09342
tags:
- moderation
- speech
- hate
- prompting
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-agent moderation system for detecting
  implicitly hateful speech, addressing the limitations of existing approaches that
  rely solely on text-based analysis and struggle with contextual and identity-aware
  moderation. The core method involves a central Moderator Agent and dynamically constructed
  Community Agents representing targeted demographic groups, which consult on ambiguous
  cases by integrating socio-cultural context from Wikipedia.
---

# Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework

## Quick Facts
- **arXiv ID**: 2601.09342
- **Source URL**: https://arxiv.org/abs/2601.09342
- **Reference count**: 10
- **Primary result**: Multi-agent system with Community Agents achieves bACC=0.86, F1=0.86 average on implicit hate speech detection, outperforming prompting baselines

## Executive Summary
This paper addresses the challenge of detecting implicitly hateful speech on social media, where traditional text-based moderation systems struggle due to lack of contextual and identity-aware understanding. The proposed solution is a multi-agent framework that combines a central Moderator Agent with dynamically constructed Community Agents representing targeted demographic groups. The system consults Community Agents on ambiguous cases by retrieving socio-cultural context from Wikipedia, enabling more nuanced moderation decisions that consider the perspectives of marginalized groups. Evaluated on the ToxiGen dataset, the framework demonstrates significant improvements in both accuracy and fairness compared to established prompting methods.

## Method Summary
The core method involves a three-phase workflow: preliminary assessment by a Moderator Agent, conditional consultation with Community Agents for cases below uncertainty thresholds, and final synthesis combining both perspectives. Community Agents are constructed using Wikipedia retrieval (Kd=5 queries) with cross-attention embedding to capture socio-cultural context specific to target demographic groups. The system uses Gemini-2.5-Flash for both agent types and implements decision fusion through an unspecified CombineScores function. The approach addresses the limitation of existing methods that rely solely on text analysis by incorporating identity-aware moderation through in-group consultation.

## Key Results
- Multi-agent system achieves balanced accuracy of 0.86 and F1 score of 0.86 on average
- Outperforms zero-shot, few-shot, and chain-of-thought prompting baselines on ToxiGen dataset
- Maintains balanced performance across six demographic groups (Black, Asian, Jewish, Muslim, Women, LGBTQ)
- Ablation study shows 0% TPR for LGBTQ groups without Community Agent consultation

## Why This Works (Mechanism)
The framework succeeds by integrating socio-cultural context through Community Agents, which provide identity-specific knowledge that general moderation systems lack. When the Moderator Agent encounters ambiguous cases, Community Agents retrieve relevant Wikipedia content about target groups and use cross-attention mechanisms to identify contextually significant information. This consultative approach enables detection of subtle, identity-based microaggressions and implicit stereotypes that would otherwise be missed. The decision to consult is governed by uncertainty thresholds, ensuring the system leverages expert input only when needed while maintaining efficiency.

## Foundational Learning
- **Multi-agent consultation frameworks**: Collaborative AI systems where specialized agents contribute domain expertise; needed to understand how different agents coordinate in moderation tasks
- **Cross-attention embedding for context retrieval**: Mechanism for identifying relevant information from retrieved documents; needed to understand how Community Agents extract socio-cultural context
- **Balanced accuracy metrics**: Evaluation approach that equally weights true positive and true negative rates; needed to assess fairness across demographic groups
- **Uncertainty-based agent activation**: Conditional consultation based on confidence thresholds; needed to understand when and why Community Agents are engaged
- **Wikipedia-based knowledge grounding**: Using publicly available information as contextual foundation; needed to understand the source of socio-cultural knowledge
- **Decision fusion in multi-agent systems**: Combining outputs from multiple agents into final classification; needed to understand how Moderator and Community Agent outputs are reconciled

## Architecture Onboarding
**Component map**: Input Text -> Moderator Agent -> (Uncertainty Check) -> Community Agent (if triggered) -> CombineScores -> Final Output (Hate/Not Hate)

**Critical path**: Input → Moderator Assessment → Uncertainty Threshold Check → Community Consultation (conditional) → Decision Fusion → Output

**Design tradeoffs**: The framework trades computational efficiency for accuracy by conditionally engaging Community Agents only when needed, rather than using them for all cases. This balances performance gains against latency and cost concerns.

**Failure signatures**: Over-reliance on Community Agent consultation for certain groups (evidenced by 0% TPR for LGBTQ without consultation), potential knowledge base biases from Wikipedia-only retrieval, and sensitivity to the unspecified CombineScores function.

**First experiments**:
1. Verify the extreme ablation result by testing Moderator Agent performance on LGBTQ cases alone
2. Test different Wikipedia query strategies to assess impact on Community Agent effectiveness
3. Implement and test various CombineScores functions to understand their impact on final decisions

## Open Questions the Paper Calls Out
- Does the framework generalize to demographic groups beyond the six tested (Black, Asian, Jewish, Muslim, Women, LGBTQ)?
- Would supplementary knowledge sources beyond Wikipedia improve Community Agent effectiveness or reduce knowledge base biases?
- How does the framework compare to human moderators from represented communities in classification accuracy and fairness?
- Does the framework maintain consistent performance across different underlying LLM architectures?

## Limitations
- Heavy reliance on Wikipedia-based context retrieval without specifying query generation strategy or evaluating retrieval quality
- Cross-attention embedding mechanism for Community Agent construction lacks implementation details
- Unspecified CombineScores function for fusing moderator and community outputs
- Extreme ablation result (0% TPR for LGBTQ without consultation) suggests potential brittleness

## Confidence
- **High confidence**: Reported bACC=0.86 and F1=0.86 averages on manually annotated ToxiGen subset
- **Medium confidence**: Framework's generalizability to other implicit hate speech datasets
- **Low confidence**: Claimed improvement over prompting baselines without exact reproduction details

## Next Checks
1. Implement ablation study with Moderator Agent only to verify the claimed 0% TPR for LGBTQ groups and assess whether Community Agent consultation is indeed necessary for balanced performance across all demographic groups
2. Conduct controlled experiments varying the Wikipedia query generation method (e.g., simple keyword vs. contextual queries) to measure impact on Community Agent performance and validate the importance of retrieval quality
3. Test the framework on an independent implicit hate speech dataset (e.g., a different subset of ToxiGen or another dataset like HateXplain) to evaluate generalizability beyond the manually annotated subset used in the paper