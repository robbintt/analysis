---
ver: rpa2
title: 'The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks'
arxiv_id: '2508.16371'
source_url: https://arxiv.org/abs/2508.16371
tags:
- mediomatix
- idioms
- romansh
- https
- classa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Mediomatix corpus is the first parallel dataset for the five
  idioms of Romansh, a low-resource Romance language. It was created by extracting
  291 schoolbook volumes and aligning their segments using embedding-based methods
  with a pivot consensus approach, resulting in 207k multi-parallel segments and over
  2M tokens.
---

# The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks

## Quick Facts
- arXiv ID: 2508.16371
- Source URL: https://arxiv.org/abs/2508.16371
- Reference count: 40
- Primary result: First parallel corpus for five Romansh idioms, created from schoolbooks, achieving 99.7% precision and improving MT BLEU by 7.5 points

## Executive Summary
The Mediomatix corpus provides the first parallel dataset for the five idioms of Romansh, a low-resource Romance language. Created by extracting 291 schoolbook volumes and aligning their segments using embedding-based methods with a pivot consensus approach, the corpus contains 207k multi-parallel segments and over 2M tokens. Human evaluation confirmed high precision (99.7% of 472 segments). The dataset was successfully used to fine-tune a commercial LLM for machine translation between idioms, improving BLEU scores by 7.5 points on average.

## Method Summary
The corpus was created by extracting 291 schoolbook volumes from a content management system, segmenting by HTML elements, and aligning segments using VecAlign with Cohere embed-v4.0 embeddings. Multi-parallel alignment was constructed via pivot consensus across all five idioms, then filtered by length heuristics. For MT evaluation, GPT-4o and GPT-4o-mini were evaluated with 3-shot prompting, and GPT-4o-mini was fine-tuned multilingually on 5,000 examples (250 per direction × 20 directions) with standard OpenAI defaults.

## Key Results
- First parallel corpus for all five Romansh idioms (Sursilvan, Vallader, Puter, Surmiran, Sutsilvan)
- 207k multi-parallel segments with 99.7% precision (human evaluation of 472 segments)
- MT fine-tuning improved BLEU scores by 7.5 points (37.9 vs 30.4 baseline)
- Corpus available under CC-BY-NC-SA license at https://huggingface.co/datasets/ZurichNLP/mediomatix

## Why This Works (Mechanism)

### Mechanism 1
Embedding-based alignment can identify parallel segments across related language varieties without translation supervision. VecAlign compares vector embeddings of text segments across idioms, computing similarity scores to identify corresponding content. By limiting output to 1-1 alignments and deletions (via the "maximum alignment size" hyperparameter), the system reduces false positives in the low-resource, near-monotonic setting of comparable schoolbooks. Core assumption: multilingual embedding models transfer sufficiently to Romansh idioms despite limited explicit pretraining data. Evidence: "aligning their segments using embedding-based methods" [abstract], VecAlign effectiveness in low-resource settings [section 3.2]. Break condition: if embedding spaces fail to capture semantic similarity across idioms, alignment precision degrades rapidly.

### Mechanism 2
Pivot consensus alignment increases multi-parallel precision by requiring agreement across all pairwise pivot paths. Rather than relying on a single pivot language, the approach computes five separate pivot alignments (one per idiom as pivot) and takes their intersection. A segment pair is retained only if aligned via all pivot paths. Core assumption: errors in individual pivot alignments are not systematically correlated across different pivots. Evidence: "pivot consensus approach" [abstract], Table 2 shows consensus achieves 97.2 precision vs 94.8 for single-pivot (Sursilvan) [section 3.2]. Break condition: if alignment errors are correlated, consensus provides marginal benefit while discarding valid alignments.

### Mechanism 3
Fine-tuning commercial LLMs on small but high-quality parallel data improves MT between related language varieties. Multilingual fine-tuning on 5,000 examples enables the model to learn idiom-specific orthographic, lexical, and morphosyntactic patterns. Joint training across all translation directions allows cross-directional transfer. Core assumption: the base LLM has sufficient cross-lingual capacity from pretraining; fine-tuning primarily adapts output to idiom-specific conventions. Evidence: "improving BLEU scores by 7.5 points on average" [abstract], GPT-4o-mini fine-tuned achieves 37.9 avg BLEU vs 30.4 baseline [section 5]. Break condition: if training data contains systematic noise or translationese artifacts, fine-tuning may amplify these biases.

## Foundational Learning

- **Concept: Parallel Corpora**
  - Why needed: The entire contribution depends on understanding what makes a corpus "parallel" (segment-level translations) vs. "comparable" (similar content, not translations). The Mediomatix source is comparable; alignment creates parallel data.
  - Quick check: If two textbooks teach the same topic but use completely different example sentences, can they yield parallel segments?

- **Concept: Embedding-Based Sentence Alignment**
  - Why needed: VecAlign uses sentence embeddings and dynamic programming to align sequences. Understanding that embeddings compress semantic content into vectors—and that cosine similarity measures alignment quality—is essential for debugging alignment failures.
  - Quick check: Why might embedding-based alignment fail for idioms with cognates but different meanings?

- **Concept: Precision vs. Recall Trade-offs in Data Creation**
  - Why needed: The paper explicitly optimizes for precision over recall. Table 2 shows consensus alignment sacrifices 5.1 recall points for 2.4 precision gain. This design choice affects downstream MT quality.
  - Quick check: If your downstream task is data augmentation, would you prioritize precision or recall?

## Architecture Onboarding

- **Component map:** Source (291 schoolbooks) -> CMS extraction -> HTML segmentation -> Cohere embed-v4.0 -> VecAlign (1-1 + deletions) -> Pivot consensus intersection -> Length filtering -> Output (207K segments)
- **Critical path:** Manual chapter title alignment → VecAlign per chapter pair → Pivot consensus intersection → Length filtering → Human validation
- **Design tradeoffs:** Precision-optimized filtering reduces corpus size by ~57% (481K → 207K segments); non-commercial license limits industrial application; grade-level splits create imbalanced train/test sizes
- **Failure signatures:** Empty rows in multi-parallel output (no consensus), high variance in segment lengths within a row (misalignment), low BLEU improvement after fine-tuning (noise or insufficient capacity)
- **First 3 experiments:**
  1. Reproduce alignment with different embedding models (sentence-swissBERT, Qwen3-Embedding) on validation set; compare precision/recall against Cohere baseline
  2. Ablate pivot consensus; measure quality using single-pivot vs consensus; quantify precision/recall and downstream MT impact
  3. Scale fine-tuning data; train on full corpus vs 5K subset with open-source NMT model; compare BLEU gains against GPT-4o-mini results

## Open Questions the Paper Calls Out

### Open Question 1
Can training an open-source NMT model on the full Mediomatix corpus outperform the fine-tuned commercial LLM baselines? The authors restricted experiments to fine-tuning GPT-4o-mini on 5,000 examples and did not benchmark traditional NMT architectures on the full dataset. Evidence: "Future work could experiment with training an open-source NMT model on the complete training split of Mediomatix."

### Open Question 2
How does scaling the fine-tuning dataset from 5,000 examples to the complete corpus affect translation performance and in-context learning capabilities of LLMs? The paper demonstrates viability with a small subsample but leaves potential performance gains from utilizing the full volume of extracted data unexplored. Evidence: "To manage the cost of this experiment, we limit fine-tuning to GPT-4o-mini, and we restrict the training set to 5000 examples."

### Open Question 3
To what extent does the "translationese" content in the Surmiran volumes limit the utility of the corpus for training robust MT systems? While the authors acknowledge this structural bias as a limitation, they do not quantify its impact on the quality or naturalness of resulting translations. Evidence: "The authors of the books report that content in the later additions... often translated from the earlier additions, meaning the Mediomatix dataset contains so-called 'translationese' to some extent."

## Limitations

- Corpus size reduction via precision filtering likely excludes valid parallel content, particularly for Surmiran where available schoolbooks were limited
- Human evaluation sample (472 segments) provides high confidence in precision but does not assess recall or corpus-level coverage
- Embedding-based alignment relies on cross-lingual transfer to Romansh idioms without explicit validation of embedding quality for these specific varieties
- MT evaluation uses a small fine-tuning dataset (5,000 examples) that may not fully capture the diversity of the full 207K-segment corpus

## Confidence

- **High Confidence**: Alignment precision (99.7% on 472 segments), corpus creation methodology, MT evaluation setup and results (37.9 avg BLEU for fine-tuned model)
- **Medium Confidence**: Generalization of embedding quality to all Romansh idioms, sufficiency of 5,000 examples for MT fine-tuning, length filtering effectiveness
- **Low Confidence**: True recall of parallel segments, coverage of rare vocabulary across all five idioms, long-term stability of fine-tuned model performance

## Next Checks

1. **Embedding Quality Validation**: Test alignment with alternative embedding models (sentence-swissBERT, Qwen3-Embedding) on the validation set to confirm Cohere embed-v4.0 performance is not an artifact of specific model choices.

2. **Pivot Consensus Ablation Study**: Measure alignment quality using each idiom as single pivot versus consensus approach to quantify the claimed 2.4 precision gain and assess whether this justifies the 5.1 recall reduction.

3. **Full Corpus MT Scaling**: Train an open-source NMT model (NLLB or mBART) on the complete 207K-segment corpus and compare BLEU improvements against the 5,000-example fine-tuning results to assess data scaling effects.