---
ver: rpa2
title: Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance
arxiv_id: '2505.21101'
source_url: https://arxiv.org/abs/2505.21101
tags:
- guidance
- distribution
- diffusion
- where
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Classifier-free guidance (CFG) enhances conditional diffusion\
  \ models but reduces sample diversity due to combining conditional and unconditional\
  \ denoisers without preserving the correct target distribution. This work identifies\
  \ that CFG lacks a R\xE9nyi divergence term that acts as a diversity-preserving\
  \ repulsive force."
---

# Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance

## Quick Facts
- arXiv ID: 2505.21101
- Source URL: https://arxiv.org/abs/2505.21101
- Reference count: 40
- One-line primary result: Classifier-free guidance reduces diversity in conditional diffusion models; CFG^IG iteratively refines samples to improve both quality and diversity

## Executive Summary
Classifier-free guidance (CFG) improves sample quality in conditional diffusion models but suffers from reduced diversity due to omitting a Rényi divergence term that acts as a diversity-preserving repulsive force. This work proposes Classifier-Free Gibbs-like Guidance (CFG^IG), which iteratively refines samples starting from a high-diversity no-guidance conditional model and applying CFG in the low-noise regime. By leveraging the fact that the missing divergence term vanishes as noise approaches zero, CFG^IG approximates a Gibbs sampler for the target tilted distribution, consistently outperforming standard CFG across multiple image and audio generation tasks.

## Method Summary
CFG^IG generates samples by first creating a diverse draft with low/no guidance, then iteratively adding noise to a low level σ* and denoising with high-guidance CFG. The algorithm runs for R repetitions, where each iteration adds noise to reach σ* then uses an ODE solver to denoise back to zero noise. Key hyperparameters include initial guidance w₀, number of iterations R, noise threshold σ*, and guidance strength w. The method uses pretrained conditional and unconditional denoisers without additional training.

## Key Results
- CFG^IG consistently outperforms standard CFG, limited-interval CFG, and CFG++ on ImageNet-512 and text-to-audio tasks
- Improves both perceptual quality and sample diversity across FID, FDDINOv2, Precision/Recall, and Density/Coverage metrics
- Achieves better trade-offs between alignment and diversity compared to baseline methods
- Demonstrated effectiveness on both image generation (EDM2 models) and text-to-audio synthesis (AudioLDM)

## Why This Works (Mechanism)

### Mechanism 1
Standard CFG reduces sample diversity because it implicitly drops a "repulsive force" term required to sample from the correct tilted distribution. The target distribution requires a score term proportional to the gradient of a Rényi divergence, which pushes samples away from high-density unconditional regions that conflict with the conditional constraint. CFG's linear combination of conditional/unconditional scores mathematically cancels this divergence term, causing sample collapse.

### Mechanism 2
The error introduced by the missing Rényi divergence term vanishes as the noise level σ approaches zero. The Rényi divergence gradient term is proportional to σ², so at high noise levels omitting this term creates significant trajectory errors. At low noise levels (clean data), the CFG denoiser becomes a valid approximation of the ideal denoiser because the missing term is negligible.

### Mechanism 3
Iteratively adding noise up to a low level σ* and denoising with CFG approximates a Gibbs sampler for the target tilted distribution. The algorithm creates a Markov chain that refines samples toward the target stationary distribution while maintaining diversity established in the initial step, since σ* is chosen to be small enough for CFG to be valid.

## Foundational Learning

- **Concept: Tweedie's Formula and Score Functions**
  - Why needed here: Understanding D_σ(x) ≈ x + σ²∇log p(x) is required to follow the derivation of the missing Rényi term
  - Quick check question: How does the denoiser output change if the underlying data distribution is a Gaussian vs. a mixture of Gaussians?

- **Concept: Tilted Distributions**
  - Why needed here: CFG implicitly tries to sample from π₀(x|c; w) ∝ p₀(x)g₀(c|x)ʷ, where w "tilts" the base distribution toward regions satisfying condition c
  - Quick check question: If w → ∞, what does the tilted distribution concentrate on?

- **Concept: Markov Chain Stationary Distributions**
  - Why needed here: CFG^IG is proven to work by defining a Markov chain whose stationary distribution is the desired tilted target
  - Quick check question: Why does starting the chain with a "no-guidance" sample (high diversity) help prevent the collapse seen in standard CFG?

## Architecture Onboarding

- **Component map:**
  Pre-trained Backbone -> Noise Scheduler -> CFG^IG Wrapper

- **Critical path:**
  1. Initialization: Generate draft sample X₀⁽⁰⁾ using low/no guidance (w₀) for high diversity
  2. Gibbs Loop (R times): Add noise to σ*, then denoise with CFG using high guidance w
  3. Output: Return refined sample X₀⁽ᴿ⁾

- **Design tradeoffs:**
  - σ* (Noise Level): Lower σ* reduces bias but slows mixing; higher σ* speeds refinement but risks collapse
  - R (Repetitions): More repetitions improve quality but increase compute cost; R=2 often optimal
  - w₀ (Initial Guidance): w₀=1 maximizes initial diversity; w₀ > 1 starts with better alignment

- **Failure signatures:**
  - "Burned" textures/Color shifts: σ* too high, behaving like standard high-guidance CFG
  - Stagnation/No refinement: σ* too low or insufficient ODE steps in Gibbs loop
  - High NFE cost: R too high relative to total step budget T

- **First 3 experiments:**
  1. Baseline Comparison: CFG^IG (R=1, σ*=2, w=2.5) vs Standard CFG (w=2.5) on ImageNet-256
  2. Ablation on σ*: Fix R=1, vary σ* ∈ [0.5, 5.0], plot FID vs Recall to find sweet spot
  3. Gibbs Iterations: Fixed NFE budget, compare R=1 vs R=2 vs R=4 for same compute

## Open Questions the Paper Calls Out

### Open Question 1
Can an efficient and stable training loss be derived that explicitly accounts for the Rényi divergence term, eliminating the need for heuristic noise threshold selection (σ* or σ_hi)? The paper identifies the missing Rényi divergence term but does not propose a training objective that incorporates it, with current methods relying on empirically tuned thresholds.

### Open Question 2
What is the quantitative relationship between the noise level σ* and the bias-variance trade-off in CFG^IG for non-Gaussian data distributions? The theoretical analysis focuses on a simplified Gaussian setting, while real-world distributions may exhibit different convergence behavior.

### Open Question 3
Can the two-noise-level score expression from Proposition 4 yield a guidance scheme that outperforms standard CFG^IG without requiring the unconditional denoiser? Preliminary D-CFG^IG experiments show improved FDDINOv2 but degraded FID compared to CFG^IG.

## Limitations
- Theoretical analysis relies on strong smoothness assumptions that may not hold for real-world data distributions
- Method's effectiveness depends critically on choosing appropriate hyperparameters (σ*, R, w) which are dataset and model-specific
- Computational overhead of multiple denoising passes may limit practical adoption for real-time applications

## Confidence
- **High Confidence**: Empirical improvements in sample quality and diversity metrics across multiple datasets are well-supported by quantitative evidence
- **Medium Confidence**: Theoretical mechanism explaining why CFG loses diversity is mathematically sound but relies on assumptions that may not fully capture real-world behavior
- **Medium Confidence**: Claim that CFG^IG behaves as a Gibbs sampler is theoretically justified but requires empirical validation

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary σ* and R across multiple orders of magnitude to map the full performance landscape and identify failure modes
2. **Theoretical Verification**: Test the smoothness assumptions on real data by analyzing the behavior of the missing Rényi divergence term across different datasets and noise levels
3. **Computational Efficiency Study**: Compare CFG^IG against alternative methods using a fixed compute budget to quantify the trade-off between quality gains and increased NFE requirements