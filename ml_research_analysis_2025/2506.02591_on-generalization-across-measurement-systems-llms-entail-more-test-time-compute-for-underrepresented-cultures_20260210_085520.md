---
ver: rpa2
title: 'On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute
  for Underrepresented Cultures'
arxiv_id: '2506.02591'
source_url: https://arxiv.org/abs/2506.02591
tags:
- measurement
- system
- systems
- llms
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  generalize across different measurement systems (e.g., currencies, lengths, weights)
  used in diverse cultural contexts. Using newly compiled datasets spanning fiscal
  data, food prices, and city distances across multiple countries, the authors find
  that LLMs default to the measurement system predominant in their training data (e.g.,
  USD for currency, metric/imperial units based on data context).
---

# On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures

## Quick Facts
- **arXiv ID**: 2506.02591
- **Source URL**: https://arxiv.org/abs/2506.02591
- **Reference count**: 12
- **Primary result**: LLMs show significant accuracy drops and increased compute costs when handling non-default measurement systems, especially for underrepresented cultures.

## Executive Summary
This study investigates whether large language models can generalize across different measurement systems used in diverse cultural contexts. Using newly compiled datasets spanning fiscal data, food prices, and city distances across multiple countries, the authors find that LLMs default to the measurement system predominant in their training data (e.g., USD for currency, metric/imperial units based on data context). When prompted with non-default systems, LLMs show significant performance drops—especially for underrepresented cultures—with accuracy gaps as large as 77% relative to default systems. Reasoning methods like chain-of-thought (CoT) improve performance but increase test-time compute by up to 300%, disproportionately affecting users from marginalized backgrounds.

## Method Summary
The authors created three novel datasets: Fiscal100 (99 examples of fiscal data across 9 countries), FoodPrices100 (100 examples of food prices across 5 countries), and CityDistances100 (101 examples of city distances across 8 countries). They tested three LLMs (GPT-4o, GPT-4o-mini, Llama-3.1-8B-Instruct) on these datasets using three prompting strategies: direct response, CoT, and few-shot CoT. Performance was measured using accuracy and exact match metrics across different measurement systems, with comparisons made between default systems (those predominant in training data) and non-default systems.

## Key Results
- LLMs default to measurement systems predominant in their training data (e.g., USD for currency, metric/imperial units based on data context)
- Performance drops up to 77% relative to default systems when prompted with non-default measurement systems
- Chain-of-thought prompting improves accuracy but increases test-time compute by up to 300%, disproportionately affecting marginalized users

## Why This Works (Mechanism)
The study reveals that LLMs' performance on measurement systems is strongly influenced by their training data composition. Models trained predominantly on Western-centric data struggle to generalize to measurement systems from underrepresented cultures, suggesting a fundamental limitation in cross-cultural generalization capabilities. The default behavior to use familiar measurement systems indicates that LLMs rely heavily on pattern matching from training data rather than true understanding of measurement system conversion or context.

## Foundational Learning
- **Cultural representation in training data**: Understanding which measurement systems appear in training data is crucial for predicting LLM behavior
  - *Why needed*: Explains why models default to certain systems and struggle with others
  - *Quick check*: Analyze training data distribution of measurement systems

- **Chain-of-thought reasoning**: A prompting technique where models explain their reasoning step-by-step
  - *Why needed*: Shows how reasoning methods can mitigate but not eliminate performance gaps
  - *Quick check*: Compare performance with and without CoT prompting

- **Computational cost trade-offs**: The relationship between reasoning depth and resource consumption
  - *Why needed*: Highlights the disproportionate burden on marginalized users
  - *Quick check*: Measure token usage and latency for different prompting strategies

## Architecture Onboarding
**Component map**: LLMs -> Measurement System Processing -> Output Generation -> Performance Evaluation

**Critical path**: Prompt → Model Inference → Measurement System Recognition → Calculation/Conversion → Response Generation → Accuracy Evaluation

**Design tradeoffs**: 
- Training data diversity vs. model size/cost
- Reasoning depth vs. computational efficiency
- Cultural coverage vs. dataset size constraints

**Failure signatures**: 
- Defaulting to familiar measurement systems when uncertain
- Systematic errors on underrepresented cultural contexts
- Increased computational overhead for accuracy gains

**First experiments**:
1. Test additional LLM architectures to determine if biases are model-specific or systemic
2. Conduct ablation studies on prompt phrasing and linguistic style
3. Expand dataset diversity to include more granular cultural contexts

## Open Questions the Paper Calls Out
None

## Limitations
- Datasets are relatively small (e.g., 99 examples in Fiscal100) and may not fully capture real-world diversity
- Focus on limited set of models without exploring broader architectural differences
- Uncertainty whether performance gaps stem from measurement system unfamiliarity or other confounding factors like linguistic style differences

## Confidence
- **High confidence**: LLMs default to measurement systems predominant in their training data when no explicit prompt guidance is provided
- **Medium confidence**: Performance drops are significant and measurable when LLMs are prompted with non-default measurement systems, particularly for underrepresented cultures
- **Medium confidence**: Chain-of-thought prompting improves accuracy but at a substantial computational cost, disproportionately affecting marginalized users

## Next Checks
1. Expand dataset size and diversity to include more granular cultural contexts and measurement system variants
2. Conduct ablation studies to isolate the impact of linguistic style, economic context, and prompt phrasing on performance disparities
3. Test additional LLM architectures and training paradigms to determine whether observed biases are model-specific or systemic across the field