---
ver: rpa2
title: Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement
  Learning via Graph Representations
arxiv_id: '2511.10872'
source_url: https://arxiv.org/abs/2511.10872
tags:
- graph
- learning
- state
- subgoal
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces G4RL, a graph-encoder-decoder-based method
  to enhance Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL). The method
  addresses sample inefficiency and poor subgoal representation by constructing a
  state graph from visited states and training a graph encoder-decoder to generate
  meaningful subgoal representations.
---

# Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations

## Quick Facts
- arXiv ID: 2511.10872
- Source URL: https://arxiv.org/abs/2511.10872
- Reference count: 29
- Primary result: G4RL improves sample efficiency and success rates in GCHRL by encoding spatial connectivity into subgoal representations via graph learning

## Executive Summary
This paper introduces G4RL, a method that enhances Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) by incorporating spatial information through graph representations. The approach constructs a state graph from visited states and trains a graph encoder-decoder to generate meaningful subgoal representations that capture spatial relationships and connectivity. By adding intrinsic rewards at both high and low levels based on distances in this subgoal space, G4RL significantly improves exploration efficiency and learning performance. Experiments on AntMaze and AntGather demonstrate substantial improvements over existing GCHRL methods, particularly in sparse reward settings.

## Method Summary
G4RL enhances GCHRL by constructing a state graph where nodes represent visited states and edges capture transition frequencies. A graph encoder-decoder network learns to embed states into a subgoal representation space that respects graph connectivity. The high-level policy receives intrinsic rewards based on graph-distance between current state and proposed subgoals, encouraging selection of reachable targets. The low-level policy receives similar intrinsic rewards to correct misleading state-space distances. The graph is adaptively updated during training, with the encoder-decoder retrained when sufficient changes occur. This approach is compatible with various HRL algorithms and adds only modest computational overhead.

## Key Results
- G4RL significantly improves success rates on AntMaze environments compared to vanilla HIRO, achieving faster convergence and higher final performance
- The method demonstrates strong performance in sparse reward settings, where traditional GCHRL methods struggle
- Intrinsic rewards at both hierarchy levels show complementary effects, with ablation studies confirming their combined importance
- Computational overhead remains modest, with sampling frequency adjustments offering further efficiency gains

## Why This Works (Mechanism)

### Mechanism 1: Graph-Embedded Subgoal Representations Capture True Reachability
- Claim: Subgoal representations learned via graph encoder-decoder encode spatial connectivity, enabling more accurate distance estimates than raw state-space Euclidean distance.
- Mechanism: The encoder E maps state representations ϕ(s) to subgoal representations g(s) = E(ϕ(s)). The decoder D computes dot-product similarity between pairs, trained to predict normalized adjacency weights A_ij/max(A). This forces connected states to have similar representations, embedding graph topology into the representation space.
- Core assumption: Environments have primarily symmetric and reversible transitions, allowing undirected graphs to meaningfully represent reachability.
- Evidence anchors:
  - [abstract]: "constructing a state graph from visited states and training a graph encoder-decoder to generate meaningful subgoal representations. These representations capture spatial relationships and connectivity in the state space."
  - [section 3.2]: "This loss function can enforce the subgoal representation provided by the encoder to respect neighbouring features in the graph."
  - [corpus]: Related work "Skeleton-Guided Learning for Shortest Path Search" explores graph-based path learning but does not address hierarchical RL integration.
- Break condition: If transitions are highly asymmetric or irreversible, undirected graph distances misrepresent true reachability.

### Mechanism 2: High-Level Intrinsic Reward Encourages Reachable Subgoal Selection
- Claim: Adding graph-based distance as intrinsic reward at the high level guides the agent to propose subgoals that are actually reachable, improving exploration efficiency.
- Mechanism: High-level reward: r_h = r_ext + αh · D(E(ϕ(st)), E(gt)). D computes similarity in the graph-embedded space, so the intrinsic term is higher when proposed subgoals are graph-connected to the current state, discouraging infeasible targets.
- Core assumption: Subgoal representations from the graph encoder meaningfully reflect reachability; the encoder is sufficiently trained.
- Evidence anchors:
  - [abstract]: "G4RL adds intrinsic rewards at both high and low levels based on distances in the subgoal space, improving exploration and learning efficiency."
  - [section 3.4]: "To encourage it to propose a subgoal that is not too difficult to reach from the current state st for more efficient exploration, we add an intrinsic term to the high-level reward"
  - [corpus]: No direct corpus evidence on hierarchical intrinsic reward design; this is a novel contribution.
- Break condition: If αh is too large, the agent may prioritize easy subgoals over task-relevant ones; if too small, the effect diminishes.

### Mechanism 3: Low-Level Intrinsic Reward Corrects Misleading State-Space Distances
- Claim: Augmenting low-level intrinsic reward with graph-based distance compensates for cases where Euclidean state-space distance poorly reflects true progress toward subgoals.
- Mechanism: Low-level reward: r_l = -||ϕ(st+1) - gt||² + αl · D(E(ϕ(st+1)), E(gt)). The graph-based term provides higher reward when progress is made along graph-connected paths, even if Euclidean distance is uninformative.
- Core assumption: The graph captures the relevant connectivity for the task; low-level actions operate on timescales where graph structure is meaningful.
- Evidence anchors:
  - [section 3.4]: "By computing the intrinsic reward in the subgoal space rather than in the state space, the function provides high values when proposed subgoals are easy to reach from the current location"
  - [section 1]: "the low-level agent is trained solely by the reward signals derived from the distances between the current state and subgoals in the representation space, making the reward signals highly susceptible to poor subgoal representations."
  - [corpus]: "Strict Subgoal Execution" addresses subgoal infeasibility but via planning constraints, not intrinsic rewards.
- Break condition: If graph topology is incomplete or noisy (early training), the correction may be misleading.

## Foundational Learning

- Concept: Goal-Conditioned Hierarchical RL (GCHRL)
  - Why needed here: G4RL is an add-on to existing GCHRL methods; you must understand two-level policies (high-level proposes subgoals, low-level executes actions) before integrating the graph module.
  - Quick check question: Can you describe how the high-level policy πh(gt|ϕ(st)) and low-level policy πl(at|ϕ(st), gt) interact in HIRO?

- Concept: Graph Representation Learning (Encoder-Decoder)
  - Why needed here: The core contribution is a graph encoder-decoder trained to predict adjacency from node embeddings. Understanding link prediction objectives is essential.
  - Quick check question: How does minimizing reconstruction loss of adjacency weights force embeddings to encode connectivity?

- Concept: Intrinsic Motivation in RL
  - Why needed here: G4RL injects novelty/distance-based intrinsic rewards at both hierarchy levels. You should understand how intrinsic rewards drive exploration.
  - Quick check question: What is the difference between curiosity-driven intrinsic reward and the graph-distance-based intrinsic reward used here?

## Architecture Onboarding

- Component map:
  - State Graph G=(V,E): Fixed N nodes (e.g., 200), node features = ϕ(s), edge weights = transition counts
  - Graph Encoder E: 4-layer FFN (hidden=128, ReLU), maps ϕ(s) → g(s) ∈ R^d
  - Graph Decoder D: Dot product D(g(su), g(sv)) = g(su)^T g(sv), trained to reconstruct normalized adjacency
  - High-Level Agent πh: Proposes subgoals every K steps, trained with r_h = r_ext + αh · D(E(ϕ(st)), E(gt))
  - Low-Level Agent πl: Executes atomic actions, trained with r_l = -||ϕ(st+1)-gt||² + αl · D(E(ϕ(st+1)), E(gt))
  - Adaptive Scheduler: Counter c tracks graph changes (node replacement adds N-1, edge update adds 1); triggers encoder training when c ≥ β·(N²-N)

- Critical path:
  1. Initialize graph with empty nodes; explore with random policies
  2. As states are visited, populate nodes (threshold ϵd) and update edge weights
  3. Once graph is full, begin training encoder-decoder using adjacency reconstruction loss
  4. Every time c exceeds threshold, perform one gradient update on encoder-decoder
  5. At each high-level step, sample subgoal gt; compute intrinsic reward using D(E(ϕ(st)), E(gt))
  6. At each low-level step, execute action; compute intrinsic reward using D(E(ϕ(st+1)), E(gt))
  7. Update πh, πl using any policy-based algorithm (e.g., TD3)

- Design tradeoffs:
  - Graph size N: Larger N captures more states but quadratically increases adjacency matrix cost. Paper uses N=200
  - Sampling interval t_c: Reducing graph update frequency (e.g., every 5 steps instead of 1) speeds up training with minor performance loss
  - Training data subsampling: Using 50-75% of node pairs for encoder training has negligible impact on performance but saves computation
  - Undirected vs directed graph: Undirected is simpler and works for symmetric environments; fails to model irreversible transitions accurately

- Failure signatures:
  - Success rate stagnates early: Check if graph is too sparse (ϵd too strict) or encoder training starts too late
  - High-level proposes unreachable subgoals: αh may be too low, or encoder is undertrained
  - Low-level reward is uninformative: αl may be too low, or graph connectivity is incomplete
  - Training time doubles: Reduce sampling frequency or encoder training data

- First 3 experiments:
  1. Replicate AntMaze success rate curve (Figure 1a) using HIRO+G4RL vs vanilla HIRO; verify convergence improvement
  2. Ablate high-level vs low-level intrinsic rewards (Figures 4-7) on AntMaze-Sparse to confirm complementary roles
  3. Test on AntPush (asymmetric environment) to observe performance degradation; verify robustness claim from Section 4.1

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades significantly on asymmetric environments (AntPush, AntFall) due to reliance on undirected graph construction
- State representation ϕ(s) is only vaguely specified as containing "spatial information," leaving critical implementation details ambiguous
- The computational overhead claim lacks precise characterization and scaling analysis for larger state spaces

## Confidence
- **High Confidence:** The mechanism by which graph-embedded subgoal representations improve distance estimation in symmetric environments is well-supported by experimental results on AntMaze and AntGather
- **Medium Confidence:** The claim that intrinsic rewards at both hierarchy levels are complementary is supported by ablation studies, but the optimal balance between αh and αl is not explored
- **Low Confidence:** The robustness claims regarding performance on asymmetric environments are based on limited evidence (AntPush/AntFall results show degradation but no systematic analysis of failure modes)

## Next Checks
1. **Asymmetric Environment Stress Test:** Systematically evaluate G4RL on environments with varying degrees of transition asymmetry (e.g., slippery surfaces, one-way doors) to quantify the relationship between transition reversibility and performance degradation

2. **State Representation Sensitivity Analysis:** Test G4RL with different state subsets (varying dimensionality and spatial coverage) to determine the minimum viable representation and identify which spatial features are most critical for graph construction

3. **Graph Topology Robustness:** Compare undirected vs. directed graph variants on environments with known irreversible transitions to measure the performance gap and validate whether simple directed graphs can mitigate the current limitation