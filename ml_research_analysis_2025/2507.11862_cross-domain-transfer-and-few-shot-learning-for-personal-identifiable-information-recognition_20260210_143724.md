---
ver: rpa2
title: Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information
  Recognition
arxiv_id: '2507.11862'
source_url: https://arxiv.org/abs/2507.11862
tags:
- data
- performance
- i2b2
- domains
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recognizing personally identifiable
  information (PII) across different text domains for privacy protection. The authors
  evaluate cross-domain model transfer, multi-domain data fusion, and sample-efficient
  learning using datasets from healthcare (I2B2), legal (TAB), and biographical (Wikipedia)
  domains.
---

# Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition

## Quick Facts
- arXiv ID: 2507.11862
- Source URL: https://arxiv.org/abs/2507.11862
- Authors: Junhong Ye; Xu Yuan; Xinying Qiu
- Reference count: 15
- Primary result: Legal-domain PII models transfer effectively to biographical texts (F1=0.847) but fail on medical domains (F1=0.264)

## Executive Summary
This paper investigates cross-domain PII recognition using three datasets: I2B2 (medical), TAB (legal), and Wikipedia (biographical). The authors evaluate model transfer across domains, multi-domain data fusion, and sample-efficient learning. They find that specialized domains provide effective outgoing transfer but resist incoming transfer, multi-domain fusion benefits vary significantly by domain, and low-specialization domains can achieve high performance with substantially reduced training data. Legal documents maintain 97% performance using only 50 samples, while biographical texts peak at 44% of full dataset.

## Method Summary
The study uses Longformer-base with 4096-token attention windows for processing long documents. Text is tokenized using SpaCy and converted to BIO tagging format. Three datasets are used: I2B2 (790 train/514 test, 23 entity types), TAB (1,014 train/127 val/127 test, 8 types), and Wikipedia (453 train/100 test, 7 types). Four experiment types are conducted: in-domain training, cross-domain transfer, multi-domain fusion, and few-shot sampling at 50/100/200/300/500+ samples with 7 random seeds each.

## Key Results
- Medical-to-biographical transfer achieves F1=0.828 while biographical-to-medical fails (F1=0.187)
- Legal-domain models achieve F1=0.847 on biographical texts but only F1=0.264 on medical domains
- Multi-domain fusion improves TAB recall (+1.6%) but degrades I2B2 performance (-0.5%)
- Wikipedia PII recognition peaks at 200 samples (F1=0.845) before declining with additional data

## Why This Works (Mechanism)

### Mechanism 1: Domain Complexity Hierarchy Enables Asymmetric Transfer
- Claim: Transfer effectiveness depends on source-target domain complexity gaps, with specialized domains providing effective outgoing transfer but resisting incoming transfer.
- Mechanism: High-specialization domains (medical: 23 entity types, clinical vocabulary) develop transferable features that generalize to lower-specialization domains (biographical: 7 types, general vocabulary). The reverse fails because low-specialization training lacks exposure to domain-specific entities and patterns.
- Core assumption: Entity type granularity and vocabulary specialization proxy for domain complexity.
- Evidence anchors:
  - [abstract] "medical domains resist incoming transfer"
  - [Section 4.2] Medical training achieves F1=0.828 on Wikipedia (0% drop), while Wikipedia→I2B2 drops 80.8% (F1=0.187). TAB→Wikipedia gains +2.3% but TAB→I2B2 drops 72.8%.
  - [corpus] Weak direct validation. Neighbor papers focus on LLM memorization and network traffic detection, not cross-domain NER transfer dynamics.

### Mechanism 2: Multi-Domain Fusion Improves Recall Through Entity Coverage Expansion
- Claim: Combining heterogeneous training data increases recall by exposing models to broader entity surface forms, with domain-specific F1 tradeoffs.
- Mechanism: Fusion introduces additional entity mentions and surface variations (e.g., TAB+Wikipedia+I2B2 = 194,587 mentions vs. 28,867 for I2B2 alone). For receptive domains (TAB, Wikipedia), this coverage gain outweighs distribution mismatch. For high-specialization domains (I2B2), noise from mismatched entity distributions degrades precision.
- Core assumption: Entity coverage gains translate to recall improvements; distribution mismatch primarily affects precision.
- Evidence anchors:
  - [Section 4.3] TAB three-domain fusion: recall +1.6% (0.952), F1 -0.7%. Wikipedia TWI: F1 +2.5%. I2B2: all fusion combinations reduce F1 (-0.2% to -0.5%).
  - [abstract] "fusion benefits are domain-specific"
  - [corpus] No direct validation. Related work on adaptive PII mitigation (arxiv:2501.12465) suggests domain-specific handling but doesn't test fusion.

### Mechanism 3: Non-Monotonic Sample Efficiency in Low-Specialization Domains
- Claim: Beyond a threshold, additional training data can degrade performance in low-complexity domains due to annotation noise or overfitting to spurious patterns.
- Mechanism: Low-specialization domains have limited entity type diversity and simpler surface patterns. Models rapidly learn core patterns (50-200 samples), after which additional samples introduce inconsistent annotations or edge cases that reduce generalization. High-specialization domains (medical: 23 types, complex vocabulary) show monotonic improvement.
- Core assumption: Annotation consistency varies with dataset size; entity type count correlates with sample requirements.
- Evidence anchors:
  - [Section 4.4] Wikipedia peaks at 200 samples (F1=0.845, +2.1% vs. full 453 samples). TAB peaks at 300 samples (-0.8% vs. full). I2B2 requires full 790 samples, showing monotonic improvement.
  - [abstract] "high-quality recognition is achievable with only 10% of training data in low-specialization domains"
  - [corpus] Not validated. Corpus papers don't address sample efficiency in PII recognition.

## Foundational Learning

- Concept: **BIO Tagging for Sequence Labeling**
  - Why needed here: All entity spans converted to BIO format (Section 3.1). Understanding B-PERSON, I-PERSON, O tagging is required to interpret token-level F1 scores and why precision/recall tradeoffs occur at entity boundaries.
  - Quick check question: Given "Dr. Jane Smith visited," what BIO tags would mark the name as a single entity?

- Concept: **Transformer Attention Windows**
  - Why needed here: Longformer selected specifically for 4096-token attention window vs. standard 512 tokens (Section 3.2). Legal documents average 1,442 tokens, making architecture choice critical for capturing document-level context.
  - Quick check question: Why would a 512-token limit cause PII recognition failures in legal documents?

- Concept: **Transfer Learning Direction Asymmetry**
  - Why needed here: Results show transfer is not symmetric—I2B2→Wikipedia works (F1=0.828), but Wikipedia→I2B2 fails (F1=0.187). This challenges naive assumptions about domain adaptation and informs data collection priorities.
  - Quick check question: If you have labeled medical PII data and need to anonymize legal documents, should you expect effective transfer?

## Architecture Onboarding

- Component map:
  Tokenizer -> Longformer-base (4096 tokens) -> Token-level classifier (BIO labels) -> Evaluation (token-level F1)

- Critical path:
  1. Data preparation: Convert entity spans → BIO tags; verify label alignment across domains
  2. Model selection: Use Longformer for documents >512 tokens; RoBERTa acceptable for short texts
  3. Training strategy: Domain-specific for medical (I2B2); multi-domain fusion (TWI) for legal/biographical
  4. Sample size: Start with 200-300 samples for legal/biographical; require full dataset for medical

- Design tradeoffs:
  - **Recall vs. Precision**: Fusion improves recall (+1.6% for TAB) but reduces precision (-2.6%); for privacy applications, prioritize recall to minimize missed PII
  - **Speed vs. Coverage**: Longformer handles long documents but is slower than RoBERTa; use RoBERTa for short texts (<512 tokens)
  - **Data vs. Annotation Cost**: Legal documents achieve 97% performance with 50 samples vs. 1,014 full; weigh annotation cost against 3% performance gain

- Failure signatures:
  - **Cross-domain collapse**: F1 <0.3 when transferring into medical domain (expected; use domain-specific training)
  - **Fusion degradation**: F1 drop when adding non-medical data to medical training (expected; avoid fusion for medical)
  - **Precision collapse**: Very low precision (0.10-0.26) with high recall (0.99+) indicates model over-predicting PII; reduce training data or add negative examples

- First 3 experiments:
  1. **Baseline validation**: Train Longformer on TAB (1,014 samples), evaluate on TAB test set; target F1 >0.85 to reproduce paper results
  2. **Transfer probe**: Train on Wikipedia (453 samples), test on TAB; if F1 <0.5, confirm transfer direction matters and proceed with TAB→Wikipedia instead
  3. **Sample efficiency curve**: Train on TAB with [50, 100, 200, 300] samples; verify non-monotonic pattern peaks around 300 samples before committing to full annotation effort

## Open Questions the Paper Calls Out
- Can cross-domain PII recognition approaches generalize to conversational and social media domains, which exhibit different linguistic patterns and entity distributions?
- Can synthetic data generation enable privacy-preserving few-shot learning for PII recognition without compromising model performance?
- What underlying factors cause the medical domain to resist incoming transfer while providing effective outgoing transfer to other domains?
- Why does performance exhibit non-monotonic relationships with training data size in some domains (e.g., Wikipedia peaking at 44% of full data)?

## Limitations
- Critical hyperparameters (learning rate, batch size, epochs) remain unspecified, preventing faithful reproduction
- Entity mapping strategy across datasets with different type granularities (23 vs 8 vs 7 types) is not documented
- Cross-domain transfer results show concerning variability with 140% performance swings that lack statistical validation

## Confidence
- High Confidence: Domain complexity hierarchy enables asymmetric transfer (I2B2→Wikipedia F1=0.828 vs Wikipedia→I2B2 F1=0.187)
- Medium Confidence: Multi-domain fusion improves recall through entity coverage expansion, though medical domain degradation mechanism unclear
- Low Confidence: Non-monotonic sample efficiency in low-specialization domains (Wikipedia 2.1% F1 drop from 200→453 samples lacks statistical validation)

## Next Checks
1. Document and test the entity type mapping strategy across the three datasets to validate F1 score comparisons
2. Replicate the Wikipedia→I2B2 transfer experiment five times with different random seeds to assess stability
3. Perform statistical analysis of sample efficiency results to determine whether non-monotonic patterns are significant or due to random variation