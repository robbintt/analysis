---
ver: rpa2
title: 'iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous
  Improvement'
arxiv_id: '2507.22896'
source_url: https://arxiv.org/abs/2507.22896
tags:
- robot
- user
- question
- round
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents iLearnRobot, an interactive learning-based
  multi-modal robot system powered by a Multi-modal Large Language Model (MLLM). The
  system addresses the challenge of improving robot performance after deployment in
  novel scenarios by enabling robots to learn from natural dialogues with non-expert
  users.
---

# iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement

## Quick Facts
- arXiv ID: 2507.22896
- Source URL: https://arxiv.org/abs/2507.22896
- Authors: Kohou Wang; ZhaoXiang Liu; Lin Bai; Kun Fan; Xiang Liu; Huan Hu; Kai Wang; Shiguo Lian
- Reference count: 19
- Primary result: Interactive learning robot improves accuracy from 28.8% to 71.6% across three rounds using natural dialogue and continuous fine-tuning

## Executive Summary
This paper presents iLearnRobot, an interactive learning-based multi-modal robot system powered by a Multi-modal Large Language Model (MLLM). The system addresses the challenge of improving robot performance after deployment in novel scenarios by enabling robots to learn from natural dialogues with non-expert users. Through three key components - iterative questioning for intent clarification, dual-modality retrieval for immediate correction, and data distillation for model updates - the system demonstrates significant performance improvements in recognizing and responding to unfamiliar objects like medicine bottles.

## Method Summary
iLearnRobot employs a three-stage architecture: (1) a chain of question module that clarifies user intent through iterative dialogue before answering, (2) a dual-modality retrieval module that leverages past interaction events to avoid repeating mistakes, and (3) a data construction module that distills interaction dialogues into structured format for model training. The system uses LLaVA-NeXT as the base MLLM, implements LoRA fine-tuning for efficient model updates, and explores both frozen and unfrozen visual encoder configurations to optimize performance on novel scenarios.

## Key Results
- Accuracy improved from 28.8% to 71.6% across three rounds of testing with 10 medicine bottles
- User satisfaction scores increased from 3.34 to 7.1 out of 10
- Retrieval-only condition achieved 61.6% accuracy without any model updates
- Fine-tuning with unfrozen visual encoder achieved 71.6% accuracy vs 65.8% with frozen encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative questioning before answering improves intent clarification in ambiguous user queries.
- Mechanism: The chain of question module prompts the MLLM to ask follow-up questions when the initial query lacks specificity, collecting additional context across multiple dialogue turns before attempting to answer. This reduces incorrect assumptions about user intent.
- Core assumption: Users can articulate their true intent when prompted with clarifying questions, and the MLLM can determine when sufficient information has been gathered.
- Evidence anchors:
  - [abstract] "chain of question to clarify the exact intent of the question before providing an answer"
  - [section 3.1] "This iterative process continues until the robot can confidently ascertain the user's intent"
  - [corpus] Weak external validation; no comparable systems in corpus explicitly test iterative questioning for intent clarification
- Break condition: If users cannot provide clearer descriptions when prompted, or if the MLLM cannot reliably detect when intent is resolved, the module may loop indefinitely or still produce irrelevant responses.

### Mechanism 2
- Claim: Dual-modality retrieval enables immediate performance gains between model updates by referencing corrected past interactions.
- Mechanism: The system encodes both the cropped subject region (via CLIP image encoder) and the distilled question (via CLIP text encoder) into embeddings. For new queries, cosine similarity matching retrieves relevant past events, and their correct answers condition the final response generation.
- Core assumption: Similar visual subjects and similar questions map to similar correct answers; the embedding space captures this relationship sufficiently.
- Evidence anchors:
  - [abstract] "dual-modality retrieval modules to leverage these interaction events to avoid repeating same mistakes"
  - [section 3.1] "If the cosine similarities each meet a certain specific threshold, it is considered a successful retrieval"
  - [section 4.1] Round 2 retrieval-only condition improved accuracy from 28.8% to 61.6% without any model updates
  - [corpus] No direct corpus comparison; related work (SIME, SymBridge) addresses self-improvement but not retrieval-based short-term correction
- Break condition: If novel objects share visual similarity with incorrect database entries, or if threshold settings are too permissive, retrieval may return misleading references.

### Mechanism 3
- Claim: Fine-tuning the visual encoder alongside the LLM improves adaptation to novel scenarios with subtle visual differences.
- Mechanism: Standard MLLM fine-tuning freezes the visual encoder while updating the projection layer and LLM. For highly novel scenarios (e.g., distinguishing similar medicine bottles), the paper hypothesizes that updating visual encoder weights enables better feature extraction for fine-grained discrimination.
- Core assumption: Novel scenarios contain visual distinctions that the pre-trained visual encoder cannot adequately capture without weight updates.
- Evidence anchors:
  - [section 4.1] Ablation shows updatable visual encoder achieved 71.6% accuracy vs 65.8% with frozen encoder
  - [section 4.1] "updating the weights of the visual encoder simultaneously can help to extract more informative features"
  - [corpus] No corpus papers test this specific fine-tuning variation
- Break condition: If fine-tuning data is limited or noisy, unfreezing the visual encoder may cause overfitting or catastrophic forgetting of general visual knowledge.

## Foundational Learning

- Concept: Multi-modal Large Language Models (MLLMs)
  - Why needed here: The entire system builds on LLaVA-NeXT for visual understanding, reasoning, and natural language generation.
  - Quick check question: Can you explain how an MLLM processes both image and text inputs through a projection layer to a shared LLM backbone?

- Concept: CLIP embeddings and cosine similarity
  - Why needed here: The dual-modality retrieval module relies entirely on CLIP image/text encoders and cosine similarity for matching queries to past events.
  - Quick check question: Given two normalized embedding vectors, how would you compute their cosine similarity, and what value indicates maximum similarity?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: Model updates use LoRA to efficiently adapt the MLLM without full parameter retraining.
  - Quick check question: What is the key advantage of LoRA over full fine-tuning in terms of trainable parameters and memory requirements?

## Architecture Onboarding

- Component map:
  - **Chain of Question Module**: Prompt-based iterative clarification loop within LLaVA-NeXT
  - **Dual-Modality Retrieval Module**: CLIP encoders → embedding storage → cosine similarity search → history event database
  - **Data Construction Module**: Distills multi-turn dialogues into (question, bounding box, correct answer) triples
  - **Response Generation Module**: Conditions final output on retrieved past events (if available) plus current context
  - **Model Update Pipeline**: LoRA fine-tuning on accumulated distilled data; decision on visual encoder freezing

- Critical path: User query → Chain of Question (clarification turns) → Bounding box extraction + question distillation → CLIP encoding → Dual-modality retrieval → Response generation (with or without retrieved reference) → Dialogue storage → Data distillation → Periodic model fine-tuning

- Design tradeoffs:
  - Retrieval provides immediate improvement but increases inference latency and requires storage
  - Fine-tuning with unfrozen visual encoder improves novel-scenario accuracy but risks overfitting on small datasets
  - Chain of question improves intent resolution but adds interaction turns, potentially reducing user patience

- Failure signatures:
  - Retrieval variance spike (Round 2 showed variance 8.40 vs 4.90 baseline) indicates inconsistent retrieval utility
  - If bounding box extraction fails, retrieval uses wrong visual region
  - If threshold too low, irrelevant past events mislead responses; if too high, retrieval never triggers

- First 3 experiments:
  1. **Baseline capability test**: Present the base MLLM with novel objects (e.g., unfamiliar product bottles) without any retrieval or fine-tuning; measure raw accuracy and user satisfaction scores
  2. **Retrieval-only test**: Enable dual-modality retrieval after collecting one round of interaction data; verify that cosine similarity thresholds successfully retrieve relevant corrections and measure accuracy gain without any model weight updates
  3. **Fine-tuning ablation**: Compare frozen vs unfrozen visual encoder fine-tuning on the same collected data; measure accuracy, score, and variance differences to validate the paper's finding that visual encoder updates benefit novel-scenario adaptation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation dataset using only 10 medicine bottles, potentially not generalizable to broader real-world applications
- Exclusive focus on Chinese user queries raises questions about cross-lingual performance
- Reliance on iterative questioning may increase interaction time and frustrate users preferring immediate responses
- Long-term effectiveness not validated beyond three rounds, leaving questions about model drift over extended deployment

## Confidence
- **High confidence**: The dual-modality retrieval mechanism's effectiveness in improving accuracy without model updates (61.6% vs 28.8% baseline) is well-supported by controlled experiments and shows consistent improvement across multiple test rounds.
- **Medium confidence**: The claim that unfreezing the visual encoder provides superior adaptation to novel scenarios is supported by ablation results but requires validation on more diverse object categories beyond medicine bottles.
- **Low confidence**: The assertion that iterative questioning consistently improves user satisfaction across all interaction contexts is based on limited user feedback data and may not hold for time-sensitive applications or impatient users.

## Next Checks
1. **Cross-domain generalization test**: Evaluate iLearnRobot's performance on a diverse set of object categories (household items, electronics, clothing) beyond medicine bottles to assess whether retrieval and fine-tuning mechanisms generalize across visual domains.

2. **Cross-lingual capability validation**: Test the system with non-Chinese queries to determine whether the MLLM's iterative questioning and retrieval mechanisms function effectively across different languages and cultural contexts.

3. **Long-term deployment study**: Deploy the system for extended periods (e.g., 30+ days) with continuous user interactions to measure whether accuracy improvements plateau, decline, or show sustainable growth, and assess user satisfaction over time.