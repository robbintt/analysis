---
ver: rpa2
title: Incorporating Token Usage into Prompting Strategy Evaluation
arxiv_id: '2505.14880'
source_url: https://arxiv.org/abs/2505.14880
tags:
- prompting
- token
- usage
- fewshot
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Big-Otok, a theoretical framework to analyze\
  \ token usage growth in prompting strategies, and Token Cost (TC), a metric quantifying\
  \ tokens per performance point. The authors evaluate common strategies\u2014Vanilla\
  \ IO, Zeroshot CoT, Vanilla Fewshot, Fewshot CoT, and CoT-SC\u2014across three benchmarks\
  \ and three models."
---

# Incorporating Token Usage into Prompting Strategy Evaluation

## Quick Facts
- arXiv ID: 2505.14880
- Source URL: https://arxiv.org/abs/2505.14880
- Reference count: 40
- Primary result: Introduces Big-Otok framework and Token Cost metric to analyze and compare prompting strategy efficiency, finding that increased token usage yields diminishing accuracy gains with efficiency dropping over 20× across strategy complexities.

## Executive Summary
This paper introduces Big-Otok, a theoretical framework applying asymptotic analysis to describe token usage growth in prompting strategies, and Token Cost (TC), a metric quantifying tokens per accuracy percentage point. The authors evaluate five common prompting strategies—Vanilla IO, Zeroshot CoT, Vanilla Fewshot, Fewshot CoT, and CoT-SC—across three benchmarks and three models. Their findings show that while increased token usage improves accuracy, the gains diminish sharply, with efficiency dropping over 20× from low- to high-complexity strategies. Big-Otok predictions align closely with empirical data, validating its utility for efficiency-aware prompt engineering in real-world LLM deployments.

## Method Summary
The paper derives Big-Otok complexity classes (O(1), O(k), O(pk)) for five prompting strategies by identifying scaling variables like few-shot examples (k) and sampled reasoning chains (p). Token Cost is computed as total tokens divided by accuracy percentage. Experiments run on Llama 3.1 8B and Qwen 2.5 14B/32B models using LM Evaluation Harness across BBH, GSM8K, and MMLU benchmarks. The authors collect combined input+output token counts and accuracy, then validate Big-Otok by comparing theoretical vs. observed token usage ratios between strategies.

## Key Results
- Big-Otok successfully categorizes prompting strategies into constant O(1), linear O(k), and polynomial O(pk) complexity classes
- Token Cost reveals over 20× efficiency drop between lowest and highest performing strategies, with diminishing returns following log(log(x)) curves
- Empirical token usage ratios align with Big-Otok theoretical predictions despite noise from chat templates and model idiosyncrasies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting strategies exhibit predictable, categorizable token usage growth patterns that can be described mathematically.
- Mechanism: Big-Otok applies asymptotic analysis (analogous to time complexity in software engineering) to prompting strategies by identifying variables that scale token usage—such as the number of few-shot examples (k) or sampled reasoning chains (p)—and deriving the dominant growth term. Strategies fall into three complexity classes: constant O(1) (e.g., Vanilla IO, Zeroshot CoT), linear O(k) (e.g., Fewshot CoT), and polynomial O(pk) (e.g., CoT-SC).
- Core assumption: Token usage growth is dominated by structural variables in the prompting strategy, with lower-order terms and constants becoming negligible as variables increase; this holds even for relatively small variable values (k ≤ 100).
- Evidence anchors:
  - [abstract]: "we propose Big-Otok, a theoretical framework for describing the token usage growth of prompting strategies"
  - [section 3.1]: Table 1 and text explicitly derive O(1), O(k), and O(pk) for each strategy, with derivations in Appendix A.
  - [corpus]: Weak direct support; related work addresses token reduction or prompting evaluation but not the Big-Otok formalism specifically.
- Break condition: If prompting strategies introduce fundamentally different token scaling mechanisms (e.g., adaptive token budgets, recursive self-prompting), the current three-class taxonomy may not capture their growth patterns.

### Mechanism 2
- Claim: Token Cost (TC)—tokens per percentage point of accuracy—provides a single-number efficiency metric that surfaces sharply diminishing returns as prompting complexity increases.
- Mechanism: TC is computed as (total tokens) / (accuracy %). Average TC compares overall efficiency; marginal TC measures incremental tokens needed for incremental accuracy gains between strategies. The paper observes log(log(x))-shaped curves: early token investments yield proportionally larger accuracy gains, while later investments yield progressively smaller returns.
- Core assumption: Accuracy gains from additional tokens follow a saturating (diminishing-returns) function rather than linear improvement, at least within the evaluated strategy families (IO, CoT, CoT-SC).
- Evidence anchors:
  - [abstract]: "Token Cost (TC), a metric quantifying tokens per performance point" and "gains diminish sharply, with efficiency dropping over 20×"
  - [section 4]: "average TC for the prompting strategy with the lowest accuracy is 5.0 t/p, while that of the highest performing... is 119.4 t/p, a more than 20x increase"; marginal TC increases from 65.3 t/p to 6701.8 t/p across strategy ranges.
- Break condition: If accuracy and token usage are not monotonically related (e.g., more tokens yield lower accuracy due to prompt confusion), TC interpretation becomes invalid or requires reformulation.

### Mechanism 3
- Claim: Empirical token usage ratios between strategies align with Big-Otok theoretical predictions, validating the framework's descriptive power.
- Mechanism: The authors derive theoretical token usage ratios from Big-Otok expressions (e.g., CoT-SC10 / Fewshot CoT = 10p / 1 = 10×), then compare to observed ratios from experiments across benchmarks and models. Observed ratios follow the predicted ordering and approximate magnitude, despite noise from chat templates, model idiosyncrasies, and low variable values.
- Core assumption: Noise factors are either constant across strategies or small enough not to disrupt relative ordering; the theoretical complexity class (constant < linear < polynomial) dominates observed behavior even at modest variable values.
- Evidence anchors:
  - [section 4]: Table 2 shows theoretical vs. observed ratios (e.g., Vanilla IO → CoT-SC10: 50× theoretical, 29.3× observed), with text noting alignment to complexity classes: "T Otok(1),b,m() < T Otok(k),b,m(k) < T Otok(pk),b,m(p, k)".
  - [corpus]: No direct corroboration; this is a novel empirical validation introduced in the paper.
- Break condition: If models or benchmarks exhibit fundamentally different token scaling behaviors (e.g., token budgets that adapt per-instance), the observed ratios may deviate significantly from Big-Otok predictions.

## Foundational Learning

- Concept: Big-O notation and asymptotic analysis
  - Why needed here: Big-Otok directly borrows from algorithmic complexity analysis; understanding dominant terms, growth classes, and simplification is essential to derive and interpret token complexity expressions.
  - Quick check question: Given a prompting strategy that uses 5 + 3k + 2k² tokens (where k is the number of few-shot examples), what is its Big-Otok complexity class?

- Concept: Common prompting strategies (IO, Few-shot, CoT, CoT-SC)
  - Why needed here: The paper evaluates a specific lineage of strategies that build on each other; knowing their structure (e.g., CoT adds reasoning chains, CoT-SC samples multiple chains) clarifies where token scaling variables originate.
  - Quick check question: Which variable(s) cause CoT-SC to have polynomial O(pk) complexity rather than linear O(k)?

- Concept: Tokenization and LLM inference economics
  - Why needed here: TC is a proxy for real-world cost; understanding why output tokens are more expensive than input tokens (autoregressive generation) and how token counts map to latency/cost grounds the metric in deployment realities.
  - Quick check question: If a prompting strategy increases input tokens by 100 and output tokens by 50, which increase is likely more expensive at inference time, and why?

## Architecture Onboarding

- Component map:
  - Big-Otok derivation module: maps prompting strategy description → variable identification → complexity expression (e.g., O(k), O(pk))
  - TC computation module: accepts (total tokens, accuracy) → returns average TC; accepts (tokens₁, accuracy₁, tokens₂, accuracy₂) → returns marginal TC
  - Experimental harness: orchestrates benchmark runs across strategies/models, collects token counts and accuracy, normalizes outputs
  - Validation comparator: computes theoretical vs. observed token usage ratios, checks ordering consistency with complexity classes

- Critical path:
  1. Formalize the prompting strategy in natural language, identifying all variables that scale (k, p, etc.)
  2. Derive Big-Otok expression by isolating the highest-growth term (e.g., O(pk) for CoT-SC)
  3. Configure experiments with controlled variable values (e.g., k=3,8; p=5,10) across benchmarks and models
  4. Run inference, collect total input+output token counts and accuracy per (strategy, benchmark, model)
  5. Compute observed token usage ratios between strategy pairs; compare to theoretical ratios from Big-Otok
  6. Calculate average and marginal TC; plot accuracy vs. tokens to visually inspect diminishing-returns curves

- Design tradeoffs:
  - The paper collapses input and output tokens into a single count; output tokens are more expensive in practice (autoregressive generation). This simplification aids clarity but reduces cost precision.
  - Strategy selection is limited to a coherent lineage (IO → CoT → CoT-SC); fundamentally different strategies (Tree-of-Thought, Least-to-Most) may not follow the same curves.
  - Open-source models (Llama 3.1 8B, Qwen 2.5 14B/32B) are used for budget reasons; results may differ for larger or commercial models.

- Failure signatures:
  - Observed token ratios contradict Big-Otok ordering (e.g., O(1) strategy uses more tokens than O(k) strategy)—indicates unmodeled variables or implementation bugs
  - Negative or undefined marginal TC (e.g., more tokens yield lower accuracy)—suggests strategy mismatch or benchmark unsuitability
  - Extremely high variance in token counts across runs—may require outlier removal or longer sampling

- First 3 experiments:
  1. Reproduce Table 2 on a new open-source model not in the original paper (e.g., Mistral 7B) using the same benchmarks and strategy configurations; compare observed ratios to theoretical predictions
  2. Extend the few-shot ablation (Appendix D.1) to k=0,2,4,6,8,10 for both Vanilla Fewshot and Fewshot CoT on GSM8K; compute marginal TC between each interval to quantify diminishing returns
  3. Derive Big-Otok for a strategy not in the paper (e.g., Tree-of-Thought with branching factor b and depth d → expected O(b^d)), then empirically validate token scaling on a small subset of BBH tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do multimodal LLMs and reasoning-focused models exhibit the same diminishing-returns relationship between token usage and accuracy observed in traditional autoregressive LLMs?
- Basis in paper: [explicit] "We see prompting strategies designed for multimodal and, particularly, reasoning LLMs as a significant avenue for future research and are hopeful that Big-Otok and TC will be incorporated into their development."
- Why unresolved: The study excluded multimodal and reasoning models due to their recency, limited open-source availability, and increased compute requirements.
- What evidence would resolve it: Empirical evaluation of Big-Otok and TC across multimodal benchmarks and reasoning models (e.g., DeepSeek-R1) using analogous prompting strategies.

### Open Question 2
- Question: Do structurally different prompting paradigms such as Least-to-Most, Tree-of-Thoughts, or Algorithm-of-Thoughts follow similar efficiency curves, or do they deviate from the log(log(x)) trend observed for CoT-based strategies?
- Basis in paper: [explicit] "It is likely that fundamentally different prompting strategies, such as Least-to-Most or Algorithm of Thoughts, would not follow the trend lines in our plots."
- Why unresolved: The selected strategies represent a single evolutionary thread of CoT-based methods without structural variation.
- What evidence would resolve it: Comparative Big-Otok analysis and TC measurement for multi-hop decomposition and search-based strategies across the same benchmarks.

### Open Question 3
- Question: How does explicitly distinguishing input tokens from output tokens in the Token Cost metric affect efficiency rankings and optimal strategy selection?
- Basis in paper: [inferred] The authors acknowledge Big-Otok's "lack of differentiation between input and output tokens" and note that output tokens are inherently more expensive due to autoregressive generation.
- Why unresolved: Combining input and output tokens simplifies analysis but may obscure real cost differences, especially for strategies with verbose outputs.
- What evidence would resolve it: Reanalysis of TC using weighted token costs reflecting API pricing structures (e.g., 3–5× higher cost for output tokens).

## Limitations

- Limited strategy scope: The framework is validated only for a specific lineage of prompting strategies and may not generalize to fundamentally different approaches like Least-to-Most or Tree-of-Thought reasoning.
- Simplification of cost structure: By combining input and output tokens, the analysis obscures the fact that output tokens are typically 2-3× more expensive in terms of latency and cost.
- Model and benchmark generalizability: Results are based on three open-source models and three specific benchmarks; performance on larger commercial models or different task domains may show different efficiency curves.

## Confidence

- **High confidence**: The Big-Otok formalism correctly categorizes the five evaluated strategies into constant, linear, and polynomial complexity classes, and the empirical token usage ordering matches theoretical predictions within acceptable noise bounds.
- **Medium confidence**: TC values are comparable and interpretable across strategies, but the specific 20× efficiency drop between low- and high-complexity strategies may be benchmark-specific.
- **Low confidence**: The derived Big-Otok expressions would remain accurate for fundamentally different prompting strategies or when scaling variables far beyond the tested ranges.

## Next Checks

1. Apply Big-Otok derivation to Tree-of-Thought reasoning (with branching factor b and depth d) and validate predicted O(b^d) token scaling on a subset of BBH tasks, comparing observed ratios to theoretical predictions.

2. Extend the few-shot ablation to k=0,2,4,6,8,10 for both Vanilla Fewshot and Fewshot CoT on GSM8K; compute marginal TC between each interval to test whether the diminishing-returns curve maintains its log(log(x)) shape at higher k values.

3. Reproduce the full analysis pipeline on a new open-source model not in the original paper (e.g., Mistral 7B) using the same benchmarks and strategy configurations; compare observed token ratios to theoretical Big-Otok predictions to assess model-agnostic validity of the framework.