---
ver: rpa2
title: 'Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models'
arxiv_id: '2503.19707'
source_url: https://arxiv.org/abs/2503.19707
tags:
- spatial
- reasoning
- rotation
- vlms
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents the first comprehensive benchmark for spatial\
  \ reasoning in Vision-Language Models (VLMs). The authors define four core spatial\
  \ reasoning components\u2014spatial relations, orientation and navigation, mental\
  \ rotation, and spatial visualization\u2014and create a novel benchmark dataset\
  \ with 1,800 image-question pairs combining programmatically generated, GenAI-synthesized,\
  \ and real-world images."
---

# Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2503.19707
- Source URL: https://arxiv.org/abs/2503.19707
- Authors: Ilias Stogiannidis; Steven McDonagh; Sotirios A. Tsaftaris
- Reference count: 40
- Primary result: VLMs average near random chance (32.37%) on spatial reasoning benchmark, with best models reaching only 48.83% accuracy

## Executive Summary
This study presents the first comprehensive benchmark for spatial reasoning in Vision-Language Models (VLMs), revealing critical performance gaps in fundamental spatial cognition capabilities. The authors define four core spatial reasoning components—spatial relations, orientation and navigation, mental rotation, and spatial visualization—and create a novel benchmark dataset with 1,800 image-question pairs combining programmatically generated, GenAI-synthesized, and real-world images. Evaluating 13 state-of-the-art VLMs reveals that their spatial reasoning performance averages near random chance (32.37%), with the best models reaching only 48.83% accuracy. While models show competence in spatial relations and orientation tasks, they struggle significantly with mental rotation and paper folding tasks. The findings expose a critical gap in VLM capabilities, highlighting the need for improved spatial reasoning mechanisms. The benchmark and dataset are publicly available to support future research in this domain.

## Method Summary
The study evaluates 13 VLMs on a novel spatial reasoning benchmark (SRBench) consisting of 1,800 image-question pairs across four components: mental rotation, spatial visualization, navigation, and spatial relations. The dataset combines programmatically generated stimuli (paper folding, mental rotation), GenAI-synthesized images, and real-world images. Models are loaded using HuggingFace Transformers in bfloat16 precision with Flash Attention 2, and evaluated using greedy decoding without chain-of-thought prompting. Responses are parsed via regex matching against expected multiple-choice answers. Experiments run on 4x A100 40GB GPUs, with commercial models accessed via Azure API.

## Key Results
- VLMs achieve near-random performance (32.37% average) across the full spatial reasoning benchmark
- Best-performing model (InternVL2.5) reaches only 48.83% accuracy, far below human-level performance
- Models excel at spatial relations (up to 70.75% accuracy) but struggle significantly with mental rotation (as low as 28.00% on hard variants)
- Performance varies dramatically across components, with navigation showing high variance (9.75% to 45.25% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing spatial reasoning into discrete, isolatable components enables precise diagnostic evaluation of VLM capabilities.
- Mechanism: The benchmark separates spatial reasoning into four distinct categories (spatial relations, orientation/navigation, mental rotation, spatial visualization) with non-overlapping test stimuli, allowing performance attribution to specific cognitive operations rather than conflated skills.
- Core assumption: Spatial reasoning comprises independent sub-capabilities that can be meaningfully isolated and tested separately.
- Evidence anchors: [abstract] "we present a detailed analysis that first delineates the core elements of spatial reasoning: spatial relations, orientation and navigation, mental rotation, and spatial visualization" [section 4.2] Shows dramatic performance variance across components (e.g., InternVL2.5 achieves 70.75% on Spatial Relations but only 28.00% on MRT-Hard) [corpus] SpaRE paper confirms spatial relations are "generally rare in widely used VL datasets," suggesting component isolation reveals training gaps
- Break condition: If spatial reasoning components are highly interdependent in practice, isolated testing may not predict real-world spatial task performance.

### Mechanism 2
- Claim: Synthetic test stimuli with controlled visual properties expose fundamental reasoning deficits that natural images mask.
- Mechanism: By using programmatically generated polycube shapes and paper-folding diagrams stripped of texture/color cues (in MRT-hard), the benchmark removes shortcuts models might exploit (object recognition, semantic context), forcing reliance on geometric transformation reasoning.
- Core assumption: Models performing well on natural images but poorly on synthetic variants are using non-spatial visual cues rather than true spatial reasoning.
- Evidence anchors: [section 4.2] "This raises a critical question: Is the limitation in mental rotation due to the synthetic nature of the images lacking essential visual cues, or does it reflect a fundamental inability of VLMs to reason about spatial transformations?" [section 4.2] GenAI mental rotation ablation shows models perform better on realistic images (Figure 5), suggesting reliance on contextual cues [corpus] Referring Expressions paper similarly finds VLMs struggle when spatial language grounding cannot rely on object priors
- Break condition: If synthetic stimuli differ fundamentally from natural image distributions, poor performance may reflect distribution shift rather than reasoning failure.

### Mechanism 3
- Claim: Adapting validated human psychometric tests provides a grounded, interpretable difficulty calibration for VLM evaluation.
- Mechanism: The Mental Rotation Test and Paper Folding Test are well-studied in cognitive psychology with known human performance baselines; adapting these creates a direct human-machine capability comparison with established construct validity.
- Core assumption: Tasks measuring human spatial cognition are appropriate and sufficient for evaluating artificial spatial reasoning.
- Evidence anchors: [section 3.1] "To assess a VLM's capability to mentally rotate an object, we draw upon the mental rotation test [14], a test designed to measure this ability in humans" [section 3.2] "The Paper Folding Test is a non-verbal reasoning assessment commonly used in psychometric evaluations" [corpus] Concurrent work (Xu et al., cited in section 5.3) used similar psychometric approach with "VLMs underperform relative to humans"
- Break condition: If VLMs process spatial information through fundamentally different mechanisms than human cognition, psychometric validity may not transfer.

## Foundational Learning

- **Mental Rotation as Transform Invariance**
  - Why needed here: Understanding that mental rotation requires maintaining object identity under rotational transformation, not just pattern matching; explains why models fail MRT-hard despite success on recognition tasks.
  - Quick check question: Given a 3D shape rotated 120°, can you identify it as the same object without mentally simulating the rotation?

- **Egocentric vs. Allocentric Reference Frames**
  - Why needed here: The orientation tasks (EgoOrientBench) test egocentric (viewer-relative) spatial encoding, while navigation requires switching between reference frames; confusion here leads to systematic orientation errors.
  - Quick check question: If facing north, is the object "to your left" always west, or does it depend on your subsequent rotation?

- **Spatial Visualization as Multi-Step Transform Composition**
  - Why needed here: Paper folding requires chaining multiple transformations (fold → punch → unfold = inverse transforms), where each step must be tracked; failures cascade exponentially.
  - Quick check question: If paper is folded twice (vertical then horizontal) and a hole punched in the corner, how many holes appear when fully unfolded and where?

## Architecture Onboarding

- **Component map:**
  Benchmark Dataset (1,800 pairs) -> Mental Rotation (400) -> MRT-Easy (200, colored+grid) + MRT-Hard (200, white/no background)
  -> Spatial Visualization (200) -> Paper Folding (1-2 folds, 1-3 holes)
  -> Spatial Relations (400) -> Spatial-Obj subset (natural images, 36 relation types)
  -> Navigation (400) -> Maze-Nav (counting turns, path following)
  -> Orientation (400) -> EgoOrientBench (8-class egocentric, yes/no format)

- **Critical path:** Evaluation requires: (1) load VLM via HuggingFace Transformers, (2) apply greedy decoding without CoT/LTM prompting, (3) present image-question pairs, (4) extract multiple-choice answer via regex matching, (5) aggregate by component and compute weighted overall accuracy.

- **Design tradeoffs:**
  - Synthetic vs. natural images: Synthetic enables controlled ablation but may introduce distribution shift
  - Multiple-choice vs. open-ended: Constrained answers simplify evaluation but reduce task authenticity
  - Component isolation vs. integrated tasks: Clean attribution vs. potential loss of ecologically valid complexity
  - No prompting strategies (CoT) used: Measures base capability but may underestimate potential performance

- **Failure signatures:**
  - Near-chance overall accuracy (~32%) with strong component-specific performance indicates task-specific exploitation of non-spatial cues
  - Large gap between MRT-Easy (colored shapes) and MRT-Hard (white shapes) suggests texture/color reliance
  - Performance below random (e.g., MiniCPM-V-2.6 at 9.75% on Navigation) indicates systematic reasoning inversions

- **First 3 experiments:**
  1. **Baseline replication:** Run all 13 models on the full benchmark using provided code (PyTorch 2.5.1, bfloat16, greedy decoding) to confirm reported accuracy ranges (25-49% overall).
  2. **Cue ablation study:** Compare MRT-Easy vs. MRT-Hard performance gap per model; models with >10% gap likely rely on visual texture cues rather than geometric reasoning.
  3. **Error pattern analysis:** For models scoring <30% on Paper Folding, classify error types (missing holes, mirrored positions, rotation errors) to identify which transform composition step fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed deficiency in mental rotation tasks stem primarily from the synthetic nature of benchmark images lacking visual cues, or does it represent a fundamental inability of current VLMs to reason about spatial transformations?
- Basis in paper: [explicit] The authors explicitly pose this "critical question" in Section 4.2 when analyzing the disparity between MRT performance and GenAI-generated image performance.
- Why unresolved: While the authors introduce GenAI images to investigate this, results are mixed; some models improve on GenAI images while others do not, failing to definitively distinguish whether the failure is due to data distribution shifts or a lack of reasoning mechanisms.
- What evidence would resolve it: Targeted ablation studies where texture and shading cues are systematically added to synthetic shapes to measure performance recovery, or conversely, showing that models fail rotation tasks even when provided with explicit chain-of-thought supervision.

### Open Question 2
- Question: How do distinct spatial reasoning components (e.g., relations, navigation, rotation) interact within VLMs, and does competency in one area (like spatial relations) predict or facilitate success in others (like visualization)?
- Basis in paper: [explicit] The Conclusion explicitly lists the "interaction between spatial reasoning components and their respective roles in complex spatial tasks" as a necessary direction for future research.
- Why unresolved: The evaluation treats these as discrete components, revealing that models often excel at relations but fail at rotation (e.g., MiniCPM-V-2.6), but the paper does not explore if these capabilities share underlying representations or if they are learned independently.
- What evidence would resolve it: A study analyzing the transfer learning capabilities of models fine-tuned solely on spatial relations when evaluated on mental rotation tasks, or an analysis of attention head overlaps between different spatial sub-tasks.

### Open Question 3
- Question: To what extent do VLMs rely on incidental visual shortcuts (such as object outlines or textures) in natural images rather than genuine geometric understanding to solve spatial reasoning tasks?
- Basis in paper: [inferred] The Conclusion notes the limitation that the study "didn't explore in depth which cues models use in natural images," and the results suggest performance on natural/GenAI images may be inflated by "resemblance to the original training dataset" or other visual cues.
- Why unresolved: The paper establishes that models perform better on natural images than synthetic ones, but it does not isolate whether this is due to better spatial reasoning or simply the presence of familiar real-world context cues that allow for guessing.
- What evidence would resolve it: A "stylized" evaluation where natural images are converted into edge-maps or silhouettes to strip away texture and context, requiring the model to rely solely on geometric structure to answer spatial questions.

## Limitations

- The benchmark exclusively uses multiple-choice format tasks, which may underestimate true spatial reasoning capabilities compared to open-ended spatial reasoning tasks.
- Heavy reliance on synthetic stimuli for mental rotation and paper folding tasks raises questions about ecological validity and whether poor performance reflects fundamental reasoning deficits or distribution shift from natural image training data.
- Evaluation protocol uses greedy decoding without chain-of-thought prompting or few-shot examples, which may significantly underestimate the true spatial reasoning capabilities of VLMs.

## Confidence

- **High confidence**: The finding that VLMs perform near chance (32.37% average) on spatial reasoning tasks is well-supported by systematic evaluation across 13 diverse models and comprehensive component analysis showing consistent underperformance across all four spatial reasoning categories.
- **Medium confidence**: The claim that spatial reasoning components can be meaningfully isolated for diagnostic evaluation is supported by the observed performance variance across components, though the interdependence of spatial reasoning skills in real-world applications remains unexplored.
- **Medium confidence**: The assertion that synthetic stimuli expose fundamental reasoning deficits rather than distribution shift is plausible given the performance gap between MRT-Easy and MRT-Hard, but requires further validation through controlled natural image ablations.

## Next Checks

1. **Natural image ablation study**: Evaluate model performance on mental rotation tasks using natural images of rotated objects (e.g., furniture, vehicles) to distinguish between synthetic stimulus bias and genuine spatial reasoning deficits.

2. **Prompting strategy comparison**: Compare greedy decoding performance against chain-of-thought prompting and few-shot examples on the full benchmark to establish whether reported accuracies represent lower bounds on VLM spatial reasoning capability.

3. **Human-machine performance comparison**: Administer the benchmark (particularly the mental rotation and paper folding subtests) to human participants across different expertise levels to establish whether VLMs are performing at sub-human levels or if the tasks themselves are misaligned with artificial spatial reasoning mechanisms.