---
ver: rpa2
title: Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters
arxiv_id: '2507.05807'
source_url: https://arxiv.org/abs/2507.05807
tags:
- adapter
- soup-adapter
- arxiv
- vision
- dinov2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two challenges in few-shot domain adaptation
  of foundation models: the impracticality of hyperparameter tuning due to limited
  validation data, and the need for improved model robustness under distribution shifts.
  The proposed Soup-Adapter method trains multiple independent CLIP-Adapters with
  diverse hyperparameters and averages their outputs, yielding higher performance
  and greater robustness compared to individual adapters.'
---

# Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters

## Quick Facts
- arXiv ID: 2507.05807
- Source URL: https://arxiv.org/abs/2507.05807
- Reference count: 25
- Key outcome: Soup-Adapter trains multiple CLIP-Adapters with diverse hyperparameters and averages outputs, yielding higher performance and greater robustness than individual adapters while reducing sensitivity to residual ratio hyperparameter.

## Executive Summary
This paper addresses two fundamental challenges in few-shot domain adaptation of foundation models: the impracticality of hyperparameter tuning with limited validation data, and the need for improved model robustness under distribution shifts. The proposed Soup-Adapter method trains multiple independent CLIP-Adapters with diverse hyperparameters and averages their outputs, yielding higher performance and greater robustness compared to individual adapters. This approach effectively tackles hyperparameter tuning challenges since it works well with randomly sampled configurations. The method is also significantly less sensitive to the critical residual ratio hyperparameter. Soup-Adapter can be reparameterized into a single adapter through parameter concatenation. Experiments on various datasets demonstrate that Soup-Adapter outperforms individual adapters for both CLIP and DINOv2 models, with particular benefits in robustness under distribution shifts.

## Method Summary
The method trains K independent CLIP-Adapters (typically K=8) with randomly sampled hyperparameters including learning rate, reduction factor, weight decay, and augmentation strength. Each adapter consists of a two-layer MLP with residual connection that modifies the frozen foundation model's features before classification. The outputs of all adapters are averaged during inference, and the ensemble can be reparameterized into a single adapter by concatenating weights (W1) and averaging (W2), preserving inference efficiency. The approach is tested on both CLIP and DINOv2 models across 11 datasets with 2-16 shots, evaluating both in-distribution and out-of-distribution performance on ImageNet variants.

## Key Results
- Soup-Adapter outperforms individual adapters across all tested datasets and shot counts (2, 4, 8, 16)
- The method achieves consistent improvements while being significantly less sensitive to residual ratio hyperparameter selection
- Soup-Adapter demonstrates superior robustness under distribution shifts, particularly for CLIP models where high residual ratios typically harm OOD performance
- Performance saturates around K=8-10 adapters, with diminishing returns for larger ensembles

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Averaging Reduces Hyperparameter Sensitivity
Averaging outputs from multiple independently trained adapters with diverse hyperparameters yields higher and more stable performance than any single adapter. Individual adapters trained with different random hyperparameters converge to different local optima, and averaging smooths over poor configurations, reducing dependence on any single hyperparameter setting. This works because the performance distribution across random hyperparameter configurations has sufficient density near optimal regions that averaging captures beneficial signals while cancelling noise.

### Mechanism 2: Residual Averaging Improves Distribution Shift Robustness
Soup-Adapter produces more favorable in-distribution vs. out-of-distribution accuracy tradeoffs when varying the residual ratio. The residual ratio interpolates between zero-shot/prototype features (r≈0) and adapted features (r≈1). Individual adapters overfit at high r, harming OOD performance. Averaging multiple adapters dampens overfitting effects, producing smoother interpolation curves that reduce overfitting artifacts while preserving adaptation benefits.

### Mechanism 3: Structural Reparameterization Preserves Inference Efficiency
K independently trained adapters can be merged into a single equivalent adapter with expanded hidden dimension. By concatenating first-layer weights vertically, second-layer weights horizontally, and averaging biases, the resulting single forward pass computes the same output as averaging K separate adapter outputs. This enables the benefits of ensemble averaging while maintaining single-model inference efficiency.

## Foundational Learning

- **CLIP-Adapter Architecture**: Why needed here: Soup-Adapter builds directly on CLIP-Adapter's residual MLP design. Without understanding the base adapter (two-layer MLP with residual connection), the ensemble and reparameterization logic is opaque. Quick check: Can you explain why the residual ratio `r` is needed and what happens when r=0 vs r=1?

- **Prototypical Classification for Foundation Models**: Why needed here: Both CLIP (text prompts) and DINOv2 (image exemplars) construct class prototypes in embedding space. The adapter modifies features before prototype comparison. Quick check: How does prototype construction differ between CLIP (zero-shot) and DINOv2 (few-shot)?

- **Distribution Shift Robustness Concepts**: Why needed here: The paper evaluates on ImageNet variants to measure OOD robustness. Understanding why fine-tuning typically harms OOD performance clarifies why Soup-Adapter's benefits matter. Quick check: Why does standard fine-tuning often improve in-distribution accuracy but degrade out-of-distribution accuracy?

## Architecture Onboarding

- **Component map**: Frozen vision encoder (CLIP ViT or DINOv2 ViT) → outputs D-dimensional feature vector x → Adapter MLP: Linear(D→D/red) → GeLU → Linear(D/red→D) → outputs adapted features a → Residual combination: f = normalize(x + r·a) → Prototype classifier: logits = W·f/τ

- **Critical path**: Sample K random hyperparameter configurations → Train K adapters independently on same few-shot data → Reparameterize via weight concatenation → Tune single residual ratio r for merged adapter (significantly less sensitive than individual adapters)

- **Design tradeoffs**: K (number of adapters): K=3-4 already helps, K=8-10 approaches saturation. Higher K = more training compute, diminishing returns. Training epochs: varies by dataset (8-300 epochs). Too few → underfitting; too many → overfitting (though ensemble mitigates this). Mask vs. no-mask for DINOv2 prototypes: mask current sample for 8/16-shot, no mask for 2/4-shot.

- **Failure signatures**: Individual adapters show high variance across runs (expected, Soup-Adapter should stabilize). OOD accuracy drops sharply as r increases (Soup-Adapter should produce flatter curves). If Soup-Adapter doesn't outperform average of individuals: check hyperparameter diversity is sufficient.

- **First 3 experiments**: 1) Baseline replication: Train 8 individual CLIP-Adapters on ImageNet 8-shot with random hyperparameters. Plot individual accuracies, their average, and Soup-Adapter reparameterized version. Verify Soup-Adapter matches or exceeds best individual. 2) Residual ratio sensitivity: Sweep r ∈ {0.1, 0.2, ..., 1.0} for both average individual adapter and Soup-Adapter. Plot ID vs. OOD accuracy curves. Confirm Soup-Adapter curve is flatter. 3) DINOv2 adapter transfer: Apply same pipeline to DINOv2 ViT-B/14 on Flowers102 4-shot. Compare prototypical baseline, KNN baseline, individual adapters, and Soup-Adapter.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the Soup-Adapter paradigm transfer effectively to Large Language Models (LLMs) using LLaMa-adapter architectures? The Conclusion states, "An interesting direction for future work is the use of Soup-Adapters for language models using LLaMa-adapters." This remains unverified as the current study restricts validation to vision and vision-language foundation models.

- **Open Question 2**: Is there a unified, theoretically grounded strategy for constructing prototypes in DINOv2 adapters, or can the Soup-Adapter mechanism mitigate the need to switch strategies based on shot count? The paper reveals a heuristic approach where authors manually switch between "mask" and "no-mask" strategies for DINOv2 prototypes depending on shot count without establishing a general rule.

## Limitations
- The core claims rely on ensemble averaging effects that are not rigorously proven beyond empirical observation
- The reparameterization equivalence assumes linear adapter layers, which may not hold if activation functions or architectures change
- The choice of K=8 adapters appears empirically motivated without systematic analysis of how performance scales with K
- The temperature scaling hyperparameter is mentioned but not specified, potentially affecting reproducibility

## Confidence
- **High confidence**: The empirical demonstration that Soup-Adapter outperforms individual adapters on multiple datasets and tasks. The reparameterization equations are mathematically sound for linear adapters.
- **Medium confidence**: The claimed robustness benefits under distribution shifts, as the evaluation relies on specific OOD datasets that may not generalize to all domain shift scenarios.
- **Low confidence**: The mechanism explanations for why ensemble averaging reduces hyperparameter sensitivity—the paper shows this works empirically but doesn't provide theoretical justification for why averaging diverse hyperparameters produces better results than any single configuration.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary K (number of adapters) from 2 to 16 and measure the point of diminishing returns. Also test whether hyperparameter diversity affects performance gains.
2. **OOD robustness stress test**: Evaluate Soup-Adapter on additional OOD benchmarks beyond ImageNet variants, including natural domain shifts and synthetic shifts, to verify robustness claims generalize.
3. **Ablation on reparameterization**: Train K individual adapters, average their outputs at inference (without reparameterization), and compare to the single concatenated adapter to isolate whether benefits come from ensemble averaging itself versus the reparameterization trick.