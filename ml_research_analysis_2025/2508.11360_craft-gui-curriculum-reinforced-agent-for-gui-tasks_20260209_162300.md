---
ver: rpa2
title: 'CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks'
arxiv_id: '2508.11360'
source_url: https://arxiv.org/abs/2508.11360
tags:
- training
- tasks
- arxiv
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents CRAFT-GUI, a curriculum-reinforced agent for
  GUI tasks that addresses the limitations of existing GUI agents by introducing a
  curriculum learning framework. The key contributions are: (1) a curriculum RL strategy
  that systematically progresses from simpler to more complex GUI tasks based on trajectory
  characteristics, and (2) a fine-grained hybrid reward mechanism combining rule-based
  verification with model-judged evaluation.'
---

# CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks

## Quick Facts
- arXiv ID: 2508.11360
- Source URL: https://arxiv.org/abs/2508.11360
- Reference count: 4
- Key result: Achieves 75.7% average success rate, outperforming baselines by 5.6% on Android Control and 10.3% on internal benchmarks

## Executive Summary
CRAFT-GUI introduces a curriculum-reinforced agent for GUI tasks that addresses the challenges of training agents on heterogeneous GUI environments. The method employs curriculum learning with Group Relative Policy Optimization (GRPO) to systematically progress from simpler to more complex tasks based on trajectory characteristics. A fine-grained hybrid reward mechanism combines rule-based verification with model-judged evaluation to provide more informative policy updates than sparse outcome-only rewards. The approach achieves significant improvements over state-of-the-art methods, demonstrating the effectiveness of integrating reinforcement learning with curriculum learning for complex GUI interaction tasks.

## Method Summary
CRAFT-GUI uses a curriculum-based GRPO framework with three sequential training stages defined by trajectory length: Basic (≤3 steps), Intermediate (4-8 steps), and Advanced (>8 steps plus visual understanding tasks). The method employs a fine-grained hybrid reward function decomposing into tool selection, argument accuracy, format compliance, and adaptive length penalties. Training uses Qwen2.5VL as the policy network with a frozen reference model for KL regularization. The curriculum scheduler partitions data by difficulty tier, and the training loop samples multiple outputs per task to compute normalized advantages for policy updates.

## Key Results
- Achieves 75.7% average success rate on internal benchmarks vs 71.9% for Vanilla GRPO (+3.8%) and 60.8% for SFT (+14.9%)
- Outperforms baselines by 5.6% on Android Control benchmark
- Improves by 10.3% on internal online benchmarks
- Joint training on operation+understanding tasks achieves 75.7% vs 73.2% for operation-only (+2.5%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum-based training progression improves sample efficiency and final task success rate compared to uniform training.
- Mechanism: Tasks are stratified into three difficulty tiers based on trajectory length (≤3 steps, 4-8 steps, >8 steps + visual understanding). The model trains sequentially from basic to advanced, allowing progressive skill acquisition before tackling complex multi-step reasoning.
- Core assumption: Trajectory length correlates with task difficulty; skills learned on shorter trajectories transfer to longer ones.
- Evidence anchors: Table 4 shows Curriculum GRPO achieves 75.7% vs Vanilla GRPO 71.9% (+3.8%) and SFT 60.8% (+14.9%); related work confirms multi-turn RL challenges in GUI agents.

### Mechanism 2
- Claim: Fine-grained hybrid reward signals enable more informative policy updates than sparse outcome-only rewards.
- Mechanism: The reward function decomposes into: (1) tool selection reward (R_tool: binary match), (2) argument accuracy reward (R_args: widget containment for clicks, <30° angle tolerance for swipes, semantic matching for text), (3) format reward (R_format: HTML-style tag compliance), and (4) adaptive length penalty (P_length: graduated penalty for overlong outputs).
- Core assumption: Each reward component independently guides a learnable sub-skill; rule-based verification is sufficiently reliable for non-semantic rewards.
- Evidence anchors: Equation 2-7 define the multi-component reward structure; GUI-Eyes and related work emphasize perception-grounded reward challenges.

### Mechanism 3
- Claim: Joint training on operation and understanding tasks improves generalization on operation tasks.
- Mechanism: Visual understanding tasks (VQA, element localization, information extraction) are included in Stage 3. These require fine-grained visual-semantic reasoning, which strengthens the agent's perception capabilities for downstream operations.
- Core assumption: Visual understanding tasks share underlying perceptual representations with operation tasks; transfer occurs across task types.
- Evidence anchors: Table 5 shows Operation+Understanding training achieves 75.7% vs Operation-Only 73.2% (+2.5%); related surveys note most GUI agents focus narrowly on operation tasks.

## Foundational Learning

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO replaces PPO's value function with group-wise advantage estimation, reducing memory overhead for large VLMs.
  - Quick check question: Can you explain why GRPO uses group-relative advantages instead of a learned value function?

- Concept: Curriculum Learning
  - Why needed here: GUI tasks vary from single-tap actions to multi-app workflows; uniform sampling causes optimization instability.
  - Quick check question: What heuristic does CRAFT-GUI use to define task difficulty, and what are its limitations?

- Concept: Verifiable Rewards in RL
  - Why needed here: GUI operations have ground-truth labels (correct tool, correct coordinates), enabling rule-based reward computation without reward models.
  - Quick check question: For a swipe action, what tolerance threshold defines a "correct" prediction?

## Architecture Onboarding

- Component map: Qwen2.5VL (7B/32B) -> Reference model (frozen) -> Reward calculator (rule-based + LLM-judge) -> Curriculum scheduler -> Training loop (4-phase GRPO)

- Critical path:
  1. Data preparation: Label trajectories by step count; assign to difficulty tier
  2. Stage 1 training: Basic tasks (≤3 steps) until convergence
  3. Stage 2 training: Intermediate tasks (4-8 steps)
  4. Stage 3 training: Advanced tasks (>8 steps) + understanding tasks
  5. Evaluation: Pass@1 success rate on held-out benchmarks

- Design tradeoffs:
  - Trajectory length as difficulty proxy is simple but may misclassify semantically hard short tasks
  - Rule-based rewards are efficient but brittle to annotation noise
  - Sequential staging may overfit early stages; no explicit rehearsal mechanism mentioned

- Failure signatures:
  - Exploding thinking token length → check P_length penalty thresholds (L_max, L_cache)
  - High tool accuracy but low argument accuracy → R_args may be underweighted
  - Stage 3 performance collapse → verify understanding task quality; LLM-judge may be inconsistent

- First 3 experiments:
  1. **Reproduce ablation**: Train Vanilla GRPO vs Curriculum GRPO on internal dataset; compare success rates across all 6 app categories to validate the +3.8% improvement claim.
  2. **Reward component analysis**: Ablate individual reward terms (R_tool, R_args, R_format, P_length) to measure contribution; watch for collapsing to trivial solutions.
  3. **Difficulty definition sensitivity**: Replace trajectory-length heuristic with an alternative (e.g., number of UI elements, semantic complexity classifier) and compare Stage progression outcomes.

## Open Questions the Paper Calls Out

- **Question**: Can the curriculum-based GRPO strategy transfer effectively to desktop environments with differing visual densities and interaction paradigms?
  - Basis in paper: The conclusion states the intention to "extend CRAFT-GUI to computer usage scenarios" in future work.
  - Why unresolved: The current framework is optimized for mobile UIs; desktop environments involve higher resolution, overlapping windows, and mouse/keyboard inputs, requiring redefinition of the "trajectory length" difficulty heuristic.
  - What evidence would resolve it: Evaluation on desktop-specific benchmarks compared against mobile performance baselines.

- **Question**: How can trial-and-error with rollback mechanisms be integrated into the current GRPO framework without destabilizing policy convergence?
  - Basis in paper: The authors propose "incorporating trial-and-error with rollback mechanisms" as a future direction.
  - Why unresolved: The current model optimizes for strict Pass@1 success rates using fixed trajectories; introducing rollback implies a non-linear MDP requiring reformulation of advantage estimation.
  - What evidence would resolve it: An ablation study comparing standard GRPO against a rollback-enhanced variant, measuring convergence speed and error recovery rates.

- **Question**: Is trajectory length a sufficient proxy for task difficulty, or does it misclassify semantically complex short tasks?
  - Basis in paper: The paper acknowledges "varying semantic complexity" but defines curriculum stages primarily by "number of interaction steps."
  - Why unresolved: A short trajectory requiring deep semantic reasoning might be classified as "Basic," potentially overwhelming the model early in training.
  - What evidence would resolve it: An ablation comparing the current step-based curriculum against a "semantic-difficulty" curriculum to assess learning stability.

## Limitations

- The proprietary training dataset and internal online benchmark are not public, preventing independent validation of empirical claims.
- Critical hyperparameters (learning rates, batch sizes, group sizes, training steps, reward weights) are unspecified, limiting reproducibility.
- The trajectory length heuristic for difficulty classification may not capture semantic complexity, potentially misclassifying tasks.
- Cross-platform generalization (iOS to Android) and out-of-distribution task handling are not extensively evaluated.

## Confidence

- **High confidence**: The curriculum learning framework and hybrid reward design are well-motivated by existing literature with clear mathematical formulations.
- **Medium confidence**: The empirical improvements are plausible but limited by reproducibility constraints and absence of public validation data.
- **Low confidence**: Claims about joint training benefits (+2.5% improvement) lack sufficient ablation analysis to rule out alternative explanations.

## Next Checks

1. **Ablation study replication**: Systematically remove individual reward components (R_tool, R_args, R_format, P_length) to quantify their independent contributions and identify potential reward hacking vulnerabilities.

2. **Curriculum heuristic validation**: Replace the trajectory-length difficulty heuristic with an alternative metric (e.g., number of UI elements or semantic complexity classifier) and compare training progression outcomes to assess the robustness of the curriculum design.

3. **Cross-dataset generalization**: Train the model on Android Control and evaluate on iOS or vice versa to measure cross-platform transfer, revealing whether the curriculum benefits generalize beyond the training distribution.