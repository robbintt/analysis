---
ver: rpa2
title: 'REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic
  Resources'
arxiv_id: '2504.07584'
source_url: https://arxiv.org/abs/2504.07584
tags:
- retrieval
- test
- tables
- relevance
- collections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REANIMATOR is a framework for revitalizing existing IR test collections
  by extracting additional resources (tables, full texts, captions, in-text references)
  from PDF documents and generating synthetic relevance labels using large language
  models. The framework is demonstrated by transforming TREC-COVID into TREC-COVID+,
  enabling table retrieval and retrieval-augmented generation (RAG) applications.
---

# REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic Resources

## Quick Facts
- **arXiv ID:** 2504.07584
- **Source URL:** https://arxiv.org/abs/2504.07584
- **Reference count:** 40
- **Key outcome:** Extracted 144k tables from PDFs with 95% accuracy and generated synthetic relevance labels achieving human-LLM agreement comparable to human-human agreement.

## Executive Summary
REANIMATOR revitalizes existing IR test collections by extracting structured resources (tables, captions, full texts) from PDFs and generating synthetic relevance labels using large language models. The framework transforms TREC-COVID into TREC-COVID+ by adding table retrieval and RAG capabilities, enabling new evaluation scenarios for retrieval systems. Evaluation shows table-based retrieval improves RAG output usefulness while maintaining label quality comparable to human assessors.

## Method Summary
The REANIMATOR framework extracts tables, captions, and in-text references from PDFs using Docling, then generates synthetic relevance labels through LLM assessment. Documents are acquired via scholarly APIs (OpenAlex, Unpaywall) from DOI lists, parsed for structured content, and combined with original text chunks. A pooling strategy using BM25 and embedding retrievers creates candidate sets, which are assessed by multiple LLMs using UMBRELA-style prompts for 4-level relevance scoring. The framework outputs enriched test collections supporting table retrieval and RAG evaluation.

## Key Results
- Extracted 144,206 tables with 95% parsing accuracy from 64,358 documents
- Generated synthetic relevance labels with Kappa scores 0.35-0.58, comparable to human-human agreement
- Table-based RAG retrieval achieved Elo score 1605 vs. 1190 for text-only, with longer outputs (323 tokens vs. 226-252)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Advanced PDF parsing can reliably extract structured table content and context from scientific documents.
- Mechanism: Docling parses PDFs using layout analysis, reading order detection, and table structure recognition to extract tables, captions, and in-text references as machine-readable resources.
- Core assumption: Scientific PDFs follow sufficiently consistent layouts that automated parsing can capture table structure and context.
- Evidence anchors:
  - [section 4.1]: "REANIMATOR was able to extract 144,206 tables, 99,286 table captions, and 77,252 in-text references."
  - [section 4.2]: "62.94% were parsed perfectly, while 25.22% were deemed 'good'... roughly 95% of actual tables are at least substantially correct."
  - [corpus]: Weak direct corpus support for Docling-specific extraction; related work (TalentMine) addresses LLM-based table extraction but not PDF parsing pipelines.
- Break condition: Tables with complex nested structures, merged cells, or documents behind paywalls without API access.

### Mechanism 2
- Claim: LLM-generated synthetic relevance labels can achieve agreement rates with human assessors comparable to human-human agreement.
- Mechanism: UMBRELA-style prompting presents query (title, description, narrative) alongside candidate passage/table to LLM, which assigns 4-level relevance scores (irrelevant, related, highly relevant, perfectly relevant).
- Core assumption: LLMs can internalize and apply TREC-style relevance criteria consistently across modalities.
- Evidence anchors:
  - [abstract]: "generated synthetic relevance labels with human-LLM agreement comparable to human-human agreement"
  - [section 4.3]: "human inter-rater agreement scores for passages and tables are almost identical, with both at around 0.35. The best models perform on par with human raters."
  - [corpus]: GenTREC (arXiv:2501.02408) provides supporting evidence that LLM-generated test collections are viable, though bias concerns remain underexplored.
- Break condition: Highly technical domain terminology, ambiguous narratives, or models lacking sufficient context windows for full tables.

### Mechanism 3
- Claim: Incorporating structured table data into RAG retrieval improves output usefulness compared to text-only retrieval.
- Mechanism: Multi-modal retrieval (text-only, table-only, interleaved) feeds top-k results to LLM for generation; Elo-based pairwise comparison ranks output usefulness across configurations.
- Core assumption: Tables contain complementary information that LLMs can synthesize into more useful responses.
- Evidence anchors:
  - [section 5.3]: "Cosine_table achieves the highest Elo score (1604.8), followed by Cosine_interleave (1576.6)... Text-only retrieval ranks lower, with BM25_text receiving the lowest Elo score (1189.9)."
  - [section 5.3]: "outputs generated from table-based retrieval configurations tend to be longer... likely due to the inherently higher character count of tables and their associated context."
  - [corpus]: mmRAG (arXiv:2505.11180) supports multi-modal RAG evaluation over text, tables, and knowledge graphs.
- Break condition: Tables without captions or context, or retrieval models that fail to preserve table structure during encoding.

## Foundational Learning

- Concept: **Cranfield/TREC Test Collection Paradigm** (documents, topics, relevance judgments)
  - Why needed here: REANIMATOR extends this triad by adding modalities and synthetic labels; understanding the baseline evaluation framework is essential.
  - Quick check question: Can you name the three core components of a Cranfield-style test collection?

- Concept: **Cohen's Kappa for Inter-Annotator Agreement**
  - Why needed here: The paper uses Kappa to validate LLM labels against human assessments; interpreting these scores determines trust in synthetic labels.
  - Quick check question: What does a Kappa of 0.35 vs. 0.57 indicate about agreement levels?

- Concept: **Retrieval-Augmented Generation (RAG) Pipeline**
  - Why needed here: The case study evaluates RAG with extracted tables; understanding retrieval → context → generation flow is prerequisite.
  - Quick check question: How does the retriever's output affect the generator's faithfulness and answer relevance?

## Architecture Onboarding

- Component map:
  Input Layer -> Scholarly APIs -> Document acquisition
  Extraction Layer -> Docling -> Tables, captions, in-text references, figures -> Relational database
  Pooling Layer -> BM25 + Cosine similarity retrievers -> Query variants (LLM-generated) -> Reciprocal Rank Fusion -> Top-200 candidates per topic
  Assessment Layer -> UMBRELA prompts -> Multiple LLMs (GPT-4o, Qwen, Mistral, etc.) -> 4-level relevance labels
  Validation Layer -> Human-in-the-loop option -> Cohen's Kappa calculation -> Majority vote labels
  Output Layer -> TREC-COVID+ test collection -> RAG pipeline evaluation

- Critical path: PDF acquisition → Docling extraction → Pooling generation → LLM relevance assessment → Human validation (optional) → Test collection release

- Design tradeoffs:
  - Open-source vs. proprietary LLMs: Open models (Qwen, Mistral) have higher latency but lower cost; GPT-4o-mini offers best cost/accuracy ratio (0.017¢/assessment)
  - Surrogate labels vs. fresh assessment: Surrogate (document-level) labels underperform all but worst models; re-assessment is recommended
  - Chunking strategy: 512 characters with 100 overlap (default) vs. semantic chunking (future work)

- Failure signatures:
  - **Parsing failures**: 8.8% of sampled items misclassified as tables (figures, references); 5.26% parsed with significant errors
  - **Missing context**: 31% of tables lack captions; 53% lack identified in-text references
  - **Low relevance agreement**: Some open models (Mistral-Small) achieve Kappa <0.2; indicates poor prompt/model fit
  - **Answer relevance drops**: RAGAS scores show answer relevance lowered across all configurations; suggests retrieval-context mismatch

- First 3 experiments:
  1. Validate table extraction quality on a held-out sample (n=100) across different document layouts (single-column, two-column, mixed).
  2. Compare LLM relevance assessment across prompt variants (UMBRELA vs. binary vs. domain-specific) using human annotations as ground truth.
  3. Run RAG evaluation with controlled retrieval (swap BM25 for semantic search, vary k=5/10/20) to isolate retrieval impact on Elo scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the REANIMATOR framework be extended to effectively extract and assess complex content beyond tables, such as figures and mathematical equations?
- Basis in paper: [explicit] The authors state in the conclusion that their "approach does not fully capture the challenges associated with handling complex content, such as figures and equations."
- Why unresolved: The current implementation focuses primarily on text and table extraction using Docling, lacking specific modules for parsing visual or mathematical data into machine-readable formats for retrieval.
- What evidence would resolve it: An extension of the framework that successfully parses figures and equations, coupled with an evaluation of synthetic relevance assessment quality for these modalities.

### Open Question 2
- Question: Can automatic topic generation be integrated into the framework to improve the scalability of test collection reanimation across different domains?
- Basis in paper: [explicit] The conclusion notes that "future work could explore automatic topic generation to improve scalability and refine the evaluation process, allowing for broad applicability across different domains."
- Why unresolved: The current workflow relies on existing topics from legacy collections or manual specification; it does not yet generate novel information needs automatically.
- What evidence would resolve it: A study demonstrating a pipeline that generates coherent, diverse topics for a new domain and validates them through retrieval effectiveness metrics.

### Open Question 3
- Question: To what extent does the utility of REANIMATOR generalize to non-academic domains, such as legal or news corpora, which may possess different document structures and table densities?
- Basis in paper: [inferred] The authors note their "evaluation was performed within a specific use case [TREC-COVID], which may limit the generalizability of our findings."
- Why unresolved: The framework was tested on scientific literature (CORD-19), which has distinct structural norms compared to newswire or legal documents mentioned in the text.
- What evidence would resolve it: Application of the framework to a non-scientific dataset (e.g., legal case law or newswire) showing similar parsing accuracy and RAG improvements.

## Limitations
- PDF parsing introduces quality variability with 5.26% substantial parsing errors and 8.8% misclassified tables
- Synthetic labeling shows significant variation across LLM models (Kappa 0.35-0.58) with potential bias inheritance
- RAG evaluation shows decreased answer relevance across all configurations, suggesting retrieval-context mismatches

## Confidence
- **High Confidence**: Table extraction accuracy claims (95% valid tables from manual validation) and basic framework architecture descriptions are well-supported by the paper's evaluation data.
- **Medium Confidence**: Synthetic label quality claims and RAG performance improvements are supported but show significant variation across models and configurations that requires deeper investigation.
- **Low Confidence**: Claims about real-world applicability and generalizability beyond the TREC-COVID domain, given the specialized biomedical context and limited evaluation scope.

## Next Checks
1. **Cross-Domain Validation**: Apply REANIMATOR to a non-biomedical test collection (e.g., TREC Robust) to assess generalization across domains and document types, particularly focusing on table extraction accuracy for non-scientific layouts.
2. **Bias Characterization**: Systematically evaluate synthetic relevance labels for demographic and topical biases using techniques from GenTREC (arXiv:2501.02408), measuring label distribution shifts across query types and document sources.
3. **Longitudinal Performance**: Track RAG output quality over extended generation chains to identify degradation patterns when incorporating table-based contexts, comparing token-level faithfulness decay against text-only baselines.