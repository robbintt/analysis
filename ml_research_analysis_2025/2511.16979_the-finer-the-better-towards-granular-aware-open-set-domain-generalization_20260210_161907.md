---
ver: rpa2
title: 'The Finer the Better: Towards Granular-aware Open-set Domain Generalization'
arxiv_id: '2511.16979'
source_url: https://arxiv.org/abs/2511.16979
tags:
- known
- domain
- prompt
- semantic
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeeCLIP tackles Open-Set Domain Generalization (OSDG) by enhancing
  vision-language models with fine-grained semantic cues, addressing the challenge
  of distinguishing unknown categories that are visually similar to known classes.
  It introduces semantic-aware prompt enhancement using multi-head attention to extract
  discriminative semantic tokens, duplex contrastive learning with repulsion and cohesion
  losses to position unknown prompts, and semantic-guided diffusion generation to
  synthesize challenging pseudo-unknowns.
---

# The Finer the Better: Towards Granular-aware Open-set Domain Generalization

## Quick Facts
- arXiv ID: 2511.16979
- Source URL: https://arxiv.org/abs/2511.16979
- Reference count: 8
- Key outcome: SeeCLIP improves OSDG accuracy by 3% and H-score by 5% over state-of-the-art methods

## Executive Summary
SeeCLIP tackles Open-Set Domain Generalization by enhancing vision-language models with fine-grained semantic cues to distinguish unknown categories that are visually similar to known classes. It introduces semantic-aware prompt enhancement using multi-head attention to extract discriminative semantic tokens, duplex contrastive learning with repulsion and cohesion losses to position unknown prompts, and semantic-guided diffusion generation to synthesize challenging pseudo-unknowns. Extensive experiments on five benchmarks show consistent improvements over state-of-the-art methods, demonstrating superior robustness in recognizing hard unknown samples under distribution shifts.

## Method Summary
SeeCLIP operates on a frozen CLIP backbone, extracting semantic tokens from image patches using K-head attention pooling with learnable queries. These tokens are projected and integrated into text prompts for enhanced semantic alignment. A duplex contrastive loss jointly optimizes known-class discrimination (alignment) and unknown-class separability (repulsion + cohesion). Pseudo-unknown samples are generated via semantic token perturbation and conditional diffusion (Stable Diffusion v1.5) with dual prompts. The model is trained end-to-end with learnable unknown embeddings, queries, and projection layers, optimizing only these parameters while keeping CLIP frozen.

## Key Results
- Consistently improves accuracy by 3% and H-score by 5% over state-of-the-art OSDG methods
- Ablation confirms each component (attention pooling, duplex loss, diffusion generation) contributes to performance
- Demonstrates robustness across five diverse benchmarks with leave-one-domain-out protocol

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-head attention pooling extracts discriminative semantic tokens that enable fine-grained vision-language alignment beyond coarse category labels.
- **Mechanism:** K learnable query vectors attend to patch embeddings via scaled attention weights, producing K semantic tokens each capturing distinct discriminative regions (e.g., ears, nose, tail). These tokens are projected and integrated into text prompts, creating enhanced prompts that align visual features with textual semantics at a granular level.
- **Core assumption:** The visual encoder's patch embeddings contain localized semantic information that can be extracted via attention without part-level supervision, and that CLIP's vision-language alignment benefits from explicit fine-grained token injection.
- **Evidence anchors:**
  - [abstract]: "semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens"
  - [section 3.1, Eq. 1-3]: Details the K-head attention pooling mechanism with learnable queries and weighted aggregation
  - [corpus]: Limited direct validation; neighbor papers address VLM open-set challenges but not this specific attention-based semantic decomposition
- **Break condition:** When images lack distinctive local features (e.g., highly textured scenes without clear object parts) or when patch embeddings don't encode spatially-localized semantics, the attention mechanism may extract noisy or redundant tokens.

### Mechanism 2
- **Claim:** Duplex contrastive learning positions unknown prompts in a semantically meaningful region—sufficiently distant from known classes to maintain separability, yet close enough to preserve semantic coherence.
- **Mechanism:** Two complementary losses operate on the unknown prompt: (1) Repulsion loss (margin-based) pushes the unknown prompt away from all known-class image embeddings; (2) Cohesion loss restricts the unknown prompt's distance from the centroid of known prompt embeddings. This dual constraint prevents the unknown prompt from drifting too far (losing semantic meaning) or staying too close (causing misclassification of hard unknowns).
- **Core assumption:** The optimal unknown prompt representation lies in a bounded region near but separable from the known-class manifold, and that L2 distance in the CLIP embedding space reflects semantic similarity.
- **Evidence anchors:**
  - [abstract]: "duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity"
  - [section 3.1, Eq. 5-6]: Formally defines repulsion and cohesion losses with margin δ and centroid computation
  - [corpus]: No direct validation of this duplex formulation; neighbor papers use standard contrastive losses without the cohesion constraint
- **Break condition:** When the margin δ is too large, the unknown prompt may be pushed into semantically meaningless regions; when cohesion weight β is too high, the unknown prompt collapses toward the known centroid, failing to reject hard unknowns.

### Mechanism 3
- **Claim:** Perturbing extracted semantic tokens and conditioning diffusion models with these perturbed tokens generates pseudo-unknown samples that are globally similar yet locally different from known classes, forcing the model to learn finer decision boundaries.
- **Mechanism:** Semantic tokens are perturbed with Gaussian noise (controlled by σ), then concatenated with positive and negative textual prompts as conditions for Stable Diffusion. The positive prompt includes domain context with "unknown class" specification; the negative prompt lists all known class names. Classifier-Free Guidance strengthens the distinction between conditions.
- **Core assumption:** Small perturbations to semantic tokens produce controlled visual variations that mimic the distribution of real "hard unknowns"—samples near the decision boundary but belonging to unknown classes.
- **Evidence anchors:**
  - [abstract]: "semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences"
  - [section 3.2, Eq. 8-9]: Describes perturbation with Gaussian noise and joint condition construction
  - [corpus]: No direct replication; ODG-CLIP (neighbor) uses diffusion for pseudo-open generation but without semantic token perturbation
- **Break condition:** When perturbation σ is too small, generated samples are nearly identical to known classes; when σ is too large, samples diverge excessively and become easy negatives that don't sharpen decision boundaries.

## Foundational Learning

- **Concept:** Vision-Language Model (VLM) alignment
  - **Why needed here:** SeeCLIP builds directly on CLIP's joint embedding space; understanding how CLIP aligns images and text via contrastive learning is prerequisite to comprehending prompt enhancement and the duplex losses.
  - **Quick check question:** Given an image of a "Persian cat" and two text prompts—"a photo of a cat" and "a photo of a Persian cat"—which should CLIP score higher, and why might it fail to distinguish them?

- **Concept:** Open-set recognition (OSR) fundamentals
  - **Why needed here:** OSDG extends OSR to domain shift scenarios; you need to understand the structural risk vs. open-space risk tradeoff to see why SeeCLIP's approach matters.
  - **Quick check question:** In standard OSR, what happens if your threshold is too conservative (rejects too much) vs. too aggressive (accepts too much)?

- **Concept:** Attention pooling and multi-head attention
  - **Why needed here:** The semantic-aware prompt enhancement relies on K-head attention to extract discriminative tokens; you should understand how learnable queries aggregate patch features.
  - **Quick check question:** How does attention pooling differ from average pooling, and why might multiple heads capture different semantic regions?

## Architecture Onboarding

- **Component map:**
  Input Image → CLIP Vision Encoder → Patch Embeddings → K-Head Attention Pooling (learnable queries) → Semantic Tokens → [Prompt Enhancement → Enhanced Prompts (known) → Duplex Contrastive Learning (repulsion + cohesion + alignment)] and [Diffusion Generation → Perturbed Tokens + Dual Prompts → Pseudo-Unknown Images] → Optimized Prompts → Inference via Similarity

- **Critical path:** The semantic tokens extracted via attention pooling (Mechanism 1) are the foundation—they feed both prompt enhancement and diffusion generation. If these tokens are low-quality, both downstream modules degrade.

- **Design tradeoffs:**
  - **K (number of attention heads):** Higher K captures more granular features but risks redundancy; paper uses K=4. Too few heads miss discriminative regions.
  - **σ (perturbation intensity):** Controls hardness of pseudo-unknowns. Paper uses σ=0.2. Lower σ generates easier negatives; higher σ generates out-of-distribution samples that don't help.
  - **Margin δ (repulsion distance):** Paper uses δ=0.2. Too large pushes unknown prompt into meaningless space; too small causes overlap with known classes.
  - **α, β (loss weights):** α=0.5 (repulsion), β=0.3 (cohesion). Imbalanced weights cause the unknown prompt to either drift too far or collapse toward known centroids.

- **Failure signatures:**
  - **Low H-score despite high accuracy:** Unknown prompt has collapsed toward known class centroid; increase β or decrease δ.
  - **Both accuracy and H-score low:** Semantic tokens are noisy; check attention visualizations, reduce K, or increase L1 regularization weight γ.
  - **Generated pseudo-unknowns look unrealistic or too similar to known classes:** Adjust σ or verify dual-prompt conditioning is working (negative prompt should suppress known-class features).

- **First 3 experiments:**
  1. **Sanity check attention heads:** Visualize the K=4 attention maps on sample images (as in Figure 3). Verify each head focuses on distinct discriminative regions. If heads attend to the same region or background, reduce K or check query initialization.
  2. **Ablate σ systematically:** Generate pseudo-unknowns with σ ∈ {0.1, 0.2, 0.3, 0.5} and evaluate on a held-out domain. Plot accuracy vs. H-score to find the sweet spot where both metrics are balanced.
  3. **Probe unknown prompt positioning:** After training, compute the distance from the unknown prompt embedding to (a) the centroid of known prompts, and (b) the nearest known-class image embedding. Verify the unknown prompt is within the expected range (close to centroid but beyond margin δ from individual classes).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the semantic-aware prompt enhancement module generalize effectively to other Vision-Language Models (e.g., ALIGN) that utilize different vision encoder architectures or tokenization strategies?
- Basis in paper: [explicit] The authors state, "We implement SeeCLIP on the CLIP ViT-B/32 architecture," and rely on its specific Transformer-based text and vision encoders for token extraction and alignment.
- Why unresolved: The proposed method leverages CLIP's specific attention mechanisms and patch embeddings; it remains unverified if the multi-head attention pooling for semantic tokens transfers to CNN-based or other Transformer-based VLMs without architectural modifications.
- What evidence would resolve it: Experimental results applying SeeCLIP to alternative VLM backbones (e.g., ALIGN or BLIP) on the same OSDG benchmarks.

### Open Question 2
- Question: Does the optimal number of semantic attention heads ($K$) scale with the structural complexity of the object categories, or does a fixed $K$ limit the capture of discriminative features in highly complex or fine-grained datasets?
- Basis in paper: [explicit] The paper specifies, "The number of semantic token heads $K$ is set to 4," and this value is "shared across all domains" rather than being dynamically learned or dataset-adaptive.
- Why unresolved: While $K=4$ works for the tested benchmarks, complex datasets (like Mini-DomainNet) might require more semantic tokens to capture subtle local differences, whereas simpler datasets might suffer from noise if $K$ is too high.
- What evidence would resolve it: An ablation study varying $K$ (e.g., $K \in \{2, 8, 16\}$) specifically on the largest and most diverse dataset (Mini-DomainNet) to analyze the trade-off between feature granularity and noise.

### Open Question 3
- Question: How sensitive is the model's open-space risk to the intensity of the semantic token perturbation ($\sigma$), and does a static standard deviation ($\sigma=0.2$) fail to adapt to domains with high intra-class variance?
- Basis in paper: [explicit] The introduction notes that generating samples "too far from or too close to known categories... leads to model bias," yet the methodology fixes the perturbation standard deviation $\sigma$ to 0.2 for all experiments.
- Why unresolved: A static $\sigma$ may generate "hard unknowns" that are either too distinct for simple domains or indistinguishable for complex domains, potentially failing to optimize the boundary between structural and open-space risks universally.
- What evidence would resolve it: Analysis of H-score fluctuations across domains when $\sigma$ is dynamically adjusted or treated as a learnable parameter relative to the source domain's feature variance.

## Limitations
- Semantic token extraction assumes attention heads capture meaningful local features without part-level supervision; no validation that tokens correspond to interpretable object parts across diverse domains
- Optimal hyperparameters (K=4, σ=0.2, δ=0.2, α=0.5, β=0.3) are not justified through sensitivity analysis and may be dataset-specific
- Unknown prompt optimization relies on L2 distance in CLIP embedding space as a proxy for semantic similarity, which may break for cross-domain scenarios

## Confidence
- **High confidence** in the duplex contrastive learning mechanism: The formulation (repulsion + cohesion) is mathematically coherent and grounded in established OSR principles
- **Medium confidence** in semantic token extraction: While the attention mechanism is standard, there is no ablation showing that K=4 is optimal or that tokens improve over simpler pooling methods
- **Low confidence** in diffusion-based pseudo-unknown generation: The perturbation strategy (σ=0.2) is arbitrary and there's no validation that generated samples are truly "hard unknowns" vs. easy negatives

## Next Checks
1. **Probe semantic token interpretability:** For a held-out test set, visualize the K=4 attention maps for each image and label the attended regions. Compute the consistency of attention across similar objects to verify that tokens capture discriminative, localized features rather than global or irrelevant patterns.
2. **Ablate perturbation intensity:** Generate pseudo-unknowns with σ ∈ {0.1, 0.2, 0.3, 0.5} and evaluate the model's accuracy and H-score on a held-out domain. Plot the trade-off curve to identify if σ=0.2 is indeed optimal, or if harder (higher σ) or easier (lower σ) negatives improve robustness.
3. **Analyze unknown prompt positioning:** After training, compute the distance from the unknown prompt to (a) the centroid of known prompts, and (b) the nearest known-class image embedding. Verify the unknown prompt is within the expected range (close to centroid but beyond margin δ from individual classes). If the unknown prompt is too close to known classes, increase β or δ; if too far, decrease β.