---
ver: rpa2
title: Systematic Abductive Reasoning via Diverse Relation Representations in Vector-symbolic
  Architecture
arxiv_id: '2501.11896'
source_url: https://arxiv.org/abs/2501.11896
tags:
- attribute
- representations
- relation
- reasoning
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a systematic abductive reasoning model, Rel-SAR,
  for solving Raven's Progressive Matrices (RPM) tasks using Vector-symbolic Architecture
  (VSA). The model introduces diverse relation representations, including atomic HD
  vectors with semantic representations (random, numeric, circular, and Boolean vectors)
  and novel numerical and logical relation functions.
---

# Systematic Abductive Reasoning via Diverse Relation Representations in Vector-symbolic Architecture

## Quick Facts
- **arXiv ID:** 2501.11896
- **Source URL:** https://arxiv.org/abs/2501.11896
- **Reference count:** 40
- **Primary result:** Rel-SAR achieves state-of-the-art performance on RAVEN and I-RAVEN RPM datasets, outperforming previous deep learning and neuro-symbolic approaches

## Executive Summary
This paper proposes Rel-SAR, a systematic abductive reasoning model for Raven's Progressive Matrices (RPM) tasks using Vector-symbolic Architecture (VSA). The model introduces diverse relation representations including atomic high-dimensional vectors with semantic representations (random, numeric, circular, and Boolean vectors) and novel numerical and logical relation functions. These functions enable systematic rule abduction and execution by leveraging the algebraic properties of VSA. Rel-SAR achieves state-of-the-art performance on RPM datasets, demonstrating significant improvements on configurations involving rules based on the position attribute and robust out-of-distribution generalization.

## Method Summary
Rel-SAR maps visual attributes to specific high-dimensional algebraic structures (numeric, circular, boolean) that preserve semantic properties through vector operations. The model uses Fractional Power Encoding (FPE) for numeric values and Circular Vectors for periodic attributes like position rotation, allowing reasoning to be literal algebraic calculation rather than statistical inference. A ResNet-50 encoder maps images to Structured HD Representation (SHDR) vectors, which are then "unbound" to retrieve specific attribute vectors. Rule abduction searches for operator parameters that maximize consistency across context panels, while relation functions apply the rules to predict missing vectors.

## Key Results
- Achieves state-of-the-art performance on RAVEN and I-RAVEN RPM datasets
- Demonstrates significant improvements on configurations involving rules based on position attribute
- Exhibits robust out-of-distribution generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
The model achieves systematic reasoning by mapping visual attributes to specific high-dimensional (HD) algebraic structures (numeric, circular, boolean) that preserve semantic properties through vector operations. Rel-SAR uses Fractional Power Encoding (FPE) for numeric values where binding represents addition, and Circular Vectors for periodic attributes like position rotation. This allows reasoning to be a literal algebraic calculation rather than a statistical inference.

### Mechanism 2
The system generalizes to out-of-distribution (OOD) cases by optimizing for "rule consistency" rather than mere pattern matching of pixel features. The rule abduction module searches for operator parameters that maximize similarity between relation function outputs applied to different context panels, isolating the abstract rule independent of specific attribute values.

### Mechanism 3
The model interprets complex visual scenes by disentangling attributes via "unbinding" operations on Structured High-Dimensional Representation (SHDR). A CNN encodes images into a single SHDR vector formed by bundling role-filler pairs, then unbinds specific roles to retrieve specific attribute vectors, decoupling perception from reasoning.

## Foundational Learning

- **Concept:** Vector-Symbolic Architecture (VSA) Operations
  - **Why needed here:** You cannot understand the reasoning backend without grasping Binding, Bundling, and Unbinding operations
  - **Quick check question:** If I bind a "Shape" vector with a "Square" vector, how do I retrieve the "Square" vector later if I possess the "Shape" key?

- **Concept:** Fractional Power Encoding (FPE)
  - **Why needed here:** This mathematical trick handles numeric progression and circular position
  - **Quick check question:** In FPE, does the vector representing "5" look more similar to "4" or "10"?

- **Concept:** Abductive Reasoning vs. Inductive Reasoning
  - **Why needed here:** The paper frames the task as finding the most likely explanation/rule for observations rather than predicting the next token
  - **Quick check question:** In an RPM puzzle, are you inferring a general law from specific examples (induction) or finding the specific missing premise that makes the scene consistent (abduction)?

## Architecture Onboarding

- **Component map:** Frontend (Image → Vector) → Codebook Mapping (Raw Vector → Semantic Vector) → Rule Abduction (Find OP) → Execution (Predict v_missing)
- **Critical path:** Frontend (Image → Vector) → Codebook Mapping (Raw Vector → Semantic Vector) → Rule Abduction (Find OP) → Execution (Predict v_missing)
- **Design tradeoffs:** The model requires specific vector types to be pre-defined, reducing learning burden but increasing engineering rigidity compared to end-to-end learning
- **Failure signatures:** Position confusion if Circular Vectors aren't used for position attributes; orthogonality collapse if frontend generates non-distinct vectors; logic/numeric confusion if model fails to distinguish rule types
- **First 3 experiments:**
  1. Codebook Ablation: Replace Circular Vectors for "Position" with standard Random Vectors and verify accuracy drop on rotational rules
  2. Noise Tolerance Test: Inject Gaussian noise into SHDR vector before unbinding and measure retrieval SNR
  3. OOD Generalization: Train on values {0,1,2} for an attribute and test on values {3,4,5} to test FPE arithmetic generalization

## Open Questions the Paper Calls Out

- **Question:** Can Rel-SAR maintain high accuracy if the supervised perception frontend is replaced with an unsupervised model to extract SHDR without auxiliary attribute labels?
  - **Basis:** Page 13, Section VI suggests employing learnable frontend codebook with slot attention for unsupervised extraction
  - **Why unresolved:** Current model relies on auxiliary rule and attribute labels to guide learning of meaningful SHDRs
  - **Evidence:** Experimental results showing comparable performance between unsupervised frontend variant and current supervised Rel-SAR

- **Question:** Does integrating a "relational bottleneck" into the VSA reasoning backend improve OOD generalization for attributes with large value ranges where the current model struggles?
  - **Basis:** Page 14, Section VI suggests combining relational bottlenecks with VSA algebra for complex relations
  - **Why unresolved:** Current MLP rule learner struggles to generalize to unseen OOD attribute values, particularly for Color
  - **Evidence:** Improved OOD test accuracy on Color attribute compared to baseline Rel-SAR

- **Question:** To what extent does utilizing CNNs with smaller kernel sizes and strides in shallow layers improve feature extraction and accuracy for Out-InGrid configuration?
  - **Basis:** Page 13, Section VI suggests using CNNs with smaller kernel sizes and strides in shallow layers
  - **Why unresolved:** Current model has lower accuracy on Out-InGrid relative to other configurations
  - **Evidence:** Significant increase in Out-InGrid accuracy when using modified CNN architecture

## Limitations

- Performance gains may stem from implicit pattern matching via CNN backbone rather than genuine systematic reasoning
- Lack of rigorous ablation studies comparing diverse relation representations against simpler alternatives
- Unclear whether vector-symbolic components provide meaningful advantage over standard deep learning features

## Confidence

- **High Confidence:** Technical implementation of VSA operations is mathematically sound and model architecture is clearly specified
- **Medium Confidence:** Performance claims on RAVEN and I-RAVEN are verifiable through provided code and standard datasets
- **Low Confidence:** Claims about diverse relation representations enabling "systematic" reasoning beyond standard deep learning require more rigorous validation

## Next Checks

1. **VSA Core Function Ablation:** Implement the model using only random vectors for all attribute types and compare performance drop on configurations requiring numerical/circular reasoning versus logical reasoning

2. **Generalization Stress Test:** Create RPM problems with out-of-distribution numerical values (training on {0,1,2} but testing on {100,200,300}) to verify if FPE encoding truly enables arithmetic generalization

3. **Rule Consistency Analysis:** For each solved problem, extract learned operator powers and verify they satisfy claimed consistency across context panels to check if model is finding consistent rules or fitting to specific panel patterns