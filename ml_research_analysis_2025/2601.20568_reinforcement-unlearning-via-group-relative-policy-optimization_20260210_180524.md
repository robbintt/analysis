---
ver: rpa2
title: Reinforcement Unlearning via Group Relative Policy Optimization
arxiv_id: '2601.20568'
source_url: https://arxiv.org/abs/2601.20568
tags:
- unlearning
- purge
- forget
- conference
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PURGE is a reinforcement learning-based method for LLM unlearning
  that treats forgetting as a verifiable task using the Group Relative Policy Optimization
  framework. By constructing synthetic forget corpora and using an intrinsic reward
  signal that penalizes mentions of forbidden concepts, PURGE achieves up to 46x lower
  token usage per target than state-of-the-art methods while improving fluency by
  5.48% and adversarial robustness by 12.02% over the base model.
---

# Reinforcement Unlearning via Group Relative Policy Optimization

## Quick Facts
- arXiv ID: 2601.20568
- Source URL: https://arxiv.org/abs/2601.20568
- Reference count: 33
- Primary result: Reinforcement learning-based unlearning method achieving 46× lower token usage than state-of-the-art while maintaining 98% utility

## Executive Summary
PURGE introduces a novel approach to LLM unlearning by treating forgetting as a verifiable task using Group Relative Policy Optimization (GRPO). The method constructs synthetic forget corpora and uses an intrinsic binary reward signal to penalize mentions of forbidden concepts, achieving efficient and targeted unlearning without external reward models. PURGE demonstrates up to 46× lower token usage per target than existing methods while improving fluency by 5.48% and adversarial robustness by 12.02% over the base model.

## Method Summary
PURGE treats unlearning as a reinforcement learning task where forgetting is defined as a verifiable binary outcome. The method constructs a synthetic forget corpus by extracting descriptive entities from model outputs using conditioned NER. During training, GRPO samples multiple completions per prompt, computes group-relative advantages from binary rewards that penalize forbidden entity mentions, and updates the policy while penalizing KL divergence from the original model. This approach enables efficient, targeted forgetting with theoretical guarantees on information suppression and utility retention.

## Key Results
- Achieves 46× lower token usage per target compared to state-of-the-art methods
- Maintains 98% of original model utility while achieving 11% unlearning effectiveness
- Improves adversarial robustness by 12.02% over base model on RWKU benchmark

## Why This Works (Mechanism)

### Mechanism 1: Verifiable Reward Signal for Unlearning
PURGE defines a binary intrinsic reward φ(x) = 1[x ∩ D′F = ∅] that assigns 0 if forbidden entities appear in outputs and 1 otherwise. GRPO samples multiple completions, computes group-relative advantages, and updates the policy to maximize advantage-weighted likelihood while penalizing KL divergence. This drives down forbidden token probabilities while anchoring near the original distribution. The synthetic forget corpus D′F (top-K entities extracted via conditioned NER) must sufficiently cover latent knowledge to be removed.

### Mechanism 2: Geometric Decay of Forbidden-Token Probabilities
Theorem 1 proves that under bounded rewards and sampling mixing, forbidden-token leakage decays geometrically: pt+1 ≤ (1−α)(1−ηε)pt + αpθ∗. This ensures pt asymptotically stays below the base model's leakage pθ∗, providing theoretical suppression bounds. The decay relies on proper hyperparameter tuning and mixing approximations.

### Mechanism 3: Utility Retention via KL-Regularized Policy Updates
Theorem 2 establishes that bounded utility metrics satisfy Δu ≤ √(½ KL(πθ′‖πθ∗)). GRPO explicitly penalizes KL divergence in the objective, keeping the updated policy within a bounded KL-ball around the original. This limits utility loss while enabling effective forgetting, with the KL penalty weight β trading off forgetting strength versus capability preservation.

## Foundational Learning

- **Proximal Policy Optimization (PPO) and clipping**: GRPO inherits PPO's clipped surrogate objective to stabilize policy updates. Understanding how the clip threshold ε controls update magnitude is essential for tuning suppression (ηε in Theorem 1) and avoiding divergence.
  - Quick check question: What happens to policy updates if ε is set too high or too low in PPO?

- **KL Divergence as a Regularizer**: The KL term constrains how far the unlearned model can drift from the original, directly linking to utility retention (Theorem 2). Tuning β trades off forgetting strength vs. capability preservation.
  - Quick check question: How does increasing β affect the KL penalty term and what downstream impact would you expect on utility retention?

- **Group Relative Advantage Estimation**: GRPO computes advantages by normalizing rewards within a group of sampled outputs, removing the need for a value network. Understanding this normalization is key to diagnosing reward noise and setting group size W.
  - Quick check question: Why might a small group size W lead to noisy advantage estimates, and how could that affect unlearning stability?

## Architecture Onboarding

- **Component map**: Synthetic Forget Corpus Constructor -> GRPO Training Loop -> KL Monitor -> Reward Tracker
- **Critical path**: 1) Generate probe answers from base model → build D′F via NER → validate entity set. 2) For each epoch: sample batches Qb → generate answer groups W(q) → compute φ → compute advantages → update πθt. 3) Monitor KL and reward curves; early-stop if KL exceeds threshold or reward plateaus.
- **Design tradeoffs**: Larger group size W → more stable advantage estimates but higher compute. Higher β → better utility retention but potentially weaker forgetting. More epochs with fewer tokens vs. fewer epochs with more tokens (token budget tradeoff).
- **Failure signatures**: KL spikes → policy drift; reduce β or learning rate. Reward stagnation → insufficient suppression; check D′F coverage or increase η/epochs. Fluency degradation → KL too low or aggressive clipping; increase β or reduce ε.
- **First 3 experiments**: 1) Baseline sanity check: Run PURGE with default hyperparameters on a single target, plot reward and KL curves, verify reward increases and KL ≈ 0.05. 2) Ablation on group size W: Test W ∈ {4, 8, 16} on the same target, compare reward stability and final suppression. 3) Token efficiency comparison: Run PURGE vs. NPO on the same target with matched compute budget, compare final forget quality and utility retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unlearning targets be identified more precisely by extracting semantically related concepts directly from the model's internal embedding space?
- Basis in paper: Conclusion: "In the future, we want to refine the definition of unlearning targets by extracting semantically related concepts directly from the model’s embedding space..."
- Why unresolved: The current synthetic corpus construction relies on external GPT-4 extraction rather than utilizing the model's own latent representations.
- What evidence would resolve it: A modified PURGE pipeline that clusters forget-targets using model hidden states, demonstrating higher unlearning precision or faster convergence than the current NER-based approach.

### Open Question 2
- Question: What is the trade-off between the reduced token usage of the synthetic forget corpus and the number of epochs required for convergence?
- Basis in paper: Section 5.2: "Naturally, fewer tokens require more epochs to reach the same forgetting level as NPO, a trade-off that future work could further explore..."
- Why unresolved: While PURGE uses significantly fewer tokens, the potential increase in required training epochs to match the performance of denser corpora is not fully quantified in terms of total compute.
- What evidence would resolve it: A comparative analysis plotting total compute cost (tokens × epochs) against forgetting quality to determine if the token efficiency leads to longer wall-clock training times.

### Open Question 3
- Question: How does PURGE perform in batch unlearning scenarios involving multiple simultaneous unlearning requests?
- Basis in paper: Conclusion: "We also aim to explore and apply our method to batch unlearning, enabling more realistic and scalable applications of unlearning."
- Why unresolved: The experimental evaluation focuses on independent unlearning targets; it is unknown if simultaneously optimizing multiple reward constraints causes gradient interference or utility degradation.
- What evidence would resolve it: Evaluation on a benchmark where a set of diverse entities is unlearned in a single training run, measuring the interference between different forgetting objectives and overall model utility.

## Limitations

- **Synthetic Corpus Completeness**: Effectiveness depends on D′F capturing all relevant latent associations, but no analysis of coverage completeness or sensitivity to K is provided.
- **Reward Function Robustness**: The binary reward is simple but potentially brittle, with no examination of reward hacking scenarios where models might produce evasive but semantically related content.
- **Hyperparameter Transparency**: PURGE-specific hyperparameters (group size W, learning rate η, KL weight β, PPO clip ε) are not specified in the main paper.

## Confidence

- **High Confidence**: The core mechanism of using GRPO with binary rewards for unlearning is sound, and the utility retention bound via KL regularization is well-established theory.
- **Medium Confidence**: The geometric decay proof is theoretically valid, but real-world applicability depends on synthetic corpus quality and whether idealized mixing assumptions hold with finite group sampling.
- **Low Confidence**: The 46× token efficiency claim lacks direct comparison methodology details, and the adversarial robustness improvement is measured without thorough examination of reward function robustness against evasion tactics.

## Next Checks

1. **Synthetic Corpus Coverage Analysis**: Run the entity extraction pipeline on 10 targets and manually verify that the top-50 entities capture all major concepts. Test sensitivity by varying K from 25 to 100 and measuring downstream unlearning effectiveness.

2. **Reward Function Robustness Test**: Design adversarial prompts that semantically reference forbidden entities without triggering the exact entity match in φ. Test whether the model learns to evade detection or whether the GRPO framework naturally penalizes such semantic proximity.

3. **Hyperparameter Sensitivity Study**: Systematically vary group size W (4, 8, 16), KL weight β (0.01, 0.05, 0.1), and PPO clip ε (0.1, 0.2, 0.5) on a single target. Measure impact on: (a) reward convergence speed, (b) final suppression quality, (c) utility retention, and (d) training stability.