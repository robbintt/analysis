---
ver: rpa2
title: 'Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping
  Embeddings'
arxiv_id: '2508.13729'
source_url: https://arxiv.org/abs/2508.13729
tags:
- norms
- features
- embeddings
- feature
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines property inference methods for explaining
  word embeddings, challenging the common assumption that accurate prediction of semantic
  features implies genuine feature-based interpretability. The authors apply Partial
  Least Squares Regression and Feed Forward Neural Networks to map BERT embeddings
  to three feature norms (McRae, Buchanan, Binder) and conduct extensive ablation
  experiments.
---

# Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings

## Quick Facts
- arXiv ID: 2508.13729
- Source URL: https://arxiv.org/abs/2508.13729
- Authors: Hanna Herasimchyk; Alhassan Abdelhalim; Sören Laue; Michaela Regneri
- Reference count: 40
- Primary result: Prediction accuracy in property inference is predominantly determined by algorithmic upper bounds and geometric similarity rather than genuine semantic representation

## Executive Summary
This paper critically examines whether accurate prediction of semantic features from word embeddings implies genuine interpretability. Through extensive experiments using Partial Least Squares Regression and Feed Forward Neural Networks to map BERT embeddings to three feature norms (McRae, Buchanan, Binder), the authors demonstrate that high prediction accuracy primarily reflects geometric similarity within vector spaces rather than the emergence of semantic properties. The study reveals that random or nonsensical features can be predicted to some degree, and that the accuracy is bounded by methodological constraints rather than information overlap. These findings challenge the common assumption in the field that prediction accuracy serves as a valid proxy for feature-based interpretability.

## Method Summary
The study employs two mapping methods - PLSR and FFNN - to predict feature norms from BERT base layer 0 embeddings. The target norms include McRae (541×2526 sparse), Buchanan (4436×3981 sparse), and Binder (535×62 dense). Models are trained with optimal latent dimensions determined by MSE on test data to prevent overfitting. Evaluation uses F1@10 for sparse categorical norms and Spearman's ρ for dense continuous norms. The experiments include ablation studies with shuffled data and nonsense features to test whether results reflect genuine semantic overlap or methodological artifacts.

## Key Results
- Prediction accuracy is predominantly determined by algorithmic upper bounds rather than meaningful semantic representation
- High accuracy does not reliably indicate information overlap between embeddings and feature norms
- Random or nonsensical features can be predicted to some degree, indicating geometric similarity rather than semantic understanding
- Mapping embeddings primarily reflects geometric similarity within vector spaces rather than genuine emergence of semantic properties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High accuracy in mapping embeddings to feature norms is predominantly determined by the algorithmic upper bound and data structure (sparsity) rather than the emergence of semantic properties.
- **Mechanism:** The mapping function (PLSR/FFNN) hits a performance ceiling defined by how well the target matrix predicts itself (the upper bound). In sparse feature norms, this ceiling is low, meaning even "perfect" information overlap yields modest results, making it difficult to distinguish true semantic overlap from noise.
- **Core assumption:** The "self-prediction" accuracy of a feature norm matrix serves as a valid proxy for the methodological limit of any mapping approach.
- **Evidence anchors:**
  - [abstract] "results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation."
  - [section 5.1] "The prediction quality is close to the (very low) upper bound for the sparse norms."
- **Break condition:** If the target matrix is dense and low-dimensional (e.g., Binder norm), the upper bound rises, potentially conflating correlation metrics with semantic understanding even when predicting nonsense rankings (Section 5.4).

### Mechanism 2
- **Claim:** Mapping embeddings primarily captures geometric similarity (neighborhood structure) rather than specific property knowledge.
- **Mechanism:** Mapping algorithms optimize for variance or distance reduction in a latent space. They successfully align the "shape" of the embedding cloud with the feature norm cloud (preserving nearest neighbors) without necessarily decoding the individual boolean features that define specific concepts.
- **Core assumption:** Vector neighborhood preservation is distinct from feature-level decoding, though they may share a common cause (e.g., taxonomic grouping).
- **Evidence anchors:**
  - [abstract] "mapping embeddings primarily reflects geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties."
  - [section 6.1] "The information overlap is detectable by mapping embeddings to norms, but the information shared is geometric similarity, not individual features."
- **Break condition:** When evaluation relies solely on rank correlation (e.g., Spearman's ρ) on dense matrices, the geometric similarity metric may validate nonsensical data (e.g., character count differences) as meaningful.

### Mechanism 3
- **Claim:** Linear (PLSR) and non-linear (FFNN) mapping methods are functionally equivalent when hyperparameters are correctly tuned to prevent overfitting.
- **Mechanism:** Both methods effectively perform reduced-rank regression by projecting data into a lower-dimensional latent space (k). If the hidden layer size k is optimized via MSE on test data (the "elbow"), the non-linear advantages of FFNNs vanish, and both methods converge to similar performance limits.
- **Core assumption:** The equivalence holds specifically for the property inference task where the target space is heavily regularized by sparsity or specific rating distributions.
- **Evidence anchors:**
  - [section 3] "PLSR and FFNN... are mathematically very similar... only minor difference explain why their empirical performance typically differs only slightly."
  - [section 4.4] "The optimal 'elbow' point... is the same for PLSR and FFNNs."
- **Break condition:** If k is set too high (as in some prior work), FFNNs may appear superior due to overfitting training data, but this does not generalize to test data.

## Foundational Learning

- **Concept: Property Inference & Feature Norms**
  - **Why needed here:** This is the object of study. You must understand that "Feature Norms" are human-curated sets of properties (e.g., <"raven", "has_wings">), and "Property Inference" is the attempt to predict these from vector coordinates.
  - **Quick check question:** Can you distinguish between a "dense" continuous norm (ratings) and a "sparse" categorical norm (binary features)?

- **Concept: Sparsity & Upper Bounds**
  - **Why needed here:** The paper's central critique relies on how matrix sparsity limits predictability. Understanding that a 95% sparse matrix is harder to regress upon than a dense one is key to interpreting the "low upper bound" results.
  - **Quick check question:** Why does achieving 25% accuracy on a sparse norm might be technically "close to the upper bound" even if it seems low intuitively?

- **Concept: Reduced-Rank Regression**
  - **Why needed here:** To understand why PLSR and FFNNs behave similarly. Both compress the input dimension d down to a latent dimension k. If you don't get this, the equivalence of linear and non-linear models seems counter-intuitive.
  - **Quick check question:** What happens to the generalization of a mapping model if the latent dimension k exceeds the optimal "elbow" point?

## Architecture Onboarding

- **Component map:** BERT embeddings (X) -> PLSR/FFNN mapper (f) -> Feature norm matrix (Y) -> Evaluation metrics (F1@10, Spearman's ρ, Neighborhood Accuracy)

- **Critical path:**
  1. **Extraction:** Pull static embeddings for concept words
  2. **Alignment:** Align X and Y rows by concept
  3. **Tuning:** Find optimal latent dimension k using MSE on test data (prevent overfitting)
  4. **Mapping:** Train f: X → Y
  5. **Sanity Check:** Compare results against "Shuffle" and "Upper Bound" baselines to verify if the signal is semantic or geometric

- **Design tradeoffs:**
  - **Metric Selection:** F1@10 measures feature retrieval but fails on dense norms; Correlation (ρ) works for dense norms but can be fooled by structured noise (e.g., character counts). Use Neighborhood Accuracy to specifically target geometric structure.
  - **Complexity (k):** Lower k generalizes better but may miss variance; higher k fits training noise. Stick to the MSE "elbow."

- **Failure signatures:**
  - **The "Clever Hans" Mapping:** High training accuracy but test accuracy indistinguishable from the "Shuffle" baseline (predicting random features)
  - **Nonsense Correlation:** High ρ on Binder norms when predicting "Character Difference" (CDiff) instead of actual semantic features

- **First 3 experiments:**
  1. **Reproduce Equivalence:** Run PLSR and FFNN on McRae norms. Vary k (10 to 300) to verify they share the same MSE "elbow" point and test performance.
  2. **Upper Bound Calibration:** Map the McRae norm matrix to itself (Y → Y) to establish the maximum theoretical performance for that dataset.
  3. **Geometric vs. Semantic Ablation:** Run the mapping with the "Taxonomic Shuffle" (corrupting hypernyms). If results don't drop significantly, confirm the mechanism relies on geometry, not specific features.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the degree of sparsity in feature norms systematically affect the explanatory capacity of property inference methods, and is there a sparsity threshold beyond which meaningful interpretation becomes impossible?
- **Basis in paper:** [explicit] The authors state: "We did, however, not simulate experiments that evaluate different degrees of sparsity or different geometric shapes. Relating those metrics to explanatory potential is an important subject of future work."
- **Why unresolved:** The paper demonstrates that sparse categorical norms (McRae, Buchanan) and dense continuous norms (Binder) produce different artifacts, but does not systematically vary sparsity levels to establish causal relationships or identify critical thresholds.
- **What evidence would resolve it:** Controlled experiments with synthetic feature norms of varying sparsity levels, measuring when prediction accuracy begins to reflect information overlap versus methodological upper bounds.

### Open Question 2
- **Question:** Under what specific conditions can property inference methods genuinely reveal feature-based knowledge rather than merely capturing geometric similarity between vector spaces?
- **Basis in paper:** [explicit] From Section 8: "under which circumstances emerging property knowledge could be analyzed with inference methods. This will be subject to a range of future analyses."
- **Why unresolved:** The paper shows that current methods primarily capture geometric similarity, but does not establish whether modifications to methods, data, or evaluation could enable genuine property-based explanation.
- **What evidence would resolve it:** Identification of experimental configurations where corrupting semantic features produces expected performance drops, or development of methods that can predict individual features independently of neighborhood structure.

### Open Question 3
- **Question:** Can alternative evaluation metrics be developed that disentangle geometric similarity from property-based interpretation in embedding-to-norm mapping?
- **Basis in paper:** [inferred] The paper demonstrates that F1@N and Spearman correlation conflate geometric similarity with property prediction, and neighborhood accuracy captures geometric structure but not individual features, suggesting no current metric adequately measures property-based interpretability.
- **Why unresolved:** All standard evaluation measures examined either fail to distinguish genuine feature knowledge (F1, correlation) or measure a different construct entirely (neighborhood accuracy).
- **What evidence would resolve it:** Development of metrics that show sensitivity to feature-level corruption while remaining robust to geometric perturbations that preserve feature semantics.

## Limitations
- The equivalence of PLSR and FFNN assumes optimal hyperparameter tuning across all datasets
- The "upper bound" calibration method assumes self-prediction accuracy meaningfully represents methodological limits
- Geometric similarity mechanism requires more direct evidence to rule out alternative explanations
- Limited to BERT layer 0 embeddings and two mapping methods

## Confidence
- High confidence: The mathematical similarity between PLSR and FFNN, and their shared dependence on latent dimension k
- Medium confidence: The claim that high prediction accuracy predominantly reflects algorithmic upper bounds rather than semantic knowledge
- Medium confidence: The interpretation that mapping results primarily capture geometric similarity rather than feature-level decoding

## Next Checks
1. Conduct ablation studies systematically varying feature norm density and dimensionality to test how upper bound constraints affect prediction quality across different dataset structures
2. Design experiments using artificially constructed norms (e.g., based on character counts, random features, or controlled taxonomic structures) to directly test whether geometric similarity alone can produce high correlation scores
3. Compare prediction results using contextualized embeddings (BERT layers 1-12) versus static embeddings to determine if deeper layers provide genuinely different semantic information or simply maintain geometric similarity patterns