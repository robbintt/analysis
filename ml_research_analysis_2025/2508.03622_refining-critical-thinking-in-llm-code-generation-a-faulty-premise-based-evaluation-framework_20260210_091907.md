---
ver: rpa2
title: 'Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based
  Evaluation Framework'
arxiv_id: '2508.03622'
source_url: https://arxiv.org/abs/2508.03622
tags:
- premises
- code
- faulty
- function
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the vulnerability of large language models\
  \ (LLMs) in code generation tasks when encountering faulty premises, which can lead\
  \ to hallucinations and reduced trustworthiness. The authors propose FPBench, a\
  \ novel evaluation framework that systematically constructs three types of faulty\
  \ premises\u2014unrelated perturbation insertion, random-based deletion, and rule-based\
  \ deletion\u2014using importance score analysis."
---

# Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework

## Quick Facts
- arXiv ID: 2508.03622
- Source URL: https://arxiv.org/abs/2508.03622
- Reference count: 40
- Models show poor proactive error detection (PRER as low as 0.12) when encountering faulty premises in code generation tasks

## Executive Summary
This paper addresses the vulnerability of large language models (LLMs) in code generation when encountering faulty premises, which can lead to hallucinations and reduced trustworthiness. The authors propose FPBench, a novel evaluation framework that systematically constructs three types of faulty premises—unrelated perturbation insertion, random-based deletion, and rule-based deletion—using importance score analysis. They evaluate 15 representative LLMs across 1,800 problems, measuring proactive and passive error recognition rates along with self-scrutiny overhead ratios. Results show most models exhibit poor proactive error detection, heavily rely on explicit prompts for error detection, and demonstrate inefficient reasoning overhead when handling faulty premises.

## Method Summary
The authors constructed FPBench by sampling 600 problems from HumanEval and MBPP+, then creating three faulty premise variants for each: Rule-Based Deletion (RUD) using importance score-based deletion of the 2nd-ranked premise, Unrelated Perturbation Insertion (UPI) using GPT-4o-generated misleading hints at 8 AST node types, and Random-Based Deletion (RAD) deleting one category of elements. They evaluated 15 LLMs using three metrics: Proactive Error Recognition Rate (PRER), Passive Error Recognition Rate (PAER), and overhead ratios measuring token inflation. Importance scores were computed using a weighted combination of token frequency, node type, and structural position, with weights optimized via grid search.

## Key Results
- Most models exhibit poor proactive error detection (PRER averaging 0.23-0.57) compared to passive detection with explicit prompts (PAER 0.49-0.81)
- Different faulty premise types activate distinct defect patterns in models, revealing tripartite cognitive dissociation
- High overhead ratios (>1.8) often fail to improve accuracy, indicating verbose compensation without enhanced reasoning

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Dependent Scrutiny Activation
- **Claim:** LLMs possess latent error detection capabilities that remain dormant unless explicitly triggered by user instructions.
- **Mechanism:** The paper observes a significant gap between Proactive (PRER) and Passive (PAER) Error Recognition Rates. The mechanism suggests that model weights encode the ability to validate premises, but the default inference path prioritizes code generation over verification. Explicit prompts ("Check for errors") re-route attention mechanisms to prioritize validation logic.
- **Core assumption:** The capability to detect errors is learned during pre-training but suppressed during standard instruction-following alignment.
- **Evidence anchors:** [abstract] "Most models exhibit poor reasoning abilities... heavily relying on explicit prompts for error detection." [section] Table 1 shows PRER averaging 0.23-0.57 vs PAER 0.49-0.81 across 15 models.

### Mechanism 2: Tripartite Cognitive Dissociation
- **Claim:** Different categories of faulty premises activate distinct, independent failure modes in code generation models.
- **Mechanism:** The paper posits a "Triple Dissociation" where Unrelated Perturbation (UPI) tests logical conflict resolution, Rule-Based Deletion (RUD) tests dependency reasoning, and Random-Based Deletion (RAD) tests syntactic pattern matching. Performance variance across these types suggests independent cognitive subsystems rather than a single "reasoning" capability.
- **Core assumption:** Models do not possess a unified "critical thinking" module; rather, they rely on separate heuristics for syntax, logic, and context.
- **Evidence anchors:** [section] "Extensive Experiment" section notes RAD induces an 89% compilation error rate but explicit prompts can rescue it, confirming distinct pathways. [section] Table 2 & 3 show inconsistent model rankings across RUD, UPI, and RAD columns.

### Mechanism 3: Compensation via Redundancy (Overhead Ratio)
- **Claim:** Models compensate for logical uncertainty by inflating response length (verbosity) rather than increasing logical precision.
- **Mechanism:** The Self-Scrutiny Overhead Ratio (PROR/PAOR) measures token inflation. The paper suggests models use "statistical correlation" to generate plausible-looking tokens when logic fails. Beyond a threshold (overhead > 1.8), added length introduces noise (redundant code) without improving accuracy, indicating a "hallucination via verbosity" mechanism.
- **Core assumption:** The model's confidence mechanism is decoupled from its logical verification; low confidence manifests as verbose explanation rather than refusal or clarification.
- **Evidence anchors:** [section] Page 7 notes "point of diminishing returns... blindly increasing length fails to enhance quality." [section] Table 1 identifies GPT-4.1-mini with high overhead (2.42) but only moderate PRER (0.31).

## Foundational Learning

- **Concept: Proactive vs. Passive Error Recognition**
  - **Why needed here:** Standard benchmarks (like HumanEval) assume the prompt is correct. This concept distinguishes between a model's ability to *generate* code given a correct prompt (standard) vs. *validating* the prompt itself (proactive).
  - **Quick check question:** Does the model generate code that fails because the requirements were impossible, or does it flag the impossibility?

- **Concept: Importance-Based Deletion (vs. Random Deletion)**
  - **Why needed here:** Random deletion often creates syntactic garbage, while deleting the "second most important" premise creates logical traps. Understanding this distinction is required to replicate the FPBench dataset construction.
  - **Quick check question:** Why does deleting the *top* ranked premise cause a "collapse" that prevents evaluating self-scrutiny?

- **Concept: Hallucination as Conformity**
  - **Why needed here:** In code generation, "hallucination" is often functional but wrong (e.g., fits the wrong spec perfectly). The paper frames this as "conformist reasoning"—blindly following faulty premises.
  - **Quick check question:** If a model generates syntactically correct code that runs but produces the wrong output based on a misleading hint, is that a hallucination?

## Architecture Onboarding

- **Component map:** FPBench (1,800 problems) -> Perturbation Engine (UPI, RUD, RAD constructors) -> Evaluation Layer (PRER, PAER, Overhead metrics) -> 15 LLMs
- **Critical path:** Calculate Importance Score (Equation 4) for all premises in a problem -> Delete the **2nd ranked** premise -> Inject perturbation -> Model Inference -> Check for "error flag" in output string
- **Design tradeoffs:** Deleting 2nd vs 1st Premise: Deleting #1 premise causes immediate crash/compilation failure (too easy to spot or too hard to recover). Deleting #2 creates "logical debt" that requires reasoning to detect.
- **Failure signatures:**
  - **The "Compensator":** High PAOR (> 2.0) but low PRER. The model writes essays to explain code but misses the simple fact that the requirements are wrong.
  - **The "Obedient Servant":** High PAER but near-zero PRER. The model is smart enough to check if told, but servile enough to accept garbage inputs silently otherwise.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run 10 samples from the RUD set on a model (e.g., GPT-4o) with and without the "check for errors" suffix to confirm the PRER/PAER gap exists.
  2. **Ablation on Deletion Rank:** Compare model performance when deleting the #1 vs #2 vs #3 ranked premises. Verify if #2 is indeed the "sweet spot" for testing reasoning.
  3. **Overhead vs. Accuracy Scatter:** Plot PAOR (x-axis) vs PAER (y-axis) for all 15 models to visualize if "thinking longer" actually correlates with "thinking better" (Hypothesis: It does not beyond 1.8 ratio).

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes prompt-based instruction can reliably trigger latent error-detection capabilities, but this may not generalize to all model architectures
- Human annotation with 2/3 agreement threshold introduces potential subjectivity in faulty premise construction and validation
- The study focuses on Python code generation, limiting generalizability to other programming languages or domains

## Confidence
- **High Confidence:** The tripartite dissociation mechanism (RUD/UPI/RAD showing distinct failure patterns) is strongly supported by quantitative evidence across 15 models
- **Medium Confidence:** The claim that models possess latent error-detection capabilities is well-evidenced by PRER/PAER gaps, but the exact mechanism of how instruction triggers these capabilities remains theoretical
- **Medium Confidence:** The overhead ratio analysis showing diminishing returns at 1.8x verbosity is statistically supported but may vary with different problem complexities or model families

## Next Checks
1. **Cross-Domain Transfer:** Test the same FPBench framework on non-code generation tasks (e.g., mathematical reasoning or factual QA) to verify if tripartite dissociation holds across domains
2. **Temperature Sensitivity:** Systematically vary inference temperature (0.0 to 1.0) to determine if lower temperatures reduce the PRER/PAER gap, indicating a stochastic rather than structural limitation
3. **Model Surgery:** Compare models with different instruction-tuning approaches (e.g., RLHF vs. DPO) to isolate whether the instruction-dependency stems from alignment methodology rather than pre-training