---
ver: rpa2
title: 'Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems'
arxiv_id: '2512.01661'
source_url: https://arxiv.org/abs/2512.01661
tags:
- unsolvable
- solvable
- reasoning
- uni00000013
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of LLM reliability by addressing
  the gap between objective unsolvability (logical contradictions) and subjective
  capability limits (tasks exceeding model competence). Current models often conflate
  these, leading to hallucinations on unsolvable problems.
---

# Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems

## Quick Facts
- arXiv ID: 2512.01661
- Source URL: https://arxiv.org/abs/2512.01661
- Reference count: 40
- Primary result: UnsolvableRL achieves >85% unsolvable detection and boosts solvable reasoning accuracy from 43.4% to 69.4% on Qwen3-4B-Instruct

## Executive Summary
This paper addresses LLM reliability by tackling the gap between objective unsolvability (logical contradictions) and subjective capability limits (tasks exceeding model competence). Current models often conflate these, leading to hallucinations on unsolvable problems. The authors introduce UnsolvableQA, a dataset constructed via "Reverse Construction" that injects logical contradictions into valid reasoning chains, and UnsolvableRL, an RL alignment framework with three orthogonal objectives: detecting inherent unsolvability, penalizing false unsolvable declarations, and calibrating refusal for difficult tasks. Empirically, their method achieves >85% unsolvable detection and boosts solvable reasoning accuracy from 43.4% to 69.4% on Qwen3-4B-Instruct.

## Method Summary
The UnsolvableRL framework trains LLMs to distinguish between three problem types: solvable, inherently unsolvable (logical contradictions), and difficult-but-solvable (subjective limits). The method uses GRPO optimization with three reward components: R_acc for correct solvable answers, R_detect for correctly identifying unsolvable problems with a −0.5 penalty for false declarations, and R_cal for calibrated refusal on difficult tasks. Training uses a progressive threshold schedule (τ: 0.4→1.0) to maintain refusal incentives. The UnsolvableQA dataset is constructed via reverse construction for math problems (injecting contradictions into AIME problems) and SAT/CSP/DFS-based generation for puzzles (Hamiltonian cycles, mazes, etc.).

## Key Results
- >85% unsolvable detection rate on test set
- Solvable reasoning accuracy improves from 43.4% to 69.4% on Qwen3-4B-Instruct
- Ablation studies show unsolvable data is prerequisite for detection; penalty acts as regularizer for rigor when data is included
- Pareto improvement over baselines with balanced S/U/M metrics

## Why This Works (Mechanism)

### Mechanism 1: Unsolvable Data as Detection Prerequisite
Explicit unsolvable training examples are necessary for models to learn objective unsolvability detection; optimizing solely on solvable tasks does not transfer. The model learns a decision boundary between solvable and unsolvable problem structures only when exposed to supervised unsolvable instances. Without these, the model has no signal to distinguish inherent contradictions from merely difficult problems. If unsolvable data distribution diverges significantly from deployment distribution, detection may not transfer.

### Mechanism 2: Penalty as Conditional Regularizer
The false unsolvability penalty (ρ = −0.5) has dual effects depending on data context: it causes capability collapse without unsolvable data, but improves solvable accuracy when paired with unsolvable data. The penalty discourages over-refusal on solvable problems. Without positive examples of valid refusal (unsolvable instances), the model minimizes expected penalty by never refusing. With unsolvable data, the model learns when refusal is justified, and the penalty enforces stricter reasoning on borderline cases. This mechanism is sensitive to hyperparameter choice; stricter penalties (e.g., ρ = −2.0) suppress detection excessively.

### Mechanism 3: Dynamic Threshold Prevents Calibration Collapse
A progressive target accuracy threshold (τ) schedule is required to maintain refusal incentives throughout training; fixed thresholds cause reward collapse once model capability exceeds τ. When β (rollout accuracy) > τ with fixed threshold, refusal reward becomes negative, disincentivizing all refusal. Progressive τ increases (0.4 → 1.0) maintains τ ≳ β, keeping refusal reward positive for genuinely uncertain queries. If training converges faster than τ increases, the schedule may become misaligned with actual capability.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL backbone uses group-based advantage normalization instead of a critic model; understanding this clarifies why rewards are computed as deviations from group mean.
  - Quick check question: Can you explain why GRPO eliminates the need for a value function and how this affects reward scale sensitivity?

- **Concept: Detection vs Calibration Distinction**
  - Why needed here: The framework treats objective unsolvability (inherent contradictions) and subjective capability limits as orthogonal axes with different reward structures.
  - Quick check question: Given a math problem with contradictory constraints vs. a valid but extremely hard problem, which reward mechanism applies to each?

- **Concept: Reward Shaping with Penalties**
  - Why needed here: The −0.5 penalty for false unsolvability declarations is theoretically motivated via decision-theoretic analysis to prevent universal rejection.
  - Quick check question: If penalty were 0, what would the critical belief threshold for refusal become, and why is this problematic?

## Architecture Onboarding

- **Component map:** Reverse Construction pipeline (math) → contradiction injection → SAT/CSP/DFS verification → UnsolvableQA dataset → GRPO RL training with three reward components → calibrated model
- **Critical path:** 1. Verify data quality (unsolvable instances must be provably unsolvable; solvable instances must have verified solutions). 2. Initialize τ = 0.4, penalty ρ = −0.5, group size G = 12. 3. Monitor both Solvable Accuracy (S) and Unsolvable Detection (U) during training; if U collapses, check unsolvable data ratio.
- **Design tradeoffs:** Stricter penalties → higher S, lower U (e.g., ρ = −2.0 drops U from 75.8% to 69.4%). Higher initial τ → more early refusal, potentially slower convergence. KL penalty disabled → broader exploration but less stable; relies on clipping.
- **Failure signatures:** Capability Collapse: Detection rate drops to near-zero (e.g., 4.2% for 1.7B model with penalty but no unsolvable data). Over-refusal: Rejection rate high but accuracy drops (TruthRL shows this pattern). Static threshold collapse: Rejection rate becomes very low (<5%) as model improves.
- **First 3 experiments:** 1. Reproduce the ablation w/o unsolvable data w/ penalty on a small model to observe capability collapse; confirm U drops dramatically. 2. Compare fixed τ = 0.4 vs progressive τ (0.4 → 1.0) for 40 steps; verify dynamic threshold maintains higher rejection rate. 3. Test penalty values {−0.5, −1.0, −2.0} at step 40 to confirm ρ = −0.5 is optimal; log S/U/M for each domain.

## Open Questions the Paper Calls Out

- **Question 1:** Does unsolvability detection capability scale effectively to larger model architectures (70B+ parameters), or does the capability-refusal trade-off require different calibration strategies at scale?
  - Basis: "Due to computational resource constraints, we were unable to conduct a more exhaustive set of ablation studies, such as scaling experiments on larger model architectures."
  - Resolution evidence: Evaluation of UnsolvableRL on 70B+ models (e.g., Qwen3-72B) with identical hyperparameters, comparing S/U/M metrics to baseline.

- **Question 2:** Can unsolvability detection trained on structured puzzles and mathematics generalize to more ambiguous, real-world unsolvable problems (e.g., legal contradictions, clinical infeasibility)?
  - Basis: "While the UnsolvableQA dataset is relatively comprehensive within the selected domains (logic puzzles, mathematics, mazes), it may not fully encompass the various forms of inherently unsolvable problems encountered in real-world applications."
  - Resolution evidence: Zero-shot evaluation on a constructed dataset of real-world contradictory scenarios (e.g., conflicting legal clauses, medically impossible conditions).

## Limitations

- Only Qwen3-1.7B and 4B were tested; scalability to larger models remains unverified
- Current contradictions are synthetic and formally verifiable; real-world unsolvability involves semantic ambiguity and domain-specific knowledge
- The linear threshold schedule (τ: 0.4→1.0) is empirically chosen without comparison to alternative progressive schedules

## Confidence

- **High confidence:** Unsolvable data is prerequisite for detection; dynamic threshold prevents calibration collapse
- **Medium confidence:** Penalty acts as conditional regularizer; 85% detection threshold is meaningful
- **Low confidence:** Generalizability of unsolvable distribution; optimal penalty value sensitivity

## Next Checks

1. **Generalization Stress Test:** Apply the trained model to externally sourced unsolvable problems from different domains (e.g., programming challenges, logical puzzles not in UnsolvableQA) to verify detection boundary generalization beyond the training distribution.

2. **Penalty Sensitivity Analysis:** Systematically vary the false unsolvability penalty (ρ ∈ {−0.2, −0.5, −1.0, −2.0}) and measure the full Pareto front of solvable accuracy vs. unsolvable detection rate to confirm ρ = −0.5 is truly optimal.

3. **Cross-Model Transferability:** Train the UnsolvableRL framework on Qwen2.5-7B and test transfer to Qwen3-8B to determine whether the alignment strategy scales with model size and whether detection capability transfers between model families.