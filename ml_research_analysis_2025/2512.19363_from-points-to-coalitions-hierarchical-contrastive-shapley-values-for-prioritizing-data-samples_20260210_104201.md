---
ver: rpa2
title: 'From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing
  Data Samples'
arxiv_id: '2512.19363'
source_url: https://arxiv.org/abs/2512.19363
tags:
- data
- valuation
- shapley
- hcdv
- leaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the computational challenge of applying Shapley
  value theory to large-scale data valuation, which traditionally suffers from factorial
  complexity. HCDV introduces a three-stage framework: learning contrastive geometry-preserving
  embeddings, recursively clustering data into a balanced hierarchy, and computing
  Shapley-style payoffs for coalitions via local Monte-Carlo games with budget propagation.'
---

# From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples

## Quick Facts
- arXiv ID: 2512.19363
- Source URL: https://arxiv.org/abs/2512.19363
- Reference count: 33
- Primary result: Hierarchical contrastive Shapley framework that improves predictive accuracy by up to 5 percentage points and reduces valuation time by up to 100x compared to state-of-the-art methods

## Executive Summary
This paper introduces HCDV, a hierarchical data valuation framework that addresses the factorial computational complexity of traditional Shapley value estimation. By leveraging contrastive learning to capture data geometry and recursively clustering data into a balanced hierarchy, HCDV transforms the valuation problem from point-wise to coalition-based games. The framework achieves sub-Gaussian concentration and bounded regret for top-k selection while demonstrating significant improvements in predictive accuracy (up to 5 percentage points) and runtime efficiency (up to 100x speedup) across diverse benchmarks including tabular data, vision tasks, streaming scenarios, and large-scale CTR applications.

## Method Summary
HCDV implements a three-stage pipeline: (I) contrastive embedding learning with predictive utility and dispersion objectives, (II) balanced k-means hierarchy construction on embeddings, and (III) top-down Monte Carlo Shapley computation with budget propagation. The method approximates point-wise Shapley values by computing coalition Shapley values in local games at each hierarchy level, then distributing parent budgets to children. This reduces computational complexity from factorial to O(TK_max log n) where T is permutation budget, K_max is maximum branching factor, and n is dataset size. The framework includes incremental update capability for streaming data by reusing cached hierarchy values and recomputing only affected subtrees.

## Key Results
- Improves predictive accuracy by up to 5 percentage points on top-30% valued points compared to state-of-the-art methods
- Reduces valuation runtime by up to 100x compared to exact Shapley computation
- Maintains 99.6% utility with 2.5× speedup in streaming settings using incremental updates
- Demonstrates strong performance across diverse tasks: tabular (UCI Adult), vision (Fashion-MNIST), streaming, and large-scale CTR (Criteo-1B)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Coalition Games for Complexity Reduction
HCDV converts factorial point-wise Shapley computation to log-linear hierarchical coalition games by building a balanced cluster hierarchy and computing Shapley values for coalitions via local Monte Carlo games with budget propagation. This confines O(n!) combinatorial search to small K-player games per level. The core assumption is that data has exploitable geometric/semantic structure capturable by clustering. If data lacks such structure (e.g., pure noise), clustering fails to produce meaningful groups, degrading valuation quality even if complexity reduction holds.

### Mechanism 2: Contrastive Dispersion as a Payoff Component
The framework rewards points that sharpen decision boundaries by including a contrastive dispersion term in the characteristic function. This term (sum of inter-class distances) becomes part of v_ℓ(S), so coalitions contributing to class separation receive higher Shapley payoffs. The core assumption is that points increasing inter-class distance in embedding space improve generalization. If the downstream task is not classification or the distance metric is not meaningful, the contrastive term becomes undefined or nonsensical.

### Mechanism 3: Incremental Updates via Hierarchy Reuse
HCDV enables efficient incremental updates for streaming data without full recomputation by embedding new points and assigning them to nearest leaf nodes, then recomputing only affected leaf and ancestor Shapley values. The core assumption is that distribution shifts are modest enough for meaningful cluster assignment. Significant distribution shifts (new classes, feature modes) spawn many new leaves, forcing widespread recomputation and negating efficiency gains.

## Foundational Learning

- **Concept: Shapley Value (Game Theory)**
  - Why needed here: HCDV approximates this core value. You must understand its definition (marginal contribution over permutations) and axioms (efficiency, symmetry, dummy, additivity) to grasp what HCDV preserves and what it trades off.
  - Quick check question: Why is exact Shapley computation O(n!) infeasible for n=1000?

- **Concept: Contrastive Representation Learning**
  - Why needed here: Stage I learns embeddings where the contrastive loss Δ_c(S) pushes different-class points apart. The entire valuation relies on this geometry.
  - Quick check question: What does the contrastive term encourage the model to do with embeddings from different classes?

- **Concept: Monte-Carlo Estimation**
  - Why needed here: Coalition Shapley values are estimated via permutation sampling. Theoretical guarantees (e.g., Proposition 1's Hoeffding bound) are probabilistic.
  - Quick check question: How does increasing permutation budget T affect estimation accuracy?

## Architecture Onboarding

- **Component map:** Raw data -> Contrastive Embedding Model -> Balanced Cluster Hierarchy -> Hierarchical Shapley Valuation Engine -> Data Values
- **Critical path:** Valuation Engine—the loop of: (a) estimate coalition Shapley at level ℓ, (b) normalize, (c) distribute budget to children, (d) repeat at ℓ+1. Bugs in propagation or normalization invalidate allocation.
- **Design tradeoffs:**
  - Hierarchy depth (L) / branching (K) vs. cost/granularity: Deeper trees = finer valuation but higher cost
  - Permutation budget (T) vs. accuracy: Higher T = more stable estimates but linear runtime increase
  - Leaf size (M) vs. precision/speed: Small M = more exact leaf Shapley; large M forces uniform splits, losing granularity
- **Failure signatures:**
  - Value explosion/implosion: Incorrect normalization causes budgets to grow/shrink exponentially with depth
  - Uniform leaf values: M set too large triggers uniform split, erasing fine-grained valuation
  - Stagnant streaming values: Threshold τ too high forces new data into existing leaves, failing to capture new concepts
- **First 3 experiments:**
  1. Ablation on T: Run HCDV on synthetic data with T ∈ {16, 64, 256, 1024}. Measure ranking stability (rank correlation) and runtime; verify sub-Gaussian concentration.
  2. Hyperparameter scan (K, L): On UCI Adult, vary branching factors and depths; evaluate downstream performance (AUC) on top-30% valued data. Identify cost/quality sweet spot.
  3. Incremental stress test: Simulate a stream with sudden distribution shift. Compare HCDV-Inc vs. HCDV-Full on final utility and tree rebuilds. Monitor where efficiency breaks down.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can HCDV be adapted to perform data valuation in federated learning environments where data is distributed across silos and cannot be centralized?
  - Basis in paper: The conclusion states, "Future work includes federated valuation and active data acquisition."
  - Why unresolved: The current framework relies on centralized embedding training and global hierarchy construction requiring access to all samples simultaneously.
  - What evidence would resolve it: A federated implementation that computes local Shapley payoffs or cluster structures without sharing raw data while maintaining O(TK_max log n) efficiency bounds.

- **Open Question 2:** Can HCDV be integrated into an active data acquisition pipeline to dynamically select which unlabeled samples to label next?
  - Basis in paper: The conclusion explicitly lists "active data acquisition" as a future direction.
  - Why unresolved: The paper demonstrates "active sampling" (re-ranking a fixed pool) but not the iterative, budgeted process of querying new labels from an unlabeled pool where embeddings must be updated dynamically.
  - What evidence would resolve it: An active learning loop where HCDV identifies high-value unlabeled regions, queries their labels, and updates the model with provable sample complexity improvements over standard uncertainty sampling.

- **Open Question 3:** How sensitive is the valuation stability and surplus loss to the specific choice of hierarchy construction algorithm (e.g., balanced k-means vs. spectral clustering)?
  - Basis in paper: The paper uses "balanced k-means... by default" but notes "any deterministic or stochastic clustering is admissible" without providing ablations on this component.
  - Why unresolved: The quality of the "geometry-aware" hierarchy directly determines the validity of local Shapley games; poor clustering could lead to suboptimal budget propagation or high surplus loss.
  - What evidence would resolve it: An ablation study comparing HCDV performance using various clustering methods (agglomerative, spectral, random partitioning) against the default balanced k-means on OpenDataVal benchmarks.

## Limitations

- The framework's effectiveness depends on data having exploitable geometric/semantic structure capturable by clustering; pure noise data would degrade valuation quality
- Streaming update efficiency claims remain theoretical without extensive validation on severe concept drift scenarios with significant distribution shifts
- Several hyperparameters (λ, α, M, K_l) appear dataset-specific and their optimal selection is not fully automated

## Confidence

- **High:** Complexity reduction from factorial to log-linear, sub-Gaussian concentration of estimates, and runtime benchmarks are well-supported by mathematical derivation and controlled experiments
- **Medium:** Theoretical guarantees for top-k selection regret and approximation of Shapley axioms hold under stated conditions but depend on clustering quality assumption
- **Low:** Streaming update efficiency claims and the general applicability of contrastive dispersion in non-classification tasks lack extensive empirical validation across diverse data regimes

## Next Checks

1. **Ablation on permutation budget T:** Run HCDV on synthetic data with T ∈ {16, 64, 256, 1024}. Measure ranking stability (rank correlation) and runtime; verify sub-Gaussian concentration.
2. **Hyperparameter scan (K, L):** On UCI Adult, vary branching factors and depths; evaluate downstream performance (AUC) on top-30% valued data. Identify cost/quality sweet spot.
3. **Incremental stress test:** Simulate a stream with sudden distribution shift. Compare HCDV-Inc vs. HCDV-Full on final utility and tree rebuilds. Monitor where efficiency breaks down.