---
ver: rpa2
title: 'DepressionX: Knowledge Infused Residual Attention for Explainable Depression
  Severity Assessment'
arxiv_id: '2501.14985'
source_url: https://arxiv.org/abs/2501.14985
tags:
- depression
- attention
- severity
- knowledge
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DepressionX is a knowledge-infused residual attention model for
  explainable depression severity detection from social media posts. It integrates
  word-, sentence-, and post-level embeddings with a domain-specific knowledge graph
  to capture both textual and contextual depression-related information.
---

# DepressionX: Knowledge Infused Residual Attention for Explainable Depression Severity Assessment

## Quick Facts
- **arXiv ID**: 2501.14985
- **Source URL**: https://arxiv.org/abs/2501.14985
- **Reference count**: 10
- **Primary result**: F1 scores of 82.5% on imbalanced and 90.9% on balanced datasets, outperforming state-of-the-art models by over 7% in F1 score.

## Executive Summary
DepressionX is a knowledge-infused residual attention model for explainable depression severity detection from social media posts. It integrates word-, sentence-, and post-level embeddings with a domain-specific knowledge graph to capture both textual and contextual depression-related information. The model employs residual multi-head attention for representation learning and uses ordinal regression to classify depression severity into four levels. Knowledge graph representation is learned via GIN and GAT layers to enhance the model's contextual understanding. Explainability is achieved through attention-based feature highlighting and knowledge subgraph extraction.

## Method Summary
The method integrates multi-level textual representations (word, sentence, post) with domain-specific knowledge graph infusion. FastText provides word-level embeddings while Sentence Transformer handles sentence and post-level representations. Residual multi-head attention captures hierarchical semantic information across these levels. A depression-specific knowledge graph is constructed via relation extraction from Wikipedia, filtered by cosine similarity to BDI-II symptoms, and processed through GIN and GAT layers. Ordinal regression with soft labels respects the ordered nature of severity levels. The model concatenates textual and knowledge graph representations for final classification.

## Key Results
- Achieves F1 scores of 82.5% on imbalanced dataset and 90.9% on balanced dataset
- Outperforms state-of-the-art models by over 7% in F1 score
- Demonstrates improved class separation through knowledge graph infusion, validated by UMAP visualizations

## Why This Works (Mechanism)

### Mechanism 1: Multi-level Textual Representation with Residual Attention
Depression indicators manifest at multiple linguistic granularities—individual words, sentence-level emotional statements, and overall post context. The model uses FastText for word-level embeddings (300-dim, handles OOV words via subwords), Sentence Transformer for sentence and post-level embeddings (768-dim), then applies residual multi-head attention across these levels. Word-level MHA produces attentive word representations; sentence-level MHA aggregates these with sentence embeddings; final representation combines with post-level embedding via concatenation and feed-forward networks.

### Mechanism 2: Knowledge Graph Infusion for Domain Context
Clinical depression knowledge (symptoms, treatments, related conditions) provides contextual priors that improve detection beyond statistical text patterns. The model constructs a depression KG by feeding depression-related datasets to REBEL, extracting entities/relations from Wikipedia, then filtering entities by cosine similarity (>0.5) to BTI-II symptoms. Node embeddings are generated via Sentence Transformer on entity summaries. Graph representation is learned through 2 sequential GIN layers (2-hop neighbor aggregation) followed by 1 GAT layer (3-hop attention-weighted aggregation), then max-pooled to produce final KG vector.

### Mechanism 3: Ordinal Regression for Ordered Severity Classes
Depression severity exists on a continuum; the ordinal relationship between classes provides supervisory signal that standard categorical cross-entropy ignores. Instead of standard one-hot labels, soft labels are computed using distance-based cost: yp_i = exp(-φ(rp,ri))/Σexp(-φ(rp,rk)), where φ(rp,ri) = β|rp - ri| and β controls penalty magnitude. This ensures misclassifying "severe" as "moderate" has lower loss than misclassifying as "minimal."

## Foundational Learning

### Concept: Multi-Head Attention Mechanisms (Transformer Architecture)
- **Why needed here**: Core to capturing which words/sentences drive severity predictions. Attention weights provide explainability (highlighting important words) and representation learning (weighted aggregation of input features).
- **Quick check question**: Given Q (query), K (key), V (value) matrices, can you write the attention formula Attention(Q,K,V) = softmax(QK^T/√d) × V? What does √d achieve, and why use multiple heads instead of one?

### Concept: Graph Neural Networks—GIN (Graph Isomorphism Network) and GAT (Graph Attention Network)
- **Why needed here**: Required to understand how the model learns knowledge graph representations by iteratively aggregating neighbor information. GIN aggregates via sum (structurally discriminative); GAT uses learned attention weights (interpretable importance).
- **Quick check question**: In GIN, node v's representation is updated as: h_v^(k) = MLP((1+ε)·h_v^(k-1) + Σ h_u^(k-1) for u∈N(v)). What does the (1+ε) term achieve? How does GAT's attention α_ij = softmax(LeakyReLU(a^T[Θh_i || Θh_j])) differ from GIN's uniform neighbor weighting?

### Concept: Ordinal vs. Nominal Classification
- **Why needed here**: Understanding why the model treats severity classes as ordered rather than independent categories—a design choice that affects loss computation and evaluation.
- **Quick check question**: In standard cross-entropy loss, misclassifying "severe" as "mild" has the same penalty as misclassifying as "minimal." Given the soft label formula yp_i = exp(-β|rp - ri|)/Σexp(-β|rp - rk|), what happens to the probability distribution when β increases? What value of β would make the loss approach standard categorical cross-entropy?

## Architecture Onboarding

### Component map:
```
Input: Social media post (text)
│
├── TEXTUAL BRANCH
│   ├── Word-level: FastText pretrained embeddings (300-dim)
│   │   └── Multi-head Attention → ΓW'ij (attention weights) + W'ij (representations)
│   ├── Sentence-level: SentenceTransformer (768-dim, RoBERTa-based)
│   │   └── Multi-head Attention → ΓS'i (attention weights) + S'i (representations)
│   ├── Post-level: SentenceTransformer (768-dim)
│   │   └── Feed-forward Network
│   └── Fusion: Si = {W'j} ⊕ FFN(S'i) → p'i = Si ⊕ FFN(pi)
│
├── KNOWLEDGE GRAPH BRANCH
│   ├── Construction: REBEL extracts (head, relation, tail) triplets from Wikipedia
│   │   └── Filter by BDI-II cosine similarity > 0.5 → ~100 entities
│   ├── Node embeddings: SentenceTransformer on entity summaries (768-dim)
│   └── Graph encoding: GIN(layer 1) → GIN(layer 2) → GAT → MaxPool
│       └── Output: graph vector g
│
├── KNOWLEDGE INFUSION
│   └── Concatenate: zi = p'i ⊕ g → FFN → Softmax
│
└── OUTPUT: 4-class severity (minimal, mild, moderate, severe)
    └── Loss: Cross-entropy with ordinal soft labels
```

### Critical path:
1. **Input preprocessing → FastText/SentenceTransformer encoding**: Must handle informal language, slang, and OOV words. FastText's subword approach is specifically chosen for social media data quality.
2. **KG construction → Entity extraction**: REBEL quality directly impacts downstream. Cosine similarity filtering against BDI-II symptoms determines which entities are retained.
3. **Residual attention fusion**: Hierarchical attention (word → sentence → post) captures importance at each level. Residual connections preserve information flow.
4. **Ordinal loss computation**: Soft label generation based on distance from true severity; β=3.0 per hyperparameter tuning.

### Design tradeoffs:
- **FastText vs. contextual embeddings (BERT/RoBERTa)**: FastText chosen for computational efficiency and subword handling, but sacrifices context-dependent meaning. Paper reports BERT baseline achieves only 0.610 F1 on D1—suggesting domain adaptation matters more than architecture choice.
- **GIN → GIN → GAT sequencing**: GIN provides strong structural representation (theoretically maximally expressive among GNNs); GAT adds interpretability via attention on 3-hop neighbors. **Assumption**: This sequencing captures both local structural patterns and attention-weighted distant relations. No ablation comparing alternative orderings.
- **Max pooling vs. mean pooling for KG**: Max pooling retains strongest signals across nodes but may lose distributed information. Alternative (mean pooling) would preserve average signal.
- **UMAP visualization (Figure 3) shows**: KG infusion improves class separation—clusters become more distinct after knowledge adaptation. **Evidence**: "After infusion domain knowledge, we can clearly see a significant enhancement in the distinction between various classes."
- **Block 1 alone outperforms full model on D1** (0.840 vs 0.825 F1): Shorter posts in D1 may not benefit from higher-level context; KG introduces redundancy or overfitting risk. Model complexity should match data characteristics.

### Failure signatures:
- **D1 precision/recall imbalance**: 97.4% precision but only 71.8% recall—model is highly conservative, generating fewer false positives but missing true positives. Clinical application may prefer higher recall.
- **Performance drop on imbalanced dataset**: F1 drops from 90.9% (D2 balanced) to 82.5% (D1 imbalanced). Class imbalance handling is not explicitly addressed beyond ordinal loss.
- **Attention heatmap reliance on explicit signals**: Figure 5 shows model attends to "abusive words, words with sexual references, slang, swear words, compulsive words"—may fail on subtle depression expressions that lack explicit emotional language.
- **KG subgraph extraction dependency**: Explainability via KG requires extracting relevant subgraph; unclear how this performs for posts with no clear KG entity matches.

### First 3 experiments:
1. **Reproduce ablation baseline (Block 1 only)**: Train with word-level embeddings + MHA alone. Target ~0.840 F1 on D1 per paper's Table 2. Verify FastText handling of social media informal language. This establishes reproducibility baseline.
2. **Quantify KG marginal contribution**: Train full model vs. Blocks 1+2+3 (without KG). Compare F1 scores and visualize UMAP clusters before/after knowledge infusion. This isolates KG benefit and validates Figure 3's qualitative claim.
3. **Attention sanity check on error cases**: Extract top-10 attended words/sentences for correctly classified vs. misclassified posts across all severity levels. Verify model attends to clinically relevant terms (per BDI-II symptoms) and not dataset artifacts. Check if misclassifications correlate with specific attention patterns (e.g., attending to irrelevant keywords).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the inclusion of multimodal features (e.g., images, behavioral metadata) alongside textual content significantly improve the robustness of depression severity detection?
- **Basis in paper**: [explicit] The authors explicitly identify this as a limitation, stating the methodology "exclusively concentrates on the textual content... disregarding other potential multimodal features, including behavioural information."
- **Why unresolved**: The current architecture is designed solely for text and knowledge graph inputs, lacking the input channels and fusion mechanisms required to process visual or user-behavior data streams.
- **What evidence would resolve it**: An extension of the model to ingest and fuse visual/metadata features, followed by a comparative performance analysis against the text-only baseline.

### Open Question 2
- **Question**: To what extent does incorporating social interconnections and network structures enhance the model's contextual understanding compared to analyzing posts in isolation?
- **Basis in paper**: [explicit] The conclusion notes that "the model's insights could be further enhanced by considering the social interconnections among users," implying the current single-post approach is a limitation.
- **Why unresolved**: The current "Post-Level" representation treats each post as an independent instance, ignoring the user's social graph (interactions, followers, community standing) which often provides context in mental health analysis.
- **What evidence would resolve it**: A modified architecture that utilizes Graph Neural Networks to aggregate user-neighborhood data, demonstrating improved F1 scores on the same datasets.

### Open Question 3
- **Question**: Is the Knowledge Graph (KG) infusion module detrimental to performance on datasets dominated by shorter, concise posts?
- **Basis in paper**: [inferred] The ablation study (Table 2) shows that the full model with KG (F1 0.825) performed slightly worse than the text-only model (F1 0.835) on dataset D1, which the authors suggest is due to D1 having "shorter posts."
- **Why unresolved**: It is unclear if the static KG introduces noise or redundancy when the input text lacks sufficient context or entity density to link effectively to the knowledge base.
- **What evidence would resolve it**: A diagnostic experiment measuring the performance delta of the KG module across varying post lengths to identify the specific text-length threshold where KG infusion becomes beneficial.

## Limitations

- **Multimodal data limitation**: The model exclusively uses textual content, disregarding potentially valuable behavioral and multimodal features that could enhance depression severity detection.
- **Single post isolation**: The architecture analyzes posts in isolation without considering social interconnections among users, missing contextual information from social network structures.
- **KG relevance uncertainty**: The knowledge graph infusion may introduce noise or redundancy, particularly for shorter posts that lack sufficient context to benefit from external knowledge.

## Confidence

- **High Confidence**: The ordinal regression mechanism and multi-level textual encoding architecture are well-specified and theoretically sound.
- **Medium Confidence**: The KG infusion benefits are supported by qualitative UMAP visualizations showing improved class separation, but lack rigorous ablation studies.
- **Low Confidence**: The reproducibility of results across different datasets and the generalizability to non-Reddit social media platforms are not validated.

## Next Checks

1. **KG Sensitivity Analysis**: Vary the cosine similarity threshold (0.4 to 0.7) for entity filtering and measure the impact on F1 scores to quantify robustness to KG quality variations.
2. **Cross-Platform Generalization**: Test the model on a different social media corpus (e.g., Twitter) to assess whether learned representations transfer beyond Reddit.
3. **Clinical Expert Evaluation**: Have clinical psychologists review the top-20 attended words/sentences for correctly and incorrectly classified posts to validate whether attention aligns with clinical depression indicators.