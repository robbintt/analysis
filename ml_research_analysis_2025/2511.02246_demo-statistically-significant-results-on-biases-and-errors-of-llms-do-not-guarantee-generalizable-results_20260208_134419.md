---
ver: rpa2
title: 'Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not
  Guarantee Generalizable Results'
arxiv_id: '2511.02246'
source_url: https://arxiv.org/abs/2511.02246
tags:
- patient
- disorder
- medical
- llms
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the reliability of LLM-based evaluations for
  detecting hallucinations, omissions, and demographic biases in medical chatbot responses.
  The authors introduce a comprehensive pipeline that automatically generates diverse
  patient prompts, produces answers from multiple LLMs, and evaluates them using several
  LLM-as-a-judge setups and agentic workflows.
---

# Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results

## Quick Facts
- arXiv ID: 2511.02246
- Source URL: https://arxiv.org/abs/2511.02246
- Reference count: 40
- Primary result: Low inter-LLM agreement (κ=0.118) shows that statistically significant bias findings from a single LLM evaluator may not generalize

## Executive Summary
This work investigates the reliability of LLM-as-a-judge evaluations for detecting hallucinations, omissions, and demographic biases in medical chatbot responses. The authors generate 3.2M diverse synthetic patient prompts across 37 mental health disorders and evaluate 29K responses from three LLMs using four different evaluation setups. Their key finding is that while some (answering, evaluator) LLM pairs produce statistically significant bias results, these findings are not generalizable—different evaluator LLMs can reach opposite conclusions on the same data. This demonstrates that single-LLM evaluations can produce non-generalizable results, emphasizing the need for multiple evaluators and reporting inter-LLM agreement metrics.

## Method Summary
The authors develop a comprehensive pipeline that automatically generates synthetic patient prompts with diverse demographics, clinical histories, and writing styles. These prompts are used to elicit responses from three medical LLMs (Llama3-ChatQA-1.5-8B, BioMistral-7B, MedGemma-4B-it) at temperature 0.1. Four evaluation methods assess the responses: three LLM-as-a-judge approaches (Llama3-ChatQA-1.5-8B, Qwen2.5-7B-Instruct, OLMo-2-1124-13B) and an agentic workflow (Mistral Nemo with critic agents). The evaluators assess hallucinations, omissions, and treatment categories (Manage/Visit/Resource) using structured JSON outputs. Results are partitioned by demographic/style metadata and statistical significance is computed for each (answering LLM, evaluator LLM) pair.

## Key Results
- Inter-LLM agreement across three evaluator models averaged Cohen's Kappa = 0.118, far below the 0.60 threshold for substantial human agreement
- High raw agreement percentages (e.g., 96%+ on "Resource" category) masked low Kappa due to class imbalance
- Different evaluator LLMs reached opposite, statistically significant conclusions on the same dataset—demonstrating non-generalizability of single-LLM findings
- Only specific (answering, evaluation) LLM pairs showed higher variance across demographic partitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diverse synthetic prompt generation exposes evaluation fragility that single-dataset studies miss.
- **Mechanism:** The pipeline samples demographics (age, gender, race), clinical histories, disorders, symptoms, and writing styles (e.g., "8th grade, lack of details" vs. "university, attention to detail") to generate 3.2M prompts from 37 mental health disorders. Each prompt combines a Patient Expression (demographics + clinical info) with a Desire (patient question theme), creating contrastive sets where prompts differ only in Patient Expression. This controlled variation enables isolation of bias sources.
- **Core assumption:** Synthetic prompts with stakeholder-seeded desires adequately represent real patient queries.
- **Evidence anchors:**
  - [Section 3]: "Our infrastructure automatically generates all possible Patient Expressions with corresponding questions to construct a prompt dataset."
  - [Section 3.1]: Desires seeded through "four focus groups for mental health and interviews with six patients."
  - [Corpus]: Related work on bias frameworks exists (Relative Bias paper, FMR=0.62), but no direct corpus evidence validates synthetic-to-real prompt transfer.
- **Break condition:** If clinical presentation patterns in real patient queries differ systematically from the SAMPLE-based generation logic, bias detection may not transfer.

### Mechanism 2
- **Claim:** Low inter-evaluator agreement indicates LLM judges operate as noisy raters rather than convergent assessments.
- **Mechanism:** Three evaluator LLMs (Llama3-ChatQA-1.5-8B, Qwen2.5-7B-Instruct, OLMo-2-1124-13B) assess identical answer sets using structured JSON outputs for hallucinations, omissions, and treatment categories. Cohen's Kappa averaged 0.118—far below the 0.60 threshold for substantial human annotator agreement. High raw agreement percentages on "Resource" category (96%+) masked low Kappa due to class imbalance.
- **Core assumption:** Cohen's Kappa thresholds designed for human annotators meaningfully apply to LLM evaluators.
- **Evidence anchors:**
  - [Abstract]: "LLM annotators exhibit low agreement scores (average Cohen's Kappa κ=0.118)."
  - [Section 6, Table 1]: Olmo vs. Qwen achieved highest agreement (Manage: 79%, κ=0.480), but Llama vs. others showed κ near 0.
  - [Corpus]: Meta-Judging paper (arXiv:2601.17312) identifies similar vulnerabilities in LLM-as-a-judge including "sensitivity to prompts, system biases," supporting the fragility observation.
- **Break condition:** If specific LLM pairs or prompting strategies achieve consistently high Kappa (>0.60), the "noisy rater" characterization would not generalize.

### Mechanism 3
- **Claim:** Statistically significant bias detection by one evaluator does not predict findings from other evaluators.
- **Mechanism:** When partitioning treatment annotation rates by style, gender, and race, different (answering LLM, evaluator LLM) pairs produced contradictory statistically significant conclusions. Example: On Llama responses, Olmo found higher "Visit" rates for "8th grade, lack of details" prompts, while Qwen found the opposite pattern—both statistically significant.
- **Core assumption:** Statistical significance at p<0.05 indicates a meaningful effect rather than evaluator-specific artifact.
- **Evidence anchors:**
  - [Section 7]: "Using data from different evaluation LLMs can lead to opposite, statistically significant conclusions."
  - [Section 7]: Specific pairs—(Medgemma, Olmo), (Llama, Olmo), (Medgemma, Qwen), (Llama, Qwen)—showed higher variance across partitions.
  - [Corpus]: Evaluating Scoring Bias in LLM-as-a-Judge (arXiv:2506.22316) documents evaluation reliability issues but does not directly address generalizability of statistical significance findings.
- **Break condition:** If a consensus protocol (e.g., majority voting, calibrated ensembles) produces stable cross-evaluator findings, single-evaluator studies could be rehabilitated with proper methodology.

## Foundational Learning

- **Cohen's Kappa for Agreement:**
  - Why needed here: The paper uses Kappa to quantify whether LLM evaluators agree beyond chance. Understanding that κ=0.118 means "slight" agreement (vs. κ>0.60 "substantial") is essential for interpreting the core finding.
  - Quick check question: If two evaluators agree 90% of the time on a binary classification where 85% of labels are class A, is their agreement meaningful? (Answer: Not necessarily—calculate expected agreement by chance first.)

- **LLM-as-a-Judge Paradigm:**
  - Why needed here: The evaluation pipeline uses LLMs to assess other LLMs' outputs. This is the methodological foundation being critiqued.
  - Quick check question: What are two failure modes when using an LLM to evaluate medical advice quality? (Answer: The judge may have its own biases; it may lack medical grounding to identify subtle hallucinations.)

- **Contrastive Evaluation Design:**
  - Why needed here: The prompt generation creates controlled pairs differing only in demographics/style. This enables bias attribution but requires understanding of experimental control.
  - Quick check question: If you observe higher hallucination rates for responses to female patients vs. male patients, what additional check strengthens the bias claim? (Answer: Verify the questions differ only in gender indication, not in clinical complexity or question phrasing.)

## Architecture Onboarding

- **Component map:**
  - Prompt Generation: Patient profile sampler → Desire expander → Question generator → Style rewriter
  - Answer Generation: Three LLMs (Llama3-ChatQA-1.5-8B, BioMistral-7B, MedGemma-4B-it) at temp=0.1
  - Evaluation: LLM-as-a-judge (3 models) + Agentic workflow (Mistral Nemo with critic agents)
  - Analysis: Metadata partitioning by demographic/style → statistical comparison → Kappa computation

- **Critical path:**
  1. Define disorders, symptoms, desires, and style templates
  2. Generate prompt universe (3.2M) via combinatorial expansion
  3. Sample and generate answers (29K)
  4. Run multi-evaluator assessment (684K evaluations)
  5. Compute inter-LLM agreement and partition-level statistics

- **Design tradeoffs:**
  - Synthetic vs. real prompts: Synthetic enables controlled experiments but may not reflect actual patient query distributions
  - Agentic vs. single-pass evaluation: Agentic adds critic feedback loops but increases cost and latency
  - Threshold selection: Semantic similarity filter at 0.7 removes near-duplicate answers; harm-level thresholds affect detection rates

- **Failure signatures:**
  - Evaluator ceiling/floor effects: Llama annotated nearly all answers as "Manage" and "Visit," reducing discrimination
  - Class imbalance masking: High raw agreement on "Resource" (96%+) concealed low Kappa (models disagreed on which rare cases were True)
  - Cross-evaluator contradiction: Same dataset, different evaluators → opposite statistically significant conclusions

- **First 3 experiments:**
  1. Replicate the Kappa analysis on your own domain: Generate 100 prompts with demographic variation, have 3 LLMs evaluate the same 30 responses, compute pairwise Kappa. Expect low agreement if the paper's finding generalizes.
  2. Test evaluator calibration: Compare LLM-as-a-judge against human expert annotations on a held-out subset (even n=50) to establish whether low inter-LLM agreement reflects genuine ambiguity or systematic evaluator errors.
  3. Ensemble evaluation stability: Run majority voting across 3+ evaluators and measure whether partition-level differences (by style/gender/race) shrink or persist compared to single-evaluator results.

## Open Questions the Paper Calls Out
- How does the inter-LLM agreement rate (Cohen's Kappa) for hallucination and omission detection correlate with human expert clinician evaluations?
- Does the observed low inter-LLM agreement persist when evaluating LLMs for safety and ethical biases rather than just hallucinations and omissions?
- What is the minimum number of diverse LLM evaluators required to ensure statistically significant results are generalizable in medical chatbot evaluations?

## Limitations
- The paper does not validate LLM evaluator judgments against human expert annotations, leaving uncertainty about whether low agreement reflects genuine ambiguity or systematic evaluator errors.
- While synthetic prompts enable controlled experiments, it remains unclear whether these adequately capture the complexity and variation of real patient queries.
- The study focuses specifically on mental health domain, limiting generalizability to other medical specialties or non-medical applications.

## Confidence
- **High confidence**: The methodology for computing Cohen's Kappa and identifying statistically significant differences is sound and reproducible.
- **Medium confidence**: The finding that different LLM evaluators reach contradictory conclusions on the same data is robust, but the generalizability to other domains beyond mental health remains uncertain.
- **Low confidence**: Whether the synthetic prompt generation adequately captures real patient query distributions, and whether human expert validation would confirm or refute the evaluator disagreements.

## Next Checks
1. **Cross-domain replication**: Apply the same evaluation pipeline to a different medical specialty (e.g., cardiovascular disease) to test whether inter-LLM agreement patterns persist across domains.
2. **Human expert benchmarking**: Annotate a random sample of 100 responses with human experts and compare their agreement patterns to those between LLM evaluators to determine if low κ reflects genuine ambiguity or evaluator-specific artifacts.
3. **Ensemble stability test**: Implement majority voting or weighted ensemble evaluation across the three LLM judges and measure whether partition-level bias findings become more stable compared to single-evaluator results.