---
ver: rpa2
title: 'AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured
  Interviews with Large Language Models'
arxiv_id: '2503.21911'
source_url: https://arxiv.org/abs/2503.21911
tags:
- interview
- interviews
- conflicts
- psychodynamic
- manual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoPsyC, the first method to automatically
  recognize psychodynamic conflicts from full-length OPD interviews using Large Language
  Models. The approach combines parameter-efficient fine-tuning and Retrieval-Augmented
  Generation (RAG) with a summarization strategy to process 90-minute interviews.
---

# AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models

## Quick Facts
- arXiv ID: 2503.21911
- Source URL: https://arxiv.org/abs/2503.21911
- Authors: Sayed Muddashir Hossain; Simon Ostermann; Patrick Gebhard; Cord Benecke; Josef van Genabith; Philipp Müller
- Reference count: 25
- Key outcome: AutoPsyC achieves weighted F1 scores of 0.78-0.81 for Self-dependency/Dependency conflicts and 0.59-0.58 for Self-sufficiency and Self-value/esteem conflicts, demonstrating automated recognition of unconscious psychodynamic conflict themes from clinical conversations.

## Executive Summary
This paper presents AutoPsyC, the first method to automatically recognize psychodynamic conflicts from full-length OPD interviews using Large Language Models. The approach combines parameter-efficient fine-tuning and Retrieval-Augmented Generation (RAG) with a summarization strategy to process 90-minute interviews. AutoPsyC uses an ensemble of four models, each fine-tuned on different interview segments, and aggregates results via weighted voting. Evaluated on 141 OPD interviews, AutoPsyC consistently outperforms baselines and ablation conditions, achieving weighted F1 scores of 0.78-0.81 for Self-dependency/Dependency conflicts, 0.59-0.58 for Self-sufficiency and Self-value/esteem conflicts.

## Method Summary
AutoPsyC processes 90-minute OPD interview transcripts by first generating summaries using LLaMA 3.1 8B guided by OPD Manual examples. The method splits each interview into four equal segments (~5,000 words each) and fine-tunes separate LLaMA models on each segment using LoRA adapters. A RAG vector database stores full training interviews (unlabeled), test interview, and OPD Manual excerpts. During inference, each segment classifier receives the segment summary plus RAG context with 5 few-shot examples. Predictions from all four models are aggregated using weighted voting via multinomial logistic regression, where segment weights are learned from training data to account for varying informativeness across interview progression.

## Key Results
- AutoPsyC achieves weighted F1 scores of 0.78-0.81 for Self-dependency/Dependency conflicts and 0.59-0.58 for Self-sufficiency and Self-value/esteem conflicts
- Middle interview segments (Q2, Q3) are most informative, outperforming other sections due to increased condition-relevant information sharing
- Weighted voting mechanism provides consistent performance gains (0.58-0.81 F1) over unweighted aggregation (0.55-0.78 F1)
- Minimal gender bias detected in fairness analysis across all conflict types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Segment-specialized models capture temporally distributed diagnostic cues in semi-structured interviews
- **Mechanism:** Interviews are split into 4 quarters (~5,000 words each); separate LoRA-fine-tuned models learn segment-specific patterns. Middle segments (Q2, Q3) yield highest performance because patients share more condition-relevant information after initial rapport-building but before wrap-up.
- **Core assumption:** Diagnostic signals are not uniformly distributed across interview duration; semi-structured formats create natural information gradients.
- **Evidence anchors:**
  - [abstract]: "The method demonstrates that middle interview segments are most informative"
  - [Section 6.3]: "models fine-tuned using the middle sections of the interviews outperform those tuned with other sections... in quarter 2 & 3, the interviewees often share information more closely related to their condition"
  - [corpus]: Weak direct corpus support; related work on clinical ASR (benchmarking ASR-LLM for medical diagnostics) doesn't address segmentation strategies

### Mechanism 2
- **Claim:** Summarization + RAG combination enables reasoning over both compressed semantics and verbatim detail
- **Mechanism:** A LLaMA-generated summary provides diagnostically-focused abstraction (filtering extraneous content per OPD Manual style). RAG gives models access to full transcripts and domain knowledge, retrieving specifics when summaries are inconclusive.
- **Core assumption:** Summaries capture sufficient signal for most classifications; RAG handles edge cases requiring exact quotations or extended context.
- **Evidence anchors:**
  - [Section 3.1]: "the generated summaries capture the diagnostically relevant information while filtering out extraneous details"
  - [Section 6.2]: "removing the test interview transcript from the vector database" causes 0.50-0.73 F1 drop; removing summary causes up to 0.04 F1 loss
  - [corpus]: RAFT (Retrieval-Augmented Facilitation for Text) cited as related method for prioritizing relevant medical literature, supporting RAG-domain knowledge integration

### Mechanism 3
- **Claim:** Weighted voting via learned aggregation captures asymmetric segment contributions
- **Mechanism:** A multinomial logistic regression learns weights wi for each segment's probability distribution. This accounts for quarters 2-3 being more predictive than 1 or 4, rather than simple averaging.
- **Core assumption:** Segment informativeness varies systematically and can be learned from training data.
- **Evidence anchors:**
  - [Section 3.4]: "we train a multinomial logistic regression model that assigns a weight wi to the prediction pi(c) of the i-th segment"
  - [Section 6.2]: "weighted voting mechanism... provides moderate but consistent performance gains (0.58–0.81 F1 vs. 0.55–0.78 for unweighted aggregation)"
  - [corpus]: No direct corpus evidence for this specific aggregation method in psychiatric NLP

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: Full fine-tuning of 8B-parameter models on 141 interviews is impractical; LoRA enables segment-specific adaptation without catastrophic forgetting
  - Quick check question: Can you explain why LoRA adapters allow training 4 specialized models without multiplying storage by 4x?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: 90-minute transcripts exceed context windows; RAG provides principled access to full interview content plus OPD Manual domain knowledge
  - Quick check question: What happens to performance if the test interview is removed from the vector database but the summary is retained?

- **Concept: OPD Psychodynamic Conflicts**
  - Why needed here: Unlike DSM categories, these are unconscious themes (e.g., Self-dependency vs. Dependency) rated on 5-class significance scales; understanding the clinical construct shapes prompt design
  - Quick check question: Why might "Self-sufficiency" conflict (F1=0.59) be harder to detect than "Self-dependency" (F1=0.78)?

## Architecture Onboarding

- **Component map:** Input: 90-min transcript -> [1] Summarizer (LLaMA 3.1 8B) -> Summary -> [2] RAG Vector DB <- {Training interviews, Test interview, OPD Manual} -> [3] Four Segment Classifiers (LoRA-tuned LLaMA, quarters 1-4) -> Each receives: segment summary + RAG context + 5 few-shot examples -> [4] Weighted Voting (Multinomial Logistic Regression) -> Output: 5-class conflict rating per conflict type

- **Critical path:** Summarization quality -> RAG retrieval relevance -> Segment classifier accuracy -> Weight calibration. If summaries miss diagnostically relevant content, downstream classifiers lack signal.

- **Design tradeoffs:**
  - 4 segments vs. more: Paper shows >4 or <4 segments hurt performance (Figure 4), but word-count splitting ignores interview structure
  - Including ground truth in RAG: Preliminary experiments showed no improvement; kept training interviews unlabelled
  - Full OPD Manual vs. excerpts: Full manual didn't improve over Axis III excerpts alone

- **Failure signatures:**
  - Low F1 on Self-sufficiency (0.59) vs. Self-dependency (0.78): Likely class imbalance or subtler linguistic markers
  - Removing test interview from RAG: 0.50-0.73 F1 drop indicates over-reliance on retrieval vs. summary
  - Fine-tuning without RAG: More robust to manual removal but still benefits from domain knowledge

- **First 3 experiments:**
  1. **Ablate summarization:** Run classification with RAG-only access to full transcript (no summary) to quantify summary contribution
  2. **Segment importance analysis:** Evaluate each quarter's model independently on held-out interviews; correlate with manual annotation of "diagnostically dense" passages
  3. **Cross-conflict weight transfer:** Train weighted voting on Self-dependency conflict, apply weights to Self-sufficiency; test if segment importance generalizes across conflict types

## Open Questions the Paper Calls Out

- **Question:** Can automatic semantic segmentation of interview transcripts improve conflict classification performance compared to the current fixed word-count splitting?
  - Basis in paper: [explicit] The authors state, "Future work could focus on automatically detecting interview segments for the fine-tuning process," noting that the current method splits interviews into four parts based strictly on word counts.
  - Why unresolved: The current fixed-split method does not account for the semi-structured nature of the interviews, potentially misaligning segment boundaries with actual topical shifts.
  - What evidence would resolve it: A comparative study measuring classification performance when using semantic topic segmentation algorithms versus the fixed 4-quarter splits.

- **Question:** Does the AutoPsyC approach generalize to diverse cultural contexts and linguistic backgrounds outside of German-speaking Europe?
  - Basis in paper: [explicit] The paper notes, "It remains unclear, whether our approach would also work in very different cultural contexts," identifying this geographic constraint as a key limitation.
  - Why unresolved: The dataset is restricted to German speakers in Europe, and psychodynamic expression or interview dynamics may differ significantly across cultures.
  - What evidence would resolve it: Evaluation results on OPD interviews collected from non-European, diverse cultural populations or via cross-lingual transfer experiments.

- **Question:** How robust is the model when analyzing interview segments containing defensive processes (e.g., resistance, avoidance)?
  - Basis in paper: [explicit] The paper explicitly lists "A more detailed investigation of how AutoPsyC handles defensive processes" as an area for future research.
  - Why unresolved: Defensive mechanisms can obscure the very unconscious conflicts the model attempts to detect, potentially leading to misclassification or low confidence.
  - What evidence would resolve it: An error analysis isolating interview sections with high defensive process ratings to measure the model's classification accuracy in these specific contexts.

## Limitations

- Data scale constraints: Only 141 OPD interviews across four conflict types, creating uncertainty about generalization to broader populations or conflict types
- Implementation specifics unknown: Critical hyperparameters like LoRA rank, training epochs, and RAG configuration remain unspecified
- Clinical validation gap: The paper validates against expert ratings but doesn't assess whether automated classifications would change clinical decision-making or improve patient outcomes

## Confidence

- **High confidence:** Comparative performance improvements over baselines are well-supported by experimental results; finding that middle interview segments are most informative is consistent across evaluations
- **Medium confidence:** Mechanism claims about segment specialization and RAG+summarization synergy are plausible but could benefit from more detailed error analysis
- **Low confidence:** Claim of being "first method" lacks comprehensive literature review; scalability claims don't address computational costs for clinical deployment

## Next Checks

1. **Cross-dataset generalization test:** Evaluate AutoPsyC on OPD interviews from different clinical centers or interview protocols to assess robustness beyond the Kassel dataset
2. **Class-specific error analysis:** Conduct detailed analysis of false positives/negatives for each conflict type, particularly focusing on why Self-sufficiency (F1=0.59) performs worse than Self-dependency (F1=0.78)
3. **Clinical utility pilot:** Design a small-scale study where clinicians use AutoPsyC outputs alongside standard assessments, measuring whether the automated system changes diagnostic decisions or treatment planning