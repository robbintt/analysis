---
ver: rpa2
title: A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection
  in Industrial IoT Systems
arxiv_id: '2505.18234'
source_url: https://arxiv.org/abs/2505.18234
tags:
- learning
- tabular
- detection
- policy
- intrusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a robust network intrusion detection system
  (NIDS) for Industrial IoT (IIoT) environments that addresses class imbalance and
  few-shot attack detection challenges. The proposed framework integrates a TabTransformer
  for effective tabular feature representation with Proximal Policy Optimization (PPO)
  for reinforcement learning-based classification optimization.
---

# A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems

## Quick Facts
- arXiv ID: 2505.18234
- Source URL: https://arxiv.org/abs/2505.18234
- Authors: Yuanya She
- Reference count: 6
- Primary result: Achieves 97.73% macro F1-score and 98.85% accuracy on TON IoT dataset with exceptional rare-class detection

## Executive Summary
This paper presents a novel network intrusion detection system for Industrial IoT environments that addresses the dual challenges of class imbalance and few-shot attack detection. The framework combines a TabTransformer encoder for effective tabular feature representation with Proximal Policy Optimization (PPO) for reinforcement learning-based classification. Evaluated on the TON IoT dataset, the model demonstrates exceptional performance particularly on rare attack classes like man-in-the-middle (MITM) with an F1-score of 88.79%, while maintaining overall accuracy of 98.85%.

## Method Summary
The proposed framework processes heterogeneous tabular data through a TabTransformer encoder that captures semantic interactions between categorical and numerical features using attention mechanisms. The encoder outputs contextualized representations that feed into dual heads: a policy head for class prediction and a value head for return estimation. PPO optimization with a composite reward function (classification reward + confidence adjustment + temporal penalty) enables class-balanced learning, particularly benefiting rare attack detection. The model is trained on the TON IoT dataset containing 42,209 samples with 30 categorical and 10 numerical features across 10 attack classes.

## Key Results
- Achieves 97.73% macro F1-score and 98.85% accuracy on TON IoT dataset
- MITM class detection: 88.79% F1-score and 91.08% recall despite only 213 test samples
- Ablation studies confirm both TabTransformer encoder and PPO optimization are essential, with MLP+PPO dropping to 86.67% macro F1 and TT+CE dropping to 92.00% macro F1

## Why This Works (Mechanism)

### Mechanism 1: TabTransformer for Heterogeneous Feature Encoding
The TabTransformer captures semantic interactions between categorical and numerical features more effectively than MLP-based encoders, particularly for rare-class patterns. Categorical features are embedded into learnable dense vectors; numerical features are linearly projected into the same embedding space. A 2-layer Transformer encoder with 4 attention heads per layer models cross-feature interactions via self-attention. Positional encoding is omitted because feature order is semantically fixed.

### Mechanism 2: PPO for Reward-Driven Class Balancing
Framing classification as a policy learning problem with PPO optimization mitigates class imbalance by directly rewarding correct predictions across all classes, rather than minimizing aggregate loss. The policy head outputs a probability distribution over 10 classes. PPO updates this policy via a clipped surrogate objective, using rewards that incentivize correct predictions. This allows the model to allocate learning capacity to rare classes that would otherwise be suppressed by cross-entropy's frequency-weighted gradients.

### Mechanism 3: Composite Reward Function for Rare-Class Focus
A three-component reward function (classification reward + confidence adjustment + temporal penalty) guides the policy toward robust, well-calibrated predictions on minority classes. R = α·R_cls + β·R_conf + γ·R_temp. R_cls gives +r for correct predictions, -r for incorrect. R_conf scales by prediction confidence. R_temp penalizes repeated mistakes via log-scaled penalty. This prevents the policy from ignoring rare classes while discouraging overconfident errors.

## Foundational Learning

- **Concept: TabTransformer Architecture**
  - Why needed here: Core encoder enabling attention-based feature interaction modeling for heterogeneous tabular data.
  - Quick check question: Can you explain why positional encoding is omitted for tabular features but essential for NLP transformers?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: Replaces cross-entropy loss with policy gradient optimization; clipping stabilizes training while allowing reward-driven class balancing.
  - Quick check question: What problem does the clipping operation (1-ε, 1+ε) solve in PPO's objective function?

- **Concept: Class Imbalance in Multi-Class Classification**
  - Why needed here: Understanding why cross-entropy favors frequent classes and how reward-based learning can counteract this bias.
  - Quick check question: Why does standard cross-entropy loss tend to achieve high accuracy but poor recall on minority classes?

## Architecture Onboarding

- **Component map:**
  Input (30 categorical + 10 numerical features) -> [Categorical Embedding Layer] + [Numerical Projection Layer] -> [Dimension Adjustment Layer] -> [Transformer Encoder: 2 layers × 4 attention heads] -> [Policy Head (10 classes) + Value Head (scalar)] -> [PPO Optimizer with GAE]

- **Critical path:**
  1. Preprocess: Standardize numerical features; embed categorical features
  2. Encode: Pass through TabTransformer for contextualized representations
  3. Predict: Policy head samples class action; value head estimates expected return
  4. Reward: Compute composite reward (R_cls + R_conf + R_temp)
  5. Update: PPO clipped objective with GAE advantage estimation

- **Design tradeoffs:**
  - TabTransformer vs. simpler encoder: More expressive but requires more data
  - PPO vs. cross-entropy: Better class balance but more hyperparameters (α, β, γ, ε, K epochs)
  - No positional encoding: Assumes feature order is semantically irrelevant

- **Failure signatures:**
  - **Encoder failure**: MLP+PPO achieves MITM F1=0.0, macro F1=86.67% (rare classes collapse)
  - **Optimizer failure**: TT+CE achieves macro F1=92.00% (class imbalance dominates)
  - **Reward misconfiguration**: If R_conf dominates, model may become overconfident on wrong predictions

- **First 3 experiments:**
  1. Reproduce ablation study: Train three variants (Full Model, TT+CE, MLP+PPO) and compare macro F1 and per-class MITM F1 to validate architecture claims.
  2. Reward sensitivity analysis: Vary α, β, γ weights systematically (e.g., α∈{0.5,1.0,2.0}, β∈{0.1,0.5,1.0}, γ∈{0.0,0.1,0.5}) and observe impact on rare-class recall.
  3. Attention visualization: Extract attention weights from Transformer layers for MITM samples to identify which feature interactions drive rare-class detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can temporal correlations be effectively incorporated into the framework via sequential modeling?
- Basis in paper: [explicit] Section 5.1 states that future research should explore "Incorporating temporal correlation via sequential modeling."
- Why unresolved: The current architecture processes static tabular snapshots without positional encoding or time-series analysis, limiting its ability to detect attacks that span multiple time steps.
- What evidence would resolve it: An extension of the model utilizing recurrent layers or temporal attention mechanisms, evaluated on sequence-based intrusion datasets.

### Open Question 2
- Question: Can the PPO agent maintain performance when adapting online in streaming scenarios?
- Basis in paper: [explicit] Section 5.1 lists "online adaptation of PPO in streaming scenarios" as a specific direction for future work.
- Why unresolved: The current study relies on offline training and testing on a static benchmark dataset (TON IoT), leaving the dynamics of continuous, streaming data unexplored.
- What evidence would resolve it: Evaluation of the model's stability and learning speed in a simulated streaming environment where traffic distributions shift over time.

### Open Question 3
- Question: How can the framework be scaled to multi-agent settings for distributed IIoT environments?
- Basis in paper: [explicit] Section 5.1 suggests "Extending the framework to multi-agent settings" as a necessary step for distributed systems.
- Why unresolved: The proposed system operates as a single-agent classifier; distributed deployment introduces challenges regarding communication overhead and coordinated policy learning.
- What evidence would resolve it: A study demonstrating a cooperative multi-agent PPO framework that maintains detection accuracy while minimizing inter-device communication costs.

## Limitations

- The paper demonstrates strong performance on a single dataset (TON IoT) without cross-dataset validation, limiting generalizability claims
- Key hyperparameters for TabTransformer (embedding dimensions, hidden sizes) and PPO training (learning rate, batch size, PPO epochs, reward weights α/β/γ) are unspecified, preventing exact reproduction
- The TON dataset's creation methodology and potential labeling biases are not discussed, which could affect external validity

## Confidence

- **High confidence**: Macro F1-score of 97.73% and accuracy of 98.85% on TON IoT dataset; effectiveness of TabTransformer encoder over MLP for rare-class detection (MITM F1: 88.79% vs 0.0%); PPO optimization contribution to class balance (macro F1 drop from 97.73% to 92.00% without PPO)
- **Medium confidence**: Generalization to other IIoT datasets and real-world deployment scenarios; optimal reward function weights and hyperparameter settings; scalability to larger feature spaces or more attack classes
- **Low confidence**: Claims about specific attention mechanisms for rare-class detection without attention visualization; effectiveness of temporal penalty component without sensitivity analysis; superiority over other imbalance-handling techniques like focal loss or class-balanced sampling

## Next Checks

1. **Cross-dataset validation**: Evaluate the trained model on at least two other NIDS datasets (e.g., CICIDS2017, CSE-CIC-IDS2018) to assess generalization beyond TON IoT.

2. **Hyperparameter ablation**: Systematically vary TabTransformer depth (1-3 layers), attention heads (2-8), and PPO reward weights (α, β, γ) to identify which factors most influence rare-class detection performance.

3. **Comparison with imbalance baselines**: Implement and compare against established class imbalance techniques (focal loss, weighted cross-entropy, SMOTE) using identical TabTransformer architecture to isolate PPO's specific contribution.