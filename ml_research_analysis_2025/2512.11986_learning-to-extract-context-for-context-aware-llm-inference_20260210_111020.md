---
ver: rpa2
title: Learning to Extract Context for Context-Aware LLM Inference
arxiv_id: '2512.11986'
source_url: https://arxiv.org/abs/2512.11986
tags:
- context
- prompt
- inference
- response
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CONTEXTLENS, a framework for context-aware
  LLM inference that extracts and leverages contextual information from user prompts
  to improve safety and reliability. The method uses a lightweight context generator,
  trained with reinforcement learning in an autoencoder-like fashion, to infer contextual
  signals from the prompt and guide response generation.
---

# Learning to Extract Context for Context-Aware LLM Inference

## Quick Facts
- arXiv ID: 2512.11986
- Source URL: https://arxiv.org/abs/2512.11986
- Reference count: 40
- Primary result: CONTEXTLENS reduces harmful responses by 5.6% on SafetyInstruct and improves harmonic mean of attack success rate and compliance by 6.2% on XSTest and WildJailbreak

## Executive Summary
CONTEXTLENS introduces a lightweight context generator trained via reinforcement learning to extract structured contextual signals from user prompts before LLM inference. This context snippet captures user intent, ambiguity level, risk factors, and safe response planning, which is then appended to the original prompt to guide safer generation decisions. The approach particularly benefits safety-critical scenarios where ambiguous prompts might otherwise bypass safeguards or trigger unnecessary refusals. Trained in an autoencoder-like fashion, the context generator produces information-rich snippets that transfer effectively across different base models without retraining.

## Method Summary
CONTEXTLENS trains a small context generator (Qwen2.5-3B-Instruct) using GRPO to produce structured context snippets from user prompts. These snippets, covering intent, ambiguity, risks, and response plans, are appended to corrupted versions of the original prompt and fed to a frozen decoder (Llama-3.2-3B-Instruct) that reconstructs the original prompt and generates responses. The training objective combines reconstruction similarity (SIM), safety rewards (Safe), and KL regularization, with rewards differentially applied to context versus response tokens. At inference, the generated context is appended to the prompt before sending to the target base model (GPT-4o, Llama-3-8B, or Qwen2.5-3B), improving safety decisions without model fine-tuning.

## Key Results
- Reduces harmful responses by 5.6% average across models on SafetyInstruct dataset
- Improves harmonic mean of attack success rate and compliance by 6.2% on XSTest and WildJailbreak
- Outperforms zero-shot contexts and reasoning traces across all evaluated base models
- Demonstrates effective cross-model transferability without retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extracting explicit contextual signals before response generation improves safety decisions on ambiguous prompts.
- **Mechanism:** The context generator produces structured snippets covering user intent, ambiguity level, potential risks, action decision, and safe response plan, which are appended to the original prompt for grounded safety decisions.
- **Core assumption:** LLMs can more accurately decide whether to refuse or comply when given explicit intent and risk assessments rather than inferring from raw prompts alone.
- **Evidence anchors:** Abstract emphasizes contextual cues shaped by user intentions influence appropriate responses; section 3.2 states LLMs can more accurately decide with complete prompt descriptions; limited corpus support exists from related context-aware NLP work.
- **Break condition:** Diminishing returns on already unambiguous and low-risk prompts, where added context may introduce noise.

### Mechanism 2
- **Claim:** The autoencoder-style reconstruction objective forces context to retain prompt-grounded information rather than collapsing into generic reasoning or direct response copying.
- **Mechanism:** The frozen decoder must reconstruct the original prompt from corrupted prompt plus generated context, creating pressure on the context generator to encode sufficient information about the prompt's actual content and intent.
- **Core assumption:** Context that enables accurate prompt reconstruction contains more grounded, informative signals than context optimized only for response safety.
- **Evidence anchors:** Abstract describes autoencoder-like architecture where context generator produces snippets for prompt reconstruction; section 3.3 incorporates auto-encoder loss; section 4.4 ablation shows removing decoder rewards drops performance from 93.07 to 91.14 and context quality from 8.66 to 6.67.
- **Break condition:** If context generator learns to simply copy prompt segments into context, reconstruction becomes trivial but context quality degrades; penalty for copying partial prompts mitigates this.

### Mechanism 3
- **Claim:** Training the context generator against a frozen, architecturally distinct decoder promotes cross-model transferability of generated contexts.
- **Mechanism:** The context generator is trained with a frozen decoder using rewards that evaluate the decoder's outputs, preventing co-adaptation and yielding contexts that transfer to GPT-4o and Llama-3-8B without retraining.
- **Core assumption:** Preventing co-adaptation between generator and consumer model encourages more universal context representations.
- **Evidence anchors:** Abstract states context is trained to benefit both itself and external decoder model, preventing co-adaptation; section 4.1 shows CONTEXTLENS trained on Qwen2.5-3B transfers effectively to other base models; section 4.4 notes variations in decoder type exhibit minimal impact on downstream performance.
- **Break condition:** If decoder and generator architectures are too similar or capacity mismatch is extreme, transfer benefits may diminish.

## Foundational Learning

- **Concept: Reinforcement Learning from Reward Signals (GRPO/RLHF basics)**
  - Why needed here: The context generator is trained via GRPO, which samples multiple rollouts and updates policy based on relative reward rankings. Understanding advantage computation and policy gradients is essential to debug training dynamics.
  - Quick check question: Can you explain how GRPO differs from standard PPO in how advantages are computed per-sample?

- **Concept: Autoencoder Objectives and Latent Representations**
  - Why needed here: The core training loop uses reconstruction loss where context serves as compressed representation enabling prompt recovery. Understanding information bottlenecks helps diagnose when context is under- or over-informative.
  - Quick check question: What happens to a standard autoencoder's latent space if the decoder is too powerful relative to the bottleneck size?

- **Concept: Safety Benchmarks and Evaluation Metrics (ASR, Compliance, Harmonic Mean)**
  - Why needed here: The paper evaluates on SafetyInstruct, AdvBench, WildJailbreak, and XSTest using attack success rate, compliance rate, and their harmonic mean. Understanding what each benchmark tests is critical for interpreting results.
  - Quick check question: Why use harmonic mean of (1-ASR) and Compliance rather than simple average?

## Architecture Onboarding

- **Component map:**
  - Context Generator (g_θ): Qwen2.5-3B-Instruct (trainable) → takes prompt p, outputs context c and intermediate response r_g
  - Frozen Decoder (d): Llama-3.2-3B-Instruct → takes corrupted prompt p' and context c, outputs reconstructed prompt p_d and response r_d
  - Reward Functions: Safe(·) for attack success, SIM(·) for reconstruction similarity
  - Base Models (inference-time): GPT-4o, Llama-3-8B, Qwen2.5-3B → consume prompt + context for final response

- **Critical path:**
  1. Prompt corruption: Split p at random index k, retain longer segment as p'
  2. Context generation: g_θ produces c and r_g from full prompt p
  3. Decoder inference: Frozen d generates p_d and r_d from (p', c)
  4. Reward computation: Safe(r_g), Safe(r_d), SIM(p_d, p) → combined reward R
  5. GRPO update: Advantage computed across K samples, applied differentially to context tokens (full reward) vs. response tokens (Safe only)

- **Design tradeoffs:**
  - Smaller generator (3B) vs. larger: Lower inference cost but may produce lower-quality context for complex prompts; paper shows 3B transfers well
  - Freezing decoder vs. joint training: Freezing prevents co-adaptation and improves transfer, but may limit ceiling on reconstruction quality
  - Penalty for prompt copying vs. information richness: Harsh penalty may discourage useful near-copies; current design uses partial-match check

- **Failure signatures:**
  - Context collapses to generic safety boilerplate → reconstruction reward (SIM) low, over-refusal on benign prompts
  - Context copies prompt segments → reward becomes 0 per penalty rule, no gradient signal
  - High variance across K samples → advantage estimates unstable; consider increasing K or adjusting KL coefficient
  - Good safety but poor compliance on benign prompts → Safe(·) reward dominates; rebalance reward weights

- **First 3 experiments:**
  1. **Ablate reconstruction reward:** Set SIM(p_d, p) = 0 and measure impact on context quality scores (Coherence, Relevance) and downstream ASR/H-Avg. Expect degradation per Table 4 findings.
  2. **Swap decoder architecture:** Train with Phi-4 or Qwen3-4B as frozen decoder instead of Llama-3.2-3B; verify minimal performance difference as claimed in Table 3.
  3. **Test zero-shot context transfer:** Generate context with GPT-4o using the zero-shot template, feed to Llama-3-8B-Instruct; compare to CONTEXTLENS-generated context to quantify the gap RL training closes (reference Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does context-aware inference perform on non-safety tasks such as reducing hallucinations or improving factual accuracy in knowledge-intensive queries?
- Basis in paper: All experiments focus exclusively on safety benchmarks (AdvBench, SafetyInstruct, WildJailbreak, XSTest). The method extracts contextual information to guide generation, but whether this mechanism generalizes to other generation quality dimensions remains untested.
- Why unresolved: No experiments were conducted outside safety-related tasks. The reward function and training data are safety-specific, so transferability to other objectives is unknown.
- What evidence would resolve it: Experiments applying ContextLens to hallucination benchmarks (e.g., TruthfulQA) or factual QA datasets, potentially with modified reward functions targeting accuracy rather than safety compliance.

### Open Question 2
- Question: How does context-aware inference extend to multi-turn conversational settings where context must accumulate and evolve across dialogue turns?
- Basis in paper: All experiments use single-turn prompts. The paper states "User requests are situated within broader contexts such as intentions, knowledge, and prior experience," yet the method only extracts context from the immediate prompt without maintaining conversational history.
- Why unresolved: The context generator processes each prompt independently. Whether the autoencoder objective and transferability benefits hold when context must persist or update across turns is unexplored.
- What evidence would resolve it: Experiments on multi-turn dialogue benchmarks showing whether accumulated context snippets improve or degrade performance, and whether the generator needs architectural modifications for temporal context tracking.

### Open Question 3
- Question: What are the minimal training data requirements for ContextLens to achieve effective context generation, and how does performance scale with dataset size?
- Basis in paper: "Our ContextLens model is trained on a 5,000 samples subset of the WildJailbreak training set." The paper does not explore whether this sample size is necessary or whether smaller/larger datasets would yield different trade-offs between training cost and performance gains.
- Why unresolved: Only one training set size is reported. The data efficiency of the RL training pipeline for context generation is not analyzed.
- What evidence would resolve it: Ablation studies varying training set size (e.g., 500, 1,000, 2,500, 5,000, 10,000 samples) and reporting performance curves across benchmarks.

### Open Question 4
- Question: Is the context generator itself vulnerable to adversarial manipulation if attackers can craft prompts designed to produce misleading context snippets?
- Basis in paper: The paper demonstrates robustness to adversarial prompts in the base model, but the context generator is a separate model that could be targeted. If an adversary crafts prompts that cause ContextLens to generate reassuring but incorrect risk assessments, the decoder could be misled.
- Why unresolved: No experiments test attacks specifically designed to fool the context generator rather than the final response model.
- What evidence would resolve it: White-box or black-box adversarial attacks targeting the context generator's output, measuring whether such attacks can induce unsafe responses in the decoder model.

## Limitations
- Relies on LLM-as-judge for safety evaluation, introducing evaluator-dependent variance that may not generalize to real-world deployment
- Claims about preventing over-refusal are indirect, measured via harmonic mean rather than explicitly validated with human studies on ambiguous prompts
- All experiments use single-turn prompts, leaving multi-turn conversational settings unexplored

## Confidence
- **High**: Reconstruction autoencoder design improves context quality (ablation in Table 4); freezing decoder enables cross-model transfer (Table 3)
- **Medium**: Safety improvements on SafetyInstruct and XSTest are robust across models; however, reliance on LLM-as-judge for ASR/Compliance introduces evaluator-dependent variance
- **Low**: Claims about context preventing over-refusal are indirect—measured via harmonic mean but not explicitly validated with human studies on ambiguous prompts

## Next Checks
1. **Evaluator robustness test**: Replace GPT-4o-as-judge with another LLM (e.g., Claude-3.5) and re-evaluate CONTEXTLENS on SafetyInstruct; check if ASR/Compliance trends persist
2. **Zero-shot generalization probe**: Generate contexts for prompts outside WildJailbreak distribution (e.g., from RealToxicityPrompts) and measure safety transfer without retraining
3. **Human evaluation on ambiguity detection**: Conduct user study on CONTEXTLENS's ability to distinguish truly harmful vs. benign-but-ambiguous prompts, measuring over-refusal rate vs. true positive safety detection