---
ver: rpa2
title: Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration
  of Video LLMs
arxiv_id: '2507.07990'
source_url: https://arxiv.org/abs/2507.07990
tags:
- token
- tokens
- video
- merging
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently processing long
  video contexts in video large language models (LLMs), which suffer from quadratic
  computational scaling due to the high number of spatio-temporal tokens required.
  To solve this, the authors propose a training-free spatio-temporal token merging
  method called STTM.
---

# Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs

## Quick Facts
- arXiv ID: 2507.07990
- Source URL: https://arxiv.org/abs/2507.07990
- Authors: Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim
- Reference count: 40
- Primary result: Training-free method achieving up to 3× speed-up on video LLMs with minimal accuracy loss

## Executive Summary
Video large language models (LLMs) struggle with computational efficiency due to the quadratic scaling of processing high numbers of spatio-temporal tokens. This paper introduces STTM (Spatio-Temporal Token Merging), a training-free method that exploits local redundancy in video data through hierarchical quadtree-based spatial merging followed by temporal merging across frames. The approach produces multi-granular tokens that preserve essential visual details while significantly reducing token count. Experiments demonstrate superior performance over existing token reduction methods, achieving substantial speed-ups with minimal accuracy degradation across six video QA benchmarks.

## Method Summary
STTM addresses video LLM efficiency by decomposing token merging into sequential spatial and temporal operations. Spatial merging uses a multi-granular quadtree structure to identify and merge similar 2×2 patches within each frame, creating tokens of varying granularity based on local content complexity. Temporal merging then connects similar tokens across frames, merging toward earlier occurrences to accumulate temporal changes. This query-agnostic approach enables KV cache reuse across multiple questions on the same video, making it particularly suitable for multi-turn scenarios. The method is evaluated on six video QA benchmarks, showing up to 3× speed-up with minimal accuracy loss.

## Key Results
- Achieves up to 3× speed-up on video QA benchmarks with minimal accuracy degradation
- Outperforms existing token reduction methods by 0.9-3.1% on five video QA benchmarks
- Demonstrates generalization across different MLLMs, scaling effectively to 72B models
- Enables KV cache reuse for query-agnostic token reduction, amortizing prefill cost across multiple queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing token merging into sequential spatial then temporal operations preserves more information than joint or 1D approaches.
- **Mechanism:** Spatial merging first identifies which regions need fine-grained representation via quadtree subdivision—similar 2×2 patches merge into coarser tokens when similarity exceeds threshold τS. Temporal merging then chains these multi-granular tokens across frames, merging toward earlier occurrences to accumulate temporal changes. This decomposition respects the independent structure of spatial redundancy (within-frame) versus temporal redundancy (cross-frame).
- **Core assumption:** Video redundancy factorizes cleanly along spatial and temporal dimensions—that regions needing fine spatial detail don't simultaneously require fine temporal resolution, and vice versa.
- **Evidence anchors:**
  - [abstract]: "STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension."
  - [Section 4.3, Table 6]: Spatial-only merging achieves 57.2% relative NV with 98.7% accuracy on VNBench; temporal-only achieves 53.7% NV with 100% accuracy; combined achieves 25.8% NV with 98% accuracy—demonstrating synergistic effect.
  - [Section 4.3]: Joint octree-based merging caused significant accuracy drops on VNBench (rapid spatial changes within short intervals), validating the decomposed approach.

### Mechanism 2
- **Claim:** Multi-granular token representation via quadtree subdivision preserves critical details while achieving aggressive compression.
- **Mechanism:** A hierarchical quadtree is built per frame by recursively averaging 2×2 patches. During traversal, if all four child nodes exceed similarity threshold τS with their parent, the parent represents that region (coarse token retained). Otherwise, subdivision continues to preserve high-frequency details. This adaptively allocates tokens based on local content complexity—uniform regions collapse to single tokens, textured regions retain fine granularity.
- **Core assumption:** Visual detail importance correlates with local feature variance—that regions with high intra-patch dissimilarity contain semantically important information the downstream LLM needs.
- **Evidence anchors:**
  - [Section 3.2, Figure 3]: "If all four fine child nodes exhibit high similarity with the coarse parent node, the search process terminates, and the parent node is used to represent the corresponding region."
  - [Section 4.3, Table 5]: Single-granularity bilinear interpolation (41.3% NV) drops to 78.9% accuracy on VNBench versus 93.0% for multi-granular (35.8% NV) at τS=0.75, demonstrating the importance of adaptive granularity.
  - [Figure 5 visualization]: Shows fine-grained tokens preserved for small text regions while uniform areas use coarse tokens.

### Mechanism 3
- **Claim:** Query-agnostic token reduction enables KV cache reuse, amortizing prefill cost across multiple queries on the same video.
- **Mechanism:** Unlike query-aware methods (FastV, FrameFusion) that use attention scores between video tokens and query tokens to guide reduction, STTM relies solely on spatio-temporal similarity independent of any question. The reduced token set and their KV states can be cached once and reused for any subsequent query about that video—critical for conversational and multi-turn scenarios common in deployment.
- **Core assumption:** Video content has sufficient inherent redundancy that query-independent compression preserves enough information for diverse downstream questions—that no single query requires access to tokens discarded as redundant.
- **Evidence anchors:**
  - [Section 1]: "However, existing training-free token reduction methods overlook this scenario [KV cache reuse] and instead focus on query-aware strategies."
  - [Section 4.2, Table 4]: On 72B model with 17.7s average TTFT, STTM achieves 44.2% NV usage with 1.3% accuracy improvement—demonstrating practical deployment value when KV cache can be reused.
  - [Section 4.4]: "On VideoMME (30% budget), its token retention ranges from 3.3% to 51.2% across videos, demonstrating its adaptability to varying content complexity"—query-agnostic but content-adaptive.

## Foundational Learning

- **Concept: Quadtree data structure**
  - **Why needed here:** Core to spatial merging—understanding how hierarchical 2D subdivision works is essential before implementing the coarse-to-fine search and understanding the O(4HW/3) complexity bound.
  - **Quick check question:** Given a 16×16 feature map, what is the maximum depth of a quadtree, and how many nodes exist at the third level (where root is level 0)?

- **Concept: KV caching in autoregressive transformers**
  - **Why needed here:** The entire motivation for query-agnostic design depends on understanding why KV cache reuse matters—the prefill phase computes Key/Value states for all input tokens, which are then reused during token-by-token generation.
  - **Quick check question:** Why does query-aware token reduction require recomputing the KV cache for each new question, even on the same video?

- **Concept: Union-Find with path compression**
  - **Why needed here:** Temporal merging uses a vectorized union-find to efficiently identify connected components (token chains) and find root nodes—understanding amortized near-constant time complexity explains the efficiency claim.
  - **Quick check question:** In the context of temporal token merging, what does the "root" of a union-find component represent, and why is merging directed toward earlier frames?

## Architecture Onboarding

- **Component map:**
  Video Input (T frames × H×W patches) -> Vision Encoder (e.g., CLIP-ViT) -> Token Reshape (T×H×W×C) -> [LLM Layer 1-3] -> STTM Module -> Reduced tokens (NST ≪ T×H×W) -> [Remaining LLM Layers]

- **Critical path:** The quadtree threshold τS and temporal threshold τT directly control the compression-accuracy tradeoff. The paper notes these are "empirically adjusted"—there is no learned or automatic threshold selection, making hyperparameter tuning the practical bottleneck for deployment.

- **Design tradeoffs:**
  - **Merging layer position (Table 9):** Earlier layers (1-3) give better speed-up but slightly lower accuracy; later layers (19) preserve accuracy but compute more attention on full tokens. The paper uses layer 3 for 7B models, layer 1 for 72B.
  - **RoPE handling (Table 8):** Reassigning position IDs based on new token order outperforms averaging or surviving RoPEs—but only applicable for standard RoPE. Qwen2VL uses M-RoPE (t, y, x coordinates), requiring the survival strategy instead.
  - **Root node resolution (Table 5):** Coarser roots (2×2) enable higher compression but risk losing details; 4×4 roots with lower τS achieve better accuracy-compression balance.
  - **Top-left vs. optimal matching (Table 7):** The top-left approximation for temporal merging enables vectorization and actually performs comparably or better than exhaustive similarity search—a fortunate implementation simplification.

- **Failure signatures:**
  - **VNBench degradation pattern:** If accuracy on fine-grained spatio-temporal tasks (needle-in-haystack) drops disproportionately vs. general QA, spatial threshold τS is too aggressive—fine details are being merged away.
  - **Static scene over-compression:** If nearly all tokens in a static video collapse to minimal tokens but queries fail, temporal threshold τT is too high—legitimate content variation is being treated as redundancy.
  - **Position mismatch artifacts:** If model outputs incoherent spatial references after merging, RoPE reassignment was not properly applied or is incompatible with the model's positional encoding scheme.
  - **GPU underutilization during merging:** If STTM overhead is noticeable, the union-find is not properly vectorized—the paper explicitly notes this is critical for efficiency.

- **First 3 experiments:**
  1. **Baseline reproduction with single video:** Select one video from VideoMME and one from VNBench. Run LLaVA-Video-7B with 100% tokens, then STTM at 50% and 30% budgets. Measure TTFT and accuracy. Verify the paper's claimed 2× and 3× speed-ups and ~0.5% / 2% accuracy drops on these specific samples. This validates your STTM implementation is correct before broader testing.
  2. **Threshold sweep on diverse content:** Take 5 videos: one with static background, one with rapid motion, one with text overlays, one with scene cuts, one synthetic VNBench-style video. Sweep τS from 0.70 to 0.90 and τT from 0.85 to 0.95. Plot NV ratio vs. accuracy for each video type. This reveals content-dependent sensitivity and informs automatic threshold selection (noted as future work in the paper).
  3. **KV cache reuse benchmark:** Implement a multi-turn query scenario: cache KV states after first query, measure TTFT for 5 subsequent queries on the same video with STTM vs. query-aware methods (FastV, FrameFusion). Verify that query-agnostic methods show constant subsequent-query latency while query-aware methods require full recomputation. This validates the practical deployment advantage claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an adaptive mechanism be developed to automatically determine spatial and temporal merging thresholds ($\tau_S$, $\tau_T$) to precisely meet a target token budget?
- **Basis in paper:** [explicit] The Conclusion states: "Exploring adaptive threshold selection is a promising direction, enabling automatic adjustment of token merging based on the given token budget."
- **Why unresolved:** The current STTM framework relies on empirically adjusted, static threshold values, requiring manual intervention to manage the trade-off between compression and retention for different videos.
- **Evidence:** A proposed module (heuristic or learned) that dynamically adjusts thresholds per video to hit a user-defined token ratio (e.g., exactly 30%) without sacrificing accuracy compared to fixed thresholds.

### Open Question 2
- **Question:** Is it possible to implement the optimal "most similar" selection for one-to-many temporal merging without sacrificing the processing speed provided by the current "top-left" approximation?
- **Basis in paper:** [inferred] Section 3.3 explains that while selecting the most similar token is the "naive" and ideal method for merging, the authors use a "top-left" approximation to enable a vectorized union-find algorithm for GPU efficiency.
- **Why unresolved:** The approximation was chosen strictly for vectorization constraints; it is untested whether a custom CUDA kernel or alternative parallel algorithm could make the optimal similarity search computationally feasible.
- **Evidence:** An implementation of the optimal similarity selection that achieves inference speeds comparable to the approximated version on standard GPU hardware.

### Open Question 3
- **Question:** Can a joint spatio-temporal merging approach (e.g., octree-based) be modified to handle rapid spatial changes effectively, unlike the rigid implementation tested in the ablation study?
- **Basis in paper:** [inferred] Table 6 shows that a joint octree-based merging method significantly underperforms on VNBench. The text notes this is because "rigid hierarchical partitioning across spatio-temporal dimensions is not effective in dynamic scenarios."
- **Why unresolved:** While the authors settled on a decomposed approach (spatial then temporal), a joint approach could theoretically offer better compression if it could adapt to non-rigid motion or rapid scene cuts.
- **Evidence:** A joint spatio-temporal token merging method that matches the decomposed STTM's accuracy on dynamic datasets like VNBench.

## Limitations

- **Threshold sensitivity:** The method relies on empirically chosen thresholds τS and τT with no learned or automatic selection mechanism, creating a significant practical barrier for deployment.
- **Evaluation scope constraints:** While tested on six video QA benchmarks, the evaluation focuses primarily on single-turn question answering, with multi-turn scenarios theoretically described but not empirically validated.
- **Model architecture dependencies:** STTM's effectiveness depends on specific positional encoding schemes, with degraded performance noted for models using alternative schemes like M-RoPE.

## Confidence

- **High Confidence (Mechanistic validity):** The core algorithmic approach—sequential spatial-then-temporal merging via quadtree decomposition and union-find—is sound and well-justified by the decomposition assumption. The theoretical complexity analysis and implementation details are rigorous.
- **Medium Confidence (Empirical claims):** The reported speed-ups (up to 3×) and accuracy preservation (98% at 30% budget) are supported by experiments, but threshold sensitivity and content-dependent performance variations suggest these numbers may not generalize uniformly across all video types without careful tuning.
- **Low Confidence (Practical deployment):** The KV cache reuse advantage for multi-turn scenarios and the practical significance of threshold selection methodology are asserted but not empirically validated in real-world deployment scenarios. The paper's ablation on layer positioning and RoPE handling provides guidance but lacks systematic exploration of the full design space.

## Next Checks

1. **Threshold selection methodology:** Implement an automatic threshold selection strategy based on video content statistics (entropy, motion magnitude, spatial variance). Validate whether content-adaptive thresholds can match or exceed the paper's empirical settings across diverse video types without manual tuning.

2. **Multi-turn scenario benchmark:** Create a conversational video QA benchmark with 5-10 sequential questions per video requiring reference to different video regions. Measure actual KV cache reuse benefits—compare TTFT for subsequent questions after initial cache computation versus query-aware methods that recompute for each question.

3. **Cross-architecture generalization:** Test STTM on a model with fundamentally different positional encoding (e.g., M-RoPE as used in Qwen2VL) and attention patterns (e.g., grouped attention or sliding window approaches). Quantify whether the method's effectiveness degrades and identify architectural constraints that limit applicability.