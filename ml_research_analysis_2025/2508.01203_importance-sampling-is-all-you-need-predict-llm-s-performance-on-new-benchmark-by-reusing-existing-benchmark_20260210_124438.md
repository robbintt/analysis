---
ver: rpa2
title: 'Importance Sampling is All You Need: Predict LLM''s performance on new benchmark
  by reusing existing benchmark'
arxiv_id: '2508.01203'
source_url: https://arxiv.org/abs/2508.01203
tags:
- source
- prompt
- distribution
- target
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIS, a novel framework that predicts large
  language model performance on code generation tasks without requiring ground-truth
  execution. The key idea leverages importance sampling theory, using Importance Weighted
  Autoencoders (IWAE) to model prompt distributions and reweight samples from existing
  annotated benchmarks to estimate performance on new, unseen benchmarks.
---

# Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark

## Quick Facts
- **arXiv ID:** 2508.01203
- **Source URL:** https://arxiv.org/abs/2508.01203
- **Authors:** Junjie Shi; Wei Ma; Shi Ying; Lingxiao Jiang; Yang liu; Bo Du
- **Reference count:** 40
- **Primary result:** Predicts LLM performance on unseen code benchmarks with 1.1% average absolute error using importance sampling and IWAE

## Executive Summary
This paper introduces BIS, a novel framework that predicts large language model performance on code generation tasks without requiring ground-truth execution. The key idea leverages importance sampling theory, using Importance Weighted Autoencoders (IWAE) to model prompt distributions and reweight samples from existing annotated benchmarks to estimate performance on new, unseen benchmarks. The framework addresses two major challenges in LLM evaluation: the high cost of constructing detailed test suites and the risk of data contamination. Through extensive experiments involving 8,000 evaluation points across 4 CodeLlama models and 9 diverse benchmarks, BIS achieves an average absolute prediction error of 1.1% for code correctness scores, with best- and worst-case errors of 0.3% and 1.9%, respectively. The framework also generalizes well to other metrics like pass@1, achieving average absolute errors of 2.15%.

## Method Summary
BIS predicts LLM performance on a target benchmark by reweighting scores from a source benchmark using importance sampling. The method extracts BERT embeddings from prompts, trains separate IWAE models on source and target prompt distributions, computes importance weights as density ratios, applies weight truncation at the 90th percentile, and aggregates weighted source scores to estimate target performance. The framework requires a source benchmark with known LLM scores and a target benchmark with prompts only, eliminating the need for expensive code execution on the target set.

## Key Results
- Achieves 1.1% average absolute error predicting code correctness scores across 8,000 evaluation points
- Best-case prediction error of 0.3% and worst-case of 1.9% for CodeLlama models on diverse benchmarks
- Maintains accuracy for multiple metrics: 2.15% average absolute error for pass@1 predictions
- Validated across 4 CodeLlama model sizes (7B-70B) and 9 different code generation benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Distribution-to-Performance Mapping via Importance Sampling
- Claim: Expected LLM performance on a target benchmark can be estimated by reweighting observed performance from a source benchmark, without executing code on the target
- Mechanism: When LLM parameters and evaluation metrics are fixed, expected score is an integral over prompt distribution (Equation 2). Importance sampling rewrites this expectation using samples from a proposal distribution, with weights equal to the density ratio P_target(t)/P_source(t). Theorem 3.1 proves this provides an unbiased estimator under Assumption 1 (target distribution absolutely continuous w.r.t. source)
- Core assumption: Source and target prompt distributions have sufficient overlap; high-probability regions in target exist in source (Assumption 1)
- Evidence anchors:
  - [abstract] "reweights samples from existing annotated benchmarks to estimate performance on new, unseen benchmarks"
  - [section 3.1] Equation 2 formalizes score expectation; Section 3.3 provides unbiasedness proof
  - [corpus] No direct corpus validation; related work on importance sampling exists (e.g., "Sampling as Bandits") but does not test this specific application to LLM benchmarking
- Break condition: Severe distribution shift (e.g., cross-language or cross-domain tasks) where weight variance becomes extreme; paper notes this may increase variance but framework still provides coarse ranking (Section 3.3.1)

### Mechanism 2: IWAE for Tractable Density Ratio Estimation
- Claim: Importance Weighted Autoencoders can approximate prompt distributions well enough to compute reliable importance weights
- Mechanism: Two IWAE models are trained—one on source prompts, one on target prompts. Each learns a latent-variable model approximating the marginal likelihood. The importance weight for source sample i is computed as IWAE_target(t_source^i) / IWAE_source(t_source^i) (Equation 4). IWAE uses K latent samples per observation to tighten the ELBO bound (Equation 1), better capturing multimodal prompt distributions than standard VAEs
- Core assumption: IWAE approximates true distributions with error ε(x) where |ε(x)| << 1; Theorem 3.2 shows prediction error is bounded by μ · sup(ε_source - ε_target)
- Evidence anchors:
  - [abstract] "using Importance Weighted Autoencoders (IWAE) to model prompt distributions"
  - [section 3.2] Describes IWAE module architecture; Table 7 shows IWAE outperforms GMM, VAE, RBM baselines
  - [corpus] Weak evidence; corpus contains work on "PIS: Linking Importance Sampling and Attention" but not direct validation of IWAE for this use case
- Break condition: Insufficient training samples or high-dimensional sparse embeddings leading to poor distribution fit; Table 13 shows smaller prompt sets significantly degrade performance

### Mechanism 3: Weight Truncation for Variance-Bias Tradeoff
- Claim: Clipping extreme importance weights improves prediction stability with minimal bias introduction
- Mechanism: When proposal and target distributions have misaligned high-probability regions, some samples receive extreme weights, causing a few samples to dominate prediction. Truncation caps weights at the x-th percentile (default 0.9), then renormalizes. This trades some bias for reduced variance
- Core assumption: Truncation percentile is tuned to balance variance reduction against bias; paper empirically selects 0.9
- Evidence anchors:
  - [abstract] "we introduce weight truncation strategies and compute marginal expectations"
  - [section 3.2] Importance Weight Module description; Table 12 shows truncation at 0.9 yields best results vs. 1.0, 0.95, 0.85, 0.8
  - [corpus] No corpus evidence on this specific truncation strategy
- Break condition: Over-truncation (too low percentile) introduces excessive bias; under-truncation (too high) leaves variance uncontrolled

## Foundational Learning

- **Concept:** Importance Sampling Theory
  - Why needed here: BIS is built on importance sampling; understanding how to estimate expectations under one distribution using samples from another is essential for grasping why the framework works
  - Quick check question: Given samples x_i ~ q(x) and weights w_i = p(x_i)/q(x_i), write the estimator for E_p[f(x)]

- **Concept:** Variational Autoencoders and IWAE
  - Why needed here: The paper uses IWAE to learn prompt distributions; understanding ELBO, latent variables, and why multiple samples improve bounds is necessary to interpret the density estimation component
  - Quick check question: How does IWAE's ELBO differ from standard VAE's ELBO, and why does this matter for multimodal distributions?

- **Concept:** LLM Code Evaluation Metrics
  - Why needed here: The framework predicts metrics like CodeBLEU, pass@1, cyclomatic complexity, and security scores; understanding what these measure is needed to interpret prediction targets
  - Quick check question: What does CodeBLEU capture that simple token overlap does not?

## Architecture Onboarding

- **Component map:** Embedding Module -> IWAE_source fit || IWAE_target fit -> Importance Weight Module -> Weighted aggregation -> Predicted score
- **Critical path:** Target prompt → Embedding → IWAE_target fit || Source prompts → Embedding → IWAE_source fit + LLM execution → Weight computation → Weighted aggregation → Predicted score
- **Design tradeoffs:**
  - **Dimensionality:** Higher dimensions preserve information but risk sparsity; PCA reduction helps but linear layers harm performance (Table 9 vs. Table 10)
  - **IWAE samples (K):** Too few (K=1) gives high variance; too many (K>25) also destabilizes (Table 11); default K=10
  - **Truncation percentile:** 0.9 empirically optimal (Table 12); lower introduces bias, higher leaves variance
  - **Prompt set size:** Smaller sets degrade predictions significantly (Table 13); recommend >500 samples
- **Failure signatures:**
  - **Extreme weight concentration:** Few samples with weights >> others indicates distribution mismatch; check weight histogram
  - **High prediction variance across runs:** May indicate insufficient IWAE samples or training instability
  - **Consistent over/under-prediction:** May indicate systematic distribution shift violating Assumption 1
- **First 3 experiments:**
  1. **Sanity check with held-out source samples:** Split source benchmark 80/20; train IWAE on 80%, predict performance on 20% using only importance weights. Error should be near zero if implementation is correct
  2. **Cross-benchmark validation:** Train on BigCodeBench, predict on HumanEval (or similar held-out benchmark). Compare predicted vs. actual CodeBLEU scores to validate transfer
  3. **Ablation on truncation:** Run predictions with truncation percentiles [0.8, 0.85, 0.9, 0.95, 1.0] to reproduce Table 12 behavior and tune for your specific source/target pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BIS maintain prediction accuracy when source and target benchmarks span different programming languages or coding paradigms (e.g., predicting smart contract performance from web development benchmarks)?
- Basis in paper: [explicit] Discussion section states: "Testing across significantly divergent scenarios and languages remains unexplored in this work. We posit this as a highly valuable future direction."
- Why unresolved: Current experiments use benchmarks with "largely consistent coding scenarios," and Assumption 1 (distribution overlap) becomes harder to satisfy across disparate domains, potentially causing high-variance importance weights
- What evidence would resolve it: Cross-domain experiments (e.g., systems programming vs. frontend vs. smart contracts) reporting error rates and weight distribution statistics

### Open Question 2
- Question: What methods can effectively reduce prediction variance when source and target prompt distributions have minimal overlap, beyond the current weight truncation approach?
- Basis in paper: [explicit] Discussion section notes: "scenarios where prompt distributions exhibit minimal overlap, causing the computed weights might disproportionately favor a minority of samples. Investigating methods to mitigate this issue is crucial."
- Why unresolved: Current percentile-based truncation (0.9) introduces bias and was selected empirically; no principled solution addresses fundamentally misaligned distributions
- What evidence would resolve it: Systematic comparison of adaptive weighting schemes, kernel-based density estimation alternatives, or hierarchical sampling strategies

### Open Question 3
- Question: How can BIS be extended to evaluate LLMs in dynamic, multi-turn coding environments where prompt distributions evolve based on prior model outputs?
- Basis in paper: [explicit] Conclusion states: "real-world code generation tasks often involve complex interactive processes, how to evaluate LLM performance in dynamic environments in real-time represents another valuable direction."
- Why unresolved: The framework assumes static prompt distributions; interactive settings introduce temporal dependencies violating standard importance sampling assumptions
- What evidence would resolve it: Integration with sequential off-policy evaluation methods, validated on interactive benchmarks like SWE-Bench

### Open Question 4
- Question: Does BIS's prediction accuracy generalize to LLM families with different architectures and training objectives beyond CodeLlama?
- Basis in paper: [inferred] Section 4.1 exclusively tests CodeLlama (7B–70B); while theoretically model-agnostic, different architectures may exhibit different prompt sensitivity patterns
- Why unresolved: All tested models share architectural lineage; alternative model families (e.g., Mistral, GPT-style, encoder-decoder) remain unvalidated
- What evidence would resolve it: Cross-model experiments spanning diverse architectures (Mistral, GPT-style, encoder-decoder) measuring whether prediction errors remain within the 1.1–2.15% range

## Limitations

- **Distribution overlap requirement:** The framework relies heavily on Assumption 1 (target distribution absolutely continuous w.r.t. source), which breaks down for cross-language or cross-domain tasks, causing extreme weight variance
- **IWAE architecture underspecification:** Critical hyperparameters like encoder/decoder architecture, latent dimension, learning rate, and number of latent samples are not specified, creating reproducibility challenges
- **Empirical truncation selection:** The 0.9 truncation percentile is chosen empirically without theoretical justification, and its sensitivity to task characteristics remains unclear

## Confidence

- **High Confidence:** The unbiasedness of the importance sampling estimator under Assumption 1 (Theorem 3.1) and the general framework mechanics are mathematically sound
- **Medium Confidence:** The IWAE's ability to model prompt distributions and compute reliable density ratios is supported by empirical comparisons but lacks theoretical guarantees about approximation quality
- **Medium Confidence:** The overall prediction accuracy (1.1% average absolute error) is well-validated across 8,000 evaluation points, but the results may be optimistic given the controlled experimental setup

## Next Checks

1. **Cross-Domain Transfer Test:** Apply BIS to predict LLM performance when source and target benchmarks have minimal prompt overlap (e.g., Python vs. Java code generation). Measure how prediction error scales with distribution shift magnitude.

2. **Truncation Sensitivity Analysis:** Systematically vary the truncation percentile [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0] across multiple source-target pairs to characterize the bias-variance tradeoff and identify optimal thresholds for different task types.

3. **IWAE Architecture Sensitivity:** Test alternative density models (GMM, VAE, normalizing flows) as replacements for IWAE to quantify the specific contribution of IWAE's multiple-sample ELBO to prediction accuracy.