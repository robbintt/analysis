---
ver: rpa2
title: 'Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation
  Deferral'
arxiv_id: '2510.27017'
source_url: https://arxiv.org/abs/2510.27017
tags:
- approximation
- deferral
- alignment
- primal
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of aligning large
  language models (LLMs) by proposing a novel framework called KAD (Knapsack Approximation
  Deferral) for proxy-based test-time alignment using guidance from a small aligned
  model. The core method models token-specific deferral as a 0-1 knapsack problem
  and derives primal and dual approximations to optimize the deferral decision, allowing
  for a mixture distribution that focuses deferral on uncertain tokens rather than
  using a global confidence threshold.
---

# Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral

## Quick Facts
- arXiv ID: 2510.27017
- Source URL: https://arxiv.org/abs/2510.27017
- Authors: Ayoub Hammal; Pierre Zweigenbaum; Caio Corro
- Reference count: 40
- Primary result: KAD framework outperforms nudging and implicit reward baselines in accuracy while achieving 1.3-1.5x speedup through speculative decoding on six math and reasoning datasets.

## Executive Summary
This paper addresses the computational challenge of aligning large language models by introducing KAD (Knapsack Approximation Deferral), a framework that selectively defers low-probability tokens to a smaller aligned model using token-specific decisions modeled as a 0-1 knapsack problem. The approach constructs a mixture distribution where deferral is determined either by a dual threshold (simple but may violate budget) or a primal budget (respects constraints but requires sorting). Experiments with OLMo 2 and Qwen 3 models demonstrate consistent accuracy improvements over strong baselines including nudging and implicit reward methods, while speculative decoding acceleration achieves 1.3-1.5x throughput improvements through higher acceptance rates.

## Method Summary
KAD formulates test-time alignment as a token-specific deferral problem where each token position is either kept from the base model p or deferred to an aligned small model q*. The deferral decision is modeled as a constrained optimization problem minimizing risk subject to a budget constraint on total deferred probability mass. This reduces to a 0-1 knapsack problem. The framework provides two approximations: a dual approximation using Lagrangian relaxation that yields a simple threshold rule (defer when p_v < λ), and a primal approximation that respects the budget through sorting. For acceleration, the mixture distribution π serves as the target in speculative decoding with q* as the draft model, achieving higher acceptance rates than distribution-level deferral methods.

## Key Results
- KAD consistently outperforms nudging and implicit reward baselines in task accuracy across six datasets
- Dual approximation (λ=0.3-0.4) and primal approximation (b=0.9) show similar performance with different computational tradeoffs
- Speculative decoding with KAD achieves 1.3-1.5x speedup (higher acceptance rates: 73-82% vs 63-78%)
- Framework works across model families (OLMo 2 and Qwen 3) and task types (math and commonsense reasoning)

## Why This Works (Mechanism)

### Mechanism 1: Token-Specific Deferral via Knapsack Formulation
- Claim: Selectively deferring low-probability tokens to an aligned small model improves alignment quality compared to distribution-level deferral.
- Mechanism: The framework constructs a mixture distribution π where each token v is either kept from the base model p or deferred to the aligned proxy q* based on a binary decision d_v. The deferral decision is formulated as a constrained optimization problem: minimize risk r(d) = Σ P_v · ℓ(p,v) · (1-d_v) subject to a budget constraint Σ P_v · d_v ≤ b. This reduces to a 0-1 knapsack problem where the "items" are tokens and the "budget" is the maximum probability mass that can be deferred.
- Core assumption: Most alignment-relevant changes occur at positions where the base model is uncertain (low-probability tokens), while high-confidence tokens already capture correct behavior from pretraining.
- Evidence anchors:
  - [abstract] "Our approach can be described as token-specific cascading method, where the token-specific deferral rule is reduced to 0-1 knapsack problem."
  - [Section 3.1] Defines the mixture π_v = p_v(1-d_v) + q*_v·α and the constrained risk minimization problem (P1).
  - [corpus] Related work on "Budgeted Multiple-Expert Deferral" (arxiv 2510.26706) similarly addresses deferral under budget constraints but for classification cascades.
- Break condition: If the small aligned model q* is less capable than the base model p on the target domain, deferral may harm rather than help. The paper notes this risk: for Qwen 3, "the small aligned model q* does not outperform the larger base model p" (Section 6.1).

### Mechanism 2: Dual Approximation via Lagrangian Relaxation
- Claim: A Lagrangian relaxation of the budget constraint yields computationally tractable deferral rules with provable risk bounds.
- Mechanism: The hard budget constraint is replaced with a penalty term in the objective: L(d,μ) = r(d) + μ(Σ P_v·d_v - b). The dual approximation d^-(μ) is obtained by setting d_v = 1 when ℓ(p,v) > μ (deferring high-loss tokens). For decreasing loss functions like negative log-likelihood, this becomes d_v = 1 when p_v < exp(-μ), i.e., defer low-probability tokens.
- Core assumption: The dual variable μ captures a meaningful tradeoff between deferral cost and risk reduction; setting it appropriately (e.g., λ = 0.3-0.4) generalizes across tasks.
- Evidence anchors:
  - [Section 3.2] "Our dual approximation is simply defined as a dual solution for a given multiplier μ: d^-(μ) ∈ argmin_{d∈{0,1}^|V|} L(d,μ)."
  - [Table 1] Shows different loss functions (logistic, squared, perceptron, 0-1) and their corresponding deferral conditions.
  - [corpus] Corpus does not contain direct comparisons to this specific dual approximation method for LLM alignment.
- Break condition: The dual approximation may not satisfy the budget constraint (it's not primal feasible). If strict budget adherence is required, primal approximation is needed.

### Mechanism 3: Speculative Decoding Acceleration
- Claim: The token-specific mixture distribution π achieves higher acceptance rates in speculative decoding than distribution-level deferral (nudging), improving throughput.
- Mechanism: In speculative decoding, a draft model (q*) proposes tokens that are verified against the target distribution (π). Acceptance rate equals 1 - D_TV(π, q*). Lemma 6 proves D_TV(π^<λ, q*) ≤ D_TV(φ, q*) + α(1-β), where φ is the nudging distribution. Since α(1-β) ≪ 1 typically, π achieves comparable or better acceptance rates.
- Core assumption: The aligned small model q* provides a good proposal distribution for the mixture π; tokens where they agree are accepted faster.
- Evidence anchors:
  - [Section 4] "As such, a lower divergence between the target and the draft models means a higher acceptance probability."
  - [Figure 2] Shows empirical acceptance rates: dual approximation achieves ~73-82% vs. nudging's ~63-78% across different draft window sizes.
  - [corpus] "$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts" (arxiv 2506.15733) explores related speculative decoding for test-time scaling.
- Break condition: If the draft window γ is too large, acceptance rates drop significantly (from ~75% at γ=3 to ~52% at γ=7 for OLMo 2), potentially negating speed benefits.

## Foundational Learning

- **Concept: 0-1 Knapsack Problem**
  - Why needed here: The deferral decision is NP-hard when formulated exactly; understanding knapsack structure explains why approximations (dual/primal) are necessary.
  - Quick check question: Given items with values (loss reduction) and weights (probability mass), can you select items to maximize value under a weight budget? If yes, you understand the knapsack formulation used here.

- **Concept: Lagrangian Duality**
  - Why needed here: The dual approximation relies on relaxing constraints via Lagrangian multipliers; understanding this reveals why d^-(μ) provides a lower bound on optimal risk.
  - Quick check question: If you minimize L(d,μ) = f(d) + μ·g(d) over d for fixed μ, and then maximize over μ, what relationship does the result have to min f(d) subject to g(d) ≤ 0? (Answer: It provides a lower bound.)

- **Concept: Speculative Decoding**
  - Why needed here: The paper leverages speculative decoding for acceleration; understanding the draft-verify paradigm clarifies why acceptance rates matter.
  - Quick check question: In speculative decoding, if the draft model proposes token x and the target distribution assigns probability p(x), under what condition is x accepted? (Answer: Typically with probability min(1, p(x)/q(x)) for rejection sampling formulations.)

## Architecture Onboarding

- **Component map**:
  1. Base model p: Large unaligned LLM (e.g., OLMo 2 13B, Qwen 3 14B) providing raw generation capability
  2. Aligned proxy q*: Small aligned LLM (e.g., OLMo 2 1B, Qwen 3 1.7B) providing alignment guidance
  3. Deferral decision module d: Computes token-specific deferral (dual: threshold λ; primal: budget b with sorting)
  4. Mixture constructor: Combines p and q* according to d: π_v = p_v(1-d_v) + q*_v·α
  5. Speculative decoder (optional): Uses q* as draft model, π as target for accelerated generation

- **Critical path**:
  1. Forward pass through base model p to get distribution over vocabulary
  2. Compute deferral decision d_v for each token (either via dual threshold λ or primal budget b)
  3. Forward pass through aligned proxy q* (only when any d_v = 1)
  4. Construct mixture π and sample next token
  5. If using speculative decoding: draft γ tokens from q*, verify against π

- **Design tradeoffs**:
  - **Dual vs. Primal approximation**: Dual (threshold λ) is simpler (no sorting) but requires hyperparameter tuning; Primal (budget b) respects budget but requires sorting vocabulary by loss
  - **Deferral budget b / threshold λ**: Higher deferral → more influence from q* but higher computation; λ=0.4 and b=0.9 performed well in experiments
  - **Speculative draft window γ**: Larger γ → more potential speedup but lower acceptance rates; γ=3-5 recommended

- **Failure signatures**:
  1. Performance degradation: If q* accuracy < p accuracy on target domain, deferral hurts (observed for Qwen 3 where q* = 68.2% vs. p = 71.4% average)
  2. Speed regression: Without speculative decoding, mixture requires both model forward passes, slowing generation from ~27 to ~19 tokens/sec (Table 4)
  3. Budget violation: Dual approximation may defer more probability mass than intended; use primal if strict budget matters

- **First 3 experiments**:
  1. **Dual approximation ablation**: Compare λ ∈ {0.3, 0.4} across math (GSM8K, MATH500, SVAMP) and commonsense (ARC, CSQA, TQA) datasets to identify optimal threshold for your model pair
  2. **Primal vs. dual comparison**: With fixed compute budget, compare primal (b=0.6, 0.9) vs. dual (λ=0.3, 0.4) on acceptance rate and accuracy to select approximation strategy
  3. **Speculative decoding sweep**: Test γ ∈ {3, 5, 7} to find the draft window that maximizes throughput (tokens/sec) while maintaining >65% acceptance rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum capability threshold required between the small aligned model q* and the large base model p for effective proxy-based alignment?
- Basis in paper: [inferred] The authors observe a "plateau effect" for Qwen 3 where primal approximations cluster in a narrow range [77.0, 77.4], potentially because "the small aligned model q* does not outperform the larger base model p, hence possibly making it more difficult to provide relevant guidance."
- Why unresolved: The paper hypothesizes this relationship but does not systematically characterize the capability gap requirements across different model families or tasks.
- What evidence would resolve it: Controlled experiments varying the capability ratio between q* and p, measuring alignment quality as a function of relative model performance gaps.

### Open Question 2
- Question: How can the unfilled budget problem in primal approximation be addressed to achieve more predictable deferral rates?
- Basis in paper: [explicit] The authors note in Figure 4 and surrounding text that "setting a high budget for the primal approximation does not mean that it will necessarily be fully met, as the remaining unfilled budget depends on the probability (in the large base model p) of the critical element."
- Why unresolved: The critical element's probability mass varies across tokens and contexts, making actual deferral rates unpredictable despite fixed budget settings.
- What evidence would resolve it: Analysis of unfilled budget distributions across datasets and development of adaptive budget mechanisms that account for critical element variability.

### Open Question 3
- Question: Can the theoretical bounds derived in Lemma 5 be empirically validated, and how large is the TV distance between p and P in practice?
- Basis in paper: [inferred] Lemma 5 establishes that approximation quality depends on DTV(P, p), but this quantity is "unknown" in practice. The paper relies on p as a "plugin estimator" without empirical characterization of this approximation error.
- Why unresolved: The gap between theoretical guarantees and empirical performance depends on how well p approximates the ground truth P, which remains unquantified.
- What evidence would resolve it: Empirical estimation of TV distances using held-out test distributions, correlation analysis between theoretical bounds and observed performance gaps.

### Open Question 4
- Question: What are the performance implications of the shared vocabulary constraint between large and small models?
- Basis in paper: [explicit] "The only extra assumption we introduce is that the large and small LMs share the same tokenization vocabulary."
- Why unresolved: This constraint limits applicability to model pairs from the same family and may exclude beneficial cross-family combinations.
- What evidence would resolve it: Experiments with vocabulary projection techniques or cross-family model pairs, measuring alignment quality degradation when vocabularies differ.

## Limitations

- The framework's effectiveness depends critically on the aligned proxy q* being sufficiently capable on the target domain, with performance degrading when q* is weaker than p
- The knapsack formulation assumes a fixed budget constraint that may not adapt well to task-specific alignment needs across diverse domains
- The shared vocabulary requirement between large and small models limits applicability to model pairs from the same family

## Confidence

**High Confidence** (Empirical evidence well-supported, mechanism clearly demonstrated):
- Token-specific deferral via knapsack formulation improves over global thresholding (Section 5 results)
- Speculative decoding acceptance rates are higher for KAD mixture than nudging (Figure 2)
- Throughput improvements with speculative decoding (Table 4)

**Medium Confidence** (Theoretical claims supported but empirical validation limited):
- Dual approximation provides risk bounds (Section 3.2 theoretical analysis)
- Mixture distribution achieves comparable TV divergence to nudging (Lemma 6 proof)
- Primal approximation respects budget constraint (Algorithm 1 correctness)

**Low Confidence** (Claims not fully validated or depend on unstated assumptions):
- Generalization to domains beyond math and commonsense reasoning
- Performance when aligned proxy q* is significantly weaker than base model p
- Optimal budget λ selection across diverse task types

## Next Checks

1. **Proxy Quality Ablation**: Systematically vary the alignment quality of q* (e.g., using partially aligned checkpoints or fine-tuned variants with different supervision strengths) to quantify the minimum viable proxy capability for KAD to improve over base model p. Measure accuracy degradation as q* quality decreases below p.

2. **Budget Sensitivity Analysis**: Perform a comprehensive sweep of deferral budgets b (primal) and thresholds λ (dual) across all six datasets to identify dataset-specific optimal values. Currently, λ=0.4 and b=0.9 are fixed; different tasks may require different deferral rates for optimal performance.

3. **Speculative Decoding Scaling Limits**: Test speculative decoding with larger draft windows γ (e.g., γ=10, 15) and measure the point where acceptance rates drop below 50%, making speedups negative. This identifies the practical limits of the acceleration mechanism and informs deployment constraints.