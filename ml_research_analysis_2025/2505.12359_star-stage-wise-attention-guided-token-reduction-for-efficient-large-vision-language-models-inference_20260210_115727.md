---
ver: rpa2
title: 'STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language
  Models Inference'
arxiv_id: '2505.12359'
source_url: https://arxiv.org/abs/2505.12359
tags:
- visual
- tokens
- star
- pruning
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STAR, a stage-wise attention-guided token reduction
  framework for efficient large vision-language models (LVLMs) inference. STAR addresses
  the computational overhead caused by redundant visual tokens in LVLMs.
---

# STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference

## Quick Facts
- arXiv ID: 2505.12359
- Source URL: https://arxiv.org/abs/2505.12359
- Authors: Yichen Guo; Hanze Li; Zonghao Zhang; Jinhao You; Kai Tang; Xiande Huang
- Reference count: 40
- Primary result: Two-stage pruning achieves 95% token reduction with 97.95% performance retention

## Executive Summary
This paper introduces STAR, a training-free, plug-and-play framework for efficient LVLM inference that addresses the computational overhead of redundant visual tokens. STAR employs a novel two-stage attention-guided token reduction approach: first pruning redundant low-level features using visual self-attention, then discarding task-irrelevant tokens using cross-modal attention. The method consistently outperforms existing single-stage pruning approaches across multiple LVLM architectures and benchmarks, achieving over 28.7% FLOPs reduction while retaining 97.95% of baseline performance.

## Method Summary
STAR implements a two-stage token pruning framework for LVLMs. Stage 1 performs visual self-attention pruning after the vision encoder, computing importance scores from the attention matrix to dynamically retain the most informative visual tokens while removing redundant low-level features. Stage 2 applies cross-modal attention pruning at an intermediate decoder layer (typically layer 14), using the attention between visual and language tokens to identify and discard task-irrelevant visual information. The method uses conservative pruning ratios in early stages (R < P) and enforces that R < P to ensure critical information survives to the cross-modal stage. The approach is model-agnostic and requires no training, making it practical for deployment across different LVLM architectures.

## Key Results
- Achieves up to 95% visual token pruning on LLaVA-1.5-7B while retaining 97.95% of baseline performance across 7 tasks
- Reduces inference FLOPs by more than 28.7% compared to baseline models
- Consistently outperforms single-stage pruning methods across multiple benchmarks including VQAv2, GQA, VizWiz, ScienceQA-IMG, TextVQA, POPE, MME, and MM-Vet
- Maintains performance improvements across different LVLM architectures (LLaVA-1.5-7B, LLaVA-1.5-13B, LLaVA-NeXT-7B)

## Why This Works (Mechanism)
STAR's effectiveness stems from its staged approach to token reduction that aligns with the information processing hierarchy of LVLMs. The first stage removes redundant low-level visual features before cross-modal fusion, preventing computational waste in early processing. The second stage then leverages cross-modal attention to identify task-relevant tokens, discarding information that the language model determines is unnecessary for the specific query. This complementary approach preserves essential visual information while eliminating both redundancy and irrelevance, achieving better efficiency-performance trade-offs than single-stage methods that must balance both concerns simultaneously.

## Foundational Learning
**Visual Self-Attention Pruning** - Uses attention weights within the vision encoder to score token importance. Why needed: Identifies redundant low-level features before they propagate through the model. Quick check: Verify attention matrix has meaningful variance and scores show clear separation between high/low importance tokens.

**Cross-Modal Attention Pruning** - Leverages attention between visual and language tokens to score relevance for specific tasks. Why needed: Enables task-aware pruning that adapts to query-specific visual requirements. Quick check: Confirm cross-modal attention scores vary meaningfully across different input images and prompts.

**Dynamic Thresholding** - Computes per-token importance scores and applies thresholds to select tokens for retention. Why needed: Enables adaptive pruning that maintains minimum information while maximizing reduction. Quick check: Verify token counts after pruning match expected values (e.g., ~518 tokens after Stage 1 from 576 initial tokens).

**Layer Indexing Convention** - Identifies specific decoder layers for cross-modal pruning. Why needed: Determines optimal point in the architecture for task-relevant pruning. Quick check: Confirm whether K=14 is 0-indexed or 1-indexed and whether it includes embedding layers.

## Architecture Onboarding
**Component Map**: Vision Encoder -> Visual Self-Attention Pruning -> LLM Decoder (with Cross-Modal Attention Pruning at layer K) -> Output

**Critical Path**: The visual token reduction pipeline must preserve essential features through Stage 1 to ensure Stage 2 has sufficient information for accurate cross-modal pruning decisions. The order of operations (visual pruning before cross-modal pruning) is critical.

**Design Tradeoffs**: Fixed pruning ratios offer simplicity and stability but may not adapt optimally to all inputs; dynamic thresholds could improve adaptation but add complexity. The conservative early pruning (R < P) ensures information preservation but may limit maximum reduction potential.

**Failure Signatures**: Uniform importance scores indicate incorrect attention extraction; memory not decreasing despite pruning suggests tokens are masked rather than removed; accuracy drops >5% suggest over-aggressive pruning or incorrect stage ordering.

**First Experiments**: 
1. Implement Stage 1 pruning with R=0.1 and verify token count reduction from 576 to ~518 while maintaining attention score variance
2. Add Stage 2 pruning at layer K=14 and measure final token counts at 288, 115, 58, and 29 levels
3. Run VQAv2 benchmark subset to verify 97.95% performance retention with 95% token reduction

## Open Questions the Paper Calls Out
**Open Question 1**: Can the optimal intermediate pruning layer $K$ be determined dynamically or theoretically rather than via empirical grid search for different LVLM architectures? The paper empirically identifies layer 14 as optimal but lacks a generalizable rule for other architectures.

**Open Question 2**: Does replacing the fixed pruning ratios ($R$ and $P$) with input-adaptive thresholds improve the trade-off between inference speed and task performance? Fixed ratios may over-prune complex images or under-prune simple ones, a limitation not addressed by the static strategy.

**Open Question 3**: Can the performance of STAR be further improved by integrating a training-aware fine-tuning phase to compensate for the distribution shift caused by token removal? The training-free design prevents the model from learning to adapt to pruned token distributions.

## Limitations
- Layer indexing convention (K=14) lacks clarity on whether it's 0-indexed or 1-indexed and whether it includes embedding layers
- Dynamic threshold calculation formula lacks implementation details for tie-breaking and exact token selection
- Cross-modal attention extraction mechanism during autoregressive generation is not fully specified
- Scalability claims remain unverified beyond LLaVA-1.5 models to other LVLM architectures

## Confidence
- **High confidence**: Overall conceptual framework of staged attention-guided pruning is theoretically sound and well-motivated
- **Medium confidence**: Reported quantitative results are reasonable but implementation ambiguities create uncertainty about exact reproduction
- **Low confidence**: Scalability claims to various LVLM architectures remain unverified beyond the tested LLaVA models

## Next Checks
1. Implement both pruning stages with exact attention extraction and threshold calculation as specified, then verify token count reduction matches reported values and confirm GPU memory usage decreases proportionally rather than just masking tokens.

2. Reproduce the reported failure modes by testing aggressive pruning combinations (R > 0.2 in Stage 1, or insufficient P in Stage 2) and verify that accuracy drops exceed 5% on VQAv2, confirming the necessity of the two-stage approach over single-stage alternatives.

3. Visualize the importance score distributions r_i for both stages across multiple samples, confirming that Stage 1 shows clear separation between redundant and essential low-level features, while Stage 2 demonstrates task-dependent token relevance patterns as shown in Figure 4 of the paper.