---
ver: rpa2
title: 'V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs'
arxiv_id: '2509.25773'
source_url: https://arxiv.org/abs/2509.25773
tags:
- humor
- video
- understanding
- videos
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces v-HUB, a visual-centric humor understanding
  benchmark designed to evaluate multimodal large language models (MLLMs) on video
  humor comprehension. The benchmark includes 960 short videos from silent films and
  user-generated content, focusing on humor that can be appreciated purely through
  visual cues.
---

# V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs

## Quick Facts
- arXiv ID: 2509.25773
- Source URL: https://arxiv.org/abs/2509.25773
- Authors: Zhengpeng Shi; Hengli Li; Yanpeng Zhao; Jianqun Zhou; Yuxuan Wang; Qinrong Cui; Wei Bi; Songchun Zhu; Bo Zhao; Zilong Zheng
- Reference count: 40
- Key outcome: MLLMs perform significantly better with text-only inputs compared to raw video, indicating heavy reliance on linguistic cues for humor understanding.

## Executive Summary
This paper introduces v-HUB, a visual-centric humor understanding benchmark designed to evaluate multimodal large language models (MLLMs) on video humor comprehension. The benchmark includes 960 short videos from silent films and user-generated content, focusing on humor that can be appreciated purely through visual cues. v-HUB features three tasks: Caption Matching, Humor Explanation, and Open-ended QA, each designed to test different aspects of humor reasoning. The authors evaluate a diverse set of MLLMs under three settings: Text-Only, Video-Only, and Video+Audio. Results show that MLLMs perform significantly better with text-only inputs compared to raw video, indicating a heavy reliance on linguistic cues and a struggle to derive humor from visual information alone.

## Method Summary
The v-HUB benchmark evaluates MLLMs on visual-centric video humor understanding through three tasks: Caption Matching (discriminative), Humor Explanation (generative), and Open-ended QA (temporal/descriptive/causal questions). The dataset consists of 960 videos (5-60s, avg 15s) from Charlie Chaplin silent films (267) and user-generated content (693). Annotations include descriptive/creative captions, video descriptions, humor explanations, humor element labels, and 960 manually verified QA pairs. The evaluation uses three settings: Text-Only with human descriptions, Video-Only with frames (no audio), and Video+Audio for OmniLLMs. Models are evaluated using accuracy for Caption Matching and SentBERT, METEOR, BERTScore (F1) for the generative tasks.

## Key Results
- MLLMs achieve substantially higher performance in Text-Only settings compared to Video-Only, with Qwen2.5-VL-72B dropping from 0.792 to 0.459 SentBERT score in Open-ended QA
- Models perform better on Open-ended QA than on Humor Explanation, indicating difficulty with autonomous humor discovery
- User-Generated Videos consistently receive higher scores than Chaplin films, suggesting greater difficulty with historically distant content
- Audio addition provides only marginal improvements (e.g., MiniCPM-2.6-o accuracy improves from 0.364 to 0.404)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MLLMs rely on linguistic shortcuts rather than visual reasoning for humor understanding, causing performance collapse when text is removed.
- **Mechanism:** The models process visual inputs superficially, defaulting to strong linguistic priors (textual descriptions or visual text) to infer meaning. When stripped of text, the visual encoder fails to capture the causal chain of events required to understand the "punchline."
- **Core assumption:** The gap between Text-Only and Video-Only scores reflects a visual encoding deficit rather than just limited context window.
- **Evidence anchors:** "MLLMs perform significantly better with text-only inputs compared to raw video... indicating a heavy reliance on linguistic cues." "Text-Only setting yields substantially higher scores than the video-based settings... Qwen2.5-VL-72B drops in accuracy from 0.719 to 0.673."

### Mechanism 2
- **Claim:** Models fail to autonomously "discover" humor without explicit prompting, revealing a gap between perception and reasoning.
- **Mechanism:** Humor Explanation requires end-to-end inference (identifying *what* is funny), while Open-ended QA provides explicit constraints (answering *specific* questions). Models lack the agency to isolate relevant features in raw video without guidance.
- **Core assumption:** The drop in explanation scores isn't just due to generation verbosity but a failure to localize the humor source.
- **Evidence anchors:** "Models consistently perform better on Open-ended QA than on Humor Explanation... they struggle with the more cognitively demanding task of discovering humor directly." "Caption Matching... challenges MLLMs to go beyond surface-level matching... [requiring] creative captions."

### Mechanism 3
- **Claim:** Humor understanding is bounded by training data temporal distribution, causing failures on historically distant content (e.g., silent films).
- **Mechanism:** Humor relies on social norms and visual tropes. Models appear to encode contemporary visual styles better than historical ones (Chaplin), not just due to grayscale/quality, but due to a mismatch in "common sense" expectations derived from modern internet data.
- **Core assumption:** The performance gap is content-driven, not just resolution-driven.
- **Evidence anchors:** "MLLMs consistently achieve higher scores on [User-Generated Videos]... greater difficulty in comprehending humor in historically distant videos." "Silent films... have rather narrow themes and employ limited storytelling techniques."

## Foundational Learning

- **Concept: Multimodal Alignment (Visual-Text)**
  - **Why needed here:** Understanding how the projector aligns visual features to the LLM's embedding space is critical. The paper shows this alignment is brittle; visual features aren't triggering the correct semantic "humor" tokens without text assistance.
  - **Quick check question:** Does the model map a visual "slip" event to the semantic concept of "clumsiness" or just describe "a person falling"?

- **Concept: Temporal Reasoning in Video Encoders**
  - **Why needed here:** Humor is often timing-based (setup vs. punchline). Static frame analysis is insufficient. You must understand how the model aggregates frames (e.g., spatio-temporal attention) to capture the sequence causing the humor.
  - **Quick check question:** Can the model distinguish between a person simply lying on the floor vs. a person tripping and falling, based on the temporal sequence?

- **Concept: Omni-Modal Integration (Audio-Visual)**
  - **Why needed here:** The paper evaluates OmniLLMs (Audio+Video). You need to understand if audio is fused deeply (joint attention) or just concatenated. The paper suggests audio helps only marginally, implying weak fusion.
  - **Quick check question:** Does removing the audio track of a "slapstick" sound effect significantly drop the model's confidence in classifying the clip as funny?

## Architecture Onboarding

- **Component map:** Video Frames (Visual Encoder) + Audio Track (Audio Encoder) + Text (LLM Backbone) -> Projector -> LLM Backbone -> Generated response

- **Critical path:**
  1. Sample frames (Video-Only or Video+Audio)
  2. Encode visual tokens (crucial point of failure per paper)
  3. Fuse audio (if available; marginal utility per paper)
  4. LLM generates response

- **Design tradeoffs:**
  - **Frame Sampling Rate:** High rates capture fast gags (Chaplin) but increase compute/sequence length
  - **Audio Inclusion:** The paper shows audio helps slightly but adds complexity. Is the FLOPs cost worth the <5% gain?
  - **Evaluation Metric:** Using BERTScore/SentBERT for open-ended generation vs. Accuracy for matching

- **Failure signatures:**
  - **The "Literalist" Trap:** Model describes the video ("A man falls") but fails to explain *why* it is funny ("He slipped on a banana peel")
  - **The "Text Crutch":** Massive performance disparity between Text-Only (using descriptions) and Video-Only conditions
  - **Hallucination:** Inventing visual text or dialogue in silent films to bridge the reasoning gap

- **First 3 experiments:**
  1. **Modality Ablation:** Run the benchmark on your specific architecture in Text-Only vs. Video-Only mode to quantify the "Visual Deficit"
  2. **Visual Text OCR check:** Filter the dataset for videos with/without embedded text to see if your model is cheating by reading subtitles rather than watching action
  3. **Temporal Sensitivity Test:** Evaluate performance on the "Chaplin" subset vs. "User-Generated" subset to diagnose if your training data has a historical/cultural bias

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLM architectures be advanced to derive humor from visual cues without relying on linguistic crutches, thereby closing the performance gap between text-only and video-only inputs?
- **Basis in paper:** [explicit] The authors report that "MLLMs generally perform better with text-only inputs than with video-only inputs" (e.g., Qwen2.5-VL-72B drops from 0.792 to 0.459 SentBERT score), indicating a "heavy reliance on linguistic cues."
- **Why unresolved:** Current cross-modal fusion capabilities are described as "underdeveloped," causing models to struggle with capturing subtle visual cues necessary for sophisticated humor comprehension.
- **What evidence would resolve it:** The development of models that achieve comparable performance on the Video-Only setting relative to the Text-Only baseline, specifically by enhancing intrinsic visual reasoning.

### Open Question 2
- **Question:** What specific multimodal fusion mechanisms are required to allow audio signals to significantly improve video humor understanding, rather than providing only marginal gains?
- **Basis in paper:** [explicit] While the authors note that "incorporating audio helps," they emphasize that the improvement is often "marginal but consistent" (e.g., MiniCPM-2.6-o accuracy improves from 0.364 to only 0.404).
- **Why unresolved:** OmniLLMs currently fail to effectively integrate sound context (environmental audio) with visual data to boost complex reasoning tasks.
- **What evidence would resolve it:** A substantial performance increase in the Video+Audio setting over the Video-Only setting on videos labeled as "Visual+Audio," potentially rivaling text-based performance.

### Open Question 3
- **Question:** How can MLLMs be trained to perform the abstract, implicit cross-modal reasoning required for "creative" humor understanding, rather than relying on surface-level matching?
- **Basis in paper:** [explicit] The authors identify an "incapability for subtle humor inference," noting that models struggle with the Caption Matching task when it requires understanding "creative captions" that extend the original humor.
- **Why unresolved:** Models currently struggle to connect non-obvious text to visual contexts, lacking the capacity for the abstract reasoning fundamental to comprehending sophisticated humor.
- **What evidence would resolve it:** Higher accuracy on the Caption Matching task and improved performance on the Humor Explanation task without explicit guiding questions.

## Limitations
- The benchmark relies on human-generated textual descriptions as ground truth, creating circularity in evaluating humor understanding
- The dataset composition (267 Chaplin vs. 693 user-generated videos) may not provide sufficient statistical power for historical content analysis
- Focus on visual-centric humor excludes verbal or audio-driven humor, potentially narrowing generalizability

## Confidence
**High Confidence:** The fundamental finding that MLLMs show dramatic performance drops when transitioning from text-only to video-only inputs is robust. The 0.792 to 0.459 SentBERT score drop for Qwen2.5-VL-72B in Open-ended QA is substantial and consistently observed across models.

**Medium Confidence:** The claim that models struggle specifically with "discovering" humor without explicit prompting is supported but requires further validation. The performance gap between Humor Explanation and Open-ended QA could reflect multiple factors beyond autonomous humor detection.

**Low Confidence:** The assertion that performance differences between Chaplin films and user-generated content stem primarily from cultural/temporal distance rather than technical factors lacks sufficient evidence.

## Next Checks
1. **Controlled Visual Style Experiment:** Test whether performance differences between Chaplin and modern videos persist after applying colorization and resolution enhancement to the silent film subset.
2. **OCR Dependency Analysis:** Systematically identify videos containing visual text (subtitles, signs) and measure performance separately for text-present vs. text-absent videos.
3. **Temporal Reasoning Stress Test:** Create synthetic videos that isolate temporal elements by presenting the same frames in different sequences (setup-punchline vs. punchline-setup). Measure whether models' humor ratings change based on temporal order.