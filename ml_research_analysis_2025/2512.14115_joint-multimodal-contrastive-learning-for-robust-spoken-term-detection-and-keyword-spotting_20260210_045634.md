---
ver: rpa2
title: Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and
  Keyword Spotting
arxiv_id: '2512.14115'
source_url: https://arxiv.org/abs/2512.14115
tags:
- audio
- word
- text
- embeddings
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint multimodal contrastive learning framework
  for Acoustic Word Embeddings (AWEs) that addresses limitations in existing approaches
  by unifying audio-audio and audio-text supervision. The proposed method combines
  CLAP-style audio-text contrastive alignment with DWD-style audio-audio discrimination
  to learn a shared embedding space.
---

# Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting

## Quick Facts
- **arXiv ID:** 2512.14115
- **Source URL:** https://arxiv.org/abs/2512.14115
- **Reference count:** 28
- **Primary result:** 85.05% AP on in-vocabulary word discrimination, 94.06% on out-of-vocabulary words; 15.71% EER on IV queries, 18.35% on OOV queries

## Executive Summary
This paper introduces a joint multimodal contrastive learning framework for Acoustic Word Embeddings (AWEs) that addresses limitations in existing approaches by unifying audio-audio and audio-text supervision. The proposed method combines CLAP-style audio-text contrastive alignment with DWD-style audio-audio discrimination to learn a shared embedding space. Experiments on LibriSpeech demonstrate that the model achieves 85.05% AP on in-vocabulary word discrimination and 94.06% on out-of-vocabulary words, outperforming baselines. For Spoken Term Detection and Keyword Spotting, it achieves up to 15.71% EER on IV queries and 18.35% on OOV queries. The framework supports both tasks flexibly within a single model and shows robustness to noisy and speaker-variant conditions.

## Method Summary
The framework combines symmetric audio-text contrastive learning (inspired by CLAP) with audio-audio discrimination (inspired by DWD) to learn robust AWEs. The model uses a 3-layer BiLSTM audio encoder and a 3-layer BiLSTM text encoder, both mapping to 512-dimensional embeddings. The total loss combines audio-text (α1=0.1) and audio-audio (α2=1.0) components with learnable temperature scaling. Training uses AdamW with OneCycleLR scheduling on LibriSpeech (100h), evaluating on STD/KWS tasks with sliding windows (0.2-0.6s, 0.15s hop) and word discrimination with N-choose-2 positive pairs.

## Key Results
- Achieves 85.05% AP on in-vocabulary word discrimination and 94.06% on out-of-vocabulary words
- Outperforms baselines with 15.71% EER on IV queries and 18.35% on OOV queries for STD/KWS
- Single model supports both Query-by-Example (audio-audio) and text-based keyword search (audio-text)

## Why This Works (Mechanism)
The framework unifies two complementary objectives: audio-text alignment ensures semantic consistency across modalities, while audio-audio discrimination enforces acoustic compactness within word classes. This dual supervision creates a shared embedding space that supports both text-to-audio and audio-to-audio retrieval. The symmetric CLAP-style loss ensures bidirectional alignment, while the DWD-style centroid-based loss maintains intra-class acoustic coherence. The joint optimization balances cross-modal alignment with intra-class discrimination, yielding embeddings that are both semantically meaningful and acoustically discriminative.

## Foundational Learning

**Acoustic Word Embeddings (AWEs):** Dense vector representations of variable-length spoken words that enable similarity-based retrieval. Needed to bridge the gap between speech and text representations. Quick check: Verify embeddings capture phonetic content while being robust to speaker variation.

**Contrastive Learning:** Framework that learns representations by pulling similar samples together and pushing dissimilar ones apart in embedding space. Needed to align audio and text modalities while maintaining acoustic discriminability. Quick check: Ensure temperature scaling prevents gradient explosion and embeddings remain normalized.

**Sliding Window Segmentation:** Fixed-length segments extracted from continuous speech for AWE extraction. Needed because AWEs operate on fixed-length inputs but speech is continuous. Quick check: Validate window overlap (0.15s hop) sufficiently covers word boundaries without excessive redundancy.

## Architecture Onboarding

**Component map:** Raw audio -> Mel-spectrogram -> Audio encoder (BiLSTM) -> 512-dim embedding; Text phonemes -> Text encoder (BiLSTM) -> 512-dim embedding; Joint contrastive loss -> Optimization

**Critical path:** Audio input → Mel-spectrogram extraction (128-dim, 25ms window, 10ms hop) → BiLSTM encoder (256 hidden) → 512-dim embedding → Cosine similarity with text/audio anchors → Contrastive loss computation

**Design tradeoffs:** BiLSTM chosen over Transformers for computational efficiency on limited data; symmetric audio-text loss adds training overhead but improves cross-modal alignment; joint optimization balances complementary objectives but may require careful hyperparameter tuning.

**Failure signatures:** Low cross-modal AP indicates collapsed embeddings or misaligned temperature scaling; high STD/KWS EER relative to word discrimination suggests segmentation errors or insufficient cross-modal alignment; poor OOV performance indicates limited generalization of learned representations.

**First experiments:**
1. Train audio-only baseline with DWD loss to establish acoustic discrimination performance
2. Train text-only CLAP-style baseline to establish cross-modal alignment capability
3. Joint training with varying α1:α2 ratios (0.01:1, 0.1:1, 1:1) to find optimal balance

## Open Questions the Paper Calls Out

**Open Question 1:** How can the framework be modified to prevent the Deep Word Discrimination (DWD) loss from degrading Keyword Spotting (KWS) performance? The authors state that "inclusion of the DWD loss in CLAP+DWD slightly degrades performance [in KWS]," attributing this to DWD operating solely on audio embeddings without enhancing cross-modal alignment. A modified loss function or training strategy that yields statistically significant improvements in KWS accuracy over the CLAP baseline while retaining the acoustic discrimination benefits of DWD would resolve this.

**Open Question 2:** Can the model be adapted to handle continuous speech streams without relying on sliding windows that often cut through target words? The current evaluation relies on fixed-size segmentation which inherently introduces misalignment noise, limiting the practical performance of the embeddings in real-world retrieval scenarios. Integration of an end-to-end segmentation mechanism or attention mechanism that achieves lower EER on continuous streams compared to the sliding window baseline would address this.

**Open Question 3:** Does the joint multimodal framework maintain its effectiveness when transferred to low-resource languages or domains with limited parallel data? It is unclear if the unified embedding space remains robust when the acoustic and textual representations are derived from noisier, lower-quality, or limited parallel data typical of low-resource settings. Zero-shot or few-shot cross-lingual transfer experiments demonstrating competitive Average Precision and EER on a target language outside the LibriSpeech training distribution would resolve this.

## Limitations
- Claims about robustness to noise and speaker variation are supported by controlled LibriSpeech test-other experiments but not validated on truly out-of-domain data
- Performance gains over baselines are incremental (1-2% absolute EER improvements) rather than transformative
- Framework's generalizability to real-world scenarios with severe noise, accents, and domain shifts is not empirically validated

## Confidence

**High confidence:** The experimental methodology is clearly specified and reproducible. The joint multimodal contrastive learning framework is technically sound, and the reported results on LibriSpeech (85.05% AP IV, 94.06% AP OOV, 15.71% EER IV) appear internally consistent with the described model architecture and training procedure.

**Medium confidence:** The claimed advantages over existing AWE approaches (DWD, CPC) are supported by the reported numbers, but the differences are incremental rather than transformative. The robustness claims to noise and speaker variation are plausible given the test-other results but not conclusively proven.

**Low confidence:** The framework's performance on truly out-of-domain data (different recording conditions, languages, or application domains) is unknown. The computational efficiency claims relative to existing methods are not quantified.

## Next Checks

1. Evaluate the pre-trained model on an external, noisy speech corpus (e.g., CHiME-5 or AMI) to verify robustness claims beyond LibriSpeech test-other.

2. Perform a more extensive ablation study by removing each contrastive component (audio-text vs audio-audio) to quantify their individual contributions to the final performance.

3. Test the model's generalization to a different domain (e.g., TED talks or conversational speech) to assess whether the learned embeddings transfer beyond read speech.