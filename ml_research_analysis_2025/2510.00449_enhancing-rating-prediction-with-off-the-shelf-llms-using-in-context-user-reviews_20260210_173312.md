---
ver: rpa2
title: Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews
arxiv_id: '2510.00449'
source_url: https://arxiv.org/abs/2510.00449
tags:
- review
- llms
- user
- dataset
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using off-the-shelf LLMs for Likert-scale rating
  prediction, a regression task requiring both language understanding and mathematical
  reasoning. The authors provide user-written reviews as in-context preference information
  to improve personalization performance.
---

# Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews

## Quick Facts
- arXiv ID: 2510.00449
- Source URL: https://arxiv.org/abs/2510.00449
- Authors: Koki Ryu; Hitomi Yanaka
- Reference count: 27
- Primary result: Off-the-shelf LLMs using in-context user reviews improve Likert-scale rating prediction performance, achieving results comparable to traditional matrix factorization baselines.

## Executive Summary
This paper explores using off-the-shelf large language models (LLMs) for Likert-scale rating prediction, a regression task requiring both language understanding and mathematical reasoning. The authors provide user-written reviews as in-context preference information to improve personalization performance. Experiments on eight models across three datasets show that incorporating review texts consistently improves rating prediction, with absolute improvements up to 0.147 in Spearman correlation and 13% reduction in RMSE. The approach achieves results comparable to traditional matrix factorization baselines, demonstrating the potential for lightweight recommendation systems that don't require costly fine-tuning.

## Method Summary
The core method compares three prompting formats for rating prediction: using only past numerical ratings (S → S), using past reviews with scores (RS → S), and generating hypothetical reviews before prediction (RS → RS). The approach leverages in-context learning by providing user review histories as examples during inference. Experiments were conducted on eight different LLM models (including Gemma 3 12B, Gemma 27B, and others) across three datasets: Movies, Recipe, and Books. The study evaluates performance using Spearman correlation and RMSE metrics, comparing against traditional matrix factorization baselines and simple heuristics like user average rating.

## Key Results
- In-context review texts consistently improve rating prediction performance across all tested models
- Absolute improvements of up to 0.147 in Spearman correlation and 13% reduction in RMSE achieved
- Review-based approaches achieve results comparable to traditional matrix factorization baselines
- Per-item reviews are more effective than general preference descriptions
- Prompting LLMs to generate hypothetical reviews before prediction further enhances performance, particularly for smaller models

## Why This Works (Mechanism)
The effectiveness stems from LLMs' ability to process rich textual information in context, extracting nuanced preference signals from user-written reviews that numerical ratings alone cannot capture. By providing review histories, the models can infer complex preference patterns, sentiment, and contextual factors that influence rating decisions. The review-to-review prompting strategy (RS → RS) appears to help LLMs better understand the relationship between textual content and numerical ratings by explicitly modeling the review generation process before prediction.

## Foundational Learning
- **In-context learning**: Why needed - enables zero-shot adaptation without fine-tuning; Quick check - verify prompt format consistency across experiments
- **Spearman correlation**: Why needed - measures ranking quality for ordinal rating prediction; Quick check - confirm monotonic relationship between predicted and actual ratings
- **RMSE (Root Mean Square Error)**: Why needed - quantifies prediction accuracy on rating scale; Quick check - validate against baseline naive predictors
- **Matrix factorization**: Why needed - establishes baseline for collaborative filtering performance; Quick check - compare convergence speed and accuracy
- **Likert-scale regression**: Why needed - handles ordinal rating prediction as continuous regression; Quick check - verify proper scaling of predicted ratings
- **Prompt engineering**: Why needed - optimizes LLM input format for task-specific performance; Quick check - test multiple prompt variations systematically

## Architecture Onboarding

**Component map**: User review history -> Prompt construction -> LLM inference -> Rating prediction -> Evaluation metrics

**Critical path**: Review history extraction → Prompt formatting (RS → RS preferred) → LLM generation → Rating conversion → Spearman correlation/RMSE calculation

**Design tradeoffs**: 
- In-context learning vs. fine-tuning: avoids costly adaptation but may limit performance on complex patterns
- Review text vs. numerical features: richer semantic information but requires more tokens and processing
- Hypothetical review generation: additional reasoning step that improves small models but increases inference cost

**Failure signatures**: 
- Performance degradation on skewed rating distributions (Recipe, Books datasets)
- Suboptimal results for larger models with RS → RS prompting
- Limited effectiveness when review histories are short or noisy

**3 first experiments**:
1. Test different prompt formats (S → S vs RS → S vs RS → RS) on a small subset of data
2. Compare review-based predictions against user average baseline on skewed datasets
3. Evaluate performance impact of review length and quality on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do smaller LLMs (e.g., Gemma 3 12B) outperform larger models (e.g., Gemma 27B) when using the Review+Score-to-Review+Score (RS → RS) prompting strategy?
- Basis in paper: [explicit] Section 7.2 notes this anomaly suggests a potential exception to scaling laws and leaves a detailed analysis for future work.
- Why unresolved: The paper reports the empirical result but does not identify the mechanism (e.g., distribution flattening, over-reasoning in larger models) causing the performance inversion.
- What evidence would resolve it: A mechanistic analysis comparing attention patterns or calibration errors between model sizes on the RS → RS task.

### Open Question 2
- Question: Why do LLMs utilizing review texts underperform simple "User Average" baselines on datasets with heavily skewed rating distributions (e.g., Recipe, Books)?
- Basis in paper: [explicit] Section 5.4 observes this exception and suggests a deeper analysis is needed to understand why heuristics outperform qualitative reasoning on skewed data.
- Why unresolved: It is unclear if the LLMs fail to extract signals from reviews or if the evaluation setup (skewed labels) inherently favors naive mean prediction.
- What evidence would resolve it: Experiments on synthetic datasets with controlled rating skewness to isolate the effect of distribution imbalance on review-based reasoning.

### Open Question 3
- Question: To what extent does dataset memorization inflate the reported performance of off-the-shelf LLMs on these rating prediction benchmarks?
- Basis in paper: [explicit] The Limitations section notes that model knowledge cutoffs occurred after dataset publication and states, "We need further investigation about how large the effect is."
- Why unresolved: The study does not quantify the overlap between the models' pre-training data and the specific reviews or plots in the Movies, Recipe, and Books datasets.
- What evidence would resolve it: Evaluation on temporally shifted datasets released after the models' training cutoffs or using membership inference attacks to detect memorization.

## Limitations
- Scalability concerns with larger user-item matrices beyond the relatively small datasets tested
- Dependency on rich user-written reviews that may not be available in many real-world scenarios
- Potential degradation with longer review histories or noisy, contradictory reviews
- Limited generalizability beyond Movies, Recipe, and Books domains with Likert scales

## Confidence
- In-context review texts consistently improve performance: High confidence (consistent improvements across multiple models and datasets)
- Results comparable to matrix factorization baselines: High confidence (quantitative comparisons provided)
- Hypothetical review generation enhances performance: Medium confidence (effect particularly noted for smaller models, may not generalize uniformly)

## Next Checks
1. Test the approach on larger-scale datasets with millions of users and items to evaluate scalability
2. Conduct experiments with synthetic or noisy reviews to assess robustness to data quality issues
3. Compare performance across different rating scales (e.g., 5-star vs 10-point scales) to determine generalizability beyond Likert scales