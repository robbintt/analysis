---
ver: rpa2
title: Iterative Self-Improvement of Vision Language Models for Image Scoring and
  Self-Explanation
arxiv_id: '2506.02708'
source_url: https://arxiv.org/abs/2506.02708
tags:
- score
- image
- consistency
- dataset
- aesthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a self-training method for vision-language\
  \ models (VLMs) to simultaneously predict image scores and generate natural language\
  \ justifications. The approach leverages only an image scoring dataset and an instruction-tuned\
  \ VLM, generating training data through self-explanation conditioned on correct\
  \ and incorrect scores, then optimizing with Direct Preference Optimization on two\
  \ datasets\u2014one for scoring and one for improving score-text alignment."
---

# Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation

## Quick Facts
- **arXiv ID:** 2506.02708
- **Source URL:** https://arxiv.org/abs/2506.02708
- **Reference count:** 0
- **Primary result:** Proposed self-training method improves VLM score prediction accuracy (SRCC from 0.446 to 0.739 on AVA) while maintaining explanation coherence through iterative DPO and model merging.

## Executive Summary
This paper presents a self-training framework for Vision Language Models (VLMs) to simultaneously predict image quality scores and generate natural language justifications. The method uses only an image scoring dataset and an instruction-tuned VLM, generating training data through self-explanation conditioned on correct and incorrect scores. By iteratively training on scoring and consistency datasets via Direct Preference Optimization and merging the resulting models, the approach improves both score prediction accuracy and alignment between predicted scores and generated explanations. Experiments on aesthetic image assessment datasets demonstrate state-of-the-art performance while maintaining coherent self-generated explanations.

## Method Summary
The approach trains VLMs to predict image scores and generate explanations without human-annotated explanations. It generates training data by having the VLM create explanations conditioned on ground truth scores ("chosen") and random incorrect scores ("rejected"). A second "consistency" dataset is created by replacing incorrect scores in rejected samples with correct scores. Direct Preference Optimization is applied to both datasets separately, then models are merged using TIES-Merging. This process iterates, with each iteration using the merged model to generate new data, progressively improving both scoring accuracy and explanation consistency.

## Key Results
- SRCC improved from 0.446 to 0.739 on AVA dataset after iterative training
- Explanation consistency maintained while improving score prediction accuracy
- Performance comparable to state-of-the-art models despite using only image-score data
- Iterative approach shows diminishing returns after 4 iterations
- 0.5B parameter model showed format degeneration issues on AADB dataset

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Preference Alignment via DPO
The method treats score prediction as a preference optimization problem, constructing DPO pairs where the "chosen" sample is an explanation conditioned on the ground truth score and the "rejected" sample is conditioned on an incorrect score. The VLM learns to maximize the likelihood of correct score-explanation bundles while minimizing incorrect ones. This works because the VLM can generate plausible justifications even for wrong scores, providing hard negative examples for contrastive learning.

### Mechanism 2: Score-Text Consistency Decoupling
A second DPO dataset is created by retrospectively replacing the incorrect score in "rejected" samples with the ground truth score. This creates pairs with identical scores but differing explanation qualities—one text truly justifies the score while the other (originally for a wrong score) is inconsistent with it. DPO on this data forces the model to output text consistent with the score token, addressing the alignment between predictions and explanations separately from score accuracy.

### Mechanism 3: Iterative Model Merging
Rather than training a single model on mixed objectives, the authors train two LoRA adapters—one on scoring data and one on consistency data—then merge them using TIES-Merging. This unified model generates the next iteration's data, bootstrapping better quality data over time. The iterative approach allows the model to specialize on each capability before combining them, avoiding catastrophic interference.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - *Why needed:* Core training loop replacing reinforcement learning for aligning VLM behavior using static preference datasets
  - *Quick check:* Can you explain why DPO is more stable than PPO for aligning this specific score-explanation task?

- **Conditional Text Generation**
  - *Why needed:* Data generation pipeline relies on forcing the model to rationalize specific scores
  - *Quick check:* How does appending `#Score: 2` to a prompt force the model to hallucinate a justification for a low score?

- **TIES-Merging**
  - *Why needed:* Architecture relies on merging "Scoring Expert" and "Consistency Expert" models
  - *Quick check:* What does the "density" parameter in TIES-Merging do, and why might discarding low-magnitude weights help combine two fine-tuned VLMs?

## Architecture Onboarding

- **Component map:** Base VLM -> Data Generator (Chosen/Rejected samples) -> Post-Processor (Consistency Dataset) -> DPO Trainer (Scoring/Consistency LoRA) -> Merger (TIES-Merging) -> Merged Model
- **Critical path:** Construction of the Consistency Dataset. If rejected explanations aren't semantically inconsistent with overwritten scores, the model learns nothing about alignment.
- **Design tradeoffs:** Iteration vs. Compute (diminishing returns after iteration 4), LoRA Rank (ranks 16-64), Evaluation Cost (GPT-4o for consistency is expensive, downsampled to 1000 instances)
- **Failure signatures:** Generic Explanations (model outputs non-committal justifications), Score Collapse (0.5B model on AADB showed consistency drops due to broken "max score" bias)
- **First 3 experiments:**
  1. Sanity Check: Test base VLM with wrong score prompts to verify it generates plausible justifications
  2. Data Pipeline Validation: Manually inspect Consistency pairs to ensure semantic inconsistency
  3. Single-Iteration Baseline: Train only scoring model, confirm SRCC rises but consistency drops before implementing full merging

## Open Questions the Paper Calls Out

- Can the self-generated explanations be optimized for faithfulness to the model's internal decision-making process, rather than just consistency with the output score? The current training optimizes for textual alignment with a score, which may produce plausible post-hoc rationalizations rather than true causal explanations.

- Does improving consistency via self-training translate to practical usefulness for human users in real-world decision-making tasks? Evaluation relies on GPT-4o as a proxy, leaving actual utility for human operators unverified.

- Do alternative model merging techniques yield better performance trade-offs between score prediction and explanation consistency than TIES-Merging? The authors note they haven't fully explored alternative merging methods.

- Can this self-training paradigm generalize to objective scoring domains (e.g., medical imaging) where "aesthetic" language is insufficient? Experiments are restricted to subjective Image Aesthetic Assessment, leaving domain-specific technical justification capabilities unclear.

## Limitations

- The method depends critically on the quality of "rejected" explanations—if the VLM generates generic justifications for wrong scores, the contrastive DPO signal becomes ineffective
- Reliance on GPT-4o for consistency evaluation introduces cost and potential evaluation bias
- The approach hasn't been validated on objective scoring domains beyond aesthetic assessment
- Long-term stability of the iterative loop is unexplored—potential for poor-quality "rejected" samples to poison future iterations

## Confidence

- **High Confidence:** Iterative merging mechanism works as described (clear SRCC improvement from 0.446 to 0.739 on AVA)
- **Medium Confidence:** Consistency improvement mechanism is plausible but harder to verify without actual examples of inconsistent explanations
- **Low Confidence:** Long-term stability of iterative loop is unexplored—paper stops at iteration 4 showing diminishing returns but doesn't test for potential collapse

## Next Checks

1. **Explain Rejection Quality:** Manually inspect a sample of "rejected" explanations (conditioned on wrong scores) to verify they contain semantically inconsistent justifications (e.g., praising brightness for an image that should be scored low for composition)

2. **Exclusion Radius Test:** Run the data generation pipeline with different exclusion radii (±1 vs. ±2 from the correct score) and measure the impact on SRCC and consistency to determine optimal negative sample difficulty

3. **Consistency Evaluation Bias:** Generate explanations for a held-out test set using the base VLM and final merged model, then evaluate consistency with both GPT-4o and a smaller, cheaper LLM (e.g., GPT-3.5) to assess whether GPT-4o scores are inflated or consistent across model qualities