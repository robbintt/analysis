---
ver: rpa2
title: 'MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision
  Tokenizer'
arxiv_id: '2509.16197'
source_url: https://arxiv.org/abs/2509.16197
tags:
- image
- arxiv
- understanding
- generation
- unified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Manzano is a unified multimodal large language model that achieves
  state-of-the-art performance in both image understanding and generation by addressing
  the typical trade-off between these capabilities. The core innovation is a hybrid
  vision tokenizer that produces continuous embeddings for understanding tasks and
  discrete tokens for generation tasks, both derived from a shared visual encoder
  to minimize task conflict.
---

# MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer

## Quick Facts
- arXiv ID: 2509.16197
- Source URL: https://arxiv.org/abs/2509.16197
- Reference count: 40
- Primary result: Achieves SOTA performance in both image understanding and generation by using a hybrid vision tokenizer that minimizes task conflict

## Executive Summary
MANZANO is a unified multimodal large language model that achieves state-of-the-art performance in both image understanding and generation. The core innovation is a hybrid vision tokenizer that produces continuous embeddings for understanding tasks and discrete tokens for generation tasks, both derived from a shared visual encoder. This architecture addresses the typical trade-off between understanding and generation capabilities in unified models. MANZANO demonstrates competitive or superior performance across diverse benchmarks including knowledge reasoning, text-rich document understanding, and generation tasks like GenEval and WISE, while scaling effectively from 3B to 30B parameters.

## Method Summary
MANZANO employs a hybrid vision tokenizer with a shared ViT backbone feeding into two lightweight adapters (continuous and discrete). The continuous adapter preserves dense spatial features for understanding tasks while the discrete adapter quantizes features for efficient autoregressive generation. A unified autoregressive LLM decoder handles both text and image tokens, while a separate diffusion-based image decoder renders high-fidelity details from the semantic tokens. The training follows a multi-stage recipe: tokenizer pre-training with a small LLM, unified LLM training with frozen vision encoder, and progressive diffusion decoder training. The model uses a 1:0.5 text:image loss ratio and scales effectively from 3B to 30B parameters.

## Key Results
- Achieves superior performance on text-rich understanding benchmarks (66.5 vs 63.6) compared to pure-discrete and dual-encoder baselines
- Demonstrates competitive generation quality on GenEval and WISE while maintaining strong understanding capabilities
- Shows minimal task interference when scaling from 3B to 30B parameters, with understanding and generation improving proportionally

## Why This Works (Mechanism)

### Mechanism 1: Semantic Homogeneity via Shared Visual Encoder
The model uses one shared ViT backbone feeding into two lightweight adapters, reducing the task interference commonly observed in dual-encoder systems. By deriving both token types from the same feature space, the LLM avoids the conflict of reconciling heterogeneous inputs.

### Mechanism 2: Format Specialization for Distinct Task Heads
The continuous adapter preserves dense spatial features necessary for text-rich understanding, while the discrete adapter quantizes features for efficient autoregressive prediction. This avoids information loss typical of pure-discrete approaches while maintaining generation tractability.

### Mechanism 3: Semantic-Pixel Decoupling (LLM + Diffusion)
The LLM acts as a "semantic planner" predicting discrete tokens representing high-level image structure, while a separate diffusion decoder handles pixel-level synthesis. This decouples reasoning scaling from rendering quality.

## Foundational Learning

- **Concept: Finite Scalar Quantization (FSQ)**
  - *Why needed:* The discrete adapter uses FSQ instead of standard VQ-VAE for scalable codebook creation without training instabilities
  - *Quick check:* How does FSQ differ from VQ-VAE in terms of codebook maintenance and implicit vs. explicit lookup?

- **Concept: Autoregressive vs. Diffusion Synthesis**
  - *Why needed:* Manzano is a hybrid system requiring understanding of LLM's AR next-token prediction versus diffusion decoder's denoising role
  - *Quick check:* Does the LLM predict pixel values directly, or does it predict latent codes that condition a separate generator?

- **Concept: Task Conflict in Unified Models**
  - *Why needed:* Understanding why previous dual-encoder systems caused "interference" in LLM attention mechanisms
  - *Quick check:* Why would feeding an LLM features from two fundamentally different visual spaces hurt performance compared to a shared space?

## Architecture Onboarding

- **Component map:** Vision Encoder (ViT) -> Continuous Adapter (STC + MLP) + Discrete Adapter (STC + FSQ + MLP) -> Unified LLM Decoder -> Image Decoder (DiT-Air)

- **Critical path:**
  1. Tokenizer Pre-training: Train Hybrid Tokenizer + Adapters with 300M LLM to align features
  2. Unified LLM Training: Freeze Vision Encoder/Adapters, train main LLM (up to 30B) on mixed data using Cross-Entropy loss
  3. Image Decoder Training: Train diffusion decoder separately to map discrete tokens back to pixels

- **Design tradeoffs:** Hybrid tokenizer complexity justified by abating interference seen in simpler baselines; multi-stage training pipeline rather than end-to-end joint training

- **Failure signatures:**
  - Understanding Degradation: Check continuous adapter training and data mixture balance
  - Generation Incoherence: Verify discrete token prediction quality rather than diffusion decoder failure
  - Codebook Collapse: Monitor discrete token diversity and usage

- **First 3 experiments:**
  1. Tokenizer Ablation: Train three 1B models with Hybrid, Pure-Discrete, and Dual-Encoder tokenizers to verify performance balance
  2. Task Interference Test: Compare Unified model against separate Understanding-only vs. Generation-only models
  3. Scaling Validation: Swap 0.9B vs 3.5B Image Decoders with frozen 3B LLM to confirm structural vs semantic scaling effects

## Open Questions the Paper Calls Out

1. **Aesthetic Quality vs. Scaling:** Why does scaling the image decoder result in decreased aesthetic quality despite improvements in structural integrity? The authors note this trade-off empirically but don't investigate the underlying mechanism in the diffusion decoder.

2. **Benchmark Redesign:** How can evaluation benchmarks be redesigned to assess emergent capabilities of unified models that saturate standard metrics? Current benchmarks may not capture qualitative gains observed in human evaluations.

3. **Modality Extension:** Can the hybrid tokenizer + unified AR backbone architecture successfully extend to modalities beyond images, such as video or audio? The current study validates only for static images.

## Limitations

- Architecture Scaling Assumptions: Performance at scales beyond 30B parameters is unexplored, limiting understanding of asymptotic behavior
- Discrete Adapter Quantization: 64K codebook size appears arbitrary without justification for optimal dimensionality
- Diffusion Decoder Conditioning: No quantification of information loss when translating discrete tokens to pixel-level synthesis
- Data Composition Transparency: Exact distribution and quality of training data across phases is not specified

## Confidence

**High Confidence:**
- Hybrid tokenizer reduces task conflict (confirmed by Table 1)
- Competitive performance on both understanding and generation benchmarks
- Semantic-pixel decoupling via diffusion decoder is effective

**Medium Confidence:**
- Minimal task interference when scaling (Fig 5 shows trends but doesn't quantify directly)
- Effectiveness of 1:0.5 text:image loss ratio (no ablation on different ratios)
- Superiority of FSQ over VQ-VAE (claimed but not empirically validated against alternatives)

**Low Confidence:**
- Generalization to vision encoders beyond ViT
- Robustness to distribution shifts or out-of-domain inputs
- Long-term stability of multi-stage training pipeline

## Next Checks

1. **Cross-Encoder Generalization:** Train Manzano using SigLIP or DINOv2 encoder instead of ViT to verify hybrid tokenization generalizes beyond the specific vision backbone used.

2. **Quantization Method Ablation:** Implement and train Manzano variants using VQ-VAE, VQ-GAN, and different FSQ configurations (varying codebook sizes from 16K to 256K) to determine optimal quantization strategy.

3. **Task Interference Quantification:** Design experiment computing normalized performance gap between single-task models and unified model to provide quantitative interference metric rather than qualitative comparison.