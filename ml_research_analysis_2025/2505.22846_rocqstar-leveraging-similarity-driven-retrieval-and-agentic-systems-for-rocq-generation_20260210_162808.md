---
ver: rpa2
title: 'RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq
  generation'
arxiv_id: '2505.22846'
source_url: https://arxiv.org/abs/2505.22846
tags:
- proof
- theorem
- theorems
- rocq
- proofs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating formal proofs
  in Rocq by combining retrieval-based premise selection with an agentic system. The
  authors propose a similarity-driven retrieval mechanism using a self-attentive embedder
  model trained to predict proof similarity between theorems, significantly improving
  context selection for proof generation.
---

# RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation

## Quick Facts
- arXiv ID: 2505.22846
- Source URL: https://arxiv.org/abs/2505.22846
- Reference count: 40
- Key outcome: Proves 60% of theorems on IMM-300 benchmark using retrieval + agentic system, outperforming previous methods by 9-31 percentage points

## Executive Summary
This paper addresses the challenge of generating formal proofs in Rocq by combining retrieval-based premise selection with an autonomous agentic system. The authors propose a similarity-driven retrieval mechanism using a self-attentive embedder model trained to predict proof similarity between theorems, significantly improving context selection for proof generation. They also develop an autonomous multi-stage agentic system that uses multi-agent debate for planning and reflection mechanisms for adaptive strategy refinement. Evaluated on the IMM-300 dataset, the RocqStar ranker achieves up to 28% relative improvement over baselines, and the full agentic system successfully proves 60% of theorems—outperforming previous methods like Claude 3.5 Sonnet (51%) and Tactician (29%).

## Method Summary
The RocqStar system trains a self-attentive embedder (CodeBERT-base, 108M params) using InfoNCE contrastive loss on a dataset of 76,524 statement-proof pairs mined from CompCert, IMM, Promising2Imm, and XMM projects. The model learns vector representations where distances mirror Levenshtein edit distances between actual proofs. The agentic system uses a multi-stage workflow: Planning (multi-agent debate generates k=4 candidate strategies), Execution (executor calls tools via MCP server), and Reflection (retrieves similar proofs when failures exceed threshold). The system is evaluated on IMM-300, a dataset of 300 theorems with proofs up to 20 tactics, split into difficulty tiers.

## Key Results
- RocqStar ranker achieves 28% relative improvement in retrieval quality over Jaccard/ModernBERT baselines on IMM-300
- Full agentic system proves 60% of theorems, outperforming Claude 3.5 Sonnet (51%) and Tactician (29%)
- Multi-agent debate increases proof success rate by 20% overall and nearly doubles it for complex theorems (9-20 tactics)
- Reflection mechanism improves stability, with "Agent w/o Reflection" dropping to 48% success from 66%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proof-aware semantic retrieval outperforms syntactic similarity baselines for premise selection.
- **Mechanism:** The system trains a self-attentive embedder (encoder) on a dataset of Rocq theorems. Instead of training on statement text alone, the model learns vector representations where the distance between two statement embeddings mirrors the Levenshtein edit distance between their actual proofs. This ensures retrieved context contains structurally similar proof strategies, not just syntactically similar keywords.
- **Core assumption:** The relevance of a premise to a target theorem is better predicted by the structural similarity of their proofs than by the lexical similarity of their statements (which shows a weak negative correlation in the paper).
- **Evidence anchors:**
  - [abstract] "propose a novel approach based on a self-attentive embedder model... trained to predict proof similarity... 28% relative increase."
  - [section 2] "We design our retrieval method to guide context selection based on the similarity of the proofs..."
  - [corpus] **Weak/Contextual.** While the corpus contains general work on "Agentic Program Verification" and retrieval in Lean (LeanDojo), there is no specific external validation of the *proof-distance* loss function used here.
- **Break condition:** If proof strategies in the target domain are highly diverse or non-compositional, structural similarity may fail to generalize, rendering the embedder no better than lexical search (BM25).

### Mechanism 2
- **Claim:** Multi-Agent Debate (MAD) during planning significantly improves success rates for complex theorems by refining high-level strategy before execution.
- **Mechanism:** The system separates planning from execution. In the planning stage, two LLMs (proponent and opponent) debate a proposed proof strategy. A judge LLM synthesizes the debate into a final plan. This forces the exploration of counter-arguments and potential dead ends before costly tool interactions begin.
- **Core assumption:** A verified, debated plan reduces the search space during execution more effectively than single-pass planning or immediate execution.
- **Evidence anchors:**
  - [abstract] "incorporating multi-agent debate... increases the proof success rate by 20% overall and nearly doubles it for complex theorems."
  - [section 3.1] "By repeating this procedure, we generate k candidate strategies... evaluated by a plan scoring LLM."
  - [corpus] **Supported.** Neighbors like "Agentic Program Verification" and "Ax-Prover" align with the efficacy of multi-agent structures in formal reasoning.
- **Break condition:** If the execution environment is highly unpredictable or the theorem requires a single non-intuitive insight that defies "reasonable" debate, MAD might average out to safe but ineffective strategies.

### Mechanism 3
- **Claim:** Reflection and adaptive retrieval allow the agent to recover from execution errors and dead ends.
- **Mechanism:** During execution, if the agent exceeds a threshold of consecutive failed proof checks, a "reflection" mechanism triggers. It uses the RocqStar embedder to retrieve theorems similar to the *current* (stuck) goal, not just the original theorem. A critic model analyzes the deviation, and a replanner revises the strategy based on this new context.
- **Core assumption:** Failure is often caused by a lack of specific local context (lemmas/tactics) or a straying strategy, which can be fixed by dynamically re-querying the knowledge base.
- **Evidence anchors:**
  - [section 3.1] "...monitors the progress... once this number exceeds a predefined threshold... acriticmodel is called..."
  - [table 3] Shows "Agent w/o Reflection" drops to 48% success (from 66%), proving the mechanism is critical for stability.
  - [corpus] **Missing.** The corpus does not explicitly detail this specific reflection-retrieval loop in other works, though general "repair" concepts exist.
- **Break condition:** If the initial plan is fundamentally flawed, reflecting on partial progress without restarting the planning phase may compound errors rather than correcting them.

## Foundational Learning

- **Concept: Proof States vs. Theorem Statements**
  - **Why needed here:** The paper emphasizes that two theorems can have similar statements but dissimilar proofs. Understanding that a "proof state" (the specific context and goals at a given step) drives tactic selection is crucial for understanding why the retrieval mechanism targets proof similarity.
  - **Quick check question:** Can you explain why the authors claim BM25/Jaccard similarity on theorem statements is a weak proxy for proof relevance?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** The RocqStar ranker uses InfoNCE loss to train the embedder. You must understand how the model pulls "positive" pairs (similar proofs) closer in vector space and pushes "negative" pairs (dissimilar proofs) apart to comprehend the retrieval mechanism.
  - **Quick check question:** Why did the authors add "hard negatives" (pairs with intermediate distance) to stabilize training?

- **Concept: Agentic Workflows (Planning vs. Execution)**
  - **Why needed here:** The system isn't a single prompt. It is a pipeline of Planning -> Execution -> Reflection. Understanding the separation of concerns (reasoning about the plan vs. running the code) is necessary to debug the agent's behavior.
  - **Quick check question:** According to the ablation study, why does removing the planning stage result in the agent "wandering through the search space"?

## Architecture Onboarding

- **Component map:** BigRocq -> RocqStar Ranker -> MCP Server -> Agent Core
- **Critical path:** Input Theorem -> RocqStar Ranker (retrieves top-k premises) -> Planning Stage (MAD generates k plans) -> Plan Scoring (Selects top-l plans) -> Execution Stage (Executor calls tools via MCP) -> Reflection (If errors > threshold: Retrieve new similar proofs -> Replan -> Resume)
- **Design tradeoffs:**
  - **Cost vs. Accuracy:** The full agentic system costs ~1.3 USD per theorem vs. 0.25 USD for standard generation, traded for a ~9% absolute improvement in success.
  - **Generality vs. Specificity:** The embedder is trained on specific projects (CompCert, etc.). It may generalize poorly to Rocq projects with drastically different proof styles without retraining.
- **Failure signatures:**
  - **Plan Hallucination:** The planner suggests a non-existent tactic or lemma (detectable in the Execution stage).
  - **Search Loop:** The agent repeatedly queries `search_pattern` or `about_term` without making progress on `check_proof` (visible in trace logs, Appendix G).
  - **Goal Selector Issues:** The BigRocq tool fails on proofs using specific goal selectors (e.g., `all:`), potentially limiting the retriever's knowledge base.
- **First 3 experiments:**
  1. **Retriever Validation:** Isolate the ranker. Compare RocqStar embeddings vs. Jaccard/BM25 on a fixed validation set of theorems. Verify the "28% relative improvement" claim using the provided HuggingFace checkpoint.
  2. **Planning Ablation:** Run the agent on the IMM-50 subset with the MAD layer disabled (single-pass planning). Compare trace lengths and success rates to confirm if the agent "wanders" without debate.
  3. **Tool Tracing:** Execute the agent on a failing theorem and visualize the tool call distribution (as in Appendix G). Check if the agent is under-utilizing specific tools (like `print_term`) or over-indexing on `check_proof` retries.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the intermediate state extraction mechanism (BigRocq) be adapted to robustly handle proofs utilizing goal selectors (e.g., `all:`, `1:`)?
- **Basis in paper:** [explicit] Appendix A states the current heuristic algorithm breaks down for goal selectors and explicitly identifies this as an issue to be solved.
- **Why unresolved:** The current implementation relies on information from Coq-LSP that is insufficient for parsing the control flow of goal selectors, which are increasingly common in modern Rocq code.
- **What evidence would resolve it:** A modified extraction algorithm that successfully parses theorems with goal selectors and a resulting dataset including these previously excluded proofs.

### Open Question 2
- **Question:** Does the RocqStar agentic system maintain its performance advantage on complex theorems with reference proofs significantly exceeding the 20-tactic limit?
- **Basis in paper:** [explicit] Appendix D notes the evaluation dataset (IMM-300) is restricted to theorems with proofs $\le 20$ tactics, acknowledging they have "not evaluated our solution" on longer proofs.
- **Why unresolved:** The search space explosion for longer proofs affects neural methods differently; it is unknown if the multi-agent debate scales to sustain the 60% success rate on more complex proofs.
- **What evidence would resolve it:** Benchmark results on a dataset containing theorems with proof lengths > 20 tactics showing the success rate relative to baselines.

### Open Question 3
- **Question:** Can introducing a dedicated phase for contextual exploration mitigate the "wandering" behavior observed in failing agent execution trajectories?
- **Basis in paper:** [explicit] Appendix G observes that failing runs show uniform tool usage without a clear strategy and states, "We consider introducing a dedicated... phase... in future iterations."
- **Why unresolved:** It is unclear if forcing a structured exploration phase before execution will improve strategy formation or simply increase the cost and latency of the agent.
- **What evidence would resolve it:** A comparative study of execution traces showing reduced variance in tool calls and higher success rates when a pre-exploration phase is implemented.

## Limitations
- Domain-specificity: The RocqStar embedder is trained exclusively on proofs from CompCert, IMM, Promising2Imm, and XMM projects, limiting generalization to different proof styles
- High computational cost: The system costs ~1.3 USD per theorem due to multiple expensive LLM calls, compared to 0.25 USD for standard generation
- Complex failure handling: The reflection mechanism could potentially compound errors if the initial plan is fundamentally flawed rather than merely incomplete

## Confidence
- **High Confidence:** The retrieval mechanism's 28% relative improvement over baselines is well-supported by the evaluation on IMM-300 with multiple models (Claude 3.5 Sonnet, GPT-4o). The ablation study showing reflection's critical role (48% → 66% success) is also robust.
- **Medium Confidence:** The multi-agent debate's 20% overall improvement and 2× gain for complex theorems is supported by the data, but the mechanism's effectiveness may depend heavily on the quality of the judge model and the specific debate format.
- **Medium Confidence:** The claim that proof similarity is a better retrieval signal than statement similarity is supported by the weak negative correlation observed, but this may not generalize to all proof assistant domains.

## Next Checks
1. **Cross-Domain Retrieval Test:** Evaluate the RocqStar ranker on a held-out subset of theorems from a different Rocq project (not in the training corpus) to measure generalization. Compare performance against BM25 and ModernBERT to verify the proof-distance training objective provides consistent benefits across domains.

2. **Dynamic Planning Ablation:** Modify the agent to allow reflection-triggered replanning to also consider restarting from scratch (discarding the initial plan entirely). Compare success rates against the current reflection mechanism to determine if compounding errors is a significant concern.

3. **Cost-Performance Trade-off Analysis:** Systematically vary the number of debate rounds (k plans), reflection thresholds, and tool call limits to map the Pareto frontier of cost versus success rate. Identify the minimal agent configuration that maintains ≥55% success on IMM-300 to assess practical viability.