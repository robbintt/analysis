---
ver: rpa2
title: 'ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play
  Attacks'
arxiv_id: '2510.02677'
source_url: https://arxiv.org/abs/2510.02677
tags:
- attack
- response
- evaluation
- red-teaming
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARMS, a novel adaptive red-teaming agent
  framework designed to systematically uncover vulnerabilities in multimodal models.
  ARMS combines reasoning-enhanced multi-step orchestration with a layered memory
  module to optimize diverse multimodal attack strategies, including 11 novel approaches
  spanning visual context cloaking, typographic transformation, and visual reasoning
  hijacking.
---

# ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks

## Quick Facts
- arXiv ID: 2510.02677
- Source URL: https://arxiv.org/abs/2510.02677
- Reference count: 40
- This paper introduces ARMS, a novel adaptive red-teaming agent framework designed to systematically uncover vulnerabilities in multimodal models.

## Executive Summary
This paper introduces ARMS, a novel adaptive red-teaming agent framework designed to systematically uncover vulnerabilities in multimodal models. ARMS combines reasoning-enhanced multi-step orchestration with a layered memory module to optimize diverse multimodal attack strategies, including 11 novel approaches spanning visual context cloaking, typographic transformation, and visual reasoning hijacking. Experiments across both instance-based and policy-based evaluations demonstrate that ARMS achieves state-of-the-art attack success rates, improving performance by an average of 52.1% over existing baselines and exceeding 90% on Claude-4-Sonnet. Additionally, ARMS generates significantly more diverse red-teaming instances, revealing emerging vulnerabilities. Leveraging ARMS, the authors construct ARMS-BENCH, a large-scale multimodal safety dataset with over 30K instances across 51 risk categories. Safety fine-tuning with ARMS-BENCH substantially improves model robustness while preserving utility, offering actionable guidance for enhancing multimodal safety alignment.

## Method Summary
ARMS is an adaptive red-teaming agent that treats vulnerability discovery as a reasoning-driven, multi-step optimization problem. The framework maintains an "action turn" state where it iteratively queries a library of plug-and-play attacks, evaluates the victim's response using a judge, and refines the next attack based on feedback. A layered memory architecture indexed by risk category and attack strategy prevents mode collapse and enforces diversity through an epsilon-greedy exploration algorithm. The system includes 11 novel multimodal attack strategies targeting cross-modal misalignment vulnerabilities in VLMs.

## Key Results
- ARMS achieves state-of-the-art attack success rates, improving performance by an average of 52.1% over existing baselines
- The framework exceeds 90% attack success rate on Claude-4-Sonnet and demonstrates significant vulnerability discovery capabilities
- ARMS generates significantly more diverse red-teaming instances, revealing emerging vulnerabilities across 51 risk categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ARMS achieves higher attack success rates (ASR) by treating red-teaming as a reasoning-driven, multi-step optimization problem rather than a static single-shot execution.
- **Mechanism:** The agent maintains an "action turn" state where it iteratively queries a library of plug-and-play attacks (via MCP), evaluates the victim's response using a judge, and refines the next attack based on feedback. This allows it to chain weak attacks into strong ones or switch strategies upon failure.
- **Core assumption:** Victim models exhibit consistent, exploitable failure modes that an external reasoning agent can model and probe iteratively.
- **Evidence anchors:** [abstract] "...ARMS automatically optimizes diverse red-teaming strategies with reasoning-enhanced multi-step orchestration..."; [section] "At each turn t, the ARMS first reasons and outputs the action at with base instance..."; [corpus] The related "AutoRedTeamer" framework also supports lifelong integration, but ARMS specifically attributes a 12.3% ASR drop to the removal of reasoning (Table 5).
- **Break condition:** If the victim model introduces sufficient randomness or inconsistency in its refusals, the feedback signal becomes too noisy for the agent to form a coherent optimization path.

### Mechanism 2
- **Claim:** A layered memory architecture indexed by (risk category, attack strategy) prevents mode collapse and enforces diversity.
- **Mechanism:** The agent stores successful attack trajectories in a 2D grid. During retrieval, it uses an epsilon-greedy strategy: with probability $\epsilon$ it explores, otherwise it retrieves the top-$k$ relevant memories via embedding similarity. This balances exploiting known successful patterns for a specific risk (e.g., "malware") while forcing exploration of different attack vectors (e.g., "visual cloaking" vs "typographic").
- **Core assumption:** Embedding similarity effectively maps new harmful requests to semantically similar past experiences that remain valid attack vectors.
- **Evidence anchors:** [abstract] "...design a layered memory with an epsilon-greedy attack exploration algorithm."; [section 3.4] "By storing a high scoring attack trajectory $\zeta$ in each two-dimensional memory slot... guarantees the attack effectiveness... prevents overfitting to a single attack type."; [corpus] "Active Attacks" mentions RL for diverse prompts; ARMS explicitly uses retrieval-based memory to manage this trade-off.
- **Break condition:** If the memory capacity threshold is too low or the weighting parameter $\alpha$ (balancing category vs. prompt similarity) is misconfigured, the agent retrieves irrelevant memories, leading to inefficient optimization.

### Mechanism 3
- **Claim:** VLMs are vulnerable to cross-modal misalignment where safety filters trained primarily on text fail to process adversarial signals embedded in images.
- **Mechanism:** ARMS introduces 11 novel strategies (e.g., Visual Context Cloaking, Typographic Transformation) that hide harmful intent in images (e.g., flowcharts, fake emails). The victim VLM processes the visual context as benign or authoritative, bypassing textual safety classifiers.
- **Core assumption:** Safety alignment in current VLMs is often shallow or modality-disjoint, failing to reason about the combined semantic meaning of "image + text".
- **Evidence anchors:** [abstract] "...11 novel multimodal attack strategies, covering diverse adversarial patterns... (e.g., reasoning hijacking, contextual cloaking)"; [section 3.3] "Visual context cloaking... hides harmful content in obfuscated visual-text formats... Rule-based wraps adversarial prompts in procedural or compliance-like images."
- **Break condition:** If VLMs adopt robust cross-modal encoders that jointly reason over image-text pairs before safety classification (e.g., internal chain-of-thought alignment), these visual obfuscations would be detected.

## Foundational Learning

**Model Context Protocol (MCP)**
- **Why needed here:** ARMS uses MCP to wrap attack strategies as independent servers. Understanding this interface is required to add new attack modules or debug tool calls.
- **Quick check question:** How does the agent decide which MCP tool to call based on the current reasoning step?

**Epsilon-Greedy Decay ($\lambda$)**
- **Why needed here:** The transition from exploring diverse attacks to exploiting successful ones is controlled by this decay rate.
- **Quick check question:** If $\lambda$ is set to 0 (no decay), how does it affect the agent's ability to exploit learned successful strategies?

**Policy-Based Judging (Likert Scale)**
- **Why needed here:** Unlike binary classification, the judge uses a 1-5 scale. Understanding the threshold $\tau$ is critical for defining "attack success."
- **Quick check question:** In the policy-based evaluation, does the judge score only the final turn of a multi-turn conversation or all turns?

## Architecture Onboarding

**Component map:**
Backbone Agent -> MCP Attack Library -> Layered Memory -> Victim VLM -> Judge

**Critical path:**
Risk Definition → Memory Retrieval (ε-greedy) → Reasoning (Select Strategy) → MCP Tool Call → Victim Query → Judge Evaluation → Memory Update

**Design tradeoffs:**
- **Memory Top-$k$:** Ablation shows $k=3$ is optimal (95.2% ASR); $k=7$ drops performance to 85.6% due to context noise.
- **Backbone Choice:** GPT-4o significantly outperforms Qwen3-235B (95.2% vs 80.6% ASR), indicating the attacker's reasoning capability is a bottleneck.

**Failure signatures:**
- **Mode Collapse:** Agent repeats the same strategy (e.g., "Email thread") despite low judge scores. Check if $\epsilon$ decay is too aggressive or if memory retrieval weights ($\alpha$) are too low.
- **API Refusals:** Victim returns empty response with stop reason 'refusal' (observed in Claude 4 Opus with SI-Attack). ARMS must switch strategies dynamically when specific attack patterns trigger hard-coded filters.

**First 3 experiments:**
1. **Memory Ablation:** Run ARMS on StrongReject with $k=0$ (no memory) vs. default $k=3$. Verify the ~6% ASR drop reported in Table 6 to confirm the memory module's contribution.
2. **Strategy Diversity Audit:** Run 100 instances and measure the frequency of each strategy used. Ensure the distribution is not dominated (>50%) by a single strategy like "Flowchart."
3. **Judge Sensitivity:** Compare evaluation results using GPT-4o as judge vs. o3-mini on the EU AI Act policy set. Check if the "reasoning-enhanced" judge shifts the ASR distribution (Table 8 suggests it does).

## Open Questions the Paper Calls Out
None

## Limitations
- **Generalization Risk:** ARMS achieves high attack success rates (up to 95.2% with StrongReject) but this performance is measured against specific VLM implementations. The framework's effectiveness may degrade against VLMs with stronger cross-modal alignment or integrated safety reasoning that jointly processes image-text pairs before classification.
- **Memory Dependency:** The layered memory architecture shows significant contribution to performance (6% ASR drop when disabled), creating a potential brittleness. The system's effectiveness depends heavily on maintaining relevant attack trajectories in memory, which may not transfer well to new risk categories or victim models with different failure modes.
- **Safety Fine-tuning Caveat:** While ARMS-BENCH improves model robustness, the evaluation only measures resistance to ARMS-generated attacks. This creates a potential overfitting scenario where models become specifically resistant to these attack patterns without gaining broader safety capabilities against unknown attack vectors.

## Confidence
**High Confidence:** The multi-step reasoning mechanism and memory architecture are well-supported by ablation studies (Tables 5 and 6 show 12.3% and 6% performance drops respectively when these components are removed).

**Medium Confidence:** The cross-modal vulnerability claims are supported by the novel attack strategies but lack extensive evaluation against VLMs with state-of-the-art cross-modal safety alignment, which may limit generalizability.

**Low Confidence:** The safety improvement claims from ARMS-BENCH fine-tuning are based on resistance to ARMS-generated attacks only, without broader validation against diverse attack methodologies or real-world safety metrics.

## Next Checks
1. **Cross-Modal Safety Evolution:** Evaluate ARMS against VLMs that have implemented explicit cross-modal safety reasoning (joint image-text processing before safety classification) to test the framework's robustness to advanced safety measures.

2. **Attack Transferability:** Test whether models fine-tuned on ARMS-BENCH maintain resistance when attacked by completely different red-teaming frameworks (e.g., AutoRedTeamer or Active Attacks) to validate genuine safety improvement versus pattern-specific resistance.

3. **Memory Generalization:** Conduct experiments where ARMS encounters risk categories not present in its training memory to assess how well the epsilon-greedy exploration strategy adapts to truly novel attack scenarios versus exploiting learned patterns.