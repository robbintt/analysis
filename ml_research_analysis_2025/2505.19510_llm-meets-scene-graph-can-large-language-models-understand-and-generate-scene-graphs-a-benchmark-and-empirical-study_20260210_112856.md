---
ver: rpa2
title: 'LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene
  Graphs? A Benchmark and Empirical Study'
arxiv_id: '2505.19510'
source_url: https://arxiv.org/abs/2505.19510
tags:
- scene
- graph
- node
- graphs
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study

## Quick Facts
- arXiv ID: 2505.19510
- Source URL: https://arxiv.org/abs/2505.19510
- Reference count: 40
- Key outcome: None

## Executive Summary
This paper introduces TSG Bench, a benchmark evaluating LLMs on scene graph understanding and generation tasks. The study reveals a significant performance gap between understanding (high accuracy) and generation (low F1 scores), with action decomposition emerging as the primary bottleneck. The benchmark includes four tasks: Scene Graph Description Selection (SGDS), Scene Graph Question Answering (SGQA), Single Action Scene Graph Generation (SA-SGG), and Multiple Action Scene Graph Generation (MA-SGG). Through systematic evaluation of various LLMs, the authors demonstrate that while models excel at comprehending structured scene graphs, they struggle to generate them from narratives, particularly when actions are implicit or repetitive.

## Method Summary
The benchmark uses 120 scenarios with 2,041 descriptions and 4,289 scene graphs from the EASG dataset. Four tasks are evaluated: SGDS (multiple-choice description selection), SGQA (question answering over graphs), SA-SGG (single action generation), and MA-SGG (multiple action generation from narratives). Evaluation metrics include Exact Match for QA tasks, Accuracy for SGDS, and Precision/Recall/F1 for generation tasks. The study tests zero-shot prompting, Chain-of-Thought, and 10-shot in-context learning across multiple LLM models via OpenRouter API with temperature=0.1. Scene graphs use triplet representations with nodes (person, action, object, hand) and edges (verb, dobj, preposition), with strict vocabulary constraints for evaluation.

## Key Results
- Claude-3.5-Sonnet achieves 98.40 on SGDS but only 58.80 F1 on MA-SGG, revealing a ~40% performance gap
- Action decomposition is the primary bottleneck, with models showing higher precision but lower recall in MA-SGG
- Few-shot in-context learning improves generation performance, while Chain-of-Thought aids reasoning-heavy understanding tasks
- Smaller models (Mistral-7B, Qwen-2.5-7B) show higher hallucination rates with elements outside the predefined vocabulary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit asymmetric capability in scene graph processing, excelling at understanding but struggling with generation due to fundamental differences in cognitive demands.
- Mechanism: Understanding tasks require pattern matching over structured inputs, whereas generation demands explicit decomposition of implicit actions, temporal ordering, and structured output synthesis.
- Core assumption: The performance gap reflects inherent task complexity rather than training data distribution alone.
- Evidence anchors:
  - Table 2 shows Claude-3.5-Sonnet achieving 98.40 on SGDS but only 58.80 F1 on MA-SGG—a ~40% gap
  - Limited direct corpus validation; related work (Schema-Guided Scene-Graph Reasoning, arXiv:2502.03450) corroborates structured reasoning challenges
- Break condition: If models can perform generation equally well when actions are explicitly stated (no implicit decomposition needed), the bottleneck is decomposition, not generation per se.

### Mechanism 2
- Claim: Action decomposition failure is the primary bottleneck in multi-action scene graph generation, not node/edge extraction.
- Mechanism: When narratives contain implicit or repeated actions, LLMs fail to segment the text into discrete temporal units before mapping to graphs.
- Core assumption: The decomposition capability is separable from graph construction ability.
- Evidence anchors:
  - Figure 4 shows action decomposition (ACT) scores consistently lower than edge/node generation across all models
  - No corpus papers directly isolate action decomposition as a mechanism; this is a novel contribution
- Break condition: If providing explicit action segmentation cues eliminates the MA-SGG performance gap with SA-SGG, decomposition is confirmed as the bottleneck.

### Mechanism 3
- Claim: Few-shot in-context learning improves generation by providing structural templates, while Chain-of-Thought aids reasoning-heavy understanding tasks.
- Mechanism: Few-shot examples demonstrate the target output format and decomposition patterns, reducing structural uncertainty. CoT decomposes multi-hop reasoning in QA tasks but may confuse models already struggling with output structure in generation.
- Core assumption: The improvement mechanism differs by task type rather than being uniformly applicable.
- Evidence anchors:
  - Table 4: "10-shot ICL generally improved performance across tasks, particularly for SGDS, SA-SGG, and MA-SGG"
  - Table 4: CoT helps SGQA (+5.2 for GPT-4o) but minimal gains for generation tasks on smaller models
- Break condition: If CoT + few-shot combined show no additive benefit over few-shot alone for generation, structural templating is the dominant mechanism.

## Foundational Learning

- Concept: Scene graphs as structured triplet representations (source node, edge, target node) encoding entities, attributes, and relationships.
  - Why needed here: Understanding TSG Bench tasks requires fluency in how textual narratives map to discrete graph elements.
  - Quick check question: Given "The person picked up the cup with their left hand," can you identify the nodes and edges?

- Concept: Action-centric scene graphs tracking temporal sequences with hand state management (hand1/hand2 assignment rules).
  - Why needed here: MA-SGG evaluation depends on correctly sequencing actions and tracking hand occupancy across scenes.
  - Quick check question: If hand1 holds object A, what happens when a second object B is grasped?

- Concept: Implicit vs. explicit action decomposition—narratives may describe compound actions requiring inference of intermediate steps.
  - Why needed here: The paper's core finding is that implicit action decomposition is where models fail most severely.
  - Quick check question: "The surface was cleaned thoroughly" implies what discrete actions that should appear in a scene graph?

## Architecture Onboarding

- Component map: TSG Bench (4 tasks) -> Evaluation (EM/Accuracy/F1) -> Analysis (subtask decomposition, error typing, hallucination tracking)
- Critical path: 1. Understand scene graph schema 2. Map narrative to action sequence 3. Generate triplets respecting hand state rules 4. Validate against predefined element set L
- Design tradeoffs: Providing action count cues in MA-SGG reduces ambiguity but doesn't solve ordering/segmentation; strict element vocabulary prevents open-vocabulary generation but enables precise evaluation
- Failure signatures: Low recall, high precision in MA-SGG → incomplete scene decomposition; "Repetitive action" failures → numerical/counting limitation; Hallucination in small models → elements not in L or description
- First 3 experiments:
  1. Establish baseline on SA-SGG with zero-shot prompting to isolate generation ability from decomposition
  2. Run MA-SGG ablation: provide ground-truth action segmentation vs. action count only to test decomposition vs. ordering hypotheses
  3. Test refinement capability: inject known error types and measure correction success with/without error-type labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be enhanced to effectively decompose complex narratives containing implicit or repetitive actions into discrete, sequential scene graphs?
- Basis in paper: The authors identify the failure to "effectively decompose discrete scenes from a complex narrative" as the primary bottleneck in generation tasks.
- Why unresolved: Current models show a significant performance drop in Multiple Action Scene Graph Generation (MA-SGG), specifically failing on "Implicit" and "Repetitive" action subsets.
- What evidence would resolve it: A method achieving significantly higher Recall on the MA-SGG task, particularly for implicit actions, without external decomposition cues.

### Open Question 2
- Question: Does integrating visual grounding via Vision-Language Models (VLMs) improve scene graph generation fidelity compared to text-only LLMs?
- Basis in paper: The authors state in Limitations that "Future directions may integrate multi-modal approaches, such as utilizing Vision-Language Models (VLMs)."
- Why unresolved: TSG Bench relies on text narratives derived from videos, but all experiments used text-only LLMs, leaving the potential of direct visual grounding untested.
- What evidence would resolve it: Performance metrics from VLMs on a multimodal extension of TSG Bench showing improved accuracy over text-only baselines.

### Open Question 3
- Question: Can LLMs be trained to autonomously detect and refine specific error types in generated scene graphs without explicit error labels?
- Basis in paper: Section 5.4 reveals LLMs refine graphs significantly better when provided with error types ("w/ Error Type"), suggesting the bottleneck is autonomous error detection.
- Why unresolved: The study only tested refinement given the ground-truth error type; it did not demonstrate if models can self-diagnose to leverage this corrective capability.
- What evidence would resolve it: An iterative refinement pipeline where an LLM achieves similar performance to the "w/ Error Type" baseline without external supervision.

## Limitations
- The evaluation uses single inference runs with temperature=0.1, limiting robustness assessment and statistical variance reporting
- Strict element vocabulary (set L) constrains open-vocabulary generation capability, potentially underestimating model flexibility
- Proprietary model access via OpenRouter API introduces version uncertainty with no documented model versions or API timestamps

## Confidence
- **High Confidence (CL=0.8-1.0):** The systematic performance gap between understanding and generation tasks is robust
- **Medium Confidence (CL=0.6-0.7):** Action decomposition is the primary bottleneck in MA-SGG generation
- **Low Confidence (CL=0.4-0.5):** The specific benefits of few-shot vs. Chain-of-Thought prompting mechanisms

## Next Checks
1. Run each task with 10 different random seeds at temperature=0.1 and temperature=0.7 to establish performance variance
2. Create a controlled MA-SGG experiment where ground-truth action segmentation is provided but ordering is scrambled vs. only action counts provided
3. Remove the strict element vocabulary constraint (set L) and evaluate hallucination rates and generation completeness