---
ver: rpa2
title: 'Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction'
arxiv_id: '2505.20164'
source_url: https://arxiv.org/abs/2505.20164
tags:
- visual
- reasoning
- abstract
- tasks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Visual Abstract Thinking (VAT) addresses the challenge of redundant\
  \ information in images that can impair multimodal reasoning. It introduces a novel\
  \ paradigm where Multimodal Large Language Models (MLLMs) are prompted with visual\
  \ abstracts\u2014simplified representations that retain essential visual elements\u2014\
  rather than verbose intermediate reasoning steps."
---

# Thinking with Visual Abstract: Enhancing Multimodal Reasoning via Visual Abstraction

## Quick Facts
- **arXiv ID**: 2505.20164
- **Source URL**: https://arxiv.org/abs/2505.20164
- **Reference count**: 40
- **Primary result**: VAT consistently outperforms Chain-of-Thought and tool-using baselines, achieving 2.21% accuracy gain over GPT-5 and 90% token reduction

## Executive Summary
Visual Abstract Thinking (VAT) introduces a novel paradigm for multimodal reasoning by providing Multimodal Large Language Models with visual abstractsâ€”simplified sketch representations that preserve essential visual elements while removing redundant information. Unlike Chain-of-Thought methods that rely on verbose intermediate reasoning steps, VAT encodes reasoning guidance directly in the visual input through dual-image prompting (original image + visual abstract). Experimental results demonstrate consistent accuracy improvements across six benchmarks and significant efficiency gains, with VAT achieving 63.09% accuracy versus 54.51% for single-image approaches and reducing token consumption by 90% compared to visual tool-using methods.

## Method Summary
VAT transforms input images into visual abstracts using various sketch conversion techniques (Canny edge detection, PhotoSketch, OpenSketch, etc.) and provides both the original image and abstract to MLLMs through a dual-image prompting approach. The method is evaluated across six benchmarks (BLINK, CoSpace, HallusionBench, TextVQA-GT, ActiView, Odd-One-Out) using multiple MLLM backends (GPT-4o, GPT-5, Gemini-2.5-Flash, Qwen2.5-VL variants). Accuracy is measured via substring matching, and efficiency is evaluated through token consumption, runtime, and API cost. The visual abstract generation can be implemented using available sketch conversion tools or deep learning models, with prompt templates provided in Appendix B.

## Key Results
- VAT achieves 63.09% accuracy on average benchmarks, outperforming single-image visual abstracts (54.51%) and repeated images (60.41%)
- Dual-image VAT reduces token consumption by 90% compared to visual tool-using methods while maintaining superior accuracy
- OpenSketch visual abstracts provide the best average improvement (+9.77%) across tasks, though style performance varies by task type
- VAT shows particular effectiveness for object relation and spatial reasoning tasks, while performance degrades on fine-grained perception tasks like counting

## Why This Works (Mechanism)

### Mechanism 1
Visual abstracts reduce redundant visual information, improving reasoning accuracy by focusing model attention on essential elements. By transforming images into sketch representations that preserve structural and relational features while discarding surface textures, VAT enables models to concentrate on core visual concepts. Evidence shows ascending accuracy trends when progressively revealing ground-truth regions via sketch, though this relies on controlled benchmarks.

### Mechanism 2
Dual-image prompting (original + visual abstract) outperforms single-image or text-only augmentation approaches. The visual abstract serves as a perceptual scaffold that allows models to cross-reference detailed visual context with abstract structural cues. Experimental results show dual-image VAT achieving 63.09% accuracy versus 54.51% for visual abstracts alone, demonstrating the complementary value of both representations.

### Mechanism 3
VAT achieves efficiency gains by shifting reasoning cost from expensive output tokens to cheap input tokens. Unlike CoT (which generates verbose intermediate rationales) or tool-using methods (which call external visual tools), VAT encodes reasoning guidance statically in the visual abstract input. API logs show VAT costs $5.70 versus CoT at $11.42 and reduces output tokens from 1,481.95 to 538.28.

## Foundational Learning

- **Visual Abstraction Types and Tradeoffs**: Why needed - VAT requires selecting appropriate abstraction style (Canny, Binary, PhotoSketch, Contour, OpenSketch) based on task requirements. Quick check - Given a spatial reasoning task, which abstraction style would preserve layout while removing texture?
- **Attention Distribution in Vision-Language Models**: Why needed - Understanding how VAT shifts attention from distractors to ground-truth regions. Quick check - How would you verify whether a visual abstract successfully redirects model attention to task-relevant regions?
- **Explicit vs. Implicit Reasoning Paradigms**: Why needed - VAT positions itself against CoT and tool-using approaches; understanding tradeoffs guides implementation choices. Quick check - For a task requiring both fine-grained perception and multi-step planning, would VAT alone suffice or require CoT combination?

## Architecture Onboarding

- **Component map**: Image Preprocessing Module -> Prompt Template Constructor -> MLLM Backbone -> Optional Style Selector
- **Critical path**: 1) Receive (image, question) pair, 2) Apply visual abstract transformation T (default: OpenSketch), 3) Construct prompt with original image + visual abstract + minimal instructions, 4) Query MLLM; extract answer via pattern matching
- **Design tradeoffs**: OpenSketch best overall (+9.77% avg improvement) but PhotoSketch better for specific tasks; dual-image consistently better but increases input size; VAT + CoT combination helps planning-heavy tasks but dilutes efficiency
- **Failure signatures**: Performance degrades on fine-grained tasks (counting, text recognition); smaller models (Qwen-7B) benefit less due to context strain; multiple abstract styles together reduce performance vs. single style
- **First 3 experiments**: 1) Ablation on abstraction type - test Canny, Binary, PhotoSketch, OpenSketch on validation set, 2) Single vs. dual image comparison - test V^a only vs. dual-input on simple tasks, 3) Attention visualization - replicate Figure 12 analysis on target task distribution

## Open Questions the Paper Calls Out

- **Embodied AI applicability**: Can VAT paradigms be effectively applied to real-world embodied AI and GUI interaction tasks beyond static academic benchmarks? Current evaluation is limited to static image-based benchmarks; embodied scenarios require temporal reasoning and action generation.
- **Internal information flow dynamics**: What are the internal information flow dynamics within MLLMs when processing visual abstracts versus raw images? The paper provides attention visualizations but lacks mechanistic interpretability analysis of layer-wise information propagation.
- **Adaptive abstraction optimization**: How can visual abstract generation be adaptively optimized to preserve fine-grained details required for tasks like counting and illusion detection? Current methods uniformly filter detail without task-specific calibration.

## Limitations

- Performance degrades on fine-grained tasks (counting, text recognition) where visual abstracts lose critical details
- Limited gains on smaller open-source models due to increased input context length strain
- Effectiveness depends on appropriate abstraction style selection, which varies by task type
- Generalizability to complex real-world images beyond controlled benchmarks remains uncertain

## Confidence

**High Confidence**: Dual-image prompting advantage, token efficiency gains, core methodology
**Medium Confidence**: Redundant information reduction claims, complementary reasoning paradigm positioning
**Low Confidence**: Optimal abstraction style universality, real-world complexity generalization

## Next Checks

1. **Attention Mapping Validation**: Replicate attention visualization analysis across all tested MLLM types and representative tasks to verify consistent attention redirection to ground-truth regions
2. **Task-Specific Style Selection**: Implement rule-based style selection approach and evaluate whether it outperforms single-style OpenSketch baseline across full benchmark suite
3. **Real-World Complexity Test**: Apply VAT to held-out real-world images with complex visual scenes to assess performance degradation patterns and identify success/failure categories