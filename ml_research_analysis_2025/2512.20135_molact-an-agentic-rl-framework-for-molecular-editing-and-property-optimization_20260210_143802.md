---
ver: rpa2
title: 'MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization'
arxiv_id: '2512.20135'
source_url: https://arxiv.org/abs/2512.20135
tags:
- molecular
- optimization
- editing
- tasks
- molact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MolAct introduces an agentic reinforcement learning framework
  for molecular editing and optimization, treating these tasks as sequential, tool-guided
  decision processes. The method uses a two-stage training paradigm: first building
  editing capabilities, then optimizing properties while reusing learned editing behaviors.'
---

# MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization

## Quick Facts
- arXiv ID: 2512.20135
- Source URL: https://arxiv.org/abs/2512.20135
- Reference count: 10
- Primary result: Two-stage agentic RL achieves 100% valid edits and 92% LogP optimization success

## Executive Summary
MolAct introduces an agentic reinforcement learning framework for molecular editing and optimization, treating these tasks as sequential, tool-guided decision processes. The method uses a two-stage training paradigm: first building editing capabilities, then optimizing properties while reusing learned editing behaviors. MolAct employs a large language model agent that interleaves reasoning, tool-use, and molecular optimization, interacting with external chemical tools for validity checking, property assessment, and similarity control. Experimental results on ChemCoTBench demonstrate strong performance: MolEditAgent-7B achieves 100%, 95%, and 98% valid add, delete, and substitute edits, outperforming strong baselines like DeepSeek-R1. MolOptAgent-7B surpasses closed "thinking" baselines like Claude 3.7 on LogP optimization and remains competitive on solubility while maintaining balanced performance across other objectives.

## Method Summary
MolAct is an agentic reinforcement learning framework that treats molecular editing and optimization as sequential, tool-guided decision processes. The method employs a two-stage training paradigm: Stage 1 trains MolEditAgent on atomic editing operations (add/delete/substitute) using validity and similarity feedback, while Stage 2 initializes MolOptAgent from Stage 1 weights and adds property-driven rewards for optimization. The framework uses a large language model agent that interleaves reasoning, tool-use, and molecular optimization, interacting with external chemical tools for validity checking, property assessment, and similarity control. Group-relative policy gradient optimization stabilizes long-horizon credit assignment across parallel rollout chains.

## Key Results
- MolEditAgent-7B achieves 100%, 95%, and 98% valid add, delete, and substitute edits
- MolOptAgent-7B achieves 92% LogP optimization success rate
- Outperforms strong baselines including DeepSeek-R1 and Claude 3.7 on key benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Curriculum Learning Transfers Edit Primitives to Optimization
Pretraining on edit operations before property optimization enables learning of tool-usage and termination policies that direct optimization cannot acquire. Stage 1 trains MolEditAgent on atomic operations with validity and similarity feedback. Stage 2 initializes MolOptAgent from Stage 1 weights, reusing learned edit behaviors while adding property-driven rewards. This transfer works because edit primitives share substructure with property optimization objectives.

### Mechanism 2: External Tool Feedback Enforces Chemical Validity
Multi-turn tool calls for validity checking and property assessment prevent hallucination of chemically invalid structures. The agent interleaves reasoning → tool call → observation cycles. Tools return validity flags, property scores, and similarity metrics that remain in context as grounded feedback for subsequent edits.

### Mechanism 3: Group-Relative Advantage Estimation Stabilizes Long-Horizon Credit Assignment
Normalizing rewards within parallel rollout groups reduces variance in multi-turn, tool-mediated trajectories. For each prompt, K parallel chains form one group sharing the same source molecule. Advantages computed group-relatively; gradients applied only to agent-generated tokens via binary mask, treating tool outputs as fixed context.

## Foundational Learning

- **SMILES String Representation**
  - Why needed: States are molecules encoded as SMILES; edits apply pattern-based operators to strings. Without understanding SMILES syntax, tool feedback is uninterpretable.
  - Quick check: Can you identify the functional group in "CCO" and predict what happens if you delete the terminal carbon?

- **Reinforcement Learning with Tool-Using Agents**
  - Why needed: MolAct is explicitly an "Agentic RL" framework where the policy learns when to call tools vs. when to terminate. Standard RL without tool interfaces won't transfer.
  - Quick check: How does a tool-calling action differ from an environment transition action in a standard MDP?

- **Curriculum Learning / Staged Training**
  - Why needed: The two-stage paradigm is the core contribution. Without understanding why editing before optimization matters, practitioners may skip Stage 1 and fail.
  - Quick check: Why might learning to add a functional group help with optimizing LogP, even though the objectives differ?

## Architecture Onboarding

- **Component map**: Qwen-2.5-3B/7B backbone -> Tool Interface (validity checker, property oracles) -> Rollout Engine (K parallel chains) -> Reward Calculator -> GRPO Optimizer

- **Critical path**: Stage 1 (editing) → checkpoint → Stage 2 (optimization) → final MolOptAgent. Skipping Stage 1 yields ~0% success.

- **Design tradeoffs**: 3B vs 7B (3B trains faster but inefficient, 7B more effective); Turn budget (16) limits exploration; Reward weights (0.8/0.15/0.05) prioritize task success over similarity.

- **Failure signatures**: One-stage training yields near-zero success rates despite tool access; Small models achieve high rewards but low success due to inefficient tool sequencing; JNK3 bioactivity shows negative Δ indicating domain knowledge gaps.

- **First 3 experiments**: 1) Replicate Stage 1 on ChemCoTBench subset to validate validity improvement; 2) Ablate tool access in Stage 1 to confirm hallucination rates increase; 3) Test transfer by initializing Stage 2 from random vs. Stage 1 checkpoint on LogP.

## Open Questions the Paper Calls Out

- **Can incorporating synthetic feasibility and reaction-aware constraints during optimization improve the practical utility of generated molecules without sacrificing property optimization performance?**
  - Basis: Authors identify lack of synthetic feasibility modeling as limitation and future direction
  - Why unresolved: Current framework only enforces chemical validity through SMILES-level checks, not synthesizability
  - Evidence needed: Experiments integrating synthesis prediction tools into MolAct framework

- **What adaptive interaction strategies or error recovery mechanisms could enable smaller models (e.g., 3B) to reliably execute learned tool-augmented policies within interaction budgets?**
  - Basis: Authors observe 3B models struggle with tool sequencing despite similar rewards to 7B
  - Why unresolved: Gap between reward learning and policy executability remains unexplained
  - Evidence needed: Ablation studies testing curriculum-based approaches or dynamic turn budgets

- **How does bias or inaccuracy in external property oracles affect optimization trajectories and final molecule quality in the MolAct framework?**
  - Basis: Authors identify oracle dependency as limitation
  - Why unresolved: Framework assumes oracle outputs are reliable ground truth
  - Evidence needed: Experiments injecting controlled noise into oracle outputs

## Limitations

- Two-stage training is critical but exact duration and curriculum design remain underspecified
- 3B model's poor performance appears to stem from inefficient tool usage rather than fundamental capability gaps
- Property oracle implementations, particularly for bioactivity targets, are black-boxed
- Group-relative advantage normalization assumes homogeneous difficulty within prompt groups

## Confidence

- **High confidence**: Two-stage curriculum approach works; tool-augmented RL prevents hallucination; external validity checking is essential
- **Medium confidence**: 7B outperforms 3B due to efficiency rather than raw capability; group-relative advantages improve training stability
- **Low confidence**: Specific reward weights (0.8/0.15/0.05) are optimal; 16-turn budget is sufficient for all optimizations; GRPO's group-relative normalization is superior to alternatives

## Next Checks

1. **Transfer ablation test**: Initialize MolOptAgent-7B from random weights vs. MolEditAgent-7B checkpoint on LogP optimization, measuring Δ success rates

2. **Tool dependency validation**: Train Stage 1 without validity checker tool, measuring hallucination rates and comparing to baseline

3. **Model scale efficiency analysis**: Track token usage per episode during Stage 1 training for 3B vs 7B models, correlating response length with edit success rates