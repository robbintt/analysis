---
ver: rpa2
title: Improving Deep Tabular Learning
arxiv_id: '2509.16354'
source_url: https://arxiv.org/abs/2509.16354
tags:
- learning
- tabular
- data
- transformer
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of deep learning on tabular data,
  which is complicated by heterogeneous feature types, lack of natural structure,
  and limited augmentations. The authors introduce RuleNet, a transformer-based architecture
  that incorporates learnable rule embeddings, piecewise linear quantile projection
  for numerical features, and feature masking ensembles for robustness.
---

# Improving Deep Tabular Learning

## Quick Facts
- arXiv ID: 2509.16354
- Source URL: https://arxiv.org/abs/2509.16354
- Reference count: 40
- Primary result: Transformer-based RuleNet matches or exceeds tree-based methods on tabular benchmarks, especially larger datasets

## Executive Summary
This work addresses the challenge of deep learning on tabular data, which is complicated by heterogeneous feature types, lack of natural structure, and limited augmentations. The authors introduce RuleNet, a transformer-based architecture that incorporates learnable rule embeddings, piecewise linear quantile projection for numerical features, and feature masking ensembles for robustness. Evaluated on eight benchmark datasets, RuleNet matches or surpasses state-of-the-art tree-based methods in most cases, while remaining computationally efficient. The experiments demonstrate the effectiveness of the proposed method, particularly on larger datasets, and provide insights into the importance of each component through ablation studies. RuleNet offers a practical neural alternative for tabular prediction tasks, addressing key challenges in the field.

## Method Summary
RuleNet is a transformer-based architecture for tabular data that processes heterogeneous feature types through learnable embeddings. The encoder handles input features, while a decoder with learnable rule embeddings attends to encoder outputs. Numerical features are projected via piecewise linear quantile interpolation between learnable embeddings, preserving ordinal relationships while capturing non-linear patterns. Feature masking is applied stochastically during training, and predictions are ensembled at inference to provide robustness and uncertainty estimates. The model uses MaxPool aggregation over rule embeddings, followed by a linear projection to outputs.

## Key Results
- RuleNet matches or exceeds state-of-the-art tree-based methods on 8 benchmark datasets
- Performance advantages are most pronounced on larger datasets (>50K samples)
- Feature masking ensemble shows the highest contribution to performance improvements
- Encoder-decoder architecture provides computational efficiency scaling as O(M) vs O(M²)

## Why This Works (Mechanism)

### Mechanism 1: Encoder-Decoder Architecture with Learnable Rule Embeddings
The decoder with N learnable rule embeddings processes encoder outputs to produce predictions, decoupling model capacity from input feature count. This allows FLOPS scaling as O(M) rather than O(M²) while maintaining expressive power through rule-based attention patterns.

### Mechanism 2: Piecewise Linear Quantile Projection for Numerical Features
Numerical features are represented through quantile-based interpolation between learnable embeddings, preserving ordinal relationships while capturing non-linear patterns. This bridges purely categorical encoding (breaks ordinality) and linear transforms (limited expressivity).

### Mechanism 3: Feature Masking Ensembles for Robustness and Uncertainty
Stochastic feature masking during training forces the model to learn redundant representations, reducing over-reliance on any single feature. Ensembling predictions at inference provides calibrated uncertainty estimates and improves generalization.

## Foundational Learning

- **Encoder-Decoder Transformer Architecture**: Needed to understand cross-attention patterns where decoder queries attend to encoder keys/values. Quick check: Can you explain why decoder attention (N rules × M features) differs from encoder self-attention (M × M)?

- **Quantile Transformation**: Essential for understanding how piecewise linear embedding maps arbitrary distributions to uniform [0,1]. Quick check: Given values [1, 5, 5, 100], what are the 0.25, 0.5, 0.75 quantiles?

- **Feature Masking as Regularization**: Important because tabular data lacks obvious augmentations unlike images. Masking is the tabular analog of dropout at feature level. Quick check: Why does ensembling masked predictions reduce OOD risk compared to single unmasked prediction?

## Architecture Onboarding

- **Component map**: Input Features (M) → [Categorical → Embedding | Numerical → Piecewise Quantile Embedding] → Encoder (Le layers) → Decoder (Ld layers, N rules) → MaxPool over N rules → Linear layer → Prediction

- **Critical path**: Implement quantile projection correctly using the interpolation formula (1-f)·e^{qᵢ} + f·e^{qᵢ₊₁}; ensure decoder cross-attention uses full visibility; implement K-fold ensemble inference at test time.

- **Design tradeoffs**: More rules increase capacity but add O(N×M) decoder cost; n_quantiles should start at 2 and increase only if validation plateaus; ensemble size K trades uncertainty quality for inference latency.

- **Failure signatures**: Small datasets (<30K) show high sensitivity to hyperparameters; performance drops when disabling masking at inference; n_quantiles > 2 without validation improvement suggests collapsing to linear baseline.

- **First 3 experiments**: (1) Baseline on CA dataset with nq=2, N_rules=64, mask_rate=0.1; target RMSE ~0.45. (2) Ablation sweep on CO comparing full RuleNet vs encoder-only vs no masking. (3) Mask rate sensitivity on CO with rates {0.0, 0.1, 0.2, 0.4}, expecting peak around 0.1-0.15.

## Open Questions the Paper Calls Out

### Open Question 1
Does the encoder-decoder architecture provide significant performance benefits over encoder-only transformers when scaling to datasets with extremely high feature dimensionality? The authors hypothesize benefits for large datasets but couldn't evaluate due to computational constraints, having restricted experiments to datasets train-able under a few hours on single GPU.

### Open Question 2
Can the learned "rule" embeddings in the decoder be reverse-engineered to yield explicit, human-readable logical rules? While the architecture is inspired by rule-based logic, the authors provide no analysis demonstrating that specific attention heads or embedding dimensions correspond to discrete conditions like "Age > 50."

### Open Question 3
What statistical properties of a dataset determine the superiority of Piecewise Linear Quantile Projection over simpler linear embeddings? Results show 5/8 datasets preferred nq=2 (linear), and the authors call the utility of their complex projection "somewhat a disappointing finding" without identifying dataset characteristics that necessitate higher nq values.

## Limitations
- Performance advantages diminish on smaller datasets (<30K samples) where hyperparameter sensitivity becomes critical
- Piecewise linear quantile projection shows limited utility, with 5/8 datasets preferring linear (nq=2) over piecewise structure
- High mask rates (>0.2) can significantly degrade performance, especially on smaller datasets

## Confidence
- Transformer-based architectures matching tree-based methods: **Medium** confidence (strong on larger datasets, weaker on smaller ones)
- Encoder-decoder architecture benefits: **Medium** confidence (FLOPS analysis rigorous but lacks direct tabular comparison)
- Piecewise linear quantile projection: **Medium** confidence (mathematical formulation clear but limited empirical benefit)
- Feature masking ensembles: **High** confidence (most consistent improvements across datasets)

## Next Checks
1. Replicate quantile projection implementation on CA dataset with nq=2 and nq=100 to verify interpolation formula produces different embeddings for same feature value under different quantile contexts.

2. Run ablation study on CO dataset comparing: (a) full RuleNet, (b) encoder-only transformer, (c) encoder-only + masking to confirm expected performance ordering and check if decoder hurts when N_rules is suboptimal.

3. Test mask rate sensitivity systematically on AD dataset with mask_rate ∈ {0.0, 0.1, 0.2, 0.3, 0.4} to validate performance peaks around 0.1-0.15 and degrades sharply beyond 0.3 as Figure 4 suggests.