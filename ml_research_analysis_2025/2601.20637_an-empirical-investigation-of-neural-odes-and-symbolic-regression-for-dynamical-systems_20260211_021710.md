---
ver: rpa2
title: An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical
  Systems
arxiv_id: '2601.20637'
source_url: https://arxiv.org/abs/2601.20637
tags:
- data
- equations
- equation
- training
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the extrapolation capabilities of Neural
  Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR)
  to recover governing equations from noisy data of two damped oscillatory systems.
  The study finds that NODEs can effectively extrapolate to new boundary conditions
  when trajectories share dynamic similarity with training data.
---

# An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems

## Quick Facts
- arXiv ID: 2601.20637
- Source URL: https://arxiv.org/abs/2601.20637
- Reference count: 40
- This paper investigates NODEs' extrapolation capabilities and SR's ability to recover governing equations from noisy data of damped oscillatory systems.

## Executive Summary
This study examines whether Neural Ordinary Differential Equations (NODEs) can extrapolate beyond their training data boundaries and whether Symbolic Regression (SR) can recover governing equations from noisy trajectories. The authors test two damped oscillatory systems: a cart-pole mechanical system and a bacterial adaptation model. They find that NODEs can effectively extrapolate to new boundary conditions when trajectories share dynamic similarity with training data. Additionally, they demonstrate that NODEs can serve as effective data augmentation tools for SR, filtering noise from raw data to improve equation discovery. The study also reveals important limitations of SR, particularly its dependence on appropriate input variable selection.

## Method Summary
The authors simulate two damped oscillatory systems using SciPy's odeint solver, adding uniform noise to create training data. For the cart-pole system, they train NODEs with MLPs (2 hidden layers × 20 nodes) using JAX and Diffrax to learn the vector field, then test extrapolation to unseen initial conditions. For the bacterial adaptation model, they train NODEs on sparse data and use them to generate full trajectories, which are then fed into PySR for equation discovery. The NODEs are trained using Adabelief optimizer, while SR searches for algebraic expressions mapping inputs to outputs using binary operations (+, -, ×, ÷) with complexity limits.

## Key Results
- NODEs can extrapolate effectively to new boundary conditions when trajectories share dynamic similarity with training data
- NODEs act as denoising filters, enabling better SR performance on noisy datasets
- SR successfully recovers governing equations from noisy ground-truth data when appropriate input variables are selected
- When using NODE-generated data from 10% of full simulation, SR recovers two out of three governing equations with good approximations for the third

## Why This Works (Mechanism)

### Mechanism 1: Phase-Space Trajectory Generalization
NODEs learn the vector field governing the system rather than memorizing specific time-series. If a new point lies downstream or upstream of a training point on the same dynamic manifold, the learned vector field allows the model to propagate the state accurately, even if that specific coordinate was unseen during training. This mechanism fails if the new initial condition places the system in a completely different basin of attraction not represented by any trajectory segment in the training data.

### Mechanism 2: Implicit Denoising via Continuous Integration
Raw data requires finite-difference methods to estimate derivatives for SR, which amplifies noise. A NODE learns a continuous function f(t, y) that minimizes the integral error. When this smooth function is solved to generate new data points, it produces "clean" trajectories that preserve the underlying physics but discard random noise, improving SR's ability to identify governing equations. This mechanism breaks down if the signal-to-noise ratio is extremely low or if the noise is systematic (bias).

### Mechanism 3: Structural Bias in Variable Selection
SR algorithms search for algebraic expressions mapping inputs X to outputs Y. If a governing variable Z is omitted, SR may fail or find a spurious correlation. Similarly, if a term k_a is insignificant relative to noise or data range, the algorithm simplifies the expression, effectively hiding the constant. This mechanism breaks if the user fails to engineer features that simplify the target expression, or if the data collection range is too narrow to activate latent constants.

## Foundational Learning

- **Phase Space and State Vectors**: Understanding that a dynamical system is defined by its state (angle + angular velocity) is critical to understanding the "dynamic similarity" extrapolation results. Quick check: Can you explain why a model trained on initial angle θ=0 might still predict the trajectory for θ=3.0 correctly based on "dynamic similarity"?

- **Derivative Estimation (Finite Differences vs. Continuous)**: The paper contrasts noisy finite-difference derivatives with the smooth derivatives learned implicitly by the NODE. Understanding the noise amplification in the former explains why the two-step pipeline works. Quick check: Why does calculating dy/dt from noisy data using (y_{t+1} - y_t) / Δt often result in garbage, whereas a NODE might handle it better?

- **Symbolic Regression Search (Pareto Front)**: The results show SR finding "good approximations" rather than exact equations. Understanding that SR balances equation complexity against loss (MSE) explains why it might drop a small term to lower complexity. Quick check: If PySR returns an equation with a slightly higher Loss but much lower Complexity than the "true" equation, which one will the algorithm prefer?

## Architecture Onboarding

- **Component map**: Data Source (Synthetic + Noise) -> NODE Core (MLP 2×20) -> ODE Solver (Diffrax) -> SR Engine (PySR) -> Governing Equations
- **Critical path**: The quality of SR discovery is strictly bottlenecked by the NODE's ability to generate smooth, accurate trajectories. If the NODE fails to converge or overfits noise, the SR step receives corrupted data.
- **Design tradeoffs**: Small network size (2 layers × 20 nodes) acts as a regularizer against overfitting noise but may fail on more complex systems. Limiting SR complexity to 25 speeds up search but risks excluding valid complex physical laws.
- **Failure signatures**: "Flat Landscape" Failure (SR finds simple approximations instead of true equations when signals are below noise floor), Constant Absorption (low-magnitude terms get absorbed into constants).
- **First 3 experiments**:
  1. Train the NODE on 10% of data with 0% noise vs. 20% noise. Verify if the NODE smooths the 20% noise curve effectively or creates artifacts.
  2. Run SR on the NODE-generated data but exclude the engineered feature λ. Confirm failure to recover Eqs 2 & 3.
  3. Take the trained Cart-Pole model and predict trajectories starting from θ values outside the training range but with ω values inside the range, and vice versa.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does extending the symbolic regression analysis to diverse, multi-condition datasets improve the recovery of governing equations compared to the single-shift simulation used in this study? The current study tested SR using data from only one specific up-shift simulation (ν_i = 2.53 to ν_f = 3.78), limiting the dynamic variety available for the algorithm to learn from.

- **Open Question 2**: Can the integration of physical priors, such as unit matching, enhance the ability of symbolic regression to distinguish between true physical laws and mathematical approximations? Without physical constraints, the SR algorithm relies solely on mathematical fit, causing it to miss terms masked by data scaling or to absorb small signals into constants.

- **Open Question 3**: How can the pipeline be modified to reliably recover low-signal terms (e.g., the -λψ_A term in Equation 2) that are currently obscured by dominant constants or noise? Standard loss functions (MSE) allow the algorithm to minimize error by adjusting constants rather than discovering low-magnitude structural terms.

## Limitations
- The study's conclusions are constrained by the relatively simple dynamics of the two damped oscillatory systems tested
- The extrapolation results rely heavily on "dynamic similarity" which may not hold for chaotic or multistable systems
- The symbolic regression results depend critically on appropriate feature engineering, and the paper does not provide systematic methods for identifying which features to include

## Confidence
- **High Confidence**: NODEs can effectively extrapolate to new boundary conditions when trajectories share dynamic similarity with training data
- **Medium Confidence**: NODEs act as denoising filters that improve SR performance
- **Medium Confidence**: SR can recover governing equations from noisy data when appropriate input variables are provided

## Next Checks
1. Test the NODE's extrapolation capability on a chaotic system (e.g., Lorenz attractor) to determine whether the "dynamic similarity" requirement holds when the vector field exhibits sensitive dependence on initial conditions
2. Systematically vary the noise level and signal-to-noise ratio in the Bio-model to quantify the threshold at which SR fails to recover governing equations, even with NODE denoising
3. Implement an automated feature engineering pipeline that identifies missing variables (like λ) by analyzing residuals between SR predictions and actual trajectories, then measure whether this improves equation recovery rates