---
ver: rpa2
title: 'The Optimiser Hidden in Plain Sight: Training with the Loss Landscape''s Induced
  Metric'
arxiv_id: '2509.03594'
source_url: https://arxiv.org/abs/2509.03594
tags:
- metric
- momentum
- optimiser
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new class of neural network optimisers\
  \ based on the Riemannian metric naturally induced when the loss landscape is embedded\
  \ in higher-dimensional space. This metric, which underlies common visualisations\
  \ of loss landscapes, automatically adapts the effective learning rate based on\
  \ local curvature\u2014decreasing it in highly curved regions and maintaining larger\
  \ updates in flatter areas."
---

# The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric

## Quick Facts
- arXiv ID: 2509.03594
- Source URL: https://arxiv.org/abs/2509.03594
- Reference count: 33
- One-line primary result: Introduces Riemannian metric-based optimizers that automatically adapt learning rates based on loss landscape curvature, achieving competitive performance across diverse tasks.

## Executive Summary
This paper introduces a new class of neural network optimizers derived from the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. By treating the loss value as a literal geometric dimension, these optimizers automatically adapt the effective learning rate—decreasing it in highly curved regions and maintaining larger updates in flatter areas. This mechanism acts as a smoothed form of gradient clipping and, in one variant, induces an implicit learning rate schedule with warm-up and decay phases.

The optimizers are computationally efficient with O(N) complexity comparable to Adam, requiring only a single additional dot product computation per iteration. They can be applied as a modification to any existing preconditioning method and naturally incorporate decoupled weight decay. Benchmarking across low-dimensional functions, MLP regression, MNIST, CIFAR-10, and TinyShakespeare shows strong performance, with one variant based on RMSprop performing competitively with state-of-the-art methods like Adam, AdamW, and Muon.

## Method Summary
The core innovation involves embedding the scalar loss function L into a higher-dimensional space by adding it as an extra dimension. This creates an ambient space with coordinates (θ, L), where θ are the model parameters. The Riemannian metric induced by this embedding naturally creates a geometry on the parameter space that adapts to local curvature. The key insight is that the effective learning rate is scaled by a factor inversely proportional to the gradient magnitude, creating a smooth gradient clipping effect. The framework generalizes to incorporate any existing preconditioner (like RMSprop) by using its metric as the base, then applying the pull-back mechanism to add the curvature-adaptive scaling. Three variants are proposed: using Euclidean metric with linear loss embedding, Euclidean metric with log-loss embedding, and RMSprop metric with linear loss embedding.

## Key Results
- In low-dimensional pathological functions, the optimizers achieved superior convergence and were the only methods to find global minima across all tested functions.
- For neural network training on MNIST, CIFAR-10, and TinyShakespeare, the RMSprop-based variant performed competitively with state-of-the-art methods like Adam, AdamW, and Muon, showing slight improvement on average.
- The log-loss embedding variant was highly effective in low dimensions but showed inconsistent performance in higher-dimensional tasks, particularly failing on CIFAR-10 and regression tasks.
- Computational efficiency is maintained at O(N) complexity, requiring only one additional dot product computation per iteration compared to standard optimizers.

## Why This Works (Mechanism)

### Mechanism 1: Smoothed Gradient Clipping
- **Claim:** The optimizer implements smoothed gradient clipping by scaling the update step inversely with the local gradient magnitude.
- **Mechanism:** When the loss landscape is embedded in higher-dimensional space, the induced "pull-back" metric modifies the geometry of the parameter space. The resulting inverse metric produces an update rule where the effective learning rate is divided by a term proportional to the squared norm of the gradient, dampening steps in high-curvature regions.
- **Core assumption:** The optimization trajectory benefits from a metric that treats the loss value as a literal geometric dimension, effectively stretching distances in regions of steep descent.
- **Evidence anchors:** [abstract] "The effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping." [section 2] Equation 10 shows the update scaling factor $1 / (1 + \sum l_k l^k)$ which limits divergence. [figure 2] Demonstrates the profile of gradient updates saturating as input gradient increases.
- **Break condition:** If the metric coefficient $\xi$ is set to 0, the mechanism reduces to standard gradient descent (or the base preconditioner), removing the clipping effect.

### Mechanism 2: Implicit Learning Rate Scheduling
- **Claim:** Using a logarithmic embedding function creates an implicit learning rate schedule (warm-up followed by decay).
- **Mechanism:** By embedding the loss as $f(L) = \ln(L)$, the geometric derivation injects the loss value $L$ directly into the update scaling term. This results in an effective learning rate $\eta_{eff} \approx L / (L^2 + \dots)$. Early in training (large $L$), the rate is low (warm-up); as $L$ decreases, the rate changes non-monotonically.
- **Core assumption:** The loss function is positive definite, and the training dynamics follow a rough power-law decay.
- **Evidence anchors:** [abstract] "One variant... can also be viewed as inducing an effective scheduled learning rate." [section 2] Equation 14 and Figure 3 illustrate how the effective learning rate evolves with training time under power-law assumptions.
- **Break condition:** If the loss becomes negative or fails to decrease, the schedule logic inverts or fails to provide stable decay.

### Mechanism 3: Enhanced Preconditioning
- **Claim:** The framework generalizes to enhance existing preconditioners (e.g., RMSprop), combining induced metric clipping with adaptive scaling.
- **Mechanism:** The "ambient space" metric $\gamma_{ij}$ can be set to the metric implied by any optimizer, such as RMSprop. The pull-back mechanism then adds the gradient-dependent clipping term on top of the RMS scaling. This hybrid approach (SGD RMS in the paper) proved most robust.
- **Core assumption:** The benefits of geometric clipping are complementary to, rather than exclusive of, diagonal preconditioning methods.
- **Evidence anchors:** [section 3.5] "The custom optimiser incorporating the metric from RMSprop proved to be a consistently strong performer." [corpus] Related work "Monge SAM" explores similar themes of loss geometry and reparameterization-invariant updates, supporting the focus on geometric properties.
- **Break condition:** If the chosen base metric $\gamma$ conflicts with the pull-back term (e.g., numerical instability in the denominator), convergence may degrade relative to the base optimizer.

## Foundational Learning

- **Concept: Riemannian Metric & Pull-back**
  - **Why needed here:** The core innovation relies on defining distance on a "surface" (the loss landscape) embedded in a larger space. Understanding that the metric tensor $g_{ij}$ defines local geometry and that "pulling back" translates the Euclidean metric from the ambient space to the parameter space is essential.
  - **Quick check question:** How does the length of a vector change if the metric tensor $g_{ij}$ is multiplied by a scalar factor?

- **Concept: Sherman-Morrison Formula**
  - **Why needed here:** The paper uses this formula to efficiently invert the specific rank-1 update structure of the induced metric ($A + uv^T$). This keeps the computational complexity at $O(N)$ rather than the $O(N^3)$ of naive matrix inversion.
  - **Quick check question:** Why is the Sherman-Morrison formula preferred over standard matrix inversion for the metric $g_{ij} = \gamma_{ij} + \partial_i L \partial_j L$?

- **Concept: Gradient Clipping**
  - **Why needed here:** The paper frames its method as a "smoothed" version of this standard technique. Knowing standard clipping (thresholding) helps contrast why the induced metric provides a softer, more continuous dampening.
  - **Quick check question:** Standard clipping caps the gradient norm. How does the proposed method's denominator achieve a similar effect without a hard threshold?

## Architecture Onboarding

- **Component map:** Inputs (gradients, parameters, loss) -> Metric Estimator (computes scalar trace) -> Scaling Factor (calculates $r_t$) -> Preconditioner (optional base metric) -> Update Engine (momentum update with scaled parameters)

- **Critical path:** The calculation of the global gradient norm (dot product) and its combination into the scalar denominator. This single scalar multiplier is what differentiates the update from standard SGD or Adam.

- **Design tradeoffs:**
  - **Euclidean vs. RMS Base:** Using $\gamma = I$ (Euclidean) adds only clipping but lacks per-parameter adaptivity. Using $\gamma = \text{RMSprop}$ adds adaptivity but introduces more hyperparameters ($\beta_{rms}$).
  - **Linear vs. Log Embedding:** The Log variant adds scheduling capabilities but introduces instability if loss is not strictly positive or varying predictably. It performed worse on regression tasks in the paper.

- **Failure signatures:**
  - **Vanishing Updates:** In regions of extremely high curvature/gradients, the denominator $1 + \dots$ becomes large, reducing the step size near-zero. If $\xi$ is too large, training stalls.
  - **Loss Explosion (Log Variant):** If loss becomes negative or numerically unstable, the effective learning rate in the Log variant can flip signs or spike.

- **First 3 experiments:**
  1. **Low-Dimensional Sanity Check:** Implement the "Linear" optimizer on the Rosenbrock function. Verify that it converges faster than vanilla SGD by plotting the trajectory and the adaptive learning rate $r_t$.
  2. **Ablation on $\xi$:** Train an MLP on MNIST using the SGD RMS variant. Sweep $\xi$ (e.g., $0.0, 10^{-5}, 10^{-3}$) to find the "sweet spot" where the additional metric term improves over the baseline RMSprop ($\xi \to 0$).
  3. **Log-Loss Instability Test:** Run the Log variant on a regression task. Observe if the training becomes unstable compared to the Linear variant, confirming the paper's observation that Log embedding is task-sensitive.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does adding extra dimensions for independent loss components improve optimization?
  - **Basis in paper:** [explicit] The authors ask, "in cases where there are k independent contributions to the loss, is it beneficial to add k extra dimensions, and pull-back to the loss-landscape from that space?"
  - **Why unresolved:** Current work only considers embedding the scalar total loss into one extra dimension.
  - **What evidence would resolve it:** Empirical testing on tasks with separable loss components (e.g., multi-task learning) using a k-dimensional embedding.

- **Open Question 2:** Can alternative embedding functions f(L) succeed in high dimensions where log-loss failed?
  - **Basis in paper:** [explicit] Noting the log-loss variant's success in low dimensions but failure in others, the authors ask if there are "other embedding functions that may be the reverse."
  - **Why unresolved:** The specific geometric properties causing the log-loss variant's inconsistent performance are not fully understood.
  - **What evidence would resolve it:** Deriving and testing new embedding functions specifically designed for high-dimensional neural network training.

- **Open Question 3:** Does the method scale effectively to significantly larger models?
  - **Basis in paper:** [explicit] The conclusion states, "ideally one would want to observe these methods when applied to significantly larger models."
  - **Why unresolved:** Benchmarks were limited to small-scale architectures like ResNet-18 and TinyShakespeare.
  - **What evidence would resolve it:** Benchmarking the optimizers on Large Language Models (LLMs) with billions of parameters.

- **Open Question 4:** Is there a benefit to utilizing off-diagonal elements of the ambient space metric?
  - **Basis in paper:** [explicit] The authors ask, "is there any benefit to turning on the off-diagonal elements of the metric on the ambient space...?"
  - **Why unresolved:** The paper restricts the ambient metric to a diagonal form (Equation 5) for simplicity.
  - **What evidence would resolve it:** Theoretical analysis and empirical benchmarking of non-diagonal metric variants.

## Limitations

- The log-loss embedding variant shows inconsistent performance across different task types, failing on CIFAR-10 and regression tasks despite success in low-dimensional settings.
- The paper does not explore interaction with modern training techniques like data augmentation, weight standardization, or normalization layers.
- Scalability to significantly larger models remains untested, with current benchmarks limited to ResNet-18 and small-scale language models.
- Exact hyperparameter values for best-performing runs are not specified, limiting reproducibility.

## Confidence

- **High Confidence:** The computational efficiency claim (O(N) complexity) and the mathematical derivation of the induced metric and its inverse via Sherman-Morrison are sound and well-supported.
- **Medium Confidence:** The empirical performance claims for the RMSprop-based variant on CIFAR-10 and MNIST are credible given the competitive results, but the lack of detailed hyperparameter specifications limits full reproducibility. The claim that the linear variant acts as a "smoothed gradient clipping" is conceptually supported but would benefit from more direct ablation studies.
- **Low Confidence:** The log-loss variant's implicit scheduling mechanism is well-motivated theoretically, but its inconsistent performance in high dimensions and failure on regression tasks indicates the mechanism may be fragile or highly task-dependent.

## Next Checks

1. **Ablation on Embedding Function:** Train the same model (e.g., MLP on MNIST) using both the linear and log-loss variants with identical hyperparameters (η, ξ, μ). Track not only final accuracy but also the evolution of the effective learning rate r_t to confirm the scheduling behavior of the log variant and its stability compared to the linear one.

2. **Gradient Norm Analysis:** For the RMSprop-based variant, record the gradient norm ||∇L|| and the denominator term (1 + ξΣl_k l_k) throughout training on CIFAR-10. Plot their ratio to verify that the "clipping" effect is active and correlates with regions of high curvature or large gradients.

3. **Cross-Task Hyperparameter Transfer:** Take the best-found hyperparameters (η, ξ, μ, β_rms) for the RMSprop variant on CIFAR-10 and apply them to a different task (e.g., TinyShakespeare). Assess whether the optimizer maintains its performance without re-tuning, testing the claim of robustness and general applicability.