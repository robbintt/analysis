---
ver: rpa2
title: Improved Ear Verification with Vision Transformers and Overlapping Patches
arxiv_id: '2503.23275'
source_url: https://arxiv.org/abs/2503.23275
tags:
- recognition
- dataset
- datasets
- performance
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates Vision Transformer (ViT) models for ear verification,
  focusing on the impact of overlapping patches. Using four ViT configurations (Tiny,
  Small, Base, Large), the study trains models on the UERC2023 dataset and tests on
  four datasets: OPIB, AWE, WPUT, and EarVN1.0.'
---

# Improved Ear Verification with Vision Verification with Vision Transformers and Overlapping Patches

## Quick Facts
- arXiv ID: 2503.23275
- Source URL: https://arxiv.org/abs/2503.23275
- Reference count: 40
- Primary result: Overlapping patches improved ear verification performance in 44 of 48 experiments, with up to 10% better results on EarVN1.0

## Executive Summary
This study investigates the impact of overlapping patches on Vision Transformer (ViT) performance for ear verification tasks. The researchers evaluated four ViT configurations (Tiny, Small, Base, Large) across different patch sizes and strides using the UERC2023 dataset for training and four datasets (OPIB, AWE, WPUT, EarVN1.0) for testing. The key finding is that overlapping patches consistently improved verification accuracy, with the ViT-T model performing best overall. The optimal configuration used a patch size of 28×28 with a stride of 14 pixels, demonstrating that strategic overlap can significantly enhance biometric recognition performance.

## Method Summary
The researchers trained four Vision Transformer configurations (ViT-Tiny, Small, Base, Large) on the UERC2023 ear dataset, testing various patch sizes (16×16 to 28×28) and strides (7 to 14 pixels) to create overlapping patches. Models were evaluated on four benchmark ear datasets (OPIB, AWE, WPUT, EarVN1.0) using verification accuracy as the primary metric. The study systematically compared non-overlapping versus overlapping patch configurations, measuring performance differences across all model sizes and patch configurations. The experimental design focused on identifying optimal patch parameters while maintaining consistent model architectures and training procedures.

## Key Results
- Overlapping patches improved performance in 44 out of 48 experiments across all datasets
- ViT-T model consistently outperformed other configurations regardless of patch settings
- Best performance achieved with 28×28 patch size and 14-pixel stride on EarVN1.0 dataset
- Performance improvements reached up to 10% on EarVN1.0 compared to non-overlapping patches

## Why This Works (Mechanism)
The effectiveness of overlapping patches stems from their ability to capture contextual information between adjacent regions that would otherwise be lost with non-overlapping partitions. When patches overlap, features near patch boundaries receive attention from multiple patch representations, reducing information loss at edges and improving the model's ability to learn continuous spatial patterns. This is particularly valuable for ear verification where fine-grained details and smooth transitions between anatomical features are critical for accurate recognition. The overlapping mechanism effectively increases the receptive field without changing the model architecture, allowing the transformer to better understand local spatial relationships.

## Foundational Learning
- Vision Transformers: Self-attention mechanisms that process image patches as tokens, requiring understanding of how patch size affects feature granularity and computational complexity.
- Ear Biometrics: Specialized recognition domain with unique challenges including pose variation, lighting conditions, and inter-class similarity that affect model design choices.
- Overlapping Patch Design: Trade-off between increased feature continuity and computational overhead, where stride < patch size creates overlap regions.
- Verification vs Identification: Binary matching task requiring distance metrics and threshold optimization rather than classification, affecting evaluation methodology.
- Dataset Domain Adaptation: Training on UERC2023 and testing on other datasets requires understanding of domain shift and generalization challenges in biometric systems.

## Architecture Onboarding

**Component Map:**
Input Images → Patch Extraction (with overlapping) → Linear Projection → Transformer Encoder (Multi-head Self-Attention + Feed-Forward) → Classification Head

**Critical Path:**
Patch extraction with overlap → Linear embedding → Multi-head self-attention → Feed-forward networks → Classification/verification output

**Design Tradeoffs:**
The primary tradeoff involves balancing patch overlap against computational cost and memory usage. Larger overlaps (smaller strides) improve feature continuity but increase the number of patches and computational load. The study found that moderate overlap (stride = 50% of patch size) provided optimal performance gains without prohibitive computational overhead. Model size selection also involves tradeoffs between accuracy and efficiency, with ViT-T showing surprisingly strong performance despite its smaller parameter count.

**Failure Signatures:**
Poor performance with overlapping patches typically manifests as marginal improvements or degradation when stride becomes too small (<25% of patch size), indicating that excessive overlap introduces redundant information and computational waste. Another failure mode occurs when patch sizes are too large relative to ear feature scale, causing loss of fine-grained details regardless of overlap strategy.

**First Experiments:**
1. Compare verification accuracy using non-overlapping patches (stride = patch size) versus minimal overlap (stride = 0.75 × patch size) on a validation subset
2. Test different overlap ratios (25%, 50%, 75% of patch size) to identify the optimal stride for the specific ear dataset
3. Evaluate the impact of overlap on computational requirements by measuring inference time and memory usage across different stride configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to ear biometrics, restricting generalizability to other biometric modalities
- All experiments use single training dataset (UERC2023), potentially introducing domain-specific biases
- Performance evaluation focuses on accuracy without comprehensive analysis of computational efficiency trade-offs
- ViT-T model's consistent superiority may be dataset-dependent rather than a universal architectural advantage

## Confidence
- High Confidence: Overlapping patches improve performance (44/48 experiments showing improvement) is statistically robust
- Medium Confidence: ViT-T superiority across configurations is well-demonstrated but may be dataset-specific
- Medium Confidence: Optimal configuration (28×28 patch size, stride 14) is clearly identified but may not generalize across different datasets

## Next Checks
1. Test overlapping patch approach on non-ear biometric modalities (face, fingerprint, iris) to assess generalizability
2. Conduct cross-dataset validation by training on UERC2023 and testing on additional ear datasets not included in original study
3. Perform ablation studies comparing overlapping patches against other patch configuration strategies (hierarchical patches, adaptive sizing)