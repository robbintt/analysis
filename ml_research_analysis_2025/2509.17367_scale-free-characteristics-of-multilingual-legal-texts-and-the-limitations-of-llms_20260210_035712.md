---
ver: rpa2
title: Scale-free Characteristics of Multilingual Legal Texts and the Limitations
  of LLMs
arxiv_id: '2509.17367'
source_url: https://arxiv.org/abs/2509.17367
tags:
- legal
- texts
- text
- language
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper uses scale-free metrics\u2014Heaps\u2019 exponent \u03B2\
  , Taylor\u2019s exponent \u03B1, and gzip compression rate r\u2014to compare linguistic\
  \ complexity across legal, general, and AI-generated texts. Legal corpora (statutes,\
  \ cases, deeds) show slower vocabulary growth (lower \u03B2), higher term consistency\
  \ (higher \u03B1), and greater redundancy (higher r) than general-language texts."
---

# Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs

## Quick Facts
- arXiv ID: 2509.17367
- Source URL: https://arxiv.org/abs/2509.17367
- Reference count: 40
- Primary result: Legal texts show distinct statistical signatures (lower β, higher α) compared to general and AI-generated texts, revealing limitations in LLM's ability to replicate specialized drafting conventions

## Executive Summary
This paper analyzes linguistic complexity across legal, general, and AI-generated texts using scale-free metrics: Heaps' exponent β, Taylor's exponent α, and gzip compression rate r. Legal corpora (statutes, cases, deeds) exhibit slower vocabulary growth, higher term consistency, and greater redundancy than general-language texts. GPT-generated texts align more with general patterns and show less variance. These findings reveal domain-specific drafting conventions in legal language that current AI models do not replicate, highlighting fundamental limitations in AI's ability to capture specialized textual structures.

## Method Summary
The analysis involves preprocessing text corpora (lowercase, tokenize), segmenting them into fixed-size chunks of approximately 300,000 tokens, and computing statistical exponents. Heaps' exponent β is calculated via log-log regression of vocabulary size versus total word count. Taylor's exponent α is derived from the scaling relationship between word frequency standard deviation and mean across text segments. Compression rate r is measured using gzip compression ratios. The study compares legal, general, and GPT-generated texts across these metrics without any training procedure.

## Key Results
- Legal texts show lower Heaps' exponent β (0.52-0.77) indicating slower vocabulary growth compared to literature (β ≈ 0.62)
- Legal corpora exhibit higher Taylor's exponent α (0.624-0.733) reflecting greater term clustering and consistency
- GPT-generated legal text clusters with general language patterns (higher β, lower α) rather than true legal texts
- Legal texts demonstrate higher compression rates r, indicating greater redundancy in specialized vocabulary usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal drafting conventions impose structural constraints that slow vocabulary growth relative to text length.
- Mechanism: Statutes and formal legal documents utilize a restricted, specialized lexicon (defined terms, boilerplate). This limits the rate at which unique new terms ($V$) are introduced as total word count ($N$) increases, resulting in a lower Heaps' exponent $\beta$ compared to general language.
- Core assumption: The observed statistical patterns are primarily driven by prescriptive drafting rules (precision/consistency) rather than purely semantic content.
- Evidence anchors:
  - [abstract] "Legal corpora... show slower vocabulary growth (lower $\beta$)..."
  - [Section 6.2] "Statutes... cluster at low $\beta$... reflecting uniform, precise drafting."
  - [corpus] Weak direct causal link in corpus; inferred from correlation between "strict drafting" (Section 1) and low $\beta$ in statutes.
- Break condition: If a legal text is poorly drafted or uses highly variable, non-standardized terminology, $\beta$ would likely rise, converging with general text patterns.

### Mechanism 2
- Claim: Logical consistency requirements in legal text force terms to appear in clustered, repetitive contexts.
- Mechanism: To reduce ambiguity, legal documents use specific terms within specific logical contexts (e.g., "plaintiff" appears in clusters during case descriptions). This causes the fluctuation of word occurrences ($\sigma$) to scale non-linearly with mean frequency ($\mu$), yielding a higher Taylor exponent $\alpha$.
- Core assumption: Contextual clustering of words is a measurable proxy for the "logical consistency" required in legal discourse.
- Evidence anchors:
  - [abstract] "...higher term consistency (higher $\alpha$)..."
  - [Section 3.1] "We compute $\alpha$ in order to capture contextual clustering of words: a higher $\alpha$ indicates a greater context dependence."
  - [Section 6.1] Legal corpora occupy the upper-left region of the $\alpha$–$\beta$ plane with strong term clustering ($\alpha=0.624$–$0.733$).
- Break condition: If a text is shuffled or structured purely randomly, $\alpha$ would approach $0.5$, breaking the observed pattern.

### Mechanism 3
- Claim: LLMs fail to replicate legal statistical signatures because their generative priors are dominated by general-language scaling laws.
- Mechanism: General-purpose LLMs (like GPT) are pre-trained on vast, diverse corpora where vocabulary growth is faster and clustering is lower. When prompted for legal text, the model generates fluent content but retains the statistical "fingerprint" ($\beta \approx 0.65, \alpha \approx 0.60$) of its training distribution rather than adapting to the restricted domain signature.
- Core assumption: The scale-free metrics of generated text are robust indicators of the model's internal distribution, not just artifacts of the prompt.
- Evidence anchors:
  - [abstract] "GPT-generated texts align more with general patterns and exhibit less variance."
  - [Section 6.3] "GPT-generated texts shift toward higher $\beta$ and lower $\alpha$, reflecting faster vocabulary turnover and weaker clustering."
  - [Section 7] "...current language models... struggle to replicate true legal writing and lack sufficient variance."
- Break condition: If an LLM were specifically fine-tuned on a massive corpus of *only* strict legal codes, its $\beta$ and $\alpha$ generation statistics might shift toward the legal cluster.

## Foundational Learning

- Concept: **Heaps' Law ($V \propto N^\beta$)**
  - Why needed here: This is the primary metric used to quantify "vocabulary growth." Understanding that $\beta < 1$ implies slowing growth is essential to interpreting why legal texts (lower $\beta$) are statistically distinct from literature (higher $\beta$).
  - Quick check question: If a text doubles in length ($2N$), does its vocabulary double? Why does a lower $\beta$ imply a more "restricted" vocabulary?

- Concept: **Taylor's Law ($\sigma \propto \mu^\alpha$)**
  - Why needed here: This quantifies "clustering." The paper relies on the premise that $\alpha > 0.5$ indicates context-dependent usage. You must grasp this to understand why legal texts are "high consistency."
  - Quick check question: If a word appears frequently in a text, how does its standard deviation scale? What does $\alpha = 0.5$ vs. $\alpha = 0.8$ signify about that word's distribution?

- Concept: **Scale-Free Properties**
  - Why needed here: The paper argues these metrics are "scale-free" to justify comparing texts of different lengths (e.g., a 300k chunk vs. a 10M corpus).
  - Quick check question: Why is it critical that $\beta$ and $\alpha$ are independent of corpus size for this cross-domain comparison to be valid?

## Architecture Onboarding

- Component map:
  1. **Data Loader:** Ingests raw text (Legal/General/GPT).
  2. **Preprocessor:** Lowercases, removes punctuation, tokenizes.
  3. **Chunker:** Splits corpora into fixed-size segments (≈ 3 × 10^5 tokens) to stabilize regression.
  4. **Metric Engine:** Calculates β (log-log regression of V vs N), α (regression of log σ vs log μ), and r (gzip ratio).
  5. **Visualizer:** Plots the α–β plane to inspect clustering.

- Critical path: The **Chunking** and **Metric Engine** are critical. Incorrect chunking (too small/large) introduces noise in the regression slopes (β, α), rendering the scale-free comparison invalid.

- Design tradeoffs:
  - **Compression Rate (r) vs. Scale-Free Exponents:** The paper finds r less discriminative for GPT detection (overlap) but easier to compute. (β, α) provides a richer signature but requires careful regression.
  - **Chunk Size:** Smaller chunks allow more data points but increase variance in exponent estimation; the paper chose ≈ 300k tokens as a stability threshold.

- Failure signatures:
  - **GPT "Legal" Failure:** GPT generates text *about* law but fails the statistical test (β too high, α too low).
  - **Metric Failure:** If H_{norm} (normalized entropy) is used alone, it fails to separate categories (Section 6.4).

- First 3 experiments:
  1. **Reproduce the α–β Plane:** Download the specified corpora (US Code vs. Literature), chunk them, and verify that legal texts cluster in the upper-left.
  2. **Test GPT Robustness:** Generate legal text with varying temperatures. Does the α–β cluster expand, or does it remain tightly bound in the "general" region?
  3. **Fine-Tuning Stress Test:** Fine-tune a small language model (e.g., a mini-GPT) strictly on statutory law. Check if its generated text shifts β downward and α upward toward the legal cluster.

## Open Questions the Paper Calls Out
None

## Limitations
- The interpretation that statistical patterns directly reflect prescriptive drafting conventions is correlational rather than proven causal
- The study does not explore whether fine-tuning LLMs on restricted legal corpora could shift their statistical signatures toward legal patterns
- The paper does not specify critical implementation details like tokenization and segment size for Taylor's Law calculations

## Confidence
- **High confidence:** Legal texts exhibit distinct scale-free properties (Low β, High α) compared to general language
- **High confidence:** GPT-generated legal text fails to replicate these statistical signatures
- **Medium confidence:** These patterns directly reflect prescriptive drafting conventions rather than other factors
- **Medium confidence:** Current LLM limitations reflect fundamental constraints in capturing domain-specific structures

## Next Checks
1. **Tokenization Audit**: Re-run the analysis with and without punctuation to quantify its impact on β and verify the correct preprocessing protocol
2. **Segmented Taylor's Law**: Systematically vary the segment size used for α calculation to determine sensitivity and optimize the window for capturing contextual clustering
3. **Fine-Tuning Experiment**: Fine-tune a base LLM exclusively on statutory law and evaluate whether its generated text shifts toward the legal cluster in the α-β plane, testing the hypothesis about training distribution effects