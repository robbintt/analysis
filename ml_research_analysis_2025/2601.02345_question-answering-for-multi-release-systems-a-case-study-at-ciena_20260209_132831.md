---
ver: rpa2
title: 'Question Answering for Multi-Release Systems: A Case Study at Ciena'
arxiv_id: '2601.02345'
source_url: https://arxiv.org/abs/2601.02345
tags:
- qamr
- answer
- query
- chunks
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QAMR, a chatbot designed for question answering
  over multi-release software documentation. The key challenge addressed is the overlap
  and differences in documentation across multiple versions of the same software,
  which can confuse traditional retrieval-augmented generation (RAG) approaches.
---

# Question Answering for Multi-Release Systems: A Case Study at Ciena

## Quick Facts
- arXiv ID: 2601.02345
- Source URL: https://arxiv.org/abs/2601.02345
- Reference count: 40
- Primary result: QAMR achieves 88.5% average answer correctness and 90% retrieval accuracy, outperforming baseline by 16.5% and 12% respectively

## Executive Summary
This paper introduces QAMR, a chatbot designed for question answering over multi-release software documentation. The key challenge addressed is the overlap and differences in documentation across multiple versions of the same software, which can confuse traditional retrieval-augmented generation (RAG) approaches. QAMR tackles this by using separate corpora for each release, query rewriting to handle multi-release contexts, context reduction to filter irrelevant information, and dual-chunking to optimize both retrieval and answer generation. It also employs RankRAG to select the most relevant contexts. Evaluated on Ciena's industrial dataset and REQuestA, QAMR achieves an average answer correctness of 88.5% and retrieval accuracy of 90%, outperforming the baseline by 16.5% and 12% respectively. The approach also reduces response time by about 8%.

## Method Summary
QAMR extends traditional RAG for multi-release software documentation by implementing five key mechanisms: (1) separate corpora per release instead of unified indexing, (2) query rewriting into three variants (Base, Filtered, Versionless) to handle release-aware and release-agnostic queries, (3) dual-chunking that divides documents into smaller search chunks for retrieval and larger context chunks for answer generation, (4) context reduction using chain-of-thought prompts to extract query-relevant information from retrieved chunks, and (5) context selection inspired by RankRAG to rank and select the most useful reduced contexts. The system uses Llama-3-70B-Instruct with BAAI/bge-base-en-v1.5 embeddings, Chroma vectorstore, and LangChain v0.2.8, with fixed parameters k=2 search chunks/page and ps=500 character padding.

## Key Results
- QAMR achieves 88.5% average answer correctness across all evaluation metrics, compared to baseline RAG
- Retrieval accuracy reaches 90% with QAMR, outperforming baseline by 12 percentage points
- Response time improves by approximately 8% compared to baseline approach
- Dual-chunking and context reduction mechanisms contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Creating separate corpora per release and rewriting queries into release-aware variants improves retrieval accuracy when documentation overlaps across versions.
- Mechanism: QAMR segments documents into release-specific corpora. Query rewriting extracts release numbers (via LLM, not regex) and generates three standalone query types: Base (history-aware), Filtered (stop-word reduced), and Versionless (release-agnostic for fallback retrieval). This prevents the retrieval component from mixing information across similar but distinct releases.
- Core assumption: Users frequently query properties that vary across releases, and explicit release disambiguation is necessary for correct retrieval.
- Evidence anchors:
  - [abstract] "QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases."
  - [Section III-A] "Given a set of multi-release documents, QAMR creates a separate corpus for each release, instead of combining all releases into a single corpus."
- Break condition: If user queries are predominantly release-agnostic or documentation has minimal inter-release overlap, the overhead of separate corpora yields diminishing returns.

### Mechanism 2
- Claim: Decoupling chunk sizes for retrieval (small) and answer generation (large) improves both retrieval precision and answer quality compared to single-chunk RAG.
- Mechanism: Dual-chunking (Algorithm 1) divides each page into k smaller "search chunks" for vector similarity matching, while maintaining full-page "context chunks" with padding from adjacent pages for generation. When a search chunk matches, its corresponding context chunk is retrieved.
- Core assumption: Retrieval accuracy degrades with large chunks (more noise), while answer generation quality degrades with small chunks (insufficient context).
- Evidence anchors:
  - [abstract] "QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy."
  - [Section III-A, Step 2] "For search, smaller chunks tend to increase retrieval accuracy. And, for answer generation, opting for the largest possible chunks... is not necessarily the best option due to the 'needle in the haystack' problem."
- Break condition: If documents are already short or uniformly structured, dual-chunking adds complexity without measurable gain.

### Mechanism 3
- Claim: Context reduction followed by LLM-based ranking (RankRAG-inspired) reduces hallucination and improves answer correctness by filtering irrelevant information before generation.
- Mechanism: After retrieval, QAMR applies few-shot chain-of-thought prompts to extract query-relevant information from each context chunk ("context reduction"). A second LLM call ranks reduced chunks by usefulness ("context selection"), selecting only top-ranked chunks for answer generation.
- Core assumption: LLMs can reliably identify relevant vs. irrelevant content within chunks when explicitly prompted with reasoning steps.
- Evidence anchors:
  - [abstract] "It also employs RankRAG to select the most relevant contexts."
  - [Section III-B, Step 3] "QAMR uses few-shot, chain-of-thought (CoT) prompts to extract only the information relevant to the user query from each retrieved context chunk."
  - [Section III-B, Step 4] "The prompt we use here is inspired by the one proposed by Yu et al. [12]."
- Break condition: If retrieved chunks contain dense, interdependent information where CoT extraction loses critical context, reduction may prune essential details.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) fundamentals**
  - Why needed here: QAMR extends standard RAG; understanding baseline architecture is prerequisite.
  - Quick check question: Can you explain why RAG reduces hallucination compared to pure LLM generation?

- Concept: **Vector embeddings and semantic similarity search**
  - Why needed here: QAMR uses embedding functions (BAAI/bge-base-en-v1.5) for chunk retrieval.
  - Quick check question: What does cosine similarity measure in the context of text chunk retrieval?

- Concept: **Chunking tradeoffs in document QA systems**
  - Why needed here: Dual-chunking is QAMR's core novelty; understanding the precision vs. context tension is essential.
  - Quick check question: Why does a smaller chunk size typically improve retrieval precision but hurt answer generation?

## Architecture Onboarding

- Component map: Preprocessing → Corpus Creation (per release) → Dual-Chunking (Algorithm 1) → Vectorization → Query → Query Rewriting (3 variants) → Retrieval (corpus selection + multi-query union) → Context Reduction (CoT) → Context Selection (RankRAG-inspired ranking) → Answer Generation

- Critical path: Query Rewriting → Retrieval → Context Reduction → Context Selection → Answer Generation. Errors in retrieval propagate; context reduction cannot recover from missing chunks.

- Design tradeoffs:
  - `k` (search chunks per page): Higher k = finer retrieval granularity but more embedding operations. Paper uses k=2.
  - `ps` (padding size): Larger padding = more context continuity but larger chunks. Paper uses 500 characters.
  - Retrieval budget: 4 chunks per standalone query, top 3 used for generation.

- Failure signatures:
  - **Incorrect corpus selection**: Query lacks release number → defaults to latest corpus; may retrieve stale info.
  - **Over-pruning in context reduction**: Empty reduced chunks indicate retrieval failure or aggressive filtering.
  - **Hallucination despite retrieval**: Check if "I don't know" guardrail prompt is correctly applied when context is insufficient.

- First 3 experiments:
  1. **Ablation by component**: Run QAMR with each mechanism disabled (no query rewriting, single-chunk, no context reduction) to isolate individual contributions on your domain data.
  2. **Chunk size sensitivity**: Vary k ∈ {1, 2, 4} and ps ∈ {250, 500, 750} characters; measure retrieval accuracy vs. response time tradeoff.
  3. **Corpus boundary test**: Construct synthetic multi-release documents with controlled overlap (e.g., 20%, 50%, 80%) to quantify when separate corpora cease to provide advantage over unified indexing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does QAMR's dual-chunking strategy improve chatbot accuracy in software engineering contexts that lack multi-release characteristics?
- Basis in paper: [explicit] Page 2 states that the dual-chunking method "has the potential to increase chatbot accuracy in broader contexts, meriting further study beyond multi-release systems."
- Why unresolved: The evaluation combined multi-release specific mechanisms (corpus separation) with dual-chunking, making it difficult to isolate the benefits of dual-chunking alone in general, single-release domains.
- What evidence would resolve it: An evaluation of a QAMR variant applying only the dual-chunking strategy to standard, single-version software engineering benchmarks.

### Open Question 2
- Question: To what extent does integrating a domain-specific glossary into the query rewriting process mitigate retrieval errors caused by vocabulary mismatches?
- Basis in paper: [inferred] Page 10 (Implication of Lesson 2) suggests applying "query rewriting based on a domain-specific glossary," noting that retrieval currently fails when users employ synonyms or acronyms not present in the literal document text.
- Why unresolved: The current implementation relies on user terminology matching the corpus; the authors identify vocabulary mismatch as a "prominent source of error" but do not test the proposed glossary solution.
- What evidence would resolve it: A comparative analysis of answer correctness between the current query rewriter and a glossary-augmented rewriter handling domain-specific synonyms and acronyms.

### Open Question 3
- Question: How does QAMR's performance vary when implemented with alternative Large Language Models (e.g., GPT-4) instead of Llama 3-70B?
- Basis in paper: [inferred] Page 9 acknowledges that "benchmarking other LLMs is valuable," though the study exclusively used Llama 3-70B due to industry partner policies and computational constraints.
- Why unresolved: QAMR relies heavily on prompt engineering (CoT, RankRAG) which may behave differently across model architectures, and the current results are tied to a single model's instruction-following capabilities.
- What evidence would resolve it: Replication of the QAMR experiments using proprietary and open-source model alternatives to measure consistency in accuracy and latency.

### Open Question 4
- Question: How sensitive is QAMR's accuracy to the specific configuration of the dual-chunking parameters, specifically the number of search chunks ($k$) and the padding size ($ps$)?
- Basis in paper: [inferred] Page 7 sets fixed parameters ($k=2$, $ps=500$) based on balancing trade-offs, but the paper does not analyze performance deviations if these values are altered for different document structures.
- Why unresolved: The optimal chunk granularity likely depends on the density of information (e.g., tables vs. narrative text), and the fixed configuration may not generalize to all document types.
- What evidence would resolve it: A parameter sweep analysis measuring retrieval accuracy and answer correctness across a range of $k$ and $ps$ values on datasets with varying document structures.

## Limitations

- Evaluation relies heavily on LLM-as-a-judge rather than human evaluation, which may not capture nuanced correctness in technical software documentation
- Primary Ciena dataset used for evaluation is proprietary and cannot be independently verified, limiting reproducibility
- External validation using REQuestA benchmark is limited to only 6 documents and 159 Q&A pairs, which is relatively small for assessing multi-release system performance

## Confidence

- **High confidence**: Dual-chunking strategy improves both retrieval precision and answer generation quality (supported by clear mechanism and ablation results showing 16.5% improvement over baseline)
- **Medium confidence**: Query rewriting effectively handles multi-release contexts (mechanism is sound but limited to 6 REQuestA documents for external validation)
- **Medium confidence**: Context reduction and RankRAG-inspired ranking reduce hallucination (mechanism is well-specified but lacks direct external validation beyond the cited RankRAG paper)

## Next Checks

1. **Cross-release consistency test**: Create synthetic multi-release documents with controlled overlap percentages (0%, 25%, 50%, 75%) and measure retrieval accuracy and answer correctness to quantify the exact threshold where separate corpora outperform unified indexing.

2. **Chunking sensitivity analysis**: Systematically vary k (search chunks per page) from 1 to 8 and ps (padding size) from 250 to 1000 characters, measuring the precision-recall tradeoff and response time to identify optimal parameters for different document lengths and structures.

3. **Query rewriting robustness test**: Evaluate QAMR's query rewriting mechanism on queries that deliberately omit release numbers, use ambiguous version formats (e.g., "latest", "R17.2 vs R17.20"), or reference features that exist across multiple releases to assess fallback retrieval effectiveness.