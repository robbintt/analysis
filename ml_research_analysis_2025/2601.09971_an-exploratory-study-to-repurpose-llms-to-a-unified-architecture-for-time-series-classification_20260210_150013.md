---
ver: rpa2
title: An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series
  Classification
arxiv_id: '2601.09971'
source_url: https://arxiv.org/abs/2601.09971
tags:
- time
- series
- encoder
- llms
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLMs can be effectively repurposed
  for time series classification by stacking them with specialized time series encoders.
  While prior work has focused on aligning time series data to the textual domain,
  this study systematically explores the role of encoder architecture in hybrid LLM-based
  models.
---

# An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification

## Quick Facts
- arXiv ID: 2601.09971
- Source URL: https://arxiv.org/abs/2601.09971
- Reference count: 7
- This paper finds that stacking specialized time series encoders with frozen LLMs generally degrades classification performance, except for Inception encoders which improve and achieve the highest standalone accuracy.

## Executive Summary
This study systematically explores whether large language models can be repurposed for time series classification by combining them with specialized time series encoders. Using the 2015 UCR Time Series Archive, the authors evaluate five encoder families (MLP, CNN, Inception, ResNet, Transformer) both standalone and when stacked with a frozen Llama-3.1-8B LLM backbone. The surprising finding is that while LLM integration typically degrades performance, Inception-based encoders uniquely benefit from LLM addition while also achieving the highest standalone accuracy. The authors hypothesize this advantage stems from Inception's multi-scale convolutional kernels capturing temporal patterns at multiple resolutions, producing representations more compatible with LLM semantic reasoning.

## Method Summary
The paper evaluates hybrid architectures combining time series encoders with frozen LLMs for classification. Five encoder families (MLP, CNN, Inception, ResNet, Transformer) are tested both standalone and when paired with frozen Llama-3.1-8B. Encoders map continuous time series to latent vectors matching LLM hidden dimensions, then concatenate with prompts for LLM processing. Training uses ADAM optimizer (100 epochs, lr=0.001, batch size=32) with grid search over learning rate, kernel count, and kernel size. Evaluation uses the 2015 UCR Time Series Archive across diverse domains (sensor, motion, physiological data) with classification accuracy as the primary metric.

## Key Results
- LLM integration generally degrades classification performance across all encoder families except Inception
- Standalone Inception encoder achieves the highest classification accuracy (0.6568) in grid search
- Transformer encoder shows poor standalone performance (42.63%), suggesting data scarcity issues
- Learning rate critically impacts performance: 10^-3 achieves ~65% accuracy while 10^-5 drops to ~49%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-scale convolutional kernels improve LLM compatibility for time series classification
- **Mechanism:** The Inception architecture employs parallel convolutional kernels with varying receptive fields (kernel sizes 8 and 16 tested), capturing temporal patterns at multiple resolutions simultaneously. This produces latent representations that align better with LLM semantic reasoning capabilities.
- **Core assumption:** Time series patterns exist at multiple temporal scales, and encoders that capture this multi-scale structure produce representations more amenable to LLM processing.
- **Evidence anchors:** [abstract] "Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone"; [section 4.2] "We hypothesize that this effect arises from the use of multi-scale convolutional kernels in the Inception architecture, which provide receptive fields at multiple resolutions"
- **Break condition:** If time series datasets primarily contain single-scale patterns, or if the LLM backbone lacks sufficient reasoning capacity, multi-scale encoding gains may diminish.

### Mechanism 2
- **Claim:** Latent encoding bypasses tokenization-induced information loss
- **Mechanism:** Rather than converting numerical values to text tokens (which introduces quantization effects), dedicated encoders map continuous time series directly to latent vectors matching LLM hidden dimensions. This preserves fine-grained temporal structure.
- **Core assumption:** Continuous numerical signals contain information that textual tokenization obscures or destroys.
- **Evidence anchors:** [section 1] "LLMs are fundamentally designed to operate on discrete text tokens rather than continuous numerical signals. This mismatch introduces quantization effects during tokenization, leading to precision loss"; [section 3.2] "Encoding continuous time series into learned latent representations prior to LLM processing preserves fine-grained temporal structure without relying on textual tokenization"
- **Break condition:** If the encoder lacks capacity to extract meaningful features, or if the compression ratio (T timesteps → L tokens) is too aggressive, information loss simply shifts from tokenization to encoding.

### Mechanism 3
- **Claim:** Encoder lightweight-ness matters for small-scale time series datasets
- **Mechanism:** The Inception encoder's relative architectural efficiency prevents overfitting on limited data while still capturing multi-scale features. Heavier architectures may extract features incompatible with frozen LLM representations.
- **Core assumption:** Small-scale datasets (typical in TSC) require encoders that don't overfit and produce generalizable representations.
- **Evidence anchors:** [section 1] "while remaining sufficiently lightweight for small-scale time series datasets"; [section 4.3] Hyperparameter grid search shows learning rate 10^-3 outperforms 10^-5 substantially (0.6568 vs 0.4866), suggesting architectural capacity and optimization interact significantly
- **Break condition:** On larger datasets, heavier encoders (ResNet, Transformer) may match or exceed Inception performance when combined with LLMs.

## Foundational Learning

- **Concept: Multi-scale convolution (Inception modules)**
  - **Why needed here:** Understanding why parallel kernels of different sizes capture complementary temporal patterns explains the performance differential across encoder families.
  - **Quick check question:** Can you explain why a kernel of size 8 and a kernel of size 16 detect different types of patterns in the same time series?

- **Concept: Frozen backbone fine-tuning**
  - **Why needed here:** The experimental setup freezes LLM parameters while training the encoder end-to-end. This transfer learning paradigm differs from full fine-tuning and affects what representations are learned.
  - **Quick check question:** What gradients flow through the system during training, and what parameters remain static?

- **Concept: Representation alignment across modalities**
  - **Why needed here:** The encoder output (dimension h) must match LLM hidden dimensions for concatenation. Misalignment in representation space may explain why most encoders degrade with LLM addition.
  - **Quick check question:** If encoder outputs occupy a different region of the embedding space than the LLM expects, what would happen to classification performance?

## Architecture Onboarding

- **Component map:** Input Time Series [T×d] → Encoder E_φ (MLP/CNN/Inception/ResNet/Transformer) → Latent Tokens Z [L×h] → Concatenate with prompt/padding embeddings → Frozen LLM (Llama-3.1-8B) → Classification Head → ŷ

- **Critical path:** Encoder architecture selection → latent dimension matching (h=LLM hidden size) → learning rate tuning (10^-3 optimal in grid search) → kernel count and size selection (more kernels, larger sizes trend positive)

- **Design tradeoffs:**
  - Standalone encoder vs. hybrid: Most encoders perform worse with LLM (MLP -5.67%, CNN -6.34%), only Inception improves (+3.06%)
  - Computational cost: Frozen LLM adds inference overhead without trainable parameters
  - Grid search shows learning rate sensitivity: 10^-3 achieves ~65% accuracy, 10^-5 drops to ~49%

- **Failure signatures:**
  - Performance degradation when adding LLM suggests encoder-LLM representation mismatch
  - Transformer encoder shows poor standalone performance (42.63%), indicating potential data scarcity issues
  - Accuracy drops with smaller kernel counts/sizes, suggesting insufficient multi-scale coverage

- **First 3 experiments:**
  1. Replicate Inception standalone vs. Inception+LLM on 5 UCR datasets to validate multi-scale hypothesis
  2. Ablate kernel sizes (test single-scale vs. multi-scale configurations) to isolate the multi-scale mechanism
  3. Probe representation alignment: visualize encoder output distributions with/without LLM to diagnose why other encoders degrade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does LLM integration improve Inception-based encoders while degrading performance for all other encoder architectures tested?
- Basis in paper: [explicit] "We hypothesize that this advantage stems from the Inception architecture's ability to capture multi-scale temporal patterns through parallel convolutional kernels of varying receptive fields, while remaining sufficiently lightweight for small-scale time series datasets."
- Why unresolved: This is stated as a hypothesis without experimental validation (e.g., no ablation on kernel scales or analysis of representation alignment).
- What evidence would resolve it: Ablation studies varying receptive field scales independently, analysis of latent representation compatibility with LLM embedding space, or probing experiments on feature alignment.

### Open Question 2
- Question: Do these findings generalize across different LLM backbones or with fine-tuning enabled?
- Basis in paper: [inferred] The study uses only frozen Llama-3.1-8B; compatibility may vary across model families, scales, or training configurations.
- Why unresolved: Results could be specific to this single frozen LLM configuration.
- What evidence would resolve it: Systematic experiments with multiple LLM backbones (encoder-only vs. decoder-only, varying scales) and frozen vs. fine-tuned comparisons.

### Open Question 3
- Question: What representational properties make Inception encoder outputs more compatible with frozen LLM processing?
- Basis in paper: [explicit] Inception is "the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone," but no analysis of latent representations is provided.
- Why unresolved: The paper reports accuracy differences without analyzing what features enable LLM compatibility.
- What evidence would resolve it: Representation analysis (distribution statistics, mutual information, probing classifiers) comparing encoder outputs across architectures before and after LLM processing.

### Open Question 4
- Question: Would results generalize to more recent benchmarks or multivariate time series beyond the 2015 UCR archive?
- Basis in paper: [inferred] Evaluation is limited to one benchmark; generalization to newer datasets or multivariate settings is unexplored.
- Why unresolved: The 2015 UCR archive has known limitations; results may not extend to modern datasets with different characteristics.
- What evidence would resolve it: Evaluation on UEA multivariate archive, Monash archive, or domain-specific benchmarks (medical, financial).

## Limitations

- Architectural specification gaps: The paper specifies encoder families but omits critical implementation details including exact layer counts, hidden dimensions, activation functions, and Inception module configurations.
- Limited dataset analysis: Only aggregated averages are reported without per-dataset breakdowns, obscuring whether Inception's advantage is consistent across domains.
- Representation alignment hypothesis: The claim about multi-scale kernels producing better-aligned representations lacks direct empirical validation through visualization or analysis.

## Confidence

**High Confidence:** The experimental finding that standalone Inception encoders outperform other encoder families in time series classification is well-supported by the results table.

**Medium Confidence:** The observation that adding a frozen LLM typically degrades performance (except for Inception) is well-documented, but the explanation for why Inception uniquely benefits from LLM integration remains hypothetical.

**Low Confidence:** The specific mechanism by which multi-scale kernels improve LLM compatibility is not empirically validated. The paper provides a theoretical rationale but no ablation studies or direct evidence.

## Next Checks

1. Generate a detailed breakdown of classification accuracy for each UCR dataset, separating standalone vs. hybrid performance across all encoder types to reveal domain-specific patterns.

2. Create controlled experiments varying kernel sizes systematically—testing single-scale (only K=8, only K=16) versus multi-scale configurations—to isolate whether multi-scale representation is the causal factor.

3. Visualize and analyze the distribution of encoder outputs with and without LLM integration using t-SNE or UMAP to empirically test whether representation mismatch explains performance degradation in non-Inception encoders.