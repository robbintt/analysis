---
ver: rpa2
title: 'End-to-end Automatic Speech Recognition and Speech Translation: Integration
  of Speech Foundational Models and LLMs'
arxiv_id: '2510.10329'
source_url: https://arxiv.org/abs/2510.10329
tags:
- speech
- translation
- whisper
- language
- hubert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end architecture that combines pre-trained
  speech encoders with Large Language Models (LLMs) to perform Automatic Speech Recognition
  (ASR) and Speech Translation (ST) simultaneously. The method uses a frozen speech
  encoder to extract audio features, a projection layer to adapt features to LLM embedding
  space, and fine-tuned LLMs to generate both transcription and translation.
---

# End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs

## Quick Facts
- **arXiv ID:** 2510.10329
- **Source URL:** https://arxiv.org/abs/2510.10329
- **Reference count:** 8
- **Primary result:** End-to-end architecture combining frozen speech encoders with fine-tuned LLMs achieves up to 8% COMET-DA22 improvement over SeamlessM4T on English-to-German speech translation

## Executive Summary
This paper proposes an end-to-end architecture that combines pre-trained speech encoders with Large Language Models (LLMs) to perform Automatic Speech Recognition (ASR) and Speech Translation (ST) simultaneously. The method uses a frozen speech encoder to extract audio features, a projection layer to adapt features to LLM embedding space, and fine-tuned LLMs to generate both transcription and translation. Experiments on English-to-German translation show that the proposed models outperform the end-to-end SeamlessM4T model and match the performance of a cascaded system using Whisper and NLLB, achieving up to 8% improvement in COMET-DA22 metric. The architecture demonstrates that LLMs can effectively integrate speech and language tasks while maintaining ASR capability.

## Method Summary
The architecture uses a frozen pre-trained speech encoder (HuBERT or Whisper) to extract acoustic features, which are compressed using CTC-based collapsing or convolutional downsampling, then projected to the LLM's embedding space via a single feed-forward layer. The LLM (Gemma 7B/2 9B, Llama 2 7B, or Mistral 7B v0.1) is fine-tuned with QLoRA adapters to generate both transcription and translation simultaneously, trained with cross-entropy loss on tokens after a special "<transcript>" separator. Training uses 500K steps for HuBERT and 100K steps for Whisper with batch size 1 or 2, AdamW optimizer, and cosine scheduler. The model processes audio features as a special prompt token sequence, generating aligned transcript and translation outputs.

## Key Results
- Proposed models outperform end-to-end SeamlessM4T on English-to-German translation
- Achieve up to 8% improvement in COMET-DA22 metric compared to baseline
- Match performance of cascaded system using Whisper encoder + NLLB model
- Successfully maintain ASR capability while performing simultaneous ST

## Why This Works (Mechanism)

### Mechanism 1: Modality Alignment via Projection
Mapping pre-trained speech encoder outputs into the LLM's embedding space allows the LLM to treat audio features as a structured "prompt" for generative ASR and ST. A frozen speech encoder produces high-quality acoustic representations that a single feed-forward projection layer converts to the LLM's hidden dimension, enabling next-token-prediction training where the LLM generates transcription and translation conditioned on projected speech embeddings.

### Mechanism 2: Sequence Compression for Context Alignment
Compressing speech feature sequences mitigates length mismatches with the LLM's context window while preserving task-critical information. For HuBERT, CTC-based collapsing averages features with repeated labels; for Whisper encoder, a 5×5 strided convolution downsamples the sequence. This shortens inputs before projection, reducing memory and compute while maintaining acoustic information distribution.

### Mechanism 3: Frozen Encoder with Efficient LLM Adaptation
Keeping the speech encoder frozen and fine-tuning only the projection and LLM (via QLoRA) preserves strong acoustic features while efficiently adapting the system for generation. The encoder remains fixed, avoiding catastrophic forgetting, while the LLM is adapted using low-rank adapters with cross-entropy loss on transcript and translation tokens. Only the projection and adapters update during training.

## Foundational Learning

- **Concept:** Self-supervised speech representations (e.g., HuBERT, Whisper encoder)
  - **Why needed here:** Provides robust, pre-learned acoustic features that the projection layer maps into the LLM's space
  - **Quick check question:** Can you explain how HuBERT and Whisper encoder outputs differ in dimensionality and temporal granularity?

- **Concept:** Parameter-efficient fine-tuning (QLoRA)
  - **Why needed here:** Enables training large LLMs on modest compute while preserving generalization
  - **Quick check question:** What are the practical implications of setting LoRA rank (r=8) and alpha (α=8) for gradient updates?

- **Concept:** Encoder–decoder/decoder-only modeling and token-level supervision
  - **Why needed here:** The LLM operates autoregressively; training uses cross-entropy on transcript/translation tokens with special separator tokens
  - **Quick check question:** Why is cross-entropy computed only after the "<transcript>" separator?

## Architecture Onboarding

- **Component map:** Frozen Speech Encoder -> Length Adapter -> Projection Layer -> LLM Decoder
- **Critical path:** Data → frozen encoder → length adapter → projection → LLM → next-token loss on transcript/translation tokens
- **Design tradeoffs:**
  - HuBERT vs. Whisper encoder: HuBERT+CTC compression may better preserve temporal structure for translation; Whisper encoder+conv is faster but may lose fine detail
  - LLM scale: Larger models (Gemma 2 9B) improve translation but increase latency
  - Adapter trainability: Trainable adapters can improve alignment but risk overfitting
- **Failure signatures:**
  - Hallucinated outputs: LLM generates text unrelated to audio → likely projection misalignment or insufficient supervision masking
  - High WER with good BLEU/COMET: Encoder robust but translation over-prioritized → rebalance loss weighting or data
  - OOM errors: Long audio sequences not sufficiently compressed → increase downsampling stride or implement chunking
- **First 3 experiments:**
  1. Projection capacity ablation: Compare 1-layer vs. 2-layer projection; evaluate WER and BLEU/COMET
  2. Length adapter comparison: Benchmark CTC collapse vs. 5×5 conv for HuBERT encoder on MuST-C v2/v3; measure speed and quality
  3. LLM scale vs. latency: Evaluate Gemma 7B vs. Gemma 2 9B for COMET-DA22 and inference time; establish Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed architecture maintain its performance advantage when expanded to diverse language pairs and translation directions?
- **Basis in paper:** [explicit] The authors state in Section 4 that they "could only conduct experiments for the English-to-German direction" and plan to "expand our experiments to more language pairs and directions."
- **Why unresolved:** The current superior performance over SeamlessM4T is established only for a single high-resource language pair (English-to-German).
- **What evidence would resolve it:** Reporting BLEU and COMET scores for the architecture when fine-tuned on low-resource languages or different source languages.

### Open Question 2
- **Question:** Can the system be miniaturized for mobile deployment without sacrificing translation robustness?
- **Basis in paper:** [explicit] Section 6 explicitly asks, "Can end-to-end speech translation systems be smaller in size, while still keeping the robustness in translation...?" and Section 4 proposes experimenting with smaller LLM variants via knowledge distillation.
- **Why unresolved:** LLM-based models suffer from slow inference speeds compared to cascaded baselines, creating a trade-off between the proposed end-to-end quality and practical efficiency.
- **What evidence would resolve it:** Successful knowledge distillation into a smaller model that retains comparable COMET scores on the MuST-C and IWSLT test sets.

### Open Question 3
- **Question:** Would replacing the CTC collapsing procedure with a Q-Former or convolution-based adapter improve feature alignment for HuBERT-based models?
- **Basis in paper:** [explicit] Section 4 lists future plans to "Try replacing the CTC collapsing procedure with a length adapter of convolution layers" and to "Try other modal adapter methods, like Q-Former."
- **Why unresolved:** The paper used specific adapters for specific encoders (CTC for HuBERT, Conv for Whisper) but did not test if alternative adapters could bridge the performance gap between encoder types.
- **What evidence would resolve it:** Ablation studies showing translation accuracy and training convergence when applying Q-Former adapters to the HuBERT encoder.

## Limitations
- Performance advantages only demonstrated for English-to-German translation direction
- No testing on noisy or domain-specific speech data to validate frozen encoder assumption
- Hardware requirements and inference latency not reported, limiting deployment assessment

## Confidence

**High Confidence** in experimental methodology and evaluation setup (MuST-C datasets, standard metrics, clear implementation details). The reported performance improvements over SeamlessM4T and competitive results against cascaded systems appear well-supported by the data presented.

**Medium Confidence** in core architectural claims. While the overall framework is sound and performance gains are convincing, exact mechanisms for projection layer alignment are not fully empirically validated. Superiority of CTC-based compression over convolutional downsampling is asserted based on external citations rather than direct ablation studies.

**Low Confidence** in scalability and generalization claims. Paper does not test architecture on noisy or domain-specific data, nor report on inference latency or memory consumption for longer audio sequences. Frozen encoder assumption's limitations in domain adaptation scenarios remain untested.

## Next Checks

1. **Projection Layer Capacity Ablation Study:** Implement and compare 1-layer vs. 2-layer projection architectures with HuBERT encoder. Measure WER, BLEU, and COMET-DA22 metrics across all MuST-C test sets to validate whether "simple feed-forward layer" assumption is optimal.

2. **Length Adapter Comparative Benchmark:** For HuBERT encoder specifically, implement both CTC-based compression and 5×5 convolutional downsampling. Evaluate on MuST-C v2 and v3 test sets with identical LLM configurations, measuring quality metrics, sequence lengths, GPU memory usage, and inference speed.

3. **Domain Adaptation Failure Analysis:** Test frozen encoder approach on non-MUSt-C datasets: (a) noisy speech from LibriSpeech test-other, and (b) domain-shifted medical or technical speech if available. Compare frozen vs. fine-tuned encoder performance to quantify domain adaptation penalty and identify break conditions.