---
ver: rpa2
title: Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems
arxiv_id: '2512.23132'
source_url: https://arxiv.org/abs/2512.23132
tags:
- attack
- data
- vulnerabilities
- attacks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the growing security risks in machine learning
  (ML) systems, which are increasingly targeted by adversarial attacks like data poisoning,
  model extraction, and jailbreaking. The authors propose a multi-agent framework
  that integrates retrieval-augmented generation (RAG) with graph neural networks
  (GNNs) to map threat tactics, techniques, and procedures (TTPs) to ML lifecycle
  stages and vulnerabilities.
---

# Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems

## Quick Facts
- arXiv ID: 2512.23132
- Source URL: https://arxiv.org/abs/2512.23132
- Reference count: 40
- Primary result: A multi-agent framework integrates RAG with GNNs to map ML threats to lifecycle stages, achieving 63% Spearman correlation with real-world severity scores.

## Executive Summary
This paper presents a multi-agent framework that addresses the growing security risks in machine learning systems through automated threat mapping and severity prediction. The system uses a coordinated RAG pipeline with ChatGPT-4o to extract Tactics, Techniques, and Procedures (TTPs) from unstructured threat literature, then constructs a heterogeneous graph neural network to predict vulnerability severity and identify supply chain risks. The framework processes 93 threats from MITRE ATLAS and the AI Incident Database, discovering previously undocumented risks including model-stealing attacks against LLM APIs and preference-guided jailbreaks. The GNN model achieves 63% Spearman correlation with real-world severity metrics and demonstrates 24% improvement in operational responsiveness.

## Method Summary
The framework combines a five-agent RAG pipeline with a heterogeneous GNN to analyze ML security threats. The multi-agent system uses ChatGPT-4o (temperature 0.4) to automatically extract TTPs, vulnerabilities, and ML lifecycle stages from over 300 scientific articles and threat databases. Extracted data is structured into a heterogeneous graph with 57,812 nodes and 218,906 edges connecting CVEs, CPEs, GitHub issues, attack techniques, and cluster centroids. A GraphSAGE-based GNN with R-GCN-style relational convolutions predicts vulnerability severity scores using mean-squared error training on labeled data. The system also performs dependency analysis to identify ML libraries with dense vulnerability clusters and poor patch propagation.

## Key Results
- GNN model achieves 63% Spearman correlation with real-world severity scores from CVSS and incident costs
- Operational responsiveness improves by 24% in controlled study scenarios
- Graph analysis reveals dense vulnerability clusters in ML libraries with poor patch propagation
- 93 threats analyzed, including 26 from MITRE ATLAS and 12 from AI Incident Database
- CVE classification accuracy improves from 0.71 to 0.87 macro-F1 with Chain-of-Thought reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent RAG pipeline can automatically extract TTPs, vulnerabilities, and ML lifecycle stages from unstructured threat literature with sufficient accuracy for ontology construction.
- Mechanism: Five coordinated agents (query refiner, retriever, extractor, ontology aligner, graph builder) process scientific articles and incident reports using ChatGPT-4o with temperature 0.4 and reranking. SHAP/LIME interpretability justifies CIA label assignments.
- Core assumption: LLM extraction with temperature 0.4 and evidence-grounded reasoning produces structured threat mappings with acceptably low hallucination rates.
- Evidence anchors: "A multi-agent reasoning system with enhanced RAG—powered by ChatGPT-4o (temperature 0.4)—automatically extracts TTPs, vulnerabilities, and lifecycle stages from over 300 scientific articles using evidence-grounded reasoning."
- Break condition: If LLM extraction produces inconsistent or unverifiable TTP mappings, the ontology and downstream GNN reasoning may propagate errors.

### Mechanism 2
- Claim: Heterogeneous GNN can predict vulnerability risk scores correlating with real-world severity metrics by fusing CVE metadata, attack success rates, stealth characteristics, and dependency structures.
- Mechanism: GNN (R-GCN with GraphSAGE-style neighbor sampling) operates on heterogeneous graph with CVE, CPE, GitHub Issue, Attack Technique, and cluster centroid nodes. Node features include BERT embeddings of CVE descriptions and CVSS vectors.
- Core assumption: Heterogeneous graph structure captures sufficient relational signal for GNN to learn severity patterns that generalize beyond isolated CVE features.
- Evidence anchors: "The GNN model achieves a 63% Spearman correlation with real-world severity scores and improves operational responsiveness by 24% in a controlled study."
- Break condition: If critical edge types are missing or graph has insufficient labeled anchors for training, GNN may fail to capture cross-domain risk propagation.

### Mechanism 3
- Claim: Graph-based dependency analysis can identify ML libraries with dense vulnerability clusters and poor patch propagation, revealing supply chain risks not apparent from isolated CVE counts.
- Mechanism: Agents mine CVE–CPE–Tool relationships from GitHub/PyPI repositories and construct dependency graphs. Community detection and node centrality metrics identify influential dependencies.
- Core assumption: Vulnerability propagation in ML ecosystem follows detectable graph patterns that correlate with real-world exploit risk.
- Evidence anchors: "Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation."
- Break condition: If dependency graphs are incomplete or vulnerability data is outdated, cluster analysis may miss emerging supply chain risks.

## Foundational Learning

- Concept: MITRE ATLAS/ATT&CK TTPs (Tactics, Techniques, Procedures)
  - Why needed here: The entire threat modeling framework is built on mapping ML-specific attack patterns to TTPs from ATLAS and traditional TTPs from ATT&CK.
  - Quick check question: Can you name two ATLAS tactics and explain how they differ from ATT&CK tactics in scope (ML-specific vs. general cybersecurity)?

- Concept: ML Lifecycle Stages (data collection → pre-training → fine-tuning → RLHF → deployment/inference)
  - Why needed here: The paper maps TTPs and vulnerabilities to specific lifecycle stages to identify phase-specific risks. The GNN and mitigation strategies are lifecycle-aware.
  - Quick check question: Which lifecycle stages does the paper identify as most targeted by threat TTPs, and why might inference be particularly vulnerable?

- Concept: Heterogeneous Graph Neural Networks (HGNNs) with relational convolution
  - Why needed here: The core predictive model is an HGNN that processes a graph with multiple node types (CVE, CPE, Issue, Technique). Understanding message passing and type-specific aggregation is essential to interpret severity predictions.
  - Quick check question: In an R-GCN layer, how are relations handled differently compared to a standard GCN, and why is this important for vulnerability graphs?

## Architecture Onboarding

- Component map: Data Ingestion Layer -> Multi-Agent RAG Pipeline -> Threat Graph -> GNN Reasoner -> Monitoring & Alerting -> Mitigation Matrix

- Critical path:
  1. Data collection (mining threats, vulnerabilities, repositories)
  2. Multi-agent extraction and ontology linking (RAG pipeline)
  3. Heterogeneous graph construction
  4. GNN training on labeled severity data
  5. Deployment for real-time severity prediction and alerting
  6. Integration with mitigation matrix for actionable guidance

- Design tradeoffs:
  - LLM extraction accuracy vs. latency: Temperature 0.4 balances creativity and determinism
  - Graph heterogeneity vs. complexity: More node/edge types capture richer context but increase training cost
  - Severity threshold tuning: High threshold (>0.8) reduces false positives but may miss emerging threats

- Failure signatures:
  - High false-positive rate: GNN over-predicts severity on benign CVEs; check label quality and graph connectivity
  - Stale threat graph: If CVE ingestion lags, predictions miss new vulnerabilities; verify NVD/API update frequency
  - LLM extraction inconsistencies: Conflicting TTP labels for same threat; review reranking thresholds and evidence grounding

- First 3 experiments:
  1. Validate RAG extraction: Run multi-agent pipeline on held-out 50 articles, manually verify TTP/lifecycle mapping accuracy, measure inter-annotator agreement
  2. Ablate GNN edge types: Retrain GNN with each edge family removed to quantify impact on Spearman correlation; identify most critical relations
  3. Operational replay test: Using 87 AI Incident Database cases with cost annotations, compare GNN-predicted severity to actual incident costs; assess if model ranks high-cost incidents correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Reinforcement Learning (RL) be utilized to develop self-improving security mechanisms that dynamically optimize ML defenses against evolving threats?
- Basis in paper: Section VII identifies "Reinforcement Learning (RL)-based self-improving security mechanisms" as key avenue for future research to enable dynamic defense optimization.
- Why unresolved: Current study relies on static multi-agent RAG and GNN framework rather than dynamic, adaptive defense agent.
- What evidence would resolve it: Prototype system demonstrating RL agents successfully adapting defense parameters in real-time to neutralize novel, evolving attack vectors.

### Open Question 2
- Question: What is the empirical accuracy of the proposed threat assessment pipeline when validated retrospectively against real-world post-mortem incident labels?
- Basis in paper: Section VII.A invites "retrospective incident analysis" comparing pipeline predictions to 112 cases in AI Incident Database to quantify real-world accuracy.
- Why unresolved: Current validation relies on controlled SOC study and synthetic scenarios, lacking validation against historical, verified failure post-mortems.
- What evidence would resolve it: Study calculating statistical agreement (e.g., Cohen's κ) between pipeline's severity scores and ground-truth labels derived from historical incident post-mortems.

### Open Question 3
- Question: How does the proposed framework perform when subjected to large-scale red-team campaign against production-grade MLOps stack?
- Basis in paper: Section VII.A calls for "large-scale red-team campaign against production-grade MLOps stack" to expose failure modes synthetic benchmarks cannot capture.
- Why unresolved: Paper's evaluation focuses on repository mining and controlled operational study, rather than active adversarial testing in live production environment.
- What evidence would resolve it: Detailed report on framework's detection rates and failure modes during live, constrained adversarial exercise on deployed system like Azure ML on Kubernetes.

## Limitations

- LLM extraction accuracy and hallucination rates are not validated with human benchmarks, making it unclear whether ontology construction is reliable
- GNN model hyperparameters and training details are underspecified, preventing exact reproduction of results
- Claims about discovering "previously undocumented" threats lack systematic comparison to existing threat intelligence feeds

## Confidence

- High confidence: Multi-agent framework design, heterogeneous graph construction methodology, CVE severity prediction correlation (0.63), and operational responsiveness improvement (24%) are directly reported and methodologically sound
- Medium confidence: Threat extraction accuracy and novelty claims depend on LLM performance, which is not validated with human benchmarks; dependency analysis may be incomplete due to API limits or missing transitive links
- Low confidence: Severity prediction generalizability to unseen threat classes is unproven; CVE–CPE–GitHub Issue linkages may miss transitive dependencies, biasing vulnerability propagation analysis

## Next Checks

1. **RAG extraction fidelity**: Run the multi-agent pipeline on a held-out set of 50 articles, manually verify TTP/lifecycle mapping accuracy, and measure inter-annotator agreement (e.g., Cohen's κ > 0.7 required)

2. **GNN edge importance**: Retrain the GNN with each edge family removed (Table VIII) to quantify the impact on Spearman correlation; identify the most critical relations and test robustness to edge sparsity

3. **Severity prediction generalization**: Using the 87 AI Incident Database cases with cost annotations, compare GNN-predicted severity to actual incident costs; assess if the model ranks high-cost incidents correctly and generalizes beyond CVSS-labeled CVEs