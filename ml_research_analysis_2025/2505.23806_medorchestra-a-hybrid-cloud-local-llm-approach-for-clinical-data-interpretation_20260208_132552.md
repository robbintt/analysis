---
ver: rpa2
title: 'MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation'
arxiv_id: '2505.23806'
source_url: https://arxiv.org/abs/2505.23806
tags:
- clinical
- local
- medorchestra
- cloud
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedOrchestra addresses the privacy-utility trade-off in deploying
  LLMs for clinical data interpretation by introducing a hybrid cloud-local framework.
  The cloud LLM decomposes complex clinical tasks into subtasks and generates prompts
  using clinical guidelines, while the local LLM executes these subtasks on sensitive
  data without external transmission.
---

# MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation

## Quick Facts
- arXiv ID: 2505.23806
- Source URL: https://arxiv.org/abs/2505.23806
- Reference count: 9
- Primary result: 70.21% accuracy on free-text pancreatic cancer staging, outperforming local LLM baselines and board-certified clinicians

## Executive Summary
MedOrchestra addresses privacy-utility trade-offs in clinical LLM deployment through a hybrid cloud-local architecture. The cloud LLM decomposes complex clinical tasks into subtasks and generates optimized prompts using clinical guidelines, while the local LLM executes these subtasks on sensitive data without external transmission. In pancreatic cancer staging from 100 radiology reports, MedOrchestra achieved 70.21% accuracy on free-text reports and 85.42% on structured reports, significantly outperforming local LLM baselines (48.94-56.59%) and board-certified clinicians (59.57-65.96%).

## Method Summary
The framework uses a cloud LLM (gemini-2.5-pro-preview-03-25, temperature 0.8) to decompose pancreatic cancer staging tasks into subtasks and generate self-contained prompts using NCCN guidelines. These prompts are validated against synthetic test cases until reaching 80% accuracy, then transferred to a local LLM (gemma3:27b-it-qat via Ollama, temperature 0.2, structured JSON output) for inference on isolated clinical data. The local LLM executes each subtask independently, and outputs are synthesized according to predefined logic. Five inference runs per case with majority voting produce final predictions.

## Key Results
- 70.21% accuracy on free-text radiology reports, significantly exceeding local LLM baselines (48.94-56.59%) and board-certified clinicians (59.57-65.96%)
- 85.42% accuracy on structured reports, demonstrating superior performance across both input formats
- Validation loop ensures prompts achieve ≥80% accuracy on synthetic test cases before clinical deployment
- Privacy-preserving architecture: PHI never leaves local environment while cloud LLM optimizes task decomposition

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition as Cognitive Offloading
The cloud LLM reduces local LLM reasoning burden by decomposing multi-step clinical staging into discrete, self-contained subtasks. Each prompt includes all necessary context for single-step execution, eliminating the need for end-to-end coordination. This works because local LLMs can reliably extract features when prompts contain complete decision criteria.

### Mechanism 2: Privacy-Preserving Data Separation
Cloud LLM operates only on guidelines and synthetic cases, never patient data. Refined prompts are manually transferred to the isolated local environment for inference. This creates a clean PHI boundary: sensitive data remains local while reasoning scaffolding enters from outside.

### Mechanism 3: Iterative Prompt Validation with Synthetic Feedback
Cloud LLM generates synthetic cases from guidelines and validates prompts locally. If accuracy <80%, the cloud analyzes failure traces and refines prompts iteratively. This loop continues until threshold is met, ensuring prompts are robust before clinical deployment.

## Foundational Learning

- **Concept: Planner-Executor Orchestration**
  - Why needed: MedOrchestra implements this pattern with cloud LLM as planner (task decomposition, prompt generation) and local LLM as executor (inference on sensitive data)
  - Quick check: If outputs are syntactically correct but clinically wrong, which layer (planner or executor) should you investigate first?

- **Concept: Prompt Engineering with Self-Contained Context**
  - Why needed: Each subtask prompt must be executable without access to original task, guideline, or other subtasks
  - Quick check: What information must appear in a subtask prompt for "detect vascular involvement" if the local LLM has no access to the NCCN guideline?

- **Concept: Majority Voting for Inference Stabilization**
  - Why needed: Local LLM outputs are stochastic; majority voting over 5 runs produces reliable final predictions
  - Quick check: If T=5 runs produce [Stage 2, Stage 2, Stage 3, Stage 2, Stage 3], what is the final prediction and what tie-breaking rule applies?

## Architecture Onboarding

- **Component map:** Task definition + guideline → Cloud LLM (gemini-2.5-pro) → Subtasks S, draft prompts P_draft, synthesis logic L, synthetic test cases (X_syn, Y_syn) → Local LLM (gemma3:27b) → Subtask results → Apply synthesis logic → Final staging prediction

- **Critical path:**
  1. Task definition + guideline uploaded to cloud LLM
  2. Cloud generates subtasks, prompts, synthetic cases
  3. Local LLM validates prompts on synthetic cases (iterative refinement)
  4. Refined prompts manually transferred to isolated environment
  5. Local LLM runs inference on real clinical data (5 runs per case)
  6. Majority voting produces final prediction

- **Design tradeoffs:**
  - Temperature settings: Cloud uses 0.8 (creative prompt generation); local uses 0.2 (consistent outputs)
  - Subtask granularity: Finer decomposition improves local execution but increases synthesis complexity
  - Validation threshold: 80% chosen empirically; lower threshold speeds iteration but risks underperforming prompts
  - Manual vs. automated transfer: Current design prioritizes security over operational efficiency

- **Failure signatures:**
  - Overstaging on ambiguous language: Model interprets "suspicious for" as definitive
  - Low validation pass rate: Prompts fail to reach 80% on synthetic cases
  - High variance across T runs: Majority voting frequently encounters ties
  - Structured vs. free-text gap: 15+ percentage point improvement on structured inputs

- **First 3 experiments:**
  1. Reproduce baseline comparison: Run Local LLM (Base), Local LLM (with Guideline), and MedOrchestra on small validation set to verify performance delta
  2. Ablate validation loop: Skip synthetic validation and use draft prompts directly; measure accuracy drop
  3. Test subtask independence: Randomly shuffle subtask execution order and verify outputs remain consistent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MedOrchestra generalize to other clinical domains and disease types beyond pancreatic cancer staging?
- Basis in paper: The authors state in Limitations: "We have yet to verify its generalizability across other diseases, institutions, and data formats. The framework works best for clinical tasks with explicit, structured guidelines and may struggle in domains where guidelines remain ambiguous or nonexistent."
- Why unresolved: The study evaluated only 100 radiology reports from pancreatic cancer patients at a single institution, using well-defined NCCN guidelines.

### Open Question 2
- Question: How can MedOrchestra be refined to better handle ambiguous or speculative language in free-text reports without overstaging?
- Basis in paper: The Conclusion states: "handling ambiguity in free-text reports remains an open challenge." The authors found that "MedOrchestra tended to interpret speculative expressions (e.g., 'likely,' 'suspicious for') as definitive indicators of advanced disease, which led to overstaging."

### Open Question 3
- Question: What is the optimal number of inference repetitions and temperature settings for reliable majority voting in the local LLM?
- Basis in paper: The Limitations section states: "we did not perform a systematic validation to determine the optimal number of repetitions or to assess output consistency across runs. Future work should introduce clear metrics to evaluate the effectiveness and reliability of multi-round inference strategies."

### Open Question 4
- Question: How can expert oversight be structurally integrated into MedOrchestra to ensure safe clinical deployment?
- Basis in paper: The Ethics Statement notes: "there remains a risk that it may be used to make clinical decisions autonomously in practice. To mitigate this risk, future work should investigate mechanisms to explicitly require and structurally integrate expert oversight throughout the framework."

## Limitations
- Generalizability: Framework effectiveness across different diseases, institutions, and data formats remains unverified
- Ambiguity handling: Current implementation struggles with speculative language in free-text reports, tending to overstage
- Validation optimization: No systematic study of optimal validation thresholds, repetition counts, or temperature settings

## Confidence
- **High confidence**: Hybrid architecture concept and privacy-preserving separation are clearly demonstrated and theoretically sound
- **Medium confidence**: Synthetic validation mechanism's contribution is inferred but not directly isolated in experiments
- **Medium confidence**: Clinical accuracy claims are well-documented but clinician comparison lacks detail on experience levels

## Next Checks
1. **Ablation of validation loop**: Implement system without synthetic validation and measure accuracy degradation to quantify refinement mechanism's contribution
2. **Synthetic case sensitivity analysis**: Generate synthetic cases with varying linguistic complexity and test whether 80% threshold consistently produces clinically reliable prompts
3. **Subtask independence verification**: Randomly shuffle subtask execution order and verify outputs remain consistent, identifying implicit dependencies requiring correction