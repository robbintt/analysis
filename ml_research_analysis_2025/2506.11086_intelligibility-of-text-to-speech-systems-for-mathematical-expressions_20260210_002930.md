---
ver: rpa2
title: Intelligibility of Text-to-Speech Systems for Mathematical Expressions
arxiv_id: '2506.11086'
source_url: https://arxiv.org/abs/2506.11086
tags:
- audio
- categories
- audiomx
- latex
- intelligibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the intelligibility of five state-of-the-art
  TTS models when converting mathematical expressions into speech. Since TTS models
  cannot directly process LaTeX, two LLMs (QWEN and GPT4) are used to generate English
  pronunciations, which are then input to the TTS models.
---

# Intelligibility of Text-to-Speech Systems for Mathematical Expressions

## Quick Facts
- arXiv ID: 2506.11086
- Source URL: https://arxiv.org/abs/2506.11086
- Reference count: 0
- Key outcome: TTS models for mathematical expressions show varying intelligibility across categories, with performance significantly below expert audio

## Executive Summary
This study evaluates five state-of-the-art TTS models for converting mathematical expressions into speech. Since TTS models cannot directly process LaTeX, two LLMs (QWEN and GPT4) generate English pronunciations from mathematical expressions, which are then input to the TTS models. The evaluation uses listening tests with 49 participants and measures intelligibility through Mean Opinion Scores and transcription accuracy using Count of Correct, LCER, and TeXBLEU metrics. Results show that TTS outputs for mathematical expressions are not always intelligible, with performance varying significantly across expression categories and TTS models. For most categories, TTS models perform substantially worse than expert-rendered audio.

## Method Summary
The evaluation methodology involves converting LaTeX mathematical expressions to English pronunciations using LLMs (QWEN and GPT4), then feeding these pronunciations to five TTS models. A listening test with 49 participants assesses user perception through Mean Opinion Scores and transcription accuracy. Three metrics are used: Count of Correct (number of accurately transcribed expressions), LCER (Levenshtein Character Error Rate), and TeXBLEU (a metric adapted for mathematical expressions). The study compares TTS model performance against expert-rendered audio and examines the impact of different LLMs on final intelligibility.

## Key Results
- TTS outputs for mathematical expressions show varying levels of intelligibility across different expression categories
- Performance of TTS models is significantly worse than expert-rendered audio for most mathematical expression categories
- Choice of LLM has limited impact on intelligibility when the generated pronunciations are correct

## Why This Works (Mechanism)
The study's methodology addresses the fundamental challenge that TTS systems cannot directly process LaTeX mathematical notation. By using LLMs to convert mathematical expressions into spoken English, the approach bridges this gap. The evaluation framework combines subjective human perception (MOS) with objective transcription accuracy metrics, providing a comprehensive assessment of intelligibility. The comparison against expert audio establishes a performance baseline, while testing multiple TTS models and LLMs reveals variability in system performance and the relative importance of the LLM component.

## Foundational Learning
- **LaTeX mathematical notation**: Essential for representing mathematical expressions in a machine-readable format. Quick check: Can you identify basic LaTeX commands for common mathematical symbols?
- **Text-to-Speech conversion**: The process of converting text into spoken audio. Quick check: What are the main stages in a typical TTS pipeline?
- **Levenshtein Character Error Rate (LCER)**: A metric for measuring the difference between reference and hypothesis strings at the character level. Quick check: How does LCER differ from word error rate?
- **BLEU score adaptation**: Modified for mathematical expressions to account for their unique structure. Quick check: Why might standard BLEU be insufficient for evaluating mathematical expression intelligibility?

## Architecture Onboarding
Component map: LaTeX expression -> LLM (QWEN/GPT4) -> English pronunciation -> TTS model -> Audio output
Critical path: The conversion from LaTeX through LLM to TTS is the core workflow, with intelligibility determined at the final audio output stage
Design tradeoffs: Using LLMs introduces an additional processing step that may introduce errors, but enables TTS systems to handle mathematical content
Failure signatures: Incorrect LLM pronunciation, TTS mispronunciation, loss of mathematical structure in verbalization
First experiments: 1) Test individual mathematical expression categories separately 2) Compare TTS performance with and without LLM intermediate step 3) Evaluate impact of different TTS model architectures on mathematical expression intelligibility

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 49 participants may limit statistical power and generalizability
- Use of two different LLMs introduces variability not fully controlled for in the evaluation
- Focus on subjective MOS and transcription metrics may not capture all aspects of mathematical expression intelligibility

## Confidence
High confidence: TTS outputs show varying intelligibility across expression categories and TTS models
Medium confidence: LLM choice has limited impact when pronunciations are correct
Low confidence: Need for improved TTS models tailored to mathematical expressions

## Next Checks
1. Conduct a larger-scale listening test with a more diverse participant pool to increase statistical power
2. Perform detailed error analysis of LLM-generated pronunciations across different mathematical expression types
3. Evaluate additional TTS models, including those specifically designed for mathematical content