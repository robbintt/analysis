---
ver: rpa2
title: 'Starting Positions Matter: A Study on Better Weight Initialization for Neural
  Network Quantization'
arxiv_id: '2506.10463'
source_url: https://arxiv.org/abs/2506.10463
tags:
- quantization
- quantized
- accuracy
- initialization
- conv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of improving quantization robustness
  in deep neural networks (DNNs) by exploring the impact of weight initialization
  on quantization performance. The authors conduct an extensive study examining the
  effects of different weight initialization methods on various CNN building blocks,
  revealing that the choice of initializer significantly affects final quantization
  robustness.
---

# Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization

## Quick Facts
- arXiv ID: 2506.10463
- Source URL: https://arxiv.org/abs/2506.10463
- Authors: Stone Yun; Alexander Wong
- Reference count: 37
- Key outcome: GHN-based weight initialization significantly improves quantization robustness, achieving better-than-random accuracy at 2-bit precision

## Executive Summary
This paper addresses the critical challenge of improving quantization robustness in deep neural networks by examining how weight initialization strategies affect final quantized performance. Through extensive experimentation across various CNN architectures and initialization methods, the authors demonstrate that initialization choices significantly impact quantization robustness, often more than their effect on float32 accuracy. They propose GHN-QAT, a novel approach using Graph Hypernetworks to predict quantization-robust parameters, which achieves substantial accuracy improvements across multiple bitwidths (W4/A4, W2/A2) while maintaining float32-level performance at W8/A8.

## Method Summary
The authors employ Graph Hypernetworks (GHNs) to predict parameters for quantized neural networks. They first finetune a pre-trained GHN-2 on a mobile-friendly CNN architecture space (ConvNets-250K), creating GHN-Q which predicts float32 parameters. To improve quantization robustness, they further finetune GHN-Q using quantization simulation (SimQuant for 4/8-bit, NoiseQuant for 2-bit) during training, resulting in GHN-QAT. The approach leverages the GHN's ability to learn architecture representations and predict parameters that minimize loss under quantized conditions, effectively creating initialization strategies optimized for quantization.

## Key Results
- GHN-QAT achieves 52.5% accuracy for W4/A4 quantization (vs. 37.2% without QAT)
- W2/A2 quantization accuracy improves from 11.2% to 26.3% with GHN-QAT (better-than-random)
- GHN-Q maintains 70.9% W8/A8 accuracy vs. 71.1% float32 (minimal degradation)
- Initialization choice affects quantization robustness significantly (30.10% vs. 12.36% accuracy drop for different initializers with similar FP32 accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Weight Initialization Shapes Final Quantization-Robust Distributions
- Claim: Random weight initialization strategies affect not only float32 accuracy but also the final quantization robustness of trained CNNs, mediated through their impact on learned weight and activation distributions.
- Mechanism: Initialization determines the starting point on the loss surface. Since gradient descent takes small incremental steps, the final dynamic ranges of weights and activations are influenced by initial distributions. Different initializers lead to varying trained ranges, which affect quantization noise and clipping behavior.
- Core assumption: The relationship between initialization and final quantized accuracy is causal, not merely correlational.
- Evidence anchors:
  - [abstract] "This analysis reveals that even with varying CNN architectures, the choice of random weight initializer can significantly affect final quantization robustness."
  - [section 3.2, Table 1] DWS_Conv_With_BN_HeNorm shows 30.10% accuracy decrease after quantization vs. 12.36% for GlorotUni despite similar FP32 accuracy (~80%).
  - [corpus] Related work "Optimal Condition for Initialization Variance in Deep Neural Networks" supports initialization variance affecting SGD dynamics, but does not directly address quantization.

### Mechanism 2: GHN Parameter Prediction Yields Compact, Quantization-Friendly Distributions
- Claim: GHN-predicted parameters exhibit inherent quantization robustness at 8-bit precision without explicit quantization training, due to architectural constraints in the prediction process.
- Mechanism: GHNs use channel-wise weight tiling (copying predicted parameters across channels) and differentiable parameter normalization. Tiling minimizes channel-wise distributional mismatch—a known issue for depthwise separable convolutions. Normalization produces less heavy-tailed distributions.
- Core assumption: The compactness of GHN-predicted distributions is the primary driver of quantization robustness.
- Evidence anchors:
  - [section 4.3, Table 3] GHN-Q achieves 70.9% W8/A8 accuracy vs. 71.1% Float32 (minimal degradation) on in-distribution test sets.
  - [section 4.3] "A likely explanation is that the channel-wise weight tiling and differentiable parameter normalization lead to layerwise distributions that are compact and quantization-friendly."
  - [corpus] No direct corpus evidence on GHN-quantization interaction; this appears novel.

### Mechanism 3: GHN-QAT Learns Quantization-Aware Graph Representations
- Claim: Finetuning GHNs on quantized CNN graphs (GHN-QAT) improves predicted parameter quality for low-bitwidth networks by exposing the hypernetwork to quantization-induced perturbations during training.
- Mechanism: During GHN-QAT training, sampled CNN graphs are quantized using SimQuant (W4/A4, W8/A8) or NoiseQuant (W2/A2, due to SimQuant instability). The GHN learns to predict parameters that minimize loss under quantized forward passes, effectively learning quantization-robust graph representations.
- Core assumption: The improvement comes from learning quantization-aware representations rather than overfitting to specific quantization artifacts.
- Evidence anchors:
  - [section 4.4, Table 4] GHN-QAT achieves 52.5% W4/A4 accuracy (vs. 37.2% without QAT) and 26.3% W2/A2 (vs. 11.2%), demonstrating substantial gains.
  - [section 4.4] "W2/A2 is now better-than-random accuracy and W8/A8 performance is just as good as Float32 after GHN-QAT."
  - [corpus] "Differentiable, Bit-shifting, and Scalable Quantization" (FMR=0.726) discusses differentiable quantization approaches but does not address hypernetwork-based initialization.

## Foundational Learning

- Concept: **Uniform Affine Quantization**
  - Why needed here: Understanding Eq. 1-3 is essential for interpreting how step size (s) and zero-point (Z) affect quantization noise, and why dynamic range matters.
  - Quick check question: Given a weight tensor with range [-2.0, 1.5] and 4-bit quantization, what is the step size?

- Concept: **Message Passing in Graph Neural Networks**
  - Why needed here: GHNs encode CNN architectures as graphs where nodes are operations and edges are connections. Understanding how node features are updated via message passing explains how GHNs learn architecture representations.
  - Quick check question: How does a GHN encode a ResNet skip connection differently from a sequential Conv-ReLU block?

- Concept: **Straight-Through Estimator (STE) for Quantization Gradients**
  - Why needed here: SimQuant uses STE to approximate gradients through the rounding operation. Understanding this explains why W2/A2 training was unstable with SimQuant.
  - Quick check question: Why might STE fail at very low bitwidths where clipping dominates rounding?

## Architecture Onboarding

- Component map:
  - Graph Generator -> GHN Core -> Quantization Simulator -> Evaluation Pipeline

- Critical path:
  1. Generate/obtain pretrained GHN-2 checkpoint (CIFAR-10 or ImageNet pretrained)
  2. Generate ConvNets-250K graph dataset with mobile-friendly ops
  3. Finetune GHN on float32 loss (GHN-Q baseline)
  4. Finetune GHN with SimQuant/NoiseQuant loss at target bitwidth (GHN-QAT)
  5. Evaluate on held-out test splits with BatchNorm folding before quantization

- Design tradeoffs:
  - SimQuant vs. NoiseQuant: SimQuant directly simulates quantization but requires STE; NoiseQuant provides exact gradients but doubles memory. Paper uses NoiseQuant for W2/A2 due to SimQuant instability.
  - Bitwidth-specific vs. unified GHN-QAT: Current approach trains separate models per bitwidth. Encoding bitwidth in graph could enable unified models (noted as future work).
  - Absolute vs. percentile clipping: W4/A4 BN-Free training required percentile clipping (top/bottom 1%) for stability; absolute ranges caused divergence.

- Failure signatures:
  - Exploding loss during W2/A2 SimQuant training → Switch to NoiseQuant
  - Poor BN-Free accuracy → Architecture is out-of-distribution; train dedicated BN-Free GHN-QAT on BNFree-ConvNets-50K
  - Low W4/A4 accuracy with absolute clipping → Use percentile clipping and increase meta-batchsize to 16

- First 3 experiments:
  1. **Reproduce GHN-Q baseline**: Finetune GHN-2 on ConvNets-250K subset (10K graphs) with float32 loss only. Verify W8/A8 accuracy matches float32 within 0.5% on in-distribution test set.
  2. **Ablate initialization impact**: Train 5 CNNs from Table 1 (e.g., DWS_Conv_With_BN with GlorotUni, HeNorm, RandUni_Large) and measure W8/A8 accuracy degradation. Confirm HeNorm shows >20% degradation vs. <5% for RandUni_Large.
  3. **Compare SimQuant vs. NoiseQuant at W4/A4**: Train two GHN-QAT models with identical hyperparameters but different quantization simulation methods. Compare training stability and final W4/A4 accuracy on the Deep test split.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GHN-QAT-predicted parameters be used as an initialization point for standard quantization-aware training (QAT) to reduce overall training time and resources?
- Basis: [explicit] The abstract and conclusion explicitly propose "using GHN-QAT-initialized parameters for quantization-aware training" as a method to streamline the quantization process.
- Why unresolved: The paper demonstrates that GHN-QAT predicts robust parameters but does not validate whether these parameters serve as a better *starting point* for further fine-tuning compared to random initialization.
- What evidence would resolve it: Experiments measuring the convergence speed and final accuracy of models fine-tuned via QAT starting from GHN-QAT predictions versus random initialization.

### Open Question 2
- Question: Can the GHN-QAT framework be successfully adapted to handle mixed-precision quantization strategies?
- Basis: [explicit] Section 5 lists "adapting GHN-QAT for mixed-precision quantized CNNs" as a specific direction for future work.
- Why unresolved: The current study restricts experiments to uniform bit-widths (e.g., W4/A4 or W8/A8), whereas mixed-precision requires the model to handle heterogeneous quantization constraints across layers.
- What evidence would resolve it: Extending the GHN graph representation to include bit-width attributes and evaluating the accuracy of parameters predicted for architectures with layer-wise precision variations.

### Open Question 3
- Question: Can GHN-QAT be generalized to predict parameters for non-CNN architectures, specifically Vision Transformers (ViT) or MobileBERT?
- Basis: [explicit] Sections 4.1 and 5 suggest that "deeper exploration of transformer-like self-attention layers such as in ViT or MobileBERT has great potential."
- Why unresolved: The methodology is currently validated only on convolutional building blocks (ResNets, MobileNets); it is unknown if the graph-based reasoning applies to attention mechanisms.
- What evidence would resolve it: Training GHN-QAT on a search space of transformer architectures and evaluating the quantization robustness of the predicted parameters.

## Limitations

- Architecture-space specificity: GHN-QAT shows significant accuracy drops on BatchNorm-free architectures, indicating learned representations are not universally quantization-aware.
- Initialization-causality: While the paper shows initialization affects quantization robustness, the mechanism is correlational rather than definitively proven causal.
- Distributional analysis: Claims about GHN-predicted parameters being "compact" lack direct distributional evidence and are noted as future work.

## Confidence

- **High**: GHN-QAT improves quantized accuracy over baselines (Table 4), and initialization choice affects final quantization robustness (Table 1).
- **Medium**: The mechanism that initialization shapes final quantization-friendly distributions is plausible but not definitively proven causal.
- **Low**: Claims about inherent quantization robustness of GHN-predicted parameters being primarily due to "compact distributions" lack direct distributional evidence.

## Next Checks

1. **Quantization-robustness correlation**: For 5 CNN architectures from Table 1, measure W8/A8 accuracy degradation vs. float32 accuracy. Verify HeNorm shows >20% degradation vs. <5% for RandUni_Large.

2. **Weight distribution analysis**: Plot histogram of weight distributions for GHN-Q vs. randomly initialized models at W8/A8. Confirm GHN-Q shows tighter, less heavy-tailed distributions.

3. **Architecture generalization test**: Train GHN-QAT on in-distribution graphs, evaluate on BN-Free split (BNFree-ConvNets-50K). Verify accuracy drops significantly (target: <30% W4/A4 vs. 52.5% in-distribution).