---
ver: rpa2
title: 'GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial
  AI'
arxiv_id: '2511.15658'
source_url: https://arxiv.org/abs/2511.15658
tags:
- dataset
- datasets
- figure
- data
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GEO-Bench-2 provides a comprehensive framework for evaluating geospatial
  foundation models across 19 permissively-licensed datasets spanning classification,
  segmentation, regression, object detection, and instance segmentation. The framework
  introduces "capability" groups to rank models based on shared characteristics like
  resolution, bands, and temporality, enabling targeted assessment of model strengths
  and weaknesses.
---

# GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI

## Quick Facts
- arXiv ID: 2511.15658
- Source URL: https://arxiv.org/abs/2511.15658
- Reference count: 40
- Key outcome: GEO-Bench-2 introduces capability-based evaluation groups to rank geospatial foundation models across 19 permissively-licensed datasets, revealing that no single model dominates all tasks and highlighting the importance of task-specific architecture choices and data modalities.

## Executive Summary
GEO-Bench-2 addresses the challenge of evaluating geospatial foundation models (GeoFMs) by introducing a comprehensive framework that moves beyond simple performance metrics to assess model capabilities across diverse Earth observation tasks. The benchmark organizes 19 permissively-licensed datasets into nine overlapping capability groups based on shared characteristics like resolution, spectral bands, and temporality, enabling targeted assessment of model strengths and weaknesses. Experiments reveal that while EO-specific models excel in multispectral applications, natural image pretrained models perform better on high-resolution RGB tasks, demonstrating that optimal model selection depends on task requirements and data modalities.

## Method Summary
GEO-Bench-2 evaluates nine geospatial foundation models using a prescriptive yet flexible protocol that standardizes dataset preparation, model initialization, and evaluation methodology while allowing methodological innovation in adaptation strategies. The framework employs hyperparameter optimization with 16 trials per dataset, trains with 5 different seeds for 50 epochs, and aggregates results using normalized interquartile mean (IQM) to ensure fair comparison. Datasets are organized into overlapping capability groups (Core, Pixel-wise, Multi-Temporal, Multi-Spectral-dependent) to reveal model strengths that single aggregate metrics obscure. Models are adapted through full fine-tuning with task-appropriate decoders (linear for classification, UNet for pixel-wise tasks, Faster/Mask R-CNN for detection).

## Key Results
- No single model dominates across all tasks, with EO-specific models excelling in multispectral applications while natural image pretrained models perform better on high-resolution RGB tasks
- Full fine-tuning consistently outperforms frozen encoders, with ranking changes for over 20% of model pairs, demonstrating the importance of adapting pretrained representations to EO distributions
- UNet decoders consistently outperform linear decoders on pixel-wise tasks, while ConvNeXt architectures show competitive results on RGB tasks despite limited use in EO
- Multi-spectral bands prove critical for tasks like burn scar detection, with significant performance drops when removed, validating the advantage of EO-specific pretraining

## Why This Works (Mechanism)

### Mechanism 1: Capability-Based Evaluation Grouping
Organizing benchmark datasets into overlapping "capability" groups reveals model strengths that single aggregate metrics obscure. Datasets sharing characteristics (resolution, bands, temporality) form subsets that isolate specific challenges. Normalization per dataset followed by interquartile mean (IQM) aggregation enables fair cross-dataset comparison while discarding outliers.

### Mechanism 2: Full Fine-Tuning Enables Effective Adaptation
Unfreezing the backbone during fine-tuning consistently improves downstream performance and alters model rankings relative to frozen encoders. End-to-end training allows pretrained representations to adapt to EO-specific distributions, band combinations, and spatial scales that differ from pretraining data.

### Mechanism 3: Domain-Specific Pretraining Confers Spectral Advantages
Models pretrained on EO data (with multi-spectral bands) outperform natural-image models on tasks requiring spectral information beyond RGB. EO-specific pretraining exposes models to band combinations and spectral signatures absent in natural image datasets.

## Foundational Learning

- Concept: **Multi-Spectral Remote Sensing Basics**
  - Why needed here: Understanding that EO data includes bands beyond visible light (NIR, SWIR, SAR) is essential to interpret why certain models excel on specific capabilities.
  - Quick check question: Can you name two satellite bands useful for vegetation monitoring that are not visible to humans?

- Concept: **Transfer Learning and Fine-Tuning Paradigms**
  - Why needed here: The benchmark assumes familiarity with pretrained backbones, decoder heads, and the distinction between frozen and fine-tuned adaptation.
  - Quick check question: What is the difference between using a frozen encoder and full fine-tuning?

- Concept: **Hyperparameter Optimization Protocols**
  - Why needed here: GEO-Bench-2 prescribes specific HPO budgets (16 trials, 5 repeated runs) and aggregation methods (bootstrap, IQM) that require understanding of statistical evaluation.
  - Quick check question: Why might averaging raw scores across datasets be misleading?

## Architecture Onboarding

- Component map:
  - Input Bands -> Band Adapter -> Encoder (Backbone: ViT, ConvNeXt, Swin) -> Decoder (Linear/UNet/R-CNN) -> Output Task Results
  - For multi-temporal: Temporal Fusion (embeddings averaged along temporal dimension)

- Critical path:
  1. Load dataset in TACO format with per-band z-score normalization
  2. Initialize backbone and attach task-appropriate decoder
  3. Run HPO (16 trials) on validation set using Optuna
  4. Train best config with 5 seeds for 50 epochs each
  5. Aggregate via normalized bootstrapped IQM

- Design tradeoffs:
  - **UNet vs Linear Decoder**: UNet consistently outperforms linear (Section 6.3.3) but increases compute and memory.
  - **Multi-modal vs Optical-only**: Inconclusive; TerraMind benefits from S1+S2 on some datasets, others do not (Section 6.3.6).
  - **ViT vs ConvNeXt**: ConvNeXt architectures (rarely used in EO) show competitive results on RGB tasks; ViTs with smaller patch sizes (Clay) excel on pixel-wise tasks.

- Failure signatures:
  - Saturated performance (>98% accuracy) on overly simple datasets (e.g., EuroSat) indicates insufficient discriminative power.
  - Large performance drops when removing multi-spectral bands signal tasks requiring spectral information (validated via ablation).
  - Ranking instability across frozen/fine-tuned modes suggests overfitting to pretraining distribution.

- First 3 experiments:
  1. **Baseline Capability Assessment**: Run TerraTorch Iterate on Core capability (subset of discriminative datasets) with a single model to establish HPO workflow and verify normalization pipeline.
  2. **Decoder Comparison**: On a pixel-wise dataset (e.g., FLAIR2), compare UNet vs linear decoder to validate expected performance gap before committing to architecture choice.
  3. **Temporal Ablation**: On a multi-temporal dataset (e.g., PASTIS), run single-timestamp vs multi-timestamp to quantify temporal value for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single Geospatial Foundation Model (GeoFM) achieve strong performance across all EO tasks, resolutions, and modalities?
- Basis in paper: [explicit] "This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research."
- Why unresolved: Current models specialize—natural image models excel on high-resolution RGB while EO-specific models dominate multispectral tasks. Architectural choices and pretraining data create inherent trade-offs.

### Open Question 2
- Question: What are optimal adaptation strategies for transferring GeoFMs to downstream tasks across different domains?
- Basis in paper: [explicit] "This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks."
- Why unresolved: The paper allows flexibility in adaptation protocols precisely because no consensus exists; frozen backbones showed significant ranking changes versus fine-tuning.

### Open Question 3
- Question: How should SAR-specific capabilities be evaluated and integrated into geospatial benchmarks?
- Basis in paper: [explicit] "A SAR-specific capability is missing despite five SAR-inclusive datasets—future work should address this gap."
- Why unresolved: Five datasets include SAR but no dedicated capability group exists; the paper's multispectral ablation focused on optical bands.

### Open Question 4
- Question: To what extent does multi-modal input (SAR + optical) improve performance over optical-only data?
- Basis in paper: [explicit] "It remains unclear how much S1 helps when S2 is available" and ablation results showed "performance appears to vary by model and dataset."
- Why unresolved: The multi-modality ablation produced inconsistent results—beneficial for Biomassters, detrimental for PASTIS, and mixed for BEN V2.

## Limitations

- The benchmark relies on a limited set of 19 permissively-licensed datasets, which may not capture the full diversity of real-world EO scenarios and edge cases.
- The choice of full fine-tuning over parameter-efficient methods limits applicability to resource-constrained scenarios, representing a tradeoff between establishing upper bounds and practical deployment.
- The temporal fusion strategy of averaging embeddings may oversimplify complex temporal relationships that could benefit from more sophisticated attention mechanisms.

## Confidence

- **High Confidence**: The claim that no single model dominates across all tasks is well-supported by experimental results showing task-specific performance variations.
- **Medium Confidence**: The assertion that capability-based grouping reveals model strengths more effectively than aggregate metrics is supported by framework design but requires validation on additional datasets.
- **Low Confidence**: The conclusion about superiority of ConvNeXt architectures on RGB tasks is based on limited experimental data, as the benchmark primarily uses ViT backbones.

## Next Checks

1. **Capability Stability Analysis**: Test capability-based rankings on an additional 5-10 permissively-licensed datasets spanning underrepresented domains (urban planning, hydrology) to validate that capability groupings produce consistent model rankings across broader data distributions.

2. **Parameter-Efficient Adaptation Comparison**: Re-run the benchmark using LoRA adapters and frozen encoders with the same HPO budget to quantify the performance gap between resource-efficient and full fine-tuning approaches across different capability groups.

3. **Temporal Relationship Modeling**: Replace the simple temporal averaging with a temporal attention mechanism or recurrent layer on a multi-temporal dataset like PASTIS to assess whether more sophisticated temporal fusion yields measurable performance improvements.