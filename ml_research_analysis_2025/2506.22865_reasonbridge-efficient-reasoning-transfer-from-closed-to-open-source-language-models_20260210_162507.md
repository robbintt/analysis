---
ver: rpa2
title: 'ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language
  Models'
arxiv_id: '2506.22865'
source_url: https://arxiv.org/abs/2506.22865
tags:
- reasoning
- arxiv
- preprint
- reasonbridge
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReasonBridge addresses the performance gap between closed-source
  and open-source language models on reasoning tasks by efficiently transferring reasoning
  capabilities through a hierarchical knowledge distillation framework. The method
  uses a carefully curated dataset of 1,000 reasoning traces selected for difficulty,
  diversity, and quality, combined with reasoning-specialized adapter modules that
  modify only 0.3% of model parameters.
---

# ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models

## Quick Facts
- arXiv ID: 2506.22865
- Source URL: https://arxiv.org/abs/2506.22865
- Reference count: 40
- Primary result: Transfers reasoning capabilities from closed to open-source models with 23% accuracy improvement using only 0.3% parameter modification

## Executive Summary
ReasonBridge addresses the performance gap between closed-source and open-source language models on reasoning tasks through an efficient knowledge distillation framework. The method uses a carefully curated dataset of 1,000 reasoning traces combined with reasoning-specialized adapter modules that modify only 0.3% of model parameters. It also incorporates a test-time compute scaling mechanism using guided inference interventions. Comprehensive evaluations show ReasonBridge improves reasoning performance by up to 23% across benchmarks, with the enhanced Qwen2.5-14B-Coder outperforming Claude-3.5-Sonnet on MATH500 and matching its performance on AIME problems, while demonstrating consistent gains across model scales and reasoning domains.

## Method Summary
ReasonBridge transfers reasoning capabilities through a hierarchical knowledge distillation framework using specialized adapter modules. The method curates 1,000 reasoning traces filtered for difficulty (unsolved by 7B and 32B models), diversity (MSC domains), and quality. Three types of adapters are placed at specific transformer depths: strategic adapters after early self-attention layers for problem decomposition, tactical adapters after middle FFN layers for approach selection, and operational adapters in later layers for execution accuracy. Training uses a hierarchical loss combining output, strategic, tactical, and operational components. At inference, guided reasoning interventions detect premature termination or uncertainty and inject adaptive prompts to extend reasoning, providing additional accuracy gains without retraining.

## Key Results
- Improves reasoning accuracy by up to 23% across benchmarks compared to base models
- Enhanced Qwen2.5-14B-Coder outperforms Claude-3.5-Sonnet on MATH500 and matches on AIME problems
- Curated 1,000 examples achieves performance comparable to 58× larger unfiltered dataset
- Parameter-efficient (0.3% of parameters modified) while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Reasoning Transfer via Layer-Specialized Adapters
Strategic adapters in early layers capture problem decomposition, tactical adapters in middle layers handle approach selection, and operational adapters in later layers improve step-by-step execution. The hierarchical loss (L_strat, L_tact, L_op) trains each level with decoupled supervision. Removing strategic adapters causes -6.7pt drop on AIME24, the largest ablation impact, confirming the importance of this hierarchy.

### Mechanism 2: Difficulty-Filtered Data Selection Enables Sample-Efficient Transfer
Curating 1,000 examples filtered for difficulty (unsolved by 7B and 32B models), diversity (uniform across MSC domains), and quality outperforms using 58× more unfiltered data. Random 1K dataset underperforms curated Reason1K by 13.4pt on AIME24, demonstrating the value of this filtering approach.

### Mechanism 3: Adaptive Test-Time Intervention Scales Reasoning Compute
Injecting context-aware guidance tokens ("Wait," "Let me try differently," "Let me verify") based on detected reasoning state improves accuracy without retraining. 4 interventions improve AIME24 from 41.7% to 46.7%, with guided intervention outperforming simple budget forcing by 3.4pt on AIME24.

## Foundational Learning

- **Knowledge Distillation Fundamentals**: Why needed here: Hierarchical distillation transfers teacher reasoning patterns at multiple abstraction levels, not just output matching. Quick check: Can you explain why matching intermediate representations differs from matching final output distributions?

- **Parameter-Efficient Fine-Tuning (Adapters/LoRA)**: Why needed here: ReasonBridge modifies only 0.3% of parameters via bottleneck adapters (r=64); understanding residual connections and adapter injection points is essential. Quick check: What is the computational overhead of a bottleneck adapter with r=64 on a d=4096 hidden dimension?

- **Chain-of-Thought and Test-Time Scaling**: Why needed here: Guided inference extends reasoning traces dynamically; understanding CoT and self-consistency helps interpret why intervention works. Quick check: How does test-time compute scaling differ from training-time scaling?

## Architecture Onboarding

- **Component map**: Input problem → Backbone LLM → Adapter Layers (Strategic, Tactical, Operational) → Training with hierarchical loss → Inference with optional Guided Inference Intervention

- **Critical path**: 1) Curate Reason1K via quality → difficulty → diversity filters 2) Generate teacher traces with Gemini Flash Thinking API 3) Insert adapters at designated layers (0.3% params, r=64) 4) Train with hierarchical loss for 0.7-1.2h on 8× H100 5) Optionally enable GII at inference for +3-5pt gains

- **Design tradeoffs**: Adapter placement vs. LoRA everywhere (specialized placement outperforms standard LoRA by 5.0pt but requires architecture-specific tuning); 1K curated vs. 58K full (curated saves 58× data with marginal performance loss; curation pipeline has upfront cost); GII latency vs. accuracy (each intervention adds generation steps; 4 interventions ≈ 2× latency for 5pt gain)

- **Failure signatures**: Random data selection (expect ~13pt drop on AIME24 vs. curated); missing strategic adapters (expect ~7pt drop, largest single-component loss); simple budget forcing instead of adaptive GII (expect ~3pt lower than full GII)

- **First 3 experiments**: 1) Train ReasonBridge with random 1K subset on Qwen2.5-7B; should underperform curated Reason1K by >10pt on AIME24 2) Remove all strategic adapters; verify ~6-7pt drop on AIME24 confirming hierarchical importance 3) Run inference with 0, 2, 4, 6 interventions on AIME24; should see monotonic (possibly diminishing) gains

## Open Questions the Paper Calls Out

- Can enhanced open-source models generate and refine their own reasoning traces without closed-source teacher models? Current method depends on Gemini Flash Thinking API for initial trace generation, limiting accessibility.

- Does ReasonBridge scale effectively to models with 70B+ parameters? All empirical validation uses ≤14B models; architectural dynamics may differ at larger scales.

- Why does strategic selection of only 1,000 examples achieve performance comparable to the full 58K dataset? The paper demonstrates the empirical phenomenon without investigating whether this stems from reasoning pattern coverage, difficulty concentration, or other factors.

## Limitations

- Exact implementation details for adapter placement across different model architectures are not fully specified, requiring architectural adaptation
- PARTIAL/UNCERTAIN/UNVERIFIED detection logic for guided inference lacks precise thresholds or detection criteria
- Reproducibility of the quality and difficulty filtering pipeline depends on subjective judgments about trace quality and solution correctness

## Confidence

- **High Confidence**: Hierarchical adapter placement outperforms standard adapter or LoRA approaches (5.0pt gain verified via ablation showing 6.7pt drop when removing strategic adapters); data efficiency claim (1K curated ≈ 58K full) is well-supported by direct comparison experiments
- **Medium Confidence**: Guided inference intervention mechanism's effectiveness (4 interventions improving AIME24 by 5.0pt) is demonstrated but relies on unspecified state detection thresholds; scalability across model families is shown but with varying absolute gains
- **Low Confidence**: Claim about matching Claude-3.5-Sonnet on AIME is based on the specific Qwen2.5-14B-Coder variant and may not generalize to all base models or reasoning domains

## Next Checks

1. **Ablation of Hierarchical Loss Components**: Train models with only single loss components (Lout alone, Lstrat alone, etc.) to verify the claimed importance hierarchy and ensure the 0.5/0.3/0.2 weighting is optimal

2. **Cross-Domain Generalization**: Apply ReasonBridge to non-mathematical reasoning domains (legal, commonsense, scientific) to test whether the hierarchical adapter architecture generalizes beyond the MATH/AIME benchmarks

3. **Robustness to Data Quality Variation**: Systematically vary the quality filtering threshold (e.g., include noisier traces) to determine how sensitive performance is to the curated dataset's quality requirements