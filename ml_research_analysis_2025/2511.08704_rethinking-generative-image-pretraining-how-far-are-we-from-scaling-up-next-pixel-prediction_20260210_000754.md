---
ver: rpa2
title: 'Rethinking generative image pretraining: How far are we from scaling up next-pixel
  prediction?'
arxiv_id: '2511.08704'
source_url: https://arxiv.org/abs/2511.08704
tags:
- image
- scaling
- prediction
- optimal
- next-pixel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the scaling properties of autoregressive\
  \ next-pixel prediction for unified vision models. The authors train a family of\
  \ Transformers on 32\xD732 images using IsoFlops profiles up to 7\xD710^19 FLOPs,\
  \ evaluating next-pixel prediction loss, ImageNet classification accuracy, and generation\
  \ quality via Fr'echet Distance."
---

# Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?

## Quick Facts
- **arXiv ID:** 2511.08704
- **Source URL:** https://arxiv.org/abs/2511.08704
- **Authors:** Xinchen Yan; Chen Liang; Lijun Yu; Adams Wei Yu; Yifeng Lu; Quoc V. Le
- **Reference count:** 4
- **Primary result:** Raw pixel-by-pixel modeling could achieve 80% ImageNet accuracy and compelling generation within five years with 4-5× annual compute growth

## Executive Summary
This paper investigates the scaling properties of autoregressive next-pixel prediction for unified vision models. The authors train Transformers on 32×32 images using IsoFlops profiles up to 7×10^19 FLOPs, evaluating next-pixel prediction loss, ImageNet classification accuracy, and generation quality via Fréchet Distance. They find that optimal scaling strategy is task-dependent, with generation requiring 3-5× more data than classification. As resolution increases, model size must grow much faster than data size. Surprisingly, compute—not data—is the primary bottleneck. With compute growing 4-5× annually, they project raw pixel-by-pixel modeling could achieve 80% ImageNet accuracy and compelling generation within five years.

## Method Summary
The authors train a family of Transformers on 32×32 images using IsoFlops profiles up to 7×10^19 FLOPs. They evaluate next-pixel prediction loss, ImageNet classification accuracy, and generation quality via Fréchet Distance. The IsoFlops methodology prescribes specific training dataset sizes based on model parameters and FLOPs budgets. Experiments systematically vary model size, dataset size, and compute budget to identify optimal scaling strategies for different vision tasks. The study examines how these relationships change across different IsoFlops profiles and tasks.

## Key Results
- Optimal compute/data allocation varies by task (generation needs 3-5× more data than classification)
- Model size must grow much faster than data size as resolution increases
- Compute—not data—is the primary bottleneck for scaling autoregressive vision models
- With 4-5× annual compute growth, 80% ImageNet accuracy and compelling generation could be achieved within five years

## Why This Works (Mechanism)
None

## Foundational Learning
- **IsoFlops profiles**: A methodology prescribing optimal dataset sizes based on model parameters and compute budget. Needed to systematically study scaling relationships. Quick check: Verify that prescribed dataset sizes scale as (model size)^(3/4).
- **Autoregressive next-pixel prediction**: Sequentially predicting each pixel from left to right, top to bottom. Needed for unified generative modeling. Quick check: Confirm pixel prediction loss correlates with downstream task performance.
- **Fréchet Distance**: A metric measuring similarity between feature distributions for evaluating generation quality. Needed to quantify perceptual quality of generated images. Quick check: Compare Fréchet scores with human perceptual ratings.
- **Compute scaling laws**: Empirical relationships between compute budget, model size, and task performance. Needed to project future capabilities. Quick check: Validate scaling laws hold across multiple orders of magnitude.

## Architecture Onboarding

### Component Map
Pixel Prediction Model -> IsoFlops Profile -> Dataset Size -> Compute Budget -> Task Performance

### Critical Path
The critical path is the relationship between model size, dataset size, and compute budget as prescribed by IsoFlops profiles. The model predicts pixels sequentially, with performance evaluated through next-pixel loss, classification accuracy, and generation quality metrics.

### Design Tradeoffs
The paper identifies that optimal scaling strategies differ by task: generation benefits from larger datasets (3-5× classification) while classification can achieve good performance with less data. However, model size must grow disproportionately faster than dataset size as resolution increases, creating tension between model capacity and data efficiency.

### Failure Signatures
Models following IsoFlops profiles consistently under-train relative to their capacity, with losses plateauing before reaching ideal training thresholds. This suggests that current compute budgets are insufficient for fully utilizing large model capacity, particularly for high-resolution generation tasks.

### First 3 Experiments
1. Verify that next-pixel prediction loss correlates with downstream classification accuracy across different model sizes
2. Test whether generation quality (Fréchet Distance) improves proportionally with the 3-5× data increase suggested for generation tasks
3. Validate that model size must indeed grow faster than dataset size as resolution increases by testing at 64×64 resolution

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on 32×32 resolution may not translate to practical vision models at higher resolutions
- IsoFlops methodology assumes idealized FLOPs utilization that may not hold in real-world implementations
- Data scaling recommendations for generation are derived from next-pixel prediction loss, but relationship to perceptual quality at higher resolutions untested
- Projected timelines depend on sustained compute growth rates that may face economic or technological bottlenecks

## Confidence

| Claim Cluster | Confidence |
|---|---|
| Scaling Strategy Dependence | Medium |
| Compute as Primary Bottleneck | High |
| 5-Year Projections | Low |

## Next Checks
1. **Resolution Scaling Validation**: Conduct controlled experiments at 64×64 and 128×128 resolutions to verify whether the identified scaling relationships hold as image complexity increases, particularly examining whether model size must indeed grow faster than data size at higher resolutions.

2. **Real-World Compute Efficiency**: Implement the IsoFlops-profiles on actual GPU clusters to measure achieved FLOPs utilization and training efficiency, comparing theoretical predictions with practical performance including communication overheads and memory constraints.

3. **Quality vs Loss Correlation**: Establish empirical correlations between next-pixel prediction loss improvements and downstream task performance (classification accuracy, generation quality metrics) at multiple resolution levels to validate whether the chosen loss metric adequately predicts practical utility.