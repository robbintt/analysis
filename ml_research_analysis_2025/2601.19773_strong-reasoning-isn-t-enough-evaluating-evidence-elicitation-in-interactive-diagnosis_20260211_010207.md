---
ver: rpa2
title: 'Strong Reasoning Isn''t Enough: Evaluating Evidence Elicitation in Interactive
  Diagnosis'
arxiv_id: '2601.19773'
source_url: https://arxiv.org/abs/2601.19773
tags:
- information
- diagnosis
- evidence
- clinical
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating and improving
  information collection in interactive medical diagnosis, where agents must actively
  elicit clinical evidence from patients under uncertainty. The authors introduce
  an interactive evaluation framework with a simulated patient and reporter, using
  atomic evidence units to quantify information coverage via the Information Coverage
  Rate (ICR).
---

# Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis

## Quick Facts
- arXiv ID: 2601.19773
- Source URL: https://arxiv.org/abs/2601.19773
- Reference count: 40
- Strong static diagnostic reasoning does not ensure effective interactive evidence gathering in medical diagnosis

## Executive Summary
This paper addresses a critical gap in interactive medical diagnosis: the disconnect between strong diagnostic reasoning capabilities and effective evidence elicitation from patients. The authors introduce a comprehensive framework for evaluating how well diagnostic agents can actively collect clinical evidence under uncertainty. Through their EviMed benchmark and novel REFINE strategy, they demonstrate that interactive information gathering is a distinct bottleneck, even for models with excellent static diagnostic performance. Their findings suggest that improving evidence collection, rather than reasoning alone, is key to advancing practical diagnostic agents.

## Method Summary
The authors develop an interactive evaluation framework using simulated patients and reporters to assess evidence elicitation in medical diagnosis. They introduce atomic evidence units and quantify information coverage through Information Coverage Rate (ICR). The framework evaluates 10 different models on the EviMed benchmark across diverse medical scenarios. To address the identified bottleneck, they propose REFINE, a feedback-driven strategy that uses diagnostic verification to guide evidence collection. REFINE operates by checking whether sufficient evidence has been collected before proceeding with diagnosis, creating a more systematic approach to information gathering.

## Key Results
- Strong static diagnostic reasoning models do not ensure effective interactive information gathering
- REFINE consistently improves both ICR and diagnostic success across different models and configurations
- Smaller diagnostic agents can achieve superior performance when supervised by stronger reasoning models through REFINE

## Why This Works (Mechanism)
The approach works because it addresses the fundamental mismatch between diagnostic reasoning and evidence collection as separate cognitive tasks. While strong reasoning models excel at diagnosis given sufficient evidence, they often lack strategies for systematically eliciting that evidence from patients. REFINE bridges this gap by providing a verification-driven feedback loop that ensures adequate information collection before diagnosis, preventing premature conclusions based on incomplete data.

## Foundational Learning

**Atomic Evidence Units**: The smallest indivisible pieces of clinical information (symptoms, test results, patient history elements). *Why needed*: To create a standardized metric for measuring information coverage. *Quick check*: Can all clinical evidence be decomposed into these atomic units without loss of diagnostic value?

**Information Coverage Rate (ICR)**: A quantitative metric measuring the proportion of relevant evidence units collected during interaction. *Why needed*: To objectively evaluate the effectiveness of evidence elicitation strategies. *Quick check*: Does ICR correlate with actual diagnostic accuracy across different medical scenarios?

**Diagnostic Verification Module**: A component that assesses whether sufficient evidence has been collected before proceeding with diagnosis. *Why needed*: To prevent premature diagnosis based on incomplete information. *Quick check*: How accurately can this module distinguish between sufficient and insufficient evidence across diverse cases?

## Architecture Onboarding

**Component Map**: Patient Simulation -> Evidence Elicitation Agent -> Diagnostic Verification -> Diagnostic Output -> ICR Calculation

**Critical Path**: Evidence elicitation must occur before diagnostic verification, which must complete before final diagnosis. The ICR calculation provides feedback for REFINE optimization.

**Design Tradeoffs**: The framework prioritizes systematic evidence collection over speed of interaction, potentially increasing diagnostic accuracy at the cost of longer patient interactions. The use of simulated patients enables controlled evaluation but may miss real-world communication complexities.

**Failure Signatures**: Models may exhibit premature diagnosis syndrome (diagnosing before adequate evidence collection), evidence tunnel vision (focusing on certain evidence types while missing others), or question redundancy (asking repetitive questions already answered).

**First Experiments**:
1. Compare ICR across models with varying reasoning strengths on identical diagnostic cases
2. Test REFINE's impact on diagnostic success rates with different verification thresholds
3. Evaluate whether REFINE enables smaller models to outperform larger models on evidence collection metrics

## Open Questions the Paper Calls Out

The paper identifies several open questions including how the approach generalizes across different medical specialties, the long-term effectiveness of REFINE in diverse clinical settings, and whether the atomic evidence unit approach captures the full complexity of clinical evidence collection. The authors also note the need to validate findings in real-world clinical trials beyond the simulated environment.

## Limitations

The evaluation framework's reliance on simulated patients may not capture real clinical interaction complexity, particularly regarding patient communication styles and contextual factors. The atomic evidence unit approach may oversimplify nuanced clinical evidence where some information carries different diagnostic weights. The REFINE strategy's effectiveness depends heavily on the diagnostic verification module's accuracy, which could lead to premature termination or excessive questioning if errors occur.

## Confidence

*High Confidence*: The core finding that strong static reasoning doesn't guarantee effective interactive evidence elicitation is well-supported by empirical results across multiple model comparisons.

*Medium Confidence*: The claim that smaller agents can excel when supervised by stronger models through REFINE shows improvement but needs further validation across diverse clinical domains.

*Medium Confidence*: The EviMed benchmark's comprehensiveness and representativeness of real-world diagnostic scenarios is suggested but not definitively established.

## Next Checks

1. Conduct real-world clinical trials comparing REFINE-guided agents against standard diagnostic protocols with actual patients to validate simulated environment findings.

2. Perform ablation studies on the diagnostic verification module to quantify how its accuracy directly impacts REFINE's effectiveness and identify failure modes.

3. Test the approach across multiple medical specialties with varying diagnostic complexity to evaluate whether improvements generalize beyond EviMed scenarios.