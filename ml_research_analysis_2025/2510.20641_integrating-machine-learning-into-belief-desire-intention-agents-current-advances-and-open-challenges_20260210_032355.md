---
ver: rpa2
title: 'Integrating Machine Learning into Belief-Desire-Intention Agents: Current
  Advances and Open Challenges'
arxiv_id: '2510.20641'
source_url: https://arxiv.org/abs/2510.20641
tags:
- agents
- agent
- learning
- beliefs
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the integration of machine learning (ML) into
  BDI agents, a key area of research in autonomous systems. The survey systematically
  reviews 98 primary works, categorizing them by their interaction with BDI modules
  such as sensing, belief revision, belief representation, desire generation, intention
  filtering, planning, and acting.
---

# Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges

## Quick Facts
- arXiv ID: 2510.20641
- Source URL: https://arxiv.org/abs/2510.20641
- Reference count: 40
- Majority of research focuses on Planning and Belief components, with minimal work on Desire Generation and Intention Filtering

## Executive Summary
This survey systematically examines 98 primary works that integrate machine learning (ML) into Belief-Desire-Intention (BDI) agents, revealing significant gaps in the current research landscape. The analysis shows a strong emphasis on using ML for belief representation and planning tasks, particularly through large language models (LLMs), while components like desire generation and intention filtering remain largely unexplored. Most approaches rely on static ML models without online learning capabilities, limiting their adaptability in real-world scenarios. The survey identifies critical challenges around reliability, safety, and the integration of symbolic and subsymbolic reasoning in ML-BDI systems, calling for more robust frameworks that can support practical deployment in complex environments.

## Method Summary
The survey employed a systematic literature review methodology using five academic databases (Google Scholar, Scopus, Springer Link, ACM DL, DBLP) with specific boolean search queries targeting ML-BDI integration. Researchers executed four defined boolean queries combining BDI agent terminology with various machine learning approaches, screening the top five pages of results from each engine. Papers were classified as Primary Works (proposing specific ML-BDI integration), Secondary Works (surveys or discussions), or Unrelated. The selected primary works were then categorized according to their interaction with specific BDI modules including sensing, belief revision, desire generation, intention filtering, planning, and acting. This framework enabled systematic analysis of research trends and gaps across the BDI architecture.

## Key Results
- Research overwhelmingly focuses on Planning (48 papers) and Belief components (44 papers), while Desire Generation (8 papers) and Intention Filtering (2 papers) remain severely underexplored
- Large Language Models dominate current ML-BDI approaches, particularly for belief representation and planning tasks
- Most approaches employ static ML models without online learning capabilities, limiting adaptability in dynamic real-world environments
- Multi-agent BDI systems and explicit intention representation are notably absent from current research efforts

## Why This Works (Mechanism)
The survey's effectiveness stems from its systematic approach to mapping the ML-BDI landscape through rigorous literature screening and categorization. By establishing clear criteria for distinguishing primary research from secondary sources and applying a structured classification framework based on BDI architecture, the survey provides a comprehensive overview of current trends and gaps. The methodology's strength lies in its ability to quantify research distribution across BDI components and identify dominant architectural patterns, particularly the prevalence of LLM-based approaches. This systematic analysis reveals not only what has been done but also what remains unexplored in the integration of machine learning with rational agent architectures.

## Foundational Learning
- **BDI Architecture Components**: Understanding the separation between Beliefs (knowledge state), Desires (goals), and Intentions (committed plans) is crucial for identifying which ML techniques are applied to which agent modules. Quick check: Can you map a given ML-BDI paper to its specific BDI component interactions?
- **Belief Revision vs. Knowledge Enrichment**: Distinguishing between belief updates (information that affects current beliefs) and knowledge enrichment (new information that expands belief base) is essential for proper classification of approaches. Quick check: Does the ML approach modify existing beliefs or add entirely new information?
- **Symbolic vs. Subsymbolic Integration**: Recognizing the challenges of combining neural approaches (LLMs, deep learning) with symbolic BDI reasoning frameworks is key to understanding reliability concerns. Quick check: Does the approach maintain explicit symbolic representations alongside neural components?
- **Online vs. Static Learning**: Identifying whether ML models are updated during agent operation versus trained offline determines the agent's adaptability to changing environments. Quick check: Is there a mechanism for continuous learning or does the model remain fixed after initial training?
- **Plan Verification Methods**: Understanding how to validate generated plans against BDI constraints and real-world feasibility is critical for deployment readiness. Quick check: Does the approach include explicit verification steps before plan execution?

## Architecture Onboarding
- **Component Map**: Sensing -> Belief Manager -> Desire Generator -> Intention Filter -> Planner -> Acting, with ML interventions potentially at each node
- **Critical Path**: The most common implementation path follows: LLM-based Belief Manager → LLM-based Planner → Acting, bypassing explicit Desire Generation and Intention Filtering
- **Design Tradeoffs**: Static ML models offer reliability but lack adaptability; online learning provides adaptability but introduces safety concerns; pure LLM approaches are flexible but may hallucinate
- **Failure Signatures**: Hallucinated plans when LLMs act as planners, inconsistent belief states across time steps, inability to generate new desires beyond initial programming, and lack of multi-agent coordination
- **First Experiments**: 1) Implement a minimal LLM-BDI agent with belief manager and planner, testing consistency across simple planning tasks; 2) Compare static vs. online learning approaches on a dynamic environment benchmark; 3) Evaluate plan verification mechanisms on safety-critical scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The survey does not specify explicit temporal boundaries for the literature search, potentially affecting completeness of coverage
- Classification boundaries between Belief Update and Knowledge Enrichment components may lead to subjective interpretation variations
- Limited number of primary works demonstrate actual deployment or empirical evaluation, constraining reliability assessments
- Emerging areas like multi-agent BDI systems and real-time online learning are underrepresented in the current corpus

## Confidence
- **High confidence** in the characterization of ML-BDI landscape trends, particularly the dominance of Planning and Belief components
- **Medium confidence** in reliability analysis given limited empirical deployment evidence in primary works
- **Low confidence** in completeness of coverage for emerging areas like multi-agent systems and online learning approaches

## Next Checks
1. Execute the exact boolean search strings across specified databases and verify the 98-paper count through independent screening
2. Replicate module classification on a random sample of 20 papers to assess inter-rater reliability for Belief Update vs. Knowledge Enrichment distinctions
3. Implement a minimal baseline LLM-BDI agent following the dominant architectural pattern and test its ability to maintain consistent belief states across simple planning tasks