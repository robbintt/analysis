---
ver: rpa2
title: Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context
arxiv_id: '2503.18283'
source_url: https://arxiv.org/abs/2503.18283
tags:
- point
- sparse
- level
- context
- cloud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of voxel-based point cloud geometry
  compression, particularly for high-bit depth point clouds where limited receptive
  field in sparse convolutions hinders compression performance. The authors propose
  a Space-to-Channel (S2C) context model that shifts from point-wise to channel-wise
  autoregressive prediction, effectively expanding the receptive field without increasing
  kernel size.
---

# Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context

## Quick Facts
- **arXiv ID**: 2503.18283
- **Source URL**: https://arxiv.org/abs/2503.18283
- **Reference count**: 38
- **Primary result**: 3.8% bit savings over state-of-the-art on dense point clouds, 16.4-15.4% savings on sparse LiDAR point clouds

## Executive Summary
This paper addresses the challenge of voxel-based point cloud geometry compression, particularly for high-bit depth point clouds where limited receptive field in sparse convolutions hinders compression performance. The authors propose a Space-to-Channel (S2C) context model that shifts from point-wise to channel-wise autoregressive prediction, effectively expanding the receptive field without increasing kernel size. For dense point clouds and low-level sparse point clouds, they develop a stage-wise S2C model, while for high-level sparse LiDAR point clouds, they introduce a level-wise S2C model with Geometry Residual Coding (GRC) and Residual Probability Approximation (RPA) modules. The method employs spherical coordinates for compact representation and efficient processing of LiDAR point clouds.

## Method Summary
The proposed S2C framework operates in two modes depending on point cloud sparsity. For dense and low-level sparse point clouds, it uses stage-wise S2C with 8 channels per voxel representing sub-voxel occupancy, processed through channel-wise autoregressive prediction. For high-level sparse LiDAR point clouds, it switches to level-wise S2C with GRC for consistent-resolution prediction and RPA with large kernel size for context capture. The method transforms spatial information into channel dimensions to maintain computational efficiency while expanding effective receptive fields.

## Key Results
- Achieves 3.8% bit savings over SparsePCGC on dense point clouds while reducing encoding/decoding time by 20-25%
- For sparse LiDAR point clouds, achieves 16.4% and 15.4% bit savings on KITTI and Ford datasets respectively
- With 8.8-17.2% faster encoding times compared to state-of-the-art methods
- Kernel size ablation shows monotonic improvement from 5 to 11, with size 9 chosen as optimal balance

## Why This Works (Mechanism)

### Mechanism 1: Space-to-Channel Context Transformation
Converting spatial resolution increases to channel dimension expansion preserves sparse convolution receptive field while maintaining computational efficiency for dense and low-level sparse point clouds. Instead of upsampling voxels (which reduces effective receptive field), the method stores occupancy information across 8 channels representing 8 sub-voxels at the coarser resolution. Channel-wise autoregressive prediction replaces point-wise prediction, keeping the number of processed voxels constant across stages.

### Mechanism 2: Geometry Residual Coding for Consistent-Resolution Prediction
Predicting residual positions between consecutive levels avoids resolution increases while reducing prediction state space from 254 to 8, enabling more precise probability estimation. At high levels, assuming each voxel has only one occupied sub-voxel, the method predicts only the residual position (3 bits → 8 states) stored in feature channels without changing coordinates.

### Mechanism 3: Spherical Coordinates with Large-Kernel Residual Probability Approximation
Spherical coordinate representation combined with large-kernel sparse convolutions (size 9) enables earlier GRC application and captures sufficient context for residual prediction in rotating LiDAR data. LiDAR's rotational acquisition creates non-uniform density patterns that spherical coordinates match effectively, reaching target point counts at level 12 vs 15 in Cartesian.

## Foundational Learning

### Concept: Octree-based Point Cloud Representation
- **Why needed here**: The S2C framework operates on multi-level voxelization mirroring octree structure. Understanding how bit-depth determines octree levels and cross-level occupancy prediction works is prerequisite.
- **Quick check question**: Given an 18-bit depth point cloud, how many octree levels exist, and what happens to spatial resolution at each successive level?

### Concept: Sparse Convolution and Receptive Field Dynamics
- **Why needed here**: The core innovation addresses receptive field limitations. Must understand that sparse convolutions operate only on occupied voxels, and why upsampling reduces effective receptive field per unit volume.
- **Quick check question**: Why does a 3×3×3 sparse convolution applied after 8× upsampling have smaller effective receptive field relative to the voxelized space than the same convolution before upsampling?

### Concept: Autoregressive Entropy Coding
- **Why needed here**: Both stage-wise and level-wise S2C use autoregressive probability prediction. Understanding sequential dependency, probability estimation for arithmetic coding, and the difference between point-wise and channel-wise autoregression is necessary.
- **Quick check question**: In point-wise autoregressive decoding (8 stages), what information must be available before predicting the 5th sub-voxel, and how does channel-wise autoregression change this dependency?

## Architecture Onboarding

### Component Map
Point Cloud → [Cartesian→Spherical for LiDAR]
    ↓
Multi-level Voxelization (levels 1-18)
    ↓
┌──────────────────────────────────────────────┐
│ Low Levels: Stage-wise S2C                   │
│  • 8 channels/voxel (sub-voxel mapping)      │
│  • DFA + 1×1×1 sparse conv                   │
│  • Channel-wise AR (8 stages)                │
│  • Feature-to-point reconstruction           │
└──────────────────────────────────────────────┘
    ↓ (switch at point count ≈ original)
┌──────────────────────────────────────────────┐
│ High Levels: Level-wise S2C + GRC            │
│  • RPA with kernel=9, dilated IRN            │
│  • 2-group context (parallel decoding)       │
│  • Residual in channels (no coord change)    │
└──────────────────────────────────────────────┘
    ↓
Arithmetic Encoding → Bitstream

### Critical Path
1. **Resolution boundary detection** (level ~12 spherical): Identify when point count matches original to switch models
2. **Channel update sequence**: Each stage must correctly update relevant channel before next prediction
3. **Residual reconstruction**: Final coordinates = base + Σ(weighted residuals) per Equation 3

### Design Tradeoffs
- **Kernel size vs speed**: Kernel 9 chosen as balance (Table V: larger kernels improve bpp but increase time)
- **Channel count vs capacity**: 32 channels balance information and inference speed (Table VI)
- **Spherical vs Cartesian**: Spherical enables earlier GRC but prevents lossless compression due to precision errors
- **Group parallelism vs context**: 2-group parallel decoding faster than 8-stage serial but may sacrifice some context quality

### Failure Signatures
- **~3 bpp at high levels**: GRC not activating properly; receptive field insufficient
- **Decoder point count mismatch**: Check feature-to-point transformation and channel accumulation
- **Large distant errors in spherical**: Expected due to non-uniform partitioning; may need additional mitigation
- **Wasted channels at sparse levels**: Stage-wise S2C applied where voxels have single children

### First 3 Experiments
1. **Reproduce stage-wise S2C on MPEG 8i**: Target ~3.8% savings vs SparsePCGC, ~40% vs G-PCC (Table I), ~20% encoding speedup (Table II)
2. **Kernel size ablation on Ford**: Vary RPA kernel 5→11, plot bpp vs kernel size, expect monotonic decrease with diminishing returns
3. **Coordinate comparison on KITTI**: Train level-wise S2C in both coordinate systems, verify spherical reaches target points at level 12 vs 15 (Figure 3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the precision error inherent in Spherical coordinate transformations be mitigated to enable lossless compression for the Level-wise S2C model?
- **Basis in paper:** [explicit] The paper states in Section IV-B regarding the Level-wise S2C results: "It can not perform lossless compression in the Spherical coordinate system due to the precision error inherent in the space transformations."
- **Why unresolved:** The Level-wise model relies on Spherical coordinates to achieve compact representation and early GRC application, but the mathematical transformation introduces floating-point or quantization errors that destroy the exact reconstruction required for lossless coding.

### Open Question 2
- **Question:** Can the "ongoing challenge" of receptive field limitations in sparse point clouds be resolved without resorting to computationally expensive large kernel sizes?
- **Basis in paper:** [explicit] In the Ablation Study (Section IV-D), the authors note that performance improves with larger kernels (up to 11) and state: "This observation underscores the voxel-based method's ongoing challenge with the limitation of the receptive field... suggesting potential avenues for further improvement."
- **Why unresolved:** While the proposed RPA module uses a kernel size of 9, the ablation shows that a size of 11 yields even better bit savings. Simply increasing kernel size increases computational cost, implying the optimal balance between context capture and efficiency remains elusive.

### Open Question 3
- **Question:** Is there a dynamic or learnable mechanism to determine the optimal transition point between the Stage-wise and Level-wise S2C models?
- **Basis in paper:** [inferred] The paper manually sets the transition to the Level-wise model at level 12 based on the statistic that point counts match the original cloud around that level (Section IV-A). It also notes that for lossless Cartesian coding (Table III), starting at level 15 vs 16 changes the bitrate distribution significantly.
- **Why unresolved:** The system relies on a heuristic or fixed level to switch strategies. A fixed level might not be optimal for LiDAR scans with varying densities or sparsity patterns (e.g., different sensors or environments).

## Limitations
- The spherical coordinate transformation prevents lossless compression due to precision errors, limiting applicability to scenarios requiring exact reconstruction
- The GRC assumption (single child per voxel at high levels) may not hold for non-LiDAR point clouds with different density distributions
- The method's performance gain over existing techniques may diminish for point clouds with different acquisition patterns or density distributions

## Confidence
- **High**: Stage-wise S2C context transformation for dense and low-level sparse point clouds
- **Medium**: GRC effectiveness for high-level LiDAR point clouds
- **Medium**: Spherical coordinate advantage
- **Low**: Generalizability to non-LiDAR rotating sensor data and different point cloud acquisition methods

## Next Checks
1. **Dataset Diversity Testing**: Apply the method to non-rotating sensor point clouds (e.g., static depth cameras) and synthetic point clouds to verify GRC and spherical coordinate assumptions hold beyond KITTI and Ford LiDAR datasets
2. **Precision Error Analysis**: Quantify and visualize reconstruction errors introduced by spherical coordinate transformation across different distance ranges to establish safe operational boundaries
3. **Computational Complexity Profiling**: Measure memory consumption and inference time scaling with increasing point cloud size and bit depth to identify practical limits for real-time applications