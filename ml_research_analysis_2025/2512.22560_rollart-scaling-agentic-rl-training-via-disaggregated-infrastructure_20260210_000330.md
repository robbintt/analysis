---
ver: rpa2
title: 'RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure'
arxiv_id: '2512.22560'
source_url: https://arxiv.org/abs/2512.22560
tags:
- training
- rollout
- environment
- wang
- gpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RollArt is a system designed to improve the efficiency of agentic
  reinforcement learning (RL) training for large language models (LLMs) on disaggregated
  infrastructure. Agentic RL workloads are highly heterogeneous, combining compute-intensive
  prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations,
  making efficient training challenging on monolithic clusters.
---

# RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure

## Quick Facts
- arXiv ID: 2512.22560
- Source URL: https://arxiv.org/abs/2512.22560
- Authors: Wei Gao; Yuheng Zhao; Tianyuan Wu; Shaopan Xiong; Weixun Wang; Dakai An; Lunxi Cao; Dilxat Muhtar; Zichen Liu; Haizhou Zhao; Ju Huang; Siran Yang; Yongbin Li; Wenbo Su; Jiamang Wang; Lin Qu; Bo Zheng; Wei Wang
- Reference count: 40
- One-line primary result: 1.35-2.05× end-to-end training time reduction vs monolithic and synchronous baselines on 3,000+ GPU cluster

## Executive Summary
RollArt is a system designed to improve the efficiency of agentic reinforcement learning (RL) training for large language models (LLMs) on disaggregated infrastructure. Agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations, making efficient training challenging on monolithic clusters. RollArt addresses this by leveraging disaggregated infrastructure and implementing three core design principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to best-fit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components to serverless infrastructure for elastic scaling. The system achieves 1.35-2.05× end-to-end training time reduction compared to monolithic and synchronous baselines and demonstrates scalability and robustness by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs.

## Method Summary
RollArt is a system for efficient agentic RL training on disaggregated infrastructure. It uses hardware-affinity mapping to route compute-bound prefill tasks to H800 GPUs and bandwidth-bound decoding to H20 GPUs. Fine-grained asynchrony at the trajectory level overlaps LLM generation with environment simulation to mask straggler effects. Stateless components like reward models are offloaded to serverless infrastructure to maximize GPU utilization. The system employs an asynchronous training loop with configurable staleness bounds and uses Mooncake for cross-cluster weight synchronization over RDMA/Ethernet. ROLL is trained using GRPO with batch size 512 and group size 8 on Qwen3 models (8B/14B/32B) across environments like SWE-bench, WebShop, FrozenLake, GEM-math, and GEM-game.

## Key Results
- 1.35-2.05× end-to-end training time reduction vs monolithic and synchronous baselines
- Achieves 88% GPU utilization for rollout stage vs 6% with dedicated GPU workers
- Scales to train hundreds-of-billions-parameter MoE model on 3,000+ GPU cluster
- Reduces rollout time from 158s to 77s through serverless reward worker offloading

## Why This Works (Mechanism)

### Mechanism 1: Hardware-Affinity Workload Mapping
- Claim: Mapping specific agentic RL phases to specialized hardware (compute-optimized vs. bandwidth-optimized GPUs) reduces step time compared to homogeneous allocation.
- Mechanism: The system distinguishes between compute-bound phases (e.g., long-context prefill in FrozenLake) and bandwidth-bound phases (e.g., decoding in GEM-Math). It routes prefill-heavy trajectories to high-TFLOPS devices (e.g., H800) and decoding-heavy tasks to high-memory-bandwidth devices (e.g., H20), optimizing for the specific bottleneck of each task.
- Core assumption: Agentic workloads are heterogeneous and cannot be efficiently served by a single hardware profile; the overhead of routing and data transfer is lower than the performance gain from specialization.
- Evidence anchors:
  - [abstract] Mentions "hardware-affinity workload mapping to route tasks to best-fit GPUs."
  - [section 3.1] Figure 4 shows prefill-heavy FrozenLake runs faster on H800 (0.53x H20 time), while decode-heavy GEM-Math runs faster on H20 (0.49x-0.79x H800 time).
  - [corpus] StreamRL [2504.15930] validates the general approach of disaggregating generation and training across heterogeneous resources for efficiency.
- Break condition: If data transfer overhead between disaggregated GPU pools (e.g., over Ethernet) exceeds the compute savings, or if workloads suddenly shift to a uniform profile, the benefit degrades.

### Mechanism 2: Fine-Grained Asynchrony (Trajectory-Level)
- Claim: Managing execution at the trajectory level rather than the batch level prevents "straggler" environments from stalling the entire pipeline, improving GPU utilization.
- Mechanism: Instead of waiting for a full batch of environments to finish (batched interaction), the system uses an event loop where LLM generation for one trajectory overlaps with environment steps for another. This creates a continuous pipeline that masks long-tail latencies (e.g., container initialization failures).
- Core assumption: Environment interactions and LLM generations have high variance in latency; the system can effectively pipeline these without introducing excessive synchronization complexity.
- Evidence anchors:
  - [abstract] States "fine-grained asynchrony... manages execution at the trajectory level to mitigate resource bubbles."
  - [section 3.1] Figure 5b illustrates how batched interactions force fast environments to wait for slow ones.
  - [corpus] Explicit corpus support for "trajectory-level" granularity specifically is limited in the provided neighbors; validation primarily relies on the paper's internal profiling (Figure 12a).
- Break condition: If the overhead of managing thousands of independent event loops (context switching/coordination) exceeds the latency hiding benefits, throughput may drop.

### Mechanism 3: Statefulness-Aware Computation (Serverless Offload)
- Claim: Offloading stateless components like Reward Models to serverless infrastructure maximizes expensive GPU utilization for core training and rollout.
- Mechanism: The system identifies components that do not require persistent state (Reward Workers) and moves them to Function Compute (serverless). This allows these resources to scale to zero when idle, freeing up dedicated GPUs for the stateful Training and Rollout stages.
- Core assumption: The latency of serverless cold-starts or network calls for reward computation is lower than the opportunity cost of keeping dedicated GPUs idle.
- Evidence anchors:
  - [abstract] Mentions "statefulness-aware computation which offloads stateless components... to serverless."
  - [section 7.5] Figure 14 shows GPU utilization increasing from 6% (dedicated) to 88% (serverless shared) and rollout time dropping from 158s to 77s.
  - [corpus] MegaFlow [2601.07526] and RLBoost [2510.19225] support the general need for sophisticated orchestration and resource harvesting, though specific serverless offload mechanisms are less detailed.
- Break condition: If the reward model becomes stateful or requires complex shared memory access, or if serverless latency spikes unpredictably, the training loop may starve.

## Foundational Learning

### LLM Inference Phases (Prefill vs. Decoding)
- Why needed here: Understanding the difference between compute-heavy prefill and memory-bandwidth-heavy decoding is essential to configure the hardware-affinity mapping (Mechanism 1).
- Quick check question: Does a task with long context windows but few output tokens likely prefer compute-optimized or bandwidth-optimized GPUs?

### Asynchronous RL Training
- Why needed here: Comprehending the trade-off between "freshness" of gradients (staleness) and throughput is required to tune the "Asynchronous Bound" and manage the disaggregated training loop.
- Quick check question: What is the risk of setting the asynchronous bound too high in terms of model convergence?

### Disaggregated Infrastructure
- Why needed here: Users must understand that different compute pools (Training Cluster, Inference Cluster, CPU Cluster) are connected via heterogeneous networks (NVLink vs. Ethernet) to debug communication bottlenecks.
- Quick check question: Why does weight synchronization between training and rollout clusters require a specialized transfer engine like Mooncake?

## Architecture Onboarding

### Component map:
User Layer -> Control Plane -> Runtime -> Resource Plane
Python decorators (@rdist.register, @rdist.hw_mapping) -> Cluster abstraction managing Workers (Train, Gen, Reward) -> Rollout Scheduler managing EnvManagers and LLMProxy -> Resource Manager allocating hardware and serverless endpoints

### Critical path: The Rollout-Training Loop
1. *Rollout:* EnvManager requests action from LLMProxy -> InferenceWorker generates -> EnvManager steps environment
2. *Reward:* Completed trajectory -> Serverless Reward Worker
3. *Buffer:* Trajectory pushed to SampleBuffer
4. *Train:* TrainWorker pulls batch -> computes gradients -> updates weights -> LLMProxy pulls weights (Async)

### Design tradeoffs:
- **Consistency vs. Throughput:** A lower "Asynchronous Bound" (e.g., 1) ensures fresher gradients but may abort more straggler trajectories, potentially lowering throughput compared to a higher bound.
- **Cost vs. Complexity:** Using H20 GPUs for decoding saves cost but introduces cross-cluster communication overhead when syncing weights from H800 training clusters.

### Failure signatures:
- **Straggler Lockup:** If EnvManager is not truly asynchronous, a single frozen Docker container stalls the entire batch.
- **Staleness Collapse:** If the training step is faster than rollout generation, the SampleBuffer empties, causing get_batch to block (observed in Section 8 as 62% of iteration time).
- **Serverless Latency Spikes:** If serverless reward workers experience cold-starts or network congestion, the rollout pipeline may starve.

### First 3 experiments:
1. **Hardware Affinity Validation:** Run FrozenLake and GEM-Math on isolated H800 vs. H20 pools to reproduce the throughput delta (Section 3.1) before enabling dynamic mapping.
2. **Straggler Injection:** Inject artificial delay into 10% of environments and measure the delta between Batch-mode and Trajectory-mode execution to quantify the "resource bubble" effect.
3. **Serverless Latency Profiling:** Measure the round-trip time for a reward request via Function Compute vs. a dedicated GPU worker to ensure the "stateless" assumption holds for your specific reward model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the disaggregation of prefill and decoding phases be automated to prevent load imbalance?
- Basis in paper: [explicit] Section 5.2 states that real deployments "currently require manual configuration of prefill and decoding instances, which easily leads to load imbalance. We hence leave it as future work."
- Why unresolved: The current implementation relies on static, manual assignment of tasks to H800 (prefill-heavy) or H20 (decode-heavy) GPUs, lacking a runtime mechanism to adapt to changing workload characteristics.
- What evidence would resolve it: An adaptive scheduler that dynamically profiles token generation rates and KV-cache requirements to route phases in real-time without user intervention.

### Open Question 2
- Question: Can the asynchronous bound (α) be dynamically adjusted to optimize the trade-off between training stability and throughput?
- Basis in paper: [inferred] Section 7.4 notes that the "optimal bound differs across LLMs" and affects convergence speed, yet the system relies on a static configuration (e.g., bound=1) determined by empirical tuning.
- Why unresolved: A static bound cannot adapt to the varying latency spikes of heterogeneous environments or the changing gradient stability requirements during different training phases.
- What evidence would resolve it: A control-loop mechanism or meta-learner that adjusts α in real-time based on observed gradient variance and rollout latency distributions.

### Open Question 3
- Question: What mechanisms can optimally balance the throughput ratio between the rollout and training stages to prevent stalls?
- Basis in paper: [inferred] Section 8 identifies that the training stage stalls while waiting for the SampleBuffer to fill, consuming "up to 62% of the iteration time with GPU idleness."
- Why unresolved: The current system relies on manual "characterization-driven optimization" to set resource ratios (e.g., 1:5 training-to-generation), which is static and reactive rather than proactive.
- What evidence would resolve it: An autoscaling policy that monitors buffer depth and dynamically reallocates resources (e.g., scaling serverless reward workers or inference GPUs) to minimize the get_batch blocking time.

## Limitations
- Hardware affinity mapping assumes predictable workload profiles that may not hold for real-world agentic RL tasks with shifting compute/bandwidth requirements
- Trajectory-level asynchrony introduces gradient staleness that could harm convergence stability, though paper reports no degradation
- Serverless offload is validated only for reward computation; scaling to other stateless components could reveal hidden coordination overheads
- Production deployment uses 3,000+ GPUs but scalability ceiling under peak load is not explored

## Confidence

### Mechanism Confidence Mapping
- **High confidence** in hardware affinity mechanism, supported by direct microbenchmarks (Figure 4) and aligned with prior work (StreamRL)
- **Medium confidence** in trajectory-level asynchrony, as paper provides strong profiling data but lacks ablation studies on staleness impact or alternative granularity choices
- **Low confidence** in serverless offload generalization, given narrow scope (only reward workers) and absence of serverless failure mode analysis

## Next Checks

1. **Ablation on Asynchronous Bound**: Systematically vary the async_bound parameter (1, 2, 4, 8) and measure both throughput and final validation score to quantify the staleness-convergence trade-off.

2. **Workload Shift Stress Test**: Design a mixed-phase benchmark where individual trajectories alternate between compute- and bandwidth-bound behavior mid-execution, and measure the decay in hardware affinity gains.

3. **Serverless Overhead Scaling**: Offload additional stateless components (e.g., policy embedding generation) to serverless and profile cold-start latency and network contention as the number of concurrent requests increases.