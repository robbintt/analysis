---
ver: rpa2
title: Multi-layer Stack Ensembles for Time Series Forecasting
arxiv_id: '2511.15350'
source_url: https://arxiv.org/abs/2511.15350
tags:
- forecasting
- time
- linear
- series
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of 33 ensemble methods
  for time series forecasting across 50 real-world datasets. The authors demonstrate
  that stacking consistently improves forecast accuracy over individual models and
  simple averaging, with multi-layer stacking providing superior performance by combining
  multiple stacker models.
---

# Multi-layer Stack Ensembles for Time Series Forecasting

## Quick Facts
- arXiv ID: 2511.15350
- Source URL: https://arxiv.org/abs/2511.15350
- Reference count: 40
- Primary result: Multi-layer stacking improves forecast accuracy with up to 5% error reduction and 200 Elo points improvement

## Executive Summary
This paper systematically evaluates 33 ensemble methods for time series forecasting across 50 real-world datasets, demonstrating that stacking consistently improves forecast accuracy over individual models and simple averaging. The authors introduce a two-level stacking framework: L2 models combine base forecasts, and an L3 model optimally aggregates the L2 outputs. Results show that multi-layer stacking particularly excels when strong individual forecasting models are unavailable, challenging the "forecast combination puzzle" by achieving up to 5% error reduction compared to baselines.

## Method Summary
The paper presents a systematic evaluation of ensemble methods using a multi-layer stack ensemble approach for time series forecasting. The framework consists of three layers: L1 base models (11 models including Chronos-Bolt, DeepAR, PatchTST, TFT) trained with 5-fold time-series cross-validation, L2 stackers (14 models including Linear variants, LightGBM, Greedy Ensemble) trained on out-of-fold predictions, and an L3 aggregator (Greedy Ensemble) trained on L2 outputs. The method uses Scaled Quantile Loss for probabilistic forecasting and Mean Absolute Scaled Error for point forecasting, evaluated across 50 univariate datasets with performance aggregated via Elo ratings and average rank.

## Key Results
- Multi-layer stacking consistently outperforms individual models and simple averaging across 50 datasets
- Up to 5% error reduction and 200 Elo points improvement compared to baseline methods
- Method is robust to L1 model choice and particularly effective when strong individual forecasting models are unavailable
- The approach challenges the "forecast combination puzzle" by demonstrating that optimal combination can yield significant improvements

## Why This Works (Mechanism)
The method works by creating a hierarchical aggregation system that learns optimal combinations of multiple forecasting models. The first layer (L2) learns how to combine base model predictions through various stacker architectures, while the second layer (L3) learns how to optimally aggregate these intermediate combinations. This multi-level approach allows the system to capture both the strengths of individual models and the synergistic relationships between them, effectively overcoming the limitations of simple averaging or single-model approaches.

## Foundational Learning
- **Time-series cross-validation**: Why needed - to generate reliable out-of-fold predictions for training stackers; Quick check - verify K-fold predictions use only past data
- **Scaled Quantile Loss (SQL)**: Why needed - to evaluate probabilistic forecasts while accounting for dataset scale; Quick check - confirm SQL calculations use correct scaling factors
- **Elo ratings**: Why needed - to aggregate performance across diverse datasets into a single comparable metric; Quick check - verify Elo updates follow proper rating system mechanics
- **Greedy ensemble selection**: Why needed - to efficiently select optimal model combinations from a large pool; Quick check - ensure S=100 parameter is consistently applied
- **Out-of-fold predictions**: Why needed - to prevent data leakage when training stackers; Quick check - confirm training inputs are strictly from earlier time windows

## Architecture Onboarding

Component map: L1 base models → L2 stackers → L3 aggregator

Critical path: Base models generate OOF predictions → L2 stackers train on OOF → L3 trains on L2 outputs → Final predictions

Design tradeoffs: The multi-layer approach provides superior accuracy but increases computational complexity and inference latency due to requiring predictions from all base models for stacker evaluation

Failure signatures: 
- Data leakage between folds leading to overfitting
- Stale L2 weights when skipping final retraining on all validation windows
- Underperformance when insufficient validation data exists for L3 training

First experiments:
1. Verify out-of-fold prediction generation with proper time-series splitting
2. Train a single L2 stacker (e.g., Linear model) and validate on held-out fold
3. Compare final performance with and without L3 aggregator to quantify its contribution

## Open Questions the Paper Calls Out
1. How can pruning strategies be designed to reduce the inference latency of multi-layer stack ensembles without significantly degrading forecast accuracy? The authors note that most stacker models require predictions from all base models, creating computational bottlenecks.

2. Can offline portfolio optimization or meta-learning approaches reduce the substantial training costs associated with training multiple L2 stacker models? The current implementation trains a fixed set of 14 L2 models for every dataset, which is computationally expensive.

3. What alternative aggregation models for the L2 and L3 layers can consistently outperform the current best performers, such as greedy ensemble selection and linear models? The search for better aggregation models remains an important challenge.

## Limitations
- Substantial computational overhead from training 14 L2 models and requiring all base model predictions at inference
- Missing implementation details for exact hyperparameters of Linear stackers and LightGBM normalization
- Performance gains depend on sufficient validation data availability for training the L3 aggregator

## Confidence
- High confidence in core methodology and systematic evaluation approach across 50 diverse datasets
- Medium confidence in specific magnitude of improvements due to missing implementation details and hyperparameter specifications
- Medium confidence in robustness claims regarding L1 model choice, as not exhaustively tested across different model families

## Next Checks
1. Implement and test the exact OOF prediction generation pipeline to verify no data leakage between training and validation windows
2. Recreate the L2 stacker training using only the first 4 folds and validate that the 5th fold predictions are used exclusively for L3 training
3. Compare final test performance with and without retraining L2 models on all K validation windows to confirm the impact on forecast accuracy