---
ver: rpa2
title: Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation
  on Reflective Surfaces
arxiv_id: '2503.22209'
source_url: https://arxiv.org/abs/2503.22209
tags:
- depth
- image
- distillation
- ours
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first end-to-end approach that leverages
  intrinsic image decomposition to address the challenges posed by reflective surfaces,
  which are known to compromise depth accuracy in self-supervised monocular depth
  estimation (SSMDE). We introduce a new method for localizing the pixel-level reflective
  regions without the need for additional labeled annotation by using estimated intrinsic
  components.
---

# Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces

## Quick Facts
- arXiv ID: 2503.22209
- Source URL: https://arxiv.org/abs/2503.22209
- Reference count: 25
- Primary result: First end-to-end approach using intrinsic image decomposition to improve self-supervised monocular depth estimation on reflective surfaces

## Executive Summary
This paper addresses the fundamental challenge of depth estimation in reflective surfaces for self-supervised monocular depth estimation (SSMDE). The method introduces a novel intrinsic image decomposition framework that separates diffuse and residual components to identify and exclude reflective regions from depth learning. By leveraging a dual-branch architecture that jointly estimates depth and intrinsic components, the approach significantly improves depth accuracy on reflective surfaces while maintaining competitive performance with methods requiring 3D knowledge distillation.

## Method Summary
The method employs a dual-branch architecture where a shared encoder feeds into separate depth and intrinsic decoders. The intrinsic branch decomposes input images into diffuse (L) and residual (R) components using a log-space formulation (log I = log L + log R). Reflective regions are identified through a Mahalanobis distance comparison between photometric errors of the original image and the pseudo-diffuse image. A reflection-aware mask M_R excludes these regions from the depth photometric loss, preventing erroneous depth learning from specular reflections. The framework includes three intrinsic losses (reconstruction, cross-reconstruction, and contrastive) and is trained end-to-end with the depth branch.

## Key Results
- Significantly outperforms conventional self-supervised approaches on indoor datasets with reflective surfaces
- Achieves competitive performance compared to methods using 3D knowledge distillation
- Demonstrates robust depth estimation on challenging reflective surfaces in ScanNetv2, 7-Scenes, and Booster datasets
- Shows that the proposed method requires less training cost than 3D knowledge distillation approaches

## Why This Works (Mechanism)

### Mechanism 1: Isolation of View-Dependent Artifacts via Intrinsic Decomposition
The framework isolates reflective surface errors by decomposing the image into a view-independent "diffuse" component and a view-dependent "residual" component. Standard SSMDE fails because the photometric loss assumes scene brightness is constant (Lambertian). Reflective surfaces violate this. By enforcing an intrinsic residual model (log I = log L + log R), the network learns to push the unstable, view-dependent lighting effects (specularities) into the Residual (R) while keeping the stable texture/color in the Diffuse (L). The core assumption is that the residual component varies with camera viewpoint, whereas the diffuse component remains static relative to the surface geometry.

### Mechanism 2: Gradient Exclusion via Residual-Aware Masking
The method prevents depth hallucination by identifying pixels where the RGB photometric error significantly exceeds the Diffuse photometric error and excluding them from training. The network calculates photometric error for both the raw image (E_I) and the pseudo-diffuse image (E_L). If E_I >> E_L (statistically measured via Mahalanobis distance), it implies the error was caused by the removed residual (reflection). A binary mask M_R is generated to zero out the gradient at these pixels, preventing the network from trying to "explain" a reflection with depth geometry. The core assumption is that photometric errors caused by reflections are reducible by removing the residual component; errors caused by geometry are not.

### Mechanism 3: Synergistic Depth-Intrinsic Feedback Loop
Accurate depth estimation enables multi-view intrinsic consistency, which in turn improves depth accuracy, creating a positive feedback loop. The "Cross-reconstruction Loss" (L_cross) warps the source diffuse image to the reference view using the estimated depth. If the depth is accurate, the warped diffuse image matches the reference diffuse image (Diffuse Consistency). This forces the depth branch to produce accurate geometry to satisfy the intrinsic loss, while the intrinsic branch provides the clean signals for the depth loss. The core assumption is that the initial depth estimation is sufficient to roughly align views for the intrinsic loss to begin converging.

## Foundational Learning

- **Lambertian vs. Specular Reflection**: Understanding that standard SSMDE relies entirely on the Lambertian assumption (brightness consistency) is the prerequisite to understanding why the paper introduces intrinsic decomposition. *Quick check*: Why does a standard photometric loss fail on a mirror surface when the camera moves?

- **Intrinsic Image Decomposition (Retinex Theory)**: The paper reformulates the input image I into L (Diffuse) and R (Residual). You must understand that L represents "true" surface color and R represents lighting/view effects to interpret the network outputs. *Quick check*: In the paper's formulation (log I = log L + log R), which component holds the specular highlight?

- **Self-Supervised View Synthesis (Warping)**: The core engine of this method is warping a source image to a reference view using depth and pose. The intrinsic branch uses this exact mechanism to verify diffuse consistency. *Quick check*: What two pieces of information are required to warp pixel (u, v) from a source view to a reference view?

## Architecture Onboarding

- **Component map**: Shared Encoder -> Depth Decoder (outputs D_r) and Intrinsic Decoder (outputs L_r, R_r) -> Pose Network (outputs relative transformation) -> Warping -> Mask Generation (M_R) -> Loss Calculation

- **Critical path**: Image I is fed to Encoders. Depth Decoder outputs D. Intrinsic Decoder outputs L and R. **Warping**: D and Pose are used to warp I_s and L_s to the reference frame (I_s2r, L_s2r). **Masking**: Compare photometric error of I vs L. Generate mask M_R. **Loss**: Apply M_R to the standard photometric loss to update weights.

- **Design tradeoffs**: End-to-End vs. Distillation: The paper notes that simple masking might lose high-frequency detail. They propose a distillation step (Teacher fusion) as an optimization, but the core contribution is the end-to-end joint training. Contrastive Loss (λ_cts): Essential to prevent the trivial solution where the network outputs a blank diffuse image. If omitted (Table 5), performance degrades.

- **Failure signatures**: "White Diffuse" Collapse: The Diffuse output L turns into a uniform color, and R contains the entire image structure. Check the contrastive loss weight (λ_cts). Over-masking: The mask M_R covers the entire image. Check the Mahalanobis distance threshold (δ) or the initialization of the intrinsic decoder. Depth Holes: Reflective surfaces appear as "infinite holes" (black depth). This indicates the masking is working, but the network has no signal to fill the gap (expected baseline behavior; the paper reduces this).

- **First 3 experiments**: Static Scene Decomposition: Train only the intrinsic branch on a static indoor scene (fixed pose, varying time/lighting) to verify L and R separate correctly before adding the complexity of depth/pose. Ablate Contrastive Loss: Train the full model with λ_cts = 0. Visualize the Diffuse image. If it is white or uniform, confirm the need for the contrastive term as per Table 5. Reflective Masking Visualization: Run inference on a scene with a known mirror. Visualize the mask M_R to confirm it activates specifically on the mirror surface and not on the surrounding Lambertian walls.

## Open Questions the Paper Calls Out

### Open Question 1
Can the intrinsic residual framework be extended to accurately estimate depth for "hard-case" non-Lambertian objects, specifically transparent surfaces or mirrors? The current method localizes reflective regions to exclude corrupted gradients, but it does not explicitly model the complex light transport physics required to solve for the geometry of transparent or mirror surfaces.

### Open Question 2
Does implementing a 3-channel residual component improve depth estimation performance compared to the current grayscale residual? The authors note they use a grayscale residual for stability, but "based on dichromatic modeling... a three-channel configuration for the residual could potentially be more effective, considering the chromatic differences."

### Open Question 3
To what extent does incorporating established intrinsic priors (e.g., sparsity, smoothness) into the loss function enhance the accuracy of the joint decomposition and depth estimation? The current method relies on reconstruction and contrastive losses but does not utilize the specialized priors often found in standalone intrinsic decomposition literature.

## Limitations
- Depends critically on the quality of intrinsic decomposition - if the network learns to trivially minimize the loss by collapsing the diffuse image to a uniform color, the entire mechanism fails
- Assumes the residual component is primarily view-dependent while the diffuse is view-independent, which may not hold perfectly for complex real-world materials
- While handling specular highlights, the method does not explicitly model the complex light transport physics required for transparent or mirror surfaces

## Confidence

- **High confidence**: The core mechanism of using intrinsic decomposition to separate Lambertian and specular components is theoretically sound and supported by the paper's ablation studies (Table 5 shows contrastive loss is essential)
- **Medium confidence**: The Mahalanobis distance approach for reflection detection is well-motivated but depends on proper threshold selection (δ=5.0) and may struggle with complex lighting scenarios
- **Low confidence**: The paper claims competitive performance with 3D knowledge distillation methods while requiring less training cost, but the comparison methodology and training cost analysis are not fully detailed

## Next Checks

1. **Ablation study validation**: Replicate the contrastive loss ablation (λ_cts=0) to confirm that removing this term causes the intrinsic decomposition to fail (diffuse becomes uniform/white)

2. **Mask visualization**: Generate and visualize the reflection mask M_R on test scenes with known mirrors/reflections to verify it correctly identifies only the reflective regions

3. **Cross-dataset robustness**: Test the method on non-indoor datasets (e.g., outdoor scenes with water surfaces) to assess generalization beyond the ScanNet/7-Scenes domains used in evaluation