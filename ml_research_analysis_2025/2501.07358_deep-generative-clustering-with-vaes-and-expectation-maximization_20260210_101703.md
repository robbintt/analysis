---
ver: rpa2
title: Deep Generative Clustering with VAEs and Expectation-Maximization
arxiv_id: '2501.07358'
source_url: https://arxiv.org/abs/2501.07358
tags:
- clustering
- data
- cluster
- each
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel deep clustering method that integrates
  Variational Autoencoders (VAEs) with the Expectation-Maximization (EM) algorithm.
  The approach models each cluster's distribution with a separate VAE and alternates
  between updating VAE parameters via ELBO maximization and refining cluster assignments.
---

# Deep Generative Clustering with VAEs and Expectation-Maximization

## Quick Facts
- arXiv ID: 2501.07358
- Source URL: https://arxiv.org/abs/2501.07358
- Reference count: 26
- Achieves up to 94.6% clustering accuracy on MNIST using VAEs with EM algorithm

## Executive Summary
This paper proposes a novel deep clustering method that integrates Variational Autoencoders (VAEs) with the Expectation-Maximization (EM) algorithm. The approach models each cluster's distribution with a separate VAE and alternates between updating VAE parameters via ELBO maximization and refining cluster assignments. Unlike existing VAE-based methods, this approach eliminates the need for a Gaussian Mixture Model prior or additional regularization techniques. Experiments on MNIST and FashionMNIST datasets demonstrate superior clustering performance compared to state-of-the-art VAE-based methods.

## Method Summary
The method alternates between E-step and M-step of the EM algorithm. In the E-step, each sample is assigned soft cluster memberships based on softmax-transformed ELBO values computed by K cluster-specific VAEs. The M-step updates each VAE's parameters by maximizing a weighted ELBO loss, where weights are the soft assignments from the E-step. The framework uses 10 MC samples for ELBO estimation in the E-step and 1 sample in the M-step. VAEs have encoder/decoder architecture of 784→500→20→500→784 with LeakyReLU(0.2) activations and dropout of 0.2.

## Key Results
- Achieves 94.6% clustering accuracy on MNIST and 58.9% on FashionMNIST
- Outperforms state-of-the-art VAE-based clustering methods on both datasets
- Enables generation of new samples from each learned cluster distribution
- Demonstrates stable training with lower variance than baseline methods (±2.9 vs ±4.0-5.8 on FashionMNIST)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating ELBO optimization enables joint clustering and distribution learning without GMM priors or regularization.
- **Mechanism:** The E-step assigns soft cluster memberships via softmax(ELBO values), converting intractable posterior inference into tractable closed-form. The M-step trains each cluster-specific VAE using weighted ELBO loss, where weights are soft assignments from E-step. This decouples cluster assignment from distribution modeling.
- **Core assumption:** Data follows a mixture model with uniform mixing coefficients π_k = 1/K, and ELBO sufficiently approximates log-likelihood for discriminative assignment.
- **Evidence anchors:**
  - [abstract] "alternates between updating model parameters by maximizing the Evidence Lower Bound (ELBO)... and refining cluster assignments based on the learned distributions"
  - [section 3, Eq. 3.4] Optimization objective explicitly uses ELBO(α_k; x_i) weighted by u_i,k
  - [corpus] Related work on federated EM (arxiv:2601.21160) confirms EM's effectiveness for mixture models with unknown components, though corpus lacks direct validation of ELBO-as-proxy approach
- **Break condition:** If ELBO poorly correlates with true log-likelihood (e.g., posterior collapse), assignments become noisy and clusters may merge. High variance in ELBO estimates across MC samples (>15% relative) indicates instability.

### Mechanism 2
- **Claim:** Per-cluster VAEs eliminate need for GMM prior by directly modeling each cluster's distribution in data space.
- **Mechanism:** Rather than imposing Gaussian structure in latent space (as VaDE/GMVAE do), each VAE independently learns its cluster's generative process. The decoder p_θ(x|z) captures cluster-specific data distributions without architectural constraints on latent geometry.
- **Core assumption:** K VAEs with separate parameters can specialize to distinct data modes without collapsing to similar solutions; cluster assignments provide sufficient supervision signal.
- **Evidence anchors:**
  - [abstract] "eliminates the need for a Gaussian Mixture Model (GMM) prior or additional regularization techniques"
  - [section 3] "we now propose modeling the log-probability function log(p_αk(·)) for each cluster k using K distinct VAEs"
  - [corpus] Weak direct evidence—corpus papers on VAE variants don't specifically validate per-cluster specialization; assumption remains plausible but unverified
- **Break condition:** If multiple VAEs converge to similar decoders (measure by decoder weight cosine similarity >0.9), clusters lack meaningful separation and the method reduces to K redundant models.

### Mechanism 3
- **Claim:** Soft assignment weighting prevents hard assignment errors from propagating into VAE training.
- **Mechanism:** Rather than assigning each sample to a single VAE (hard EM), soft assignments u_i,k create weighted gradients. Samples with uncertain membership contribute partially to multiple VAEs, allowing gradual refinement as VAEs specialize.
- **Core assumption:** The entropy regularization term ∑u_i,k log(u_i,k) in Eq. 3.3 prevents assignment collapse to single clusters prematurely.
- **Evidence anchors:**
  - [section 3] Objective includes -∑u_i,k log(u_i,k) which encourages soft assignments
  - [section 4.2] Our method shows lower variance than baselines on FashionMNIST (±2.9 vs ±4.0-5.8), suggesting soft assignments stabilize training
  - [corpus] Assumption: No corpus papers directly test soft vs hard assignment in this context
- **Break condition:** If u_i,k becomes near-binary (>0.95 on single cluster) early in training, VAEs receive insufficient gradient signal from boundary samples and may overfit to initial assignment noise.

## Foundational Learning

- **Concept: Evidence Lower Bound (ELBO)**
  - Why needed here: ELBO replaces intractable log p(x) throughout the algorithm—it defines cluster assignments (E-step) and VAE training objectives (M-step). Understanding its reconstruction + KL decomposition is essential.
  - Quick check question: Given a VAE with reconstruction loss 150 and KL divergence 20, what is the ELBO? (Answer: -170, since ELBO = -recon_loss - KL)

- **Concept: Softmax for Probability Simplex Projection**
  - Why needed here: The E-step must convert K ELBO scores into valid probability distribution over clusters. Softmax naturally maps real values to simplex.
  - Quick check question: If ELBO scores for 3 clusters are [-100, -120, -150], what are the soft assignments? (Answer: Heavily concentrated on cluster 1, since exp(-100) >> exp(-150))

- **Concept: Monte Carlo Gradient Estimation**
  - Why needed here: ELBO involves expectation over latent z. The reparameterization trick enables gradient flow through sampling, and MC samples estimate the expectation (paper uses 10 MC samples in E-step).
  - Quick check question: Why does the paper use 10 MC samples for E-step but only 1 for M-step? (Answer: E-step needs accurate ELBO estimates for assignment; M-step can use noisier gradients since Adam averages over batches)

## Architecture Onboarding

- **Component map:**
  Input x_i (N samples) → [E-step] For each sample: Encode through all K encoders E_φk(x_i) → (μ_k, σ²_k), Sample z ~ q_φk(z|x_i) using 10 MC samples, Compute ELBO_k = E[log p_θk(x|z)] - KL(q_φk||p(z)), Soft assignment: u_i = softmax([ELBO_1, ..., ELBO_K]) → [M-step] For each cluster k: Weighted VAE update: L_k = -∑_i u_i,k × ELBO(α_k; x_i), Adam step on (φ_k, θ_k) for 20 epochs

- **Critical path:** E-step ELBO computation → soft assignment → M-step weighted loss → VAE parameter update → repeat. Errors in ELBO estimation cascade directly to cluster quality.

- **Design tradeoffs:**
  - More MC samples (E-step): More accurate assignments but slower; paper uses 10
  - Fewer training epochs (M-step): Faster iteration but VAEs may underfit; paper uses 20 epochs per EM iteration
  - Latent dimension: Higher dim captures more structure but risks overfitting; paper uses 20 for MNIST

- **Failure signatures:**
  - Cluster collapse: All u_i concentrate on 1-2 clusters → check assignment entropy
  - Posterior collapse: KL→0, all z ignored → check latent variance statistics
  - Mode overlap: Generated samples from different clusters appear similar → visualize decoder outputs

- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Replicate Figure 1 results on 5-half-moons dataset. Expected: Visual separation of arcs by iteration 200. If clusters merge, increase EM iterations or reduce learning rate.
  2. **ELBO stability test:** Track variance of ELBO estimates across MC samples during training. If relative std >15%, increase MC samples or check encoder variance outputs.
  3. **Assignment evolution visualization:** Plot soft assignment matrix U (N×K) at iterations 10, 50, 100, 300. Expected: Gradual hardening (entropy decrease) without premature collapse. If assignments harden before iteration 50, increase softmax temperature or add entropy bonus.

## Open Questions the Paper Calls Out
- Can this generative EM framework be successfully extended to normalizing flows, enabling exact log-likelihood computation rather than relying on the ELBO bound?
- How does the method perform on more complex, higher-dimensional datasets such as natural images (e.g., CIFAR-10, ImageNet) compared to the simple MNIST and FashionMNIST benchmarks?
- Does the fixed uniform mixing coefficient assumption (πk = 1/K) degrade performance on datasets with substantially imbalanced cluster sizes?

## Limitations
- The assumption that ELBO values alone provide sufficient signal for cluster assignment remains weakly supported by direct evidence
- The per-cluster VAE specialization mechanism is proposed but not rigorously validated with weight similarity measurements
- The synthetic half-moons experiment provides visual intuition but lacks quantitative metrics for clustering quality progression

## Confidence
- **High Confidence:** The algorithmic framework (EM alternating between E-step and M-step) is technically correct and implementable as described
- **Medium Confidence:** The claim of superior performance relative to state-of-the-art VAE clustering methods is supported by MNIST results but limited to two datasets
- **Low Confidence:** The claim that the method "eliminates the need for GMM priors or regularization" is asserted but not empirically tested by removing these components in ablation studies

## Next Checks
1. **Ablation on ELBO-based assignments:** Run the algorithm using alternative assignment criteria (e.g., maximum likelihood, minimum reconstruction error) to quantify the contribution of ELBO to clustering performance
2. **Decoder specialization measurement:** Compute pairwise cosine similarity between decoder weights across the K VAEs throughout training to verify that clusters learn distinct generative distributions rather than collapsing to similar solutions
3. **Uncertainty quantification:** Report standard deviation and confidence intervals for all clustering accuracy results across the 10 random runs, and test statistical significance against baseline methods using paired t-tests