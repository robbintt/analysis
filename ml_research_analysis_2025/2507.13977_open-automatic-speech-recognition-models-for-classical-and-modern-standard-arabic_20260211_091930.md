---
ver: rpa2
title: Open Automatic Speech Recognition Models for Classical and Modern Standard
  Arabic
arxiv_id: '2507.13977'
source_url: https://arxiv.org/abs/2507.13977
tags:
- arabic
- speech
- recognition
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two open Arabic ASR models for Modern Standard
  Arabic (MSA) and Classical Arabic (CA), addressing the scarcity of public Arabic
  ASR models and the complexity of the language. The authors developed a universal
  methodology for Arabic speech and text processing, including data preprocessing
  and model training, using the FastConformer architecture.
---

# Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic

## Quick Facts
- arXiv ID: 2507.13977
- Source URL: https://arxiv.org/abs/2507.13977
- Reference count: 40
- Introduces open ASR models for MSA and CA with SOTA results

## Executive Summary
This paper addresses the scarcity of public Arabic ASR models by developing two open models for Modern Standard Arabic (MSA) and Classical Arabic (CA). The authors created a universal methodology for Arabic speech processing, including data preprocessing, filtration, and model training using the FastConformer architecture. Two models were trained: one specialized for MSA achieving SOTA performance on related datasets, and a unified model for both MSA and CA that achieves SOTA accuracy with diacritics for CA while maintaining strong performance for MSA. The work includes open-sourcing the models and training recipes to promote reproducibility and further research.

## Method Summary
The methodology involves collecting and preprocessing MSA and CA speech data, applying iterative filtration to remove noisy samples, and training FastConformer-based models with hybrid RNN-T/CTC losses. A key innovation is using Spanish-language checkpoint initialization for faster convergence. The models employ a 1,024-token SentencePiece tokenizer that includes Arabic characters, diacritics, and punctuation. Two models were developed: an MSA-only model optimized for broadcast-style speech, and a unified model capable of handling both MSA and CA with full diacritization.

## Key Results
- MSA model achieves WER of 11.37% on MASC, 9.76% on MCV Arabic, and 7.73% on FLEURS Arabic subset
- Unified model achieves diacritic- and punctuation-aware WER of 6.65% on EveryAyah dataset
- Spanish checkpoint initialization outperforms English and random initialization on Arabic ASR tasks
- Iterative data filtration improves MASC WER from ~9.95% to 8.50%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spanish checkpoint initialization improves convergence and final WER for Arabic ASR
- **Core assumption:** Phonetic similarities between Spanish and Arabic allow beneficial transfer learning
- **Evidence:** Spanish checkpoint achieves 11.63% WER on MASC vs 12.23% (English) and 12.74% (random)

### Mechanism 2
- **Claim:** Iterative data filtration using WER/CER thresholds extracts high-quality training data
- **Core assumption:** High WER/CER in noisy subsets indicates misaligned or poor-quality audio
- **Evidence:** Filtration improves MASC WER from ~9.95% to 8.50% using WER > 60 / CER > 30 thresholds

### Mechanism 3
- **Claim:** Joint modeling of audio, text, and diacritics resolves acoustic ambiguity in CA
- **Core assumption:** Acoustic signals contain sufficient information to distinguish diacritics
- **Evidence:** Unified model achieves SOTA on EveryAyah (6.65% WER) while maintaining MSA performance

## Foundational Learning

- **FastConformer Architecture**
  - **Why needed here:** Replaces standard self-attention with convolution-augmented modules for speed/accuracy trade-off
  - **Quick check:** How does FastConformer differ from standard Transformer encoder regarding convolution layers?

- **Hybrid RNN-T / CTC Loss**
  - **Why needed here:** Combines alignment-free sequence prediction (RNN-T) with regularization (CTC)
  - **Quick check:** Why combine RNN-T and CTC instead of using RNN-T alone?

- **Arabic Diacritization & Orthography**
  - **Why needed here:** Understanding MSA vs CA differences critical for interpreting WER vs WER-PC,D metrics
  - **Quick check:** What is the impact of "NFKC normalization" on Arabic text preprocessing?

## Architecture Onboarding

- **Component map:** Log-mel spectrograms -> FastConformer Encoder -> Hybrid RNN-T/CTC Decoder -> SentencePiece Tokenizer -> Text output

- **Critical path:**
  1. Apply NeMo-speech-data-processor (NFKC norm, drop out-of-alphabet)
  2. Filter noisy data using WER > 60 / CER > 30 thresholds
  3. Load Spanish FastConformer encoder weights
  4. Train 200 epochs with AdamW, LR=5e-3, Hybrid Loss (CTC weight 0.3)
  5. Ensemble top 5 checkpoints

- **Design tradeoffs:**
  - Use MSA-only model for broadcast/news (better WER), Unified model for Classical Arabic/recitation
  - Spanish checkpoint preferred for speed/performance; English as fallback
  - Use `WER` for general intelligibility, `WER-PC,D` for recitation/Quranic accuracy

- **Failure signatures:**
  - High WER on MSA with Unified Model: inconsistent diacritization in ground truth
  - Slow convergence: verify Spanish checkpoint loading
  - Garbage output: check alphabet config includes Hamza forms and Ta' Marbuta

- **First 3 experiments:**
  1. Run MSA model on MASC test set to reproduce ~11.37% WER
  2. Train model without filtration to quantify cleaning impact (~2-3% WER increase)
  3. Evaluate Unified model on Dialectal Arabic to confirm performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the preprocessing methodology generalize to Dialectal Arabic (DA)?
- **Basis:** Introduction defines DA as major variation used in daily communication but pipeline validated only on MSA and CA
- **Why unresolved:** Current work excludes DA entirely, leaving applicability to non-standardized dialects untested
- **What evidence would resolve it:** Evaluation on diverse Dialectal Arabic dataset

### Open Question 2
- **Question:** How can inconsistency of diacritization in standard benchmarks be addressed?
- **Basis:** Section V observes inconsistencies in MCV17 containing mix of undiacritized and fully diacritized samples
- **Why unresolved:** Inconsistency artificially inflates WER for models trained to predict diacritics
- **What evidence would resolve it:** Consistently diacritized benchmark set or normalized evaluation metric

### Open Question 3
- **Question:** Can a single unified model be optimized for both CA and MSA without performance degradation?
- **Basis:** Table V shows Unified model suffers significant WERPC,D increase on MSA datasets (e.g., ~10% to ~25% on MCV)
- **Why unresolved:** Attributed to lack of standardized diacritization in MSA, but unclear if fundamental limitation or solvable issue
- **What evidence would resolve it:** Unified model maintaining WER parity with specialized MSA model

## Limitations
- Relatively small training data (2,000 hours) compared to large-scale English ASR systems
- Filtration process relies on preliminary model judgment that may not generalize across dialects
- Spanish checkpoint advantage requires validation across different Arabic sub-variants
- No evaluation on Dialectal Arabic, limiting real-world applicability

## Confidence
- **High Confidence:** MSA model's SOTA performance on MASC, MCV, FLEURS datasets
- **Medium Confidence:** Unified model's SOTA performance on EveryAyah (diacritic-dependent)
- **Medium Confidence:** Spanish checkpoint initialization advantage (requires phonological validation)
- **Low Confidence:** Generalization of filtration thresholds across different data sources

## Next Checks
1. **Cross-Dialect Generalization Test:** Evaluate MSA model on Levantine/Egyptian dialect data to quantify performance degradation
2. **Checkpoint Ablation Study:** Train identical models with English, Spanish, and random initialization measuring convergence speed across 10 seeds
3. **Filtration Sensitivity Analysis:** Systematically vary WER/CER thresholds to determine optimal balance between data retention and WER improvement