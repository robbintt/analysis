---
ver: rpa2
title: 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance'
arxiv_id: '2511.13254'
source_url: https://arxiv.org/abs/2511.13254
tags:
- souping
- soce
- performance
- arxiv
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Soup Of Category Experts (SoCE), a novel
  approach for model souping that improves LLM performance by leveraging benchmark
  composition and non-uniform weighted averaging. Unlike uniform averaging methods,
  SoCE identifies expert models for weakly-correlated benchmark categories and combines
  them using optimized weights.
---

# Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance

## Quick Facts
- **arXiv ID**: 2511.13254
- **Source URL**: https://arxiv.org/abs/2511.13254
- **Reference count**: 37
- **Primary result**: Achieves state-of-the-art results on Berkeley Function Calling Leaderboard, improving accuracy by 2.7% over previous best for 70B models

## Executive Summary
This paper introduces Soup Of Category Experts (SoCE), a novel approach for model souping that improves LLM performance by leveraging benchmark composition and non-uniform weighted averaging. Unlike uniform averaging methods, SoCE identifies expert models for weakly-correlated benchmark categories and combines them using optimized weights. The method achieves state-of-the-art results on the Berkeley Function Calling Leaderboard, improving accuracy by 2.7% over the previous best individual model for 70B models (80.68% vs 78.56%) and by 5.7% for 8B models (76.50% vs 72.37%). The approach also enhances performance consistency across categories, with souped models showing significantly higher Pearson correlations between category performances compared to unsouped models.

## Method Summary
SoCE operates by first identifying benchmark categories that are weakly correlated with each other, then selecting expert models that excel in specific categories. Rather than using uniform averaging like traditional model soup methods, SoCE applies non-uniform weighted averaging where weights are optimized based on each model's performance within their respective categories. The approach exploits the observation that different models excel at different types of function calling tasks, and that these task categories exhibit low correlation. By strategically combining category-specific experts with optimized weights, SoCE achieves superior overall performance while maintaining consistency across all benchmark categories.

## Key Results
- Achieves state-of-the-art accuracy of 80.68% on Berkeley Function Calling Leaderboard for 70B models, a 2.7% improvement over previous best individual model
- Improves 8B model performance to 76.50%, representing a 5.7% gain over the prior best (72.37%)
- Demonstrates significantly higher Pearson correlations between category performances compared to unsouped models, indicating improved consistency across diverse function calling tasks
- Validates effectiveness on additional benchmarks including MGSM and ∞-Bench, showing consistent improvements across multilingual, tool calling, and math reasoning domains

## Why This Works (Mechanism)
SoCE works by exploiting the heterogeneous nature of function calling tasks and the fact that different models develop specialized strengths for different task types. The key insight is that benchmark categories for function calling are often weakly correlated, meaning a model that excels at one category may perform poorly on another. By identifying these categories and their correlations, SoCE can strategically combine models that are experts in specific areas rather than averaging across all models uniformly. The non-uniform weighted averaging allows the system to give more influence to models that are particularly strong in categories where other models are weak, creating a more balanced and capable overall system. This targeted approach prevents the dilution of strengths that occurs with uniform averaging while compensating for individual model weaknesses.

## Foundational Learning

**Benchmark Category Analysis**
- *Why needed*: Understanding how different function calling tasks relate to each other is crucial for identifying opportunities for specialization
- *Quick check*: Calculate Pearson correlation coefficients between all benchmark categories to verify weak correlation assumption

**Model Specialization Patterns**
- *Why needed*: Identifying which models excel at specific task types enables targeted expert selection
- *Quick check*: Rank models by performance within each category and identify consistent top performers

**Weighted Averaging Optimization**
- *Why needed*: Non-uniform weights are essential for balancing strengths and weaknesses across categories
- *Quick check*: Compare performance of optimized weights against uniform averaging baseline

## Architecture Onboarding

**Component Map**: Benchmark Categories -> Expert Model Identification -> Weight Optimization -> Model Souping

**Critical Path**: The most critical sequence is Benchmark Analysis → Expert Selection → Weight Optimization → Final Model Combination. Each step builds on the previous one, with weight optimization being particularly sensitive to accurate expert identification.

**Design Tradeoffs**: SoCE trades computational complexity in the optimization phase for superior final performance. The method requires more computation to identify optimal experts and weights compared to simple averaging, but this investment pays off in significantly better results. There's also a tradeoff between granularity of category separation and available data - finer categories may better capture specialization but risk data sparsity.

**Failure Signatures**: The approach may fail when benchmark categories are strongly correlated, when expert models don't exist for certain categories, or when the optimization overfits to benchmark distributions. Poor performance on out-of-distribution data or inability to maintain consistency across all categories are key failure indicators.

**First Experiments**:
1. Test on a single benchmark category with known model specializations to verify basic mechanism
2. Compare weighted averaging against uniform averaging on correlated vs uncorrelated categories
3. Validate performance consistency by measuring category-wise performance variance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond function calling domain to other LLM applications
- Computational overhead of identifying optimal expert models and weights for each new task category
- Potential overfitting risks when optimizing weights for specific benchmark compositions without proper regularization

## Confidence

**High confidence**: Experimental results demonstrating performance improvements on Berkeley Function Calling Leaderboard are well-supported by data with clear statistical significance.

**Medium confidence**: Claims regarding improved consistency across categories and enhanced performance on additional benchmarks are supported but require broader validation across more diverse domains.

**Low confidence**: The assertion that SoCE represents a broadly applicable methodology for model souping across diverse LLM domains lacks sufficient empirical support from the current experimental scope.

## Next Checks
1. Apply SoCE to completely different LLM tasks such as text summarization, question answering, or code generation to verify cross-domain generalizability
2. Conduct systematic benchmark correlation analysis across multiple domains to empirically validate the weak correlation assumption
3. Design experiments testing souped models on out-of-distribution data and real-world scenarios to compare robustness against individual expert models and uniformly averaged models