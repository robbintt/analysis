---
ver: rpa2
title: 'Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents'
arxiv_id: '2506.14246'
source_url: https://arxiv.org/abs/2506.14246
tags:
- agents
- tile
- tiles
- mahjong
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mxplainer introduces a novel framework to explain the decision-making
  processes of black-box Mahjong agents through a parameterized search-based template.
  The core idea is to design a manually engineered, domain-specific framework with
  interpretable parameters, convert its calculation component into an equivalent neural
  network, and use supervised learning to fit target agents.
---

# Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents

## Quick Facts
- **arXiv ID**: 2506.14246
- **Source URL**: https://arxiv.org/abs/2506.14246
- **Reference count**: 36
- **Primary result**: Parameterized search-based template achieves >90% top-3 action prediction on black-box Mahjong agents

## Executive Summary
Mxplainer introduces a novel framework to explain the decision-making processes of black-box Mahjong agents through a parameterized search-based template. The core idea is to design a manually engineered, domain-specific framework with interpretable parameters, convert its calculation component into an equivalent neural network, and use supervised learning to fit target agents. Experiments on both AI and human agents show Mxplainer achieves top-three action prediction accuracy of over 92% and 90%, respectively, significantly outperforming decision-tree methods (34.8% top-three accuracy). The learned parameters effectively reflect agents' strategic preferences, such as fan and tile selection tendencies, enabling both strategy-level insights and actionable, step-by-step explanations for individual decisions.

## Method Summary
Mxplainer constructs a parameterized search algorithm (Framework F = SC|CC) where the Search Component proposes valid game goals and the Calculation Component evaluates them using interpretable parameters. The Calculation Component is converted into an equivalent neural network (Network N) via Boolean-to-Arithmetic Masking and identity Padding Values, enabling gradient-based optimization while preserving semantic equivalence. The framework is trained via supervised learning on state-action pairs from target agents, with cross-entropy loss and L2 regularization on parameters. This approach balances expressiveness and interpretability, allowing for both high-fidelity imitation and meaningful parameter-level analysis of strategic preferences.

## Key Results
- Mxplainer achieves 93.47% top-3 action prediction accuracy on AI agents and 90.34% on human agents
- Outperforms decision-tree methods (34.82% top-3) and behavior cloning with 34M parameters (95.69% top-3)
- Learned parameters accurately reflect agent strategies: Seven Pairs-only agent shows 56.19 weight for that pattern vs 3.59 for general agent
- Parameter manipulation experiments confirm causal influence on strategy: increasing All Types fan weight by 10× raised its frequency in winning hands from 2.59% to 5.76%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A manually engineered, domain-specific search framework with interpretable parameters can be converted into an equivalent neural network and trained to approximate black-box agent behavior.
- **Mechanism:** The authors design Framework F = SC|CC, where the Search Component (SC) proposes valid game goals (up to 64) via dynamic programming, and the Calculation Component (CC) evaluates each goal using three parameter groups (Θ_fan, Θ_tile, Θ_held). CC is converted to Network N via Boolean-to-Arithmetic Masking (M) for conditional branches and identity Padding Values (P) for fixed-bound loops, enabling gradient-based optimization while preserving semantic equivalence (Lemma A1).
- **Core assumption:** The search-based paradigm with linear parameterization captures the strategic reasoning of target agents; strategies outside this template (complex non-linear state evaluations, deceptive plays) may not be fully captured.
- **Evidence anchors:**
  - [abstract] "Mxplainer introduces a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents."
  - [Section 4.4] Shows exact PyTorch implementations of Algorithm 1 and 2, with equivalence verified on 10,000 test cases.
  - [corpus] Weak/missing—no directly comparable parameterized-search-to-neural conversion frameworks in neighbors. Adjacent work (PIRL) uses parameterized policies with Bayesian Optimization in simpler domains (TORCS).
- **Break condition:** If target agent strategies rely on reasoning fundamentally outside goal-search logic (e.g., long-term deceptive plays, non-linear state abstractions not captured by Θ), fidelity degrades. This is a deliberate interpretability-expressiveness tradeoff.

### Mechanism 2
- **Claim:** Interpretable parameters (Θ_fan, Θ_tile, Θ_held) learned via supervised learning correlate with and can manipulate agent strategic preferences.
- **Mechanism:** Θ_fan (80 weights) represents relative preference for scoring patterns; Θ_tile (34 weights) breaks ties for discards; Θ_held (12 weights) models tile-hold probabilities via linear features. These are trained with cross-entropy loss on state-action pairs from target agents, plus L2 regularization to penalize negative fan preferences. Post-training, parameters are normalized and analyzed to extract strategy-level insights.
- **Core assumption:** Strategic preferences can be decoupled into interpretable parameter dimensions; the correlation between learned weights and observed behavior reflects genuine agent characteristics rather than fitting artifacts.
- **Evidence anchors:**
  - [Section 5.1.1] For a Seven Pairs-only baseline agent (ψ₁), learned θ₁_fan assigns highest weight to Seven Pairs (56.19 vs. 3.59 for general agent ψ₂).
  - [Section 5.2] Multiplying All Types fan weight by 10× increased its frequency in winning hands from 2.59% to 5.76% (+3.17%), validating causal influence.
  - [corpus] Weak/missing—no comparable parameter-to-strategy manipulation studies in neighbors.
- **Break condition:** If parameters are under-identified or collinear (multiple parameter configurations produce identical action predictions), strategic interpretation becomes ambiguous. The compact parameter size (126) vs. behavioral complexity provides some regularization.

### Mechanism 3
- **Claim:** Mxplainer achieves high imitation accuracy (>90% top-3) while providing step-by-step local explanations by tracing the search-and-calculate deduction process.
- **Mechanism:** For any game state, SC proposes goals; CC computes values per Algorithm 1 (multiplying tile acquisition probabilities, weighted by fan preferences); Decision Selector DS aggregates values per Algorithm 2 to select discards. Tracing intermediate outputs (goal values, redundant tile scores) yields local explanations; comparing learned Θ across agents yields global strategic profiles.
- **Core assumption:** The deduction trace (which goals were proposed, how they were valued, which tiles deemed redundant) faithfully reflects target agent reasoning, not just post-hoc rationalization.
- **Evidence anchors:**
  - [Section 5.3] For a specific game state, ˆψ₂'s trace explains discarding B9: it is redundant for high-value goals (Knitted Straight, Seven Pairs) despite being required for Pure Straight.
  - [Table 6B] Mxplainer (126 params) achieves 93.47% top-3 vs. Decision Tree 34.82% and Behavior Clone (34M params) 95.69%.
  - [corpus] Weak/missing—neighbors focus on imitation in robotics/embodiment; no direct XAI comparisons in game domains.
- **Break condition:** If multiple decision paths lead to identical actions, the trace may select an arbitrary explanation. Spurious correlations in training data could also mislead parameter interpretation.

## Foundational Learning

- **Concept: Supervised Imitation Learning (Behavior Cloning)**
  - **Why needed here:** Mxplainer learns from state-action pairs (no environment interaction) to approximate black-box policies; cross-entropy loss on discrete action spaces is the core training objective.
  - **Quick check question:** Can you explain why distribution shift between training and deployment states is less critical here (imitating a fixed agent) than in standard RL?

- **Concept: Differentiable Programming / Neural-Symbolic Conversion**
  - **Why needed here:** Converting classical search algorithms (CC) to neural networks (N) requires masking conditionals and padding loops while preserving semantics—this enables gradient descent on interpretable structures.
  - **Quick check question:** For a loop accumulating via product, what padding value ensures equivalence (hint: identity element)?

- **Concept: Domain-Specific Heuristic Design**
  - **Why needed here:** The framework's effectiveness hinges on hand-crafted heuristics (Θ groups, probability models p_draw, p_discard) that encode domain knowledge; understanding Mahjong strategy is prerequisite to meaningful parameter design.
  - **Quick check question:** Why might uniform p_discard (opponent discard probability) be a deliberate simplification despite its theoretical limitations?

## Architecture Onboarding

- **Component map:**
  - Goal Proposer (SC) -> Value Calculator (CC) -> Decision Selector (DS)
  - State -> Up to 64 goals (M, R, Φ) -> Per-goal values via tile acquisition probabilities × fan weights -> Worthless degree per tile -> Discard selection

- **Critical path:**
  1. Design Θ groups (fan, tile, held) with domain expert input.
  2. Implement SC (goal proposer) with search limit (K=64 recommended per ablation).
  3. Convert CC to N using masking/padding rules (Appendix B).
  4. Train on target agent data; validate top-3 accuracy (>90% threshold).
  5. Normalize Θ; analyze strategic profiles; trace local deductions.

- **Design tradeoffs:**
  - **Expressiveness vs. Interpretability:** More parameters (e.g., ˆψ″₂ with 3,280) yield higher accuracy (94.57%) but harder interpretation; default 126 params balances both.
  - **Goal cap (K):** K=16 → 79.31% coverage; K=64 → 96.82% coverage; K=128 → marginal gain (Table 4). Default K=64.
  - **Probability model:** Network for p_draw + Uniform for p_discard yields best accuracy/parameter tradeoff (Table 5).

- **Failure signatures:**
  - **Low accuracy (<80% top-3):** Likely cause—inadequate Θ expressiveness or misaligned SC (goals not covering target agent's reasoning). Debug: inspect goal recall per state.
  - **Uninterpretable Θ (e.g., negative fan weights despite regularization):** Check data quality; verify L2 penalty implementation.
  - **OOD state failure:** Mxplainer generalizes via search structure, but extreme unseen states may propose irrelevant goals. Monitor goal distribution on validation.

- **First 3 experiments:**
  1. **Baseline sanity check:** Train on synthetic agent ψ₁ (Seven Pairs only); verify learned θ_fan has dominant Seven Pairs weight (>50% normalized).
  2. **Ablation on goal cap K:** Train on ψ₂ with K∈{16, 32, 64, 128}; plot accuracy vs. K; confirm K=64 is near-optimal.
  3. **Parameter manipulation test:** Multiply one fan weight (e.g., All Types) by 10×; run self-play (8K games); compare winning hand frequency distribution vs. original.

## Open Questions the Paper Calls Out
- **Generalization to other domains:** The authors identify generalizing the explanation template to applications beyond Mahjong as a primary future direction, noting that while they tested Mountain Car and Blackjack, the scope requires further study.
- **Multimodal physiological integration:** Extending Mxplainer to incorporate physiological data to explain how decisions are modulated by real-time psychological states is highlighted as a potential enhancement.
- **Improved opponent modeling:** Replacing the uniform discard distribution with a sophisticated opponent model is suggested as worthwhile future work to reduce systematic bias in value estimations.

## Limitations
- Framework effectiveness depends on Mahjong strategies being capturable by linear parameterized search; complex or deceptive strategies may fall outside this template
- Manual engineering of domain-specific components requires expert knowledge and may not generalize to other games without significant adaptation
- Parameter interpretability assumes well-identified weights without multicollinearity, which could lead to ambiguous strategy attribution

## Confidence
- **High confidence**: Imitation accuracy claims (>90% top-3) supported by experimental results and ablation studies
- **Medium confidence**: Parameter interpretability claims—strong internal validation but limited external comparison
- **Medium confidence**: Neural-symbolic conversion mechanism—mathematically sound but not widely validated in other domains

## Next Checks
1. Test Mxplainer on a Mahjong agent with known complex strategies (e.g., deceptive play patterns) to identify limits of the linear parameterization approach
2. Perform a parameter collinearity analysis to verify that learned weights are well-identified and not interchangeable
3. Apply the Mxplainer framework to a different game domain (e.g., simplified poker) to assess generalizability of the search-to-neural conversion mechanism