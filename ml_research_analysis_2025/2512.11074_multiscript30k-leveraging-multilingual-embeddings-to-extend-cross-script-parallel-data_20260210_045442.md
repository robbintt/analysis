---
ver: rpa2
title: 'MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script
  Parallel Data'
arxiv_id: '2512.11074'
source_url: https://arxiv.org/abs/2512.11074
tags:
- data
- dataset
- multi30k
- multiscript30k
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiScript30k addresses the lack of multilingual support in the
  widely-used Multi30k dataset, which is limited to four European languages. The authors
  generate a new extension using machine translation with NLLB200-3.3B to produce
  parallel text data in Arabic, Spanish, Ukrainian, and Chinese.
---

# MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data

## Quick Facts
- arXiv ID: 2512.11074
- Source URL: https://arxiv.org/abs/2512.11074
- Authors: Christopher Driggers-Ellis; Detravious Brinkley; Ray Chen; Aashish Dhawan; Daisy Zhe Wang; Christan Grant
- Reference count: 23
- Primary result: Synthetic MT-generated parallel corpora can extend multimodal datasets to new language pairs, but quality gaps persist vs. human-refined data

## Executive Summary
MultiScript30k addresses the lack of multilingual support in the widely-used Multi30k dataset, which is limited to four European languages. The authors generate a new extension using machine translation with NLLB200-3.3B to produce parallel text data in Arabic, Spanish, Ukrainian, and Chinese. Translation quality is evaluated using COMETKiwi, cosine similarity, and symmetric KL divergence. The results show that MultiScript30k achieves strong semantic alignment with the original data, with cosine similarity consistently above 0.8 for most languages and COMETKiwi scores comparable to top-performing systems. However, the Ukrainian and Chinese translations show slightly lower performance, suggesting limitations in synthetic MT data without human refinement. The study highlights the potential and constraints of synthetic data generation for expanding multimodal machine translation datasets.

## Method Summary
The study extends the Multi30k dataset from 4 European languages to Arabic, Spanish, Ukrainian, and Chinese using NLLB200-3.3B translation. The pipeline translates Multi30k-En source sentences to target languages, then evaluates semantic alignment using multilingual embeddings (cosine similarity, symmetric KL divergence) and reference-free quality estimation (COMETKiwi). The approach leverages HuggingFace infrastructure with 2× NVIDIA A100 GPUs and compares synthetic translations against human-refined baselines when available.

## Key Results
- MultiScript30k achieves cosine similarity >0.8 and symmetric KL divergence <0.000251 for most languages, indicating strong semantic alignment
- Ukrainian translations show 6.4% lower COMETKiwi scores than human-refined Multi30k-Uk, confirming quality gap for synthetic data
- Chinese translations exhibit degraded performance (cosine similarity ~0.70, KL divergence ~0.0004) despite moderate semantic similarity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic parallel corpus generation via NLLB200-3.3B can produce semantically aligned translations for resource-constrained language pairs.
- Mechanism: NLLB200-3.3B translates English source sentences from Multi30k-En into target languages (Ar, Es, Uk, Zh_Hans, Zh_Hant). Semantic alignment is validated by computing cosine similarity and symmetric KL divergence between multilingual embeddings of source and translated text using `distiluse-base-multilingual-cased-v2`.
- Core assumption: High cosine similarity (>0.8) and low symmetric KL divergence (<0.000251) in embedding space indicate preservation of semantic content across translation.
- Evidence anchors:
  - [abstract] "Similarity analysis shows that Multi30k extension consistently achieves greater than 0.8 cosine similarity and symmetric KL divergence less than 0.000251 for all languages supported except Zh_Hant"
  - [section: Methodology] "Following the example set by [13], the HuggingFace sentence transformer distiluse-base-multilingual-cased-v2 [19] is used to calculate multilingual embedding vectors"
  - [corpus] Related work on parallel corpora for low-resource languages (CorIL, arXiv:2509.19941) similarly uses synthetic generation but focuses on Indian languages; corpus provides limited direct validation of embedding-based quality metrics.
- Break condition: Languages with significant structural dissimilarity from English (e.g., Zh_Hant) show degraded performance—cosine similarity drops to ~0.70, KL divergence triples, COMETKiwi scores decline sharply.

### Mechanism 2
- Claim: Reference-free quality estimation via COMETKiwi enables dataset validation when human reference translations are unavailable.
- Mechanism: COMETKiwi (Unbabel/wmt23-cometkiwi-da-xl) uses XLM-R XL as a pretrained encoder to estimate translation quality without reference texts, providing scores comparable to WMT24 top systems.
- Core assumption: COMETKiwi scores correlate with human judgment of translation quality across diverse language pairs and scripts.
- Evidence anchors:
  - [abstract] "COMETKiwi scores reveal mixed assessments... ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores 6.4% greater than MultiScript30k-Uk per split"
  - [section: Related Work] "COMETKiwi has also become widely adopted among MT researchers. It served as one of the primary automated metrics in the general MT shared task at WMT24"
  - [corpus] Corpus papers on cross-lingual retrieval (arXiv:2511.19324) discuss weak cross-lingual semantic alignment in embedding models, suggesting COMETKiwi may be capturing different signals than embedding similarity.
- Break condition: COMETKiwi scores diverge from semantic similarity metrics for certain languages—Zh translations score poorly on COMETKiwi despite moderate cosine similarity, indicating potential metric sensitivity to grammatical rather than semantic quality.

### Mechanism 3
- Claim: Human post-editing of synthetic translations yields measurably superior quality compared to raw MT output.
- Mechanism: Comparison between MultiScript30k-Uk (raw NLLB200 output) and Multi30k-Uk (Google Cloud Translate + human annotator cleaning) shows the human-refined dataset achieves 6.4% higher COMETKiwi scores.
- Core assumption: The quality difference is attributable to human refinement rather than differences between NLLB200 and GCT base models.
- Evidence anchors:
  - [abstract] "Multi30k-Uk scores 6.4% greater than MultiScript30k-Uk per split"
  - [section: Discussion] "This finding quantitatively affirms the expectation that the workflow elaborated by [13], involving human evaluators, will produce superior translations to unrefined synthetic MT data without post-processing"
  - [corpus] Corpus provides no direct comparative studies of human vs. synthetic refinement workflows for parallel corpora.
- Break condition: Assumption: Quality difference may partially stem from model differences (NLLB200 vs. GCT)—the paper acknowledges this confound but attributes the gap primarily to human refinement.

## Foundational Learning

- Concept: **Multilingual sentence embeddings**
  - Why needed here: Core to evaluating semantic preservation across languages; cosine similarity operates on embeddings from `distiluse-base-multilingual-cased-v2`.
  - Quick check question: Given two sentences in different languages, would you expect their embeddings to have high cosine similarity if they convey the same meaning? What structural differences might lower similarity despite equivalent semantics?

- Concept: **Reference-free MT evaluation (Quality Estimation)**
  - Why needed here: COMETKiwi enables quality assessment without ground-truth translations—critical when extending to languages lacking human-annotated data.
  - Quick check question: Why would a reference-free metric be necessary for evaluating a newly generated parallel corpus? What risks arise from using the same model family for both translation and evaluation?

- Concept: **Synthetic data generation for low-resource NLP**
  - Why needed here: MultiScript30k's primary contribution is demonstrating viability (and limits) of MT-generated parallel data when human annotation is prohibitively expensive.
  - Quick check question: What quality signals would you monitor to determine whether synthetic parallel data is suitable for training downstream models? At what threshold would you reject the synthetic data?

## Architecture Onboarding

- Component map:
  - Multi30k-En source data -> NLLB200-3.3B translation engine -> distiluse-base-multilingual-cased-v2 embedding encoder -> cosine similarity and symmetric KL divergence computation -> COMETKiwi quality estimation -> final dataset

- Critical path:
  1. Load Multi30k-En source sentences
  2. Translate to target language(s) via NLLB200-3.3B
  3. Compute multilingual embeddings for source and target
  4. Calculate cosine similarity and symmetric KL divergence
  5. Run COMETKiwi quality estimation
  6. If reference exists (Ar, Uk): compute BLEU/ChrF++ for lexical comparison

- Design tradeoffs:
  - **Speed vs. quality**: Raw MT output enables rapid dataset creation but underperforms human-refined data by measurable margins (6.4% for Uk)
  - **Coverage vs. reliability**: Zh_Hant shows degraded metrics; decide whether to include lower-quality language variants or restrict to well-performing pairs
  - **Embedding similarity vs. grammatical quality**: High cosine similarity may mask grammatical errors (confirmed by native speaker spot-check of Zh translations)

- Failure signatures:
  - Cosine similarity <0.8 combined with KL divergence >0.0004 indicates potential semantic drift
  - COMETKiwi scores significantly below reference baselines (>5% gap) suggest quality concerns
  - Native speaker review identifies grammatical issues not captured by embedding metrics (Zh case)

- First 3 experiments:
  1. **Baseline replication**: Translate Multi30k-En to a single target language (e.g., Es) and reproduce the reported cosine similarity (>0.9) and COMETKiwi (~0.77) scores to validate pipeline correctness.
  2. **Ablation on script variants**: Compare Zh_Hans vs. Zh_Hant systematically—hypothesis: performance gap stems from training data imbalance in NLLB200; test by computing per-sentence metric distributions.
  3. **Human annotation pilot**: Select 100 sentences from lowest-performing language (Zh_Hant), have native speakers rate grammaticality and semantic accuracy, correlate with automated metrics to validate whether embedding similarity hides systematic errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do semantic similarity metrics (cosine similarity, symmetric KL divergence) correlate reliably with human evaluations of machine translation quality, particularly regarding syntax and grammar?
- Basis in paper: [explicit] The authors state that "Further research is needed to assess the validity of semantic similarity metrics... as proxy measures of MT quality."
- Why unresolved: High cosine similarity scores for Chinese translations obscured poor grammatical quality identified by native speakers, suggesting these metrics may ignore syntax in favor of meaning.
- What evidence would resolve it: A study directly correlating these embedding-based metrics with human evaluations of grammatical correctness across different languages.

### Open Question 2
- Question: To what extent does incorporating human post-editing improve the quality of synthetic dataset extensions compared to raw machine translation output?
- Basis in paper: [explicit] The authors note that "Human evaluators who speak the target languages... should review and correct the data whenever available" to improve the workflow.
- Why unresolved: The human-refined Multi30k-Uk dataset outperformed the raw synthetic MultiScript30k-Uk by 6.4% in COMETKiwi scores.
- What evidence would resolve it: A comparative analysis of downstream MMT model performance when trained on raw synthetic data versus human-post-edited synthetic data.

### Open Question 3
- Question: Can MultiScript30k-Es serve as a reliable substitute for human-translated Spanish data in the absence of a baseline comparison?
- Basis in paper: [inferred] The authors admit it is "uncertain whether MultiScript30k-Es provides a good substitute for human translated data" because no prior extension or human evaluation exists for that language.
- Why unresolved: While Spanish achieved the highest quantitative scores, the authors warn that high scores in other languages (e.g., Chinese) masked poor grammar, and no evidence confirms Spanish avoids this issue.
- What evidence would resolve it: A human evaluation of the MultiScript30k-Es grammatical correctness or a comparison against a newly created human-translated Spanish baseline.

## Limitations
- Synthetic MT data shows measurable quality gaps vs. human-refined translations (6.4% for Ukrainian)
- Embedding-based semantic similarity metrics may mask grammatical deficiencies in structurally distant languages
- COMETKiwi reference-free evaluation may not perfectly align with human judgments across all language pairs and scripts

## Confidence
- **High confidence**: The core methodology for extending Multi30k using NLLB200-3.3B is technically sound and reproducible. The reported metric values (cosine similarity >0.8 for most languages, symmetric KL divergence <0.000251) are internally consistent and validated through multiple evaluation approaches.
- **Medium confidence**: The interpretation that human refinement drives the 6.4% quality improvement over synthetic data assumes model equivalence between NLLB200 and Google Cloud Translate. While the paper acknowledges this confound, the exact contribution of human refinement versus model differences remains uncertain.
- **Low confidence**: The semantic alignment conclusions for Chinese languages rely heavily on embedding similarity, which native speaker spot-checks revealed can miss grammatical errors. The divergence between COMETKiwi and cosine similarity for Zh translations suggests the embedding metrics may not fully capture translation quality for these language pairs.

## Next Checks
1. **Model ablation study**: Replicate the Ukrainian translation pipeline using Google Cloud Translate (the model used for Multi30k-Uk) rather than NLLB200-3.3B to isolate the contribution of human refinement versus base model quality to the 6.4% performance gap.
2. **Grammaticality vs. semantic alignment**: Conduct a controlled human evaluation on 100 sentences from each target language, rating both semantic preservation and grammatical correctness separately. Correlate these scores with embedding similarity and COMETKiwi to quantify how well automated metrics capture grammatical quality.
3. **Cross-lingual embedding robustness**: Test whether the `distiluse-base-multilingual-cased-v2` embeddings maintain consistent performance across script types by evaluating translations between languages with different scripts (e.g., Arabic→English) and comparing to same-script pairs (Spanish→English). This would validate whether the observed performance differences are script-related or model-specific.