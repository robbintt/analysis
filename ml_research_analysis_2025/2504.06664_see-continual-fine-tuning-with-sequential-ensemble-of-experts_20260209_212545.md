---
ver: rpa2
title: 'SEE: Continual Fine-tuning with Sequential Ensemble of Experts'
arxiv_id: '2504.06664'
source_url: https://arxiv.org/abs/2504.06664
tags:
- tasks
- experts
- learning
- routing
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  continual fine-tuning of large language models. The authors propose Sequential Ensemble
  of Experts (SEE), a framework that integrates routing and response mechanisms within
  each expert, using a distributed sequential routing approach.
---

# SEE: Continual Fine-tuning with Sequential Ensemble of Experts

## Quick Facts
- arXiv ID: 2504.06664
- Source URL: https://arxiv.org/abs/2504.06664
- Authors: Zhilin Wang, Yafu Li, Xiaoye Qu, Yu Cheng
- Reference count: 27
- Key outcome: SEE achieves near-zero forgetting in continual fine-tuning while matching or exceeding multi-task learning performance

## Executive Summary
This paper addresses catastrophic forgetting in continual fine-tuning of large language models by proposing Sequential Ensemble of Experts (SEE). SEE integrates routing and response generation within each expert through supervised fine-tuning, using a distributed sequential routing mechanism. The framework achieves near-zero forgetting with appropriate rehearsal ratios and demonstrates strong generalization ability by effectively routing out-of-distribution queries to the base model.

## Method Summary
SEE trains each expert on a new task while maintaining knowledge of previous tasks through rehearsal data. During training, current-task samples are formatted as (query, `<<pos>>`, response) while rehearsed samples from previous tasks become (query, `<<neg>>`) without responses. Each expert learns to output the positive indicator only for in-domain queries. At inference, queries traverse experts from newest to oldest, with the base model serving as final fallback for truly novel queries. The framework uses LoRA adapters (rank=8) for efficient parameter addition.

## Key Results
- Achieves BWT ≈ 0 on 10 SuperNI tasks at τ=10% rehearsal ratio
- Outperforms prior rehearsal-based methods on Average ROUGE-L
- Routes 99.72% of MMLU instances to base model for OOD queries
- Special indicators (`<<pos>>`, `<<neg>>`) outperform semantic alternatives

## Why This Works (Mechanism)

### Mechanism 1: Integrated Routing-Response Learning
By embedding routing decisions into the generation process via supervised fine-tuning, each expert jointly learns domain recognition and response generation without a separate router. During training, current-task samples are formatted as (query, `<<pos>>`, response), while rehearsed samples from previous tasks become (query, `<<neg>>`) with no response. The expert learns to output the positive indicator only for in-domain queries, effectively internalizing the routing function.

### Mechanism 2: Sequential Fallback with Base Model Anchor
The reverse-chronological routing order with base model fallback provides OOD robustness without explicit OOD training. Queries traverse experts from newest (E_N) to oldest (E_1). Each expert generates an indicator token first; `<<neg>>` triggers handoff to the next expert. If no expert claims the query, the base model M_0 handles it by default.

### Mechanism 3: Rehearsal as Boundary Supervision
Rehearsal data serves dual purposes: mitigating forgetting AND teaching experts their decision boundaries through negative sampling. When training expert E_i, samples from tasks T_1...T_{i-1} are included as negative examples (query, `<<neg>>`) without responses. This teaches E_i what NOT to handle. Replacing these with pseudo-negatives (unrelated data) collapses routing F1 from 99.80% to 47.22%.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Each expert is a LoRA adapter (rank=8 in experiments) attached to a frozen base model. This enables N independent experts with ~1% parameter overhead per task.
  - Quick check question: Why can multiple LoRA adapters coexist with a shared base model without interference?

- **Concept: Catastrophic Forgetting & Backward Transfer**
  - Why needed here: BWT measures forgetting (negative = degradation on prior tasks). SEE achieves BWT ≈ 0 by isolating expert parameters and using rehearsal for routing, not knowledge retention.
  - Quick check question: How does SEE's architecture prevent the negative BWT seen in sequential fine-tuning?

- **Concept: Special Token Indicators**
  - Why needed here: SEE adds new vocabulary tokens (`<<pos>>`, `<<neg>>`) rather than reusing existing tokens. The paper shows this outperforms semantic alternatives ("Yes"/"No") for classification tasks.
  - Quick check question: Why might semantic tokens like "Yes/No" conflict with downstream task generation?

## Architecture Onboarding

- **Component map:** Base Model M_0 -> LoRA Adapters L_1...L_N -> Experts E_i = M_0 + L_i -> Special Indicators (`<<pos>>`, `<<neg>>`) -> Rehearsal Buffer
- **Critical path:**
  1. **Task Reconstruction:** Current data → (q, `<<pos>>`, r); prior data → (q, `<<neg>>`)
  2. **SFT Training:** Train new LoRA on reconstructed dataset (3 epochs, lr=2e-4, batch=32)
  3. **Inference:** Query E_N → E_{N-1} → ... → E_1 → M_0 until `<<pos>>` or exhaustion
- **Design tradeoffs:**
  - τ=1%: Lower storage, but BWT degraded (-0.33 avg vs. 0.00 at τ=10%)
  - τ=10%: Near-zero forgetting, slightly lower AR on some tasks vs. τ=1%
  - Indicator type: Special tokens > non-semantic existing tokens > semantic tokens (for classification)
- **Failure signatures:**
  - Low routing F1 (<95%): Insufficient τ or pseudo-negative contamination
  - Experts answering OOD queries: Base model fallback not triggered; check indicator generation
  - Latency spike at high expert count: Sequential overhead scales as (M+1)/2 prefix operations
- **First 3 experiments:**
  1. Reproduce 5-task SuperNI results with τ=10%: Verify AR ≈ 53 and BWT ≈ 0 on Llama-2-7B
  2. Routing ablation: Train with τ∈{1%, 5%, 10%, 20%} and plot routing F1 vs. AR tradeoff curve
  3. OOD routing test: Run trained model on MMLU; confirm >99% queries route to base model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the SEE framework be adapted to mitigate the linear growth of parameter storage and rehearsal data volume when scaling to hundreds or thousands of tasks?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "the increasing number of parameters as the task count grows calls for further consideration" and that the expanding volume of rehearsal data "underscores the necessity for continued advancements."
- **Why unresolved:** The provided experiments are limited to a maximum of only 10 sequential tasks, leaving the scalability to larger, long-term deployments unproven.
- **What evidence would resolve it:** Experiments evaluating performance and storage efficiency on a continuous sequence of 50+ tasks with a fixed memory budget.

### Open Question 2
- **Question:** How does SEE perform when sequential tasks have high semantic overlap or ambiguous boundaries, rather than the distinct domains used in the current study?
- **Basis in paper:** [inferred] The study utilizes a subset of 10 distinct SuperNI tasks (e.g., QA, Translation, Paraphrasing), which may simplify the routing challenge compared to real-world scenarios where task definitions blur.
- **Why unresolved:** The paper does not analyze routing accuracy in "gray area" scenarios where an expert might generate a false positive indicator for a query that belongs to a subsequent, similar expert.
- **What evidence would resolve it:** Evaluation on a dataset specifically designed to contain fine-grained tasks with overlapping decision boundaries.

### Open Question 3
- **Question:** Can the sequential routing mechanism be optimized to reduce latency for "hot" tasks that appear frequently, rather than assuming a uniform query distribution?
- **Basis in paper:** [inferred] The overhead analysis explicitly assumes the probability of hitting any expert is uniform ($1/(M+1)$) for calculation simplicity, which rarely holds in real-world usage where recent tasks often dominate queries.
- **Why unresolved:** The current sequential routing always starts from the last expert ($L_N$); if a user frequently queries the first expert ($L_1$), the latency accumulates unnecessarily.
- **What evidence would resolve it:** A dynamic routing order analysis based on a non-uniform, Zipfian query distribution.

## Limitations

- Sequential routing latency scales linearly with expert count, becoming prohibitive for large task numbers
- Rehearsal ratio optimization remains empirical without systematic analysis across task characteristics
- Indicator token design superiority lacks theoretical justification
- Base model fallback generalization is assumed but not empirically validated

## Confidence

**High confidence:** The core mechanism of integrating routing into the generation process works as described. The sequential routing with base model fallback achieves the reported routing accuracy (99.80% F1) and the empirical demonstration that pseudo-negatives degrade performance is convincing.

**Medium confidence:** The claim of near-zero forgetting (BWT ≈ 0) at τ=10% is well-supported by the SuperNI experiments, but generalization to other datasets and task types is unverified. The OOD routing effectiveness is demonstrated on MMLU but not systematically tested across diverse benchmarks.

**Low confidence:** Claims about scalability to many tasks (M>10) and efficiency of the sequential approach lack empirical support. The optimal τ=10% recommendation is based on limited experimentation without sensitivity analysis across different task characteristics.

## Next Checks

1. **Latency benchmarking:** Measure end-to-end inference time for queries routed through 5, 10, 20, and 50 experts to quantify the sequential routing overhead and verify the "affordable" claim at M=10.

2. **Cross-dataset generalization:** Apply SEE to a different continual learning benchmark (e.g., CLIF or Stream-LM) with varying task characteristics to test whether τ=10% remains optimal and whether the routing mechanism generalizes beyond SuperNI.

3. **Base model fallback validation:** Directly compare M_0's performance on OOD queries versus the last expert's performance to empirically verify that the base model provides superior generalization for truly novel queries.