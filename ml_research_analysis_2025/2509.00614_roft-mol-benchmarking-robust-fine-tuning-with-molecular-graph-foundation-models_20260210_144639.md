---
ver: rpa2
title: 'RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation
  Models'
arxiv_id: '2509.00614'
source_url: https://arxiv.org/abs/2509.00614
tags:
- fine-tuning
- tasks
- pre-training
- pre-trained
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks fine-tuning methods for molecular graph foundation
  models across classification and regression tasks under various data regimes. It
  categorizes eight fine-tuning strategies into weight-based, representation-based,
  and partial fine-tuning mechanisms and evaluates them across six diverse pre-trained
  models.
---

# RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models

## Quick Facts
- arXiv ID: 2509.00614
- Source URL: https://arxiv.org/abs/2509.00614
- Reference count: 40
- Primary result: DWiSE-FT method achieves top performance for both regression and classification tasks while maintaining ease of use

## Executive Summary
This paper presents a comprehensive benchmark of fine-tuning methods for molecular graph foundation models across diverse classification and regression tasks. The study evaluates eight fine-tuning strategies across six pre-trained models under various data regimes, revealing that self-supervised models benefit from weight-based fine-tuning while supervised models perform better with representation-based methods. The authors propose a new method, DWiSE-FT, that combines weight ensemble strengths from WiSE-FT and L2-SP, achieving superior performance across both task types while maintaining simplicity.

## Method Summary
The study benchmarks eight fine-tuning strategies categorized into weight-based (Full-FT, WiSE-FT, L2-SP, Surgical-FT), representation-based (Linear Probing, Feature-map), and partial fine-tuning (LP-FT, BSS) mechanisms. These methods are evaluated across six pre-trained molecular graph models (Mole-BERT, GraphMAE, MoleculeSTM for self-supervised; Graphium-Toy/Large, GraphGPS for supervised) on 12 downstream tasks (8 classification, 4 regression). The new DWiSE-FT method implements layer-wise parameter interpolation with θ[i] = (1-α_i)·θ_pre[i] + α_i·θ_ft[i], where α_i values are optimized via gradient descent on validation loss. Experiments cover few-shot scenarios (50, 100, 500 samples) and out-of-distribution settings with random, scaffold, and size-based data splits.

## Key Results
- Self-supervised models benefit from weight-based fine-tuning (Full-FT, WiSE-FT, L2-SP), while supervised models perform better with representation-based methods (Feature-map, BSS)
- LP-FT and BSS improve few-shot performance for classification but underperform in regression tasks
- DWiSE-FT achieves top performance for both regression and classification tasks, unifying the strengths of WiSE-FT and L2-SP
- The choice of fine-tuning strategy is highly determined by the pre-training objective, creating a dichotomy between self-supervised and supervised models

## Why This Works (Mechanism)
DWiSE-FT works by combining the weight ensemble strength of WiSE-FT with the layer-wise regularization of L2-SP. By optimizing layer-wise interpolation weights α_i through gradient descent on validation loss, the method dynamically balances between preserving pre-trained knowledge and adapting to downstream tasks. This approach is particularly effective because it addresses the fundamental trade-off between catastrophic forgetting and insufficient adaptation that plagues traditional fine-tuning methods.

## Foundational Learning

- **Graph neural networks (GNNs)**: Why needed - Molecular property prediction relies on graph-structured data where atoms are nodes and bonds are edges; quick check - Verify that the GIN architecture can process molecular graphs with variable size and connectivity
- **Fine-tuning mechanisms**: Why needed - Understanding how different regularization strategies affect model adaptation is crucial for selecting appropriate methods; quick check - Confirm that weight-based methods modify model parameters while representation-based methods only adapt final layers
- **Molecular pre-training objectives**: Why needed - The choice between self-supervised and supervised pre-training fundamentally impacts which fine-tuning strategy performs best; quick check - Identify whether the pre-training used node masking or supervised property prediction
- **Data splitting strategies**: Why needed - Scaffold and size splits test generalization to structurally different molecules; quick check - Ensure scaffold split groups molecules by shared molecular scaffolds
- **Hyperparameter optimization**: Why needed - Finding optimal α values and learning rates is critical for DWiSE-FT performance; quick check - Verify that α is initialized from {0.9, 0.7, 0.5} and optimized with specified learning rates

## Architecture Onboarding

Component map: Pre-trained model → Fine-tuning method → Downstream task → Evaluation metric

Critical path: The core workflow involves loading a pre-trained molecular foundation model, applying one of eight fine-tuning strategies with specific hyperparameters, training on downstream tasks, and evaluating using AUC (classification) or RMSE (regression) metrics.

Design tradeoffs: The paper trades implementation complexity for performance by developing DWiSE-FT, which requires additional hyperparameter tuning for layer-wise α values but achieves superior results compared to simpler methods.

Failure signatures: If LP performs well on self-supervised models, this indicates incorrect implementation since the paper shows LP should underperform on self-supervised representations. If partial FT methods (LP, LP-FT, Surgical-FT) outperform Full-FT in few-shot regression, this suggests the model may be underfitting due to excessive parameter freezing.

First experiments: 1) Implement and verify WiSE-FT with α ∈ 0.1-0.9 on Mole-BERT for BBBP classification; 2) Implement L2-SP with δ ∈ 0.001-1 on GraphMAE for Esol regression; 3) Compare Full-FT vs. Feature-map on Graphium models for classification tasks to observe supervised model behavior.

## Open Questions the Paper Calls Out

The paper explicitly identifies several open questions for future research. First, whether the observed fine-tuning dynamics generalize to scientific domains beyond small molecules, such as proteins or materials, remains unexplored. Second, designing a unified fine-tuning strategy that automatically selects the optimal mechanism based on the pre-training objective could bridge the current dichotomy between self-supervised and supervised models. Third, graph-specific parameter-efficient fine-tuning methods like LoRA or GraphAdapters warrant investigation, as the current study focused on general PEFT approaches rather than adapter architectures specifically designed for graph data.

## Limitations

The DWiSE-FT implementation details for gradient-based α optimization are not fully specified, requiring assumptions about whether optimization is joint or per-layer. The robustness evaluation focuses primarily on generalization across data splits rather than adversarial perturbations or out-of-distribution detection. The findings may not generalize to molecular graph foundation models outside the evaluated 12 specific datasets and six pre-trained models.

## Confidence

High confidence: The benchmarking framework structure and comparative analysis of eight fine-tuning methods are well-documented and reproducible, with systematic evaluation across multiple model types and tasks supporting the key findings.

Medium confidence: The DWiSE-FT performance claims are based on reported results, but the exact implementation details for gradient-based α optimization require reasonable assumptions for reproduction, and the robustness evaluation may not capture all aspects typically considered in the literature.

Low confidence: Generalization of findings to completely different molecular graph foundation models or tasks outside the evaluated 12 datasets cannot be guaranteed without additional validation.

## Next Checks

1. Reproduce the comparative analysis of all eight fine-tuning methods on the BBBP classification task using Mole-BERT with scaffold split, verifying that Full-FT achieves approximately 67-68 AUC and that LP shows significantly lower performance (~62-63 AUC).

2. Implement and validate DWiSE-FT on the Esol regression task with 50-shot samples, comparing against WiSE-FT and L2-SP baselines to confirm the unified performance advantage claimed for both regression and classification tasks.

3. Conduct an ablation study on the DWiSE-FT method by varying the initial α values (0.9, 0.7, 0.5) and learning rates (0.001, 0.005, 0.01) to verify the sensitivity analysis and confirm optimal hyperparameter ranges.