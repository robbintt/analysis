---
ver: rpa2
title: A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support
arxiv_id: '2505.10188'
source_url: https://arxiv.org/abs/2505.10188
tags:
- explanations
- user
- diagnosis
- explanation
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated three types of AI-generated explanations (attribution-based,
  counterfactual, and exclusion-based) for diagnostic decision support in neurology.
  The explanations were generated using Bayesian Networks and transformed into natural
  language arguments.
---

# A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support

## Quick Facts
- arXiv ID: 2505.10188
- Source URL: https://arxiv.org/abs/2505.10188
- Reference count: 40
- Primary result: Attribution-based explanations scored highest in plausibility and comprehensibility among medical experts evaluating AI diagnostic support

## Executive Summary
This study evaluated three types of AI-generated explanations for diagnostic decision support in neurology, focusing on differential diagnosis of Transient Loss of Consciousness. Using Bayesian Networks and template-based natural language generation, the researchers created attribution-based, counterfactual, and exclusion-based arguments. Eight medical experts rated these explanations across four dimensions, with attribution-based arguments receiving the highest overall scores. Despite moderate ratings, experts showed reluctance to use AI explanations in clinical settings due to lack of transparency about model reliability and generic nature of arguments.

## Method Summary
The study used a three-layer Bayesian Network (risk factors → diseases → symptoms) trained on anonymized outpatient letters and an external dataset of 300 patients with 36 features. Three explanation types were generated: attribution-based using LIME for feature importance, counterfactual using Alibi Explain, and exclusion-based arguments. Natural language templates transformed these into arguments highlighting 2 features each. Eight medical experts evaluated 10 randomly selected cases across four dimensions (comprehensibility, plausibility, completeness, applicability) using 5-point Likert scales.

## Key Results
- Attribution-based arguments received highest overall ratings, especially in plausibility and comprehensibility
- Counterfactual and exclusion-based arguments were rated lower and less preferred
- Experts were reluctant to use AI explanations in clinical discussions despite moderate plausibility scores
- Participants criticized the generic nature of explanations across all cases

## Why This Works (Mechanism)
The Bayesian Network structure connects risk factors to diseases to symptoms, enabling probabilistic reasoning about diagnostic relationships. LIME and Alibi Explain extract feature importance and counterfactuals respectively, which are then transformed into natural language arguments through template-based generation. This approach bridges the gap between complex model internals and human-understandable explanations by focusing on the most influential diagnostic features.

## Foundational Learning
- **Bayesian Networks**: Probabilistic graphical models for representing dependencies between variables - needed for capturing medical diagnostic relationships; quick check: verify CPT learning from data matches expected conditional probabilities
- **LIME (Local Interpretable Model-agnostic Explanations)**: Method for explaining individual predictions by approximating local decision boundaries - needed to identify most influential features; quick check: confirm feature importance rankings align with medical expert expectations
- **Counterfactual explanations**: "What if" scenarios showing how changes to input features would alter predictions - needed to provide actionable diagnostic insights; quick check: verify counterfactuals represent clinically plausible alternative diagnoses
- **Template-based NLG**: Using predefined sentence structures to convert model outputs into natural language - needed for consistent, understandable explanations; quick check: ensure templates capture the semantic meaning of model explanations
- **User-centered evaluation**: Measuring explanation effectiveness through expert ratings across multiple dimensions - needed to assess real-world utility; quick check: confirm Likert scale responses show expected variance between explanation types

## Architecture Onboarding
- **Component map**: Raw data -> Bayesian Network (CPT learning) -> Model predictions -> XAI methods (LIME, Alibi Explain) -> Template-based NLG -> Expert evaluation survey
- **Critical path**: Data → BN structure & parameters → Predictions → Explanation generation → Expert evaluation
- **Design tradeoffs**: Fixed templates vs. adaptive explanations; binary features vs. continuous measurements; single explanation type vs. composite arguments
- **Failure signatures**: Low plausibility scores indicate mismatch between model reasoning and clinical logic; inconsistent feature selection suggests incorrect CPT values or BN structure; low completeness ratings suggest template limitations
- **Exactly 3 first experiments**:
  1. Verify BN predictions match expected diagnostic outcomes on held-out cases
  2. Test template generation with known input-output pairs to confirm argument structure
  3. Run LIME and counterfactual generation on sample cases to verify feature selection

## Open Questions the Paper Calls Out
### Open Question 1
Does including explicit confidence scores and model reliability metrics alongside argumentative explanations increase physicians' willingness to use AI-generated explanations in clinical discussions with colleagues?

Basis in paper: Participants stated they "had not received any information about the quality and reliability of the generated explanations and the uncertainty of the prediction," which caused reluctance to use explanations despite moderate plausibility/comprehensibility ratings.

### Open Question 2
Can combining multiple explanation types (attribution-based, counterfactual, exclusion-based) into composite arguments better match expert explanation patterns and improve perceived completeness?

Basis in paper: Analysis of user-provided explanations revealed experts frequently combine patterns (e.g., "Prototype & Exclusion of other diagnoses," "List of symptoms + Exclusion"). The paper's conclusion explicitly states intent to "combine different XAI methods to generate more comprehensive argumentative explanations."

### Open Question 3
Does adapting explanation complexity and content to case-specific factors (ambiguity, available evidence) and physician experience level improve the utility of AI diagnostic support?

Basis in paper: Follow-up interviews revealed participants found it problematic that "the same types of argument were generated and displayed for all cases," and suggested "adapting explanations to the complexity of the case" and "taking into account the experience of the doctor."

## Limitations
- Small sample size (8 experts) limits statistical power and generalizability
- Fixed template approach lacks adaptation to case complexity or physician experience
- Missing transparency about model reliability and prediction uncertainty
- Limited methodological detail prevents exact reproduction (partial BN structure, unspecified hyperparameters)

## Confidence
- High confidence in reported user preference for attribution-based explanations
- Medium confidence in overall methodology of template-based explanation generation
- Low confidence in exact reproduction of numerical results due to missing BN parameters

## Next Checks
1. Verify Bayesian Network prediction accuracy matches the paper's reported 8/10 expert agreement by testing on the same 10 cases used in the user study
2. Compare generated explanation text structure and feature selection (top 2 features per explanation) against the paper's examples to ensure template fidelity
3. Assess explanation plausibility scores using a small validation set of medical experts (n≥3) to confirm the directional findings about explanation type preferences before full-scale reproduction