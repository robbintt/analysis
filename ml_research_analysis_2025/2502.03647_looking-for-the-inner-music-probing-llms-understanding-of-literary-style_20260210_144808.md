---
ver: rpa2
title: 'Looking for the Inner Music: Probing LLMs'' Understanding of Literary Style'
arxiv_id: '2502.03647'
source_url: https://arxiv.org/abs/2502.03647
tags:
- genre
- words
- style
- authorship
- author
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models (LLMs) can identify
  literary style in very short texts, distinguishing both author and genre. Using
  datasets of novels, the authors evaluate multiple models, finding that larger generative
  LLMs like Llama-3 and Flan-T5 outperform traditional baselines.
---

# Looking for the Inner Music: Probing LLMs' Understanding of Literary Style

## Quick Facts
- arXiv ID: 2502.03647
- Source URL: https://arxiv.org/abs/2502.03647
- Reference count: 18
- Primary result: Large language models can identify authorship and genre from very short literary passages (20-50 words) by capturing stylistic features

## Executive Summary
This paper investigates whether large language models can detect literary style in extremely short text passages, distinguishing both authorship and genre. The authors evaluate multiple models on classification tasks using sentences of only 20-50 words from novels spanning 1839-1937. They find that generative LLMs like Llama-3 and Flan-T5 significantly outperform traditional baselines, demonstrating that stylistic signals exist at this small scale. The work reveals that different models employ different strategies—some rely on memorization while others use learned representations—and provides insights into what distinguishes authorial style (punctuation, word order, contextual usage) from genre-level style (common words, topical trends).

## Method Summary
The study evaluates LLMs on two classification tasks: authorship attribution (27 authors, 81 novels) and genre identification (5 genres, 30 novels). Texts are preprocessed to standardize punctuation, remove metadata, and tokenize into sentences of 20-50 words. Models are fine-tuned using Flan-T5 (various sizes) with standard Seq2SeqTrainer parameters and Llama-3-8b with LoRA fine-tuning. Performance is measured by classification accuracy on withheld novels, with error bars computed via bootstrapping. Baselines include SVM with linear kernel and TF-IDF unigrams, plus Cosine Delta. Input format uses a prompt structure with free-text label output.

## Key Results
- Generative LLMs (Llama-3, Flan-T5) outperform traditional baselines (SVM, Cosine Delta) on both authorship and genre classification tasks
- Larger generative models show superior performance, with Flan-T5-xl achieving highest accuracy
- Authorial style relies more on punctuation, word order, and contextual usage, while genre style depends more on common words and topical trends
- Pronouns are important features for both authorship and genre classification
- Different models balance style identification and source memorization differently, with Llama-3 showing more memorization behavior

## Why This Works (Mechanism)
LLMs capture stylistic features through learned representations that encode subtle linguistic patterns beyond simple word frequency. The models identify authorship through punctuation patterns, syntactic structures, and contextual word usage that remain consistent across an author's work. Genre classification relies more heavily on common vocabulary and topical content that characterize different literary categories. The effectiveness at short text lengths suggests that stylistic signatures are embedded in local linguistic patterns rather than requiring long-range dependencies. Fine-tuning on sentence-level classification tasks allows models to develop specialized representations for these subtle stylistic features.

## Foundational Learning
- **Sentence tokenization and filtering**: Needed to create standardized short passages for analysis; quick check: verify 20-50 word sentence distribution matches paper
- **Fine-tuning methodology**: Required for adapting pre-trained LLMs to stylometry tasks; quick check: monitor training loss and accuracy curves
- **Bootstrapping for error estimation**: Essential for reliable confidence intervals; quick check: replicate SE calculations on held-out data
- **Ablation study design**: Critical for understanding feature importance; quick check: ensure shuffled word order baseline performs as expected
- **Shapley value computation**: Used for feature attribution analysis; quick check: validate attributions align with known stylistic features
- **Cross-attention analysis**: Helps understand model decision-making; quick check: examine attention patterns for consistency across samples

## Architecture Onboarding

**Component Map**: Pre-trained LLM (Flan-T5/Llama-3) -> Fine-tuning layer (LoRA or standard) -> Classification head -> Prompt formatting -> Evaluation

**Critical Path**: Text preprocessing → Model fine-tuning → Classification → Performance evaluation → Ablation analysis

**Design Tradeoffs**: Encoder-decoder models (Flan-T5) vs decoder-only (Llama-3) show different memorization vs generalization behaviors; larger models generally perform better but require more computational resources; prompt formatting choices affect model performance

**Failure Signatures**: 
- High accuracy on in-training novels but poor generalization to withheld novels indicates memorization
- Accuracy dropping below SVM when word order is shuffled confirms model relies on sequence information
- Excessive scapegoating (misattributions to few authors) suggests bias in learned representations

**3 First Experiments**:
1. Fine-tune Flan-T5-xl on authorship task with provided hyperparameters and evaluate on withheld novels
2. Compare SVM baseline performance using identical train/test splits
3. Conduct word order shuffling ablation to verify model relies on sequence information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can probing methods be advanced to characterize human-recognizable elements of style beyond individual word importance?
- Basis in paper: [explicit] The conclusion states, "In future work, we hope to develop more sophisticated methods for probing for and characterizing human-recognizable elements of style beyond individual words."
- Why unresolved: The authors' current probing methods (Shapley values and cross-attention) largely highlighted frequent words or rare proper nouns, failing to capture complex stylistic nuances like "inner music."
- What evidence would resolve it: The development of a probing framework that successfully identifies and categorizes abstract syntactic or semantic style markers that align with qualitative literary theory.

### Open Question 2
- Question: Do the identified stylistic signals for authorship and genre transfer to 20th and 21st-century commercial fiction?
- Basis in paper: [explicit] The authors note in the conclusion that they "will also benefit from... extending to more recent 20th and 21st century works following the growth of commercial genre fiction."
- Why unresolved: The current study is limited to texts mostly written between 1839 and 1937, leaving the distinct stylistic signatures of modern commercial fiction unexplored.
- What evidence would resolve it: A replication of the ablation and classification experiments on a dataset of post-1950 commercial fiction to see if pronouns and word order remain dominant features.

### Open Question 3
- Question: To what extent does pre-training data memorization versus architectural design drive the performance differences between decoder-only and encoder-decoder models in stylometry?
- Basis in paper: [inferred] The paper observes that Llama-3 (decoder-only) relies on memorization while Flan-T5 (encoder-decoder) relies on learned representations, but it does not determine if this is caused by architecture or training data.
- Why unresolved: The confounding variables of model architecture, size, and pre-training corpora prevent a definitive conclusion on why models "balance style identification and source memorization differently."
- What evidence would resolve it: A controlled study comparing model variants trained on identical corpora to isolate whether the tendency to memorize is a function of the decoder-only architecture or the specific training data.

## Limitations
- The study is limited to texts from 1839-1937, not reflecting modern commercial fiction styles
- Some SVM baseline hyperparameters are unspecified, affecting reproducibility
- The LoRA dropout parameter contains a likely typo ("Dropout is 9")
- Bootstrapping procedure details are missing, creating ambiguity about resampling methodology

## Confidence
- High confidence in LLMs' superior performance over traditional baselines and their ability to capture stylistic signals in short texts
- Medium confidence in ablation study results showing different feature importance for author vs. genre classification
- Medium confidence in probing experiments identifying specific linguistic features

## Next Checks
1. Replicate the core classification results (accuracy on withheld novels for both tasks) with specified random seeds and complete hyperparameter documentation
2. Extend the ablation studies beyond Llama-3 to confirm feature importance patterns across multiple models
3. Conduct controlled experiments varying sentence length to test the robustness of the 20-50 word window finding