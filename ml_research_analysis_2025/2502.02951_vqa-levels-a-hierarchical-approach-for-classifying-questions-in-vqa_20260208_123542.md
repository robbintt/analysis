---
ver: rpa2
title: 'VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA'
arxiv_id: '2502.02951'
source_url: https://arxiv.org/abs/2502.02951
tags:
- questions
- image
- question
- level
- levels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark dataset called VQA-Levels to
  systematically evaluate Visual Question Answering (VQA) systems. The dataset contains
  751 questions across 210 images, organized into 7 hierarchical levels of complexity
  ranging from low-level visual features (Level 1) to abstract whole-scene analysis
  (Level 7).
---

# VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA

## Quick Facts
- arXiv ID: 2502.02951
- Source URL: https://arxiv.org/abs/2502.02951
- Reference count: 21
- Key outcome: Proposes VQA-Levels dataset to diagnose VQA systems across 7 hierarchical complexity levels

## Executive Summary
This paper introduces VQA-Levels, a new benchmark dataset designed to systematically evaluate Visual Question Answering (VQA) systems across seven hierarchical levels of complexity. The dataset contains 751 questions across 210 images, ranging from low-level visual features to abstract whole-scene analysis. Testing on state-of-the-art VQA models revealed significant performance variations across levels, with highest accuracy on Levels 1-2 (low-level features and object classification) and lowest on Levels 3 (scene text), 6 (extrapolation), and 7 (whole scene analysis). This demonstrates that current VQA systems rely more on language models than visual semantics, performing poorly on questions requiring external knowledge, scene text understanding, or abstract reasoning.

## Method Summary
The authors created a pilot VQA dataset with 751 questions across 210 images, organized into 7 hierarchical levels of complexity. They evaluated several state-of-the-art VQA models including Pythia, ViLT, OFA, BLIP, and GIT using inference-only approaches. The dataset was constructed by sampling from MS COCO and web sources, with questions manually annotated according to the 7-level taxonomy. Performance was measured using exact-match accuracy across each level and overall.

## Key Results
- VQA models achieved highest accuracy on Levels 1-2 (40-90% range) and lowest on Levels 3 (12.9-29.03%), 6, and 7
- Performance gaps indicate current models rely heavily on language priors rather than visual grounding
- Scene text understanding (Level 3) emerged as a critical bottleneck, with near-random performance
- Abstract reasoning questions (Levels 6-7) exposed fundamental limitations in model reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
A 7-level hierarchical categorization systematically isolates distinct reasoning demands in VQA, enabling granular diagnostic evaluation. Questions are sorted by the type and complexity of processing required—from low-level visual features (L1–L2), through scene text (L3), to semantic and abstract reasoning (L4–L7). This maps loosely to Marr's theory of vision and CBIR-inspired property categories. Core assumption: Question difficulty correlates with the level of abstraction and external knowledge required, and this progression is stable across images. Evidence: Table 3 shows systematic performance variation across levels. Break condition: If model performance does not vary significantly across levels, or levels do not show a consistent ordering of difficulty, the hierarchy lacks discriminative validity.

### Mechanism 2
Level-wise performance gaps expose architectural weaknesses in current VQA systems, particularly in scene text understanding and abstract reasoning. By evaluating the same models across all 7 levels, systematic failure patterns emerge—high performance on L1–L2, sharp drops on L3 (scene text), L6 (extrapolation), and L7 (whole-scene analysis). Core assumption: Performance differences across levels reflect model capability gaps rather than dataset artifacts or question phrasing biases. Evidence: Table 3 shows L1 accuracy ranges 40–90%; L3 ranges 12.9–29.03%; L7 ranges 26.32–47.37% across models. Break condition: If all models perform uniformly poorly (or well) across all levels, the diagnostic value of the hierarchy collapses.

### Mechanism 3
Current VQA models rely heavily on language priors rather than grounded visual semantics, leading to failures when answers cannot be inferred from question words alone. Models succeed when the question contains lexical cues associated with the answer but fail when reasoning requires integration of full-scene context without linguistic shortcuts. Core assumption: Poor performance on semantically demanding questions with no lexical answer hints indicates insufficient visual grounding. Evidence: Section 7 Discussion shows models answer "East/West" for direction questions and give absurd answers (e.g., "plate" for an eclipsed moon) due to lack of causal or semantic understanding. Break condition: If models perform equally well on questions with and without lexical answer cues, the language-dominance hypothesis is unsupported.

## Foundational Learning

- Concept: **Visual Question Answering (VQA) as a multimodal task**
  - Why needed here: VQA-Levels is a benchmark specifically designed to diagnose VQA systems; understanding the task's requirement to jointly process vision and language is essential.
  - Quick check question: Given an image of a kitchen and the question "What is stored in the top shelf?", what modalities must be integrated to answer correctly?

- Concept: **Marr's levels of vision (computational, algorithmic, implementational)**
  - Why needed here: The authors explicitly ground their 7-level hierarchy in Marr's theory; L1–L2 loosely correspond to early visual processing, while L4–L7 require semantic scene understanding.
  - Quick check question: Which of Marr's levels would be required to answer "What emotion is the person in the image expressing?"

- Concept: **Language priors in VQA**
  - Why needed here: The paper's central finding is that models exploit language correlations rather than visual reasoning; recognizing this bias is critical for interpreting results.
  - Quick check question: If a model answers "tennis" to "What sport is shown?" based solely on the word "sport" without processing the image, what type of bias is this?

## Architecture Onboarding

- Component map: Image encoder -> Text encoder -> Multimodal fusion layer -> Answer decoder -> Optional scene text module
- Critical path: 1. Extract visual features → classify objects (required for L2) 2. Detect and read scene text (required for L3—current bottleneck) 3. Fuse visual + text representations → perform reasoning (required for L4–L7)
- Design tradeoffs:
  - Unified transformer vs. modular pipeline: ViLT/OFA unify modalities but lack specialized OCR; modular systems could add scene text but increase complexity.
  - Language prior vs. visual grounding: Strong language models boost overall accuracy but reduce sensitivity to visual content.
  - Dataset size vs. annotation precision: VQA-Levels is small (751 questions) but curated for unambiguous answers; larger datasets may introduce ambiguity.
- Failure signatures:
  - L3 near-random performance: Missing or weak scene text module.
  - L6–L7 systematic underperformance: Lack of reasoning modules or external knowledge integration.
  - Answers like "East/West" for direction questions: Language model hallucinating without visual grounding.
- First 3 experiments:
  1. Baseline diagnostic: Run your VQA model on VQA-Levels; plot accuracy per level to identify weakest levels.
  2. Scene text ablation: Add an OCR module (e.g., differentiable text spotting) and measure L3 improvement.
  3. Language bias probe: Compare performance on questions with vs. without lexical answer cues; quantify language prior strength.

## Open Questions the Paper Calls Out

### Open Question 1
Can automated methods be developed to generate complex questions for Levels 6 (Extrapolation) and 7 (Abstraction), or is manual annotation strictly necessary? The authors state that "Automating generation of L6 and L7 questions appears difficult with the current systems." This is unresolved because current automation relies on object labels or descriptions, but high-level questions require reasoning about unseen elements or abstract concepts that standard generation pipelines cannot easily synthesize. A functioning algorithm that can generate L6/L7 questions from images with accuracy comparable to human annotators would resolve this.

### Open Question 2
Will the performance disparities between visual (L1-L2) and semantic (L6-L7) levels persist or narrow as the dataset scales to 2,000 questions? The paper notes that "Performance at Levels L6 and L7 requires greater experimentation as the numbers of questions at these levels is low." This is unresolved because the current pilot dataset is too small to determine if low performance on abstract questions is a fundamental limitation of the models or a result of data sparsity. Evaluation results on the full dataset (2000+ questions) showing a statistically significant improvement in accuracy for higher-level questions would resolve this.

### Open Question 3
To what extent does the integration of specialized scene-text understanding modules mitigate the performance drop observed in Level 3? The paper notes the least performance occurs on Level 3 (Scene Text) and concludes that "there is a significant need to work on scene text," implying current vision transformers may lack this specific capability. This is unresolved because it is unclear if the failure is due to a lack of OCR capabilities in the tested architectures or the inability to reason about the text once recognized. A comparative study of standard VQA models versus VQA models augmented with explicit OCR engines on the Level 3 subset would resolve this.

## Limitations
- Dataset availability: The VQA-Levels dataset with 751 questions and level annotations is not publicly released
- Limited theoretical grounding: The 7-level hierarchy lacks strong theoretical foundation beyond Marr's levels of vision
- Potential dataset artifacts: Performance differences could reflect dataset artifacts rather than genuine capability gaps

## Confidence
- High confidence in language-prior dominance claim based on systematic performance gaps across levels
- Medium confidence in the hierarchical categorization's discriminative validity due to relatively small dataset size (751 questions)
- Low confidence in the 7-level hierarchy as a universal taxonomy of VQA complexity without broader validation

## Next Checks
1. Reconstruct the VQA-Levels dataset using the described methodology and verify that the 7-level categorization produces consistent difficulty rankings across different annotators
2. Systematically compare model performance on questions where answer words appear in the question versus those requiring genuine reasoning, to quantify the extent of language prior exploitation
3. Test whether augmenting VQA models with external knowledge bases (e.g., scene text OCR, common sense reasoning modules) improves performance specifically on L3, L6, and L7 questions where current models fail most dramatically