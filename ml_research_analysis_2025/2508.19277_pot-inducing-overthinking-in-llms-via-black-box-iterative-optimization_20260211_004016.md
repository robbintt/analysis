---
ver: rpa2
title: 'POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization'
arxiv_id: '2508.19277'
source_url: https://arxiv.org/abs/2508.19277
tags:
- reasoning
- prompt
- attack
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents POT, a black-box attack framework that induces
  overthinking in large language models (LLMs) through iterative prompt optimization.
  Unlike prior overthinking attacks that rely on external knowledge poisoning and
  retrieval-dependent templates, POT generates covert adversarial prompts using an
  LLM-based optimizer to manipulate reasoning chains while maintaining output consistency.
---

# POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization

## Quick Facts
- **arXiv ID**: 2508.19277
- **Source URL**: https://arxiv.org/abs/2508.19277
- **Reference count**: 6
- **Primary result**: Black-box attack framework achieving 6-8× reasoning token inflation while preserving >90% accuracy across three mainstream LLMs

## Executive Summary
POT is a black-box attack framework that induces overthinking in large language models through iterative prompt optimization. Unlike prior overthinking attacks requiring external knowledge poisoning or retrieval-dependent templates, POT generates covert adversarial prompts using an LLM-based optimizer to manipulate reasoning chains while maintaining output consistency. The framework achieves significantly higher reasoning token inflation rates (8.3× on GPT-o1, 7.8× on Claude-Sonnet-3.7, and 6.1× on Gemini-2.5-pro for MathQA) compared to baseline methods, with attack hit rates ranging from 81% to 90% while preserving answer accuracy above 90%.

## Method Summary
POT employs a three-stage pipeline: (1) Initial Prompt Constructor generates seed phrases using an LLM to create guiding phrases that induce complex reasoning; (2) Prompt Assembler integrates these phrases with task inputs through prepending, clause integration, or paragraph embedding; (3) LLM-Based Optimizer runs 50 rounds of Optimization-by-Prompting (OPRO), generating 30 candidates per round and selecting top performers while maintaining semantic diversity. The scoring function balances reasoning token inflation against answer consistency, and diversity filtering prevents mode collapse during optimization.

## Key Results
- Achieved 8.3× reasoning token inflation on GPT-o1, 7.8× on Claude-Sonnet-3.7, and 6.1× on Gemini-2.5-pro for MathQA dataset
- Maintained answer accuracy above 90% across all target models while achieving attack hit rates of 81-90%
- Demonstrated strong cross-model transferability, with prompts optimized on one model achieving comparable inflation rates on unseen models
- Outperformed baseline methods including genetic algorithms and handcrafted templates across all three datasets (MathQA, AIME 2024, MATH-500)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantically natural guiding phrases can induce excessive reasoning token generation without triggering detection mechanisms.
- **Mechanism:** Autoregressive generation creates dependencies between sequential reasoning steps, where injected phrases like "You are an experienced logician. Try to analyze the problem step by step from multiple perspectives" implicitly steer reasoning trajectories.
- **Core assumption:** LLMs cannot distinguish between benign and adversarial complexity-inducing prompts when both are semantically fluent.
- **Evidence anchors:** Abstract mentions "covert adversarial prompts using an LLM-based optimizer to manipulate reasoning chains while maintaining output consistency."
- **Break condition:** If target models implement semantic intent classification that identifies "over-analysis encouragement" patterns.

### Mechanism 2
- **Claim:** Iterative LLM-based optimization discovers high-efficacy adversarial prompts that outperform manually-crafted templates.
- **Mechanism:** Optimizer LLM receives meta-prompts containing historical prompt-score pairs, generates new candidates evaluated by scoring model using Eq. 2 balancing token inflation against answer consistency.
- **Core assumption:** Scoring function surfaces meaningful gradients for optimization.
- **Evidence anchors:** Section 3.3 describes OPRO paradigm; Table 1 shows POT achieves 8.3× RTI vs. 6.9× for ICL Genetic.
- **Break condition:** If optimizer and scorer models share alignment training that suppresses generation of reasoning-extension prompts.

### Mechanism 3
- **Claim:** Diversity-aware filtering prevents prompt mode collapse and enables cross-model transferability.
- **Mechanism:** After each optimization round, selects most semantically diverse subset by maximizing pairwise cosine distances between prompt embeddings.
- **Core assumption:** Semantic diversity in prompt embeddings correlates with diversity of induced reasoning behaviors.
- **Evidence anchors:** Section 3.3 provides mathematical formulation of diversity filtering; Table 3 shows cross-model transfer performance.
- **Break condition:** If diverse semantic prompts activate similar reasoning pathways (semantic diversity ≠ reasoning diversity).

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) Prompting
  - **Why needed here:** POT exploits the explicit reasoning structure introduced by CoT; understanding that each reasoning step conditions subsequent generation is essential.
  - **Quick check question:** Explain why CoT's step-by-step structure creates attack surface that standard prompting does not.

- **Concept:** Optimization by Prompting (OPRO)
  - **Why needed here:** The paper's optimizer operates via natural language meta-prompts rather than gradient descent; you must understand this paradigm to replicate the search loop.
  - **Quick check question:** How does OPRO differ from traditional hyperparameter optimization, and what role does the meta-prompt play?

- **Concept:** Black-Box Attack Constraints
  - **Why needed here:** POT's threat model assumes no access to model parameters or external data sources; distinguishing black-box from white-box assumptions clarifies deployment scope.
  - **Quick check question:** List three capabilities POT explicitly does NOT require that prior overthinking attacks did.

## Architecture Onboarding

- **Component map:** Seed Prompt Generator (Mg) -> Prompt Assembler (Ma) -> Scoring Model (Ms) -> Optimizer Model (Mopt) -> Diversity Filter
- **Critical path:** 1) Generate 50 seed prompts → Assemble with task input → Score all → Initialize H0 with top kinit=30; 2) Construct meta-prompt Pr from (O, Hr, J) → Generate 30 candidates → Score → Merge with history → Select top 40 by score → Filter to 30 by diversity; 3) Repeat for R=50 rounds or until convergence → Output final prompt set HR
- **Design tradeoffs:** Higher α increases inflation but risks answer degradation; larger kinit improves exploration but increases API costs; temperature parameter v controls candidate diversity vs. refinement trade-off.
- **Failure signatures:** RTI < 2× with accuracy preserved indicates prompts not sufficiently persuasive; accuracy drops <85% suggests α too high or prompts introduce genuine confusion; transferability fails (RTI drops >50% cross-model) indicates candidate pool overfit to scorer model.
- **First 3 experiments:** 1) Baseline validation on MathQA subset (n=100) with GPT-o1 as target, verify RTI >5× with accuracy >90%; 2) Ablation on diversity - disable diversity filtering and compare RTI and transferability to full POT on 3 models; 3) Cross-dataset transfer - optimize prompts on MathQA, deploy on MATH-500 and AIME 2024, measure RTI degradation.

## Open Questions the Paper Calls Out

None

## Limitations
- Hyperparameters α and β controlling the trade-off between reasoning inflation and answer consistency are not reported, making it unclear whether the 90%+ accuracy preservation is robust.
- The exact meta-prompt template used to instruct the optimizer LLM is unspecified, leaving ambiguity about whether the "task objective O" and "generation instructions J" components are optimal.
- The paper does not clarify how reasoning tokens are extracted from each target model's outputs, despite API response formats varying significantly across models.

## Confidence

- **High Confidence**: The basic OPRO framework is well-specified and replicable; the three-stage pipeline architecture is clearly described; baseline comparisons show POT outperforms existing methods.
- **Medium Confidence**: The cross-model transferability claims are supported by Table 3 but lack ablation evidence for the diversity mechanism; the 90%+ accuracy preservation claim depends on unspecified hyperparameters.
- **Low Confidence**: The semantic stealth claim lacks empirical validation beyond the qualitative statement that prompts are "semantically natural"; the mechanism by which iterative optimization discovers effective adversarial prompts is asserted but not empirically isolated.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary α and β in Eq. 2 across a grid and measure RTI and accuracy trade-offs on MathQA with GPT-o1 to determine whether the reported 90%+ accuracy is robust or hyperparameter-dependent.

2. **Diversity Filtering Ablation**: Run POT with diversity filtering disabled and compare RTI and cross-model transferability on all three target models and three datasets to isolate the contribution of the diversity mechanism.

3. **Reasoning Token Extraction Validation**: Implement and validate a consistent method for extracting reasoning tokens vs. answer tokens from each target model's outputs and measure how different extraction methods affect reported RTI values across models.