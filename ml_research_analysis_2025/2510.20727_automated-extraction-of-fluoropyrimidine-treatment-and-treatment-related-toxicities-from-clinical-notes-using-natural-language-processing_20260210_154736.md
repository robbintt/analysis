---
ver: rpa2
title: Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities
  from Clinical Notes Using Natural Language Processing
arxiv_id: '2510.20727'
source_url: https://arxiv.org/abs/2510.20727
tags:
- heart
- treatment
- skin
- clinical
- toxicities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated natural language processing (NLP) methods
  for extracting fluoropyrimidine treatment and toxicity information from oncology
  clinical notes. A gold-standard dataset of 236 notes was annotated for five categories:
  drug of interest, arrhythmia, heart failure, valvular complications, and hand-foot
  syndrome treatment/prevention therapies.'
---

# Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing

## Quick Facts
- arXiv ID: 2510.20727
- Source URL: https://arxiv.org/abs/2510.20727
- Reference count: 0
- Primary result: LLM error-analysis prompting (LLaMA3.8B) achieved F1=1.000 for both fluoropyrimidine treatment and toxicity extraction, outperforming all other methods.

## Executive Summary
This study evaluates multiple natural language processing approaches for extracting fluoropyrimidine treatment and toxicity information from oncology clinical notes. The researchers developed a gold-standard dataset of 236 annotated notes covering five categories: drug of interest, arrhythmia, heart failure, valvular complications, and hand-foot syndrome treatment/prevention therapies. Various NLP methods were compared, including rule-based systems, traditional machine learning, deep learning models, and large language models using both zero-shot and error-analysis prompting strategies. The error-analysis prompting approach with LLaMA3.8B demonstrated optimal performance with perfect F1 scores for both treatment and toxicity extraction, significantly outperforming all other methods tested.

## Method Summary
The study employed a multi-method comparison approach using a 236-note oncology clinical note corpus annotated for five toxicity categories. Traditional methods included rule-based systems (MedTagger) and machine learning models (SVM, Logistic Regression, Random Forest) with TF-IDF and n-gram features. Deep learning approaches used BERT and ClinicalBERT fine-tuned on the dataset. Large language models were evaluated using zero-shot prompting and error-analysis prompting, where the model first processed the training set to identify errors, then received enhanced prompts with chain-of-thought reasoning examples derived from those errors. All methods were evaluated using weighted F1 scores with an 80:20 train-test split.

## Key Results
- LLaMA3.8B with error-analysis prompting achieved F1=1.000 for both treatment and toxicity extraction
- Zero-shot prompting reached F1=1.000 for treatment but only F1=0.876 for toxicities
- Rule-based systems achieved F1=0.857 for treatment and 0.858 for toxicities
- Traditional ML models showed mixed performance (F1=0.776-0.945 for treatment, 0.776-0.945 for toxicities)
- Deep learning models struggled particularly with heart failure detection (F1=0.696-0.723)

## Why This Works (Mechanism)

### Mechanism 1: Error-Analysis Prompting Corrects Systematic Misclassifications via Chain-of-Thought Examples
Incorporating explicit reasoning examples derived from training-set errors enables LLMs to correctly classify previously misclassified patterns. Zero-shot prompts are first applied to the training set; false positives and false negatives are systematically analyzed; corrective chain-of-thought (CoT) examples demonstrating proper classification are then embedded into enhanced prompts. The prompt includes task description, medical terminology lists, and CoT reasoning examples for error patterns. Assumption: Error patterns identified in training are representative of test-set errors and the corrective reasoning generalizes. Evidence: Error-analysis prompting with LLaMA3.8B achieved optimal performance with F1=1.000 for both treatment and toxicities extraction, outperforming all other methods. The 4-step error analysis process is detailed in Pages 16-17, with concrete examples showing how "bilateral lower extremity edema" was corrected to recognize heart failure.

### Mechanism 2: LLM Pre-training Enables Recognition of Indirect Clinical Manifestations
LLMs' pre-existing knowledge allows them to associate indirect clinical findings with diagnostic categories without explicit training examples. The model leverages pre-trained semantic relationships between clinical concepts, such as recognizing that "bilateral lower extremity edema" indicates "fluid overload," which is associated with "heart failure" even when HF is not explicitly mentioned. Assumption: The pre-training corpus contained sufficient clinical text to establish these semantic associations. Evidence: Zero-shot prompting achieved F1=1.000 for treatment and F1=0.876 for toxicities without any training examples. The HF error-analysis prompt taught the model that "bilateral edema is a common and recognized symptom associated with congestive heart failure," demonstrating the model's ability to recognize indirect clinical findings.

### Mechanism 3: Traditional ML/Deep Learning Performance Constrained by Training Data Scale
The 236-note dataset provides insufficient examples for traditional ML and deep learning models to learn complex clinical language patterns, particularly for rare categories. Traditional methods learn patterns from training data; with limited examples (e.g., n=11 for Heart Failure in test set), models cannot generalize to linguistic variations. Deep learning models require more data to fine-tune pre-trained weights effectively. Assumption: Performance gap is primarily due to data quantity rather than architectural limitations of traditional methods. Evidence: Machine and deep learning approaches were limited by small training data and showed limited generalizability, particularly for rare categories. Heart Failure was the worst-performing category across traditional methods (BERT F1=0.696, RF F1=0.723, Rule-based F1=0.714), and deep learning models were most challenged by heart failure categories.

## Foundational Learning

- **Concept: Zero-shot vs. Error-Analysis (Few-shot) Prompting**
  - Why needed here: The paper's central finding hinges on understanding why adding error-derived examples (error-analysis prompting) improves over zero-shot. Zero-shot provides only task description; error-analysis adds corrective CoT examples.
  - Quick check question: If you add 3 error-derived examples to a zero-shot prompt, what type of prompting strategy are you using?

- **Concept: F1 Score and Class Imbalance**
  - Why needed here: The paper uses weighted F1 scores because toxicity categories are imbalanced. Understanding why HF (n=11 test samples) is harder than Arrhythmia (n=14) or HFS therapies (n=25) requires understanding how small sample sizes affect metric reliability.
  - Quick check question: Why might a model achieve F1=1.000 on one category but F1=0.696 on another within the same dataset?

- **Concept: Clinical NLP Challenges—Negation, Uncertainty, Indirect References**
  - Why needed here: The paper notes that "heart failure findings are indirect (e.g. leg edema)." Clinical notes contain assertions that must be interpreted in context: "patient denies chest pain" (negation), "possible arrhythmia" (uncertainty), "bilateral edema" (indirect HF indicator).
  - Quick check question: Why would "denies heart failure" require different handling than "heart failure exacerbation"?

## Architecture Onboarding

- **Component map:**
  Clinical Notes (n=236) -> Annotation Layer (5 categories) -> Rule-based/ML/DL/LLM methods -> Binary Classification per Category -> Evaluation (Precision, Recall, F1)

- **Critical path:**
  1. Define toxicity categories with domain experts (CTCAE v5.0 + UMLS expansion)
  2. Annotate gold-standard corpus (236 notes, inter-annotator κ=0.77)
  3. For LLM error-analysis: Apply zero-shot to training set -> identify errors -> create CoT examples -> enhance prompts -> evaluate on test set
  4. Compare across method families using consistent 80:20 train-test split

- **Design tradeoffs:**
  - Rule-based: High precision, lower recall; requires manual keyword curation; interpretable; works with limited data
  - Traditional ML: Better generalization than rules; requires feature engineering; limited by training data
  - Deep Learning: Learns contextual representations; requires more data to fine-tune; ClinicalBERT outperformed BERT modestly
  - LLM Zero-shot: No training required; fast deployment; inconsistent on complex categories (HF F1=0.696)
  - LLM Error-analysis: Best performance; requires training set access for error mining; may not generalize to new error patterns

- **Failure signatures:**
  - Heart Failure consistently underperforms (F1=0.696-0.818 across non-LLM methods) -> signal: indirect clinical manifestations
  - Deep learning models (BERT, ClinicalBERT) underperform ML (SVM, LR) -> signal: insufficient data for fine-tuning
  - Zero-shot LLM fails on HF (F1=0.696) but succeeds after error-analysis (F1=1.000) -> signal: prompting strategy matters more than model scale

- **First 3 experiments:**
  1. **Replicate zero-shot baseline**: Apply LLaMA 3.1 8B zero-shot to your clinical notes with category-specific terminology lists; document per-category F1 scores to identify which categories need error-analysis enhancement.
  2. **Implement error-analysis loop**: On your training split, run zero-shot, manually analyze false positives/negatives for the worst-performing category, construct 2-3 CoT examples following the paper's 4-step reasoning format, evaluate improvement on test set.
  3. **Ablation study**: Test whether performance gains come from (a) CoT reasoning structure, (b) specific error examples, or (c) both by creating prompts with only CoT structure (no error examples), only error examples (no CoT), and full error-analysis prompting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the error-analysis prompting approach maintain perfect performance (F1=1.000) when applied to multi-institutional datasets with varying documentation styles?
- Basis in paper: The "Future Work" section prioritizes "multi-institutional validation across diverse healthcare systems" to assess generalizability.
- Why unresolved: The current study relies on a single-institution dataset of 236 notes, which limits the diversity of clinical documentation practices evaluated.
- What evidence would resolve it: External validation results showing stable F1 scores when the model is applied to EHRs from unrelated healthcare systems.

### Open Question 2
- Question: Does incorporating temporal modeling improve the extraction of time-dependent relationships between fluoropyrimidine exposure and toxicity onset?
- Basis in paper: The authors state the current approach "does not explicitly model time-dependent relationships" and identify this as an important direction.
- Why unresolved: The current binary classification framework treats clinical notes as static text, failing to capture the chronological sequence of drug administration versus symptom appearance.
- What evidence would resolve it: A model architecture that integrates timestamp data to successfully differentiate acute toxicity events from historical or incidental mentions.

### Open Question 3
- Question: To what extent does integrating structured data (ICD codes, labs) with LLM-based NLP improve detection accuracy for toxicities with available codes (e.g., cardiotoxicity)?
- Basis in paper: The paper notes that "integration of structured data elements could improve toxicity detection" and suggests combining NLP with structured sources.
- Why unresolved: Relying solely on unstructured text may miss toxicities documented only in structured fields or fail to resolve ambiguities present in clinical narratives.
- What evidence would resolve it: Comparative performance metrics of a hybrid model versus the text-only approach, specifically for coded conditions like arrhythmia.

## Limitations
- Small test set size (8-25 samples per category) limits statistical confidence in performance metrics
- 236-note corpus is not publicly available, preventing independent validation
- Specific error-analysis CoT examples that drove perfect F1 scores are incompletely documented

## Confidence

**High Confidence:** The comparative performance ranking across method families (LLM error-analysis > LLM zero-shot > traditional ML > rule-based) is well-supported by the experimental results and consistent across multiple categories.

**Medium Confidence:** The mechanism by which error-analysis prompting achieves perfect F1 scores is plausible but not definitively proven. The improvement could stem from the CoT reasoning structure, specific error examples, or their combination. The small sample size limits statistical confidence in the absolute performance claims.

**Low Confidence:** The claim that LLaMA3.8B is universally optimal for this task is not supported, as the study only evaluated one LLM architecture. The paper does not explore whether smaller models or alternative prompting strategies might achieve similar results.

## Next Checks
1. **Statistical Validation:** Replicate the experiment with cross-validation or bootstrapping to establish confidence intervals for the F1 scores, particularly for the small test sets.

2. **Error Pattern Generalization:** Test whether the error-analysis prompting approach maintains performance when applied to a different clinical domain or when the test set contains qualitatively different error patterns than the training set.

3. **Component Ablation:** Systematically evaluate the contribution of CoT reasoning structure versus specific error examples by testing prompts with only structure, only examples, and both components to isolate what drives the performance gains.