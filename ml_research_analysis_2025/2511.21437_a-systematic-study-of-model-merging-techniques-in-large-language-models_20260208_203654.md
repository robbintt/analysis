---
ver: rpa2
title: A Systematic Study of Model Merging Techniques in Large Language Models
arxiv_id: '2511.21437'
source_url: https://arxiv.org/abs/2511.21437
tags:
- merging
- number
- accuracy
- task
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We systematically evaluate six state-of-the-art model merging techniques
  across four large language models, twelve fine-tuned checkpoints per model, and
  sixteen benchmarks. Task Arithmetic, the oldest and simplest method, is the only
  approach that consistently improves performance over the base model, while interference-aware
  and subspace-based methods typically degrade accuracy.
---

# A Systematic Study of Model Merging Techniques in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.21437
- **Source URL:** https://arxiv.org/abs/2511.21437
- **Reference count:** 40
- **Primary result:** Task Arithmetic is the only method that consistently improves LLM performance; other approaches typically degrade accuracy.

## Executive Summary
This paper systematically evaluates six model merging techniques across four large language models, twelve fine-tuned checkpoints per model, and sixteen benchmarks. The study reveals that Task Arithmetic, the simplest method, is the only approach that reliably produces constructive interference—where merged models outperform both the base and best individual checkpoints. More sophisticated interference-aware and subspace-based methods generally fail, degrading performance as they deviate from the base model. These findings suggest current merging techniques are inadequate for modern LLMs and highlight the need for LLM-specific merging algorithms and merging-aware fine-tuning methods.

## Method Summary
The study evaluates six merging techniques (Task Arithmetic, TIES, Model Stock, TSV-Merge, Iso-C, Subspace Boosting) on four base LLMs (Llama-3.2-3B, Llama-3.1-8B, Qwen3-4B, Qwen3-8B) using 12 randomly sampled fine-tuned checkpoints per base. Using mergekit for merging and lm-evaluation-harness for evaluation, the researchers systematically merge subsets of size n∈{2,4,6,8,10,12} across 15 random samples per size. The primary metrics are probability of outperforming the base model and relative accuracy gain over the best individual checkpoint, with a focus on L2-norm distance from the base model as a predictor of success.

## Key Results
- Task Arithmetic is the only method that consistently improves performance over the base model across all tested configurations
- TIES-Merging and subspace-based methods (TSV-Merge, Iso-C, Subspace Boosting) suffer catastrophic performance degradation, with success probability dropping to 0% at n=12
- Constructive interference—where merged models outperform both base and best individual checkpoints—is rare and only reliably achieved with Task Arithmetic
- L2-norm distance from base model is the strongest predictor of success: methods staying below L2-norm 50 work, while those exceeding 100-300 fail

## Why This Works (Mechanism)

### Mechanism 1: Task Arithmetic Stays in the Loss Basin
Simple weighted averaging of task vectors produces constructive interference because the merged model remains close to the base model in parameter space. Task vectors computed as ΔW^i = W^i - W_0 are summed with scaling λ, and the averaging operation (α_i = 1/n) keeps the combined update bounded even as n increases, maintaining the merged model within the linearly mode-connected region around the base LLM.

### Mechanism 2: Aggressive Interference Reduction Destroys Useful Information
TIES-Merging's sign consensus and magnitude trimming remove contradictory updates but also discard complementary signal, pushing the model outside the valid region. The method trims to top-k% magnitude, enforces sign consensus, then merges only aligned parameters. For heterogeneous LLM checkpoints, this forces unnatural agreement and amplifies deviation from base.

### Mechanism 3: Subspace Methods Assume Orthogonal Task Structure That Lacks in Heterogeneous Checkpoints
SVD-based methods (TSV-Merge, Iso-C, Subspace Boosting) fail because they assume fine-tuned updates decompose into clean, task-specific subspaces—an assumption violated by diverse, overlapping LLM fine-tunes. These methods project task vectors onto dominant singular directions, orthogonalize across tasks, or flatten spectrums. When task updates overlap substantially, orthogonalization discards shared information and distorts the combined update.

## Foundational Learning

- **Concept: Mode Connectivity**
  - **Why needed here:** Task Arithmetic's success depends on merged models staying within a low-loss connected region around the base model
  - **Quick check question:** Can you explain why averaging two fine-tuned models could ever outperform either one, given they're both "optimal" for different objectives?

- **Concept: Task Vectors as First-Order Approximations**
  - **Why needed here:** Task Arithmetic treats ΔW as a transferable direction in weight space, a linear approximation to a highly nonlinear fine-tuning process
  - **Quick check question:** What happens to the validity of task vector addition if fine-tuning moves the model far enough that the loss landscape curvature changes significantly?

- **Concept: Catastrophic Forgetting vs. Forward Transfer**
  - **Why needed here:** The paper shows merged models often underperform the base model—this is forgetting. Constructive interference requires forward transfer (gaining new capabilities without losing old ones)
  - **Quick check question:** In the paper's results, why does merging more checkpoints with TIES worsen performance, while merging more with Task Arithmetic improves it?

## Architecture Onboarding

- **Component map:**
  Base LLM (W_0) → 12 Fine-tuned Checkpoints (W^i) → Task Vectors: ΔW^i = W^i - W_0 → Merging methods → Merged Model: W_merged = W_0 + λ·ΔW → lm-evaluation-harness (16 benchmarks)

- **Critical path:** The L2-norm distance from base model (‖θ_merged - θ_base‖₂) is the strongest predictor of success. Methods that stay <50 norm (Task Arithmetic, Model Stock) work; methods exceeding 100 (TIES, subspace methods) fail.

- **Design tradeoffs:**
  - Simplicity vs. Robustness: Task Arithmetic is trivial but works; TIES adds complexity and fails on LLMs
  - Diversity vs. Coherence: Random checkpoint sampling is realistic but breaks subspace assumptions; curated task-specific checkpoints would help subspace methods but reduce practical utility
  - Scale factor λ: Paper uses λ=1.0; evidence suggests this is near-optimal but sensitivity exists

- **Failure signatures:**
  - L2-norm >100 from base → almost certain degradation
  - Success probability dropping as n increases (e.g., TIES: 31% → 0%) → method is accumulating destructive interference
  - High variance at small n stabilizing at large n (TA+SB) → method is unstable but benefits from averaging diverse updates

- **First 3 experiments:**
  1. Reproduce Task Arithmetic baseline: Take Llama-3.2-3B + 12 checkpoints, merge n=4,6,8,10,12 with λ=1.0. Verify L2-norm stays <50 and success rate approaches 100% by n=6.
  2. Stress test the L2-norm hypothesis: Systematically increase λ for Task Arithmetic (λ=1.0, 1.5, 2.0, 2.5) and plot performance vs. L2-norm. Expect degradation once norm exceeds ~75-100.
  3. Controlled task separation: Instead of random checkpoints, select 4-6 checkpoints with clearly distinct tasks (e.g., math, code, medical, multilingual). Retest TIES and TSV-Merge. If performance improves, confirms the "orthogonal task structure" assumption is the limiting factor.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can pre-merging strategies, such as spectral filtering of task vectors or clustering, mitigate the failure of subspace-based merging methods on LLMs?
- **Basis in paper:** [explicit] The authors state in Section 5 that future work should "investigate whether pre-merging strategies like spectral filtering of task vectors or clustering improve performance."
- **Why unresolved:** The experimental design intentionally omitted these alignment steps to isolate the intrinsic effects of the merging algorithms themselves, leaving the potential of these pre-processing steps untested.
- **What evidence would resolve it:** Experiments showing that applying spectral filtering or clustering to the diverse fine-tuned checkpoints allows TSV-Merge or Iso-C to achieve constructive interference or at least outperform the base model.

### Open Question 2
- **Question:** Does merging-aware fine-tuning, which explicitly encourages complementary specializations, amplify constructive interference in LLMs?
- **Basis in paper:** [explicit] The Conclusion states that "merging-aware fine-tuning... may further amplify the benefits of model merging," and Section 5 suggests it as a priority for future work.
- **Why unresolved:** This study relied on arbitrary, publicly available checkpoints rather than models trained with specific objectives to be mergeable, so the potential synergy between training and merging remains theoretical.
- **What evidence would resolve it:** A study comparing the merging performance of standard fine-tuned checkpoints against those trained with a "merging-aware" loss function, measuring the frequency of constructive interference.

### Open Question 3
- **Question:** Do the failure modes of interference-aware and subspace-based merging methods generalize to encoder-decoder or multimodal architectures?
- **Basis in paper:** [explicit] Section 5 notes that the authors "intentionally focused on LLMs and did not evaluate encoder-decoder or multimodal models, where subspace geometry and fine-tuning dynamics may differ."
- **Why unresolved:** The paper establishes that subspace assumptions fail for decoder-only LLMs, but it remains unknown if the distinct architecture of models like T5 or multimodal models preserves the orthogonal task structure required for these methods to succeed.
- **What evidence would resolve it:** Applying the same systematic evaluation protocol to encoder-decoder and multimodal models to verify if Task Arithmetic remains the only reliable method.

### Open Question 4
- **Question:** Is the performance degradation in TIES-Merging caused specifically by the method moving the model outside the linearly mode-connected region of the base LLM?
- **Basis in paper:** [inferred] The authors hypothesize in Section 5 and Figure 4 that TIES-Merging's "aggressive" weight changes correlate with performance drops, noting a correlation between high L2-norm task vectors and accuracy degradation.
- **Why unresolved:** While the paper observes a correlation between parameter displacement (norm) and performance loss, it does not causally prove that exiting the loss basin is the sole reason for failure, as opposed to the specific mechanics of sign consensus or trimming.
- **What evidence would resolve it:** Interventional experiments that constrain the TIES update magnitude or force it to remain within the estimated mode connectivity radius to see if performance stabilizes.

## Limitations

- The study uses random checkpoint sampling, which may systematically disadvantage methods requiring coherent task separation
- Only one lambda value (λ=1.0) is tested for Task Arithmetic, leaving sensitivity to scaling factors unexplored
- The evaluation framework using lm-evaluation-harness may not capture domain-specific requirements where certain merging methods could excel
- The paper focuses exclusively on decoder-only LLMs without testing encoder-decoder or multimodal architectures

## Confidence

**High Confidence:**
- Task Arithmetic's consistent outperformance over other methods is well-supported by systematic evaluation across multiple model sizes and checkpoint subsets
- The correlation between L2-norm distance from base model and performance degradation is strongly evidenced by numerical experiments

**Medium Confidence:**
- The explanation that Task Arithmetic succeeds due to staying within the loss basin assumes mode connectivity holds for the specific checkpoints studied
- The claim that aggressive interference reduction destroys useful information in TIES is supported by results but could be context-dependent

**Low Confidence:**
- The assertion that subspace methods fundamentally require orthogonal task structure may be premature without testing on deliberately separated task sets
- The conclusion that current merging methods don't generalize to modern LLMs is based on random sampling methodology that may not represent all practical use cases

## Next Checks

1. **Lambda Sensitivity Analysis:** Systematically vary λ from 0.5 to 2.5 for Task Arithmetic and measure the breaking point where L2-norm exceeds 100 and performance degrades.

2. **Controlled Task Separation Experiment:** Repeat the merging experiments using 4-6 checkpoints with explicitly distinct tasks (math, code, medical, multilingual) rather than random sampling.

3. **Fine-tuning from Merged Models:** Take successful Task Arithmetic merged models and continue fine-tuning them on the same task distribution to measure whether merged models converge faster or achieve better final performance than original fine-tuned checkpoints.