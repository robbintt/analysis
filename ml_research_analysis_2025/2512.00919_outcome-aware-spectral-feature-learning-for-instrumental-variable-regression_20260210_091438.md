---
ver: rpa2
title: Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression
arxiv_id: '2512.00919'
source_url: https://arxiv.org/abs/2512.00919
tags:
- features
- learning
- spectral
- singular
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of causal effect estimation with
  hidden confounders using nonparametric instrumental variable (IV) regression, focusing
  on a fundamental limitation of existing spectral feature learning methods that are
  agnostic to the outcome variable. The authors introduce Augmented Spectral Feature
  Learning, a framework that makes feature learning outcome-aware by incorporating
  information from the outcome into the spectral learning process.
---

# Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression

## Quick Facts
- arXiv ID: 2512.00919
- Source URL: https://arxiv.org/abs/2512.00919
- Reference count: 40
- Primary result: Outcome-aware spectral feature learning that incorporates outcome information into the spectral learning process, achieving superior performance when standard methods fail due to spectral misalignment.

## Executive Summary
This paper addresses a fundamental limitation of spectral feature learning methods for instrumental variable regression: their inability to incorporate outcome information when learning features. The authors introduce Augmented Spectral Feature Learning (ASFL), a framework that learns outcome-aware features by minimizing a novel contrastive loss derived from an augmented operator that incorporates outcome information. The method is theoretically grounded with non-asymptotic excess-risk bounds and demonstrates significant empirical improvements over standard spectral methods, particularly in challenging regimes where hidden confounders create spectral misalignment between the structural function and dominant singular functions of the operator.

## Method Summary
The method learns features by minimizing an augmented contrastive loss L_δ(θ,ω) = L₀(θ) - 2δE[Yψ_θ(Z)]^T ω + ω^T C_ψ_θ ω, where θ parameterizes feature networks φ_θ(x) and ψ_θ(z), and ω is an auxiliary parameter optimized jointly. This is equivalent to learning features from the top singular subspaces of an augmented operator T_δ = [T | δr₀] that incorporates outcome information r₀ = E[Y|Z]. After feature learning on dataset D̃_m, a 2SLS estimator ĥ(x) = φ(x)^T β̂ is computed on independent dataset D_n. The key innovation is making feature learning outcome-aware to address spectral misalignment, where the structural function h₀ aligns with low-singular-value directions that standard methods cannot capture.

## Key Results
- ASFL outperforms standard SpecIV (δ=0) by a wide margin in challenging regimes where standard methods fail due to spectral misalignment
- Theoretical analysis provides non-asymptotic excess-risk bound showing robustness to spectral misalignment
- Experiments on synthetic data, dSprites benchmarks (including more challenging structural functions), and off-policy evaluation in reinforcement learning demonstrate consistent improvements
- Proposed heuristic for selecting hyperparameter δ by balancing original spectral loss and outcome-aware regularization term shows practical effectiveness

## Why This Works (Mechanism)

### Mechanism 1
Augmenting the conditional expectation operator T with outcome information shifts spectral alignment so h₀ projects onto learned features even when standard methods fail. The paper constructs T_δ = [T | δr₀], an operator that appends a "column" aligned with r₀ = E[Y|Z]. When minimized via contrastive loss, this learns features from the top singular subspaces of T_δ rather than T alone. If h₀ has weak projection onto top singular functions of T but strong alignment with r₀, the perturbation can pull relevant signal into the dominant subspace. Core assumption: h₀ is at least partially recoverable from r₀ and δ is chosen so the spectral gap γ_d(δ) = ||[Λ_d(I + δ²αα^T)^{1/2}]^{-1}||^{-1} - ||Λ_d|| > 0.

### Mechanism 2
The equivalent reformulation L_δ(θ,ω) = L₀(θ) - 2δE[Yψ_θ(Z)]^T ω + ω^T C_ψ_θ ω enables stable gradient-based optimization by avoiding direct backpropagation through a matrix inverse. Instead of computing ω* = δC_ψ^{-1} E[Yψ_θ(Z)] analytically, the paper treats ω as an auxiliary parameter optimized jointly. For fixed θ, the loss is convex in ω with closed-form optimum matching the profile loss. This avoids numerical instability from differentiating through C_ψ^{-1} during training. Core assumption: C_ψ_θ remains well-conditioned throughout training.

### Mechanism 3
In the "bad" scenario where h₀ aligns with a low-singular-value direction v_k, choosing δ large enough ensures h₀ becomes the dominant direction of T_δ, enabling recovery with d=1 feature. The perturbation structure T_δ = [T | δr₀] = [T | δTh₀] reshapes the spectrum. When h₀ ≈ α_k v_k for large k (deep in the spectrum of T), setting δ ≫ λ_k^{-1} amplifies the perturbation term until γ_1(δ) > 0, making the aligned direction recoverable. Core assumption: the signal-to-noise ratio ||s₁||/||q₁|| ≫ λ_k^{-2}, i.e., h₀'s projection onto the target direction dominates residual components.

## Foundational Learning

- **Singular Value Decomposition of Compact Operators**: The entire method hinges on understanding that T admits SVD T = Σ λ_i u_i ⊗ v_i and that SpecIV approximates T_d = Σ_{i=1}_d λ_i u_i ⊗ v_i. Without grasping that low-rank approximation targets top singular subspaces, the "spectral misalignment" problem is unintelligible. Quick check: If h₀ has coefficients α_i = ⟨h₀, v_i⟩ concentrated at i=100 with λ_100 ≪ λ_1, why would a d=10 spectral feature learner fail?

- **Two-Stage Least Squares (2SLS) in Feature Space**: The final estimator is β̂_{2SLS} = Ĉ_{ZX}^{-1} ĝ where ĝ = (1/n) Σ Y_i ψ_θ(Z_i). Understanding that 2SLS solves an inverse problem in the learned basis is essential for interpreting Theorem 1's error decomposition. Quick check: In 2SLS, why do we regress Y on ψ(Z) rather than on φ(X)?

- **Perturbation Theory for Singular Subspaces (Wedin's sin-Θ Theorem)**: The analysis bounds ||Π_{U_d} - Π_{ψ_θ}|| using perturbation results. This quantifies how much the learned Z-features deviate from ideal signal-capturing subspaces when T is perturbed to T_δ. Quick check: If ||T_δ - T|| < γ_d/2 where γ_d = σ_d(T) - σ_{d+1}(T), what does Wedin's theorem guarantee about subspace perturbation?

## Architecture Onboarding

- **Component map**: Feature Networks φ_θ(x) ∈ R^d and ψ_θ(z) ∈ R^d (MLPs) -> Auxiliary Parameter ω ∈ R^d -> Loss Computation L_δ = L₀(θ) - 2δE[Yψ_θ(Z)]^T ω + ω^T Ĉ_ψ_θ ω -> 2SLS Stage β̂_{2SLS} = Ĉ_{ZX}^{-1} ĝ

- **Critical path**: 1) Initialize φ_θ, ψ_θ, ω randomly 2) On each batch from D̃_m: compute φ(X), ψ(Z), Ĉ_ψ, then L_δ(θ,ω) 3) Backprop to update θ; update ω via its convex subproblem 4) After convergence, freeze features and compute β̂_{2SLS} on held-out D_n

- **Design tradeoffs**: δ selection: monitor L₀ vs R_δ balance (Figure 4). Increase δ while L₀ remains stable; stop when R_δ dominates. Feature dimension d: Lower d (16 vs 32) reduces overfitting risk but limits expressivity. Paper found d=16 better for dSprites h_new.

- **Failure signatures**: 1) δ too large: L₀ spikes, features collapse to only predict r₀, 2SLS stage fails with ill-conditioned Ĉ_{ZX} 2) No spectral gap: If γ_d(δ) ≤ 0, Proposition 7 bounds become vacuous; features may not converge to meaningful subspace 3) Instrument weakness: If smallest singular value σ_d(Ĉ_{ZX}) is near zero, Theorem 1's c_φ,ψ^{-1} term explodes estimation error

- **First 3 experiments**: 1) Synthetic alignment test: Replicate Figure 2 with known T = 1_Z ⊗ 1_X + Σ σ_i u_i ⊗ v_i. Vary c_α (controls h₀ alignment) and c_σ (controls spectral decay). Confirm δ>0 helps most when c_α is large (misaligned) and c_σ is moderate (some singular values learnable). 2) Ablate δ selection heuristics: Compare the two proposed methods (L₀/R_δ balance vs. estimated projection length) on dSprites h_new. Check if they select similar δ values (Figure 13 suggests yes). 3) Diagnostic: estimate spectral alignment pre/post augmentation: For a fixed δ, compute empirical SVD of learned T̂_δ and estimate ||Π_{v̂_i} h₀||². Verify that δ>0 increases projection onto learned features compared to δ=0 (as in Figure 4 right panel).

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical analysis of Augmented Spectral Feature Learning be extended to general rank-K perturbations? The paper states that "A complete theoretical analysis of general rank K perturbations requires further development of the perturbation framework and is left for future work." This is unresolved because the current theoretical guarantees are restricted to the rank-one augmentation case. Extending this to higher ranks requires a more complex analysis of how the perturbation affects the singular value decomposition of the operator T_δ. A derivation of non-asymptotic excess-risk bounds for the 2SLS estimator under a rank-K perturbation regime would resolve this.

### Open Question 2
Is there a theoretically consistent method for selecting the hyperparameter δ? The authors identify "Further investigation on selection of this parameter... as an important topic for future work" and note that the theoretical justification for minimizing the stage-2 loss remains "elusive." The paper proposes heuristics, such as balancing the original spectral loss against the regularization term, but these lack rigorous theoretical justification or consistency guarantees. A theoretically grounded criterion for δ selection that provably converges to an optimal value or a proof of consistency for the proposed heuristic methods would resolve this.

### Open Question 3
Can architecture-specific generalization bounds be derived for the optimality gap E_d in the spectral contrastive learning setting? The authors state that bounding the optimality gap E_d "requires architecture-specific generalization bounds for DNN training with the spectral contrastive loss, an open problem in its own right." The current error analysis relies on the optimality gap E_d as a term but does not provide explicit bounds for it based on the neural network architecture (e.g., depth, width) or sample complexity of the feature learning stage. A theoretical bound relating the size of the neural network and the number of training samples to the error in approximating the truncated SVD of the operator T_δ would resolve this.

## Limitations
- Reliance on the operator T being compact and requiring a spectral gap γ_d(δ) > 0 for feature learning to succeed; when this gap vanishes, error bounds become vacuous
- The proposed δ selection heuristic is empirical and may not generalize across all problem regimes
- Orthonormal regularization scheme and exact initialization scheme are underspecified in the main text, potentially affecting reproducibility

## Confidence

- **High Confidence**: The core mechanism of augmenting T with outcome information to address spectral misalignment is well-grounded theoretically and supported by empirical evidence across multiple domains (synthetic, dSprites, OPE)
- **Medium Confidence**: The equivalence between the contrastive loss and low-rank decomposition is rigorously established, but the practical impact of this formulation on optimization stability requires further validation
- **Medium Confidence**: The non-asymptotic excess-risk bound (Theorem 1) provides theoretical justification, though the assumptions about compactness and spectral gaps may not hold in all real-world scenarios

## Next Checks

1. **Spectral Alignment Diagnostics**: For a fixed δ, compute empirical SVD of learned T̂_δ and estimate ||Π_{v̂_i} h₀||² to verify that δ>0 increases projection onto learned features compared to δ=0, confirming the mechanism operates as intended

2. **δ Selection Heuristic Robustness**: Compare the two proposed δ selection methods (L₀/R_δ balance vs. estimated projection length) across different structural functions to determine which heuristic performs more consistently and whether they select similar δ values

3. **Failure Mode Characterization**: Systematically vary c_α (misalignment parameter) and c_σ (spectral decay) in synthetic data to identify precise failure boundaries where δ=0 fails but δ>0 succeeds, and where even augmented methods break down