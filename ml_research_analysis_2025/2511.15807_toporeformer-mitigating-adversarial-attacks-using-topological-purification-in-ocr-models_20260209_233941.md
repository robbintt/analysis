---
ver: rpa2
title: 'TopoReformer: Mitigating Adversarial Attacks Using Topological Purification
  in OCR Models'
arxiv_id: '2511.15807'
source_url: https://arxiv.org/abs/2511.15807
tags:
- adversarial
- attacks
- topological
- defense
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoReformer introduces a model-agnostic adversarial defense for
  OCR systems by integrating topological purification with a lightweight reformer.
  It uses a topological autoencoder to preserve the global structure of text images,
  removing adversarial perturbations without requiring adversarial training or affecting
  clean-input performance.
---

# TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models

## Quick Facts
- arXiv ID: 2511.15807
- Source URL: https://arxiv.org/abs/2511.15807
- Authors: Bhagyesh Kumar; A S Aravinthakashan; Akshat Satyanarayan; Ishaan Gakhar; Ujjwal Verma
- Reference count: 10
- TopoReformer introduces a model-agnostic adversarial defense for OCR systems by integrating topological purification with a lightweight reformer.

## Executive Summary
TopoReformer presents a novel approach to defending OCR systems against adversarial attacks by integrating topological purification with a lightweight reformer. The method leverages a topological autoencoder to preserve the global structure of text images while removing adversarial perturbations, without requiring adversarial training or degrading clean-input performance. Evaluated across multiple attack scenarios including standard attacks (FGSM, PGD, C&W), adaptive attacks (EOT, BPDA), and OCR-specific watermark attacks (FAWA), TopoReformer demonstrates significant reductions in attack success rates while maintaining high accuracy on clean inputs.

The approach is validated on MNIST, EMNIST, and a word-based OCR dataset, showing improvements in classification F1-scores and precision under adversarial conditions. Grad-CAM visualizations confirm that purified inputs lead to more focused and confident predictions. TopoReformer offers a generalizable, robust solution for safeguarding OCR systems against a wide range of adversarial threats while maintaining model-agnostic applicability across different OCR architectures.

## Method Summary
TopoReformer introduces a model-agnostic adversarial defense for OCR systems by integrating topological purification with a lightweight reformer. The core innovation lies in using a topological autoencoder to preserve the global structure of text images while effectively removing adversarial perturbations. Unlike traditional adversarial training approaches, TopoReformer does not require retraining on adversarial examples and maintains clean-input performance while providing robust defense. The method is designed to be lightweight and generalizable across different OCR architectures, making it a versatile solution for various text recognition systems.

## Key Results
- Reduces attack success rates across standard attacks (FGSM, PGD, C&W), adaptive attacks (EOT, BPDA), and OCR-specific watermark attacks (FAWA)
- Improves classification F1-scores and precision under adversarial conditions
- Maintains over 94% accuracy on clean inputs while providing robust defense
- Grad-CAM visualizations confirm more focused and confident predictions on purified inputs

## Why This Works (Mechanism)
TopoReformer works by leveraging topological purification to preserve the essential structural features of text images while removing adversarial perturbations. The topological autoencoder identifies and maintains the global structure of the input text, effectively filtering out noise and perturbations that don't conform to the expected topological patterns of text. This approach is particularly effective because adversarial attacks often introduce local perturbations that disrupt the global structure, while the topological purification process can distinguish between legitimate text features and adversarial modifications.

The lightweight reformer component then processes the purified output, ensuring that the defense mechanism remains computationally efficient and model-agnostic. By focusing on structural preservation rather than adversarial training, TopoReformer avoids the performance degradation typically associated with defensive mechanisms that rely on training on adversarial examples.

## Foundational Learning
- **Topological Autoencoder**: A neural network variant that learns to preserve global topological structures rather than just pixel-level features. Needed to distinguish between legitimate text structure and adversarial perturbations. Quick check: Verify that the autoencoder can reconstruct clean text while filtering adversarial noise.
- **Adversarial Attack Taxonomy**: Understanding different attack types (white-box, black-box, adaptive) is crucial for evaluating defense effectiveness. Needed to design comprehensive evaluation benchmarks. Quick check: Confirm that evaluation covers both standard and adaptive attack variants.
- **OCR Model Architecture**: Knowledge of how text recognition models process inputs and generate predictions. Needed to ensure the defense mechanism is compatible with different OCR architectures. Quick check: Validate compatibility with multiple OCR model types.
- **Grad-CAM Visualization**: Technique for visualizing model attention and prediction confidence. Needed to demonstrate that purified inputs lead to more focused predictions. Quick check: Verify that attention maps show increased focus on relevant text regions post-purification.
- **Model-Agnostic Design**: Understanding principles for creating defense mechanisms that work across different model architectures. Needed to ensure broad applicability. Quick check: Test the method across multiple OCR architectures.
- **Computational Efficiency Tradeoffs**: Balancing defense effectiveness with processing overhead. Needed to maintain practical usability. Quick check: Measure inference time overhead compared to baseline OCR models.

## Architecture Onboarding

**Component Map**: Input Image -> Topological Autoencoder -> Lightweight Reformer -> Purified Output -> OCR Model

**Critical Path**: The topological autoencoder serves as the primary defense mechanism, processing input images to remove adversarial perturbations while preserving essential text structure. The lightweight reformer then optimizes the purified output for OCR processing, ensuring minimal computational overhead.

**Design Tradeoffs**: The method trades some potential defense strength for computational efficiency and model-agnostic compatibility. By avoiding adversarial training, it maintains clean-input performance but may be less effective against highly sophisticated attacks specifically designed to bypass topological purification.

**Failure Signatures**: Potential failures include inability to remove complex adversarial patterns that mimic legitimate text structure, or over-purification that removes genuine text features. The method may also struggle with real-world OCR scenarios involving complex fonts, backgrounds, and layouts not represented in evaluation datasets.

**First Experiments**:
1. Baseline comparison: Measure OCR accuracy on clean inputs with and without TopoReformer to verify clean-input performance preservation
2. Standard attack evaluation: Test against FGSM, PGD, and C&W attacks to establish baseline defense effectiveness
3. Adaptive attack testing: Evaluate performance against EOT and BPDA attacks to assess robustness to white-box scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on MNIST, EMNIST, and word-based OCR dataset limits generalizability to complex real-world OCR scenarios
- Robustness against novel attack strategies specifically targeting topological purification mechanisms not fully explored
- Effectiveness in real-world scenarios with complex fonts, backgrounds, and layouts remains uncertain
- Limited testing across diverse OCR architectures despite model-agnostic claims

## Confidence

| Claim | Confidence |
|-------|------------|
| Maintains clean-input performance while providing robust defense | Medium |
| Offers generalizable, model-agnostic solution | Low |
| Grad-CAM visualizations demonstrate more focused predictions | Low |

## Next Checks
1. Evaluate TopoReformer on real-world OCR datasets with complex fonts, backgrounds, and layouts to assess generalizability beyond MNIST and EMNIST.
2. Test TopoReformer against a broader range of novel adversarial attacks, including those specifically designed to bypass topological purification mechanisms.
3. Conduct ablation studies to quantify the individual contributions of the topological autoencoder and the lightweight reformer components to overall defense performance.