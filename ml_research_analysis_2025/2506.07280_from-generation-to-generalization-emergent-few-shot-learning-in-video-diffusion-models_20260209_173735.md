---
ver: rpa2
title: 'From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion
  Models'
arxiv_id: '2506.07280'
source_url: https://arxiv.org/abs/2506.07280
tags:
- training
- tasks
- input
- video
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a few-shot fine-tuning framework for adapting
  pre-trained Video Diffusion Models (VDMs) to diverse vision tasks. They reformat
  input-output image pairs into short video sequences representing transitions between
  them, then fine-tune LoRA adapters on these videos using a frozen VDM.
---

# From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models

## Quick Facts
- **arXiv ID**: 2506.07280
- **Source URL**: https://arxiv.org/abs/2506.07280
- **Reference count**: 40
- **Primary result**: Achieves 16.75% accuracy on ARC-AGI using few-shot fine-tuning of pre-trained Video Diffusion Models

## Executive Summary
This paper introduces a novel few-shot learning framework that adapts pre-trained Video Diffusion Models (VDMs) to diverse vision tasks by reformulating input-output image pairs as short video sequences. The authors fine-tune LoRA adapters on these transition videos while keeping the VDM frozen, enabling efficient adaptation across tasks ranging from low-level vision (segmentation, pose estimation) to high-level reasoning (ARC-AGI). Their approach demonstrates strong generalization using only 3-30 examples per task, achieving state-of-the-art few-shot performance on ARC-AGI (16.75% accuracy) and competitive results on segmentation and pose estimation benchmarks.

## Method Summary
The framework converts task image pairs into transition videos using interpolation functions, then fine-tunes LoRA adapters on these videos while keeping the VDM frozen. The method leverages CogVideoX1.5-5B I2V model with rank-64 LoRA targeting QKVO attention modules. Training uses quadratic interpolation for video creation, AdamW optimizer (lr=1e-4, batch_size=2), and inference extracts the final frame from generated videos. The approach achieves few-shot adaptation across diverse tasks by aligning them with the VDM's generative interface.

## Key Results
- Achieves 16.75% accuracy on ARC-AGI benchmark using only 3-30 examples
- Matches or exceeds supervised state-of-the-art on COCO segmentation (39.6 mIoU) and pose estimation (86.8% match rate) with minimal training data
- Demonstrates strong generalization from low-level vision tasks to abstract reasoning problems
- Shows quadratic and discrete interpolation strategies outperform linear interpolation for complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
Video Diffusion Models internalize structured visual priors through temporal coherence learning. The objective of modeling coherent video sequences forces the model to learn physical dynamics, object permanence, and compositional understanding. These pre-existing structures are then repurposed for downstream tasks via fine-tuning. This assumes video pretraining provides superior priors compared to static image training.

### Mechanism 2
Static image tasks are reframed as visual transitions between input and output frames, aligning downstream tasks with the VDM's pre-training interface. By encoding task pairs as video sequences, the model generates solutions by denoising paths from query input to answer frame. This assumes the VDM can represent semantic transformations as valid spatiotemporal trajectories in pixel space.

### Mechanism 3
Low-Rank Adaptation (LoRA) efficiently routes the frozen VDM's existing visual knowledge toward task-specific outputs without overwriting base capabilities. By constraining adaptation to a low-dimensional subspace, LoRA allows learning task-specific "transition styles" from very few examples. This assumes solutions lie within a low-dimensional subspace accessible from pre-trained weights.

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (DDPM)**
  - **Why needed here**: To understand how VDMs generate videos by reversing noise processes and why the last frame of denoising sequences provides results
  - **Quick check**: Can you explain how a model generates an image by starting from Gaussian noise and iteratively predicting the noise component at each step t?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here**: This is the only trained component; understanding ΔW = BA is critical for grasping sample efficiency and avoiding catastrophic forgetting
  - **Quick check**: If a weight matrix is 1024×1024, how many parameters does a LoRA adapter with rank r=64 add, and why is this "low-rank"?

- **Concept: Inductive Biases in Video Models**
  - **Why needed here**: The core argument is that video models have better priors than image models due to understanding time/motion; you need to distinguish spatial vs. temporal attention
  - **Quick check**: Why might a model trained on video be better at predicting the physical stability of an object than a model trained only on static images?

## Architecture Onboarding

- **Component map**: Task Encoder -> Frozen CogVideoX1.5 VDM -> LoRA Adapter -> Video Decoder + Last Frame Extraction
- **Critical path**:
  1. Data Prep: Implement interpolation function φ (Linear vs. Quadratic). Note: Discrete transitions worked best for ARC-AGI reasoning
  2. Training: Minimize noise prediction loss on transition videos while updating only LoRA weights (A, B)
  3. Inference: Feed test image as frame 1, diffuse for 50 steps (or fewer for speed), extract frame F
- **Design tradeoffs**:
  - Interpolation Strategy: Linear is simple, but Quadratic or Discrete transitions often yield higher mIoU/accuracy, presumably as they better align with model's natural motion priors or "snap" to semantic states
  - Sampling Steps: Reducing steps from 50 to 5 works for segmentation/pose (efficiency gain) but degrades performance on complex reasoning (ARC-AGI)
- **Failure signatures**:
  - Color Shifts: Model consistently shifts colors between training and inference; suggests learning corrective mapping as fix
  - Motion Blur/Artifacts: If transition φ is not smooth or semantically aligned, VDM generates "motion blur" rather than clean transformation
  - Catastrophic Forgetting: If LoRA rank is too high or training steps excessive, model may hallucinate motion or lose input identity
- **First 3 experiments**:
  1. Sanity Check (Geometric): Fine-tune on 1-3 examples of simple transforms (Rotate, Flip) using Linear interpolation. Verify if VDM "transforms" image or overlays rotated texture
  2. Ablation (Interpolation): Train segmentation task (COCO) using Linear vs. Discrete interpolation. Compare mIoU to validate hypothesis that "transition path" affects visual reasoning capability
  3. Reasoning Stress Test (ARC-AGI): Run framework on single ARC task using "Discrete" interpolation. Check if output is coherent grid or degraded texture (testing "Generative World Model" hypothesis)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can composable LoRA approaches reduce per-task retraining costs while maintaining or improving few-shot generalization across diverse visual tasks? [explicit] "Future work could explore composable LoRAs [29, 43], to reduce retraining costs while enhancing generalization."
- **Open Question 2**: Can learning deterministic corrective color mappings from training samples systematically address RGB precision limitations in tasks requiring accurate color encoding/decoding? [explicit] "We observed consistent color shifts between training and test samples, suggesting the potential of learning deterministic corrective mappings from the training data."
- **Open Question 3**: Would adaptive evaluation strategies that adjust training duration based on problem difficulty improve both accuracy and computational efficiency on reasoning benchmarks? [explicit] "Our fixed schedule of training and evaluation steps represents a reasonable compromise... We believe that improvements in both performance and computational efficiency are possible through a refined evaluation strategy."

## Limitations

- Color shifts between training and test samples represent a fundamental limitation of RGB encoding for task-specific outputs
- Discrete interpolation strategy may provide strong visual cues that could be conflated with genuine abstract reasoning capabilities
- Per-task retraining requirement remains computationally intensive despite LoRA parameter efficiency
- Fixed training schedules may be suboptimal for problems of varying difficulty

## Confidence

- **High Confidence**: Empirical demonstration of LoRA fine-tuning achieving competitive performance across diverse vision tasks with minimal examples
- **Medium Confidence**: Claim that video pretraining provides superior visual priors compared to image-only pretraining (direct ablation studies not provided)
- **Low Confidence**: Assertion of "emergent" reasoning capabilities making VDMs suitable as vision foundation models (ARC-AGI results could reflect pattern matching)

## Next Checks

1. **Ablation on Pretraining Data**: Compare few-shot performance of VDM approach against identically-architected image diffusion model fine-tuned with same LoRA strategy to isolate contribution of video pretraining
2. **Interpolation Strategy Analysis**: Systematically vary interpolation methods (linear, quadratic, discrete) across multiple reasoning tasks to quantify how much ARC-AGI performance is attributable to interpolation strategy versus VDM's reasoning capabilities
3. **Generalization Stress Test**: Evaluate approach on out-of-distribution visual reasoning tasks requiring novel combinations of learned concepts to test compositional reasoning versus pattern memorization