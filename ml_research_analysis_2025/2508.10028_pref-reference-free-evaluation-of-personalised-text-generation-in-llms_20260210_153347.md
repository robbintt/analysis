---
ver: rpa2
title: 'PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs'
arxiv_id: '2508.10028'
source_url: https://arxiv.org/abs/2508.10028
tags:
- pref
- personalised
- user
- preference
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PREF introduces a reference-free evaluation framework for personalised
  text generation, using a two-stage rubric: first, a general guideline covering universal
  quality criteria, then a personalised guideline re-weighted using user profiles
  and preferences. It scores outputs with an LLM judge without needing gold references.'
---

# PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs

## Quick Facts
- arXiv ID: 2508.10028
- Source URL: https://arxiv.org/abs/2508.10028
- Reference count: 12
- Primary result: PREF achieves up to 92% accuracy on implicit preference-following tasks in personalized text generation

## Executive Summary
PREF introduces a reference-free evaluation framework for personalized text generation in LLMs, designed to assess how well generated text aligns with individual user preferences without requiring gold references. It employs a two-stage rubric—first a universal quality guideline, then a personalized guideline weighted by user profiles and preferences—and uses an LLM judge for scoring. Experiments on the PrefEval benchmark show PREF outperforms strong baselines, improving accuracy, model calibration, and ranking quality. The framework also enables smaller models to approximate the performance of larger ones, offering a scalable and interpretable approach to user-aligned evaluation.

## Method Summary
PREF operates via a two-stage rubric system: a general guideline covering universal quality criteria, followed by a personalized guideline re-weighted using user profiles and preferences. Outputs are scored by an LLM judge without requiring gold references, enabling reference-free evaluation. The framework is tested on the PrefEval benchmark, where it demonstrates improved accuracy, MSE, and nDCG compared to baselines, and supports smaller models in approximating larger model performance.

## Key Results
- Achieves up to 92% accuracy on implicit preference-following tasks
- Improves model calibration (MSE) and ranking (nDCG)
- Enables smaller models to approximate performance of larger ones

## Why This Works (Mechanism)
PREF leverages a structured rubric and LLM-as-a-judge to evaluate personalization without gold references. The two-stage rubric ensures both universal quality and user-specific alignment, while LLM scoring provides scalable, interpretable judgments. The framework's reference-free design addresses the challenge of evaluating personalized outputs, where user preferences are subjective and diverse.

## Foundational Learning
- **Reference-free evaluation**: Needed to assess outputs where gold standards don't exist; quick check: compare scores with/without gold references.
- **LLM-as-a-judge**: Provides scalable scoring; quick check: test consistency across multiple runs and LLM judges.
- **Two-stage rubric**: Ensures both universal and personalized quality; quick check: ablation studies isolating general vs. personalized components.
- **User profile weighting**: Aligns evaluation with individual preferences; quick check: user studies correlating rubric scores with satisfaction.

## Architecture Onboarding
- **Component map**: User input -> General rubric -> Personalized rubric (weighted) -> LLM judge -> PREF score
- **Critical path**: User profile → rubric re-weighting → LLM scoring → final evaluation
- **Design tradeoffs**: Reference-free vs. need for LLM judge reliability; personalization vs. rubric complexity
- **Failure signatures**: LLM judge bias, inconsistency across runs, poor rubric personalization
- **First experiments**: 1) Test rubric scoring consistency, 2) Compare PREF vs. baselines on PrefEval, 3) Ablation: general vs. personalized rubric impact

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on LLM-as-a-judge, inheriting issues of bias and inconsistency
- Lack of ablation studies isolating the impact of general vs. personalized rubric components
- No direct user studies to validate real-world satisfaction or alignment

## Confidence
- High: Claims about improved quantitative metrics (accuracy, MSE, nDCG) on PrefEval benchmark
- Medium: Claims about user alignment and interpretability, inferred from rubric design and LLM scoring
- Low: Claims about real-world applicability and robustness to diverse personalization tasks, not directly validated

## Next Checks
1. Conduct user studies to correlate PREF scores with actual user satisfaction and preference adherence in real-world personalization tasks.
2. Perform ablation studies to isolate the impact of the general and personalized rubric components on overall evaluation performance.
3. Test PREF's robustness and consistency across diverse personalization scenarios and model sizes not covered in the original benchmark.