---
ver: rpa2
title: 'Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees'
arxiv_id: '2510.04088'
source_url: https://arxiv.org/abs/2510.04088
tags:
- learning
- policy
- which
- bellman
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This article provides a comprehensive theoretical overview of offline
  reinforcement learning in large state spaces. It introduces key concepts including
  expressivity assumptions on function approximation (e.g., Bellman completeness vs.
---

# Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees

## Quick Facts
- arXiv ID: 2510.04088
- Source URL: https://arxiv.org/abs/2510.04088
- Reference count: 17
- Primary result: Comprehensive theoretical overview showing how different assumptions lead to different offline RL algorithms and guarantees, particularly focusing on the role of pessimism in relaxing data coverage requirements

## Executive Summary
This article provides a comprehensive theoretical framework for understanding offline reinforcement learning in large state spaces. The authors establish that the key challenge in offline RL stems from the "deadly triad" of function approximation, bootstrapping, and off-policy learning. They demonstrate how different expressivity assumptions (Bellman completeness vs. realizability) and data coverage requirements (all-policy vs. single-policy) lead to distinct algorithmic approaches and theoretical guarantees. A central insight is that pessimism in face of uncertainty enables algorithms to succeed with single-policy data coverage rather than requiring all-policy coverage, significantly relaxing data requirements.

## Method Summary
The paper surveys multiple algorithmic approaches to offline RL, each requiring different assumptions about function classes and data coverage. Core methods include Fitted-Q Evaluation (FQE) for when Bellman completeness holds, pessimistic variants like PEVI and PSPI that use uncertainty quantification to relax coverage requirements, and marginalized importance sampling methods (MQL/MWL) that address the curse of horizon. The analysis focuses on sample complexity bounds that depend on coverage concentrability parameters, function class complexity, and the Bellman completeness property. The framework connects theoretical assumptions directly to algorithmic design choices and performance guarantees.

## Key Results
- Pessimism allows relaxation of coverage requirements from all-policy to single-policy coverage through uncertainty quantification
- Bellman completeness is necessary for stability in iterative value function approximation, preventing divergence under realizability alone
- Marginalized importance sampling methods overcome the curse of horizon by estimating density ratios directly rather than trajectory importance weights
- Different algorithms require different combinations of expressivity assumptions and data coverage properties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing a lower confidence bound (LCB) of the value function allows an algorithm to succeed with single-policy data coverage, relaxing the requirement that the dataset must cover all candidate policies.
- **Mechanism:** Standard policy optimization fails when data lacks coverage for the optimal policy because point estimates are unreliable (high variance/undefined). Pessimism addresses this by penalizing the value estimate based on uncertainty (e.g., subtracting a bonus or minimizing over a version space). This ensures the algorithm only competes with policies that are well-supported by the data distribution $d_D$.
- **Core assumption:** Single-policy concentrability ($C_{\pi_{cp}} < \infty$), meaning the data must sufficiently cover at least one "comparator" policy.
- **Evidence anchors:**
  - [abstract] Mentions pessimism helps relax coverage from "all-policy to single-policy coverage."
  - [section 4.1] Demonstrates this via a multi-armed bandit example where LCB avoids overestimating an undersampled arm.
  - [section 4.3] Theorem 7 formally guarantees performance relative to a comparator $\pi_{cp}$ under single-policy coverage.
  - [corpus] *A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory* supports the theoretical necessity of pessimism.
- **Break condition:** If the data distribution $d_D$ has zero overlap with the visitation distribution of the best possible comparator policy (density ratio is infinite), the uncertainty/penalty term becomes unbounded, and the algorithm may output a trivial or useless policy.

### Mechanism 2
- **Claim:** Stability in iterative value function approximation (e.g., FQE) requires the hypothesis class $F$ to be closed under the Bellman operator (Bellman Completeness), rather than just containing the true value function (Realizability).
- **Mechanism:** In algorithms like Fitted-Q Evaluation, the regression target at iteration $k$ is the Bellman backup of the previous estimate ($T^\pi f_{k-1}$). If this target falls outside the function class $F$, the projection step (solving the regression) induces an approximation error that can compound and cause divergence, even if the true fixed point $Q^\pi$ lies inside $F$.
- **Core assumption:** Bellman Completeness ($T^\pi F \subseteq F$).
- **Evidence anchors:**
  - [section 3.1] Proposition 1 explicitly states FQE can diverge under realizability alone.
  - [section 3.1] Defines Assumption 1 (Bellman Completeness) as the major expressivity assumption required to mimic Value Iteration.
- **Break condition:** If the function class $F$ lacks closure (e.g., generic deep nets without specific structure), the iterative projection can create a "deadly triad" scenario leading to divergence.

### Mechanism 3
- **Claim:** Marginalized Importance Sampling (MIS) overcomes the "Curse of Horizon" by estimating the density ratio $d_\pi/d_D$ directly, rather than multiplying trajectory importance weights.
- **Mechanism:** Standard importance sampling (IS) relies on a product of action probabilities over time, leading to exponential variance scaling with horizon $H$. MIS methods (like MQL/MWL) use a discriminator to solve for the stationary density ratio $w_\pi$. This leverages the Bellman flow equation, allowing variance to scale with the state-action space density ratio rather than the trajectory probability ratio.
- **Core assumption:** Realizability of the density ratio $w_\pi \in W$ or the existence of an effective linear weight $w_{eff}$.
- **Evidence anchors:**
  - [section 2.2] Describes the "Curse of Horizon" where IS variance scales as $(C_A)^H$.
  - [section 6] Introduces MQL/MWL algorithms that achieve guarantees based on $\|W\|_\infty$ (density ratio) rather than horizon $H$.
- **Break condition:** If the data distribution $d_D$ has poor coverage, the density ratio $w_\pi$ becomes unbounded or unestimable (infinite variance). MIS also requires solving a minimax optimization, which can be computationally unstable compared to dynamic programming.

## Foundational Learning

- **Concept: Data Coverage (Concentrability)**
  - **Why needed here:** The paper fundamentally categorizes algorithms by the coverage they require. "All-policy coverage" ($\max_\pi C_\pi$) is restrictive and requires exploratory data. "Single-policy coverage" ($C_{\pi_{cp}}$) is the goal of pessimistic algorithms, requiring data only to cover one good policy. Understanding the difference between density ratio coverage ($C_\pi$) and feature coverage ($C_{sq}, C_{avg}$) is critical for assessing feasibility in large state spaces.
  - **Quick check question:** Does the dataset contain trajectories that demonstrate high-reward behaviors, or is it generated by a highly randomized or suboptimal policy?

- **Concept: The Deadly Triad (Function Approx + Bootstrapping + Off-policy)**
  - **Why needed here:** This is the theoretical root of why offline RL is hard. The paper notes (Section 3.1) that combining these three elements causes divergence. Algorithms must strictly control one of these variables (e.g., enforcing structure via Bellman Completeness or pessimism) to ensure stability.
  - **Quick check question:** Does the proposed architecture use TD-learning (bootstrapping) on data from a different distribution (off-policy) with function approximation? If so, how does it mitigate divergence (e.g., via pessimism or specific MDP structures)?

- **Concept: Expressivity vs. Structure Trade-off**
  - **Why needed here:** The paper highlights a tension between generic function approximators (unstructured) and structured ones (linear, piecewise constant). Generic classes often require Bellman Completeness (strong assumption). Structured classes (like linear MDPs or abstractions) can sometimes learn under just Realizability (Section 5).
  - **Quick check question:** Is the function class flexible enough to represent the Bellman targets of its own functions, or does it rely on the environment having low-rank dynamics?

## Architecture Onboarding

- **Component map:**
  1.  **Data Protocol:** Transition tuples $(s, a, r, s')$ or trajectories.
  2.  **Estimator:** The core engine (e.g., FQE, BRM, MIS).
  3.  **Pessimism Module:** The uncertainty quantification component (e.g., Bonus terms in PEVI, Version Spaces in PSPI).
  4.  **Optimization Oracle:** The subroutine used (e.g., Least-squares regression, Minimax optimization).

- **Critical path:**
  1.  **Coverage Analysis:** Determine if data is "exploratory" (all-policy) or "biased" (single-policy).
  2.  **Algorithm Selection:**
      - If structured data/features available + coverage is uniform: **FQE/FQI** (Simple regression).
      - If coverage is partial: **Pessimism is required.**
      - If linear features available: **PEVI** (Bonus-based, DP-friendly).
      - If general function approx needed: **PSPI** (Requires minimax oracle for pessimistic value) or **MIS** (Requires density ratio learning).
  3.  **Policy Extraction:** Greedy or Softmax (NPG) update based on the pessimistic value estimate.

- **Design tradeoffs:**
  - **PEVI vs. PSPI:** PEVI is computationally efficient (Dynamic Programming) but relies on "pointwise" uncertainty quantification (hard to compute for deep nets) and often requires specific structures (Linear MDP). PSPI is theoretically more general (handles general $F$) but requires solving a constrained optimization/minimax problem in every iteration.
  - **Pessimism vs. Behavior Regularization:** The paper notes (Section 4.6) that simple KL-regularization is often insufficient compared to explicit pessimism because it fails to bound the density ratio for OOD actions effectively.

- **Failure signatures:**
  - **Divergence:** Loss or value estimates growing unbounded during training (indicates Deadly Triad violation).
  - **Underperformance:** Policy converges to the behavior policy or worse (indicates excessive pessimism or insufficient coverage).
  - **Overestimation:** Policy outperforms behavior on the training set but fails in evaluation (indicates failure of pessimism/uncertainty quantification).
  - **Extrapolation Error:** High variance in value estimates for actions not present in the dataset.

- **First 3 experiments:**
  1.  **Validation on Linear MDP:** Implement PEVI on a synthetic linear MDP. Verify that the optimal policy is recovered under single-policy coverage and that the bonus terms correctly shrink as data increases.
  2.  **Pessimism Ablation:** Compare a standard FQI agent against a pessimistic PEVI/PSPI agent on a dataset generated by a partially random (exploratory) policy vs. a deterministic (narrow) policy. Check if FQI fails in the narrow case while PSPI succeeds relative to the best-covered policy.
  3.  **Oracle Stress Test:** Test PSPI vs. FQE. Measure the computational cost of the "pessimistic oracle" (minimax) vs. standard regression to quantify the price of single-policy coverage in wall-clock time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we handle standalone policy classes in a computationally efficient manner for continuous action spaces?
- **Basis in paper:** [explicit] The paper states in Section 4.6 that "How to handle standalone policy classes in a computationally efficient manner is still an open problem."
- **Why unresolved:** Algorithms like PSPI and PEVI rely on per-state action optimization (greedy or softmax) which is efficient for small action spaces but does not scale. A standalone class $\Pi$ requires controlling optimization errors under unknown distributions ($d_{\pi^{cp}}$), and standard error translation (Lemma 3) fails for two-sided errors.
- **What evidence would resolve it:** An algorithm using standard regression oracles that achieves single-policy coverage guarantees with a user-specified, generic policy class $\Pi$.

### Open Question 2
- **Question:** How can we reconcile the inapplicability of algorithms like PSPI to Robust MDPs (RMDPs) with transition uncertainty?
- **Basis in paper:** [explicit] Section 4.7 notes that while offline RL and RMDP are related, PSPI fails when the uncertainty set $M$ contains varying transition dynamics. The authors ask: "How to reconcile this inapplicability... will be interesting directions for future investigation."
- **Why unresolved:** PSPI's analysis relies on pushing the expectation of a sum inside the regret calculation. This fails when transition dynamics change across the uncertainty set $M$, breaking the standard performance difference analysis used in the single-agent setting.
- **What evidence would resolve it:** A provably efficient algorithm for RMDPs with large state spaces and transition uncertainty that does not assume shared dynamics across the uncertainty set.

### Open Question 3
- **Question:** How can we replace union bounds over function classes with modern complexity measures for algorithms where training labels depend on the hypothesis class?
- **Basis in paper:** [explicit] Section 7 highlights the "Intersection with Deep Learning Theory," noting that replacing union bounds with proper complexity measures for algorithms like Fitted-Q Iteration (where targets depend on previous iterates) "has proved difficult."
- **Why unresolved:** In supervised learning, labels are independent. In algorithms like FQI, the regression target ($r + \gamma f(s')$) depends on the function $f$ from the previous iteration. This dependency breaks standard concentration arguments used for VC-dimension or Rademacher complexity in deep learning.
- **What evidence would resolve it:** A generalization bound for dynamic programming algorithms using scale-sensitive complexity measures (beyond log-cardinality) that applies to neural network function approximation.

## Limitations

- Theoretical guarantees rely heavily on strong expressivity assumptions (Bellman completeness) rarely satisfied by generic deep neural networks
- Pessimism mechanisms can lead to overly conservative policies if uncertainty quantification is not well-calibrated
- Limited treatment of generalization gap between theory and practice when using deep networks trained with SGD

## Confidence

**High confidence**: The theoretical framework connecting coverage assumptions to algorithmic requirements is well-established and internally consistent. The characterization of the deadly triad and its resolution through either structure (completeness) or pessimism is theoretically sound.

**Medium confidence**: The practical applicability of these results depends critically on unknown factors - the function class structure, the quality of uncertainty estimation, and the actual coverage of real-world datasets. While MIS methods address horizon scaling, their computational stability and effectiveness in practice remain empirical questions.

**Low confidence**: The paper's treatment of generalization from theory to practice is limited. The gap between assuming a well-specified function class and using deep networks trained with SGD is substantial and not fully addressed.

## Next Checks

1. **Coverage Verification**: Develop empirical metrics to assess whether a dataset satisfies single-policy coverage for a given candidate policy, beyond theoretical concentrability bounds.

2. **Uncertainty Calibration**: Design experiments to compare different pessimism mechanisms (bonus-based vs. version space vs. MIS) on their ability to maintain calibrated uncertainty estimates in function approximation settings.

3. **Function Class Design**: Investigate architectures that might satisfy Bellman completeness for specific MDP structures (e.g., linear MDPs, low-rank dynamics) and test whether these structures can be learned from data.