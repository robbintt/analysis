---
ver: rpa2
title: Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation
arxiv_id: '2505.10781'
source_url: https://arxiv.org/abs/2505.10781
tags:
- classes
- segmentation
- network
- pseudo-labels
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles completely weakly supervised class-incremental
  learning for semantic segmentation, enabling models to learn segmentation for both
  base and novel classes using only image-level labels. The key challenge is to avoid
  dependence on dense pixel-level annotations, which are expensive to obtain, and
  to mitigate catastrophic forgetting when incrementally learning new classes.
---

# Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation

## Quick Facts
- arXiv ID: 2505.10781
- Source URL: https://arxiv.org/abs/2505.10781
- Authors: David Minkwan Kim; Soeun Lee; Byeongkeun Kang
- Reference count: 40
- One-line primary result: Achieves state-of-the-art performance in most CI-WSSS scenarios using only image-level supervision, outperforming even partially weakly supervised approaches.

## Executive Summary
This paper addresses the challenge of class-incremental learning for semantic segmentation under completely weak supervision, meaning the model must learn to segment both base and novel classes using only image-level labels, without any dense pixel-level annotations. The key innovation is a method that generates robust pseudo-labels by fusing outputs from a task-specific localizer and frozen foundation models (Grounded DINO + SAM-HQ), weighted by the localizer's uncertainty. Additionally, an exemplar-guided data augmentation approach uses a diffusion model to blend old-class exemplars into current training images, mitigating catastrophic forgetting. Experiments on PASCAL VOC and COCO-to-VOC settings demonstrate that this approach outperforms even partially weakly supervised methods, achieving state-of-the-art results in most scenarios.

## Method Summary
The method combines a trainable segmentation network (DeepLabV3 with ResNet-101) with frozen foundation models (Grounded DINO and SAM-HQ) to generate pseudo-labels from image-level supervision. A convolutional localizer is trained alongside the decoder. Pseudo-labels are created by fusing localizer outputs with foundation model outputs, weighted by the localizer's pixel-wise entropy. To prevent forgetting, an exemplar set stores cropped object images, and a diffusion model (Paint by Example) blends these exemplars into current training images for augmentation. Knowledge distillation losses ensure the current model mimics the previous task's decoder for base classes. The overall objective combines pixel-level and image-level losses with contrastive and distillation terms.

## Key Results
- The proposed method achieves state-of-the-art performance in most CI-WSSS scenarios using only image-level supervision.
- Fusing pseudo-labels from localizer and foundation models, weighted by uncertainty, significantly improves segmentation robustness compared to using either source alone.
- Exemplar-guided diffusion augmentation outperforms simple copy-paste methods in preserving old-class knowledge and improving mIoU on base classes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing pseudo-labels from a task-specific localizer with those from frozen foundation models improves segmentation robustness when dense annotations are unavailable.
- Mechanism: The system calculates pixel-wise entropy from the localizer's output to determine uncertainty. High uncertainty triggers higher weighting of the foundation model's output (Grounded DINO + SAM-HQ), while low uncertainty favors the localizer. This weighted sum produces the final soft pseudo-label used for supervision.
- Core assumption: The foundation models provide a stronger spatial prior than the task-specific localizer in ambiguous regions, and the localizer is more precise in discriminative regions.
- Evidence anchors:
  - [abstract] "...generates robust pseudo-labels by combining localizer-based pseudo-labels with those from a sequence of foundation models, weighted by the uncertainty of the localizer."
  - [section] Section 3.2, Eq. (1): "When the localizer has higher certainty... we increase reliance on Mloc... Certainty is quantified using the entropy."
  - [corpus] Related work ("Exploring CLIP's Dense Knowledge...") confirms the growing trend of leveraging vision-language models to compensate for weak supervision, though it suggests mechanisms vary by architecture.
- Break condition: If the foundation model hallucinates objects or fails to detect the specific class granularity required, and the localizer is simultaneously uncertain, the fused pseudo-label will propagate noise rather than signal.

### Mechanism 2
- Claim: Exemplar-guided diffusion augmentation mitigates catastrophic forgetting better than simple copy-paste by synthesizing realistic contexts for old classes.
- Mechanism: Instead of rigidly pasting cropped objects from a memory bank into new images, the method uses an exemplar-guided diffusion model to blend the exemplar into the background of a current training image. This creates diverse, realistic scenes containing both previous and novel classes.
- Core assumption: Realistic blending via diffusion preserves the semantic features of old classes better than disjointed copy-paste, preventing the model from learning boundary artifacts.
- Evidence anchors:
  - [abstract] "...introduce an exemplar-guided data augmentation approach that blends object images from a memory bank with current training images..."
  - [section] Section 3.3: "We believe this is because our model generates images with greater diversity... than the copy-paste augmentation method in [35]." Table 6 supports this quantitatively.
  - [corpus] Limited direct corpus evidence on diffusion-based replay for CISS specifically, but "Dynamic Robot-Assisted Surgery..." highlights the general necessity of handling evolving environments in segmentation.
- Break condition: If the diffusion model alters the texture or geometry of the exemplar object significantly during blending, the "old" class feature representation may drift, weakening the memory signal.

### Mechanism 3
- Claim: Transferring knowledge from the previous task's segmentation decoder to the current task's localizer preserves old class boundaries without accessing old ground truth.
- Mechanism: A binary cross-entropy loss ($L_{bce-loc}$) forces the current localizer to mimic the output of the frozen previous decoder for old classes. This ensures the localizer generates pseudo-labels that are consistent with the network's historical understanding of base classes.
- Core assumption: The previous decoder's outputs act as a reliable "soft teacher" for the current localizer regarding old classes.
- Evidence anchors:
  - [section] Section 3.3, Eq. (7): "...transfer knowledge from the previous network... to the current localizer... facilitating the retention of the knowledge..."
  - [corpus] "CW-BASS" and related incremental learning papers emphasize the role of consistency losses in preventing drift, supporting this transfer mechanism.
- Break condition: If the previous decoder was highly inaccurate for certain base classes, enforcing mimicry hard-codes these errors into the new localizer, blocking correction.

## Foundational Learning

- Concept: **Weakly Supervised Semantic Segmentation (WSSS)**
  - Why needed here: The entire pipeline depends on generating pixel-level pseudo-labels from image-level tags. Without understanding CAMs (Class Activation Maps) and localizer training, the "uncertainty weighting" mechanism is unintelligible.
  - Quick check question: Can you explain why a standard classifier trained on image-level labels struggles to delineate object boundaries?

- Concept: **Class-Incremental Learning (Catastrophic Forgetting)**
  - Why needed here: The architecture is designed to solve the stability-plasticity dilemma. Understanding why a neural network overwrites weights for old classes when learning new ones is required to grasp the necessity of the distillation and exemplar modules.
  - Quick check question: What happens to the accuracy of "base classes" if we train the model on "novel classes" without any constraints (e.g., no distillation loss)?

- Concept: **Open-Vocabulary / Foundation Models**
  - Why needed here: This method offloads the heavy lifting of localization to Grounded DINO and SAM. You must understand that these models have "zero-shot" capabilities allowing them to segment objects they weren't explicitly trained on in this specific dataset.
  - Quick check question: How does an open-set detector differ from a standard object detector when processing an image with a text prompt?

## Architecture Onboarding

- Component map:
  - **Trainable:** ResNet-101 Encoder, DeepLabV3 Decoder, Convolutional Localizer.
  - **Frozen Assets:** Grounded DINO (Detector), SAM-HQ (Segmentor), Diffusion Model (Augmenter), Previous Task Encoder/Decoder.
  - **Data Stores:** Exemplar Set $E$ (stores cropped object images).

- Critical path:
  1. **Input:** Image $I$ + Image-level labels.
  2. **Pseudo-label Gen:** $I \to$ Grounded DINO $\to$ SAM-HQ $\to$ Foundation Mask.
  3. **Pseudo-label Gen:** $I \to$ Encoder $\to$ Localizer $\to$ Localizer Mask.
  4. **Fusion:** Entropy(Localizer Mask) $\to$ Weighted Sum $\to$ Final Pseudo-label.
  5. **Training:** Encoder/Decoder/Localizer optimize against Final Pseudo-label + Distillation Loss + Augmented Images.

- Design tradeoffs:
  - **Compute vs. Annotation:** The system eliminates human pixel-annotation costs but significantly increases training compute by running inference on large foundation models (DINO, SAM, Diffusion) for every training sample.
  - **Memory vs. Diversity:** The exemplar set is limited (e.g., 50 images/class). The diffusion augmentation is a bet that synthetic diversity is better than real-data scarcity.

- Failure signatures:
  - **Pseudo-label Collapse:** If the localizer uncertainty is miscalibrated (e.g., high confidence on wrong pixels), the fusion mechanism fails.
  - **Artifacts in Augmentation:** If the diffusion model creates unrealistic shadows or textures, the segmentation model might learn to segment the artifacts rather than the object.
  - **Base Class Drift:** If $L_{bce-loc}$ is under-weighted, the localizer stops predicting old classes, starving the decoder of old-class pseudo-labels.

- First 3 experiments:
  1. **Ablate Pseudo-label Source:** Run the pipeline using *only* Localizer pseudo-labels vs. *only* Foundation pseudo-labels vs. Fused (Table 4 style) to validate the fusion benefit.
  2. **Augmentation Comparison:** Compare simple copy-paste augmentation vs. the proposed diffusion-based exemplar guidance to visualize the boundary quality and context preservation (Table 6 style).
  3. **Uncertainty Calibration:** Visualize the entropy maps of the localizer to verify that high entropy regions actually correspond to object boundaries or difficult backgrounds where foundation models should intervene.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed completely weakly supervised framework be effectively adapted to few-shot class-incremental scenarios where novel classes have extremely limited image samples?
- Basis in paper: [explicit] The authors state in the Discussion, "we do not address scenarios where only a few images of the novel class are present."
- Why unresolved: The current method likely assumes a sufficient distribution of images to train the localizer and generate reliable pseudo-labels; it is unknown if the foundation model integration or exemplar augmentation would overfit or fail with sparse data.
- What evidence would resolve it: Benchmarking the method's performance on standard few-shot incremental learning datasets (e.g., 1-shot or 5-shot settings) and analyzing the stability of pseudo-label generation with limited samples.

### Open Question 2
- Question: In which specific domains does the core assumption—that foundation model pseudo-labels are more accurate than localizer pseudo-labels in uncertain regions—fail, and how can this be detected?
- Basis in paper: [explicit] The Conclusion notes the method "assumes that when the pseudo-labels from the localizer are unreliable, the pseudo-labels from the foundation models are more accurate," but admits this "may not apply to images from particular domains."
- Why unresolved: The uncertainty-based fusion strategy relies on this complementary relationship. If foundation models (trained on web data) are also weak in specialized domains (e.g., space, medical), the fusion weights will be misguided, but the specific failure modes are not mapped.
- What evidence would resolve it: A comparative analysis of localizer entropy vs. foundation model IoU on out-of-distribution datasets to identify cases where high localizer uncertainty correlates with high foundation model error.

### Open Question 3
- Question: How can the proposed method mitigate the propagation of noise when constructing the memory bank, given that exemplar crops are derived from imperfect pseudo-labels?
- Basis in paper: [explicit] The Discussion highlights that "inaccuracies in the pseudo-labels can result in exemplar sets with poor-quality dense labels," potentially degrading the quality of the exemplar-guided data augmentation.
- Why unresolved: The method relies on these cropped exemplars to prevent catastrophic forgetting. Noise in these exemplars could introduce artifacts or incorrect semantic boundaries during the diffusion-based augmentation process, creating a negative feedback loop.
- What evidence would resolve it: An ablation study measuring the sensitivity of final segmentation accuracy to the IoU threshold used when filtering pseudo-labels for inclusion in the exemplar set.

## Limitations
- The method's performance heavily depends on the quality and calibration of foundation models; errors in these models can propagate through the uncertainty-weighted fusion.
- The computational overhead from running multiple foundation models (DINO, SAM, Diffusion) per training sample is significant, potentially limiting scalability.
- The paper lacks complete architectural details for the localizer (e.g., exact layer configurations), which could impact reproducibility.

## Confidence

- **High confidence:** The core mechanism of fusing pseudo-labels from localizer and foundation models weighted by uncertainty is well-supported by experimental results and ablation studies.
- **Medium confidence:** The exemplar-guided diffusion augmentation approach is innovative, but its long-term effectiveness compared to simpler methods is less certain due to limited direct comparative evidence in the literature.
- **Medium confidence:** The knowledge transfer via binary cross-entropy loss from previous decoders to current localizers is theoretically sound, but its robustness against decoder errors is not fully validated.

## Next Checks
1. **Ablate Pseudo-label Source:** Run the pipeline using only Localizer pseudo-labels vs. only Foundation pseudo-labels vs. Fused (Table 4 style) to validate the fusion benefit.
2. **Augmentation Comparison:** Compare simple copy-paste augmentation vs. the proposed diffusion-based exemplar guidance to visualize the boundary quality and context preservation (Table 6 style).
3. **Uncertainty Calibration:** Visualize the entropy maps of the localizer to verify that high entropy regions actually correspond to object boundaries or difficult backgrounds where foundation models should intervene.