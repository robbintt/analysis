---
ver: rpa2
title: 'You Only Forward Once: An Efficient Compositional Judging Paradigm'
arxiv_id: '2511.16600'
source_url: https://arxiv.org/abs/2511.16600
tags:
- yofo
- arxiv
- image
- training
- requirements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: YOFO addresses the challenge of fine-grained multimodal judging
  in high-throughput settings by observing that judgment can be reduced to verifying
  whether inputs satisfy structured requirements. The method uses a template-conditioned
  approach where an autoregressive model judges all requirements in a single forward
  pass by reading logits at designated positions, achieving orders-of-magnitude speedups
  while preserving interpretability.
---

# You Only Forward Once: An Efficient Compositional Judging Paradigm

## Quick Facts
- arXiv ID: 2511.16600
- Source URL: https://arxiv.org/abs/2511.16600
- Authors: Tianlong Zhang; Hongwei Xue; Shilin Yan; Di Wu; Chen Xu; Guannan Zhang; Yunyun Yang
- Reference count: 18
- Primary result: YOFO achieves state-of-the-art fine-grained multimodal judging with 10-100x speedups by extracting multiple binary judgments from a single forward pass

## Executive Summary
YOFO addresses the challenge of fine-grained multimodal judging in high-throughput settings by observing that judgment can be reduced to verifying whether inputs satisfy structured requirements. The method uses a template-conditioned approach where an autoregressive model judges all requirements in a single forward pass by reading logits at designated positions, achieving orders-of-magnitude speedups while preserving interpretability. Extensive experiments show YOFO achieves state-of-the-art results on recommendation tasks compared to traditional single-score methods, supports dependency-aware analysis where later judgments condition on earlier ones, and benefits from post-hoc Chain-of-Thought reasoning. The method demonstrates strong generalization across domains without requiring domain-specific fine-tuning.

## Method Summary
YOFO is a compositional judging paradigm that transforms multimodal judgment tasks into structured requirement verification. The method formats inputs as templates with requirements ending in "unknown" tokens, then performs a single forward pass through an MLLM to extract binary yes/no judgments by reading logits at designated positions. Training uses selective position masking, supervising only answer and reason positions while the model learns to predict without observing ground-truth answers to previous requirements. The approach supports both independent judgments and dependency-aware analysis where later judgments can condition on earlier ones through causal attention.

## Key Results
- Achieves 10-100x speedups compared to sequential autoregressive judging methods
- Outperforms traditional single-score methods on recommendation tasks with 3.7-4.8% ranking error rate vs 16.2% baseline
- Demonstrates strong generalization across domains without fine-tuning (SA-1B → Fashion transfer)
- Supports dependency-aware analysis with 99.1% accuracy on sequential reasoning tasks after dependency training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple binary judgments can be extracted simultaneously from a single forward pass by reading logits at designated template positions
- Mechanism: During forward propagation, decoder-based MLLMs compute next-token distributions at every position. YOFO exploits this by inserting an `unknown` token after each requirement and reading the logits at position `pos_i - 1` (the position immediately before each `unknown`). The logit distribution at this position naturally encodes the yes/no probability for that requirement without autoregressive decoding.
- Core assumption: The model can produce accurate predictions at each target position without having observed ground-truth labels for earlier positions during training—a departure from standard autoregressive supervision.
- Evidence anchors:
  - [abstract]: "produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement"
  - [Section 3.2, Eq. 3-5]: Explicitly defines logit extraction `l_i = h[pos_i - 1]` and binary decision rule comparing P("yes") vs P("no")
  - [corpus]: Limited direct evidence; related work "Judge Anything" explores MLLM judging broadly but not this parallel extraction technique

### Mechanism 2
- Claim: Supervising only answer positions (not intermediate tokens) enables non-autoregressive multi-judgment learning while preserving causal structure
- Mechanism: The loss `L_answer` applies cross-entropy only at positions predicting yes/no. Crucially, when computing `l_i` for requirement `i`, the model does not observe answers to requirements `1...(i-1)` during training. This forces each judgment to be conditioned on image+requirement text alone, rather than previous ground-truth answers.
- Core assumption: The model can generalize to predict correctly without autoregressive observation of prior labels during training.
- Evidence anchors:
  - [Section 3.3]: "when the model computes l_i for the i-th requirement, it does not observe the answers to previous requirements. Therefore, L_answer fundamentally differs from the autoregressive objective"
  - [Section 4.6, Table 3]: YOFO training improves property-wise accuracy from 70.6% (base) to 91.2%
  - [corpus]: No corpus evidence for this specific position-masked supervision paradigm

### Mechanism 3
- Claim: Post-hoc Chain-of-Thought training (generating reasons before answers) can improve judgment accuracy even when reasons are omitted at inference
- Mechanism: During training, the model generates rationales (supervised via `L_reason`) before each answer. This encourages implicit reasoning representations. At inference, the reason field is omitted from the template, but the model retains improved decision boundaries. Loss is scaled: `L = L_answer + λL_reason` with `λ = 0.55`.
- Core assumption: Reasoning能力 learned during training transfers to improved judgments even without explicit reason generation at inference.
- Evidence anchors:
  - [abstract]: "benefits from post-hoc Chain-of-Thought reasoning"
  - [Section 3.3, Eq. 8]: Defines combined loss with scaling coefficient
  - [Section 4.6, Table 3]: YOFO + CoT achieves 91.3% property-wise accuracy vs 91.2% without CoT (marginal +0.1%)
  - [corpus]: No corpus evidence; the improvement is small in reported experiments

## Foundational Learning

- **Autoregressive Language Models and Next-Token Distributions**
  - Why needed here: YOFO's core innovation depends on understanding that a forward pass computes logit distributions at all positions—not just the final output position.
  - Quick check question: Can you explain why `h[pos_i - 1]` contains the prediction for the token at `pos_i`?

- **Causal (Autoregressive) Attention in Decoder-Only Transformers**
  - Why needed here: Enables dependency-aware analysis where later template positions can attend to earlier ones, even though earlier answers aren't ground-truth labels.
  - Quick check question: In a causal attention mask, which positions can position 10 attend to?

- **Cross-Entropy Loss with Selective Position Masking**
  - Why needed here: Training only applies supervision at answer/reason positions while masking requirement tokens (which are inputs, not predictions).
  - Quick check question: If you have a 100-token sequence but only want loss at positions 20, 40, 60—how would you implement this?

## Architecture Onboarding

- **Component map**:
  - Query → LLM decomposition → Structured requirements list
  - Requirements → Template formatting with `unknown` tokens
  - Template + Image → Tokenization (with special tokens for masking)
  - Single forward pass through MLLM
  - Extract logits at `(pos_i - 1)` for all `i`
  - Compare P("yes") vs P("no") → Binary judgments
  - (Optional) Map judgments to downstream score via user-defined expression

- **Critical path**:
  1. Query → LLM decomposition → Structured requirements list
  2. Requirements → Template formatting with `unknown` tokens
  3. Template + Image → Tokenization (with special tokens for masking)
  4. Single forward pass through MLLM
  5. Extract logits at position `pos_i - 1` for each requirement `i`
  6. Compare P("yes") vs P("no") → Binary judgments
  7. (Optional) Map judgments to downstream score via user-defined expression

- **Design tradeoffs**:
  - **Template length vs accuracy**: More requirements increase sequence length; Figure 6 shows accuracy maintained across positions after training, but base models degrade
  - **LoRA rank**: Table 4 shows r=64 optimal; r=32 and r=128 both slightly worse
  - **Post-hoc CoT**: Adds training complexity for marginal accuracy gain (0.1%); worth it only if reasoning is genuinely valuable for task
  - **Throughput vs model size**: Qwen3-VL-2B achieves 48 pairs/s with 3.7% error; larger models (LamRA-Rank-7B) achieve only 5 pairs/s with 9.3% error

- **Failure signatures**:
  - **Position degradation**: Base models show accuracy drop after first requirement (Figure 6); if observed post-deployment, indicates insufficient template-position training
  - **Non-binary outputs**: If model outputs tokens other than yes/no, check that template formatting is consistent with training
  - **Domain shift**: Despite claimed generalization, evaluate on target domain before deployment; paper only tests SA-1B → Fashion transfer
  - **Dependency failures**: If later judgments need earlier answers but weren't trained with dependency data, accuracy drops (Section 4.5: 57.6% → 99.1% with dependency training)

- **First 3 experiments**:
  1. **Reproduce SA-1B validation metrics**: Train on SA-1B split, report `Acc_property` and `Acc_sample`. Target: >90% property-wise, >40% sample-wise. Sanity check against Table 3.
  2. **Probe dependency-aware capability**: Create test set with "The answer to this question is the opposite of the previous answer" requirements. Without dependency training, expect ~57% accuracy; with training, target >99% (Table 2).
  3. **Evaluate cross-domain reranking**: Apply trained model (no fashion fine-tuning) to LRVS-Fashion pairs. Compute ranking error rate. Target: <5% to match paper's 3.7-4.8%. Compare against Jina-Reranker-M0 baseline (16.2% error).

## Open Questions the Paper Calls Out
- Can YOFO's structured, per-requirement judgments serve as effective fine-grained reward signals for reinforcement learning of MLLMs and diffusion models?
- How does YOFO perform on tasks beyond reranking, such as multi-label classification or personalized recommendation with product/user tagging?
- How robust is YOFO to errors in the upstream LLM-based query decomposition step that generates requirement templates?
- How does YOFO's accuracy and throughput scale as the number of requirements per template increases substantially?

## Limitations
- Post-hoc Chain-of-Thought training shows only marginal 0.1% accuracy improvement, questioning its practical value
- Generalization claims are based on single domain transfer (SA-1B → Fashion); performance on truly diverse domains untested
- Heavy reliance on structured requirement templates introduces brittleness if decomposition fails or templates are malformed

## Confidence

- **High confidence**: The core mechanism of extracting multiple judgments from a single forward pass via position-indexed logits is well-supported by mathematical formulation and experimental results showing 10-100x speedups with preserved accuracy.
- **Medium confidence**: The non-autoregressive training approach is theoretically sound and validated on SA-1B, but its effectiveness on truly sequential reasoning tasks without dependency training is uncertain.
- **Low confidence**: The post-hoc Chain-of-Thought benefit has minimal empirical support, with only a 0.1% accuracy gain that may not justify the added training complexity.

## Next Checks

1. **Test post-hoc CoT effectiveness**: Design a task where reasoning should clearly improve judgments (e.g., multi-step visual reasoning) and compare YOFO with/without CoT training. Measure whether the marginal benefit scales beyond 0.1% in more reasoning-intensive scenarios.

2. **Evaluate failure modes on sequential reasoning**: Create a test set with requirements that genuinely require previous answers (e.g., "Is there a cat?" followed by "Is it wearing a collar?"). Measure accuracy degradation without dependency training and validate whether the proposed dependency-aware variant recovers performance.

3. **Cross-domain robustness testing**: Apply a YOFO model trained on SA-1B to three diverse domains (e.g., medical imaging, satellite imagery, and industrial defect detection) without any fine-tuning. Report property-wise accuracy drop across domains to quantify true generalization capability beyond the single Fashion domain test.