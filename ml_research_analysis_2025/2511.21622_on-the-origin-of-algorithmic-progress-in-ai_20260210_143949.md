---
ver: rpa2
title: On the Origin of Algorithmic Progress in AI
arxiv_id: '2511.21622'
source_url: https://arxiv.org/abs/2511.21622
tags:
- scaling
- algorithmic
- efficiency
- progress
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper decomposes the sources of algorithmic progress in AI
  training efficiency over the past decade. The authors conduct ablation experiments
  on key innovations in language models and find that most scale-invariant improvements
  contribute less than 10x efficiency gains, representing under 10% of total progress
  when extrapolated to frontier compute scales.
---

# On the Origin of Algorithmic Progress in AI

## Quick Facts
- arXiv ID: 2511.21622
- Source URL: https://arxiv.org/abs/2511.21622
- Reference count: 23
- Most scale-invariant improvements contribute <10x efficiency gains, representing <10% of total progress when extrapolated to frontier compute scales

## Executive Summary
This paper decomposes the sources of algorithmic progress in AI training efficiency over the past decade. Through ablation experiments on key innovations in language models, the authors find that most improvements are scale-invariant and contribute less than 10x efficiency gains. Scaling experiments reveal that two scale-dependent innovations—the transition from LSTMs to Transformers and the shift from Kaplan to Chinchilla scaling—account for 91% of efficiency gains at frontier compute levels. The study demonstrates that algorithmic progress is highly reference-dependent: the same sequence of innovations yields exponential growth when measured against LSTMs but zero growth when measured against dense transformers.

## Method Summary
The authors conduct ablation experiments comparing Modern Transformers (rotary positional encoding, SwiGLU activation, pre-RMSNorm, cosine decay, AdamW) against Retro Transformers (sinusoidal encoding, GeLU, post-layernorm, inverse sqrt decay) and LSTMs. They measure Compute Equivalent Gain (CEG) multipliers at fixed performance thresholds and fit power-law scaling curves across different compute scales. The experiments decompose algorithmic progress into scale-invariant improvements (<10× gains) and scale-dependent innovations that compound at larger scales, with extrapolation to frontier compute levels (~10^23 FLOPs).

## Key Results
- Scale-invariant innovations contribute less than 10× efficiency gains and under 10% of total progress at frontier compute scales
- LSTM-to-Transformer transition and Kaplan-to-Chinchilla scaling shift account for 91% of efficiency gains at large compute scales
- Algorithmic progress measurements are reference-dependent: exponential growth when measured against LSTMs, zero growth when measured against dense transformers
- Continued algorithmic improvement requires ever-increasing compute investment, disproportionately benefiting large-scale model builders

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Efficiency Gains
- **Claim**: Algorithmic efficiency gains are not constant across compute scales; certain architectural innovations provide increasing returns at larger scales.
- **Mechanism**: Different architectures exhibit different scaling exponents in their compute-optimal scaling laws. When Architecture A has a steeper exponent than Architecture B, the efficiency gap compounds as compute increases—the ratio CEG(C) grows with C rather than remaining constant.
- **Core assumption**: Power-law scaling behavior holds across the extrapolated compute range (10^16 to 10^23 FLOPs).
- **Evidence anchors**:
  - [abstract]: "Scaling experiments reveal that two scale-dependent innovations—the transition from LSTMs to Transformers and the shift from Kaplan to Chinchilla scaling—account for 91% of efficiency gains at frontier compute levels."
  - [Section 4.1.2]: "Our scaling graphs suggest that improvements in neural network architecture are not scale invariant and have increasing returns to scale effects... at a validation threshold of 4.4, the efficiency gap is 26×" compared to 6.28× at smaller scale.
  - [corpus]: "Rethinking LLM Advancement" examines compute-dependent vs independent paths, supporting scale-dependence as a real phenomenon.

### Mechanism 2: Sub-Multiplicative Algorithmic Interactions
- **Claim**: Combining multiple algorithmic improvements yields smaller gains than multiplying their individual contributions.
- **Mechanism**: Algorithmic components exhibit interdependence—replacing one component in an otherwise-modern system may degrade performance more than expected due to "system incompatibilities," analogous to putting a Model T engine in a modern car.
- **Core assumption**: The interaction effect is consistent across scales and architectures tested.
- **Evidence anchors**:
  - [Section 3.2]: "Using the estimates in Figure 1 and assuming multiplicative improvement we would expect CEG gains of 3.43×. However, the efficiency gain is only 1.33×."
  - [Section 3.2]: "Multiplying the individual improvements together, along with the LSTM to Retro Transformer improvements, yields a substantially higher predicted gain of 16.66×" vs measured 6.28×.
  - [corpus]: No direct corpus evidence on interaction effects; related papers focus on scaling exponents rather than algorithm combinations.

### Mechanism 3: Reference-Dependence of Progress Measurements
- **Claim**: The measured rate of algorithmic progress depends entirely on the choice of reference algorithm when scale-dependent innovations exist.
- **Mechanism**: CEG multipliers compare models at fixed performance, but CEG functions compare algorithms across scales. With different scaling exponents, measuring against LSTM baseline shows exponential growth, while measuring against dense Transformer shows zero growth.
- **Core assumption**: Frontier compute continues growing exponentially (~4.2×/year per Epoch AI).
- **Evidence anchors**:
  - [Section 5.2]: "Measuring the CEG multiplier with respect to LSTMs for each model Mt yields an exponential in t, with a growth rate of about 63% annually (Figure 7). However, measuring with respect to the dense Transformers yields a constant multiplier of 2×, and therefore a 0% growth rate."
  - [Section 5.2]: "One can continue to measure arbitrarily large rates of progress from some fixed point while realizing none of these gains in practice."

## Foundational Learning

- **Concept: Compute Equivalent Gain (CEG) Function vs CEG Multiplier**
  - **Why needed here**: The paper's central distinction—CEG functions describe efficiency relationships across scales, while CEG multipliers fix a performance level—is essential to understand why small-scale experiments underestimate frontier progress.
  - **Quick check question**: If Algorithm A trained with 100 FLOPs and Algorithm B trained with 25 FLOPs reach the same loss, what is B's CEG multiplier relative to A? (Answer: 4×)

- **Concept: Scaling Exponents in Neural Scaling Laws**
  - **Why needed here**: The LSTM-to-Transformer efficiency gap grows because they have different scaling exponents (~0.094 vs shallower for LSTM). Understanding that steeper exponents yield greater returns at scale is critical.
  - **Quick check question**: If Architecture A has loss scaling L ∝ C^(-0.05) and Architecture B has L ∝ C^(-0.10), which benefits more from 10× compute increase? (Answer: B—the loss reduction ratio grows with compute)

- **Concept: Kaplan vs Chinchilla Compute-Optimal Scaling**
  - **Why needed here**: The Kaplan-to-Chinchilla transition (rebalancing parameter vs data allocation) accounts for significant scale-dependent gains. Kaplan under-allocates data relative to parameters.
  - **Quick check question**: Chinchilla scaling recommends what ratio between parameter and data scaling as compute grows? (Answer: 1:1—parameters and tokens should scale equally)

## Architecture Onboarding

- **Component map**:
  - Modern Transformer: Rotary positional encoding + SwiGLU activation + pre-RMSNorm + cosine decay LR + AdamW
  - Retro Transformer: Sinusoidal encoding + GeLU + post-layernorm + inverse sqrt decay + Adam
  - LSTM baseline: 1-layer, orthogonal recurrent weights, forget gate bias +1, no dropout (set to 0 for fair comparison)

- **Critical path**:
  1. Implement baseline architectures (Modern/Retro Transformer, LSTM)
  2. Run ablations at small scale (~3.6M params, NLL=5.3 threshold)
  3. Scale across hidden dims [32–256], collect compute-optimal frontier points from 10^16 FLOP
  4. Fit power-law scaling curves, extract exponents
  5. Extrapolate CEG functions to 10^23 FLOPs frontier
  6. Decompose gains: scale-invariant (<10×) vs scale-dependent (LSTM→Transformer, Kaplan→Chinchilla)

- **Design tradeoffs**:
  - **Experiment scale**: Small-scale ablations (~10^15 FLOPs) are tractable but miss scale-dependent effects; scaling experiments require more compute
  - **Loss threshold**: Must be achievable by all architectures (paper uses NLL=5.3); lower thresholds exclude weaker models
  - **Token-to-parameter ratio**: 20 for ablations (Chinchilla-optimal), 40 for scaling (ensures frontier envelope isn't data-limited)
  - **LSTM dropout**: Historically used but set to 0 here for fair comparison; improves LSTM loss 6.5→5.9

- **Failure signatures**:
  - **SGD with batch size >128**: "Very poor performance" on transformers—use AdamW or small batches with high momentum (0.98)
  - **Post-layernorm in modern stack**: 87% worse than pre-RMSNorm
  - **Multiplicative combination assumption**: Overestimates by 2-3×; test combined ablations directly
  - **Fixed reference for progress tracking**: Can show spurious exponential or zero growth depending on baseline choice

- **First 3 experiments**:
  1. **Single-component ablations**: Train baseline transformer, revert one component at a time (SwiGLU→GeLU, rotary→sinusoidal, pre-RMSNorm→post, cosine→sqrt decay). Measure CEG multipliers at NLL=5.3.
  2. **Scaling law comparison**: Scale LSTM, Retro Transformer, Modern Transformer across hidden dimensions 32–256 with Chinchilla-optimal data. Fit power laws to frontier, compare exponents.
  3. **Combined ablation test**: Train fully retro transformer (all 4 reversions) and compare to predicted multiplicative gain. Expected: ~1.33× actual vs ~3.43× multiplicative prediction.

## Open Questions the Paper Calls Out

- **Open Question 1**: Could discovering optimal hyperparameters for older architectures (like LSTMs) close the scaling exponent gap with Transformers?
  - **Basis in paper**: [Explicit] Section 6.3 states, "We leave open the possibility of a 'golden' hyperparameters... that could significantly lower the gap... or even change the experimental scaling relationship."
  - **Why unresolved**: Finding optimal scaling for architectures requires computationally infeasible hyperparameter sweeps at large scales.
  - **What evidence would resolve it**: Large-scale hyperparameter sweeps on LSTMs that yield improved scaling exponents comparable to those of Transformers.

- **Open Question 2**: How does data quality or pruning interact with the scale-dependence of algorithmic efficiency?
  - **Basis in paper**: [Explicit] Section 6.1 notes the authors did not perform dataset ablations and cites work suggesting data pruning can change scaling laws.
  - **Why unresolved**: Perplexity is dataset-relative, making it methodologically difficult to compare performance across different training sets fairly.
  - **What evidence would resolve it**: Ablation studies showing whether data curation alters the scaling exponents of specific algorithms or merely shifts the loss curve.

- **Open Question 3**: Do Mixture-of-Experts (MoE) architectures exhibit scale-dependent exponent differences relative to dense models?
  - **Basis in paper**: [Inferred] Section 3.3 relies on literature estimates for MoE efficiency due to implementation challenges, noting that "given the uncertainty surrounding the scaling effects of MoE training, we leave detailed investigation to future work."
  - **Why unresolved**: Complex routing mechanisms make controlled small-scale ablations of MoEs difficult.
  - **What evidence would resolve it**: Compute-optimal scaling curves for MoE and dense models to determine if their performance ratios remain constant or diverge at frontier compute scales.

## Limitations

- **Extrapolation Validity**: The 91% attribution to two innovations relies on extrapolating scaling laws from ~10^15-10^16 FLOPs to ~10^23 FLOPs (10 million-fold), assuming power-law scaling continues unchanged across 7 orders of magnitude.
- **Architecture Choice Bias**: Results are based on comparing LSTMs, Retro Transformers, and Modern Transformers. The LSTM-to-Transformer transition accounts for 64% of efficiency gains, but this may reflect historical architectural evolution rather than fundamental limits.
- **Small-Scale Extrapolation**: Most efficiency gains from scale-dependent innovations manifest only at large compute scales. Measuring progress at small scales would show minimal algorithmic improvement, potentially misleading researchers about the true rate of algorithmic progress.

## Confidence

- **High Confidence**: Scale-dependent vs scale-invariant distinction is well-supported. The empirical finding that algorithmic progress is highly reference-dependent (exponential growth vs LSTM baseline, zero growth vs dense Transformer baseline) is robust and directly follows from the mathematical relationship between scaling exponents.
- **Medium Confidence**: The specific 91% attribution to two innovations is plausible but depends on extrapolation. The sub-multiplicative interaction effects (combined ablations underperform multiplicative predictions) are observed but may be architecture-specific.
- **Low Confidence**: The claim that continued algorithmic improvement requires ever-increasing compute investment. While supported by the data, this assumes no paradigm-shifting innovations emerge that provide large scale-invariant gains, which history suggests is unlikely.

## Next Checks

1. **Scaling Law Breakpoint Detection**: Train Modern and Retro Transformers across a broader range of compute scales (including 10^17-10^18 FLOPs) to empirically verify whether scaling exponents remain constant or show signs of deviation before reaching the extrapolated frontier.

2. **Cross-Architecture Scaling Comparison**: Extend scaling experiments to include alternative architectures (e.g., Mamba, RWKV, or other emerging designs) to test whether the LSTM→Transformer efficiency gap is representative or an artifact of comparing two specific architectures.

3. **Combined Innovation Interaction Testing**: Systematically test all 16 possible combinations of the 4 architectural innovations (not just individual ablations and full retro) to better characterize the interaction surface and determine if the sub-multiplicative effects are consistent or vary by combination.