---
ver: rpa2
title: 'Frame Sampling Strategies Matter: A Benchmark for small vision language models'
arxiv_id: '2509.14769'
source_url: https://arxiv.org/abs/2509.14769
tags:
- sampling
- frames
- frame
- video
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes the first controlled benchmark for small vision-language
  models (SVLMs) on video question-answering, addressing the issue of frame-sampling
  bias in existing evaluations. By standardizing frame sampling across multiple strategies
  (uniform-FPS, single-frame, and adaptive methods like MaxInfo and CSTA), the authors
  compare four leading SVLMs (Qwen2.5-VL-3B, Ovis2-2B, InternVL3-2B, and SmolVLM2-2.2B)
  on two benchmarks: VideoMME and MVBench.'
---

# Frame Sampling Strategies Matter: A Benchmark for small vision language models

## Quick Facts
- arXiv ID: 2509.14769
- Source URL: https://arxiv.org/abs/2509.14769
- Reference count: 0
- Key outcome: This paper proposes the first controlled benchmark for small vision-language models (SVLMs) on video question-answering, addressing the issue of frame-sampling bias in existing evaluations. By standardizing frame sampling across multiple strategies (uniform-FPS, single-frame, and adaptive methods like MaxInfo and CSTA), the authors compare four leading SVLMs (Qwen2.5-VL-3B, Ovis2-2B, InternVL3-2B, and SmolVLM2-2.2B) on two benchmarks: VideoMME and MVBench. Results reveal substantial performance differences when frame sampling is controlled, with uniform-FPS sampling performing best on VideoMME across all models, while performance on MVBench is model-specific. Notably, controlling for frame sampling bias reveals that previously reported rankings were skewed, with Ovis2 outperforming Qwen2.5 on MVBench under standardized sampling, contrary to literature reports. The study highlights the need for standardized frame-sampling strategies in future video understanding benchmarks and provides open-source code for reproducible evaluations.

## Executive Summary
This paper addresses a critical gap in video understanding benchmarks by identifying and controlling for frame sampling bias when evaluating small vision-language models (SVLMs). The authors systematically compare four leading SVLMs across four different frame sampling strategies on two video question-answering benchmarks. Their controlled evaluation reveals that previously reported model rankings were distorted by inconsistent sampling protocols, with the optimal sampling strategy varying significantly across both models and tasks. The work provides the first standardized framework for evaluating SVLMs on video understanding tasks and demonstrates that uniform frame sampling often provides superior temporal coherence for long-form video reasoning.

## Method Summary
The authors evaluate four small VLMs (Qwen2.5-VL-3B, Ovis2-2B, InternVL3-2B, and SmolVLM2-2.2B) on two video QA benchmarks (VideoMME and MVBench) using four frame sampling strategies: uniform-FPS (2 FPS with 96-frame cap), single-frame (center or first), MaxInfo (1000-frame pre-sampling with SVD-based selection), and CSTA (attention-based frame scoring with GoogLeNet embeddings). All models are constrained to the same visual token budget (N_max=96) to eliminate capacity-based advantages. Inference is performed on ~4 Nvidia A10G GPUs over approximately two weeks, with results showing substantial performance variations across sampling strategies and benchmarks.

## Key Results
- Uniform-FPS sampling outperforms all other strategies on VideoMME across all four SVLMs, demonstrating the importance of temporal coherence for long-form video reasoning
- On MVBench, optimal sampling strategy varies by model: InternVL3 performs best with CSTA, while Ovis2 peaks with uniform-FPS, indicating model-specific preferences
- Controlling for frame sampling bias reveals Ovis2 outperforms Qwen2.5 on MVBench, contrary to literature reports where Qwen2.5 appeared superior due to larger input capacity
- The performance gap between optimal and suboptimal sampling strategies is substantial enough to warrant standardized evaluation protocols in future video understanding benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Uniform-FPS sampling appears to preserve temporal coherence necessary for long-video reasoning, whereas adaptive methods may discard critical intermediate states.
- **Mechanism:** Uniform sampling captures frames at fixed intervals (e.g., 2 FPS), maintaining the sequential order of events. The paper suggests this "steady temporal coverage" reduces the risk of missing brief, repeated motions common in long-duration content.
- **Core assumption:** The model's temporal reasoning modules rely on consistent sequential input rather than just salient keyframes.
- **Evidence anchors:**
  - [Page 3, Results]: "Uniform-FPS sampling is best for every model... on VideoMME."
  - [Page 4, Discussion]: "VideoMME rewards steady temporal coverage: evenly spaced frames preserve event order and reduce the risk of missing brief, repeated motions."
  - [Corpus]: KFS-Bench confirms key frame sampling is crucial for efficient long-form understanding, supporting the sensitivity to selection strategies.
- **Break condition:** If a video is static or contains significant redundancy where uniform frames offer no new information, this mechanism provides diminishing returns over adaptive methods.

### Mechanism 2
- **Claim:** Optimal sampling strategy is likely contingent on the specific alignment between the model's pre-training data distribution and the target benchmark's temporal characteristics.
- **Mechanism:** Different SVLMs utilize distinct connectors (e.g., probabilistic tokenization vs. MLP projection) and pre-training objectives. The paper observes that models align better with specific strategies; for example, InternVL3 peaks with CSTA on MVBench, while Ovis2 prefers uniform-FPS.
- **Core assumption:** The "visual tokenization" method (how the connector maps features) dictates whether a model benefits more from dense uniform sequences or sparse salient frames.
- **Evidence anchors:**
  - [Page 3, Table 1]: Shows divergent optimal strategies for MVBench (InternVL3 peaks at CSTA, Ovis2 at FPS).
  - [Page 4, Discussion]: "These differences plausibly reflect mismatches in model's training data distributions..."
  - [Corpus]: Neighbor paper "Improving Video Question Answering through query-based frame selection" suggests query-relevant selection improves results, implying generic sampling is suboptimal for specific model-task pairs.
- **Break condition:** If a model is trained with a specific, fixed sampling augmentation strategy (e.g., always uniform), it may rigidly underperform on other strategies regardless of the logic.

### Mechanism 3
- **Claim:** Standardizing the visual token budget (frame count) removes evaluation bias, revealing that performance gains often stem from increased input capacity rather than superior architecture.
- **Mechanism:** By fixing the maximum frame count ($N_{max} = 96$) based on the smallest model's capacity, the authors control for input information density. This prevents larger-capacity models from "cheating" by simply ingesting more frames (e.g., 768 frames vs. 12 frames).
- **Core assumption:** Accuracy correlates positively with the number of informative frames provided, up to the model's context limit.
- **Evidence anchors:**
  - [Page 2, Inference setup]: "Fixing this upper bound ensures that all models operate under a uniform token budget..."
  - [Page 4, Discussion]: Notes Ovis2's literature score was lower partly because it was evaluated with only 12 frames vs. Qwen's 768, whereas controlled evaluation reversed the ranking.
  - [Corpus]: "Moment Sampling in Video LLMs" supports the constraint that context windows limit scaling, necessitating efficient sampling.
- **Break condition:** If the model's position embeddings or attention mechanisms fail to generalize to the fixed frame count (e.g., cannot handle 96 frames effectively), the comparison remains unfair.

## Foundational Learning

- **Concept:** **Frame Sampling Bias in Video LLMs**
  - **Why needed here:** The core contribution of the paper is identifying that current benchmarks are flawed because different models use different frame extraction rates (e.g., 1 FPS vs. 2 FPS), making cross-model comparison invalid.
  - **Quick check question:** If Model A scores 60% with 100 frames and Model B scores 55% with 10 frames, which model has better visual reasoning capacity? (Answer: Cannot determine without re-evaluating Model B with 100 frames).

- **Concept:** **Visual Token Budgeting**
  - **Why needed here:** Small VLMs (SVLMs) have limited context windows. Understanding how to map a video (infinite frames) into a fixed token limit (e.g., 96 frames) is the primary technical constraint addressed.
  - **Quick check question:** Why does the author choose $N_{max} = 96$ for all models, even those capable of handling more?

- **Concept:** **Uniform vs. Adaptive Sampling**
  - **Why needed here:** The paper compares standard (Uniform/Single) vs. Adaptive (MaxInfo/CSTA) methods. Understanding the trade-off between simple temporal coverage (Uniform) and semantic saliency (Adaptive) is required to interpret the results.
  - **Quick check question:** Which sampling method is theoretically best for a video where the critical event happens in a split second but the rest is static?

## Architecture Onboarding

- **Component map:** Raw Video -> Frame Sampling Strategy (Uniform/Adaptive) -> Vision Encoder -> Connector -> LLM -> Answer
- **Critical path:** The Frame Sampler determines the input to the Encoder; it is not learned end-to-end in this study.
- **Design tradeoffs:**
  - **Uniform FPS:** Low compute, preserves temporal order, best for long videos (VideoMME). Risk: Redundant frames, misses fast non-uniform events.
  - **Adaptive (MaxInfo/CSTA):** High compute (requires processing 1000 frames first to select 96), selects salient frames. Good for short, dense tasks. Risk: Destroys temporal continuity.
  - **Single Frame:** Lowest compute, zero temporal reasoning.
- **Failure signatures:**
  - **Large performance gap vs. literature:** Indicates the model was previously evaluated on a different token budget (sampling bias).
  - **Plateauing accuracy:** Occurs when increasing frames (e.g., >256 for Qwen2.5) introduces noise or exceeds optimal context utilization.
  - **Temporal reasoning failure:** Model fails on "Action Recognition" or "Temporal Reasoning" tasks when using Single-frame or aggressive Adaptive sampling.
- **First 3 experiments:**
  1. **Baseline Calibration:** Run the open-sourced benchmark code on a target SVLM using the paper's FPS:2:4:96 configuration. Compare the result against the paper's Table 1 to verify environment setup.
  2. **Token Budget Ablation:** Replicate Figure 2 by varying $N_{max}$ (16, 64, 96) on a subset of VideoMME. Verify if accuracy scales with frame count for your specific model.
  3. **Strategy Swap:** Evaluate the model on MVBench using both Uniform-FPS and CSTA. Determine if the model behaves like InternVL3 (prefers Adaptive) or Ovis2 (prefers Uniform) to characterize its inductive bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can frame selection be integrated directly into VLM training to jointly optimize which frames to sample for specific target tasks?
- **Basis in paper:** [explicit] The conclusion states: "A more effective strategy is to integrate frame selection into VLM training, enabling the model to jointly optimize which frames to sample for the target task."
- **Why unresolved:** Current adaptive methods (MaxInfo, CSTA) operate as training-free preprocessing steps, adding computational overhead and decoupling frame selection from task-specific learning.
- **What evidence would resolve it:** A VLM trained with learnable frame selection showing improved performance over training-free adaptive methods with reduced inference-time computation.

### Open Question 2
- **Question:** Do the frame-sampling findings generalize to larger VLMs beyond the small model category (4B parameters)?
- **Basis in paper:** [explicit] The authors acknowledge: "this study is restricted to small VLMs due to hardware constraints."
- **Why unresolved:** Larger models have greater visual token capacity and may exhibit different sensitivities to frame redundancy, temporal coverage, or adaptive selection than resource-constrained SVLMs.
- **What evidence would resolve it:** Controlled benchmarking of 7B+ parameter VLMs under identical sampling protocols, showing whether uniform-FPS remains optimal for VideoMME and whether model-dependent patterns persist on MVBench.

### Open Question 3
- **Question:** What architectural factors (vision encoder type, connector design, token compression) determine a model's optimal frame sampling strategy?
- **Basis in paper:** [inferred] The four SVLMs use distinct lightweight strategies (e.g., probabilistic visual tokenization vs. pixel unshuffle vs. token-compression MLP), yet exhibit different optimal sampling strategies on MVBench. The paper hypothesizes mismatches in training data distributions but does not isolate architectural contributions.
- **Why unresolved:** Model architecture, pretraining data, and size are confounded in the current comparison, making it unclear which factors drive sampling preferences.
- **What evidence would resolve it:** Ablation studies varying one architectural component at a time while holding others constant, measuring sampling strategy sensitivity.

## Limitations
- **Model-specific implementation details:** The paper does not specify exact model checkpoints, versions, or prompt templates used for each SVLM, which could introduce subtle variations in results.
- **Limited generalization scope:** The study focuses exclusively on multiple-choice question answering tasks, leaving open questions about whether these sampling strategy findings generalize to open-ended generation or other video understanding tasks.
- **Computational constraints vs. real-world deployment:** The standardized visual token budget (N_max=96) may not reflect real-world deployment scenarios where larger models with greater context windows are available.

## Confidence
- **High Confidence:** Uniform-FPS sampling generally outperforms other strategies on VideoMME due to its preservation of temporal coherence
- **High Confidence:** Controlling for frame sampling bias reveals previously reported model rankings were skewed by different input capacities
- **High Confidence:** The computational cost difference between strategies (MaxInfo and CSTA requiring 1000-frame pre-processing) is significant and should be considered in practical applications
- **Medium Confidence:** The optimal sampling strategy is contingent on alignment between model's pre-training data distribution and benchmark characteristics
- **Medium Confidence:** The performance differences between strategies are substantial enough to warrant standardized evaluation protocols
- **Medium Confidence:** VideoMME's emphasis on temporal reasoning makes it particularly sensitive to sampling strategies that preserve event order
- **Low Confidence:** The exact magnitude of performance improvements from optimal sampling strategy will remain consistent across all future SVLM architectures
- **Low Confidence:** Adaptive sampling methods (MaxInfo/CSTA) will never outperform uniform sampling on any video understanding task
- **Low Confidence:** The specific frame count threshold (N_max=96) is universally optimal across all small VLMs regardless of their specific architecture

## Next Checks
1. **Model Architecture Sensitivity Test:** Evaluate the benchmark across a broader range of SVLM architectures with different connector types (probabilistic visual tokenization, MLP projection, pixel unshuffle) to determine if the observed sampling strategy preferences generalize beyond the four tested models.

2. **Task Diversity Validation:** Extend the controlled sampling framework to non-MCQA video understanding tasks (e.g., open-ended generation, action detection, temporal reasoning) to assess whether the sampling strategy effects are consistent across different evaluation paradigms.

3. **Token Budget Scaling Study:** Systematically vary the visual token budget (N_max from 16 to 256) across all models and sampling strategies to precisely map the relationship between input capacity and accuracy, identifying potential plateaus or diminishing returns that could inform practical deployment decisions.