---
ver: rpa2
title: 'BLIA: Detect model memorization in binary classification model through passive
  Label Inference attack'
arxiv_id: '2503.12801'
source_url: https://arxiv.org/abs/2503.12801
tags:
- label
- attack
- inference
- privacy
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BLIA, a passive label inference attack framework
  designed to detect model memorization in binary classification models. The approach
  evaluates label memorization by intentionally flipping 50% of labels in a subset
  of training data ("canaries") and measuring the model's ability to infer these flipped
  labels using only model outputs like confidence scores and log-loss values.
---

# BLIA: Detect model memorization in binary classification model through passive Label Inference attack

## Quick Facts
- **arXiv ID**: 2503.12801
- **Source URL**: https://arxiv.org/abs/2503.12801
- **Reference count**: 18
- **Primary result**: Introduces BLIA, a passive label inference attack framework that detects model memorization by evaluating label inference on intentionally flipped training labels across six benchmarks

## Executive Summary
This paper introduces BLIA, a passive label inference attack framework designed to detect model memorization in binary classification models. The approach evaluates label memorization by intentionally flipping 50% of labels in a subset of training data ("canaries") and measuring the model's ability to infer these flipped labels using only model outputs like confidence scores and log-loss values. Experiments are conducted both without and with label differential privacy (Label-DP) using randomized response mechanisms. Results show that across six benchmarks (Census, IMDB, FashionMNIST, CIFAR-10, CIFAR-100, and Big-Vul), the proposed attacks consistently achieve success rates exceeding 50%, demonstrating that models memorize training labels even when these labels are deliberately uncorrelated with features. Notably, even with varying degrees of Label-DP, the attacks maintain moderate effectiveness, highlighting limitations in existing privacy-preserving techniques and underscoring the need for more robust methods to prevent label memorization.

## Method Summary
The BLIA framework works by injecting "canary" examples into the training data with intentionally flipped labels, then using passive inference techniques to determine whether the model has memorized these flipped labels. The attack uses only publicly available model outputs (confidence scores and log-loss values) to make inferences about the true labels. The framework evaluates effectiveness both with and without label differential privacy applied through randomized response mechanisms. The attack is specifically designed for binary classification tasks and measures success rates based on the model's ability to correctly infer the flipped labels of the canary examples.

## Key Results
- BLIA achieves success rates exceeding 50% across six benchmarks (Census, IMDB, FashionMNIST, CIFAR-10, CIFAR-100, and Big-Vul)
- The attack remains effective even when varying degrees of label differential privacy are applied through randomized response mechanisms
- Models demonstrate memorization of training labels even when labels are deliberately uncorrelated with features
- The framework successfully detects memorization vulnerabilities that existing privacy-preserving techniques fail to prevent

## Why This Works (Mechanism)
The attack exploits the fundamental property that neural networks tend to memorize training data rather than learn generalizable patterns, particularly for label information. By introducing canaries with flipped labels, the framework tests whether the model has memorized these specific label associations rather than learning meaningful feature-label relationships. The passive nature of the attack allows inference using only publicly available model outputs, making it practical and scalable.

## Foundational Learning
- **Label Memorization**: Neural networks' tendency to store training data verbatim rather than learning generalizable patterns - why needed: Core vulnerability being exploited by the attack
- **Randomized Response Mechanism**: Privacy technique that adds noise to label information - why needed: Provides the label-DP baseline for comparison
- **Canary Injection**: Technique of adding specially crafted examples to training data - why needed: Creates controlled test cases for memorization detection
- **Passive Inference**: Attack methodology that uses only publicly available model outputs - why needed: Enables practical deployment without requiring model access
- **Log-loss Analysis**: Statistical method for evaluating model confidence and prediction quality - why needed: Key metric for determining memorization
- **Binary Classification Specificity**: Framework design tailored for two-class problems - why needed: Defines the scope and limitations of the approach

## Architecture Onboarding

**Component Map**: Training Data -> Model Training -> Canary Injection -> Passive Inference -> Success Rate Evaluation

**Critical Path**: The core workflow involves: (1) injecting canaries with flipped labels into training data, (2) training the model, (3) collecting model outputs on canaries, (4) applying passive inference techniques to predict original labels, (5) measuring success rate as the attack metric.

**Design Tradeoffs**: The framework trades generality (limited to binary classification) for effectiveness and simplicity. The passive approach avoids the need for model access but may be less powerful than active attacks. The use of canaries provides controlled testing but may not perfectly represent real-world memorization patterns.

**Failure Signatures**: Attack failure occurs when success rates fall below 50%, indicating the model has not memorized the flipped labels. This could happen due to strong regularization, effective privacy-preserving techniques, or insufficient training. The attack may also fail if the canaries are too easily distinguishable from regular training data.

**First Experiments**: 1) Baseline test without canaries to establish model performance, 2) Canary injection with 10% flipped labels to test sensitivity, 3) Cross-benchmark comparison to evaluate generalizability across different data types and model architectures.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Framework is specifically designed for binary classification tasks and generalizability to multi-class problems is unclear
- The impact of dataset size and class imbalance on attack effectiveness is not thoroughly explored
- Computational complexity for large-scale models is not discussed
- Practical implications and real-world deployment considerations are not fully addressed

## Confidence

**Effectiveness of BLIA across benchmarks**: High
- Consistent success rates exceeding 50% across six diverse benchmarks
- Clear methodology and reproducible experimental setup

**Limitations in privacy-preserving techniques**: High
- Demonstrated effectiveness even with label-DP applied
- Systematic evaluation across different privacy levels

**Generalizability to non-binary tasks**: Low
- Framework explicitly designed for binary classification only
- No experiments or discussion of multi-class extensions

## Next Checks
1. Test BLIA framework on multi-class classification problems to evaluate generalizability beyond binary tasks
2. Conduct experiments with varying dataset sizes and class imbalance ratios to understand scalability limitations
3. Perform computational complexity analysis for large-scale models and real-time inference scenarios