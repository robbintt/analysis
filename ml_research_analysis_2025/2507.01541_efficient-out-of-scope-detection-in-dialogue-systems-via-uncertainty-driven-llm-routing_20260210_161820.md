---
ver: rpa2
title: Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven
  LLM Routing
arxiv_id: '2507.01541'
source_url: https://arxiv.org/abs/2507.01541
tags:
- intent
- detection
- arora
- llms
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of out-of-scope (OOS) intent
  detection in task-oriented dialogue systems, which is crucial for handling unseen
  and ambiguous user queries. The proposed method, UDRIL, is a two-step framework
  that combines a BERT-based in-scope intent classifier with uncertainty modeling
  and a fine-tuned LLM.
---

# Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing

## Quick Facts
- **arXiv ID**: 2507.01541
- **Source URL**: https://arxiv.org/abs/2507.01541
- **Authors**: Álvaro Zaera; Diana Nicoleta Popa; Ivan Sekulic; Paolo Rosso
- **Reference count**: 4
- **Primary result**: UDRIL achieves state-of-the-art results on both public benchmarks and real-world industry data with relative improvements of 2-3% over existing methods, and up to 34% compared to similar-sized LLMs.

## Executive Summary
This paper addresses the challenge of out-of-scope (OOS) intent detection in task-oriented dialogue systems, which is crucial for handling unseen and ambiguous user queries. The proposed method, UDRIL, is a two-step framework that combines a BERT-based in-scope intent classifier with uncertainty modeling and a fine-tuned LLM. The system first applies uncertainty estimation to the classifier's output, and if the uncertainty score exceeds a threshold, it routes the query to a fine-tuned LLM for final decision-making. This approach effectively balances computational efficiency and performance.

## Method Summary
UDRIL implements a modular cascade architecture where a lightweight DistilBERT classifier first processes user utterances, generating both intent predictions and [CLS] embeddings. An EC-NNK-Means uncertainty scoring mechanism computes reconstruction error on these embeddings, with high scores indicating potential OOS or misclassified cases. If the uncertainty score exceeds threshold τ, the system routes the query to a fine-tuned Llama 3.1-8B LLM via LoRA adaptation. The LLM is trained on synthetically-generated OOS examples created by sampling incorrect intents from the label space, enabling OOS detection using only in-scope training data. The framework achieves state-of-the-art performance while maintaining practical efficiency through selective LLM invocation.

## Key Results
- UDRIL achieves state-of-the-art results on both public benchmarks (HINT3 datasets) and real-world industry data
- Relative improvements of 2-3% over existing methods, and up to 34% compared to similar-sized LLMs
- The method demonstrates practical applicability with low latency and computational efficiency suitable for deployed systems
- Moderate routing strategy (τ=0.10) shows consistent improvements over both high and low routing configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Routing decisions based on reconstruction error effectively identify both out-of-scope queries and misclassified in-scope inputs.
- **Mechanism:** EC-NNK-Means learns a dictionary that minimizes reconstruction error on training embeddings. At inference, high reconstruction error correlates with distributional distance from known intents, signaling either OOS or ambiguous in-scope cases requiring refinement.
- **Core assumption:** Reconstruction error from soft-clustering approximates epistemic uncertainty for intent classification boundaries.
- **Evidence anchors:**
  - [abstract] "first step applies uncertainty estimation to the output of an in-scope intent detection classifier"
  - [section 3.2] "In Gulati et al. (2024), it is shown that new data with high reconstruction error is more likely to be OOS. We observe that this method also has satisfactory results in identifying potentially misclassified INS data"
  - [corpus] Limited direct corpus validation; neighbor papers explore uncertainty-based routing but not specifically EC-NNK-Means for OOS
- **Break condition:** If training data distribution shifts significantly without dictionary retraining, reconstruction error may become uncalibrated.

### Mechanism 2
- **Claim:** Fine-tuning an LLM with synthetically-generated OOS examples enables OOS detection using only in-scope training data.
- **Mechanism:** For each labeled in-scope utterance, the framework creates a negative example by sampling k incorrect intents and labeling the pair as OOS. The LLM learns to discriminate between semantically appropriate intent matches and forced mismatches.
- **Core assumption:** Sampling incorrect intents from the label space produces meaningful OOS training signal; the model generalizes this discrimination to true OOS inputs.
- **Evidence anchors:**
  - [section 3.3] "our method is designed to provide the LLM with OOS detection capabilities using only INS data"
  - [section 4.3] "fine-tuning improves the OOS detection capabilities of UDRIL by substantially increasing recall, with only a minor reduction in precision"
  - [corpus] LANID (arXiv:2510.14110) similarly uses LLM assistance for new intent discovery, supporting the general approach
- **Break condition:** If OOS queries are semantically similar to in-scope intents but pragmatically different, sampled negative examples may not provide adequate training signal.

### Mechanism 3
- **Claim:** The modular cascade architecture preserves classifier efficiency while selectively invoking LLM resources for uncertain cases.
- **Mechanism:** A lightweight DistilBERT classifier handles confident predictions directly. Only instances exceeding the uncertainty threshold τ trigger the more expensive LLM, creating a tunable efficiency-accuracy tradeoff through threshold adjustment.
- **Core assumption:** The uncertainty scoring function successfully separates clean predictions from those benefiting from LLM refinement.
- **Evidence anchors:**
  - [section 1] "This hierarchical approach leverages the efficiency of the BERT model for the majority of cases, while utilizing the LLM's capabilities for more ambiguous or complex inputs"
  - [section 4.4] "routing strategies above moderate yield improvements over existing models"
  - [corpus] "Confident or Seek Stronger" (arXiv:2502.04428) validates uncertainty-based on-device LLM routing as a general paradigm
- **Break condition:** If τ is set too low (excessive routing), computational savings diminish; if too high, accuracy gains are lost.

## Foundational Learning

- **Concept: Soft Clustering for OOD Detection**
  - Why needed here: Understanding why reconstruction error indicates out-of-distribution inputs
  - Quick check question: Can you explain why a trained dictionary would produce higher reconstruction error for inputs semantically distant from training data?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The LLM component uses LoRA fine-tuning; understanding parameter-efficient adaptation
  - Quick check question: How does LoRA enable fine-tuning with minimal trainable parameters while preserving base model capabilities?

- **Concept: Threshold Calibration for Routing**
  - Why needed here: System behavior is controlled by τ; understanding precision-recall tradeoffs
  - Quick check question: If you observe too many false OOS classifications, should τ increase or decrease?

## Architecture Onboarding

- **Component map:**
  User utterance → DistilBERT classifier → Top-k intents + [CLS] embedding → EC-NNK-Means uncertainty scoring → su > τ? → Yes → Llama 3.1-8B (LoRA) → Final decision (INS or OOS)
  No → Return classifier prediction

- **Critical path:**
  1. DistilBERT inference generates both prediction logits and [CLS] embedding
  2. EC-NNK-Means computes reconstruction error on [CLS] embedding
  3. Threshold comparison determines routing decision
  4. If routed, LLM receives utterance + top-3 intent descriptions via prompt

- **Design tradeoffs:**
  - **τ = 0.05 (high-routing):** More LLM calls, higher accuracy, higher latency and cost
  - **τ = 0.15 (low-routing):** Fewer LLM calls, lower accuracy, better efficiency
  - Moderate routing (τ = 0.10) offers balanced operation
  - Paper reports 70-98% of OOS utterances routed across thresholds

- **Failure signatures:**
  - Low OOS recall with high INS accuracy: τ too high, LLM underutilized
  - High OOS recall with degraded INS accuracy: τ too low, LLM over-confident on OOS
  - Routing percentage unexpectedly high: Classifier quality degraded or distribution shift
  - LLM latency violations: τ too low for SLA requirements

- **First 3 experiments:**
  1. Establish baseline: Run DistilBERT + EC-NNK-Means with τ variations to measure routing rates before LLM integration
  2. Calibrate threshold: Sweep τ ∈ {0.05, 0.08, 0.10, 0.12, 0.15} on held-out validation set, plotting OOS F1 vs. routing percentage
  3. Ablate fine-tuning: Compare UDRIL-noFT vs. UDRIL-FT to quantify contribution of LoRA fine-tuning on your domain data

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis of the paper's content and limitations, the following questions emerge:

### Open Question 1
- Question: What is the optimal, principled method for selecting the routing threshold τ that maximizes the accuracy-efficiency trade-off, rather than relying on manual threshold tuning?
- Basis in paper: [explicit] The authors state "Threshold τ can be tuned to route higher, or lower, ratio of utterances to the LLM, balancing the effectiveness and efficiency as needed. In this work, we experiment with three specific thresholds to showcase its effect on the routing ratio."
- Why unresolved: The paper only explores fixed threshold values (0.05, 0.10, 0.15) through manual experimentation without proposing an adaptive or data-driven selection mechanism.
- What evidence would resolve it: A systematic study comparing automated threshold selection methods (e.g., validation set optimization, cost-sensitive learning) across varying data distributions and latency constraints.

### Open Question 2
- Question: How does the quality and size of the first-stage classifier's training data impact UDRIL's overall effectiveness, and can the framework be made robust to noisy or limited training data?
- Basis in paper: [explicit] The authors note "Powerplay11 is the only exception... this can be attributed to the fact that ∼68% of the utterances in the test split of Powerplay11 are OOS... most likely due to the first-stage classifier: since Powerplay11's training set is of lower quality, this directly impacts both the DistilBERT classifier and the overall performance of the framework."
- Why unresolved: The framework's dependence on first-stage classifier quality creates a vulnerability when training data is poor, but no mitigation strategies are proposed.
- What evidence would resolve it: Experiments with synthetically degraded training data quality and quantity, plus evaluations of potential robustness mechanisms (e.g., ensemble classifiers, data augmentation).

### Open Question 3
- Question: Can more sophisticated negative sampling strategies for OOS training data improve the LLM's discrimination capabilities beyond simple random sampling from non-gold intents?
- Basis in paper: [inferred] Algorithm 1 samples k intents uniformly from Y \ {yu} to create negative OOS examples, but this may not capture semantically similar or adversarially confusing intent pairs that challenge real-world systems.
- Why unresolved: The paper does not compare alternative negative sampling strategies (e.g., hard negative mining, semantically similar intents) that could better prepare the LLM for difficult edge cases.
- What evidence would resolve it: Ablation studies comparing random sampling against hard negative mining, confusion-matrix-guided sampling, and diversity-aware sampling strategies.

## Limitations

- The EC-NNK-Means uncertainty scoring mechanism lacks detailed hyperparameter specification (dictionary size, training iterations) necessary for exact reproduction
- The synthetic OOS generation strategy may not capture the full semantic diversity of real OOS queries, potentially limiting robustness to truly novel user intents
- The framework's effectiveness depends heavily on the quality of the first-stage classifier's training data, creating vulnerability when training data is poor

## Confidence

**High Confidence**: The modular cascade architecture and threshold-based routing mechanism are well-established concepts with clear empirical support. The efficiency-accuracy tradeoff is demonstrated across multiple thresholds and datasets, with the moderate routing strategy (τ=0.10) showing consistent improvements over baselines.

**Medium Confidence**: The EC-NNK-Means uncertainty estimation's effectiveness relies on the assumption that reconstruction error correlates with epistemic uncertainty for intent classification. While the paper shows this works empirically, the theoretical connection between soft-clustering reconstruction error and classification uncertainty is not rigorously established.

**Low Confidence**: The LoRA fine-tuning configuration and exact prompt template for the LLM component are not fully specified. The synthetic OOS generation strategy's ability to generalize to truly novel intents remains somewhat speculative, as the evaluation focuses on known OOS categories from benchmarks rather than truly unseen intent distributions.

## Next Checks

1. **Threshold Calibration Validation**: Sweep τ across a wider range (0.03 to 0.20) on your specific domain data, plotting OOS F1 vs. routing percentage to identify the optimal efficiency-accuracy operating point. Verify that moderate routing consistently outperforms both extreme settings.

2. **Distribution Shift Sensitivity**: Test UDRIL's performance when the input distribution shifts (e.g., by introducing utterances with novel vocabulary or phrasing). Measure how quickly reconstruction error calibration degrades and whether periodic dictionary retraining is necessary.

3. **Synthetic OOS Coverage Analysis**: Systematically evaluate whether the sampled negative examples during LLM fine-tuning adequately represent the semantic space of real OOS queries. Test with OOS categories that are semantically similar to in-scope intents to assess whether the synthetic generation strategy provides sufficient training signal.