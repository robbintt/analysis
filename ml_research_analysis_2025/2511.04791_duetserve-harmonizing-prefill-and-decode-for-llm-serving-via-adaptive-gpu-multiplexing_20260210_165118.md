---
ver: rpa2
title: 'DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU
  Multiplexing'
arxiv_id: '2511.04791'
source_url: https://arxiv.org/abs/2511.04791
tags:
- prefill
- decode
- latency
- duetserve
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DuetServe addresses the challenge of maintaining high throughput
  and low latency in LLM serving by harmonizing prefill and decode phases on a single
  GPU. It dynamically partitions GPU SMs to isolate these phases only when latency
  service level objectives (SLOs) are threatened.
---

# DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing

## Quick Facts
- arXiv ID: 2511.04791
- Source URL: https://arxiv.org/abs/2511.04791
- Authors: Lei Gao; Chaoyi Jiang; Hossein Entezari Zarch; Daniel Wong; Murali Annavaram
- Reference count: 16
- Key outcome: DuetServe achieves up to 1.3x higher total throughput while maintaining lower TBT latency compared to state-of-the-art frameworks

## Executive Summary
DuetServe addresses the challenge of maintaining high throughput and low latency in LLM serving by harmonizing prefill and decode phases on a single GPU. It dynamically partitions GPU SMs to isolate these phases only when latency service level objectives (SLOs) are threatened. Key components include an attention-aware roofline model for latency prediction, a GPU partitioning optimizer, and an interruption-free execution engine. Evaluations show DuetServe achieves up to 1.3x higher total throughput while maintaining lower TBT latency compared to state-of-the-art frameworks.

## Method Summary
DuetServe implements adaptive GPU multiplexing through three core components: (1) an attention-aware roofline model that predicts latency by separately modeling token-level and sequence-level operations, (2) a partition optimizer that dynamically allocates SMs between prefill and decode streams based on TBT SLO constraints, and (3) an interruption-free execution engine using CUDA Graphs to eliminate per-iteration CPU-GPU synchronization. The system defaults to temporal sharing but switches to spatial partitioning when predicted TBT exceeds the SLO threshold, using libsmctrl for stream-level SM binding.

## Key Results
- Achieves up to 1.3x higher total throughput compared to state-of-the-art frameworks
- Maintains lower TBT latency while improving throughput
- Dynamic SM partitioning activates only when needed, avoiding permanent overhead of static disaggregation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token-budget-based schedulers underestimate latency when attention operations dominate, particularly for long-context requests, leading to TBT SLO violations even under "optimal" linear-layer utilization.
- **Mechanism:** DuetServe uses an attention-aware roofline model that separately estimates token-level operators (linear, layer norm—scale with total tokens) and sequence-level operators (attention—scale with query length × KV cache size). Total latency is computed as `t_total = L × t_block + t_cls`, where attention cost per request is `max(F_attn/Π_SM, B_attn/B_HBM)`.
- **Core assumption:** Operator-level compute and memory costs scale predictably with batch configuration and can be aggregated to estimate end-to-end latency accurately.
- **Evidence anchors:** [Section 3, Figure 2(b-c)]: Under fixed 8K token budget, prefill latency exceeds 180ms and attention accounts for ~25% of forward latency for long prompts; decode latency varies 4× across context lengths due to KV cache reads.

### Mechanism 2
- **Claim:** Adaptive SM-level spatial partitioning provides disaggregation-level isolation only when needed, avoiding permanent overhead of static disaggregation.
- **Mechanism:** By default, prefill and decode share all SMs (temporal sharing). When the roofline model predicts TBT > τ_SLO, the optimizer selects (S_p, S_d, k) to maximize throughput `ρ = (k×T_decode + T_prefill) / max(k×t_d, t_p)` subject to `t_d ≤ τ_SLO`. Prefill and decode then run concurrently on disjoint SM partitions via libsmctrl.
- **Core assumption:** GPU SMs can be dynamically repartitioned with negligible overhead, and memory bandwidth scales super-linearly with active SMs (enabling decode to meet TBT even with fewer SMs).
- **Evidence anchors:** [Section 3.2, Figure 4(a)]: 20% of SMs achieve ~60% of peak HBM bandwidth on H100; FLOPs scale linearly.

### Mechanism 3
- **Claim:** Eliminating per-iteration CPU-GPU synchronization via look-ahead decode enables uninterrupted concurrent prefill-decode execution.
- **Mechanism:** Decode kernels are pre-captured as a CUDA Graph; k future decode steps' KV cache slots and metadata are preallocated. CPU launches k graphs consecutively without waiting for intermediate sampling/filtering. Prefill kernels (dynamic shapes) launch individually but decode launches first to avoid stalls.
- **Core assumption:** Decode tensor shapes are static enough for graph capture, and k can be chosen to match prefill duration (`k ≈ t_p / t_d`).
- **Evidence anchors:** [Section 4.3, Figure 5]: Timeline shows conventional decoding has sync bubbles every iteration; look-ahead eliminates them.

## Foundational Learning

- **Concept: Roofline Model**
  - **Why needed here:** The attention-aware roofline model underpins latency prediction. Without understanding that compute-bound ops scale with FLOPs/SM throughput while memory-bound ops scale with bytes/bandwidth, the partitioning optimizer's cost model is opaque.
  - **Quick check question:** For a memory-bound decode operation on 20% of SMs achieving 60% peak bandwidth, does latency improve linearly or sub-linearly when doubling SMs?

- **Concept: GPU Spatial Multiplexing (SM Partitioning)**
  - **Why needed here:** DuetServe's core novelty is dynamically partitioning SMs within a single GPU context. Understanding MIG (static, coarse), MPS (process-level), and libsmctrl (stream-level, fine-grained) clarifies why prior approaches don't suffice.
  - **Quick check question:** Why can't MIG be used for per-iteration adaptive partitioning in LLM serving?

- **Concept: CUDA Graphs**
  - **Why needed here:** The interruption-free engine relies on capturing decode kernels as a graph. Understanding graph capture constraints (static shapes, no dynamic control flow) explains why prefill can't use graphs and why look-ahead preallocation is required.
  - **Quick check question:** If decode adds a request mid-sequence, can the pre-captured graph still be replayed? Why or why not?

## Architecture Onboarding

- **Component map:** Request Queue → Scheduler (chunked prefill + TBT check) → Roofline Model (predict t_mixed) → [If t_mixed > τ_SLO] → Partition Optimizer → (S_p, S_d, k) → Execution Engine: Stream_p (prefill, libsmctrl-bound to S_p SMs) + Stream_d (decode, CUDA Graph, libsmctrl-bound to S_d SMs) → Concurrent GPU Execution → Token Output

- **Critical path:** Scheduler → Roofline prediction → Partition decision → libsmctrl binding → Concurrent kernel launch. If roofline underestimates latency, TBT violation occurs; if partition selection is slow (>1ms), GPU idle time increases.

- **Design tradeoffs:**
  - Aggregated (high throughput, TBT risk) vs. partitioned (lower TBT, potential SM underutilization)
  - Larger k (better overlap, more KV preallocation) vs. smaller k (less memory overhead, faster adaptation)
  - Granular TPC enumeration (better optimal) vs. faster coarse search

- **Failure signatures:**
  - TBT SLO violations under high QPS: roofline model underestimates attention cost for long contexts → verify profiling covers max context length
  - GPU idle bubbles during partitioned mode: k mismatch (decode finishes before prefill) → check look-ahead k selection matches `floor(t_p/t_d)`
  - Memory exhaustion: look-ahead preallocates too many slots → reduce k or cap batch size

- **First 3 experiments:**
  1. **Validate roofline accuracy:** Profile Qwen3-8B prefill and decode across batch sizes [1, 8, 16, 32] and context lengths [512, 2048, 8192, 16384]. Compare measured latency vs. model prediction. Target: <15% error.
  2. **Stress test partitioning overhead:** Measure libsmctrl bind+launch latency for switching between aggregated and partitioned modes. Target: <1ms per switch.
  3. **End-to-end A/B test:** Run Azure-Code trace at QPS=16 with (a) static aggregated mode, (b) static partitioned S_d=22/S_p=44, (c) DuetServe adaptive. Compare TBT P99 and throughput. Target: adaptive matches or beats both static configs.

## Open Questions the Paper Calls Out

- **Question:** How does DuetServe's partitioning strategy generalize to Mixture-of-Experts (MoE) architectures where expert routing introduces dynamic, non-linear compute and memory access patterns?
  - **Basis in paper:** [inferred] The evaluation is restricted to dense models (Qwen3-8B/14B), and the roofline model relies on deterministic compute-to-token ratios.
  - **Why unresolved:** MoE models activate only a subset of experts per token, creating variable memory bandwidth and compute demands that may invalidate the static profiling assumptions used by the optimizer.
  - **What evidence would resolve it:** Benchmarking DuetServe on MoE models (e.g., Mixtral 8x7B) to analyze if the partitioning optimizer can adapt to the stochastic nature of expert activation.

- **Question:** To what extent does runtime hardware variability (e.g., thermal throttling, DVFS) degrade the accuracy of the attention-aware roofline model?
  - **Basis in paper:** [inferred] The system depends on offline profiling of peak compute throughput ($\Pi_{SM}$) and memory bandwidth ($B_{HBM}$) to determine optimal SM splits.
  - **Why unresolved:** The optimizer treats these profiled metrics as constants, but real-world sustained serving can lower hardware performance due to heat or power limits, potentially leading to unexpected SLO violations.
  - **What evidence would resolve it:** A sensitivity analysis correlating prediction error rates against GPU temperature fluctuations and clock speed variations during sustained load.

- **Question:** Does the "interruption-free" look-ahead execution mechanism negatively impact the scheduling latency of high-priority incoming requests?
  - **Basis in paper:** [inferred] The system launches $k$ decode steps consecutively via CUDA graphs to eliminate CPU synchronization overhead.
  - **Why unresolved:** While this improves throughput for ongoing requests, it creates a scheduling "blackout" period where the CPU cannot inject urgent prefill requests into the GPU until the current sequence completes.
  - **What evidence would resolve it:** Measuring the TTFT penalty for requests arriving immediately after a look-ahead dispatch versus those arriving during standard iterative scheduling.

## Limitations
- The attention-aware roofline model's accuracy may degrade for non-transformer architectures with different attention patterns
- TPC-level partitioning granularity (2 SMs per TPC) may be too coarse for optimal SM splits on certain GPU architectures
- CUDA Graph-based interruption-free engine assumes decode shapes remain static across iterations, limiting flexibility for advanced decoding strategies

## Confidence
- **High Confidence:** The fundamental observation that token-budget schedulers underestimate latency for long-context requests due to attention operations. This is well-supported by profiling data showing attention accounting for 25% of forward latency in long prompts.
- **Medium Confidence:** The effectiveness of adaptive SM partitioning in balancing TBT and throughput. While the optimization framework is sound and profiling shows linear scaling for FLOPs and sub-linear for bandwidth, real-world overhead from libsmctrl and the optimality of TPC-level granularity remain uncertain.
- **Medium Confidence:** The interruption-free execution engine's latency benefits. CUDA Graph overhead measurements (<0.5ms) and timeline comparisons support the claim, but the approach's robustness to dynamic decoding scenarios (beam search, early stopping) requires further validation.

## Next Checks
1. **Roofline Model Generalization:** Profile the attention-aware roofline model across at least three different LLM architectures (e.g., transformer-based, Mamba, RWKV) and validate prediction accuracy within 15% error margin for TBT across the full operating range.

2. **Partitioning Overhead Measurement:** Measure the complete cycle time for libsmctrl-based SM partitioning (including bind, launch, and synchronization) under high-QPS conditions to verify the claimed <1ms overhead and assess impact on GPU utilization.

3. **End-to-End SLO Compliance:** Run a comprehensive benchmark using diverse request traces (varying context lengths, batch sizes, and inter-arrival distributions) to measure actual TBT P99 latency distribution and confirm sustained SLO compliance under the target QPS levels reported in the paper.