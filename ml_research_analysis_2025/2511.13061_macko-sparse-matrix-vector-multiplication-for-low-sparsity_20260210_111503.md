---
ver: rpa2
title: 'MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity'
arxiv_id: '2511.13061'
source_url: https://arxiv.org/abs/2511.13061
tags:
- macko
- sparsity
- sparse
- memory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACKO, a GPU-optimized sparse matrix-vector
  multiplication (SpMV) format and kernel designed for low and unstructured sparsity
  levels (30-90%) common in pruned large language models. MACKO uses coordinate compression
  with mutual alignment between coordinates and values, employing strategic padding
  to enable efficient GPU execution without specialized hardware or precomputation.
---

# MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity

## Quick Facts
- arXiv ID: 2511.13061
- Source URL: https://arxiv.org/abs/2511.13061
- Authors: Vladimír Macko; Vladimír Boža
- Reference count: 40
- Primary result: 1.5× memory reduction and 1.5× faster inference for pruned LLMs at 50% sparsity

## Executive Summary
MACKO introduces a GPU-optimized sparse matrix-vector multiplication (SpMV) format designed for low and unstructured sparsity levels (30-90%) common in pruned large language models. The format uses coordinate compression with delta encoding and strategic padding to enable efficient GPU execution without specialized hardware or precomputation. MACKO outperforms existing SpMV baselines by 2.8-13.0× over cuSPARSE, 1.9-2.6× over Sputnik, and 2.2-2.5× over DASP, achieving 1.5× memory reduction and 1.5× speedup over dense representation at 50% sparsity when applied to Llama2-7B.

## Method Summary
MACKO stores sparse matrices using delta-encoded column indices with 4-bit deltas and strategically inserted zero padding to maintain small deltas. The format ensures mutual alignment between compressed coordinates and values, enabling coalesced GPU memory access. A SplitK-based GPU kernel processes each row with one warp of 32 threads, using warp-level prefix sums and shuffle instructions to reconstruct column indices on-the-fly. The approach avoids GPU-specific optimizations for generality but focuses on memory bandwidth reduction for memory-bound SpMV operations at low sparsity levels.

## Key Results
- At 50% sparsity: 1.5× memory reduction and 1.2-1.5× speedup over dense representation
- Outperforms cuSPARSE by 2.8-13.0×, Sputnik by 1.9-2.6×, and DASP by 2.2-2.5×
- Applied to Llama2-7B pruned to 50% sparsity: 1.5× memory reduction and 1.5× faster inference at fp16 precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delta encoding with strategic padding reduces memory bandwidth requirements for 30-90% sparse matrices.
- Mechanism: Column indices are stored as 4-bit deltas instead of full 16-32 bit integers. Zero values are inserted as padding when gaps exceed 2^4=16, keeping deltas small. This reduces storage from 48 bits (CSR32+fp16) to 20 bits (MACKO fp16 with 4-bit deltas).
- Core assumption: Gaps between non-zero column indices follow distributions amenable to small delta encoding with bounded padding overhead.
- Evidence: [abstract] "strategic padding", [section 4.1] delta encoding with range [1,2^b∆], [corpus] hash-based SpMV work doesn't validate delta encoding for LLMs.
- Break condition: Large, irregular gaps between non-zeros could increase padding overhead, negating compression benefits.

### Mechanism 2
- Claim: Mutual alignment enables efficient GPU parallelization without format-specific precomputation.
- Mechanism: 1:1 correspondence between compressed coordinates and values allows coalesced memory transactions. Each warp processes one row, loading fixed-size chunks of both deltas and values simultaneously, enabling on-the-fly index reconstruction using warp-level prefix sums.
- Core assumption: Memory bandwidth is the primary bottleneck, so index reconstruction overhead doesn't degrade performance.
- Evidence: [abstract] "without specialized hardware units", [section 4.4.2] memory-bound justification, [section 4.3] SplitK adaptation, [corpus] no corpus evidence on mutual alignment for GPU SpMV.
- Break condition: Compute saturation or severe load imbalance from row length variation could make index reconstruction overhead limiting.

### Mechanism 3
- Claim: Effective density improvement enables practical speedup at low sparsity.
- Mechanism: Theoretical speedup limit is 1/effd (inverse of effective density). At 50% sparsity, MACKO's effd ≈ 0.75 yields ~1.33× theoretical speedup. Observed 1.2-1.5× confirms memory bandwidth reduction drives performance gains.
- Core assumption: SpMV is memory-bandwidth-bound on modern GPUs for target sparsity range.
- Evidence: [abstract] "1.5× memory reduction and 1.2-1.5× speedup", [section 3.1] Roofline analysis, [section 4.2, Figure 2] effective density curves, [corpus] FPGA co-design work targets different sparsity structures.
- Break condition: Batch size increases or cache behavior changes could make memory bandwidth no longer the sole bottleneck.

## Foundational Learning

- Concept: **Roofline Model and Compute Intensity**
  - Why needed: Understanding memory-bound vs. compute-bound operations explains why compression yields speedups. The paper uses this to justify memory bandwidth focus.
  - Quick check: For GPU with peak FLOPS = 330 TFLOPS and peak bandwidth = 1008 GB/s, ops-per-byte ratio = 330/1008 ≈ 0.33. A kernel requiring 1 FLOP per 4 bytes transferred is memory-bound (needs 0.25 ops/byte).

- Concept: **Sparse Matrix Storage Formats (CSR, Tiled-CSL, Bitmask)**
  - Why needed: MACKO's novelty is contextualized by comparison to existing formats. Understanding trade-offs clarifies why MACKO is different.
  - Quick check: 50% sparse matrix in CSR32 with fp16 values has effective density = (0.5×2 + 0.5×4)/6 = 0.5. MACKO's effd ≈ 0.65-0.7 at same sparsity.

- Concept: **GPU Execution Model (Warps, SIMT, Memory Coalescing)**
  - Why needed: MACKO's kernel design relies on warp-level parallelism, memory transaction sizes, and shuffle operations. Without this background, index reconstruction is opaque.
  - Quick check: MACKO processes each row with one warp (32 threads) to ensure coalesced access and balanced load. This mapping affects memory coalescing patterns and load balancing across rows.

## Architecture Onboarding

- Component map: Storage Format Conversion (CPU) -> GPU SpMV Kernel -> PyTorch Integration -> Pruning Interface
- Critical path: Format conversion (one-time) -> Memory bandwidth for deltas/values (dominant) -> Compute for prefix sum reconstruction (masked by memory) -> Vector V access (mitigated by cache)
- Design tradeoffs:
  - b∆ selection: 4 bits balances compression vs. padding at 50% sparsity; b∆=2 better for 15-45%, b∆=8 better for >95%
  - ROMA alignment: Handles misaligned row starts without per-row padding, saving memory at mask logic cost
  - GPU-specific optimizations: Avoided for generality, leaving performance on the table
- Failure signatures:
  - No speedup vs. dense at expected sparsity: Check effective density; padding overhead may be higher than expected
  - Slow format conversion: For large matrices, conversion takes seconds; ensure done once at load time
  - Incorrect results: Verify 0-indexed row pointers, correct delta computation including padding, zero-padding not treated as non-zero
  - Regression at high sparsity (>95%): MACKO not designed for this regime; switch to Sputnik
- First 3 experiments:
  1. Microbenchmark: Convert 4096x4096 matrix at 50% random sparsity to MACKO. Measure memory footprint (~0.66x dense) and SpMV runtime vs. cuBLAS/cuSPARSE. Expected: 1.2-1.5× speedup over cuBLAS.
  2. End-to-end LLM: Prune Llama2-7B with Wanda to 50% sparsity. Convert to MACKO. Measure tokens/sec and peak memory. Expected: ~1.5× memory reduction, ~1.4× speedup.
  3. Sparsity sweep: Benchmark MACKO across 20-95% sparsity on 12288x12288 matrices. Plot speedup over cuBLAS. Identify crossover point (~20-30% sparsity) where MACKO matches dense performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on sparsity pattern regularity; highly irregular gaps could increase padding overhead and negate compression benefits
- Avoids GPU-specific optimizations for generality, potentially leaving significant performance on the table compared to architecture-tuned implementations
- End-to-end inference results depend on specific pruning method (Wanda) and workload characteristics not fully detailed in the paper

## Confidence
- High Confidence: Memory reduction claims (1.5× at 50% sparsity) and speedup over dense representations (1.2-1.5×) are well-supported by Roofline analysis and empirical validation
- Medium Confidence: Speedup claims over other SpMV implementations (2.8-13.0× over cuSPARSE, 1.9-2.6× over Sputnik) are credible but specific to tested LLM workloads
- Medium Confidence: End-to-end inference performance (1.5× memory reduction, 1.5× faster inference on Llama2-7B) is plausible but depends on specific pruning and workload details

## Next Checks
1. Sparsity Pattern Robustness Test: Generate matrices with varying gap distributions (uniform, clustered, diagonal-biased) at 50% sparsity. Measure MACKO's effective density and speedup to identify break conditions where padding overhead negates compression benefits.

2. Architecture Portability Validation: Benchmark MACKO across different GPU architectures (RTX 3090, 4090, A100) and compare performance against architecture-optimized SpMV libraries to quantify impact of avoiding GPU-specific optimizations.

3. Non-LLM Workload Evaluation: Apply MACKO to matrices from scientific computing (finite element methods, graph algorithms) and measure performance relative to cuSPARSE to validate LLM-specific nature of claimed improvements and identify potential cross-domain applications.