---
ver: rpa2
title: Retrieval-augmented Encoders for Extreme Multi-label Text Classification
arxiv_id: '2502.10615'
source_url: https://arxiv.org/abs/2502.10615
tags:
- label
- rae-xmc
- training
- input
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAE-XMC, a retrieval-augmented framework
  for extreme multi-label classification that enhances dual-encoder models with better
  memorization capability. The method trains a dual-encoder model to contrast over
  a joint knowledge memory space consisting of both input instances and labels, then
  retrieves top-K keys during inference to generate predictions.
---

# Retrieval-augmented Encoders for Extreme Multi-label Text Classification

## Quick Facts
- **arXiv ID:** 2502.10615
- **Source URL:** https://arxiv.org/abs/2502.10615
- **Reference count:** 24
- **Primary result:** RAE-XMC achieves 10x faster training than DEXML on LF-AmazonTitles-1.3M while maintaining comparable or better prediction performance

## Executive Summary
This paper introduces RAE-XMC, a retrieval-augmented framework for extreme multi-label classification that enhances dual-encoder models with better memorization capability. The method trains a dual-encoder model to contrast over a joint knowledge memory space consisting of both input instances and labels, then retrieves top-K keys during inference to generate predictions. RAE-XMC achieves state-of-the-art results on four large-scale LF-XMC benchmarks, outperforming DEXML by more than 10x in training speedup on the largest LF-AmazonTitles-1.3M dataset while maintaining comparable or better prediction performance.

## Method Summary
RAE-XMC constructs a joint knowledge memory containing both training instance embeddings and label text embeddings. During training, a dual-encoder is optimized with a decoupled softmax contrastive loss that includes in-batch instance negatives. At inference, the system retrieves top-K keys from this memory and aggregates corresponding values to produce predictions. The framework uses a lookup-based predictor instead of OVA classifiers, enabling efficient scaling to millions of labels while providing controllable trade-offs between memorization and generalization through a tunable interpolation parameter λ.

## Key Results
- Achieves 10x faster training than DEXML on LF-AmazonTitles-1.3M (9.5h vs 107.5h)
- Outperforms DEXML on LF-AmazonTitles-1.3M with 58.48 P@1 vs 54.67
- Demonstrates competitive performance with zero-shot inference using pretrained GTE-base (47.03 P@1 without fine-tuning)
- Shows controllable trade-offs between head and tail label performance through λ parameter

## Why This Works (Mechanism)

### Mechanism 1: Joint Knowledge Memory Unifies Memorization and Generalization
The framework combines training instances and label text into a single searchable memory, enabling controllable trade-offs between head-label memorization and tail-label generalization. During inference, retrieved instance keys contribute ground-truth label vectors (memorization), while retrieved label keys contribute semantic similarity signals (generalization). The coefficient λ in the value matrix V weights these sources.

### Mechanism 2: Lightweight Non-Parametric Predictor Avoids OVA Classifier Scaling
Using a lookup-based predictor on retrieved keys eliminates the need for OVA classifiers whose parameters scale linearly with label space size L. The predictor directly looks up whether label ℓ is associated with retrieved key k—either from ground-truth labels Y (if k is an instance) or from diagonal identity I_L (if k is a label).

### Mechanism 3: In-Batch Instance Negatives Enforce Joint Space Contrast
Including in-batch input instances as negatives during contrastive training forces the encoder to jointly distinguish both instance-level and label-level similarities. The decoupled softmax loss adds exp(s_θ(x,x')/τ) to the denominator for instances sharing no labels with the anchor, preventing semantic drift while maintaining discrimination.

## Foundational Learning

- **Concept: Dual-Encoder Architecture for Semantic Matching**
  - Why needed: RAE-XMC builds on DE models that map input text and label text into a shared embedding space; understanding why DE struggles with memorization is essential for grasping the retrieval augmentation motivation.
  - Quick check: Given a query embedding q and label embedding z_ℓ, what does a high inner product ⟨q, z_ℓ⟩ indicate about their relationship?

- **Concept: Approximate Nearest Neighbor (ANN) Search**
  - Why needed: Inference efficiency depends on ANN algorithms to retrieve top-b keys in O(log(N+L)) rather than O(N+L); the method is impractical without ANN.
  - Quick check: Why can't XMC systems afford exact nearest neighbor search over millions of labels at inference time?

- **Concept: Contrastive Learning with InfoNCE Loss**
  - Why needed: Training uses decoupled softmax (a variant of InfoNCE) with in-batch negatives; understanding positive/negative pair construction is critical for reproducing results.
  - Quick check: In Equation 10, why are positive labels P(y) excluded from the denominator, and what problem does this solve?

## Architecture Onboarding

- **Component map:** Encoder f_θ → Knowledge Memory (K, V) → ANN Indexer → Predictor
- **Critical path:** 1) Encode test query → q ∈ R^d, 2) ANN search → retrieve top-b keys with similarity scores, 3) Sparse matrix multiply → Softmax(scores/τ) · V → L-dimensional prediction vector, 4) Return top-k labels by score
- **Design tradeoffs:**
  - **λ (memorization vs. generalization):** Higher λ favors head labels; lower λ favors tail labels. Default λ=0.5 works for most datasets; LF-Wikipedia-500K uses λ=0.01.
  - **b (retrieval budget):** Larger b improves recall but increases latency. Paper uses b=200.
  - **With vs. without supervised fine-tuning:** Table 5 shows RAE inference on pretrained GTE-base achieves 47.03 P@1 without any fine-tuning on LF-AmazonTitles-1.3M—competitive with supervised methods—but full training still helps.
- **Failure signatures:**
  - **Near-zero improvement from RAE:** Encoder embeddings are already well-aligned (e.g., DEXML+RAE shows only marginal gains)—indicates saturation.
  - **Large λ sensitivity:** If small λ changes cause large performance swings, training-test distribution mismatch is likely.
  - **Retrieval dominated by one source:** If almost all retrieved keys come from X or Z exclusively, the encoder may be failing to align one space.
- **First 3 experiments:**
  1. **Baseline DE inference vs. RAE inference (λ=0.5):** Train encoder with standard contrastive loss, compare standard MIPS retrieval against RAE inference. Expected: RAE should improve head-label metrics with minimal tail degradation.
  2. **Ablation on b (top-k keys retrieved):** Vary b ∈ {50, 100, 200, 500} and plot P@1, R@100 vs. latency. Expected: Diminishing returns after b≈200.
  3. **Zero-shot RAE on pretrained encoder:** Apply RAE inference (no fine-tuning) using off-the-shelf GTE-base or DistilBERT encoder. Expected: Significant gains over vanilla DE inference.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can representative key-value pairs be selected to reduce the size of the knowledge memory without significantly impacting predictive performance? The authors note this as an interesting avenue for future work, as the current framework utilizes the entire joint input and label space (N+L) as memory, incurring substantial space and time overhead.

- **Open Question 2:** Can a data-driven or instance-specific λ be learned to dynamically balance the contributions of the input and label embedding spaces? The paper suggests this as another interesting direction, as the current approach uses a static λ tuned per dataset.

- **Open Question 3:** Could integrating a lightweight parametric predictor, rather than a simple value lookup, further bridge the performance gap with cross-attention models? While large LMs excel at processing retrieved documents, they are impractical to scale for XMC, so the paper resorts to a lookup p(y_ℓ | k, q) = V_{k,ℓ} to satisfy latency constraints.

## Limitations

- The framework's effectiveness critically depends on the similarity between training and test distributions; performance degrades when test queries diverge significantly from training instances.
- The Value matrix V requires 1.1TB RAM for LF-AmazonTitles-1.3M, necessitating sparse matrix representations or distributed computing for practical reproduction on standard hardware.
- The dynamic update strategy for hard negative mining indices during training is underspecified, creating ambiguity about optimal implementation.

## Confidence

- **High confidence:** Training speedup claims (10x improvement) and retrieval efficiency (sublinear ANN search) are well-supported by empirical results and clear theoretical justification.
- **Medium confidence:** The controllability of head-tail label performance through λ interpolation is demonstrated but requires dataset-specific tuning, suggesting limited generalizability without careful calibration.
- **Low confidence:** The claim that RAE-XMC maintains "comparable or better" prediction performance across all datasets requires closer examination, as tail label performance shows significant variance with λ settings.

## Next Checks

1. **Distribution shift sensitivity test:** Evaluate RAE-XMC on test sets with controlled semantic drift from training data to quantify the degradation of the kNN memorization component under distribution shift.

2. **Memory optimization benchmark:** Compare sparse matrix implementations versus full matrix approaches for the Value matrix V on LF-AmazonTitles-1.3M to establish practical memory requirements for different hardware configurations.

3. **Dynamic HNM update frequency ablation:** Systematically vary the frequency of hard negative mining index updates during training to determine optimal update intervals and their impact on convergence and final performance.