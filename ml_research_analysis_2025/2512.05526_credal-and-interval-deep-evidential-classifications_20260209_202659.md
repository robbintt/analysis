---
ver: rpa2
title: Credal and Interval Deep Evidential Classifications
arxiv_id: '2512.05526'
source_url: https://arxiv.org/abs/2512.05526
tags:
- uncertainty
- idec
- credal
- ihdr
- cdec-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  (UQ) in deep learning classification by introducing Credal and Interval Deep Evidential
  Classifications (CDEC and IDEC). These methods distinguish between aleatoric (irreducible)
  and epistemic (reducible) uncertainty using credal sets and intervals of measures,
  enabling principled abstention when uncertainty exceeds thresholds.
---

# Credal and Interval Deep Evidential Classifications

## Quick Facts
- arXiv ID: 2512.05526
- Source URL: https://arxiv.org/abs/2512.05526
- Authors: Michele Caprio; Shireen K. Manchingal; Fabio Cuzzolin
- Reference count: 40
- Primary result: CDEC-3 achieves state-of-the-art OoD detection (AUROC up to 99.63 on MNIST)

## Executive Summary
This paper addresses uncertainty quantification in deep learning classification by introducing Credal and Interval Deep Evidential Classifications (CDEC and IDEC). These methods distinguish between aleatoric and epistemic uncertainty using credal sets and intervals of measures, enabling principled abstention when uncertainty exceeds thresholds. CDEC leverages an ensemble of models to form a credal set, while IDEC uses a single model with an optimal interval inflation parameter. Experiments on MNIST, CIFAR-10, and CIFAR-100 show that CDEC-3 achieves state-of-the-art OoD detection with tight and well-calibrated prediction regions.

## Method Summary
The method uses Posterior Networks as the base architecture, combining Dirichlet-Categorical conjugacy with credal sets for uncertainty quantification. CDEC constructs a credal set by aggregating predictions from S ensemble models, enabling decomposition of total uncertainty into aleatoric (minimum entropy) and epistemic (entropy range) components. IDEC approximates this with a single model using interval inflation controlled by parameter d*. Both methods use Imprecise Highest Density Regions (IHDR) for set-valued predictions with guaranteed coverage. The approach relies on Normalizing Flows to estimate latent densities and generate virtual counts for Dirichlet posterior parameters.

## Key Results
- CDEC-3 achieves AUROC up to 99.63 on MNIST OoD detection
- IHDR size reliably expands from 1-2 classes (in-distribution) to 23+ classes (distribution shift)
- CDEC stabilizes with S≈3 ensemble members, maintaining high coverage and consistent uncertainty estimates
- IDEC shows computational efficiency but instability on complex datasets like CIFAR-100

## Why This Works (Mechanism)

### Mechanism 1: Credal Decomposition of Uncertainty
Constructing a credal set (convex hull of predictive distributions) enables principled disentanglement of aleatoric and epistemic uncertainty. CDEC aggregates predictions from S models, computing aleatoric uncertainty as minimum entropy across extreme points and epistemic uncertainty as entropy range. This captures model disagreement and irreducible data noise.

### Mechanism 2: Imprecise Highest Density Regions (IHDR) for Abstention
IHDR computes the smallest set of labels with lower probability exceeding 1-γ threshold, enabling safe abstention. Under high uncertainty, the set expands to include all labels, preventing overconfident errors. This provides guaranteed coverage rather than point estimates.

### Mechanism 3: Interval Inflation (IDEC) for Efficiency
IDEC approximates ensemble uncertainty bounds using single model with optimal inflation parameter d*. It derives categorical ℓ from one model and inflates by (1+d) to form interval I(ℓ,(1+d)ℓ). d* is solved analytically to ensure target coverage, offering computational efficiency at potential cost of uncertainty discriminability.

## Foundational Learning

- **Concept: Dirichlet-Categorical Conjugacy**
  - Why needed: Maps neural network outputs to Dirichlet parameters to form posterior over class probabilities
  - Quick check: For α = [2, 10, 2], expected probability of second class is 10/14; uncertainty is low compared to α = [1, 1, 1]

- **Concept: Imprecise Probability & Credal Sets**
  - Why needed: Replaces single distributions with sets of distributions for uncertainty quantification
  - Quick check: For P₁=[0.9,0.1], P₂=[0.1,0.9], P₃=[0.5,0.5], lower probability of {Class 1} is max(0.1, 0.1, 0.5) = 0.5

- **Concept: Normalizing Flows (Posterior Networks)**
  - Why needed: Estimates density of latent embeddings to generate virtual counts for Dirichlet prior
  - Quick check: Jacobian determinant relates to density estimation for computing uncertainty weights

## Architecture Onboarding

- **Component map:** Encoder (f_θ) -> Latent z -> Class-Conditional Flow (q_φ) -> Virtual Counts (n_virt) -> Dirichlet Posterior (α) -> Credal/Interval Builder -> IHDR Solver

- **Critical path:** Normalizing Flow training is critical. Flow failure to separate iD and OoD densities leads to incorrect virtual counts and uninformative Dirichlet uncertainty.

- **Design tradeoffs:** Choose CDEC for robustness and tighter bounds (requires S≈3 models) vs IDEC for inference speed (single model, potential instability on complex data).

- **Failure signatures:** Stuck at max uncertainty (ensemble too diverse), zero epistemic uncertainty (S=1 or collapsed convex hull), massive TU values (IDEC on CIFAR-100).

- **First 3 experiments:** 1) MNIST sanity check: Verify AU low, EU/TU high on Fashion-MNIST (AUROC > 99%). 2) Ablation S: Run CDEC with S∈{1,3,5} on CIFAR-10, verify S=1 misses OoD, S=3 stabilizes IHDR coverage ≈1.0. 3) Abstention threshold tuning: Adjust ε to balance abstention rate vs error rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CDEC and IDEC be extended to regression tasks without losing theoretical guarantees?
- Basis: Authors plan to extend to regression problems, noting one-to-one parameter relationship may not hold
- Why unresolved: Framework relies on categorical distribution properties that don't transfer to continuous outputs
- What resolves it: Theoretical derivation and empirical validation of credal regression method maintaining uncertainty separation

### Open Question 2
- Question: How can methodology adapt to open-set scenarios with unknown class counts?
- Basis: Authors intend to drop assumption of known label number, proposing extended Dirichlet priors
- Why unresolved: Current algorithms require fixed k for Dirichlet distribution and credal set definition
- What resolves it: Demonstrating mixture of finite mixtures prior can identify and classify inputs without pre-defined label space

### Open Question 3
- Question: Does relaxing i.i.d. assumption for training samples improve reliability?
- Basis: Authors intend to drop i.i.d. assumption, proposing Lazy Naive Credal Classifier for dependent cases
- Why unresolved: Standard evidential learning assumes independence; dependency could introduce bias
- What resolves it: Comparative experiments showing LNCC approach yields better calibration on datasets with sample dependencies

## Limitations

- CDEC computational cost scales linearly with ensemble size S, prohibitive for large-scale applications
- IDEC interval inflation shows instability on high-dimensional datasets (CIFAR-100) with uncertainty values >10^15
- Abstention mechanism depends critically on dataset-specific threshold tuning not addressed in main experiments

## Confidence

- **High Confidence:** OoD detection performance (AUROC/AUPRC) on MNIST and CIFAR-10
- **Medium Confidence:** AU/EU decomposition mechanism (theoretical justification but limited empirical validation)
- **Medium Confidence:** IHDR coverage calibration (single calibration dataset provides limited robustness evidence)

## Next Checks

1. **Stress Test on Complex Datasets:** Evaluate CDEC/IDEC on ImageNet or long-tail distributed datasets to assess uncertainty decomposition with imbalanced class distributions.

2. **Temporal Consistency Analysis:** Measure AU/EU/TU score stability across multiple training runs with different random seeds, particularly for IDEC where inflation parameters may vary.

3. **Real-World Application Benchmark:** Deploy method on medical imaging task where abstention has safety implications, measuring coverage vs error reduction trade-off.