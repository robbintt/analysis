---
ver: rpa2
title: The Effect of Label Noise on the Information Content of Neural Representations
arxiv_id: '2510.06401'
source_url: https://arxiv.org/abs/2510.06401
tags:
- representations
- information
- noise
- label
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how label noise affects the information
  content of neural network hidden representations using a task-agnostic statistical
  measure called Information Imbalance (II). II quantifies the relative information
  between two feature spaces by analyzing how well relative distances between samples
  are preserved when transitioning between spaces.
---

# The Effect of Label Noise on the Information Content of Neural Representations

## Quick Facts
- **arXiv ID:** 2510.06401
- **Source URL:** https://arxiv.org/abs/2510.06401
- **Authors:** Ali Hussaini Umar; Franky Kevin Nando Tezoh; Jean Barbier; Santiago Acevedo; Alessandro Laio
- **Reference count:** 40
- **Primary result:** Label noise primarily degrades information in the last layer (between hidden and pre-softmax representations), while feature encoding within hidden representations remains relatively preserved.

## Executive Summary
This study investigates how label noise affects the information content of neural network hidden representations using a task-agnostic statistical measure called Information Imbalance (II). The authors establish a theoretical connection between II and conditional mutual information, computing explicit lower bounds for II in two Gaussian models. Empirically, they analyze single-layer fully connected networks on MNIST and convolutional networks on CIFAR-10, varying label noise levels to observe how information content changes across different network capacities.

The research reveals that noisy labels yield more informative representations than clean labels in underparameterized regimes, while in overparameterized regimes, representations become approximately equally informative regardless of label noise. This suggests that hidden representations in overparameterized networks are robust to label noise. The study also demonstrates that the double descent phenomenon in test error is mirrored by a double descent in II between independent network representations, providing new insights into the generalization capabilities of neural networks under varying levels of label corruption.

## Method Summary
The authors use Information Imbalance (II) as a task-agnostic statistical measure to quantify the relative information between two feature spaces by analyzing how well relative distances between samples are preserved when transitioning between spaces. They establish theoretical connections between II and conditional mutual information, computing explicit lower bounds for II in one-dimensional embedding and Gaussian denoising models. The empirical validation involves training single-layer fully connected neural networks on MNIST and convolutional neural networks on CIFAR-10 with varying levels of label noise, then measuring II between different layers and across multiple independent runs to assess information content and robustness to label corruption.

## Key Results
- In underparameterized regimes, noisy labels yield more informative representations than clean labels
- In overparameterized regimes, hidden representations become approximately equally informative regardless of label noise, indicating robustness
- Label noise primarily degrades information in the last layer (between hidden and pre-softmax representations), while feature encoding within hidden representations remains relatively preserved

## Why This Works (Mechanism)
The mechanism relies on the Information Imbalance (II) measure's ability to capture how well relative distances between samples are preserved when transitioning between feature spaces. In underparameterized regimes, the network struggles to memorize noisy labels, forcing it to extract more general features that better preserve sample relationships. In overparameterized regimes, the network's capacity allows it to maintain informative representations regardless of label noise because it can simultaneously fit the noisy labels while preserving the underlying data structure in hidden layers. The degradation primarily occurs in the final layer because this is where the network attempts to map general features to specific (potentially noisy) labels, creating a bottleneck for information preservation.

## Foundational Learning

**Information Imbalance (II):** A task-agnostic statistical measure quantifying relative information between feature spaces by analyzing distance preservation. Needed to assess information content without relying on specific downstream tasks. Quick check: Verify that II values range between 0 (no information) and 1 (perfect information preservation).

**Conditional Mutual Information:** Measures the amount of information that one random variable contains about another, given knowledge of a third variable. Needed to establish theoretical connections with II. Quick check: Confirm that I(X;Y|Z) = H(X|Z) - H(X|Y,Z) holds for the Gaussian models used.

**Double Descent Phenomenon:** The observation that test error can decrease again after the interpolation threshold in overparameterized models. Needed to connect information-theoretic measures with generalization behavior. Quick check: Plot test error vs. model capacity to verify the characteristic double descent curve.

**Gaussian Models:** Simplified probabilistic models used for theoretical analysis. Needed to derive explicit bounds for II that can be compared with empirical results. Quick check: Verify that the Gaussian assumptions hold approximately for the learned representations in real networks.

## Architecture Onboarding

**Component Map:** Data -> Neural Network (Input -> Hidden Layers -> Pre-Softmax -> Softmax) -> II Measurements between layers

**Critical Path:** Data → Network Forward Pass → Hidden Representations → Pre-Softmax → Softmax → II Computation between representations

**Design Tradeoffs:** The study uses simplified architectures (single-layer fully connected and convolutional networks) to enable clearer analysis, trading model complexity for interpretability and theoretical tractability.

**Failure Signatures:** When II between hidden representations drops significantly with increasing label noise, this indicates loss of information preservation in the feature encoding stage, potentially leading to poor generalization.

**First Experiments:** 1) Train networks with varying label noise levels and measure II between hidden and pre-softmax layers. 2) Compare II values across underparameterized and overparameterized regimes. 3) Verify the double descent pattern in both test error and II measurements.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis is confined to simplified Gaussian models, limiting generalizability to complex real-world neural networks
- Empirical validation examines only specific architectures (single-layer fully connected and convolutional networks) without exploring deeper or more diverse network structures
- The study focuses on MNIST and CIFAR-10 datasets, which may not represent the full complexity of real-world data distributions

## Confidence
- Hidden representations in overparameterized networks are robust to label noise: Medium confidence (based on limited experimental evidence)
- Label noise primarily affects information in the last layer before softmax: Medium confidence (restricted to tested architectures)
- Theoretical connection between Information Imbalance and conditional mutual information: Low confidence (simplified Gaussian assumptions)

## Next Checks
1. Replicate the experiments across multiple diverse architectures including deeper networks and transformers to verify robustness conclusions
2. Test the II-based information degradation patterns on additional datasets with varying complexity and data distributions
3. Conduct ablation studies systematically varying noise levels and network capacities to map the precise relationship between overparameterization, label noise, and information preservation across all layers