---
ver: rpa2
title: 'Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon
  LLM Agents'
arxiv_id: '2510.03253'
source_url: https://arxiv.org/abs/2510.03253
tags:
- action
- group
- agent
- learning
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the granularity mismatch problem in offline
  LLM agent alignment, where trajectory-level feedback is too coarse and step-level
  feedback is too myopic for long-horizon tasks. To resolve this, the authors propose
  Hierarchical Preference Learning (HPL), a framework that optimizes agents using
  preference signals at three levels: trajectory, action, and semantically coherent
  action groups.'
---

# Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents

## Quick Facts
- **arXiv ID**: 2510.03253
- **Source URL**: https://arxiv.org/abs/2510.03253
- **Reference count**: 40
- **Primary result**: HPL outperforms state-of-the-art methods on ALFWorld, WebShop, and InterCode-SQL benchmarks with average scores of 67.81 (7B) and 58.73 (1.5B)

## Executive Summary
This paper addresses the granularity mismatch problem in offline LLM agent alignment, where trajectory-level feedback is too coarse and step-level feedback is too myopic for long-horizon tasks. To resolve this, the authors propose Hierarchical Preference Learning (HPL), a framework that optimizes agents using preference signals at three levels: trajectory, action, and semantically coherent action groups. HPL's key innovation is a dual-layer curriculum that schedules training from simple to complex based on group length and sample difficulty. Experiments on ALFWorld, WebShop, and InterCode-SQL benchmarks show HPL outperforms state-of-the-art methods, with HPL (Semantic) achieving average scores of 67.81 (7B) and 58.73 (1.5B), surpassing baselines by 3-5 points. Ablation studies confirm the group-level loss and curriculum are critical to performance.

## Method Summary
Hierarchical Preference Learning introduces a three-tier preference optimization framework that bridges the gap between trajectory-level and step-level feedback. The method operates by first identifying semantically coherent action groups within trajectories using clustering techniques (HuggingFace transformers for semantic similarity in ALFWorld and Jaccard similarity for WebShop). The framework then applies a dual-layer curriculum that schedules training from simple to complex groups based on group length and sample difficulty. During training, HPL optimizes the LLM agent using preference signals at three hierarchical levels: trajectory-level (final outcome), action-level (individual step), and group-level (semantically coherent sequences). The group-level loss is computed using a customized pairwise loss function that encourages actions within the same group to be more similar than actions across different groups.

## Key Results
- HPL (Semantic) achieves average scores of 67.81 (7B) and 58.73 (1.5B) across ALFWorld, WebShop, and InterCode-SQL benchmarks
- Outperforms state-of-the-art methods by 3-5 points on average
- Ablation studies show both group-level loss and dual-layer curriculum are critical for performance gains
- Successfully addresses granularity mismatch by providing intermediate supervision between trajectory and step levels

## Why This Works (Mechanism)
HPL works by providing the right level of supervision at the right time. Traditional approaches suffer because trajectory-level feedback is too sparse (only at the end) while step-level feedback is too myopic (ignoring long-term dependencies). HPL's three-tier approach captures the right granularity: trajectory feedback provides high-level goals, action feedback handles immediate decisions, and group-level feedback captures semantically meaningful sequences that bridge short-term actions with long-term objectives. The dual-layer curriculum ensures the model learns simple patterns first (short groups, easy samples) before tackling complex ones, preventing the model from being overwhelmed by the full complexity of long-horizon tasks.

## Foundational Learning
- **Granularity Mismatch Problem**: The disconnect between coarse trajectory-level feedback and fine-grained step-level feedback in long-horizon tasks. Why needed: Traditional RLHF approaches fail to provide appropriate supervision signals for complex sequential decision-making.
- **Semantic Coherence**: Identifying groups of actions that share meaningful relationships based on their semantic content. Why needed: Provides intermediate supervision level between individual steps and entire trajectories. Quick check: Verify that automatically generated action groups actually capture meaningful semantic relationships through human evaluation.
- **Dual-Layer Curriculum Learning**: Scheduling training from simple to complex groups based on group length and sample difficulty. Why needed: Prevents model collapse by gradually increasing task complexity. Quick check: Monitor training stability across different curriculum schedules.
- **Pairwise Preference Optimization**: Using relative comparisons between action groups to train the reward model. Why needed: Provides more stable and informative signals than absolute rewards. Quick check: Validate that pairwise comparisons lead to consistent reward model training.

## Architecture Onboarding

**Component Map**: Trajectory-level reward model -> Group-level reward model -> Action-level reward model -> LLM policy

**Critical Path**: User input → LLM → Action sequence → Semantic grouping → Multi-level reward computation → Policy update → New action

**Design Tradeoffs**: The paper trades computational complexity for improved alignment quality. HPL requires additional preprocessing to identify semantic groups and more complex training with three reward models, but this investment pays off in better long-horizon task performance.

**Failure Signatures**: 
- Poor semantic grouping leads to ineffective group-level supervision
- Incorrect curriculum scheduling can cause training instability
- Over-reliance on group-level signals may neglect important step-level details

**First 3 Experiments**:
1. Verify semantic grouping quality on a small subset of trajectories through manual inspection
2. Test HPL's performance on a single benchmark (e.g., ALFWorld) with varying curriculum schedules
3. Conduct ablation studies removing group-level loss to confirm its contribution to performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond its core contributions and experimental results.

## Limitations
- Evaluation confined to three specific domains (ALFWorld, WebShop, InterCode-SQL), limiting generalizability claims
- Assumes automatically generated semantic groups capture meaningful relationships without thorough validation
- Statistical significance of performance improvements not explicitly discussed across multiple runs

## Confidence

**High Confidence**: The technical framework of HPL (trajectory, action, and group-level preference learning) is well-defined and implementable

**Medium Confidence**: The experimental results showing HPL outperforming baselines on the tested benchmarks

**Low Confidence**: The claim that HPL's improvements will generalize to unseen long-horizon tasks beyond the three evaluated domains

## Next Checks

1. Conduct ablation studies specifically isolating the impact of semantic grouping quality versus curriculum scheduling effectiveness
2. Perform cross-domain transferability tests by evaluating HPL on at least three additional long-horizon task domains not used in the original training
3. Implement statistical significance testing with multiple random seeds to establish confidence intervals for HPL's performance improvements over baselines