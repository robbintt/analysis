---
ver: rpa2
title: 'Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages
  Memorization but Mixed Training Teaches Knowledge'
arxiv_id: '2505.16178'
source_url: https://arxiv.org/abs/2505.16178
tags:
- parameters
- shared
- training
- fact
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates why two-stage training (fact-storing then\
  \ fact-recalling) encourages memorization while mixed training (joint fact-storing\
  \ and fact-recalling) promotes knowledge generalization in language models. The\
  \ authors introduce a novel tool called \"cross-task gradient trace\" to identify\
  \ shared parameters\u2014those strongly influenced by both fact-storing and fact-recalling\
  \ tasks during training."
---

# Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge

## Quick Facts
- arXiv ID: 2505.16178
- Source URL: https://arxiv.org/abs/2505.16178
- Reference count: 40
- Two-stage training yields up to 38% lower out-of-distribution accuracy compared to mixed training

## Executive Summary
This paper investigates why two-stage training (fact-storing then fact-recalling) encourages memorization while mixed training (joint fact-storing and fact-recalling) promotes knowledge generalization in language models. The authors introduce a novel tool called "cross-task gradient trace" to identify shared parameters—those strongly influenced by both fact-storing and fact-recalling tasks during training. Analysis on Llama-3.2B and Pythia-2.8B models shows that mixed training yields more shared parameters (up to 1.2x larger) compared to two-stage training. Ablating these shared parameters causes significantly larger performance drops in mixed-trained models (66.0% vs 6.7% accuracy reduction), demonstrating their critical role in fact recall. The study also reveals that shared parameters are concentrated in critical attention heads, with over 60% concentrated in the top 10% of heads, and their ablation causes over 90% drop in fact recall accuracy. These findings suggest that shared parameters enable models to generalize factual knowledge across different task formulations.

## Method Summary
The paper introduces a cross-task gradient trace method to identify shared parameters influenced by both fact-storing (BIO) and fact-recalling (QA) tasks. Using synthetic biographical data with 10,000 individuals, models are fine-tuned using either two-stage training (BIO→QA sequentially) or mixed training (BIO+QA jointly). The gradient trace method tracks parameter influence during backpropagation to identify parameters that serve both tasks. Shared parameters are then localized using a grafting procedure that identifies fact recall-related parameters. Ablation studies validate the functional importance of these shared parameters, while attention head analysis reveals their concentration in specific critical heads. The study uses Llama-3.2B and Pythia-2.8B models with AdamW optimization.

## Key Results
- Mixed training yields up to 1.2x more shared parameters compared to two-stage training
- Ablating shared parameters causes 66.0% accuracy drop in mixed-trained models vs 6.7% in two-stage models
- Over 60% of shared parameters concentrate in the top 10% of attention heads
- Shared parameter ablation causes over 90% drop in fact recall accuracy
- Mixed-trained models require only 0.10-0.13% of parameters for fact recall recovery vs 0.50-0.18% for two-stage models

## Why This Works (Mechanism)

### Mechanism 1: Shared Parameter Formation via Simultaneous Optimization
- Claim: Mixed training produces more shared parameters because simultaneous optimization of BIO and QA objectives encourages parameter reuse under capacity constraints.
- Mechanism: When BIO and QA examples are interleaved in training batches, gradient updates from both tasks influence the same parameters concurrently. Under limited capacity, the model optimizes for parameters that serve both objectives rather than maintaining separate parameter sets.
- Core assumption: Limited parameter capacity and training data naturally incentivize parameter reuse during joint optimization.
- Evidence anchors: [abstract] "mixed training yields more shared parameters (up to 1.2x larger) compared to two-stage training"; [Section 3.1, Figure 2a] Shows |S|/k > 0.6 for k < 10^5 in mixed training, indicating stronger dual-task influence.
- Break condition: If models have substantially overparameterized capacity relative to training data, parameter reuse pressure may diminish.

### Mechanism 2: Attention Head Concentration as Functional Switches
- Claim: Shared parameters concentrate in a small subset of attention heads that function as critical components for activating fact recall behavior.
- Mechanism: Attention heads with high shared parameter counts develop reusable attention patterns across BIO and QA inputs (e.g., "Subject Linking," "Relation Focusing"). These patterns enable consistent entity-relation processing regardless of input format.
- Core assumption: Attention heads with more shared parameters play more important functional roles.
- Evidence anchors: [abstract] "over 60% concentrated in the top 10% of heads, and their ablation causes over 90% drop in fact recall accuracy"; [Section 4.1, Figure 4] Shared Size metric outperforms Grafted Size and Random for identifying critical heads.
- Break condition: If factual knowledge is primarily stored in MLP neurons, attention head concentration may be recall-specific rather than storage-mechanism.

### Mechanism 3: Functional Efficiency Through Parameter-Efficient Performance Recovery
- Claim: Mixed-trained models achieve more efficient parameter utilization for fact recall, requiring fewer fact recall-related parameters to recover performance.
- Mechanism: The grafting procedure identifies a sparse subset of parameters sufficient for fact recall. Mixed-trained models show higher shared parameter proportion within this subset, suggesting shared parameters contribute disproportionately to functional recovery.
- Core assumption: The grafting approach correctly identifies task-relevant parameters.
- Evidence anchors: [Section 3.2, Table 1] Mix-tuned achieves 11.0% |S∩γ|₀/|γ|₀ vs 3.9% for Stage-tuned (in-distribution); requires only 0.10-0.13% α vs 0.50-0.18%; [Section 3.2] Ablating shared parameters before grafting causes +58-60% accuracy drops for Mix-tuned vs +28% for Stage-tuned.
- Break condition: If grafting optimization converges to local minima, parameter efficiency comparisons may be artifacts of the procedure.

## Foundational Learning

- **Gradient Attribution for Parameter Influence**
  - Why needed here: The cross-task gradient trace method relies on understanding how gradient magnitude during backpropagation indicates parameter importance for specific tasks.
  - Quick check question: Given a parameter θ_i with high gradient magnitude for both BIO and QA tasks, what does this suggest about its role?

- **Transformer Attention Mechanics**
  - Why needed here: The paper analyzes fine-grained attention heads (query/key/value/output projections) as nodes in fact recall circuits.
  - Quick check question: In the attention computation o = W_O · Concat(h₁, ..., h_m), what does each head h_b contribute, and how might shared parameters in W_O^b affect cross-task behavior?

- **Ablation Studies for Mechanistic Interpretability**
  - Why needed here: The paper establishes causal relationships through parameter/head ablation rather than correlation alone.
  - Quick check question: Why does resetting parameters to pre-trained values (rather than zeroing them) better isolate their learned contribution?

## Architecture Onboarding

- **Component map**: Synthetic BIO/QA datasets → Two training strategies (Mixed/BIO+QA vs Two-stage BIO→QA) → Cross-task gradient trace → Shared parameters identification → Grafting → Fact recall parameter localization → Ablation studies

- **Critical path**: 1. Training strategy choice determines shared parameter distribution (Figure 2a) → 2. Shared parameters concentrate in critical attention heads (Figure 4c: >60% in top 10%) → 3. Critical heads enable fact recall through reusable attention patterns (Figure 5 case study) → 4. MLP neurons function as knowledge base (assumed, supported by continuity analysis in Appendix A.6.1)

- **Design tradeoffs**: Cross-task gradient trace is computationally expensive (requires gradient tracking across training) vs simple magnitude-based importance; Grafting may not identify all relevant parameters (sparse approximation) vs full model analysis; Synthetic data provides clean analysis but limited ecological validity vs real-world datasets with noise/conflicts

- **Failure signatures**: Low shared parameter count (<2% of total parameters); Diffuse attention patterns without consistent BIO/QA alignment (compare Figure 18 vs Figure 19); High |γ|₀ (>1% of parameters) indicating inefficient fact recall localization; Minimal accuracy drop from shared parameter ablation (<10% drop)

- **First 3 experiments**: 1. **Replicate training comparison**: Fine-tune Llama-3.2-3B or Pythia-2.8B using both strategies on synthetic BIO/QA data; verify QA out-of-distribution accuracy gap (should see ~38+ percentage point difference per Figure 1a) → 2. **Implement cross-task gradient trace**: Track gradients for 20 individuals across BIO and QA; compute influence rankings; verify mixed training produces ~1.2x larger shared parameter set at k=10^8 → 3. **Validate functional impact**: Perform ablation study resetting shared parameters to pre-trained values; confirm asymmetric accuracy drops (target: ~66% for Mix-tuned vs ~6.7% for Stage-tuned on QA out-of-distribution)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed mechanisms of shared parameter concentration persist in models significantly larger than 3 billion parameters or in multimodal architectures?
- **Basis in paper:** [explicit] The authors explicitly state in the limitations (Appendix A.1) that the analysis is limited to text-only models of approximately 3 billion parameters and that "It remains to be seen whether the observed patterns generalize to larger or multimodal models."
- **Why unresolved:** The study restricts its empirical analysis to Llama-3.2B and Pythia-2.8B; it is unknown if the efficiency and centralization of shared parameters hold true at larger scales or across different data modalities.
- **What evidence would resolve it:** Applying the cross-task gradient trace tool to larger models (e.g., 70B+ parameters) or vision-language models to verify if mixed training still yields centralized shared parameters in critical attention heads.

### Open Question 2
- **Question:** How does the presence of noisy, imbalanced, or conflicting data affect the formation and utility of shared parameters in fact recall?
- **Basis in paper:** [explicit] The authors note in Appendix A.1 that "Real-world data often involve noise, imbalance, and outdated or conflicting information" and suggest future work must "evaluate the robustness of mixed training under these more realistic conditions."
- **Why unresolved:** The current findings rely on synthetic, clean, and balanced datasets; it is unclear if shared parameters can emerge effectively when fact-storing and fact-recalling examples contain contradictions or noise.
- **What evidence would resolve it:** Experiments utilizing the proposed gradient trace method on datasets with injected noise or factual conflicts to analyze if shared parameters degrade or if the model fails to consolidate knowledge.

### Open Question 3
- **Question:** What are the precise causal mechanisms by which shared parameters in attention heads improve the model's ability to generalize factual knowledge?
- **Basis in paper:** [inferred] The limitations section (Appendix A.1) notes the paper "stops short of a detailed causal analysis of how these parameters improve the fine-tuned model" and calls for a "Deeper understanding of the underlying mechanisms."
- **Why unresolved:** While the paper identifies where shared parameters are located (critical attention heads) and correlates their ablation with performance drops, it does not fully map the specific computational transformations these parameters enforce.
- **What evidence would resolve it:** Causal intervention studies (e.g., activation patching) that isolate the specific information flow manipulated by shared parameters to prove they directly enable the transfer of knowledge from BIO to QA formats.

## Limitations

- Analysis relies on synthetic data that may not capture real-world fact recall complexity
- Gradient-based identification assumes linear relationships in parameter influence
- Ablation studies demonstrate correlation but establishing definitive causal mechanisms requires additional controlled experiments
- Concentration of shared parameters in specific attention heads as primary mechanism remains partially speculative
- Alternative explanations like differences in optimization dynamics are not fully explored

## Confidence

**High Confidence**: The observation that mixed training yields more shared parameters (1.2x larger) and achieves better out-of-distribution QA performance is well-supported by experimental results.

**Medium Confidence**: The interpretation that shared parameters function as reusable knowledge representations requires additional validation. While ablation results show asymmetric impacts, specific mechanisms remain partially speculative.

**Low Confidence**: The concentration of shared parameters in specific attention heads as the primary mechanism for fact recall generalization is the weakest link in the causal chain.

## Next Checks

1. **Cross-Architecture Validation**: Replicate the shared parameter analysis on GPT-2, BERT, and other transformer architectures to verify whether the 1.2x shared parameter advantage for mixed training holds across different model families.

2. **Real-World Dataset Test**: Replace synthetic BIO/QA data with real-world knowledge sources (Wikipedia facts, Freebase triples) to test whether shared parameter formation and its benefits generalize beyond controlled synthetic environments.

3. **Alternative Attribution Methods**: Compare cross-task gradient trace results with other parameter importance methods (Integrated Gradients, SHAP) to verify that the identified shared parameters are not artifacts of the specific gradient accumulation approach.