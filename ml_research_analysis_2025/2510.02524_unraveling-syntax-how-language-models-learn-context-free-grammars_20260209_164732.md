---
ver: rpa2
title: 'Unraveling Syntax: How Language Models Learn Context-Free Grammars'
arxiv_id: '2510.02524'
source_url: https://arxiv.org/abs/2510.02524
tags:
- subgrammar
- language
- learning
- pretraining
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for studying how language
  models acquire syntax by analyzing their learning dynamics on probabilistic context-free
  grammars (PCFGs). The authors develop recursive formulas for the training loss and
  KL-divergence over subgrammar structures, showing that loss decomposes into sums
  over subgrammars rather than learning in stages.
---

# Unraveling Syntax: How Language Models Learn Context-Free Grammars

## Quick Facts
- arXiv ID: 2510.02524
- Source URL: https://arxiv.org/abs/2510.02524
- Reference count: 22
- Language models learn all subgrammars in parallel rather than mastering simpler structures first

## Executive Summary
This paper develops a theoretical and empirical framework for studying how language models acquire syntax by analyzing their learning dynamics on probabilistic context-free grammars (PCFGs). The authors introduce recursive formulas for training loss and KL-divergence over subgrammar structures, showing that loss decomposes additively over subgrammars rather than accumulating hierarchically. They prove that subgrammar pretraining can improve final loss for smaller models and leads to more structured internal representations. Empirically, they find that models learn all subgrammars in parallel rather than mastering simpler structures first, contrary to child language acquisition patterns. The study also reveals that models struggle with deeper recursive structures, a limitation that persists even in large language models.

## Method Summary
The authors study learning dynamics by training small transformers (2-4 layers, 2 heads) on synthetic languages generated from PCFGs. They develop recursive formulas for KL-divergence decomposition over subgrammar structures, enabling tracking of per-subgrammar loss throughout training. The method includes subgrammar pretraining phases followed by full-grammar training, with representation analysis using Centered Kernel Alignment (CKA) to measure internal alignment. Experiments test parallel learning behavior, pretraining benefits, and generalization to deeper recursion structures.

## Key Results
- Training loss decomposes additively over subgrammar structure rather than accumulating hierarchically
- Transformers learn all subgrammars in parallel rather than mastering simple structures first
- Subgrammar pretraining induces more aligned internal representations and improves final loss for smaller models
- Models struggle with deeper recursive structures, failing to generalize even with low training loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training loss decomposes additively over subgrammar structure rather than accumulating hierarchically
- Mechanism: KL-divergence between a PCFG distribution and model distribution equals a weighted sum of KL-divergences over each subgrammar, plus constant overhead from fixed terminal strings. Recursive application yields a sum over leaf subgrammars in the grammar's DAG decomposition.
- Core assumption: Model is autoregressive; subgrammar structure is properly defined
- Evidence anchors: [abstract] "They prove that subgrammar pretraining can improve final loss for smaller models"; [Section 4.2] Theorem 4.3 and Corollary 4.4 provide the recursive decomposition formula

### Mechanism 2
- Claim: Transformers reduce loss across all subgrammars in parallel rather than mastering simple structures first
- Mechanism: Gradient-based optimization simultaneously decreases the KL contribution from each subgrammar because the total loss is a sum; there is no inductive bias in standard training that enforces stage-wise acquisition.
- Core assumption: Standard training with no curriculum; subgrammars have non-zero probability in the data distribution
- Evidence anchors: [abstract] "Empirically, they find that models learn all subgrammars in parallel rather than mastering simpler structures first"; [Section 4.3] Figures 1 and 2 show simultaneous loss reduction across subgrammars

### Mechanism 3
- Claim: Subgrammar pretraining induces more aligned internal representations and can improve final loss for smaller models
- Mechanism: Pretraining moves the model into a region of weight space where the subgrammar is stably represented; subsequent training on the full grammar retains this structure, as measured by higher CKA similarity across attention layers.
- Core assumption: Smaller model capacity makes the inductive bias from pretraining beneficial; excessive pretraining can over-specialize and reduce gains
- Evidence anchors: [abstract] "They prove that subgrammar pretraining can improve final loss for smaller models and leads to more structured internal representations"; [Section 5.2] Table 1 shows CKA increases of +8.9% to +21.7% in attention layers with pretraining

## Foundational Learning

- Concept: Context-Free Grammars (CFGs) and PCFGs
  - Why needed here: The entire framework is built on PCFGs as a surrogate for natural language syntax; understanding production rules, non-terminals, and subgrammar decomposition is essential
  - Quick check question: Given a CFG with rules S â†’ aSb | c, what strings are in the language and what is the inner subgrammar rooted at S?

- Concept: KL-Divergence and Language Modeling Loss
  - Why needed here: The theoretical results link training loss directly to KL-divergence over subgrammars; Proposition 3.9 shows loss = KL + entropy
  - Quick check question: If two models have identical loss on a distribution but one has lower KL-divergence, what must be true about the entropy term?

- Concept: Transformer Representations and CKA
  - Why needed here: Section 5.2 uses Centered Kernel Alignment to quantify representation similarity across layers and training conditions
  - Quick check question: What would a CKA score near 0 vs. near 1 indicate about two sets of layer activations?

## Architecture Onboarding

- Component map: PCFG generator -> Transformer LM -> Subgrammar analyzer -> Representation probe
- Critical path: 1) Define PCFG with explicit subgrammar structure; 2) Generate training data from PCFG; 3) Train transformer with optional subgrammar pretraining; 4) Track per-subgrammar KL throughout training; 5) Probe representations with CKA; 6) Test generalization to deeper recursion
- Design tradeoffs: Model size vs. pretraining benefit (smaller models gain more); pretraining duration (too little fails, too much over-specializes); grammar complexity (deeper recursion enables stronger tests but exposes sharper failures)
- Failure signatures: Parallel learning fails if subgrammar probabilities are near-zero; pretraining hurts if excessive; depth generalization collapses (expected behavior)
- First 3 experiments: 1) Replicate Figure 1: Train 2-layer transformer on simple PCFG with 2-3 subgrammars; plot per-subgrammar KL over training; 2) Pretraining ablation: Compare final KL distributions (30 seeds) with vs. without 10-epoch subgrammar pretraining on 2-layer vs. 4-layer models; 3) Depth generalization test: Train on Nested Parentheses grammar; evaluate next-token prediction error on contexts of fixed depth vs. increasing depth

## Open Questions the Paper Calls Out

- Question: Do small transformers possess the representational capacity to perfectly model deeply recursive PCFGs, or is the failure to generalize to greater depths a fundamental architectural limitation?
  - Basis in paper: [explicit] The authors state in the Discussion: "First, we conjecture that despite the results of Section 6 there exists a setting of the weights of, say, a 2-layer, 2-head transformer... that does correctly model the PCFG."
  - Why unresolved: The paper demonstrates that gradient descent fails to find solutions for deep recursion, but it remains unproven whether this is due to the optimization process getting stuck or the model class being inherently incapable of representing the necessary recursive stack.

- Question: What is the optimal training strategy to consistently converge to the best loss optima when utilizing the known subgrammar structure of a target language?
  - Basis in paper: [explicit] Section 5.2 notes: "We leave open the question of how to train a model to consistently converge to the best optima, given the rather strong prior of the subgrammar structure of the target CFG."
  - Why unresolved: While the paper shows subgrammar pretraining can help smaller models, the trade-off is delicate; too little pretraining fails to provide a strong inductive bias, while too much may over-specialize the model and hinder transfer to the full grammar.

- Question: How does the "difficulty of depth" (recursion) compare to other kinds of dependent structures in terms of learning dynamics for neural networks?
  - Basis in paper: [explicit] The Discussion asks: "How does 'difficulty of depth' compare to other kinds of dependent structure?"
  - Why unresolved: The paper focuses heavily on the failure of models to generalize to deeper recursion, but it does not isolate whether this difficulty is unique to hierarchical nesting or simply a symptom of complex long-range dependencies in general.

## Limitations
- Theoretical framework relies on exact KL-divergence decomposition which may not hold for approximate inference
- Empirical validation limited to small-scale transformers (2-4 layers), uncertain generalization to large language models
- Synthetic PCFGs may not capture all complexities of natural language syntax, particularly semantic dependencies

## Confidence
- High Confidence: Parallel learning mechanism and KL-decomposition framework
- Medium Confidence: Subgrammar pretraining benefits and representation alignment findings
- Low Confidence: Generalization failure on deeper recursion and underlying causes

## Next Checks
1. **Architecture Scaling Test**: Replicate parallel learning and pretraining experiments with larger transformers (8-16 layers) to determine if findings scale and whether pretraining benefits persist
2. **Robustness to Grammar Variations**: Test framework across diverse PCFG structures (center-embedding, cross-serial dependencies) to verify parallel learning holds beyond specific grammars
3. **Alternative Representation Metrics**: Validate CKA-based representation alignment findings using alternative similarity metrics (CCA, SVCCA) to ensure effects are not metric-specific artifacts