---
ver: rpa2
title: 'Mixtures Closest to a Given Measure: A Semidefinite Programming Approach'
arxiv_id: '2509.22879'
source_url: https://arxiv.org/abs/2509.22879
tags:
- mixture
- mixtures
- measure
- gaussian
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of approximating a target probability
  measure by a mixture of distributions from a parametric family, with the approximation
  quality measured by the 2-Wasserstein (W2) or total variation (TV) distance. Unlike
  many existing approaches, the parameter set is not assumed to be finite; it is modeled
  as a compact basic semi-algebraic set.
---

# Mixtures Closest to a Given Measure: A Semidefinite Programming Approach

## Quick Facts
- **arXiv ID:** 2509.22879
- **Source URL:** https://arxiv.org/abs/2509.22879
- **Reference count:** 40
- **Primary result:** Approximates a target measure by a mixture from a parametric family using semidefinite relaxations, with finite recovery under rank conditions.

## Executive Summary
This paper addresses the problem of approximating a target probability measure by a mixture of distributions from a parametric family, using the 2-Wasserstein (W2) or total variation (TV) distance as the approximation quality metric. The method is notable for not requiring the parameter set to be finite; instead, it models the parameter set as a compact basic semi-algebraic set and introduces a hierarchy of semidefinite relaxations with asymptotic convergence to the optimal value. When a specific rank condition is satisfied, the convergence is finite and exact recovery of the optimal mixing measure is obtained.

The approach leverages the Moment-SOS (Sum of Squares) framework, reformulating the problem as a generalized moment problem with polynomial data. For the TV distance, the authors exploit a domination property from the Hahn-Jordan decomposition to derive tractable relaxations. The method is demonstrated on clustering applications, serving as both a standalone method and a preprocessing step that provides both the number of clusters and strong initial parameter estimates, accelerating convergence of standard clustering algorithms.

## Method Summary
The paper introduces a hierarchy of semidefinite relaxations to approximate a target measure μ by a mixture ν_ϕ from a parametric family, minimizing W2 or TV distance with regularization. The method optimizes over probability measures by solving a sequence of convex semidefinite programs (SDPs) that converge to the global optimum. The approach reformulates mixture fitting as a Generalized Moment Problem (GMP), optimizing pseudo-moment sequences subject to Positive Semidefiniteness (PSD) constraints on moment matrices. For TV distance, the method uses the Hahn-Jordan decomposition to enforce tractable domination constraints. When a flat extension rank condition is met, the algorithm recovers exact mixture components and order in finite time through Curto-Fialkow extraction. The method is applied to clustering, providing both cluster number estimation and initialization parameters.

## Key Results
- Introduces a hierarchy of semidefinite relaxations with asymptotic convergence for approximating measures by mixtures
- Achieves finite convergence and exact recovery when a rank condition (flat extension) is satisfied
- Provides a tractable formulation for total variation distance using Hahn-Jordan decomposition domination constraints
- Demonstrates clustering applications with improved initialization over random methods

## Why This Works (Mechanism)

### Mechanism 1: Lasserre Hierarchy for Moment Relaxation
The paper transforms the non-convex mixture fitting problem into a sequence of convex semidefinite programs that converge to the global optimum. By reformulating optimization over probability measures as a Generalized Moment Problem, the method optimizes pseudo-moment sequences subject to PSD constraints on moment matrices. This approximation becomes exact as the relaxation order increases. The approach requires the parameter set to be a compact basic semi-algebraic set and the target measure to satisfy the multivariate Carleman condition.

### Mechanism 2: Finite Recovery via Flat Extension
When a specific rank condition is met, the algorithm recovers exact mixture components in finite time. The flat extension criterion (rank stability between moment matrices) implies the moment sequence corresponds to a discrete measure. Under this condition, mixture parameters can be extracted as eigenvalues of multiplication matrices derived from the moment matrix. The method requires appropriate regularization to promote sparsity without distortion.

### Mechanism 3: Tractable Total Variation via Hahn-Jordan Domination
The paper makes TV distance tractable by leveraging the Hahn-Jordan decomposition. The difference between measures is decomposed into positive and negative parts, with domination constraints encoded as linear matrix inequalities. This allows TV distance minimization within the SDP framework, requiring polynomial moment maps for the parametric family.

## Foundational Learning

- **Concept:** **Moment-SOS (Sum of Squares) Hierarchy**
  - **Why needed here:** This is the mathematical engine of the paper. You must understand that "optimizing over measures" is relaxed to "optimizing over sequences of moments" which is further relaxed to "optimizing over positive semidefinite matrices."
  - **Quick check question:** Can you explain why a Positive Semidefinite (PSD) matrix constraint (M ⪰ 0) implies that the underlying measure is non-negative?

- **Concept:** **Rank Condition / Flat Extension**
  - **Why needed here:** This is the stopping criterion. A new engineer needs to distinguish between "asymptotic convergence" (theoretical ideal) and "finite convergence" (practical extraction point).
  - **Quick check question:** If the moment matrix M_d has rank 3, what does that imply about the number of clusters (components) in the optimal mixture?

- **Concept:** **Generalized Moment Problem (GMP)**
  - **Why needed here:** The paper reformulates mixture fitting as a GMP. Understanding this shift from "fitting likelihoods" to "matching moments" is crucial for implementing the constraints.
  - **Quick check question:** In a standard Gaussian Mixture Model (GMM), how does the mean parameter μ relate to the first moment of the distribution?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Constraint Builder -> SDP Solver Interface (TSSOS) -> Extractor
- **Critical path:** Input sample data → Compute moments up to order 2d → Define parameter support S_θ → Select relaxation order d → Solve SDP → Check rank of solution → If not "flat," increment d
- **Design tradeoffs:** Order d vs. Scalability (increasing d improves accuracy but causes combinatorial growth in matrix size); Regularization ε vs. Accuracy (high ε promotes sparsity and flat extension but biases solution away from true global optimum)
- **Failure signatures:** Rank instability (eigenvalues decay slowly without clear rank gap; fix: increase tol or ε); Infeasibility (SDP returns "infeasible"; fix: relax support constraints or check moment computation); Boundary Solutions (parameters lie exactly on boundary of S_θ; fix: enlarge support set)
- **First 3 experiments:** 1) Uniform Distribution Test: Fit mixture to uniform noise to verify smooth approximation and validate basic pipeline. 2) Initialization Benchmark: Compare EM iterations required with SDP solution vs. random initialization on 2D Gaussian mixture. 3) Scalability Limit: Run method on datasets with increasing dimensionality (n=2, 4, 8) while keeping d fixed to observe computational limits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specific problem structures, such as symmetries, be exploited to improve the scalability of the SDP relaxations for high-dimensional data?
- **Basis in paper:** [explicit] The conclusion states that a "promising direction for future research is to enhance scalability by distribution-family-specific problem structure exploitation (e.g., through symmetry detection)."
- **Why unresolved:** The authors identify "moderate scalability" as a main limitation, noting that the size of Positive Semidefinite (PSD) constraints grows rapidly with the relaxation order and dimensionality.
- **What evidence would resolve it:** A modified algorithm or theoretical framework demonstrating that exploiting symmetry allows for solving relaxations of order d in significantly higher dimensions n or p than the current method.

### Open Question 2
- **Question:** How do alternative regularization schemes influence the recovery of optimal solutions and the choice between W2 and TV distances?
- **Basis in paper:** [explicit] The conclusion suggests that "alternative regularization schemes can be explored and their effect on the recovery of optimal solutions systematically assessed" to "understand the differences between W2-and-TV distances."
- **Why unresolved:** Experiments showed varying performance between W2 and TV distances (e.g., EM favored W2 while k-means favored TV), but the role of the regularization parameter ε and polynomial R in this dynamic is not fully characterized.
- **What evidence would resolve it:** A systematic study mapping different regularization strengths and polynomials to the convergence speed and accuracy of W2 vs. TV formulations across distinct problem classes.

### Open Question 3
- **Question:** What is the convergence rate of the SDP hierarchy relative to the sample size N?
- **Basis in paper:** [explicit] The authors list "Quantifying convergence rate with respect to the sample size" as "another interesting direction."
- **Why unresolved:** The paper proves asymptotic convergence and finite convergence under rank conditions, but it does not quantify how quickly the solution derived from the empirical measure μ_N converges to the solution for the true measure μ as N increases.
- **What evidence would resolve it:** Theoretical bounds establishing the probability that the optimal value τ_{d, ε, R} is within a specific δ of the true optimal value, given a sample size N.

## Limitations
- The flat extension rank condition is sufficient but not necessary for finite convergence, limiting practical applicability when optimal measures are not atomic
- Tractability of TV formulation depends on Carleman condition and polynomial moment maps, potentially failing for non-polynomial exponential families
- Regularization parameters ε and polynomial R are tuned heuristically without systematic selection methods

## Confidence

- **High confidence:** The SDP relaxation framework (Lasserre hierarchy) is a well-established, theoretically sound method for approximating optimal transport distances over measures. The polynomial constraints and moment-based reformulation are mathematically rigorous.
- **Medium confidence:** The finite convergence via flat extension is proven under algebraic conditions (rank stability), but its practical detection is sensitive to numerical precision. The clustering initialization benefits are demonstrated empirically but lack a theoretical guarantee on improvement over random initialization.
- **Low confidence:** The novel Hahn-Jordan domination approach for TV distance is presented as a contribution, but the paper provides limited experimental validation of this specific mechanism compared to the W2 case.

## Next Checks

1. **Rank Stability Under Noise:** Generate synthetic data from a known 3-component Gaussian mixture. Compute empirical moments with varying sample sizes N ∈ {100, 500, 1000}. For each N, solve the SDP at relaxation orders d ∈ {2,3,4} and plot the eigenvalue spectrum of M_d(φ*). Verify that a clear rank gap emerges for sufficient N and d.

2. **TV vs W2 Accuracy Trade-off:** Replicate a single clustering experiment (e.g., Figure 2) using both the W2 and TV formulations. Measure not only the number of EM iterations to convergence but also the final log-likelihood and misclassification rate. This tests whether the TV formulation's computational tractability comes at the cost of approximation accuracy.

3. **Scalability Benchmark:** Implement the method for Gaussian mixtures in dimensions n ∈ {2, 4, 8}. For each n, fix K=3 components and measure the wall-clock time to solve the SDP at d=3 using the same solver (e.g., Mosek). Plot time vs. dimension to empirically determine the computational limit of the approach.