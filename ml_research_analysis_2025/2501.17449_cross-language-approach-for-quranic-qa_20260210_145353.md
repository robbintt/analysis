---
ver: rpa2
title: Cross-Language Approach for Quranic QA
arxiv_id: '2501.17449'
source_url: https://arxiv.org/abs/2501.17449
tags:
- dataset
- quranic
- quran
- arabic
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of Quranic QA systems, which
  face linguistic disparities between Modern Standard Arabic questions and Classical
  Arabic Quranic verses, compounded by limited dataset sizes. To overcome these challenges,
  the authors adopt a cross-language approach involving dataset expansion through
  machine translation, paraphrasing, and the integration of diverse question sets,
  followed by fine-tuning pre-trained models such as BERT-Medium, RoBERTa-Base, DeBERTa-v3-Base,
  ELECTRA-Large, Flan-T5, Bloom, and Falcon.
---

# Cross-Language Approach for Quranic QA

## Quick Facts
- arXiv ID: 2501.17449
- Source URL: https://arxiv.org/abs/2501.17449
- Reference count: 28
- Key outcome: Cross-language approach using dataset expansion and fine-tuned models (RoBERTa-Base MAP@10=0.34, MRR=0.52) improves Quranic QA performance

## Executive Summary
This study addresses Quranic QA challenges arising from linguistic disparities between Modern Standard Arabic questions and Classical Arabic Quranic verses. The authors employ a cross-language approach involving dataset expansion through machine translation, paraphrasing, and integration of diverse question sets, followed by fine-tuning pre-trained models including BERT-Medium, RoBERTa-Base, DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon. Experimental results demonstrate significant performance improvements, with RoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24).

## Method Summary
The approach involves expanding the original 251-question Quran QA dataset through integration with external sources and paraphrasing each question twice to create 1,895 total questions. Arabic questions are translated to English using Google Translate API to leverage English-optimized pre-trained models. The system retrieves passages from the Pickthall English Quran translation and employs cross-encoder models fine-tuned on SQuAD v2 and the expanded Quranic dataset. Training uses positive/negative passage contrast to improve context-sensitive ranking, with models evaluated on MAP@10, MRR, Recall@k, and Precision@k metrics.

## Key Results
- RoBERTa-Base achieves highest MAP@10 (0.34) and MRR (0.52) among tested models
- DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24)
- Cross-language approach successfully bridges MSA-CA linguistic gap
- Dataset expansion from 251 to 1,895 questions significantly improves performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating MSA questions to English enables better exploitation of English-optimized pre-trained models for Quranic passage retrieval
- Mechanism: Arabic questions → Google Translate API → English questions → retrieve from English Quran translation → English-optimized transformer models process both in shared semantic space
- Core assumption: Translation preserves sufficient semantic meaning for retrieval, and English pre-trained representations capture cross-lingual transferable semantic relationships
- Evidence anchors: [abstract] "machine translation to convert Arabic questions into English... retrieving answers from an English translation of the Quran" [section 3.2] "many pre-trained large language models... are primarily optimized for English, this translation step was essential to bridge the linguistic gap"

### Mechanism 2
- Claim: Dataset expansion through paraphrasing and multi-source integration improves model robustness for low-resource Quranic QA
- Mechanism: Original 251 questions → integrate external sources → paraphrase each question twice → final 1,895 questions with linguistic diversity
- Core assumption: Paraphrases maintain semantic equivalence while introducing surface-form variation that improves generalization
- Evidence anchors: [abstract] "paraphrasing questions to create linguistic diversity" [section 3.1] "Each question was rephrased twice, resulting in a rich dataset of 1,895 questions categorized into single-answer, multi-answer, and zero-answer types"

### Mechanism 3
- Claim: Cross-encoder architecture with positive/negative passage contrast improves context-sensitive ranking for ambiguous queries
- Mechanism: Combine question + passage as single input → cross-encoder generates relevance score → randomizer mixes positive and negative passages → contrastive learning sharpens boundary
- Core assumption: Joint processing captures subtle question-passage interactions better than separate encoding
- Evidence anchors: [section 3.3] "a cross-encoder model was implemented... by evaluating them as a single input, particularly when it came to recognizing the most significant portions" [section 3.4] "The fine-tuned model generates output in the form of relevance scores for each passage... used to rank the passages"

## Foundational Learning

- **Cross-Language Transfer in QA**
  - Why needed here: The entire approach hinges on translating to English to leverage superior pre-trained models; understanding when this works vs. fails is critical
  - Quick check question: What types of semantic content are most likely to be lost or distorted when translating Quranic Arabic questions to English?

- **Cross-Encoder vs. Dual-Encoder Retrieval**
  - Why needed here: The paper uses cross-encoders for fine-grained relevance scoring; knowing the tradeoff (accuracy vs. speed) informs deployment decisions
  - Quick check question: Why would a cross-encoder be more accurate but slower than a dual-encoder for passage retrieval?

- **Ranking Metrics (MAP@k, MRR, Recall@k, Precision@k)**
  - Why needed here: Results are reported across multiple metrics; understanding what each measures helps interpret which model is best for which use case
  - Quick check question: If a model has high Recall@10 but low MAP@10, what does that tell you about its ranking behavior?

## Architecture Onboarding

- **Component map:**
  - Quran QA 2023 dataset (251 Q) → external sources → cleaned → paraphrased (2×) → 1,895 questions → Google Translate API → English questions → Pickthall English Quran → cross-encoder models (BERT-Medium, RoBERTa-Base, DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, Falcon) → relevance scoring → ranked top-k passages

- **Critical path:**
  1. Collect and validate questions from multiple authentic sources
  2. Clean and standardize question formats
  3. Paraphrase each question twice (verify semantic preservation)
  4. Translate all questions to English using Google Translate API
  5. Align questions with Pickthall English Quran passages
  6. Create positive/negative passage pairs with relevance labels
  7. Fine-tune cross-encoder models with mixed batches
  8. Evaluate on MAP@10, MRR, Recall@k, Precision@k

- **Design tradeoffs:**
  - Translation fidelity vs. model capability: English models are stronger but translation may blur theological nuance
  - Dataset size vs. quality: Expansion (251→1,895) improves coverage but risks noise from automated paraphrasing
  - Model size vs. inference latency: Flan-T5 (11B) and Falcon (7B) vs. BERT-Medium (86M)—larger models may not justify cost if gains are marginal

- **Failure signatures:**
  - High Recall@10 with low MAP@10: Relevant passages retrieved but poorly ranked
  - Large gap between baseline and fine-tuned on some models but not others: Data augmentation benefits model-dependent
  - Consistently low Precision@5 across all models: Candidate pool too noisy or negative sampling flawed

- **First 3 experiments:**
  1. **Baseline replication:** Run all listed models on original 251 Arabic questions without translation or augmentation to establish floor performance
  2. **Ablation by augmentation type:** Test (a) paraphrasing only, (b) translation only, (c) full pipeline to isolate contribution of each component
  3. **Cross-encoder vs. dual-encoder comparison:** For best-performing model (RoBERTa-Base or DeBERTa-v3-Base), compare cross-encoder and dual-encoder variants on MAP@10 and inference latency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the conclusion suggests future research directions including fine-tuning multilingual models to improve accuracy and generalizability.

## Limitations
- Translation fidelity remains a critical unknown - no validation of semantic preservation when converting MSA questions to English
- Paraphrasing methodology is underspecified - whether automated or manual, the semantic consistency of paraphrased questions directly impacts downstream model performance
- Cross-encoder training details are incomplete - specific loss functions, negative sampling ratios, and batch configurations are not reported

## Confidence
- **High confidence** in the general cross-language approach and observed performance improvements (MAP@10: 0.34, MRR: 0.52 for RoBERTa-Base)
- **Medium confidence** in the dataset expansion methodology - while size increased from 251 to 1,895 questions, the quality and semantic consistency of augmented data is unverified
- **Low confidence** in precise implementation details needed for exact reproduction - hyperparameters, negative sampling strategies, and evaluation protocols require clarification

## Next Checks
1. **Translation quality audit**: Select 50 Arabic questions and compare Google Translate outputs with human translations to measure semantic drift
2. **Paraphrase consistency test**: Evaluate 100 paraphrased questions with human raters to verify semantic equivalence between original and paraphrased versions
3. **Ablation study**: Re-run experiments with (a) translation only, (b) paraphrasing only, (c) both components to quantify individual contributions to performance gains