---
ver: rpa2
title: 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention'
arxiv_id: '2506.13585'
source_url: https://arxiv.org/abs/2506.13585
tags:
- arxiv
- training
- attention
- minimax-m1
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MiniMax-M1 is the first open-weight large-scale reasoning model\
  \ using hybrid MoE + lightning attention for efficient long-context scaling. It\
  \ natively supports 1M input tokens and 80K generation tokens, enabling efficient\
  \ scaling of test-time compute\u2014consuming ~25% FLOPs of DeepSeek R1 at 100K\
  \ tokens."
---

# MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention

## Quick Facts
- arXiv ID: 2506.13585
- Source URL: https://arxiv.org/abs/2506.13585
- Reference count: 15
- Primary result: First open-weight large-scale reasoning model with hybrid MoE + lightning attention scaling to 1M context, trained via CISPO RL to achieve 25% FLOPs of DeepSeek R1 at 100K tokens while surpassing leading baselines.

## Executive Summary
MiniMax-M1 is the first open-weight large-scale reasoning model to combine hybrid MoE + lightning attention for efficient long-context scaling. It supports 1M input tokens and 80K generation tokens, achieving 25% of the FLOPs of DeepSeek R1 at 100K tokens while outperforming open-weight baselines on software engineering, agentic tool use, and long-context tasks. The model uses a novel RL algorithm, CISPO, which clips importance sampling weights rather than token updates, enabling more efficient training and better preservation of critical reasoning tokens. Trained on 512 H800 GPUs over 3 weeks at ~$534,700, MiniMax-M1 establishes a strong foundation for next-generation LLM agents requiring long-context reasoning.

## Method Summary
MiniMax-M1 uses a 456B parameter MoE architecture with 45.9B active parameters and 32 experts, employing a hybrid attention mechanism with 7 lightning-attention blocks followed by 1 softmax-attention block. The model was pretrained on 7.5T tokens (70% STEM/code/reasoning) with a 4-stage context curriculum extending from 32K to 1M tokens, then underwent cold-start SFT with long chain-of-thought data. Large-scale RL training used CISPO, an algorithm that clips importance sampling weights rather than token updates, enabling more efficient learning of reflective reasoning behaviors. The RL pipeline included sandbox-based software engineering environments, math and logic tasks, and competitive programming, with context length scaling from 40K to 80K tokens through staged expansion.

## Key Results
- Consumes 25% of FLOPs compared to DeepSeek R1 at 100K generation tokens
- Achieves SOTA on TAU-bench tool-use evaluation, surpassing Gemini 2.5 Pro
- Outperforms DeepSeek-R1 and Qwen3-235B on software engineering, agentic tool use, and long-context tasks

## Why This Works (Mechanism)

### Mechanism 1
Lightning attention enables near-linear computational scaling for long generation sequences by interleaving 7 transnormer blocks with linear-complexity lightning attention followed by 1 transformer block with softmax attention. This reduces per-token computation from O(n) to O(1) amortized for the majority of layers while retaining softmax attention for precise recall. The core assumption is that linear attention preserves sufficient modeling capacity for reasoning tasks while softmax attention every 8th block provides adequate "anchor points" for complex retrieval. Evidence shows 25% FLOPs reduction versus DeepSeek R1 at 100K tokens. Break condition: if linear attention layers degrade multi-hop reasoning accuracy beyond acceptable thresholds or if generation quality collapses past 80K tokens due to accumulated approximation error.

### Mechanism 2
Clipping importance sampling weights rather than token updates preserves gradient signals from rare but critical reasoning tokens. PPO/GRPO clip the objective function when IS ratios exceed thresholds, effectively zeroing gradients for low-probability tokens with high advantages. CISPO instead clips the IS weight directly while always including the token in gradient computation, preventing "fork" tokens like "However" and "Recheck" from being silently dropped. The core assumption is that low-probability reflective tokens are disproportionately important for reasoning emergence and should never be excluded from updates. Evidence shows CISPO outperforms other RL variants on AIME 2024 with Qwen2.5-32B. Break condition: if clipped IS weights introduce sufficient bias to prevent convergence or if entropy explodes without token-level clipping constraints.

### Mechanism 3
Execution-based sandbox environments with binary test rewards improve software engineering capabilities more reliably than model-based evaluation. Containerized sandboxes execute model-generated code against test suites, providing unambiguous pass/fail signals. This avoids reward model biases and grounds training in verifiable outcomes. The core assumption is that sandbox test coverage adequately represents real-world software engineering failure modes. Evidence shows sandbox-based rewards used for several thousand SE samples. Break condition: if sandbox tests are too narrow and incentivize overfitting to test patterns rather than genuine bug fixing.

## Foundational Learning

- **Concept**: Linear attention fundamentals (Qin et al., 2022a)
  - **Why needed here**: Lightning attention is a specific linear attention variant; understanding the core approximation—replacing softmax(QK^T)V with feature-map-based decomposition—is essential to diagnose recall failures.
  - **Quick check question**: Explain why standard attention is O(n²) and how linear attention reduces this to O(n) with a fixed-size state.

- **Concept**: Importance sampling in policy gradient methods
  - **Why needed here**: CISPO modifies how IS weights are handled; without understanding off-policy correction, the motivation for clipping weights versus clipping token updates will be opaque.
  - **Quick check question**: In PPO, why does the IS ratio r_t(θ) appear in the objective, and what happens when r_t >> 1?

- **Concept**: Mixture-of-Experts routing and load balancing
  - **Why needed here**: The 456B/45.9B parameter split implies sparse routing; understanding auxiliary losses for expert load balancing helps interpret training instability.
  - **Quick check question**: What is the purpose of the MoE auxiliary loss, and why might decreasing its coefficient improve model quality?

## Architecture Onboarding

- **Component map**: Input → Embedding → [7× Transnormer (Lightning Attn + FFN) → 1× Transformer (Softmax Attn + FFN)] × N blocks → MoE Layers (32 experts, top-k routing) → LM Head (FP32)

- **Critical path**: 1) Continual pretraining (7.5T tokens, STEM-heavy, 32K→1M context curriculum) 2) SFT cold-start (long CoT injection, 60% math/code) 3) RL Phase 1 (40K generation limit, CISPO, rule-based + model-based rewards) 4) RL Phase 2 (40K→80K staged expansion, difficulty filtering, repetition early-stopping)

- **Design tradeoffs**: Linear vs. softmax attention ratio (7:1) balances efficiency versus recall precision; CISPO bias versus coverage tradeoff guarantees all tokens contribute gradients; sandbox complexity versus throughput tradeoff uses sparse reward signals versus full execution environments.

- **Failure signatures**: Training/inference probability mismatch (>0.01 absolute difference) indicates LM head precision issues; repetition loops with probability >0.99 on 3000+ consecutive tokens require early truncation; pattern collapse in late-generation positions during length scaling suggests negative samples dominate; GRPO/DAPO failing to improve reasoning on hybrid architecture indicates token clipping drops reflective tokens.

- **First 3 experiments**: 1) Validate training/inference alignment by comparing token probabilities between training-mode and inference-mode code (target: Pearson correlation >0.995) 2) CISPO vs. GRPO ablation on Qwen2.5-32B to confirm 2x sample efficiency claim before scaling to full MoE model 3) Sandbox reward correlation analysis on held-out SWE-bench issues to verify sandbox signal aligns with real-world usefulness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical characterization of the bias introduced by clipping importance sampling weights in CISPO, and how does this bias scale with sequence length and model size?
- **Basis in paper**: [explicit] "Although the gradient of Eq. 4 is slightly biased due to weight clipping, this approach preserves gradient contributions from all tokens"
- **Why unresolved**: The authors acknowledge the bias but provide no theoretical bounds or empirical characterization of how it affects convergence or final performance at scale.
- **What evidence would resolve it**: Theoretical analysis of CISPO bias bounds; empirical comparison of CISPO vs unbiased baselines across varying sequence lengths and model scales.

### Open Question 2
- **Question**: What is the optimal ratio between softmax attention blocks and lightning attention blocks in hybrid architectures for different task distributions?
- **Basis in paper**: [inferred] The paper uses a fixed 1:7 ratio (one softmax block follows every seven lightning attention blocks) without ablation or justification for this specific configuration.
- **Why unresolved**: Different ratios may better suit different tasks (e.g., math reasoning vs. long-context retrieval), but no systematic study is presented.
- **What evidence would resolve it**: Ablation experiments varying the softmax-to-lightning ratio across diverse benchmark suites; analysis of which layers benefit most from each attention type.

### Open Question 3
- **Question**: Can the solutions to hybrid-architecture-specific RL instabilities (precision mismatch, early truncation, pattern collapse) generalize to other linear attention variants?
- **Basis in paper**: [explicit] "As pioneers in conducting large-scale RL experiments with this novel architecture, we encountered unique challenges and developed targeted solutions"
- **Why unresolved**: The fixes (FP32 LM head, 3000-token probability threshold, combined sample-level loss) were developed specifically for MiniMax-M1's lightning attention hybrid, but their transferability remains untested.
- **What evidence would resolve it**: Applying the same solutions to other linear attention architectures (e.g., Mamba, RWKV) during RL training; documenting which issues are architecture-specific vs. general to efficient attention mechanisms.

## Limitations

- Limited independent validation of lightning attention integration at 456B scale with MoE routing
- CISPO algorithm novel but under-validated with sparse comparative evaluation at full scale
- Sandbox environment scope and generality may be too narrow relative to 7.5T pretraining tokens
- Resource intensity limits replication with 512 H800 GPUs for 3 weeks at ~$534,700 cost

## Confidence

- **High Confidence**: Basic architectural claims (456B total/45.9B active params, 32 expert MoE, 1M context, 80K generation) and training costs are well-specified and verifiable. Benchmark results against established open-weight models appear reproducible.
- **Medium Confidence**: Efficiency claims (25% FLOPs of DeepSeek R1 at 100K tokens) depend on specific hybrid attention implementation lacking external validation. CISPO's 2x speedup versus DAPO shown only on 32B model.
- **Low Confidence**: Claims about reasoning quality emergence from hybrid architecture and CISPO algorithm at full scale primarily based on internal testing. Critical assumption that linear attention preserves multi-hop reasoning capacity through softmax "anchor points" remains unverified by independent studies.

## Next Checks

1. **Independent Lightning Attention Scaling Study**: Replicate the 7:1 hybrid architecture ratio on a medium-scale MoE model (e.g., 70B total) and test reasoning performance degradation on multi-hop tasks as linear attention proportion increases from 0% to 87.5%. Target: quantify recall loss per additional lightning block.

2. **CISPO Algorithm Generalization Test**: Implement CISPO on three diverse architectures (encoder-decoder, decoder-only, MoE) and three task families (math, code, general reasoning). Compare sample efficiency against GRPO across 10 random seeds. Target: confirm 2x speedup claim generalizes beyond Qwen2.5-32B on AIME.

3. **Sandbox Reward Coverage Analysis**: On SWE-bench, measure correlation between sandbox pass rate and human-evaluated patch quality across different bug types (regression, logic error, integration). Target: identify domains where sandbox signals may be too narrow or noisy to drive robust RL learning.