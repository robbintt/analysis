---
ver: rpa2
title: Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU
  Neural Networks
arxiv_id: '2505.21791'
source_url: https://arxiv.org/abs/2505.21791
tags:
- sout
- which
- solutions
- networks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies sparse single-hidden-layer ReLU neural networks,\
  \ where the goal is to find the sparsest interpolating network\u2014one with the\
  \ fewest nonzero parameters or neurons. The main contribution is showing that minimizing\
  \ the \u2113^p quasinorm of network weights for 0 < p < 1 provably yields the sparsest\
  \ interpolating ReLU network, for sufficiently small p."
---

# Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks

## Quick Facts
- **arXiv ID:** 2505.21791
- **Source URL:** https://arxiv.org/abs/2505.21791
- **Reference count:** 40
- **Primary result:** Minimizing $\ell^p$ quasinorm with 0 < p < 1 provably yields sparsest interpolating ReLU networks

## Executive Summary
This paper establishes a rigorous theoretical foundation for obtaining the sparsest ReLU neural networks through $\ell^p$ regularization with small p values. The authors prove that for single-hidden-layer ReLU networks trained to interpolation, global minimizers of $\ell^p$ path norm regularization recover the sparsest possible networks (with respect to $\ell^0$ norm) when p is sufficiently small. This bridges the gap between the intractable combinatorial $\ell^0$ minimization and smooth optimization, enabling gradient-based training methods to directly produce sparse networks without pruning. The work provides the first theoretical justification for using smooth $\ell^p$ penalties to obtain truly sparsest interpolating ReLU networks via gradient descent.

## Method Summary
The paper reformulates the $\ell^0$ minimization problem (finding the sparsest network) as a smooth optimization problem using $\ell^p$ quasinorms for 0 < p < 1. This quasinorm-based regularization can be expressed as a finite-dimensional concave optimization over a polytope, allowing application of the Bauer maximum principle. The authors prove that global minimizers of this $\ell^p$ objective are unique (almost everywhere) and require no more than N-2 neurons for univariate data. They also establish the existence of a data-dependent threshold p* such that for all p < p*, the $\ell^p$ minimizer coincides with the $\ell^0$ sparsest solution. The paper proposes a reweighted $\ell^1$ algorithm that approximates this optimization and demonstrates its effectiveness on synthetic data.

## Key Results
- Global minimizers of $\ell^p$ path norm are unique (almost everywhere) and require no more than N-2 neurons for univariate data
- There exists a data-dependent threshold p* such that for all p < p*, $\ell^p$ minimization recovers the sparsest $\ell^0$ solution
- The $\ell^p$ minimization problem can be reformulated as a finite-dimensional concave optimization over a polytope
- Reweighted $\ell^1$ algorithm with small p recovers much sparser solutions earlier than unregularized or weight decay-regularized networks

## Why This Works (Mechanism)
The key insight is that $\ell^p$ quasinorms with 0 < p < 1 act as smooth approximations to the $\ell^0$ norm, which directly counts nonzero parameters. By minimizing the $\ell^p$ path norm of network weights, the optimization naturally drives many weights to exactly zero, yielding sparse networks. The theoretical framework leverages the properties of ReLU activation functions and interpolation conditions to show that sufficiently small p values guarantee recovery of the sparsest solution. The reformulation as a concave optimization problem over a polytope enables rigorous analysis of global minimizers using established optimization principles.

## Foundational Learning

**ReLU activation and piecewise linearity:** Understanding that ReLU networks are piecewise linear functions is crucial because it enables counting the number of linear regions and relating this to network sparsity.

*Why needed:* The proof relies on the structure of piecewise linear interpolants and how ReLU networks partition the input space.

*Quick check:* Verify that a ReLU network with N neurons can create at most N linear regions in the univariate case.

**Quasinorms and their properties:** $\ell^p$ norms with 0 < p < 1 are quasinorms (not true norms) because they violate the triangle inequality, but they still promote sparsity through their non-convex penalty structure.

*Why needed:* The theoretical results depend on the specific properties of these quasinorms and how they approximate the $\ell^0$ norm.

*Quick check:* Confirm that for vectors with entries in [0,1], the $\ell^p$ norm decreases as p approaches 0.

**Interpolation and universal approximation:** The paper assumes networks are trained to interpolation (zero training error), which connects to classical results about ReLU networks' ability to fit any continuous function.

*Why needed:* The sparsest network problem is only well-defined under the interpolation constraint.

*Quick check:* Verify that a single-hidden-layer ReLU network can interpolate any finite dataset in general position.

**Bauer maximum principle:** This principle guarantees that a continuous concave function on a compact convex set achieves its maximum, which is used to establish existence of global minimizers.

*Why needed:* The $\ell^p$ minimization problem is reformulated as a concave optimization problem, requiring this principle for existence proofs.

*Quick check:* Confirm that the feasible set in the reformulation is indeed a compact convex polytope.

## Architecture Onboarding

**Component map:** Input data -> ReLU hidden layer -> Output layer -> $\ell^p$ regularization term -> Total loss function

**Critical path:** The most critical aspect is the path norm computation, which involves summing absolute values of incoming weights to each neuron, then taking the $\ell^p$ norm across neurons.

**Design tradeoffs:** The main tradeoff is between p value (smaller p gives sparser solutions but may be harder to optimize) versus training stability and generalization performance.

**Failure signatures:** If p is too large, the regularization won't sufficiently promote sparsity; if too small, the optimization landscape becomes more non-convex and may lead to poor local minima or training instability.

**First experiments:**
1. Verify that $\ell^p$ regularization with p=0.5 produces sparser networks than p=1.0 on a simple univariate interpolation task
2. Test the reweighted $\ell^1$ algorithm's ability to recover the sparsest solution on a synthetic dataset with known optimal sparsity
3. Compare training dynamics of $\ell^p$-regularized networks versus unregularized networks on a simple regression problem

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit in the work. The authors note that their results are limited to single-hidden-layer networks and perfect interpolation, leaving open questions about extension to deeper architectures, noisy data scenarios, and practical training with stochastic gradient descent. The computation and use of the data-dependent threshold p* in practical settings also remains an open challenge.

## Limitations
- Results are limited to single-hidden-layer ReLU networks and may not generalize to deeper architectures
- Assumes perfect interpolation (zero training loss), which may not hold for noisy datasets or when generalization is prioritized
- Theoretical results assume global optimization, while practical training uses gradient descent that may get stuck in local minima
- The data-dependent threshold p* exists theoretically but its practical computation remains unclear

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical characterization of global minimizers for single-hidden-layer ReLU networks under interpolation | High |
| Practical applicability to gradient-based training | Medium |
| Scaling to deeper architectures or noisy data scenarios | Low |

## Next Checks
1. Implement the reweighted $\ell^1$ algorithm on real-world datasets (e.g., CIFAR-10) to verify practical sparsity benefits and compare with pruning methods
2. Test whether the theoretical p* threshold can be approximated from finite samples and whether using such p values consistently yields sparser networks in practice
3. Extend experiments to deeper ReLU networks to assess whether $\ell^p$ regularization maintains its sparsity benefits beyond the single-hidden-layer case