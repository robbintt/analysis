---
ver: rpa2
title: 'MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution
  to Automate Cross-App Instructions'
arxiv_id: '2502.16796'
source_url: https://arxiv.org/abs/2502.16796
tags:
- task
- execution
- arxiv
- mobilesteward
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions

## Quick Facts
- **arXiv ID:** 2502.16796
- **Source URL:** https://arxiv.org/abs/2502.16796
- **Reference count:** 40
- **Primary result:** Introduces a multi-agent mobile automation system achieving higher cross-app task success rates through object-oriented task decomposition and memory-based self-evolution.

## Executive Summary
This paper presents MobileSteward, a novel multi-agent framework designed to automate complex cross-app mobile phone instructions. It departs from linear, step-by-step planning by decomposing tasks into app-specific sub-tasks organized as a Directed Acyclic Graph (DAG), leveraging specialized StaffAgents for execution and a StewardAgent for centralized planning and evaluation. The system incorporates memory-based self-evolution to continuously improve its task scheduling and execution capabilities by learning from successful interactions. Evaluated on the CAPBench dataset, MobileSteward demonstrates superior performance in cross-app task automation compared to existing baselines.

## Method Summary
MobileSteward employs a two-agent architecture: a StewardAgent for centralized planning, evaluation, and memory management, and StaffAgents specialized for individual mobile applications. The StewardAgent decomposes user instructions into a DAG based on information flow, schedules StaffAgents accordingly, and performs adjusted evaluation to extract and transfer key result information between tasks, mitigating error propagation. A memory-based self-evolution mechanism updates Staff Expertise Memory (for scheduling) and Task Guideline Memory (for execution guidance) using successful execution trajectories. The system is evaluated in an Android Studio emulator environment using the CAPBench and SAPBench datasets.

## Key Results
- MobileSteward achieves higher success rates on cross-app tasks compared to procedure-oriented baselines like MobileAgent-v2.
- The object-oriented DAG-based task decomposition reduces scheduling complexity in multi-app environments.
- The memory-based self-evolution mechanism enables continuous improvement without model retraining, with learned expertise comparable to hand-crafted rules.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Object-oriented task decomposition (splitting by app expertise rather than procedural steps) reduces scheduling complexity in cross-app environments.
- **Mechanism:** The StewardAgent uses Dynamic Recruitment to parse instructions into a Directed Acyclic Graph (DAG). Instead of a linear chain of actions, it identifies distinct apps and assigns sub-tasks to specialized StaffAgents based on information flow.
- **Core assumption:** Tasks can be cleanly partitioned by the app they belong to, and inter-app dependencies can be resolved via a static schedule graph.
- **Evidence anchors:**
  - [abstract] "Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction."
  - [section 3.4] "StewardAgent decomposes the instruction into sub-tasks on the specific apps... and constructs the scheduling graph $SG$ among them."
  - [corpus] "Mobile-Agent-E: Self-Evolving Mobile Assistant" (Paper ID 90381) similarly explores self-evolution, validating the direction but lacking the specific "Staff/Steward" hierarchy defined here.
- **Break condition:** If a task requires tight interleaving of steps between two apps (e.g., copy-paste-repeat loops), the DAG model may fail to capture the dynamic switching required.

### Mechanism 2
- **Claim:** Centralized evaluation and explicit information extraction mitigate error propagation and information loss common in long-horizon mobile tasks.
- **Mechanism:** After a StaffAgent finishes, the StewardAgent performs Adjusted Evaluation. It checks the history, extracts specific "result information" ($r_j$) if successful, and explicitly injects this into the prompt of the next StaffAgent in the graph.
- **Core assumption:** The StewardAgent (an LLM) can accurately extract the relevant "result information" from the previous app's state to fulfill the precondition of the next app's task.
- **Evidence anchors:**
  - [abstract] "Adjusted Evaluation conducts evaluation... which alleviates error propagation and information loss during multi-step execution."
  - [section 3.6] "StewardAgent will extract the task result information $r_j$... and used to adjust the succeeding task schedules."
  - [corpus] "Chain-of-Memory" (Paper ID 40498) supports this by arguing that implicit historical states are insufficient and explicit memory chains are needed for cross-app navigation.
- **Break condition:** If the StewardAgent fails to extract the correct key information (e.g., missing a confirmation number), the subsequent StaffAgent will fail regardless of its own capability.

### Mechanism 3
- **Claim:** Decoupling long-term learning into specialized "Expertise" and "Guideline" memories enables continuous improvement without model retraining.
- **Mechanism:** The Memory-based Self-evolution mechanism updates two distinct databases: Staff Expertise Memory (what a StaffAgent is good at) and Task Guideline Memory (how to do specific tasks). Successful executions trigger updates that refine future scheduling and execution.
- **Core assumption:** Successful trajectories contain extractable "guidelines" that generalize to future tasks.
- **Evidence anchors:**
  - [abstract] "We develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution."
  - [section 4.5.4] "The self-evolving staff expertise can achieve comparable results with hand-crafted."
  - [corpus] "Learning on the Job" (Paper ID 64517) confirms that standard agents are "test-time static" and emphasizes the necessity of experience-driven updates.
- **Break condition:** If the "successful" execution actually achieved the goal by luck (e.g., a wrong path that happened to work), the system may reinforce incorrect behaviors (hallucinated expertise).

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs)**
  - **Why needed here:** The core "Dynamic Recruitment" mechanism relies on representing the task flow as a DAG to manage dependencies between apps.
  - **Quick check question:** Can you explain why a cycle in the scheduling graph would break the current MobileSteward logic?

- **Concept: Object-Oriented vs. Procedure-Oriented Programming**
  - **Why needed here:** The paper explicitly contrasts its "object-oriented" (app-centric) approach against "procedure-oriented" (step-centric) baselines like MobileAgent-v2.
  - **Quick check question:** In this context, does an "object" represent the user or the mobile application?

- **Concept: Few-Shot In-Context Learning (ICL)**
  - **Why needed here:** The StewardAgent utilizes a 2-shot prompt for recruitment and evaluation (Section 4.2.3), and retrieves from memory to guide StaffAgents.
  - **Quick check question:** How does the BM25 retrieval of "Task Guideline Memory" relate to the standard few-shot prompting paradigm?

## Architecture Onboarding

- **Component map:** User Instruction -> StewardAgent (Planning/Evaluation) -> Scheduling Graph ($SG$) -> StaffAgents (Execution) -> History ($H_j$) -> StewardAgent (Evaluation/Reflection) -> Memories ($M_E$, $M_G$)
- **Critical path:**
  1. **Input:** User instruction hits StewardAgent.
  2. **Recruitment:** Steward queries $M_E$, generates DAG ($SG$).
  3. **Execution:** Topological sort of $SG$. For each node, retrieve guidelines from $M_G$ -> StaffAgent acts -> History generated.
  4. **Evaluation:** Steward reviews History.
      - *If Error:* Generate reflection ($t_j$) -> Retry (max 3).
      - *If Success:* Extract result ($r_j$) -> Pass to next node in graph.
  5. **Evolution:** On success, extract expertise/guidelines -> Update Memories.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Multi-agent coordination (Steward $\to$ Staff $\to$ Steward) adds significant inference overhead compared to single-agent reactive baselines.
  - **Specialization vs. Generalization:** StaffAgents are app-specific (e.g., a "Clock" agent), which improves performance on known apps but limits zero-shot capability on arbitrary new apps without memory updates.

- **Failure signatures:**
  - **Infinite Loops:** StaffAgent retries exceed $N_{step}$ (20) without FINISH.
  - **Graph Disconnect:** StewardAgent fails to link dependencies in $SG$, leading to missing information transfer (e.g., Alarm set for wrong time because flight info wasn't passed).
  - **App Rate Drop:** StewardAgent schedules the wrong app entirely (e.g., opens Maps instead of Calendar).

- **First 3 experiments:**
  1. **Sanity Check (SAPBench):** Run MobileSteward on 10 single-app instructions to ensure StaffAgents can handle basic UI interaction without Steward overhead breaking them.
  2. **Ablation on Information Flow:** Run a 3-app instruction (e.g., "Find a contact, send them an email, and create an event") with the *Adjusted Evaluation* disabled. Verify if information (the contact/email content) is lost between steps.
  3. **Self-Evolution Speed:** Run a batch of 50 tasks, measure Success Rate at intervals (e.g., every 10 tasks) to visualize the slope of improvement as $M_G$ and $M_E$ populate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Memory-based Self-evolution mechanism be adapted to learn from failed executions rather than updating only upon successful task completion?
- **Basis in paper:** [inferred] Section 3.7 states the mechanism "summarizes the experience from successful execution" and updates memory only if `e_j == SUCCESS`.
- **Why unresolved:** The current design explicitly ignores negative feedback, limiting the agent's ability to avoid repeating scheduling or execution errors in future attempts.
- **What evidence would resolve it:** An ablation study demonstrating performance improvements when the memory is updated with reflection tips derived from failed executions.

### Open Question 2
- **Question:** Can the Assigned Execution module maintain effectiveness in environments where the XML layout is unavailable or obfuscated, relying solely on visual perception?
- **Basis in paper:** [inferred] Section 3.5 describes Assigned Execution relying on extracting interactive widgets and textual content from the "XML layout file" to feed into the StaffAgent.
- **Why unresolved:** The method assumes the availability of the XML hierarchy; however, real-world secure apps may restrict accessibility services, causing the extraction pipeline to fail.
- **What evidence would resolve it:** Evaluation of StaffAgent performance on apps with disabled accessibility services (XML unavailable) using only screenshot inputs.

### Open Question 3
- **Question:** How can the Dynamic Recruitment module be optimized for smaller open-source models to reduce reliance on proprietary models like GPT-4o?
- **Basis in paper:** [explicit] Table 5 displays a performance gap where the leading open-source model (InternVL2 40B) significantly underperforms (0.50 vs 0.72 on 2-App tasks) compared to GPT-4o.
- **Why unresolved:** The complexity of decomposing instructions and managing information flow currently requires the advanced reasoning capabilities of large, closed-source models.
- **What evidence would resolve it:** Implementation of a distilled or fine-tuned lightweight model that achieves comparable Task Rates to GPT-4o on the CAPBench.

## Limitations
- The DAG-based task decomposition may struggle with tasks requiring tight, repeated back-and-forth between apps.
- The StewardAgent's reliability in extracting critical result information for downstream tasks is crucial and a potential failure point.
- Performance is evaluated on a specific emulator setup (Pixel 8 Pro, API 34) and a curated set of 14 apps, limiting generalizability to diverse real-world device configurations.

## Confidence
- **High:** The core multi-agent architecture (Steward/StaffAgent roles) is well-defined and its basic functionality is sound.
- **Medium:** The effectiveness of the DAG-based scheduling and Adjusted Evaluation for information transfer between apps is demonstrated, but its limits in highly interleaved tasks are not fully explored.
- **Medium:** The Memory-based Self-Evolution mechanism shows promise, but the long-term stability and the quality of the "learned" expertise/guidelines are not yet proven.

## Next Checks
1. **Stress Test DAG Scheduling:** Design a task requiring tight, repeated back-and-forth between two apps (e.g., "Copy text from App A, paste to App B, copy the result back to App A, repeat 3 times"). Verify if the current static DAG model can handle this without significant performance degradation.
2. **Evaluate Information Extraction Accuracy:** After a StaffAgent completes a task, manually inspect the StewardAgent's extracted "result information" ($r_j$) for a sample of executions. Quantify the error rate in this critical step to assess the Adjusted Evaluation's reliability.
3. **Analyze Self-Evolution Stability:** Run a long sequence of tasks (e.g., 100+) and track not just the success rate, but also the nature of the "guidelines" and "expertise" being learned. Identify if the system converges on robust strategies or if it sometimes learns and reinforces incorrect behaviors from "lucky" successes.