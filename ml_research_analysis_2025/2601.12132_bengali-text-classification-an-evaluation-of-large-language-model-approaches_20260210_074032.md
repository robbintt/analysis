---
ver: rpa2
title: 'Bengali Text Classification: An Evaluation of Large Language Model Approaches'
arxiv_id: '2601.12132'
source_url: https://arxiv.org/abs/2601.12132
tags:
- classification
- text
- bengali
- language
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated three large language models (LLMs)\u2014\
  Qwen 2.5 7B, LLaMA 3.1 8B, and LLaMA 3.2 3B\u2014for Bengali text classification\
  \ using a dataset of over 400,000 newspaper articles. Models were fine-tuned using\
  \ LoRA and QLoRA techniques, with a balanced subset of nine categories."
---

# Bengali Text Classification: An Evaluation of Large Language Model Approaches

## Quick Facts
- arXiv ID: 2601.12132
- Source URL: https://arxiv.org/abs/2601.12132
- Authors: Md Mahmudul Hoque; Md Mehedi Hassain; Md Hojaifa Tanvir; Rahul Nandy
- Reference count: 31
- Primary result: Qwen 2.5 7B achieved 72% accuracy, outperforming LLaMA 3.1 8B (53%) and LLaMA 3.2 3B (56%) on Bengali text classification

## Executive Summary
This study evaluates three large language models for Bengali text classification using a dataset of over 400,000 Prothom Alo newspaper articles. The authors fine-tuned Qwen 2.5 7B, LLaMA 3.1 8B, and LLaMA 3.2 3B using LoRA and QLoRA techniques with a balanced subset of nine categories. Qwen 2.5 achieved the highest overall accuracy at 72%, with particularly strong performance in the Sports category (81%). The results suggest that model architecture and pre-training data composition may matter more than raw parameter count for Bengali NLP tasks.

## Method Summary
The researchers used the Kaggle "Bangla newspaper dataset" containing 437,948 Prothom Alo articles, keeping only 'Content' and 'Category' columns for nine target categories. They applied Random Under Sampler to address class imbalance, split the data 80/20 for training and testing, and fine-tuned three LLMs using 4-bit quantization and LoRA/QLoRA adapters on a GPU P100. The models were evaluated using accuracy, precision, recall, F1-score, and confusion matrices.

## Key Results
- Qwen 2.5 7B achieved highest overall accuracy at 72%
- LLaMA 3.1 8B achieved 53% accuracy
- LLaMA 3.2 3B achieved 56% accuracy, outperforming its larger counterpart
- Qwen 2.5 showed particular strength in Sports category (81% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Model performance on Bengali text correlates more strongly with architecture suitability and pre-training data composition than raw parameter count. The study observed that Qwen 2.5 (7B) outperformed LLaMA 3.1 (8B), and the smaller LLaMA 3.2 (3B) outperformed the larger LLaMA 3.1 (8B). This suggests that the "instruct" variants and specific transformer architectures (Qwen uses RoPE, SwiGLU, RMSNorm) may align better with Bengali morphology or were trained on more relevant multilingual corpora than the LLaMA series, which often prioritizes English.

### Mechanism 2
Classification accuracy depends on the "semantic distinctness" of the category; domains with specialized, consistent terminology outperform those with subjective or overlapping vocabulary. The Qwen model achieved 81% accuracy on "Sports" (likely containing distinct entity names and action verbs) but only 59% on "Opinion" (containing fluid, subjective language). The confusion matrix showed misclassifications primarily between semantically adjacent categories (e.g., Economy/International).

### Mechanism 3
Parameter-efficient fine-tuning via QLoRA enables effective domain adaptation on resource-constrained hardware but introduces a performance ceiling compared to theoretical full fine-tuning. The authors utilized 4-bit quantization and Low-Rank Adaptation to fit LLMs onto a GPU P100, reducing memory usage but potentially discarding nuanced semantic information required for complex classifications.

## Foundational Learning

- **Instruction Tuning vs Base Models**: The paper uses "Instruct" variants (e.g., LLaMA 3.1 8B *Instruct*). These models are pre-trained to follow prompts, which is critical for the methodology used here where a prompt template is injected. If you fed a raw prompt to a "Base" (non-instruct) version of LLaMA, what would it likely do instead of classifying the text? (Answer: It would likely try to continue the text generation rather than outputting a label).

- **Quantization (4-bit/8-bit)**: The study runs on a GPU P100 (older hardware). Understanding that quantization converts 16/32-bit weights to 4-bit integers is necessary to explain how 7B/8B parameter models fit into limited VRAM. Does converting a model from FP16 to 4-bit increase or decrease the model's memory footprint, and what is the trade-off?

- **Class Imbalance & Under-sampling**: The dataset had 400,000 samples but was heavily imbalanced. The paper uses Random Under Sampler. Understanding this is key to interpreting why the model might struggle with minority classes despite high overall accuracy. Why might random under-sampling be dangerous for a "Minority Class" with very few samples? (Answer: You might remove the only representative examples of that class).

## Architecture Onboarding

- **Component map**: Kaggle Dataset (Prothom Alo) -> Pandas DataFrame -> Duplicate Removal -> Random Under Sampler (balances 9 categories) -> AutoTokenizer (Text to Token IDs) -> LoRA/QLoRA config (targets specific modules, e.g., `q_proj`, `v_proj`) added on top of the frozen base model -> 4-bit Quantized Base Model (LLaMA/Qwen) -> Gradient Checkpointing (saves RAM) -> Weights & Biases logging -> Confusion Matrix visualization

- **Critical path**: The prompt engineering step is the most critical failure point. The prompt must strictly define the 9 classes (Bangladesh, Sports, Technology, etc.). If the tokenizer truncates the prompt or the article content (Context Window limit), the classification logic breaks.

- **Design tradeoffs**: Chose LoRA over Full Fine-Tuning for computational feasibility (P100 GPU) at the cost of potential maximum accuracy (lower F1 scores on subjective classes). Chose Under-sampling over Over-sampling to prevent the model from biasing toward the majority class, at the cost of discarding valuable training data from the raw 400k dataset.

- **Failure signatures**: High false positives between "Economy" and "International" (e.g., an article about global oil prices). If the model outputs a category not in the list of 9 (indicates prompt adherence failure). OOM (Out of Memory) errors if `llm_int8_enable_fp32_cpu_offload` is disabled or batch size is too high.

- **First 3 experiments**: 
  1. Run the tokenizer on a sample of Bengali text to calculate the "compression ratio" (tokens per word). High ratios indicate the model is "blind" to word roots, likely hurting LLaMA performance.
  2. Run the models *without* LoRA fine-tuning to establish a baseline. This quantifies exactly how much the fine-tuning contributed vs. the pre-existing multilingual capability.
  3. Isolate the "Opinion" vs. "Lifestyle" misclassifications. Read 10-20 specific articles to determine if the labels themselves are ambiguous (data quality issue) or if the model lacks semantic nuance.

## Open Questions the Paper Calls Out

- Can a broader evaluation of state-of-the-art LLMs reveal architectures superior to Qwen 2.5 for low-resource Bengali text classification? The authors state in the Conclusion and Abstract that future research will "explore additional models" and "a broader range of LLMs" to enhance classification accuracy.

- How does expanding the dataset beyond a single newspaper source and employing advanced resampling techniques affect classification performance? The Conclusion notes that the dataset will be "expanded by incorporating additional sources" and that future work will focus on "addressing this imbalance," as the current Random Under Sampler approach may have deleted useful data.

- Does model architecture influence Bengali text classification performance more significantly than parameter count? The Discussion notes that the smaller LLaMA 3.2 3B outperformed the larger LLaMA 3.1 8B (56% vs 53%), leading the authors to suggest that "architecture may matter more than size."

## Limitations

- The study relies on a single newspaper source (Prothom Alo) and applies Random Under Sampler, potentially discarding valuable data from the raw 400k dataset
- Critical hyperparameters (LoRA rank, learning rate, epochs, batch size) are unspecified, making it impossible to determine if performance differences are due to architectural advantages or hyperparameter tuning
- The study reports only overall accuracy without confidence intervals or statistical significance testing, using a single 80/20 train-test split with no cross-validation

## Confidence

- **High Confidence**: The claim that "model architecture and parameter size influence performance" is well-supported by the direct comparison showing Qwen 2.5 (7B) outperforming LLaMA 3.1 (8B) and LLaMA 3.2 (3B) outperforming LLaMA 3.1 (8B).
- **Medium Confidence**: The claim that "Qwen 2.5 shows particular strength in the 'Sports' category" is supported by the 81% accuracy figure, but without knowing the dataset size per class post-balancing, we cannot rule out that Sports simply had more samples or cleaner annotation.
- **Low Confidence**: The broader claim that "LLMs can effectively classify news content despite limited Bengali NLP resources" is questionable given the modest 72% accuracy ceiling and the lack of comparison to traditional ML baselines.

## Next Checks

1. **Tokenizer Analysis Validation**: Run the HuggingFace tokenizers for LLaMA and Qwen on 1,000 random Bengali articles to calculate average tokens per word. Compare compression ratios to determine if the performance gap correlates with tokenization quality rather than model capacity. A high compression ratio (many tokens per word) for LLaMA would confirm that architectural differences in tokenization, not model size, drive the performance difference.

2. **Zero-Shot Baseline Experiment**: Evaluate all three models on the test set without any fine-tuning to establish baseline multilingual capabilities. This will quantify exactly how much of the 72% accuracy comes from the pre-training versus the fine-tuning process. If zero-shot performance is already 60-65%, the LoRA fine-tuning contribution is minimal.

3. **Statistical Significance Testing**: Re-run the experiments with 5-fold cross-validation and report mean accuracy with 95% confidence intervals. Perform paired t-tests between model pairs (Qwen vs LLaMA 3.1, LLaMA 3.2 vs LLaMA 3.1) to determine if the performance differences are statistically significant or within expected variance ranges.