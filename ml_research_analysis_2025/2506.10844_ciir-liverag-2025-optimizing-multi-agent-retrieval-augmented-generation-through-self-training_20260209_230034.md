---
ver: rpa2
title: 'CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through
  Self-Training'
arxiv_id: '2506.10844'
source_url: https://arxiv.org/abs/2506.10844
tags:
- information
- question
- agent
- response
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mRAG, a multi-agent retrieval-augmented generation
  framework that employs specialized agents for planning, searching, reasoning, and
  coordination. The system uses self-training with reward-guided trajectory sampling
  to optimize inter-agent collaboration and enhance response generation.
---

# CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training

## Quick Facts
- arXiv ID: 2506.10844
- Source URL: https://arxiv.org/abs/2506.10844
- Reference count: 40
- Primary result: mRAG system achieves 7th place among 20 teams in SIGIR 2025 LiveRAG competition

## Executive Summary
This paper introduces mRAG, a multi-agent retrieval-augmented generation framework that employs specialized agents for planning, searching, reasoning, and coordination. The system uses self-training with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines and demonstrates the effectiveness of specialized agent architectures in complex information retrieval tasks.

## Method Summary
mRAG implements a multi-agent system where each agent has a distinct role: planning agents decompose complex questions into manageable sub-tasks, search agents perform targeted information retrieval, reasoning agents synthesize evidence, and coordination agents manage inter-agent communication. The system employs self-training with reward-guided trajectory sampling to optimize agent behaviors and collaboration patterns. This approach allows the system to learn from its own outputs and refine its strategies based on performance feedback, creating a continuous improvement loop that enhances overall effectiveness.

## Key Results
- Achieved 7th place ranking among 20 competing teams in the SIGIR 2025 LiveRAG competition
- Demonstrated superior performance compared to conventional RAG baselines
- Successfully decomposed complex questions, performed adaptive searches, and integrated multi-aspect evidence into coherent responses

## Why This Works (Mechanism)
The effectiveness of mRAG stems from its specialized agent architecture that mirrors human problem-solving approaches. By decomposing complex tasks into planning, searching, reasoning, and coordination phases, the system can tackle information retrieval challenges more systematically than monolithic approaches. The self-training mechanism with reward-guided trajectory sampling enables continuous optimization of agent behaviors based on actual performance outcomes, allowing the system to learn from its successes and failures. This creates a dynamic learning environment where agent collaboration patterns are refined over time, leading to improved response quality and more effective information synthesis across multiple sources.

## Foundational Learning

**Multi-agent systems**: Distributed architectures where specialized components collaborate to solve complex problems - needed to enable task decomposition and parallel processing, quick check: verify agent independence and communication protocols.

**Retrieval-augmented generation**: Combining information retrieval with language generation to produce more accurate responses - needed to ground generated content in factual evidence, quick check: validate retrieval quality and integration with generation.

**Self-training with reinforcement learning**: Using system outputs as training data with performance-based rewards - needed to enable continuous improvement without external supervision, quick check: monitor reward signal stability and learning convergence.

**Question decomposition**: Breaking complex queries into simpler sub-questions - needed to enable focused retrieval and more accurate responses, quick check: evaluate decomposition quality and coverage of original intent.

**Reward-guided trajectory sampling**: Selecting and reinforcing successful execution paths - needed to optimize agent collaboration patterns, quick check: analyze trajectory diversity and reward distribution.

## Architecture Onboarding

**Component map**: User Query -> Planning Agent -> Search Agent -> Reasoning Agent -> Coordination Agent -> Final Response, with self-training feedback loop from Response to all agents.

**Critical path**: Query → Planning → Search → Reasoning → Coordination → Response, where each stage must complete successfully for high-quality output.

**Design tradeoffs**: Specialized agents provide better task decomposition but increase system complexity and communication overhead; self-training enables adaptation but requires careful reward design to avoid reward hacking.

**Failure signatures**: Poor planning leads to irrelevant searches; inadequate coordination causes inconsistent reasoning; suboptimal reward signals result in learning stagnation or collapse.

**First experiments**: 1) Test individual agent performance in isolation, 2) Evaluate end-to-end system on simple queries, 3) Measure self-training effectiveness on controlled datasets with known ground truth.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to DataMorgana-derived datasets, raising generalizability concerns
- 7th place ranking indicates room for improvement and suggests non-state-of-the-art performance
- Lacks detailed quantitative metrics and ablation studies to identify most impactful components

## Confidence

**High confidence**: The specialized agent architecture is technically sound and well-defined with clear roles for planning, searching, reasoning, and coordination.

**Medium confidence**: Self-training effectiveness is supported by competition results but lacks detailed validation and statistical significance testing.

**Medium confidence**: Baseline comparisons are reasonable but limited in scope and depth, lacking comprehensive quantitative analysis.

## Next Checks
1. Conduct extensive ablation studies to quantify the contribution of each agent type and the self-training mechanism to overall performance.
2. Test the system on diverse real-world datasets beyond DataMorgana to assess generalizability across different domains and query types.
3. Perform statistical significance testing between mRAG and baseline approaches to verify that performance improvements are not due to random variation.