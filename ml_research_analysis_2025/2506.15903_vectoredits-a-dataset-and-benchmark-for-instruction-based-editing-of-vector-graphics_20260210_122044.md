---
ver: rpa2
title: 'VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector
  Graphics'
arxiv_id: '2506.15903'
source_url: https://arxiv.org/abs/2506.15903
tags:
- image
- vector
- editing
- dataset
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VectorEdits, a large-scale dataset for instruction-based
  vector graphics editing consisting of 271,306 SVG image pairs with natural language
  edit instructions. The dataset was created by sampling image pairs within SVG collections
  using CLIP similarity and generating instructions using vision-language models.
---

# VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics

## Quick Facts
- arXiv ID: 2506.15903
- Source URL: https://arxiv.org/abs/2506.15903
- Reference count: 3
- This paper introduces VectorEdits, a large-scale dataset for instruction-based vector graphics editing consisting of 271,306 SVG image pairs with natural language edit instructions.

## Executive Summary
This paper introduces VectorEdits, a large-scale dataset for instruction-based vector graphics editing consisting of 271,306 SVG image pairs with natural language edit instructions. The dataset was created by sampling image pairs within SVG collections using CLIP similarity and generating instructions using vision-language models. Initial experiments with state-of-the-art LLMs show that current models struggle with this task, with all tested models failing to outperform a simple baseline of leaving the original image unedited. The best-performing model (DeepSeek V3 0324) achieved a CLIP score of 0.9203 and DINOv2 similarity of 0.8444, but still underperformed compared to the no-edit baseline. The dataset includes training (269,106 pairs), validation (200 pairs), and test (2,000 pairs) splits, and is publicly available to support research in language-driven vector manipulation.

## Method Summary
VectorEdits provides a benchmark for instruction-based editing of vector graphics by pairing source and target SVG images with natural language edit instructions. The dataset was constructed by first sampling SVG image pairs from collections using CLIP-based similarity to identify visually similar images, then generating edit instructions using vision-language models (VLMs). The evaluation protocol involves rasterizing SVGs to 512×512 images and computing semantic similarity metrics (CLIP score, DINOv2 similarity) as well as pixel-level MSE. Initial experiments tested state-of-the-art LLMs by providing source SVG and instruction as prompts to generate edited SVGs, revealing that all models fail to outperform a no-edit baseline, indicating significant challenges in combining visual understanding with code generation for vector graphics.

## Key Results
- VectorEdits dataset contains 271,306 SVG triplets (source SVG, target SVG, instruction) with train/val/test splits of 269,106/200/2,000 pairs
- All tested state-of-the-art LLMs fail to outperform the "no-edit" baseline (return source SVG unchanged)
- Best-performing model (DeepSeek V3 0324) achieves CLIP score 0.9203 and DINOv2 similarity 0.8444, both below no-edit baseline (CLIP 0.9634, DINOv2 0.9011)
- Model outputs frequently contain invalid SVG syntax, contributing to poor performance

## Why This Works (Mechanism)
None

## Foundational Learning

### Vector Graphics Representation
- Why needed: SVGs are structured XML code describing shapes, paths, and styles rather than pixel data
- Quick check: Can you identify basic SVG elements like <rect>, <circle>, <path> in sample code?

### Vision-Language Models for Code Generation
- Why needed: VLMs must understand visual differences and translate them into structured SVG code
- Quick check: Does the VLM maintain syntactic correctness while modifying visual elements?

### Semantic Similarity Metrics
- Why needed: CLIP and DINOv2 provide semantic understanding beyond pixel-level MSE
- Quick check: Can you explain why semantic metrics are preferred over pixel MSE for this task?

## Architecture Onboarding

### Component Map
Source SVG + Instruction -> LLM -> Generated SVG -> Rasterizer -> CLIP/DINOv2 Similarity vs Target SVG

### Critical Path
LLM evaluation: Source SVG + Instruction prompt -> Generated SVG -> Rasterization -> Metric computation

### Design Tradeoffs
- Using VLMs for instruction generation vs human annotation: scalability vs instruction quality
- Semantic metrics (CLIP/DINOv2) vs pixel MSE: better captures visual similarity vs simpler computation
- Large dataset size vs potential noise: 271K pairs provides coverage but includes VLM-generated instruction noise

### Failure Signatures
- Invalid SVG syntax from LLM outputs
- No-edit baseline outperforming models (models failing to follow instructions)
- MSE improvement with semantic metric degradation

### First 3 Experiments
1. Replicate no-edit baseline evaluation on test set to verify CLIP 0.9634 and DINOv2 0.9011 scores
2. Run DeepSeek V3 0324 evaluation pipeline and compare results to reported metrics
3. Analyze invalid SVG output rate across all tested models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized models be developed that successfully integrate visual understanding and code generation to outperform the "no-edit" baseline on this benchmark?
- Basis in paper: The Results section shows that all tested state-of-the-art models failed to outperform the "Baseline – no edit," frequently producing malformed or incorrect SVG outputs.
- Why unresolved: The paper establishes the benchmark and the failure of current LLMs but does not propose specific architectural solutions to bridge the gap between language instructions and structural code manipulation.
- What evidence would resolve it: A model achieving a CLIP score and DINOv2 similarity significantly higher than 0.9634 and 0.9011 respectively (the no-edit baseline scores).

### Open Question 2
- Question: How can the dataset curation process be refined to prevent instructions from containing excessive detail that allows models to generate the target from scratch?
- Basis in paper: The authors state in the Limitations section that "some instructions may contain excessive detail, effectively allowing a model to generate the 'edited' SVG from scratch without relying on the source image."
- Why unresolved: The instructions were generated automatically by VLMs which may describe the entire target scene rather than the minimal edit required, potentially biasing the task toward generation rather than editing.
- What evidence would resolve it: A comparative evaluation showing that models perform worse when denied access to the source image, proving they rely on the input rather than just the instruction text.

### Open Question 3
- Question: To what extent does the noise introduced by automatic VLM labeling affect the reliability of the dataset for training and evaluation?
- Basis in paper: The Limitations section notes that "automatic generation of edit instructions using vision-language models may occasionally introduce noise and inconsistencies, as these models are not perfect."
- Why unresolved: While the authors manually ranked models for the labeling task, they did not quantify the error rate of the final chosen labeler (Qwen2.5-VL 70B) across the full dataset.
- What evidence would resolve it: A human evaluation of a statistically significant random sample from the training set to determine the percentage of instructions that accurately describe the visual transformation.

## Limitations
- Models fail to outperform the no-edit baseline, indicating significant challenges in instruction-following and SVG generation
- Some instructions contain excessive detail that allows models to generate targets from scratch rather than editing
- Automatic VLM-generated instructions may introduce noise and inconsistencies affecting dataset reliability

## Confidence

- **High confidence**: Dataset statistics (271,306 total pairs, 269,106/200/2,000 train/val/test splits) and public availability at the stated HuggingFace location
- **Medium confidence**: The no-edit baseline results (CLIP 0.9634, DINOv2 0.9011, MSE 10488) and DeepSeek V3 0324 results (CLIP 0.9203, DINOv2 0.8444), as these are directly reported and reproducible given the evaluation code
- **Low confidence**: The claim that "current models fail to follow instructions" as the primary cause of poor performance, since the paper does not distinguish between instruction-following failures, SVG generation failures, or inherent task difficulty

## Next Checks

1. Replicate the no-edit baseline evaluation on the test set to verify reported CLIP 0.9634 and DINOv2 0.9011 scores, ensuring the evaluation pipeline is functioning correctly
2. Run the same LLM evaluation pipeline (source SVG + instruction prompt to DeepSeek V3 0324) and compare results to reported CLIP 0.9203 and DINOv2 0.8444, checking for consistency
3. Analyze the edit magnitude and semantic similarity gap between no-edit baseline and model outputs across the test set to determine if failures are due to instruction non-compliance, SVG invalidity, or inherent task difficulty