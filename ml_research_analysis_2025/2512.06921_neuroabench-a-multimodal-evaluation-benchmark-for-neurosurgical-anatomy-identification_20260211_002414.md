---
ver: rpa2
title: 'NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification'
arxiv_id: '2512.06921'
source_url: https://arxiv.org/abs/2512.06921
tags:
- anatomical
- neurosurgical
- mllms
- surgical
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeuroABench is the first benchmark for evaluating anatomical understanding
  in neurosurgical videos, addressing the gap between surgical workflow-focused datasets
  and the need for fine-grained anatomical comprehension. It uses a multi-stage annotation
  pipeline with clinician-reviewed teaching manuals and expert validation to generate
  1,079 question-answer pairs covering 68 anatomical structures across 9 hours of
  videos from 89 neurosurgical procedures.
---

# NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification

## Quick Facts
- arXiv ID: 2512.06921
- Source URL: https://arxiv.org/abs/2512.06921
- Reference count: 33
- Primary result: First benchmark for evaluating anatomical understanding in neurosurgical videos; best MLLM achieves 40.87% accuracy vs. human trainees at 46.5%

## Executive Summary
NeuroABench introduces the first benchmark specifically designed to evaluate anatomical understanding in neurosurgical videos. Unlike existing surgical video datasets that focus on workflow comprehension, NeuroABench targets fine-grained anatomical identification through 1,079 question-answer pairs covering 68 structures across 9 hours of neurosurgical procedures. The benchmark employs a rigorous multi-stage annotation pipeline combining LLM-assisted processing with expert clinician validation. Evaluation of state-of-the-art MLLMs reveals significant limitations, with the top-performing model (Gemini-2.0-Flash) achieving only 40.87% accuracy, highlighting the substantial gap between current AI systems and human performance in surgical anatomy identification.

## Method Summary
NeuroABench uses a three-stage pipeline: (1) curate 89 high-quality neurosurgical videos and 32 teaching manuals from the Neurosurgical Atlas, (2) process manuals with OpenAI-o1 to create structured procedural workflows, then use Gemini-1.5-Pro to annotate videos with anatomy labels and timestamps in JSON format, and (3) conduct expert validation through multi-round examination to refine frame-anatomy pairs. The final benchmark contains 1,079 multiple-choice QA pairs (5 options each) with balanced answer distribution. Evaluation follows a zero-shot paradigm, measuring instance-level and anatomy-level macro-averaged precision, recall, F1-score, and accuracy across diverse MLLMs including general, proprietary, and medical-specialized models.

## Key Results
- Best MLLM (Gemini-2.0-Flash) achieves 40.87% accuracy on benchmark
- Human neurosurgical trainees score between 28% and 56%, averaging 46.5%
- Medical-specialized models show only marginal improvements (+7.52% accuracy) over general models
- Anatomical deformation from surgical manipulation significantly degrades model performance
- Top medical model (HuatuoGPT-Vision) achieves 36.52% accuracy, only slightly better than GPT-4o at 30.21%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-stage annotation pipeline with clinician-reviewed teaching manuals and expert validation generates high-quality anatomical Q&A pairs
- **Mechanism:** LLMs extract procedural workflows from manuals and annotate videos with steps, key anatomy, and timestamps. Expert neurosurgeons review, correct, and standardize terminology, then filter frames through multi-round checks to remove ambiguities
- **Core assumption:** Expert reviewers can effectively correct AI errors and source manuals accurately represent standard surgical workflows
- **Evidence anchors:** Abstract confirms multi-stage pipeline with clinician review; section details expert review and multi-round verification process; corpus evidence is weak
- **Break condition:** Inconsistent expert reviewers or outdated source manuals would propagate systematic annotation errors

### Mechanism 2
- **Claim:** Anatomical deformation significantly degrades MLLM identification accuracy as models learn primarily from static representations
- **Mechanism:** MLLMs' visual encoders rely on features from static, standard-morphology images. Surgical manipulation changes tissue shape and appearance, breaking the match between learned representations and visual input
- **Core assumption:** Current MLLMs lack sufficient training data depicting deformed anatomical structures in surgical contexts
- **Evidence anchors:** Section states models are limited to static textbook representations; Figure 3 shows Claude-3.5-Sonnet failing on deformed structures it correctly identified in exposure phase; corpus evidence is weak
- **Break condition:** Training on extensive datasets with varied surgical states and deformations would likely mitigate this performance drop

### Mechanism 3
- **Claim:** Medical-specialized MLLMs show only marginal improvements due to lack of relevant surgical context in training data
- **Mechanism:** Medical MLLMs are fine-tuned on general medical corpora (radiology, textbooks) that don't capture surgical video complexities (variable lighting, tool occlusion, bleeding). Data mismatch prevents domain-specific fine-tuning from translating to surgical anatomy gains
- **Core assumption:** Data relevance, not model architecture or scale, is the primary bottleneck
- **Evidence anchors:** Section identifies deficiency in medical training data; Table II shows medical model (HuatuoGPT-Vision, 36.52%) only slightly better than GPT-4o (30.21%) and worse than best general model (Gemini-2.0-Flash, 40.87%); corpus consistent with Anatomy-R1
- **Break condition:** A medical model trained on large surgical video corpus failing to outperform general models would disprove this

## Foundational Learning

- **Concept:** Multi-Modal Large Language Models (MLLMs)
  - **Why needed here:** MLLMs are the evaluation subject. Understanding they process both visual and textual information is essential for interpreting benchmark design and results
  - **Quick check question:** How does an MLLM's input differ from a standard LLM's input?

- **Concept:** Zero-Shot Evaluation
  - **Why needed here:** All models evaluated without prior examples or fine-tuning on specific dataset, testing inherent generalized ability
  - **Quick check question:** What does "zero-shot" mean in context of evaluating a model on a benchmark?

- **Concept:** Instance-Level vs. Anatomy-Level Metrics
  - **Why needed here:** Paper reports performance at two granularities. Instance-level averages across all pairs, anatomy-level averages across specific structures, helping identify model biases toward frequent structures
  - **Quick check question:** If a model performs well at instance-level but poorly at anatomy-level, what might that indicate about its predictions?

## Architecture Onboarding

- **Component Map:** 886 Neurosurgical Atlas videos → filtered to 89 high-quality videos → 32 teaching manuals → OpenAI-o1 processes manuals → structured procedural workflows → Gemini-1.5-Pro annotates videos → JSON output (step, anatomy, timestamp) → expert review and standardization → final QA pairs generated

- **Critical Path:** Quality and validity of entire benchmark hinge on expert review and validation steps. Errors escaping human review create flawed evaluation data

- **Design Tradeoffs:**
  - Scalability vs. Quality: AI scales annotation but human expert review ensures quality
  - Generalizability vs. Specificity: Highly specific to neurosurgical anatomy, deep evaluation in one domain but not general surgical workflow understanding

- **Failure Signatures:**
  - Ambiguous Frames: Removed by experts due to multiple landmark anatomies, could have introduced noisy labels
  - Data Leakage: Addressed by creating new dataset from primary sources rather than re-labeling existing ones
  - Static vs. Dynamic Failure: Models failing on frames showing deformed or manipulated anatomy

- **First 3 Experiments:**
  1. Baseline Evaluation: Run zero-shot inference on full NeuroABench using diverse MLLMs to establish baseline accuracy and identify performance gap
  2. Human Baseline: Have neurosurgical trainees evaluate subset to create human performance baseline for direct MLLM comparison
  3. Error Analysis: For subset of failures, especially involving deformed anatomy, have MLLMs generate free-form reasoning and manually analyze outputs to pinpoint failure causes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLMs be trained or fine-tuned to robustly identify anatomical structures deformed during surgical procedures?
- **Basis in paper:** [explicit] Paper states current MLLMs possess understanding limited to static textbook representations rather than dynamic surgical presentations. Figure 3 demonstrates Claude-3.5-Sonnet correctly identifying structures in exposure phase but failing when anatomy is actively manipulated
- **Why unresolved:** Medical-specific MLLMs achieve only modest improvements (+7.52% accuracy), suggesting existing medical training data doesn't adequately capture dynamic surgical anatomy
- **What evidence would resolve it:** Study comparing MLLM performance before and after training on datasets explicitly labeled with deformation states; or experiments showing improved accuracy on deformed vs. non-deformed structures after targeted training

### Open Question 2
- **Question:** What specific characteristics of medical training data would enable MLLMs to achieve clinically meaningful surgical anatomy comprehension?
- **Basis in paper:** [explicit] Paper concludes current medical special models neglect clinical surgical anatomy comprehension when training with medical data. Medical-specialized models achieve ceiling accuracy of only 36.52%, indicating current approaches insufficient
- **Why unresolved:** Despite availability of medical VQA datasets, models trained on them still underperform significantly. Paper doesn't identify which data modalities, annotation types, or curriculum strategies would address this deficiency
- **What evidence would resolve it:** Ablation studies isolating specific training data characteristics (intraoperative video vs. static imaging, temporal annotations, multi-view data) showing which factors most improve surgical anatomy identification performance

### Open Question 3
- **Question:** How representative is trainee-level human performance as benchmark target compared to expert surgeon performance?
- **Basis in paper:** [inferred] Human evaluation includes only four neurosurgical trainees with high variance (28-56%, mean 46.5%). Paper compares MLLMs to this group, but expert surgeon performance on same benchmark not reported
- **Why unresolved:** Trainees are still developing expertise, so their performance may not represent gold standard for surgical anatomy identification. Wide variance among trainees (28 percentage points) suggests sample may not provide stable reference point
- **What evidence would resolve it:** Follow-up study evaluating expert neurosurgeons (attending physicians with 10+ years experience) on same NeuroABench subset to establish more meaningful upper bound for target performance

## Limitations

- Expert review process lacks detailed documentation of inter-rater reliability or systematic bias assessment
- Performance gap between models and humans could partially reflect annotation inconsistencies rather than purely model limitations
- Evaluation uses relatively small dataset of 1,079 QA pairs that may not fully capture real surgical scenario complexity

## Confidence

- Multi-stage annotation pipeline effectiveness: Medium
- Anatomical deformation as primary failure mode: Medium-High
- Medical model underperformance relative to general models: Medium
- Benchmark represents real surgical complexity: Low-Medium

## Next Checks

1. Conduct inter-rater reliability analysis on subset of annotated frames to quantify expert agreement and identify systematic biases
2. Perform controlled experiments isolating specific visual challenges (lighting, occlusion, deformation) to determine their relative impact on model performance
3. Expand evaluation to include models fine-tuned on surgical video datasets to test whether domain-specific training closes performance gap