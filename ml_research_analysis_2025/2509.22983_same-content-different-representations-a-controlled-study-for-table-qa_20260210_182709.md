---
ver: rpa2
title: 'Same Content, Different Representations: A Controlled Study for Table QA'
arxiv_id: '2509.22983'
source_url: https://arxiv.org/abs/2509.22983
tags:
- table
- tables
- structured
- semi-structured
- column
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first controlled study isolating the
  impact of table representation (structured vs. semi-structured) on Table QA performance.
---

# Same Content, Different Representations: A Controlled Study for Table QA

## Quick Facts
- **arXiv ID**: 2509.22983
- **Source URL**: https://arxiv.org/abs/2509.22983
- **Reference count**: 40
- **Primary result**: Representation (structured vs. semi-structured) is a dominant factor in Table QA performance, with NL2SQL dropping 30-45% on semi-structured tables while LLMs show <5% decline.

## Executive Summary
This paper presents the first controlled study isolating the impact of table representation on Table QA performance. Using a novel verbalization pipeline, the authors generate paired structured and semi-structured tables with identical content, enabling systematic comparison across three major paradigms: NL2SQL, LLM-based, and hybrid methods. They construct RePairTQA, a diagnostic benchmark with controlled splits along table size, joins, query complexity, and schema quality. The results reveal that representation is a dominant factor: NL2SQL excels on structured inputs (69.14% accuracy) but drops sharply on semi-structured ones, while LLMs are more robust (only 3.5% decline) but less precise. Hybrid methods strike a balance, especially under noisy schemas, but no single paradigm excels across all conditions.

## Method Summary
The study uses a verbalization pipeline to convert structured tables into semi-structured ones while preserving content. The pipeline uses GPT-4o to select verbalizable columns and generate natural language templates that embed column values as free text. This creates paired datasets with identical semantic content but different representations. The RePairTQA benchmark, constructed from BIRD, MMQA, and TableEval, contains 1,628 samples across 7 diagnostic splits testing different dimensions. Seven model paradigms are evaluated: GPT-4o, Gemini-2.5-flash, Qwen3 (LLM-based), LLM-NL2SQL and XiYAN (NL2SQL), and H-STAR and Weaver (hybrid). Evaluation uses an LLM-as-judge (GPT-4o) for semantic equivalence with 96% human agreement.

## Key Results
- Representation is a dominant factor: NL2SQL accuracy drops 30-45% on semi-structured tables vs. structured (69.14% → ~25-39%), while LLMs show <5% decline
- LLMs are more robust to representation changes but less precise overall compared to NL2SQL on structured inputs
- Hybrid methods balance precision and robustness, particularly effective under noisy schemas
- Performance trade-offs intensify with larger tables and complex queries
- No single paradigm excels across all conditions (table size, joins, query complexity, schema quality)

## Why This Works (Mechanism)
The verbalization pipeline enables controlled comparison by creating paired datasets where only the representation differs while content remains identical. This isolation reveals that the dramatic performance differences across paradigms are fundamentally due to how each paradigm processes structured vs. semi-structured information. NL2SQL methods rely on explicit schema elements for SQL generation, making them brittle when attributes are verbalized into text cells. LLMs process both representations more uniformly through their language modeling capabilities, though they sacrifice precision for robustness. Hybrid methods attempt to leverage both approaches but inherit limitations when SQL components require explicit schema.

## Foundational Learning

**Table QA Paradigms**
- Why needed: Understanding the three core approaches (NL2SQL, LLM-based, Hybrid) and their fundamental assumptions is prerequisite to interpreting the controlled comparison and resulting trade-offs
- Quick check: Can you name one key strength and one key weakness of NL2SQL methods compared to LLM-based methods for Table QA?

**Verbalization / Linearization**
- Why needed: The paper's core contribution is a verbalization pipeline to generate controlled semi-structured tables from structured ones. Understanding this transformation is key to grasping the experimental design
- Quick check: How does the verbalization pipeline allow the authors to isolate the effect of "representation" from "content"?

**Schema Linking**
- Why needed: A central finding is that schema quality (and ability to link question to schema elements) is a dominant factor for NL2SQL and hybrid systems
- Quick check: Why might a question like "What is the standing of Alice?" fail on a semi-structured table where 'standing' is embedded in a free-text remarks column?

## Architecture Onboarding

**Component map:**
RePairTQA benchmark (BIRD, MMQA, TableEval sources) → Verbalization Pipeline (GPT-4o for column selection and template generation) → Model Paradigms (LLMs: GPT-4o, Gemini-2.5-flash, Qwen3; NL2SQL: LLM-NL2SQL, XiYAN; Hybrids: H-STAR, Weaver) → LLM-as-judge evaluation (GPT-4o)

**Critical path:**
The most important workflow for a new engineer is to run evaluation on a diagnostic split:
1. Load data for a split (e.g., S1: short tables, lookup queries)
2. Generate predictions using a model from each paradigm (e.g., LLM-NL2SQL, GPT-4o, Weaver)
3. Run the evaluation script to compare predictions against gold answers using the LLM-judge
4. Calculate and compare accuracy. This directly demonstrates the core finding: performance varies dramatically across representations for different paradigms.

**Design tradeoffs:**
- **Verbalization Quality vs. Control**: Uses GPT-4o for template generation, introducing potential LLM bias but enabling scalable creation of controlled, semantically-paired data
- **Evaluation Metric**: Uses LLM-judge (vs. Exact Match) better captures semantic correctness but introduces dependency on another LLM
- **Benchmark Coverage**: Focuses on tables fitting in context windows (under ~100s of rows), does not address ultra-long tables requiring retrieval or chunking

**Failure signatures:**
- **NL2SQL on Semi-Structured Data**: Outputs SQL with missing column references or execution errors when attributes are verbalized into text cells
- **LLMs on Long Tables**: Accuracy drops precipitously (>25% in some cases), models may hallucinate or return arbitrary answers when context is overloaded
- **Hybrid Methods on Complex Multi-table Joins**: Limited support for multi-table reasoning under semi-structured conditions leads to performance degradation

**First 3 experiments:**
1. Reproduce RQ1: Run all three paradigm baselines on S3 split (structured vs. semi-structured) and verify the 30-45% drop for NL2SQL and <5% drop for LLMs/Hybrids
2. Ablate Verbalization Strategy: On a small subset, replace GPT-4o-generated templates with simple, rule-based ones (e.g., "The {col1} is {val1}"). Re-run a baseline to test pipeline's sensitivity to template quality
3. Test a New Model: Select a recent open-source LLM or new Text-to-SQL model not in the paper. Run it on "Clean vs. Noisy Schema" split (RQ5) to see if it follows same trend as existing baselines

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can Table QA systems be adapted to effectively handle "ultra-long" tables (hundreds or thousands of rows) that exceed current context windows, potentially requiring chunking or retrieval mechanisms?
- Basis: Section 2.2 states extending framework to ultra-long tables "which likely require chunking or retrieval, is an interesting direction for future work"
- Why unresolved: Current study explicitly limits scope to tables fitting within current LLM context windows
- What evidence would resolve it: Evaluation of retrieval-augmented or chunking-based Table QA methods on modified RePairTQA with thousands of rows

**Open Question 2**
- Question: What specific architectural components or internal representations allow LLMs to maintain robustness on semi-structured data while NL2SQL models degrade sharply?
- Basis: Section 6 calls for "deeper theoretical understanding of representation sensitivity" via "architectural ablations, targeted probing of intermediate representations, and instrumentation of model-internal decision pathways"
- Why unresolved: Paper establishes empirical trade-offs but does not explain underlying mechanisms or internal processing causing divergent behaviors
- What evidence would resolve it: Probing studies or ablation experiments correlating specific model layers or attention heads with successful reasoning over implicit schema information

**Open Question 3**
- Question: How can hybrid architectures be enhanced to support multi-table reasoning (joins) on semi-structured inputs where explicit keys are absent?
- Basis: Section 5.3 notes hybrid models "currently offer limited support for multi-table reasoning under semi-structured conditions, constraining their effectiveness"
- Why unresolved: Hybrids balance trade-offs for single tables but reliance on SQL components makes them brittle for joins when schemas are verbalized and keys are implicit
- What evidence would resolve it: Hybrid method utilizing semantic matching or LLM-based linking to perform joins on semi-structured multi-table splits without relying on explicit foreign keys

## Limitations
- Controlled benchmark may not fully capture real-world semi-structured table complexity and diversity
- Relies on GPT-4o for template generation, introducing potential LLM bias into controlled data
- Focuses on tables fitting within current context windows, excluding ultra-long table scenarios requiring retrieval/chunking
- LLM-as-judge evaluation introduces dependency on another model's judgment

## Confidence

**High Confidence**: The core finding that representation (structured vs. semi-structured) is a dominant factor in Table QA performance is strongly supported by systematic comparisons across 7 model paradigms and 7 diagnostic splits

**Medium Confidence**: The relative performance rankings of model paradigms (NL2SQL excelling on structured inputs, LLMs being more robust but less precise) are well-established within controlled setting but may shift with different model families or larger real-world datasets

**Medium Confidence**: The claim that schema quality is a dominant factor, particularly for NL2SQL and hybrid systems, is supported by noisy schema split results but requires further validation on tables with more diverse schema variations

## Next Checks
1. Evaluate the three paradigm baselines on a held-out set of real-world semi-structured tables (e.g., from financial reports or Wikipedia infoboxes) to test if controlled study findings hold outside benchmark
2. Extend evaluation to tables exceeding 100 rows to identify exact point where context overflow impacts LLM performance and validate observation of >25% accuracy drops on long tables
3. Test a new, recent open-source LLM or Text-to-SQL model not included in original study on "Clean vs. Noisy Schema" split to verify if it follows established trend of NL2SQL/Hybrid sensitivity to schema quality