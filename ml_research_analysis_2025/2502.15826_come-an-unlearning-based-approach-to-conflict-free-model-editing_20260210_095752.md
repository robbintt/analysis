---
ver: rpa2
title: 'CoME: An Unlearning-based Approach to Conflict-free Model Editing'
arxiv_id: '2502.15826'
source_url: https://arxiv.org/abs/2502.15826
tags:
- knowledge
- editing
- outdated
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of knowledge conflicts in large
  language model (LLM) editing, where outdated information interferes with new knowledge
  during editing. The authors propose Conflict-free Model Editing (CoME), a framework
  that leverages unlearning to selectively remove outdated knowledge while simultaneously
  integrating new information.
---

# CoME: An Unlearning-based Approach to Conflict-free Model Editing

## Quick Facts
- arXiv ID: 2502.15826
- Source URL: https://arxiv.org/abs/2502.15826
- Authors: Dahyun Jung; Jaehyung Seo; Jaewook Lee; Chanjun Park; Heuiseok Lim
- Reference count: 20
- Primary result: CoME improves editing accuracy by mitigating knowledge conflicts through targeted unlearning while preserving linguistic features

## Executive Summary
CoME addresses knowledge conflicts in LLM editing by combining targeted unlearning with knowledge updates. The framework extracts parameters storing outdated knowledge, removes these components from new knowledge updates, and restricts modifications to critical parameters. Applied to GPT-J and LLaMA-3 on Counterfact and ZsRE datasets, CoME significantly improves editing accuracy and model reliability by suppressing interference from outdated information while maintaining fluency and unrelated knowledge.

## Method Summary
CoME augments locate-then-edit methods (MEMIT, PMET) by introducing a dual-update process. For each editing task, it computes both a new knowledge update vector δ_i and an outdated knowledge update vector δ'_i. The framework extracts shared linguistic features δ''_i through vector projection, then applies restricted parameter subtraction z'_i = z_i − α(δ'_i − δ''_i) to only the top-p% of parameters by magnitude. This process removes outdated knowledge components while preserving linguistic capabilities and unrelated facts. The final weight updates are distributed across target layers using the original MEMIT/PMET covariance-based approach.

## Key Results
- CoMEPMET achieves 89.4% efficacy and 83.1% generality on GPT-J with ZsRE dataset
- Locality improves significantly compared to baselines when parameter restrictions are applied
- Fluency and Consistency metrics remain stable, demonstrating preservation of linguistic features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted parameter subtraction reduces interference from outdated knowledge during model editing.
- Mechanism: CoME computes two update vectors—δ_i for new knowledge and δ'_i for outdated knowledge—then subtracts the outdated-specific component from the new knowledge update before applying weight changes. This removes the overlapping representation that causes interference.
- Core assumption: Outdated and new knowledge occupy overlapping parameter regions, and their update vectors can be decomposed into shared linguistic features and knowledge-specific components.
- Evidence anchors:
  - [abstract] "CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features."
  - [section 4.1] "By performing zi − δ′_i, we aim to remove the portions of the parameters updated with new knowledge that still contain outdated information."
  - [corpus] Related work "Resolving Editing-Unlearning Conflicts" (arXiv:2502.00158) similarly addresses editing-unlearning tradeoffs, suggesting this is a recognized problem space, but no direct validation of the subtraction mechanism was found.

### Mechanism 2
- Claim: Preserving shared linguistic features during unlearning maintains model fluency and locality.
- Mechanism: CoME extracts the common direction vector δ⃗_i by adding normalized δ_i and δ'_i, then projects δ'_i onto this direction to isolate δ''_i (shared linguistic component). Only δ'_i − δ''_i is subtracted, preserving language-processing capabilities.
- Core assumption: Both update vectors trained on the same input prompt contain overlapping linguistic capacity that, if removed, would degrade general language performance.
- Evidence anchors:
  - [section 4.1] "Since the two linearly independent vectors span distinct hyperplanes, we obtain the direction vector δ⃗_i representing the common component by adding their normalized values."
  - [Table 3 ablation] Removing δ'' causes Locality to drop significantly (86.4 → 84.1 for CoME-PMET on GPT-J Counterfact).
  - [corpus] Limited corpus evidence for this specific projection-based preservation technique.

### Mechanism 3
- Claim: Restricting unlearning to top-p% parameters by magnitude preserves unrelated knowledge (locality).
- Mechanism: Only the top 20% of parameters ranked by |δ'_i − δ''_i| magnitude are subject to unlearning; remaining parameters stay unchanged. This prevents unnecessary modifications to parameters storing unrelated facts.
- Core assumption: Parameters with largest unlearning update magnitudes are most critical for outdated knowledge storage.
- Evidence anchors:
  - [section 4.3] "We restrict unlearning to the top-p% of parameters based on the magnitude of the unlearning update."
  - [Table 3 ablation] Without restriction, Locality drops from 73.2 to 68.9 (CoME-MEMIT on GPT-J Counterfact).
  - [corpus] "Edit Less, Achieve More" (arXiv:2510.22139) uses sparse neuron masking for lifelong editing, supporting the efficacy of selective parameter modification strategies.

## Foundational Learning

- Concept: **Locate-then-Edit Knowledge Editing**
  - Why needed here: CoME builds on MEMIT/PMET, which identify MLP layers storing factual knowledge and compute update vectors δ_i via gradient descent on target outputs.
  - Quick check question: Can you explain how MEMIT distributes a single-layer update across multiple target layers using the covariance matrix C?

- Concept: **Machine Unlearning via Gradient Ascent/Parameter Arithmetic**
  - Why needed here: CoME applies "unlearning" not via fine-tuning but via parameter subtraction, inspired by task arithmetic approaches.
  - Quick check question: How does parameter subtraction differ from gradient ascent for unlearning, and what are the tradeoffs?

- Concept: **Knowledge Conflict in Model Editing**
  - Why needed here: The core problem is that post-edit, models may generate outputs reflecting both old and new knowledge simultaneously (see case study Table 4 where PMET outputs "English Spanish").
  - Quick check question: What evaluation metrics detect knowledge conflicts during inference versus parameter-level analysis?

## Architecture Onboarding

- Component map: Input triples → prompts x_i and x_gen_i → δ_i and δ'_i extraction → δ''_i projection → masked subtraction → weight update distribution across target layers

- Critical path: δ_i and δ'_i extraction → δ''_i projection → masked subtraction → weight update distribution. Errors in projection or masking compound through distribution.

- Design tradeoffs:
  - α (unlearning weight): Higher α improves Efficacy/Generality but degrades Locality (Figure 3 shows α=0.1 as optimal)
  - p% restriction: Lower p preserves Locality but may leave residual outdated knowledge
  - Layer selection: GPT-J uses layers {3-8}, LLaMA-3 uses {4-8}; incorrect layer targeting reduces edit efficacy

- Failure signatures:
  - Persistent outdated knowledge in outputs: Model generates text containing both old and new facts (Table 4 PMET example) → increase α or verify δ'_i extraction
  - Fluency degradation: N-gram repetition or incoherent outputs → δ''_i extraction may be insufficient; check linguistic preservation
  - Locality collapse on unrelated facts: Large Locality drops after editing → reduce α or increase p% threshold

- First 3 experiments:
  1. Reproduce single-edit baseline: Apply MEMIT to 100 Counterfact samples on GPT-J, verify Efficacy ~98% and Locality ~73%; establish that conflicts appear in generative outputs.
  2. Ablate linguistic preservation: Run CoME-MEMIT with and without δ''_i extraction on 500 samples; quantify Locality delta (expected ~4% drop from Table 3).
  3. Sweep α and p%: Grid search α ∈ {0.05, 0.1, 0.2} and p ∈ {10, 20, 30} on 1000 Counterfact samples; plot Efficacy-Generality-Locality tradeoff surface to validate paper claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of CoME's unlearning stage be reduced while maintaining editing effectiveness?
- Basis in paper: [explicit] The authors state in the Limitations section that "the unlearning process requires additional computational resources. Since CoME introduces a separate stage to remove outdated knowledge, it incurs higher computational costs than traditional model editing techniques."
- Why unresolved: The paper demonstrates effectiveness but does not propose or evaluate efficiency optimizations for the dual-process approach.
- What evidence would resolve it: Runtime and memory benchmarks comparing CoME against baselines, plus experiments with modified architectures that share computation between unlearning and editing stages.

### Open Question 2
- Question: How should model editing frameworks handle temporal knowledge where outdated information may still have contextual value?
- Basis in paper: [explicit] The authors acknowledge that "CoME is designed to remove outdated or false knowledge, which may not always be desirable in cases of temporal knowledge. For example, older information that reflects past realities can still be useful in certain contexts."
- Why unresolved: The current approach treats outdated knowledge uniformly as interference, without mechanisms for conditional retention or context-aware unlearning.
- What evidence would resolve it: Experiments on temporally-scoped queries and a modified CoME variant with context-sensitive unlearning, evaluated on datasets explicitly containing temporal knowledge.

### Open Question 3
- Question: Can the trade-off between Locality and editing accuracy (Efficacy/Generality) be further improved beyond the current α-based balancing?
- Basis in paper: [inferred] The results consistently show Locality decreases when α increases (Figure 3), and the authors note "this trade-off between editing accuracy and Locality is expected, as our primary objective is to inject new knowledge rather than minimize changes to the model."
- Why unresolved: The hyperparameter α provides only a linear trade-off, and restricting unlearning to top-p% parameters (p=20) partially mitigates but does not eliminate the locality degradation.
- What evidence would resolve it: Novel mechanisms such as layer-wise or knowledge-specific α values, or adaptive parameter selection strategies that improve the Pareto frontier between accuracy and locality metrics.

## Limitations
- CoME introduces significant computational overhead due to the separate unlearning stage, requiring additional resources beyond traditional model editing
- The approach assumes outdated knowledge uniformly interferes with new information, without mechanisms for preserving temporally-relevant facts
- Fixed hyperparameters (α=0.1, p=20%) may not generalize optimally across all domains and knowledge types

## Confidence

- **High confidence**: The core unlearning mechanism through parameter subtraction is well-grounded in existing parameter arithmetic literature and produces measurable improvements in editing accuracy metrics (Efficacy, Generality).
- **Medium confidence**: The linguistic preservation mechanism via vector projection is theoretically sound but relies on assumptions about linear separability of knowledge and language features that may not hold universally.
- **Medium confidence**: The restricted parameter selection approach improves locality, but the claim that top-20% by magnitude captures critical outdated knowledge parameters may not generalize to all knowledge types or model architectures.

## Next Checks

1. **Cross-domain generalization test**: Apply CoME to biomedical knowledge editing using MedMKEB benchmark and evaluate whether α=0.1 and p=20% remain optimal, or if domain-specific tuning is required.

2. **Longer sequence stress test**: Edit knowledge in contexts requiring 512+ token generation to verify that CoME's parameter restrictions and linguistic preservation maintain performance under extended inference, where outdated knowledge interference typically increases.

3. **Real-time editing validation**: Implement CoME in an online learning scenario where models receive continuous knowledge updates, measuring degradation rates of Locality and Fluency over 100+ sequential edits to assess long-term stability.