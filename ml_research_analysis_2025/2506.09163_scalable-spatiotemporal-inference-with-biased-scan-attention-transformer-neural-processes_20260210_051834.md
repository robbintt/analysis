---
ver: rpa2
title: Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural
  Processes
arxiv_id: '2506.09163'
source_url: https://arxiv.org/abs/2506.09163
tags:
- attention
- test
- points
- bsa-tnp
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Biased Scan Attention Transformer Neural
  Process (BSA-TNP), a novel architecture for scalable spatiotemporal inference that
  addresses the accuracy-scalability tradeoff in Neural Processes. The key innovation
  is combining Kernel Regression Blocks with group-invariant attention biases and
  a memory-efficient Biased Scan Attention mechanism.
---

# Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes

## Quick Facts
- arXiv ID: 2506.09163
- Source URL: https://arxiv.org/abs/2506.09163
- Reference count: 40
- Primary result: BSA-TNP achieves state-of-the-art performance on multiple spatiotemporal benchmarks while scaling to 100K+ context points with sub-minute inference

## Executive Summary
This paper introduces BSA-TNP, a novel architecture for scalable spatiotemporal inference that addresses the accuracy-scalability tradeoff in Neural Processes. The key innovation combines Kernel Regression Blocks with group-invariant attention biases and a memory-efficient Biased Scan Attention mechanism. The model achieves state-of-the-art performance on multiple benchmarks including 2D Gaussian Processes, epidemiology, climate forecasting, and air quality prediction, while running inference on over 1M test points with 100K context points in under a minute on a single GPU.

## Method Summary
BSA-TNP builds on Transformer Neural Processes by introducing KRBlocks that use Nadaraya-Watson kernel regression with shared Q/K weights, and Biased Scan Attention that computes custom RBF-based attention biases on-the-fly with constant memory. The architecture enforces translation invariance by passing spatial/temporal features only to bias functions rather than embeddings. Training uses AdamW with gradient clipping and learning rate schedules, with single RTX 4090 24GB hardware requirements.

## Key Results
- Achieves state-of-the-art NLL and MAE across 2D GPs, SIR epidemiology, ERA5 climate, and Beijing air quality benchmarks
- Runs inference with 100K+ context points and 1M+ test points in under a minute on single GPU
- Demonstrates translation invariance enabling multi-resolution learning simultaneously
- Shows 2-3x faster training times compared to TNP-D baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Group-invariant attention biases may improve generalization and convergence for translation-invariant spatiotemporal processes.
- **Mechanism:** Instead of embedding spatial/temporal coordinates directly, RBF-network-based bias terms compute pairwise distances between points. Since distance is translation-invariant, the posterior predictive map becomes translation-invariant by Theorem 2.
- **Core assumption:** The underlying stochastic process exhibits stationarity or partial stationarity under translation.
- **Evidence anchors:**
  - [abstract] "exhibit translation invariance, enabling learning at multiple resolutions simultaneously"
  - [section 4.2] Equation 4: "these biases depend only on pairwise distances between terms, they are translation invariant"
  - [corpus] Limited direct evidence; neighbor papers focus on pseudo-token and EM approaches rather than invariance.
- **Break condition:** Non-stationary processes where location carries absolute semantic meaning (e.g., city-specific pollution profiles without fixed effects).

### Mechanism 2
- **Claim:** Memory-efficient attention with on-the-fly bias computation enables scaling to 100K+ context points.
- **Mechanism:** BSA uses scan-based tiling with gradient checkpointing (from Rabe & Staats 2022) plus custom JIT-compiled JAX bias functions. Attention scores and bias terms are computed incrementally, maintaining O(block_size²) memory instead of O(n²).
- **Core assumption:** Inference requires large batches but can tolerate sequential processing of tiles.
- **Evidence anchors:**
  - [abstract] "running inference with over 1M test points with 100K context points in under a minute"
  - [section 4.3] "calculates attention scores and custom bias terms on the fly with constant memory"
  - [corpus] Weak corpus linkage—no neighbor papers explicitly address custom bias integration with scan-based attention.
- **Break condition:** Dense observations with sparse predictions (video tasks) where O(n²c) context-to-context attention remains prohibitive.

### Mechanism 3
- **Claim:** KRBlocks with weight-sharing between Q and K may reduce parameter count while maintaining expressiveness.
- **Mechanism:** Inspired by Nadaraya-Watson kernel regression, queries and keys share weights within each block. Stacking allows iterative refinement of representations while residual connections preserve spatial/temporal features for bias functions.
- **Core assumption:** Kernel-based attention suffices for spatiotemporal interpolation without separate Q/K projections.
- **Evidence anchors:**
  - [section 4.1] "sharing weights for query and key updates"
  - [table 10] BSA-TNP achieves 94 GFLOPS vs TNP-D's 217 GFLOPS during training
  - [corpus] No direct comparison—neighbor papers don't discuss weight-sharing in attention.
- **Break condition:** Tasks requiring asymmetric query-key relationships (e.g., directional temporal prediction).

## Foundational Learning

- **Concept: Stationary stochastic processes**
  - Why needed here: The G-invariance framework assumes translation-invariant marginal distributions; understanding when this holds determines applicability.
  - Quick check question: Would shifting all spatial coordinates by +10 change the predicted distribution?

- **Concept: Nadaraya-Watson kernel regression**
  - Why needed here: KRBlocks are explicitly motivated by this classical nonparametric regression technique.
  - Quick check question: How does kernel bandwidth affect interpolation smoothness?

- **Concept: Scan-based attention tiling (FlashAttention family)**
  - Why needed here: BSA extends FlashAttention 2's chunking with custom bias support.
  - Quick check question: Why does computing softmax row-by-row require tracking the running maximum?

## Architecture Onboarding

- **Component map:** Embedding layer (separate MLPs per feature group) → KRBlock stack (BSA + FFN with LayerNorm) → Prediction head (MLP to distribution parameters). Spatial/temporal features bypass embedding for bias-only use.

- **Critical path:** Implementing BSA correctly is the highest-risk component. The tiling logic (Equations 6-8) must track m(x), ℓ(x), and Õ across tiles with proper rescaling.

- **Design tradeoffs:**
  - Invariance vs. expressiveness: Excluding space/time from embeddings enforces invariance but loses absolute location information.
  - Memory vs. flexibility: BSA enables custom biases but sequential scanning is slower than FlashAttention's parallelism for standard attention.

- **Failure signatures:**
  - Exploding NLL on shifted domains → invariance not enforced (embedding locations instead of bias-only).
  - OOM at 50K context points → block_size too large or bias matrix materialized.
  - Oversmoothed predictions → too many basis functions or excessive bias weight.

- **First 3 experiments:**
  1. Replicate 2D GP benchmark with varying lengthscales (Beta(3,7) sampling); verify NLL matches Table 1.
  2. Ablate translation invariance by embedding locations; compare NLL on shifted domain (should degrade like TNP-D).
  3. Profile memory scaling: sweep context points from 1K to 100K, confirm constant memory usage per block size.

## Open Questions the Paper Calls Out

- **Question:** Can BSA-TNP be efficiently extended to handle dense observation settings (e.g., video prediction) where context points are numerous, given its O(n²) complexity in context points?
  - **Basis in paper:** [explicit] The authors state: "While most NP tasks assume sparse observations and dense predictions, some tasks have dense observations and sparse predictions, e.g. video prediction tasks. In this case, BSA-TNP is still has quadratic complexity in the number of context points, making training and inference over a collection of dense frames computationally burdensome."
  - **Why unresolved:** The current architecture targets category (5) memory-efficient attention but does not address quadratic context scaling; inducing point approaches (category 2) were deliberately avoided.
  - **What evidence would resolve it:** An extension incorporating frame summarizations (e.g., from I-JEPA) or a hybrid architecture that reduces context complexity while maintaining accuracy.

- **Question:** How does BSA-TNP perform on non-stationary processes where G-invariance assumptions are strongly violated?
  - **Basis in paper:** [inferred] The paper notes that attention biases "can still prove useful when a process is only partially stationary" (Section 4.2), but benchmarks primarily feature stationary or partially stationary processes. No systematic ablation on strongly non-stationary data is provided.
  - **Why unresolved:** The Beijing air quality experiment shows improvement but does not isolate performance degradation when invariance assumptions fail entirely.
  - **What evidence would resolve it:** Benchmarks on explicitly non-stationary synthetic processes (e.g., spatially varying lengthscales, trended temporal processes) with and without G-invariant biases.

- **Question:** What is the optimal configuration of basis functions (F) for RBF-network attention biases across different spatiotemporal domains?
  - **Basis in paper:** [inferred] The authors use F=5 for space and F=3 for time, stating "in initial tests we found little benefit to using more" (Section 4.2), but no systematic ablation or theoretical justification is provided.
  - **Why unresolved:** The choice appears heuristic and may not generalize to processes with different correlation structures or dimensionalities.
  - **What evidence would resolve it:** Ablation studies varying F across benchmarks with varying lengthscale distributions and reporting sensitivity of NLL, MAE, and calibration metrics.

## Limitations

- BSA memory-efficiency mechanism is incompletely specified — critical implementation details (block size, gradient checkpointing strategy, JAX kernel compilation) are missing from the main text and referenced appendices
- Translation invariance proof relies on RBF biases depending only on pairwise distances, but experimental validation is limited to benchmark performance rather than dedicated stress tests
- Weight-sharing between Q and K in KRBlocks lacks ablation studies quantifying impact on accuracy vs. efficiency tradeoff

## Confidence

- **High confidence** in overall accuracy claims (NLL, MAE, RMSE) on standard benchmarks, supported by multiple datasets and comparison with established TNP-D baseline
- **Medium confidence** in scalability claims (100K context points in under a minute) due to incomplete BSA specification — theoretical mechanism is sound but practical implementation details are missing
- **Low confidence** in translation invariance benefits without dedicated experiments — claim that BSA "enables learning at multiple resolutions simultaneously" is inferred from benchmark performance rather than explicitly tested

## Next Checks

1. **Memory scaling verification:** Profile GPU memory usage while increasing context points from 1K to 100K with fixed block size. Memory should remain constant per block, not scale quadratically. If O(n²) scaling occurs, BSA implementation is incorrect.

2. **Invariance stress test:** Train on 2D GP with Beta(3,7) lengthscales, then evaluate on shifted domain (+10) and scaled domain (2×). NLL should remain stable (~-0.32 shifted, ~-0.28 scaled) — degradation to TNP-D levels (~21 NLL) indicates s/t were incorrectly embedded rather than bias-only.

3. **Ablation of Q/K weight-sharing:** Implement BSA-TNP with separate Q/K projections (standard attention). Compare parameter count, GFLOPS, training time, and final NLL to original. Quantify expressiveness vs. efficiency tradeoff.