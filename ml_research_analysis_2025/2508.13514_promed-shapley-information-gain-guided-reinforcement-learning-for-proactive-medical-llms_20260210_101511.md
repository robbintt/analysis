---
ver: rpa2
title: 'ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive
  Medical LLMs'
arxiv_id: '2508.13514'
source_url: https://arxiv.org/abs/2508.13514
tags:
- information
- patient
- medical
- question
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of medical LLMs operating in a
  reactive paradigm during interactive consultations, where they answer without seeking
  additional information, leading to misdiagnosis. The authors propose ProMed, a reinforcement
  learning framework that shifts LLMs toward a proactive paradigm by enabling them
  to ask clinically valuable questions before decision-making.
---

# ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs

## Quick Facts
- arXiv ID: 2508.13514
- Source URL: https://arxiv.org/abs/2508.13514
- Reference count: 40
- Key outcome: ProMed improves proactive medical LLM performance by 6.29% over SOTA and delivers 54.45% gain over reactive paradigm on partial-information medical benchmarks

## Executive Summary
This paper addresses the limitation of medical LLMs operating in a reactive paradigm during interactive consultations, where they answer without seeking additional information, leading to misdiagnosis. The authors propose ProMed, a reinforcement learning framework that shifts LLMs toward a proactive paradigm by enabling them to ask clinically valuable questions before decision-making. The core method is the Shapley Information Gain (SIG) reward, which quantifies question utility by combining information gain with contextual importance via Shapley values from cooperative game theory.

The proposed framework uses a two-stage training pipeline: SIG-Guided Model Initialization uses MCTS to construct high-reward interaction trajectories for supervised warm-up, and SIG-Augmented Policy Optimization integrates SIG into GRPO with a novel reward distribution mechanism. Experiments on two newly curated partial-information medical benchmarks show ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.

## Method Summary
ProMed addresses the reactive paradigm limitation in medical LLMs by introducing a proactive reinforcement learning framework that enables models to ask clinically valuable questions before decision-making. The core innovation is the Shapley Information Gain (SIG) reward, which quantifies question utility by combining information gain with contextual importance using Shapley values from cooperative game theory. The framework employs a two-stage training pipeline: first, SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories for supervised warm-up; second, SIG-Augmented Policy Optimization integrates SIG into GRPO with a novel reward distribution mechanism that assigns higher rewards to informative questions. This approach shifts LLMs from reactive answering to proactive information gathering, resulting in more accurate diagnoses on partial-information medical consultation tasks.

## Key Results
- ProMed significantly outperforms state-of-the-art methods by an average of 6.29% on newly curated medical benchmarks
- Delivers a 54.45% gain over the reactive paradigm baseline
- Demonstrates robust generalization to out-of-domain cases

## Why This Works (Mechanism)
The SIG reward mechanism effectively quantifies question utility by combining information gain with contextual importance via Shapley values, providing a principled way to evaluate which questions will most improve diagnostic accuracy. The two-stage training pipeline allows the model to first learn from high-quality trajectories generated by MCTS before fine-tuning with the SIG reward, ensuring stable and effective learning. By shifting from reactive to proactive questioning, the model can gather critical information that would otherwise be missing, leading to more accurate diagnoses in partial-information scenarios.

## Foundational Learning

**Shapley Values in Cooperative Game Theory**: Measures fair contribution of each player to the overall game outcome by averaging marginal contributions across all possible coalitions. Needed to fairly distribute credit among multiple questions in a conversation and quantify each question's contextual importance. Quick check: Verify that the Shapley value calculation correctly handles the sequential nature of medical consultations.

**Information Gain Quantification**: Measures reduction in uncertainty about the target variable after observing new evidence. Required to evaluate how much each question reduces diagnostic uncertainty. Quick check: Ensure entropy calculations are correctly implemented for medical diagnosis scenarios.

**Monte Carlo Tree Search (MCTS)**: A search algorithm that balances exploration and exploitation to find optimal decision sequences through repeated simulation. Necessary for constructing high-quality interaction trajectories during the initialization stage. Quick check: Validate that MCTS explores diverse question paths rather than converging to a narrow set of questions.

**Generalized Reward Policy Optimization (GRPO)**: A reinforcement learning algorithm that optimizes policy through reward maximization while maintaining stability. Required for fine-tuning the LLM with the SIG reward signal. Quick check: Monitor policy stability during training to ensure GRPO doesn't cause catastrophic forgetting.

**Medical Partial-Information Scenarios**: Consultation settings where patients provide incomplete symptom information, requiring physicians to ask follow-up questions. Essential context for evaluating the practical utility of proactive questioning. Quick check: Verify that benchmark datasets accurately represent real-world clinical uncertainty.

## Architecture Onboarding

**Component Map**: Input Context -> MCTS Trajectory Generator -> SIG Reward Calculator -> GRPO Trainer -> Proactive LLM

**Critical Path**: Patient symptom input → MCTS generates optimal question sequence → Model asks questions → Receives answers → Updates diagnosis → Receives SIG reward → Policy update via GRPO

**Design Tradeoffs**: The framework trades computational complexity (MCTS and Shapley calculations) for improved diagnostic accuracy. The two-stage training adds training time but provides more stable learning compared to end-to-end RL.

**Failure Signatures**: If SIG rewards are not properly calibrated, the model may ask irrelevant or redundant questions. Poor MCTS initialization could lead to suboptimal trajectories that bias the entire learning process. Without proper generalization, the model may overfit to the specific question patterns in training data.

**First 3 Experiments**:
1. Verify MCTS generates diverse and clinically relevant question trajectories on a small validation set
2. Test SIG reward calculation on synthetic consultation scenarios with known optimal question sequences
3. Evaluate whether the two-stage training improves stability compared to direct GRPO fine-tuning

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation limited to two newly created datasets without external validation on established medical benchmarks
- SIG reward mechanism assumes information gain can be effectively quantified through Shapley values, but this theoretical framework has not been validated against clinical expert judgment
- The MCTS-based trajectory construction may introduce bias toward certain question types that perform well in simulation but may not generalize to real clinical scenarios

## Confidence

**Performance improvements over SOTA methods**: High - The paper reports specific percentage improvements (6.29% average) with comparative results against multiple baselines.

**SIG reward effectiveness**: Medium - While the theoretical framework is well-explained, the practical superiority of SIG over simpler information gain measures is not definitively proven.

**Generalization to out-of-domain cases**: Medium - The paper claims robust generalization but provides limited evidence beyond the two curated datasets.

## Next Checks

1. External validation on established medical QA benchmarks like MedQA-USMLE or PubMedQA to verify performance claims beyond the two newly created datasets.

2. Human evaluation study where clinical experts assess the quality and appropriateness of questions generated by ProMed versus reactive baselines in simulated patient consultations.

3. Ablation study comparing SIG reward against simpler information gain metrics and alternative proactive questioning strategies to isolate the specific contribution of the Shapley value component.