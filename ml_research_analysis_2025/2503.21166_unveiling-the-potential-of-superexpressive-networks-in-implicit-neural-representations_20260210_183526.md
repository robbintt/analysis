---
ver: rpa2
title: Unveiling the Potential of Superexpressive Networks in Implicit Neural Representations
arxiv_id: '2503.21166'
source_url: https://arxiv.org/abs/2503.21166
tags:
- nestnet
- neural
- image
- learning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of NestNets, a class of
  superexpressive networks with nested structures, for implicit neural representations
  (INRs). Unlike existing INR approaches that focus on specialized activation functions,
  NestNets employ a nonstandard network architecture with an additional "height" dimension,
  where each hidden neuron is activated by a trainable subnetwork.
---

# Unveiling the Potential of Superexpressive Networks in Implicit Neural Representations

## Quick Facts
- **arXiv ID:** 2503.21166
- **Source URL:** https://arxiv.org/abs/2503.21166
- **Reference count:** 22
- **Key outcome:** NestNets achieve 32.91 dB PSNR and 0.95 SSIM on image representation, outperforming state-of-the-art INRs by +1.61 dB and +0.03 SSIM.

## Executive Summary
This paper investigates NestNets, a class of superexpressive networks with nested structures, for implicit neural representations (INRs). Unlike existing INR approaches that focus on specialized activation functions, NestNets employ a nonstandard network architecture with an additional "height" dimension, where each hidden neuron is activated by a trainable subnetwork. The study evaluates NestNets across diverse benchmark tasks including image representation, occupancy volume representation, inverse problems (single-image/multi-image super-resolution, image denoising, CT reconstruction), and physics-informed neural networks for solving PDEs. Results show that NestNets consistently outperform state-of-the-art INRs including WIREs, SIRENs, Gaussian, MFNs, and FFNs across all tasks.

## Method Summary
NestNets are MLPs with an additional "height" dimension where each hidden neuron is activated by a trainable subnetwork ϱ(h) = w₂ᵀ ReLU(w₁h + b₁) + b₂. The main network has 2 hidden layers with 256 neurons each, and subnetworks are initialized with fixed parameters (w₁=[1,1,1], w₂=[1,1,-1], b₁=[-0.2,-0.1,0.0], b₂=0). Fourier feature mapping is applied to coordinates at the input layer with αᵢ=1, βᵢ=i. The model is trained using Adam optimizer with learning rates ranging from 0.005-0.01 depending on the task, across 5 random seeds with the best result reported.

## Key Results
- Image representation: NestNet achieves 32.91 dB PSNR and 0.95 SSIM, outperforming second-best method by +1.61 dB and +0.03 SSIM
- Occupancy volume representation: NestNet reaches 0.9964 IOU
- CT reconstruction: NestNet achieves 32.37 dB PSNR and 0.93 SSIM
- Physics-informed neural networks: NestNet outperforms SIREN, WIRE, and MFNs on solving 1D convection equations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Subnetworks serving as learnable activation functions provide superior expressivity compared to fixed hand-designed activations.
- **Mechanism:** Instead of applying a fixed nonlinearity (e.g., ReLU, sine, Gaussian), NestNets replace activation functions with small trainable MLPs applied element-wise to pre-activations.
- **Core assumption:** The learned activation shapes can capture signal-specific nonlinearities better than any universal fixed activation function.
- **Evidence anchors:** [abstract] "The nested structure allows NestNets to learn flexible nonlinear activation functions with superior expressiveness compared to standard activation functions." [section 2] "each hidden neuron of NestNet(s) is activated by one of the subnetworks, ϱ₁, · · · , ϱᵣ, where each ϱᵢ : R ↦ R is a trainable function" [Figure 6] Shows learned activations at layers 1-3 have nontrivial shapes different from standard activations.
- **Break condition:** If subnetworks converge to near-linear functions or collapse to similar shapes across layers, the expressivity advantage diminishes.

### Mechanism 2
- **Claim:** The additional "height" dimension enables compositional expressivity through nested activation hierarchies.
- **Mechanism:** A NestNet of height s has its hidden neurons activated by a NestNet of height s-1.
- **Core assumption:** Hierarchical composition of nonlinearities yields representational power unavailable to depth/width scaling alone.
- **Evidence anchors:** [abstract] "superexpressive networks...which employ a specialized network structure characterized by having an additional dimension, namely width, depth, and 'height'" [section 2] "hidden neurons of a NestNet of height s are activated by a NestNet of height s − 1"
- **Break condition:** If increasing height beyond 2 provides diminishing returns or training instability, the practical benefit is limited.

### Mechanism 3
- **Claim:** Expressivity gains derive from architecture rather than specialized activation functions, allowing standard ReLU to achieve state-of-the-art results.
- **Mechanism:** NestNets use standard ReLU within subnetworks. The compositional structure—rather than exotic activations—enables fitting high-frequency signal components.
- **Core assumption:** Architectural expressivity can substitute for activation function engineering.
- **Evidence anchors:** [section 1] "we focus on MLPs with non-standard network architectures, but with the standard ReLU activation functions" [results] NestNet outperforms WIRE, SIREN, Gaussian, MFNs, FFNs across all tasks despite using ReLU
- **Break condition:** If tasks require inductive biases beyond what nested ReLU provides (e.g., explicit periodicity for audio), specialized activations may still be necessary.

## Foundational Learning

- **Concept: Implicit Neural Representations (INRs)**
  - Why needed here: NestNets are evaluated specifically as an INR architecture; understanding the coordinate-to-signal mapping paradigm is essential.
  - Quick check question: Can you explain why a 2D image can be represented as f(x,y) → (R,G,B) using an MLP?

- **Concept: Universal Approximation and Expressivity**
  - Why needed here: The paper positions NestNets as "superexpressive"—this builds on universal approximation theory but claims practical expressivity gains.
  - Quick check question: Why does universal approximation capability not guarantee good performance on finite data with limited training?

- **Concept: Fourier Feature Mapping / Positional Encoding**
  - Why needed here: NestNets use Fourier features at the input layer; this is critical for mitigating spectral bias in coordinate-based MLPs.
  - Quick check question: What problem does mapping coordinates to sinusoidal features solve in INRs?

## Architecture Onboarding

- **Component map:** Input coordinates → Fourier feature mapping → Main MLP (2 hidden layers, 256 neurons) → Subnetwork activations (learnable ϱ(·)) → Output signal values

- **Critical path:**
  1. Initialize subnetwork parameters with specific values (w₁=[1,1,1], w₂=[1,1,-1], b₁=[-0.2,-0.1,0.0], b₂=0)
  2. Forward pass: affine transform → subnetwork activation → affine transform → output
  3. Loss: ℓ₂ distance between predicted and target signal values
  4. Optimize: Adam with learning rates 0.005-0.01 depending on task

- **Design tradeoffs:**
  - Parameter count: NestNet has ~2.3× more parameters than comparable WIRE (153K vs 67K)
  - Training time: ~1.7× slower than WIRE (17 min vs 10 min on RTX 3090 for image representation)
  - Height choice: Paper only evaluates height=2; higher heights unexplored

- **Failure signatures:**
  - Subnetwork collapse: If activations converge to near-identity or near-zero functions, expressivity is lost
  - Overfitting in low-data regimes: High capacity may memorize noise (monitor validation metrics)
  - Training instability at high learning rates: Paper shows LR>0.05 can destabilize (Figure 17)

- **First 3 experiments:**
  1. Image representation on a single Kodak image (2000 epochs, LR=0.005): Verify PSNR improvement over SIREN/WIRE baseline.
  2. Ablation on subnetwork initialization: Train with random vs. prescribed initialization to test sensitivity.
  3. Occupancy volume representation (200 epochs): Test 3D signal fitting capability and compare IOU to baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the superior expressivity of NestNets be further enhanced by integrating them with advanced PINN optimization algorithms?
- **Basis in paper:** [explicit] The authors state in Section 3.2 that they focused on standard PINNs and "leave this [combining with advanced algorithms] to future work."
- **Why unresolved:** The study limited its scope to comparing base architectures to isolate expressivity gains derived strictly from the network structure rather than the training pipeline.
- **What evidence would resolve it:** Performance benchmarks of NestNets on complex PDEs when utilizing techniques like adaptive collocation points (PINNACLE) or operator preconditioning.

### Open Question 2
- **Question:** How does increasing the NestNet height parameter ($s > 2$) impact optimization stability and performance?
- **Basis in paper:** [inferred] The experimental setup in Section 3 explicitly states, "For all experiments, we consider NestNets of height 2," leaving the potential of deeper nesting unexplored.
- **Why unresolved:** While the theory supports greater expressivity with higher height, the authors did not test if the optimization becomes difficult or if diminishing returns set in beyond height 2.
- **What evidence would resolve it:** Ablation studies showing convergence rates and reconstruction accuracy for NestNets with heights $s \in \{3, 4, 5\}$ on the same benchmarks.

### Open Question 3
- **Question:** Is the performance improvement of NestNets Pareto-optimal compared to baselines when strictly accounting for parameter count and FLOPs?
- **Basis in paper:** [inferred] The appendix notes NestNet has ~2.3x the parameters (153k vs 67k) and longer training times than WIRE, yet increasing parameters in WIRE actually degraded its performance.
- **Why unresolved:** It is unclear if NestNet's success is due to the "superexpressive" structure or simply a better scaling law, as the baselines failed to utilize extra capacity effectively.
- **What evidence would resolve it:** A comparative analysis of PSNR/SSIM versus FLOPs and parameter counts, specifically testing if smaller NestNets can match the performance of the larger baselines.

## Limitations
- Height scalability: The paper only tests NestNet with height=2; higher heights could introduce training instability or diminishing returns.
- Hyperparameter sensitivity: Fixed subnetwork initialization parameters (w₁, w₂, b₁, b₂) may be critical for success and were not explored.
- Parameter efficiency: NestNet has ~2.3× more parameters than comparable baselines, making expressivity gains partly attributable to increased capacity.

## Confidence
- **High confidence**: NestNet consistently outperforms baselines on all tested tasks (PSNR/SSIM improvements of +1.61 dB/+0.03 SSIM on image representation). The architectural design and training methodology are clearly specified.
- **Medium confidence**: The expressivity advantage stems from learnable activation functions rather than specialized activations. While the ablation on activation types (Figure 6) supports this, the paper doesn't directly compare against other architectural innovations with standard ReLU.
- **Low confidence**: Generalization to tasks requiring strong inductive biases (audio, graphs) or whether the benefits extend beyond coordinate-based INRs. The nested structure's advantages in non-spatial domains are unexplored.

## Next Checks
1. **Height scaling experiment**: Test NestNet(3) and NestNet(4) on the Kodak image representation task to identify the optimal height and verify training stability.
2. **Subnetwork initialization ablation**: Train with random initialization versus prescribed initialization to quantify sensitivity to the fixed parameters (w₁, w₂, b₁, b₂).
3. **Cross-domain transfer**: Evaluate on audio INR benchmarks (e.g., sound waves) where specialized activations like periodic functions are typically required, to test whether nested ReLU generalizes beyond spatial data.