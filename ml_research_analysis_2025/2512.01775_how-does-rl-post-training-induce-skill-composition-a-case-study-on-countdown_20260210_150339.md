---
ver: rpa2
title: How Does RL Post-training Induce Skill Composition? A Case Study on Countdown
arxiv_id: '2512.01775'
source_url: https://arxiv.org/abs/2512.01775
tags:
- pattern
- training
- coverage
- patterns
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how reinforcement learning (RL) post-training
  affects skill composition in large language models. The authors study the Countdown
  task, where models must form expressions from numbers to reach a target, and represent
  solutions as expression trees to track skill development.
---

# How Does RL Post-training Induce Skill Composition? A Case Study on Countdown

## Quick Facts
- arXiv ID: 2512.01775
- Source URL: https://arxiv.org/abs/2512.01775
- Authors: Simon Park; Simran Kaur; Sanjeev Arora
- Reference count: 40
- Key outcome: RL post-training enables compositional generalization in Countdown task, with models learning to synthesize novel reasoning patterns from known substructures and mastering balanced compositional structures before unbalanced ones.

## Executive Summary
This paper analyzes how reinforcement learning (RL) post-training affects skill composition in large language models through the lens of the Countdown task. The authors represent solutions as expression trees and track skill development through canonical pattern analysis. They find that RL training enables models to generalize to larger puzzle sizes and unseen compositional patterns, demonstrating both length and compositional generalization. The analysis reveals a structure-dependent hierarchy of learnability, with balanced patterns mastered more reliably than unbalanced ones, and right-heavy patterns being particularly challenging due to lookahead requirements. The work provides evidence that RL helps models synthesize novel reasoning patterns by reusing learned substructures.

## Method Summary
The study uses the Countdown task where models must form expressions from numbers to reach a target, representing solutions as expression trees. The authors employ GRPO (Group Relative Policy Optimization) on Qwen-2.5 models (1.5B, 3B, 7B) with a balanced dataset generated by sampling patterns first rather than random trees. The training objective combines format reward (0.1) and correctness reward (0.9) with KL coefficient 10^-3. Evaluation uses Pass@32, All-correct@32, Pattern Coverage, and other metrics. The key innovation is the canonical pattern representation that enables structural analysis of learning dynamics across different compositional patterns.

## Key Results
- RL enables models to generalize to larger puzzle sizes (n=5,6) and unseen compositional patterns, demonstrating compositional reuse of substructures
- Balanced compositional structures are learned before unbalanced ones, with right-heavy patterns ([1]◦[3]) being particularly challenging even at equivalent depth
- Models successfully learn to generate held-out patterns despite zero training exposure, showing bottom-up composition rather than top-down template matching
- RL outperforms supervised fine-tuning for compositional generalization, with SFT exhibiting opposite learning dynamics by pattern structure

## Why This Works (Mechanism)

### Mechanism 1: Structural Hierarchy of Learnability
Balanced compositional structures are learned before unbalanced ones, with right-heavy trees being the most challenging, even when depth is equivalent to left-heavy structures. Autoregressive models face a "lookahead bottleneck" when generating right-heavy patterns because they must commit to the root operator before generating a complex subroutine that appears later in the sequence. This requires non-greedy planning that conflicts with the left-to-right generation bias. Evidence shows this hierarchy persists regardless of normalization choices.

### Mechanism 2: Compositional Reuse of Substructures
RL enables synthesis of entirely novel reasoning patterns by reusing learned substructures, not merely memorizing seen templates. When specific patterns are held out from training, models can still generate them and their higher-order extensions. Learning dynamics show coverage emerges on simpler subpatterns before complex dependents, suggesting bottom-up composition rather than top-down template matching.

### Mechanism 3: Exploration-Driven vs. Imitation-Based Learning
RL's exploratory nature outperforms supervised fine-tuning for compositional generalization, particularly for out-of-distribution lengths. RL allows models to discover reasoning paths through trial-and-error with verifiable rewards, while SFT binds the model to imitating demonstrated solutions. SFT exhibits different learning dynamics by pattern structure, suggesting it optimizes for different objectives than structural understanding.

## Foundational Learning

- **Expression Trees and Canonical Patterns**: Why needed here: The entire analysis depends on mapping solutions to tree structures and canonicalizing equivalent expressions. Quick check: Given expression (8÷2)+3, can you draw its tree and identify its canonical pattern signature?

- **Length vs. Compositional Generalization**: Why needed here: The paper's core contribution is disentangling these. Length generalization means applying known procedures to longer inputs; compositional generalization means synthesizing novel structures from known components. Quick check: If a model trained on n=3,4 patterns can solve n=6 problems using only [3]+[3] patterns, is this length generalization, compositional generalization, or both?

- **Group Relative Policy Optimization (GRPO)**: Why needed here: Understanding the training algorithm is essential for interpreting why RL succeeds where SFT fails, and for reproducing results. Quick check: How does GRPO's group-level advantage normalization differ from standard PPO, and what does the paper find about std normalization's effect on response length?

## Architecture Onboarding

- **Component map**: Pattern canonicalization module -> Balanced dataset generator -> GRPO training loop -> Evaluation suite
- **Critical path**: Implement canonical pattern mapping -> Build balanced dataset generator -> Set up GRPO with format reward + correctness reward -> Handle "formatting-only shortcut" with group size warmup -> Track pattern-level metrics
- **Design tradeoffs**: Pattern-based generation eliminates distributional bias but requires more compute per valid example; GRPO vs PPO has entropy tradeoff; larger groups improve exploration stability but hurt final performance; removing std normalization improves performance but changes response length dynamics
- **Failure signatures**: Formatting-only shortcut (response length drops, reward plateaus); over-training on 7B models (performance degrades before epoch completion); missing hard patterns (prevents learning); dataset bias (produces missing patterns and frequency imbalance)
- **First 3 experiments**:
  1. Reproduce held-out pattern experiment: Train on dataset with A/B+C removed (and all 14 dependent patterns). Verify model recovers this pattern. Track whether n=3 subpattern emerges before n=4 dependents.
  2. Ablate group size schedule: Compare three conditions for 1.5B model: (a) G=4 throughout, (b) G=16 throughout, (c) G=16→4 transition. Measure success rate of escaping formatting shortcut and final pattern coverage.
  3. Test structural hierarchy with controlled depth: For n=4, isolate patterns with depth=3 but different structures: [3]÷[1], [2]÷[2], [1]÷[3]. Train separate models on each and compare convergence speed and final precision.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does dynamically augmenting RL post-training with a curriculum of structurally hard instances (e.g., right-heavy patterns) improve sample efficiency compared to training on a random distribution? The paper states this remains a crucial direction for future work.

- **Open Question 2**: Do the structural hierarchies of learnability (specifically the difficulty of right-heavy trees) transfer to non-arithmetic domains like program synthesis or theorem proving? The authors posit this likely extends to other domains but haven't tested it.

- **Open Question 3**: What are the precise limits of generalization when held-out strategies are designed to maximize structural divergence from the training set? The paper acknowledges that a more careful design of held-out strategy will be useful to test the limit of generalization capability.

## Limitations

- The structural analysis depends on the canonical pattern representation being meaningful and consistent, though ablations provide some validation
- The comparison between RL and SFT uses random chain-of-thought reasoning rather than expert demonstrations, raising questions about whether performance gaps stem from exploration mechanism or data quality
- The study focuses on a single task (Countdown), so the extent to which findings generalize to other compositional reasoning tasks remains uncertain

## Confidence

**High Confidence**: The finding that balanced compositional structures are learned more reliably than unbalanced ones, with right-heavy patterns being particularly challenging, is well-supported by multiple experiments and ablation studies.

**Medium Confidence**: The claim that RL enables true compositional synthesis rather than template matching is supported by held-out pattern experiments, but could be strengthened by stricter controls.

**Low Confidence**: The mechanism explaining why right-heavy patterns are harder (lookahead bottleneck) is plausible but not directly tested.

## Next Checks

1. **Structure-Controlled Depth Experiment**: Isolate patterns with identical depth but different structures ([3]÷[1], [2]÷[2], [1]÷[3]) and train separate models on each to provide cleaner evidence for the lookahead bottleneck hypothesis.

2. **Strict Substructure Removal**: In the held-out pattern experiment, remove not just the held-out pattern and its direct dependents, but also all patterns that contain any substructure that could indirectly transfer to the held-out pattern.

3. **Alternative Tree Generation Method**: Compare the pattern-based generation approach with a purely random tree generation method while controlling for distribution uniformity to help isolate whether the structural hierarchy emerges from the generation method itself.