---
ver: rpa2
title: 'Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble
  Methods'
arxiv_id: '2506.01901'
source_url: https://arxiv.org/abs/2506.01901
tags:
- exex
- fine-tuning
- ensemble
- have
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of overadaptation in supervised
  fine-tuning of language models, where fine-tuned models tend to forget knowledge
  from pretraining while potentially overfitting to noisy downstream data. The authors
  demonstrate that ensembling pretrained and fine-tuned models mitigates overadaptation
  by balancing bias (from insufficient fine-tuning) and variance (from overfitting),
  leading to improved performance on both fine-tuning and pretraining tasks.
---

# Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods

## Quick Facts
- arXiv ID: 2506.01901
- Source URL: https://arxiv.org/abs/2506.01901
- Reference count: 40
- This paper studies overadaptation in supervised fine-tuning of language models and demonstrates that ensembling pretrained and fine-tuned models mitigates this problem by balancing bias and variance.

## Executive Summary
This paper addresses the problem of overadaptation in supervised fine-tuning (SFT) of language models, where fine-tuned models tend to forget knowledge from pretraining while potentially overfitting to noisy downstream data. The authors demonstrate that ensembling pretrained and fine-tuned models mitigates overadaptation by balancing bias (from insufficient fine-tuning) and variance (from overfitting), leading to improved performance on both fine-tuning and pretraining tasks. Theoretical analysis in an over-parameterized linear regression setting shows that ensemble models achieve better bias-variance trade-offs than either pretrained or fine-tuned models alone. Empirical validation on instruction-following tasks with Llama-3-8B, Qwen2-7B, and Gemma-2-9B confirms that ensembling improves performance on MT-Bench while mitigating forgetting on MMLU and Commonsense-QA benchmarks.

## Method Summary
The method involves fine-tuning foundation models on downstream tasks with optional regularization (Norm-Penalty or DiffNorm-Penalty), then creating an ensemble by linearly interpolating the weights of the pretrained and fine-tuned models. The interpolation coefficient τ is tuned to balance performance on the fine-tuning task against retention of pretraining knowledge. The approach is validated on three model families (Llama-3-8B, Qwen2-7B, Gemma-2-9B) using the Dolly dataset for fine-tuning and MT-Bench, MMLU, and Commonsense-QA for evaluation. The theoretical framework analyzes the bias-variance trade-off in an over-parameterized linear regression setting, showing conditions under which ensembles outperform both regularized and unregularized fine-tuning.

## Key Results
- Ensembling pretrained and fine-tuned models consistently improves MT-Bench scores while mitigating forgetting on MMLU and Commonsense-QA benchmarks.
- Norm-Penalty regularization (∥θ∥²) consistently outperforms DiffNorm-Penalty (∥θ - θ₁∥²) in almost all settings.
- The ensemble approach achieves strictly better Lpre+Lft trade-offs than regularization alone, though requiring storage of both model weights.
- Weight interpolation with τ typically optimal in [0.5, 0.9] for the tested settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning without regularization causes overadaptation, degrading performance even on the downstream task itself.
- Mechanism: The fine-tuning process on noisy downstream data introduces a harmful "variance" term in the test error. As training extends into the overfitting regime, the model sacrifices the beneficial inductive biases from pretraining, resulting in a net increase in test error. This is modeled theoretically as "ridgeless" regression in an over-parameterized linear setting, which minimizes bias but maximizes variance from data noise.
- Core assumption: The fine-tuning dataset contains non-negligible noise, and the fine-tuning task operates in a "sparse" feature space with limited effective model complexity relative to pretraining.
- Evidence anchors:
  - [abstract] "...overfitting to noisy downstream data...regularization techniques aim to address this trade-off..."
  - [Section 3.3, Figure 1] "In Figure 1, we show the training process...without applying early-stopping. It becomes evident that performance deteriorates quickly with additional epochs, indicating harmful overfitting."
  - [Section 5, Theorem 5.1] "Lft(ˆθ2) < Lft(ˆθ1)" but with high variance from the "ridgeless" estimator causing poor overall performance.
  - [corpus] Related work "Improved Supervised Fine-Tuning...to Mitigate Catastrophic Forgetting" (arXiv:2506.09428) corroborates that SFT often degrades general abilities, though this paper uniquely quantifies the overadaptation mechanism through bias-variance decomposition.
- Break condition: If fine-tuning data is extremely clean or the task shares near-identical structure with pretraining, the variance term may remain small, reducing overadaptation risk.

### Mechanism 2
- Claim: Ensembling pretrained and fine-tuned models via weight interpolation mitigates overadaptation by optimizing the bias-variance trade-off.
- Mechanism: The ensemble estimator ˆθτ = (1-τ)ˆθ1 + τˆθ explicitly balances two error sources: (1) bias from insufficient task-specific adaptation (dominated by the pretrained model) and (2) variance from overfitting to fine-tuning noise (dominated by the fine-tuned model). By tuning the interpolation coefficient τ, the ensemble can achieve lower combined excess risk on both fine-tuning and pretraining tasks than either model alone.
- Core assumption: The pretrained and fine-tuned models share a substantial common feature structure (θc) while differing in task-specific components (α1, α2), and the optimal τ lies in a predictable range (τ′(λ)/2 ≤ τ < 1 for forgetting mitigation).
- Evidence anchors:
  - [abstract] "Ensembling mitigates this by balancing two primary sources of error: bias, caused by insufficient fine-tuning, and variance, introduced by overfitting..."
  - [Section 5, Theorem 5.1] "Lpre(ˆθτλ)+Lft(ˆθτλ) < Lpre(ˆθλ)+Lft(ˆθλ) < Lpre(ˆθ2)+Lft(ˆθ2)" and "Lft(ˆθτλ) < Lft(ˆθλ)" under specified conditions.
  - [Section 6.2] Formal derivation showing the ensemble's excess risk approximation and optimal τ selection through gradient analysis.
  - [corpus] Limited direct corpus evidence; the paper extends prior vision model work (Wortsman et al., 2022b) to LLMs with new theoretical justification.
- Break condition: If the pretraining and fine-tuning tasks are fundamentally incompatible (minimal shared θc), or if the fine-tuned model has catastrophically diverged, interpolation may not provide benefit.

### Mechanism 3
- Claim: Regularization during fine-tuning partially addresses overadaptation but is less effective than ensembling for the combined objective of task performance and knowledge retention.
- Mechanism: Adding a penalty term (∥ˆθ∥² or ∥ˆθ - ˆθ1∥²) during fine-tuning acts as a form of ridge regression, reducing variance at the cost of slightly increased bias. This improves fine-tuning performance compared to unregularized fine-tuning (Lft(ˆθλ) < Lft(ˆθ2)) and helps with forgetting mitigation (Lpre(ˆθλ) + Lft(ˆθλ) < Lpre(ˆθ2) + Lft(ˆθ2)). However, regularization alone cannot achieve the same optimal trade-off as ensembling, which provides an additional mechanism to balance errors.
- Core assumption: The regularization strength λ is chosen within the effective range (0 < λ ≤ 2λ′ where λ′ ≈ eσ²/nζ2), and the regularizer's effect can be approximated through the ridge regression framework.
- Evidence anchors:
  - [Section 1.1] "Applying ridge regression during fine-tuning helps mitigate overfitting by achieving a better balance in the 'bias-variance' trade-off..."
  - [Section 3.3, Table 1] Norm-Penalty and DiffNorm-Penalty variants consistently outperform Vanilla-FT on MT-Bench scores (e.g., Llama-3-8B: 5.84 vs 5.68).
  - [Section 5, Theorem 5.1] Formal comparison showing regularization improves both Lft and Lpre+Lft but is further improved by ensembling.
  - [corpus] "Anchored Supervised Fine-Tuning" (arXiv:2509.23753) explores similar regularization strategies, corroborating the trade-off between memorization and generalization.
- Break condition: If λ is set too high (over-regularization), the model may underfit the downstream task, increasing bias excessively; if too low, it provides insufficient variance reduction.

## Foundational Learning

- Concept: Bias-variance decomposition
  - Why needed here: The entire theoretical framework relies on decomposing test error into bias (systematic error from model assumptions) and variance (error from sensitivity to training data noise). Understanding this trade-off is essential to grasp why ensembling outperforms regularization alone.
  - Quick check question: Can you explain why increasing model complexity typically reduces bias but increases variance, and how this relates to the "ridgeless" vs. ridge regression distinction in the paper?

- Concept: Over-parameterized models and benign overfitting
  - Why needed here: The theoretical analysis assumes an over-parameterized linear regression setting (p >> n) where the pretrained model achieves good generalization through "benign overfitting." This contrasts with the fine-tuning task's "sparse" structure, which exhibits harmful overfitting. Without understanding this duality, the paper's conditions (Condition 2) will be opaque.
  - Quick check question: How can a model that perfectly fits noisy training data still generalize well, and under what conditions does this "benign overfitting" break down during fine-tuning?

- Concept: Weight-space ensembling vs. prediction ensembling
  - Why needed here: The paper focuses on weight interpolation (averaging model parameters), not output averaging. This distinction is critical because weight-space ensembling enables the bias-variance balance at the parameter level, preserving computational efficiency at inference time. Misunderstanding this could lead to incorrect implementation attempts.
  - Quick check question: Why does averaging weights of two models differ fundamentally from averaging their predictions, and what are the computational implications for inference?

## Architecture Onboarding

- Component map:
  - Pretrained model (ˆθ1) -> Fine-tuned model (ˆθ or ˆθλ) -> Ensemble model (ˆθτ)
  - Regularization module (optional) -> Fine-tuned model

- Critical path:
  1. Start with pretrained foundation model
  2. Fine-tune on downstream task with optional regularization (early stopping + penalty)
  3. Compute ensemble weights: ˆθτ = (1-τ)ˆθ1 + τˆθ
  4. Tune τ on validation set (paper searched {0.1, 0.2, ..., 0.9})
  5. Evaluate on both fine-tuning task (e.g., MT-Bench) and pretraining retention tasks (e.g., MMLU, Commonsense-QA)

- Design tradeoffs:
  - τ selection: Higher τ favors fine-tuning performance at the cost of forgetting; lower τ preserves general knowledge but may underperform on the downstream task. Paper found τ ≥ 0.5 generally optimal.
  - Regularization choice: Norm-Penalty outperformed DiffNorm-Penalty in most settings, suggesting penalizing total weight magnitude may be more effective than penalizing deviation from pretrained weights. This is a counterintuitive finding requiring further investigation.
  - Ensemble vs. regularization only: Ensembling provides strictly better Lpre+Lft trade-off but requires storing/accessing both model weights. Regularization is simpler but less effective.
  - LoRA compatibility: Appendix A.3 shows ensembling also benefits LoRA fine-tuning, providing additional performance gains across all benchmarks, though LoRA alone exhibited more forgetting on Commonsense-QA.

- Failure signatures:
  - Forgetting on pretraining tasks: If MMLU/Commonsense-QA scores drop significantly after fine-tuning without ensemble mitigation, this indicates overadaptation.
  - Degraded fine-tuning performance: If MT-Bench scores decrease with extended training (beyond early stopping point), this signals harmful overfitting.
  - No improvement from ensembling: If varying τ doesn't produce performance gains, either the fine-tuning task is too dissimilar from pretraining or the fine-tuned model has already diverged too far.
  - High variance across seeds: If results are unstable, the fine-tuning data may be too small or noisy; consider increasing data or regularization strength.

- First 3 experiments:
  1. **Baseline overadaptation quantification**: Fine-tune your model on the target dataset (e.g., instruction-following) for multiple epochs without early stopping. Plot fine-tuning task performance (e.g., MT-Bench) vs. epoch to confirm the overfitting regime. Concurrently measure pretraining retention (MMLU, Commonsense-QA) to quantify forgetting. This establishes the baseline degradation.
  2. **Regularization ablation**: Fine-tune with both Norm-Penalty and DiffNorm-Penalty, searching λ in {10⁻³, 5×10⁻³, 10⁻²}. Compare against vanilla fine-tuning (with early stopping) to isolate regularization effects. Use validation loss to select the best λ. Document which regularizer performs better on your specific task.
  3. **Ensemble sweep with optimal τ identification**: Take the best regularized model from experiment 2 and ensemble with the pretrained model, sweeping τ ∈ {0.1, 0.2, ..., 0.9}. Evaluate on both fine-tuning and pretraining tasks. Plot the Pareto frontier to visualize the bias-variance trade-off. Compare against regularization-only baselines to quantify ensemble benefit. The paper found τ typically optimal in [0.5, 0.9] for their settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the theoretical analysis of ensembling extend to non-linear neural networks beyond the NTK regime?
- Basis in paper: [inferred] The theoretical framework uses over-parameterized linear regression, with the NTK regime mentioned as a connection to neural networks (Section 4). The linear setting is a simplification that may not fully capture complex non-linear behavior in modern LLMs.
- Why unresolved: The analysis relies on explicit matrix decompositions and eigenvalue structures that may not directly transfer to non-linear settings where feature representations evolve during training.
- What evidence would resolve it: A theoretical extension showing similar bias-variance trade-off benefits in non-linear settings, or empirical evidence demonstrating that the linear theory predictions break down in specific non-linear regimes.

### Open Question 2
- Question: Why does Norm-Penalty (∥θ∥²) consistently outperform DiffNorm-Penalty (∥θ - θ₁∥²), and what is the optimal regularizer design for ensembling?
- Basis in paper: [explicit] "Interestingly, in our experiments, Norm-Penalty consistently outperforms DiffNorm-Penalty in almost all settings, suggesting that the choice of regularizer plays a crucial role and warrants further exploration" (Section 3.3).
- Why unresolved: The paper does not provide theoretical or empirical analysis explaining this observed difference between regularization strategies.
- What evidence would resolve it: Controlled experiments isolating the mechanisms, or theoretical analysis comparing the bias-variance properties induced by different penalty terms.

### Open Question 3
- Question: Can the technical assumptions in Condition 2 be relaxed while preserving the theoretical insights about ensemble benefits?
- Basis in paper: [explicit] "Since the specific order relationships are technical assumptions intended to clarify our theoretical results more clearly, we believe that relaxing them will not compromise our theoretical intuition. We consider this a possible direction for further exploration" (Section 5, Discussion on conditions).
- Why unresolved: The conditions impose specific relationships between sample size n, dimension p, and task difference parameters that may be stricter than necessary.
- What evidence would resolve it: Proof of Theorem 5.1 under weaker assumptions, or counterexamples showing where the theory breaks down when conditions are violated.

## Limitations

- The theoretical framework relies on a linear regression approximation of neural network fine-tuning, which may not fully capture the nonlinear dynamics of modern LLMs.
- The MT-Bench evaluation methodology lacks precise specification, potentially limiting reproducibility.
- The study focuses on instruction-following tasks and may not generalize to all fine-tuning scenarios.

## Confidence

- **High Confidence**: The empirical observation that ensembling improves both fine-tuning performance and pretraining retention is well-supported across multiple models and benchmarks.
- **Medium Confidence**: The theoretical bias-variance decomposition provides useful intuition, but the linear approximation may oversimplify real LLM behavior.
- **Medium Confidence**: The finding that Norm-Penalty outperforms DiffNorm-Penalty is consistently observed but lacks clear theoretical justification.

## Next Checks

1. Extend theoretical analysis to nonlinear settings: Validate whether the bias-variance decomposition holds for small-scale neural network experiments with known analytical properties.

2. Cross-task generalization study: Apply the ensemble approach to fine-tuning on tasks beyond instruction-following (e.g., code generation, mathematical reasoning) to test robustness of the bias-variance trade-off framework.

3. Multi-task fine-tuning comparison: Compare ensemble methods against multi-task fine-tuning approaches to understand whether the benefits stem from knowledge preservation or task-specific optimization.