---
ver: rpa2
title: 'Rethinking BPS: A Utility-Based Evaluation Framework'
arxiv_id: '2505.22316'
source_url: https://arxiv.org/abs/2505.22316
tags:
- process
- framework
- data
- evaluation
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for evaluating business process
  simulation (BPS) models by shifting focus from direct log-to-log comparisons to
  assessing the utility of simulated data in downstream predictive tasks. The authors
  argue that the current standard approach conflates simulation with forecasting and
  relies on Earth Mover's Distance-based metrics that can obscure temporal patterns
  and misrepresent model quality.
---

# Rethinking BPS: A Utility-Based Evaluation Framework

## Quick Facts
- **arXiv ID:** 2505.22316
- **Source URL:** https://arxiv.org/abs/2505.22316
- **Reference count:** 25
- **Key outcome:** Proposes utility-based framework for BPS evaluation that trains PPM models on simulated data and measures downstream task performance differences

## Executive Summary
This paper challenges the standard practice of evaluating business process simulation (BPS) models through direct log-to-log comparisons using Earth Mover's Distance metrics. Instead, the authors propose a utility-based framework that assesses simulation quality by measuring how well predictive process monitoring (PPM) models trained on simulated data perform on downstream tasks compared to those trained on real data. The framework decomposes evaluation into five distinct prediction tasks across control-flow, resource, temporal, and congestion perspectives, enabling targeted diagnosis of BPS model weaknesses. Experiments demonstrate that this approach can distinguish between model accuracy and data complexity, identify specific process perspectives where models fail, and provide more meaningful insights than traditional approaches.

## Method Summary
The framework trains BPS models on real event log training data (L_train) to generate simulated logs (L_sim) that match L_train's case count and temporal characteristics. For evaluation, PPM models are trained separately on both L_train and L_sim for five downstream tasks: Next Activity Prediction (NAP), Next Role Prediction (NRP), Next Processing Time (NPP), Next Waiting Time (NWP), and Remaining Time Prediction (RTP). These models are then evaluated on a hold-out test set (L_test), and utility loss is computed as the absolute difference between task performance metrics achieved on L_train versus L_sim. The framework uses two PPM architectures (ProcessTransformer and LSTM) and averages results over multiple runs with different seeds to ensure stability.

## Key Results
- The utility-based framework successfully distinguishes between model accuracy and data complexity, identifying that high absolute errors may reflect dataset difficulty rather than simulation quality
- AgentSimulator consistently shows strongest performance across control-flow tasks, while no single approach dominates across all perspectives
- Framework enables targeted diagnosis of BPS model weaknesses by revealing which process perspectives (control-flow, resource, temporal, congestion) are poorly captured
- Traditional EMD-based metrics can obscure temporal patterns and conflate simulation with forecasting, whereas utility loss provides clearer quality assessment

## Why This Works (Mechanism)

### Mechanism 1: Utility-Based Evaluation via Downstream Task Performance
- Claim: BPS model quality can be assessed by measuring how well simulated data supports downstream prediction tasks compared to real data.
- Mechanism: The framework trains predictive process monitoring (PPM) models on both real training data (L_train) and simulated data (L_sim), then compares their performance on the same hold-out test set. If models trained on simulated data achieve comparable performance to those trained on real data, the simulation is capturing relevant process patterns.
- Core assumption: Prediction task performance serves as a valid proxy for the practical utility of simulated data in representing the underlying process.
- Evidence anchors:
  - [abstract] "we evaluate whether predictive process monitoring models trained on simulated data perform comparably to those trained on real data for downstream analysis tasks"
  - [section 4] "The core idea is to assess the quality of BPS models by measuring how well their simulated data supports downstream prediction tasks compared to real training data"
  - [corpus] Esteban et al. (2017) introduced TSTR paradigm; the paper adapts this to process data with domain-specific tasks
- Break condition: If downstream tasks do not capture the relevant process characteristics needed for the intended use case, utility loss scores may not reflect true simulation quality.

### Mechanism 2: Perspective-Specific Task Decomposition
- Claim: Decomposing evaluation into multiple prediction tasks across process perspectives (control-flow, resource, temporal, congestion) enables targeted diagnosis of BPS model weaknesses.
- Mechanism: The framework employs five distinct tasks—Next Activity Prediction (NAP), Next Role Prediction (NRP), Next Processing Time (NPP), Next Waiting Time (NWP), and Remaining Time Prediction (RTP)—each capturing different process dimensions. Utility loss is computed per-task rather than aggregated.
- Core assumption: Each task isolates a specific process perspective; errors in one task primarily reflect deficiencies in that