---
ver: rpa2
title: 'Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity'
arxiv_id: '2509.23449'
source_url: https://arxiv.org/abs/2509.23449
tags:
- code
- function
- such
- functions
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of binary code similarity detection
  (BCSD), which is crucial for tasks like malware analysis and vulnerability discovery.
  The paper proposes a novel method that uses large language models (LLMs) to extract
  interpretable features from assembly code, overcoming the limitations of existing
  embedding-based approaches.
---

# Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity

## Quick Facts
- arXiv ID: 2509.23449
- Source URL: https://arxiv.org/abs/2509.23449
- Authors: Charles E. Gagnon; Steven H. H. Ding; Philippe Charland; Benjamin C. M. Fung
- Reference count: 40
- Key outcome: Proposes LLM-generated interpretable features for BCSD, achieving competitive performance without training data, with potential for combined approaches exceeding state-of-the-art.

## Executive Summary
This paper addresses binary code similarity detection (BCSD) by introducing a novel method that leverages large language models (LLMs) to extract human-readable, interpretable features from assembly code. Unlike traditional embedding-based approaches that sacrifice interpretability for performance, this method generates structured JSON representations capturing algorithmic intent, side effects, and input/output types. The approach demonstrates competitive recall rates for cross-architecture and cross-optimization tasks without requiring training data, while maintaining scalability through standard text indexing. When combined with traditional embeddings, the method significantly outperforms existing state-of-the-art BCSD techniques.

## Method Summary
The method uses LLMs to analyze assembly functions and generate structured JSON features describing algorithmic intent, side effects, and data types. These features are then indexed using standard text search engines for similarity detection. The approach employs Gemini 2.5 Flash for feature extraction, with a carefully engineered prompt schema defining 11 specific fields. Assembly functions are first preprocessed to remove comments and normalize formatting, then fed to the LLM with detailed instructions. The resulting JSON features are stored in a database and can be efficiently searched using traditional text indexing methods. The method also explores combining these interpretable features with traditional embeddings to achieve superior performance.

## Key Results
- Achieves 42% recall@1 for cross-architecture tasks without training data
- Achieves 62% recall@1 for cross-optimization tasks without training data
- Combined LLM feature + embedding approach significantly outperforms state-of-the-art BCSD methods
- Demonstrates that accuracy, scalability, and interpretability can coexist in BCSD systems

## Why This Works (Mechanism)
The approach works by leveraging LLMs' natural language understanding capabilities to extract semantic meaning from assembly code, which traditional static analysis tools struggle to capture. By generating human-readable JSON features, the method transforms binary similarity into a text search problem that can leverage efficient indexing techniques. The structured output schema ensures consistency across different assembly functions while capturing the essential semantic properties needed for similarity detection. The combination of interpretable features with traditional embeddings addresses the limitations of both approaches, providing both semantic understanding and mathematical precision.

## Foundational Learning
- **Binary Code Similarity Detection**: Understanding how different compiled functions can be recognized as performing similar operations despite differences in syntax or optimization. Why needed: This is the core problem being addressed.
- **Large Language Model Prompt Engineering**: Crafting specific instructions that guide LLMs to extract relevant features from assembly code. Why needed: The quality of generated features depends entirely on prompt design.
- **Text-based Similarity Search**: Using traditional search engines to find similar text documents efficiently. Why needed: Enables scalable similarity detection without expensive vector operations.
- **Assembly Language Structure**: Understanding the components and conventions of assembly code. Why needed: Essential for designing prompts that can accurately interpret assembly syntax.
- **Feature Engineering for Similarity**: Designing representations that capture semantic similarity between code fragments. Why needed: The quality of features directly determines detection accuracy.
- **Cross-architecture Code Analysis**: Recognizing equivalent functionality across different instruction sets. Why needed: Critical for practical BCSD where binaries may target different platforms.

## Architecture Onboarding

**Component Map**: Assembly Code -> LLM Processor -> JSON Features -> Text Index -> Similarity Search

**Critical Path**: Input assembly function → LLM prompt processing → JSON feature generation → Text indexing → Query matching → Similarity scoring

**Design Tradeoffs**: The method trades computational cost of LLM inference for interpretability and potentially better semantic understanding. While traditional embeddings are faster to compute, LLM features provide human-readable explanations and may capture semantic relationships that embeddings miss.

**Failure Signatures**: Poor similarity detection may occur when LLM features lack sufficient discriminative power, when assembly code is too obfuscated for LLM comprehension, or when the JSON schema fails to capture critical semantic differences between functions.

**First 3 Experiments**:
1. Compare recall@1 scores between LLM features and traditional embeddings on cross-architecture tasks
2. Evaluate the impact of different prompt schemas on feature quality and detection accuracy
3. Test scalability by increasing the size of the function database from 1,000 to 100,000 entries

## Open Questions the Paper Calls Out

**Open Question 1**: Can knowledge distillation effectively transfer the feature extraction capabilities of large commercial LLMs (e.g., Gemini) to smaller local models (e.g., Qwen 2.5 Coder) to reduce hardware requirements without significant accuracy loss?

Basis in paper: Section 5.1 "Future research" explicitly proposes using distillation, where a large model like Gemini 2.5 Flash could be used to fine-tune smaller models to close the performance gap observed in Section 3.4.

**Open Question 2**: Does relaxing the strict JSON output schema in favor of natural language descriptions or basic block analysis improve the trade-off between computational cost and similarity detection accuracy?

Basis in paper: Section 5.1 poses the open problem of finding the best trade-off between interpretability, compute, and detection capabilities, suggesting "sentence similarity" or analyzing "single basic blocks" as alternatives.

**Open Question 3**: How robust is the LLM-based feature extraction method against adversarial obfuscation techniques (e.g., control flow flattening or opaque predicates) compared to standard compiler optimizations?

Basis in paper: The introduction explicitly identifies "adverserial programs" and obfuscation as a challenge in binary analysis, yet the experiments (Section 3) are restricted to cross-optimization (O0-O3) and cross-architecture settings.

## Limitations
- Performance may degrade significantly when scaling to millions of functions due to collision rates in text-based features
- Heavy reliance on commercial LLM APIs creates deployment constraints and potential costs
- Limited evaluation on real-world obfuscated malware and zero-day vulnerability scenarios
- Computational overhead of LLM inference may be prohibitive for large-scale deployment

## Confidence
- Feature interpretability and human-readability: High
- Competitive performance without training: Medium
- Scalability and efficiency improvements: Low
- Combined approach outperforming state-of-the-art: Medium

## Next Checks
1. Conduct comprehensive testing across diverse malware families and vulnerability types to assess generalization capabilities
2. Perform ablation studies to quantify the impact of LLM-generated features versus traditional embeddings on various BCSD tasks
3. Measure and compare the computational overhead and memory requirements of the LLM-based feature extraction approach against existing methods on large-scale binary codebases