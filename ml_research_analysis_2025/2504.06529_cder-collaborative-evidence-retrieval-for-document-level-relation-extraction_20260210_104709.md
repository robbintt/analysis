---
ver: rpa2
title: 'CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction'
arxiv_id: '2504.06529'
source_url: https://arxiv.org/abs/2504.06529
tags:
- entity
- evidence
- pairs
- pair
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CDER, a novel evidence retrieval framework
  for document-level relation extraction (DocRE) that explicitly models the collaborative
  nature among semantically similar entity pairs within the same document. The core
  idea is to construct a bipartite graph with entity pair and sentence nodes, partitioning
  it into three sub-graphs: Entity Pair, Entity Pair-Sentence, and Sentence sub-graphs.'
---

# CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction

## Quick Facts
- **arXiv ID:** 2504.06529
- **Source URL:** https://arxiv.org/abs/2504.06529
- **Reference count:** 26
- **Primary result:** 84.75% Pos Evi F1 on DocRED evidence retrieval, outperforming baselines by 6.6-0.11%

## Executive Summary
CDER introduces a novel evidence retrieval framework for document-level relation extraction that explicitly models collaborative patterns among semantically similar entity pairs. The core innovation is constructing a bipartite graph with entity pair and sentence nodes, partitioned into three sub-graphs, and employing Entity Pair-aware Graph Attention Networks (EP-GATs) for enhanced robustness. CDER achieves state-of-the-art evidence retrieval performance on DocRED while also improving the overall effectiveness of baseline DocRE systems when used as a plug-in component.

## Method Summary
CDER builds a bipartite graph representation of documents, connecting entity pairs with sentences that contain them. The graph is split into three sub-graphs: Entity Pair (GP_P), Entity Pair-Sentence (GP_S), and Sentence (GS_S). CDER employs EP-GATs on GP_P and GP_S to capture collaborative patterns and prioritize informative evidence, while GCNs handle GS_S. A dynamic structure prunes or adds edges in GP_P based on relation similarity computed using TransE embeddings, with threshold θ=0.5. The model is trained end-to-end using Focal Loss to address class imbalance, and serves as a plug-in module for existing DocRE pipelines.

## Key Results
- Achieves 84.75% Pos Evi F1 on DocRED evidence retrieval, outperforming existing methods by 6.6-0.11%
- Improves baseline DocRE systems' F1 scores and Evi F1 metrics when integrated
- Ablation studies show dynamic structure contributes 0.15 F1 and EP-GAT attention weights contribute 0.89-0.90 F1

## Why This Works (Mechanism)

### Mechanism 1
Modeling collaborative patterns among semantically similar entity pairs improves evidence retrieval accuracy by enabling information aggregation that reinforces shared evidence signals during message passing. Entity pairs sharing common entities or correlated relations tend to have overlapping evidence sentence sets, benefiting from shared representations rather than treating each pair independently.

### Mechanism 2
Dynamic edge pruning reduces noise from spurious entity pair connections by computing relevance scores using TransE relation embeddings and removing edges below threshold θ (0.5), or adding edges for semantically similar but unconnected pairs. This prevents unrelated entity pairs from polluting the collaborative modeling process.

### Mechanism 3
Entity Pair-aware attention (EP-GAT) prioritizes informative sentences and entity pairs during aggregation by computing attention weights using localized context embeddings for pair-sentence edges and relational information for pair-pair edges. This enables the model to weight more relevant neighbors higher during message aggregation, improving evidence identification.

## Foundational Learning

- **Graph Attention Networks (GAT)**: Needed for EP-GAT's pair-specific attention computation using localized context; quick check: Can you explain how multi-head attention aggregates neighbor information differently from mean aggregation in GCN?
- **Document-level Relation Extraction (DocRE)**: Needed because CDER is designed to improve DocRE systems by filtering relevant sentences; quick check: What distinguishes DocRE from sentence-level RE in terms of reasoning requirements?
- **Bipartite Graphs**: Needed for the structured representation connecting entity pairs with sentences; quick check: How does message passing differ between bipartite and homogeneous graphs?

## Architecture Onboarding

- **Component map**: BERT-base encoder → Entity/mention embeddings + Sentence embeddings → Bipartite Graph (GP_P + GP_S + GS_S) → EP-GAT layers on GP_P/GP_S; GCN layers on GS_S → Evidence prediction head
- **Critical path**: Document encoding → Graph construction (including dynamic edge pruning on GP_P) → L layers of EP-GAT/GCN → Sentence representation aggregation → Bilinear evidence prediction
- **Design tradeoffs**: Using all entity pairs at inference (|ED|² nodes) vs. positive pairs only at training impacts memory; TransE for relation similarity is pre-computed; Focal loss addresses positive/negative evidence imbalance
- **Failure signatures**: Low Pos Evi Recall may indicate overly aggressive edge pruning or attention suppressing valid evidence; high variance across seeds suggests dynamic structure instability
- **First 3 experiments**: 1) Reproduce baseline ER F1 to validate environment; 2) Ablate dynamic structure (θ=0.0 and 1.0) to observe F1 sensitivity; 3) Test EP-GAT with single-head vs. multi-head attention

## Open Questions the Paper Calls Out

- How can the quadratic expansion of entity pair nodes be mitigated to improve scalability and memory efficiency during inference?
- Does the "collaborative nature" of evidence retrieval observed in general Wikipedia text (DocRED) generalize to specialized domains like biomedicine?
- How sensitive is the dynamic graph structure to errors in the pre-trained TransE embeddings used for relation inference?

## Limitations

- Scalability constraints due to quadratic entity pair expansion requiring batch size 1 at inference
- Domain specificity unknown - methodology only validated on Wikipedia-derived DocRED dataset
- TransE embedding quality uncertainty - no external validation of relation similarity for edge pruning

## Confidence

- **High**: Evidence retrieval improves DocRE performance when CDER is used as a plug-in component (validated by significant F1 improvements on DocRED)
- **Medium**: Dynamic edge pruning with TransE improves evidence retrieval (supported by ablation but lacking external validation)
- **Low**: Collaborative modeling provides consistent benefits across all documents (mechanism assumes semantic similarity exists between many entity pairs, which may not hold in practice)

## Next Checks

1. **TransE Validation**: Test CDER with random relation embeddings instead of TransE to isolate whether the dynamic pruning mechanism provides real benefit or is overfitting to the DocRED dataset structure.
2. **Cross-Dataset Generalization**: Evaluate CDER on a different document-level relation extraction dataset (if available) to verify that collaborative modeling isn't exploiting DocRED-specific artifacts.
3. **Edge Pruning Sensitivity**: Systematically vary the pruning threshold θ from 0.1 to 0.9 to measure stability of F1 scores and identify whether the 0.5 threshold is optimal or overfitted.