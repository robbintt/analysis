---
ver: rpa2
title: 'CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement'
arxiv_id: '2510.12029'
source_url: https://arxiv.org/abs/2510.12029
tags:
- prompts
- prompt
- dataset
- descriptions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# CPR: Mitigating Large Language Model Hallucinations with Cururative Prompt Refinement

## Quick Facts
- arXiv ID: 2510.12029
- Source URL: https://arxiv.org/abs/2510.12029
- Authors: Jung-Woo Shim; Yeong-Joon Ju; Ji-Hoon Park; Seong-Whan Lee
- Reference count: 35
- Primary result: CPR reduces hallucination rates by 20-30% across inference LLMs (GPT-3.5, GPT-4, Llama-2) while improving content quality scores

## Executive Summary
CPR is a plug-and-play prompt refinement framework that mitigates LLM hallucinations by addressing ill-formed or vague user prompts through a two-stage process: cleaning/paraphrasing and context generation. The framework uses a fine-tuned Small Language Model (SLM) to first correct linguistic errors in the prompt, then generate informative task descriptions that constrain the generation space. By reranking descriptions using perplexity and selecting only high-quality context, CPR aligns user intent with model output without requiring retraining of the inference LLM.

## Method Summary
CPR employs a fine-tuned SLM (using LoRA on Llama-2 7B or Phi-2) to perform two sequential operations on user prompts: first cleaning and paraphrasing ill-formed inputs, then generating multiple task descriptions that are reranked by perplexity (threshold τ=15) and the top-k selected. The refined prompt (cleaned text + selected descriptions) is then fed to the inference LLM. The framework is trained on a combined dataset of WikiEn (grammar correction), MQR (paraphrasing), and WikiD (keyword→description) pairs, and evaluated using 8,000 low-quality queries from Google's Well-formed Query dataset.

## Key Results
- CPR reduces Hallucination Index (HI) by 20-30% compared to baseline across GPT-3.5, GPT-4, and Llama-2
- Win Rate improves from ~70% to ~96% when CPR is applied to Llama-2 inference
- Content Quality Score (CQS) increases by 15-25 points across all evaluated models
- Ablation studies confirm both cleaning and description generation are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cleaning and paraphrasing ill-formed inputs reduces hallucinations by shifting the prompt distribution closer to the training data of the inference LLM.
- **Mechanism:** Ill-formed prompts (e.g., typos, ambiguity) act as out-of-distribution inputs, causing the LLM to infer intent incorrectly. By correcting linguistic errors using a fine-tuned SLM, the input aligns with the grammatical structures the LLM was trained on, reducing the need for the model to "guess" user intent.
- **Core assumption:** The inference LLM has been sufficiently trained on well-formed text, and linguistic errors are the primary cause of misalignment between user intent and model output.
- **Evidence anchors:** [abstract] "...poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions." [section I] "...inputs that often act as out-of-distribution examples, which lead LLMs to generate inaccurate or 'hallucinated' content." [corpus] Paper 25034 ("The Illusionist's Prompt") supports the premise that non-formal queries increase factual vulnerability in LLMs.
- **Break condition:** If the prompt is grammatically correct but semantically false or logically incoherent, cleaning mechanisms will not mitigate hallucination.

### Mechanism 2
- **Claim:** Augmenting prompts with generated task descriptions mitigates hallucination by explicitly injecting context that constrains the generation search space.
- **Mechanism:** Ambiguous prompts force LLMs to rely on high-probability but potentially incorrect internal associations. By generating and appending informative descriptions via the SLM, the framework provides "grounding" context, steering the inference model toward relevant knowledge domains.
- **Core assumption:** The small fine-tuned SLM has sufficient parametric knowledge to generate accurate descriptions and does not hallucinate context itself.
- **Evidence anchors:** [abstract] "...generates additional informative task descriptions to align the intention of the user and the prompt..." [section III.D] "Merely cleaning the prompt still leaves it lacking in necessary contextual information... we enhance prompts by adding supplementary contextual information..." [corpus] Paper 66277 ("Multi-stage Prompt Refinement") similarly suggests that prompt refinement stages improve LLM robustness.
- **Break condition:** If the SLM generates plausible but factually incorrect descriptions, it may reinforce or introduce new hallucinations rather than mitigating them.

### Mechanism 3
- **Claim:** Reranking generated descriptions using perplexity filters out low-quality context, ensuring only high-confidence information is appended to the prompt.
- **Mechanism:** Not all generated descriptions are equally accurate. By calculating perplexity (PPL) and selecting only the top-k descriptions below a specific threshold (τ=15), the system filters out incoherent or irrelevant text that could otherwise degrade the inference model's output quality.
- **Core assumption:** Lower perplexity in the SLM generation correlates positively with the factual accuracy and relevance of the context for the downstream inference LLM.
- **Evidence anchors:** [section III.D] "...selecting the top-k based on their average perplexity... reranked to prioritize those with the lowest perplexity..." [algorithm 1] Shows the explicit break condition if PPL(d_i) > τ. [corpus] Corpus evidence on specific perplexity thresholds for hallucination reduction is weak; this appears to be a heuristic specific to this paper's implementation.
- **Break condition:** If the relationship between low perplexity and factuality is weak for a specific domain, the reranker may prioritize fluent but incorrect descriptions.

## Foundational Learning

- **Concept: Instruction Fine-Tuning (IFT)**
  - **Why needed here:** The core of CPR relies on a Small Language Model (SLM) capable of following instructions to "clean" or "describe" rather than just complete text. Understanding IFT explains why the SLM can perform these specific tasks.
  - **Quick check question:** How does the model distinguish between a request to correct grammar versus a request to answer the question?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The paper posits that ill-formed prompts are OOD examples for large inference models. Recognizing OOD behavior is essential to understanding why models hallucinate when facing typos or syntax errors.
  - **Quick check question:** Why would a model trained on internet text struggle with a typo-heavy user query?

- **Concept: Perplexity (PPL)**
  - **Why needed here:** CPR uses perplexity as a proxy for quality to rank and select descriptions. You must understand PPL as a measure of "surprise" or uncertainty to interpret the reranking logic.
  - **Quick check question:** Does a high perplexity score indicate that the model is confident or uncertain about the generated text?

## Architecture Onboarding

- **Component map:** Input Prompt (Ill-formed) -> Refiner (SLM) -> Pipeline Step 1: Cleaning/Paraphrasing -> Pipeline Step 2: Description Generator -> Filter: Perplexity-based Reranker -> Refined Prompt -> Inference LLM

- **Critical path:** The Fine-tuning process (Section III.A & III.B) is the most critical initialization step. If the SLM is not fine-tuned on the specific dataset (WikiEn, MQR, WikiD), the cleaning and description generation capabilities will fail.

- **Design tradeoffs:**
  - **SLM Size vs. Win Rate:** Table I suggests that larger SLMs (e.g., Llama-2 7B) achieve higher Win Rates (96%) compared to smaller SLMs (e.g., Gemma 2B at 92%), but inference costs are higher.
  - **Description vs. No Description:** Ablation studies show that removing descriptions significantly drops performance (e.g., Win Rate drops from 96% to ~78% for Llama-2), indicating a tradeoff between prompt length/latency and accuracy.

- **Failure signatures:**
  - **Looping/Timeout:** If the perplexity threshold (τ) is set too low, the generator may loop indefinitely trying to find a description that meets the criteria.
  - **Context Overload:** Appending too many descriptions (k is too high) might distract the inference LLM or hit context window limits.

- **First 3 experiments:**
  1. **Unit Test (Cleaning):** Feed noisy prompts (e.g., "See from spaiin moroco?") into the SLM and verify it outputs "Is Spain visible from Morocco?" without changing the semantic meaning.
  2. **Threshold Sweep:** Run the description generator on a test set with varying perplexity thresholds (τ) to plot the trade-off between generation latency and description relevance (Coherence/Relevance scores).
  3. **End-to-End A/B Test:** Compare the Inference LLM's output on the Google Well-formed Query dataset (low score subset) using (A) raw prompts vs. (B) CPR-refined prompts, measuring the Hallucination Index (HI) via GPT-3 API evaluation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does CPR performance scale when applied to significantly larger inference models?
  - **Basis in paper:** [explicit] The authors explicitly state that resource constraints limited the evaluation scope and that future work requires "experimenting on a wider range of inference models."
  - **Why unresolved:** The study was restricted by hardware limitations to models like Llama-2 (7B) and GPT-3.5.
  - **What evidence would resolve it:** Benchmarks of Hallucination Index and Win Rate when CPR is applied to frontier models (e.g., GPT-4 or larger Llama variants).

- **Open Question 2:** Can an automated dataset construction process outperform the manual method used for fine-tuning the SLM?
  - **Basis in paper:** [explicit] The conclusion identifies the manual crafting of the dataset as a limitation, suggesting a "better crafted dataset potentially could amplify the effectiveness."
  - **Why unresolved:** The current approach relied on manual verification and construction, which may limit the diversity and robustness of the model's refinement capabilities.
  - **What evidence would resolve it:** A comparative study evaluating SLMs fine-tuned on automated high-quality datasets versus the current manual dataset.

- **Open Question 3:** Why does CPR underperform compared to SelfCheckGPT when applied to minimally ill-formed prompts?
  - **Basis in paper:** [inferred] Table IV reveals that for "Low" ill-formed prompts, CPR achieves a Win Rate of 0.68, which is lower than SelfCheckGPT (0.71).
  - **Why unresolved:** The paper asserts overall success but does not analyze the specific failure mode where description generation might introduce noise to already adequate inputs.
  - **What evidence would resolve it:** An ablation study analyzing the specific impact of description generation on prompts that require minimal cleaning.

## Limitations

- The framework's effectiveness depends on the SLM's ability to generate factually accurate descriptions, but the paper only evaluates linguistic quality (perplexity) rather than factual accuracy of generated context.
- Performance degrades on prompts that are grammatically correct but semantically ambiguous, suggesting the approach primarily addresses syntactic rather than semantic misalignment.
- The perplexity threshold (τ=15) is presented as a fixed hyperparameter without domain-specific validation or sensitivity analysis.

## Confidence

- **High confidence:** The empirical results showing reduced hallucination scores (HI ↓) and improved Content Quality (CQS ↑) across multiple inference LLMs (GPT-3.5, GPT-4, Llama-2). The ablation studies demonstrating the necessity of both cleaning and description generation are methodologically sound.
- **Medium confidence:** The claim that CPR shifts prompts closer to the inference LLM's training distribution. While linguistically coherent, this remains an inference about internal model behavior rather than a directly measured phenomenon.
- **Low confidence:** The universal applicability of the perplexity threshold (τ=15) across domains. The paper presents this as a fixed hyperparameter without domain-specific validation or sensitivity analysis.

## Next Checks

1. **Factual accuracy audit:** Manually verify 50 randomly selected descriptions generated by the SLM for factual correctness rather than just linguistic quality, to test whether low perplexity correlates with truth.
2. **Semantic coherence test:** Evaluate CPR on prompts that are grammatically correct but semantically ambiguous (e.g., "What happened in 1947?") to determine if the framework fails when the issue is meaning rather than form.
3. **Cross-domain threshold validation:** Apply CPR to domains with different linguistic patterns (medical, legal, technical) and test whether τ=15 remains optimal or requires adjustment based on domain perplexity distributions.