---
ver: rpa2
title: 'SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean
  Public Documents'
arxiv_id: '2511.04910'
source_url: https://arxiv.org/abs/2511.04910
tags:
- visual
- retrieval
- multimodal
- page
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SDS KoPub VDR, the first large-scale, public
  benchmark for visual document retrieval (VDR) in Korean public documents. It addresses
  the gap in existing benchmarks by focusing on non-English languages and the structural
  complexity of official publications.
---

# SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents

## Quick Facts
- arXiv ID: 2511.04910
- Source URL: https://arxiv.org/abs/2511.04910
- Reference count: 38
- Primary result: Introduces first large-scale public benchmark for visual document retrieval in Korean, showing 8.4% Recall@5 improvement with multimodal vs text-only retrieval.

## Executive Summary
This paper introduces SDS KoPub VDR, the first large-scale public benchmark for visual document retrieval (VDR) in Korean public documents. It addresses the gap in existing benchmarks by focusing on non-English languages and the structural complexity of official publications. The benchmark comprises 361 real-world documents (40,781 pages) and 600 query-page-answer triples across six domains and three query types (text-based, visual-based, and cross-modal). Two complementary tasks are defined: text-only retrieval and multimodal retrieval, which leverages visual features alongside text. The evaluation reveals substantial performance gaps, particularly in multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art models. The dataset enables rigorous evaluation and provides a roadmap for advancing multimodal AI in real-world document intelligence. The dataset is publicly available at https://huggingface.co/datasets/SamsungSDS-Research/SDS-KoPub-VDR-Benchmark.

## Method Summary
The benchmark defines two tasks: text-only retrieval using parsed text and multimodal retrieval using page images. Documents are preprocessed into PNG images at 300 DPI and text via PyPDF/Docling. 600 query-page-answer triples are generated using LLMs with instruction and persona prompts, then validated through BM25 deduplication, GPT-4.5 semantic verification, and human expert review. Retrieval uses FAISS indexing with cosine similarity. Evaluation metrics include Recall@k and nDCG@k, stratified by domain (6) and query type (3). The dataset is publicly available on Hugging Face.

## Key Results
- Multimodal retrieval improves text-only retrieval by 8.4% in Recall@5 across all query types.
- Visual queries show 28pp gain with multimodal vs text-only index (Recall@3: 0.86 vs 0.58).
- Domain-adapted model SDS-Multimodal-Embedding-7B outperforms general models by over 21% in Recall@5.

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Joint Embedding Space
Encoding document pages as images alongside text captures structural-semantic information lost in OCR-only extraction. Full-page PNG images + PDF-parsed text → unified multimodal embedding via vision-language model → cosine similarity retrieval against text queries projected into shared space. Visual elements (tables, charts, layouts) encode semantics that text extraction fails to preserve.

### Mechanism 2: Query Modality Stratification
Classifying queries by evidence source (Text/Visual/Cross) isolates retrieval capability gaps. Analyze ground-truth evidence location → label as Text (body only), Visual (tables/charts), or Cross (both) → compute per-type Recall@k to diagnose failures. Different evidence modalities require distinct retrieval competencies.

### Mechanism 3: Domain-Adaptive Fine-Tuning
Fine-tuning on target-domain documents improves retrieval over general-purpose multimodal models. Qwen2.5-VL-7B base → multi-stage fine-tuning on Korean public document corpus → specialized representations for government layouts. Public document visual structures (dense tables, policy charts) differ from web-scale pretraining distributions.

## Foundational Learning

- **Concept: Visual Document Retrieval (VDR)**
  - Why needed: Core task requires matching text queries to document pages using both textual and visual evidence, not just OCR output.
  - Quick check: Why does a pie chart showing budget allocation fail to match a query "2024 R&D spending" when using text-only retrieval?

- **Concept: Cross-Modal Embedding Alignment**
  - Why needed: Queries are text but documents are images; retrieval requires a shared embedding space where semantic similarity crosses modalities.
  - Quick check: How does projecting the text query "GDP growth trend" into the same vector space as a line chart enable relevant retrieval?

- **Concept: Late Interaction / Multi-Vector Retrieval**
  - Why needed: Single-vector embeddings may lose fine-grained visual details; multi-vector approaches (ColBERT-style) preserve patch-level information.
  - Quick check: What storage-accuracy tradeoff exists when encoding a document page as 1,000+ patch embeddings vs. a single pooled vector?

## Architecture Onboarding

- **Component map**: PDFs → PNG (300 DPI) + PyPDF text + Docling layout analysis → visual element crops (72 DPI) → "Evidence Candidate Pool" → GPT-4o/Qwen2.5-VL-72B with instruction + persona + dynamic few-shot → 600 triples → BM25 deduplication → GPT-4.5 semantic verification → human expert review → Text-only (PyPDF → text embeddings) vs Multimodal (page image → vision-language embeddings) → FAISS index → cosine similarity → Recall@k, nDCG@k stratified by domain (6) and query type (3)

- **Critical path**: Page preprocessing quality → determines if evidence-rich pages are correctly identified; QA generation prompts → query realism, type distribution, difficulty calibration; Human verification → ground truth reliability; Embedding model selection → text-only vs multimodal, general vs domain-adapted

- **Design tradeoffs**: Scale vs. depth (600 verified triples enable controlled analysis but limit robustness testing); Single-hop vs. multi-hop (current benchmark targets single-page evidence; real policy analysis requires cross-page synthesis); LLM-generated vs. user-sourced queries (ensures consistency but may miss colloquial diversity)

- **Failure signatures**: Text-only models collapse on Visual queries (SDS model Recall@1: 0.58 multimodal → 0.30 text-only); General multimodal models underperform on Korean government layouts without domain tuning; Cross-modal queries expose vision-language alignment gaps; Non-informative pages (covers, TOCs) must be filtered or they dilute retrieval precision

- **First 3 experiments**: 1) Reproduce baselines: Run BGE-M3 (text-only) and Jina-Embeddings-v4 (multimodal) on the 600 queries; compute Recall@1/3/5/10 by query type to validate published numbers. 2) Modality ablation: For SDS-Multimodal-Embedding-7B, compare (a) text-only input, (b) image-only input, (c) combined multimodal input on Visual queries to isolate visual contribution. 3) Domain transfer gap: Evaluate Jina-v4 (no Korean public doc training) vs. SDS model on held-out documents from an unseen domain (e.g., healthcare policy) to quantify domain adaptation benefit vs. generalization risk.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do state-of-the-art visual retrieval models perform on multi-hop queries requiring cross-page reasoning or multi-document synthesis in Korean public administrative contexts? The authors identify the current focus on single-hop queries as a limitation, noting that real-world scenarios frequently require synthesizing information scattered across multiple pages or documents. The current benchmark restricts ground truth evidence to single pages, making it impossible to evaluate a model's ability to aggregate information from disparate sources.

- **Open Question 2**: To what extent does retrieval performance on the benchmark generalize to colloquial or noisy queries generated by real users rather than LLMs? The authors acknowledge that while LLM-generated queries (GPT-4o) are factually accurate, they may not capture the "diversity and unpredictable nature of colloquial queries made by real users." The benchmark relies on synthetic queries and lacks actual user interaction logs or crowdsourced questions to test robustness against natural language variance.

- **Open Question 3**: Does improved retrieval performance on visual and cross-modal queries directly correlate with reduced hallucination and higher factual accuracy in downstream Multimodal RAG generation? The paper outlines a long-term goal to evolve the benchmark into an "End-to-End Multimodal RAG evaluation framework" that assesses the entire pipeline from retrieval to answer generation. The current study evaluates retrieval (Recall@k, nDCG@k) in isolation, but does not quantify the impact of visual retrieval errors on the final generated output quality.

## Limitations

- Limited query volume (600 triples) constrains statistical robustness and may not capture full domain variability.
- Ground truth generation relies on LLM-generated queries with human verification, which may introduce subtle bias compared to naturally occurring user queries.
- Custom fine-tuned model (SDS-Multimodal-Embedding-7B) achieves top performance but is not publicly available, limiting reproducibility of absolute scores.
- Single-hop retrieval design misses real-world multi-page policy analysis scenarios requiring cross-document synthesis.

## Confidence

- **High**: Multimodal retrieval consistently outperforms text-only retrieval (Mechanism 1 supported by Section 5.4.2 showing 8.4% Recall@5 improvement)
- **High**: Query modality stratification effectively isolates retrieval capability gaps (Mechanism 2 supported by Section 5.4.4 showing 28pp gain on Visual queries)
- **Medium**: Domain-adaptive fine-tuning yields substantial gains (Mechanism 3 supported by Section 5.4.3 showing 21% Recall@5 improvement, but custom model unavailable for verification)

## Next Checks

1. Reproduce baselines: Run BGE-M3 (text-only) and Jina-Embeddings-v4 (multimodal) on the 600 queries; compute Recall@1/3/5/10 by query type to validate published numbers.
2. Modality ablation: For SDS-Multimodal-Embedding-7B, compare (a) text-only input, (b) image-only input, (c) combined multimodal input on Visual queries to isolate visual contribution.
3. Domain transfer gap: Evaluate Jina-v4 (no Korean public doc training) vs. SDS model on held-out documents from an unseen domain (e.g., healthcare policy) to quantify domain adaptation benefit vs. generalization risk.