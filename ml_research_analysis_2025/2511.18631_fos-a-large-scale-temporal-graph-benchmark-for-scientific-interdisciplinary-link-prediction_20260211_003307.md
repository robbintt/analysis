---
ver: rpa2
title: 'FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary
  Link Prediction'
arxiv_id: '2511.18631'
source_url: https://arxiv.org/abs/2511.18631
tags:
- temporal
- graph
- edges
- node
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FOS is a large-scale temporal graph benchmark for forecasting\
  \ interdisciplinary scientific emergence. It represents 65,027 research fields across\
  \ 19 domains as a yearly co-occurrence graph from 1827\u20132024, with edges timestamped\
  \ by publication year and enriched with semantic embeddings."
---

# FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction

## Quick Facts
- arXiv ID: 2511.18631
- Source URL: https://arxiv.org/abs/2511.18631
- Reference count: 40
- FOS is a large-scale temporal graph benchmark for forecasting interdisciplinary scientific emergence.

## Executive Summary
FOS introduces a challenging temporal link prediction benchmark for forecasting emerging interdisciplinary research directions. The benchmark represents 65,027 research fields across 19 domains as yearly co-occurrence graphs from 1827-2024, enriched with semantic embeddings from field descriptions. The task is to predict novel cross-field links that signal emerging research directions. We evaluate state-of-the-art temporal GNNs under random, inductive, and historical negative-sampling regimes, finding that semantic field descriptions and extended training history are critical for accurate predictions.

## Method Summary
The FOS benchmark constructs yearly co-occurrence graphs from publication data, where nodes represent research fields and edges represent co-occurrences within publications. Each node is enriched with five semantic features derived from hierarchical taxonomy and textual descriptions, embedded using AllenAI/SPECTER and reduced to 100 dimensions via PCA. The task is formalized as temporal link prediction with three evaluation regimes: random negatives (easy), historical negatives (distinguishing from past patterns), and inductive negatives (generalizing to unseen structures). Models are trained on 2002-2017 data and evaluated on 2018-2024, with performance measured by AP and AUC-ROC.

## Key Results
- DyGFormer achieves highest performance under random sampling (AP=0.923, AUC-ROC=0.934)
- Performance declines significantly under harder evaluation regimes (historical: AP drops to 0.606 for DyGFormer)
- Semantic field descriptions provide the strongest predictive signal, with 2.7% AP drop when removed
- Extended training history improves forecasting accuracy, with diminishing returns beyond 15 years

## Why This Works (Mechanism)

### Mechanism 1: Semantic Description Embeddings Capture Latent Field Compatibility
Long-form textual field descriptions provide the strongest predictive signal by encoding rich conceptual semantics that identify semantically compatible field pairings before they materialize as publications.

### Mechanism 2: Extended Temporal History Enables Path-Dependent Forecasting
Longer training history improves forecasting accuracy by capturing non-Markovian dynamics and rare long-range temporal dependencies in scientific field relationships.

### Mechanism 3: Negative-Sampling Regime Determines Optimal Architecture
Different temporal GNN architectures excel under different evaluation regimes—DyGFormer for random sampling, TGAT for inductive scenarios, GraphMixer for historical negatives—due to distinct inductive biases.

## Foundational Learning

- **Concept: Temporal Link Prediction**
  - Why needed here: Core task is predicting whether two research fields will co-occur at a future timestamp given historical graph snapshots
  - Quick check question: Can you explain the difference between "recurrent links" (τ(u,v) ≤ t) and "emerging links" (t < τ(u,v)) as defined in the paper?

- **Concept: Negative Sampling Strategies**
  - Why needed here: Three regimes (random, historical, inductive) test different generalization capabilities
  - Quick check question: For a positive edge (u, v, t), what distinguishes a historical negative from an inductive negative?

- **Concept: Temporal Graph Neural Network Architectures**
  - Why needed here: Six architectures compared with different temporal encoding and memory strategies
  - Quick check question: Why does TGAT perform better under inductive evaluation than DyGFormer despite DyGFormer's higher random-sampling scores?

## Architecture Onboarding

- **Component map**: OpenAlex knowledge graph → hierarchical concept taxonomy → annual co-occurrence graphs → semantic feature engineering → temporal GNN processing → link scoring head → evaluation
- **Critical path**: Map publications to concept nodes → generate annual co-occurrence graphs → compute semantic embeddings → PCA reduction → chronological split → train temporal GNN → evaluate under three regimes
- **Design tradeoffs**: Semantic richness vs. dimensionality; history length vs. relevance; model complexity vs. regime robustness
- **Failure signatures**: Memorization trap (EdgeBank AP=0.77), regime collapse (>20% performance drop), feature ablation sensitivity
- **First 3 experiments**: 1) Baseline DyGFormer with full features on random sampling, 2) Feature ablation without desc feature, 3) History sensitivity with truncated training windows

## Open Questions the Paper Calls Out

- **Open Question 1**: Do performance rankings generalize to high-growth domains like Computer Science or Biology?
  - Basis: Paper plans to extend experiments to additional domains
  - Why unresolved: Evaluation restricted to Art & Business for tractability
  - Evidence needed: Benchmarking results on Computer Science and Biology subsets

- **Open Question 2**: How to distinguish genuine interdisciplinary emergence from taxonomy update artifacts?
  - Basis: Current timestamping may conflate field births with research convergence
  - Why unresolved: Method assigns edges based on later subfield's introduction
  - Evidence needed: Modified evaluation metric accounting for subfield maturation intervals

- **Open Question 3**: Can semantic embeddings + extended history close performance gap in historical regime?
  - Basis: Models show significant performance drops in historical regime despite identified critical features
  - Why unresolved: Unclear if semantic enrichment + history is sufficient for difficult negatives
  - Evidence needed: Ablation studies specifically targeting historical regime

## Limitations

- Semantic feature quality depends on availability of structured metadata and field descriptions
- Temporal dynamics assumptions may break with paradigm shifts creating discontinuous relationships
- Negative-sampling regimes require careful balance to avoid artificial performance inflation

## Confidence

- **High Confidence**: Semantic description embeddings significantly improve prediction accuracy (validated by ablation showing 2.7% AP drop)
- **Medium Confidence**: Extended temporal history improves forecasting (supported by monotonic performance decline in Figure 2)
- **Medium Confidence**: No single model dominates across regimes (confirmed by regime-specific performance patterns)

## Next Checks

1. Run ablation studies removing each semantic feature individually to quantify relative contributions
2. Evaluate model performance across varying training window lengths to map diminishing returns curve
3. Test model transferability from FOS_art&business to other domain subsets to assess domain-specific limitations