---
ver: rpa2
title: Fundamental Principles of Linguistic Structure are Not Represented by o3
arxiv_id: '2502.10934'
source_url: https://arxiv.org/abs/2502.10934
tags:
- murphy
- structure
- sentence
- language
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: o3-mini-high fails basic linguistic structure tasks, struggling
  with phrase structure, Escher sentences, and generating ungrammatical outputs. It
  misidentifies grammatical sentences as ungrammatical and vice versa, lacks compositional
  reasoning, and incorrectly handles syntactic violations.
---

# Fundamental Principles of Linguistic Structure are Not Represented by o3

## Quick Facts
- arXiv ID: 2502.10934
- Source URL: https://arxiv.org/abs/2502.10934
- Authors: Elliot Murphy; Evelina Leivada; Vittoria Dentella; Fritz Gunther; Gary Marcus
- Reference count: 0
- Key outcome: o3-mini-high fails basic linguistic structure tasks, struggling with phrase structure, Escher sentences, and generating ungrammatical outputs.

## Executive Summary
o3-mini-high demonstrates fundamental limitations in representing linguistic structure, failing tasks that require hierarchical compositionality while succeeding on surface-level sequential patterns. The model cannot reliably parse nested structures, generate syntactic violations, or distinguish between semantic anomaly and syntactic unacceptability. These results suggest that current large language models, despite their size and reasoning capabilities, have not overcome the compositional language barrier that has long challenged artificial intelligence.

## Method Summary
The paper evaluates o3-mini-high on 26 prompts testing various aspects of linguistic competence, including phrase structure, grammaticality judgments, Escher sentences, center-embedding, and syntactic violation generation. All prompts were run through OpenAI's o3-mini-high via ChatGPT interface between January 31 and February 6, 2025. The evaluation relies on descriptive assessment of model responses without systematic statistical analysis, examining whether the model can handle tasks requiring hierarchical structure versus linear statistics.

## Key Results
- o3-mini-high fails basic phrase structure tasks, hallucinating elements when drawing syntactic trees for center-embedded sentences
- The model cannot generate genuinely ungrammatical sentences when explicitly requested, instead producing semantically surreal but syntactically well-formed outputs
- It struggles with distinguishing syntax from semantics, conflating semantic anomaly with syntactic unacceptability in both generation and judgment tasks

## Why This Works (Mechanism)

### Mechanism 1: Linear Statistical Pattern Matching
- **Claim:** o3 succeeds on tasks reducible to surface-level sequential statistics but fails when hierarchical composition is required.
- **Mechanism:** The model appears to compute transitional probabilities and lexical co-occurrence patterns rather than building abstract phrase structure representations. When a task can be solved via linear token relationships, performance is adequate; when it requires representing nested constituent structure or evaluating multiple possible parses, the model defaults to statistical associations.
- **Core assumption:** The model's next-token prediction training creates strong biases toward surface statistics over hierarchical structure, and additional "reasoning modules" do not override this.
- **Evidence anchors:**
  - [abstract]: "while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules"
  - [section 3.4]: The model hallucinates an additional 'met' when drawing syntactic trees for center-embedded sentences, indicating it cannot faithfully map string input to hierarchical structure
  - [corpus]: Related work (paper ID 104706) notes that probing for syntax "fails to explain performance on targeted syntactic evaluations," suggesting surface features drive behavior
- **Break condition:** If o3 were provided explicit symbolic structure as input and could then perform compositional operations correctly, this mechanism would be insufficient.

### Mechanism 2: Mono-Configurational Parse Locking
- **Claim:** The model cannot simultaneously represent multiple syntactic parses to evaluate against alternative semantic interpretations.
- **Mechanism:** When tasked with generating syntactic violations, the model commits to a single parse interpretation and cannot "step back" to consider whether alternative structural readings exist. It identifies sentences as ungrammatical based on one assumed configuration, failing to recognize that the same string may be acceptable under a different parse.
- **Core assumption:** This reflects a fundamental architectural constraint rather than prompting inadequacy—even after multiple correction attempts, the model could not generate violations causally linked to properties of a first sentence's syntax.
- **Evidence anchors:**
  - [abstract]: "When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations"
  - [section 3.7, Table 1]: Across 6 successive prompts requesting specific violation types, the model never satisfied all three constraints (unacceptable structure, multiple violation types, causally driven violation)
  - [corpus]: Weak/missing—corpus neighbors focus on compositionality outcomes but not specifically on multi-parse representation
- **Break condition:** If restructuring the task as a multi-turn debate enabled success, the constraint would be task-formatting rather than architectural.

### Mechanism 3: Semantic-Structural Conflation
- **Claim:** The model cannot reliably distinguish between semantic anomaly and syntactic unacceptability.
- **Mechanism:** When asked to generate ungrammatical sentences, the model produces semantically surreal but syntactically well-formed outputs. When asked to judge acceptability, it rates grammatical sentences with unusual interpretations as ungrammatical. This suggests the model's "acceptability" sense is derived from semantic typicality rather than structural well-formedness.
- **Core assumption:** Training on text corpora creates a conflated signal where structural and semantic "oddness" are not distinguished.
- **Evidence anchors:**
  - [abstract]: "it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs"
  - [section 3.6, Prompt 15]: Model's output "may be rather Joycean and surreal in its content, but it does not satisfy the clear instruction to be ungrammatical"
  - [section 3.10]: Model correctly identifies blatantly unacceptable sentences but struggles with partially acceptable ones (only 1/10 correctly classified), suggesting gradient acceptability sensitivity is absent
- **Break condition:** If explicit metalinguistic framing dramatically improved performance, the conflation would be attentional rather than representational.

## Foundational Learning

- **Concept: Phrase Structure Rules**
  - Why needed here: The paper's core argument hinges on the distinction between linear word sequences and hierarchical constituent structure (e.g., [NP [Det N] VP]). Without this, you cannot understand why "Dogs dogs dog dog dogs" is grammatical or why center-embedding creates processing demands.
  - Quick check question: Can you draw a bracketed tree for "The cat the dog chased ran"?

- **Concept: Acceptability Gradients**
  - Why needed here: The paper exploits the fact that linguistic acceptability is not binary—sentences can be "partially acceptable" (?-marked). The model's failure to capture this gradient is key evidence for its non-human-like linguistic competence.
  - Quick check question: Why might "The salmon was fast and delicious" receive a lower acceptability rating than "The salmon was fast and it was delicious"?

- **Concept: Compositional Semantics**
  - Why needed here: The paper argues o3 lacks "grounded compositional abstractions"—the ability to combine meanings of parts via rules to derive meanings of wholes. This distinguishes statistical pattern-matching from true compositional understanding.
  - Quick check question: If "glart" means both "alien creature" and "to please," how would composition determine whether "Glarts glarts glart" is meaningful?

## Architecture Onboarding

- **Component map:** Token prediction core -> Reasoning modules (o3-specific) -> Output generation via token prediction
- **Critical path:** Input prompt → tokenization → statistical pattern retrieval (surface co-occurrences) → reasoning module engagement → output generation via token prediction → Failure point: No stage represents hierarchical structure independently of surface statistics
- **Design tradeoffs:** Statistical approach: Robust to noise, handles frequent patterns well, requires massive data; Symbolic approach: Handles recursion, compositionality, and edge cases, but requires explicit rule engineering; Current o3: Maximizes statistical coverage at the cost of compositional generalization
- **Failure signatures:** Accepts ungrammatical strings with plausible word-level statistics; Generates grammatical sentences when explicitly asked for ungrammatical ones; Cannot explain why sentences are partially acceptable—only binary judgments; Tree drawings do not match input strings (hallucinated elements)
- **First 3 experiments:**
  1. **Bracketed input test:** Provide sentences with explicit constituency brackets and test whether o3 can perform compositional operations. If performance improves, the bottleneck is structure representation; if not, it's compositional computation.
  2. **Multi-parse forced-choice:** Present sentences like "John likes Mary's picture of himself" and ask o3 to generate all possible binding interpretations with explicit structural justifications. Measure whether it can enumerate alternatives.
  3. **Violation generation with verification loop:** Ask o3 to generate an ungrammatical sentence, then immediately ask it to verify the ungrammaticality with reference to a specific rule. Track whether it catches its own errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would alternative output formats (e.g., formalized symbolic languages) reveal different syntactic representation capabilities in LLMs than natural language tree-drawing interfaces?
- **Basis in paper:** [explicit] "Future work could attempt to have o3 output distinct types of configurational representations, perhaps via formalized languages that may be more approximate to native features of the model" (p. 35)
- **Why unresolved:** The authors acknowledge their tree-drawing failure results may reflect interface limitations rather than syntactic competence deficits.
- **What evidence would resolve it:** Testing o3 and similar models using formalized output formats (e.g., bracketed notation, LISP-style trees) to assess whether syntactic structure improves when interface constraints are removed.

### Open Question 2
- **Question:** Which specific sub-components of syntax can LLMs reliably acquire, and which design features (beyond compute scaling) would enable improved performance?
- **Basis in paper:** [explicit] "Future research should more carefully apply the tools of linguistics to isolate specific sub-components of syntax that might be in principle achievable by language models, given specific design features" (p. 37)
- **Why unresolved:** The current study shows broad failures but does not systematically isolate which syntactic operations are tractable versus intractable for current architectures.
- **What evidence would resolve it:** Targeted experiments decomposing complex syntactic phenomena into atomic operations, combined with architectural interventions to test which design features enable success.

### Open Question 3
- **Question:** How would systematic large-scale testing with direct human performance comparisons alter conclusions about LLM linguistic competence?
- **Basis in paper:** [explicit] "A related caveat is that we have no direct human performance scores to directly make claims about certain 'human-level' performance, which will be needed to make such comparisons" (p. 35)
- **Why unresolved:** The study relies on assumptions about human performance without empirical baseline data from the same prompts.
- **What evidence would resolve it:** Collecting human judgment data on the same linguistic tasks to enable direct quantitative comparison.

## Limitations

- The evaluation relies entirely on descriptive judgments without systematic statistical validation or variance measures
- Results may reflect sensitivity to stochastic generation rather than fundamental representational limits due to unspecified temperature and sampling parameters
- The paper does not control for potential priming effects from previous conversational turns in the ChatGPT interface

## Confidence

**High Confidence:** The model's inability to perform basic phrase structure tasks (center-embedding, tree drawing with hallucinated elements) and its consistent failure to generate syntactically ill-formed sentences when explicitly requested.

**Medium Confidence:** The claim that o3 lacks "grounded compositional abstractions" and cannot distinguish syntax from semantics, though alternative explanations involving prompt engineering limitations cannot be definitively ruled out.

**Low Confidence:** The assertion that this represents a "stubbornly resilient wall" in deep learning architecture, as the evaluation examines only one model variant without comparing against alternative architectures.

## Next Checks

1. **Multi-run validation test:** Repeat each of the 26 prompts five times with fixed temperature and top-p parameters, measuring inter-run consistency. If failure patterns persist across runs, this strengthens claims of architectural limitations; if results vary dramatically, the conclusions may reflect sampling artifacts rather than fundamental deficits.

2. **Explicit structure input test:** Modify Prompts 1-26 to include explicit constituency brackets for all sentences, then retest phrase structure and violation generation tasks. Improved performance with bracketed input would indicate the bottleneck is structure representation rather than compositional reasoning.

3. **Cross-model comparison test:** Replicate the full evaluation battery on both GPT-4 and Claude 3.5 Sonnet using identical prompts and parameters. Systematic differences in performance across models would help isolate whether the observed failures are specific to o3's architecture or represent broader limitations in current large language models.