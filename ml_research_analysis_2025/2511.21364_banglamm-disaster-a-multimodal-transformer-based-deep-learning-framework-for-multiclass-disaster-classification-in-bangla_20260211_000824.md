---
ver: rpa2
title: 'BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework
  for Multiclass Disaster Classification in Bangla'
arxiv_id: '2511.21364'
source_url: https://arxiv.org/abs/2511.21364
tags:
- disaster
- multimodal
- bangla
- classification
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents BanglaMM-Disaster, a multimodal deep learning
  framework for classifying disaster-related social media content in Bangla. The approach
  combines transformer-based text encoders (BanglaBERT, mBERT, XLM-RoBERTa) with CNN
  image backbones (ResNet50, DenseNet169, MobileNetV2) using early fusion.
---

# BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla

## Quick Facts
- arXiv ID: 2511.21364
- Source URL: https://arxiv.org/abs/2511.21364
- Authors: Ariful Islam; Md Rifat Hossen; Md. Mahmudul Arif; Abdullah Al Noman; Md Arifur Rahman
- Reference count: 16
- Primary result: 83.76% accuracy achieved with mBERT + ResNet50 fusion for Bangla disaster classification

## Executive Summary
This study introduces BanglaMM-Disaster, a multimodal deep learning framework that combines transformer-based text encoders with CNN image backbones for multiclass disaster classification in Bangla social media posts. The framework employs early fusion to concatenate text and visual features before classification, achieving 83.76% accuracy across nine disaster categories. The authors created a novel dataset of 5,037 Bangla posts with captions and images, demonstrating that multimodal fusion outperforms both text-only (79.92%) and image-only (66.85%) baselines by 3.84% and 16.91% respectively. This work addresses a critical gap in Bangla disaster response capabilities for low-resource language settings.

## Method Summary
BanglaMM-Disaster uses early fusion to combine transformer text encoders (BanglaBERT, mBERT, XLM-RoBERTa) with CNN visual encoders (ResNet50, DenseNet169, MobileNetV2). The framework processes Bangla social media posts containing both captions and images, extracting [CLS] token embeddings from transformers (768-dim) and global average pooled features from CNNs (2048-dim for ResNet50), then concatenating these representations for classification. The dataset comprises 5,037 posts split 70/10/20 for training, validation, and testing. Text preprocessing includes cleaning, Banglish-to-Bangla translation via Google Translate API, and WordPiece tokenization. Image preprocessing involves resizing to 224×224, normalization, and augmentation (horizontal flip, rotation ±15°, zoom 0.8-1.2x). Models are trained with Adam optimizer using learning rates of 1e-5 for text encoders and 3e-5 for fusion layers, with dropout 0.1 and early stopping.

## Key Results
- Multimodal fusion achieved 83.76% accuracy, outperforming text-only baseline by 3.84% and image-only baseline by 16.91%
- mBERT + ResNet50 combination delivered optimal performance across all nine disaster categories
- Error analysis showed consistent improvements across all classes, with largest gains for Fires (error reduction from 45.3% to 22.7%) and Economic Loss categories
- Framework requires 1.8GB memory and achieves 0.45s inference time per sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early fusion of text and visual features enables cross-modal disambiguation for disaster classification
- Mechanism: Concatenating transformer-based text embeddings (d_text = 768) with CNN visual features (d_visual = 2048) allows the final classifier to learn joint patterns where one modality compensates for ambiguities in the other. Visual evidence confirms or corrects textual descriptions.
- Core assumption: Text and image modalities provide complementary disaster-relevant information that is jointly more informative than either alone.
- Evidence anchors:
  - [abstract]: "surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%"
  - [section V-D]: "Fires (FR) class error drops from 45.3% (visual-only) and 28.4% (text-only) to 22.7% with multimodal fusion"
  - [corpus]: Weak corpus evidence—neighbor papers focus on different fusion paradigms (swarm optimization, graph-based) rather than direct validation of early fusion for disaster tasks.
- Break condition: When text and image are semantically unrelated or contradictory (e.g., generic caption paired with unrelated disaster image), early fusion may introduce noise rather than signal.

### Mechanism 2
- Claim: Transfer learning from ImageNet-pretrained CNNs provides usable visual features for disaster scene understanding despite domain shift
- Mechanism: Removing classification heads and applying global average pooling extracts hierarchical features—from edges/textures to semantic concepts—that remain partially transferable to disaster imagery.
- Core assumption: Low-level and mid-level visual patterns learned on ImageNet (edges, textures, object parts) generalize to disaster scenes.
- Evidence anchors:
  - [section IV-C]: "These models trained on ImageNet offer stable feature representations for disaster images despite domain mismatch"
  - [section V-A1]: ResNet50 achieved 66.85% accuracy, substantially above random (11.1% for 9 classes)
  - [corpus]: Neighbor paper "BRIGHT" validates building damage assessment using pretrained visual backbones, supporting transferability to disaster domains.
- Break condition: Highly abstract or context-dependent disaster classes (e.g., "Economic Loss") lack distinctive visual signatures, limiting CNN effectiveness alone.

### Mechanism 3
- Claim: Multilingual transformer encoders capture sufficient Bangla semantic structure for disaster classification even with limited domain-specific pretraining
- Mechanism: Multi-head self-attention over subword tokens produces contextual representations; the [CLS] token aggregates sentence-level semantics for classification.
- Core assumption: Pretraining on large multilingual corpora transfers adequately to informal Bangla social media text about disasters.
- Evidence anchors:
  - [section V-A2]: XLM-RoBERTa achieved 79.92% accuracy, outperforming BanglaBERT (76.73%)
  - [section IV-B]: WordPiece tokenization handles out-of-vocabulary words common in social media
  - [corpus]: Neighbor "Transformer-Driven Triple Fusion Framework" reports similar findings for Bangla multimodal tasks, reinforcing transformer effectiveness.
- Break condition: Heavy use of Banglish (Romanized Bangla), misspellings, or disaster-specific jargon not well-represented in pretraining data may degrade performance.

## Foundational Learning

- Concept: **Early Fusion vs. Late Fusion**
  - Why needed here: The paper explicitly chooses early (feature-level) fusion over late (decision-level) fusion for computational efficiency and cross-modal interaction learning.
  - Quick check question: If you fused at the prediction level instead of feature level, would the model still learn that "flood" text + water image are jointly predictive?

- Concept: **Transfer Learning with Domain Shift**
  - Why needed here: CNNs pretrained on ImageNet (natural images) are applied to disaster images—a significant domain shift that partially works but leaves room for improvement.
  - Quick check question: Why does ResNet50 still achieve 66.85% on disaster images despite never seeing disaster examples during ImageNet pretraining?

- Concept: **Subword Tokenization for Low-Resource Languages**
  - Why needed here: Bangla social media contains informal expressions, Banglish, and rare words; WordPiece tokenization prevents OOV issues.
  - Quick check question: What happens to model performance if you use word-level tokenization instead of subword tokenization on informal Bangla text?

## Architecture Onboarding

- Component map:
  - Bangla caption (tokenized) → BanglaBERT/mBERT/XLM-RoBERTa → [CLS] embedding (768-dim)
  - Image (224×224×3) → ResNet50/DenseNet169/MobileNetV2 → GlobalAvgPool output (2048-dim)
  - [CLS] embedding + CNN features → Concatenation → FC + Dropout(0.1) → Softmax(9 classes)

- Critical path:
  1. Preprocess text (clean, translate Banglish, tokenize)
  2. Preprocess image (resize, normalize, augment)
  3. Extract features via frozen/unfrozen encoders
  4. Concatenate features at representation level
  5. Train fusion classifier with different learning rates (1e-5 for encoders, 3e-5 for fusion)

- Design tradeoffs:
  - Early fusion chosen over late fusion for computational efficiency (0.45s inference, 1.8GB memory)
  - CNNs chosen over Vision Transformers for real-time deployment feasibility
  - mBERT selected over BanglaBERT despite lower native Bangla support—possibly due to better generalization on informal text

- Failure signatures:
  - **Class confusion between visually similar categories**: Fires vs. Economic Loss; Infrastructure Damage vs. Landslides
  - **Modality mismatch**: Caption describes flood but image shows people (Non Damage class)
  - **Overfitting on small classes**: Fires (6% of data) shows 45.3% error in image-only model

- First 3 experiments:
  1. **Unimodal baselines**: Train text-only (XLM-RoBERTa) and image-only (ResNet50) models separately to establish performance floors (target: ~80% and ~67% accuracy respectively).
  2. **Ablation across encoder combinations**: Test all 9 combinations of {BanglaBERT, mBERT, XLM-RoBERTa} × {ResNet50, DenseNet169, MobileNetV2} to identify optimal pairing (expect mBERT + ResNet50 to lead).
  3. **Per-class error analysis**: Compute confusion matrices and error rates per class to identify where multimodal fusion helps most (expect largest gains on Fires and Economic Loss classes).

## Open Questions the Paper Calls Out
- Can attention-based fusion mechanisms or graph neural networks (GNNs) improve the capture of complex cross-modal relationships compared to the current early fusion approach?
- To what extent does the automated translation of "Banglish" and English text into standard Bangla introduce semantic errors that affect classification accuracy?
- Does incorporating social context metadata (e.g., user network or posting time) enhance classification performance for the Bangla disaster taxonomy?

## Limitations
- Dataset generalizability concerns due to specific time period and social media sources used for collection
- Pretraining mismatch between general multilingual corpora and disaster-specific text domains
- Early fusion optimality not validated against late fusion or hybrid approaches

## Confidence

### High Confidence
- Multimodal fusion improves disaster classification accuracy over unimodal baselines

### Medium Confidence
- Specific architecture choices (mBERT + ResNet50) represent optimal configuration

### Low Confidence
- Real-world deployment readiness without temporal drift testing or out-of-distribution validation

## Next Checks
1. **Temporal validation**: Test model performance on disaster posts from different time periods and disaster events not represented in the training data to assess temporal generalization and concept drift.
2. **Annotation reliability**: Conduct inter-annotator agreement studies on a subset of the dataset to quantify labeling consistency and identify classes with systematic disagreement that could explain error patterns.
3. **Fusion method comparison**: Implement and compare late fusion and hybrid fusion approaches against the current early fusion baseline to determine if the computational efficiency gain comes at the cost of classification accuracy.