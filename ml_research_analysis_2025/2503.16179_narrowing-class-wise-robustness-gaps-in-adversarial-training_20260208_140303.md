---
ver: rpa2
title: Narrowing Class-Wise Robustness Gaps in Adversarial Training
arxiv_id: '2503.16179'
source_url: https://arxiv.org/abs/2503.16179
tags:
- adversarial
- training
- robustness
- error
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of class-wise robustness gaps in
  adversarial training, where improvements in robustness to adversarial examples can
  come at the cost of increased class imbalances and reduced performance on clean
  data. The authors propose to mitigate this by incorporating label augmentation into
  the adversarial training process.
---

# Narrowing Class-Wise Robustness Gaps in Adversarial Training

## Quick Facts
- arXiv ID: 2503.16179
- Source URL: https://arxiv.org/abs/2503.16179
- Authors: Fatemeh Amerehi; Patrick Healy
- Reference count: 6
- Primary result: Label augmentation improves adversarial robustness by 53.50% and reduces class imbalances by 5.73%

## Executive Summary
This paper addresses the problem of class-wise robustness gaps in adversarial training, where adversarial robustness improvements can exacerbate class imbalances and reduce clean data performance. The authors propose a label augmentation approach that concatenates original class labels with transformation operation labels during training. This method maintains label integrity and promotes invariance to class identity while encouraging separation between class identity and transformations. The approach significantly improves robustness while reducing class-wise disparities and better balancing accuracy across different corruption types.

## Method Summary
The method extends standard adversarial training by augmenting labels with transformation operation information. During training, the model receives concatenated labels that combine class identity (1-δ)y_i with transformation identity δz_j, where δ=0.03 and z_j is a one-hot vector for the applied transformation. The loss function is modified to predict over K+M dimensions instead of K classes. The approach is evaluated on ResNet-50 using 10-step PGD adversarial training with ℓ∞ ε=0.03, comparing performance across clean accuracy, mCE, PGD-40 attack error, and class-wise error statistics.

## Key Results
- Label augmentation boosts adversarial robustness by 53.50% compared to standard adversarial training
- Reduces class imbalances by 5.73% as measured by standard deviation of class-wise errors
- Improves overall accuracy in both clean and adversarial settings while better balancing performance across corruption types
- Demonstrates improved error ratios for style and texture variations while maintaining corruption robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating transformation operation labels to class labels during adversarial training reduces class-wise robustness gaps while improving overall robustness
- Mechanism: The label augmentation approach extends the original K-class label vector to length K+M by concatenating one-hot transformation labels z_j scaled by δ. The loss function L_LA = -Σ_{k=1}^{K+M} ỹ_{ik} log p̃_{ik} forces the model to explicitly predict both class identity and applied transformation, which discourages the network from conflating transformation artifacts with class features. This separation helps prevent uneven degradation across classes when adversarial perturbations are applied uniformly
- Core assumption: Explicitly modeling transformation identity during training creates internal representations that factorize class-relevant features from transformation-specific signals, reducing interference during adversarial perturbation
- Evidence anchors:
  - [abstract]: "incorporating label augmentation into the adversarial training process... involves concatenating original class labels with transformation operation labels during training, which helps maintain label integrity and promotes invariance to class identity while encouraging separation between class identity and transformations"
  - [section 2]: Equation 3 shows ỹ_i = Concat[(1-δ)y_i, δz_j] and Equation 4 shows the extended loss over K+M dimensions
  - [corpus]: Weak direct evidence—neighbor papers (TRIX, MemLoss) address adversarial fairness through different mechanisms (mixed training, example recycling) but do not validate the label concatenation hypothesis specifically
- Break condition: If transformation labels are not discriminative (e.g., identical perturbations applied uniformly without variation), the auxiliary supervision signal collapses and provides no class-separating benefit

### Mechanism 2
- Claim: Adversarial training amplifies existing class-wise performance disparities, and scaling factor δ controls the trade-off between robustness gains and imbalance mitigation
- Mechanism: Standard adversarial training optimizes for worst-case perturbations without distinguishing how different classes respond to the perturbation budget. The scaling factor δ=0.03 (matching ε=0.03 in experiments) limits how much the model shifts toward the augmented label component, preventing over-regularization that could harm clean accuracy while still providing enough signal to counteract class-conditional vulnerability differences
- Core assumption: The perturbation budget ε and label scaling δ should be commensurate; mismatched values would either under-utilize the transformation signal or overwhelm class identity
- Evidence anchors:
  - [section 3]: "We set the scaling factor δ = 0.03 to reflect the strength of the added perturbation"
  - [section 4, Table 2]: Adv+ reduces mean error and standard deviation compared to Adv across Clean (26.69 vs 32.17 mean, 16.76 vs 17.78 SD) and PGD-40 (32.70 vs 70.32 mean, 15.33 vs 15.77 SD)
  - [corpus]: TRIX paper similarly addresses class-wise vulnerability disparities but via "mixed adversarial training" rather than label scaling, suggesting the problem is recognized but solutions differ
- Break condition: If δ is set too high relative to class label strength, the model may prioritize transformation prediction over classification, degrading primary task performance

### Mechanism 3
- Claim: Label-augmented adversarial training achieves better robustness-accuracy trade-offs by reducing the "spillover" effects where robustness gains in one dimension harm performance in another
- Mechanism: The paper observes that augmentations can produce unintended consequences—e.g., color jittering weakens pose robustness. By making transformation explicit in the label space, the model learns to attribute feature changes to their correct source (transformation vs. class) rather than incorrectly updating class decision boundaries. This containment of side effects manifests as improved error ratios on style/texture variations (Figure 4) while maintaining corruption robustness
- Core assumption: The internal representation factorization induced by label augmentation generalizes beyond the specific transformations seen during training to holdout corruption types
- Evidence anchors:
  - [section 1]: "Augmentations may also produce unintended spillover effects; for example, color jittering strengthens robustness to brightness and color shifts yet unexpectedly weakens robustness to pose"
  - [section 4]: "adversarial models demonstrate improved error ratios in style and texture variations" (Figure 4)
  - [corpus]: No direct validation from neighbor papers on spillover reduction mechanism
- Break condition: If test-time corruptions involve transformation types not represented in the M transformation labels during training, the factorization benefit may not transfer

## Foundational Learning

- Concept: Adversarial Training (PGD/FGSM)
  - Why needed here: The entire method builds on standard adversarial training; without understanding how perturbations maximize loss (Eq. 1-2), the intervention point for label augmentation is unclear
  - Quick check question: Can you explain why PGD-10 generates stronger adversarial examples than FGSM, and how this affects the training distribution?

- Concept: Multi-label Classification Loss
  - Why needed here: The method extends labels from K to K+M dimensions and uses a unified cross-entropy over all outputs. Understanding multi-label supervision is essential to implement the loss correctly
  - Quick check question: If you have 10 classes and 3 transformation types, what is the dimension of ỹ_i and how does the softmax computation change compared to standard classification?

- Concept: Class-wise Error Analysis (Mean and Variance)
  - Why needed here: The paper's core contribution is reducing class-wise gaps, measured via mean error and standard deviation across classes. Without this lens, improvements appear as aggregate metrics only
  - Quick check question: Why does a lower standard deviation of class-wise errors indicate more "fair" robustness, even if mean error is unchanged?

## Architecture Onboarding

- Component map: Input images -> PGD-10 adversarial generation -> Label augmentation (Concatenate y_i and z_j) -> ResNet-50 (modified output head K+M) -> Extended cross-entropy loss
- Critical path: 1) Generate adversarial perturbation δ_adv via PGD-10 on input x_i, 2) Apply transformation to get x̃_i = x_i + δ_adv, 3) Create extended label ỹ_i with transformation identity, 4) Forward pass through modified ResNet-50 head (K+M outputs), 5) Compute L_LA and backpropagate
- Design tradeoffs:
  - δ selection: Higher δ strengthens transformation signal but risks overwhelming class identity; paper uses δ=0.03 matching ε
  - Number of transformations M: More transformation types provide finer-grained supervision but increase output head size and require labeling infrastructure
  - Adversarial strength (PGD steps, ε): Stronger attacks improve robustness but exacerbate the class imbalance problem that LA aims to fix
- Failure signatures:
  - Clean accuracy drops significantly: δ may be too high; class label signal is being suppressed
  - No improvement over standard Adv: Transformation labels may not be discriminative (check if z_j varies across samples)
  - High variance in class-wise errors persists: M may be insufficient to capture transformation diversity; consider increasing transformation types
- First 3 experiments:
  1. **Baseline reproduction**: Run standard PGD-10 adversarial training (no label augmentation) on ResNet-50 for 10 epochs; measure clean error, mCE, PGD-40 error, and class-wise SD to establish gaps
  2. **Label augmentation ablation**: Implement LA with δ=0.03 and M matching perturbation type count; compare mean and SD across clean/adversarial settings against baseline to verify 53.50% robustness gain and 5.73% imbalance reduction
  3. **δ sensitivity sweep**: Test δ ∈ {0.01, 0.03, 0.05, 0.1} while fixing ε=0.03; plot clean accuracy vs. adversarial robustness vs. class-wise SD to identify optimal scaling factor for your dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. The provided open questions are inferred from gaps in the paper's methodology and experimental scope.

## Limitations

- The definition and selection of transformation operation labels (M and z_j) is not explicitly specified, creating ambiguity in implementation
- The method's dependence on matching ε and δ values introduces a hyperparameter coupling that may limit generalization across different perturbation budgets
- The spillover reduction mechanism lacks direct empirical validation through ablation studies isolating the specific benefit of label augmentation

## Confidence

- **High confidence**: The claim that label augmentation improves robustness (53.50% gain) and reduces class-wise gaps (5.73% reduction) is directly supported by Table 1 comparisons across multiple metrics and datasets
- **Medium confidence**: The mechanism explanation for how label concatenation creates factorization of class-relevant features from transformation signals is plausible but relies on implicit assumptions about internal representations
- **Low confidence**: The spillover reduction mechanism lacks direct validation. While Figure 4 shows improved error ratios for style/texture variations, there is no ablation demonstrating that this is specifically due to the factorization induced by label augmentation

## Next Checks

1. **Transformation label definition validation**: Implement and test with different definitions of M (e.g., M=1 for "adversarial" vs. M=3 for specific perturbation types like FGSM, PGD, and random noise). Measure whether robustness gains and imbalance reductions persist across these configurations

2. **Feature factorization analysis**: Use techniques like Centered Kernel Alignment (CKA) or probing classifiers to directly measure whether internal representations factorize class-relevant features from transformation signals when label augmentation is applied versus standard adversarial training

3. **δ-ε coupling sensitivity**: Systematically test mismatched δ and ε values (e.g., δ=0.01 with ε=0.03, δ=0.05 with ε=0.03) to determine if the claimed commensurate relationship is critical, or if the method works across a broader range of parameter combinations