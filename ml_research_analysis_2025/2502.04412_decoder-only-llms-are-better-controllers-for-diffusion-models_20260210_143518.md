---
ver: rpa2
title: Decoder-Only LLMs are Better Controllers for Diffusion Models
arxiv_id: '2502.04412'
source_url: https://arxiv.org/abs/2502.04412
tags:
- diffusion
- text
- llms
- image
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving semantic understanding
  in text-to-image diffusion models. Existing methods rely on encoder-based language
  models that are trained on limited image-caption pairs, leading to suboptimal image
  generation quality and reliability.
---

# Decoder-Only LLMs are Better Controllers for Diffusion Models

## Quick Facts
- arXiv ID: 2502.04412
- Source URL: https://arxiv.org/abs/2502.04412
- Reference count: 35
- Primary result: Achieves 31% improvement in SigLIP Score compared to SDXL baseline using decoder-only LLM adapter

## Executive Summary
This paper addresses the challenge of improving semantic understanding in text-to-image diffusion models by replacing traditional encoder-based text encoders with decoder-only large language models. The authors develop LLMDiff-Adapter, a novel adapter that extracts text encodings from LLM transformer blocks using score function estimation and integrates them into diffusion models via cross-attention. The approach demonstrates significant improvements in image generation quality, achieving 31% better SigLIP Score and superior performance in complexity, aesthetic appeal, and logical entity relationships.

## Method Summary
The method introduces LLMDiff-Adapter (~45M parameters) that modifies pre-trained SD 1.5 diffusion models to incorporate decoder-only LLMs. Text encodings are extracted using Langevin dynamics to approximate score functions from LLM transformer blocks, with the score defined as the difference between context-conditioned and word-only predictions. A dual cross-attention mechanism combines original CLIP-aligned attention with new LLM-aligned attention using learnable weights. The adapter is trained on ~1M image-text pairs while freezing both the LLM and U-Net backbone, using MSE loss and AdamW optimizer.

## Key Results
- Achieves 31% improvement in SigLIP Score compared to SDXL baseline
- Superior performance in image quality metrics: Quality, Complexity, and Beauty (via CLIP-IQA)
- Strong reasoning abilities demonstrated through accurate entity relationships, spatial structures, and complex descriptions
- Effective across different LLM scales (Phi-1.5 and Vicuna-7B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoder-only LLMs can be reinterpreted as diffusion models, enabling text encoding extraction via score function estimation.
- **Mechanism:** Transformer blocks in decoder-only LLMs perform sequential transformations with causal attention, analogous to DDPM denoising steps. By modeling this as a diffusion process, the score function of the conditional text encoding distribution can be approximated through block-wise predictions.
- **Core assumption:** The sequential transformer block structure is sufficiently analogous to diffusion timesteps that Langevin dynamics sampling applies meaningfully.
- **Evidence anchors:**
  - [abstract]: "derive theoretical underpinnings for extracting text encodings from the blocks of LLMs"
  - [Section 3.2]: "This process is akin to the denoising process of DDPM with conditioning. So, the transformer-based LLMs can be viewed as diffusion models."
  - [corpus]: Limited direct validation; corpus papers focus on encoder-decoder or CLIP-based approaches, not this specific diffusion reinterpretation.
- **Break condition:** If transformer block transformations lack the Markov-like properties assumed in diffusion, score estimation becomes unreliable.

### Mechanism 2
- **Claim:** Text encodings emerge from the difference between context-conditioned and word-only score functions.
- **Mechanism:** The score function for text encodings is approximated as: `sentence_score - word_score`, where `sentence_score` captures full contextual dependency and `word_score` captures isolated token prediction. This subtraction isolates the semantic contribution of preceding context.
- **Core assumption:** Contextual dependencies are cleanly separable from individual token predictions through this subtraction operation.
- **Evidence anchors:**
  - [Section 3.4, Eq. 5]: `∇_c log p(c_{<d}|x_d^t, x_d^{t-1}) ≈ g(t)(∇_x log p(x_d^{t-1}|x_d^t, x_{<d}^t) - ∇_x log p(x_d^{t-1}|x_d^t))`
  - [Algorithm 1]: Explicitly computes `s_sentence ← g(t)S_θ(x_d^{t-1}, x_d^t, x_{<d}^t)` and `s_word ← g(t)S_θ(x_d^{t-1}, x_d^t)`
  - [corpus]: No corpus papers validate this specific subtraction-based encoding extraction.
- **Break condition:** If word-only and sentence-conditioned predictions overlap substantially (high mutual information), the subtraction may produce noisy or degenerate encodings.

### Mechanism 3
- **Claim:** Dual cross-attention with learned weight blending preserves pre-trained diffusion knowledge while incorporating LLM semantics.
- **Mechanism:** The adapter adds a parallel cross-attention module for LLM-derived encodings alongside the original CLIP-conditioned attention. Learnable scalar weights (`a1, a2, b1, b2`) modulate their contributions, initialized to favor the original module initially.
- **Core assumption:** Pre-trained cross-attention knowledge remains valuable and should be preserved rather than replaced.
- **Evidence anchors:**
  - [Section 4.2, Eq. 9]: `f = attn(τ̂_q(q), τ̂_k(φ(c)), τ̂_v(φ(c)))a1e^b1 + attn(τ_q(q), τ_k(c), τ_v(c))a2e^b2`
  - [Section 5.5, Figure 6]: Shows weight distribution across layers—original attention preserved more in parameter-sparse layers.
  - [corpus]: Causal-Adapter paper uses similar modular adapter approach but for counterfactual generation, validating adapter preservation strategies.
- **Break condition:** If weight initialization or learning rates cause the new attention to dominate too quickly, catastrophic forgetting of pre-trained spatial generation capabilities may occur.

## Foundational Learning

- **Concept: Cross-attention in diffusion models**
  - **Why needed here:** The adapter injects LLM encodings via cross-attention; understanding how queries/keys/values flow is essential for debugging alignment issues.
  - **Quick check question:** Given a U-Net feature map `q` and text encoding `c`, what does `attn(q, K(c), V(c))` compute geometrically?

- **Concept: Langevin dynamics sampling**
  - **Why needed here:** Algorithm 1 uses Langevin-style updates with gradient and noise terms to sample text encodings; misunderstanding this leads to incorrect implementation.
  - **Quick check question:** Why does Langevin sampling add Gaussian noise `ε_t` alongside the gradient term, and what happens if you remove it?

- **Concept: Score functions in diffusion models**
  - **Why needed here:** The core theoretical contribution reinterprets LLM blocks as score estimators; confusion here propagates through the entire encoding extraction pipeline.
  - **Quick check question:** What does `∇_x log p(x_{t-1}|x_t)` represent intuitively, and why does it point toward higher-probability regions?

## Architecture Onboarding

- **Component map:**
  Input Text → LLM (frozen, decoder-only, e.g., Vicuna-7B) → Extract block outputs at each layer → LLMDiff-Adapter (~45M params) - Score computation (sentence vs word) - Langevin sampling loop (Algorithm 1) → Text encoding c_{<d} → Diffusion U-Net (frozen SD1.5 backbone) ├── Original Cross-Attention (CLIP-aligned, frozen) └── New Cross-Attention (LLM-aligned, trainable) → Weighted blending (a1,a2,b1,b2)

- **Critical path:**
  1. LLM forward pass must expose intermediate block outputs (requires model modification or hook registration—closed-source models like GPT-4 are incompatible per Section 6).
  2. Score subtraction must operate on matching tensor dimensions and time steps.
  3. Langevin loop must execute `T` iterations with correct noise scheduling.

- **Design tradeoffs:**
  - **LLM size vs. latency:** Vicuna-7B provides better semantic understanding (SigLIP 8.5) than Phi-1.5 (SigLIP 5.8) but increases inference overhead.
  - **Adapter capacity vs. overfitting:** 45M parameters is sufficient for alignment but larger adapters may overfit on limited training data (~1M samples).
  - **Preservation vs. adaptation:** Aggressive weight initialization toward new attention speeds LLM adoption but risks losing pre-trained spatial priors.

- **Failure signatures:**
  - Generated images ignore text entirely → likely weight initialization bug (`a2, b2` not starting near zero).
  - Images lack spatial coherence → original cross-attention contribution suppressed too aggressively; check learned weights in middle U-Net layers.
  - Encoding extraction produces NaN → score subtraction may create numerical instability; add gradient clipping or normalization.
  - Entity count incorrect → LLM context window truncation or block output extraction error.

- **First 3 experiments:**
  1. **Sanity check with controlled prompts:** Generate images for "a red circle next to a blue square" and verify basic entity/attribute separation before testing complex prompts.
  2. **Weight ablation study:** Force `a1=1, a2=0` (original only) vs `a1=0, a2=1` (LLM only) to isolate each attention pathway's contribution.
  3. **LLM scale comparison:** Run identical prompts through Phi-1.5 and Vicuna-7B variants to quantify semantic improvement vs. computational cost.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the LLMDiff-Adapter framework be modified to operate with closed-source LLMs (e.g., GPT-4) where internal transformer block outputs are inaccessible?
  - **Basis in paper:** [explicit] The authors explicitly state in Section 6 that the method is incompatible with closed-source models because it requires outputs from each transformer block.
  - **Why unresolved:** The current methodology relies fundamentally on accessing intermediate layer activations (score functions), which proprietary APIs do not expose.
  - **What evidence would resolve it:** A distillation method that trains a student adapter using only the final logits of a teacher LLM, or a proxy mechanism that approximates block-wise states.

- **Open Question 2:** What is the computational and latency overhead introduced by utilizing large decoder-only LLMs (e.g., Vicuna-7B) compared to standard encoders like CLIP during inference?
  - **Basis in paper:** [inferred] The paper utilizes Vicuna-7B but does not report inference speed or VRAM consumption relative to the much smaller CLIP text encoder.
  - **Why unresolved:** While semantic performance improved, the practicality of this approach for real-time applications depends on the trade-off between semantic understanding and generation speed.
  - **What evidence would resolve it:** Benchmarking frames-per-second (FPS) and memory usage for LLMDiff versus standard SDXL under identical hardware constraints.

- **Open Question 3:** Is the retention of the original cross-attention module strictly necessary, or can the model be trained to convergence using only the LLM-based adapter?
  - **Basis in paper:** [inferred] The method retains the original cross-attention module to preserve pre-trained knowledge (Section 4.2), suggesting a potential stability or capacity loss if the LLM adapter acts alone.
  - **Why unresolved:** It remains unclear if the LLM features are complementary to, or strictly superior to, the original encoder features, or if the "hybrid" approach masks a failure mode in direct replacement.
  - **What evidence would resolve it:** An ablation study comparing the proposed hybrid attention against a variant where the original cross-attention is completely removed.

## Limitations

- The theoretical reinterpretation of decoder-only LLMs as diffusion models lacks rigorous empirical validation beyond the proposed application.
- The score function subtraction method assumes clean separation between contextual dependencies and word-level predictions, which may not hold when word and sentence score functions have high mutual information.
- The method is incompatible with closed-source LLMs due to the requirement for intermediate transformer block outputs.

## Confidence

- **High confidence:** The adapter architecture design and implementation details (Section 4) are well-specified with clear equations and pseudocode.
- **Medium confidence:** The experimental results showing improved SigLIP scores and qualitative improvements are convincing, but the ablation studies could be more thorough.
- **Low confidence:** The theoretical reinterpretation of decoder-only LLMs as diffusion models lacks rigorous empirical validation beyond the proposed application.

## Next Checks

1. **Score function stability analysis:** Systematically vary the Langevin dynamics hyperparameters (T, g(t), h(t)) and measure the stability of extracted text encodings. Test whether the score subtraction produces degenerate outputs when word-level and contextual predictions overlap substantially.

2. **Cross-attention pathway isolation:** Conduct controlled experiments where each cross-attention module operates independently (a1=1,a2=0 vs a1=0,a2=1) to quantify their individual contributions and identify any catastrophic forgetting in the original module.

3. **LLM size scaling study:** Extend the comparison beyond Phi-1.5 and Vicuna-7B to include intermediate scales (e.g., Llama-7B, Mistral-7B) to better understand the semantic improvement vs. computational cost relationship and identify potential diminishing returns.