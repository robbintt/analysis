---
ver: rpa2
title: Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring
  System
arxiv_id: '2511.12196'
source_url: https://arxiv.org/abs/2511.12196
tags:
- domain
- driver
- learning
- data
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of driver monitoring systems
  in real-world scenarios where camera viewpoints and sensor modalities vary, leading
  to performance degradation. The authors propose a two-phase framework called C2UDA
  that combines cross-view generalization with cross-modal unsupervised domain adaptation.
---

# Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System

## Quick Facts
- arXiv ID: 2511.12196
- Source URL: https://arxiv.org/abs/2511.12196
- Reference count: 40
- Primary result: Up to 50% gain in top-1 accuracy on RGB video data using cross-view cross-modal adaptation vs. contrastive learning alone

## Executive Summary
This paper addresses the challenge of driver monitoring systems in real-world scenarios where camera viewpoints and sensor modalities vary, leading to performance degradation. The authors propose a two-phase framework called C2UDA that combines cross-view generalization with cross-modal unsupervised domain adaptation. In the first phase, supervised contrastive learning is used to learn view-invariant representations from multi-view data within a single modality. In the second phase, information bottleneck loss is applied to adapt the model to a new modality using unlabeled target data. The approach is evaluated on the Drive&Act dataset using video transformers (Video Swin, MViT) and shows significant improvements, with up to 50% gain in top-1 accuracy on RGB video data compared to contrastive learning alone, and up to 5% improvement over UDA-only methods. Cross-dataset evaluation on DAD dataset demonstrates the method's robustness to domain shifts.

## Method Summary
The proposed C2UDA framework operates in two sequential phases. Phase 1 uses supervised contrastive learning to train on synchronized multi-view NIR data (V1, V2, V3), treating same-class samples from different views as positive pairs to learn view-invariant representations. A video transformer backbone (Video Swin or MViT) is fine-tuned with cross-entropy and supervised contrastive loss for 20 epochs. Phase 2 performs cross-modal adaptation by freezing lower spatiotemporal layers and fine-tuning upper layers with the target RGB domain using information bottleneck loss. The method employs pseudo-labeling via queue-based matching to construct positive source-target pairs, maximizing mutual information between source features and target instances while minimizing domain-specific information. The framework is evaluated using top-1 and top-5 accuracy metrics across different camera views and modalities.

## Key Results
- Up to 50% improvement in top-1 accuracy on RGB video data compared to contrastive learning alone
- Up to 5% improvement over UDA-only methods when adapting from NIR to RGB
- Cross-dataset evaluation on DAD dataset shows robustness to domain shifts
- MViT backbone achieves 89.25% top-1 accuracy on RGB, slightly outperforming Video Swin at 87.42%

## Why This Works (Mechanism)

### Mechanism 1: Supervised Contrastive Learning for View-Invariant Representations
- Claim: Aligning representations of synchronized multi-view samples from the same class encourages the encoder to discard viewpoint-specific information while preserving action-discriminative features.
- Mechanism: Uses V1 as anchor with V2/V3 as positives in a supervised contrastive loss (Eq. 3), where all same-class samples across views are treated as positives. The projection head maps embeddings before computing cosine similarity with temperature scaling. This explicitly forces the shared backbone to produce similar representations for the same action regardless of camera angle.
- Core assumption: Synchronized multi-view data exists during training, and semantic content (driver action) is invariant to viewpoint within the same temporal window.
- Evidence anchors:
  - [abstract] "learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data"
  - [Page 4, Section III] "We adopt a supervised contrastive objective [39], where all other samples of the same class are treated as positives"
  - [corpus] Weak direct evidence; neighbor papers focus on cross-domain adaptation but not specifically multi-view contrastive learning for DAR.

### Mechanism 2: Information Bottleneck Loss for Cross-Modal Domain Alignment
- Claim: Minimizing the information bottleneck loss between source and target feature distributions encourages domain-invariant representations without requiring target labels.
- Mechanism: Approximates IB principle using a Barlow Twins-style cross-correlation matrix (Eq. 5). The loss minimizes off-diagonal correlations while maximizing diagonal correlations, encouraging source and target embeddings to have similar feature-wise statistics. Pseudo-labeling via queue-based matching constructs positive source-target pairs.
- Core assumption: Target domain shares the same label space as source; pseudo-labels can reliably pair semantically similar samples across domains.
- Evidence anchors:
  - [abstract] "perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain"
  - [Page 5, Section III] "This framework aims to maximize the mutual information between the learned source features and the corresponding target instances... while minimizing the mutual information between the features and the original source inputs"
  - [corpus] Neighbor paper "Unsupervised Domain Adaptation via Similarity-based Prototypes" supports prototype-based alignment but uses different mechanism.

### Mechanism 3: Selective Layer Freezing with Two-Phase Sequential Training
- Claim: Freezing lower spatiotemporal layers during Phase 2 preserves generalizable features while allowing upper layers to adapt to the target modality.
- Mechanism: Phase 1 trains all layers on multi-view NIR data. Phase 2 freezes lower encoding layers and fine-tunes only upper layers + classifier with IB loss. This prevents catastrophic forgetting of view-invariant features learned in Phase 1 while enabling modality-specific adaptation.
- Core assumption: Lower layers capture modality-agnostic spatiotemporal patterns; upper layers are more task/modality-specific.
- Evidence anchors:
  - [Page 5, Section III] "we freeze a subset of layers, especially the lower layers responsible for spatiotemporal feature encoding... This selective freezing enables us to scale the batch size during adaptation"
  - [Page 5, Section IV] "In Phase 2... the lower spatiotemporal encoding layers were frozen, while the upper layers and classifier head were fine-tuned"
  - [corpus] Insufficient corpus evidence; related papers don't explicitly analyze layer-wise freezing strategies.

## Foundational Learning

- **Supervised Contrastive Learning**
  - Why needed here: Standard cross-entropy doesn't explicitly enforce view-invariance; supervised contrastive learning uses label information to create structured positive/negative pairs across views.
  - Quick check question: Can you explain why treating same-class samples from different views as "positives" helps the model ignore viewpoint-specific features?

- **Information Bottleneck Principle**
  - Why needed here: IB formalizes the trade-off between compression (removing domain-specific information) and prediction (preserving task-relevant information), providing a principled objective for domain adaptation.
  - Quick check question: What does maximizing I(z_s, D_t) while minimizing I(z_s, D_s) achieve in the context of cross-modal transfer?

- **Video Transformer Architectures (Swin/MViT)**
  - Why needed here: These backbones provide spatiotemporal attention mechanisms that jointly model spatial and temporal dependencies, which is essential for action recognition from video.
  - Quick check question: How does the unified spatiotemporal transformer in C2UDA differ from separate spatial+temporal modules used in prior work?

## Architecture Onboarding

- **Component map**: Multi-view NIR data -> Video Transformer Backbone -> Projection Head (Phase 1) -> Classifier Head -> Cross-entropy + Supervised Contrastive Loss (Phase 1) -> Information Bottleneck Loss (Phase 2) -> RGB target data

- **Critical path**:
  1. Data preparation: Synchronize multi-view NIR clips (V1, V2, V3) using temporal metadata
  2. Phase 1 training: Fine-tune backbone + projection head with L_CE + λ₁L_CL for 20 epochs
  3. Phase 2 setup: Freeze lower layers, increase batch size to 64, raise learning rate to 0.005
  4. Phase 2 training: Fine-tune upper layers + classifier with L_CE + αL_IB for 20 epochs
  5. Evaluation: Test on held-out views (V4) and target modality (RGB)

- **Design tradeoffs**:
  - Batch size vs. memory: Phase 2 requires large batch (64) for stable IB loss computation; may need gradient accumulation on limited GPU memory
  - Freezing strategy: Freezing too many layers limits adaptation; freezing too few risks forgetting view-invariant features from Phase 1
  - Backbone choice: MViT shows slightly better results (89.25% vs 87.42% on RGB) but may have different computational requirements

- **Failure signatures**:
  - Phase 1 underperforms on unseen views: Check synchronization quality; misaligned clips create noisy contrastive pairs
  - Phase 2 shows no improvement over Phase 1: Verify pseudo-labeling is working; check if IB loss weight α is too small
  - Cross-dataset evaluation collapses: Domain gap may be too large; consider semi-supervised adaptation if some target labels are available

- **First 3 experiments**:
  1. Ablation on contrastive loss weight (λ₁): Run Phase 1 with λ₁ ∈ {0.1, 0.5, 1.0, 2.0} and evaluate on held-out view V4 to find optimal view-invariance strength.
  2. Freezing depth sensitivity: Test freezing at different layer boundaries (e.g., freeze first 25%, 50%, 75% of layers) in Phase 2 to identify optimal adaptation capacity.
  3. Cross-dataset transfer without Phase 1: Run UDA-only baseline on DAD dataset to isolate the contribution of view-invariant pretraining to cross-dataset generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the view-invariant pre-training phase (Phase 1) be modified to eliminate the requirement for synchronized multi-view video data?
- Basis in paper: [explicit] The authors state in the Discussion that "The approach relies on synchronized multi-view data during training, which may not always be available in all deployment environments."
- Why unresolved: The current supervised contrastive learning objective depends strictly on temporally aligned positive pairs from different camera angles.
- What evidence would resolve it: A modified framework utilizing unsynchronized data, synthetic view generation, or single-view augmentation strategies that achieves comparable top-1 accuracy on unseen views.

### Open Question 2
- Question: How does the framework perform when adapting to target domains with drastically different activity label spaces (Open-Set DA)?
- Basis in paper: [explicit] The Discussion notes that "domain shifts involving... different activity labels may require further fine-tuning or adaptation," implying the current closed-set assumption is a limitation.
- Why unresolved: The current method relies on cross-entropy loss and pseudo-labeling mechanisms that assume the source and target share identical categorical labels.
- What evidence would resolve it: Evaluation on a cross-modal dataset where the target domain contains "unknown" or private classes not present in the source NIR data.

### Open Question 3
- Question: Does the Information Bottleneck (IB) adaptation mechanism function effectively when the source modality is RGB and the target is NIR?
- Basis in paper: [inferred] The evaluation strictly uses NIR as the source domain and RGB as the target; the authors note NIR has distinct "lighting and sensor characteristics" which may imply the direction of adaptation matters.
- Why unresolved: It is unclear if the IB loss is symmetric or if the transition from a sensor-rich domain (NIR) to a standard domain (RGB) masks difficulties in reverse adaptation.
- What evidence would resolve it: Ablation studies showing top-1 accuracy when the training direction is reversed (RGB → NIR) using the Drive&Act dataset.

## Limitations

- The framework requires synchronized multi-view data during training, which may not be available in all deployment environments
- The two-phase sequential training approach introduces complexity and hyperparameter sensitivity that requires careful tuning
- Cross-dataset evaluation on DAD is promising but limited in scope - the semantic gap between datasets could be smaller than real-world deployment scenarios

## Confidence

- **High confidence**: The supervised contrastive learning mechanism for view-invariance (Phase 1) - supported by strong theoretical grounding and reasonable empirical improvements (50% gain reported)
- **Medium confidence**: The information bottleneck-based cross-modal adaptation (Phase 2) - the mechanism is well-established but the specific implementation details and pseudo-labeling quality are not fully transparent
- **Medium confidence**: The overall C2UDA framework integration - the sequential training and selective freezing strategy shows promise but the ablation studies don't fully explore alternative architectural choices

## Next Checks

1. **Synchronization quality impact**: Systematically evaluate how temporal misalignment in multi-view data affects contrastive learning performance to establish practical requirements for view-invariance learning
2. **Cross-modal generalization breadth**: Test the approach across multiple modality pairs (e.g., RGB-to-depth, thermal-to-RGB) and more diverse activity taxonomies to assess robustness to semantic domain shifts
3. **Single-phase alternative evaluation**: Compare against a unified framework that jointly optimizes view-invariance and cross-modal adaptation to determine if sequential training is truly necessary or if simpler approaches could achieve similar results