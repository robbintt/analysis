---
ver: rpa2
title: Automatic Replication of LLM Mistakes in Medical Conversations
arxiv_id: '2512.20983'
source_url: https://arxiv.org/abs/2512.20983
tags:
- patient
- safety
- risk
- resulted
- failed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedMistake is an automated pipeline that extracts LLM mistakes
  from patient-doctor conversations and converts them into single-shot QA benchmarks.
  It generates medical dialogues, uses LLM judges to identify errors across 40 dimensions,
  and creates targeted QA scenarios.
---

# Automatic Replication of LLM Mistakes in Medical Conversations

## Quick Facts
- arXiv ID: 2512.20983
- Source URL: https://arxiv.org/abs/2512.20983
- Reference count: 40
- Primary result: Pipeline produces 3,390 mistake scenarios where GPT-5 and Gemini 2.5 Pro failed, with 211 expert-validated items showing GPT-5.2 achieved 53% vs Mistral Large at 13%

## Executive Summary
MedMistake introduces an automated pipeline that extracts LLM mistakes from patient-doctor conversations and converts them into single-shot QA benchmarks. The approach generates medical dialogues, uses LLM judges to identify errors across 40 dimensions, and creates targeted QA scenarios. Testing 12 frontier LLMs revealed significant performance differences, with GPT-5.2 achieving ~40% correct while Mistral Large scored only ~15%. The work demonstrates scalable automatic mistake extraction with sufficient sensitivity to differentiate LLM capabilities in medical domains.

## Method Summary
The pipeline operates through several stages: conversation generation using MedPI framework, committee evaluation by two Gemini 2.5 Flash judges across 105 clinical dimensions, mistake extraction from low-scoring dimensions, scenario generation that distills errors into single-shot QA pairs, replication testing on GPT-5 and Gemini 2.5 Pro, and optional reflection-based hardening. The process creates MedMistake-All (3,390 QA pairs) and MedMistake-Bench (211 expert-validated scenarios), with models evaluated using binary LLM judges.

## Key Results
- 3,390 mistake scenarios replicated on GPT-5 or Gemini 2.5 Pro from 7,010 total generated mistakes (48.4% replication rate)
- GPT-5.2 achieved highest performance at 53%, while Mistral Large scored lowest at 13%
- Expert validation rate of 70.6% (211/299 reviewed scenarios) in MedMistake-Bench
- Significant model differentiation capability, with ~25 percentage point gap between top and bottom performers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional committee evaluation identifies granular failure modes that holistic scoring misses.
- Mechanism: Two LLM judges evaluate each conversation across 105 dimensions grouped into 29 categories (e.g., medication safety, contraindications, triage). Low-scoring dimensions (≤3) trigger structured mistake extraction with explicit rationale, category, and risk-level labeling. This per-turn granularity enables conversion to single-shot test cases.
- Core assumption: LLM judges can reliably localize and characterize clinical reasoning errors when given structured rubrics.
- Evidence anchors: [abstract] "runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes"; [section 2.1] "We create a medical committee of 2 LLM judges (Gemini 2.5 Flash) to evaluate the conversation on 105 dimensions"; [corpus] MedPI benchmark paper describes the foundational 105-dimension evaluation framework.

### Mechanism 2
- Claim: Distilling multi-turn failures into single-shot QA preserves the core reasoning gap while enabling scalable testing.
- Mechanism: From each identified mistake, Gemini 2.5 Flash generates a realistic patient vignette containing only information explicitly volunteered before the error occurred. The scenario is designed to trigger the same reasoning failure without conversational context, creating a reproducible test case.
- Core assumption: The reasoning failure can be isolated and triggered in a single turn without losing essential context.
- Evidence anchors: [abstract] "converts them into a benchmark of single-shot QA pairs"; [section 2.1, Scenario Generation] "includes all specific details mentioned in the mistake description... uses only information explicitly volunteered by the patient"; [corpus] No direct corpus evidence on conversation-to-QA distillation effectiveness.

### Mechanism 3
- Claim: Reflection-based hardening increases benchmark difficulty by targeting edge cases where models initially succeed.
- Mechanism: When both GPT-5 and Gemini 2.5 Pro correctly handle a scenario, a reflection prompt generates a more challenging variant using the original mistake description and correct response as guidance. The hardened scenario is retested.
- Core assumption: Models that pass initial scenarios have remaining vulnerability at boundary conditions.
- Evidence anchors: [section 2.1] "If both models handle the scenario correctly... a reflection prompt is used to generate a more challenging variant"; [section 4.1] "only two LLMs (Gemini 2.5 Pro and GPT-5) were used to replicate the mistake, and thus the selection of questions... is biased towards questions where either of these models failed"; [corpus] No corpus papers examine reflection-based scenario hardening.

## Foundational Learning

### Concept: LLM-as-judge evaluation with structured rubrics
- Why needed here: The entire pipeline depends on judges identifying and characterizing mistakes across 105 clinical dimensions.
- Quick check question: Can you explain why two-judge committees might catch errors that single judges miss?

### Concept: Medical conversation evaluation dimensions (triage, contraindications, medication safety)
- Why needed here: Understanding what the 105 dimensions measure is essential for interpreting benchmark results and failure patterns.
- Quick check question: What is the clinical difference between "Patient Safety & Triage" and "Medication Safety" as error categories?

### Concept: Replication testing and generalization
- Why needed here: The benchmark only includes mistakes that replicate on GPT-5 or Gemini 2.5 Pro; understanding selection bias is critical for interpreting model performance.
- Quick check question: Why might Gemini 2.5 Pro score poorly (20%) on a benchmark constructed from its own failures?

## Architecture Onboarding

### Component map:
Conversation Generator (MedPI) -> Committee Evaluator (2× Gemini 2.5 Flash) -> Mistake Extractor (Gemini 2.5 Flash) -> Scenario Generator -> Replication Tester (GPT-5 + Gemini 2.5 Pro) -> (optional Reflection Module) -> Expert Validator

### Critical path:
Conversation → Judge Committee → Mistake Extraction → Scenario Generation → Replication Testing → (optional Reflection) → Expert Validation. The replication gate determines whether a mistake enters the benchmark.

### Design tradeoffs:
- Automation vs. quality: Only 211/3,390 replicated mistakes (6.2%) received expert validation; full automation trades precision for scale.
- Judge selection: Using Gemini 2.5 Flash as judge may introduce model-specific blind spots.
- Replication bias: Benchmark is skewed toward failures of GPT-5 and Gemini 2.5 Pro specifically.

### Failure signatures:
- Low replication rate (<30%): Scenarios may be missing essential context or judges are over-flagging.
- Expert rejection rate >50%: Mistake extraction or scenario generation is producing clinically invalid cases.
- All models score similarly: Benchmark may lack sensitivity to differentiate capabilities.

### First 3 experiments:
1. Replicate the pipeline on a held-out medical specialty (e.g., oncology) to test generalization beyond cardiology/neurology.
2. Compare single-judge vs. two-judge committee agreement rates and impact on benchmark quality.
3. Measure how often reflection-hardened scenarios pass expert validation vs. original scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning frontier LLMs on MedMistake-Bench scenarios improve their performance on medical mistake detection?
- Basis in paper: [explicit] "Future work could also include fine-tuning the models to see if they improve their performance on such benchmarks."
- Why unresolved: No fine-tuning experiments were conducted; only inference-time evaluation was performed.
- What evidence would resolve it: Pre/post fine-tuning performance comparison on held-out medical mistake scenarios.

### Open Question 2
- Question: How reliable are the LLM judges used for mistake evaluation, and what is their inter-judge agreement?
- Basis in paper: [explicit] "to make the LLM judging more robust, we plan to use multiple LLM judges in the future and compute inter-judge reliability and consistency metrics"
- Why unresolved: Only two LLM judges were used without computing consistency metrics; binary scoring may miss nuanced partial correctness.
- What evidence would resolve it: Inter-judge agreement scores (e.g., Cohen's kappa) across multiple independent LLM judges on the same responses.

### Open Question 3
- Question: How does the selection bias toward mistakes that GPT-5 and Gemini 2.5 Pro failed affect benchmark generalizability to other models?
- Basis in paper: [inferred] from limitation "only two LLMs (Gemini 2.5 Pro and GPT-5) were used to replicate the mistake, and thus the selection of questions in the final benchmarks is biased towards questions where either of these models failed."
- Why unresolved: Gemini 2.5 Pro scored only 20% on the final benchmark, suggesting potential overfitting to its specific failure modes.
- What evidence would resolve it: Compare performance distributions when mistake selection uses different model pairs as filters.

### Open Question 4
- Question: Can the MedMistake pipeline generalize effectively to medical specialties beyond Cardiology and Neurology?
- Basis in paper: [explicit] "While many medical specialties outside Cardiology and Neurology were not included in this study, the pipeline can easily be extended to such areas."
- Why unresolved: Current benchmark is dominated by Cardiology and Neurology cases; underrepresented categories like Diagnostics & Workup have too few questions for robust conclusions.
- What evidence would resolve it: Replicate the pipeline on additional specialties and analyze whether mistake reproduction rates and model differentiation patterns hold.

## Limitations
- Only 211/3,390 replicated mistakes (6.2%) received expert validation, raising questions about clinical validity of the full benchmark
- Benchmark is biased toward mistakes that GPT-5 and Gemini 2.5 Pro failed to handle, potentially overrepresenting specific error patterns
- 105-dimensional evaluation framework is not fully specified in the paper, requiring external MedPI framework documentation

## Confidence

### High Confidence:
- The replication testing methodology and benchmark evaluation procedures are clearly specified with reproducible prompts. The comparison of 12 frontier LLMs and their relative performance differences is well-documented.

### Medium Confidence:
- The pipeline architecture and workflow are sound, but the limited expert validation (211 items) means the clinical accuracy of the remaining 3,179 scenarios remains unverified. The reflection-based hardening mechanism is described but not empirically validated for effectiveness.

### Low Confidence:
- The generalizability of the approach to medical specialties beyond cardiology and neurology, and the long-term stability of LLM-as-judge evaluations as models evolve, remain unproven.

## Next Checks
1. **Expert Validation Expansion:** Conduct systematic expert review of a stratified random sample (e.g., 500 scenarios) across all clinical categories to estimate the true clinical validity rate of the full benchmark.
2. **Cross-Model Judge Consistency:** Evaluate the same medical conversations using multiple judge configurations (single vs. committee, different judge models) to quantify agreement rates and identify systematic judge biases.
3. **Context Preservation Testing:** Design experiments comparing single-shot scenario performance against full conversation context performance to measure information loss during the distillation process and identify error types that require conversational context.