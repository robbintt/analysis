---
ver: rpa2
title: Multitask finetuning and acceleration of chemical pretrained models for small
  molecule drug property prediction
arxiv_id: '2510.12719'
source_url: https://arxiv.org/abs/2510.12719
tags:
- kermt
- data
- multitask
- performance
- kpgt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates multitask finetuning of chemical pretrained
  models for small molecule drug property prediction, focusing on improving ADMET
  (absorption, distribution, metabolism, excretion, toxicity) predictions. The authors
  compare several pretrained models including KERMT (an enhanced GROVER), KPGT, and
  MoLFormer against Chemprop (a non-pretrained baseline) on both public and internal
  Merck datasets.
---

# Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction

## Quick Facts
- arXiv ID: 2510.12719
- Source URL: https://arxiv.org/abs/2510.12719
- Reference count: 40
- Primary result: KERMT multitask outperforms other methods on internal ADMET data with Pearson r² improvements of 0.712 vs 0.657 for Chemprop multitask

## Executive Summary
This paper investigates multitask finetuning of chemical pretrained models for small molecule drug property prediction, focusing on ADMET (absorption, distribution, metabolism, excretion, toxicity) predictions. The authors compare several pretrained models including KERMT, KPGT, and MoLFormer against Chemprop (a non-pretrained baseline) on both public and internal Merck datasets. The study demonstrates that multitask finetuning significantly improves KERMT performance over single-task variants, particularly on larger datasets, with the performance advantage becoming more pronounced with larger training sizes (>10K samples per task). The authors also release two new multitask ADMET dataset splits for benchmarking and provide an accelerated KERMT implementation using cuik-molmaker for industrial drug discovery workflows.

## Method Summary
The authors evaluated multiple pretrained molecular models (KERMT, KPGT, MoLFormer) against Chemprop on ADMET property prediction tasks. They employed multitask finetuning strategies where models were trained simultaneously on multiple related ADMET endpoints, comparing this to single-task approaches. The study used both public benchmarks and internal Merck datasets, with particular focus on internal ADMET data where KERMT showed significant improvements. The KERMT model was enhanced with cuik-molmaker acceleration for practical industrial deployment. Two new multitask ADMET dataset splits were released to enable standardized benchmarking of future methods.

## Key Results
- KERMT multitask outperforms other methods on internal ADMET data with Pearson r² improvements of 0.712 vs 0.657 for Chemprop multitask
- Performance advantage of KERMT becomes more pronounced with larger training sizes (>10K samples per task)
- Multitask finetuning significantly improves KERMT performance over single-task variants
- The study releases two new multitask ADMET dataset splits for benchmarking

## Why This Works (Mechanism)
The paper's results suggest that multitask learning leverages shared chemical substructures and property relationships across ADMET endpoints, enabling better generalization. KERMT's graph-based architecture appears particularly effective at capturing molecular interactions relevant to drug properties. The unexpected finding that pretrained models perform better on larger datasets may indicate that the pretraining provides useful inductive biases that compound with more training data, rather than just helping in low-data regimes. The cuik-molmaker acceleration enables practical deployment by reducing computational overhead while maintaining accuracy.

## Foundational Learning
- Graph neural networks for molecular representations - Needed for understanding how molecular structures are encoded; Quick check: Verify message passing and aggregation steps in KERMT
- Multitask learning principles - Required to understand how joint training on multiple ADMET endpoints improves performance; Quick check: Confirm loss weighting and task balancing strategies
- ADMET property relationships - Essential for grasping why certain properties benefit from shared learning; Quick check: Map correlations between different ADMET endpoints in the datasets
- Pretraining transfer learning - Critical for understanding when and why pretraining helps; Quick check: Compare performance curves across different dataset sizes
- Molecular featurization - Needed to understand input representations; Quick check: Verify SMILES to graph conversion and feature extraction steps

## Architecture Onboarding

**Component Map:**
KERMT (Graph encoder -> Transformer decoder) -> Multitask heads -> Loss aggregation -> Backpropagation

**Critical Path:**
SMILES input -> Molecular graph construction -> KERMT encoder layers -> Task-specific heads -> Multitask loss computation -> Parameter updates

**Design Tradeoffs:**
The paper uses KERMT's graph-based approach over simpler fingerprint methods for richer molecular representations, trading computational complexity for accuracy. The multitask approach trades potential interference between tasks for improved generalization and reduced training overhead compared to training separate models.

**Failure Signatures:**
Performance degradation on individual tasks when dataset imbalance exists, potential negative transfer between unrelated ADMET endpoints, computational bottlenecks during pretraining phase, and possible overfitting when task correlations are weak.

**First Experiments:**
1. Verify that multitask training improves single-task performance compared to isolated training on the same data
2. Test whether removing pretraining (training KERMT from scratch) significantly impacts performance on small datasets
3. Evaluate whether the multitask advantage persists when tasks have varying label distributions or difficulty levels

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The performance advantage of pretrained models on larger datasets contradicts typical expectations that pretraining benefits are most pronounced in low-data regimes
- Different models use different training strategies (some multitask, others single-task), making direct comparisons less straightforward
- The study focuses primarily on KERMT performance with less detailed analysis of KPGT and MoLFormer
- Evaluation is limited to a specific set of ADMET assays, potentially constraining generalizability

## Confidence
- High confidence: KERMT multitask finetuning improves performance over single-task variants on internal Merck datasets
- Medium confidence: KERMT multitask outperforms Chemprop and other methods across tested benchmarks
- Low confidence: The observation that pretrained models perform better with larger datasets represents a general trend beyond this specific experimental setup

## Next Checks
1. Conduct ablation studies specifically testing low-data regime performance to clarify when pretraining benefits are most pronounced versus dataset size
2. Implement consistent training strategies (all models using either single-task or multitask) to enable fairer head-to-head comparisons
3. Expand evaluation to include additional ADMET endpoints and external validation datasets to assess generalizability of findings