---
ver: rpa2
title: Computer-Use Agents as Judges for Generative User Interface
arxiv_id: '2511.15567'
source_url: https://arxiv.org/abs/2511.15567
tags:
- task
- tasks
- feedback
- coder
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AUI-Gym, a benchmark for evaluating automatic
  GUI development, and proposes a Coder-CUA collaboration framework to optimize interfaces
  for agent navigation. AUI-Gym contains 52 applications across six domains, paired
  with 1560 tasks synthesized and validated via language models and rule-based verifiers.
---

# Computer-Use Agents as Judges for Generative User Interface

## Quick Facts
- arXiv ID: 2511.15567
- Source URL: https://arxiv.org/abs/2511.15567
- Authors: Kevin Qinghong Lin; Siyuan Hu; Linjie Li; Zhengyuan Yang; Lijuan Wang; Philip Torr; Mike Zheng Shou
- Reference count: 40
- Primary result: Coder-CUA collaboration improves both functional completeness (up to 81.5%) and CUA success rates (up to 31.5%) through agent-centric feedback-driven UI refinement

## Executive Summary
This work introduces AUI-Gym, a benchmark for evaluating automatic GUI development, and proposes a Coder-CUA collaboration framework to optimize interfaces for agent navigation. AUI-Gym contains 52 applications across six domains, paired with 1560 tasks synthesized and validated via language models and rule-based verifiers. The framework positions a Coder as UI Designer and a Computer-Use Agent as Judge, using navigation success and task solvability as feedback signals. A CUA Dashboard compresses long interaction traces into concise, interpretable images to guide iterative redesign. Experiments show that Coder-CUA collaboration improves both functional completeness and CUA success rates, demonstrating that agent-centric feedback-driven UI refinement can substantially improve agent usability and robustness.

## Method Summary
The framework generates HTML interfaces from text queries using a Coder LLM, then tests them with Computer-Use Agents (CUAs) that navigate via coordinate-based actions. Tasks are proposed by GPT-5 and verified for solvability using JavaScript checkers generated by the same model. Failed navigation attempts are compressed into a single Dashboard image showing only interaction regions, which is then converted to language feedback by a VLM Commenter. The Coder uses this feedback to revise the UI, iterating up to two rounds. Function Completeness measures supported tasks while CUA Success Rate measures navigation achievement.

## Key Results
- Coder-CUA collaboration improves Function Completeness from 60.0% to 81.5% and CUA Success Rate from 23.5% to 31.5% across all applications
- Dashboard compression reduces visual tokens by 76.2% while outperforming both text-only and screenshot-only feedback variants
- Both task solvability filtering and CUA navigation feedback independently contribute to performance gains, with integrated feedback showing the largest improvements
- Revision improvement saturates after approximately two rounds, particularly for stronger coders like GPT-5

## Why This Works (Mechanism)

### Mechanism 1: Task Solvability as Pre-Navigation Filter
Language-based verification prevents wasted computation by filtering impossible tasks before CUA execution. GPT-5 generates JavaScript checkers that analyze the DOM to determine if required UI elements exist. This generates precise "missing feature" signals for targeted UI revision rather than generic improvements.

### Mechanism 2: CUA Navigation Trajectory as UI Diagnostic Signal
CUA navigation failures expose design flaws invisible to static analysis, particularly around affordance visibility, interaction feedback, and multi-step workflow coherence. The framework uses these failures as direct signals for UI improvement rather than treating them as agent capability limits.

### Mechanism 3: Dashboard Compression Reduces Feedback Token Cost
Compressing interaction trajectories into a single 1920×1080 image reduces visual tokens by ~76% while retaining sufficient context for failure localization. This middle-ground approach outperforms both text-only and full-screenshot feedback by balancing efficiency with diagnostic completeness.

## Foundational Learning

- **Markov Design Process (environment-as-tunable):** UI revision is formalized as a Markov process where the state is the current UI, actions are code patches, and rewards come from CUA feedback. This framing differs from traditional fixed-environment agent training by making the environment itself tunable.

- **Rule-Based vs. VLM-as-Judge Verification:** The paper explicitly rejects VLM-as-judge for task completion verification due to reliability issues (VLM accuracy only slightly above naive all-fail baseline). Understanding this design choice clarifies why the framework generates JavaScript checkers instead of relying on vision models.

- **Agent-Centric UI Design Principles:** The framework codifies four principles (State Visibility, Interaction Robustness, Input Permissiveness, Predictable Behavior) that differ from human-centric UX heuristics. These guide what feedback the Dashboard should surface and explain why "de-stylization, higher contrast, simplified layouts" improve CUA success.

## Architecture Onboarding

- **Component map:** Coder (GPT-5/Qwen3-Coder/GPT-4o) -> Initial HTML generation -> Verifier (GPT-5) -> Task feasibility checking -> CUA (UI-TARS-1.5-7B/Operator) -> Navigation execution -> Dashboard -> Commenter (GPT-5/Qwen2.5-VL-72B) -> Language feedback -> Coder revision

- **Critical path:** Query → Coder → Initial HTML (E_0) → For each task: Verifier(E_0, task) → solvable? → if no, add to T_fail → For solvable tasks: CUA navigation → trajectory → rule-based checker passes? → success/failure → Trajectory → Dashboard → Commenter → R_nav → T_fail → R_task → R_task + R_nav → Coder → E_1 → Repeat up to 2 revision rounds

- **Design tradeoffs:** Coordinate-based vs. element-based CUA actions increases difficulty to expose layout issues but may conflate agent grounding failures with UI design flaws. Dashboard vs. full screenshots trades global context for efficiency with 76% token reduction. Rule-based vs. VLM verification offers higher reliability but requires test-time GPT-5 invocation per task.

- **Failure signatures:** Low Function Completeness, high CUA Success indicates tasks are too easy or verifier is permissive. High Function Completeness, low CUA Success indicates UI has features but poor agent accessibility (navigation bottleneck). Revision degrading performance suggests Coder overfitting to specific failure cases or breaking existing functionality.

- **First 3 experiments:** 1) Baseline calibration: Run GPT-5 Coder with no revision loop to establish lower bounds. 2) Ablate feedback types: Compare +Task Solvability only vs. +CUA Navigation only vs. Integrated to validate both signals contribute. 3) Dashboard variant test: Replace Dashboard with text-only action logs or full screenshots to confirm Dashboard's advantage isn't from VLM scale alone.

## Open Questions the Paper Calls Out

### Open Question 1
What specific visual and structural principles define "agent-native" interfaces? The paper identifies "de-stylization, higher contrast, simplified layouts" as beneficial patterns but presents these as empirical insights rather than a formal theory. Ablation studies systematically varying individual UI properties (e.g., contrast ratios vs. element size) would measure their independent causal impact on agent navigation success.

### Open Question 2
Does the Coder-CUA refinement loop face a performance ceiling for state-of-the-art models? The ablation study observes that "revision improvement may saturate for strong coders" (like GPT-5) whereas weaker coders continue to gain, suggesting a potential limit to the feedback loop. Analysis of the error boundary of strong coders would determine if failures are due to verifier ambiguity, CUA noise, or fundamental reasoning limitations.

### Open Question 3
Can an interface be simultaneously optimized for agent efficiency and human usability? The paper optimizes for CUA success by removing "stylistic details crucial for humans" (e.g., de-stylization), implying a potential trade-off between agent efficiency and human aesthetics. A dual-evaluation study measuring human usability scores (e.g., SUS) on the revised UIs alongside CUA success metrics would resolve this question.

## Limitations

- The framework's heavy reliance on GPT-5 for task generation, verification, and coding prevents true human-free operation and creates significant deployment barriers
- Performance gains plateau after approximately two revision rounds, suggesting diminishing returns for deeper collaboration cycles
- Coordinate-based action space may conflate agent capability limitations with genuine UI design flaws, potentially leading to overfitting revisions
- Agent-centric design principles that improve CUA success may degrade human usability, creating potential trade-offs between agent and human accessibility

## Confidence

- **High Confidence:** The core finding that CUA navigation bottlenecks dominate UI agent accessibility issues is well-supported by comparative experiments showing consistent improvements in both Function Completeness (up to 81.5%) and CUA Success Rate (up to 31.5%) when using integrated feedback. The Dashboard compression mechanism's effectiveness is empirically validated through ablation studies.

- **Medium Confidence:** The assumption that task solvability filtering prevents wasted computation is reasonable but depends heavily on the Verifier's accuracy. The claim that CUA failures correlate with fixable UI issues assumes agent capabilities are sufficient for well-designed interfaces.

- **Low Confidence:** The generalizability of agent-centric design principles across different CUA architectures remains unproven, as the evaluation primarily uses UI-TARS and Operator. The optimal number of revision rounds (2) may be specific to the tested Coder capabilities and task distributions.

## Next Checks

1. **False Negative Analysis:** Quantify the Verifier's false negative rate by manually validating a sample of tasks marked as unsolvable to determine if the Coder is adding unnecessary complexity based on incorrect feasibility judgments.

2. **Cross-Agent Generalization:** Test the same UI revisions with a different CUA architecture (e.g., Anthropic's Computer-Using Agent or Microsoft's Magnetic-One) to verify improvements aren't specific to UI-TARS' particular capabilities or failure modes.

3. **Human-UI Interaction Study:** Evaluate whether the agent-centric revisions that improve CUA success rates (e.g., de-stylization, simplified layouts) maintain or improve human usability metrics, or if they create a trade-off between agent and human accessibility.