---
ver: rpa2
title: 'GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection'
arxiv_id: '2504.20437'
source_url: https://arxiv.org/abs/2504.20437
tags:
- galore
- training
- memory
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GaLore 2 improves memory efficiency in LLM pre-training by projecting\
  \ gradients onto low-rank subspaces, reducing optimizer memory usage from 2mn to\
  \ 2nr. Key advancements include fast randomized SVD for efficient subspace updates\
  \ (15\xD7 speedup), integration with FSDP for distributed training, and incorporation\
  \ of low-bit quantization and tensor structures."
---

# GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection

## Quick Facts
- arXiv ID: 2504.20437
- Source URL: https://arxiv.org/abs/2504.20437
- Reference count: 15
- Primary result: Achieves baseline-level performance while reducing optimizer memory from 2mn to 2nr through gradient low-rank projection

## Executive Summary
GaLore 2 introduces a memory-efficient optimization technique for large-scale LLM pre-training by projecting gradients onto low-rank subspaces. The method reduces optimizer memory consumption from O(mn) to O(nr) while maintaining convergence quality, enabling training of larger models or longer sequences within fixed memory budgets. The authors demonstrate successful pre-training of Llama 7B on 500 billion tokens, achieving competitive performance across multiple downstream tasks while significantly reducing memory requirements through integration with FSDP and fast randomized SVD.

## Method Summary
GaLore 2 projects gradients onto low-rank subspaces using a projection matrix updated via fast randomized SVD, reducing optimizer memory from 2mn to 2nr. The method integrates with FSDP to enable per-layer weight updates and gradient discarding, and supports low-bit quantization of projection matrices. During training, full gradients are projected to low-rank space before Adam optimization, then reprojected for weight updates. The subspace is refreshed every T steps using randomized SVD, achieving ~15× speedup over standard SVD while maintaining accuracy. The approach enables training Llama 7B on 500B tokens with competitive downstream performance.

## Key Results
- Successfully pre-trained Llama 7B on 500B tokens using GaLore 2 with baseline-level validation perplexity
- Achieved +0.5 points improvement on paraphrase and semantic similarity tasks compared to baseline
- Demonstrated memory reduction from 2mn to 2nr optimizer states through low-rank gradient projection
- Validated fast randomized SVD provides ~15× speedup over standard SVD with no accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Gradient Low-Rank Projection Preserves Optimization Trajectory
- Claim: Projecting gradients onto a low-rank subspace reduces optimizer state memory from O(mn) to O(nr) while maintaining convergence.
- Mechanism: At each step, compute full gradient Gt ∈ R^m×n, project to Rt = Pt^T·Gt ∈ R^n×r where r << min(m,n). Adam maintains moments Mt, Vt in low-rank space. After Adam produces update Nt, reproject: ̃Gt = α·Pt·Nt.
- Core assumption: Gradients exhibit inherent low-rank structure throughout training; the subspace captures sufficient optimization direction.
- Evidence anchors:
  - [abstract] "leveraging the inherent low-rank structure of weight gradients"
  - [section 3] Memory reduction formula: "GaLore reduces optimizer memory from 2mn to 2nr"
  - [corpus] "Lotus" and "PLUMAGE" papers confirm low-rank gradient structure is exploitable across multiple independent implementations
- Break condition: If gradient rank increases substantially (e.g., during fine-tuning on highly diverse tasks), fixed low-rank may bottleneck learning.

### Mechanism 2: Randomized SVD Approximates Subspace Without Accuracy Loss
- Claim: Fast randomized SVD (Halko et al., 2011) provides ~15× speedup over standard SVD with negligible accuracy degradation.
- Mechanism: Instead of exact SVD, use randomized sampling to approximate top-r singular vectors. Update projection matrix Pt every T steps (default T=500).
- Core assumption: The gradient spectrum at subspace update boundaries is representative; approximation error is bounded.
- Evidence anchors:
  - [section 4.1.2] "fast randomized SVD can be 15X faster than the original SVD operation with no loss in accuracy"
  - [figure 1] Shows Fast SVD matches baseline across Llama 60M, 130M, 350M
  - [corpus] Weak direct evidence on randomized SVD specifically for GaLore; related work focuses on other aspects
- Break condition: If subspace must change very frequently (small T), sign indeterminacy and randomization noise may destabilize training.

### Mechanism 3: FSDP Integration Via Per-Layer Update Hooks
- Claim: FSDP's reduce-scatter hook combined with GaLore enables per-layer gradient projection and immediate gradient discarding.
- Mechanism: After FSDP reduce-scatters gradients per layer, invoke GaLore optimizer to update weights, then discard gradients before next layer—eliminating gradient accumulation memory.
- Core assumption: Per-layer updates don't require cross-layer gradient information; synchronization overhead is acceptable.
- Evidence anchors:
  - [section 4.3] "FSDP introduces a new PyTorch hook that enables per-layer weight updates, fusing the backward pass and weight update"
  - [table 1] Llama3 8B at seq 2048: GaLore+FSDP uses 72.84GB vs AdamW+FSDP at 77.64GB
  - [corpus] No direct corpus validation of FSDP+GaLore integration specifics; this appears novel to GaLore 2
- Break condition: With gradient accumulation steps > 1, memory savings diminish since gradients must be retained.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD)**
  - Why needed here: Core to identifying gradient subspaces; understanding U, S, V matrices clarifies what "projection matrix" means.
  - Quick check question: Given a 4096×11048 matrix, what does the top-r left singular matrix U[:,:r] represent?

- Concept: **Adam Optimizer Moments**
  - Why needed here: GaLore projects Adam's Mt (first moment) and Vt (second moment) into low-rank space; understanding why Adam needs these clarifies what's being compressed.
  - Quick check question: Why does Adam require O(2×parameters) memory beyond the model weights themselves?

- Concept: **Data Parallelism vs. Sharded Data Parallelism**
  - Why needed here: FSDP shards model states; GaLore reduces per-shard memory. Understanding the difference explains why the combination matters.
  - Quick check question: In DDP, how many copies of optimizer states exist across 8 GPUs? In FSDP?

## Architecture Onboarding

- Component map:
  Gradient Projection Layer -> Subspace Manager -> FSDP Adapter -> Quantization Extension (optional)

- Critical path:
  1. Backward pass produces Gt → 2. Project to Rt = Pt^T·Gt → 3. Adam updates Mt, Vt, produces Nt → 4. Reproject ̃Gt = α·Pt·Nt → 5. Apply weight update → 6. Every T steps: compute SVD(Gt), update Pt

- Design tradeoffs:
  - **Rank r**: Higher = more memory, better convergence; paper uses r=1024 for 7B model
  - **Update frequency T**: Lower = fresher subspace, more SVD overhead; paper uses T=500
  - **Scale factor α**: Acts as fractional learning rate; paper tuned α∈{0.125,0.25,0.75,0.1}, selected 0.125
  - Assumption: Paper tuned on first 10B tokens; your data distribution may require retuning.

- Failure signatures:
  - **Loss spikes**: Learning rate too high relative to α; reduce LR or increase α
  - **Convergence stall after subspace update**: T too small; randomization/sign flip instability
  - **OOM despite GaLore**: Gradient accumulation enabled; disable or reduce accumulation steps
  - **Slow training**: SVD still bottleneck; verify randomized SVD is enabled

- First 3 experiments:
  1. **Baseline parity check**: Train small model (e.g., Llama 60M) with both Adam and GaLore on same data; verify perplexity curves match within 2%.
  2. **Memory profiling**: Measure peak memory with/without GaLore+FSDP on target model size; confirm reduction matches theoretical (section 3 formula).
  3. **Hyperparameter sweep**: For your target scale, sweep α × {0.125, 0.25} and T × {200, 500, 1000} on first 1B tokens; log validation loss stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sign indeterminacy inherent in SVD be resolved to enable stable training with very high-frequency subspace updates (e.g., every step)?
- Basis in paper: [explicit] Section 4.1.3 states that sign indeterminacy makes frequent subspace updates unstable, though the issue is negligible at the moderate frequencies (200-500 steps) used in experiments.
- Why unresolved: The authors mitigate the issue by adjusting update frequency rather than solving the underlying sign ambiguity or randomness in the projection vectors.
- What evidence would resolve it: A training run demonstrating stable convergence with subspace updates occurring every step, utilizing a sign-correction algorithm.

### Open Question 2
- Question: What are the theoretical limits of projection matrix quantization before the "approximation gap" causes significant performance degradation?
- Basis in paper: [explicit] Section 4.1.1 notes that "performance degrades when the approximation gap is large" and emphasizes the necessity of carefully choosing the trade-off between spectrum approximation and memory savings.
- Why unresolved: The paper empirically compares random, 1-bit, and 2-bit projections but does not provide a theoretical bound or explanation for why certain approximations fail to track the gradient spectrum effectively.
- What evidence would resolve it: A formal analysis defining the maximum allowable quantization error for the projection matrix relative to the rank and gradient norm.

### Open Question 3
- Question: Does the efficiency of GaLore 2 hold for models with parameter counts significantly larger than 7B (e.g., 70B+)?
- Basis in paper: [inferred] The primary scalability demonstration (500B tokens) is conducted exclusively on a Llama 7B model, while FSDP memory tests are limited to 8B.
- Why unresolved: It is unclear if the computational overhead of coordinating SVD updates and gradient sharding scales linearly or becomes a bottleneck for massive models distributed across thousands of GPUs.
- What evidence would resolve it: Benchmarks showing training throughput and memory scaling curves for GaLore 2 on a 70B or larger model compared to standard FSDP.

## Limitations
- The randomized SVD hyperparameters are not specified, making reproduction dependent on the referenced Halko et al. (2011) implementation details
- FSDP integration lacks concrete implementation details on sharding configuration and hook behavior
- Scale factor α=0.125 was tuned on first 10B tokens without specifying search space or validation methodology
- Limited validation of GaLore 2's efficiency on models significantly larger than 7B parameters

## Confidence
- **High confidence**: Memory reduction from 2mn to 2nr is mathematically sound and verifiable through the subspace projection mechanism
- **Medium confidence**: The 15× speedup claim for randomized SVD is supported by the Halko et al. reference and Figure 1 validation on small models
- **Medium confidence**: The Llama 7B 500B token pre-training success demonstrates scalability, but exact convergence trajectory isn't shown in detail
- **Low confidence**: The claim that "GaLore 2 is compatible with FSDP" lacks demonstration of the specific integration points and potential bottlenecks

## Next Checks
1. Profile memory usage with and without GaLore+FSDP on your target model size. Measure peak memory per GPU during training and verify the theoretical reduction (2mn → 2nr) holds in practice. Check for memory spikes during subspace updates.

2. Implement a controlled ablation comparing GaLore with exact SVD vs randomized SVD on a small model. Measure both accuracy degradation and wall-clock time to verify the 15× speedup claim holds in your environment with your data.

3. Test GaLore's behavior under gradient accumulation. Train with different accumulation steps (1, 4, 8) and measure whether the memory savings persist. Document any performance degradation or training instability that emerges.