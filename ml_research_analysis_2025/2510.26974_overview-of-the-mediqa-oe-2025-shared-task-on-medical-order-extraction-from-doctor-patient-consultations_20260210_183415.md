---
ver: rpa2
title: Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from
  Doctor-Patient Consultations
arxiv_id: '2510.26974'
source_url: https://arxiv.org/abs/2510.26974
tags:
- medical
- clinical
- order
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MEDIQA-OE 2025 shared task, the first
  challenge on extracting structured medical orders from doctor-patient conversations
  to populate EHRs. Six teams participated, leveraging both closed- and open-weight
  LLMs in zero- and few-shot settings.
---

# Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations

## Quick Facts
- arXiv ID: 2510.26974
- Source URL: https://arxiv.org/abs/2510.26974
- Reference count: 6
- Six teams participated; best system (WangLab) achieved 60.2% average score using GPT-4 with JSON-constrained decoding

## Executive Summary
The MEDIQA-OE 2025 shared task introduces the first challenge on extracting structured medical orders from doctor-patient conversations to populate EHRs. Six teams participated, leveraging both closed- and open-weight LLMs in zero- and few-shot settings. The best system (WangLab) achieved a 60.2% average score using GPT-4 with JSON-constrained decoding and detailed instructions. Performance varied significantly by model size, with open-weight LLMs showing a 0.981 Pearson correlation between parameter count and leaderboard rank. Key limitations include the small training set (64 samples), persistent gaps in matching all reference orders (81.8% match score), and lower performance on free-text fields like description and reason.

## Method Summary
The task involves extracting structured medical orders (JSON list) from doctor-patient conversations, where each order has four keys: description, order_type, reason, and provenance. Teams used zero-shot GPT-4 with JSON-constrained decoding or few-shot open-weight models. The winning approach employed a 9-component prompt structure with role attribution, type definitions with rules, and explicit JSON output elicitation. Evaluation used four metrics averaged together: description (ROUGE-1 F1), reason (ROUGE-1 F1), type (accuracy), and provenance (F1 on turn numbers).

## Key Results
- WangLab achieved 60.2% average score using GPT-4 with JSON-constrained decoding and detailed instructions
- Strong Pearson correlation (0.981) between open-weight LLM parameter count and leaderboard ranking
- Provenance accuracy showed largest improvement (+62.0%) from detailed instructions
- Reason field extraction remained challenging (9-41.3% F1) due to dispersed rationale across conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JSON-constrained decoding combined with detailed prompt engineering substantially improves structured extraction accuracy.
- Mechanism: Constrained decoding forces valid JSON schema, reducing formatting errors and enabling reliable parsing. Detailed instructions reduce ambiguity in extraction boundaries, particularly for distinguishing new orders from previously prescribed medications.
- Core assumption: Model has sufficient parametric knowledge to follow complex instructions when explicitly decomposed; schema constraints primarily help with output reliability.
- Evidence anchors: WangLab's 60.2% score with 9-component prompt structure; +62.0% provenance improvement over baseline.

### Mechanism 2
- Claim: Model parameter count strongly predicts extraction performance for open-weight LLMs in few-shot settings.
- Mechanism: Larger models have greater capacity to encode clinical language patterns and handle long-context reasoning across multi-turn dialogues.
- Core assumption: Parameter count serves as proxy for medical domain knowledge and reasoning capacity.
- Evidence anchors: 0.981 Pearson correlation between leaderboard ranking and model sizes; clear progression from Llama3.2 3.2B (15.9%) to Qwen3 32B (53.4%).

### Mechanism 3
- Claim: Chain-of-thought reasoning with self-verification improves free-text field extraction (reason field) but not schema-constrained fields.
- Mechanism: Free-text fields require synthesizing information dispersed across conversation; multi-step reasoning allows evidence collection before committing.
- Core assumption: Extended reasoning time enables better evidence aggregation from long dialogues.
- Evidence anchors: silver_shaw's three-step approach achieved highest reason score at 41.3%; order types show high match scores indicating classification tasks suit LLMs well.

## Foundational Learning

- Concept: **Structured generation with constrained decoding**
  - Why needed here: Medical orders must populate EHR fields with specific schemas; invalid JSON breaks downstream systems.
  - Quick check question: Can you explain why JSON-constrained decoding would improve provenance accuracy (+62%) more than description accuracy (+27.3%)?

- Concept: **Few-shot learning with context budget constraints**
  - Why needed here: Only 64 training samples and long input-output pairs require strategic example selection.
  - Quick check question: Given a 128K context window, a 3000-token dialogue, and a 500-token output, how many few-shot examples can you fit?

- Concept: **Provenance/grounding in long-form dialogue**
  - Why needed here: Task requires identifying which dialogue turn contains the order (provenance).
  - Quick check question: Why might provenance F1 lag behind match F1 by 18% even when the correct order is identified?

## Architecture Onboarding

- Component map: Input Dialogue (raw text) -> Preprocessing (turn segmentation, speaker labeling) -> LLM Backbone (GPT-4 / Gemini / Open-weight model) -> Prompt Construction (instructions + few-shot examples) -> Constrained Decoding (JSON schema enforcement) -> Structured Output (JSON orders) -> Post-processing (validation, deduplication, provenance verification) -> EHR Integration Layer

- Critical path: Prompt engineering → Constrained decoding → Provenance accuracy. The winning approach's 9-component prompt structure is the highest-leverage intervention; provenance is the bottleneck metric (81.8% match vs 63% provenance for WangLab).

- Design tradeoffs:
  - **Closed vs. Open models**: Closed models achieved top ranks with zero-shot; open models require few-shot but allow fine-tuning and on-premise deployment.
  - **Prompt complexity vs. context budget**: Detailed 9-component prompts improve accuracy but consume tokens that could hold few-shot examples.
  - **Agentic vs. single-call approaches**: Multi-step introduces noise; single-call more accurate.

- Failure signatures:
  - **Low reason F1 (9-41%)**: Reason is dispersed across dialogue, often implicit, and optionally annotated.
  - **Order omission (~18% gap)**: Match score of 81.8% means models miss or hallucinate ~1 in 5 orders.
  - **Small model provenance collapse**: Llama3.2 3.2B achieved 5.6% provenance F1 vs. 63% for GPT-4.

- First 3 experiments:
  1. **Baseline replication**: Implement WangLab's 9-component prompt structure with GPT-4 + JSON constrained decoding on 64-sample training set.
  2. **Provenance-focused ablation**: Test whether adding explicit provenance instructions to simpler prompts recovers performance without full 9-component complexity.
  3. **Small model rescue via synthetic data**: Generate 500 synthetic dialogues with gold orders using teacher model, then fine-tune 7B open-weight model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning on larger or synthetically generated datasets enable smaller, open-weight models to match the performance of large closed-source LLMs?
- Basis in paper: Discussion states small training set (64 samples) limits ability to fine-tune, which "could particularly make open-weight small language models more competitive."
- Why unresolved: Shared task focused on zero- and few-shot prompting due to data constraints.
- What evidence would resolve it: Performance evaluation of open-weight models fine-tuned on larger synthetic corpus compared to zero-shot GPT-4 results.

### Open Question 2
- Question: Do embedding-based or hybrid retrieval systems outperform pure generative prompting for the provenance and description fields?
- Basis in paper: Authors note future work could explore embedding-based and hybrid systems to address lag in matching reference orders and identifying provenance.
- Why unresolved: Current top solutions relied on generative LLMs with constrained decoding, yet significant gaps remained.
- What evidence would resolve it: Comparative study benchmarking RAG approaches against current leaderboard results on provenance metric.

### Open Question 3
- Question: How can models better handle extraction of the "reason" field given dispersion of rationale across long dialogues?
- Basis in paper: Paper reports low performance on "reason" field and explicitly hypothesizes this is due to "dispersion of reasons across the conversation."
- Why unresolved: Current prompting strategies failed to close performance gap on this specific free-text attribute.
- What evidence would resolve it: Development of architectures or attention mechanisms specifically designed to aggregate long-range dependencies in clinical dialogue.

## Limitations
- Small training set (64 samples) limits fine-tuning potential and model generalization
- Significant gaps remain between match scores (81.8%) and actual extraction accuracy for provenance and free-text fields
- Annotation noise and non-expert conversational style likely limit maximum achievable accuracy below 100%

## Confidence

- **High Confidence**: Model parameter count strongly correlates with extraction performance (Pearson 0.981)
- **Medium Confidence**: JSON-constrained decoding + detailed prompts explain the 60.2% winning score
- **Medium Confidence**: Chain-of-thought reasoning improves reason field extraction (41.3% F1)

## Next Checks

1. **Ablation of Prompt Components**: Systematically remove each of the 9 prompt components to quantify individual contribution to the 60.2% score and isolate which elements drive provenance improvement.

2. **Scaling Study Beyond 64 Samples**: Test whether the parameter correlation holds when fine-tuning open-weight models on the full 64-sample training set, or when using synthetic data augmentation to reach 500-1000 training orders.

3. **Grounding vs. Extraction Separation**: Create a diagnostic test set where orders are clearly stated but provenance is ambiguous to measure whether performance gaps are due to grounding failures versus extraction failures.