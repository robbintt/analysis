---
ver: rpa2
title: 'Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract
  and Granite'
arxiv_id: '2507.04878'
source_url: https://arxiv.org/abs/2507.04878
tags:
- dataset
- tesseract
- training
- fine-tuning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents experiments on transcribing Spanish historical
  documents using three approaches: Transkribus (a web-based OCR platform), Tesseract
  (a traditional OCR engine), and Granite3.2-vision (a compact multimodal model).
  All experiments were conducted on consumer-grade hardware.'
---

# Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite

## Quick Facts
- **arXiv ID:** 2507.04878
- **Source URL:** https://arxiv.org/abs/2507.04878
- **Reference count:** 29
- **Primary result:** Granite3.2-vision:2b achieved competitive OCR results on historical Spanish documents using consumer hardware, particularly excelling in semantic metrics despite resolution constraints.

## Executive Summary
This paper presents experiments on transcribing Spanish historical documents using three approaches: Transkribus (a web-based OCR platform), Tesseract (a traditional OCR engine), and Granite3.2-vision (a compact multimodal model). All experiments were conducted on consumer-grade hardware. The team achieved competitive results across multiple metrics, with Granite3.2-vision:2b performing particularly well in semantic evaluation metrics (ROUGE scores) despite its small size and hardware constraints. The best performance was achieved using Transkribus with preserved line breaks, though Granite's results were close in several metrics. The work highlights the potential of fine-tuning small multimodal models for OCR tasks on historical documents, while also identifying challenges such as model hallucination and difficulty preserving original formatting.

## Method Summary
The study evaluated three OCR approaches on the PastReader 2025 dataset of historical Spanish press (19th-20th century). Transkribus used the public "Coloso Español" model with inference only. Tesseract v5.5.0 was fine-tuned on converted TIFF images (300 DPI). Granite3.2-vision:2b was fine-tuned using QLoRA with aggressive image downscaling to 414x585 pixels to fit 16GB VRAM, using specific chat-based prompts and LoRA adapters. All approaches were evaluated using Word Error Rate, Levenshtein distance, and semantic metrics (ROUGE, BLEU).

## Key Results
- Transkribus with preserved line breaks achieved the highest overall performance
- Granite3.2-vision:2b excelled in semantic metrics (ROUGE) despite hardware constraints and small size
- Tesseract fine-tuning underperformed compared to the baseline, suggesting limitations in training data quality
- Granite's aggressive image downscaling (414x585) was necessary for memory constraints but impacted literal character accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic preservation in OCR can be maintained by small multimodal models (2B parameters) even under significant hardware constraints, provided the vision-language architecture leverages contextual inference.
- **Mechanism:** The vision encoder extracts features from downsampled images, while the language model decoder uses attention mechanisms to predict text based on both visual tokens and linguistic context. This allows the model to "guess" obscured characters correctly based on surrounding words, favoring meaning over literal character accuracy.
- **Core assumption:** The model's pre-training data (e.g., IBM's DocFM) provides sufficient prior knowledge of document layouts and language structure to compensate for information loss during image downsampling.
- **Evidence anchors:**
  - [abstract]: Granite3.2-vision:2b performed particularly well in semantic evaluation metrics (ROUGE scores) despite its small size and hardware constraints.
  - [section 4.1]: Granite achieved solid second place in semantic metrics (ROUGE) but underperformed in character-level precision (Levenshtein), suggesting it prioritizes semantic coherence.
  - [corpus]: Weak/missing direct evidence for *why* DocFM specifically aids historical Spanish; relies on paper's citation of IBM claims.

### Mechanism 2
- **Claim:** Fine-tuning traditional OCR engines (like Tesseract) on heterogeneous historical data without strict quality control can degrade performance due to noise and misalignment.
- **Mechanism:** The LSTM-based recognition engine updates its weights based on ground-truth transcripts. If the training dataset contains inconsistent transcriptions, layout noise, or poor alignment between image and text (e.g., "heterogeneous" fonts and stains), the model overfits to these artifacts rather than generalizable character shapes.
- **Core assumption:** The baseline pre-trained model is already robust enough that naive fine-tuning on a "dirty" dataset introduces more variance than signal.
- **Evidence anchors:**
  - [section 3.2]: "The baseline model... consistently achieved the best performance... fine-tuning process... may point to limitations in the training data quality."
  - [section 3.2.1]: Analysis showed fine-tuned models failed specifically on low-contrast or decorative elements, exacerbating existing weaknesses.
  - [corpus]: [Design and Implementation of an OCR-Powered Pipeline for Table Extraction] supports that Tesseract requires custom logic for complex layouts, implying it is brittle to unstructured noise.

### Mechanism 3
- **Claim:** Image preprocessing resolution acts as a bottleneck for literal character accuracy in vision-language models constrained by VRAM.
- **Mechanism:** To fit a vision model into consumer VRAM (16GB), images must be resized (e.g., to 414x585 pixels) and quantized (4-bit). This discards high-frequency visual details (fine strokes, serifs) required to distinguish similar characters, forcing the model to rely on lower-resolution features.
- **Core assumption:** The resizing logic (padding to match the processor's crop size of 384x384) preserves the spatial hierarchy of the text lines adequately for the vision tower.
- **Evidence anchors:**
  - [section 3.3]: "We reduced the resolution... to 414x585 pixels... This posed the challenge of potential information loss when resizing."
  - [section 4.1]: Granite ranked 4th in Levenshtein and NED (literal metrics), which aligns with the hypothesis of lost visual fidelity.
  - [corpus]: N/A (Mechanism derived primarily from internal experimental constraints described in the paper).

## Foundational Learning

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - **Why needed here:** Essential for running the Granite experiments. You cannot fit a 2B+ parameter multimodal model into 16GB VRAM for fine-tuning without quantizing the base model and using adapters.
  - **Quick check question:** Does freezing the base model weights in 4-bit precision and only training LoRA adapters update the vision tower? (Answer: Typically no, only the projection/language layers, unless explicitly configured otherwise).

- **Concept: OCR Evaluation Taxonomy (Literal vs. Semantic)**
  - **Why needed here:** To interpret the contradictory results where Granite won on ROUGE (semantic) but lost on Levenshtein (literal). You must understand that "correcting" an archaic spelling hurts literal scores but may help semantic scores.
  - **Quick check question:** If a model transcribes "Mathe-máticas" (historical) as "Matemáticas" (modern), will the Levenshtein distance increase or decrease compared to ground truth? (Answer: Increase/get worse).

- **Concept: Chat Template Formatting for VLMs**
  - **Why needed here:** The Granite model failed to function as a pure OCR engine without specific system prompts and chat structure. The model is instruction-tuned, not a raw OCR engine.
  - **Quick check question:** What is the required structure for the input dataset? (Answer: System prompt + User [Image + Text] + Assistant response).

## Architecture Onboarding

- **Component map:**
  - Historical PDFs -> Python script (PDF → PNG → Resize/Pad to 414x585) -> Dataset wrapper (Image, Text → Chat JSON) -> Granite3.2-vision:2b (4-bit quantized base + LoRA adapters) -> Raw text string

- **Critical path:**
  1. **Data alignment:** Ensuring the `ocr_text` in the dataset perfectly matches the resized image content.
  2. **Memory Management:** Configuring `gradient_accumulation_steps` and `gradient_checkpointing` to avoid OOM on the RTX 5080.
  3. **Prompt Engineering:** Setting the System Prompt to "Return ONLY the raw text... Do not correct" to minimize hallucination.

- **Design tradeoffs:**
  - **Transkribus vs. Granite:** Transkribus offers higher literal accuracy and layout preservation (line breaks) but is a proprietary web service. Granite is open/local but suffers from hallucination and lower literal precision.
  - **Resolution vs. Context:** Lower resolution fits in memory but kills character distinction. Higher resolution OOMs. Padding preserves aspect ratio but adds "empty" tokens.

- **Failure signatures:**
  - **The "Correction" Artifact:** Model outputs valid modern Spanish that is semantically correct but historically inaccurate (low Levenshtein score).
  - **The Hallucination Loop:** Model generates plausible-sounding but fabricated text when facing blank pages or heavy stains (though the paper notes it learned to stay blank sometimes).
  - **Tesseract Overfit:** Fine-tuned model produces garbage on simple layouts because it over-indexed on complex/noisy training samples.

- **First 3 experiments:**
  1. **Establish Baseline:** Run Tesseract (no fine-tuning) vs. Granite (zero-shot) on a 10-page sample to verify the preprocessing pipeline.
  2. **Ablation on Resolution:** Fine-tune Granite on two subsets: one with 414x585 images and one with higher resolution (using gradient checkpointing), measuring the gap in Levenshtein distance.
  3. **Prompt Sensitivity:** Compare the "Strict OCR" system prompt against a "Diplomatic Transcription" prompt to see if instructions can prevent the model from "fixing" archaic spellings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would a dataset annotated with explicit philological and palaeographic transcription guidelines improve OCR model performance on historical Spanish documents?
- **Basis in paper:** [explicit] The authors state: "Our hypothesis is that this dataset variant will improve the model's learning" and are "preparing annotation and transcription guidelines based on the issues detected in the documents of this training dataset for future work."
- **Why unresolved:** Time constraints prevented creating this dataset variant; current dataset may lack coherent transcription standards for historical orthography and formatting.
- **What evidence would resolve it:** A controlled comparison training models on identical documents transcribed with and without philological guidelines, measuring performance on WER, Levenshtein, and semantic metrics.

### Open Question 2
- **Question:** What factors cause Tesseract fine-tuning to underperform compared to the baseline model on heterogeneous historical documents?
- **Basis in paper:** [explicit] "Further investigation would be required to identify the cause of this plateau and improve the efficacy of the fine-tuning approach." The fine-tuned model was outperformed by the baseline on 6/8 metrics.
- **Why unresolved:** Multiple potential causes identified (training data heterogeneity, misalignment between images and transcriptions, preprocessing inadequacy) but root cause not isolated.
- **What evidence would resolve it:** Ablation studies varying training data consistency, preprocessing strategies, and alignment quality; analysis of error patterns across document types.

### Open Question 3
- **Question:** Can addressing the train/test distribution mismatch—particularly the lack of images and different background colours in the test set—improve model generalization?
- **Basis in paper:** [inferred] The authors observed that "dev/train sets used for fine-tuning appear to be more diverse in terms of sources than the test set" which "might have negatively affected the model's learning and generalisation, although this remains a hypothesis."
- **Why unresolved:** The mismatch was identified post-hoc through qualitative analysis; no controlled experiment was conducted.
- **What evidence would resolve it:** Creating balanced dataset splits with matched document types and visual characteristics, then comparing model performance.

## Limitations

- Consumer-grade hardware constraints (16GB VRAM) required aggressive image downscaling to 414x585 pixels, likely degrading literal character accuracy
- The Granite model's tendency to "correct" historical spellings to modern variants artificially inflates semantic scores while penalizing literal accuracy metrics
- Tesseract fine-tuning results suggest data quality issues, but the underlying causal mechanism was not rigorously tested or validated

## Confidence

**High Confidence:** The experimental methodology is sound and well-documented. The hardware constraints are clearly stated, and the comparison framework using multiple evaluation metrics is appropriate. The observation that Transkribus with preserved line breaks achieves the best overall performance is well-supported by the data.

**Medium Confidence:** The claim that Granite3.2-vision:2b performs competitively in semantic evaluation despite hardware constraints is supported but potentially overstated. The model's success in ROUGE scores may be partially attributed to systematic "corrections" of historical spellings rather than genuine semantic understanding.

**Low Confidence:** The assertion that fine-tuning Tesseract on heterogeneous historical data degrades performance due to "limitations in training data quality" lacks sufficient evidence. While the experimental results show this outcome, the underlying causal mechanism is not rigorously tested or validated.

## Next Checks

1. **Resolution Ablation Study:** Conduct a controlled experiment varying input image resolutions (e.g., 414x585, 828x1170, 1242x1755) while keeping all other parameters constant to quantify the relationship between visual fidelity and literal accuracy metrics like Levenshtein distance.

2. **Hallucination Detection Protocol:** Implement a systematic evaluation to distinguish between genuine semantic preservation and spurious "corrections" by analyzing cases where the model changes historical spellings to modern variants, comparing these instances across different evaluation metrics.

3. **Data Quality Impact Analysis:** Perform a controlled fine-tuning experiment on Tesseract using cleaned, aligned training data versus the original heterogeneous dataset to isolate the effect of data quality on model performance, testing the hypothesis that data heterogeneity was the primary cause of degradation.