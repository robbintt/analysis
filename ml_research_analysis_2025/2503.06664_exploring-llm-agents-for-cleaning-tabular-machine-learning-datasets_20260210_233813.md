---
ver: rpa2
title: Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets
arxiv_id: '2503.06664'
source_url: https://arxiv.org/abs/2503.06664
tags:
- dataset
- data
- performance
- errors
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  automate the cleaning of tabular datasets to improve machine learning model performance.
  The authors developed a framework where LLMs, given access to Python and performance
  feedback, iteratively identify and correct errors in corrupted training data without
  modifying the training pipeline or adding new features.
---

# Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets

## Quick Facts
- arXiv ID: 2503.06664
- Source URL: https://arxiv.org/abs/2503.06664
- Authors: Tommaso Bendinelli; Artur Dox; Christian Holz
- Reference count: 32
- Primary result: LLMs can detect and correct instance-level errors in tabular datasets but struggle with distributional shifts, with no model achieving maximum possible performance improvement

## Executive Summary
This study investigates whether large language models can automate cleaning of tabular datasets to improve machine learning model performance. The authors developed a framework where LLMs, given access to Python and performance feedback, iteratively identify and correct errors in corrupted training data without modifying the training pipeline or adding new features. Experiments on three Kaggle datasets with systematically introduced errors show that LLMs can detect and correct some errors by leveraging row-level context and feedback, but struggle with errors requiring multi-row analysis like distribution shifts or biases.

## Method Summary
The authors created an LLM agent framework where models interact with a fixed evaluation pipeline to iteratively clean tabular datasets. The LLM receives a corrupted training dataset and can submit modified versions to an evaluation pipeline that returns performance scores on a clean held-out test set. The framework includes two tools: an IPython shell for data exploration and modification, and a performance evaluation tool that runs a fixed ML pipeline. Experiments used three datasets (Titanic, Hotel Bookings, Car Price) with four types of injected errors (numerical shifts, categorical shifts, NaN corruption, duplicate rows) and tested three hint conditions (no hint, weak hint, strong hint).

## Key Results
- LLMs can detect and correct instance-level errors like outliers and illogical values using within-row context
- No model achieved the maximum possible performance improvement, with strongest hint condition yielding best results
- Performance gains diminished after ~200k tokens, though Gemini's 2M context window enabled further improvement
- All models failed to detect distributional shifts like systematic biases requiring multi-row analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative performance feedback enables LLMs to refine data cleaning through trial-and-error correction.
- Mechanism: The LLM submits modified training datasets to a fixed evaluation pipeline; the returned performance score on a held-out clean test set serves as a reinforcement signal guiding subsequent corrections.
- Core assumption: The downstream ML model's performance is a reliable proxy for data quality improvements.
- Evidence anchors:
  - [abstract] "LLMs, given access to Python and performance feedback, iteratively identify and correct errors"
  - [section 3.2] "The LLM can iteratively submit a modified version of the training dataset to an evaluation pipeline, which returns the performance score"
  - [corpus] "Lost in the Pipeline" paper confirms data preparation is critical and often overlooked in LLM workflows

### Mechanism 2
- Claim: LLMs exploit within-row semantic consistency to detect illogical or contextually inconsistent values.
- Mechanism: By examining relationships between features in the same row, LLMs identify entries that violate common-sense or domain constraints.
- Core assumption: Errors manifest as intra-row inconsistencies rather than distributional anomalies.
- Evidence anchors:
  - [abstract] "LLMs can identify and correct erroneous entries—such as illogical values or outliers—by leveraging contextual information from other features within the same row"
  - [section 4.2.1] Listing 2 shows claude-3-5 detecting that "landlocked countries, such as Afghanistan and Nepal, have abnormally high fish consumption"

### Mechanism 3
- Claim: External hints compensate for LLMs' inability to autonomously detect multi-row distributional errors.
- Mechanism: Providing partial or complete guidance about error locations and types in the initial prompt focuses the LLM's exploration, bypassing the need for cross-row statistical analysis.
- Core assumption: Hints accurately describe the error structure; the LLM can translate hints into effective correction strategies.
- Evidence anchors:
  - [section 4.2.3] "providing hints generally enhances performance, as the models can leverage the extra contextual information"
  - [figure 4] Shows consistent improvement from "No Hint" to "Strong Hint" conditions across all models and datasets

## Foundational Learning

- **Concept: Agent-tool feedback loops**
  - Why needed here: The cleaning agent depends on two tools (IPython, Performance Evaluation) with stateful interaction; understanding how tool outputs feed back into the LLM's context window is essential for debugging failures.
  - Quick check question: Can you trace how a performance score from iteration j influences the code generated in iteration j+1?

- **Concept: Distributional vs. instance-level errors**
  - Why needed here: The paper's central finding is that LLMs handle instance-level errors but fail on distributional shifts; distinguishing these error types determines whether an LLM-based approach is appropriate.
  - Quick check question: Given a dataset with systematic bias (e.g., all 2016 lead times inflated by 10), would single-row inspection suffice to detect the error?

- **Concept: Token budget and context window management**
  - Why needed here: The experiments show diminishing returns after ~200k tokens, but Gemini's 2M context window enabled further improvement; effective deployment requires matching token budgets to task complexity.
  - Quick check question: If an LLM must analyze 100K rows with 28 columns (Hotel Bookings), how would you chunk or sample the data to stay within context limits?

## Architecture Onboarding

- **Component map:**
  - Initial Prompt (P₀) -> IPython Tool -> Performance Evaluation Tool -> Best Dataset Selector

- **Critical path:**
  1. LLM receives P₀ with dataset path and goal
  2. LLM calls IPython to explore data
  3. LLM generates cleaning code, saves modified dataset
  4. LLM calls Performance Evaluation with new dataset path
  5. Loop continues until token budget exhausted
  6. Best-performing submission is returned

- **Design tradeoffs:**
  - Fixed pipeline vs. modifiable pipeline: Authors intentionally freeze preprocessing/training to isolate data quality effects, but this prevents the LLM from compensating via feature engineering
  - Text-only feedback vs. visual artifacts: Current design returns only numerical scores; plots (currently prohibited) could aid distributional error detection
  - Token budget allocation: Higher budgets enable more exploration but with diminishing returns; optimal budget varies by error complexity

- **Failure signatures:**
  - Invalid submissions: 8-14% of submissions fail due to nonexistent file paths or added columns
  - Brute-force submission loops: Models submit datasets without meaningful analysis (29-85% of IPython calls are submission-related)
  - State amnesia: Models (except gpt-4o) regenerate code from scratch rather than building on previous IPython state
  - Distributional blindness: All models failed to detect the +10 bias in Hotel Bookings lead time for 2016

- **First 3 experiments:**
  1. Replicate on Titanic with varied hint conditions: Run the framework on the simplest dataset with no hint / weak hint / strong hint to validate that your implementation matches the paper's performance curves (Figure 4).
  2. Stress-test with increased token budget: Using gemini-2.0-flash-exp, compare performance at 200k vs. 2M tokens on Hotel Bookings to confirm the reported 0.08% → 1.90% improvement (Table 2).
  3. Analyze conversation traces for failure modes: Manually inspect 5-10 iteration logs to classify failures into: (a) reasoning flaws, (b) state amnesia, (c) invalid submission errors—this establishes a baseline for future automated failure analysis.

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research: how to scale the approach to larger datasets with millions of rows, whether LLM-based cleaning can be combined with traditional statistical methods for improved performance, and what the optimal hint strategy looks like for different error types. The authors also question whether performance feedback is the best signal for guiding corrections, or if alternative metrics like data plausibility scores might be more effective.

## Limitations
- Focus on artificial error injection may not capture the complexity of real-world data errors
- Fixed evaluation pipeline prevents the LLM from compensating through alternative feature engineering or preprocessing strategies
- All models failed to detect distributional shifts like systematic biases requiring multi-row analysis

## Confidence
- **High confidence:** The core finding that LLMs excel at instance-level error detection but struggle with distributional shifts is well-supported by experimental results across multiple datasets and models
- **Medium confidence:** The effectiveness of hints in improving performance is demonstrated, but optimal hint strategy and generalizability require further investigation
- **Low confidence:** Scalability claims are limited by absence of tests on larger datasets and observation that even with 2M token budgets, perfect performance was not achieved

## Next Checks
1. **Cross-dataset generalization:** Apply the framework to 2-3 real-world datasets with naturally occurring errors to assess whether the error type classification (instance-level vs. distributional) holds beyond controlled injection scenarios.
2. **Hint optimization study:** Systematically vary hint specificity and structure to determine the minimal effective hint that maximizes performance while minimizing false corrections.
3. **State persistence validation:** Implement a version of the framework that explicitly enforces state reuse across iterations (rather than relying on model memory) and compare performance against the original design.