---
ver: rpa2
title: 'Improving Diversity in Language Models: When Temperature Fails, Change the
  Loss'
arxiv_id: '2508.09654'
source_url: https://arxiv.org/abs/2508.09654
tags:
- recall
- temperature
- precision
- loss
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of temperature scaling on Precision-Recall
  trade-offs in language models. While lowering temperature improves Precision, increasing
  it often fails to boost Recall, contrary to common intuition.
---

# Improving Diversity in Language Models: When Temperature Fails, Change the Loss

## Quick Facts
- arXiv ID: 2508.09654
- Source URL: https://arxiv.org/abs/2508.09654
- Reference count: 40
- Primary result: Temperature scaling fails to reliably improve Recall; training with Recall-oriented losses (TruncR, c-Div, λ-PR) achieves superior P&R trade-offs

## Executive Summary
This paper challenges the common belief that increasing temperature reliably improves diversity in language models. Through theoretical analysis and empirical validation, the authors demonstrate that temperature scaling's effectiveness is fundamentally limited by the sparsity of target distributions. While lowering temperature improves Precision, increasing it often degrades both Precision and Recall beyond certain bounds. The authors propose three novel loss functions—TruncR, c-Div, and λ-PR—that explicitly optimize for Recall while maintaining Precision, achieving superior Precision-Recall trade-offs compared to standard NLL training or temperature scaling alone.

## Method Summary
The authors propose training language models using weighted Negative Log-Likelihood (WNLL) with three novel weighting schemes: TruncR (weights low-likelihood samples), c-Div (weights by ¯Q^(1-α) with α>1 for mass-covering behavior), and λ-PR (complex weighting based on target PR-curve). The framework uses detached model probabilities to compute weights without backpropagation interference. Training proceeds with NLL warmup followed by the Recall-oriented losses. Precision and Recall are evaluated using k-NN manifold support estimation with GPT2-Large embeddings.

## Key Results
- Temperature scaling cannot improve Precision-Recall beyond bounds determined by distribution sparsity (Theorem 4.2)
- TruncR and c-Div consistently achieve higher Recall than NLL baseline while maintaining comparable Precision
- c-Div with α=1.4 shows the best overall performance across multiple tasks (WritingPrompts, MathQA-Python, CodeContests)
- The proposed losses are less stable than NLL but provide superior P&R trade-offs when properly tuned

## Why This Works (Mechanism)

### Mechanism 1: Sparsity-Limited Temperature Effectiveness
- Claim: Temperature scaling cannot improve Precision-Recall beyond bounds determined by distribution sparsity.
- Mechanism: Theorem 4.2 establishes that both αλ and βλ are bounded by |Supp(P)|/(V^L) × e^(ZL/t), where sparser target distributions impose tighter bounds.
- Core assumption: The true conditional distribution P(·|x<l) is sparse due to grammatical/syntactic constraints.
- Evidence anchors: [section] Table 2 shows sparsity estimates: CodeContests 0.03-8.08%, WritingPrompts 3.86-24.9% for top-p thresholds.

### Mechanism 2: Recall-Oriented Loss Functions via Distribution Truncation
- Claim: Training on lower-likelihood samples (TruncR) or using higher α in α-divergence (c-Div) shifts model toward coverage.
- Mechanism: TruncR trains only on samples below a likelihood threshold δ, forcing the model to improve likelihood on underrepresented regions. c-Div with α>1 weights lower-probability tokens more heavily.
- Core assumption: Underrepresented samples correspond to modes of P that Q assigns insufficient probability.
- Evidence anchors: [section] Proposition 5.3: "Optimizing θ using L^(1-∆)_TruncR is equivalent to optimizing Recall for a fixed value of Precision α = 1-∆"

### Mechanism 3: Asymmetric Temperature Dynamics via λ-Regimes
- Claim: Temperature affects Precision monotonically but Recall non-monotonically with a peak at task-dependent t₀.
- Mechanism: Proposition A.4 identifies three λ-regimes on the PR-curve. In the high-λ regime, both metrics decrease with temperature.
- Core assumption: Model distribution approximates target with specific error patterns (some tokens underweighted, some noise outside support).
- Evidence anchors: [section] Proposition 4.3: "For low values of λ, the Recall βλ decreases for temperatures t ≥ t₀, provided that... V is much larger than K"

## Foundational Learning

- **Concept**: Precision-Recall for Generative Models
  - Why needed here: The paper's core framework redefines diversity evaluation. Precision = Q(Supp(P)) measures quality; Recall = P(Supp(Q)) measures coverage of valid distribution modes.
  - Quick check question: If a model generates only "the the the", does it have high or low Precision? High or low Recall?

- **Concept**: Temperature Scaling
  - Why needed here: Standard decoding intervention whose limitations motivate the paper. Temperature t sharpens (t<1) or flattens (t>1) the softmax distribution.
  - Quick check question: As t→∞, what distribution does Q_t approach?

- **Concept**: α-Divergence Minimization
  - Why needed here: Generalizes NLL (α→1) to controllable mass-covering vs mode-seeking. Higher α prioritizes covering all modes (Recall); lower α focuses on dominant modes (Precision).
  - Quick check question: What happens to gradient weighting when α=0.5 vs α=1.5 for low-probability tokens?

## Architecture Onboarding

- **Component map**: Training loop -> Forward pass computes log-likelihoods -> Detach to get ¯Q -> Compute weights based on method -> Weighted NLL backpropagation -> Maintain rolling buffer for Trunc/TruncR quantile estimation

- **Critical path**:
  1. Forward pass computes log-likelihoods
  2. Detach to get ¯Q (no gradient tracking)
  3. Compute weights based on method
  4. Weighted NLL backpropagation
  5. For Trunc/TruncR: maintain rolling buffer across batches for quantile estimation

- **Design tradeoffs**:
  - K (buffer size): Larger K → more stable quantiles but higher memory; authors use K=buffer size for dataset
  - α in c-Div: Authors find α≈1.3-1.4 effective; higher values cause instability
  - γ in λ-PR: Very small (10⁻⁵ to 10⁻⁷) to avoid degenerate solutions

- **Failure signatures**:
  - Degenerate outputs: λ-PR with large γ produces repetitive/nonsensical text
  - Training divergence: c-Div with α>1.5 may cause gradient explosion
  - No Recall improvement: If quantile threshold δ is too extreme (very low for TruncR), insufficient samples receive gradient signal

- **First 3 experiments**:
  1. Train with NLL, sweep temperature t∈[0.2, 2.0], plot P&R curves on integer multiplication task to confirm asymmetric temperature effects
  2. Compare Trunc(∆=0.25), TruncR(∆=0.25), GOLD(α=0.5), c-Div(α=1.4) on WritingPrompts at t=1; verify Trunc/GOLD favor Precision, TruncR/c-Div favor Recall
  3. Train with c-Div(α=1.4), sweep temperature; confirm superior P&R trade-off curve compared to NLL baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the training instability observed in Recall-oriented losses (TruncR, c-Div, λ-PR) be mitigated to match the robustness of Negative Log-Likelihood (NLL)?
- **Open Question 2**: How do Recall-oriented training objectives interact with standard alignment techniques like Reinforcement Learning from Human Feedback (RLHF)?
- **Open Question 3**: Do the theoretical benefits of Recall-oriented losses scale effectively to models significantly larger than 8B parameters without divergence?

## Limitations
- The theoretical framework relies heavily on the sparsity assumption for target distributions
- The λ-PR loss function shows poor empirical performance and produces degenerate outputs
- Evaluation methodology using k-NN manifold support estimation may be sensitive to implementation details

## Confidence

**High Confidence**:
- Temperature scaling has asymmetric effects on Precision (monotonically improves with lower t) versus Recall (peaks then degrades with higher t)
- Recall-oriented losses (TruncR, c-Div with α>1) consistently outperform NLL in achieving higher Recall at comparable Precision levels
- The sparsity of target distributions is empirically validated and provides a sound theoretical foundation

**Medium Confidence**:
- The theoretical bounds in Theorem 4.2 accurately predict temperature scaling limitations across all tasks
- The proposed loss functions will maintain performance advantages when scaled to larger models
- The k-NN manifold support estimation provides reliable Precision/Recall measurements that generalize

**Low Confidence**:
- λ-PR loss will be practically useful given its instability and degenerate output issues
- The specific α=1.4 parameter for c-Div is optimal across all tasks and model scales
- Temperature scaling combined with Recall-oriented training will outperform all alternative diversity-promoting methods

## Next Checks

1. **Sparsity-Scaling Experiment**: Systematically vary target distribution sparsity across multiple tasks and measure how the temperature effectiveness bounds scale to validate Theorem 4.2's predictions across the full sparsity spectrum.

2. **Cross-Scale Generalization Test**: Implement the same experimental protocol on a 7B parameter model and compare the relative performance of NLL, TruncR, c-Div, and λ-PR to test generalization beyond the 1-3B parameter range.

3. **Alternative Evaluation Protocol**: Replace the k-NN manifold support estimation with human evaluation or automated acceptability scoring to validate whether observed performance differences are robust to the evaluation methodology.