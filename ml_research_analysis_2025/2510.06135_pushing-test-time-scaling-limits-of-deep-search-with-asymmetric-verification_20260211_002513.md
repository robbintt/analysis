---
ver: rpa2
title: Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification
arxiv_id: '2510.06135'
source_url: https://arxiv.org/abs/2510.06135
tags:
- scaling
- search
- tool
- compute
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically investigates test-time scaling of deep
  search agents, combining sequential (budget forcing, max tool calls) and parallel
  (multiple trajectories, verifier-based selection) strategies. The core insight is
  asymmetric verification: verifying answers requires far fewer tool calls than generating
  them, making verification a highly efficient scaling target.'
---

# Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification

## Quick Facts
- **arXiv ID:** 2510.06135
- **Source URL:** https://arxiv.org/abs/2510.06135
- **Reference count:** 19
- **Primary result:** Asymmetric verification enables efficient test-time scaling, with GLM-4.5 Heavy achieving 54.0% on BrowseComp and Tongyi-DeepResearch Heavy reaching 69.0% on BrowseComp, surpassing proprietary agents.

## Executive Summary
This work systematically investigates test-time scaling of deep search agents by combining sequential (budget forcing, max tool calls) and parallel (multiple trajectories, verifier-based selection) strategies. The core insight is asymmetric verification: verifying answers requires far fewer tool calls than generating them, making verification a highly efficient scaling target. Experiments show that while sequential scaling initially boosts performance, gains plateau quickly. Parallel scaling expands exploration but lacks exploitation; adding a verifier to filter candidates delivers superior accuracy-cost trade-offs. Applying these methods, open-source models (GLM-4.5, K2, Qwen3-2507, Tongyi DeepResearch) are extended into "Heavy" variants, achieving up to 27 absolute points gains on benchmarks. GLM-4.5 Heavy matches top commercial agents on BrowseComp (54.0%) and GAIA (66.0%), while Tongyi-DeepResearch Heavy reaches 69.0% on BrowseComp, surpassing proprietary results.

## Method Summary
The method employs ReAct-based search agents with a combined search+browse tool to navigate the web. Sequential scaling uses budget forcing (forcing continuation after early termination) and max tool call limits to extend search depth. Parallel scaling generates multiple trajectories (K=8-32) to increase exploration. The key innovation is asymmetric verification, where a verifier agent (same model, different prompt) checks candidate answers against constraints, requiring far fewer tool calls than the original search. Heavy variants combine sequential→parallel→verifier scaling. The verifier extracts conditions from questions, verifies each via search, and outputs confidence scores. Candidates are aggregated via Best-of-K or Weighted Voting based on verifier scores.

## Key Results
- Asymmetric verification achieves superior accuracy-cost trade-offs, with verification requiring 75 vs 18 tool calls for BrowseComp
- Sequential scaling plateaus after initial gains due to reasoning degradation in long trajectories
- Parallel scaling with verifier outperforms simple majority voting, converting exploration into accuracy
- GLM-4.5 Heavy matches commercial agents at 54.0% BrowseComp and 66.0% GAIA
- Tongyi-DeepResearch Heavy surpasses proprietary results with 69.0% on BrowseComp

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Allocating compute to verification yields disproportionate accuracy gains compared to allocating compute to generation in deep search tasks.
- **Mechanism:** This relies on the **Asymmetry of Verification**. In deep search, the "forward" process requires navigating a vast, sparse web space. In contrast, the "backward" verification process starts with a candidate answer and checks specific constraints (e.g., "Does this game match the release year?"), drastically shrinking the search space.
- **Core assumption:** Assumption: The cost of checking constraints is significantly lower than the cost of discovering the satisfying entity from scratch.
- **Evidence anchors:**
  - [abstract] "verifying responses can be substantially easier than generating them... highlights the strong potential of test-time scaling"
  - [section 3.1] "verification only needs minimal effort to confirm whether a predicted answer satisfies the necessary conditions"
  - [corpus] Related work "SETS" supports this by showing improved scaling when leveraging self-verification and self-correction.
- **Break condition:** This mechanism fails if the task has "near-symmetry," where verifying an answer requires nearly as much effort as solving it (e.g., mathematical proofs or specific xbench-DeepSearch tasks mentioned in Appendix G).

### Mechanism 2
- **Claim:** Sequential scaling (budget forcing) improves performance initially but eventually plateaus or degrades, necessitating a shift to parallel scaling.
- **Mechanism:** Forcing a model to continue searching after it would naturally stop (Budget Forcing) prevents premature termination. However, extending this indefinitely taxes the model's ability to maintain coherent long-range reasoning, leading to noise and performance drops.
- **Core assumption:** Assumption: The model's internal state/context window degrades in quality or focus over excessive trajectory lengths.
- **Evidence anchors:**
  - [abstract] "sequential scaling methods, such as budget forcing, can be effective initially but soon degrade performance"
  - [section 2.4] "excessive budget forcing eventually degrades performance... saturation suggests that long trajectories challenge models to perform coherent long-range reasoning"
  - [corpus] "To Backtrack or Not to Backtrack" discusses limits of sequential search in reasoning, aligning with the finding that sequential scaling has bounds.
- **Break condition:** If a model architecture specifically solves for long-horizon dependency maintenance (e.g., specialized state management), the degradation point would likely shift further out.

### Mechanism 3
- **Claim:** Parallel scaling provides high exploration (Pass@K), but requires a Verifier to effectively exploit the generated candidates (Maj@K or Best-of-K).
- **Mechanism:** Sampling multiple trajectories increases the probability that at least one trajectory contains the correct answer. However, the model cannot reliably *select* this answer via simple majority voting because correct answers are often sparse or diverse. An external Verifier Agent filters these candidates, converting exploration into accuracy.
- **Core assumption:** Assumption: The Verifier Agent has a higher precision in judging validity than the Search Agent has in generating valid candidates on the first try.
- **Evidence anchors:**
  - [abstract] "parallel scaling expands exploration but lacks exploitation; adding a verifier to filter candidates delivers superior accuracy-cost trade-offs"
  - [section 3.4] "introducing a verifier achieves a larger gain with far less cost... [compared to] simply expanding the search agent"
  - [corpus] "Sample, Scrutinize and Scale" corroborates that scaling verification is effective for inference-time search.
- **Break condition:** If the Verifier Agent is weak or hallucinates confidence scores (over-optimistic on wrong answers), it may select incorrect candidates, negating the benefits of parallel exploration.

## Foundational Learning

- **Concept: Pass@K vs. Maj@K Metrics**
  - **Why needed here:** The paper hinges on the gap between these metrics. Pass@K measures if the answer exists in a set (exploration), while Maj@K measures if the model can settle on it (exploitation). Understanding this gap is critical to understanding why a Verifier is needed.
  - **Quick check question:** If a model has a Pass@32 of 60% but a Maj@32 of 10%, what does this imply about the distribution of correct answers in the sampled trajectories?

- **Concept: ReAct (Reasoning + Acting)**
  - **Why needed here:** The "Search Agent" described is a ReAct-style agent. It iterates between reasoning traces and tool calls (deep_websearch). You must understand this loop to implement the budget forcing or verification logic.
  - **Quick check question:** In a ReAct loop, when should the agent decide to call a tool versus outputting a final answer?

- **Concept: Tool Call Efficiency**
  - **Why needed here:** The paper uses "number of tool calls" as the proxy for compute cost. Optimizing the system requires minimizing calls for verification while maximizing calls for generation only when necessary.
  - **Quick check question:** Why is counting tool calls a better proxy for cost in a deep search agent than counting output tokens?

## Architecture Onboarding

- **Component map:** User Query -> Search Agent (Parallel Sampling, e.g., K=16) -> Collect Candidate Answers from all trajectories -> Verifier Agent (Assigns Confidence Score for each Candidate) -> Aggregator (selects answer with highest confidence via Best-of-K)

- **Critical path:**
  1. User Query -> Search Agent (Parallel Sampling, e.g., K=16).
  2. Collect Candidate Answers from all trajectories.
  3. For each Candidate -> Verifier Agent (Assigns Confidence Score).
  4. Aggregator selects the answer with the highest confidence (Best-of-K).

- **Design tradeoffs:**
  - **Sequential vs. Parallel:** Sequential (Budget Forcing) is cheaper initially but caps out. Parallel scales better but costs more.
  - **Search vs. Verifier Compute:** The "Heavy" variant relies on shifting budget *away* from massive parallel search and *toward* parallel verification (e.g., verifying each candidate 4 times).

- **Failure signatures:**
  - **Premature Termination:** Agent gives up too early. (Fix: Budget Forcing).
  - **Reasoning Drift:** Agent loops or hallucinates after too many forced steps. (Fix: Cap sequential steps, switch to parallel sampling).
  - **Verifier Overconfidence:** Verifier assigns high scores to wrong answers. (Fix: Use Weighted Voting or multiple verifier samples).

- **First 3 experiments:**
  1. **Baseline:** Run the Search Agent with a standard tool limit (e.g., 50) on a 100-sample subset of BrowseComp. Record Pass@1 and tool usage.
  2. **Sequential Saturation:** Apply Budget Forcing (force +20 tool calls after termination). Plot accuracy vs. tool calls to observe the degradation point described in Section 2.4.
  3. **Verifier Injection:** Run Pass@8 parallel sampling. Compare "Majority Vote" vs. "Verifier Best-of-8" to quantify the value of the asymmetric verification mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can step-level verification during search trajectories provide further gains beyond final-answer verification?
- **Basis in paper:** [explicit] "Looking ahead, we aim to adopt more flexible strategies. For example, verifiers could be applied at each step along the search trajectory, or even used to actively guide the search process."
- **Why unresolved:** Current work only applies verifiers to filter final candidate answers; intermediate verification could prune bad paths earlier but introduces new complexity.
- **What evidence would resolve it:** Experiments comparing final-answer verification against step-by-step process verification, measuring accuracy and compute trade-offs.

### Open Question 2
- **Question:** Can search agents internalize verification capabilities through training rather than relying on external verifiers at inference time?
- **Basis in paper:** [explicit] "We envision incorporating asymmetric verification more deeply into training, enabling search agents to internalize verification capabilities and using them directly during inference."
- **Why unresolved:** Current approach uses separately prompted verifier agents; training-based internalization requires new RL objectives and data.
- **What evidence would resolve it:** Training agents with verification-aware objectives and comparing against external verifier baselines on identical compute budgets.

### Open Question 3
- **Question:** What determines whether a task exhibits strong vs weak asymmetric verification, and can we predict this a priori?
- **Basis in paper:** [inferred] The paper shows BrowseComp has strong asymmetry (75 vs 18 tool calls) while xbench-DeepSearch shows near-symmetry, but does not provide a systematic characterization.
- **Why unresolved:** Task properties governing verification difficulty relative to generation remain unstudied.
- **What evidence would resolve it:** Analysis correlating task features (constraint types, answer space structure) with measured search-to-verification tool call ratios across diverse benchmarks.

## Limitations
- **Cross-domain generalizability:** Asymmetric verification effectiveness on highly technical domains (mathematical proofs, scientific reasoning) where verification difficulty approaches generation difficulty remains unclear.
- **Verifier calibration uncertainty:** Verifier agent confidence score calibration across different confidence thresholds hasn't been systematically evaluated.
- **Alternative exploitation strategies:** The paper doesn't thoroughly explore alternative parallel exploitation methods beyond verifier-based selection.

## Confidence
- **High Confidence:** The core mechanism of asymmetric verification - the empirical observation that verification requires fewer tool calls than generation is well-supported by the data across multiple benchmarks and models.
- **Medium Confidence:** The sequential scaling saturation point - while the paper demonstrates clear degradation with excessive budget forcing, the exact inflection point likely varies by model architecture and task complexity.
- **Medium Confidence:** The necessity of verifier agents for parallel scaling - the paper shows superior performance with verifiers, but alternative exploitation strategies weren't thoroughly explored.

## Next Checks
1. **Cross-Domain Verification Asymmetry:** Systematically measure the solver-to-verifier tool call ratio across diverse benchmarks (mathematical reasoning, code generation, scientific Q&A) to quantify where asymmetric verification breaks down.

2. **Verifier Calibration Analysis:** Evaluate verifier agent confidence score calibration by plotting predicted confidence against actual accuracy across different confidence thresholds, identifying potential overconfidence in incorrect answers.

3. **Alternative Exploitation Strategies:** Compare verifier-based selection against alternative parallel exploitation methods (ensemble voting, diversity-based selection, uncertainty sampling) to establish whether asymmetric verification is the optimal approach or one of several viable strategies.