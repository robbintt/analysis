---
ver: rpa2
title: Rethinking Regularization Methods for Knowledge Graph Completion
arxiv_id: '2505.23442'
source_url: https://arxiv.org/abs/2505.23442
tags:
- regularization
- knowledge
- graph
- hits
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses overfitting in Knowledge Graph Completion
  (KGC) models through a novel regularization approach. The authors systematically
  analyze how regularization affects KGC model performance and propose the SPR (Sparse
  Peak Regularization) method, which selectively penalizes significant components
  in embedding vectors while ignoring noise.
---

# Rethinking Regularization Methods for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2505.23442
- Source URL: https://arxiv.org/abs/2505.23442
- Authors: Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Haoran Duan, Zhengwei Tao, Xuan Zhang, Jiandong Li
- Reference count: 40
- Primary result: Novel SPR regularization method improves KGC model generalization by selectively penalizing significant embedding components while ignoring noise

## Executive Summary
This paper addresses overfitting in Knowledge Graph Completion (KGC) models through a novel regularization approach. The authors systematically analyze how regularization affects KGC model performance and propose the Sparse Peak Regularization (SPR) method, which selectively penalizes significant components in embedding vectors while ignoring noise. Through extensive experiments across multiple datasets (WN18RR, FB15K-237, YAGO3-10) and various KGC models (CP, ComplEx, GIE, RESCAL, CompGCN, HGE), SPR consistently outperforms existing regularization methods. For example, SPR improves CP model MRR from 0.438 to 0.479 on WN18RR and ComplEx MRR from 0.350 to 0.371 on FB15K-237. The method also demonstrates effectiveness in Temporal KGC tasks. Theoretical analysis shows SPR provides lower Rademacher complexity and more stable gradients compared to Dropout, explaining its superior generalization performance.

## Method Summary
SPR regularization works by selectively penalizing embedding components with significant features while ignoring those that contribute little and may represent noise. The method sorts squared components of embedding vectors in non-decreasing order, masks the smallest components whose cumulative sum falls below a threshold δ, and applies regularization only to the unmasked components. This approach differs from traditional L2 regularization by focusing on sparse peaks rather than overall vector magnitude. The regularization term is added to the main KGC loss function and is compatible with various model architectures including translation-based, tensor decomposition-based, and GNN-based models. SPR computes four sparsified terms per triple: entity head embeddings, entity tail embeddings, and their element-wise interactions with relation embeddings.

## Key Results
- SPR improves CP model MRR from 0.438 to 0.479 on WN18RR dataset
- SPR improves ComplEx MRR from 0.350 to 0.371 on FB15K-237 dataset
- SPR consistently outperforms existing regularization methods (L2, Dropout, DURA, F2, N3) across multiple KGC models and datasets
- Theoretical analysis shows SPR provides lower Rademacher complexity and more stable gradients compared to Dropout

## Why This Works (Mechanism)

### Mechanism 1: Selective Component Penalization via Rank-Based Sparsity
- Claim: SPR improves generalization by focusing regularization on embedding components with large squared magnitudes while discarding small components that may represent noise.
- Mechanism: For each embedding vector, SPR sorts squared components in non-decreasing order and masks (zeros) the smallest components whose cumulative sum falls below threshold δ. Only unmasked large components contribute to the regularization penalty (Eq. 3-4, Algorithm 1).
- Core assumption: Small-magnitude embedding dimensions encode noise or spurious correlations; large-magnitude dimensions encode task-relevant semantic features.
- Evidence anchors:
  - [abstract] "The core idea is to selectively penalize those components with significant features in the embedding vector, thus effectively ignoring many components that contribute little and may only represent noise."
  - [section 3.2] Defines sparsification operator and Lemma 1 establishing error bound ≤ δ
  - [corpus] Related VIR paper on intermediate variable regularization shares conceptual similarity but targets different mechanism; direct corpus validation of SPR's specific selective sparsity is weak
- Break condition: If embedding dimensions are uniformly informative (no noise structure) or if δ is set too high (dropping signal components), performance will degrade below baseline.

### Mechanism 2: Variance Reduction via Deterministic Masking
- Claim: SPR's deterministic mask yields lower gradient variance compared to stochastic dropout, stabilizing optimization.
- Mechanism: Unlike dropout's Bernoulli sampling, SPR's mask M(x) is a deterministic function of current embeddings. Per Proposition 1 in Appendix A.6.1, conditional gradient variance is zero for SPR vs. λ²p(1-p)‖x‖²₂ for dropout.
- Core assumption: Reduced gradient variance translates to more reliable parameter updates and faster/better convergence in SGD settings.
- Evidence anchors:
  - [section A.6.1] "SPR's deterministic nature yields zero conditional variance in the gradient, unlike Dropout's stochastic noise"
  - [section A.7] Theoretical comparison of Rademacher complexity bounds
  - [corpus] No direct corpus validation of variance-reduction claim for KGC specifically
- Break condition: If stochasticity provides beneficial exploration of parameter space, purely deterministic masking could underperform on highly non-convex loss landscapes.

### Mechanism 3: Model-Agnostic Regularization via Interaction Term Sparsification
- Claim: SPR generalizes across translation-based, tensor-decomposition, and GNN-based KGC models by regularizing entity embeddings, relation embeddings, and their element-wise interactions uniformly.
- Mechanism: SPR computes four sparsified terms per triple: ‖Vʰᵢ‖²_sparse, ‖Vᵗₖ‖²_sparse, ‖Vʰᵢ ⊙ Rʳⱼ‖²_sparse, ‖Vᵗₖ ⊙ Rʳⱼ‖²_sparse (Eq. 6-10). This formulation doesn't depend on specific scoring function form.
- Core assumption: Entity-relation element-wise interactions are meaningful regularizands across diverse model architectures.
- Evidence anchors:
  - [abstract] "SPR regularization method...can support more types of KGC models, including GNN-based, translation-based, tensor decomposition-based KGC models"
  - [section 4.4] Experiments on CompGCN (GNN), HGE (temporal), CP/ComplEx/RESCAL (tensor), GIE (embedding)
  - [corpus] VIR paper focuses only on TDB models; SPR's broader applicability is a claimed differentiator
- Break condition: For models where element-wise product ⟨Vʰᵢ, Rʳⱼ⟩ lacks semantic meaning (e.g., complex rotation-based models with phase interactions), SPR may provide weaker regularization.

## Foundational Learning

- Concept: **Knowledge Graph Completion (KGC) as Link Prediction**
  - Why needed here: SPR is a model-agnostic regularizer; understanding the underlying task clarifies what "better embeddings" means for downstream evaluation.
  - Quick check question: Given triple (Paris, capital_of, ?), does the model rank "France" higher than "Germany"?

- Concept: **Overfitting Diagnosis via Train-Valid Gap**
  - Why needed here: The paper's core empirical finding is that many KGC models show large train-valid performance gaps (Figure 1), motivating regularization.
  - Quick check question: If training MRR=0.95 and validation MRR=0.45, is the model overfitting?

- Concept: **Sparsity-Inducing Regularization (L1, Hard Thresholding)**
  - Why needed here: SPR uses hard thresholding on sorted squared components; familiarity with sparsity concepts helps understand why selective penalization differs from L2 shrinkage.
  - Quick check question: Does L2 regularization (‖x‖²₂) produce sparse solutions? Does SPR?

## Architecture Onboarding

- Component map: Embeddings -> SPR Module (sorting, masking, sparsified norm) -> Regularization term -> Loss function

- Critical path:
  1. Extract embeddings for each triple in batch
  2. Compute squared element vectors: (Vʰᵢ)², (Vᵗₖ)², (Vʰᵢ)²⊙(Rʳⱼ)², (Vᵗₖ)²⊙(Rʳⱼ)²
  3. For each vector, sort ascending -> find largest S where cumulative sum ≤ δ -> mask smallest S elements
  4. Sum unmasked elements across all four vectors -> average over batch
  5. Backprop through unmasked components only

- Design tradeoffs:
  - **δ selection**: Grid search required (paper uses δ ∈ {0.05, ..., 0.5}); too high → signal loss; too low → insufficient noise removal
  - **Computational overhead**: Sorting O(d log d) per embedding dimension d; negligible for typical d=200–2000 but adds up for large batches
  - **Compatibility**: Paper claims broad compatibility; however, F2/N3 regularizers produced NAN errors for some model combinations (Table 1), suggesting implementation subtleties

- Failure signatures:
  - **Embedding collapse**: If λ too high, all metrics drop sharply (Figure 3 shows MRR < 0.1 at λ=5×10⁻¹ on some datasets)
  - **No improvement over baseline**: If δ too small, SPR approximates full L2 norm without selective benefit
  - **Training instability**: NaN losses reported for F2 regularizer with ComplEx/GIE; monitor for similar issues with extreme δ

- First 3 experiments:
  1. **Baseline overfitting check**: Train ComplEx on FB15K-237 without regularization; plot train vs. valid MRR over epochs to confirm gap per Figure 1 pattern
  2. **δ sensitivity sweep**: With λ fixed at 5×10⁻³, vary δ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} on validation set; expect peak near δ≈0.4 (Figure 4)
  3. **Head-to-head vs. DURA**: Run CP-SPR vs. CP-DURA on WN18RR; expect MRR improvement 0.479 vs. 0.471 (Table 1); if SPR underperforms, check mask computation and gradient flow

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that small embedding components primarily encode noise assumes a specific data structure that may not hold for all KGC tasks or domains
- Theoretical advantages in gradient variance reduction and Rademacher complexity are derived but not empirically validated for KGC specifically
- The paper reports NaN issues with F2/N3 regularizers for certain model combinations but doesn't fully explore why SPR avoids similar problems

## Confidence

**High Confidence**: SPR's empirical performance improvements on standard KGC benchmarks (WN18RR, FB15K-237, YAGO3-10)

**Medium Confidence**: The theoretical justification for SPR's advantages over Dropout (variance reduction, Rademacher bounds)

**Medium Confidence**: SPR's compatibility with temporal KGC and GNN-based models based on limited experiments

## Next Checks
1. **Noise Structure Validation**: Conduct ablation studies on artificially corrupted embeddings to verify SPR's selective penalization of noise components actually improves generalization
2. **Gradient Variance Measurement**: Instrument training to measure empirical gradient variance for SPR vs. Dropout across multiple KGC models and compare with theoretical predictions
3. **Extreme Data Domain Testing**: Evaluate SPR on KGs with different statistical properties (e.g., highly asymmetric relations, sparse entities) to test robustness beyond the standard benchmark datasets