---
ver: rpa2
title: 'From Passive to Active Reasoning: Can Large Language Models Ask the Right
  Questions under Incomplete Information?'
arxiv_id: '2506.08295'
source_url: https://arxiv.org/abs/2506.08295
tags:
- reasoning
- correct
- turn
- question
- digits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AR-Bench, a novel benchmark for evaluating\
  \ large language models' active reasoning capabilities. The benchmark comprises\
  \ three task families\u2014detective cases, situation puzzles, and guessing numbers\u2014\
  designed to measure performance across commonsense, logical, and symbolic reasoning\
  \ challenges."
---

# From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?
## Quick Facts
- arXiv ID: 2506.08295
- Source URL: https://arxiv.org/abs/2506.08295
- Reference count: 40
- AR-Bench benchmark reveals state-of-the-art LLMs achieve as low as 35% accuracy on active reasoning tasks

## Executive Summary
This paper introduces AR-Bench, a novel benchmark for evaluating large language models' active reasoning capabilities. The benchmark comprises three task families—detective cases, situation puzzles, and guessing numbers—designed to measure performance across commonsense, logical, and symbolic reasoning challenges. Through systematic evaluation, the authors find that state-of-the-art models like GPT-4o achieve as low as 35% accuracy on active reasoning tasks, with performance gains plateauing rapidly despite increased interaction rounds. Advanced methods including tree-of-thought search and post-training approaches show minimal improvements. Models struggle to consistently generate high-quality questions, ask overly broad queries, and make common errors across tasks. The benchmark reveals a significant gap between passive and active reasoning capabilities, highlighting the need for new methodologies incorporating interactive learning and real-time feedback loops.

## Method Summary
AR-Bench evaluates LLMs through three multi-turn interaction tasks: Detective Cases (DC), Situation Puzzles (SP), and Guessing Numbers (GN). Models act as players asking questions to NPC judges (Llama-405B for DC/SP, rule-based oracle for GN) over 25 rounds. Performance is measured through outcome metrics (accuracy, F1, exact match) and process metrics tracking question quality. The benchmark includes zero-shot, few-shot, and tree-of-thought evaluation, plus post-training fine-tuning experiments using synthetic data generation.

## Key Results
- State-of-the-art models achieve as low as 35% accuracy on active reasoning tasks
- Performance gains plateau rapidly after round 15 despite increasing interaction rounds
- Advanced methods like tree-of-thought and post-training approaches show minimal improvements
- Models consistently generate low-quality questions and ask overly broad queries
- Different error patterns emerge across task types: evidence overlooked (DC), unsupported assumptions (SP), feedback misunderstanding (GN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question quality degrades across interaction rounds due to context accumulation failure, not capability ceiling
- Mechanism: Models show rapid early progress (+7.7% process score rounds 5-10) that plateaus dramatically (+2.5% rounds 20-25), suggesting they fail to organize and leverage accumulated clues rather than exhausting their reasoning capacity. The mechanism involves degraded context utilization—the model cannot maintain coherent hypothesis tracking as conversation history grows.
- Core assumption: Performance plateau stems from information processing limitations rather than fundamental questioning strategy failure
- Evidence anchors: Observation 4.5 documents this pattern, noting the trend suggests LLMs struggle to organize and accumulate clues to formulate new and relevant questions.

### Mechanism 2
- Claim: Search-based reasoning methods fail because verifiers are unreliable in open-ended question spaces
- Mechanism: Tree-of-Thought shows strong gains in GN (rule-based verifier: 14% vs 8% zero-shot for Llama-70B) but underperforms or matches baselines in DC/SP where verifiers must assess narrative coherence. The verifier reliability varies by task structure—symbolic domains allow precise verification while commonsense/logical domains do not.
- Core assumption: Verifier quality, not search strategy, is the bottleneck for ToT-style reasoning in active settings
- Evidence anchors: Observation 4.6 and 4.7 identify unreliable verifiers as a primary bottleneck for ToT methods, noting that symbolic feedback (GN) improved performance while narrative feedback (SP/DC) did not.

### Mechanism 3
- Claim: Models converge to local minima by asking low-information questions that confirm existing hypotheses rather than exploring alternative solution branches
- Mechanism: Error analysis reveals models repeat guesses with incorrect digits (GN: 78% feedback misunderstanding), ask vague questions extracting already-available information (SP), and fail to eliminate suspects systematically (DC). This reflects a hypothesis-space exploration failure where models optimize for immediate coherence rather than maximum information gain.
- Core assumption: Models lack explicit hypothesis-space representation and information-theoretic question utility estimation
- Evidence anchors: Observation 4.14 notes models frequently "converge to local minima" during active reasoning tasks, resulting in questions that fail to advance problem-solving.

## Foundational Learning

- Concept: **Active vs. Passive Reasoning Paradigm**
  - Why needed here: The entire benchmark hinges on distinguishing problems where all information is provided (passive) from those requiring interactive information acquisition (active). Without this conceptual foundation, evaluation metrics and failure modes are uninterpretable.
  - Quick check question: Given a math problem with all necessary numbers provided, is this active or passive reasoning? (Answer: passive—no external interaction required)

- Concept: **Process vs. Outcome Metrics**
  - Why needed here: AR-Bench evaluates both the final answer (outcome: accuracy, F1, exact match) and the quality of intermediate questions (process: key question coverage, digit correctness). Understanding both is essential for diagnosing *where* models fail.
  - Quick check question: If a model identifies the correct murderer but asked no discriminative questions, which metrics capture this discrepancy? (Answer: outcome would be high, process would be low—revealing lucky guessing vs. genuine reasoning)

- Concept: **Hypothesis Space Reduction**
  - Why needed here: Effective active reasoning requires systematically eliminating possibilities. GN has 5,040 possible answers; DC has 5 suspects with 3 attributes each. Models must understand this reduction process to ask high-value questions.
  - Quick check question: In GN, if feedback shows "2 digits correct, 1 in correct position," what does this tell you about the hypothesis space? (Answer: it constrains which digits belong and their positions—reducing possibilities from 5,040 to a smaller subset)

## Architecture Onboarding

- Component map:
  AR-Bench Pipeline: Task Generator -> Evaluation Environment -> Scoring System
  Task Generator: Core sampling -> Story expansion -> Key question extraction -> Human verification
  Evaluation Environment: Player model -> NPCs/Judge -> Interaction loop
  Scoring System: Outcome metrics -> Process metrics

- Critical path:
  1. Initialize model with task rules and partial clues
  2. For each round (1-25): model generates question -> NPC responds
  3. Track process score at intervals (rounds 5, 10, 15, 20, 25)
  4. After final round: model outputs answer -> compare to ground truth
  5. Compute outcome and process scores independently

- Design tradeoffs:
  - LLM vs. rule-based judge: SP/DC require Llama-405B judge (96% accuracy on verification tests); GN can use deterministic function. Rule-based is more reliable but constrains action space.
  - 25-round limit: Chosen to balance evaluation cost with sufficient interaction depth. Ablation shows scaling to 100 rounds yields diminishing returns (+45.8% gains first 50 rounds, +6.7% next 50).
  - Process score via Llama-405B: Uses external model to evaluate whether current state resolves key questions. Introduces some noise but enables fine-grained progress tracking.

- Failure signatures:
  - Early plateau: Process score stalls after round 15 despite increasing interaction history
  - Question repetition: Model generates near-identical questions across consecutive rounds
  - Verifier mismatch: ToT underperforms zero-shot on SP/DC despite outperforming on GN
  - Hypothesis drift: Model conclusions contradict accumulated evidence (e.g., identifying suspect without checking motive/opportunity/weapon access)

- First 3 experiments:
  1. Baseline characterization: Run GPT-4o and Llama-3.1-8B/70B across all three tasks with zero-shot prompting. Plot process scores at 5-round intervals to confirm plateau pattern. Expected: early gains, late stagnation; GPT-4o > Llama-70B > Llama-8B.
  2. Verifier ablation: Compare ToT performance with rule-based verifier (GN) vs. Llama-405B verifier (DC/SP). Expected: GN shows improvement, DC/SP show neutral or negative impact.
  3. Context summarization intervention: At round 15, inject a summarized hypothesis state (e.g., "Confirmed facts: X, Y, Z; Remaining suspects: A, B; Unresolved questions: P, Q"). Measure whether process score acceleration resumes. Expected: if context accumulation is the bottleneck, summarization should improve late-round performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can outcome-based reinforcement learning (RL) methods, such as GRPO or PPO, effectively mitigate the "early gains, late plateaus" phenomenon observed in current LLMs on active reasoning benchmarks?
- Basis in paper: Appendix A.3 suggests leveraging RL techniques with outcome-based rewards to promote planning and self-reflection without manual question labeling.
- Why unresolved: Current post-training methods (SFT, DPO) failed to provide consistent improvements (Observation 4.1), and models struggle to organize clues for new questions in later interaction rounds (Observation 4.5).
- Evidence: Training curves showing RL agents maintaining score improvements beyond round 10 and achieving outcome scores comparable to human baselines on AR-Bench.

### Open Question 2
- Question: How does the performance of search-based active reasoning strategies (e.g., Tree-of-Thought) scale with the reliability of the process verifier, specifically in tasks with open-ended answer spaces?
- Basis in paper: Observation 4.6 and 4.7 identify unreliable verifiers as a primary bottleneck for ToT methods, noting that symbolic feedback (GN) improved performance while narrative feedback (SP/DC) did not.
- Why unresolved: The paper demonstrates ToT failure but does not implement a "gold-standard" or highly optimized verifier to isolate this specific failure mode.
- Evidence: An ablation study where ToT is paired with an oracle verifier or a significantly stronger judge model (e.g., replacing Llama-3.1-405B heuristics with human-in-the-loop verification) to see if performance recovers.

### Open Question 3
- Question: Can specialized small-scale datasets containing detailed interaction traces (question-asking strategies) enable LLMs to overcome the tendency to generate broad, non-specific questions?
- Basis in paper: Observation 4.13 notes models ask broad questions; Appendix A.3 proposes creating high-quality datasets capturing "detailed thinking and interaction process" to remedy poor question generation.
- Why unresolved: The paper's SFT experiments used synthetic data which led to memorization rather than reasoning (Appendix D.2), leaving the impact of human-curated interaction traces untested.
- Evidence: Comparative evaluation of models fine-tuned on human-expert active reasoning trajectories versus the current synthetic Llama-3.1-8B generated datasets.

## Limitations
- Task generality: Three benchmark tasks may not fully represent real-world active reasoning scenarios
- Judge reliability: Limited human verification of judge model accuracy across all tasks
- Context window effects: Study doesn't explore how different context window sizes affect performance

## Confidence
- High Confidence: Performance plateau observation, Tree-of-Thought verifier dependency, error pattern consistency
- Medium Confidence: Context accumulation failure as primary mechanism, local minima convergence hypothesis
- Low Confidence: Post-training methods' limited impact, task difficulty calibration

## Next Checks
1. Implement automatic summary injection at round 15 that restates confirmed facts and remaining hypotheses. If this eliminates or reduces the late-round performance plateau, it validates the context accumulation failure hypothesis.

2. Systematically compare Tree-of-Thought performance across multiple judge models (different LLMs, rule-based hybrids, human oracles) for DC/SP tasks. Quantify how verifier quality correlates with ToT effectiveness to confirm the verifier dependency mechanism.

3. Implement explicit hypothesis tracking that maintains and visualizes the remaining solution space after each question. Track information gain per question to measure whether models actually optimize for maximum reduction in hypothesis space rather than local coherence.