---
ver: rpa2
title: Benchmarking Foundation Speech and Language Models for Alzheimer's Disease
  and Related Dementia Detection from Spontaneous Speech
arxiv_id: '2506.11119'
source_url: https://arxiv.org/abs/2506.11119
tags:
- speech
- audio
- detection
- https
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study benchmarks foundation speech and language models for
  Alzheimer''s disease and related dementia (ADRD) detection from spontaneous speech.
  Using the PREPARE Challenge dataset with over 1,600 participants, it evaluates 15
  speech models and several language models to classify cognitive status into three
  categories: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer''s
  disease (AD).'
---

# Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech

## Quick Facts
- arXiv ID: 2506.11119
- Source URL: https://arxiv.org/abs/2506.11119
- Reference count: 40
- Primary result: Whisper-medium model achieves accuracy=0.731, AUC=0.802 for ADRD detection from spontaneous speech

## Executive Summary
This study benchmarks foundation speech and language models for Alzheimer's disease and related dementia (ADRD) detection from spontaneous speech. Using the PREPARE Challenge dataset with over 1,600 participants, it evaluates 15 speech models and several language models to classify cognitive status into three categories: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer's disease (AD). The Whisper-medium model achieved the highest performance among speech models (accuracy = 0.731, AUC = 0.802), while BERT with pause annotation performed best among language models (accuracy = 0.662, AUC = 0.744). ADRD detection using state-of-the-art automatic speech recognition (ASR) model-generated audio embeddings outperformed others. Including non-semantic features like pause patterns consistently improved text-based classification. The study demonstrates the strong potential of acoustic-based approaches—particularly ASR-derived embeddings—for scalable, non-invasive, and cost-effective early detection of ADRD.

## Method Summary
The study employs two parallel pipelines for ADRD classification. The text-based pipeline uses Whisper ASR to transcribe audio, optionally annotates pauses as punctuation tokens, then extracts embeddings using BERT-family models (768-dim). The audio-based pipeline uses frozen speech encoder models to generate mean-pooled embeddings (1024-dim for Whisper), concatenates age and gender covariates, and feeds into a single-layer MLP classifier (128 hidden units). Both pipelines use cross-entropy loss, Adam optimizer (lr=0.0005), batch size 32, and up to 100 epochs with early stopping. The PREPARE Challenge dataset was filtered to 1,189 English samples, stratified 80/20 train/test splits were repeated 5 times, and performance was measured using accuracy and multi-class AUC.

## Key Results
- Whisper-medium (audio embeddings) achieved highest performance: accuracy=0.731, AUC=0.802
- BERT with pause annotation achieved best text-based performance: accuracy=0.662, AUC=0.744
- Audio-based models consistently outperformed text-based models across all variants
- Including pause annotations improved text-based classification by 0.5-1% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Audio-based embedding models likely outperform text-based models because they preserve non-semantic paralinguistic markers associated with cognitive decline. ADRD progression affects motor planning and speech production, resulting in pauses, hesitations, and altered tempo. Audio encoders (e.g., Whisper) capture these acoustic and temporal features directly, whereas text transcriptions typically discard them. Core assumption: The acoustic characteristics of spontaneous speech contain predictive signal independent of the semantic content.

### Mechanism 2
Injecting explicit pause annotations into text transcriptions acts as a surrogate for acoustic features, recovering lost non-semantic signal. By converting silence duration into textual tokens (e.g., commas for short pauses, ellipses for long pauses), the semantic model is forced to attend to temporal speech patterns it would otherwise ignore. Core assumption: The specific formatting of pauses as punctuation maps effectively to the transformer's attention mechanism.

### Mechanism 3
Performance gains are driven by the scale and diversity of pre-training data rather than just model parameter count. Foundation models trained on massive, noisy, diverse datasets (e.g., Whisper at 680k hours) learn robust generalizable representations that transfer better to clinical tasks than models trained on smaller, curated datasets (e.g., 960 hours). Core assumption: The acoustic features of dementia are sufficiently represented within the manifold of general human speech variations found in large-scale pre-training data.

## Foundational Learning

- **Concept: Mean Pooling of Embeddings**
  - Why needed here: The paper processes variable-length audio clips (up to 30s) and needs a fixed-size vector input for the classifier. Mean pooling aggregates the sequence of vectors from the transformer encoder into a single representative embedding.
  - Quick check question: How does the system handle a 10-second clip versus a 30-second clip to ensure the input dimension to the classifier remains constant?

- **Concept: Encoder-Decoder Architecture (ASR)**
  - Why needed here: This study repurposes the *encoder* part of Whisper (designed for speech recognition) as a feature extractor, discarding the decoder. Understanding this distinction is vital for extracting embeddings rather than text.
  - Quick check question: In the audio-based pipeline, which part of the Whisper architecture is used to generate the 1024-dimensional vector?

- **Concept: Stratified Sampling**
  - Why needed here: The dataset is imbalanced (703 HC vs. 81 MCI vs. 405 AD). Stratified sampling ensures that the train/test splits maintain these class proportions to prevent biased evaluation.
  - Quick check question: Why is random splitting insufficient for this specific dataset of 1,189 participants?

## Architecture Onboarding

- **Component map:**
  1. Input: Raw Audio (48kHz -> 16kHz resampling)
  2. Preprocessing: DeepFilterNet (noise suppression) -> Trimming/Padding (30s)
  3. Encoder (Frozen): Whisper-medium (Audio -> 1024-dim Vector) OR BERT (Text -> 768-dim Vector)
  4. Concatenation: Embedding + [Age, Gender] Covariates
  5. Head: Single-layer Feedforward Neural Network (Size 128) -> Softmax

- **Critical path:** The extraction of the mean-pooled embedding from the Whisper encoder is the critical feature engineering step. Errors in resampling or failing to normalize audio amplitudes will degrade the quality of this vector.

- **Design tradeoffs:**
  - Whisper-medium vs. Large: The paper shows Medium (769M params) outperforms Large (1.55B params), trading off raw capacity for better generalization on a finite clinical dataset
  - Text vs. Audio: Audio pipeline requires more compute for feature extraction but captures pauses automatically. Text pipeline is lighter but requires manual pause annotation engineering to compete

- **Failure signatures:**
  - Overfitting: Validation loss failing to improve for 5 consecutive epochs (resolved by early stopping)
  - Diminishing Returns: Whisper-large accuracy drops to 0.7019 compared to Medium's 0.7307
  - Data Loss: Excluding non-English or poor-quality audio (<3s) reduces dataset size significantly (1,646 -> 1,189)

- **First 3 experiments:**
  1. Baseline Audio Classification: Implement the audio pipeline with `Whisper-medium.en` encoder frozen, training only the 128-unit classifier head to establish the SOTA benchmark (Target: ~0.73 Acc)
  2. Ablation on Modality: Run the text-based pipeline (BERT) with and without pause annotation to quantify the contribution of non-semantic markers (Target: +0.5-1% Acc with pauses)
  3. Model Scaling Test: Benchmark `Whisper-tiny` vs `Whisper-base` vs `Whisper-medium` to verify the relationship between model size/pre-training data and AUC on the specific PREPARE dataset

## Open Questions the Paper Calls Out

- Can multilingual foundation speech models maintain comparable ADRD detection performance across diverse languages and dialects?
- What specific acoustic and linguistic features do foundation speech models encode when predicting ADRD, and can these representations be made interpretable for clinical use?
- Can longitudinal speech data improve within-subject detection of subtle cognitive changes and track ADRD progression over time?
- Why does model performance degrade when scaling Whisper beyond the medium variant, and does this reflect overfitting to general speech patterns rather than ADRD-specific cues?

## Limitations

- The PREPARE dataset composition remains incompletely specified, potentially limiting generalizability
- The study does not address potential confounding factors such as education level, native language proficiency, or comorbidities
- Text-based approach requiring manual pause annotation represents a practical limitation for real-world deployment
- Performance gap between Whisper-medium and Whisper-large raises questions about optimal model scaling for this specific task

## Confidence

**High Confidence:**
- Audio-based foundation models outperform text-based models for ADRD detection from spontaneous speech
- Including non-semantic features like pause patterns improves text-based classification performance
- ASR-derived embeddings preserve predictive acoustic information that text transcriptions lose

**Medium Confidence:**
- The superior performance of Whisper-medium over larger variants is primarily due to optimal model scaling rather than overfitting
- The specific pause annotation scheme (short/medium/long bins represented by commas, commas, and ellipses) is optimal for capturing temporal speech patterns

**Low Confidence:**
- The findings generalize to spontaneous speech tasks beyond the Cookie Theft description
- The performance advantage extends equally to all stages of cognitive impairment (particularly the minority MCI class)

## Next Checks

1. **Class-Specific Performance Analysis:** Conduct detailed per-class (HC, MCI, AD) evaluation using precision, recall, and F1-score to understand model behavior across cognitive stages, particularly for the minority MCI class where only 81 samples exist.

2. **Cross-Task Generalization Test:** Validate model performance on spontaneous speech from different elicitation tasks (e.g., story recall, conversational speech) to assess whether the acoustic advantages hold beyond the controlled picture description task.

3. **Demographic Fairness Audit:** Perform subgroup analysis across age ranges and genders to identify potential bias or performance disparities, particularly examining whether the audio advantage persists equally across different demographic groups.