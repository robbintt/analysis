---
ver: rpa2
title: 'LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents'
arxiv_id: '2602.01053'
source_url: https://arxiv.org/abs/2602.01053
tags:
- cache
- base
- sharing
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LRAgent, a KV cache sharing framework designed
  for multi-LoRA LLM agent systems. It addresses the memory and compute inefficiencies
  caused by redundant KV cache construction when multiple agents process the same
  long, tool-augmented contexts.
---

# LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents

## Quick Facts
- arXiv ID: 2602.01053
- Source URL: https://arxiv.org/abs/2602.01053
- Authors: Hyesung Jeon; Hyeongju Ha; Jae-Joon Kim
- Reference count: 40
- Primary result: Achieves throughput and TTFT close to fully shared caching while preserving accuracy near non-shared baseline

## Executive Summary
LRAgent introduces a KV cache sharing framework for multi-LoRA LLM agents that addresses memory and compute inefficiencies when multiple agents process the same long, tool-augmented contexts. The key insight is that cache differences across agents are dominated by adapter outputs while activations from the shared pretrained backbone remain highly similar. By decomposing the cache into a shared base component from pretrained weights and a lightweight, low-rank adapter-dependent component, LRAgent achieves ~70% memory reduction while maintaining accuracy close to non-shared baselines.

The framework introduces two schemes: BaseShared, which shares the base cache and stores per-agent low-rank adapter caches, and BaseLRShared, which further shares the low-rank cache in shared-A multi-LoRA architectures, reducing both memory and computation. To minimize runtime overhead, it proposes Flash-LoRA-Attention, which reorders attention computation to avoid materializing low-rank caches to full dimension. Experiments on multi-hop QA benchmarks show LRAgent achieves throughput and time-to-first-token latency close to fully shared caching while preserving accuracy near the non-shared baseline.

## Method Summary
LRAgent decomposes KV cache into shared base component (from pretrained weights W₀) and per-agent low-rank adapter component (from LoRA decomposition W = W₀ + A·B). BaseShared shares base cache across agents while storing adapter outputs in low-rank form. BaseLRShared requires shared-A architecture and shares both base and low-rank caches. Flash-LoRA-Attention kernel reorders computation to avoid materializing low-rank caches to full dimension. Training uses three role-specific LoRA adapters (plan, action, reflect) with rank r=8 on query/value projections.

## Key Results
- Achieves memory reduction of ~70% compared to non-shared caching
- Throughput and TTFT close to fully shared caching approaches
- Accuracy preservation within 1% of non-shared baseline on HotpotQA and ScienceQA benchmarks
- Flash-LoRA-Attention yields up to 1.35× throughput gain for BaseLRShared

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KV cache can be decomposed into a shared base component and adapter-dependent component with minimal accuracy loss
- Mechanism: The base cache (from pretrained weights W₀) maintains high cosine similarity (>0.95) across agents on shared context, while adapter outputs (∆Y = X·∆W) are largely decorrelated across agents. This allows sharing Y_base while storing only the decorrelated perturbation separately.
- Core assumption: The base cache similarity holds across different agent roles processing identical context tokens
- Evidence anchors:
  - [abstract] "cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar"
  - [Section 3.1, Table 1] Base cache cosine similarity 0.9726 vs full cache 0.9576 for LLaMA-3.1-8B
  - [corpus] Limited direct validation—related work (Efficient Multi-Adapter LLM Serving) addresses cross-model cache reuse but doesn't isolate base/adapter decomposition
- Break condition: If agents diverge significantly in early layers (different hidden states X_i), base cache similarity degrades and sharing introduces error accumulation

### Mechanism 2
- Claim: Storing adapter contributions in low-rank form and expanding on-demand reduces memory by ~70%
- Mechanism: Instead of materializing full-dimension adapter output ∆Y = (X·A)·B, store only Y_lr = X·A (rank r << d_out). Reconstruct via Y_lr·B only during attention computation.
- Core assumption: The low-rank representation sufficiently captures task-specific information without full expansion
- Evidence anchors:
  - [abstract] "reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form"
  - [Section 3.2] Total KV cache reduced to (1/N + r/d_out) ≈ 1/N of non-shared scheme
  - [corpus] WindowKV and related compression work validate low-rank cache approximations but don't exploit LoRA structure
- Break condition: If rank r is set too low for complex tasks, adapter information is under-represented and accuracy drops

### Mechanism 3
- Claim: Flash-LoRA-Attention minimizes runtime overhead by reordering computation
- Mechanism: Exploit associativity: compute O = P·V_base + (P·V_lr)·B instead of P·(V_base + V_lr·B). This keeps the O(L·r) accumulation in low-rank space before the O(r·d_out) up-projection.
- Core assumption: The dominant overhead comes from expanding V_lr to full dimension before attention, not from the attention computation itself
- Evidence anchors:
  - [abstract] "a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension"
  - [Section 3.3, Figure 4] Flash-LoRA-Attention yields up to 1.35× throughput gain for BaseLRShared
  - [corpus] No corpus papers address computation reordering for LoRA-specific attention
- Break condition: If attention patterns require frequent access to expanded adapter contributions (e.g., dense cross-attention), reordering gains diminish

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) parameterization W = W₀ + A·B
  - Why needed here: The entire method exploits the structure of LoRA's low-rank decomposition to create the LR cache
  - Quick check question: Given rank r=8 and hidden dimension d=4096, what is the compression ratio of storing A·B vs storing a full delta weight matrix?

- Concept: KV Cache in autoregressive LLMs
  - Why needed here: Understanding what's being cached (key/value projections) and why redundancy occurs across agents is foundational
  - Quick check question: For a sequence of length L with d_head=128, what is the memory cost of storing the value cache for one layer?

- Concept: Shared-A multi-LoRA architectures (HydraLoRA-style)
  - Why needed here: BaseLRShared requires agents to share the down-projection A to enable LR cache sharing
  - Quick check question: Why might sharing A across tasks improve generalization compared to independent A_i matrices?

## Architecture Onboarding

- Component map:
  - Base Cache: Shared across all agents, computed from pretrained W₀, size L × d_out
  - LR Cache: Per-agent (BaseShared) or shared (BaseLRShared), stores X·A, size L × r
  - Flash-LoRA-Attention Kernel: Handles P·V_base accumulation and (P·V_lr)·B expansion without materializing intermediate full-dimension tensors

- Critical path:
  1. First agent processes context → computes and stores base cache + LR cache
  2. Subsequent agents reuse base cache directly
  3. For BaseShared: each agent maintains own LR cache, recomputes X·A for unseen tokens
  4. For BaseLRShared: single LR cache shared, only new tokens need processing
  5. At attention time: Flash-LoRA-Attention computes outputs without full expansion

- Design tradeoffs:
  - BaseShared: Works with standard multi-LoRA, ~70% memory reduction, but still requires hidden state recomputation for agent switches (compute similar to non-shared)
  - BaseLRShared: Requires shared-A architecture (training change), achieves both memory and compute reduction, throughput approaches full cache sharing
  - Rank selection: Lower r improves memory/compute but risks under-capacity; paper uses r=8

- Failure signatures:
  - Accuracy drops >1% from non-shared baseline → likely base cache similarity violated (check early-layer hidden state divergence)
  - OOM despite cache sharing → hidden state cache overhead (DroidSpeak) or GQA-related scaling issues
  - Latency not improving with BaseLRShared → verify Flash-LoRA-Attention kernel is actually being used, not fallback path

- First 3 experiments:
  1. Replicate Table 1 on your target model: measure base cache vs full cache cosine similarity across agent pairs on shared context to validate decomposition assumption
  2. Profile memory breakdown: separate base cache, LR cache, and hidden state cache contributions to confirm expected 1/N scaling
  3. Ablation on rank r (Table 17): test r∈{4,8,16} on your task to find accuracy-efficiency Pareto frontier before committing to deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LRAgent scale to multi-agent systems with more than three specialized agents, and does the base cache similarity assumption hold as agent role diversity increases?
- Basis in paper: [inferred] The experiments use only three agents (plan, action, reflect). The method relies on high base cache cosine similarity (>0.95) across agent pairs, but it is unclear whether this holds for more diverse or numerous agent roles.
- Why unresolved: The scalability of cache sharing and the stability of similarity assumptions were not tested beyond the three-agent setting.
- What evidence would resolve it: Experiments with 5+ agents across diverse roles, reporting per-pair cosine similarities and accuracy/efficiency metrics.

### Open Question 2
- Question: Can the Flash-LoRA-Attention reordering strategy be extended to handle LoRA applied to key projections when RoPE is used, without sacrificing efficiency?
- Basis in paper: [explicit] Appendix D.1 notes that for qkvo LoRA, "the low-rank reordering from Q(B^⊤ K'_lr^⊤) to (QB^⊤)K'_lr^⊤ is not directly applicable because RoPE(·) applies a position-dependent rotation on the head dimension."
- Why unresolved: RoPE's position-dependent rotation prevents straightforward associativity-based reordering for key LR cache expansion.
- What evidence would resolve it: A modified attention kernel or approximation that handles RoPE-compatible key projection LoRA efficiently, with benchmarked throughput and accuracy.

### Open Question 3
- Question: How does LRAgent interact with complementary inference optimizations such as quantization, speculative decoding, or prefix caching?
- Basis in paper: [inferred] The paper evaluates LRAgent in isolation and compares to prior cache-sharing methods, but does not study compatibility with other common LLM inference optimizations that could be combined.
- Why unresolved: Practical deployment would likely layer multiple optimizations; their interactions remain unexplored.
- What evidence would resolve it: Ablation studies combining LRAgent with quantization (e.g., 4/8-bit), speculative decoding, or prefix caching, measuring accuracy, throughput, and memory.

### Open Question 4
- Question: Does the observed base cache similarity generalize to agents trained on heterogeneous trajectory distributions rather than a shared synthetic dataset?
- Basis in paper: [inferred] All agents are trained on trajectories generated by the same LLaMA-2-70B-Chat model. The high base cache similarity may partly reflect this shared data distribution.
- Why unresolved: Real-world multi-agent systems may use role-specific training data from different sources, which could alter similarity patterns.
- What evidence would resolve it: Experiments where each agent is fine-tuned on independently generated or domain-specific trajectory datasets, with reported cosine similarities and accuracy.

## Limitations
- Base cache similarity assumption validity may degrade with longer sequences or more diverse agent roles
- Rank r=8 choice may not generalize across different tasks or model scales
- AutoAct trajectory availability creates uncertainty about reproducibility for different agent architectures

## Confidence

**High Confidence (mechanisms well-supported by evidence):**
- Base cache decomposition works for BaseShared - Table 1 shows high cosine similarity and Table 3/4 demonstrates memory/compute benefits
- Flash-LoRA-Attention provides measurable throughput gains - Figure 4 shows 1.35× improvement in BaseLRShared
- BaseLRShared achieves memory reduction close to full sharing - Table 3 shows 1/N scaling with shared base and LR caches

**Medium Confidence (reasonable assumptions but limited validation):**
- Base cache similarity assumption generalizes across agent types and contexts - supported by single model experiment but needs broader validation
- Rank r=8 is optimal or near-optimal across tasks - ablation shows good performance but limited to tested benchmarks
- DroidSpeak hidden state cache scaling doesn't invalidate approach - identified as a potential issue but not systematically evaluated

**Low Confidence (mechanisms need more evidence):**
- Cross-model cache reuse (Efficient Multi-Adapter LLM Serving) validates base/adapter decomposition - this work addresses different problem (cross-model vs same-model) and doesn't isolate the specific decomposition used here
- Flash-LoRA-Attention kernel integration - pseudocode provided but production implementation details unclear

## Next Checks

1. **Base cache similarity validation across models** - Measure cosine similarity between base caches for LLaMA-3.1-8B, Ministral-8B, and another model family on identical contexts with different agent roles. Track similarity degradation across sequence lengths (100, 500, 1000 tokens) to identify if early-layer divergence occurs.

2. **Rank sensitivity analysis on complex tasks** - Test rank r ∈ {4, 8, 16, 32} on a task requiring compositional reasoning (e.g., multi-hop reasoning with tool use) beyond the tested benchmarks. Measure accuracy drop and memory savings to establish the Pareto frontier and identify minimum viable rank for your use case.

3. **Hidden state cache profiling** - Profile memory usage breakdown (base cache, LR cache, hidden state cache) on a representative long-context task with your target model. Verify that hidden state cache doesn't dominate total memory usage, and test whether BaseShared/BaseLRShared configurations properly avoid hidden state caching when expected.