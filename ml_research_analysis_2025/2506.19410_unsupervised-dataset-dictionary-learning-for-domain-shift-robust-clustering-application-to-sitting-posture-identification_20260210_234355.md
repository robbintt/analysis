---
ver: rpa2
title: 'Unsupervised Dataset Dictionary Learning for domain shift robust clustering:
  application to sitting posture identification'
arxiv_id: '2506.19410'
source_url: https://arxiv.org/abs/2506.19410
tags:
- domain
- clustering
- posture
- data
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents U-DaDiL, an unsupervised clustering approach
  designed to handle domain shift across datasets. The method combines dictionary
  learning with optimal transport to learn shared atoms across source domains and
  generate robust centroids that generalize to unseen target domains.
---

# Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification

## Quick Facts
- arXiv ID: 2506.19410
- Source URL: https://arxiv.org/abs/2506.19410
- Reference count: 17
- This paper presents U-DaDiL, an unsupervised clustering approach designed to handle domain shift across datasets, achieving 50.6% average clustering accuracy on Office31 (+24.6% over prior best).

## Executive Summary
This paper introduces U-DaDiL, an unsupervised clustering method that addresses domain shift by learning shared dictionary atoms across source domains and using Wasserstein barycenters to generate robust centroids that generalize to unseen target domains. The method combines dictionary learning with optimal transport to align distributions across domains without requiring labeled target data. Experiments demonstrate significant performance gains on both the Office31 dataset and a real-world sitting posture identification task using pressure sensor data, showing that optimal transport-based regularization helps capture domain-invariant cluster structures.

## Method Summary
U-DaDiL uses a multi-step training pipeline: (1) Generate pseudo-labels via K-Means clustering on each source domain; (2) Align clusters across domains using Wasserstein distance and Hungarian matching to resolve label correspondence ambiguity; (3) Initialize atoms (number set equal to number of source domains) as shifted domain representations; (4) Train a dictionary using DaDiL (Dictionary learning) optimization that minimizes Wasserstein reconstruction error; (5) Compute centroids via Wasserstein barycenters with barycentric mapping to improve coverage; (6) Assign clusters by Euclidean distance; (7) Iterate the process. For inference on unseen target domains, barycentric regression finds optimal coefficients to express the target as a Wasserstein barycenter of the pre-learned atoms, with the support points becoming cluster centroids.

## Key Results
- U-DaDiL achieves 50.6% average clustering accuracy on Office31, outperforming the previous best method by 24.6%
- On sitting posture identification using pressure sensor data across 5 subjects, U-DaDiL consistently outperforms K-Means
- The method demonstrates robust performance under domain shift without requiring labeled target domain data
- Wasserstein barycenter representations enable distribution-level alignment across domains, producing centroids that capture domain-invariant cluster structures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Wasserstein barycenter representations enable distribution-level alignment across domains, producing centroids that capture domain-invariant cluster structures.
- **Mechanism:** Rather than clustering based solely on spatial proximity (as in K-Means), U-DaDiL minimizes the Wasserstein distance between reconstructed distributions and original data. This incorporates distributional properties—shape, spread, statistical moments—into centroid formation. The barycentric mapping step then displaces prototypes to improve coverage of each domain.
- **Core assumption:** The true cluster structure is approximately preserved across domains despite feature-level shifts; shared statistical properties exist that can be captured through optimal transport.
- **Evidence anchors:** [abstract] "U-DaDiL addresses these challenges by aligning distributions from different datasets using Wasserstein barycenter based representation." [Section IV.C] "to do clustering, we do not only rely on spatial proximity but also consider the distributional properties of data points since our optimization process aims to minimize the Wasserstein distance"
- **Break condition:** If domains share no distributional overlap (disjoint support), Wasserstein barycenters may produce meaningless interpolations. Expect degraded performance when source and target domains have fundamentally different cluster geometries.

### Mechanism 2
- **Claim:** Shared dictionary atoms learned across source domains serve as a basis for reconstructing any domain's distribution, enabling zero-shot adaptation to unseen target domains via barycentric regression.
- **Mechanism:** During training, atoms (number set equal to number of source domains) are learned to minimize reconstruction error across all source domains. At inference, barycentric regression finds optimal coefficients to express the target domain as a Wasserstein barycenter of these pre-learned atoms. The support points of this barycenter become cluster centroids.
- **Core assumption:** Target domain distributions lie within (or near) the convex hull (in Wasserstein space) of source domain distributions; atoms capture transferable structure.
- **Evidence anchors:** [abstract] "learn shared atoms across source domains and generate robust centroids that generalize to unseen target domains" [Section II.B.2] "the inference process relies on exploiting the optimized atoms obtained during the training process... adaptation occurs through performing a barycentric regression between these atoms and the features of the target domain"
- **Break condition:** If the target domain's distribution is an out-of-distribution extrapolation beyond the source domain span, barycentric regression will produce poor approximations. Monitor reconstruction error as a diagnostic.

### Mechanism 3
- **Claim:** Wasserstein-distance-based cluster alignment resolves label correspondence ambiguity across domains without requiring manual label synchronization.
- **Mechanism:** After initial K-Means pseudo-labeling, clusters from different domains lack correspondence (cluster i in domain A ≠ cluster i in domain B). U-DaDiL computes Wasserstein distances between all cluster pairs and solves an optimal assignment problem (Hungarian-style) to establish correspondences, enabling pseudo-label sharing.
- **Core assumption:** Clusters with similar Wasserstein-distance-defined distributions correspond to the same semantic class; distributional similarity is a reliable proxy for semantic equivalence.
- **Evidence anchors:** [Section II.B.1, step 2] "a cost matrix is computed to quantify the transportation cost between pairs of clusters, utilizing the Wasserstein distance to quantify cluster dissimilarities" [Section II.B.1, step 2] "The optimal matching between two clusters is found solving [assignment problem with] cij as the Wasserstein distance between cluster i in domain 1 and cluster j in domain 2"
- **Break condition:** If intra-class variability across domains exceeds inter-class variability (e.g., same posture looks more different across subjects than different postures within a subject), Wasserstein distance will produce incorrect correspondences.

## Foundational Learning

- **Concept: Optimal Transport / Wasserstein Distance**
  - **Why needed here:** Core mathematical framework for measuring distributional dissimilarity and computing barycenters. U-DaDiL uses OT both for cluster alignment (computing pairwise cluster distances) and for dictionary learning (reconstructing distributions as barycenters).
  - **Quick check question:** Given two point clouds with different numbers of points, can you explain why the Wasserstein distance requires solving a mass-preservation constrained optimization rather than simple point-wise matching?

- **Concept: Dictionary Learning**
  - **Why needed here:** U-DaDiL extends classical dictionary learning from vector spaces to probability measure spaces. Understanding standard dictionary learning (learning a basis to sparsely represent signals) helps contextualize why atoms and barycentric coefficients matter.
  - **Quick check question:** In standard dictionary learning, a signal is reconstructed as a weighted sum of atoms. What is the analog in U-DaDiL, and what mathematical object replaces the "sum"?

- **Concept: K-Means Clustering and Label Permutation Invariance**
  - **Why needed here:** U-DaDiL initializes with K-Means pseudo-labels; understanding K-Means limitations (sensitivity to initialization, label permutation across runs/domains) clarifies why the alignment step is necessary.
  - **Quick check question:** If you run K-Means on two domains separately, why can't you directly compare cluster labels across domains even if the clustering is perfect?

## Architecture Onboarding

- **Component map:**
  Training Pipeline: Source Data -> K-Means Pseudo-label Generation -> Cluster Alignment (OT-based) -> Dictionary Training (DaDiL) -> Centroid Calculation (Barycenter) -> Cluster Assignment (Euclidean) -> Iterate
  Inference Pipeline: Target Data -> Barycentric Regression -> Centroid Generation -> Cluster Assignment

- **Critical path:**
  1. Cluster alignment correctness—wrong correspondences propagate through dictionary training
  2. Atom initialization—paper uses "slightly shifted versions of each domain dataset"; poor initialization may slow convergence
  3. Number of atoms = number of domains (fixed by design; verify this hyperparameter)

- **Design tradeoffs:**
  - OT regularization vs. computational cost: Wasserstein distance computation scales poorly with sample size; the paper does not specify computational complexity or approximations used
  - Domain count = atom count: Limits expressiveness when domains are few but cluster structures are complex
  - K-Means initialization: Fast but quality-dependent; alternative initializations (spectral clustering, hierarchical) not explored

- **Failure signatures:**
  - Cluster accuracy near random baseline (1/K for K classes): Check alignment step—correspondences may be scrambled
  - Performance degrades on target while strong on source: Barycentric regression failing; target may be out-of-distribution
  - High variance across domain combinations: Atoms may be overfitting to specific source domains

- **First 3 experiments:**
  1. Reproduce Office31 with ablation: Remove cluster alignment step (use K-Means labels directly) to quantify alignment's contribution. Compare full U-DaDiL vs. no-alignment variant.
  2. Synthetic domain shift test: Generate Gaussian clusters with controlled domain shift (mean translation, covariance change). Measure clustering accuracy as function of shift magnitude to identify break points.
  3. Atom count sensitivity: Vary number of atoms (not fixed to domain count) and measure reconstruction error + clustering accuracy. Determine if current constraint is necessary or incidental.

## Open Questions the Paper Calls Out
- How can the U-DaDiL framework be extended to handle heterogeneous multi-domain clustering where the number of clusters varies across different domains?
- Is setting the number of dictionary atoms equal to the number of source domains optimal for generalization, or does this constraint limit representational capacity?
- How robust is the iterative clustering process to the quality of the initial K-Means pseudo-labels, particularly when domains differ significantly in size or distribution?

## Limitations
- Missing critical implementation details for dictionary training optimization, atom initialization methodology, and barycentric mapping procedures
- The claim of 24.6% improvement over prior work needs validation given the sensitivity of unsupervised methods to initialization and hyperparameter choices
- Exact contribution magnitude relative to baseline methods cannot be assessed without replication

## Confidence
- **High confidence:** The core architectural concept of combining dictionary learning with Wasserstein barycenter representations is mathematically sound and the mechanism descriptions are coherent.
- **Medium confidence:** The experimental results are plausible given the approach, but cannot be independently verified without the missing implementation details.
- **Low confidence:** The exact contribution magnitude relative to baseline methods cannot be assessed without replication.

## Next Checks
1. Implement ablation study removing the cluster alignment step to quantify its contribution to the 24.6% improvement.
2. Test method robustness on synthetic data with controlled domain shifts to identify performance break points.
3. Conduct sensitivity analysis on atom count (varying from domain count) to determine if the current constraint is optimal or incidental.