---
ver: rpa2
title: 'Understanding Efficiency: Quantization, Batching, and Serving Strategies in
  LLM Energy Use'
arxiv_id: '2601.22362'
source_url: https://arxiv.org/abs/2601.22362
tags:
- energy
- arxiv
- inference
- batching
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the energy and latency impact of system-level\
  \ design choices\u2014numerical precision, batching strategy, and request scheduling\u2014\
  on LLM inference using NVIDIA H100 GPUs. It benchmarks five numerical formats across\
  \ multiple model sizes and phases (prefill, decode) with Hugging Face\u2019s Text\
  \ Generation Inference (TGI) server, and evaluates the effects of static/dynamic\
  \ batching and inter-arrival delay patterns."
---

# Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use

## Quick Facts
- arXiv ID: 2601.22362
- Source URL: https://arxiv.org/abs/2601.22362
- Reference count: 15
- Primary result: System-level optimizations like quantization, batching, and request scheduling significantly impact LLM inference energy use, with phase-specific trade-offs.

## Executive Summary
This paper investigates how numerical precision, batching strategy, and request scheduling affect the energy and latency of large language model inference on NVIDIA H100 GPUs. Using Hugging Face's Text Generation Inference server, the authors benchmark five numerical formats across multiple model sizes and phases (prefill, decode), and evaluate the effects of static/dynamic batching and inter-arrival delay patterns. Lower precision reduces latency and energy in compute-bound phases (e.g., prefill of large models), but gains vanish in memory-bound phases like decoding due to quantization overhead. Batching improves efficiency but is sensitive to padding in prefill; decode benefits plateau around batch size 4. TGI with burst-mode batching and fixed arrival shaping can reduce per-request energy by up to 100× compared to naive serving. The authors conclude that sustainable LLM deployment requires system-level optimizations beyond model design.

## Method Summary
The study benchmarks energy and latency of LLM inference under varying numerical precision, batch size, and serving configurations, decomposed into prefill and decode phases. Using NVIDIA H100 SXM (80GB) and 8× AMD EPYC 7R13 CPUs, it tests 10,000 prompts (200–4000 tokens, outputs 10–300 tokens) from UltraChat-200k subset with models including Qwen 2.5 (0.5B–14B), Mistral-7B-Instruct-v0.3, and LLaMA 3.1-8B-Instruct across five dtypes: float32, bfloat16, float16, int8, int4 via bitsandbytes. GPU/CPU/RAM energy is measured via CodeCarbon with NVML/pyRAPL, with CUDA kernel-level latency, energy per token (effective vs computed), and energy per request under different arrival patterns. Each config runs 5 warmup iterations and 10 repetitions. TGI v3.3.4 is used for serving, with static batching via Transformers and inter-arrival delays tested (fixed 50–500ms, random U(k,l)); prefill is isolated by generating one token, and decode is computed as the difference.

## Key Results
- Lower-precision formats (INT8/INT4) only yield energy gains in compute-bound regimes; they increase energy consumption in memory-bound decode phases due to quantization overhead.
- Batching improves energy efficiency primarily in the decode phase, with gains plateauing around batch size 4; prefill efficiency is degraded by padding waste in static batching.
- Structured request timing (arrival shaping) can reduce per-request energy by up to 100× compared to naive sequential serving by enabling continuous batching.

## Why This Works (Mechanism)

### Mechanism 1: Precision-Regime Coupling
- **Claim:** Lower numerical precision (e.g., INT8, BF16) reduces energy and latency *only* when the workload is compute-bound; it increases energy consumption in memory-bound phases due to dequantization overheads.
- **Mechanism:** In compute-bound prefill phases (large models, long prompts), reduced precision activates high-throughput Tensor Cores, lowering execution time enough to offset higher instantaneous power draw. However, in memory-bound decode phases, the GPU is already waiting on memory bandwidth. Aggressive quantization (INT8/INT4) adds small, fragmented dequantization kernels and launch overheads. Since the GPU cannot sleep between these frequent, small kernels, it burns energy at idle power levels (~120W) without useful throughput gains, negating theoretical bandwidth savings.
- **Core assumption:** The energy cost of kernel launch overhead and GPU idle power dominates over memory bandwidth savings in memory-bound regimes.
- **Evidence anchors:**
  - [abstract] "Lower-precision formats only yield energy gains in compute-bound regimes..."
  - [section 3.2] "...we observe higher energy consumption with int8—often 2–3× more than float32—despite moving fewer bytes."
  - [corpus] "LiquidGEMM" discusses hardware-efficient kernels for W4A8, implying that standard quantization kernels often lack efficiency without specific hardware optimization.
- **Break condition:** If the GPU architecture allows near-zero power states between kernels or if dequantization is fused into a single memory-load operation, this energy penalty in decode may vanish.

### Mechanism 2: Batch-Phase Saturation
- **Claim:** Increasing batch size improves energy efficiency primarily in the decode phase, but gains plateau quickly (around batch size 4), while prefill efficiency is degraded by padding waste.
- **Mechanism:** In the decode phase, batching amortizes the fixed cost of loading model weights and improves arithmetic intensity. However, the quadratic cost of attention over the KV cache eventually limits parallelism, causing a plateau. In the prefill phase, static batching requires padding shorter sequences to the longest in the batch. This forces the GPU to compute "useless" tokens on padding, increasing energy per *effective* token.
- **Core assumption:** The system uses static batching where padding is applied, and attention mechanism complexity scales quadratically with sequence length * batch size.
- **Evidence anchors:**
  - [abstract] "...decode benefits plateau around batch size 4. TGI with burst-mode batching... can reduce per-request energy..."
  - [section 4] "In the decode phase... optimal batch size... is reached at b=4."
  - [corpus] "BucketServe" and "Splitwise" (Section 7 references) corroborate the need for phase-aware or bucket-based batching to mitigate padding inefficiencies.
- **Break condition:** If using continuous batching with paged attention (where padding is minimized) or if sequence lengths are uniform, the prefill penalty disappears.

### Mechanism 3: Arrival Shaping and Utilization
- **Claim:** Controlling request timing (arrival shaping) allows serving systems to create consistent batches, reducing per-request energy by up to 100× compared to naive sequential serving.
- **Mechanism:** In naive serving (or random arrivals), requests often arrive when the GPU is idle or busy, leading to serialization or underutilization. By enforcing fixed inter-arrival delays (e.g., 500ms), the scheduler can accumulate requests efficiently, enabling continuous batching to maximize GPU utilization (reducing idle time) and amortize the fixed energy cost of model loading across multiple requests.
- **Core assumption:** The serving system supports continuous batching and the incoming request rate is high enough to form batches within the shaped delay window.
- **Evidence anchors:**
  - [abstract] "...structured request timing (arrival shaping) can reduce per-request energy by up to 100 times."
  - [section 5.2] "...using a constant spacing of 500 ms reduces energy to as low as 1.1×10⁻³ Wh per request..."
  - [corpus] "Adaptive Request Scheduling for CodeLLM" supports the general principle that intelligent scheduling is critical for SLA and efficiency, though specific 100× claims are unique to this paper's H100 setup.
- **Break condition:** If the traffic volume is too low to form batches even with delays, or if the latency budget is too tight to allow waiting for batch accumulation, gains will be limited.

## Foundational Learning

- **Concept:** Compute-bound vs. Memory-bound regimes
  - **Why needed here:** The paper's core finding is that optimization strategies (quantization) work differently depending on which regime the inference phase (prefill vs. decode) is in.
  - **Quick check question:** If you reduce the precision of weights from FP32 to INT8, why would that *not* speed up the generation of a single token in a short sequence?

- **Concept:** Continuous Batching (and Paged Attention)
  - **Why needed here:** The paper compares naive serving (Transformers) with TGI. Understanding continuous batching is required to understand why TGI scales so much better and why "arrival shaping" helps.
  - **Quick check question:** How does continuous batching differ from static batching in handling requests that finish at different times?

- **Concept:** GPU Idle Power
  - **Why needed here:** The paper explains that INT8 quantization hurts decode energy because the GPU cannot fully power down between the many small kernels required for dequantization.
  - **Quick check question:** Why does completing a task *faster* (lower latency) not always guarantee *less energy* used if the power draw increases significantly or if idle time fragments change?

## Architecture Onboarding

- **Component map:** Request Stream (Random vs. Fixed Intervals) -> Hugging Face TGI (Continuous Batching, Kernel Fusion) vs. Naive Loop -> NVIDIA H100 GPU (Tensor Cores, CUDA Kernels) -> Phases: Prefill (Compute-heavy, parallelizable) -> Decode (Memory-heavy, serial)

- **Critical path:**
  1. **Request Arrival:** Delays determine batch formation potential.
  2. **Prefill:** Large matrix multiplications. *Optimization target: Tensor Cores / Precision.*
  3. **Decode:** Autoregressive token generation. *Optimization target: Batching / Memory Bandwidth.*

- **Design tradeoffs:**
  - **Latency vs. Energy:** Waiting to form a batch (arrival shaping) lowers energy per request but increases individual request latency.
  - **Precision vs. Overhead:** INT8/INT4 saves memory but adds dequantization kernels which can hurt energy efficiency in decode phases.
  - **Batch Size vs. Padding:** Larger batches improve throughput but risk wasted compute on padding in prefill (static batching).

- **Failure signatures:**
  - **Quantization Backfire:** INT8 decode energy consumption is 2–3× *higher* than FP32.
  - **Batching Plateau:** Increasing batch size from 4 to 16 yields negligible energy gains per token.
  - **Padding Bloat:** Energy per *effective* input token rises with batch size despite hardware efficiency gains.

- **First 3 experiments:**
  1. **Regime Profiling:** Run LLaMA-8B inference in FP32 vs. BF16 vs. INT8. Measure Energy/Token separately for Prefill and Decode to validate the "Regime Coupling" mechanism on your hardware.
  2. **Batching Limits:** Sweep batch sizes (1, 2, 4, 8, 16) on a static workload. Plot Energy per Effective Token vs. Energy per Computed Token to visualize the padding overhead.
  3. **Arrival Shaping:** Configure TGI. Send 1000 requests with random delays vs. fixed 500ms delays. Measure total GPU energy to verify the "Arrival Shaping" efficiency claim.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the energy inefficiencies of quantization in memory-bound decode phases generalize to non-NVIDIA hardware accelerators like AMD GPUs or Google TPUs?
  - **Basis in paper:** [Explicit] The authors explicitly state in the Limitations section that results are based on NVIDIA H100 GPUs and "Extending this analysis to other platforms is crucial for generalization."
  - **Why unresolved:** The overhead of dequantization kernels and memory bandwidth constraints observed on H100s may differ significantly on architectures with distinct memory hierarchies or native lower-precision support.
  - **What evidence would resolve it:** Replicating the phase-aware quantization benchmarks (specifically int8/int4 during decode) on hardware such as AMD MI300X or TPU v5.

- **Open Question 2:** How do energy consumption dynamics shift when scaling from short prompts to long-context, multi-turn dialogues?
  - **Basis in paper:** [Explicit] The authors note their experiments "use relatively short prompts" and that "Real-world usage may involve longer multi-turn dialogues... requiring non-linear models of compute cost."
  - **Why unresolved:** The study operates within a linear scaling regime; longer contexts may trigger distinct memory management bottlenecks or attention mechanisms not captured by the current dataset.
  - **What evidence would resolve it:** Energy profiling of inference runs with prompt lengths significantly exceeding the 4000-token maximum used in the current study.

- **Open Question 3:** What is the relative energy contribution of CPU, RAM, and network I/O compared to the GPU in distributed LLM inference setups?
  - **Basis in paper:** [Explicit] The paper acknowledges that measurements "focus on GPU consumption only" and that system-level components "may contribute significantly... especially in multi-GPU or multi-node setups."
  - **Why unresolved:** Focusing solely on GPU power provides an incomplete picture of the total operational cost for large models requiring tensor parallelism (e.g., LLaMA 70B).
  - **What evidence would resolve it:** Holistic energy profiling using wall-power meters or node-level RAPL measurements during multi-GPU inference.

## Limitations
- The 100× energy reduction claim from arrival shaping is highly dependent on the specific GPU architecture and idle power characteristics of the H100.
- The prefill batching penalty assumes static batching with padding, which may not hold for modern serving systems using continuous or paged attention.
- The benchmark uses a single GPU (H100 SXM) and specific model families; results may not generalize to other architectures or newer models with different KV cache patterns.

## Confidence
- **High confidence:** Prefill vs. decode regime differences (compute-bound vs. memory-bound), batch size plateau around 4 for decode efficiency, and the general principle that system-level optimizations (batching, scheduling) significantly impact energy efficiency.
- **Medium confidence:** The specific 2–3× energy penalty for INT8 in decode is H100-specific and may not hold on other GPUs or with optimized quantization kernels like LiquidGEMM.
- **Low confidence:** The 100× energy reduction from arrival shaping is an extreme case that may not be reproducible across different traffic patterns or serving systems.

## Next Checks
1. **Cross-architecture validation:** Repeat the quantization regime analysis (float32 vs. BF16 vs. INT8) on a different GPU (e.g., A100 or AMD MI300X) to verify if the 2–3× decode energy penalty for INT8 is architecture-dependent.
2. **Padding mitigation test:** Implement bucket-based batching or continuous batching with paged attention and compare prefill energy efficiency against static batching to quantify the padding overhead.
3. **Traffic pattern sensitivity:** Vary request arrival rates and inter-arrival distributions (e.g., Poisson vs. fixed intervals) to determine the minimum traffic volume required to achieve the claimed 100× energy reduction from arrival shaping.