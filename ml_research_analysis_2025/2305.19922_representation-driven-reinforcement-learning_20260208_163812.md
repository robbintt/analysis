---
ver: rpa2
title: Representation-Driven Reinforcement Learning
arxiv_id: '2305.19922'
source_url: https://arxiv.org/abs/2305.19922
tags:
- policy
- learning
- representation
- linear
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Representation-Driven Reinforcement Learning
  (RepRL), a novel framework that reframes the exploration-exploitation problem as
  a representation-exploitation problem. The core idea is to represent policies as
  estimates of their expected values and leverage techniques from contextual bandits
  to guide exploration.
---

# Representation-Driven Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.19922
- Source URL: https://arxiv.org/abs/2305.19922
- Reference count: 40
- One-line result: RepRL reframes exploration-exploitation as representation-exploitation by embedding policies into a linear feature space for optimal exploration

## Executive Summary
This paper presents Representation-Driven Reinforcement Learning (RepRL), a novel framework that reframes the exploration-exploitation problem as a representation-exploitation problem. The core idea is to represent policies as estimates of their expected values and leverage techniques from contextual bandits to guide exploration. By embedding policy networks into a linear feature space, RepRL enables optimal exploration through good policy representations.

The framework is applied to both evolutionary strategies and policy gradient methods, showing significant performance improvements over traditional approaches. In MuJoCo environments, RepRL outperforms or matches ES performance, while in sparse reward settings, it achieves better results than ES and SAC. On MinAtar tasks, RepRL demonstrates superior performance compared to PPO across all tested domains.

## Method Summary
RepRL trains a variational encoder to map policy parameters θ to a latent space z, where a linear decoder predicts returns. This creates a linear embedding of policy value that enables contextual bandit algorithms (Thompson Sampling or OFUL) to guide exploration. The framework is instantiated in two variants: RepES (combining representation learning with ES) and RepPG (combining with policy gradients). The encoder is trained via ELBO with a linear decoder, while the bandit module maintains uncertainty estimates and selects policies from a decision set. Inner-trajectory sampling is used to approximate policy values.

## Key Results
- RepRL outperforms or matches ES on standard MuJoCo environments
- In sparse reward settings, RepRL achieves better performance than both ES and SAC
- On MinAtar tasks, RepRL demonstrates superior performance compared to PPO across all tested domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding policies into a linear feature space enables the use of optimal exploration strategies from contextual bandits.
- Mechanism: The framework learns a mapping f_φ: Π → R^d such that v(π) = ⟨f(π), w⟩. Once linear, Thompson Sampling or OFUL provide bounded regret exploration in policy space.
- Core assumption: A low-dimensional linear embedding exists (Assumption 3.1). Theoretically satisfied when d = |S||A| but practical benefit requires d ≪ |S||A|.
- Evidence anchors:
  - [abstract] "embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem"
  - [Section 3] "any policy π ∈ Π can be written as π(a|s) = ρ_π(s,a) / Σ_a' ρ_π(s,a')"
  - [corpus] Neighbor papers discuss exploration-exploitation trade-offs but do not address policy-space representations directly (weak corpus support for this specific mechanism).
- Break condition: If the learned representation fails to produce approximately linear value estimates, bandit optimism will misguide exploration.

### Mechanism 2
- Claim: A variational encoder with linear decoder enforces representation linearity with respect to policy value.
- Mechanism: The ELBO loss trains encoder f_φ(z|θ) with decoder p_κ(G|z) = N(κ^T z, σ²). The linear decoder constrains the latent space so that dot products correlate with returns.
- Core assumption: Gaussian encoder and decoder with linear mean term preserve the linearity required for bandit algorithms.
- Evidence anchors:
  - [Section 3.2] "the choice of the decoder to be linear is crucial, due to the fact that the value is supposed to be linear w.r.t learned embeddings"
  - [Appendix B] Full ELBO derivation provided
  - [corpus] No corpus papers validate this specific variational design choice.
- Break condition: Non-linear decoders or poorly conditioned posteriors degrade bandit confidence estimates.

### Mechanism 3
- Claim: Inner-trajectory sampling recovers scaled value while propagating learning signals beyond the effective horizon.
- Mechanism: Sample states from ρ_π instead of β, estimating ṽ(π) = E_{s~ρ_π}[v(π, s)] = v(π)/(1-γ), which preserves ranking equivalence.
- Core assumption: Stationary occupancy measure ρ_π; discount factor γ < 1.
- Evidence anchors:
  - [Section 3.4] "sampling along the trajectory from ρ_π approximates the scaled value"
  - [Proposition 3.2] Proof that ṽ(π) = v(π)/(1-γ)
  - [corpus] Not directly addressed in corpus neighbors.
- Break condition: Non-stationary policies or improper ρ_π estimation break the equivalence.

## Foundational Learning

- Concept: **Linear contextual bandits (OFUL/Thompson Sampling)**
  - Why needed here: RepRL reduces policy search to selecting arms in a linear bandit; understanding confidence sets and optimism is essential.
  - Quick check question: Given a feature vector x and history (V_t, b_t), can you compute the UCB bonus √(x^T V_t^{-1} x)?

- Concept: **Variational autoencoders and ELBO**
  - Why needed here: The representation encoder is trained via ELBO; the KL term regularizes latent structure.
  - Quick check question: What happens to the representation if the KL weight is too low versus too high?

- Concept: **Discounted state-action occupancy measures**
  - Why needed here: The theoretical justification for linearity relies on expressing value as ⟨ρ_π, r⟩.
  - Quick check question: For γ = 0.99, approximately how many timesteps contribute meaningablly to ρ_π?

## Architecture Onboarding

- Component map: Policy network π_θ -> Representation encoder f_φ -> Latent z -> Linear decoder p_κ -> Return distribution
- Critical path: θ → f_φ(θ) → sample z → bandit selects θ' from decision set → rollout → update V_t, b_t, f_φ, κ via ELBO
- Design tradeoffs:
  - Policy-space vs. latent-space decision sets: Policy-space is stable and simple; latent-space requires inverse mapping and risks unrealizable policies
  - History-based sets improve coverage but scale linearly with history length
  - Mixing bandit gradient with ES gradient (e.g., 80/20) stabilizes early training when representation is weak
- Failure signatures:
  - Bandit overconfidence on unseen policies (representation not yet calibrated)
  - Latent-space decision set generates unrealizable policies → gradient descent fails to find θ*
  - Sparse rewards with insufficient trajectory samples → ELBO collapses to prior
- First 3 experiments:
  1. **GridWorld sanity check**: Reproduce the 8×8 toy experiment. Verify that t-SNE of learned representations clusters policies by value.
  2. **Decision set ablation on SparseHalfCheetah**: Compare policy-space, latent-space, and history-based sets (Figure 8). Confirm policy-space is stable.
  3. **RepES vs. ES on standard MuJoCo**: Reproduce Figure 5 (top row). Check that RepES matches or exceeds ES within 300 rounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can latent space decision sets be constructed to guarantee the realizability of policies and prevent the restoration of out-of-distribution parameters?
- Basis in paper: [explicit] Section 3.3 identifies the challenge that representations may not map to valid policies ("realizability") and leaves "more sophisticated methods of latent-based decision sets for future work."
- Why unresolved: The authors note that currently, restoring parameters from latent space can result in $\theta$ values too far from the data manifold, leading to value overestimation.
- What evidence would resolve it: A proposed method for latent-based decision sets that includes a constraint or regularization technique ensuring decoded policies remain valid and performant compared to policy-space sampling.

### Open Question 2
- Question: Can RepRL leverage large-scale pretrained Transformers for representation learning while maintaining the linear features required for efficient bandit optimization?
- Basis in paper: [explicit] Section 7 lists "incorporate RepRL into more involved representation methods, including pretrained large Transformers" as a primary avenue for future work.
- Why unresolved: It is currently unknown if the complex, high-dimensional representations derived from Transformers would satisfy the linear embedding assumption (Assumption 3.1) required for the bandit algorithms to function effectively.
- What evidence would resolve it: Empirical results demonstrating that a Transformer-based encoder can be trained within the RepRL framework to achieve comparable or superior performance to the current architectures on complex control tasks.

### Open Question 3
- Question: Do general contextual bandit algorithms (e.g., SquareCB) provide superior performance over linear bandits in RepRL when the linear representation assumption is relaxed?
- Basis in paper: [explicit] Section 7 suggests future work should "explore the use of general contextual bandit algorithms... which are not restricted to linear representations."
- Why unresolved: The current framework relies entirely on enforcing linearity to apply algorithms like OFUL or Thompson Sampling; the efficacy of non-linear bandit approaches in this specific policy-search context is untested.
- What evidence would resolve it: A comparative study showing that using algorithms like SquareCB allows RepRL to succeed in environments where learning a strictly linear value representation is difficult or suboptimal.

## Limitations
- The linear embedding assumption is theoretically satisfied only in the tabular limit, with practical benefits depending on learning a low-dimensional representation that preserves value linearity
- Thompson Sampling's σ hyperparameter is unspecified, potentially affecting exploration-exploitation balance across domains
- The representation's generalization beyond the decision set is not characterized, leaving open whether bandit optimism translates to out-of-distribution policies

## Confidence
- **High**: ES and PG algorithmic frameworks are sound; the linear bandit reduction is valid given the assumptions
- **Medium**: The variational encoder-decoder design achieves the intended linearity, based on stated design choices but lacking empirical validation of latent structure
- **Medium**: The inner-trajectory sampling approximation is mathematically correct under stated conditions (γ < 1, stationary policies)

## Next Checks
1. **Representation Linearity Test**: Train RepRL on SparseHalfCheetah, then fit a linear model predicting returns from embeddings. Measure R²; if significantly below 1.0, the linear assumption is violated.
2. **Bandit Uncertainty Calibration**: Compare prediction intervals from Thompson Sampling to empirical return variance on held-out policies. If intervals are systematically too narrow or wide, adjust σ or modify the latent decoder.
3. **Decision Set Sensitivity**: On MinAtar tasks, ablate decision set size (e.g., N=512, 1024, 2048) and evaluate performance variance. This quantifies the impact of bandit sample efficiency on learning stability.