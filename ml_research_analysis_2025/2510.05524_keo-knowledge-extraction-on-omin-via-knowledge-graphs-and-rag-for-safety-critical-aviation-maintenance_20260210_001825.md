---
ver: rpa2
title: 'KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical
  Aviation Maintenance'
arxiv_id: '2510.05524'
source_url: https://arxiv.org/abs/2510.05524
tags:
- questions
- knowledge
- global
- maintenance
- records
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents KEO, a knowledge extraction framework for aviation
  maintenance that integrates knowledge graphs with retrieval-augmented generation.
  The authors address the challenge of applying large language models to safety-critical
  domains where factual accuracy and structured reasoning are essential.
---

# KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance

## Quick Facts
- arXiv ID: 2510.05524
- Source URL: https://arxiv.org/abs/2510.05524
- Authors: Kuangshi Ai; Jonathan A. Karr; Meng Jiang; Nitesh V. Chawla; Chaoli Wang
- Reference count: 28
- One-line primary result: KEO achieves 4.31-4.87/5 on global sensemaking tasks by combining knowledge graphs with retrieval-augmented generation for aviation maintenance

## Executive Summary
KEO addresses the challenge of applying large language models to safety-critical domains where factual accuracy and structured reasoning are essential. The framework constructs a knowledge graph from aviation maintenance records and uses it to enhance retrieval-augmented generation pipelines, enabling more coherent dataset-wide reasoning. Experiments show KEO significantly improves global sensemaking tasks compared to traditional text-chunk RAG, particularly when paired with stronger backbone models like Gemma-3-Instruct. However, for fine-grained procedural tasks, text-chunk RAG remains more effective, highlighting that different retrieval paradigms complement each other depending on task demands.

## Method Summary
KEO builds a domain-specific knowledge graph from aviation maintenance records using LLM-based triplet extraction with 15 predefined relation types. The framework employs semantic node identification, m-hop graph expansion with Maximum Spanning Tree filtering, and hierarchical community summarization via Leiden algorithm. Context reconstruction combines DFS traversal serialization with community summaries. The system uses backbone LLMs (Gemma-3-Instruct, Phi-4, Mistral-Nemo) and LLM-as-judge evaluation (GPT-4o, Llama-3.3-70B-Instruct) on a benchmark of 133 questions across global sensemaking and knowledge-to-action task types.

## Key Results
- KEO achieves 4.31-4.87/5 on global sensemaking tasks versus 3.65-4.42 for text-chunk RAG
- KG-RAG excels at dataset-wide reasoning while text-chunk RAG remains superior for procedural tasks
- Stronger backbone models (Gemma-3-Instruct) leverage KG context better; weaker models (Mistral-Nemo) show degraded performance
- Optimal KG construction occurs with 200-300 records; larger KGs add noise without improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-level semantic retrieval provides more coherent context than text-chunk retrieval for global reasoning tasks.
- Mechanism: KEO embeds individual entity mentions (graph nodes) rather than document chunks. Top-k semantically similar entities become seed nodes for structured expansion, preserving relational context that chunk boundaries fragment.
- Core assumption: Entity-level granularity better preserves the semantic relationships needed for multi-hop reasoning than arbitrary text segmentation.
- Evidence anchors:
  - [section 3.2.1] "KEO performs retrieval at the entity level rather than the chunk level... A key limitation of text-chunk RAG is that retrieved chunks often lack intrinsic structure or semantic cohesion"
  - [section 4.2] "KEO significantly outperforms both vanilla prompting and text-chunk RAG in global sensemaking tasks when evaluated by GPT-4o"
  - [corpus] Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs paper reports similar findings: structured KGs improve domain-specific reasoning, suggesting the mechanism may generalize.
- Break condition: When queries require precise procedural details from single documents, entity abstraction loses critical contextual nuance (shown in knowledge-to-action task degradation).

### Mechanism 2
- Claim: Importance-aware graph expansion via MST filtering prioritizes salient relationships while limiting context window consumption.
- Mechanism: From seed nodes, KEO expands m-hop neighbors to form a subgraph, converts to undirected weighted form, then applies Kruskal's algorithm to compute Maximum Spanning Trees per connected component—retaining highest-weight edges that represent most frequent co-occurrence patterns.
- Core assumption: Edge frequency in the maintenance corpus correlates with relationship importance for reasoning tasks.
- Evidence anchors:
  - [section 3.2.2] "These MSTs retain the most structurally significant links within each connected region of the expanded subgraph, ensuring that downstream reasoning is supported by a coherent, importance-aware knowledge structure"
  - [table 5] Performance peaks at 200-300 records for KG construction, suggesting there's an optimal graph density beyond which noise increases
  - [corpus] Weak evidence directly comparing MST vs other filtering approaches; corpus papers don't evaluate this specific technique.
- Break condition: When graph connectivity is sparse (few relations per entity), MST pruning may remove potentially relevant paths.

### Mechanism 3
- Claim: Combining local graph traversal paths with hierarchical community summaries enables both granular detail and global perspective.
- Mechanism: DFS traversal of MSTs produces structured textual paths. Separately, Leiden community detection identifies densely connected subgraphs; summaries are generated recursively from leaf communities upward, then concatenated with traversal output.
- Core assumption: LLMs can effectively integrate both specific relational paths and abstracted community-level summaries in a single context.
- Evidence anchors:
  - [section 3.2.3] "These summaries are concatenated with the graph traversal text to provide the LLM with both local detail and global structure"
  - [section 4.2] "The most notable improvements appear in the global perspective criterion and the overall evaluation score"
  - [corpus] SemRAG and other KG-RAG papers report benefits from combining structured retrieval with summarization, supporting the general approach.
- Break condition: When context window limits prevent including both detailed paths and community summaries, prioritization heuristics may discard relevant information.

## Foundational Learning

- Concept: **Knowledge Graph Construction via LLM Triple Extraction**
  - Why needed here: KEO dynamically builds its KG by prompting LLMs to extract (head, relation, tail) triplets from maintenance records; understanding this process is essential for debugging retrieval failures.
  - Quick check question: Given the triplet schema (OWNED_BY, HAS_CAUSE, PART_OF, etc.), what triplets would you extract from: "Engine quit during takeoff due to fuel contamination"?

- Concept: **Maximum Spanning Tree for Graph Pruning**
  - Why needed here: KEO uses MST to filter expanded subgraphs; you must understand why this prioritizes high-weight edges and how it differs from threshold-based filtering.
  - Quick check question: Given a subgraph with edges weighted [8, 5, 3, 2], which edges would an MST retain vs remove for a 4-node component?

- Concept: **LLM-as-Judge Evaluation Limitations**
  - Why needed here: All reported results depend on GPT-4o and Llama-3.3 as evaluators; the paper explicitly notes these judges are "neither fully valid nor reliable" and vulnerable to prompt manipulation.
  - Quick check question: What confounding factors might cause an LLM judge to prefer longer, more detailed answers regardless of accuracy?

## Architecture Onboarding

- Component map: Raw records -> LLM triplet extraction -> KG storage -> Query embedding -> Top-k seed nodes -> m-hop expansion + MST -> Filtered subgraph -> DFS traversal + community summaries -> Context prompt -> Local LLM -> Answer -> LLM Judge -> Score

- Critical path:
  1. KG Construction Module: LLM-based triplet extraction → weighted graph G = {(h, t, r, w)}
  2. Semantic Node Identifier: Embedding-based top-k entity retrieval from query
  3. Graph Expander: m-hop expansion → undirected conversion → MST filtering per component
  4. Context Reconstructor: DFS traversal serialization + Leiden community summaries → prompt assembly
  5. Evaluation Layer: LLM judges (GPT-4o, Llama-3.3) scoring on 5 dimensions

- Design tradeoffs:
  - KG size: Paper tests 100-500 records; 200-300 optimal. Larger KGs add noise.
  - Backbone model strength: Stronger LLMs (Gemma-27B) leverage KG context better; weaker models (Mistral-12B) show degraded performance—suggesting minimum reasoning capability thresholds.
  - Task type: KG-RAG excels at global sensemaking (4.31-4.87/5) but underperforms text-chunk RAG on procedural tasks—architectural choice depends on use case.

- Failure signatures:
  - Knowledge-to-action tasks: KEO scores ≈ text-chunk RAG or slightly worse (Table 3, Table 6)
  - Weak backbone models: Mistral-Nemo shows minimal KG advantage (Table 5, Figure 2)
  - Sparse entity mentions: Short maintenance records may not generate sufficient graph connectivity

- First 3 experiments:
  1. Baseline sanity check: Run vanilla LLM, text-chunk RAG, and KEO on 20 questions from each task type; verify KEO wins on global sensemaking, text-chunk wins on procedural.
  2. KG size ablation: Construct KGs from 50, 100, 200, 300 records; measure retrieval latency and answer quality to find your domain's optimal density.
  3. Context window stress test: With maximum MST depth and full community summaries, measure token counts; determine if your deployment context window can accommodate both or requires truncation strategy.

## Open Questions the Paper Calls Out

- Can a hybrid retrieval approach combining KEO and text-chunk RAG optimize performance across both global sensemaking and fine-grained procedural tasks?
- How does KEO performance and construction latency scale when moving beyond 500 records to significantly larger maintenance corpora?
- Can KEO be effectively adapted to other safety-critical domains such as healthcare or power systems?

## Limitations

- LLM-as-judge evaluation methodology acknowledged as "neither fully valid nor reliable" and vulnerable to prompt manipulation
- Optimal KG construction size (200-300 records) appears domain-specific rather than universal
- Performance degradation of weaker backbone models suggests minimum capability thresholds not fully characterized

## Confidence

- **High confidence**: KEO's superiority for global sensemaking tasks (4.31-4.87/5) is well-supported by controlled experiments across multiple backbone models and judges.
- **Medium confidence**: The complementary relationship between KG-RAG and text-chunk RAG for different task types is demonstrated, but real-world deployment implications need validation.
- **Low confidence**: The generalizability of the 200-300 record optimal KG size and the specific importance of MST filtering over alternative graph pruning methods lack external validation.

## Next Checks

1. Conduct human evaluation on a subset of questions to verify LLM judge scores, particularly for knowledge-to-action tasks where KEO underperforms.
2. Test KG-RAG performance with different backbone model families (e.g., Claude, Llama) to assess whether Gemma-3-Instruct's success is model-specific or represents a broader trend.
3. Implement ablation studies removing the MST filtering step to quantify its contribution versus simpler graph pruning approaches.