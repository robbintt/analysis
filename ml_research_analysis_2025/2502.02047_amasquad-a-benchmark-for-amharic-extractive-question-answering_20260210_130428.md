---
ver: rpa2
title: 'AmaSQuAD: A Benchmark for Amharic Extractive Question Answering'
arxiv_id: '2502.02047'
source_url: https://arxiv.org/abs/2502.02047
tags:
- dataset
- answer
- context
- translated
- amharic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmaSQuAD, a translation-based framework for
  creating extractive question-answering datasets for low-resource languages like
  Amharic. The framework translates SQuAD 2.0 into Amharic and addresses translation
  misalignment through cosine similarity and Longest Common Subsequence techniques.
---

# AmaSQuAD: A Benchmark for Amharic Extractive Question Answering

## Quick Facts
- **arXiv ID**: 2502.02047
- **Source URL**: https://arxiv.org/abs/2502.02047
- **Reference count**: 21
- **Primary result**: Translation-based framework creates AmaSQuAD dataset; fine-tuning XLM-R on synthetic data improves Amharic QA F1 from 36.55% to 44.41% on development set and from 67.80% to 68.80% on human-curated AmQA dataset

## Executive Summary
This paper introduces AmaSQuAD, a framework for creating extractive question-answering datasets for low-resource languages like Amharic through translation of SQuAD 2.0. The authors address the challenge of translation misalignment by combining cosine similarity with Longest Common Subsequence (LCS) techniques to recover answer spans in translated contexts. They fine-tune XLM-R on the synthetic AmaSQuAD dataset, demonstrating improved performance over baseline models. The work provides the first detailed baseline using the AmQA dataset and shows that synthetic data can enhance Amharic QA model performance, though improvements on human-curated data are marginal.

## Method Summary
The authors create AmaSQuAD by translating SQuAD 2.0 from English to Amharic using Google Translate via Deep Translator library. To address alignment issues where translated answers may not directly match context spans, they implement an answer extraction algorithm combining cosine similarity from fine-tuned BERT embeddings with LCS scoring, weighted 2/3 to 1/3 respectively, and prioritizing spans near original answer positions. The dataset is filtered by a similarity threshold of 0.6, resulting in 63,650 training and 14,479 development instances. XLM-R Large is fine-tuned on this synthetic data (3 epochs, lr=1e-5, max_seq_length=256) and evaluated on both AmaSQuAD development set and human-curated AmQA dataset.

## Key Results
- XLM-R fine-tuned on AmaSQuAD achieves F1 score of 44.41% on AmaSQuAD development set, up from baseline of 36.55%
- Model improves F1 from 67.80% to 68.80% on human-curated AmQA dataset
- Exact Match on AmQA increases from 52.50% to 52.66%
- Similarity distribution shows 23,144 training samples and 5,314 development samples in the 0.9-1.0 range
- Approximately 35% of samples filtered due to similarity scores below threshold

## Why This Works (Mechanism)

### Mechanism 1: Translation Alignment via Weighted Semantic and Syntactic Matching
A weighted combination of embedding-based cosine similarity and LCS recovers answer spans in translated contexts where direct string matching fails. The system scans translated context using sliding windows, scoring each candidate span with w1(2/3) × cosine_similarity + w2(1/3) × LCS_ratio, selecting the highest-scoring span. This works because translation preserves semantic meaning sufficiently for embeddings to capture similarity between translated answers and context positions. Evidence shows successful alignment for majority of samples (23,144 in 0.9-1.0 range), though 35% may be excluded at the 0.6 threshold.

### Mechanism 2: Cross-Lingual Transfer via Synthetic Data Pre-Training
Fine-tuning XLM-R on machine-translated QA data provides useful inductive bias for low-resource language QA, even when evaluated on human-curated datasets. XLM-R Large (pre-trained on 2.5TB including Amharic) is first fine-tuned on SQuAD 2.0, then further fine-tuned on AmaSQuAD, creating a model with both cross-lingual representations and task-specific Amharic QA capabilities. The model generalizes to cleaner human-curated data despite translation artifacts. Results show F1 improvement from 67.80% to 68.80% on AmQA, validating the approach though improvements are marginal (+1.0% F1).

### Mechanism 3: Proximity-Based Answer Disambiguation
When multiple answer instances exist in translated context, prioritizing spans near the original English answer position improves selection accuracy. During alignment scan, candidates closer to the sentence containing the original English answer receive preference when scores are comparable. This exploits the assumption that Google Translate's English→Amharic translation preserves approximate word/sentence ordering despite Amharic's different syntactic structure. The heuristic breaks ties when similarity scores are equal, though its contribution requires further validation.

## Foundational Learning

- **Extractive Question Answering Architecture**: Understanding span prediction (start/end indices) vs generative QA is essential for interpreting why alignment matters. Quick check: Given context "The Eiffel Tower is in Paris" and question "Where is the Eiffel Tower?", would an extractive model output "Paris" or a token index?

- **Machine Translation Alignment Problem**: The core challenge is that translating "answer" independently from "context" creates lexical mismatches—direct substring search fails. Quick check: If English answer "White House" translates to Amharic as two words, but context translation uses different word order, why can't you just search for the translated answer string?

- **Cross-Lingual Transfer Learning**: XLM-R's multilingual pre-training enables zero-shot transfer; understanding this explains why baseline (no fine-tuning) achieves 67.80% F1 on AmQA. Quick check: Why does XLM-R work on Amharic QA despite never seeing Amharic QA data during pre-training?

## Architecture Onboarding

- **Component map**: SQuAD 2.0 (English) → Google Translate → Raw Amharic → Alignment Engine (cosine similarity + LCS + proximity) → Filtering (threshold ≥ 0.6) → AmaSQuAD → XLM-R Large fine-tuning (3 epochs, lr=1e-5)

- **Critical path**: Alignment quality directly determines dataset utility—if similarity scores cluster below 0.6, filtering removes too much data; if false positives pass filtering, model learns incorrect span boundaries

- **Design tradeoffs**:
  - w1/w2 weights (2/3, 1/3): Higher w1 prioritizes semantic similarity (robust to morphological variation); higher w2 prioritizes exact character overlap (stricter but brittle)
  - Similarity threshold (0.6): Lower retains more data but introduces noise; higher improves precision but may over-filter valid translations
  - Unanswerable question downsampling (6,000 from 50,000+): Reduces training time but may underrepresent refusal behavior

- **Failure signatures**:
  - High EM but low F1 on dev set → model predicts correct span boundaries but wrong internal tokens (alignment noise)
  - Performance degrades on AmQA vs AmaSQuAD → synthetic data distribution shift not addressed
  - Many samples filtered (< 50% retention) → translation quality insufficient for target language pair

- **First 3 experiments**:
  1. Ablate alignment components: Run with w1=1.0 (cosine only) vs w2=1.0 (LCS only) vs combined; measure impact on similarity distribution and downstream F1
  2. Threshold sensitivity: Test 0.4, 0.5, 0.6, 0.7 thresholds; plot retention rate vs AmQA performance to find optimal noise-quantity tradeoff
  3. Validate on held-out human translations: Manually translate 100 SQuAD samples; compare alignment accuracy of Algorithm 1 against gold-standard aligned answers to quantify false positive rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal weighting between cosine similarity and LCS for extracting answer spans in translated datasets?
- **Basis**: The authors state that "various adjustments of weights w1 and w2... should be considered," noting the current 2/3 and 1/3 split involves a tradeoff between semantic and syntactic similarities
- **Why unresolved**: The paper fixed these weights based on intuition rather than empirical tuning; an ablation study was not conducted
- **What evidence would resolve it**: A comparative analysis of alignment quality and downstream F1 scores using varying weight configurations (e.g., grid search) on the validation set

### Open Question 2
- **Question**: To what extent does incorporating human feedback improve the quality of the synthetic AmaSQuAD dataset compared to the purely automatic alignment method?
- **Basis**: Section V.A explicitly notes that "incorporating human feedback on the quality of the translated AmaSQuAD dataset is essential" to identify where the algorithm falters
- **Why unresolved**: The current framework relies entirely on automated heuristics without qualitative human verification of the aligned answer spans
- **What evidence would resolve it**: A human annotation study evaluating the accuracy of the extracted answer spans, followed by a comparison of model performance trained on human-verified data versus the fully automated data

### Open Question 3
- **Question**: How does pre-training on a large, native Amharic corpus compare to fine-tuning on synthetic translated data for QA tasks?
- **Basis**: The conclusion suggests it is "imperative to explore the impact of pre-training on a large Amharic corpus" to advance QA for underrepresented languages
- **Why unresolved**: The study relies on XLM-R, which is pre-trained on multiple languages but may not be optimized specifically for Amharic nuances
- **What evidence would resolve it**: Benchmarking the fine-tuned XLM-R model against a language model pre-trained specifically on a large Amharic corpus (e.g., AmBERT) using the same QA datasets

### Open Question 4
- **Question**: Can models trained on AmaSQuAD effectively identify unanswerable questions in a human-curated Amharic dataset?
- **Basis**: Section V.B notes that the human-curated AmQA dataset lacks unanswerable questions, which limits the assessment of the model's ability to handle real-world scenarios where answers may not exist
- **Why unresolved**: While AmaSQuAD includes unanswerable questions derived from SQuAD 2.0, the model's performance on this specific capability could not be verified against the human-curated benchmark
- **What evidence would resolve it**: Evaluation of the fine-tuned model on an amended version of the AmQA dataset (or a new test set) that includes plausible questions without answers in the provided context

## Limitations

- The translation alignment mechanism's robustness across different language pairs and text domains remains uncertain, with 35% of samples filtered due to similarity scores below threshold
- Marginal improvement on AmQA (68.80% vs 67.80% F1) suggests potential ceiling effects or limitations in the human-curated evaluation set's ability to measure meaningful progress
- The proximity-based tie-breaking heuristic lacks independent validation, making it difficult to assess whether this component contributes meaningfully to alignment accuracy

## Confidence

- **High Confidence**: The overall framework design (translation + alignment + fine-tuning) is methodologically sound and aligns with established practices in cross-lingual transfer learning
- **Medium Confidence**: The specific parameter choices (w1=2/3, w2=1/3, threshold=0.6) and their optimality for Amharic remain uncertain without ablation studies
- **Low Confidence**: The extent to which synthetic data noise impacts model generalization to unseen domains, and whether the observed improvements would scale to larger, more diverse human-curated datasets

## Next Checks

1. **Ablation Study of Alignment Components**: Systematically test the model's performance using cosine similarity only (w1=1.0), LCS only (w2=1.0), and various weight combinations to determine the individual and combined contributions of each component to both alignment accuracy and downstream QA performance.

2. **Manual Validation of Alignment Quality**: Conduct human evaluation on a random sample of 100 aligned answers to measure precision and recall of the alignment algorithm, comparing Algorithm 1's output against gold-standard alignments created by bilingual annotators. This would quantify the false positive rate and validate the claimed 0.6 threshold effectiveness.

3. **Cross-Domain Generalization Test**: Evaluate the fine-tuned model on an additional Amharic QA dataset from a different domain (e.g., news articles, social media, or educational materials) to assess whether synthetic data pre-training provides benefits beyond the Wikipedia-style text distribution of SQuAD and AmQA.