---
ver: rpa2
title: Spatio-temporal transformer to support automatic sign language translation
arxiv_id: '2502.02587'
source_url: https://arxiv.org/abs/2502.02587
tags:
- sign
- language
- proposed
- translation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Transformer-based architecture for sign language
  translation (SLT) that encodes spatio-temporal motion gestures while preserving
  local and long-range spatial information. The method enhances input representation
  by using optical-flow images to capture motion patterns, creating 2D convolutional
  feature maps, and adding 2D positional encodings.
---

# Spatio-temporal transformer to support automatic sign language translation

## Quick Facts
- arXiv ID: 2502.02587
- Source URL: https://arxiv.org/abs/2502.02587
- Authors: Christian Ruiz; Fabio Martinez
- Reference count: 0
- One-line primary result: Transformer-based architecture with 2D self-attention achieves BLEU4 scores of 46.84% on CoL-SLTD and 30.77% on PHOENIX14T

## Executive Summary
This paper proposes a Transformer-based architecture for sign language translation that addresses the challenge of encoding spatio-temporal motion gestures while preserving local and long-range spatial information. The method introduces a novel 2D pixel-wise self-attention mechanism that identifies long-range spatial dependencies between gestural articulators, complementing convolutional layers. By using optical-flow images as input representation and maintaining 2D spatial structure throughout the network, the approach achieves state-of-the-art BLEU4 scores on both a Colombian Sign Language dataset (46.84%) and the established PHOENIX14T benchmark (30.77%).

## Method Summary
The architecture processes sign language videos by first converting frames to optical flow representations, then extracting 2D feature maps using a pre-trained ResNet18 backbone. These features (sequenceLength × channels × 7 × 7) are combined with 2D positional encodings to maintain spatial layout information. A single encoder layer with 2D pixel-wise self-attention captures long-range spatial dependencies, followed by a feed-forward network with two convolutional layers. The output feeds into a standard Transformer decoder with four attention heads. The model is trained end-to-end using a combination of CTC loss for gloss mapping and cross-entropy loss for translation, optimized with Adam over 10 epochs with batch size 1.

## Key Results
- Achieves BLEU4 score of 46.84% on the Colombian Sign Language Translation Dataset (CoL-SLTD)
- Achieves BLEU4 score of 30.77% on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset
- Optical flow input representation provides 35% relative improvement over RGB baseline (BLEU4 from 0.2277 to 0.3077)
- 2D positional encodings contribute approximately 8 BLEU4 points when ablated (from ~0.51 to ~0.44)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preserving 2D spatial structure in feature maps improves sign language translation by retaining local spatial relationships that are lost during flattening.
- **Mechanism:** The architecture convolves input images into 2D feature matrices (7×7 spatial dimensions) rather than flattened vectors, then adds 2D positional encodings that assign unique coordinate pairs (i, j) to each pixel position, allowing the model to maintain spatial layout semantics throughout encoding.
- **Core assumption:** Sign gestures contain spatially-distributed information (hand position, facial expressions, body orientation) where relative spatial relationships carry semantic meaning.
- **Evidence anchors:**
  - [abstract] "enhances input representation by using optical-flow images to capture motion patterns, creating 2D convolutional feature maps, and adding 2D positional encodings"
  - [section 3.1] "We use a CNN backbone as a feature extractor for sign language images, convolving them into a 2D matrix instead of a flattened feature vector. This allows us to retain spatial locality information present in the image."
  - [corpus] Weak direct corpus support for 2D preservation specifically; neighboring papers focus on different representation strategies.
- **Break condition:** If signs can be adequately represented by keypoint/pose sequences alone, the 2D spatial preservation provides diminishing returns over computationally cheaper skeleton-based approaches.

### Mechanism 2
- **Claim:** 2D pixel-wise self-attention captures long-range spatial dependencies between gestural articulators (hands, face, body) that local convolutions cannot efficiently model.
- **Mechanism:** Computes attention maps A with dimensions H×W×H×W, where each spatial position attends to all other positions, creating weighted combinations that highlight relationships between distant gesture components (e.g., hand shape correlated with facial expression).
- **Core assumption:** Meaningful sign semantics emerge from relationships between spatially distant body regions, not just local patterns.
- **Evidence anchors:**
  - [abstract] "introduces a 2D pixel-wise self-attention mechanism to identify long-range spatial dependencies, complementing convolutional layers"
  - [section 3.2] "This module allows us to efficiently model relationships between spatial regions that are widely separated, which is crucial for distinguishing gestures that involve both manual and non-manual articulators."
  - [corpus] EASLT paper emphasizes Non-Manual Signals (facial expressions) as semantically critical, supporting the need for multi-region attention.
- **Break condition:** If computational budget constraints require real-time inference, the O(H²W²) attention computation becomes prohibitive for higher-resolution feature maps.

### Mechanism 3
- **Claim:** Optical flow representations improve translation accuracy by explicitly encoding motion kinematics, helping the network focus on movement patterns essential to sign meaning.
- **Mechanism:** Pre-processes video frames into optical flow images before CNN feature extraction, capturing displacement vectors between frames that represent gestural motion independent of appearance.
- **Core assumption:** Motion patterns are more invariant to signer appearance and contain discriminative information for sign identification.
- **Evidence anchors:**
  - [abstract] "enhances input representation by using optical-flow images to capture motion patterns"
  - [section 3.6] "we utilized sign language videos represented as a sequence of frames and converted them into optical flow representations"
  - [table 3] BLEU4 improves from 0.2277 (RGB) to 0.3077 (FLOW)—a 35% relative improvement
  - [corpus] Neighbor paper "How important is motion in sign language translation?" (Rodriguez et al., 2021) cited in related work, supports motion importance.
- **Break condition:** If optical flow computation adds unacceptable latency for real-time applications, or if signs contain significant static pose components where motion is less discriminative.

## Foundational Learning

- **Concept: Self-Attention Mechanism**
  - **Why needed here:** The 2D self-attention module requires understanding how Q, K, V projections create attention weights. Without this, the pixel-wise attention implementation will be opaque.
  - **Quick check question:** Can you explain why the softmax is applied to QK^T/√d_k before multiplying by V?

- **Concept: Positional Encoding in Transformers**
  - **Why needed here:** The paper extends 1D sinusoidal encodings to 2D; understanding the original formulation is prerequisite to grasping why separate row/column encodings preserve 2D structure.
  - **Quick check question:** What problem do positional encodings solve in standard Transformers, and why might 1D encodings fail for 2D spatial data?

- **Concept: Optical Flow Representation**
  - **Why needed here:** Input preprocessing uses optical flow rather than raw RGB. Understanding what optical flow encodes (displacement, not appearance) is critical for debugging input pipelines.
  - **Quick check question:** What does a pixel in an optical flow image represent, and why might motion information help distinguish similar-appearing signs?

## Architecture Onboarding

- **Component map:**
  Video Frames → Optical Flow Computation → ResNet18 (pre-trained ImageNet) → 2D Feature Maps [seq_len × channels × 7 × 7] → + 2D Positional Encodings → 2D Self-Attention Encoder (1 layer) → Feed-Forward Network (2 conv layers, 3×3 kernels, batch norm) → Flatten → Standard Transformer Decoder (4 attention heads) → Text Output

- **Critical path:** Optical flow quality → ResNet feature quality → 2D positional encoding correctness → attention map computation → decoder attention. Errors in flow computation or 2D encoding will propagate through the entire pipeline.

- **Design tradeoffs:**
  - Single encoder layer (computational efficiency vs. representational capacity)
  - Batch size of 1 (memory constraints vs. training stability)
  - 7×7 spatial resolution (downsampling efficiency vs. spatial detail preservation)
  - Optical flow input (motion focus vs. loss of static appearance cues)

- **Failure signatures:**
  - Translations ignore facial expressions → 2D attention may not be learning cross-region dependencies; check attention map visualizations
  - Similar signs confused → optical flow may be noisy or spatial resolution too coarse
  - Overfitting on small datasets (paper acknowledges this) → monitor train/val divergence closely
  - Training instability → batch normalization placement and gamma initialization matter

- **First 3 experiments:**
  1. **Baseline sanity check:** Replicate Table 1 results with RGB input on CoL-SLTD. If BLEU4 ≠ ~0.31, debug data pipeline or architecture implementation before proceeding.
  2. **Ablation on 2D positional encoding:** Remove 2D P.E. per Table 4. Expected: BLEU4 drops from ~0.51 to ~0.44. Confirms spatial encoding contribution.
  3. **RGB vs. Flow comparison:** Run identical config on both input types. Expected: Flow should outperform RGB by ~35% relative BLEU4 improvement. If not, check optical flow computation quality.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can 3D convolutional architectures replace the current 2D CNN and optical-flow pipeline to more effectively capture spatio-temporal features without relying on pre-computed motion inputs?
  - **Basis in paper:** [explicit] The conclusion states, "Input representation could be explored by implementing different convolutional architectures such as 3D convolutions, which can capture spatio-temporal features more effectively than 2D convolutions."
  - **Why unresolved:** The current architecture depends on separate optical-flow images to capture motion; 3D convolutions are proposed as a future improvement but have not been tested.
  - **What evidence would resolve it:** A comparative study benchmarking a 3D CNN backbone against the existing 2D+Flow approach on the CoL-SLTD and PHOENIX14T datasets using BLEU scores.

- **Open Question 2:** Does introducing multi-head behavior into the 2D pixel-wise self-attention mechanism improve the model's capacity to distinguish simultaneous manual and non-manual articulators?
  - **Basis in paper:** [explicit] The conclusion suggests, "The attention 2D module could be enhanced by introducing a multi-head-attention behaviour, which enables the model to attend to different parts of the input simultaneously."
  - **Why unresolved:** The current implementation uses a single attention head which may limit the model's ability to focus on multiple distinct spatial regions at once.
  - **What evidence would resolve it:** An ablation study showing that a multi-head 2D attention layer increases the BLEU4 score by better correlating spatially distant features (e.g., hand shapes and facial expressions).

- **Open Question 3:** Can depthwise convolutions be integrated into the feed-forward network to reduce the model's high computational cost and mitigate overfitting issues?
  - **Basis in paper:** [explicit] The authors note the model is "computationally expensive, leading to long training times, and there might be some overfitting issues," and propose "depthwise convolutions as they can efficiently reduce the number of parameters."
  - **Why unresolved:** It is currently unknown if the parameter reduction from depthwise convolutions will preserve the "important spatial information" required for robust translation.
  - **What evidence would resolve it:** Performance metrics (BLEU) and training time logs from a modified architecture using depthwise convolutions, demonstrating faster convergence without accuracy loss.

## Limitations

- **Dataset size constraints:** CoL-SLTD contains only 1,020 samples, creating high overfitting risk despite reported BLEU4 of 46.84%.
- **Optical flow methodology:** The exact algorithm and parameters for flow computation are unspecified, yet flow inputs show 35% relative improvement over RGB.
- **Computational complexity:** Pixel-wise attention at 7×7 resolution has O(49²) complexity per position, limiting scalability to higher resolutions.

## Confidence

- **High Confidence:** Mechanism 2 (2D pixel-wise self-attention for long-range dependencies) - directly supported by architectural description and EASLT corpus evidence about non-manual signals.
- **Medium Confidence:** Mechanism 1 (2D spatial structure preservation) - logical extension of Transformer positional encodings, but weak direct corpus support for 2D specifically.
- **Low Confidence:** Mechanism 3 (optical flow superiority) - 35% improvement is compelling, but optical flow method unspecified and no ablation against other motion representations.

## Next Checks

1. **Implement optical flow ablation:** Run identical architecture with RGB vs. optical flow inputs on PHOENIX14T. Verify ~35% BLEU4 improvement. If absent, investigate flow computation quality or parameter settings.
2. **2D positional encoding ablation:** Remove 2D P.E. per Table 4 methodology. Expect BLEU4 drop from ~0.51 to ~0.44. Confirms spatial encoding contribution and validates Mechanism 1.
3. **Attention visualization analysis:** Generate attention maps from the 2D self-attention module. Verify that spatially distant regions (hand-face pairs) show high attention weights. Failure indicates attention not learning cross-region dependencies.