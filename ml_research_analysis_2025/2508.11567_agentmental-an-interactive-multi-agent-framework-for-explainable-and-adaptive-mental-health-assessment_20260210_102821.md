---
ver: rpa2
title: 'AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive
  Mental Health Assessment'
arxiv_id: '2508.11567'
source_url: https://arxiv.org/abs/2508.11567
tags:
- topic
- mental
- evaluation
- information
- assessment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AgentMental, a multi-agent framework for automated
  mental health assessment. The framework simulates clinical doctor-patient dialogues
  using specialized agents for questioning, evaluation, scoring, and updating, incorporating
  adaptive questioning and a tree-structured memory to reduce redundancy and enhance
  contextual tracking.
---

# AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment

## Quick Facts
- arXiv ID: 2508.11567
- Source URL: https://arxiv.org/abs/2508.11567
- Reference count: 11
- AgentMental achieves Macro F1=89.8, outperforming baselines on DAIC-WOZ depression assessment task.

## Executive Summary
AgentMental introduces a multi-agent framework for automated mental health assessment that simulates clinical doctor-patient dialogues. The system employs four specialized agents (question generator, evaluation, scoring, and updating) working in coordination, supported by tree-structured memory to track context and reduce redundancy. The framework demonstrates superior performance compared to existing methods on the DAIC-WOZ dataset, with particularly strong results in depression classification and item-level scoring while maintaining interactive dialogue quality.

## Method Summary
AgentMental implements a four-agent system using the AutoGen framework, where each agent fulfills a distinct role in the assessment process. The framework begins with a question generator agent creating topic-specific initial questions, followed by an evaluation agent assessing response adequacy to determine if targeted follow-up questions are needed. A scoring agent then assigns item-level PHQ-8 scores based on clinical rating standards, while an updating agent maintains a tree-structured memory that tracks user information, topic summaries, and extracted dimensions across the assessment. The system uses Qwen2.5-72B-Instruct for agents and Deepseek-R1-Distill-Qwen-32B for simulating patient responses, with evaluations conducted on the DAIC-WOZ dataset using development set splits.

## Key Results
- Achieves MAE of 2.514, Kappa of 0.798, F1[C] of 0.939, F1[D] of 0.857, and Macro F1 of 0.898 on DAIC-WOZ dataset
- Ablation study confirms both adaptive questioning and memory modules are critical, with memory ablation increasing MAE to 3.314 and reducing Kappa to 60.4
- GPT/human evaluations indicate strong performance in empathy, coherence, and user satisfaction
- Demonstrates ability to elicit deeper user information through multi-turn interactions compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive follow-up questioning improves information completeness when user responses are vague or incomplete.
- Mechanism: An evaluation agent (AGev) assigns a necessity score (0–2) to each response. If the score exceeds threshold θ, the question generator agent (AGq) produces targeted follow-up queries probing symptom frequency, duration, severity, and impact. This continues until either the necessity score drops below θ or the maximum follow-up depth d is reached.
- Core assumption: LLM-based adequacy assessment correlates with clinically relevant information gaps, and follow-up questions elicit diagnostically valuable details.
- Evidence anchors:
  - [abstract] "an evaluation agent assesses the adequacy of user responses to determine the necessity of generating targeted follow-up queries to address ambiguity and missing information"
  - [PAGE 3] Formula (2) defines the conditional branching logic for question generation based on AGev output
  - [PAGE 6] Ablation study shows removing in-depth questioning increases MAE from 2.514 to 3.000 and reduces Kappa from 79.8 to 58.7
- Break condition: If AGev consistently misjudges adequacy (e.g., high false-positive rate on vague responses), follow-up questioning may introduce noise or user fatigue without diagnostic gain.

### Mechanism 2
- Claim: Tree-structured memory reduces redundant questioning and maintains contextual coherence across assessment topics.
- Mechanism: Memory comprises three node types: (1) user node with basic attributes, (2) topic nodes vt_i storing scores and summaries per scale item, (3) statement nodes vs encoding extracted dimensions (emotion, frequency, duration, symptoms, impact). Upon completing topic Ti, the system instantiates a topic node, updates prior topic summaries with relevant new information, and links all associated statement nodes.
- Core assumption: Structured hierarchical storage enables more efficient cross-topic inference than flat conversation history.
- Evidence anchors:
  - [abstract] "tree-structured memory in which the root node encodes the user's basic information, while child nodes...organize key information according to distinct symptom categories and interaction turns"
  - [PAGE 3] Algorithm 1 lines 9–18 describe memory node creation and update operations
  - [PAGE 6] Ablation shows removing memory increases MAE from 2.514 to 3.314 and reduces Kappa from 79.8 to 60.4
- Break condition: If inter-topic dependencies are weak for a particular scale or population, memory overhead may not justify marginal gains in coherence.

### Mechanism 3
- Claim: Specialized agent roles with coordinated handoffs improve scoring accuracy compared to single-model classification.
- Mechanism: Four agents operate sequentially per topic: AGq generates questions → user responds → AGev evaluates adequacy → (optional follow-up loop) → AGs assigns score and summary → AGu performs global memory update and final reasoning. This division separates information elicitation from evaluation from integration.
- Core assumption: Role specialization reduces task interference and allows each agent to optimize for its specific objective (e.g., empathy in questioning vs. precision in scoring).
- Evidence anchors:
  - [PAGE 2] "AgentMental incorporates four core agents, each fulfilling distinct roles within the system"
  - [PAGE 5] Table 1 shows AgentMental (Qwen2.5-72B) achieves Macro F1=89.8 vs. MDAgents at 86.7 and Debate at 84.4
  - [corpus] Related paper "MAGI" similarly uses multi-agent guided interviews, suggesting convergence on this design pattern, though direct comparison data is limited
- Break condition: If agent coordination overhead exceeds latency budgets for real-time interaction, or if role boundaries cause contradictory behavior (e.g., empathetic questioning followed by harsh scoring), the system may feel disjointed to users.

## Foundational Learning

- Concept: **PHQ-8 Depression Scale**
  - Why needed here: AgentMental's scoring agent outputs item-level PHQ-8 scores (0–3 per symptom) that aggregate to a total score mapped to depression classification. Understanding scale structure is essential for debugging scoring logic.
  - Quick check question: Can you name at least four of the eight PHQ-8 symptom domains and explain how item scores translate to depression severity categories?

- Concept: **Multi-Agent Orchestration Patterns**
  - Why needed here: The framework relies on sequential agent handoffs with conditional branching. Engineers must understand when each agent activates and how state propagates.
  - Quick check question: Draw the control flow between AGq, AGev, AGs, and AGu, marking where loops occur and where state is written to memory.

- Concept: **LLM Prompting for Role Specialization**
  - Why needed here: Each agent is an LLM prompted for a specific role. Effective system behavior depends on well-crafted prompts that enforce role boundaries.
  - Quick check question: What prompt constraints would you add to prevent the scoring agent from generating follow-up questions or the question generator from assigning scores?

## Architecture Onboarding

- Component map:
  - AGq (Question Generator) → AGev (Evaluation) → optional follow-up loop → AGs (Scoring) → AGu (Updating)
  - Tree-structured memory with user node, topic nodes (vt), and statement nodes (vs)

- Critical path:
  1. Initialize memory with user ID → 2. For each topic Ti: AGq generates Q^1_i → 3. User responds A^1_i → 4. LLM extracts dimensions, creates vs node → 5. AGev evaluates necessity → 6. If necessity ≥ θ and j < d, loop to step 2 with follow-up → 7. AGs assigns (Si, Bi), creates vt node → 8. Memory updates prior topic summaries → 9. After all topics, AGu performs global update → 10. Aggregate scores to classification

- Design tradeoffs:
  - Follow-up depth d=3 vs. user fatigue: More follow-ups may improve accuracy but risk disengagement
  - Memory granularity: Statement nodes preserve turn-level detail but increase token usage; coarser aggregation reduces overhead but may lose diagnostic nuance
  - Threshold θ=1: Lower threshold triggers more follow-ups (higher recall, potential annoyance); higher threshold may miss information gaps

- Failure signatures:
  - Stuck in follow-up loop: AGev consistently outputs high necessity scores; check prompt calibration or add hard turn limits
  - Redundant questioning across topics: Memory not properly updating prior summaries; verify edge creation and summary propagation logic
  - Inconsistent scoring: AGs assigns scores misaligned with stated evidence; review scale rating standard prompts and few-shot examples

- First 3 experiments:
  1. Ablation by component: Run AgentMental with in-depth questioning disabled, memory disabled, and both disabled on a held-out validation set. Compare MAE, Kappa, and F1 to quantify individual contributions (replicating Table 3).
  2. Threshold sensitivity analysis: Vary θ ∈ {0.5, 1.0, 1.5, 2.0} and d ∈ {1, 2, 3, 4} on development set. Plot accuracy vs. average dialogue turns to identify optimal efficiency frontier.
  3. Per-topic error analysis: For each PHQ-8 item, compute F1 scores and examine cases where AgentMental deviates from ground truth by ≥2 points. Categorize error types (e.g., vague user input, AGev misjudgment, scoring drift) to prioritize component improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance and interaction quality of AgentMental compare when engaging with real human patients versus the LLM-simulated agents used in the experimental validation?
- Basis in paper: [inferred] The Methodology section states that the experiments "leverage transcribed text... as prior knowledge and prompt LLMs to simulate individuals," meaning the "users" were models (Deepseek-R1) role-playing rather than actual humans.
- Why unresolved: The study relies on synthetic interactions where the user agent is coherent and cooperative; real patients may exhibit resistance, incoherence, or non-compliance that the simulation does not fully capture.
- What evidence would resolve it: A user study involving real human participants interacting with the AgentMental framework to evaluate dialogue naturalness, user satisfaction, and the accuracy of the final assessment compared to the simulation results.

### Open Question 2
- Question: How can the framework be enhanced to better capture highly subjective or context-dependent symptoms, such as "psychomotor changes," which currently yield significantly lower performance?
- Basis in paper: [explicit] In the analysis of Table 2, the authors note that items characterized by high subjectivity, such as "psychomotor changes," "remain challenging to assess reliably" due to reliance on subtle linguistic cues and cognitive dissonance.
- Why unresolved: The current text-based questioning and evaluation agents appear insufficient to detect subtle behavioral or physiological indicators without explicit verbal description.
- What evidence would resolve it: Modifications to the agent prompts or the integration of specific reasoning modules for these symptoms, demonstrated by a statistically significant increase in the F1 score for the "Psychomotor" category in a revised evaluation.

### Open Question 3
- Question: Does the integration of audio-visual features (e.g., vocal prosody, facial expressions) into the tree-structured memory significantly improve assessment accuracy over text-only analysis?
- Basis in paper: [explicit] The Introduction states that the current work "focus exclusively on the text modality derived from the interview transcripts," despite the DAIC-WOZ dataset containing audio and video recordings.
- Why unresolved: Depression manifests through non-verbal cues (e.g., flat affect, pauses) which are currently ignored by the text-centric agent architecture.
- What evidence would resolve it: A multimodal implementation of the AgentMental framework evaluated on DAIC-WOZ, showing improved Kappa and Macro F1 scores compared to the text-only baseline.

### Open Question 4
- Question: What is the computational and latency overhead of the multi-agent interactive loop compared to single-pass classification methods, and does it limit real-time clinical applicability?
- Basis in paper: [inferred] The framework requires sequential execution of four agents (Question, Evaluation, Scoring, Updating) and multiple turns of interaction ($j < d$), which is inherently more complex than the static text analysis methods used as baselines.
- Why unresolved: While accuracy is improved, the paper does not report inference time or API costs, which are critical factors for real-world deployment in resource-scarce clinical settings.
- What evidence would resolve it: A comparative analysis of average inference time and computational cost (FLOPs or token count) per assessment between AgentMental and baseline methods like Zero-Shot or CoT.

## Limitations

- The framework relies on LLM-simulated patients rather than real human subjects, raising questions about ecological validity and performance with actual clinical populations
- Specific agent prompt templates are not provided, making full reproducibility difficult and preventing isolation of architectural vs. prompt engineering contributions
- Performance on diverse populations beyond the DAIC-WOZ dataset remains unknown, including cross-cultural applicability and language variations

## Confidence

**High Confidence**: The core multi-agent architecture and its sequential workflow are clearly described and internally consistent. The tree-structured memory design and its implementation via hierarchical nodes (user, topic, statement) are well-specified. The ablation study methodology and its conclusions about the importance of adaptive questioning and memory modules are robust.

**Medium Confidence**: The quantitative results (MAE, Kappa, F1 scores) are reported with precision and show clear improvements over baselines. However, the use of simulated patients rather than real human subjects introduces uncertainty about how these metrics translate to real-world clinical settings. The GPT/human evaluations of empathy, coherence, and satisfaction are promising but lack detailed methodology and inter-rater reliability measures.

**Low Confidence**: The specific prompt templates that govern each agent's behavior are not provided, making it difficult to assess whether the claimed improvements stem from architectural design or prompt engineering. The clinical validity of the system's assessments remains uncertain without comparison to gold-standard clinical diagnoses from actual mental health professionals.

## Next Checks

1. **Prompt Transparency Audit**: Request and publish the complete prompt templates for all four agents, including few-shot examples and evaluation criteria. Conduct controlled experiments varying prompts while holding architecture constant to isolate prompt effects from architectural contributions.

2. **Human Validation Study**: Conduct a small-scale study where actual human participants complete mental health assessments with both AgentMental and human clinicians. Compare item-level PHQ-8 scores and clinical judgments, measuring inter-rater reliability between AgentMental and clinicians, and between different clinicians.

3. **Cross-Dataset Generalization Test**: Evaluate AgentMental on an independent mental health dataset (e.g., DAIC for PTSD, or a culturally diverse dataset) without retraining. Measure performance degradation and identify which components (question generation, evaluation, scoring, memory) are most sensitive to population differences.