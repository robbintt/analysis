---
ver: rpa2
title: 'BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF'
arxiv_id: '2506.03234'
source_url: https://arxiv.org/abs/2506.03234
tags:
- reward
- attack
- data
- poisoning
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadReward, a clean-label poisoning attack
  that targets reward models in multi-modal reinforcement learning from human feedback
  (RLHF) pipelines for text-to-image (T2I) models. The attack exploits feature collisions
  in CLIP embedding space to corrupt reward signals without altering preference labels,
  enabling stealthy manipulation of T2I model outputs toward harmful concepts (e.g.,
  violent or biased imagery) for targeted prompts.
---

# BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF

## Quick Facts
- **arXiv ID**: 2506.03234
- **Source URL**: https://arxiv.org/abs/2506.03234
- **Reference count**: 40
- **Primary result**: Clean-label poisoning attack achieving ASR up to 1.00 against RLHF reward models while maintaining SSIM >0.86

## Executive Summary
BadReward introduces a clean-label poisoning attack that targets reward models in multi-modal reinforcement learning from human feedback (RLHF) pipelines for text-to-image (T2I) models. The attack exploits feature collisions in CLIP embedding space to corrupt reward signals without altering preference labels, enabling stealthy manipulation of T2I model outputs toward harmful concepts (e.g., violent or biased imagery) for targeted prompts. Extensive experiments on Stable Diffusion v1.4 and SD Turbo demonstrate that BadReward achieves attack success rates up to 1.00 while maintaining high visual similarity to benign images (SSIM >0.86, PSNR >24 dB). The attack shows strong transferability across different T2I architectures and adversarial models, with ASR remaining 3.8–10.6× higher than clean models even with paraphrased prompts.

## Method Summary
BadReward operates through a multi-stage process: first, it generates preference pairs where the "winner" image contains a target concept (e.g., eyeglasses) and the "loser" image does not, using adversary T2I models like SD v3.5 or CogView4. The attack then applies feature collision optimization to project a benign base image toward the target concept in CLIP feature space while constraining pixel-space deviation, creating poisoned images that appear visually benign but encode the target concept in representation space. These poisoned preference pairs are injected at 1-3% ratios into the training dataset, where the reward model learns to associate trigger prompts with the target concept. During RLHF fine-tuning, the corrupted reward signals amplify this association, leading to malicious outputs when triggers appear.

## Key Results
- Attack Success Rate (ASR) reaches 1.00 across configurations while maintaining SSIM >0.86 and PSNR >24 dB
- Transferability from CogView4 outperforms SDXL by 2.7-4.8× in cross-architecture attacks
- Semantic generalization holds under paraphrased prompts with ASR 3.8-10.6× higher than clean models
- Reward Overlap (RO) scores drop significantly (from 0.96-0.97 to 0.60-0.66) indicating corrupted reward signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Feature collision in CLIP embedding space enables visual stealth while preserving adversarial effectiveness.
- **Mechanism**: The attack optimizes a poisoned image x to minimize distance to target concept features in CLIP space while constraining pixel-space deviation from a benign base image. This decouples perceptual appearance from learned representations, allowing poisoned samples to evade human detection while corrupting reward model training.
- **Core assumption**: CLIP embeddings are sufficiently dissociated from pixel-level appearance that gradient-based optimization can shift feature representations without proportionate visual changes.
- **Evidence anchors**:
  - [abstract] "BadReward operates by inducing feature collisions between visually contradicted preference data instances"
  - [section 4.2.1] Optimization objective: min_x ||gCLIP(x) - gCLIP(xt)||² + β||x - xb||²
  - [corpus] Corpus shows related work on preference poisoning but limited direct coverage of feature collision for multi-modal reward models
- **Break condition**: If defenders deploy CLIP-space anomaly detection or use alternative encoders with tighter pixel-feature coupling, the stealth-effectiveness tradeoff degrades significantly.

### Mechanism 2
- **Claim**: Poisoned preference data corrupts reward model gradients without requiring annotation manipulation.
- **Mechanism**: By constructing preference pairs (p, x_collide, x_l) where x_collide appears benign but encodes target concept C in feature space, the Bradley-Terry likelihood optimization induces the reward model to assign higher scores to outputs containing C when trigger t is triggered. This bypasses the need for dirty-label attacks on annotation pipelines.
- **Core assumption**: Reward models trained on preference data cannot distinguish between genuine semantic alignment and feature-space manipulation.
- **Evidence anchors**:
  - [abstract] "corrupting the reward signal without altering the preference labels"
  - [section 4.2.2] "reward model r_φ is trained to assign significantly higher scores to x_collide than to x_l when the cue t is triggered"
  - [corpus] Related work (Policy Teaching via Data Poisoning) addresses preference poisoning in single-modality; BadReward extends to cross-modal setting
- **Break condition**: If reward models incorporate robust feature sanitization or multi-encoder validation, gradient corruption from poisoned pairs diminishes.

### Mechanism 3
- **Claim**: Corrupted reward signals amplify target concept emergence during RLHF policy optimization.
- **Mechanism**: The advantage function A_φ(s_t) = r_φ(p, x) - b(s_t) receives inflated rewards for generations containing C when trigger t appears. Policy gradient updates then reinforce this behavior, creating a positive feedback loop that progressively increases the probability of generating malicious outputs for targeted prompts.
- **Core assumption**: RLHF optimization does not include reward signal validation or out-of-distribution detection during training.
- **Evidence anchors**:
  - [section 4.1] "dominance function A_φ(s_t) amplifies the rewards of generations containing the target concept C"
  - [table 1] ASR increases from baseline 0.07-0.17 to 0.80-1.00 across attack configurations
  - [corpus] Corpus lacks direct coverage of reward signal amplification dynamics in multi-modal RLHF
- **Break condition**: If KL regularization is strengthened or dynamic reward monitoring is deployed, the positive feedback loop is interrupted.

## Foundational Learning

- **Concept: Bradley-Terry Preference Modeling**
  - **Why needed here**: Understanding how pairwise preference data translates to reward scores is essential for grasping why corrupted preference pairs propagate through RLHF.
  - **Quick check question**: Given a preference pair (x_w, x_l), what does the Bradley-Terry model compute, and how would a poisoned x_w affect the learned reward?

- **Concept: CLIP Joint Embedding Space**
  - **Why needed here**: Feature collision exploits the shared text-image embedding space; comprehending how CLIP maps visual and textual inputs is prerequisite to understanding the attack vector.
  - **Quick check question**: If two images have similar CLIP embeddings but different pixel content, what does this imply about the encoder's inductive biases?

- **Concept: Diffusion Policy Optimization (DDPO/SDPO)**
  - **Why needed here**: RLHF for diffusion models uses trajectory-based policy gradients; knowing how rewards influence denoising trajectories clarifies attack propagation.
  - **Quick check question**: In DDPO, how does the advantage function modulate the policy update, and what happens if the reward model is systematically biased?

## Architecture Onboarding

- **Component map**: Trigger-concept selector -> Adversary T2I generator (SDv3.5/SDXL/CogView4) -> Feature collision optimizer -> Victim reward model trainer -> Target T2I RLHF pipeline (SDv1.4/SD Turbo with DDPO/SDPO)

- **Critical path**:
  1. Generate diverse prompts containing trigger t (excluding target concept C explicitly)
  2. Create x_w with concept C and x_l without; apply feature collision to x_w using benign base
  3. Inject poisoned pairs at 1-3% of preference dataset
  4. Victim trains reward model on mixed data (clean + poisoned)
  5. RLHF optimization amplifies trigger→concept association

- **Design tradeoffs**:
  - Higher poisoning ratio → higher ASR but increased detection risk
  - Stronger feature collision (lower β) → higher effectiveness but reduced visual stealth (SSIM drops)
  - Adversary model selection → CogView4 shows best transferability; SDXL shows architecture-specific limitations

- **Failure signatures**:
  - Low RO (<0.7) indicates poisoned images flagged as outliers in reward distribution
  - ASR remains near baseline (<0.2) despite poisoning suggests feature collision optimization failed or reward model robust
  - Reward hacking artifacts (unintended styles, excessive attributes) indicate gradient explosion in RLHF

- **First 3 experiments**:
  1. **Reproduce feature collision stealth**: Generate poisoned images for (t=old, C=eyeglasses) using SDv3.5; measure SSIM/PSNR/LPIPS against clean baseline. Verify metrics match reported ranges (SSIM >0.86, PSNR >24 dB).
  2. **Validate reward corruption**: Train reward model on 3% poisoned Recraft-V2 data; compute RO scores comparing poisoned vs. clean reward model outputs on held-out poisoned pairs.
  3. **Test cross-architecture transfer**: Use CogView4-generated poisoned data to attack SDv1.4 with DDPO; compare ASR on original prompts vs. GPT-regenerated prompts to quantify semantic generalization gap.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can feature-space anomaly detection techniques effectively identify BadReward poisoned samples without filtering out benign data outliers?
  - **Basis in paper**: [explicit] The conclusion explicitly states, "In future work, we will investigate feature-space anomaly detection techniques against reward poisoning attack."
  - **Why unresolved**: The paper demonstrates the effectiveness of the attack but leaves the development and rigorous testing of specific detection algorithms as a future task.
  - **What evidence would resolve it**: Empirical results showing high precision and recall in distinguishing feature-collided images from clean images in the training set.

- **Open Question 2**: Are multi-modal consensus validation or dynamic reward monitoring robust against the visual stealth of feature collisions?
  - **Basis in paper**: [inferred] Section 5.7 proposes three countermeasures (Adversarial Feature Sanitization, Dynamic Reward Monitoring, Multi-modal Consensus Validation) but does not evaluate them against BadReward's high visual similarity (SSIM >0.86).
  - **Why unresolved**: It is currently unknown if standard validation checks can catch perturbations designed to be invisible in pixel space but distinct in CLIP feature space.
  - **What evidence would resolve it**: Attack success rates (ASR) and false positive rates when the proposed defenses are integrated into the RLHF pipeline.

- **Open Question 3**: What specific architectural properties enable Transformer-based attackers (e.g., CogView4) to achieve higher attack transferability than diffusion-based attackers?
  - **Basis in paper**: [inferred] Appendix C.2 notes that CogView4 exhibits superior transferability compared to SDXL, suggesting "inductive biases" or "contextual strength" as factors, but lacks a mechanistic explanation.
  - **Why unresolved**: The paper observes the correlation between architecture type and success but does not isolate the specific model components responsible for this advantage.
  - **What evidence would resolve it**: Ablation studies varying architectural components (e.g., attention mechanisms vs. convolutions) to measure their direct impact on cross-architecture attack success.

## Limitations

- The attack's effectiveness depends on the assumed dissociation between pixel-space appearance and feature-space representations, which may not hold uniformly across different CLIP variants
- The 1-3% poisoning ratio represents an optimal scenario; real-world deployment might face stricter data curation that could reduce effectiveness
- Evaluation focuses primarily on Stable Diffusion architectures, leaving uncertainty about performance on emerging T2I models with different reward modeling approaches

## Confidence

- **High Confidence**: The mechanism of feature collision enabling stealth poisoning (ASR up to 1.00 while maintaining SSIM >0.86) is well-supported by quantitative results across multiple architectures
- **Medium Confidence**: The transferability claims across different adversary models are supported but limited to specific model pairs (CogView4 best, SDXL limited)
- **Medium Confidence**: The semantic generalization under paraphrased prompts shows robust attack success (3.8-10.6× higher ASR than clean models) but with performance degradation that wasn't fully characterized across all prompt variations

## Next Checks

1. **Encoder Robustness Test**: Evaluate BadReward against alternative multimodal encoders (CLIP-ViT-L/14 vs. OpenCLIP vs. custom encoders) to quantify sensitivity to feature space choice
2. **Anomaly Detection Resistance**: Implement CLIP-space outlier detection and test whether BadReward maintains effectiveness when defenders monitor feature distribution shifts
3. **Real-world Deployment Simulation**: Scale poisoning ratio to 0.5-5% and evaluate detection rates using human evaluation panels combined with automated quality metrics to assess practical stealth limitations