---
ver: rpa2
title: 'TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency
  Text-to-SQL'
arxiv_id: '2601.08743'
source_url: https://arxiv.org/abs/2601.08743
tags:
- cache
- table
- query
- tablecache
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TableCache, a method designed to accelerate
  LLM inference for Text-to-SQL tasks by precomputing KV caches for database tables
  offline. The approach addresses the inefficiency of current inference engines, which
  generate redundant cache copies when processing user queries with varying table
  orders.
---

# TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL

## Quick Facts
- **arXiv ID**: 2601.08743
- **Source URL**: https://arxiv.org/abs/2601.08743
- **Reference count**: 17
- **Key outcome**: Achieves up to 3.62× speedup in Time to First Token (TTFT) with negligible accuracy degradation for Text-to-SQL inference.

## Executive Summary
TableCache addresses the inefficiency of current LLM inference engines for Text-to-SQL tasks, which generate redundant KV cache copies when processing user queries with varying table orders. The method precomputes table representations as KV caches offline while preserving primary foreign key relationships, constructs a Table Trie for efficient lookups, and employs a cache management system with query reranking and a computation loading pipeline. Experiments demonstrate that TableCache achieves significant TTFT improvements while maintaining the original model's accuracy, making it practical for production Text-to-SQL systems.

## Method Summary
TableCache introduces a three-phase approach: offline precomputation of table KV caches guided by primary foreign key relationships, online inference with efficient cache lookup using a Table Trie structure, and cache management with query reranking and pipelined computation loading. The method preserves critical inter-table dependencies during block-wise encoding by constructing a PFK graph and applying topological sorting. During inference, queries are reordered to maximize cache hits, and CPU-GPU asynchronous transfers hide memory latency while GPU computation proceeds.

## Key Results
- Achieves up to 3.62× speedup in Time to First Token (TTFT) for Text-to-SQL inference
- Maintains accuracy within 1% of baseline models across BIRD and Spider benchmarks
- Shows 1.62× TTFT improvement on BIRD benchmark and 1.85× on Spider benchmark
- Performance gains increase with batch size and number of tables

## Why This Works (Mechanism)

### Mechanism 1: Primary Foreign Key Guided Table Representation
Precomputing table KV caches offline while preserving primary-foreign key (PFK) relationships maintains model accuracy while enabling cross-query cache reuse. The method constructs a PFK graph from database schema, applies topological sorting, encodes connected tables jointly using causal attention, stores KV caches without positional encoding, and dynamically reapplies positional encodings during inference based on global sequence position.

### Mechanism 2: Table Trie for Efficient Cache Lookup
A trie-based matching structure enables O(n) identification of tables in prompts and retrieval of corresponding precomputed KV caches. The method inserts table metadata (name, columns, descriptions) into trie, stores table ID and cache path at leaf nodes, performs longest-prefix matching during inference, and retrieves and concatenates KV caches for the identified tables.

### Mechanism 3: Query Reranking and Computation Loading Pipeline
Reordering queries by table-set similarity and pipelining cache loads with GPU computation reduces TTFT by maximizing cache residency and hiding memory latency. The method represents queries as binary vectors over table IDs, greedily selects queries minimizing Hamming distance to previous query, divides batch into compute and memory micro-batches, and prefetches next memory caches while computing current compute requests.

## Foundational Learning

- **Concept: KV Cache and Prefill Phase in Transformers**
  - Why needed here: The prefill phase (processing input tokens before generation) dominates TTFT for long schema prompts; TableCache targets this bottleneck by precomputing and reusing caches.
  - Quick check question: Why does the prefill phase scale quadratically O(n²) with input length while decode scales linearly?

- **Concept: Prefix/Radix Caching in Inference Engines**
  - Why needed here: TableCache is motivated by vLLM/SGLang PrefixCache limitations—exact prefix matching fails when table order varies across queries.
  - Quick check question: If two queries access the same tables but in different orders, why does PrefixCache produce separate cache paths?

- **Concept: Attention Masking and Block-wise Attention**
  - Why needed here: Precomputing table KV caches independently creates isolated attention blocks, severing inter-table dependencies; PFK guidance partially restores them.
  - Quick check question: When encoding tables independently, which tokens can attend to which others within the modified attention mask?

## Architecture Onboarding

- **Component map**: Database schema → PFK graph → Topological sort → Table encoder → Table Trie + KV cache files (CPU memory) → User query batch → Table Trie matcher → Query reranker → Cache manager (CPU↔GPU) → Computation loading pipeline → Concatenated KV cache → LLM inference

- **Critical path**:
  1. PFK graph correctness (determines which tables encode jointly)
  2. Table Trie matching accuracy (correct cache retrieval)
  3. Query reranking effectiveness (cache hit maximization)
  4. Pipeline synchronization (prefetch completes before compute needs it)

- **Design tradeoffs**:
  - Memory vs. speed: CPU-resident caches increase memory footprint but enable async prefetching
  - Accuracy vs. efficiency: Block-wise encoding loses attention; PFK restores critical dependencies at preprocessing cost
  - Fine-tuning requirement: Models benefit from adaptive training (Table 5 shows training-free setup has minor degradation for non-specialized models)
  - Batch granularity: Smaller bc/bm increases pipeline overhead; larger reduces flexibility

- **Failure signatures**:
  - Cache miss rate high → Reranking ineffective or eviction policy misconfigured
  - Accuracy drop >1% → PFK graph incomplete OR model not adapted to block-wise attention
  - TTFT unchanged → Pipeline not overlapping (check async transfer) OR cache loading still serialized
  - Table not matched → Prompt format diverges from Table Trie construction format

- **First 3 experiments**:
  1. Validate PFK preservation: Compare accuracy w/ vs. w/o PFK on BIRD (expect ~7 point drop without, per Table 3).
  2. Ablate reranking: Run with random vs. greedy ordering on Spider, measure TTFT (expect ~1.6× slowdown without, per Table 2).
  3. Profile pipeline overlap: Instrument GPU compute time vs. PCIe transfer time; verify prefetch hides >50% of memory latency.

## Open Questions the Paper Calls Out
The paper explicitly states in the Limitations section that extending TableCache to unstructured domains like textual QA or KBQA where dependencies are implicit rather than explicitly defined by primary-foreign keys is left for future research, suggesting integrating with graph retrieval methods as a potential path.

## Limitations
- PFK-guided representation may not capture all inter-table dependencies critical for complex Text-to-SQL reasoning, particularly in schemas with complex join patterns
- Table Trie matching assumes consistent prompt formatting and table naming conventions, potentially failing with highly variable prompt structures
- Method requires significant CPU memory for storing precomputed KV caches, which could become prohibitive for databases with many large tables

## Confidence
- **High Confidence**: TTFT speedup measurements and accuracy retention on benchmark datasets (Spider and BIRD). The experimental methodology is sound, and results are consistently reported across multiple configurations.
- **Medium Confidence**: The PFK-guided attention preservation mechanism. While the theoretical framework is reasonable, the assumption that foreign key relationships capture all critical inter-table dependencies lacks extensive validation across diverse schema types.
- **Medium Confidence**: The Table Trie matching efficiency and cache hit optimization through query reranking. These components are well-justified theoretically but lack comprehensive ablation studies showing their individual contributions.

## Next Checks
1. **Ablation Study on PFK Completeness**: Systematically remove primary-foreign key relationships from schemas and measure accuracy degradation across different schema types to validate the assumption that PFK relationships capture all critical inter-table dependencies.

2. **Cache Management Overhead Analysis**: Measure the CPU memory footprint required for different database sizes and table counts, and quantify the memory-latency tradeoff across various cache eviction policies (LRU vs FIFO vs LFU) under realistic workload conditions.

3. **Prompt Format Robustness Testing**: Evaluate TableCache performance across diverse prompt formats and table naming conventions to identify the limits of Table Trie matching and determine the minimum prompt consistency required for reliable operation.