---
ver: rpa2
title: 'Beyond Softmax: A Natural Parameterization for Categorical Random Variables'
arxiv_id: '2509.24728'
source_url: https://arxiv.org/abs/2509.24728
tags:
- learning
- latent
- categorical
- function
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work improves training of models with latent categorical variables
  by replacing the standard softmax function with a hierarchical parameterization
  called catnat. The proposed parameterization induces a diagonal Fisher Information
  Matrix, leading to flatter optimization landscapes that improve gradient-based learning.
---

# Beyond Softmax: A Natural Parameterization for Categorical Random Variables

## Quick Facts
- **arXiv ID:** 2509.24728
- **Source URL:** https://arxiv.org/abs/2509.24728
- **Reference count:** 40
- **One-line primary result:** Catnat achieves lower losses, better calibration, and higher returns than softmax across Graph Structure Learning, VAEs, and Reinforcement Learning.

## Executive Summary
This work introduces catnat, a hierarchical parameterization for categorical random variables that replaces the standard softmax function. The key innovation is structuring the probability distribution as a product of binary decisions along a tree, which induces a diagonal Fisher Information Matrix. This diagonal structure improves gradient-based optimization by reducing parameter coupling. The authors demonstrate consistent performance gains across three diverse domains: Graph Structure Learning (better parameter recovery), Variational Autoencoders (lower negative log-likelihood), and Reinforcement Learning (higher episodic returns).

## Method Summary
The method replaces softmax with a hierarchical binary tree parameterization where each internal node probability is computed using a natural activation function $\nu(x)$. For $K$ categories, the tree has depth $H=\lceil\log_2(K)\rceil$, requiring only $K-1$ parameters instead of $K$. Leaf probabilities are computed as products of conditional probabilities along the path from root to leaf. The natural activation function is designed to simplify the Fisher Information Matrix diagonal entries, leaving only constant terms dependent on tree structure rather than local score values. This parameterization maintains compatibility with existing gradient estimators like Gumbel-Softmax for VAEs or REINFORCE for RL.

## Key Results
- Catnat + natural activation consistently outperforms softmax across all three experimental domains
- In VAEs on MNIST, catnat achieves lower negative log-likelihood than softmax in all configurations
- Graph Structure Learning experiments show nearly 3x reduction in parameter recovery error at high entropy settings
- Reinforcement Learning experiments demonstrate higher final returns on Atari games

## Why This Works (Mechanism)

### Mechanism 1: Diagonal Fisher Information Matrix via Hierarchical Binary Splits
The hierarchical tree structure diagonalizes the Fisher Information Matrix by reducing coupling between parameters during gradient descent. While softmax produces a dense FIM with off-diagonal entries that couple all scores, catnat's tree structure yields a diagonal FIM where parameter updates in one dimension don't interfere with others.

### Mechanism 2: Natural Activation Function Simplifies Gradient Magnitude
The proposed $\nu(x)$ activation function removes local score dependence from FIM diagonal entries, leaving only ancestor-dependent probability terms. This stabilizes gradient magnitudes across the tree by making the FIM diagonal entries constant rather than score-dependent.

### Mechanism 3: Improved Calibration and Parameter Recovery
The diagonal FIM reduces geometric distortion, allowing gradient descent to follow more direct paths. This manifests as better recovery of true latent distribution parameters, particularly at high entropy settings where softmax's dense FIM creates more interference.

## Foundational Learning

- **Concept: Fisher Information Matrix**
  - Why needed here: The paper's central theoretical contribution analyzes and modifies FIM properties. Understanding FIM as a curvature measure of the statistical manifold is essential for grasping the diagonalization motivation.
  - Quick check question: If the FIM is diagonal, what does that imply about the relationship between parameter updates in different dimensions?

- **Concept: Natural Gradient Descent**
  - Why needed here: The paper frames catnat as "natural parameterization" designed to align with natural gradient principles. The diagonal FIM approximates the pre-conditioning that natural gradient methods compute explicitly.
  - Quick check question: Why is computing the natural gradient typically expensive, and how does catnat approximate its benefits?

- **Concept: Hierarchical Softmax / Binary Tree Parameterization**
  - Why needed here: Catnat is structurally similar to hierarchical softmax used in language models. Understanding binary tree traversals and conditional probability factorization helps parse the construction.
  - Quick check question: For $K=8$ categories, how many internal nodes does catnat require, and what is the tree depth?

## Architecture Onboarding

- **Component map:**
  Input scores $\vec{s} \in \mathbb{R}^{K-1}$ -> Natural activation $\nu(s)$ -> Binary tree with $H=\lceil\log_2(K)\rceil$ levels -> Leaf probabilities $p_k$ computed as path products

- **Critical path:**
  1. Replace `softmax(logits)` with `catnat(scores)` in your categorical distribution parameterization
  2. Ensure scores dimension is $K-1$ (not $K$)
  3. Tune natural activation hyperparameters: $C$ (center, default 0) and $A$ (width, default $2\pi$)
  4. Initialize scores so initial probabilities are reasonable

- **Design tradeoffs:**
  - Catnat + $\nu$ vs. Softmax: Better optimization geometry vs. simpler implementation
  - $\nu$ vs. $\sigma$ activation: $\nu$ gives theoretical FIM benefits but has saturation regions
  - Computational cost: Catnat requires $O(K)$ operations for probability computation (same asymptotic as softmax)

- **Failure signatures:**
  1. Gradients vanishing at specific branches: Check if scores are saturating $\nu$ (outputting 0 or 1)
  2. No improvement over softmax: Verify $K > 2$ and retune learning rates
  3. Numerical instability at tree leaves: Very deep trees can have small probability products

- **First 3 experiments:**
  1. Sanity check on binary classification ($K=2$): Catnat with $\nu$ should match sigmoid performance
  2. Ablation on activation functions: Compare catnat + $\nu$ vs. catnat + $\sigma$ vs. softmax on $K=16$ VAE
  3. Learning rate sweep: Catnat changes the optimization landscapeâ€”re-tune learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical parameterization used in catnat be extended to continuous or non-categorical discrete distributions to induce a diagonal Fisher Information Matrix?
- Basis in paper: The conclusion states, "Extending this approach to other families of continuous and discrete distributions is a promising avenue for future research."
- Why unresolved: The theoretical proofs and empirical validation are strictly limited to categorical random variables.
- What evidence would resolve it: Deriving hierarchical parameterizations for distributions such as Gaussian or Poisson variants, proving the resulting Fisher Information Matrix is diagonal, and demonstrating empirical training improvements.

### Open Question 2
- Question: How does the catnat parameterization scale with the number of categories $K$ in settings such as language modeling?
- Basis in paper: The introduction mentions "selecting a word token in a language model" as a use case, but experiments only test up to $K=32$.
- Why unresolved: It is unclear if the depth of the hierarchical tree introduces optimization difficulties or latency that outweighs the geometric benefits when $K$ is very large.
- What evidence would resolve it: Benchmarking catnat against standard Softmax on a language modeling task with a large vocabulary to compare perplexity and convergence speed.

### Open Question 3
- Question: Do the performance benefits of catnat over softmax increase with the complexity of the reinforcement learning environment or action space?
- Basis in paper: The Reinforcement Learning section notes that "future work [should investigate] how the relative benefits of this parameterization scale with task difficulty, action space size, or agent capacity."
- Why unresolved: The current RL experiments are restricted to Atari environments which have relatively small, discrete action spaces.
- What evidence would resolve it: Evaluating the method on complex environments with larger action sets or hierarchical action structures to see if the variance reduction scales effectively.

### Open Question 4
- Question: Does the hard thresholding in the proposed "natural" activation function lead to gradient starvation for confident predictions?
- Basis in paper: Equation 12 defines the natural activation with fixed outputs of 0 or 1 outside the interval $[C-A/2, C+A/2]$.
- Why unresolved: Unlike Softmax, which provides non-zero gradients for all finite inputs, the natural activation function completely blocks gradients when scores move far from the center $C$.
- What evidence would resolve it: Analyzing gradient flow in scenarios with highly imbalanced classes or pre-trained models to determine if the hard cutoffs prevent fine-tuning of high-confidence predictions.

## Limitations
- The paper doesn't rigorously establish that diagonal FIM translates to better optimization in all regimes
- The exact padding/pruning strategy for $K$ not a power of 2 is not specified
- The natural activation has explicit saturation regions that could block gradient flow in practice

## Confidence
- **High:** Catnat improves calibration and achieves lower NLL in VAE experiments
- **Medium:** The diagonal FIM mechanism directly improves optimization
- **Medium:** The natural activation $\nu$ provides additional benefit beyond tree structure alone

## Next Checks
1. **Binary case ablation:** Verify catnat + $\nu$ matches sigmoid performance on binary classification ($K=2$)
2. **Activation ablation study:** Systematically compare catnat + $\nu$ vs. catnat + $\sigma$ vs. softmax on a medium-complexity task ($K=16$, VAE)
3. **Learning rate sensitivity analysis:** Run a structured learning rate sweep for catnat vs. softmax on the VAE task