---
ver: rpa2
title: Weighted Euclidean Distance Matrices over Mixed Continuous and Categorical
  Inputs for Gaussian Process Models
arxiv_id: '2503.02630'
source_url: https://arxiv.org/abs/2503.02630
tags:
- categorical
- distance
- wegp
- input
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WEGP, a Gaussian Process model for mixed
  continuous and categorical inputs. WEGP learns Euclidean distance matrices for categorical
  variables by representing them as weighted combinations of predefined base EDMs,
  allowing flexible modeling of category relationships.
---

# Weighted Euclidean Distance Matrices over Mixed Continuous and Categorical Inputs for Gaussian Process Models

## Quick Facts
- arXiv ID: 2503.02630
- Source URL: https://arxiv.org/abs/2503.02630
- Authors: Mingyu Pu; Songhao Wang; Haowei Wang; Szu Hui Ng
- Reference count: 40
- Key outcome: WEGP learns Euclidean distance matrices for categorical variables as weighted combinations of base EDMs, demonstrating superior predictive accuracy and integration with Bayesian Optimization.

## Executive Summary
This paper introduces WEGP, a Gaussian Process model designed to handle mixed continuous and categorical inputs by constructing Euclidean distance matrices (EDMs) for categorical variables. The method represents categorical similarities through a weighted combination of predefined base EDMs, allowing flexible modeling of complex relationships. WEGP employs a fully Bayesian framework with hierarchical shrinkage priors to automatically determine the importance of different distance patterns, and includes theoretical analysis showing convergence guarantees.

## Method Summary
WEGP constructs kernels for mixed inputs by combining Euclidean distance matrices (EDMs) for categorical variables with standard kernels for continuous variables. Categorical variables are represented as weighted combinations of base EDMs, which are generated using ordinal encodings via random permutations. The model uses a fully Bayesian framework with hierarchical Half-Cauchy priors, and inference is performed using the No-U-Turn Sampler (NUTS). The method is integrated with Bayesian Optimization using Expected Improvement as the acquisition function.

## Key Results
- WEGP demonstrates superior predictive accuracy compared to baseline methods on four benchmark problems (Beam Bending, Piston, Borehole, OTL).
- The theoretical analysis shows the posterior mean converges to the underlying function with rate O(h^v∧1 log(1/h)^β).
- Integration with Bayesian Optimization yields better performance on synthetic and real-world optimization tasks (Func2C/3C, Ackley4C, MLP/SVM tuning on California Housing).

## Why This Works (Mechanism)
The method works by representing categorical similarities through a flexible linear combination of base EDMs, which allows the model to capture complex relationships between categories. The hierarchical Bayesian framework with shrinkage priors enables automatic determination of the importance of different distance patterns, preventing overfitting. The theoretical convergence guarantees provide confidence in the model's ability to learn the underlying function as more data becomes available.

## Foundational Learning
- **Euclidean Distance Matrices (EDMs):** Matrices that encode pairwise distances between points in a metric space. *Why needed:* EDMs provide a mathematically rigorous way to represent categorical similarities. *Quick check:* Verify that generated EDMs are positive semidefinite and satisfy the EDM constraints.
- **Hierarchical Shrinkage Priors:** A Bayesian prior structure that applies global and local shrinkage to model parameters. *Why needed:* Controls the complexity of the model by automatically determining the importance of different distance patterns. *Quick check:* Monitor the posterior distribution of weight parameters for signs of strong shrinkage.
- **No-U-Turn Sampler (NUTS):** An adaptive Hamiltonian Monte Carlo algorithm for efficient posterior sampling. *Why needed:* Enables scalable Bayesian inference in high-dimensional parameter spaces. *Quick check:* Check effective sample size (ESS) and R-hat statistics for convergence diagnostics.

## Architecture Onboarding

**Component map:**
Data -> Base EDM Generation -> GP Kernel Construction -> NUTS Inference -> Posterior Prediction
Acquisition Function -> Bayesian Optimization Loop

**Critical path:**
1. Generate base EDMs for categorical variables
2. Construct GP kernel with hierarchical priors
3. Run NUTS to sample hyperparameters
4. Use posterior samples for prediction or BO acquisition

**Design tradeoffs:**
- Fixed vs. adaptive number of base EDMs: Current method uses fixed combinatorial number, but adaptive selection could improve scalability.
- Ordinal vs. extreme direction bases: Different base generation methods have different structural biases.
- Full vs. sparse inference: Current method uses full NUTS sampling, which may be computationally expensive for large problems.

**Failure signatures:**
- Non-positive definite covariance matrix: Indicates numerical issues with EDM construction or linear independence.
- Slow MCMC convergence: Suggests hierarchical prior structure may be too complex for the data.
- Poor predictive performance: Could indicate insufficient base EDMs or inappropriate kernel structure.

**First experiments:**
1. Verify base EDM generation algorithm produces linearly independent matrices.
2. Test GP model on synthetic data with known categorical structure.
3. Compare performance of ordinal vs. extreme direction base EDMs on a simple benchmark.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can WEGP be effectively adapted for scenarios involving extremely high-dimensional categorical spaces without incurring prohibitive computational costs?
- Basis in paper: [explicit] The Discussion section states that performance in scenarios with "extremely high-dimensional categorical spaces requires further exploration" due to the increasing complexity of estimating similarities.
- Why unresolved: The current method uses m_k = c_k(c_k-1)/2 base EDMs for a categorical variable with c_k choices, leading to quadratic parameter growth.
- What evidence would resolve it: Empirical studies on datasets with significantly larger categorical cardinalities than the benchmarks used.

### Open Question 2
- Question: Can adaptive mechanisms be designed to dynamically adjust the dimensionality of the embedding space or the number of base EDMs based on input complexity?
- Basis in paper: [explicit] The authors propose exploring adaptive mechanisms that dynamically adjust the dimensionality of the embedding space based on the complexity of the input space.
- Why unresolved: The current implementation relies on a fixed number of base EDMs determined prior to training.
- What evidence would resolve it: Development of a modified WEGP algorithm that varies m_k during the optimization process.

### Open Question 3
- Question: What is the optimal strategy for selecting or combining the two proposed base EDM generation methods (ordinal encoders vs. extreme directions) for a given dataset?
- Basis in paper: [inferred] Section 4.1 introduces two distinct methods for constructing base EDMs but does not compare their efficacy.
- Why unresolved: Ordinal encoders assume a linear structure while extreme directions span the full EDM cone, and their relative strengths are unclear.
- What evidence would resolve it: An ablation study comparing convergence rates and predictive errors of models using different base sets.

## Limitations
- The method's scalability to datasets with many high-cardinality categorical variables is limited by the quadratic growth in base EDMs.
- The hierarchical prior structure with potentially hundreds of weights per categorical variable may lead to slow mixing or divergences in NUTS.
- The theoretical convergence rate's practical relevance depends heavily on the smoothness properties of real-world functions.

## Confidence
- **High confidence:** The theoretical foundation (EDM construction, hierarchical prior structure, convergence analysis) and empirical methodology are clearly specified and reproducible.
- **Medium confidence:** The experimental results showing improved performance over baselines, though strong, depend on unspecified NUTS configuration details.
- **Low confidence:** The scalability of the method to datasets with many high-cardinality categorical variables.

## Next Checks
1. Implement and verify Algorithm 1 to ensure the generated base EDMs are linearly independent and correctly represent the categorical structure.
2. Test MCMC convergence by running NUTS with multiple chains and monitoring effective sample size (ESS) and $\hat{R}$ statistics for the weight parameters.
3. Perform sensitivity analysis on the number of posterior samples L used for EI averaging in Bayesian Optimization to determine its impact on performance.