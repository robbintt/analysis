---
ver: rpa2
title: Are LLM-generated plain language summaries truly understandable? A large-scale
  crowdsourced evaluation
arxiv_id: '2505.10409'
source_url: https://arxiv.org/abs/2505.10409
tags:
- evaluation
- plss
- comprehension
- metrics
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluated the understandability of LLM-generated
  plain language summaries (PLSs) compared to human-written ones using a large-scale
  crowdsourced experiment (150 participants, 1,346 annotations). Despite subjective
  evaluations showing no significant differences in perceived quality, objective comprehension
  tests revealed that human-written PLSs led to significantly better reader understanding
  (MCQ accuracy: 0.8047 for human vs.'
---

# Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation

## Quick Facts
- arXiv ID: 2505.10409
- Source URL: https://arxiv.org/abs/2505.10409
- Authors: Yue Guo; Jae Ho Sohn; Gondy Leroy; Trevor Cohen
- Reference count: 40
- Primary result: Human-written PLSs led to significantly better reader comprehension than LLM-generated versions despite similar subjective ratings

## Executive Summary
This study systematically evaluated the understandability of LLM-generated plain language summaries (PLSs) compared to human-written ones using a large-scale crowdsourced experiment (150 participants, 1,346 annotations). Despite subjective evaluations showing no significant differences in perceived quality, objective comprehension tests revealed that human-written PLSs led to significantly better reader understanding (MCQ accuracy: 0.8047 for human vs. 0.7507-0.7848 for LLMs, p < 0.001). Automated evaluation metrics poorly predicted comprehension outcomes, with only QAEval showing meaningful correlation (β = 0.039, p = 0.035). The study highlights that while LLMs can generate fluent PLSs, they fail to support comprehension as effectively as human-authored summaries, emphasizing the need for evaluation frameworks that prioritize actual understanding over surface-level quality metrics.

## Method Summary
The study sampled 50 abstract-PLS pairs from the CELLS dataset and used GPT-4 to generate 6 PLS variants per abstract (simplification, informativeness, coherence, faithfulness, all-combined, no-optimization). LLaMA-3-70B generated MCQs and attention checks. 150 MTurk participants provided 1,346 annotations (459 after quality control), rating PLSs on 5-point Likert scales and answering comprehension questions. Mixed-effects models with participant and abstract random intercepts analyzed the relationship between automated metrics, subjective ratings, and comprehension outcomes.

## Key Results
- Human-written PLSs achieved significantly higher MCQ accuracy (0.8047) than all LLM variants (0.7507-0.7848, p < 0.001)
- No significant differences in subjective quality ratings between human and LLM-generated PLSs
- QAEval was the only automated metric significantly correlated with comprehension (β = 0.039, p = 0.035)
- Background information inclusion showed the second strongest association with comprehension (β = 0.045, p = 0.016)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subjective quality ratings do not predict actual comprehension outcomes for LLM-generated plain language summaries.
- Mechanism: Participants rate LLM-generated PLSs similarly to human-written ones on simplicity, coherence, and informativeness (all p > 0.05), but achieve significantly lower comprehension scores (MCQ accuracy: 0.8047 human vs. 0.7507-0.7848 LLM, p < 0.001). Surface fluency creates an illusion of understandability that decouples perceived quality from actual knowledge transfer.
- Core assumption: MCQ accuracy validly measures comprehension rather than test-taking skill.
- Evidence anchors:
  - [abstract] "Despite subjective evaluations showing no significant differences in perceived quality, objective comprehension tests revealed that human-written PLSs led to significantly better reader understanding"
  - [section 2.3-2.4] "none of the LLM-generated PLSs were rated significantly higher than the human-written versions on any dimension" but "participants exhibited significantly higher accuracy when answering questions based on human-authored PLS"
  - [corpus] Weak direct support; corpus neighbor "Evaluating the Evaluators" questions readability metrics but does not address subjective-objective gaps.
- Break condition: If participants had prior exposure to PLS content, subjective ratings may conflate familiarity with perceived quality.

### Mechanism 2
- Claim: QA-based automated metrics (QAEval) better predict human comprehension than overlap-based metrics.
- Mechanism: QAEval generates question-answer pairs from reference text and tests whether a QA model extracts correct answers from generated summaries. This probes factual consistency directly, unlike ROUGE/BLEU which measure n-gram overlap. QAEval showed β = 0.039 (p = 0.035) correlation with comprehension; overlap metrics showed no significant association.
- Core assumption: The QA model's ability to extract answers from summaries approximates human information extraction.
- Evidence anchors:
  - [abstract] "only QAEval showing meaningful correlation (β = 0.039, p = 0.035)"
  - [section 2.6] "Traditional surface-form metrics such as BLEU, ROUGE, METEOR, GPT-PPL, and LENS were not significantly predictive of comprehension"
  - [corpus] Neighbor "PlainQAFact" directly addresses QA-based factual consistency evaluation for biomedical PLS, supporting the QA-evaluation paradigm.
- Break condition: If QA-generated questions do not cover key comprehension-critical information, QAEval scores may overestimate actual understandability.

### Mechanism 3
- Claim: Including background information (definitions, context not in source abstract) significantly improves layperson comprehension.
- Mechanism: Human authors add elaborative explanations absent from scientific abstracts. The "Background" subjective rating (whether necessary context was added) showed β = 0.045 (p = 0.016) correlation with comprehension—the second strongest predictor after faithfulness. LLMs instructed to add background did not reliably do so.
- Core assumption: Background information helps rather than distracts; added context is accurate and relevant.
- Evidence anchors:
  - [section 2.5] "Background showed the second strongest association with comprehension performance (β = 0.045, p = 0.016)"
  - [section 3] "This study represents the first large-scale crowdsourced evaluation to empirically demonstrate the benefits of including background information in PLSs"
  - [corpus] No direct corpus support for background-information mechanism specifically.
- Break condition: If background additions introduce inaccuracies or oversimplifications, they may reduce faithfulness while appearing helpful.

## Foundational Learning

- **Plain Language Summarization (PLS)**
  - Why needed here: The entire paper evaluates PLS quality; understanding that PLS requires translating technical content for lay audiences—not just shortening—is foundational.
  - Quick check question: How does PLS differ from standard abstractive summarization?

- **Evaluation Metric Types (overlap-based vs. model-based vs. QA-based)**
  - Why needed here: The paper's central claim is that different metric categories have different correlations with human comprehension.
  - Quick check question: Why would ROUGE scores fail to predict whether a layperson understands a medical summary?

- **Crowdsourced Evaluation Quality Control**
  - Why needed here: The paper filters 1,750 annotations down to 459 (34.1%) using attention checks and completion-time thresholds. Understanding this filtering is critical for interpreting results.
  - Quick check question: What quality control measures did the study use, and what retention rate should you expect in similar experiments?

## Architecture Onboarding

- **Component map**: CELLS dataset (50 pairs) -> GPT-4 (6 PLS variants) -> LLaMA-3-70B (MCQs + attention checks) -> MTurk (5 annotations per pair) -> Likert ratings + MCQ accuracy + recall -> Mixed-effects models with 10 automated metrics

- **Critical path**: 1) Sample abstracts → Generate 6 LLM variants per abstract 2) Generate comprehension/attention questions via LLaMA-3 3) Deploy to MTurk with 5 annotations per summary version 4) Filter responses (attention check + completion time) 5) Fit mixed-effects models predicting MCQ accuracy from metrics/ratings

- **Design tradeoffs**: Sample size vs. diversity (50 abstracts exceeds prior work but limits topical coverage); MTurk diversity vs. representativeness (broader than university samples but may not match general population health literacy); Rigorous subset (34%) vs. full data (higher quality but smaller N; results consistent across both)

- **Failure signatures**: High Likert ratings + low MCQ accuracy = fluency-comprehension gap (LLM signature); Self-identified experts with low MCQ accuracy = overconfidence effect; ROUGE/BLEU high but QAEval low = surface similarity without factual preservation

- **First 3 experiments**: 1) Replicate the comparison on a different domain (e.g., legal or financial documents) to test whether the subjective-objective gap generalizes beyond biomedical PLS; 2) Ablate the background-information prompt component to isolate its contribution to comprehension gains; 3) Test fine-tuned models optimized directly for QAEval scores rather than Likert-aligned criteria to see if comprehension improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific textual and structural features (sentence structure, lexical choice, information ordering) of human-written PLSs contribute to superior reader comprehension compared to LLM-generated versions?
- Basis in paper: [explicit] "Future work should explore which specific characteristics, such as sentence structure, lexical choice, or information ordering, are most effective in supporting layperson understanding."
- Why unresolved: While the study demonstrates a comprehension gap, it does not isolate which features drive this difference.
- What evidence would resolve it: Controlled experiments systematically varying individual features while measuring comprehension outcomes.

### Open Question 2
- Question: Can generation methods that explicitly optimize for comprehension (e.g., reinforcement learning with comprehension-based rewards) produce LLM-generated PLSs that match or exceed human-authored ones?
- Basis in paper: [explicit] "Future research should explore generation strategies that directly optimize for human understanding... These insights could inform the development of reinforcement learning objectives that prioritize comprehension over readability alone."
- Why unresolved: Current LLM optimization strategies (simplification, informativeness prompts) did not reliably improve comprehension.
- What evidence would resolve it: Training models with comprehension-based objectives and evaluating with objective comprehension measures.

### Open Question 3
- Question: What domain-adapted automated metrics can reliably capture comprehension outcomes for PLS evaluation?
- Basis in paper: [explicit] The study calls for "developing domain-adapted, comprehension-sensitive evaluation tools that move beyond surface-level text features."
- Why unresolved: Only QAEval showed meaningful correlation with comprehension (β = 0.039); traditional metrics (BLEU, ROUGE) showed no significant association.
- What evidence would resolve it: Developing and validating new metrics against large-scale human comprehension data.

## Limitations
- Findings may not generalize beyond biomedical abstracts due to specialized vocabulary and context-dependent scientific communication
- MTurk sample, while diverse, may not represent the full spectrum of health literacy levels in the general population
- MCQ format, while standardized, may not capture all dimensions of comprehension, particularly integrative understanding or ability to apply knowledge

## Confidence

- High confidence: The subjective-objective gap finding (LLM fluency ≠ comprehension) and the superior predictive power of QAEval over overlap metrics
- Medium confidence: The background-information mechanism, as the study provides correlational but not causal evidence
- Low confidence: Whether automated metrics can be optimized to reliably predict comprehension without human testing

## Next Checks

1) Replicate the subjective-objective gap on non-biomedical domains (legal, financial, or technical documentation) to test generalizability
2) Conduct a controlled ablation study isolating the background-information prompt component to establish causal contribution to comprehension gains
3) Test fine-tuned LLM models optimized directly for QAEval scores rather than Likert-aligned criteria to determine if comprehension can be improved through automated metric optimization