---
ver: rpa2
title: From Policy to Logic for Efficient and Interpretable Coverage Assessment
arxiv_id: '2601.01266'
source_url: https://arxiv.org/abs/2601.01266
tags:
- policy
- reasoning
- coverage
- language
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neuro-symbolic approach for interpretable
  coverage assessment in healthcare policy review. The method pairs a coverage-aware
  retriever with a rule-based symbolic reasoning system to surface relevant policy
  language, organize it into explicit facts and rules, and generate auditable rationales.
---

# From Policy to Logic for Efficient and Interpretable Coverage Assessment

## Quick Facts
- arXiv ID: 2601.01266
- Source URL: https://arxiv.org/abs/2601.01266
- Reference count: 16
- 44% reduction in inference cost with 4.5% F1 score improvement over LLM baselines

## Executive Summary
This paper presents a neuro-symbolic approach for interpretable coverage assessment in healthcare policy review. The method pairs a coverage-aware retriever with a rule-based symbolic reasoning system to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. The coverage-aware retriever uses expert-annotated supervision to identify policy passages governing CPT code coverage, outperforming standard semantic search. The rule-based system encodes this information using PyKnow to enable traceable, deterministic reasoning. Experiments on 7 plan documents show a 44% reduction in inference cost and a 4.5% improvement in F1 score compared to LLM-based baselines. The approach achieves 87% accuracy and 93% F1 score, demonstrating both efficiency and effectiveness while supporting human interpretability in complex policy analysis.

## Method Summary
The method fine-tunes a Longformer cross-encoder retriever on expert-labeled (CPT code, policy subsection, relevance) pairs to identify governing policy passages. It then uses GPT-4.1 to generate Boolean attributes characterizing each CPT code and PyKnow rules encoding coverage logic from the retrieved text. During inference, a PyKnow engine matches CPT attributes against rules to determine coverage status, providing deterministic, auditable traces without LLM calls. The approach achieves 44% cost reduction through pre-computed attributes and rules while maintaining accuracy.

## Key Results
- 44% reduction in inference cost through pre-computed attributes and symbolic reasoning
- 4.5% improvement in F1 score compared to LLM-based baselines
- 87% accuracy and 93% F1 score on 7 plan documents with 814 CPT codes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuned cross-encoder retrieval improves coverage-relevant passage identification over semantic search.
- **Mechanism:** The coverage-aware retriever uses contrastive multiple-choice ranking with expert-labeled supervision to distinguish dispositive policy language from thematically similar but non-governing passages. The cross-encoder jointly processes CPT queries and subsections, capturing fine-grained interactions with phrases like "prior authorization required" or "not covered."
- **Core assumption:** Policy language determining coverage status follows patterns distinguishable from thematic content via supervised learning.
- **Evidence anchors:** Coverage-aware retriever uses expert-annotated supervision; 1.84 million labeled pairs with SME arbitration; LongformerForMultipleChoice fine-tuned with contrastive objective.
- **Break condition:** If policy language varies dramatically across insurers without shared patterns, supervised transfer degrades. If cross-encoder inference latency becomes prohibitive at >1000 subsections per document, retrieval throughput collapses.

### Mechanism 2
- **Claim:** Pre-computed attributes with one-time generation reduce per-inference LLM cost.
- **Mechanism:** Attributes (boolean properties like `is_surgical`, `is_telemedicine`) are generated once per CPT code and reused across plans. Rules are generated once per plan document. Inference uses only the PyKnow symbolic engine—no LLM calls required at runtime.
- **Core assumption:** Attributes sufficiently characterize CPT codes for coverage determination; policy logic can be expressed via pre-defined boolean conditions.
- **Evidence anchors:** 44% reduction in inference cost; Attribute generation performed once per CPT code; Rule generation required only once per coverage policy.
- **Break condition:** If new CPT codes or policy structures require attributes not in the pre-computed schema, regeneration costs scale. If attribute set is incomplete (observed: 73.5% of errors from missing attributes), coverage decisions fail silently.

### Mechanism 3
- **Claim:** Symbolic rule execution provides auditable, deterministic traces for human reviewers.
- **Mechanism:** PyKnow encodes coverage logic as `@Rule` decorators with explicit conditions. When a rule triggers, the system surfaces the matching attributes and rule name, enabling reviewers to trace back to governing policy language.
- **Core assumption:** Policy logic can be faithfully translated into if-then rules without ambiguity or contradiction.
- **Evidence anchors:** Generate auditable rationales; After a rule is matched, relevant attributes are passed to human reviewer; example rule shows explicit `@Rule(Procedure(is_telemedicine=True...))` pattern.
- **Break condition:** If policy language contains contradictions or requires subjective judgment beyond boolean attributes, rules produce spurious matches or no matches. Error analysis shows 26.5% of failures from insufficient rule coverage.

## Foundational Learning

- **Concept: Cross-encoder vs. bi-encoder retrieval**
  - Why needed here: The paper uses cross-encoder architecture for passage ranking; understanding the precision-throughput tradeoff is essential.
  - Quick check question: Given a query and 50 candidate passages, can you explain why cross-encoder scoring is more accurate but slower than bi-encoder embedding similarity?

- **Concept: Expert systems and production rules**
  - Why needed here: PyKnow implements forward-chaining rule engines; understanding facts, rules, and conflict resolution is prerequisite.
  - Quick check question: In a rule engine with multiple matching rules, what determines which rule fires first?

- **Concept: Contrastive learning objectives (InfoNCE)**
  - Why needed here: The retriever uses contrastive multiple-choice ranking to distinguish positive passages from distractors.
  - Quick check question: How does contrastive loss differ from standard classification loss when training a ranker with one positive and many negatives?

## Architecture Onboarding

- **Component map:** CPT code → Retriever (identify governing passages) → Attribute Generator (one-time per CPT) → Rule Generator (one-time per plan) → PyKnow inference (deterministic, no LLM). Human reviewer receives: CPT, triggered rule, supporting attributes, source passage.
- **Critical path:** The system processes each CPT code through the retriever to find governing passages, generates attributes once per CPT, creates rules once per plan, then performs deterministic symbolic inference without LLM calls.
- **Design tradeoffs:** Cross-encoder retrieval provides higher precision vs. bi-encoder but requires exhaustive scoring per CPT (feasible with <60 subsections). Pre-computed attributes amortize LLM cost but risk staleness; 73.5% of errors attributed to missing attributes. Symbolic inference eliminates hallucination risk but cannot handle nuance outside encoded rules.
- **Failure signatures:** No rule triggered (26.5% of errors): Attribute set incomplete or rule coverage insufficient. Wrong rule triggered (73.5% of errors): Long attribute lists cause "lost-in-the-later" forgetting; model overlooks attributes late in sequence. Retrieval returns no passage above threshold τ=0.25: System should escalate to human review.
- **First 3 experiments:** 1) Retriever validation: Score retrieval precision on held-out CPT-subsection pairs; verify cross-encoder identifies correct governing passage against SME labels. 2) Attribute coverage audit: Sample 100 CPT codes; manually verify generated attributes capture clinically relevant properties. Flag missing attributes. 3) Rule trigger rate analysis: Run inference across 7 plan documents; measure percentage of CPT codes with zero rules triggered vs. correct rule triggered. Identify systematic gaps in rule generation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the rule generation phase be modified to prevent the LLM from overlooking relevant attributes in long sequences (the "lost-in-the-later" phenomenon), which currently accounts for 73.5% of system errors? The paper identifies this as the "most impactful path forward" but relies on current LLM context handling which is susceptible to forgetting. A modified generation strategy (e.g., hierarchical prompting) that shows a statistically significant reduction in attribute omission errors would resolve this.

- **Open Question 2:** Can a dynamic mechanism be developed to automatically balance the tradeoff between the higher accuracy of LLM inference and the lower cost of the rule-based system? The authors state, "Choosing between these methods involves balancing the goal of achieving optimal performance with the need to minimize operational costs. We plan to address this tradeoff... in future work." The current system requires a manual choice between a cheaper, slightly less accurate symbolic approach and a more expensive neural one. A routing algorithm that selects the inference method based on policy complexity or confidence thresholds while optimizing a cost-accuracy utility function would resolve this.

- **Open Question 3:** How robust is the coverage-aware retriever when applied to policy documents with structures or linguistic variations significantly different from the 172 Certificates of Coverage used in training? The retriever relies on 1.84 million expert-annotated pairs from a specific dataset; the paper does not evaluate zero-shot transferability to entirely new policy types or domains. Evaluation results on out-of-distribution policy documents (e.g., different insurers or legal contracts) without further fine-tuning would resolve this.

## Limitations
- Core architecture depends on proprietary SME-labeled training data (~1.84M pairs) that cannot be externally validated
- 73.5% error rate from missing attributes and 26.5% from insufficient rule coverage suggest the attribute schema may not generalize beyond the 7 plan documents tested
- Cross-encoder inference scales poorly with document length, potentially limiting applicability to comprehensive insurance policies with hundreds of subsections

## Confidence
- **High confidence:** Cross-encoder retrieval improves precision over semantic search; symbolic reasoning eliminates LLM hallucination risk; 44% cost reduction via pre-computed attributes is directly measurable
- **Medium confidence:** 87% accuracy and 93% F1 scores are valid for the 7 plan documents tested, but generalizability to new insurers/plans remains unproven without external validation
- **Low confidence:** The claim that attributes can fully characterize CPT codes for coverage determination; error analysis suggests systematic gaps in attribute coverage and rule generation

## Next Checks
1. **External generalizability test:** Apply the trained retriever and attribute/rule generators to 3-5 new healthcare policy documents from different insurers; measure accuracy drop and identify systematic failure modes
2. **Attribute schema completeness audit:** Manually annotate 100 CPT codes across multiple medical specialties; compare required attributes for coverage determination against the pre-computed schema to quantify coverage gaps
3. **Cross-encoder scaling experiment:** Benchmark retrieval latency and accuracy on synthetic documents with 50, 100, and 200 subsections; determine if inference time becomes prohibitive for real-world policy documents