---
ver: rpa2
title: Investigating self-supervised representations for audio-visual deepfake detection
arxiv_id: '2511.17181'
source_url: https://arxiv.org/abs/2511.17181
tags:
- features
- detection
- audio
- deepfake
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates self-supervised representations
  (audio, visual, multimodal) for audio-visual deepfake detection, addressing three
  research questions: detection effectiveness, interpretability of encoded information,
  and cross-modal complementarity. The authors adapt linear probing to the video domain,
  assess performance across in-domain and out-of-domain datasets, and propose a multi-faceted
  evaluation suite including anomaly detection and explainability techniques.'
---

# Investigating self-supervised representations for audio-visual deepfake detection

## Quick Facts
- arXiv ID: 2511.17181
- Source URL: https://arxiv.org/abs/2511.17181
- Reference count: 40
- Primary result: Self-supervised features capture deepfake-relevant information, with audio-informed representations generalizing best across datasets, but none reliably generalize cross-domain.

## Executive Summary
This paper systematically evaluates self-supervised representations for audio-visual deepfake detection, addressing detection effectiveness, interpretability, and cross-modal complementarity. The authors adapt linear probing to video domain, assess performance across multiple datasets, and propose a multi-faceted evaluation suite including anomaly detection and explainability techniques. Key findings show that many self-supervised features encode forensic-relevant information, with audio-informed representations providing the strongest generalization. However, cross-dataset generalization remains challenging, likely due to dataset-specific characteristics rather than superficial pattern learning.

## Method Summary
The paper employs linear probing on frozen self-supervised representations for binary classification of fake vs. real videos. Features are extracted from multiple SSL models (Wav2Vec2, AV-HuBERT, CLIP, VideoMAE, FSFM, Auto-AVSR) using frame-level embeddings, then classified with a linear layer and aggregated via log-sum-exp pooling. The approach is evaluated across four datasets (FakeAVCeleb, AV-Deepfake1M, AVLips, DeepfakeEval 2024) using AUC as the primary metric. Anomaly detection is performed via next-token prediction and audio-video synchronization tasks, and temporal explanations are analyzed to assess interpretability.

## Key Results
- Many self-supervised features capture deepfake-relevant information with strong in-domain performance (>90% AUC)
- Audio-informed representations (particularly Wav2Vec2 and AV-HuBERT) generalize best across datasets
- Feature combinations improve performance, especially when combining audio and visual modalities
- Temporal and spatial explanations align with manipulated regions and human annotations
- None of the features reliably generalize across datasets, likely due to dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Linear probing with log-sum-exp pooling isolates deepfake-relevant information already encoded in self-supervised representations without conflating it with complex classifier learning.
- **Mechanism:** A minimal linear classifier operates on frame-level embeddings, then log-sum-exp pooling aggregates per-frame predictions into a video-level score. Because log-sum-exp approximates max, the model flags a video as fake if any temporal region appears manipulated—matching the nature of partial forgeries.
- **Core assumption:** The SSL features already encode forensically relevant structure; the classifier merely reveals it rather than learning it from scratch.
- **Evidence anchors:** [abstract] "models primarily attend to semantically meaningful regions rather than spurious artifacts"; [Section 3] "Since the log-sum-exp approximates the max function, the model learns to predict that a video is fake if only a single region of the model is fake"; [corpus] ERF-BA-TFD+ (arXiv:2508.17282) similarly uses frozen SSL backbones with lightweight classification heads.

### Mechanism 2
- **Claim:** Audio-visual synchronization as a proxy task detects deepfakes by measuring cross-modal alignment without requiring fake samples during training.
- **Mechanism:** An alignment network is trained to distinguish correct audio-video frame pairings from neighboring misalignments using contrastive learning on real data only. At inference, low alignment scores indicate manipulation, pooled via log-sum-exp to produce detection scores.
- **Core assumption:** Deepfakes introduce audio-visual asynchrony that persists even when visual artifacts are subtle or absent.
- **Evidence anchors:** [abstract] "this information is complementary"; [Section 3.2] "the assumption is that mismatches between the two modalities indicate manipulations"; [Table 3] AV-HuBERT (A + V) synchronization achieves 96.3 AUC on FAVC; [corpus] SpeechForensics uses similar alignment scoring with cosine similarity.

### Mechanism 3
- **Claim:** Temporal explanations derived from linear classifier weights weakly localize manipulations, indicating features capture genuine forensic cues beyond spurious correlations.
- **Mechanism:** Because the classifier is linear, per-frame scores directly indicate temporal contribution to the fake/real decision. These are compared against ground-truth manipulation boundaries using frame-level AUC.
- **Core assumption:** If models relied purely on spurious cues (e.g., leading silence), temporal explanations would not align with actual manipulation regions.
- **Evidence anchors:** [Section 6.2] "many features still produce strong results (localization performance is close to that of classification)"; [Figure 2] Audio models maintain localization performance near classification AUC; [Section 6.2] "Audio models show strong performance, but qualitative examples indicate that they do attend to leading silence. Despite this, they also focus on the manipulated regions."

## Foundational Learning

- **Concept: Linear Probing**
  - Why needed here: The paper's core methodology uses frozen SSL features with only a linear classifier to assess what information is already encoded. Understanding this paradigm is essential to interpret all results.
  - Quick check question: Given a pretrained encoder producing 1024-dim embeddings, what would a linear probe's weight matrix dimensions be for binary classification?

- **Concept: Self-Supervised Representation Learning (Contrastive and Masked Prediction)**
  - Why needed here: All evaluated models are SSL-trained. Their pretraining objectives determine what forensic cues they encode.
  - Quick check question: Why might a model trained on masked speech prediction (HuBERT) encode different deepfake-relevant information than one trained on image-text contrastive learning (CLIP)?

- **Concept: Log-Sum-Exp Pooling**
  - Why needed here: This pooling strategy is used for both classification and anomaly detection, designed to be sensitive to localized manipulations.
  - Quick check question: How does log-sum-exp behavior differ from mean pooling when aggregating scores from a video with one short manipulated segment?

## Architecture Onboarding

- **Component map:** Input Video → [Face/Lip Detection + Cropping] → SSL Encoder (frozen) → Frame-level Embeddings φ(x)t → Linear Classifier w → Log-Sum-Exp Pooling → Video-level Score s(x;w)

- **Critical path:**
  1. Preprocessing alignment: Visual models require face/lip crops matching their pretraining (CLIP = uncropped, FSFM = face, AV-HuBERT = lips). Audio models use log filterbanks or raw waveform.
  2. Feature extraction: SSL encoder forward pass, extracting embeddings from final transformer layer (or layer 9 per ablation in Fig. 7).
  3. Temporal alignment: Audio features (50 Hz) upsampled to match video framerate (25 FPS) via concatenation.
  4. Classification: Linear projection followed by log-sum-exp across temporal dimension.

- **Design tradeoffs:**
  - Frozen vs. fine-tuned features: Paper uses frozen features to isolate representation quality. Fine-tuning would improve in-domain performance but obscure which information comes from pretraining vs. adaptation.
  - Audio-only vs. visual-only vs. multimodal: Audio features (Wav2Vec2) generalize best across datasets with speech manipulations; visual features (AV-HuBERT-V) excel on lip-sync datasets. Multimodal fusion provides gains but requires alignment handling.
  - Pooling strategy: Max-based pooling suits localized manipulations (AV1M); min-based pooling suits in-the-wild real videos with imperfect synchronization (DFE-2024). Average pooling is rarely optimal.

- **Failure signatures:**
  - In-domain success, out-of-domain collapse: Strong AUC on training dataset but 50-60% on held-out datasets indicates dataset-specific artifact exploitation.
  - Random baseline performs above chance: Randomly initialized AV-HuBERT achieves 63.5% mean OOD AUC, suggesting architectural biases or preprocessing artifacts.
  - Audio models attend to leading silence: Qualitative inspection shows high scores on silence regions—a spurious cue that may not generalize.

- **First 3 experiments:**
  1. Baseline linear probe on AV-HuBERT multimodal features: Train on FAVC training split, evaluate on FAVC test and AV1M test. Expected: ~100% in-domain, ~94% cross-domain.
  2. Cross-modal generalization test: Train linear probes on audio-only features (Wav2Vec2), test on AV-Lips (no audio manipulation) and DFE-2024 (incomplete audio manipulation). Expected: Near-chance on AV-Lips.
  3. Synchronization proxy task: Train alignment network on VoxCeleb real samples using AV-HuBERT (A+V) features, evaluate on FAVC. Expected: ~96% AUC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific dataset characteristics cause the observed generalization failure across domains, and can these factors be systematically identified and mitigated?
- Basis in paper: "Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns."
- Why unresolved: The authors hypothesize dataset characteristics as the cause but do not isolate which specific factors (artifact types, content variations, generation methods) are responsible.
- What evidence would resolve it: A controlled ablation study varying individual dataset factors while holding others constant, measuring their impact on cross-domain transfer.

### Open Question 2
- Question: What architectural or methodological advances beyond self-supervised feature representations are needed to achieve robust cross-domain deepfake detection?
- Basis in paper: "closing the generalization gap will likely require advances beyond feature representations alone."
- Why unresolved: The paper evaluates existing SSL features comprehensively but does not propose or test what additional components could improve generalization.
- What evidence would resolve it: Comparative studies of detection frameworks that combine SSL features with domain adaptation, data augmentation, or meta-learning approaches.

### Open Question 3
- Question: Why do only specific feature combinations (A V-HuBERT audio for next-token prediction, A V-HuBERT visual for synchronization) enable effective anomaly detection while others fail?
- Basis in paper: "to achieve reasonable anomaly detection results, specific feature combinations are required... robustness in anomaly detection depends on selecting features that capture the temporal dynamics necessary."
- Why unresolved: The paper demonstrates the phenomenon empirically but does not fully explain the underlying mechanism for why certain features capture these dynamics better than others.
- What evidence would resolve it: Probing experiments measuring temporal coherence and cross-modal alignment capabilities across different SSL representations.

## Limitations

- None of the evaluated self-supervised features reliably generalize across datasets, representing a fundamental limitation for practical deployment
- Random feature baselines perform above chance (63.5% mean OOD AUC), suggesting architectural biases or spurious cues may confound interpretation
- Audio models show spurious correlation with leading silence regions, indicating remaining reliance on non-forensic cues

## Confidence

- High confidence: In-domain detection performance claims (AUC > 90% on FAVC/AV1M)
- High confidence: Cross-domain generalization failure is a fundamental limitation
- Medium confidence: Interpretability findings (temporal explanations align with manipulated regions but may capture spurious cues)
- Medium confidence: Audio features generalize better across datasets

## Next Checks

1. Test linear probes on datasets with audio-only or video-only manipulations to confirm modality-specific encoding claims and identify which datasets actually contain audio manipulations.

2. Compare temporal explanation quality against human-labeled manipulation boundaries across multiple datasets to quantify spurious cue influence and validate that explanations reflect genuine forensic reasoning.

3. Evaluate synchronization models on datasets with imperfect audio-visual alignment to test robustness beyond controlled conditions and assess whether the proxy task generalizes to real-world scenarios.