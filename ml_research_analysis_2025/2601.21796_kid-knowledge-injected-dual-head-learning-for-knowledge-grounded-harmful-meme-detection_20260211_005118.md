---
ver: rpa2
title: 'KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful
  Meme Detection'
arxiv_id: '2601.21796'
source_url: https://arxiv.org/abs/2601.21796
tags:
- knowledge
- meme
- classification
- harmful
- memes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KID addresses the challenge of detecting harmful memes that rely
  on metaphors and sociocultural context not explicitly present in the meme content.
  The core method injects structured knowledge into a dual-head learning framework
  that jointly optimizes semantic generation and classification.
---

# KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection

## Quick Facts
- arXiv ID: 2601.21796
- Source URL: https://arxiv.org/abs/2601.21796
- Reference count: 40
- Primary result: Achieves 2.1%-19.7% improvement over state-of-the-art on multilingual harmful meme detection across five datasets

## Executive Summary
KID addresses harmful meme detection by injecting structured knowledge into a dual-head learning framework. The method constructs entity-anchored knowledge that binds background context to visual entities, creating explicit reasoning chains from visual evidence to harmful intent. Through joint optimization of semantic generation and classification objectives, KID achieves state-of-the-art performance across English, Chinese, and Bengali datasets, with the optimal knowledge injection quantity empirically determined to be N=2 items per sample.

## Method Summary
The framework operates in three phases: (1) knowledge construction using Gemini-2.5-Flash to generate entity-anchored knowledge descriptions in inline format, (2) dual-head fine-tuning on Qwen2.5-VL-7B with QLoRA INT4 quantization, and (3) test-time knowledge injection. The dual-head architecture jointly optimizes a causal language modeling head for label text generation and an MLP classifier head, with total loss being the sum of both objectives. The method uses entity-anchored knowledge format (⟨Entity⟩-[Knowledge]) to create direct token-level associations rather than requiring cross-positional mapping.

## Key Results
- KID achieves 2.1%-19.7% improvement over previous best methods across five multilingual datasets
- Entity-anchored knowledge format outperforms appended glossary format by 2.67-3.56 percentage points Macro-F1
- Optimal knowledge injection quantity N=2 per sample shows inverted-U performance curve
- Strong cross-lingual generalization demonstrated on English, Chinese, and Bengali datasets

## Why This Works (Mechanism)

### Mechanism 1: Entity-Anchored Knowledge Binding
- Claim: Structured knowledge injection creates explicit reasoning chains from visual evidence to harmful intent that single-modality approaches cannot capture.
- Core assumption: Harmful intent emerges from a three-step reasoning chain (visual symbol → background knowledge → harmful intent) that must be explicitly materialized.
- Evidence: Inline format outperforms glossary format by 2.67-3.56 Macro-F1; teacher models struggle with evolving memes lacking annotations.
- Break condition: When knowledge is appended as glossary rather than inline-anchored, performance degrades by 3+ percentage points.

### Mechanism 2: Dual-Head Regularization Effect
- Claim: Joint optimization of semantic generation and classification heads stabilizes decision boundaries while maintaining linguistic alignment.
- Core assumption: Pure discriminative learning causes overfitting; generation objective provides regularization by anchoring features to linguistic structure.
- Evidence: Dual-Head SFT improves over generation-only SFT by +2.56 to +5.01 Macro-F1; combined with knowledge injection achieves +7.61 on ToxiCN_MM.
- Break condition: Removing classification head loses quantitative metrics; removing generation head increases overfitting.

### Mechanism 3: Inverted-U Knowledge Quantity Effect
- Claim: Optimal knowledge injection quantity (N=2) maximizes information gain while minimizing noise, with performance degrading on either side.
- Core assumption: Most harmful memes contain 1-2 core metaphorical entities requiring contextualization; additional knowledge items tend toward redundancy.
- Evidence: Clear inverted-U relationship across all eight datasets with peak performance at N=2 for most datasets.
- Break condition: N>2 introduces noise that degrades performance; N<2 leaves semantic gaps.

## Foundational Learning

- **Entity-Level Vision-Language Grounding**: Why needed: Framework requires understanding how textual knowledge binds to specific visual entities through attention mechanisms. Quick check: Can you explain how cross-modal attention in models like CLIP creates joint representations, and why token proximity matters for association strength?

- **Multi-Task Learning with Competing Objectives**: Why needed: Dual-head architecture requires understanding how gradient signals from generation and classification losses interact. Quick check: Can you describe a scenario where adding an auxiliary task improves generalization, and explain the regularization mechanism?

- **Knowledge Distillation Paradigms**: Why needed: Framework uses teacher MLLM to generate structured knowledge that student model must internalize. Quick check: Can you explain the difference between response-based and feature-based distillation, and which pattern KID follows?

## Architecture Onboarding

- **Component map**: Teacher MLLM (Gemini-2.5-Flash) → Entity-Anchored Knowledge Generation → Knowledge-Augmented Dataset → Student MLLM (Qwen2.5-VL-7B) → Dual-Head Architecture (Backbone + CLM Head + MLP Classifier) → Joint Loss → Final Prediction

- **Critical path**: Knowledge construction quality is primary determinant; inline entity-anchored format is non-negotiable; knowledge quantity N=2 is empirically validated default.

- **Design tradeoffs**: Teacher model capability vs. API cost; QLoRA INT4 quantization reduces memory but introduces noise; dual-head adds complexity but provides regularization benefits.

- **Failure signatures**: Target misclassification → check entity detection; high variance → knowledge generation inconsistency; performance degrades with knowledge → N too high or wrong format; generation correct but classification wrong → MLP capacity issue.

- **First 3 experiments**: (1) Knowledge quality audit on 30 sampled memes across datasets, (2) Format ablation comparing inline vs glossary on validation set, (3) Knowledge quantity sweep N ∈ {0,1,2,3,4,5} to confirm inverted-U shape.

## Open Questions the Paper Calls Out
- Can adaptive, sample-specific knowledge injection strategies outperform fixed N=2 configuration?
- How sensitive is KID's performance to choice and capability of teacher model?
- Does optimal knowledge injection strategy differ for multi-label classification scenarios?
- Can KID framework be extended to temporal multimodal content (e.g., short-form videos)?

## Limitations
- Framework critically depends on quality of teacher-generated knowledge, which may be inconsistent across languages
- Dual-head architecture introduces significant hyperparameter tuning complexity
- Optimal N=2 knowledge quantity may not generalize to datasets with different semantic complexity patterns

## Confidence
- **High Confidence**: Entity-anchored knowledge binding superiority; dual-head regularization effect; inverted-U knowledge quantity relationship
- **Medium Confidence**: Cross-lingual generalization claims; state-of-the-art comparisons margins
- **Low Confidence**: Assumption about 1-2 core metaphorical entities; attention mechanism explanations without visualization evidence

## Next Checks
1. **Knowledge Generation Quality Audit**: Manually evaluate 100 randomly sampled memes across all five datasets to assess teacher model's accuracy in entity identification, knowledge relevance, and reasoning chain completeness.

2. **Attention Mechanism Analysis**: Conduct qualitative and quantitative analysis of cross-modal attention patterns comparing inline vs glossary knowledge formats using attention visualization tools.

3. **Dataset-Specific Knowledge Quantity Optimization**: Systematically sweep knowledge quantity N ∈ {0,1,2,3,4,5} on each of the eight binary/multi-class tasks individually, measuring per-class performance and error type distributions.