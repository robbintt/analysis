---
ver: rpa2
title: Data-Centric Lessons To Improve Speech-Language Pretraining
arxiv_id: '2510.20860'
source_url: https://arxiv.org/abs/2510.20860
tags:
- arxiv
- data
- preprint
- training
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the data-centric gap in speech-language model
  pretraining by systematically studying three key questions: how to process raw web
  audio into interleaved speech-text training data, how to construct synthetic datasets
  to augment web-crawled data, and how to interleave speech and text during training.
  The authors find that fine-grained interleaving of speech-text chunks significantly
  improves spoken question-answering (SQA) performance over coarse interleaving.'
---

# Data-Centric Lessons To Improve Speech-Language Pretraining

## Quick Facts
- **arXiv ID**: 2510.20860
- **Source URL**: https://arxiv.org/abs/2510.20860
- **Reference count**: 40
- **Primary result**: Data-centric interventions (fine-grained interleaving, synthetic data, deterministic sampling) improve SQA accuracy by up to 7.2% and enable a 3.8B SpeechLM to outperform models up to 3× larger by 10.2% average SQA accuracy.

## Executive Summary
This paper systematically studies data-centric interventions to improve speech-language pretraining. The authors identify three key questions: how to process raw web audio into interleaved speech-text training data, how to construct synthetic datasets to augment web-crawled data, and how to interleave speech and text during training. Through controlled ablations, they find that fine-grained interleaving of speech-text chunks significantly improves spoken question-answering performance over coarse interleaving. They introduce synthetic datasets using LLM-based rewriting and TTS synthesis, which improve domain coverage and boost SQA accuracy by up to 7.2% when mixed with web-crawled data. Deterministic sampling of speech-text chunks during training further improves SQA by 1%. These data interventions close the modality gap between speech and text distributions and enhance topic diversity. Applying these insights, the authors pretrain SpeLangy, a 3.8B-parameter SpeechLM that outperforms models up to 3× larger by 10.2% average SQA accuracy across three benchmarks while maintaining strong text understanding.

## Method Summary
The authors process >10M hours of web-crawled audio using diarization to create fine-grained speaker turn chunks, then interleave these with transcribed text. They construct synthetic datasets ("Krist" and "Quest") using LLM-based text rewriting and TTS synthesis to augment web data with knowledge-rich domains. During training, they employ deterministic modality sampling (alternating audio/text chunks) rather than stochastic sampling. The SpeLangy architecture uses a Conformer encoder with Finite Scalar Quantizer to tokenize audio, feeding into a 3B LLM backbone. The model is pretrained on 1.67T tokens (60% text, 40% speech-text) with specific mixing ratios of web and synthetic data.

## Key Results
- Fine-grained interleaving improves SQA performance by 3.1% on average compared to coarse interleaving
- Synthetic data mixing improves SQA accuracy by up to 7.2% by correcting topic skew in web-crawled data
- Deterministic modality sampling boosts SQA performance by 1% by increasing cross-modal boundary exposure
- SpeLangy achieves 10.2% better average SQA accuracy than 3× larger models while maintaining strong text understanding

## Why This Works (Mechanism)

### Mechanism 1: Fine-Grained Interlining for Alignment
- **Claim**: Alternating between speech and text at sentence boundaries (fine-grained) appears to improve cross-modal alignment compared to merging long segments (coarse-grained).
- **Mechanism**: Shorter chunks force the model to predict text tokens based on immediate acoustic context and vice versa, rather than allowing the model to rely on long-term buffering of one modality. This likely acts as a form of implicit data augmentation, increasing the frequency of modality "switches" the model must handle.
- **Core assumption**: The model possesses sufficient capacity to handle the increased complexity of frequent modality switching without catastrophic forgetting of unimodal patterns.
- **Evidence anchors**:
  - [abstract]: Mentions "fine-grained interleaving... improve SQA performance."
  - [section 3.3]: "We note fine interleaving improves SQA performance by 3.1% on average... This is a significant finding since the default approach in prior works has been to merge same-speaker diarization outputs."
  - [section 4.1]: Fine interleaving induces lower Reverse-KL divergence between text-conditioned and audio-conditioned distributions.

### Mechanism 2: Synthetic Data for Domain Coverage
- **Claim**: Augmenting web-crawled audio with synthetic speech-text pairs (derived from high-quality text via TTS) improves Spoken Question-Answering (SQA) by correcting topic skew.
- **Mechanism**: Web-crawled audio (e.g., podcasts) is heavily skewed toward "entertainment" and "sports." Synthetic datasets (e.g., "Quest") oversample "science," "health," and "finance," reducing the distribution mismatch between pretraining data and downstream evaluation benchmarks.
- **Core assumption**: Text-to-Speech (TTS) systems (like `melo-TTS` used here) produce audio features that map effectively to the semantic space of the LLM backbone without introducing significant artifacts that degrade learning.
- **Evidence anchors**:
  - [abstract]: "how to construct synthetic datasets to augment web-crawled data."
  - [section 3.4]: "We build two synthetic datasets... Knowledge-Rich Interleaved Speech-Text (Krist) and Question-Answering Speech-Text (Quest)."
  - [section 4.2]: "Web-crawled data is highly skewed... Synthetic data improves topic coverage."

### Mechanism 3: Deterministic Modality Sampling
- **Claim**: Enforcing a deterministic alternation of speech and text chunks during training (e.g., Audio, Text, Audio, Text) improves SQA performance over stochastic sampling.
- **Mechanism**: Deterministic sampling maximizes the expected number of modality switches per sequence (n-1 for deterministic vs. n/2 for stochastic), effectively increasing the density of cross-modal attention training.
- **Core assumption**: The benefits of increased cross-modal boundary exposure outweigh the potential loss of data diversity caused by a fixed sampling structure.
- **Evidence anchors**:
  - [abstract]: "deterministic modality sampling improve SQA performance."
  - [section 3.5]: "Deterministic sampling boosts SQA performance by 1% on average... [Stochastic] restricts the number of modality switches."

## Foundational Learning

- **Concept: Speaker Diarization**
  - **Why needed here**: The paper relies on splitting raw audio into speaker turns to create interleaved chunks. Without understanding diarization (segmenting audio by speaker identity), one cannot implement the "Fine-grained Interleaving" mechanism (Section 3.3) which drives the performance gains.
  - **Quick check question**: Given a 10-minute audio file with two speakers, can you identify the timestamp boundaries where the speaker changes?

- **Concept: Finite Scalar Quantization (FSQ)**
  - **Why needed here**: The SpeLangy architecture uses an FSQ layer in the speech tokenizer (Section 3.2) to convert continuous audio frames into discrete tokens that the LLM can process. Understanding FSQ is necessary to diagnose tokenization bottlenecks.
  - **Quick check question**: How does FSQ differ from standard Vector Quantization (VQ) in terms of codebook lookup?

- **Concept: Reverse KL Divergence**
  - **Why needed here**: Section 4.1 uses Reverse KL Divergence to measure the "Modality Gap." The paper argues that their methods work because they minimize the divergence between the output distribution generated from audio input versus text input.
  - **Quick check question**: Does low Reverse KL divergence imply that the audio-conditioned output distribution is "collapsing" toward the text-conditioned distribution, or that they are mutually aligning?

## Architecture Onboarding

- **Component map**: Raw Audio -> Speech Encoder (Conformer blocks) -> FSQ (Discrete Tokens) -> LLM Backbone (3B Transformer)
- **Critical path**:
  1. **Data Curation**: Processing raw audio into fine-grained (A, T) pairs (Figure 8)
  2. **Tokenization**: Mapping audio chunks to discrete tokens via the frozen FSQ tokenizer
  3. **Training**: Next-token prediction on interleaved sequences using Deterministic Sampling (Section 3.5)

- **Design tradeoffs**:
  - **Chunking Granularity**: Coarse chunks are computationally cheaper to manage but reduce modality alignment. Fine chunks (Section 3.3) require more preprocessing (diarization accuracy) but yield +3.1% SQA
  - **Loss Masking**: Training with loss masking on audio tokens (understanding-only) yields higher absolute SQA (51.8%) than training on both (42.4%) for small models (Section 3.6), but limits generative capabilities

- **Failure signatures**:
  - **High Modality Gap**: If the model ignores audio inputs and outputs generic text, check the Reverse-KL divergence (Section 4.1); the interleaving strategy may have failed to align distributions
  - **Topic Skew**: If the model performs well on conversational topics but fails on "Science" or "Finance," verify the mixing ratio of the "Krist" or "Quest" synthetic datasets (Section 4.2)

- **First 3 experiments**:
  1. **Interleaving Ablation**: Train a tiny model (e.g., 300M params) on 10k hours of data using Coarse vs. Fine interleaving to reproduce the alignment signal locally before scaling to 3.8B
  2. **Modality Switch Count**: Visualize the "number of modality switches" in a batch using the Deterministic vs. Stochastic sampler to confirm the "density of switches" hypothesis (Section 3.5, Figure 4)
  3. **Tokenization Integrity**: Verify the FSQ tokenizer does not collapse distinct acoustic features into the same discrete token, which would negate the benefits of fine-grained interleaving

## Open Questions the Paper Calls Out

- **Open Question 1**: How can one construct an optimal training mixture of web-crawled and synthetic data that accounts for complex interactions between mixing ratios and data repeats?
  - **Basis in paper**: [explicit] Section 3.4 notes that due to "complex interactions between mixing ratios and data repeats," it remains "unclear how to construct an optimal mixture extracting the best of each data source."
  - **Why unresolved**: The authors observed performance trade-offs when adjusting the proportions of web-crawled, Krist, and Quest datasets but did not establish a definitive rule for the optimal ratio.
  - **What evidence would resolve it**: A systematic ablation study correlating specific mixing ratios and epoch counts with downstream SQA performance across multiple model scales.

- **Open Question 2**: Do the identified data-centric interventions (fine-grained interleaving, synthetic data) designed for understanding tasks transfer effectively to end-to-end audio generation tasks?
  - **Basis in paper**: [inferred] Section L hypothesizes that curation strategies for understanding-only models (audio-in, text-out) might differ significantly from generation tasks (audio-in, audio-out) and calls for testing in the "full end-to-end evaluation setting."
  - **Why unresolved**: The paper primarily evaluates Spoken Question-Answering (SQA) where the output is text, leaving the impact on audio generation capabilities untested.
  - **What evidence would resolve it**: Evaluating SpeechLMs trained with these recipes on standardized audio-in/audio-out benchmarks to measure prosody, speaker consistency, and acoustic correctness.

- **Open Question 3**: How can researchers effectively detect and quantify train-test contamination specifically within the audio modality?
  - **Basis in paper**: [explicit] Section K.4 states that current text-based contamination detection is a "reasonable proxy" but fails for web-crawled audio where transcriptions are noisy, noting the "important research problem that warrants further research attention."
  - **Why unresolved**: Text-based n-gram matching cannot detect acoustic contamination or hallucinated transcriptions, leaving a blind spot in evaluating data integrity.
  - **What evidence would resolve it**: The development of audio-specific similarity metrics or detection algorithms that can identify acoustic overlaps between training sets and evaluation benchmarks without relying on transcriptions.

## Limitations

- **Internal Dataset Dependencies**: The core ablation studies rely on Apple's internal >10M hour web-crawl dataset, making exact replication difficult without access to this data or finding functionally equivalent alternatives.
- **Architecture Attribution Gaps**: The paper references specific model components (3B Base-LM, Conformer+FSQ tokenizer) as coming from Apple internal work without full specification, creating uncertainty about whether substituting comparable open models will yield identical performance patterns.
- **Scaling and Transfer Questions**: While the paper demonstrates significant gains on SQA tasks, it's unclear whether the data-centric improvements would scale proportionally to larger models or transfer to different downstream tasks beyond question-answering.

## Confidence

**High Confidence**: The data-centric improvements (fine-grained interleaving, deterministic sampling, synthetic data mixing) produce measurable gains on SQA benchmarks. This is supported by direct ablation experiments and consistent with the mechanistic explanations provided.

**Medium Confidence**: The synthetic data's effectiveness stems specifically from domain coverage correction. While the paper shows topic distribution analysis, the causal link between synthetic data's topic composition and performance gains on specific benchmark domains could be more directly established.

**Low Confidence**: The exact architectural details of the SpeechLM backbone (beyond the general Conformer+LLM structure) are not fully specified, making it difficult to assess whether reported performance is entirely attributable to data interventions versus architectural advantages.

## Next Checks

1. **Cross-Dataset Replication**: Apply the fine-grained interleaving and deterministic sampling techniques to a public speech dataset (e.g., GigaSpeech) to verify that the performance gains generalize beyond the internal web-crawl data.

2. **Architecture Substitution Test**: Replace the internal 3B Base-LM with an open alternative (e.g., Qwen-2.5-3B) while keeping all data processing techniques constant, to isolate the impact of data interventions from architectural advantages.

3. **Downstream Task Expansion**: Evaluate the pretrained SpeLangy model on non-SQA speech-language tasks (e.g., speech translation, speech summarization) to assess whether the data-centric improvements transfer beyond the primary benchmark tasks.