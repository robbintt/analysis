---
ver: rpa2
title: Assessment of L2 Oral Proficiency using Speech Large Language Models
arxiv_id: '2505.21148'
source_url: https://arxiv.org/abs/2505.21148
tags:
- speech
- language
- assessment
- performance
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of speech large language models
  (LLMs) for L2 oral proficiency assessment. It compares various training strategies
  using regression and classification targets to predict holistic scores from speech
  inputs.
---

# Assessment of L2 Oral Proficiency using Speech Large Language Models

## Quick Facts
- arXiv ID: 2505.21148
- Source URL: https://arxiv.org/abs/2505.21148
- Reference count: 0
- This paper shows speech LLMs outperform previous baselines on L2 oral assessment, achieving Pearson correlation up to 0.954.

## Executive Summary
This paper investigates speech large language models for L2 oral proficiency assessment, comparing various training strategies including regression and classification approaches. The study demonstrates that speech LLMs outperform previous baselines on two datasets, achieving superior performance with Pearson correlation coefficients up to 0.954. The model shows strong generalization capabilities across different task types, facilitated by audio understanding knowledge acquired during LLM pre-training.

## Method Summary
The approach uses Qwen2-Audio-7B-Instruct with LoRA adapters (rank 16) to adapt the model for L2 scoring tasks. Audio inputs are processed directly through the speech encoder, concatenated with CEFR-descriptor prompts, and decoded using soft classification. Training employs Fair Average loss with soft decoding, optimized for 2 epochs with learning rate 1e-4 and cosine scheduling. For audio longer than 30 seconds, the input is split into chunks with predictions averaged. The best configuration achieves PCC 0.954 on Linguaskill and 0.919 on Speak & Improve Corpus.

## Key Results
- Speech LLM achieves PCC 0.954 on Linguaskill dataset, outperforming wav2vec2 (0.847) and BERT (0.888) baselines
- Classification-based training with fair average loss outperforms regression approaches across both datasets
- Model demonstrates strong generalization from read-aloud to spontaneous speech tasks without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
Classification-based training with fair average loss outperforms regression for LLM-based scoring tasks. By treating proficiency scoring as multi-class classification, the model stays aligned with pre-training objectives of predicting discrete tokens. The fair average loss allows soft targets for intermediate scores by computing weighted averages across class predictions, preserving annotation granularity. Evidence shows fa with soft decoding achieves PCC 0.892 vs. regression at 0.890 on LinGen, with faster convergence for classification approaches.

### Mechanism 2
Speech LLM pre-training provides transferable audio understanding enabling cross-part and cross-task generalization without task-specific training. The pre-trained Qwen2-Audio model has learned general audio representations from large-scale speech data. When fine-tuned on one task type (e.g., read-aloud), these representations transfer to other task types (e.g., spontaneous speech) because underlying acoustic-linguistic features relevant to proficiency assessment are shared. Table 4 shows a grader trained only on read-aloud achieves PCC 0.911, substantially outperforming wav2vec2 (0.847) and BERT (0.888) baselines.

### Mechanism 3
End-to-end speech input prevents information loss inherent in cascaded ASR-to-text pipelines. Cascaded systems discard acoustic information during transcription, while speech LLMs process audio directly, preserving proficiency-relevant signals like pronunciation errors, disfluencies, and prosody. The paper notes this eliminates information loss that occurs in cascaded systems and addresses the limitation of text-based models that cannot explicitly capture acoustic-related information.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed - enables efficient adaptation of 7B-parameter model by adding only 10M new parameters. Quick check - Can you explain why LoRA modifies projection weights rather than freezing the entire model?
- **CEFR (Common European Framework of Reference)**: Why needed - prompt design maps outputs to CEFR descriptors (A1-C2), essential for interpreting what model is actually scoring. Quick check - What proficiency constructs differentiate a B1 speaker from a B2 speaker?
- **Soft vs. hard decoding for classification**: Why needed - paper shows soft decoding outperforms hard decoding, particularly for handling boundary cases. Quick check - Why might argmax produce worse calibrated scores than probability-weighted average?

## Architecture Onboarding

- **Component map**: Audio preprocessing (chunk to ≤30s if needed) -> Encode via speech encoder -> Project to LLM space -> Concatenate with text prompt -> LLM forward pass with LoRA-adapted weights -> Decode predictions (soft or hard) -> Map to numeric scores -> Average scores across chunks for long audio

- **Critical path**: 1. Audio preprocessing (chunk to ≤30s if needed) -> 2. Encode via speech encoder -> 3. Project to LLM space -> 4. Concatenate with text prompt -> 5. LLM forward pass with LoRA-adapted weights -> 6. Decode predictions (soft or hard) -> 7. Map to numeric scores -> 8. Average scores across chunks for long audio

- **Design tradeoffs**: Classification vs. regression - classification aligns with pre-training but discretizes scores; regression is more flexible but adds parameters and misaligns with pre-training. Hard vs. soft decoding - hard is simpler but loses granularity; soft preserves uncertainty but requires probability calibration. Training data scope - part-specific training may overfit to task format; cross-part training improves generalization but requires more diverse data.

- **Failure signatures**: Position bias - zero-shot model defaults to predicting "C" and "D" labels disproportionately. Chunk boundary artifacts - splitting long audio may introduce scoring inconsistencies at boundaries. Overfitting to prompt format - model may learn prompt-specific patterns rather than general proficiency features.

- **First 3 experiments**: 1. Baseline establishment - evaluate zero-shot classification with both hard and soft decoding on target test parts to measure pre-training transfer and identify positional biases. 2. Training objective comparison - train identical LoRA-adapted models with CE loss, FA loss, and regression targets; compare convergence speed and final PCC/RMSE. 3. Cross-part generalization test - train on most data-rich task part and evaluate on all other parts to assess transfer before committing to full multi-part training.

## Open Questions the Paper Calls Out

- How can speech LLMs be leveraged to provide explanatory feedback on the reasoning behind assigned proficiency scores? The paper notes future work will explore the model's emergent capabilities for L2 scoring, such as providing feedback on the reasoning behind its assigned scores.

- To what extent does the 30-second chunking strategy for long-form speech degrade the assessment of discourse coherence? The paper notes that for long utterances, audio is split into two 30-second chunks which are scored independently and averaged, potentially affecting cohesive devices that span across chunk boundaries.

- What specific acoustic or linguistic features enable the model to generalize from read-aloud training data to spontaneous speech assessment? The paper notes that a model trained only on read-aloud data generalizes well to spontaneous tasks, suggesting it assesses high-level features, but the specific mechanism is not isolated.

## Limitations

- Performance claims rely on private Linguaskill dataset preventing independent verification
- Paper doesn't address potential cultural or linguistic bias in CEFR-based scoring rubrics across diverse L2 learner populations
- 30-second audio chunking strategy may introduce boundary artifacts affecting scores for longer utterances

## Confidence

- **High Confidence**: The superiority of classification-based training over regression (PCC improvements of 0.002-0.003) is consistently demonstrated across both datasets with clear ablation evidence
- **Medium Confidence**: Cross-part and cross-task generalization claims (PCC 0.911-0.954) are supported by experimental results but limited by restricted task diversity in available datasets
- **Medium Confidence**: The mechanism explaining speech LLM advantages over cascaded systems is theoretically sound but lacks direct ablation studies comparing identical models with/without acoustic processing

## Next Checks

1. **Dataset Replication Test**: Reproduce the full experimental pipeline on a completely independent L2 oral assessment dataset to verify generalizability beyond the two studied corpora

2. **Acoustic Feature Ablation**: Train parallel models where the audio encoder is replaced with a text encoder processing ASR transcripts, then compare performance to the speech LLM to quantify the exact contribution of preserved acoustic information

3. **Bias and Fairness Audit**: Evaluate model predictions across different first-language backgrounds and demographic groups using available metadata to identify potential systematic biases in the proficiency scoring