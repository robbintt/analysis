---
ver: rpa2
title: Fully Decentralized Certified Unlearning
arxiv_id: '2512.08443'
source_url: https://arxiv.org/abs/2512.08443
tags:
- unlearning
- privacy
- decentralized
- rr-du
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RR-DU, a fully decentralized certified unlearning
  method for fixed communication graphs. Unlike prior approaches that require all
  clients or dynamic topologies, RR-DU uses a token-based random walk that adds Gaussian
  noise only at the unlearning client while others perform noiseless local training,
  achieving privacy amplification via network mixing and subsampling.
---

# Fully Decentralized Certified Unlearning

## Quick Facts
- arXiv ID: 2512.08443
- Source URL: https://arxiv.org/abs/2512.08443
- Reference count: 40
- One-line primary result: Token-based random walk with localized noise injection achieves certified unlearning in decentralized networks, reducing backdoor accuracy to random guessing while maintaining near-baseline clean accuracy.

## Executive Summary
This paper introduces RR-DU, a fully decentralized certified unlearning method for fixed communication graphs that overcomes the limitations of prior approaches requiring all clients or dynamic topologies. Unlike network-wide Decentralized DP (DDP) that adds noise at every hop, RR-DU uses a token-based random walk that adds Gaussian noise only at the unlearning client while others perform noiseless local training, achieving privacy amplification via network mixing and subsampling. The method combines gradient alignment via projected ascent/descent with trust region projection to stabilize convergence while preventing catastrophic unlearning. Theoretical analysis provides (ε, δ) network-unlearning certificates via subsampled Gaussian Rényi DP, convergence guarantees for convex and nonconvex objectives, and deletion-capacity bounds separating optimization variance from alignment bias.

## Method Summary
RR-DU operates on a fixed communication graph using a token that performs a random walk. At each hop, the token decides whether to visit the unlearning client u (probability p) or a random neighbor. At u, the method computes the gradient on the forget set D_f, adds Gaussian noise, and performs projected gradient ascent within a trust region around the original model to remove the sensitive data's influence. At all other clients v≠u, the method performs noiseless projected gradient descent on their local retained data. This selective noise injection preserves model utility better than network-wide DDP while the random walk and network mixing provide privacy amplification. The method requires no gradient storage and scales well with network size, achieving certified unlearning through a combination of subsampled Gaussian noise, gradient alignment, and trust region projection.

## Key Results
- RR-DU reduces backdoor accuracy to random guessing (~10%) while maintaining near-baseline clean accuracy (>85% on CIFAR-10, >99% on MNIST)
- Outperforms decentralized DP baselines on CIFAR-10 with ResNet-18, achieving significantly better trade-off between unlearning effectiveness and utility preservation
- Provides theoretical (ε, δ) = (1, 10⁻⁵) privacy guarantees via subsampled Gaussian Rényi DP composition
- Demonstrates deletion-capacity bounds that separate optimization variance from alignment bias, explaining the method's efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localizing noise injection to the unlearning client preserves model utility better than network-wide Decentralized DP (DDP).
- **Mechanism:** A token-based random walk traverses the network. Only when the token is at the unlearning client u is Gaussian noise added to the gradient update; all other clients v ≠ u perform noiseless local training. This selectively degrades the signal from the forget set while maintaining high-fidelity updates on the retained data.
- **Core assumption:** The network topology and random walk mixing provide sufficient "dilution" of the sensitive update such that a non-unlearning observer v cannot distinguish the unlearning noise from the model variance.
- **Evidence anchors:** [abstract] "adds Gaussian noise only at the unlearning client while others perform noiseless local training, achieving privacy amplification via network mixing..."; [section 4.1] "add Gaussian noise only at u, keep other users noiseless, and exploit network mixing..."; [corpus] The corpus highlights that certified unlearning typically reduces accuracy (noise injection); this method attempts to mitigate that by localizing the noise.
- **Break condition:** If the network mixing time is too slow or the graph is disconnected, the privacy amplification vanishes, and the localized noise may fail to hide the unlearning event from neighbors.

### Mechanism 2
- **Claim:** Gradient alignment via projected ascent/descent mimics retraining without requiring global coordination.
- **Mechanism:** The algorithm performs projected gradient ascent (negative gradient) on the forget set D_f at client u to remove influence, while performing standard projected descent on the retained data at other clients. This aligns the expected update direction with the gradient of the retraining objective ∇L_∖f.
- **Core assumption:** The loss function is smooth (L-smooth) and gradients are bounded, allowing the mix of ascent and descent steps to converge to a stationary point of the retraining objective rather than merely a local optimum of the original model.
- **Evidence anchors:** [abstract] "...procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps..."; [section 4.2] "Thus, moving toward the retrained optimum can be realized by standard descent on v ≠ u plus a corrective step at u..."; [corpus] Related papers (e.g., Rewind-to-Delete) rely on similar ascent-based logic to "undo" learning, though often in centralized or non-convex settings.
- **Break condition:** If the learning rate is too high or the trust region too small, the ascent step at u may destabilize convergence, causing the model to diverge from the retraining baseline.

### Mechanism 3
- **Claim:** Trust region projection prevents catastrophic unlearning (over-forgetting) and stabilizes convergence.
- **Mechanism:** Updates at the unlearning client are projected onto a trust region (a ball B(θ_ref, ϱ) around the original model). This ensures the unlearning steps do not drift the model arbitrarily far from the useful pre-unlearning state, acting as a safety rail against the noise and ascent steps.
- **Core assumption:** The optimal retrained model lies within a bounded distance of the original model, such that the trust region contains the desired solution.
- **Evidence anchors:** [abstract] "...combined with subsampled Gaussian noise and projection onto a trust region around the original model."; [section 6.3] Table 14 shows that small trust region radii (ϱ) lead to high backdoor accuracy (under-unlearning), while large radii allow effective removal.; [corpus] Certified Unlearning for Neural Networks often faces optimization challenges where unlearning degrades utility; trust regions are a common stabilization technique.
- **Break condition:** If the radius ϱ is set too small, the model cannot move far enough to "forget" the data (high residual backdoor accuracy); if too large, it loses the stability benefit.

## Foundational Learning

- **Concept: Network Differential Privacy (Network DP)**
  - **Why needed here:** Standard DP assumes a central curator. This paper operates in a decentralized graph where no single node sees all messages. You must understand "views" O_v (what a specific node observes) to interpret the privacy guarantees (ε, δ).
  - **Quick check question:** If client A receives a token from client B, does A see the raw update from B, or a mixed/processed version? (This determines the "view").

- **Concept: Rényi Differential Privacy (RDP)**
  - **Why needed here:** The paper uses RDP for composition and subsampling amplification (Theorem 5.1). RDP provides tighter bounds for Gaussian mechanisms than standard (ε, δ)-DP accounting, which is critical for the "certified" aspect.
  - **Quick check question:** Why would one use Rényi divergence over standard epsilon-delta composition when adding Gaussian noise repeatedly? (Answer: Tighter bounds for Gaussian noise).

- **Concept: Deletion Capacity**
  - **Why needed here:** This metric quantifies how much data can be forgotten before the utility degrades beyond a threshold. It formalizes the trade-off: you cannot delete infinite data for free.
  - **Quick check question:** If you increase the forget set size m, does the required noise scale linearly (DDP) or stay constant (RR-DU)? (Answer: This is the key distinction in the paper).

## Architecture Onboarding

- **Component map:** Token (model parameters) performs random walk; Unlearning Client u (holds forget set D_f) performs noisy projected ascent + trust region projection; Non-Unlearning Clients v (hold retained data) perform noiseless projected descent; Routing Logic (Bernoulli trial with probability p to visit u, else uniform random neighbor).

- **Critical path:**
  1. Initialize: Start with pre-trained model θ_0.
  2. Route: Decide if token goes to u (prob p) or random v.
  3. Update: If u: compute gradient on D_f, add Gaussian noise N(0, σ²I), project to trust region, ascend; If v: compute gradient on D_v, project to feasible set, descend.
  4. Loop: Repeat for T_u hops.

- **Design tradeoffs:**
  - Routing Probability p: High p ensures faster forgetting but risks over-unlearning (utility collapse). Low p preserves utility but slows forgetting. (Paper suggests p ≈ 0.1 is a "sweet spot").
  - Noise Scale σ: Must be calibrated to the privacy budget ε. Lower ε (stronger privacy) requires higher σ, degrading utility.
  - Trust Region Radius ϱ: Too small = unlearning failure; too large = instability.

- **Failure signatures:**
  - Symptom: Clean accuracy drops significantly.
    - Likely Cause: Trust region too large, noise scale too high, or p is too high (continuous unlearning mode).
  - Symptom: Backdoor/Forget accuracy remains high (e.g., >20%).
    - Likely Cause: Trust region radius ϱ is too restrictive, preventing the model from moving away from the backdoor manifold.

- **First 3 experiments:**
  1. Baselines Comparison: Compare RR-DU against DDP and Fine-tuning on CIFAR-10 with a Backdoor (BadNets). Verify if RR-DU achieves ~10% backdoor accuracy while maintaining >85% clean accuracy (matching Table/Fig 1).
  2. Routing Probability Ablation: Vary p (e.g., 0.0, 0.1, 1.0) on MNIST. Confirm that p=0 fails to forget and p=1.0 destroys clean accuracy (Table 3).
  3. Privacy/Utility Trade-off: Vary ε (e.g., 0.1 to 10) and plot the resulting Backdoor Accuracy. Verify that tighter privacy (low ε) naturally coincides with better forgetting (Table 16).

## Open Questions the Paper Calls Out
1. Can the view-based amplification and certification framework be extended to gossip protocols and dynamic network topologies? (Basis: conclusion states "Future work will extend our analysis to gossip and dynamic topologies...")
2. How does RR-DU scale when multiple clients issue unlearning requests concurrently or sequentially? (Basis: conclusion lists "multiple concurrent deletions" as target for future work)
3. How does data heterogeneity (non-i.i.d. data) affect the alignment bias and the resulting deletion capacity bounds? (Basis: conclusion identifies "non-i.i.d. regimes" as limitation and future direction)
4. Is the certified unlearning guarantee robust against malicious (Byzantine) clients who deviate from the protocol? (Basis: limitations section assumes "honest-but-curious observers")

## Limitations
- The method assumes a fixed communication graph with good mixing properties; performance on poorly connected or dynamic topologies is not validated
- The algorithm is designed for single unlearning requests and does not address concurrent or sequential deletion requests from multiple clients
- Experiments use largely i.i.d. data distributions, leaving open how non-i.i.d. data heterogeneity affects the alignment bias and deletion capacity

## Confidence
- **High Confidence:** The theoretical framework for network differential privacy, Rényi differential privacy composition, and the deletion capacity analysis are well-established and rigorously presented.
- **Medium Confidence:** The experimental results demonstrating the method's effectiveness on MNIST and CIFAR-10 are compelling, but reproducibility is hindered by unspecified architectural details and the need for precise hyperparameter tuning.
- **Low Confidence:** The claims about the method's scalability to large networks and its robustness to different graph topologies are not directly validated in the experiments, which are limited to a small, complete graph.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary the trust region radius ϱ, noise scale σ, and routing probability p across a wider range of values to map the full trade-off frontier between privacy (ε), unlearning effectiveness (backdoor accuracy), and utility (clean accuracy).
2. **Graph Topology Robustness:** Evaluate the method on different network topologies (e.g., line graph, ring, grid) with varying degrees of connectivity and mixing times to assess the impact of network structure on privacy amplification and unlearning performance.
3. **Real-world Data Generalization:** Test the method on a real-world federated learning dataset (e.g., FEMNIST or a production dataset) with naturally occurring data distributions and potential data drift to validate its practical applicability beyond the controlled backdoor injection scenario.