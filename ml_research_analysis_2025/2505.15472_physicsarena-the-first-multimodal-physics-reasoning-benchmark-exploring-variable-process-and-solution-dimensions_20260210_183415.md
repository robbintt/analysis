---
ver: rpa2
title: 'PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable,
  Process, and Solution Dimensions'
arxiv_id: '2505.15472'
source_url: https://arxiv.org/abs/2505.15472
tags:
- physics
- process
- reasoning
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PHYSICS ARENA is the first multimodal physics reasoning benchmark
  designed to evaluate large language models across three critical dimensions: variable
  identification, physical process formulation, and solution derivation. The benchmark
  addresses the limitation of existing physics benchmarks by requiring models to reason
  through intermediate steps rather than just providing final answers.'
---

# PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions

## Quick Facts
- **arXiv ID:** 2505.15472
- **Source URL:** https://arxiv.org/abs/2505.15472
- **Reference count:** 30
- **Primary result:** First benchmark decomposing physics reasoning into Variable Identification, Process Formulation, and Solution Derivation stages, revealing current MLLMs achieve only 33.47% accuracy on solution derivation.

## Executive Summary
PHYSICS ARENA introduces a novel multimodal physics reasoning benchmark that evaluates large language models across three critical dimensions: identifying physical variables from diagrams, formulating the physical processes involved, and deriving solutions through step-by-step reasoning. Unlike existing physics benchmarks that only provide final answers, this benchmark requires models to reason through intermediate steps with structured JSON outputs. The benchmark comprises over 5,000 problems covering high-school level physics topics across three difficulty levels, using images and text to create realistic problem scenarios.

The benchmark reveals significant gaps in current MLLM capabilities, with accuracy dropping substantially from variable identification to solution derivation stages. Experiments with state-of-the-art models including GPT-4o, Claude 3.5 Sonnet, and various Qwen and InternVL models show that even the best models achieve only modest performance, particularly struggling with visual grounding of physical entities and their relationships. The structured evaluation approach using GPT-4o as an automated judge enables granular assessment of reasoning quality beyond simple answer matching.

## Method Summary
The benchmark evaluates MLLMs across three dimensions: (1) Variable Identification with 6 categories (Entity, Geometry, Field, Structure, Connection, External Influence), (2) Process Formulation with 5 descriptors (Entity State, Process Detail, Force & Energy, State Change, Process Relation), and (3) Solution Derivation requiring step-by-step reasoning chains. The evaluation uses 5,103 multimodal physics problems with images and text, distributed across Easy (2,077), Medium (1,847), and Hard (1,179) difficulty levels. GPT-4o serves as an automatic judge, comparing model outputs to ground truth JSON annotations using structured prompts. Scoring is computed per dimension: Variable/Process accuracy as proportion of correct sub-components, and Solution accuracy as binary exact agreement.

## Key Results
- Current models achieve only 33.47% accuracy on Solution Derivation, with accuracy dropping significantly from Variable Identification to Solution stages
- GPT-4o achieves 56.82% accuracy on Variable Identification and 49.46% on Process Formulation, while best open models reach 33.47% on Solution Derivation
- Visual grounding errors are identified as the primary bottleneck, with models failing to detect components like pulleys/springs or hallucinating absent elements
- Performance correlates strongly across dimensions, with Variable Identification accuracy statistically predictive of Solution Derivation success

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Isolation for Diagnostic Precision
By decomposing physics reasoning into explicit Variable Identification, Process Formulation, and Solution Derivation stages, the benchmark isolates specific failure modes in MLLMs. The structured JSON output format forces models to extract 6 variable categories and 5 process descriptors, enabling detection of cascading errors where visual misinterpretation leads to incorrect physical laws application. The monotonic drop in accuracy from Variable to Solution stages suggests early-stage visual grounding failures propagate through the reasoning pipeline.

### Mechanism 2: Vision-Language Stress Testing via Dense Diagrams
The benchmark's 5,103 images containing high-context static diagrams (pulleys, fields, springs) expose misalignment between visual encoders and reasoning backbones. Models must ground entities to physical variables, with even advanced models like GPT-4o struggling with vision-heavy categories (Entity, Geometry) compared to Claude. This creates a rigorous stress test of visual encoder quality and its alignment with the language model.

### Mechanism 3: Automated Granular Judgement via LLM-as-Judge
Using GPT-4o to judge structured intermediate steps enables scalable, granular evaluation that multiple-choice accuracy misses. The judge compares model-extracted variables and processes against ground truth JSONs, assigning boolean scores for specific components. This correlates higher with reasoning quality than exact answer matching, though it introduces potential judge bias as a limitation.

## Foundational Learning

- **Concept:** Vision Encoder (ViT) Alignment
  - **Why needed here:** The paper explicitly links performance gaps to visual encoder quality (InternViT vs Qwen2ViT) and its alignment with the LLM. Understanding how visual tokens map to "Entity" or "Geometry" is crucial for interpreting Variable Identification results.
  - **Quick check question:** Can you explain the difference between a "visual token" and a "text embedding" in an MLLM, and why a misalignment might cause a model to hallucinate a pulley?

- **Concept:** Chain-of-Thought (CoT) vs. Structured Reasoning
  - **Why needed here:** PhysicsArena moves beyond simple CoT to structured reasoning with JSON outputs for variables/processes. Forcing models to output specific fields (e.g., `force_balance`, `state_change`) constrains the reasoning path differently than open-ended text.
  - **Quick check question:** How does forcing a model to declare "Initial Physical State" before "Process Detail" potentially reduce hallucination errors?

- **Concept:** Statistical Correlation in Error Analysis
  - **Why needed here:** The paper uses Pearson correlation to prove that accuracy in "Variable Identification" is statistically predictive of "Solution Derivation" success.
  - **Quick check question:** If the Pearson correlation between Variable Identification and Solution Derivation was low (e.g., 0.1), what would that imply about the nature of the reasoning failures?

## Architecture Onboarding

- **Component map:** Input Layer (Image, Text) -> Processing Layer (Target MLLM) -> Extraction Interface (JSON Schema Enforcement) -> Evaluation Layer (Judge Model + Ground Truth JSON)
- **Critical path:** 1) Data Ingestion: Ensure images maintain physics diagram readability, 2) Prompting: Strict adherence to JSON extraction prompts, 3) Judgement: Boolean logic of the Judge is the single point of failure
- **Design tradeoffs:** Static vs Dynamic (static images easier to grade but miss temporal reasoning), LLM-as-Judge vs Rule-based (flexible semantic matching vs potential judge bias)
- **Failure signatures:** Hallucinated Entity (high text accuracy but FALSE on Entity/Connection), Process Collapse (correct variables but fail to link in Process Relations)
- **First 3 experiments:** 1) Baseline Run: Evaluate InternVL2.5-2B on 50 Easy samples to verify pipeline, 2) Ablation on Modality: Run problems with blank images to quantify Visual Delta, 3) Correlation Check: Plot Variable vs Solution Accuracy for Hard set to verify monotonic drop pattern

## Open Questions the Paper Calls Out

- **Question:** To what extent does the GPT-4o-based automatic evaluator align with human expert assessments in grading complex physics reasoning chains?
  - **Basis in paper:** Explicit mention in Limitations that while scalable, the GPT-4o judge "can sometimes miss subtle nuances or exhibit unforeseen biases" compared to human experts.
  - **Why unresolved:** The benchmark relies entirely on this automated judge for novel Variable and Process dimensions, but correlation with expert human scoring remains unverified.
  - **What evidence would resolve it:** Large-scale comparative study between GPT-4o evaluation scores and blind human expert evaluations on intermediate reasoning steps.

- **Question:** How does MLLM performance on physics reasoning change when evaluated on dynamic multimodal inputs (e.g., videos) rather than static diagrams?
  - **Basis in paper:** Explicit statement in Limitations that current visual inputs are "primarily static diagrams," whereas real-world physics involves interpreting "dynamic scenarios" and "temporal changes."
  - **Why unresolved:** The current benchmark only assesses spatial reasoning and static interpretation, leaving temporal reasoning completely unexplored.
  - **What evidence would resolve it:** Extending the dataset to include video clips or simulations and comparing model accuracy on temporal reasoning tasks against static baseline.

- **Question:** Why does solution derivation accuracy plateau or decline for largest open-source models (72B vs 32B parameters) without task-specific fine-tuning?
  - **Basis in paper:** Section 4.3.5 notes counter-intuitive scaling trend where accuracy plateaus or drops at Large scale (72B), attributed to challenging task nature but not definitively explained.
  - **Why unresolved:** Standard scaling laws suggest larger models should perform better; the drop-off implies parameter count alone is insufficient without specialized training data or architectural adjustments.
  - **What evidence would resolve it:** Ablation studies testing various training regimes (physics-specific fine-tuning vs general pre-training) on 72B models to identify data quality or architecture bottlenecks.

## Limitations

- The evaluation relies on GPT-4o as automated judge, introducing potential bias in assessing physics reasoning quality
- Dataset access remains unclear - the 5,103 problems and ground truth annotations are not publicly available
- Model-specific hyperparameters (temperature, max tokens) were not specified, potentially affecting reported accuracy comparisons

## Confidence

**High Confidence:** The three-dimensional framework is well-defined and logically coherent; monotonic accuracy drop across dimensions is consistently observed.

**Medium Confidence:** Pearson correlation analysis appears statistically sound but correlation strength isn't explicitly quantified; visual grounding bottleneck claim supported by error analysis but needs more ablation studies.

**Low Confidence:** Assertion that PHYSICS ARENA is "first" multimodal physics benchmark requires broader literature review; comparative performance claims may be influenced by unstated evaluation parameters.

## Next Checks

1. **Judge Reliability Test:** Run same problems through multiple LLM judges (GPT-4o, Claude 3.5, Qwen2.5) and measure inter-judge agreement rates to quantify scoring consistency.

2. **Visual Ablation Study:** Evaluate text-only versions of physics problems (without images) to quantify true contribution of visual understanding versus text-based reasoning.

3. **Correlation Validation:** Compute and report exact Pearson correlation coefficient values between Variable Identification accuracy and Solution Derivation accuracy for each difficulty level, testing whether relationship holds across all problem categories.