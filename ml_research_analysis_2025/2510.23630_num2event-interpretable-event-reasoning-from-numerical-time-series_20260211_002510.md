---
ver: rpa2
title: 'NUM2EVENT: Interpretable Event Reasoning from Numerical time-series'
arxiv_id: '2510.23630'
source_url: https://arxiv.org/abs/2510.23630
tags:
- event
- numerical
- time-series
- events
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for reasoning about events
  directly from numerical time-series data when textual context is unavailable. The
  authors propose a pipeline combining an agent-guided event extractor, a Hawkes-process-based
  synthetic data generator, and a two-stage fine-tuning procedure to map time-series
  patterns to interpretable structured events (Actor-Action-Object-Direction).
---

# NUM2EVENT: Interpretable Event Reasoning from Numerical time-series

## Quick Facts
- arXiv ID: 2510.23630
- Source URL: https://arxiv.org/abs/2510.23630
- Authors: Ninghui Feng; Yiyan Qi
- Reference count: 29
- This paper introduces a novel framework for reasoning about events directly from numerical time-series data when textual context is unavailable.

## Executive Summary
This paper addresses the challenge of interpreting events from numerical time-series data when textual context is unavailable. The authors propose NUM2EVENT, a framework that combines an agent-guided event extractor, a Hawkes-process-based synthetic data generator, and a two-stage fine-tuning procedure to map time-series patterns to interpretable structured events (Actor-Action-Object-Direction). The method demonstrates significant improvements over strong LLM baselines on real-world datasets, achieving up to 71.3% precision and 35.5% recall on an energy dataset and 20.0% precision and 23.9% recall on a public health dataset.

## Method Summary
NUM2EVENT employs a two-stage pipeline to map numerical time-series patterns to structured events. First, an Agent-Guided Event Extractor (AGE) extracts Actor-Action-Object-Direction (AAOD) events from historical text using restricted vocabularies. Second, a Hawkes-process-based synthetic data generator (EveDTS) creates paired numeric-event samples to compensate for scarce real-world annotations. The framework then trains a time-series encoder on these paired samples, freezes the encoder, and fine-tunes a language model decoder (Qwen-3-8B-Instant with LoRA) to generate structured AAOD sequences and reasoning traces from numerical inputs.

## Key Results
- Outperforms strong LLM baselines by significant margins on both energy and public health datasets
- Achieves up to 71.3% precision and 35.5% recall on the energy dataset
- Achieves up to 20.0% precision and 23.9% recall on the public health dataset
- Demonstrates the effectiveness of synthetic data generation and two-stage fine-tuning for number-to-event reasoning

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Generation via Hawkes Processes
Synthetic paired data from Hawkes processes with impulse response functions compensates for scarce real-world number-to-event annotations. The EveDTS module models event arrivals via multivariate Hawkes processes and links each event type to numerical changes through local projection impulse responses, overlaid on AR-based background dynamics. Core assumption: Event-driven numerical changes in energy and public health domains can be approximated by linear impulse responses plus autoregressive noise, and synthetic patterns generalize to real inference.

### Mechanism 2: Two-Stage Fine-Tuning Pipeline
Separating representation learning (encoder-only) from semantic alignment (LLM fine-tuning) improves numerical-to-event mapping versus end-to-end training. Stage 1 trains the time-series encoder on paired numeric-event samples without LLM interference; Stage 2 freezes the encoder and fine-tunes the LLM (with LoRA) to decode structured AAOD sequences from the encoder outputs. Core assumption: Encoder representations stabilize faster and better when isolated from large LLM parameter updates, and frozen embeddings provide a reliable bridge for semantic decoding.

### Mechanism 3: Constrained AAOD Decoding with Restricted Vocabularies
Constrained AAOD decoding with restricted vocabularies (and agent-guided expansion) improves structured event consistency and interpretability. AGE extracts Actor-Action-Object-Direction slots using restricted vocabularies; agents propose new terms in a select-expand-iterate loop. The decoder generates AAOD tuples with vocabulary constraints, enabling concurrent events and reducing spurious outputs. Core assumption: Event semantics in target domains can be decomposed into AAOD slots, and vocabulary constraints reduce search space without excluding true events.

## Foundational Learning

- Concept: Hawkes processes and temporal point processes
  - Why needed here: EveDTS relies on conditional intensity functions and excitation kernels to simulate clustered event arrivals.
  - Quick check question: Can you write the conditional intensity λk(t) and explain how self-excitation differs from mutual excitation?

- Concept: Local projections and impulse response functions
  - Why needed here: The synthetic generator traces event impacts on differenced series via multi-step impulse coefficients βk,h.
  - Quick check question: What does βk,h represent in a difference-equation context, and why apply IRFs to Δy rather than levels?

- Concept: Two-stage transfer (encoder pretraining then decoder fine-tuning) with LoRA
  - Why needed here: Model training freezes the encoder after Stage 1 and uses parameter-efficient LLM adaptation in Stage 2.
  - Quick check question: Why freeze the encoder before LLM fine-tuning, and what does LoRA modify during Stage 2?

## Architecture Onboarding

- Component map: AGE (extracts AAOD from text) -> EveDTS (generates synthetic numeric-event pairs via Hawkes+IRF+AR) -> Encoder (maps numeric windows to embeddings) -> Decoder head (Qwen-3-8B-Instant generates AAOD sequences with reasoning traces)

- Critical path:
  1. Build AAOD vocabulary via AGE from historical text aligned to numeric segments.
  2. Generate synthetic numeric-event pairs via EveDTS (Hawkes+IRF+AR).
  3. Train encoder on real+synthetic pairs; freeze encoder.
  4. Fine-tune LLM decoder with reasoning-augmented supervision to output AAOD.

- Design tradeoffs:
  - Synthetic realism vs. coverage: Hawkes+IRF enables controlled patterns but may mis-specify real event dynamics.
  - Vocabulary restriction vs. expressiveness: Tight constraints aid consistency but risk missing valid events.
  - Two-stage stability vs. joint optimization: Staged training stabilizes representations but may limit cross-modal adaptation.

- Failure signatures:
  - Low recall with high precision: Vocabulary too restrictive or AGE missing event types.
  - Large synthetic-to-real performance gap: IRF/Hawkes assumptions violated in target domain.
  - Unstable decoding: Encoder representations under-trained or LoRA rank too low for semantic alignment.

- First 3 experiments:
  1. Reproduce main results on Energy and Public Health using the two-stage pipeline; verify monthly-averaged precision/recall and the 3-of-4 AAOD match criterion.
  2. Ablate EveDTS by training only on real aligned data; quantify recall drop and analyze which event types lose coverage.
  3. Swap the base LLM (e.g., Qwen vs. a smaller model) with identical encoder; measure precision/recall to isolate encoder vs. decoder contributions.

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the framework's performance to violations of the statistical assumptions (linear impulse responses and Hawkes self-excitation) used in the EveDTS synthetic data generator? The paper validates the method on specific real-world datasets but does not test performance boundaries when the synthetic prior misaligns with the true data generation process.

### Open Question 2
To what extent does error propagation from the Agent-Guided Event Extractor (AGE) limit the ceiling of event decoding accuracy? The framework relies on AGE to extract "ground truth" events from historical text for supervision, yet LLM-based extraction is known to produce hallucinations or inconsistencies.

### Open Question 3
Does the "3-out-of-4" slot matching evaluation metric mask semantic errors in critical slots (e.g., Direction or Action) that are vital for decision-making? The paper defines a correct prediction as matching at least three AAOD slots, allowing for the possibility that the model gets the "Direction" or "Action" wrong while still being counted as correct.

## Limitations

- Implementation details for critical components (encoder architecture, Hawkes parameters, LoRA configuration) are not fully specified, making exact reproduction difficult
- The assumption that Hawkes processes with linear impulse responses can capture complex event-driven changes may not hold for non-linear or regime-switching domains
- AAOD vocabulary restriction may systematically exclude valid events not captured by the initial agent extraction

## Confidence

- High confidence: The two-stage training pipeline design and its benefits are well-supported by the methodology description and ablation results
- Medium confidence: The Hawkes process + IRF synthetic data generation mechanism is theoretically sound, but real-world validation is limited to the two datasets tested
- Low confidence: The AGE agent's vocabulary expansion mechanism and its ability to comprehensively capture all relevant event types across domains is not rigorously evaluated

## Next Checks

1. Ablate synthetic data impact: Train the model on real aligned data only (no EveDTS synthetic generation) and measure the change in precision/recall, particularly for event types that may require diverse synthetic patterns.

2. Vocabulary coverage analysis: Conduct an error analysis on AGE-extracted events to identify systematic omissions in the AAOD vocabulary, and test whether expanding the vocabulary improves recall without degrading precision.

3. Cross-domain robustness test: Apply the trained model to a held-out time-series dataset from a different domain (e.g., financial market indicators) to evaluate whether the Hawkes+IRF assumptions and vocabulary constraints generalize beyond energy and public health.