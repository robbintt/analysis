---
ver: rpa2
title: Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent
  Based on Minimization of Stochastic First-order Oracle Complexity
arxiv_id: '2508.05302'
source_url: https://arxiv.org/abs/2508.05302
tags:
- constant
- gradient
- mini-batch
- training
- growth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an adaptive scheduling strategy for mini-batch\
  \ stochastic gradient descent (SGD) that adjusts both batch size and learning rate\
  \ based on the decay of the full gradient norm during training. The approach is\
  \ grounded in theoretical analysis identifying a critical batch size that minimizes\
  \ stochastic first-order oracle (SFO) complexity\u2014the expected number of gradient\
  \ evaluations needed to reach a stationary point."
---

# Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity

## Quick Facts
- **arXiv ID:** 2508.05302
- **Source URL:** https://arxiv.org/abs/2508.05302
- **Reference count:** 8
- **One-line primary result:** Adaptive joint scheduler with exponentially increasing batch size and learning rate achieves faster convergence than fixed or periodically updated hyperparameters.

## Executive Summary
This paper introduces an adaptive scheduling strategy for mini-batch stochastic gradient descent (SGD) that adjusts both batch size and learning rate based on the decay of the full gradient norm during training. The approach is grounded in theoretical analysis identifying a critical batch size that minimizes stochastic first-order oracle (SFO) complexity—the expected number of gradient evaluations needed to reach a stationary point. The adaptive scheduler increases batch size and learning rate in stages, transitioning when the gradient norm falls below predefined thresholds. Experiments on CIFAR-10 and CIFAR-100 datasets using ResNet-18 and DenseNet architectures demonstrate that the adaptive joint scheduler with exponentially increasing batch size and learning rate achieves faster convergence compared to baseline schedulers with fixed or periodically updated hyperparameters. The method provides a principled mechanism for dynamic hyperparameter tuning, improving optimization efficiency without manual adjustment. Theoretical and empirical results validate the benefits of adapting batch size and learning rate in response to the optimization landscape, particularly the gradient norm, rather than relying on predetermined schedules.

## Method Summary
The method implements an adaptive mini-batch SGD (Algorithm 2) with two schedulers: (1) Linear batch size growth with constant learning rate, and (2) Exponential growth for both batch size and learning rate. The scheduler transitions between stages when the full gradient norm falls below predefined thresholds, scaling hyperparameters according to theoretical bounds on critical batch size. The approach estimates critical batch size through empirical SFO complexity curves, then applies exponential scaling with the constraint γ < √δ to maintain convergence stability.

## Key Results
- Exponentially increasing both batch size and learning rate achieves O(1/√(γ^T)) convergence rate, outperforming O(1/√T) rate of linear batch size growth
- Adaptive joint scheduler reduces total gradient evaluations (SFO complexity) compared to fixed hyperparameters or periodic updates
- Theoretical analysis establishes critical batch size scaling as O(1/ε²), providing foundation for adaptive scheduling
- Empirical validation on CIFAR-10 and CIFAR-100 shows faster convergence and improved optimization efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adjusting batch size (BS) and learning rate (LR) based on the full gradient norm minimizes the total computational cost (SFO complexity) to reach a stationary point.
- **Mechanism:** Theoretical analysis (Proposition 2) establishes that the "Critical Batch Size" (CBS)—the point of diminishing returns for increasing BS—scales as O(1/ε²), where ε is the target gradient norm precision. By defining a schedule of decreasing precision thresholds (εₘ) and scaling the BS (bₘ) and LR (ηₘ) to match this theoretical CBS scaling, the optimizer maintains optimal efficiency throughout training stages.
- **Core assumption:** The optimization landscape allows the full gradient norm to serve as a reliable proxy for convergence progress ("distance" to a stationary point).
- **Evidence anchors:**
  - [PAGE 4] Proposition 2: "Critical BS: b*ₑ = 2C₂/ε²."
  - [PAGE 4] "The target precision in stage m is associated with the corresponding critical BS bₘ... Training begins with initial values (ε₀, b₀, η₀)."
  - [corpus] Related work *Optimal Growth Schedules for Batch Size and Learning Rate in SGD* supports the general principle that specific growth schedules reduce SFO complexity.
- **Break condition:** If the full gradient norm oscillates or plateaus without falling below the threshold εₘ, the scheduler will stall in the current stage and fail to trigger the necessary hyperparameter updates.

### Mechanism 2
- **Claim:** Exponentially increasing both BS and LR yields faster convergence rates than increasing BS alone or using fixed hyperparameters.
- **Mechanism:** Proposition 1(iii) shows that exponentially increasing BS (bₘ = b₀δᵐ) and LR (ηₘ = η₀γᵐ) achieves a convergence rate of O(1/√(γ^T)). This outperforms the O(1/√T) rate of linear BS growth with constant LR. Increasing the LR (γ > 1) effectively amplifies the step size as the gradient diminishes, while the increasing BS (δ) controls the variance to prevent divergence, provided γ < √δ.
- **Core assumption:** The loss function is L-smooth, and the relationship γ < √δ is strictly maintained to ensure the variance term V_T converges efficiently.
- **Evidence anchors:**
  - [PAGE 3] Eq (11-12): Analysis showing faster convergence O(1/√(γ^T)) when γ < √δ.
  - [PAGE 6] Figure 2: "Exponential growth BS and LR" decays the full gradient norm faster than "Linear growth BS... LR: constant".
  - [corpus] *Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling* discusses similar balancing acts, reinforcing the coupling mechanism.
- **Break condition:** If γ ≥ √δ (LR grows too fast relative to BS), the variance reduction from the larger batch cannot compensate for the aggressive step size, potentially causing divergence.

### Mechanism 3
- **Claim:** Stage transitions triggered by observed gradient decay adapt the schedule to the specific optimization landscape of the model.
- **Mechanism:** Unlike fixed-interval schedulers (e.g., "update every 5000 steps"), this method transitions stages (increasing BS/LR) only when ||∇f(θₜ)|| ≤ εₘ. This feedback loop ensures hyperparameter scaling aligns with actual geometric progress toward a stationary point rather than arbitrary time steps.
- **Core assumption:** Computing or estimating the full gradient norm is feasible and provides a distinct signal from the mini-batch gradient noise.
- **Evidence anchors:**
  - [PAGE 5] Algorithm 2, Line 5: "if ||∇f(θₜ)|| ≤ εₘ... m ← m + 1".
  - [PAGE 7] Figure 3/4: Comparison showing adaptive scheduling outperforming fixed-interval updates in specific scenarios.
  - [corpus] *DIVEBATCH* proposes gradient-diversity signals for adaptation; this paper chooses gradient norm, evidencing the trend of using optimization signals vs. fixed rules.
- **Break condition:** If the cost of computing the full gradient norm is prohibitive (requiring a full dataset pass), the mechanism creates a computational bottleneck that negates the efficiency gains from the adaptive schedule.

## Foundational Learning

- **Concept: Stochastic First-Order (SFO) Complexity**
  - **Why needed here:** This is the primary metric the paper optimizes. It represents the total number of gradient evaluations (Batch Size × Steps) required to reach an ε-stationary point.
  - **Quick check question:** Why does increasing batch size reduce the number of *steps* but potentially increase the total *gradient evaluations* (SFO complexity)?

- **Concept: L-Smoothness and the Descent Lemma**
  - **Why needed here:** The theoretical bounds for convergence (Proposition 1) rely on the assumption that the loss function is L-smooth. This allows the paper to derive the upper bound on the gradient norm and define the valid range for the learning rate (η < 2/L).
  - **Quick check question:** What constraint does L-smoothness place on the maximum learning rate to guarantee convergence?

- **Concept: Stationary Points in Non-Convex Optimization**
  - **Why needed here:** The paper defines convergence not as reaching a global minimum, but as finding a point where the gradient norm is below a threshold ε. The entire scheduling logic is predicated on driving ||∇f(θ)|| toward zero.
  - **Quick check question:** In non-convex landscapes, does a small gradient norm guarantee you have found a global minimum?

## Architecture Onboarding

- **Component map:** Gradient Monitor -> State Manager -> Update Logic
- **Critical path:** The check `if ||∇f(θₜ)|| ≤ εₘ` (Algorithm 2, Line 5). This requires careful implementation. Calculating the exact full gradient at every step t is expensive. An engineering approximation (e.g., calculating norm on a large held-out subset or periodically) is likely required for viability, though the paper assumes exact computation.

- **Design tradeoffs:**
  - **Exact vs. Approximate Norm:** Calculating the exact full gradient is accurate but computationally heavy (O(N)). Approximating it is faster but introduces noise into the scheduling trigger.
  - **Sensitivity to Initial Conditions:** The choice of ε₀ is critical. If too low, the model never transitions stages; if too high, it scales up BS/LR too aggressively early on.

- **Failure signatures:**
  - **Stalling:** The model trains but the gradient norm never drops below εₘ, causing it to stay at initial BS/LR indefinitely.
  - **Exploding Loss:** The LR scaling factor γ is set too high relative to the BS scaling δ (violating γ < √δ), causing instability as stages progress.

- **First 3 experiments:**
  1. **Sanity Check (CBS Verification):** Replicate Figure 1 on a smaller scale. Train with constant LR and various fixed BS to observe the "U-curve" of SFO complexity and verify the existence of a Critical BS for your specific model/dataset.
  2. **Threshold Sensitivity:** Run Algorithm 2 with varying initial ε₀ (e.g., 0.5 vs 1.0). Determine if the adaptive trigger is robust or if it requires careful tuning to start the stage progression.
  3. **Baseline Comparison:** Compare the "Exponential BS+LR" scheduler against a standard "Cosine Annealing LR + Constant BS" baseline. Measure both final accuracy and total computational cost (FLOPs or time) to validate the efficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this adaptive scheduling strategy be effectively extended to adaptive gradient methods like Adam?
- **Basis in paper:** [explicit] The Conclusion states, "Future work includes extending this approach to other optimizers (e.g., Adam)."
- **Why unresolved:** The theoretical convergence bounds (Propositions 1–3) and the relationship between critical batch size and learning rate are derived specifically for mini-batch SGD, whereas Adam utilizes adaptive moments which may alter the optimal scaling laws.
- **What evidence would resolve it:** Theoretical analysis deriving the critical batch size for Adam, or empirical experiments demonstrating convergence acceleration when applying the scheduler to Adam-based training.

### Open Question 2
- **Question:** How does the computational cost of calculating the full gradient norm impact the feasibility of this method on large-scale datasets?
- **Basis in paper:** [inferred] Algorithm 2 requires computing the full gradient norm ||∇f(θₜ)|| at every step to trigger updates, but the experiments are limited to the small CIFAR datasets.
- **Why unresolved:** Calculating the full gradient on large datasets (e.g., ImageNet) is computationally prohibitive (requiring a full pass per step), potentially negating the efficiency gains from reduced SFO complexity.
- **What evidence would resolve it:** An analysis of the wall-clock time complexity including the gradient check, or the proposal/validation of a proxy metric (e.g., a running estimate) that approximates the full gradient norm without a full dataset pass.

### Open Question 3
- **Question:** Does the adaptive joint scheduler improve convergence in non-image domains, such as Natural Language Processing?
- **Basis in paper:** [explicit] The Conclusion lists "applying it to broader training scenarios" as future work.
- **Why unresolved:** The empirical validation is restricted to image classification (CIFAR-10/100) using CNNs (ResNet, DenseNet), leaving the method's efficacy on the non-convex landscapes of Transformers or recurrent networks untested.
- **What evidence would resolve it:** Empirical results from training Transformer models (e.g., BERT, GPT) on text corpora, comparing the adaptive scheduler against standard warmup and decay schedules.

## Limitations
- The paper assumes exact computation of the full gradient norm for stage transitions, which is computationally expensive and likely approximated in practice
- The choice of initial threshold ε₀ and the relationship between δ and γ (γ < √δ) require careful tuning
- The theoretical analysis assumes L-smoothness, which may not hold for all deep learning landscapes
- The benefits are demonstrated primarily on CIFAR-10/100 with ResNet and DenseNet architectures, limiting generalizability to other tasks or models

## Confidence

- **High confidence:** The theoretical foundation linking Critical Batch Size to SFO complexity (Proposition 2) and the convergence rate analysis for exponential growth (Proposition 1)
- **Medium confidence:** The empirical superiority claims, as they depend on implementation details (gradient norm computation, exact hyperparameters) not fully specified
- **Medium confidence:** The mechanism connecting gradient norm decay to optimization progress, which assumes the full gradient norm is a reliable proxy for "distance" to stationary points

## Next Checks

1. **Critical Batch Size Verification:** Replicate the SFO complexity vs batch size experiment on CIFAR-10/ResNet-18 to empirically verify the U-curve and identify the CBS for your specific setup
2. **Gradient Norm Computation Cost:** Measure the computational overhead of exact full gradient norm calculation vs. using a large proxy batch, and determine if the efficiency gains from adaptive scheduling are negated by this cost
3. **Constraint Sensitivity:** Systematically test different γ/δ pairs (violating and satisfying γ < √δ) to empirically validate the theoretical constraint and identify the stability boundary