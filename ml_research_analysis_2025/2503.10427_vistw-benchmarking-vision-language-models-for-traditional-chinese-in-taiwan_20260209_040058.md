---
ver: rpa2
title: 'VisTW: Benchmarking Vision-Language Models for Traditional Chinese in Taiwan'
arxiv_id: '2503.10427'
source_url: https://arxiv.org/abs/2503.10427
tags:
- chinese
- vistw-mcq
- answer
- traditional
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VisTW, the first comprehensive vision-language
  benchmark for Traditional Chinese, addressing the underrepresentation of this language
  in multimodal AI evaluation. The benchmark comprises two components: VisTW-MCQ (3,795
  exam-style questions across 21 subjects) and VisTW-Dialogue (131 culturally-grounded
  dialogue pairs with Taiwanese contexts).'
---

# VisTW: Benchmarking Vision-Language Models for Traditional Chinese in Taiwan

## Quick Facts
- arXiv ID: 2503.10427
- Source URL: https://arxiv.org/abs/2503.10427
- Reference count: 39
- Primary result: First comprehensive vision-language benchmark for Traditional Chinese, revealing significant performance gaps between TC and SC VLMs

## Executive Summary
This paper introduces VisTW, the first comprehensive vision-language benchmark specifically designed for Traditional Chinese in Taiwan. The benchmark addresses the critical gap in multimodal AI evaluation for non-English and non-Simplified Chinese languages. VisTW consists of two components: VisTW-MCQ with 3,795 exam-style questions across 21 subjects, and VisTW-Dialogue with 131 culturally-grounded dialogue pairs featuring Taiwanese contexts. The authors develop an automated VLM-as-judge evaluation framework that achieves strong human correlation (ρ = 0.8466) and implement a calibration approach for evaluation continuity when judge models change.

## Method Summary
The benchmark evaluates VLMs on two Traditional Chinese tasks: VisTW-MCQ (3,795 exam-style questions across 21 subjects) using zero-shot Chain-of-Thought prompting with GPT-4o-mini parsing for non-compliant responses, and VisTW-Dialogue (131 free-form image-question pairs) using VLM-as-judge evaluation with Gemini-2.0-Flash scoring. The evaluation code is publicly available at https://github.com/TMMMU-Benchmark/evaluation. Open-weight models under 11B parameters run on local 3090 GPUs, while larger models use OpenRouter API or official APIs. The authors implement a linear calibration mechanism to preserve score continuity when judge models change, demonstrated by fitting Qwen2.5-VL-72B scores to align with Gemini-2.0-Flash.

## Key Results
- Traditional Chinese VLMs (Breeze2-3B, Breeze2-8B) underperform Simplified Chinese counterparts on Traditional Chinese visual content
- Smaller VLMs show graceful degradation in factual reasoning while struggling more with open-ended dialogue tasks
- CoT prompting acts as an amplifier: improves high-capability models but degrades low-capability models through hallucination
- Resolution degradation impacts dialogue tasks significantly more than structured exam questions
- VLM-as-judge evaluation achieves ρ = 0.8466 human correlation, with calibration improving continuity between judge models

## Why This Works (Mechanism)

### Mechanism 1: Linear Judge Calibration for Evaluation Continuity
The authors demonstrate that a high-performing open-weight model (Qwen2.5-VL 72B) produces systematically higher raw scores than the original judge (Gemini 2.0 Flash). By fitting a linear regression on score distributions, they align the new judge to the original scale, reducing the gap in mean scores. This calibration method addresses the continuity problem absent in many static benchmarks where judge models retire or change.

### Mechanism 2: Resolution Asymmetry in Task Sensitivity
VisTW-MCQ relies on diagrams and charts where text may already be prominent or reasoning can be inferred from partial visual context. VisTW-Dialogue involves real-world scenes with dense text and contextual details. Reducing resolution destroys OCR utility for Dialogue tasks faster than for MCQ tasks, as smaller text becomes unreadable while diagram-based reasoning remains partially viable.

### Mechanism 3: Scaling Law of Reasoning (CoT) vs. Hallucination
Larger models use CoT to correct initial visual misinterpretations, while smaller models often hallucinate visual details early in the chain, leading to incorrect conclusions. This occurs because the visual encoder in smaller models provides insufficient signal for reasoning steps, causing the reasoning process to amplify noise rather than refine understanding.

## Foundational Learning

- **Concept: VLM-as-Judge with Human Alignment**
  - Why needed: VisTW-Dialogue uses free-form text generation that cannot be graded by simple accuracy, requiring stronger VLMs to grade weaker ones while validating against human rankings.
  - Quick check: If your judge model gives a score of 10/10 to a factually incorrect but stylistically fluent response, which anchor is missing from your evaluation prompt?

- **Concept: Script and Cultural Grounding in Multimodal Models**
  - Why needed: Traditional Chinese and Taiwanese culture are distinct from Simplified Chinese, with tokenization and training data composition creating specific failure modes in OCR and cultural reasoning.
  - Quick check: Why would a model trained predominantly on Simplified Chinese data struggle to read a Traditional Chinese street sign, even if the underlying semantics are identical?

- **Concept: Zero-Shot Chain-of-Thought (CoT)**
  - Why needed: This is the standard evaluation prompting strategy for VisTW-MCQ, affecting latency and accuracy.
  - Quick check: Does asking a VLM to "think step by step" improve its ability to see the image, or does it only improve its ability to reason about what it has already seen?

## Architecture Onboarding

- **Component map:** VisTW-MCQ & VisTW-Dialogue datasets -> Target Model -> (MCQ: GPT-4o-mini Parser -> Accuracy Calc) OR (Dialogue: Gemini-2.0-Flash Judge -> Calibration Layer -> Final Score)

- **Critical path:**
  1. Judge Validation: Verify chosen Judge correlates with human ground truth (ρ > 0.84)
  2. MCQ Inference: Run Zero-Shot CoT on Target Model -> Parse answer string -> Compute Accuracy
  3. Dialogue Inference: Run inference on Target Model -> Pass to Judge -> Apply Calibration (if using non-standard judge)

- **Design tradeoffs:**
  - Judge Cost vs. Correlation: Gemini-2.0-Flash (36x cheaper than Claude Sonnet) provides only minor correlation drop (0.824 vs 0.833)
  - Format Constraints vs. Reasoning: Strict formatting risks parsing errors in smaller models; using LLM parser adds cost but recovers valid answers

- **Failure signatures:**
  - "Object Detection Mode": deepseek-vl2-small outputs bounding boxes instead of dialogue answers
  - CoT Collapse: Small model's score drops when using CoT vs. Direct Answer
  - Script Confusion: Generating Simplified Chinese characters when prompted for Traditional Chinese

- **First 3 experiments:**
  1. Blind Test Baseline: Run VisTW-MCQ without images to establish language prior baseline
  2. Resolution Stress Test: Downscale VisTW-Dialogue images to 1/8 resolution and verify trend maintenance
  3. Judge Calibration Check: Run Qwen2.5-VL-72B as judge and compare score distribution against Gemini-2.0-Flash

## Open Questions the Paper Calls Out

- Why does Chain-of-Thought (CoT) prompting degrade performance in lower-performing VLMs while benefiting higher-performing models? The authors hypothesize inferior vision processing introduces hallucinations during extended reasoning but have not tested this mechanism.

- Why does Gemini-2.0-flash-thinking achieve exceptional dialogue scores (6.51) but mediocre MCQ performance (0.376)? The authors document this 8-rank gap as an "intriguing observation" without explanation.

- What causes Traditional Chinese-trained VLMs (Breeze2) to underperform Simplified Chinese counterparts on Traditional Chinese visual content? The paper notes this performance gap but does not investigate training data composition or tokenization differences.

- Does incorporating images in VLM-as-judge evaluation reliably improve human correlation, contradicting prior findings? The authors observe improved correlations but acknowledge this contradicts earlier research without explaining the discrepancy.

## Limitations

- The MCQ dataset may inadvertently leak answers through text prompts, though the paper attempts to control this by providing only question text and image
- The calibration mechanism assumes linear relationships between score distributions, which may not hold if new judges exhibit non-linear scoring biases
- The CoT scaling law observation is based on aggregate performance differences rather than analyzing specific failure modes

## Confidence

- **High Confidence:** Traditional Chinese VLMs underperform Simplified Chinese counterparts is well-supported by direct performance comparisons across 31 models
- **Medium Confidence:** The calibration mechanism for judge continuity shows empirical support but relies on monotonic score distribution assumptions
- **Low Confidence:** The CoT scaling law mechanism is observational rather than causal - the paper demonstrates correlation but does not establish underlying reasons

## Next Checks

1. **Judge Bias Analysis:** Run VisTW-Dialogue evaluation using multiple judge models and perform correlation analysis against human scores to identify systematic scoring biases

2. **Answer Leakage Test:** Evaluate the same MCQ questions with and without answer options provided, measuring performance difference to quantify potential answer leakage

3. **Fine-grained Resolution Study:** Perform granular analysis of resolution impacts at intermediate scales (2x, 4x, 8x downscaling) and analyze which specific question types drive performance divergence