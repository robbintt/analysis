---
ver: rpa2
title: Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently
arxiv_id: '2511.17852'
source_url: https://arxiv.org/abs/2511.17852
tags:
- latexit
- gradient
- learning
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how transformers learn sparse Boolean functions
  through reinforcement learning (RL) and supervised fine-tuning (SFT) with Chain-of-Thought
  (CoT) supervision. The authors study k-sparse Boolean functions that can be recursively
  decomposed into 2-sparse functions using a one-layer transformer with intermediate
  supervision.
---

# Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently

## Quick Facts
- arXiv ID: 2511.17852
- Source URL: https://arxiv.org/abs/2511.17852
- Reference count: 40
- Transformers can learn k-sparse Boolean functions through both RL and SFT with CoT supervision, but with different learning dynamics

## Executive Summary
This paper provides the first theoretical analysis of transformer learnability through reinforcement learning with Chain-of-Thought supervision. The authors prove that transformers can learn k-sparse Boolean functions that can be recursively decomposed into 2-sparse functions using a one-layer transformer with intermediate supervision. Both reinforcement learning (RL) and supervised fine-tuning (SFT) can learn these functions under conditions ensuring separation of the critical gradient component. The theoretical results are verified for k-PARITY, k-AND, and k-OR functions, demonstrating that RL learns the entire CoT chain simultaneously in one update while SFT requires one update per reasoning step.

## Method Summary
The authors analyze a one-layer transformer with intermediate supervision that can learn k-sparse Boolean functions through both RL and SFT. They focus on functions that can be recursively decomposed into 2-sparse functions, using a theoretical framework that examines the gradient flow and learning dynamics. The analysis compares RL and SFT approaches by examining how each method updates the model parameters during training. RL is shown to learn the entire Chain-of-Thought reasoning chain in a single update, while SFT learns step-by-step, requiring separate updates for each reasoning step. The proofs establish conditions under which both methods can successfully learn these sparse Boolean functions.

## Key Results
- Transformers with RL and SFT can learn k-sparse Boolean functions that are recursively decomposable into 2-sparse functions
- RL learns the entire Chain-of-Thought chain simultaneously in one update, while SFT requires one update per reasoning step
- The theoretical analysis is verified for k-PARITY, k-AND, and k-OR functions
- Learning succeeds under conditions ensuring separation of the critical gradient component

## Why This Works (Mechanism)
The paper demonstrates that transformers can learn sparse Boolean functions through both RL and SFT because the functions have a recursive structure that can be captured by the transformer's attention mechanism. The key insight is that both methods can separate the critical gradient component needed for learning, though they do so with different update dynamics. RL's ability to learn the entire reasoning chain in one update comes from its holistic reward signal that captures the end-to-end performance, while SFT's step-by-step learning reflects the explicit supervision at each reasoning step. The separation of the critical gradient component ensures that the learning process is stable and converges to the correct solution.

## Foundational Learning
- **k-sparse Boolean functions**: Functions where only k input bits are relevant for determining the output; needed because they have a structured decomposition property that transformers can exploit; quick check: verify that the function can be expressed as combinations of 2-sparse functions
- **Recursive decomposition**: Breaking down complex functions into simpler 2-sparse components; needed to create a tractable learning problem for the one-layer transformer; quick check: ensure each decomposition step maintains the 2-sparse property
- **Critical gradient component**: The portion of the gradient responsible for meaningful parameter updates; needed to ensure stable and directed learning; quick check: verify gradient separation conditions hold during training
- **Chain-of-Thought supervision**: Providing intermediate reasoning steps as training targets; needed to enable step-by-step learning in SFT and structured learning in RL; quick check: confirm intermediate supervision is available for each reasoning step
- **Attention mechanism**: The transformer's core operation for aggregating information across positions; needed to implement the recursive decomposition structure; quick check: verify attention patterns align with the function's dependency structure
- **Gradient flow analysis**: Mathematical examination of how gradients propagate through the network; needed to prove learning convergence conditions; quick check: ensure gradient norms remain bounded and directed

## Architecture Onboarding
- **Component map**: Input features -> Transformer layer with attention -> Intermediate supervision targets -> Final output prediction
- **Critical path**: Input features → Attention computation → Value aggregation → Output logits → Loss computation (RL or SFT)
- **Design tradeoffs**: One-layer transformer limits complexity but enables theoretical analysis; recursive decomposition restricts function class but provides provable guarantees
- **Failure signatures**: Non-separation of critical gradient component leads to vanishing or exploding gradients; incorrect attention patterns prevent proper function decomposition
- **3 first experiments**: 1) Test gradient separation conditions empirically on k-PARITY functions, 2) Compare RL vs SFT learning curves on k-AND functions, 3) Verify attention patterns match theoretical expectations for k-OR functions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Analysis is restricted to k-sparse Boolean functions that can be recursively decomposed into 2-sparse functions, representing a highly limited subset of all possible Boolean functions
- The theoretical framework assumes idealized settings with specific attention mechanisms and does not account for real-world factors like noisy gradients or finite training data
- The comparison between RL and SFT learning dynamics is purely theoretical and lacks empirical validation across diverse function classes or larger-scale models

## Confidence
- Theoretical proofs are sound within stated assumptions: Medium
- Applicability to practical transformer training scenarios: Medium
- Empirical validation of learning dynamics differences: Low
- Scalability to more complex functions: Low

## Next Checks
1. **Empirical verification** of the learning dynamics differences between RL and SFT on the same k-sparse Boolean functions across multiple function classes beyond the theoretical examples provided
2. **Scaling experiments** to test whether the theoretical separation conditions scale appropriately with function complexity, model size, and training data volume
3. **Robustness analysis** examining how the theoretical guarantees hold under practical perturbations including noisy supervision, partial observability in the Chain-of-Thought reasoning, and alternative attention mechanisms beyond softmax