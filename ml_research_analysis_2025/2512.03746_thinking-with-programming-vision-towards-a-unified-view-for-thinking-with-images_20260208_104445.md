---
ver: rpa2
title: 'Thinking with Programming Vision: Towards a Unified View for Thinking with
  Images'
arxiv_id: '2512.03746'
source_url: https://arxiv.org/abs/2512.03746
tags:
- tool
- tools
- reasoning
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key weakness in current multimodal large
  language models (MLLMs): their brittleness to simple image perturbations like rotations
  and flips, which severely degrades performance. To address this, the authors propose
  CodeVision, a "code-as-tool" framework where the model generates code to invoke
  any image operation, replacing fixed tool registries.'
---

# Thinking with Programming Vision: Towards a Unified View for Thinking with Images

## Quick Facts
- arXiv ID: 2512.03746
- Source URL: https://arxiv.org/abs/2512.03746
- Reference count: 4
- Primary result: CodeVision achieves 73.4% on transformed OCRBench, a 17.4% improvement over base models

## Executive Summary
This paper addresses a critical weakness in multimodal large language models (MLLMs): their brittleness to simple image perturbations like rotations and flips, which can degrade performance by up to 80%. The authors propose CodeVision, a "code-as-tool" framework where the model generates executable code to invoke any image operation, replacing fixed tool registries. The approach uses a two-stage training process with Supervised Fine-Tuning (SFT) on curated data emphasizing multi-tool composition and error recovery, followed by Reinforcement Learning (RL) with a dense process reward function. Experiments on Qwen2.5-VL and Qwen3-VL models show significant performance gains and demonstrate emergent capabilities like flexible tool chaining and robust error recovery.

## Method Summary
CodeVision employs a two-stage training approach to teach MLLMs visual reasoning through code generation. First, SFT trains the model on ~5,000 curated examples with explicit error-handling and multi-tool composition tasks, using masked loss on assistant tokens only. The model learns to generate reasoning traces and code for image operations. Second, RL fine-tunes the model on ~40,000 items using GRPO with a dense reward function that combines outcome accuracy, strategy shaping (bonuses for required tools), and constraint penalties. The dense reward specifically targets inefficient trajectories and inappropriate tool use to prevent reward hacking. The framework uses standard Python libraries (PIL, OpenCV) as the execution environment, allowing the model to chain operations and invoke functions not explicitly seen during RL training.

## Key Results
- CodeVision-7B achieves 73.4% accuracy on transformed OCRBench (+17.4% over base models)
- Sets new state-of-the-art on multi-tool reasoning benchmark MVToolBench
- Demonstrates emergent capabilities like flexible tool chaining and robust error recovery
- Shows strong generalization to tools unseen during RL training
- Achieves 3.3% improvement on V*/HRBench for single-tool reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Code-as-Universal-Interface for Tool Composition
Generating executable code enables dynamic tool composition and generalization to unseen tools by leveraging programming language syntax and libraries. This allows the model to chain operations and bypass limitations of fixed tool registries.

### Mechanism 2: Dense Process Rewarding to Mitigate Reward Hacking
A composite reward function combining outcome accuracy, strategy shaping, and constraint penalties prevents degenerate behaviors. The penalties specifically target inefficient trajectories and inappropriate tool use, forcing deliberate planning rather than reward maximization through exploits.

### Mechanism 3: Cold-Start SFT for Error Recovery Patterns
SFT on trajectories containing explicit error-handling loops pre-teaches the model to interpret environmental feedback before RL optimization. This bootstrapping is necessary because RL-only training fails to converge without this foundational capability.

## Foundational Learning

- **Teacher Forcing with Token Masking**: Why needed? Standard causal LM training would train the model to predict user inputs and tool outputs. The paper requires learning *policy* (generating reasoning and code), necessitating loss on assistant tokens only. Quick check: If you trained on all tokens, what unwanted behavior might the model learn regarding user prompts?

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed? SFT mimics behavior but cannot easily optimize for "efficiency" or "penalties." RLVR allows custom reward functions (e.g., penalizing extra turns) that are verifiable by the environment but hard to capture in static text. Quick check: Why is a simple "accuracy" reward insufficient for teaching a model to use tools efficiently?

- **Program-of-Thoughts (PoT)**: Why needed? The core architecture shifts reasoning from natural language to code. Understanding PoT explains why the model can chain operations (compositionality) better than with natural language tool calls. Quick check: How does writing code to `rotate` and `crop` differ in precision from asking a model to "please turn the image and zoom in"?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL/Qwen3-VL (Multimodal LLM) -> Execution Sandbox (Python + PIL/cv2) -> Reward Server (calculates $R_{total}$)
- **Critical path**: 1) SFT Data Curation (5k examples with error-handling/multi-tool metadata), 2) Cold Start Training (2 epochs, masked assistant tokens), 3) RL Data Filtering (40k items with must-use tool annotations), 4) RL Optimization (GRPO with dense reward)
- **Design tradeoffs**: Fixed Registry vs. Code-as-Tool (safety/predictability vs. flexibility/unboundedness), SFT vs. RL (stability/syntax vs. strategy/efficiency)
- **Failure signatures**: Reward hacking (unnecessary tool calls to maximize step-rewards), Inefficient localization (safe crops that miss precise targets)
- **First 3 experiments**: 1) Reward Ablation (train w/o Strategy Reward and w/o Penalty to verify performance drops), 2) Cold Start Validation (attempt RL from base model to confirm failure), 3) Emergent Tool Test (evaluate on tools not in RL training set)

## Open Questions the Paper Calls Out

### Open Question 1
Can the "code-as-tool" paradigm effectively generalize to multi-image reasoning tasks and proprietary black-box APIs? The current implementation focuses on single-image operations; the framework's ability to handle multiple visual inputs or external APIs remains unexplored.

### Open Question 2
How can process supervision be evolved to incentivize "beneficial" tools over strictly "must-use" tools? The current reward structure relies heavily on predefined required tools and lacks a generalized mechanism to distinguish genuinely helpful optional tools from superfluous ones.

### Open Question 3
What modifications are needed to improve fine-grained coordinate prediction for precise cropping? The model occasionally generates adjacent-but-missed crops, suggesting current training objectives are insufficient for pixel-perfect localization.

### Open Question 4
Does the model's performance saturate or continue to improve with further scaling of data and task diversity? The current experiments show steady improvement but stop after 2 epochs; the upper bound of this scaling law for code-based visual reasoning is unknown.

## Limitations
- Evaluation focuses heavily on image transformation tasks, unclear if advantages extend to other visual reasoning domains
- Reliance on synthetic data generation via GPT-5 introduces potential bias in emergent reasoning patterns
- Dense reward function requires careful hyperparameter tuning without sensitivity analysis

## Confidence

**High Confidence**: MLLM brittleness to image perturbations (80% performance drops) and the necessity of two-stage training (SFT→RL with SFT being non-negotiable).

**Medium Confidence**: Code-as-tool enabling better tool composition and generalization to unseen tools (supported by qualitative examples but lacks systematic testing), and reward function effectiveness against reward hacking (demonstrated through failure case comparisons but relies on specific penalty values).

**Low Confidence**: Claims of setting new SOTA on multi-tool reasoning benchmarks (based on baseline comparisons without exhaustive evaluation), and assertions of "scalable" generalization (not validated with larger model sizes or different architectures).

## Next Checks

1. **Reward Function Robustness**: Conduct systematic ablation study varying β coefficients to determine if performance is robust to hyperparameter choices or represents overfitting.

2. **Generalization Beyond Transformations**: Evaluate CodeVision on visual reasoning tasks without image manipulations (e.g., visual question answering) to determine if code-as-tool provides benefits beyond structured output domains.

3. **Emergent Tool Success Rate**: Quantify success rate of using tools unseen during RL training across multiple trials to provide statistical evidence for generalization claims.