---
ver: rpa2
title: 'Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular
  Sovereignty Solves the Plasticity-Stability Paradox'
arxiv_id: '2601.21249'
source_url: https://arxiv.org/abs/2601.21249
tags:
- systems
- state
- safety
- hydra
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper identifies fundamental barriers to deploying
  monolithic foundation models in safety-critical cyber-physical systems (CPS), where
  non-stationary dynamics and strict reliability requirements demand traceable, certifiable
  state integrity. It argues that the plasticity-stability paradox cannot be resolved
  through global parameter updates, as fine-tuning causes catastrophic forgetting
  and spectral bias masks high-frequency fault signatures, while opacity obstructs
  formal verification under standards like ISO 26262 and IEC 61508.
---

# Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox

## Quick Facts
- arXiv ID: 2601.21249
- Source URL: https://arxiv.org/abs/2601.21249
- Reference count: 40
- Key outcome: Modular sovereignty paradigm (HYDRA) addresses plasticity-stability paradox through frozen regime specialists and uncertainty-aware blending, enabling certifiable state integrity in safety-critical CPS

## Executive Summary
This position paper identifies fundamental barriers to deploying monolithic foundation models in safety-critical cyber-physical systems (CPS), where non-stationary dynamics and strict reliability requirements demand traceable, certifiable state integrity. It argues that the plasticity-stability paradox cannot be resolved through global parameter updates, as fine-tuning causes catastrophic forgetting and spectral bias masks high-frequency fault signatures, while opacity obstructs formal verification under standards like ISO 26262 and IEC 61508. The proposed Modular Sovereignty paradigm, termed HYDRA, addresses these challenges through a library of frozen, regime-specific specialists combined via uncertainty-aware blending. This approach ensures regime-conditional validity, disentangles aleatoric and epistemic uncertainties, and enables modular auditability. By leveraging runtime prediction residuals as a semantic signal for mode separation and enforcing convex combinations of valid operators, HYDRA offers a certifiable path for robust state integrity across the CPS lifecycle, transforming non-stationarity into a structured, auditable asset.

## Method Summary
HYDRA implements modular sovereignty through three components: a frozen library of regime-specific specialists (Type-I: physics-derived via PIML; Type-II: data-derived from fleet history), a Governor that computes posterior weights over specialists using runtime residuals and Dirichlet prior, and an Integrity Monitor that triangulates system health through Jurisdictional Consensus (weighted residual), Ambiguity (posterior entropy), and Regime Stability (rate of weight change). The system enforces convex combinations of valid operators, ensuring system states remain within certifiable safety envelopes. Constitutional Greedy Accretion builds the specialist library offline, admitting new specialists only if they outperform any convex combination of existing ones. Online, the Governor infers mixing weights from residuals via variational inference or HMC, while the Monitor triggers deterministic fallback when integrity bounds are violated.

## Key Results
- Prediction error residuals from frozen specialists provide unambiguous semantic signals for mode separation, overcoming spectral bias that masks fault signatures in monolithic models
- Decoupling offline library construction from online uncertainty-aware blending guarantees knowledge retention and enables exchangeability assumptions for conformal prediction
- Convex combinations of valid specialists ensure system states remain within certifiable safety envelopes, enabling compositional analysis tied to polytopic LPV theory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prediction error residuals from frozen specialists provide a semantic signal that distinguishes operational modes, overcoming spectral bias that would otherwise mask fault signatures.
- **Mechanism:** Specialists trained on single regimes fail catastrophically when encountering out-of-distribution conditions, producing high-frequency residual spikes that monoliths smooth over. The paper states: "Model Brittleness is a feature, not a bug... This high-frequency residual spike overcomes spectral bias, providing a sharp, unambiguous signal for anomaly detection that a generalist model would mask."
- **Core assumption:** Specialists are trained on specific regimes and remain brittle outside their jurisdiction (validity partition).
- **Evidence anchors:**
  - [abstract]: "leveraging runtime prediction residuals as a semantic signal for mode separation"
  - [section 2]: Describes how monoliths "minimizing residuals globally... mask these signatures, destroying the runtime signal required for anomaly detection"
  - [corpus]: "Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift" (FMR=0.465) confirms spectral mismatch degrades TSFM generalization, supporting spectral bias concerns.
- **Break condition:** If specialists over-generalize within their regime, residual signals become weak and mode separation fails.

### Mechanism 2
- **Claim:** Decoupling offline library construction from online uncertainty-aware blending guarantees knowledge retention and enables exchangeability assumptions for conformal prediction.
- **Mechanism:** Frozen specialists prevent gradient conflicts (∇θL_global = ∇θL_regime1 + ∇θL_regime2) that cause catastrophic forgetting. The Governor performs inference over low-dimensional mixing weights π rather than backpropagation through high-dimensional parameters.
- **Core assumption:** The non-stationary stream can be decomposed into approximately stationary local regimes where exchangeability holds within each specialist's domain.
- **Evidence anchors:**
  - [abstract]: "Decoupling offline library construction from online uncertainty-aware blending guarantees retention (no forgetting), restores exchangeability for rigorous uncertainty quantification"
  - [section 2]: Cites Karaouli et al. (2025) showing fine-tuning time-series foundation models induces catastrophic forgetting
  - [corpus]: "iADCPS" (FMR=0.486) addresses distribution shifts in CPS via incremental meta-learning, supporting non-stationarity challenges.
- **Break condition:** If the "Local Low-Dimensionality Assumption" fails (e.g., continuous chaotic attractors without regime-separable structure), the approach falls outside scope.

### Mechanism 3
- **Claim:** Convex combinations of valid specialists ensure system states remain within a certifiable safety envelope, enabling compositional analysis tied to polytopic LPV theory.
- **Mechanism:** By enforcing Σπ_k = 1 with non-negative weights, outputs remain in the convex hull of valid local manifolds. If each specialist's reachable set is bounded by a robust positively invariant (RPI) zonotope, the convex combination is guaranteed to remain within the union's convex hull.
- **Core assumption:** Safety sets of specialists are convex (e.g., RPI Zonotopes); disjoint manifold interpolation does not produce physically invalid states.
- **Evidence anchors:**
  - [abstract]: "enforcing convex combinations of valid operators, HYDRA offers a certifiable path for robust state integrity"
  - [section 3]: "By enforcing convex combinations (Σπ=1), we ensure the system state remains within the convex hull of the valid local manifolds"
  - [corpus]: "Quantifying Robustness" (FMR=0.617) provides benchmarking framework for DL robustness in CPS but does not directly address convex hull guarantees.
- **Break condition:** The "Superposition Fallacy"—naive output blending during bifurcations may produce physically invalid states even with valid specialists, motivating parameter-space blending research.

## Foundational Learning

- **Plasticity-Stability Paradox:**
  - Why needed: Core motivation for why monolithic foundation models fail in non-stationary CPS; fine-tuning for new conditions degrades prior regime performance.
  - Quick check question: Can you explain why updating global parameters to learn "icy road" dynamics would degrade "dry asphalt" predictions?

- **Kolmogorov n-width:**
  - Why needed: Formalizes why global models struggle with transport-dominated dynamics; slow decay (~n^-1/2) for advection means high approximation error without partitioning.
  - Quick check question: Why does the paper assume "local" n-width recovers exponential decay rates?

- **Linear Parameter-Varying (LPV) Systems:**
  - Why needed: HYDRA extends classical polytopic LPV by replacing vertex matrices with neural operators and deterministic scheduling with inferred latent context.
  - Quick check question: How does HYDRA's Governor differ from classical LPV's Taylor-series extrapolation for scheduling parameters?

## Architecture Onboarding

- **Component map:** Library L = {S_1, ..., S_K} (frozen specialists) -> Governor B (computes posterior π) -> Integrity Monitor I = (R, U, T) -> Safety Core (deterministic fallback)
- **Critical path:**
  1. **Offline:** Build Library via Constitutional Greedy Accretion—admit specialists only if they outperform any convex combination of existing ones
  2. **Online:** Governor infers π_t from residuals (variational inference, HMC, or EMA smoothing)
  3. **Assurance:** Monitor I; if R irreducible → Constitutional Failure → Switch to Safety Core
- **Design tradeoffs:**
  - **α ≪ 1 (sparse prior):** Forces Winner-Take-All for discrete bifurcations (ice vs. asphalt); risks chattering
  - **α > 1 (dense prior):** Allows smooth mixing for continuous aging; risks Superposition Fallacy during regime shocks
  - **Output-space vs. Parameter-space blending:** Output blending is simpler but risks invalid interpolation; parameter-space blending via Φ = g(π) preserves physics manifold
- **Failure signatures:**
  - **High R + Low U:** Constitutional Failure (physics violation, exit known manifold)
  - **Moderate R + High U:** Hung Parliament (epistemic ambiguity, multiple specialists fit equally)
  - **High T (discontinuous):** Regime Shock (abrupt transition requiring intervention)
- **First 3 experiments:**
  1. **Residual discriminability test:** Inject known regime transitions (e.g., friction step changes) and verify residual spikes exceed detection threshold before monolithic model shows degradation.
  2. **Forgetting validation:** Sequentially present new regimes to HYDRA vs. fine-tuned monolith; measure performance retention on original regimes over 10+ adaptation cycles.
  3. **Convex hull exit detection:** Define RPI zonotopes for each specialist; verify Monitor I correctly triggers fallback when system exits Conv(Z_active) under simulated fault injection.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the HYDRA Governor algorithmically resolve the "Sparsity-Continuity Tension" to simultaneously support discrete regime switching (bifurcations) and continuous physical degradation (aging)?
- **Basis in paper:** [explicit] Section 5 explicitly labels "The Sparsity-Continuity Tension" as an "Open Challenge," noting that sparsity prevents invalid interpolation but causes chattering during drift.
- **Why unresolved:** The Dirichlet prior concentration parameter $\alpha$ forces a binary choice: sparse "winner-take-all" decisions for stability or dense mixing for continuity, lacking a mechanism for adaptive context-dependent behavior.
- **What evidence would resolve it:** An algorithm capable of dynamically modulating $\alpha$ or switching between blending strategies based on the rate of change of prediction residuals, validated on systems exhibiting both abrupt faults and gradual wear.

### Open Question 2
- **Question:** Can parameter-dependent Lyapunov functions be extended to certify stability for nonlinear neural operators within the HYDRA framework?
- **Basis in paper:** [explicit] Section 2 states that for nonlinear neural operators, constructing these functions "remains an open challenge," and Section 5 lists "Towards Certifiable Stability for Neural Operators" as a future direction.
- **Why unresolved:** While linear specialists allow for LMI optimization to prove stability, the entangled nature of neural operators obstructs the computation of the tight Lipschitz bounds required for formal verification.
- **What evidence would resolve it:** The derivation of machine-checkable Lyapunov certificates for the convex combination of frozen neural operators, or a formal proof that the convex hull of stable nonlinear operators preserves stability.

### Open Question 3
- **Question:** Does establishing an inverse mapping $\Phi = g(\pi)$ from mixture weights to physical parameters effectively prevent the "superposition fallacy" (physically invalid states) during output blending?
- **Basis in paper:** [explicit] Section 5 proposes "Parameter-to-Physics Mapping" as a critical future direction to resolve the risk where "convex combinations of disjoint specialists may produce outputs that exit the physical manifold."
- **Why unresolved:** Naive output blending ($\hat{y} = \sum \pi_k S_k(x)$) risks violating conservation laws (e.g., mass, energy) when the underlying solution manifolds are disjoint, and it is unclear if a general inverse $g(\pi)$ exists for complex physical systems.
- **What evidence would resolve it:** Empirical demonstrations showing that interpolating physical parameters within a single operator (evaluating $N_\theta(x_t; \Phi = g(\pi_t))$) maintains manifold validity and conserves physical invariants better than direct output interpolation.

## Limitations

- Theoretical guarantees for HYDRA's convex hull safety rely on strong assumptions about regime separability and specialist convexity that may not hold in practice
- The paper does not provide empirical validation of the Integrity Monitor's three-metric triangulation system (R, U, T) under real-world non-stationary CPS conditions
- The constitutional greedy accretion algorithm for library expansion lacks specification of stopping criteria and computational complexity bounds for large-scale deployment

## Confidence

- **High confidence:** The plasticity-stability paradox as a fundamental barrier to monolithic foundation models in CPS; catastrophic forgetting under fine-tuning; the need for certifiable state integrity in safety-critical systems
- **Medium confidence:** The effectiveness of frozen specialists for regime separation through residual signals; the Dirichlet-based uncertainty quantification framework; the modular sovereignty paradigm as a general solution approach
- **Low confidence:** Specific parameter choices (Dirichlet concentration α, specialist architectures, monitor thresholds); scalability to hundreds of regimes; performance guarantees in continuous chaotic attractors without clear regime boundaries

## Next Checks

1. **Empirical forgetting benchmark:** Conduct head-to-head comparison of HYDRA versus fine-tuned monolithic TSFM across 10+ sequential regime adaptation cycles, measuring performance degradation on previously learned regimes (target retention >95% for specialists, <50% for fine-tuned monolith)

2. **Residual discriminability validation:** Inject controlled regime transitions (e.g., friction coefficient step changes from 0.8 to 0.2) and measure time-to-detection: HYDRA specialists should produce detectable residual spikes within 1-2 samples, while monolithic models show degradation only after 10+ samples

3. **Convex hull safety verification:** Implement formal verification of the convex combination safety property using zonotope reachability analysis; demonstrate that the Integrity Monitor correctly triggers fallback when system states exit the convex hull of active specialists' reachable sets under simulated fault injection scenarios