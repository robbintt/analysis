---
ver: rpa2
title: An Attention-Guided Deep Learning Approach for Classifying 39 Skin Lesion Types
arxiv_id: '2501.05991'
source_url: https://arxiv.org/abs/2501.05991
tags:
- skin
- attention
- classification
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep learning approach for classifying 39
  types of skin lesions using a curated dataset synthesized from five publicly available
  sources. The methodology integrates advanced attention mechanisms, specifically
  Efficient Channel Attention (ECA) and Convolutional Block Attention Module (CBAM),
  into state-of-the-art deep learning models including MobileNetV2, Xception, InceptionV3,
  EfficientNetB1, and Vision Transformer (ViT).
---

# An Attention-Guided Deep Learning Approach for Classifying 39 Skin Lesion Types

## Quick Facts
- arXiv ID: 2501.05991
- Source URL: https://arxiv.org/abs/2501.05991
- Authors: Sauda Adiv Hanum; Ashim Dey; Muhammad Ashad Kabir
- Reference count: 40
- Key outcome: ViT+CBAM achieved 93.46% accuracy, 94% precision, 93% recall, 93% F1-score, and 93.67% specificity on 39 skin lesion types

## Executive Summary
This paper presents a deep learning approach for classifying 39 types of skin lesions using a curated dataset synthesized from five publicly available sources. The methodology integrates advanced attention mechanisms, specifically Efficient Channel Attention (ECA) and Convolutional Block Attention Module (CBAM), into state-of-the-art deep learning models including MobileNetV2, Xception, InceptionV3, EfficientNetB1, and Vision Transformer (ViT). The Vision Transformer with CBAM achieved the highest performance with 93.46% accuracy, 94% precision, 93% recall, 93% F1-score, and 93.67% specificity. The study demonstrates the effectiveness of attention-guided mechanisms in enhancing feature extraction and classification accuracy for complex multi-class skin lesion classification tasks, offering a robust tool for medical professionals in early disease detection and diagnosis.

## Method Summary
The study developed a curated dataset by merging five publicly available skin lesion datasets (ISIC 2019, Atlas Dermatology, HAM10000, MSLD 2.0, Dermnet) and balancing classes to 130 images each. Five deep learning architectures were evaluated: MobileNetV2, Xception, InceptionV3, EfficientNetB1, and Vision Transformer. Attention mechanisms (ECA and CBAM) were integrated into each model. The Vision Transformer with CBAM achieved the highest performance. Models were pre-trained on ImageNet and fine-tuned on the curated dataset using standard augmentation and trained on NVIDIA Tesla P100 GPU with Adam or RectifiedAdam optimizer at learning rate 0.001 and batch size 8.

## Key Results
- ViT+CBAM achieved the highest performance: 93.46% accuracy, 94% precision, 93% recall, 93% F1-score, and 93.67% specificity
- ViT baseline outperformed all CNN baselines (MobileNetV2: 80.03%, Xception: 87.18%, InceptionV3: 83.85%, EfficientNetB1: 82.44%)
- 18 out of 39 classes achieved 100% accuracy, while Leprosy Lepromatous showed only 50% accuracy
- CBAM integration improved ViT performance by 1.67 percentage points over baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential channel-then-spatial attention refines global transformer features for fine-grained lesion discrimination.
- **Mechanism:** CBAM first computes channel attention maps via shared MLP on pooled features (Eq. 3), emphasizing "what" features matter. The refined features then pass through spatial attention using pooled concatenation convolved with a 7×7 filter (Eq. 6), emphasizing "where" informative regions exist. This sequential refinement helps the model focus on diagnostically relevant lesion boundaries and textures.
- **Core assumption:** ViT's global representations contain sufficient signal about local discriminative features that can be amplified through learned attention weights.
- **Evidence anchors:**
  - Section 3.4.2 provides mathematical formulation; Section 3.5 describes integration with ViT: "the global feature representations obtained from the transformer are passed to the channel attention module... refined features... then passed to the spatial attention module."
  - Reports ViT+CBAM achieved 93.46% accuracy, outperforming ViT baseline (91.79%) and ViT+ECA (92.31%).
  - Related work (e.g., Swin Transformer + CNN, Deep Bottleneck Transformer) shows attention mechanisms consistently improve skin lesion classification, supporting the general mechanism but not the specific CBAM+ViT combination.

### Mechanism 2
- **Claim:** Vision Transformer's patch-as-token processing captures long-range dependencies across lesion regions that CNNs' local receptive fields may miss.
- **Mechanism:** Images are split into fixed-size patches, flattened, linearly projected to embeddings, augmented with positional encodings, and processed through multi-head self-attention layers. This allows any patch to attend to any other patch, modeling global structure without convolution's inductive bias.
- **Core assumption:** Lesion discriminative features are distributed across non-adjacent image regions requiring global context to disambiguate.
- **Evidence anchors:**
  - Section 3.3 details the 6-step ViT architecture, noting it "leverage[s] the power of self-attention to model global dependencies... understanding the relationships between distant image regions."
  - Table 4 shows ViT baseline (91.79%) outperforms all CNN baselines (MobileNetV2: 80.03%, Xception: 87.18%, InceptionV3: 83.85%, EfficientNetB1: 82.44%).
  - Multiple related papers (Swin Transformer, optimized ViT) report transformer-based approaches achieving competitive or superior performance on skin lesion datasets, supporting the global-context mechanism.

### Mechanism 3
- **Claim:** Multi-dataset curation with class balancing improves model generalization across diverse lesion presentations.
- **Mechanism:** Five publicly available datasets with varying imaging conditions (dermoscopic vs. clinical) were merged, then down-sampled to 130 images per class to prevent majority-class dominance. This exposes the model to broader phenotypic variation while ensuring balanced gradient updates.
- **Core assumption:** The 130-image-per-class threshold captures sufficient intra-class variability for effective learning.
- **Evidence anchors:**
  - Section 3.1 states: "the dataset was balanced by capping each class at 130 images, ensuring equal representation and mitigating biases during model training."
  - Section 3.1 notes the curated dataset "encompasses lesions captured under varying conditions, including dermoscopic and clinical imaging."
  - No direct corpus evidence compares multi-dataset curation strategies; this is an area where the paper extends prior work focused on single datasets.

## Foundational Learning

- **Concept: Attention mechanisms in neural networks**
  - **Why needed here:** CBAM and ECA are central to the paper's improvements. Without understanding how attention learns to weight features, you cannot debug why it helps some classes (18 classes at 100% accuracy) but not others (Leprosy Lepromatous at 50%).
  - **Quick check question:** Given a feature map F with shape (C, H, W), what are the shapes of the channel attention output M_c and spatial attention output M_s?

- **Concept: Transfer learning and fine-tuning**
  - **Why needed here:** All models (MobileNetV2, Xception, InceptionV3, EfficientNetB1, ViT) are pre-trained and fine-tuned. Understanding which layers to freeze versus fine-tune is critical for reproducing results.
  - **Quick check question:** If the curated dataset has different image statistics than ImageNet (pre-training source), which layers would benefit most from lower learning rates during fine-tuning?

- **Concept: Multi-class classification metrics**
  - **Why needed here:** The paper reports accuracy, precision, recall, F1, and specificity. For 39 classes, macro vs. micro averaging matters for interpretation, especially with persistent class imbalance (some classes harder than others).
  - **Quick check question:** If a model achieves 93% overall accuracy but 50% accuracy on Leprosy Lepromatous, what does this suggest about class-wise performance distribution?

## Architecture Onboarding

- **Component map:** Input images → Resize → Normalize → Augment → ViT backbone (patch embedding → positional encoding → transformer encoder) → CBAM (channel attention MLP → spatial attention 7×7 conv) → GELU → MLP → Softmax (39-class output)

- **Critical path:**
  1. Reproduce the data curation by downloading the 5 source datasets and applying the 130-image-per-class cap.
  2. Implement ViT+CBAM: insert CBAM after the transformer encoder output, before the classification head.
  3. Train with reported hyperparameters: learning rate 0.001, batch size 8, Adam optimizer (compare with RectifiedAdam per Figure 1).
  4. Evaluate using the 5 metrics; target ~93% accuracy to validate implementation.

- **Design tradeoffs:**
  - **ViT vs. CNN:** ViT captures global context but requires more data; CNNs have stronger inductive bias for local features. The paper shows ViT+CBAM outperforms CNNs, but this may not hold on smaller datasets.
  - **CBAM vs. ECA:** CBAM adds spatial attention (more compute) but yields +1.15% accuracy over ECA on ViT. For resource-constrained deployment, ECA may be preferable.
  - **Class balancing:** Capping at 130 images prevents bias but may underrepresent rare conditions. Upsampling or synthetic augmentation (GANs, discussed in Section 5.2) could be alternatives.

- **Failure signatures:**
  - **Visual similarity confusion:** Leprosy Lepromatous (50% accuracy) misclassified as Molluscum Contagiosum (Figure 8)—both show nodular lesions. Watch for this pattern in confusion matrices.
  - **Underperforming classes:** Basal Cell Carcinoma (85%), Benign Keratosis (85%), Psoriasis (85%), Larva Migrans (80%), Vasculitis (80%) suggest feature overlap or insufficient training diversity.
  - **Data imbalance artifacts:** If training loss plateaus but per-class accuracy varies widely, check whether balancing strategy is adequate.

- **First 3 experiments:**
  1. **Baseline reproduction:** Train plain ViT (no attention module) on the curated dataset with reported hyperparameters. Confirm ~91.79% accuracy to validate data pipeline.
  2. **Ablation study:** Compare ViT+CBAM vs. ViT+ECA vs. ViT alone. Measure not just accuracy but per-class improvements to identify where spatial attention helps most.
  3. **Class-wise error analysis:** Focus on Leprosy Lepromatous and other underperforming classes. Visualize attention maps (CBAM output) to determine if the model attends to discriminative regions or is confused by similar textures. This informs whether the issue is architectural or data-driven.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating clinical parameters, such as Bacterial Index (BI) values, effectively distinguish between visually similar conditions like Leprosy Lepromatous and Molluscum Contagiosum where image-based classification fails?
- Basis in paper: The authors note that the model struggles to distinguish these conditions due to visual similarity and suggest integrating clinical parameters (BI) to resolve this.
- Why unresolved: The current methodology relies exclusively on image data, lacking the multimodal data integration required to test this hypothesis.
- What evidence would resolve it: A study evaluating the model's performance on a dataset containing both dermoscopic images and corresponding clinical BI scores to see if misclassification rates drop.

### Open Question 2
- Question: Does an ensemble learning approach combining Vision Transformers, CNNs, and hybrid architectures yield higher robustness and accuracy than the single ViT+CBAM model proposed?
- Basis in paper: The future work section suggests that combining multiple models could capitalize on complementary strengths to improve predictions.
- Why unresolved: The study evaluates individual models with attention mechanisms but does not experiment with combining them into a unified ensemble system.
- What evidence would resolve it: Comparative performance metrics (Accuracy, F1-Score) between the proposed single model and a newly constructed ensemble model on the same 39-class dataset.

### Open Question 3
- Question: How can explainable AI (XAI) techniques be integrated to interpret the ViT+CBAM model's decisions and verify that it focuses on diagnostically relevant features?
- Basis in paper: The authors identify the need for XAI to provide insights into the decision-making process to increase trust among clinicians.
- Why unresolved: The paper focuses on classification performance metrics (accuracy, specificity) without implementing or analyzing visual explanation methods for the predictions.
- What evidence would resolve it: Implementation of XAI methods (e.g., attention maps) showing that the model highlights medically relevant lesion boundaries rather than background artifacts.

## Limitations
- The 130-image-per-class balancing may underrepresent complex lesions, as evidenced by Leprosy Lepromatous achieving only 50% accuracy
- Missing specific ViT hyperparameters (learning rate, batch size, epochs) and optimizer selection details prevent faithful reproduction
- No input resolution specified for ViT patches, which significantly impacts model performance and computational requirements

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| ViT+CBAM architecture superiority (93.46% accuracy) | Medium |
| Attention mechanism effectiveness | Medium |
| Multi-dataset curation benefits | Low |

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary learning rate (0.0001-0.01), batch size (4-16), and optimizer (Adam vs. RectifiedAdam) for ViT+CBAM to identify optimal configurations and assess result stability.
2. **Per-class attention visualization:** Generate attention heatmaps for underperforming classes (Leprosy Lepromatous, Basal Cell Carcinoma, Vasculitis) to determine whether the model attends to diagnostically relevant regions or is misled by visual similarities to other lesion types.
3. **Alternative balancing strategies:** Compare the 130-image cap approach against oversampling minority classes, synthetic data generation (GANs), or class-weighted loss functions to evaluate whether performance on hard-to-classify lesions can be improved without sacrificing overall accuracy.