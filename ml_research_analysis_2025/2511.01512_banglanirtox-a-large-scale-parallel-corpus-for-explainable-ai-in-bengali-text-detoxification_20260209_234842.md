---
ver: rpa2
title: 'BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali
  Text Detoxification'
arxiv_id: '2511.01512'
source_url: https://arxiv.org/abs/2511.01512
tags:
- detoxification
- text
- bengali
- toxic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BanglaNirTox, the first parallel corpus for
  Bengali text detoxification, containing 68,041 toxic sentences with toxicity labels,
  reasoning, and detoxified paraphrases. A novel pipeline using Pareto-optimization
  and Chain-of-Thought prompting with Gemini-2.5 Pro was developed to generate this
  data, selecting the best model outputs based on automated metrics (STA, SIM, FL,
  J Score, BERTScore, ROUGE-1, METEOR).
---

# BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification

## Quick Facts
- **arXiv ID**: 2511.01512
- **Source URL**: https://arxiv.org/abs/2511.01512
- **Reference count**: 0
- **Primary result**: First parallel corpus for Bengali text detoxification with 68,041 toxic sentences, enabling superior detoxification performance through fine-tuning

## Executive Summary
This work introduces BanglaNirTox, the first parallel corpus for Bengali text detoxification, containing 68,041 toxic sentences with toxicity labels, reasoning, and detoxified paraphrases. The corpus was constructed using a novel pipeline combining Pareto-optimization and Chain-of-Thought prompting with Gemini-2.5 Pro, with automated selection based on multiple quality metrics. The dataset enables fine-tuning of Bangla language models including BanglaLLaMA and TigerLLM, which achieved superior detoxification performance compared to baselines while maintaining semantic preservation and fluency. This work addresses a significant gap in low-resource NLP by providing the first large-scale resource for combating toxic language in Bengali.

## Method Summary
The authors developed a novel automated pipeline for constructing the BanglaNirTox corpus. They employed Gemini-2.5 Pro with Chain-of-Thought prompting to generate detoxified paraphrases of toxic Bengali sentences, then applied Pareto-optimization to select the best outputs based on multiple automated metrics including STA, SIM, FL, J Score, BERTScore, ROUGE-1, and METEOR. The resulting corpus contains 68,041 toxic-toxic_detoxified sentence pairs with associated toxicity labels and reasoning. The corpus was then used to fine-tune existing Bangla language models, with evaluation showing improved detoxification performance compared to baseline models.

## Key Results
- Constructed first parallel corpus for Bengali text detoxification with 68,041 sentence pairs
- Fine-tuned BanglaLLaMA and TigerLLM models achieved superior detoxification performance compared to baselines
- Models successfully balanced style transfer, semantic preservation, and fluency in detoxified outputs
- Corpus includes toxicity labels, reasoning, and explanations enabling explainable AI approaches

## Why This Works (Mechanism)
The automated pipeline leverages large language models' ability to understand context and generate coherent paraphrases while the Pareto-optimization framework ensures multiple quality dimensions are simultaneously optimized. Chain-of-Thought prompting enables the model to reason through the detoxification process systematically, while the selection of multiple quality metrics ensures balanced performance across fluency, semantic preservation, and detoxification effectiveness.

## Foundational Learning
- **Pareto-optimization**: Why needed: To balance multiple competing objectives (fluency, semantics, detoxification) in model selection. Quick check: Verify that selected outputs represent true Pareto-optimal trade-offs by examining metric distributions.
- **Chain-of-Thought prompting**: Why needed: To enable systematic reasoning through complex detoxification tasks. Quick check: Compare outputs with and without CoT to assess reasoning quality improvements.
- **Automated quality metrics**: Why needed: To scale corpus construction without human annotation. Quick check: Correlate metric scores with human judgments on sample outputs.

## Architecture Onboarding
- **Component map**: Toxic Sentences -> Gemini-2.5 Pro (CoT) -> Multiple Detoxified Outputs -> Pareto-Optimization -> Selected Outputs -> BanglaNirTox Corpus -> Fine-tuning -> Detoxification Models
- **Critical path**: Gemini-2.5 Pro generation → Pareto-optimization selection → Model fine-tuning → Evaluation
- **Design tradeoffs**: Automated vs. human annotation (speed vs. quality), multiple metrics vs. single metric (comprehensive vs. focused evaluation), large language model vs. rule-based approaches (flexibility vs. control)
- **Failure signatures**: Over-detoxification losing original meaning, under-detoxification retaining toxicity, generation failures producing nonsensical outputs
- **First experiments**: 1) Generate 100 toxic sentences and compare CoT vs. direct prompting outputs, 2) Apply Pareto-optimization to synthetic data with known optimal solutions, 3) Fine-tune a small model on a subset and measure quality degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Automated pipeline may introduce systematic biases or errors in detoxification process
- Reliance on automated metrics may not perfectly capture human judgment of detoxification quality
- Corpus size of 68,041 pairs may be limited for robust model training across all toxic content types
- Focus on Bengali limits generalizability to other low-resource languages

## Confidence
- **Novelty of corpus**: Medium
- **Methodology effectiveness**: Medium
- **Performance improvements**: Medium
- **Generalizability to real-world deployment**: Low

## Next Checks
1. Conduct human evaluation studies comparing model-generated detoxified outputs against gold standards to validate the automated selection pipeline's effectiveness
2. Test the fine-tuned models on external Bengali toxic text datasets not used in training to assess generalization capabilities
3. Perform ablation studies removing different components of the Pareto-optimization pipeline to determine which aspects most contribute to detoxification quality