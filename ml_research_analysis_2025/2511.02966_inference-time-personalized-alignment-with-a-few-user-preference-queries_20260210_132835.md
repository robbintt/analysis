---
ver: rpa2
title: Inference-Time Personalized Alignment with a Few User Preference Queries
arxiv_id: '2511.02966'
source_url: https://arxiv.org/abs/2511.02966
tags:
- user
- preference
- useralign
- interaction
- ycand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses personalized alignment of generative models
  by developing a novel inference-time method, UserAlign, that elicits user preferences
  with a small number of pairwise response comparisons. Building on the best-arm identification
  framework from logistic bandits, UserAlign treats user feedback as consistent and
  noise-free, leveraging version-space elimination to rapidly identify the best response
  from a fixed pool.
---

# Inference-Time Personalized Alignment with a Few User Preference Queries

## Quick Facts
- **arXiv ID**: 2511.02966
- **Source URL**: https://arxiv.org/abs/2511.02966
- **Reference count**: 40
- **Primary result**: Novel inference-time method for personalizing generative models using small number of user preference queries

## Executive Summary
This paper addresses the challenge of personalized alignment in generative models by introducing UserAlign, an inference-time method that elicits user preferences through pairwise response comparisons. The method treats user feedback as consistent and noise-free, leveraging a best-arm identification framework from logistic bandits combined with version-space elimination to rapidly identify the best response from a fixed pool. Through experiments across text and image generation tasks, UserAlign demonstrates high win-rates with low interaction cost, outperforming existing methods while maintaining computational efficiency of under a second per interaction step.

## Method Summary
UserAlign operates as an inference-time alignment method that elicits user preferences through pairwise comparisons of model responses. The approach is grounded in the best-arm identification problem from logistic bandits, where the goal is to identify the optimal response from a fixed pool based on user feedback. By treating user preferences as consistent and noise-free, the method employs version-space elimination to iteratively narrow down the set of candidate responses until the best one is identified. This framework enables rapid convergence with minimal user interaction, requiring only a small number of preference queries to achieve effective personalization across different generation tasks.

## Key Results
- Achieves high win-rates in user preference tasks with minimal interaction cost
- Outperforms existing methods in both text and image generation domains
- Maintains computational efficiency with under one second per interaction step
- Demonstrates robust performance with both persona-specified and freely chosen user preferences

## Why This Works (Mechanism)
UserAlign works by treating personalized alignment as a best-arm identification problem in the logistic bandit framework. The method leverages the assumption of consistent user preferences to eliminate large portions of the response space with each pairwise comparison, rapidly converging to the optimal response. The version-space elimination approach ensures that each user query maximally reduces uncertainty about their preferences, making the process both efficient and effective for inference-time personalization.

## Foundational Learning
- **Best-arm identification in logistic bandits**: Needed to frame personalized alignment as an optimization problem; quick check: verify that the bandit setup correctly models the preference elicitation process
- **Version-space elimination**: Required for efficient narrowing of candidate responses; quick check: confirm that each elimination step preserves the optimal response with high probability
- **Pairwise preference elicitation**: Essential for collecting user feedback in a structured manner; quick check: validate that pairwise comparisons provide sufficient information for preference learning
- **Inference-time alignment**: Critical for applying personalization at generation time rather than during training; quick check: ensure the method can operate within practical time constraints during inference
- **Consistency assumption in user preferences**: Underpins the theoretical guarantees; quick check: examine how violations of this assumption affect performance

## Architecture Onboarding
**Component map**: User queries -> Preference comparison -> Version space update -> Response pool filtering -> Best response identification
**Critical path**: Query generation → User feedback collection → Preference model update → Candidate elimination → Final response selection
**Design tradeoffs**: The method trades the assumption of noise-free preferences for computational efficiency and rapid convergence, prioritizing inference-time personalization over training-time adaptation
**Failure signatures**: Inconsistent user feedback, highly multimodal preferences, or extremely large response pools may degrade performance by violating core assumptions or increasing computational burden
**First experiments**: 1) Test with synthetic consistent preferences to verify theoretical guarantees, 2) Evaluate sensitivity to pool size variations, 3) Measure performance degradation under mild noise injection in preferences

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes user preferences are consistent and noise-free, which may not hold in real-world scenarios
- Limited human evaluation sample size (20 participants) may not capture preference heterogeneity across diverse populations
- Theoretical guarantees assume ideal conditions that may not translate fully to practical settings with approximate preference elicitation

## Confidence
- **High confidence**: Empirical demonstration of effectiveness in controlled settings with consistent preferences
- **Medium confidence**: Scalability and robustness claims given limited human evaluation sample size
- **Medium confidence**: Computational efficiency claims based on reported runtimes rather than systematic benchmarking

## Next Checks
1. Evaluate UserAlign's performance under noisy or inconsistent user feedback conditions to assess robustness beyond the current noise-free assumption
2. Conduct a larger-scale human evaluation study (n > 100) across diverse demographic groups to better assess real-world applicability and preference heterogeneity handling
3. Perform systematic benchmarking of computational efficiency across different hardware configurations and response pool sizes to validate the reported sub-second interaction times