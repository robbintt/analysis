---
ver: rpa2
title: 'InData: Towards Secure Multi-Step, Tool-Based Data Analysis'
arxiv_id: '2511.11933'
source_url: https://arxiv.org/abs/2511.11933
tags:
- tool
- data
- llms
- tools
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are widely used for data analysis
  but pose security risks when handling sensitive data, as they often generate and
  execute arbitrary code. To address this, the authors propose restricting LLMs to
  use only a predefined set of secure, vetted tools, preventing direct code generation
  and data access.
---

# InData: Towards Secure Multi-Step, Tool-Based Data Analysis

## Quick Facts
- **arXiv ID:** 2511.11933
- **Source URL:** https://arxiv.org/abs/2511.11933
- **Reference count:** 0
- **Primary result:** Restricting LLMs to predefined tools for data analysis improves security but reveals significant limitations in multi-step compositional reasoning.

## Executive Summary
Large language models (LLMs) are widely used for data analysis but pose security risks when handling sensitive data, as they often generate and execute arbitrary code. To address this, the authors propose restricting LLMs to use only a predefined set of secure, vetted tools, preventing direct code generation and data access. They introduce InData, a dataset designed to evaluate LLMs' ability to solve complex, multi-step reasoning tasks using only these tools. InData includes 2,063 questions across three difficulty levels, based on 114 real-world CSV datasets. Benchmarking 15 open-source LLMs shows that while models perform well on easy tasks, their accuracy drops significantly on hard tasks—e.g., from 97.3% to 69.6% for gpt-oss-120b. The results indicate that current LLMs lack strong compositional, multi-step tool-based reasoning ability. Providing structured hints or limiting tools to only those needed can improve performance, but challenges remain. InData offers a benchmark to advance research in secure, tool-based LLM data analysis.

## Method Summary
The InData evaluation framework restricts LLMs to a predefined set of 100 secure data analysis tools, preventing direct code generation and data access. Models interact with data exclusively through these tools in an agentic loop where each turn allows exactly one tool call. The dataset contains 2,063 questions across three difficulty levels (Easy, Medium, Hard) based on 114 real-world CSV files from Kaggle. Models are evaluated on their ability to correctly answer questions using only the provided tools, with accuracy measured as exact string matches or numeric answers within 0.1 tolerance. The evaluation tests models' compositional reasoning ability by requiring them to chain multiple tool calls together to solve complex problems.

## Key Results
- GPT-OSS-120B achieves 97.3% accuracy on Easy tasks but drops to 69.6% on Hard tasks
- Small models (xLAM-1B) show dramatic improvement when given only sufficient tools rather than all 100
- Structured hints (Python code solutions) improve Hard task performance from 69.6% to 74.9% for gpt-oss-120b
- Qwen3-Next-80B achieves highest overall accuracy at 84.4% on Easy tasks
- Hard tasks require an average of 17+ turns and often exceed 25 tool calls

## Why This Works (Mechanism)

### Mechanism 1: Tool-Only Security Boundary
Restricting LLMs to predefined, vetted tools eliminates arbitrary code execution risks while preserving data analysis capability. Tools act as a controlled interface—the LLM never receives raw data, cannot generate arbitrary code, and all operations execute within sandboxed, auditable functions. Tools return only filenames (not data rows) and error messages are truncated to prevent leakage. Core assumption: The predefined tool set is itself free of vulnerabilities and covers all necessary operations. Evidence anchors: [abstract] states the restriction to secure, verified tools; [Page 4] describes tools returning only filenames and truncated error messages. Break condition: If tools contain edge-case vulnerabilities, or if cumulative aggregate queries leak sensitive information.

### Mechanism 2: Compositional Reasoning Decomposition
Multi-step tool reasoning requires LLMs to decompose complex tasks into atomic operations and track intermediate state across turns. Each tool performs one atomic operation (e.g., `get_mean`, `filter_rows`); the LLM must plan a sequence, execute calls one at a time, and pass intermediate filenames between tools. Hard questions require 15-25+ sequential calls. Core assumption: LLMs can maintain coherent state across long conversation histories and correctly reference intermediate outputs. Evidence anchors: [abstract] shows performance drops from 97.3% on Easy to 69.6% on Hard; [Page 7] notes models require on average more than 17 turns for Hard examples. Break condition: When reasoning chains exceed model's effective context tracking or require error recovery from failed intermediate steps.

### Mechanism 3: Hint-Augmented Planning
Providing structured hints (Python code solutions or sample data) improves multi-step tool reasoning by supplying explicit reasoning scaffolds. Hints act as a reasoning template—the LLM uses the hint's logic to guide tool selection and sequencing without executing the code directly. Core assumption: The hint code is correct, relevant, and maps cleanly to available tools. Evidence anchors: [Page 7] shows gpt-oss-120b improves from 69.6% to 74.9% with hints; [Page 2] states structured hints substantially improve multi-step tool reasoning. Break condition: When hint logic diverges from tool capabilities or hint is incorrect/misleading.

## Foundational Learning

- **Concept: Agentic Loop with Single-Tool-Per-Turn Constraint**
  - Why needed here: InData's evaluation requires understanding that each turn allows exactly one tool call; multiple calls trigger retry prompts.
  - Quick check question: What happens when an LLM outputs two tool calls in a single response?

- **Concept: Context Window as Reasoning Budget**
  - Why needed here: Larger models benefit from full tool access; smaller models perform better with only relevant tools due to context dilution.
  - Quick check question: Why does xLAM-1B improve from 11.1% to 42.8% on Easy tasks when given only sufficient tools instead of all 100?

- **Concept: Intermediate State via Filename References**
  - Why needed here: Tools never return data directly—only filenames. The LLM must track and pass these filenames through the reasoning chain.
  - Quick check question: How does `filter_rows` communicate its result for use in a subsequent `get_mean` call?

## Architecture Onboarding

- **Component map:**
  - Question + system prompt → LLM generates tool call
  - Tool execution → Returns filename or error
  - Update conversation → Repeat until `report_*` or limit reached
  - Extract answer from reporting tool argument

- **Critical path:**
  1. Question + system prompt → LLM generates tool call
  2. Tool execution → Returns filename or error
  3. Update conversation → Repeat until `report_*` or limit reached
  4. Extract answer from reporting tool argument

- **Design tradeoffs:**
  - Security vs. Expressiveness: Tool-only prevents code risks but limits complex operations not covered by tool set
  - Context length vs. Tool availability: All 100 tools = flexible but selection burden; sufficient-only = easier for small models
  - Hint quality vs. Autonomy: Hints improve accuracy but require trusted code generation step

- **Failure signatures:**
  - Column name mismatches (underscores→spaces, capitalization): Triggers "column not found" errors
  - Max turns reached: GPT-family hits turn limit more often; Qwen-family calls `abort_task` more frequently
  - Hallucinated tools: Model calls non-existent tool name

- **First 3 experiments:**
  1. Establish baseline: Run top 4 models on Easy/Medium/Hard subsets with full tool set to reproduce paper's accuracy curve
  2. Hint ablation: Test gpt-oss-120b and Qwen3-Next-80B on Hard questions with/without Python code hints
  3. Tool reduction analysis: Compare small model (xLAM-1B, Qwen3-1.7B) performance with all tools vs. sufficient-only subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating an explicit planning stage—where models first generate a reasoning plan (e.g., Python code) to use as structured hints—significantly improve accuracy on complex multi-step tool-based tasks?
- Basis in paper: [explicit] The authors state, "A promising direction is introducing a planning stage before tool execution... and then uses it as a structured hint during inference."
- Why unresolved: The paper benchmarks existing models but does not implement or test this suggested planning architecture to verify its efficacy in improving tool composition.
- What evidence would resolve it: A comparative study showing that models utilizing a pre-planning step achieve statistically significant higher accuracy on the InData Hard subset compared to the baseline direct tool-calling approach.

### Open Question 2
- Question: To what extent can supervised fine-tuning (SFT), preference-based optimization, or inference-time search techniques bridge the performance gap between code generation capabilities and tool-based reasoning?
- Basis in paper: [explicit] The conclusion notes, "It is also worth exploring whether supervised fine-tuning, preference-based optimization, or inference-time techniques such as search or self-consistency can improve multi-step reasoning."
- Why unresolved: The benchmark evaluates off-the-shelf models but does not explore how specialized training regimes might improve the distinct capability of compositional tool use.
- What evidence would resolve it: Benchmarks of models fine-tuned specifically on InData or similar tool-use trajectories demonstrating improved performance over the current base models.

### Open Question 3
- Question: Is the performance drop on Hard tasks strictly due to reasoning limitations, or does the restriction to 100 tools create an incomplete environment where some ground-truth answers are unreachable?
- Basis in paper: [inferred] Section 5.3 establishes a "Lower Bound on Completeness" of 85.1% for Hard questions, suggesting up to ~15% of questions may not be solvable with the provided tools.
- Why unresolved: The authors attribute failure to "lack of compositional reasoning," but the analysis does not definitively rule out tool insufficiency as a contributing factor for the hardest problems.
- What evidence would resolve it: An oracle evaluation determining the exact percentage of "Hard" questions that possess a valid solution path within the current toolset versus those requiring unavailable tools.

## Limitations

- Security guarantee depends entirely on tool set being free of vulnerabilities and not allowing sensitive data leakage through aggregation patterns
- Performance gaps may reflect context window constraints or prompt engineering limitations rather than pure reasoning deficiencies
- No comprehensive benchmarking of frontier proprietary models (GPT-4o, Claude 3.5) on the same evaluation framework

## Confidence

**High Confidence:** The observed performance degradation from Easy (97.3%) to Hard (69.6%) tasks for gpt-oss-120b is well-supported by the evaluation methodology and clearly documented. The mechanism of tool-only security boundaries preventing arbitrary code execution is technically sound and directly demonstrated through the evaluation framework.

**Medium Confidence:** The claim that tool-based security is "fundamentally more secure" than code-generation approaches is reasonable but not empirically validated against real-world attack scenarios. The paper establishes a safer architectural pattern but doesn't prove it's immune to all attack vectors, particularly those exploiting tool composition or aggregate queries.

**Low Confidence:** The assertion that current LLMs lack strong compositional, multi-step reasoning ability is based on performance gaps in this specific tool-based context, but doesn't account for alternative reasoning approaches (like parallel tool use or different prompting strategies) that might improve performance without requiring architectural changes.

## Next Checks

1. **Security Audit of Tool Set:** Conduct a formal security analysis of the 100 tool implementations to verify they cannot be exploited for data leakage, even through complex compositions. Test edge cases like repeated filtering, aggregation queries, and error message handling.

2. **Proprietary Model Benchmarking:** Systematically evaluate frontier models (GPT-4o, Claude 3.5) on the same InData benchmarks to establish whether the performance gaps are inherent to LLM architecture or specific to open-source model limitations.

3. **Tool Set Expressiveness Analysis:** Identify specific operations that cannot be performed with the current 100 tools and quantify how many evaluation questions require these operations. This would reveal whether security boundaries come at the cost of practical expressiveness for real-world data analysis tasks.