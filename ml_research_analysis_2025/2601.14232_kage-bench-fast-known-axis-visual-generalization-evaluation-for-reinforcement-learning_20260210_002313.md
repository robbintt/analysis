---
ver: rpa2
title: 'KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement
  Learning'
arxiv_id: '2601.14232'
source_url: https://arxiv.org/abs/2601.14232
tags:
- train
- step
- eval
- visual
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KAGE-Bench addresses the challenge of evaluating visual generalization
  in pixel-based reinforcement learning agents. The core problem is that existing
  benchmarks entangle multiple sources of visual shift, making it difficult to attribute
  failures to specific visual factors.
---

# KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.14232
- Source URL: https://arxiv.org/abs/2601.14232
- Reference count: 40
- Primary result: Introduces KAGE-Bench, a controlled benchmark for evaluating visual generalization in pixel-based RL by isolating axis-specific visual shifts.

## Executive Summary
KAGE-Bench addresses the challenge of evaluating visual generalization in pixel-based reinforcement learning agents. Existing benchmarks entangle multiple sources of visual shift, making it difficult to attribute failures to specific visual factors. To solve this, KAGE-Bench introduces a framework where visual configurations are decomposed into independently controllable axes, and train–evaluation splits vary only one axis at a time. This design isolates the effect of each visual factor on agent performance.

The benchmark is built on KAGE-Env, a JAX-native platformer environment with up to 33 million environment steps per second, enabling fast and reproducible sweeps over visual parameters. Using a standard PPO-CNN baseline, KAGE-Bench reveals strong axis-dependent failures: background and photometric shifts often collapse success rates, while agent-appearance shifts are comparatively benign. Some shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. The benchmark provides a precise, controlled way to diagnose and quantify visual generalization issues in RL agents.

## Method Summary
KAGE-Bench evaluates visual generalization by decomposing visual configurations into independently controllable axes (background, filters, effects, etc.) and varying only one axis between train and evaluation splits while holding dynamics and rewards fixed. The benchmark is built on KAGE-Env, a JAX-native 2D platformer environment with up to 33 million environment steps per second. Using a PPO-CNN baseline, the method trains agents on specific visual configurations and evaluates them on paired configurations differing in only one visual axis. Performance is measured across four metrics: distance traveled, progress toward goal, success rate, and return. The benchmark provides 34 train–eval config pairs across six axis suites, enabling precise attribution of generalization failures to specific visual factors.

## Key Results
- PPO-CNN baseline shows strong axis-dependent generalization failures: photometric/lighting shifts cause near-complete task failure despite preserved locomotion
- Background diversity during training reduces generalization gaps along the background axis
- Some visual shifts preserve forward motion while breaking task completion, revealing that return alone can mask catastrophic generalization failures
- Axis-level analysis shows background and filter/effects shifts are most detrimental, while agent-appearance shifts are comparatively benign

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual generalization failures can be attributed to specific visual factors when observation shifts are isolated axis-by-axis.
- Mechanism: The environment factors visual configurations ξ into independently controllable axes (background, filters, effects, etc.). Train-eval pairs differ in only one axis while holding dynamics and rewards fixed, so performance gaps arise solely from how the pixel policy responds to different renderings of identical latent states.
- Core assumption: The transition kernel P and reward function r remain invariant across visual configurations (enforced by construction in KAGE-Env).
- Evidence anchors:
  - [abstract]: "KAGE-Bench introduces a framework where visual configurations are decomposed into independently controllable axes, and train–evaluation splits vary only one axis at a time."
  - [Section 4, Equation 8]: "J(π;M_ξ) - J(π;M_ξ') = J(π_ξ;M) - J(π_ξ';M)" — visual train-eval gap equals induced policy performance difference in the same latent MDP.
  - [corpus]: Tape benchmark (arxiv 2601.04695) similarly isolates OOD failures via controlled rule shifts, supporting the controlled-intervention design pattern.
- Break condition: If dynamics or rewards inadvertently vary with visual configuration, train-eval gaps could reflect task structure changes rather than visual sensitivity.

### Mechanism 2
- Claim: Standard PPO-CNN baselines exhibit axis-dependent generalization failures, with photometric/lighting shifts causing near-complete task failure despite preserved locomotion.
- Mechanism: Filters (brightness, contrast, hue) and effects (point lights) alter pixel statistics that CNNs exploit, collapsing success rates (0.83→0.11 for filters) while distance metrics drop only 11-21%. The policy learns spurious correlations with task-irrelevant visual features.
- Core assumption: The baseline architecture (3-layer CNN + MLP trunk) is representative of common pixel-based RL agents.
- Evidence anchors:
  - [Section 7, Table 1]: Filters show ∆SR = 86.8%, Effects show ∆SR = 80.5%, while ∆Dist remains 11.7% and 20.8% respectively.
  - [Section 7]: "Photometric and lighting perturbations primarily break completion... distance degradation is moderate, yet SR collapses."
  - [corpus]: Pre-trained Visual Representations paper (arxiv 2509.12531) finds PVMs help generalization under visual shifts, suggesting standard encoders are brittle.
- Break condition: If architecture changes substantially (e.g., different encoder, auxiliary losses), axis-specific failure patterns may shift.

### Mechanism 3
- Claim: Training with visual diversity along a target axis reduces generalization gaps for that axis.
- Mechanism: When multiple background images or distractor types appear during training, the policy learns to ignore axis-specific visual features and attend to task-relevant cues. Single-config training leads to overfitting on spurious correlations.
- Core assumption: The relationship between training diversity and generalization is monotonic within each axis.
- Evidence anchors:
  - [Section 7, Table 2]: Background configs 6-7 (3 images train → different image eval) show near-zero gaps, while config 1 (black → noise) shows 98.9% SR gap.
  - [Section 7]: "Within Background, training with more visual diversity reduces SR gaps."
  - [corpus]: No direct corpus evidence on diversity-generalization relationship in this specific benchmark context.
- Break condition: If diversity is introduced along multiple axes simultaneously, interactions may confound axis-specific attribution.

## Foundational Learning

- Concept: **Induced State Policy (π_ξ)**
  - Why needed here: This is the formal mechanism connecting visual shifts to performance changes. The induced policy marginalizes over observations: π_ξ(a|s) = ∫_Ω π(a|o)O_ξ(do|s).
  - Quick check question: If I change the background images but keep the same pixel policy, does the induced state policy change? (Answer: Yes, because the observation distribution O_ξ changes.)

- Concept: **Known-Axis Intervention**
  - Why needed here: Understanding why the benchmark isolates one visual factor at a time is essential for interpreting results and designing experiments.
  - Quick check question: Why can't I change both background AND lighting between train and eval and still attribute the gap? (Answer: The gap could come from either factor or their interaction.)

- Concept: **Trajectory-Level Metrics vs. Return**
  - Why needed here: Return alone can mask failures—a policy that moves forward but never completes the task may have similar return on train/eval despite catastrophic generalization.
  - Quick check question: If train success = 0.90 and eval success = 0.42 but return gap is small, what happened? (Answer: The policy preserves partial behaviors like forward motion while losing task completion.)

## Architecture Onboarding

- Component map: YAML Config → KAGE-Env (JAX-native renderer + simulator) → vmap/jit → 2^16 parallel environments → PPO-CNN (CleanRL-style) → policy π(a|o) → Evaluation on paired (ξ_train, ξ_eval) configs → Metrics: Distance, Progress, Success Rate, Return

- Critical path:
  1. Understand the YAML configuration schema (93 parameters across background, character, npc, distractors, filters, effects, layout, physics).
  2. Load config → create env → vmap reset/step functions.
  3. Train PPO-CNN on ξ_train config for 25M steps.
  4. Periodically evaluate on both ξ_train and ξ_eval (same latent dynamics, different rendering).
  5. Record maximum-over-training for each metric, aggregate across 10 seeds.

- Design tradeoffs:
  - JAX-native gives 33M steps/sec but requires GPU and JAX familiarity.
  - Simple platformer task reduces exploration confounds but may not reflect complexity of real domains.
  - Maximum-over-training aggregation captures peak transfer but may overestimate typical generalization.

- Failure signatures:
  - High train success + low eval success + moderate eval distance = photometric/distractor sensitivity (motion preserved, completion broken).
  - Low train success + low eval success = task learning failure (not a generalization issue).
  - Near-zero gap across all metrics = trivial shift or already-robust representation.

- First 3 experiments:
  1. Reproduce Table 1 axis-level results with PPO-CNN baseline on a single suite (e.g., Background) to validate setup.
  2. Add data augmentation (random crop, color jitter) during training and measure gap reduction on Filters suite.
  3. Sweep training background diversity (1, 3, 10, 50, 128 images) and plot generalization gap vs. diversity for Background configs.

## Open Questions the Paper Calls Out
- The paper calls out the need for future work on richer shifts and broader task families beyond the 2D platformer domain.
- It explicitly calls for testing alternative learning algorithms to assess whether different architectures or training methods can systematically reduce axis-specific generalization gaps.
- The paper notes that while the induced policy shift formulation provides a formal reduction, it is "purely representational" and does not assume optimality or suggest interventions, leaving open the question of whether this theory can be leveraged algorithmically.

## Limitations
- The controlled design assumes invariant dynamics and rewards across visual configurations, but this assumption is not empirically verified.
- Results are based on a single PPO-CNN baseline architecture, limiting generalizability to other RL algorithms.
- The platformer task, while simplifying confounding factors, may not capture the complexity of real-world visual generalization challenges.

## Confidence
- **High Confidence:** The core mechanism of isolating visual factors along independent axes (Mechanism 1) and the baseline PPO-CNN implementation details.
- **Medium Confidence:** The observed axis-dependent failure patterns (Mechanism 2) and the diversity-generalization relationship (Mechanism 3), as these depend on specific implementation details not fully specified.
- **Low Confidence:** The practical utility of the benchmark for predicting real-world generalization, given the simplified task domain.

## Next Checks
1. **Control Verification:** Systematically verify that dynamics and rewards remain invariant across all visual configurations by testing for statistical differences in transition probabilities and reward distributions.
2. **Architecture Sensitivity:** Replicate key results (e.g., Table 1) using alternative baseline architectures (e.g., ResNet encoder, data augmentation, contrastive pretraining) to assess robustness of observed failure patterns.
3. **Interaction Analysis:** Design experiments that deliberately vary multiple visual axes simultaneously to quantify the degree of interaction effects and validate the independence assumption underlying the known-axis decomposition.