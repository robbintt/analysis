---
ver: rpa2
title: Information Hidden in Gradients of Regression with Target Noise
arxiv_id: '2601.18546'
source_url: https://arxiv.org/abs/2601.18546
tags:
- noise
- covariance
- gradient
- data
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that gradients alone can recover the Hessian\
  \ (data covariance matrix \u03A3) in linear regression when Gaussian noise of variance\
  \ n is injected into the targets, where n is the batch size. Without calibration,\
  \ recovery can fail by a constant factor; with calibration, non-asymptotic operator-norm\
  \ guarantees are provided."
---

# Information Hidden in Gradients of Regression with Target Noise

## Quick Facts
- arXiv ID: 2601.18546
- Source URL: https://arxiv.org/abs/2601.18546
- Reference count: 40
- Primary result: Injecting Gaussian noise with variance equal to batch size into regression targets enables Hessian recovery from gradients alone.

## Executive Summary
This paper demonstrates that gradients alone can recover the Hessian (data covariance matrix Σ) in linear regression when Gaussian noise of variance n is injected into the targets, where n is the batch size. Without calibration, recovery can fail by a constant factor; with calibration, non-asymptotic operator-norm guarantees are provided. The method enables gradient-only applications such as preconditioning for faster optimization and adversarial risk estimation. Experiments confirm Hessian recovery and preconditioning benefits in both linear and nonlinear models on real and synthetic data.

## Method Summary
The method involves injecting Gaussian noise N(0, n) into regression targets where n is the batch size, computing batch gradients, and estimating the Hessian via empirical gradient covariance. For linear regression with sub-Gaussian inputs, the gradient covariance converges to the data covariance matrix Σ when noise variance equals batch size. The approach provides operator-norm guarantees for recovery and enables practical applications like preconditioning and adversarial risk estimation without requiring second-order derivatives.

## Key Results
- Injecting noise with variance equal to batch size causes gradient covariance to approximate the Hessian in linear regression
- Noise calibration is strictly necessary for recovery in Gaussian input settings
- Preconditioning with the recovered Hessian improves optimization convergence in both linear and nonlinear models
- The method works with unknown inherent noise, recovering Σ up to a constant factor

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting Gaussian target noise with variance equal to the batch size causes the empirical gradient covariance to converge to the data covariance (Hessian) in linear regression.
- **Mechanism:** In linear regression, the gradient covariance contains a term (σ²/n)Σ, where σ² is the target noise variance. By setting σ²=n, this term simplifies to Σ. Other "nuisance" terms dependent on the parameter error (w-w₀) are suppressed by sufficient sample complexity (k batches) and batch size (n).
- **Core assumption:** Sub-Gaussian inputs and a linear regression loss landscape.
- **Evidence anchors:** [abstract] "injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian"; [section 4.1, Lemma 4.2] "Cov(∇L(j)(w)) = σ²/n Σ + noise/n"
- **Break condition:** If batch size n is too small relative to dimension d, the estimation error may remain high.

### Mechanism 2
- **Claim:** This noise injection preserves the expected value of the gradient required for optimization.
- **Mechanism:** The injected noise is zero-mean Gaussian. When expanding the squared loss (y_noisy - ŷ)², the cross-term involving the noise vanishes in expectation, leaving the mean gradient direction unchanged.
- **Core assumption:** Noise is symmetric with zero mean.
- **Evidence anchors:** [abstract] "Our key insight is a simple variance calibration... noise does not corrupt mean gradients"; [section 1, Insight 2] "noise calibration preserves the accuracy of the gradient signal"
- **Break condition:** If the noise distribution is skewed or biased, the gradient estimator will drift from the true gradient.

### Mechanism 3
- **Claim:** Calibration is necessary; without matching noise to batch size, the gradient covariance fails to approximate the Hessian.
- **Mechanism:** If σ² is not O(n), the term (σ²/n)Σ either vanishes (if n grows but σ² is fixed) or scales incorrectly. Furthermore, bias terms depending on (w-w₀) can dominate the structure, leading to an operator norm error of Ω(1).
- **Core assumption:** Gaussian inputs (for the strict necessity proof).
- **Evidence anchors:** [section 5, Corollary 5.1.1] "recovery of Σ fails without noise injection... operator-norm distance... can remain Ω(1)"
- **Break condition:** If the model is far from the optimum (w ≠ w₀) and noise is uncalibrated, the estimate reflects parameter error rather than data curvature.

## Foundational Learning

- **Concept:** **Empirical Covariance Estimation**
  - **Why needed here:** The core operation is computing the sample covariance of batch gradients, S_g(w), and analyzing its convergence properties (sub-exponential concentration).
  - **Quick check question:** How does the concentration rate of the empirical covariance depend on the dimension d and the number of batches k?

- **Concept:** **Preconditioning (Second-Order Optimization)**
  - **Why needed here:** The recovered matrix Σ is used as a preconditioner (S_g)⁻¹ to accelerate gradient descent by aligning it with the curvature.
  - **Quick check question:** Why does inverting the Hessian approximation improve the condition number of the optimization problem?

- **Concept:** **Operator Norm**
  - **Why needed here:** The paper provides non-asymptotic guarantees using the operator norm ||·||ₒₚ to measure the distance between the estimate S_g and the true Hessian Σ.
  - **Quick check question:** Does a small operator norm error guarantee accurate recovery of the matrix eigenvectors, or just the eigenvalues?

## Architecture Onboarding

- **Component map:** Target Noiser -> Batch Gradient Computer -> Covariance Accumulator -> Preconditioner Solver

- **Critical path:**
  1. Determine batch size n
  2. Add Gaussian noise ε ~ N(0, n) to targets
  3. Collect k batch gradients
  4. Compute covariance S_g
  5. Apply (S_g)⁻¹ to the mean gradient before the optimizer step

- **Design tradeoffs:**
  - **Noise vs. Gradient Fidelity:** Higher noise variance (>n) improves the Hessian estimate scale recovery but might increase variance in the mean gradient estimate temporarily, though the paper claims preservation holds
  - **Batch Size (n):** Larger n improves estimation stability but requires proportional noise variance; small n may fail to satisfy sample complexity requirements (Theorem 4.1)

- **Failure signatures:**
  - **Scale Mismatch:** If the gradient covariance grows/shrinks unboundedly, check if noise variance was set to 1 or σ_data instead of batch size n
  - **Dimensionality Failure:** In high d with low k, the covariance estimate remains noisy and ill-conditioned
  - **Non-Linear Collapse:** For deep networks, this method only approximates the "block-diagonal" or local curvature (empirical extension), potentially failing to capture full interactions

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Run linear regression on Gaussian data with n=256, k=1000. Plot ||S_g - Σ||ₒₚ as noise variance deviates from n to confirm the "V-shape" or failure at 0
  2. **Convergence Speed:** Train a logistic regression model on a classification task (e.g., MNIST) using SGD vs. Noisy-Gradient-Preconditioned SGD. Measure epochs to accuracy
  3. **Robustness to inherent noise:** Add label noise N(0, σ²_true) to the data. Test if the "total noise = n" rule (adding n-σ²_true) works better than blindly adding n

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can theoretical guarantees for Hessian recovery via target noise injection be extended to non-linear models, specifically neural networks?
- **Basis:** [explicit] The conclusion states that "while our experiments show that target noise improves Hessian estimation in neural networks, its theoretical properties remain open."
- **Why unresolved:** The current theoretical analysis relies on linear regression assumptions where the Hessian equals the data covariance Σ.
- **What evidence would resolve it:** A proof showing operator-norm convergence of the gradient covariance to the Hessian for non-linear architectures, or counter-examples where the method fails.

### Open Question 2
- **Question:** Is the conjectured batch size complexity (n = Ω(R²/ε) and k = Õ(d/ε²)) achievable for uniform convergence?
- **Basis:** [explicit] The authors conjecture in the conclusion that the true complexity is lower than their proof suggests, suspecting the current n = Õ(d³/ε²) dependency is a proof artifact.
- **Why unresolved:** The current covering argument in Theorem 4.1 introduces dimension dependencies that may not be intrinsic to the statistical problem.
- **What evidence would resolve it:** A refined proof achieving the conjectured rates, or a lower bound proof demonstrating the necessity of higher dimension dependencies.

### Open Question 3
- **Question:** For which classes of non-Gaussian input distributions is target noise calibration strictly necessary for Hessian recovery?
- **Basis:** [inferred] Section 5 proves necessity for Gaussian inputs, but Lemma 5.2 provides a counter-example (Rademacher) where recovery succeeds without calibration.
- **Why unresolved:** The authors characterize the failure condition for Gaussian data but do not define the boundary for general sub-Gaussian distributions.
- **What evidence would resolve it:** A characterization of distributional properties (e.g., moment constraints) that determine if uncalibrated gradient covariance fails to approximate Σ.

## Limitations

- The theoretical analysis is limited to linear regression settings, with nonlinear extensions lacking formal guarantees
- Sample complexity bounds contain unknown absolute constants, making practical performance prediction difficult
- The method requires knowing or estimating inherent data noise variance for exact calibration
- In high-dimensional settings with limited batches, covariance estimation may fail to satisfy concentration requirements

## Confidence

- **High confidence:** The core mechanism of Hessian recovery through noise calibration in linear regression (supported by formal proofs and synthetic experiments)
- **Medium confidence:** The preconditioning benefits extending to nonlinear models (supported by experiments but lacks theoretical guarantees for deep networks)
- **Medium confidence:** The "set noise to n" rule being sufficient without knowing inherent noise (supported by experiments but limited theoretical analysis)
- **Low confidence:** Exact performance guarantees in high-dimensional regimes (sample complexity bounds exist but contain unknown constants)

## Next Checks

1. Test the method on a deep learning task (e.g., CIFAR-10 classification) to verify preconditioning benefits scale to realistic neural network architectures
2. Systematically vary the ratio of batch size to dimension (n/d) to empirically verify the theoretical sample complexity requirements
3. Evaluate the method's sensitivity to non-Gaussian input distributions by testing on heavy-tailed or correlated data distributions