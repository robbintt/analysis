---
ver: rpa2
title: Online Minimization of Polarization and Disagreement via Low-Rank Matrix Bandits
arxiv_id: '2510.00803'
source_url: https://arxiv.org/abs/2510.00803
tags:
- regret
- matrix
- opinions
- algorithm
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online minimization of polarization and disagreement
  in the Friedkin-Johnsen opinion dynamics model under incomplete information. Unlike
  prior work assuming full knowledge of innate opinions, this work addresses the more
  realistic online setting where innate opinions are unknown and must be learned through
  sequential observations.
---

# Online Minimization of Polarization and Disagreement via Low-Rank Matrix Bandits

## Quick Facts
- arXiv ID: 2510.00803
- Source URL: https://arxiv.org/abs/2510.00803
- Authors: Federico Cinus; Yuko Kuroki; Atsushi Miyauchi; Francesco Bonchi
- Reference count: 40
- One-line primary result: First theoretical guarantee for sequential interventions on opinion dynamics under incomplete information, achieving O(√T) regret through low-rank matrix bandit techniques.

## Executive Summary
This paper addresses the challenge of minimizing polarization and disagreement in social networks where the learner can only observe scalar feedback rather than full opinion vectors. The key innovation is recognizing that the unknown innate opinion matrix has a rank-one structure, enabling dimensionality reduction from O(|V|²) to O(|V|) through a two-stage algorithm. The approach combines nuclear-norm regularized least squares for subspace estimation with standard linear bandit techniques in the reduced space.

The theoretical analysis establishes that the algorithm achieves O(√T) cumulative regret under mild assumptions on the diversity of feasible interventions. This represents a significant advance over previous work that assumed full knowledge of innate opinions, making the framework more applicable to real-world scenarios where opinions must be learned through interaction.

## Method Summary
The method employs a two-stage approach: first, a uniform exploration phase where T₁ arms are sampled to estimate the underlying opinion subspace through nuclear-norm regularized least squares; second, a refinement phase using a linear bandit (OFUL) in the reduced 2|V|-1 dimensional space. The algorithm exploits the rank-one structure of the opinion matrix to achieve significant dimensionality reduction while maintaining theoretical regret guarantees.

## Key Results
- Achieves O(√T) cumulative regret under mild assumptions on intervention diversity
- First theoretical guarantee for sequential interventions on opinion dynamics under incomplete information
- Empirical validation shows significant improvement over linear bandit baseline in both cumulative regret and running time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The system achieves efficient learning by exploiting the rank-one structure of the unknown parameter matrix $\Theta^* = ss^\top$, reducing the effective dimension from $|V|^2$ to $2|V|-1$.
- **Mechanism:** Rather than treating the problem as a generic linear bandit in a high-dimensional space (vec$(ss^\top)$), the algorithm identifies that the signal lies in the span of the single opinion vector $s$. It estimates this subspace via nuclear-norm regularization and projects the arms (forest matrices) into this low-dimensional representation for the exploitation phase.
- **Core assumption:** The innate opinions are mean-centered, and the matrix $\Theta^*$ is strictly rank-one.
- **Evidence anchors:**
  - [abstract] "The algorithm first performs subspace estimation to identify an underlying low-dimensional structure..."
  - [Section 3] "Since $\Theta^*$ is rank-one, the problem reduces to a low-rank matrix bandits."
  - [corpus] Corpus signals primarily discuss generic regret minimization; specific evidence for rank-one opinion reduction is internal to this paper.
- **Break condition:** If the innate opinion vector $s$ is not the dominant singular vector (e.g., high noise or rank > 1), the subspace projection discards signal, causing high projection error.

### Mechanism 2
- **Claim:** Nuclear-norm regularized least squares allows recovery of the opinion subspace even when the action set consists of structured, discrete "forest matrices" rather than random continuous matrices.
- **Mechanism:** The paper utilizes a Restricted Strong Convexity (RSC) analysis tailored to forest matrices. By showing that uniform sampling over the feasible interventions satisfies RSC, the authors prove that the estimation error $\|\hat{\Theta} - \Theta^*\|_F^2$ decreases at a rate of $1/T_1$, preventing the sample complexity from exploding in high dimensions.
- **Core assumption:** The action set is sufficiently diverse such that $\kappa_{min}(\mathcal{X}) > 0$ (Assumption 2).
- **Evidence anchors:**
  - [Section 4.1] "We provide a novel theoretical analysis that explicitly leverages the Restricted Strong Convexity (RSC) condition for our specific set of structured actions."
  - [Proposition 1] Shows the high-probability bound on estimation error based on RSC.
  - [corpus] "Regret minimization in Linear Bandits with offline data" discusses exploration, but lacks specific RSC details for graph structures.
- **Break condition:** If the feasible interventions (arms) are collinear or lack diversity, $\kappa \to 0$, the RSC condition fails, and the subspace estimation error remains high regardless of sample size.

### Mechanism 3
- **Claim:** The two-stage "Explore-Subspace-Then-Refine" strategy guarantees $\tilde{O}(\sqrt{T})$ cumulative regret.
- **Mechanism:** The algorithm separates the learning process into an initial exploration phase (Stage 1) to estimate the subspace and a refinement phase (Stage 2) using a linear bandit (e.g., OFUL) in the reduced space. The total regret is the sum of the exploration cost (linear in $T_1$), the projection error (inversely proportional to $T_1$), and the subspace regret.
- **Core assumption:** The duration of the exploration phase $T_1$ is chosen optimally relative to the signal strength $\|s\|$ and curvature $\kappa$.
- **Evidence anchors:**
  - [Theorem 4.1] "The overall regret decomposes into a projection error due to misalignment... and the standard regret..."
  - [Section 4.3] Details the trade-off between exploration cost and bias due to subspace misalignment.
  - [corpus] "Generalized Kernelized Bandits" mentions dimension-free inequalities, supporting the logic of dimension reduction, but not the specific decomposition.
- **Break condition:** If the exploration phase $T_1$ is too short, the subspace estimation is poor, and the "projection error" term dominates the regret, potentially causing linear regret if the misalignment is severe.

## Foundational Learning

- **Concept:** Friedkin-Johnsen (FJ) Opinion Dynamics
  - **Why needed here:** This defines the environment. The "loss" (polarization + disagreement) is a quadratic function of the equilibrium opinions, which are determined by the FJ model. You cannot interpret the arms (interventions) or the feedback signal without understanding that expressed opinions converge to $z^* = (I+L)^{-1}s$.
  - **Quick check question:** If the network Laplacian $L$ changes (an intervention), does the equilibrium opinion $z^*$ change linearly or non-linearly with respect to $s$?

- **Concept:** Linear Bandits (OFUL)
  - **Why needed here:** This is the "Refine" stage engine. Once the problem is projected into a low-dimensional subspace, it becomes a standard linear bandit problem where the algorithm must balance optimism and uncertainty.
  - **Quick check question:** In a linear bandit, how does the confidence ellipsoid size relate to the number of times an arm has been pulled?

- **Concept:** Nuclear Norm Regularization
  - **Why needed here:** This is the "Explore" stage tool. It is the convex relaxation of the rank constraint, allowing the algorithm to recover the low-rank matrix $\Theta^*$ from limited scalar observations.
  - **Quick check question:** Why is the nuclear norm (sum of singular values) used instead of the Frobenius norm to encourage low-rank solutions?

## Architecture Onboarding

- **Component map:** Environment -> Agent (OPD-Min-ESTR) -> Subspace Estimator -> Projector -> Linear Bandit Optimizer

- **Critical path:**
  1. **Generate Arms:** Create feasible forest matrices $X_i = (I+L_i)^{-1}$ from Laplacians.
  2. **Stage 1 Exploration:** Uniformly sample arms, observe scalar loss.
  3. **Subspace Recovery:** Solve convex optimization to find $\hat{s}$; form rotation matrix $[\hat{s}, \hat{S}_\perp]$.
  4. **Stage 2 Optimization:** Project arms, run OFUL to minimize cumulative regret.

- **Design tradeoffs:**
  - **Exploration length $T_1$:** Must be large enough to satisfy RSC ($128\tau^2_{T_1} \leq \kappa$) but small enough to avoid excessive linear regret during exploration.
  - **Arm diversity vs. control:** Theoretical guarantees depend on $\kappa_{min}(\mathcal{X})$. "Diverse" arms help estimation but may correspond to unrealistic network interventions. "Local" interventions are realistic but may yield weak curvature (near-zero $\kappa$).

- **Failure signatures:**
  - **High Variance in Estimation:** If $\kappa$ is small (low arm diversity), $\hat{\Theta}$ will be a poor estimate, leading to high projection error and potentially worse performance than the full-dimensional baseline.
  - **Runtime bottleneck:** Stage 1 requires Singular Value Thresholding (SVT), which can be $O(|V|^3)$. For very large graphs ($|V| > 1000$), this becomes a bottleneck without approximation (e.g., power method).

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Run OPD-Min on a small Erdős–Rényi graph ($|V|=16$) with diverse arms. Verify that cumulative regret grows as $\tilde{O}(\sqrt{T})$ and outperforms the $|V|^2$-dimensional OFUL baseline.
  2. **Stress Test (Curvature):** Generate an arm set with "Local" interventions (low diversity/small $\kappa$). Confirm if the estimation error bound in Proposition 1 loosens and regret increases compared to the "Diverse" arm setting.
  3. **Scalability Check:** Measure wall-clock time for Stage 1 subspace estimation as $|V|$ increases (e.g., 32, 128, 1024). Verify if the practical runtime scales better than the worst-case $O(|V|^3)$ due to matrix structure.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on Assumption 2 (positive curvature κ_min(X) > 0), which requires sufficient diversity among feasible interventions
- Nuclear-norm optimization in Stage 1 has computational complexity that scales poorly with graph size |V|
- Performance may degrade significantly when arms are too similar or locally restricted

## Confidence
- **High:** O(√T) regret bound under stated assumptions, effectiveness of subspace reduction from |V|² to 2|V|-1 dimensions
- **Medium:** Practical performance claims relative to full-dimensional baseline (depends on specific implementation choices not fully detailed)
- **Low:** Exact computational complexity analysis and scalability to large graphs

## Next Checks
1. **RSC Sensitivity Test:** Systematically vary arm diversity (κ_min(X)) and measure corresponding estimation error and regret to validate the dependence on curvature.
2. **Implementation Validation:** Reproduce results using the assumed nuclear-norm solver parameters and Laplacian perturbation scheme to verify claimed performance gains.
3. **Scalability Benchmark:** Measure wall-clock time for the full algorithm as |V| increases from 32 to 1024 to empirically assess computational limits.