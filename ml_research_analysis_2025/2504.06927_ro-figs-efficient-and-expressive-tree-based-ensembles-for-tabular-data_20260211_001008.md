---
ver: rpa2
title: 'RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data'
arxiv_id: '2504.06927'
source_url: https://arxiv.org/abs/2504.06927
tags:
- ro-figs
- splits
- features
- uni00000048
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RO-FIGS, a novel tree-based ensemble method
  for tabular data that learns oblique splits as linear combinations of features to
  capture interactions and improve model expressiveness. Building on the FIGS framework,
  RO-FIGS uses a minimum impurity decrease stopping criterion and random subsets of
  features to define splits, allowing it to construct compact yet high-performing
  models.
---

# RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data

## Quick Facts
- arXiv ID: 2504.06927
- Source URL: https://arxiv.org/abs/2504.06927
- Reference count: 40
- Primary result: RO-FIGS achieves superior or comparable accuracy to state-of-the-art methods while producing significantly smaller models with fewer than five trees and under 15 splits.

## Executive Summary
This paper introduces RO-FIGS, a novel tree-based ensemble method for tabular data that learns oblique splits as linear combinations of features to capture interactions and improve model expressiveness. Building on the FIGS framework, RO-FIGS uses a minimum impurity decrease stopping criterion and random subsets of features to define splits, allowing it to construct compact yet high-performing models. Evaluated on 22 real-world binary classification datasets, RO-FIGS achieves superior or comparable accuracy to state-of-the-art tree- and neural network-based methods, while producing significantly smaller models—often with fewer than five trees and under 15 splits. The method also provides enhanced interpretability by revealing feature interactions directly within splits, offering insights beyond standard post-hoc tools like SHAP. RO-FIGS is especially well-suited for applications requiring a balance between accuracy and interpretability.

## Method Summary
RO-FIGS is a tree-based ensemble method that extends the FIGS framework by incorporating oblique splits (linear combinations of features) instead of axis-aligned splits. The method iteratively adds one oblique split at a time to the model, optimizing weights via gradient descent to minimize an objective combining L1/2 regularization and impurity reduction. A minimum impurity decrease stopping criterion with retry attempts prevents overfitting while allowing necessary complexity. The algorithm uses random beam search to select feature subsets for split optimization, balancing expressiveness with computational efficiency. The model makes predictions by summing outputs from all trees in the ensemble.

## Key Results
- Achieves superior or comparable accuracy to state-of-the-art tree- and neural network-based methods on 22 binary classification datasets
- Produces significantly smaller models (often <5 trees, <15 splits) compared to competitors
- Provides enhanced interpretability by revealing feature interactions directly within splits
- Particularly effective for applications requiring balance between accuracy and interpretability

## Why This Works (Mechanism)

### Mechanism 1: Oblique Split Expressiveness
If decision boundaries in tabular data are diagonal or involve feature interactions, linear combinations of features (oblique splits) approximate these boundaries with fewer splits than axis-aligned trees. Instead of $Feature_A \leq t$, the model optimizes $\sum w_i F_i \leq t$. This allows a single split to partition the feature space along a hyperplane, capturing interactions directly.

### Mechanism 2: Additive Residual Fitting (Greedy Sums)
If the data exhibits additive structure (sum of simpler functions), fitting trees greedily to residuals allows the model to decompose the problem naturally. RO-FIGS generalizes CART by summing predictions from multiple trees, adding one split at a time to minimize residual error.

### Mechanism 3: Regularized Stopping Criterion
If training is halted based on impurity decrease rather than fixed tree depth/count, the model balances performance and compactness (interpretability) more effectively. RO-FIGS employs a minimum impurity decrease threshold, preventing overfitting to noise while allowing necessary complexity.

## Foundational Learning

- **Oblique Decision Trees (Multivariate Trees)**: Why needed: RO-FIGS is fundamentally an oblique tree method. Quick check: Can you explain why an oblique split is geometrically more efficient at separating a diagonal class boundary than a standard axis-aligned split?

- **Gradient Descent for Split Optimization**: Why needed: The paper explicitly uses gradient descent to learn the weights ($w$) for the splits. Quick check: How does the objective function $||w||_{1/2} + C \cdot g(w, b)$ balance split complexity (sparsity) against impurity reduction?

- **Greedy-Tree Sums (FIGS)**: Why needed: RO-FIGS extends FIGS. Quick check: In the RO-FIGS iteration loop, does the algorithm add a whole new tree or just a single split node to the existing structure?

## Architecture Onboarding

- **Component map**: Input -> Feature Selector -> Split Optimizer -> Model Manager -> Controller
- **Critical path**: 1) Calculate residuals of current ensemble 2) Select random subset of features (Beam) 3) Optimize linear combination weights via gradient descent 4) Evaluate potential splits across all current leaves 5) Commit best split if impurity decrease > threshold; else retry or stop
- **Design tradeoffs**: Beam Size vs. Speed (larger beam captures more interactions but scales training time quadratically); Interpretability vs. Accuracy (oblique splits are more expressive but harder to read); Regularization ($L_{1/2}$) promotes sparsity for interpretability
- **Failure signatures**: High-dimensional hang (training stalls on datasets with thousands of features); Oscillation (if MIN_IMP_DEC is too high and BEAM_SIZE too low, causing early termination)
- **First 3 experiments**: 1) Sanity Check (Diabetes): Verify model builds a single tree with very few splits to confirm "compactness" hypothesis 2) Ablation (Beam Size): Fix MIN_IMP_DEC and vary BEAM_SIZE to plot accuracy vs. # features per split 3) Scalability Test: Compare training time against CatBoost on dataset with >400 features to validate computational cost warnings

## Open Questions the Paper Calls Out
- Can more efficient feature construction or selection heuristics significantly reduce computational cost without compromising compactness or accuracy?
- Does developing a native method for handling nominal features improve RO-FIGS's performance on mixed-type datasets compared to current reliance on One-Hot Encoding?
- To what extent does the use of the $L_{1/2}$ norm for regularization contribute to model sparsity and performance compared to standard $L_1$ or $L_2$ regularization?

## Limitations
- Computational complexity is a significant limitation, with split optimization scaling as O(irm²n²d), making the method potentially infeasible for high-dimensional datasets
- The random beam search for feature selection may miss important feature combinations, potentially limiting performance on datasets where key interactions involve many features
- The paper does not specify the regularization strength C used in the split optimization objective, which is critical for controlling feature sparsity

## Confidence
- **High confidence**: The mechanism of oblique splits providing more expressive decision boundaries than axis-aligned splits is well-supported by geometric intuition and experimental results
- **Medium confidence**: The additive residual fitting approach is theoretically sound, but its effectiveness on complex functions relies heavily on specific results presented
- **Medium confidence**: The impurity-based stopping criterion's effectiveness is demonstrated on tested datasets, but generalizability to extremely noisy datasets is uncertain

## Next Checks
1. Perform an ablation study varying the unknown regularization parameter C to determine its optimal value and assess sensitivity
2. Test RO-FIGS on a high-dimensional dataset (>400 features) to empirically validate computational complexity warnings and identify practical limits
3. Compare the interpretability of RO-FIGS oblique splits against standard post-hoc tools like SHAP on a dataset with known feature interactions to verify claimed enhanced interpretability benefits