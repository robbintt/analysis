---
ver: rpa2
title: Predicting Early-Onset Colorectal Cancer with Large Language Models
arxiv_id: '2506.11410'
source_url: https://arxiv.org/abs/2506.11410
tags:
- cancer
- predic
- were
- colorectal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explored the use of large language models (LLMs) to\
  \ predict early-onset colorectal cancer (EoCRC) in individuals aged 18\u201344 using\
  \ EHR data. Compared to 10 traditional ML models, a fine-tuned GPT-4o LLM achieved\
  \ the highest performance with 73% sensitivity and 91% specificity, outperforming\
  \ conventional models."
---

# Predicting Early-Onset Colorectal Cancer with Large Language Models

## Quick Facts
- **arXiv ID**: 2506.11410
- **Source URL**: https://arxiv.org/abs/2506.11410
- **Reference count**: 0
- **Primary result**: Fine-tuned GPT-4o LLM achieved 73% sensitivity and 91% specificity for early-onset CRC prediction, outperforming 10 traditional ML models

## Executive Summary
This study explored the use of large language models (LLMs) to predict early-onset colorectal cancer (EoCRC) in individuals aged 18–44 using EHR data. Compared to 10 traditional ML models, a fine-tuned GPT-4o LLM achieved the highest performance with 73% sensitivity and 91% specificity, outperforming conventional models. The LLM leveraged clinical codes, lab results, and observations from 6 months prior to diagnosis. Feature importance analysis and natural language explanations enhanced interpretability. The results demonstrate LLMs' potential for improving early cancer detection, particularly in younger populations, and offer a promising alternative to standard screening methods by identifying high-risk individuals before symptom onset.

## Method Summary
The study used EHR data from the Truveta database to predict early-onset colorectal cancer in patients aged 18-44. The pipeline included: cohort definition with specific ICD/SNOMED codes, 7-month history requirement filtering, feature extraction for ML (11,509 features) versus text serialization for LLM, model training with 10 traditional classifiers and GPT-4o fine-tuning, threshold optimization using Youden's J Index, and evaluation on 1%-prevalence test sets. The fine-tuned GPT-4o achieved 73% sensitivity and 91% specificity, outperforming most traditional ML models through supervised fine-tuning on domain-specific clinical data and Chain-of-Thought prompting for interpretability.

## Key Results
- Fine-tuned GPT-4o LLM achieved 73% sensitivity and 91% specificity, outperforming 10 traditional ML models
- The LLM provided natural language explanations for predictions through Chain-of-Thought prompting
- Feature importance analysis identified key predictors of early-onset CRC from clinical codes and lab results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning of LLMs on domain-specific clinical data improves rare disease prediction sensitivity by learning dataset-specific patterns.
- Mechanism: The base GPT-4o model contains general medical knowledge from pre-training, but supervised fine-tuning for 2 epochs on the EoCRC training set allows the model to learn specific associations between patient conditions, lab results, and observations that are predictive of early-onset CRC in this particular population.
- Core assumption: The training data contains signal patterns that generalize to unseen test cases and are not artifacts of the specific health systems sampled.
- Evidence anchors:
  - [abstract]: "the fine-tuned LLM achieved an average of 73% sensitivity and 91% specificity, outperforming most traditional ML models"
  - [section]: "supervised fine-tuning allowed LLM to learn the specific patterns in the training data and enhance overall performance... the fine-tuned GPT4-o model improved sensitivity or recall by more than 10%"
  - [corpus]: No direct corpus evidence on LLM fine-tuning for CRC prediction; related papers focus on CNN and transfer learning approaches.
- Break condition: If training data contains systematic coding biases or the fine-tuning set is not representative of deployment population, the model may learn spurious correlations that fail to generalize.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting with structured clinical guidelines enables more interpretable predictions and potentially more stable reasoning.
- Mechanism: The prompt template includes five components—role definition, CoT reasoning steps, USPSTF guidelines, output format, and patient-specific input—which guide the LLM to process complex longitudinal patient data through explicit reasoning rather than pattern matching alone. This yields natural language explanations alongside predictions.
- Core assumption: The LLM can reliably follow multi-step reasoning chains and apply abstract guidelines to specific patient presentations.
- Evidence anchors:
  - [abstract]: "offering a more interpretable, natural language explanation for predictions"
  - [section]: "Chain-of-Thought prompting could lead to more stable reasoning and consistent answers" and "LLM on other hand could provide a more intuitive explanation in the generated text"
  - [corpus]: No corpus evidence on CoT for clinical prediction; corpus papers address explainability through XAI integration (R-Net) and expert systems (ColonScopeX).
- Break condition: If patient data contains contradictory signals or the reasoning steps are not well-specified, CoT may produce plausible-sounding but unreliable justifications.

### Mechanism 3
- Claim: Threshold optimization using Youden's J Index on imbalanced validation sets enables better sensitivity-specificity trade-offs for rare disease screening.
- Mechanism: Given the extreme class imbalance (1% CRC prevalence in test sets), the authors use 10-fold cross-validation with deliberately imbalanced validation folds to determine optimal decision thresholds that maximize Youden's J (Sensitivity + Specificity - 1), rather than using default 0.5 thresholds which would be inappropriate for rare events.
- Core assumption: The 1% prevalence in test sets approximates real-world prevalence, and the optimal threshold will transfer to deployment.
- Evidence anchors:
  - [abstract]: Study targets EoCRC in age <45, which represents <1% of that population
  - [section]: "The decision threshold for each model was optimized to balance sensitivity and specificity, given the very low prevalence of CRC cases in our test sets. The optimal threshold was determined using Youden's J Index"
  - [corpus]: No corpus evidence on threshold optimization approaches.
- Break condition: If deployed population has different prevalence or cost structures for false positives vs. false negatives, the threshold will need recalibration.

## Foundational Learning

- **Sensitivity vs. Specificity vs. PPV in rare disease contexts**:
  - Why needed here: With 1% prevalence, even 91% specificity yields low precision (7.6% PPV); understanding this relationship is essential for interpreting results and setting expectations for screening applications.
  - Quick check question: If a model has 73% sensitivity and 91% specificity on a population with 1% disease prevalence, what is the positive predictive value? (Answer: ~7.5%)

- **Class imbalance strategies (undersampling vs. SMOTE vs. threshold tuning)**:
  - Why needed here: The paper uses balanced training sets (undersampling) but imbalanced test sets (1% prevalence); understanding why this separation matters prevents data leakage and overoptimistic estimates.
  - Quick check question: Why might LightGBM achieve 100% sensitivity but only 8.4% specificity on the imbalanced test set despite strong cross-validation performance?

- **LLM fine-tuning mechanics (epochs, temperature, token limits)**:
  - Why needed here: The paper uses only 2 epochs, temperature=0, and max_tokens=4096; understanding these hyperparameter choices helps assess reproducibility and plan experiments.
  - Quick check question: Why would temperature=0 be preferred for a clinical prediction task over higher values?

## Architecture Onboarding

- **Component map**: Truveta EHR database → ICD/SNOMED/LOINC code extraction → 6-month lookback window (months 2-7 pre-diagnosis) → Feature engineering (ML) vs. text serialization (LLM) → Model training (10 ML + GPT-4o) → Threshold optimization → Evaluation on 1% prevalence test sets

- **Critical path**:
  1. Cohort definition (ICD/SNOMED curation by gastroenterologist)
  2. 7-month history requirement filtering (excludes patients with <7 months of data)
  3. Feature extraction/formatting (diverges for ML vs. LLM pipelines)
  4. Model training/fine-tuning
  5. Threshold optimization (Youden's J on imbalanced validation)
  6. Evaluation on 1% prevalence test sets

- **Design tradeoffs**:
  - ML: High feature count (11,509) captures subtle signals but creates sparsity; regularization mitigates overfitting but may obscure rare predictors
  - LLM: No explicit feature engineering needed, but API costs and latency are higher; natural language explanations improve interpretability but cannot be used for ROC curves (outputs are Likert-scale probabilities, not continuous scores)
  - Training set balancing: Undersampling improves CRC class learning but discards non-CRC information; SMOTE not applicable to LLMs without synthetic text generation

- **Failure signatures**:
  - LightGBM/HGBoost: 100% sensitivity with near-0% specificity → overfitting to positive class on imbalanced data
  - Low precision across all models (~1-7.6%): Expected given 1% prevalence; not a failure but a baseline constraint
  - Exclusion of patients with <7 months of data: May systematically exclude healthier individuals who present late, creating selection bias

- **First 3 experiments**:
  1. Reproduce the balanced training / imbalanced test split protocol with a simpler baseline (logistic regression) to validate the evaluation pipeline before attempting LLM fine-tuning.
  2. Run ablation on the prompt template components (remove CoT, remove guidelines, remove lab values) to identify which elements contribute most to the 10% sensitivity gain from fine-tuning.
  3. Test threshold robustness by evaluating the fine-tuned LLM across a range of prevalence rates (0.5%, 1%, 2%) to understand how PPV and clinical utility change with population composition.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do alternative large language models (e.g., Llama3, Gemini) compare to GPT-4o in reasoning capability and predictive performance for early-onset colorectal cancer?
  - Basis in paper: [explicit] The authors explicitly state in the conclusion: "In future work, we plan to evaluate other LLMs, like Meta’s Llama3 and Google’s Gemini to gain better understanding of LLMs’ reasoning capability in CRC prediction."
  - Why unresolved: The current study restricted its evaluation solely to OpenAI's GPT-4o model, leaving the generalizability of these findings across different model architectures unconfirmed.
  - What evidence would resolve it: A comparative study benchmarking the sensitivity, specificity, and explanation quality of Llama3 and Gemini against GPT-4o using the same EHR dataset.

- **Open Question 2**: Can synthetic longitudinal patient events be used to augment training data for LLMs to improve rare disease prediction?
  - Basis in paper: [explicit] The authors note that while SMOTE is used in ML, it is difficult to apply to LLMs and state: "synthetic longitudinal patient events with realistic EHR medical coding would be needed... We leave this for future work."
  - Why unresolved: The current study relied on undersampling the majority class (non-CRC), potentially losing useful information, because the technology to generate realistic synthetic clinical text for LLM training is not yet implemented.
  - What evidence would resolve it: Development of a pipeline for generating synthetic patient journeys and a demonstration of improved model performance (e.g., higher F1-score) when training on this augmented dataset.

- **Open Question 3**: Can a pre-trained clinical language model better capture the nuances of patient journeys with sparse or irregular data compared to general-purpose LLMs?
  - Basis in paper: [explicit] The authors suggest: "One potential avenue for future research is to pre-train a clinical language model that captures the nuances in different patient journeys through a large amount of longitudinal medical records."
  - Why unresolved: The study excluded patients with less than 7 months of continuous data due to the need for sufficient signals, indicating that current models struggle with the data sparsity common in younger, healthier populations.
  - What evidence would resolve it: A study demonstrating that a clinically pre-trained model maintains high predictive accuracy even when applied to patients with short or fragmented medical histories.

## Limitations
- Model interpretability limitations: Natural language explanations are post-hoc justifications rather than true causal reasoning, and have not been validated through clinician review
- Generalizability constraints: Performance on other EHR platforms with different coding practices remains unknown due to training on specific health systems
- Deployment readiness gaps: Study demonstrates technical feasibility but does not address clinical workflow integration, regulatory approval, or impact on patient outcomes

## Confidence

- **High confidence**: Comparative performance results between fine-tuned LLM and traditional ML models are well-supported by experimental design and statistical analysis; the 10% sensitivity improvement is robust across multiple test runs
- **Medium confidence**: Interpretability claims are supported by the mechanism (CoT prompting) but not validated through clinician review or correlation with actual clinical reasoning patterns
- **Low confidence**: Clinical utility claims (improving early detection, reducing late-stage diagnoses) are speculative and not directly tested in this study

## Next Checks
1. **External validation study**: Test the fine-tuned LLM on EHR data from health systems not represented in the original training set, using identical preprocessing and evaluation protocols to assess true generalization performance.
2. **Clinical explanation validation**: Have practicing gastroenterologists review the LLM's natural language explanations for a sample of predictions, rating their accuracy, completeness, and clinical usefulness compared to actual reasoning.
3. **Real-world deployment pilot**: Implement the model in a controlled clinical setting to measure actual impact on screening rates, time-to-diagnosis, and patient outcomes in the target 18-44 age population.