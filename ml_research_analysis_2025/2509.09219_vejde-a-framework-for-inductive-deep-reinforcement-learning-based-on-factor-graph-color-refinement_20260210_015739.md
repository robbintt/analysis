---
ver: rpa2
title: 'Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor
  Graph Color Refinement'
arxiv_id: '2509.09219'
source_url: https://arxiv.org/abs/2509.09219
tags:
- learning
- graph
- policy
- which
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Vejde, a framework that combines data abstraction,
  graph neural networks, and reinforcement learning to produce inductive policy functions
  for decision problems with richly structured states, such as object classes and
  relations. MDP states are represented as databases of facts about entities, and
  Vejde converts each state to a bipartite graph, which is mapped to latent states
  through neural message passing.
---

# Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement

## Quick Facts
- arXiv ID: 2509.09219
- Source URL: https://arxiv.org/abs/2509.09219
- Reference count: 40
- Combines data abstraction, graph neural networks, and reinforcement learning to produce inductive policy functions for decision problems with richly structured states

## Executive Summary
This paper presents Vejde, a framework that combines data abstraction, graph neural networks, and reinforcement learning to produce inductive policy functions for decision problems with richly structured states, such as object classes and relations. MDP states are represented as databases of facts about entities, and Vejde converts each state to a bipartite graph, which is mapped to latent states through neural message passing. The factored representation of both states and actions allows Vejde agents to handle problems of varying size and structure.

The authors evaluated Vejde agents on eight problem domains defined in RDDL, with ten problem instances each, where policies were trained using both supervised and reinforcement learning. To test policy generalization, they separated problem instances into two sets, one for training and the other solely for testing. Test results on unseen instances for the Vejde agents were compared to MLP agents trained on each problem instance, as well as the online planning algorithm Prost.

## Method Summary
Vejde represents MDP states as databases of facts about entities and converts each state to a bipartite graph. The framework maps these graphs to latent states through neural message passing, leveraging graph neural networks for inductive learning. The factored representation of both states and actions enables the framework to handle problems of varying size and structure. Agents are trained using both supervised and reinforcement learning approaches, with evaluation focused on generalization to unseen problem instances.

## Key Results
- Vejde policies generalize to test instances without significant loss in score
- Inductive agents received scores on unseen test instances that were close to instance-specific MLP agents
- Framework handles problems of varying size and structure through factored state and action representation

## Why This Works (Mechanism)
The framework leverages graph neural networks to process richly structured state representations as bipartite graphs. By using message passing over these graphs, Vejde can capture relational information and object interactions that are critical for decision-making in structured environments. The factored representation of states and actions allows the model to generalize across problems of different sizes and structures, rather than learning instance-specific policies.

## Foundational Learning
- **Graph Neural Networks**: Needed for processing relational data and capturing interactions between entities; Quick check: Verify message passing effectively aggregates neighborhood information
- **Reinforcement Learning**: Required for learning optimal policies through interaction with environment; Quick check: Confirm policy converges to reasonable solutions
- **Factor Graph Color Refinement**: Essential for handling variable-sized state spaces; Quick check: Validate that color refinement correctly groups similar entities
- **Bipartite Graph Construction**: Critical for representing state-entity relationships; Quick check: Ensure bipartite structure preserves all relevant relational information
- **Supervised Learning**: Used for pre-training or initialization; Quick check: Verify supervised signals provide useful guidance for policy learning

## Architecture Onboarding

**Component Map**: State Database -> Bipartite Graph Construction -> Graph Neural Network Message Passing -> Latent State Representation -> Policy Network -> Action Selection

**Critical Path**: The critical path flows from state representation through graph neural network processing to policy output. Each state is first converted to a bipartite graph, then processed through multiple rounds of message passing to generate latent representations, which feed into the policy network for action selection.

**Design Tradeoffs**: The framework trades computational efficiency for expressiveness by using graph neural networks, which can be more expensive than traditional MLP approaches but capture relational structure. The choice of bipartite graph representation balances expressiveness with computational tractability.

**Failure Signatures**: Poor generalization may indicate insufficient message passing depth or inadequate graph construction. Performance degradation on larger instances could suggest scaling limitations in the neural architecture. Failure to capture relational structure might manifest as suboptimal policies that ignore important state interactions.

**First Experiments**: 1) Test graph construction on sample states to verify correct bipartite representation; 2) Validate message passing implementation by checking latent state consistency across similar structures; 3) Evaluate policy performance on a simple domain before scaling to complex RDDL problems.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to eight RDDL domains, raising questions about generalization to other problem classes
- Comparison with MLP agents and Prost is confined to relative performance within the specific benchmark suite
- Does not demonstrate results on real-world applications or larger, more complex environments
- Does not report on computational efficiency or training stability across domains

## Confidence
- **Generalization performance**: Medium - supported by experimental results but limited to fixed set of problem instances
- **Handling varying size and structure**: Low - claimed but not systematically demonstrated across wide range of problem sizes
- **Performance relative to baselines**: Medium - comparisons exist but are limited to specific benchmark suite

## Next Checks
1. Evaluate Vejde on additional problem domains outside the RDDL benchmark suite to assess generalization to new problem classes
2. Conduct systematic scaling experiments to determine the limits of Vejde's ability to handle varying state and action sizes
3. Compare Vejde's performance and computational efficiency against a broader set of baselines, including other state-of-the-art inductive RL methods, to better contextualize its strengths and weaknesses