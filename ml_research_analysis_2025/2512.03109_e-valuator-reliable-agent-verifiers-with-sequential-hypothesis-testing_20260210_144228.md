---
ver: rpa2
title: 'E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing'
arxiv_id: '2512.03109'
source_url: https://arxiv.org/abs/2512.03109
tags:
- 'false'
- rate
- alarm
- verifier
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliably detecting unsuccessful
  trajectories in agentic AI systems, which execute sequences of actions like reasoning
  steps or tool calls. Existing verifier models, such as LLM judges or process-reward
  models, provide heuristic scores but lack guarantees on correctness when used to
  decide trajectory success, particularly regarding the false alarm rate (incorrectly
  flagging successful trajectories).
---

# E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing

## Quick Facts
- arXiv ID: 2512.03109
- Source URL: https://arxiv.org/abs/2512.03109
- Authors: Shuvom Sadhuka; Drew Prinster; Clara Fannjiang; Gabriele Scalia; Aviv Regev; Hanchen Wang
- Reference count: 40
- Primary result: Controls false alarm rate at any step of an agent trajectory while enabling up to 90% accuracy with 80% token savings.

## Executive Summary
This paper introduces e-valuator, a method for reliably detecting unsuccessful trajectories in agentic AI systems using sequential hypothesis testing. Traditional verifier models like LLM judges or process-reward models lack guarantees on correctness when used to decide trajectory success. E-valuator frames this as a statistical testing problem, distinguishing between verifier score distributions for successful and unsuccessful trajectories using e-processes. The method provides anytime-valid control of false alarm rates, even when the total trajectory length is unknown, and requires only black-box access to the verifier with minimal calibration data.

## Method Summary
E-valuator converts verifier scores to density ratios, creating a test martingale that enables anytime-valid false alarm control. The method trains stepwise classifiers to estimate p(Y=1|S[1:t]) for each time step t, then computes density ratios M̂_t = (1-f̂_t)/f̂_t × π̂_1/(1-π̂_1). These ratios form an e-process that, by Ville's inequality, guarantees Pr[sup_t M_t ≥ 1/α] ≤ α under the null hypothesis. A calibration dataset is split to train density ratio estimators and determine thresholds, with the option for a PAC threshold variant that provides higher power under finite trajectory assumptions.

## Key Results
- Controls false alarm rate below α across six datasets (GSM8k, MATH, HotpotQA, MedQA, MMLU-Pro, LiChess) and three agents
- Achieves up to 90% of original accuracy with 80% token savings through early termination
- Outperforms baselines (raw verifier, calibrated verifier, Bonferroni test) in both false alarm rate control and statistical power
- Demonstrates applicability beyond LLM agents, including to a chess engine setting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Converting verifier scores to density ratios creates a test martingale enabling anytime-valid false alarm control
- **Mechanism**: The density ratio M_t = p_0(S[1:t])/p_1(S[1:t]) satisfies martingale properties under the null (successful trajectories). Ville's inequality then guarantees Pr[sup_t M_t ≥ 1/α] ≤ α, meaning the process stays below 1/α with probability at least 1-α even for infinite sequences.
- **Core assumption**: The verifier score distributions for successful (p_1) and unsuccessful (p_0) trajectories are distinct with well-defined densities; p_0 ≪ p_1.
- **Evidence anchors**:
  - [abstract]: "E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions."
  - [Section 2.2, Proposition 1]: "Algorithm 1 using the density ratio process, M_t = p_0(S[1:t])/p_1(S[1:t]), and the decision threshold c_α = 1/α achieves anytime-valid control of the false alarm rate."
  - [corpus]: "Automated Hypothesis Validation with Agentic Sequential Falsifications" (FMR=0.48) applies sequential falsification to LLM-generated hypotheses, supporting the sequential testing paradigm for agentic systems.

### Mechanism 2
- **Claim**: The density ratio e-process maximizes expected log-growth under the alternative, enabling earlier detection of unsuccessful trajectories than any other valid e-process.
- **Mechanism**: Under H_A (unsuccessful trajectory), the density ratio grows fastest in expectation among all e-processes. This is the sequential analog of the Neyman-Pearson lemma—log-optimality ensures M_t crosses the threshold earlier when the trajectory is truly unsuccessful.
- **Core assumption**: Score distributions are stationary within each trajectory class (successful vs. unsuccessful) during the observation window.
- **Evidence anchors**:
  - [abstract]: "distinguishing between distributions of verifier scores for successful versus unsuccessful trajectories"
  - [Section 2.2, Proposition 2]: "The density ratio process is log-optimal. That is, for any other e-process (M'_t) and stopping time τ, E_HA[log M_τ] ≥ E_HA[log M'_τ]."

### Mechanism 3
- **Claim**: Empirical quantile estimation on calibration data provides a high-probability upper bound on the true threshold while accounting for density ratio estimation error.
- **Mechanism**: The calibration set is split: one part trains density ratio estimators, the other computes max M_t per trajectory. Order statistics of these maxima yield a (1-δ)-confidence upper bound on the (1-α)-quantile via binomial tail bounds (Clopper-Pearson logic).
- **Core assumption**: Calibration trajectories are i.i.d. samples from the deployment distribution; the split between D_DRE and D_threshold is random.
- **Evidence anchors**:
  - [abstract]: "requires only black-box access to the verifier and minimal calibration data"
  - [Section 2.4, Proposition 3]: "Pr_Dcal(Pr_HN(∃t: M_t ≥ c_α | D_cal) ≤ α) ≥ 1-δ"

## Foundational Learning

- **Concept: E-values and Test Martingales**
  - **Why needed here**: These are the mathematical objects enabling anytime-valid inference without knowing trajectory length in advance. Unlike p-values, e-values compose safely across correlated sequential tests.
  - **Quick check question**: If E[M_t | F_{t-1}] = M_{t-1} under the null, what does this imply about E[M_t] for any t?

- **Concept: Ville's Inequality**
  - **Why needed here**: Provides the theoretical guarantee Pr[sup_t M_t ≥ 1/α] ≤ α for any non-negative process with E[M_0] ≤ 1. This is the foundation for threshold selection.
  - **Quick check question**: How does Ville's inequality extend Markov's inequality to infinite sequences?

- **Concept: Classifier-based Density Ratio Estimation**
  - **Why needed here**: Direct density estimation is impractical in high dimensions. Bayes' rule converts classification probabilities: p_0/p_1 = [p(Y=0|S)/p(Y=1|S)] × [p(Y=1)/p(Y=0)].
  - **Quick check question**: Given a classifier outputting p̂(Y=1|S) = 0.7 and class prior π̂_1 = 0.5, compute the density ratio estimate.

## Architecture Onboarding

- **Component map**: Calibration dataset -> Density ratio estimation -> Threshold calculation -> Online monitor
- **Critical path**: Collect D_cal → Split into D_DRE (80%) and D_threshold (20%) → Train f̂_t for t=1,...,T_max → Compute M̂_t on D_threshold → Find max M̂ per trajectory → Set c_α = M^(k) via Algorithm 3 → Deploy with online M̂_t evaluation
- **Design tradeoffs**:
  - **1/α vs. PAC threshold**: 1/α is conservative, valid for infinite trajectories, requires no calibration split; PAC is higher power, assumes finite trajectories, needs held-out data
  - **Classifier complexity**: Logistic regression is robust with limited data but may miss score dependencies; deeper models risk overfitting
  - **Calibration size**: Paper finds n ≥ 200 trajectories sufficient (Figure 6); smaller sets increase variance
- **Failure signatures**:
  - **Empirical FAR > α**: Calibration shift or insufficient calibration data; increase D_cal size or re-calibrate
  - **Low power despite controlled FAR**: Verifier scores poorly distinguish success/failure; improve verifier first
  - **M_t explodes on successful trajectories**: Density ratio estimation error; check classifier calibration, increase D_DRE size
- **First 3 experiments**:
  1. **FAR validation sweep**: For α ∈ {0.1, 0.2, 0.3, 0.4, 0.5}, run e-valuator on held-out test set and confirm empirical FAR ≤ α for both threshold variants.
  2. **Calibration size ablation**: Vary |D_cal| ∈ {50, 250, 500, 1000, 2000} to identify minimum viable calibration set for your agent/verifier combination.
  3. **Token savings vs. accuracy curve**: Terminate trajectories when M_t ≥ c_α, plot accuracy recovered vs. tokens saved; compare to raw/calibrated verifier baselines (replicate Figure 3).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the e-valuator maintain its guarantees while relaxing the assumption of full joint density estimation in favor of simpler dependencies (e.g., i.i.d. scores or k-step Markov assumptions)?
  - **Basis in paper**: [explicit] The authors state, "one can relax certain assumptions to avoid estimating the full joint density at each time t, such as assuming the verifier scores are i.i.d. at each step..."
  - **Why unresolved**: The current method models the full joint density $p(S[1:t])$, which is statistically robust but potentially inefficient or data-heavy compared to restricted dependency models.
  - **What evidence would resolve it**: An empirical comparison showing that e-valuator variants using limited history ($k$-step) or i.i.d. assumptions maintain valid false alarm rates with less calibration data.

- **Open Question 2**: How can the framework be modified to support interactive strategies like restarting or resampling trajectories without invalidating the statistical guarantees?
  - **Basis in paper**: [explicit] The authors note that using e-valuator for "resampling or restarting bad trajectories... may break the e-valuator assumptions, and it would be important to ameliorate the method accordingly."
  - **Why unresolved**: Current guarantees rely on the standard sequential trajectory distribution; interventions like restarting change the underlying stochastic process, potentially violating the test martingale properties.
  - **What evidence would resolve it**: A modified algorithm that accounts for intervention distributions, validated by showing controlled false alarm rates in a "restart-upon-rejection" setting.

- **Open Question 3**: How does the e-valuator framework perform when applied to multi-agent systems with interdependent action sequences?
  - **Basis in paper**: [explicit] The authors list as a future direction: "Finally, e-valuator can be extended to more complex agentic systems, such as multi-agent settings."
  - **Why unresolved**: The current experiments are limited to single-agent trajectories (including chess, which is two-player but treated as a single trajectory to monitor). Multi-agent dynamics introduce complex conditional dependencies in verifier scores.
  - **What evidence would resolve it**: Application of e-valuator to a standard multi-agent benchmark (e.g., negotiation or cooperative tasks) demonstrating maintained error control.

## Limitations

- The method's performance critically depends on calibration data being representative of deployment trajectories; distribution shifts can cause the false alarm rate to exceed α.
- Current implementation requires training classifiers for each timestep, which may become computationally expensive for very long trajectories.
- The assumption of distinct verifier score distributions for successful and unsuccessful trajectories may not hold for all verifier/verifier combinations.

## Confidence

- **High confidence**: Theoretical validity of sequential testing framework and density ratio e-process
- **Medium confidence**: Empirical performance across tested datasets, but calibration data requirements may vary
- **Low confidence**: "Minimal calibration data" claim without knowing exact distribution requirements

## Next Checks

1. **Distribution Shift Sensitivity Test**: Evaluate e-valuator's false alarm rate when calibration trajectories come from a different distribution than test trajectories (e.g., different agent models, different problem domains).

2. **Calibration Data Efficiency Study**: Systematically vary the calibration set size from 50 to 2000 trajectories and measure both false alarm rate control and power to identify minimum viable calibration size.

3. **Anytime vs. PAC Threshold Comparison**: For each dataset, compare 1/α threshold vs. PAC threshold performance across multiple α values to quantify the power vs. calibration data tradeoff.