---
ver: rpa2
title: Correspondence of high-dimensional emotion structures elicited by video clips
  between humans and Multimodal LLMs
arxiv_id: '2505.12746'
source_url: https://arxiv.org/abs/2505.12746
tags:
- emotion
- videos
- video
- ratings
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether current multimodal large language
  models (MLLMs) can capture the high-dimensional structure of human emotions. The
  authors compared emotion ratings from human participants watching videos with emotion
  estimates generated by models like Gemini and GPT.
---

# Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2505.12746
- **Source URL:** https://arxiv.org/abs/2505.12746
- **Reference count:** 40
- **Primary result:** MLLMs broadly capture human emotion structures at the category level, but struggle with fine-grained one-to-one matching at the individual video level

## Executive Summary
This study investigates whether multimodal large language models (MLLMs) can capture the high-dimensional structure of human emotions elicited by video clips. Researchers compared human emotion ratings from participants watching videos with emotion estimates generated by MLLMs like Gemini and GPT. They evaluated both single-video emotion ratings and the overall similarity structure across multiple videos using supervised (RSA) and unsupervised (Gromov-Wasserstein Optimal Transport) methods.

The findings reveal that while MLLMs can broadly capture human emotion structures at the category level, they face significant limitations in precise one-to-one matching at the individual video level. Specifically, MLLMs showed reasonable category matching (50-70%) and high correlation in overall emotion structure (RSA correlation ~0.5-0.6), but much lower item-level matching (2-18%). These results suggest MLLMs can infer complex human emotional responses but still struggle with contextual understanding and fine-grained emotion estimation.

## Method Summary
The researchers collected emotion ratings from human participants while they watched various video clips. These human ratings were then compared against emotion estimates generated by multimodal large language models (MLLMs) including Gemini and GPT. The comparison was conducted at two levels: single-video emotion ratings and the overall similarity structure across multiple videos. Two analytical approaches were employed - Representational Similarity Analysis (RSA) as a supervised method and Gromov-Wasserstein Optimal Transport as an unsupervised method. This dual approach allowed for comprehensive evaluation of how well MLLMs could capture the high-dimensional structure of human emotional responses to video content.

## Key Results
- MLLMs captured human emotion structures at the category level with 50-70% matching accuracy
- Overall emotion structure correlation between MLLMs and humans showed RSA correlation of approximately 0.5-0.6
- Item-level matching between MLLMs and human emotion ratings was significantly lower at 2-18%
- MLLMs demonstrated ability to infer complex human emotional responses but struggled with fine-grained emotion estimation

## Why This Works (Mechanism)
MLLMs leverage their multimodal training on vast datasets containing both visual and textual information to generate emotion estimates from video content. The models process video frames and associated descriptions to infer emotional responses, mapping these to a high-dimensional emotion space. Through their training on human-generated content, they develop representations that partially align with human emotional understanding, particularly at the categorical level where broader emotion categories can be identified. However, the models face challenges in capturing the nuanced, context-dependent nature of human emotional responses to specific videos, leading to discrepancies at the individual item level.

## Foundational Learning
- **Representational Similarity Analysis (RSA)**: A method for comparing the similarity structures between two representations - needed to quantify how similarly MLLMs and humans organize emotion space; quick check: verify correlation values fall within expected ranges
- **Gromov-Wasserstein Optimal Transport**: An unsupervised method for comparing structured data - needed to assess alignment without supervision; quick check: confirm transport plan makes intuitive sense
- **High-dimensional emotion space**: The multi-dimensional representation of emotions beyond simple categories - needed to capture complex human emotional responses; quick check: verify dimensions capture known emotional dimensions
- **Multimodal learning**: Training models on both visual and textual data - needed for MLLMs to process video content; quick check: test model on single vs. multimodal inputs
- **Emotion estimation from video**: The task of predicting emotional responses from visual content - needed as the core capability being evaluated; quick check: validate against human baselines
- **Category-level vs. item-level matching**: Different granularities of comparison - needed to distinguish broad vs. fine-grained alignment; quick check: compare performance at both levels

## Architecture Onboarding

**Component Map:**
Video Clips -> MLLM Emotion Extraction -> Emotion Feature Vectors -> Human-MLLM Structure Comparison -> RSA/GWOT Analysis

**Critical Path:**
Video input → Multimodal processing → Emotion vector generation → Structure comparison → Statistical validation

**Design Tradeoffs:**
The study balances between comprehensive evaluation (using both supervised and unsupervised methods) and practical constraints (limited video samples, specific MLLM choices). The dual-method approach provides robustness but increases computational complexity. Focusing on category-level matching provides broader insights but may mask individual video-level discrepancies.

**Failure Signatures:**
- Low RSA correlation indicates fundamental misalignment in emotion structure
- High category matching but low item matching suggests MLLMs capture broad patterns but miss specific context
- Consistent directional errors across videos may indicate systematic biases in MLLM emotion perception
- Large variance in item-level matching suggests instability in fine-grained emotion estimation

**3 First Experiments:**
1. Test MLLM emotion extraction on held-out video clips not used in main study
2. Compare MLLM performance across different model sizes and architectures
3. Evaluate impact of video duration and content type on emotion estimation accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- MLLMs showed significant limitations in fine-grained, one-to-one matching at the individual video level
- The study may be constrained by the specific set of videos and MLLMs chosen for evaluation
- Context-dependent nuances of human emotional responses may not be fully captured by current MLLM architectures

## Confidence
- **High:** Category-level emotion matching (50-70%) - robust across methods
- **Medium:** Overall emotion structure correlation (RSA ~0.5-0.6) - consistent but moderate
- **Low:** Item-level matching (2-18%) - highly variable and limited

## Next Checks
1. Validate MLLM emotion extraction on a separate, independently collected video dataset
2. Test whether fine-tuning MLLMs on emotion-labeled video data improves item-level matching
3. Compare MLLM performance against traditional computer vision emotion recognition systems on the same video set