---
ver: rpa2
title: FRQI Pairs method for image classification using Quantum Recurrent Neural Network
arxiv_id: '2512.11499'
source_url: https://arxiv.org/abs/2512.11499
tags:
- quantum
- image
- frqi
- qrnn
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the FRQI Pairs method, a novel approach for
  image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible
  Representation for Quantum Images (FRQI). The method addresses the challenge of
  classifying quantum-encoded images by leveraging the FRQI encoding scheme to efficiently
  represent grayscale images using quantum states.
---

# FRQI Pairs method for image classification using Quantum Recurrent Neural Network

## Quick Facts
- arXiv ID: 2512.11499
- Source URL: https://arxiv.org/abs/2512.11499
- Authors: Rafał Potempa; Michał Kordasz; Sundas Naqeeb Khan; Krzysztof Werner; Kamil Wereszczyński; Krzysztof Simiński; Krzysztof A. Cyran
- Reference count: 40
- Primary result: 74.6% test accuracy on 8×8 MNIST using FRQI Pairs QRNN

## Executive Summary
This paper introduces the FRQI Pairs method, a novel approach for image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI). The method leverages FRQI encoding to efficiently represent grayscale images in quantum states, reducing qubit requirements from O(n²) to O(log n). The FRQI Pairs architecture achieves exponential reduction in recurrent cells (n² instead of 2²ⁿ) by processing position-color pairs rather than individual pixels, making it more feasible for NISQ-era quantum computers.

## Method Summary
The FRQI Pairs method combines FRQI quantum image encoding with QRNN architecture to classify images. Images are first scaled to 8×8 pixels and normalized to [0, π/2] range. The FRQI encoding represents each pixel using a single qubit angle θₓ, with position encoded using ⌈log₂W⌉ + ⌈log₂H⌉ qubits. The QRNN processes these encoded images through 6 cells (for 8×8 images), each receiving paired inputs of color and position information. The network uses 4 memory qubits for recurrent state and 636 PQC parameters per cell, with an 80-parameter classical softmax layer for final classification.

## Key Results
- Achieved 74.6% test accuracy on 8×8 MNIST classification
- Uses 716 trainable parameters (4 QRNN memory + 7 FRQI qubits)
- Reduces required recurrent cells from 2²ⁿ to n² (6 cells vs 64 for 8×8 images)
- Performance comparable to other quantum methods (QCNN, VQDNN) with similar parameter counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The FRQI encoding compresses image data logarithmically, reducing qubit requirements from O(n²) to O(log n).
- Mechanism: Position encoding uses ⌈log₂W⌉ + ⌈log₂H⌉ qubits for spatial coordinates, plus a single qubit for grayscale intensity via angle encoding (θₓ ∈ [0, π/2]). For a 2ⁿ × 2ⁿ image, this yields ν = 2n + 1 total qubits.
- Core assumption: The quantum state preparation circuits can reliably prepare the FRQI state with sufficient fidelity before decoherence.
- Evidence anchors:
  - [Section II-B]: "FRQI method... allows efficient storage of single channel images using only O(⌈log₂ n⌉) qubits"
  - [Equation 1-2]: Formal state definition |I(θ)⟩ with cos/sin encoding
  - [corpus]: Limited direct validation; neighbor papers focus on representation analysis but not FRQI specifically
- Break condition: If state preparation circuit depth exceeds coherence time, the encoding advantage is lost.

### Mechanism 2
- Claim: The FRQI Pairs architecture reduces required recurrent cells from exponential (2²ⁿ) to quadratic (n²).
- Mechanism: Each QRNN cell receives paired inputs—one color qubit combined with one position-qubit combination (x,y). For an 8×8 image (n=3), this yields 6 cells instead of 64. Cells process position-color correlations rather than individual pixels.
- Core assumption: Position-color pairs contain sufficient information for classification without requiring full pixel-level sequential processing.
- Evidence anchors:
  - [Section III-B]: "total number of cells... can be calculated as N = n²"
  - [Section III-B]: Direct QRNN comparison requires "(2ⁿ)² = 2²ⁿ cells"
  - [corpus]: Hybrid Quantum RNN paper shows similar cell-reduction strategies, but not validated on identical architectures
- Break condition: If image classification requires fine-grained spatial relationships beyond what pair-wise encoding captures, accuracy degrades.

### Mechanism 3
- Claim: QRNN working memory qubits provide non-linear decision boundaries through amplitude-based hidden state evolution.
- Mechanism: 4 memory qubits maintain recurrent state across cells. The quantum neuron implementation uses amplitude amplification for non-linearity (referenced from [36]). Hidden state updates occur through parameterized quantum circuits (636 PQC parameters).
- Core assumption: Quantum amplitude amplification produces sufficient non-linearity for discriminative classification.
- Evidence anchors:
  - [Section II-C]: QRNN "utilizes an enhanced version of a quantum neuron... with amplitude amplification to create a nonlinear activation function"
  - [Section III-B]: Best model used "4 QRNN memory [qubits] and a single deep layer for each cell"
  - [corpus]: No direct corpus validation of this specific non-linearity mechanism
- Break condition: If the non-linearity is insufficient, the model behaves like a linear classifier, limiting accuracy (potentially explaining the 74.6% ceiling).

## Foundational Learning

- Concept: **FRQI Quantum Image State Preparation**
  - Why needed here: Understanding how classical pixel values map to quantum amplitudes (cos θₓ|0⟩ + sin θₓ|1⟩) is essential for debugging encoding failures.
  - Quick check question: Given a pixel value of 128 (from 0-255 scale), what θ value would encode it in FRQI?

- Concept: **Parameterized Quantum Circuits (PQC) as Trainable Layers**
  - Why needed here: The 636 PQC parameters are optimized via classical gradient descent; understanding this hybrid loop is critical for implementation.
  - Quick check question: How does one compute gradients through a quantum circuit with no explicit backpropagation?

- Concept: **Measurement-Based Quantum Inference**
  - Why needed here: Final classification requires measuring quantum states and mapping to class probabilities via softmax.
  - Quick check question: How many measurements are needed to estimate the output distribution with confidence interval ±1%?

## Architecture Onboarding

- Component map:
  - FRQI Encoder (7 qubits: 6 position + 1 color) → quantum state |I⟩
  - 6 QRNN Cells (paired color-position inputs)
  - 4 Memory Qubits (shared across cells)
  - Measurement + Softmax (80 classical parameters)
  - Total: 11 qubits, 716 trainable parameters

- Critical path:
  1. Scale MNIST 28×28 → 8×8, normalize pixels to [0, π/2]
  2. Prepare FRQI state |I(θ)⟩ via encoding circuit
  3. Process through 6 QRNN cells sequentially
  4. Measure output qubits → classical probabilities
  5. Apply softmax layer → class prediction

- Design tradeoffs:
  - Image resolution vs. qubit count: 8×8 requires 7 FRQI qubits; 28×28 would require 11 FRQI qubits but 25 cells (may exceed NISQ feasibility)
  - Memory qubits vs. expressibility: 4 memory qubits used; increasing showed positive impact but increases circuit depth
  - Cell depth vs. decoherence: Single deep layer per cell chosen; deeper layers may improve accuracy but risk decoherence

- Failure signatures:
  - Accuracy plateauing at ~50-60%: Likely insufficient training capacity or poor hyperparameters
  - Random/changing predictions across runs: Measurement sampling too sparse
  - Encoding fidelity loss (retrieved image differs significantly from input): State preparation circuit depth too long
  - Gradient explosion/vanishing: Check learning rate; classical optimizer may struggle with quantum landscape

- First 3 experiments:
  1. Baseline replication: Implement FRQI encoding on 4×4 images (4 cells), verify state preparation by measuring and reconstructing images—compare to Fig. 1 center/right panels.
  2. Ablation on memory qubits: Train identical models with 2, 3, 4 memory qubits on MNIST 8×8; plot accuracy vs. memory size to validate paper's claim that "increasing... had a positive impact."
  3. Binary classification comparison: Reduce to 2-class problem (e.g., digits 3 vs. 6 as in [38]) to compare against binary benchmarks; verify if accuracy approaches the 96-99% range seen in other methods.

## Open Questions the Paper Calls Out
None

## Limitations
- The 74.6% accuracy on 8×8 MNIST remains significantly below classical benchmarks (>97% on full-resolution MNIST), suggesting fundamental limitations in the current quantum architecture or training approach.
- State preparation fidelity is not explicitly measured or reported, representing a critical unknown that could explain performance limitations - if FRQI encoding circuits exceed coherence times, the claimed logarithmic compression advantage disappears.
- The methodology relies on amplitude amplification for non-linearity without direct validation of this mechanism, and the lack of comparison to other QRNN implementations makes it difficult to isolate whether the 74.6% ceiling reflects QRNN limitations or FRQI-specific constraints.

## Confidence
- **High confidence**: The FRQI encoding mechanism and its logarithmic qubit scaling are well-established in quantum information theory and directly supported by the formalism in Equations 1-2.
- **Medium confidence**: The exponential reduction in recurrent cells (n² vs 2²ⁿ) is mathematically sound based on the paired-input architecture, but the classification performance suggests potential information loss from this compression.
- **Medium confidence**: The comparative accuracy against QCNN and VQDNN is reasonable given similar parameter counts, but the lack of direct implementation comparisons and binary classification benchmarks limits definitive conclusions.

## Next Checks
1. **State fidelity verification**: Implement measurement-based image reconstruction from the FRQI-encoded state to quantify preparation fidelity and identify any systematic encoding errors that could degrade classification performance.
2. **Binary classification comparison**: Reduce the problem to binary classification (e.g., 3 vs 6 digits) to enable direct comparison with existing quantum image classification methods that report 96-99% accuracy, helping isolate whether the 74.6% ceiling reflects QRNN limitations or FRQI-specific constraints.
3. **Cell depth ablation study**: Systematically vary the depth of PQC circuits within each QRNN cell (beyond the single deep layer used) to determine if the accuracy plateau reflects insufficient model capacity rather than fundamental quantum limitations.