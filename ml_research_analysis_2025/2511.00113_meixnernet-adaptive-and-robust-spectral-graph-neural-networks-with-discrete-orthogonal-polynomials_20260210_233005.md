---
ver: rpa2
title: 'MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete
  Orthogonal Polynomials'
arxiv_id: '2511.00113'
source_url: https://arxiv.org/abs/2511.00113
tags:
- graph
- meixnernet
- spectral
- chebynet
- polynomials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MeixnerNet addresses the mismatch between continuous polynomial\
  \ filters and discrete graph structures in spectral GNNs. It introduces discrete\
  \ Meixner polynomials with learnable shape parameters \u03B2 and c, allowing the\
  \ filter to adapt to each graph's spectral properties."
---

# MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials

## Quick Facts
- **arXiv ID:** 2511.00113
- **Source URL:** https://arxiv.org/abs/2511.00113
- **Reference count:** 11
- **Key outcome:** MeixnerNet achieves competitive-to-superior performance against ChebyNet at K=2 and demonstrates exceptional robustness to the critical K hyperparameter.

## Executive Summary
MeixnerNet addresses the mismatch between continuous polynomial filters and discrete graph structures in spectral GNNs. It introduces discrete Meixner polynomials with learnable shape parameters β and c, allowing the filter to adapt to each graph's spectral properties. A key innovation is a stabilization technique combining Laplacian scaling and per-basis LayerNorm to overcome severe numerical instability from growing recurrence coefficients. Experiments on Cora, CiteSeer, and PubMed datasets show MeixnerNet achieves competitive-to-superior performance (winning 2/3 benchmarks) against ChebyNet at the optimal K=2 setting. More importantly, MeixnerNet demonstrates exceptional robustness to the critical K hyperparameter, maintaining stable performance across different polynomial degrees while ChebyNet's accuracy collapses by over 14% when K increases from 2 to 3.

## Method Summary
MeixnerNet is a spectral GNN that uses discrete Meixner polynomials as learnable filters. The method computes polynomial bases via a three-term recurrence relation with learnable shape parameters β and c, applies per-basis LayerNorm for stabilization, and concatenates the normalized bases before linear projection. The architecture consists of two MeixnerConv layers with K=2 polynomial degree, 16 hidden units, dropout=0.5, and ReLU activation. The Laplacian is scaled by 0.5 and optimized using Adam with learning rate=0.01 and weight decay=5e-4 for 200 epochs on citation network datasets.

## Key Results
- MeixnerNet achieves competitive-to-superior performance against ChebyNet at optimal K=2 setting
- Demonstrates exceptional robustness to K hyperparameter (stable performance across K values)
- ChebyNet's accuracy collapses by over 14% when K increases from 2 to 3, while MeixnerNet remains stable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrete orthogonal polynomials provide better theoretical alignment with graph spectral structure than continuous polynomials.
- **Mechanism:** Graph Laplacian eigenvalues form a discrete set. Meixner polynomials $M_k(x; \beta, c)$ are inherently discrete, defined over non-negative integers, matching the discrete nature of graph spectra. This reduces the "domain mismatch" that exists when Chebyshev polynomials (defined on continuous $[-1,1]$) are applied to discrete graph structures.
- **Core assumption:** The paper *hypothesizes* this mismatch causes suboptimal performance and hyperparameter fragility. This is not proven causally—rather, it is motivated by theoretical coherence and supported by empirical robustness results.
- **Evidence anchors:**
  - [abstract]: "continuous-domain filters are applied to inherently discrete graph structures. We hypothesize this mismatch can lead to suboptimal performance and fragility"
  - [Section I]: "graph data is, by its nature, discrete. The Graph Laplacian's spectrum is a discrete set of eigenvalues"
  - [corpus]: Related work (LaguerreNet, KrawtchoukNet, L-JacobiNet) explores similar adaptive polynomial families but focuses on continuous or bounded domains; MeixnerNet uniquely emphasizes discrete-domain polynomials.
- **Break condition:** If future work shows continuous polynomials can achieve equivalent robustness with proper normalization, the discrete-alignment hypothesis would be weakened.

### Mechanism 2
- **Claim:** Learnable shape parameters ($\beta$, $c$) enable per-graph adaptation of the polynomial basis to spectral properties.
- **Mechanism:** The Meixner recurrence coefficients $b_k$ and $c_k$ depend on $\beta$ and $c$ (Equation 4). By making these parameters learnable via backpropagation, the network adjusts the polynomial basis shape to match the eigenvalue distribution of each specific graph, rather than using a fixed basis.
- **Core assumption:** Gradient-based optimization can find $(\beta, c)$ values that improve filter expressivity for each dataset.
- **Evidence anchors:**
  - [Section III-B]: "The key novelty of our approach is to make $\beta$ and $c$ learnable parameters, allowing the network to find the optimal polynomial basis"
  - [Section IV-C]: "learned parameters $\beta$ and $c$ adapt to the data (e.g., for K=2, PubMed learned $\beta = 0.93, c = 0.47$ while CiteSeer learned $\beta = 1.03, c = 0.51$)"
  - [corpus]: Corpus evidence weak for direct comparison—related papers (GegenbauerNet, L-JacobiNet) study adaptive parameters in continuous domains but do not isolate this mechanism's contribution.
- **Break condition:** If ablation shows fixed $(\beta, c)$ values perform equivalently across datasets, the adaptive mechanism's importance would be diminished.

### Mechanism 3
- **Claim:** Two-fold stabilization (Laplacian scaling + per-basis LayerNorm) prevents gradient explosion from quadratic growth of recurrence coefficients.
- **Mechanism:** Meixner recurrence coefficients grow as $O(k)$ for $b_k$ and $O(k^2)$ for $c_k$. Without intervention, $\bar{X}_k$ values explode, causing training failure. The stabilization: (1) scales $\mathbf{L}_{\text{sym}}$ by 0.5, confining eigenvalues to $[0,1]$; (2) applies LayerNorm to each $\bar{X}_k$ before concatenation, normalizing scale differences across polynomial orders.
- **Core assumption:** LayerNorm applied per-basis preserves sufficient signal while preventing variance collapse across concatenated features.
- **Evidence anchors:**
  - [Section III-D]: "A naive implementation...fails. The recurrence coefficients $b_k$ and $c_k$ grow as $O(k)$ and $O(k^2)$"
  - [Section III-D]: "This stabilization (scaling L and normalizing $\bar{X}_k$) is the key that enables MeixnerNet to train stably"
  - [corpus]: HybSpecNet explicitly discusses stability-vs-adaptivity trade-offs in spectral GNNs, corroborating that numerical instability is a pervasive challenge in this design space.
- **Break condition:** If deeper networks (higher $K$) still exhibit instability despite stabilization, additional techniques (e.g., gradient clipping, residual connections) may be required.

## Foundational Learning

- **Concept:** Graph Laplacian and spectral decomposition
  - **Why needed here:** The entire MeixnerNet pipeline operates on $\mathbf{L} = \mathbf{U}\Lambda\mathbf{U}^T$. Understanding eigenvalue spectra ($[0,2]$ for normalized Laplacian) is essential to grasp why scaling matters.
  - **Quick check question:** What is the eigenvalue range of the symmetric normalized Laplacian, and why does scaling by 0.5 improve numerical stability?

- **Concept:** Three-term recurrence relations for orthogonal polynomials
  - **Why needed here:** Meixner polynomials are computed recursively (Eq. 3). Each $\bar{X}_k$ depends on previous two bases; instability propagates multiplicatively.
  - **Quick check question:** Given $M_k(x) = (x - b_{k-1})M_{k-1}(x) - c_{k-1}M_{k-2}(x)$, what happens to $M_k$ if $c_{k-1}$ grows as $O(k^2)$ and $x \in [0, 2]$?

- **Concept:** Layer normalization mechanics
  - **Why needed here:** Per-basis LayerNorm is critical to the stabilization strategy. Without understanding how LayerNorm normalizes across features, the design rationale is opaque.
  - **Quick check question:** Why is LayerNorm applied *per polynomial basis* rather than across the concatenated feature dimension?

## Architecture Onboarding

- **Component map:**
  Input X (N × F_in) → Laplacian Scaling: L_scaled = 0.5 × L_sym → Recurrence Loop (k = 0 to K-1): M_0 = X, M_1 = (L_scaled - b_0·I) @ M_0, M_k = (L_scaled - b_{k-1}·I) @ M_{k-1} - c_{k-1}·M_{k-2} → Per-Basis LayerNorm: X̂_k = LayerNorm(M_k) → Concatenation: Z = [X̂_0, X̂_1, ..., X̂_{K-1}] (N × K·F_in) → Linear Projection: Y = Z @ W + b (N × F_out)

- **Critical path:** The recurrence loop (computing $\bar{X}_k$) is where instability originates. Per-basis LayerNorm must be applied *before* concatenation—placing it after allows high-variance terms to dominate the projection.

- **Design tradeoffs:**
  - Higher $K$ → more expressive filtering but greater instability risk (mitigated but not eliminated by stabilization)
  - Learnable $(\beta, c)$ → adaptivity but adds 2 parameters and potential optimization complexity
  - LayerNorm per-basis → stabilizes training but may alter signal scale in ways that affect interpretability

- **Failure signatures:**
  - Exploding loss/NaN outputs early in training → stabilization likely missing or misconfigured
  - Performance degrades sharply as $K$ increases → Laplacian scaling may not be applied
  - High variance across runs → $(\beta, c)$ initialization may need tuning

- **First 3 experiments:**
  1. **Sanity check:** Train MeixnerNet with $K=2$ on Cora with and without stabilization (scaling + LayerNorm). Confirm loss converges only with stabilization.
  2. **Robustness sweep:** Vary $K \in \{2, 3, 4, 5\}$ on PubMed. Compare MeixnerNet vs ChebyNet accuracy curves. Expect ChebyNet collapse at $K \geq 3$.
  3. **Parameter ablation:** Fix $(\beta, c)$ to default values vs learnable. Measure accuracy delta on CiteSeer and PubMed to quantify adaptation benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do other families of discrete orthogonal polynomials (e.g., Krawtchouk, Hahn, Charlier) offer superior stability or expressiveness compared to Meixner polynomials for spectral GNNs?
- **Basis:** [explicit] The conclusion explicitly states: "this paper opens the door to exploring other families of discrete orthogonal polynomials (e.g., Krawtchouk, Hahn, and Charlier) as a rich and promising foundation."
- **Why unresolved:** Meixner polynomials were selected as a representative instance of discrete polynomials, but the specific advantages or stability trade-offs of alternative discrete bases remain uninvestigated.
- **What evidence would resolve it:** A comparative ablation study implementing MeixnerNet with different discrete polynomial bases on the same benchmarks.

### Open Question 2
- **Question:** Does MeixnerNet's robustness to the polynomial degree $K$ hold for large-scale or heterophilic graphs?
- **Basis:** [inferred] The experiments are restricted to three small, homophilic citation networks (Cora, CiteSeer, PubMed), leaving performance on diverse graph topologies unverified.
- **Why unresolved:** The robustness claims rely on the spectral properties of these specific benchmarks; the behavior of learnable $\beta$ and $c$ parameters on graphs with different spectral distributions (e.g., heterophilic data) is unknown.
- **What evidence would resolve it:** Evaluation of MeixnerNet's performance curve across $K$ values on large-scale datasets (e.g., ogbn-arxiv) or heterophilic datasets (e.g., Actor, Chameleon).

### Open Question 3
- **Question:** Does the per-basis LayerNorm stabilization distort the theoretical spectral interpretation of the filter?
- **Basis:** [inferred] The paper introduces LayerNorm to solve numerical instability. However, normalizing each basis vector $\hat{X}_k$ fundamentally alters the magnitude of the polynomial features before the linear projection.
- **Why unresolved:** While practically effective, this normalization step might decouple the learned weights from the actual spectral energy of the graph signal, potentially contradicting the theoretical motivation of discrete spectral filtering.
- **What evidence would resolve it:** An analysis comparing the learned filter shape $g(\lambda)$ with and without normalization against the ground-truth spectral distribution.

## Limitations

- The discrete-domain hypothesis remains unproven causally; performance gains may stem from stabilization alone rather than polynomial family alignment.
- Limited ablation of learnable parameters—fixed $(\beta, c)$ values could potentially match adaptive performance.
- Numerical stabilization may mask underlying architectural fragility; performance on larger or noisier graphs untested.

## Confidence

- **High**: Stabilization necessity (proven by failure modes); robustness claims (verified by K-sweep experiments).
- **Medium**: Discrete alignment hypothesis (plausible but not causally isolated); adaptation benefit (supported but weakly ablated).
- **Low**: Claims about generalizability to other graph types (no experiments beyond citation networks).

## Next Checks

1. **Causal isolation**: Train MeixnerNet with continuous polynomials (e.g., adjusted Chebyshev) but retain stabilization; compare robustness curves to isolate discretization effect.
2. **Ablation of adaptation**: Fix $(\beta, c)$ to dataset-specific constants derived from spectral statistics (e.g., mean eigenvalue) and measure performance drop.
3. **Scaling robustness**: Test MeixnerNet on graphs with extreme eigenvalue spread (e.g., power-law degree distributions) to stress-test Laplacian scaling strategy.