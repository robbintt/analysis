---
ver: rpa2
title: Faster and Memory-Efficient Training of Sequential Recommendation Models for
  Large Catalogs
arxiv_id: '2509.09682'
source_url: https://arxiv.org/abs/2509.09682
tags:
- training
- memory
- ndcg
- negative
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high memory and computational cost of
  training sequential recommendation models with large item catalogs. The key problem
  is that standard cross-entropy loss requires scoring all items, leading to memory
  usage scaling with catalog size, batch size, and sequence length.
---

# Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs

## Quick Facts
- arXiv ID: 2509.09682
- Source URL: https://arxiv.org/abs/2509.09682
- Reference count: 40
- Primary result: CCE- accelerates training by up to 2x and reduces memory consumption by more than 10x for sequential recommendation models

## Executive Summary
This paper addresses the high memory and computational cost of training sequential recommendation models with large item catalogs. The key problem is that standard cross-entropy loss requires scoring all items, leading to memory usage scaling with catalog size, batch size, and sequence length. The authors propose CCE- (Cut-Cross-Entropy with negative sampling), a GPU-optimized implementation that avoids materializing large logit tensors by fusing operations in shared memory. CCE- accelerates training by up to 2x and reduces memory consumption by more than 10x. By reallocating memory savings to increase batch size, sequence length, and negative samples, they achieve up to 30% improvement in NDCG@10 accuracy. The method also includes gradient pruning for further efficiency gains. The approach is demonstrated on six datasets and works for both SASRec and BERT4Rec architectures. Code is publicly released.

## Method Summary
The authors propose CCE- (Cut-Cross-Entropy with negative sampling), a GPU-optimized implementation that addresses the memory bottleneck of cross-entropy loss in sequential recommendation. The method fuses the linear classification head and log-sum-exp operations into a single GPU kernel that executes in fast on-chip SRAM, only materializing the correct-item logits and the LSE vector in global memory. This eliminates the need to store the entire logit tensor, resulting in substantial memory reduction. CCE- extends this with negative sampling, computing logits only for positive items and sampled negatives, with the backward pass recomputing logits on-the-fly rather than storing them. The approach also includes gradient pruning for near-zero gradients in the final layer, skipping backward-pass computations for gradients below a threshold.

## Key Results
- CCE- accelerates training by up to 2x compared to standard cross-entropy
- Memory consumption reduced by more than 10x across six datasets
- Up to 30% improvement in NDCG@10 accuracy by reallocating memory savings to larger batch sizes, sequence lengths, and negative samples
- Works effectively for both SASRec and BERT4Rec architectures
- Gradient pruning provides additional acceleration without significant accuracy loss until thresholds exceed 1×10⁻⁴–1×10⁻³

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Avoiding full logit tensor materialization reduces memory consumption by 75–97% for sequential recommendation models.
- **Mechanism**: Standard cross-entropy requires computing and storing a logits tensor of size `batch_size × sequence_length × |V|` (where |V| is catalog size). CCE fuses the linear classification head and log-sum-exp operations into a single GPU kernel that executes in fast on-chip SRAM, only materializing the correct-item logits and the LSE vector in global memory (HBM).
- **Core assumption**: The item catalog size |V| is large (thousands to millions), making the full logit tensor the dominant memory cost (up to 90% of memory footprint per the paper).
- **Evidence anchors**:
  - [abstract]: "CCE- accelerates training by up to 2x and reduces memory consumption by more than 10x."
  - [Section 3.1]: "This eliminates the need to store the entire logit tensor in the global GPU memory, resulting in a substantial reduction in GPU memory usage."
  - [corpus]: Mem-Rec (arXiv:2305.07205) addresses categorical feature scaling in DLRMs but uses embedding compression, not kernel fusion—distinct approach.

### Mechanism 2
- **Claim**: Negative sampling with CCE- (CCE-) can accelerate training by up to 88% compared to full-catalog CCE while maintaining or improving accuracy on large-catalog datasets.
- **Mechanism**: CCE- computes logits only for the positive item and `ns` sampled negatives (where `ns ≪ |V|`). The sampled subset |V_s| = 1 + ns restricts log-sum-exp and gradient computations to this compact set. The backward pass recomputes logits on-the-fly rather than storing them, with gradients non-zero only for sampled indices.
- **Core assumption**: The sampled negatives are sufficiently representative of the full catalog distribution to approximate the full softmax partition function.
- **Evidence anchors**:
  - [abstract]: "By reallocating memory savings to increase batch size, sequence length, and negative samples, they achieve up to 30% improvement in NDCG@10."
  - [Section 3.3]: "During training, at each time step, the positive items corresponding to users are augmented with a dynamically sampled set of ns negative items."
  - [corpus]: "On Negative-aware Preference Optimization" (arXiv:2508.09653) explores negative feedback in LLM-based recommenders but focuses on preference optimization, not sampling efficiency.

### Mechanism 3
- **Claim**: Pruning near-zero gradients in the final layer accelerates training without significant accuracy loss, with stable performance until threshold exceeds 1×10⁻⁴–1×10⁻³.
- **Mechanism**: For large catalogs, softmax produces near-zero probabilities for most incorrect items (`p_y → 0`), making corresponding gradients (`∂L/∂C_y = p_y × e_j`) near-zero. Filtering gradients below a threshold skips backward-pass computations. The paper shows this introduces a regularization effect that can even improve metrics for some datasets (e.g., Movielens-20M).
- **Core assumption**: Gradients below the pruning threshold contribute minimally to model updates and their removal does not alter convergence trajectory.
- **Evidence anchors**:
  - [Section 3.2]: "Because we have |V| − 1 incorrect labels, most gradients remain near zero or saturate to zero based on data format."
  - [Section 5.4]: "Model accuracy degrades only when the threshold exceeds 1×10⁻⁴ – 1×10⁻³."
  - [corpus]: StreamBP (arXiv:2506.03077) addresses activation memory in long-sequence LLM training via chunked backpropagation—related memory-efficiency goal but different mechanism.

## Foundational Learning

- **Cross-Entropy Loss and Negative Sampling**:
  - **Why needed here**: CCE- is a memory-optimized variant of sampled cross-entropy. Understanding the baseline CE loss and why negative sampling is used is prerequisite to grasping what CCE- optimizes.
  - **Quick check question**: Given a catalog of 1M items, what is the memory requirement for storing full logits with batch_size=256, sequence_length=100 in float16? (Answer: ~51 GB)

- **GPU Memory Hierarchy (HBM vs. SRAM)**:
  - **Why needed here**: CCE's efficiency comes from fusing operations in fast SRAM to avoid slow HBM transfers. Understanding this trade-off is essential for diagnosing performance.
  - **Quick check question**: Why is it beneficial to compute intermediate results in SRAM even if they must be recomputed during backpropagation?

- **Triton Kernel Programming Basics**:
  - **Why needed here**: The implementation is a custom Triton kernel. While not strictly required to use the released code, debugging or extending it requires familiarity with block-level parallelism and memory access patterns.
  - **Quick check question**: What is the purpose of the `for` loop over blocks in Algorithm 1, and what determines block size?

## Architecture Onboarding

- **Component map**:
  - CCE (Cut-Cross-Entropy): Triton kernel for full-catalog CE without materializing logits
  - CCE- (Cut-Cross-Entropy with negative sampling): Extends CCE with indexed access to classifier weights for negative samples
  - Gradient filtering: Optional pass integrated into CCE backward kernel; prunes gradients below filter_eps
  - Sampling strategies: Global uniform (default) vs. popularity-based (separate implementation, higher memory overhead from torch.multinomial)

- **Critical path**:
  1. Flatten transformer output (bs, sl, D) → (N, D) where N = bs × sl
  2. Sample negative indices for each position; form Inds of shape (N, 1+ns)
  3. Forward pass: CCE- kernel computes LSE via online softmax over sampled logits
  4. Backward pass: Recompute sampled logits, compute softmax in SRAM, accumulate gradients only for sampled indices

- **Design tradeoffs**:
  - ns (negative samples): Higher ns → better accuracy but more compute; diminishing returns observed (saturation ~512–1024 for tested datasets)
  - bs (batch size) vs. sl (sequence length) vs. ns: Paper recommends balancing all three rather than maximizing one; correlation analysis shows bs and ns are most influential
  - CCE vs. CCE-: CCE- faster for large catalogs; CCE better for small catalogs (<15K items) where negative sampling overhead exceeds savings
  - Sampling strategy: Uniform sampling better for NDCG; popularity sampling improves diversity (Coverage, Surprisal) but with higher memory cost

- **Failure signatures**:
  - OOM despite CCE-: Check if ns or bs is too large; sample index matrix (N, 1+ns) can grow large
  - NDCG degrades with CCE- vs. CE/CE-: Likely numerical precision differences in Triton kernel; the paper notes minor discrepancies requiring further investigation
  - Training slows with CCE- on small catalogs: Switch to CCE (full catalog) or reduce ns
  - Gradient filtering causes accuracy drop: Lower filter_eps threshold; default is numerical precision (~6×10⁻⁸ for FP16)

- **First 3 experiments**:
  1. **Memory baseline**: Train SASRec with standard PyTorch CE on a medium dataset (e.g., MovieLens-20M) until OOM or with very small bs; then swap to CCE and verify memory reduction and metric parity
  2. **Negative sampling sweep**: On a large-catalog dataset (e.g., Megamarket), train with CCE- varying ns ∈ {64, 256, 1024, 4096} at fixed memory budget; plot NDCG vs. training time to identify optimal ns
  3. **Gradient pruning threshold**: Train with CCE on a dataset, sweep filter_eps ∈ {1e-7, 1e-6, 1e-5, 1e-4, 1e-3}; record NDCG and epoch time to confirm the stability threshold and potential acceleration gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the minor NDCG@10 discrepancies between CCE/CCE− and standard CE/CE− implementations, and can they be eliminated?
- Basis in paper: [explicit] Section 5.2 states: "the observed metrics discrepancies may arise from minor numerical inaccuracies during intermediate computations in Triton kernels or from loss scaling in mixed-precision training. However, this issue requires further investigation, which will be addressed in future work."
- Why unresolved: The mathematical formulations are equivalent, yet small metric differences persist; the precise source (kernel arithmetic, precision handling, or loss scaling) remains unidentified.
- What evidence would resolve it: Ablation studies isolating numerical precision, loss scaling factors, and kernel-level operation ordering to identify and correct the discrepancy source.

### Open Question 2
- Question: Can adaptive gradient pruning thresholds improve the accuracy-speed trade-off compared to fixed thresholds?
- Basis in paper: [explicit] Section 6 states: "developing adaptive gradient pruning techniques to balance sparsity and model performance" as a future research direction; Section 2 mentions "based on our results, an adaptive threshold to prune gradients can be employed."
- Why unresolved: Fixed thresholds were tested, but gradients may have varying sparsity patterns across training stages and datasets, suggesting adaptive approaches could better balance regularization benefits against information loss.
- What evidence would resolve it: Experiments comparing fixed vs. training-stage-adaptive or gradient-statistics-adaptive pruning thresholds on convergence speed and final NDCG@10 across multiple datasets.

### Open Question 3
- Question: Can the CCE− Triton kernel implementation be combined with the Scalable Cross-Entropy (SCE) sampling strategy to achieve further memory and speed improvements?
- Basis in paper: [explicit] Section 5.4.1 states: "Since SCE incorporates the CE loss function, it could be further optimized in the future using our Triton kernels, which compute the CE loss with negative sampling by using only the indices of positive and negative samples."
- Why unresolved: SCE uses a different sampling strategy (top-k logits by magnitude) than CCE− (uniform/popularity sampling); combining the kernel fusion approach with SCE's sampling is unexplored.
- What evidence would resolve it: Implementation of a fused SCE kernel using the CCE− architecture and comparative evaluation against both standalone SCE and CCE− on memory, speed, and NDCG@10 metrics.

## Limitations

- Performance gains from negative sampling saturate at different thresholds across datasets, requiring per-dataset hyperparameter tuning
- Numerical precision differences between Triton kernels and standard PyTorch implementations may cause minor metric discrepancies
- For smaller catalogs (e.g., MovieLens-20M with 15K items), CCE- provides minimal benefits as negative sampling overhead exceeds savings

## Confidence

- **High confidence**: Memory reduction mechanism (75-97% savings) and basic CCE implementation correctness
- **Medium confidence**: Negative sampling effectiveness and optimal negative sample count per dataset
- **Medium confidence**: Gradient pruning benefits and stability thresholds, though results show consistent patterns

## Next Checks

1. Reproduce memory baseline with standard CE on MovieLens-20M to verify OOM conditions and CCE memory savings
2. Conduct negative sampling sweep (ns ∈ {64, 256, 1024, 4096}) on MegaMarket to identify optimal point and saturation behavior
3. Test gradient pruning across threshold range (1e-7 to 1e-3) to confirm stability threshold and acceleration benefits