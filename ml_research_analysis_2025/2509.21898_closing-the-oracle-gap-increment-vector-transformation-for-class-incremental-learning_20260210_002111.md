---
ver: rpa2
title: 'Closing the Oracle Gap: Increment Vector Transformation for Class Incremental
  Learning'
arxiv_id: '2509.21898'
source_url: https://arxiv.org/abs/2509.21898
tags:
- learning
- tasks
- accuracy
- incremental
- oracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of catastrophic forgetting in
  class incremental learning (CIL), where models must learn new classes sequentially
  without access to previous data. Despite progress, current CIL methods still underperform
  compared to oracle models that have access to all historical data.
---

# Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning

## Quick Facts
- **arXiv ID**: 2509.21898
- **Source URL**: https://arxiv.org/abs/2509.21898
- **Reference count**: 40
- **Primary result**: IVT improves class incremental learning by transforming parameter updates to preserve low-loss connections to previous task optima

## Executive Summary
This paper addresses catastrophic forgetting in class incremental learning by proposing Increment Vector Transformation (IVT), a framework that periodically transforms parameter updates to maintain connectivity to previous task optima. The method uses diagonal Fisher Information Matrices to efficiently approximate curvature information and guide updates toward regions that preserve performance on old tasks while learning new ones. IVT works as a plug-and-play module compatible with various CIL methods and demonstrates significant improvements across multiple benchmarks including CIFAR-100, FGVCAircraft, and ImageNet.

## Method Summary
IVT periodically transforms the parameter update vector during incremental learning to preserve low-loss linear connections to previous task optima. During each task, the method accumulates diagonal Fisher Information Matrices by averaging squared gradients over batches. Every I epochs, it applies a transformation that teleports parameters toward a region that better maintains old task performance while accommodating new knowledge. The transformation uses cumulative Fisher matrices from previous tasks to weight the increment vector, effectively approximating how an oracle solution would move if it had access to all historical data.

## Key Results
- On CIFAR-100 with PASS baseline, IVT improves last accuracy by +5.12% and reduces forgetting by 2.54%
- For CLIP-pre-trained SLCA on FGVCAircraft, IVT yields +14.93% in average accuracy and +21.95% in last accuracy
- IVT shows broad applicability across exemplar-based, exemplar-free, and pre-trained CIL scenarios
- The method introduces minimal computational overhead while maintaining efficiency through diagonal Fisher approximation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CIL oracles maintain low-loss linear connections to previous task optima, enabling stable knowledge integration along these paths.
- **Mechanism**: When full historical data is available, optimization naturally discovers solutions that remain within the same loss basin as previous task minima. The linear interpolation from θ*_{t-1} to θ*_t maintains stable accuracy on earlier tasks, indicating low curvature along this direction.
- **Core assumption**: The geometric properties of oracle solutions can be approximated without access to historical data by reshaping the increment vector using curvature information.
- **Evidence anchors**: [abstract] "these oracle solutions typically maintain low-loss linear connections to the optimum of previous tasks"; [Section III-A] "the CIL oracles tend to remain closer to the minima of previous tasks... The updates of the CIL oracle aligns with the direction of lower curvature"
- **Break condition**: If the loss landscape has high curvature barriers between tasks, or if tasks are fundamentally conflicting (e.g., requiring opposite predictions for same inputs), linear connectivity cannot hold.

### Mechanism 2
- **Claim**: The increment vector V_t = θ_t - θ*_{t-1} can be transformed toward oracle behavior using curvature-weighted rescaling.
- **Mechanism**: The transformation S_t = (H̄_{t-1} + H_t)^{-1} H_t re-weights the increment vector, dampening updates in directions where previous tasks have high curvature (important directions) while allowing movement in flatter directions. This approximates how oracles would move if they had access to all historical data.
- **Core assumption**: The local second-order approximation around θ*_{t-1} remains valid within the update magnitude of standard SGD training.
- **Evidence anchors**: [Section IV-A, Proposition 1] "θ*_t ≈ θ*_{t-1} + (H̄_{t-1} + H_t)^{-1} H_t(θ_t - θ*_{t-1})"; [Section IV-A] "reducing F_1 can be achieved by minimizing Δθ or by steering the model update direction away from the higher curvature directions of H_1"
- **Break condition**: If the model drifts far from θ*_{t-1} between IVT applications, higher-order curvature terms invalidate the local quadratic approximation.

### Mechanism 3
- **Claim**: The diagonal Fisher Information Matrix provides a computationally efficient approximation to the Hessian that preserves most of IVT's benefits.
- **Mechanism**: F_t = E_{(x,y)∈T_t}[(∇L_t(x,y))²] is computed online by accumulating squared gradients during backpropagation. This diagonal approximation assumes parameter independence, reducing memory from O(|θ|²) to O(|θ|) while capturing per-parameter importance.
- **Core assumption**: Off-diagonal Hessian elements (parameter interactions) contribute minimally to the transformation quality.
- **Evidence anchors**: [Section IV-B] "incorporating the full FIM yields only marginal accuracy improvements... GPU memory usage increases by approximately 2.5×"; [Section IV-B] "FIM equals the expected Hessian of the negative log-likelihood"
- **Break condition**: For architectures with strong parameter coupling (e.g., attention layers with shared projections), diagonal approximation may miss critical interactions.

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC)**
  - Why needed here: LMC is the theoretical foundation explaining why IVT works—understanding that low-loss paths exist between minima helps explain why transforming increment vectors preserves old task performance.
  - Quick check question: Can you explain why two independently trained models from different initializations typically lack linear connectivity, but models sharing initialization often exhibit LMC?

- **Concept: Fisher Information Matrix**
  - Why needed here: IVT uses diagonal FIM to approximate Hessian-based transformations; understanding its relationship to curvature and why the diagonal approximation works is essential for debugging and extensions.
  - Quick check question: What does the Fisher Information Matrix measure, and why does its diagonal capture per-parameter "importance" for a task?

- **Concept: Stability-Plasticity Trade-off in CIL**
  - Why needed here: IVT explicitly navigates this trade-off by finding directions that accommodate new knowledge while preserving old; the paper's linear path analysis visualizes this balance.
  - Quick check question: In Figure 3 of the paper, why does the oracle achieve better stability-plasticity balance along the interpolation path compared to standard incremental updates?

## Architecture Onboarding

- **Component map**: IncrementVector -> FisherAccumulator -> CumulativeFisher -> IVTTransform

- **Critical path**:
  1. Store θ*_{t-1} at task boundary
  2. During new task training, accumulate F_t via squared gradients at each batch
  3. Every I epochs, apply IVT transform using cumulative F̄_{t-1} and current F_t
  4. At task end, update cumulative F̄_t = F̄_{t-1} + F_t

- **Design tradeoffs**:
  - IVT interval: More frequent (smaller I) keeps model in valid approximation region but may interrupt learning; paper shows robustness across I ∈ {1, 5, 10, 25}
  - Diagonal vs. full FIM: Full FIM gives marginal gains (+0.5% avg accuracy) at 2.5× memory cost
  - Memory: Requires storing previous task parameters and cumulative diagonal FIM (~2× model size overhead)

- **Failure signatures**:
  - Accuracy drops sharply mid-task: IVT interval too large; model has left valid quadratic region
  - New task learning stalls: IVT being applied too frequently, interfering with gradient descent
  - Memory errors on large models: Switch to checkpointing cumulative Fisher or reduce precision

- **First 3 experiments**:
  1. **Sanity check**: Implement IVT on CIFAR-100 with PODNet baseline, 5-task split. Verify ~1% average accuracy improvement and reduced forgetting vs. baseline. Check that linear interpolation plots show smoother accuracy curves.
  2. **Interval sensitivity**: Test IVT intervals I ∈ {1, 5, 10, 20} on same setup. Confirm paper's finding that IVT is robust across wide range, with optimal around I=10 for scratch-trained models.
  3. **Memory ablation**: Test with exemplar memory sizes |M| ∈ {5, 10, 20} to verify IVT helps most in low-memory regimes (Table IV shows +7.4% improvement at |M|=5 vs +1.4% at |M|=20).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive or structured Fisher Information Matrix (FIM) approximations improve IVT's ability to capture parameter interactions?
  - **Basis in paper**: The conclusion notes that while the diagonal FIM is efficient, "future work may further explore adaptive or structured approximations to better capture complex parameter interactions."
  - **Why unresolved**: The diagonal FIM assumes parameter independence, potentially missing correlations. The full FIM improves performance but incurs a 2.5× memory increase, making it impractical.
  - **What evidence would resolve it**: Experiments using Kronecker-factored or block-diagonal approximations that balance the performance gains of the full FIM with the efficiency of the diagonal version.

- **Open Question 2**: Can the IVT transformation interval be determined dynamically based on optimization dynamics?
  - **Basis in paper**: The paper notes the interval is the sole hyperparameter and requires balancing transformation accuracy with connectivity. It states that if the model drifts too far, the approximation fails, implying a need for adaptive triggering.
  - **Why unresolved**: Currently, the interval is fixed (e.g., every 10 epochs), assuming a uniform rate of change in the loss landscape across different tasks and datasets.
  - **What evidence would resolve it**: A method that monitors gradient variance or curvature to trigger IVT only when the local second-order approximation is at risk of breaking down, outperforming fixed schedules.

- **Open Question 3**: Does IVT effectively transfer to non-classification CIL scenarios, such as object detection or NLP?
  - **Basis in paper**: The analysis relies on classification accuracy and loss landscapes. While the theory is general, the "teleportation" of parameters via FIM transformation is only validated on image classification benchmarks (CIFAR, ImageNet).
  - **Why unresolved**: It is uncertain if the linear mode connectivity property holds or if the diagonal FIM provides sufficient curvature information for the loss landscapes of dense prediction tasks or sequential text data.
  - **What evidence would resolve it**: Applying IVT to continual learning benchmarks in object detection (e.g., VOCDIL) or text classification, demonstrating consistent forgetting reduction.

## Limitations

- IVT relies on local quadratic approximations that may break down for highly non-convex or rapidly changing loss landscapes
- The diagonal Fisher approximation may miss important parameter interactions in architectures with strong coupling between weights
- The method's effectiveness for extremely long task sequences (>10 tasks) or highly diverse tasks remains untested

## Confidence

**High Confidence**: The empirical improvements on standard benchmarks (CIFAR-100, FGVCAircraft, ImageNet) and the computational efficiency of the diagonal Fisher approximation are well-supported by the experimental results.

**Medium Confidence**: The theoretical justification for why IVT approximates oracle behavior through increment vector transformation is sound but relies on specific assumptions about the loss landscape geometry that may not hold universally across all task sequences.

**Low Confidence**: The generalizability of IVT to extremely long task sequences (>10 tasks) or highly diverse tasks (e.g., crossing domain boundaries) remains untested and may encounter approximation breakdown.

## Next Checks

1. **Approximation Validity Test**: Monitor the stability of parameter transformations across IVT intervals by computing the relative change in θ_t after each transformation. If |θ_new - θ_old| / |θ_old| exceeds 5% consistently, this indicates the model is leaving the valid quadratic region.

2. **Diagonal Approximation Quality**: Implement a controlled experiment comparing diagonal vs. full Fisher on a small-scale problem (e.g., CIFAR-10 with 2-3 tasks). Measure both accuracy difference and memory overhead to quantify the trade-off the paper claims is marginal.

3. **Long-sequence Robustness**: Extend CIFAR-100 evaluation to 10-task split (10→10→...→10) and measure degradation in IVT's performance compared to the 5-task setting. This will reveal whether the cumulative Fisher approximation scales well with task count.