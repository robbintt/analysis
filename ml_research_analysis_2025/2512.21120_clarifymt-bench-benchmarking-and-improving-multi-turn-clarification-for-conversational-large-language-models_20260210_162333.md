---
ver: rpa2
title: 'ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational
  Large Language Models'
arxiv_id: '2512.21120'
source_url: https://arxiv.org/abs/2512.21120
tags:
- user
- ambiguity
- clarification
- multi-turn
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ClarifyMT-Bench, a multi-turn clarification
  benchmark for evaluating large language models (LLMs) in open-domain dialogue. The
  benchmark addresses limitations in existing clarification datasets by incorporating
  realistic, noisy user behaviors across six personas and a five-dimensional ambiguity
  taxonomy.
---

# ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models

## Quick Facts
- arXiv ID: 2512.21120
- Source URL: https://arxiv.org/abs/2512.21120
- Reference count: 40
- Key outcome: Benchmark reveals consistent under-clarification bias across LLMs; ClarifyAgent framework improves clarification robustness through persona inference and structured reasoning.

## Executive Summary
This paper introduces ClarifyMT-Bench, a multi-turn clarification benchmark for evaluating large language models (LLMs) in open-domain dialogue. The benchmark addresses limitations in existing clarification datasets by incorporating realistic, noisy user behaviors across six personas and a five-dimensional ambiguity taxonomy. Through a hybrid LLM-human pipeline, 6,120 multi-turn dialogues were constructed, capturing diverse ambiguity sources and interaction patterns. Evaluation of ten representative LLMs revealed a consistent under-clarification bias, with models tending to answer prematurely rather than engage in sufficient clarification, especially as dialogue depth increases. To address this gap, the authors propose ClarifyAgent, an agentic framework that decomposes clarification into perception, forecasting, tracking, and planning. ClarifyAgent substantially improves robustness across ambiguity conditions, demonstrating the effectiveness of integrating user persona inference and structured reasoning for multi-turn clarification. The benchmark and method provide a reproducible foundation for studying when LLMs should ask, answer, and how to navigate ambiguity in real-world human-LLM interactions.

## Method Summary
ClarifyMT-Bench constructs 6,120 multi-turn dialogues from 1,020 base ambiguous queries using six user personas and a five-dimensional ambiguity taxonomy. The benchmark uses a hybrid LLM-human pipeline with LLM-generated dialogues validated by human reviewers (κ=0.5980). Evaluation employs LLM-as-a-Judge with GPT-4.1 to assess clarification accuracy and question quality. The proposed ClarifyAgent framework implements a modular pipeline: Perceiver extracts slot values and detects conflicts, Forecaster infers user persona, Tracker maintains finite-state machine slot states, and Planner makes ask-answer decisions using a Required Slot Completion condition. The system requires five forward passes per inference turn.

## Key Results
- Models consistently under-clarify, with accuracy dropping 15-30% from Turn 1 to Turn 3 across all tested LLMs
- ClarifyAgent achieves 88.4% accuracy, outperforming strongest baselines by 15.4 absolute points using Llama-3.1-8B backbone
- The under-clarification bias persists across model families including GPT-4.1, DeepSeek-V3, GPT-5, and open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User persona inference enables adaptive ask–answer decisions that account for noisy or uncooperative behavior.
- Mechanism: The Forecaster module classifies user behavior into one of six personas (Precise, Partial–Vague, Off–Focus, Contradictory, Factually–Wrong, Refusal). This persona label conditions the Planner's threshold for continuing clarification versus switching to answering, reducing premature answers under ambiguous feedback and avoiding unnecessary clarification with cooperative users.
- Core assumption: User behavioral patterns within a dialogue are sufficiently consistent to infer and exploit for downstream decisions.
- Evidence anchors:
  - [abstract] "ClarifyAgent...integrating user persona inference and structured reasoning for multi-turn clarification"
  - [Section 6.1] "The Forecaster estimates the user's behavioral persona from six categories...It outputs a structured persona label that captures interaction-level uncertainty and conditions the Planner's ask–answer decisions."
  - [Section 6.2 Table 6] Removing the Forecaster causes accuracy on Precise/Refusal to rise (91.8/91.9) while degrading sharply on noisy personas (67.2/72.1/75.2), showing persona inference balances robustness across behaviors.
- Break condition: If user behavior shifts unpredictably mid-dialogue or if persona inference accuracy is low, the conditioning signal becomes misleading, potentially causing over- or under-clarification.

### Mechanism 2
- Claim: Explicit slot-state tracking with conflict detection reduces under-clarification by making information gaps machine-readable.
- Mechanism: The Perceiver extracts candidate slot values from dialogue and assigns each slot one of three states: unfilled, filled, or conflict. The Tracker maintains this finite-state machine across turns. The Planner uses the RSC (Required Slot Completion) condition— all required slots filled and no conflicts—to gate the transition from clarification to answering.
- Core assumption: Ambiguity resolution can be modeled as a discrete slot-filling problem with identifiable conflict states, even for open-domain queries.
- Evidence anchors:
  - [Section 3.2] "At turn t, the dialogue state can be represented as x_t = [f_t(s_1), ..., f_t(s_n)], where each slot is in one of three abstract states: unfilled, filled, conflict."
  - [Section 6.2] "eliminating the Forecaster causes the model to become very strong on Precise and Refusal personas, while its performance degrades markedly on other personas"—without state tracking, models cannot distinguish filled from unfilled slots.
  - [corpus] Neighbor paper "MAC: A Multi-Agent Framework" (FMR=0.58) similarly decomposes clarification into specialized agents, suggesting modularity is a recurring design pattern.
- Break condition: If ambiguity sources do not map cleanly to discrete slots (e.g., latent preferences, discourse-level underspecification), the FSM representation becomes incomplete or noisy.

### Mechanism 3
- Claim: Modular decomposition with structured reasoning outperforms end-to-end prompting for multi-turn clarification under noise.
- Mechanism: ClarifyAgent separates perception (Perceiver), forecasting (Forecaster), state tracking (Tracker), and decision planning (Planner) into distinct modules, each producing intermediate structured outputs. This reduces the burden on any single forward pass and creates inspectable reasoning checkpoints, improving robustness compared to CoT or majority voting baselines.
- Core assumption: The overhead of multiple forward passes (5× per turn) is justified by accuracy gains, and intermediate outputs are sufficiently reliable.
- Evidence anchors:
  - [Section 6.2] "ClarifyAgent achieves an average accuracy of 88.4%, outperforming the strongest baseline by 15.4 absolute points" with Llama-3.1-8B backbone.
  - [Section 6.2] "ClarifyAgent requires five forward passes...matching the inference cost of Majority Voting and Intent-Sim...delivering substantially larger accuracy gains."
  - [corpus] Related work (AT-CoT, ICPO) explores structured reasoning or calibrated policies for multi-turn settings, but does not integrate persona inference with slot tracking—ClarifyAgent's combination appears novel.
- Break condition: If any module produces low-quality intermediate outputs (e.g., incorrect persona prediction, missed slot extraction), errors propagate downstream; the pipeline has no explicit error recovery mechanism.

## Foundational Learning

- Concept: **Finite-state dialogue state tracking**
  - Why needed here: ClarifyAgent models multi-turn clarification as sequential slot-filling with discrete states. Understanding FSMs and state-transition logic is prerequisite to implementing or debugging the Tracker.
  - Quick check question: Given slot states [unfilled, filled, conflict] and a new user utterance that contradicts a previously filled slot, what transition should occur?

- Concept: **Ask–answer decision as classification under uncertainty**
  - Why needed here: The core task is a binary decision (Clarify vs. Answer) conditioned on dialogue state and user persona. Framing this as a thresholded classification problem clarifies evaluation metrics (accuracy, under-/over-clarification rates).
  - Quick check question: If a model answers when it should clarify, is this under-clarification or over-clarification per the paper's definition?

- Concept: **Persona-based user simulation for evaluation**
  - Why needed here: The benchmark uses six behaviorally defined user personas to generate multi-turn dialogues. Understanding how personas are constructed and validated is essential for interpreting results and extending the benchmark.
  - Quick check question: Which persona type would likely require the most clarification turns before the RSC condition is met?

## Architecture Onboarding

- Component map:
  - Input → Perceiver (slot extraction + conflict detection) → Tracker (FSM state) + Forecaster (persona inference) → Planner (decision synthesis) → Output (question/answer generation)
  - Tracker and Forecaster run in parallel after Perceiver; Planner integrates both signals.

- Critical path:
  1. Perceiver output quality directly determines Tracker state accuracy
  2. Forecaster persona prediction conditions Planner decision thresholds
  3. Planner's RSC check gates the transition from Clarify to Answer
  4. Any module failure cascades to Output

- Design tradeoffs:
  - 5 forward passes per turn vs. single-pass baselines (higher latency, higher accuracy)
  - Balanced robustness across personas vs. optimizing for cooperative users (ClarifyAgent sacrifices ~10 points on Precise/Refusal for large gains on noisy personas)
  - Discrete slot representation vs. open-ended ambiguity modeling (simpler but may miss latent factors)

- Failure signatures:
  - Persistent under-clarification: Likely Perceiver missing unfilled slots or Forecaster over-predicting Precise persona
  - Over-clarification with cooperative users: Forecaster misclassifying Precise as noisy; Tracker not recognizing filled slots
  - Performance collapse at Turn 3+: State accumulation errors; consider context window or state compression

- First 3 experiments:
  1. Ablate Forecaster on a held-out persona mixture to quantify the robustness vs. cooperative-user tradeoff (replicate Table 6 with different seed mixtures).
  2. Inject noise into Perceiver slot predictions to measure downstream Planner sensitivity and identify error propagation thresholds.
  3. Compare ClarifyAgent with and without explicit conflict state handling to isolate the value of the conflict label for contradictory user replies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent frameworks optimize for noisy user personas without sacrificing efficiency for precise, cooperative users?
- Basis in paper: [explicit] The authors note that ClarifyAgent "does not explicitly optimize for the Precise persona" and that there is a "trade-off" where removing the Forecaster boosts performance on Precise/Refusal inputs while hurting others.
- Why unresolved: The current architecture prioritizes robustness across noisy conditions (Partial-Vague, Contradictory) at the cost of efficiency for unambiguous inputs.
- What evidence would resolve it: A method that maintains high robustness on noisy personas while matching baseline performance on precise inputs.

### Open Question 2
- Question: Can alignment objectives be modified to intrinsically reduce the "under-clarification bias" observed across model families?
- Basis in paper: [explicit] The paper identifies a "consistent under-clarification bias" where models answer prematurely, attributing this to models being "primarily optimized for helpfulness and fluency."
- Why unresolved: The study addresses the issue via inference-time engineering (ClarifyAgent) rather than resolving the root alignment objective.
- What evidence would resolve it: A model trained with a novel loss function or reward model that achieves high clarification accuracy natively without agentic scaffolding.

### Open Question 3
- Question: How does clarification performance and state tracking degrade in dialogues extending beyond the benchmark's three-turn limit?
- Basis in paper: [inferred] The authors limit dialogues to 2-3 turns to avoid "accumulating distributional artifacts" and "compounding model errors," leaving robustness in longer interactions unstudied.
- Why unresolved: It is unclear if the observed performance drop at Turn 3 continues linearly or stabilizes as the conversation deepens.
- What evidence would resolve it: Evaluation results on a dataset extended to 5+ turns with controlled noise injection.

## Limitations

- The benchmark relies on LLM-generated dialogues, potentially introducing sampling bias toward LLM-style ambiguity and user behaviors
- The slot-state representation assumes ambiguity can be reduced to discrete fills and conflicts, which may not capture latent or discourse-level ambiguity sources
- The evaluation is limited to English open-domain dialogue and may not generalize to task-oriented or multilingual settings

## Confidence

**High confidence**: The empirical finding of consistent under-clarification across models (especially at Turn 3+) is well-supported by the controlled evaluation on 6,120 multi-turn dialogues. The mechanism linking persona inference to adaptive ask-answer decisions is directly validated through ablations in Table 6, showing clear tradeoffs.

**Medium confidence**: The claim that modular decomposition with structured reasoning outperforms end-to-end prompting is supported by the 15.4-point accuracy gain, but the comparison is against baselines (CoT, majority voting) that may not represent state-of-the-art prompting techniques. The slot-state tracking mechanism is theoretically sound but lacks ablation studies isolating the value of the conflict state versus simpler tracking.

**Low confidence**: The generalizability of the five ambiguity dimensions and six user personas to real-world user populations is asserted but not externally validated. The paper does not provide evidence that the persona categories capture the full space of user behaviors or that the FSM representation is sufficient for all ambiguity types.

## Next Checks

1. **External persona validation**: Test the six user personas on a held-out dataset of real human-human dialogues to measure classification accuracy and identify any missing behavioral categories.

2. **Conflict state ablation**: Compare ClarifyAgent's performance with and without explicit conflict state tracking to isolate the value of conflict detection for contradictory user replies.

3. **Cost-benefit analysis**: Measure the end-to-end latency of ClarifyAgent (5× forward passes) versus single-pass baselines in a realistic deployment setting to quantify the practical tradeoff between accuracy and speed.