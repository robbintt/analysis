---
ver: rpa2
title: 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts'
arxiv_id: '2508.07785'
source_url: https://arxiv.org/abs/2508.07785
tags:
- arxiv
- experts
- architecture
- grove
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Grove MoE, a novel Mixture-of-Experts architecture
  that incorporates varying-sized experts to improve computational efficiency. Inspired
  by the big.LITTLE CPU architecture, Grove MoE uses adjugate experts organized into
  groups, allowing shared computations to be performed once per group even when multiple
  experts are activated.
---

# Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts

## Quick Facts
- arXiv ID: 2508.07785
- Source URL: https://arxiv.org/abs/2508.07785
- Authors: Haoyuan Wu; Haoxing Chen; Xiaodong Chen; Zhanchao Zhou; Tieyuan Chen; Yihong Zhuang; Guoshan Lu; Zenan Huang; Junbo Zhao; Lin Liu; Zhenzhong Lan; Bei Yu; Jianguo Li
- Reference count: 8
- Key outcome: Grove MoE uses adjugate experts organized into groups to achieve efficiency gains while maintaining performance, demonstrated on 33B parameter models.

## Executive Summary
Grove MoE introduces a novel Mixture-of-Experts architecture that incorporates adjugate experts organized into groups, allowing shared computations to be performed once per group even when multiple experts are activated. This approach draws inspiration from the big.LITTLE CPU architecture and addresses the efficiency challenge in large MoE models. The authors developed GroveMoE-Base and GroveMoE-Inst, 33B-parameter models built by upcycling Qwen3-30B-A3B-Base through mid-training and post-training. These models dynamically activate 3.14-3.28B parameters based on token complexity.

## Method Summary
Grove MoE extends standard MoE with adjugate experts organized into groups. Each group contains multiple experts plus one adjugate expert that can be shared when multiple experts in the same group activate. The architecture upcycles Qwen3-30B-A3B-Base by adding adjugate capacity to each expert. Training uses an auxiliary-loss-free load balancing strategy with dynamic bias terms updated via sign gradient descent. The model employs decoupled routing with Softmax for weights and Sigmoid for selection. Mid-training uses 400B tokens with AdamW optimizer, followed by SFT fine-tuning on synthetic data.

## Key Results
- GroveMoE-Inst achieves MMLU-Pro score of 72.8, outperforming similar-sized models
- Model dynamically activates 3.14-3.28B parameters based on token complexity
- GroveMoE-Inst achieves 74.5 on MultiPL-E benchmark
- Model maintains performance across diverse tasks including SuperGPQA (47.7) and GPQA-Diamond (61.3)

## Why This Works (Mechanism)

### Mechanism 1: Shared Computation via Grouped Adjugate Experts
Organizing experts into groups with shared adjugate experts reduces redundant computation when multiple experts in the same group activate. When experts E_r and E_s from the same group are both activated, their outputs combine as: ρ_r·E_r(x) + ρ_s·E_s(x) + (ρ_r + ρ_s)·λ·A_j(x). The adjugate expert A_j is computed once and scaled by the sum of routing weights, rather than computed separately for each expert.

### Mechanism 2: Dynamic Activation Through Group Structure
The number of activated adjugate experts varies from ⌈k·g/n⌉ to k per layer, enabling implicit dynamic computation without specialized routing logic. With n=128 experts, k=4 activated experts, and g=64 groups, activated adjugates range from 2–4. When activated experts cluster in fewer groups, fewer adjugates compute. When spread across groups, more adjugates activate.

### Mechanism 3: Upcycling with Distribution Preservation
Adding adjugate experts (rather than duplicating base experts) preserves the original routing score distribution during mid-training upcycling. Extending each expert with parallel adjugate capacity maintains original ρ distribution. Adjugate down-projection blocks are zero-initialized; other weights use σ=0.006 initialization.

## Foundational Learning

- **Concept: Top-k Routing in Standard MoE**
  - Why needed here: Grove MoE is explicitly designed to be compatible with standard top-k routing; understanding this baseline is required to see what the architecture changes.
  - Quick check question: Given routing scores ρ = [0.4, 0.3, 0.2, 0.1] and k=2, which experts are activated and what are their weights?

- **Concept: Load Balancing in MoE**
  - Why needed here: Section 3.3 uses an auxiliary-loss-free strategy with dynamic bias terms; this differs from standard auxiliary-loss approaches and requires understanding why load imbalance causes routing collapse.
  - Quick check question: If one expert receives 80% of tokens across a batch, what two problems does this create for training and inference?

- **Concept: Parameter-Activation Trade-off in MoE**
  - Why needed here: The paper's efficiency claims depend on distinguishing total parameters (33B) from activated parameters (3.14–3.28B); understanding this ratio is essential for evaluating the architecture.
  - Quick check question: A model has 100B total parameters but activates 5B per token. How does this compare FLOPs-wise to a 10B dense model processing the same token?

## Architecture Onboarding

- **Component map:**
  Input token x → Router R → Top-k selection → For each activated expert i in group j: expert_output = E_i(x) + λ · A_j(x) → Final output = Σ ρ_i · expert_output

- **Critical path:**
  1. Group assignment: Expert i belongs to group j = ⌊(i-1) / (n/g)⌋ + 1
  2. Scaling constraint: λ ≤ g/n (e.g., λ ≤ 0.5 for n=128, g=64)
  3. Bias update for load balancing: b ← b - α · (F-Q) / ||F-Q||₂
  4. Decoupled routing: Use Softmax for weights ρ^(h), Sigmoid for selection ρ^(σ)

- **Design tradeoffs:**
  - Fewer groups (g=16): Higher efficiency (20% computation savings) but potential capacity constraints for general tasks
  - More groups (g=64): Better general task performance, lower efficiency gains (5% savings)
  - Larger λ (0.20): Better for code generation, risk of distribution shift
  - Smaller λ (0.05): Safer for general knowledge, maintained stability

- **Failure signatures:**
  - Routing collapse: If bias update rate α is too aggressive, experts may oscillate or converge to uniform (use α=0.001 as starting point)
  - Inference overhead: Current SGLang implementation uses two kernel calls per layer, causing 30% slowdown vs. theoretical 10%—requires custom kernel
  - Training instability: If λ > g/n, adjugate contributions may dominate base expert outputs

- **First 3 experiments:**
  1. **Group configuration sweep:** Train with 50B tokens comparing g∈{64, 32, 16} while keeping total params fixed at 33B. Measure: (a) average activated parameters, (b) MMLU/GSM8K scores, (c) training throughput. Expect g=64 best for general tasks, g=16 best for math/STEM.
  2. **Scaling factor validation:** With g=64 fixed, compare λ∈{0.05, 0.10, 0.20}. Monitor routing distribution shifts from baseline. Hypothesis: λ=0.05 should maintain closest distribution to original Qwen3-30B-A3B routing.
  3. **Upcycling vs. scratch comparison:** Initialize Grove MoE from Qwen3-30B-A3B-Base (zero-init adjugate down-projections) vs. random initialization. Train both for 50B tokens. Expect upcycled version to converge faster and retain more general knowledge from pre-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a customized unified kernel designed to process both expert types in a single operation align the inference speed of GroveMoE with its theoretical computational efficiency?
- Basis in paper: The authors state in Section 5.6 that the current implementation uses generic MoE kernels requiring two separate calls, resulting in a 30% latency overhead compared to the theoretical maximum.
- Why unresolved: The current implementation relies on generic kernels (fused-moe) which do not support the simultaneous processing of standard and adjugate experts.
- What evidence would resolve it: Benchmark results from a deployed GroveMoE model using a custom kernel that demonstrates inference latency within ~10% of the baseline Qwen3-30B-A3B model.

### Open Question 2
- Question: To what extent does the integration of Reinforcement Learning (RL) techniques improve the capabilities of GroveMoE compared to the current reliance on rejection sampling?
- Basis in paper: The Limitations section identifies the exclusive reliance on rejection sampling for model refinement as a constraint and lists the integration of RL methods as a key objective for future development.
- Why unresolved: The current post-training pipeline utilized rejection sampling but explicitly excluded RL approaches, leaving their potential impact on the Grove MoE architecture untested.
- What evidence would resolve it: A comparative evaluation of GroveMoE-Inst models trained with and without RL (e.g., PPO or DPO) on alignment and reasoning benchmarks.

### Open Question 3
- Question: Does the inclusion of long-Chain-of-Thought (CoT) data in the mid-training corpus enable GroveMoE to close the capability gap with reasoning-specialized LLMs?
- Basis in paper: The authors note in the Limitations section that a scarcity of long-CoT data in the mid-training corpus curtails the model's capacity for advanced reasoning compared to models like Qwen3-30B-A3B-2507.
- Why unresolved: The current mid-training corpus lacked sufficient long-CoT data, preventing an assessment of the architecture's full reasoning potential.
- What evidence would resolve it: Evaluation results from a GroveMoE variant mid-trained on a corpus enriched with long-CoT data, specifically on advanced reasoning benchmarks like AIME or OlympiadBench.

## Limitations
- The paper's efficiency claims rely on the assumption that within-group expert co-activation occurs frequently enough to realize computational savings, but this frequency is not empirically validated.
- The upcycling process claims to preserve routing score distributions when extending base experts with adjugate capacity, but the paper provides no empirical validation of this preservation.
- The dynamic computation mechanism depends on token complexity correlating with expert activation spread across groups, but no ablation study demonstrates that the claimed 3.14-3.28B parameter range corresponds to meaningful complexity differences.

## Confidence
- **High Confidence**: The architectural framework is internally consistent and mathematically sound. The load balancing mechanism using sign gradient descent with decoupled routing is well-specified and aligns with established MoE training practices.
- **Medium Confidence**: The reported benchmark performances are credible given the model scale and training approach, but independent reproduction is needed to verify the specific Grove MoE implementations achieve these results.
- **Low Confidence**: The efficiency claims (10-20% computation savings) and dynamic activation claims (3.14-3.28B parameter range) lack sufficient empirical validation in the paper to support the magnitude of benefits claimed.

## Next Checks
1. **Routing Co-activation Analysis**: Implement instrumentation to measure the actual frequency of multi-expert co-activation within groups during inference. Compare the observed co-activation rates against the theoretical maximum to quantify realized efficiency gains.

2. **Routing Distribution Shift Quantification**: Measure and compare the routing score distributions between the original Qwen3-30B-A3B and the upcycled GroveMoE-Inst. Use KL divergence or other statistical measures to quantify how much the adjugate experts shifted the learned routing patterns.

3. **Dynamic Activation Validation**: Design an experiment that systematically varies token complexity (using prompts of known difficulty) and measures the actual parameter activation range. Verify that the 3.14-3.28B range corresponds to meaningful complexity differences rather than random variation.