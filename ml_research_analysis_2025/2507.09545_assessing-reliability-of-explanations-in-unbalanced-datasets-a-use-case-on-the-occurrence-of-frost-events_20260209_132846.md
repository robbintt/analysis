---
ver: rpa2
title: 'Assessing reliability of explanations in unbalanced datasets: a use-case on
  the occurrence of frost events'
arxiv_id: '2507.09545'
source_url: https://arxiv.org/abs/2507.09545
tags:
- explanations
- class
- data
- explanation
- minority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reliability of explanations for minority
  classes in highly unbalanced datasets, using frost event prediction as a case study.
  The authors propose a method that generates on-manifold neighborhoods using k-medoids
  clustering and applies explanation aggregation with distance-based weighting.
---

# Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events

## Quick Facts
- arXiv ID: 2507.09545
- Source URL: https://arxiv.org/abs/2507.09545
- Reference count: 15
- Primary result: DeepLIFT achieved highest robustness (97.69% ±2.26%) and consistency (99.40% ±1.51%) for minority class explanations

## Executive Summary
This study addresses the challenge of evaluating explanation reliability for minority classes in highly unbalanced datasets. The authors propose generating on-manifold neighborhoods using k-medoids clustering and aggregating explanations with distance-based weighting. Tested on frost event prediction using atmospheric data, the method demonstrates that DeepLIFT provides the most reliable explanations for minority classes, with both robustness and consistency scores exceeding 97%.

## Method Summary
The approach generates on-manifold neighborhoods for minority class samples using k-medoids clustering on the validation set, then creates perturbations by interpolating between samples and cluster medoid neighbors using Beta-distributed mixing coefficients. Explanations are computed for the original sample and its neighbors, then aggregated using inverse-distance weighting. Two Spearman correlation metrics—local robustness (between original and neighbor explanations) and consistency (between original and aggregated explanations)—assess explanation reliability.

## Key Results
- DeepLIFT achieved highest robustness (97.69% ±2.26%) and consistency (99.40% ±1.51%) scores
- On-manifold neighborhoods produced more reliable explanations than random perturbations
- The dual-metric framework effectively captures explanation stability for minority classes

## Why This Works (Mechanism)

### Mechanism 1: On-Manifold Neighborhood Generation via k-Medoids
- Claim: Generating neighborhoods constrained to the data manifold produces more reliable explanations for minority classes than random perturbation.
- Mechanism: k-medoids clustering partitions the validation set; perturbations are created by interpolating between a data point and its cluster's neighbor medoid using a Beta-distributed mixing coefficient (λ). This ensures perturbations remain within the observed data distribution.
- Core assumption: The minority class data manifold can be meaningfully approximated by cluster medoids and their neighbors despite limited samples.
- Evidence anchors:
  - [abstract]: "They propose a method that generates on-manifold neighborhoods using k-medoids clustering and applies explanation aggregation with distance-based weighting."
  - [section 2.1]: "Leveraging the manifold hypothesis, we are able to construct neighbourhoods which are on-manifold... A neighbourhood should not only be made of datapoints which are close to the original one, but also be faithful to the observed data distribution."
  - [corpus]: Weak direct evidence; related XAI papers (e.g., ProtoScore framework) discuss evaluation methods but not specifically on-manifold perturbations for class imbalance.

### Mechanism 2: Distance-Weighted Explanation Aggregation
- Claim: Aggregating explanations from neighboring points using inverse-distance weighting produces more stable and reliable explanations for minority class instances.
- Mechanism: For each neighbor's explanation e(x̃), apply weight π(x, x̃) = 1/dist(x, x̃). The aggregated explanation ē(x) is the weighted average. Closer neighbors contribute more, reducing noise from farther perturbations.
- Core assumption: Similar inputs (small distance) should yield similar explanations; explanation function is locally smooth.
- Evidence anchors:
  - [abstract]: "Tested on a neural network trained to predict frost events...DeepLIFT achieved the highest robustness (97.69% ±2.26%) and consistency (99.40% ±1.51%) scores."
  - [section 2.2]: "Using the aggregated explanation allows us to take into account the results of a data augmentation on the minority class, which is ensured to be faithful as it lays on-manifold."
  - [corpus]: Limited corpus evidence; privacy-XAI integration paper mentions explanation reliability but not aggregation specifically.

### Mechanism 3: Dual-Metric Reliability Assessment (Robustness + Consistency)
- Claim: Two complementary Spearman correlation metrics—local robustness and consistency—together provide a more complete picture of explanation reliability than either alone.
- Mechanism: Local robustness R̂(x) averages rank correlation between the original explanation and each neighbor's explanation. Consistency Ĉ(x) computes rank correlation between the original and the aggregated explanation. High scores on both indicate stable, locally faithful explanations.
- Core assumption: Spearman rank correlation adequately captures explanation similarity; importance ranking (not magnitude) is the relevant signal.
- Evidence anchors:
  - [abstract]: "They introduce two metrics: local robustness (Spearman correlation of explanations for neighboring points) and consistency (correlation between original and aggregated explanations)."
  - [section 4]: "The consistency scores show that retaining local information from carefully crafted on-manifold neighbourhoods can be beneficial for the minority class explanations."
  - [corpus]: ProtoScore paper evaluates XAI quality but uses different evaluation criteria; no direct validation of this dual-metric approach.

## Foundational Learning

- **Class Imbalance and Focal Loss**
  - Why needed here: The paper uses focal loss (FL(p) = -α(1-p)^γ log(p)) to train on a 99:1 imbalanced dataset. Understanding how α and γ modulate loss weighting is essential for reproducing results.
  - Quick check question: If γ=2.5 and α=0.75, what happens to the loss for a well-classified minority class sample (p=0.9) vs. a poorly classified one (p=0.3)?

- **Manifold Hypothesis and Distribution Shift**
  - Why needed here: The core methodological contribution relies on generating perturbations that stay "on-manifold." Random Gaussian noise creates distribution shift, particularly harming minority class evaluation.
  - Quick check question: Why does adding Gaussian noise to a sparse minority class create larger distribution shift than for the majority class?

- **Backpropagation-Based Attribution Methods**
  - Why needed here: The paper compares Integrated Gradients, DeepLIFT, LRP, and an ensemble. Each uses different backpropagation rules; LRP's vanishing gradient problem is explicitly noted.
  - Quick check question: What is the key difference between DeepLIFT and Integrated Gradients in how they handle reference baselines?

## Architecture Onboarding

- **Component map:**
  1. Data pipeline: ERA5 atmospheric data → standardization → stratified train/val/test split by municipality
  2. Model: 5-layer fully connected neural network with ReLU activations, sigmoid output
  3. Training: Focal loss (γ=2.5, α=0.75), RAdam optimizer, 100 epochs, batch size 256
  4. Neighborhood generation: k-medoids on validation set → medoid neighbor selection → Beta-interpolation → class-prediction filtering
  5. Explanation: DeepLIFT/IG/LRP/Ensemble attribution computation
  6. Aggregation: Inverse-distance weighted averaging of neighbor explanations
  7. Evaluation: Robustness R̂(x) and consistency Ĉ(x) via Spearman correlation

- **Critical path:**
  1. Train model with focal loss → validate F1-score on minority class (target: >0.5)
  2. Compute k-medoids clusters on validation set (k_medoids, avg cluster size ~10)
  3. For each test minority class sample: identify cluster → find k_nn=5 neighbor medoids → generate n=100 perturbations via Eq. 1
  4. Filter perturbations that flip predicted class
  5. Compute explanations for original and all filtered neighbors
  6. Aggregate using distance weighting (Eq. 2)
  7. Compute robustness (Eq. 3) and consistency (Eq. 4)

- **Design tradeoffs:**
  - Neighborhood size (n=100): Larger n improves statistical reliability but increases computation; authors chose 100 as minimum
  - Perturbation magnitude (λ=0.05): Lower λ keeps neighbors closer but may reduce diversity; tuned to maintain 95% same-class predictions
  - Explanation method selection: DeepLIFT shows highest robustness/consistency; LRP suffers vanishing gradients but ensemble mitigates this
  - k_medoids choice: More clusters capture finer structure but reduce cluster size; authors don't specify exact k

- **Failure signatures:**
  - Zero-vector attributions: LRP with wrong propagation rule (authors used epsilon rule to minimize this)
  - Low robustness (<70%): Indicates explanations are unstable; neighborhood may span decision boundary or leave manifold
  - Low consistency (<80%): Aggregated explanation diverges from original; may indicate unreliable local structure
  - Excessive filtering: If >50% of perturbations flip class prediction, λ may be too high or model is unstable in that region

- **First 3 experiments:**
  1. **Validate neighborhood generation effect**: Replicate Figure 2—compare random Gaussian noise vs. k-medoids neighborhoods on robustness scores for both classes. Expect minority class to show larger degradation with random perturbation.
  2. **Explanation method comparison on your data**: Run all four methods (IG, DeepLIFT, LRP, Ensemble) on your test set with k-medoids neighborhoods. Compute mean and std of robustness/consistency; identify which method performs best for your specific data distribution.
  3. **Hyperparameter sensitivity analysis**: Vary λ (0.01, 0.05, 0.1) and k_nn (3, 5, 7) while monitoring the percentage of same-class predictions and robustness scores to find optimal settings for your dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does explanation reliability for minority classes vary across different class imbalance ratios and model performance levels?
- Basis in paper: [explicit] The authors state: "we plan to investigate how the results change as the performance of the neural network model varies and, at the same time, investigate the effects of a varying unbalancing ratio between the two classes."
- Why unresolved: The study only tests a single imbalance ratio (99:1) and one trained model; the relationship between imbalance severity, minority-class F1-score, and explanation robustness/consistency remains unexplored.
- What evidence would resolve it: Systematic experiments across multiple imbalance ratios (e.g., 90:10, 95:5, 99.5:0.5) with models trained to different performance levels, measuring changes in robustness and consistency scores.

### Open Question 2
- Question: Can uncertainty quantification improve the evaluation and trustworthiness of minority class explanations?
- Basis in paper: [explicit] The authors state: "it would be interesting to test how an uncertainty analysis on the minority class could be beneficial for the correct evaluation of these datasets from both a technical and a practitioner point of view."
- Why unresolved: The current framework uses robustness and consistency metrics but does not incorporate model uncertainty, which may be especially relevant for minority classes with limited training examples.
- What evidence would resolve it: Integrating uncertainty estimates (e.g., Monte Carlo dropout, deep ensembles) into the evaluation pipeline and assessing whether uncertainty-weighted explanations correlate better with practitioner judgments.

### Open Question 3
- Question: Does the proposed evaluation framework generalize to different model architectures, data modalities, and spatio-temporal datasets?
- Basis in paper: [explicit] The authors state: "we also plan to investigate different machine learning models for improving performance with unbalanced dataset (e.g. by explicitly taking into account the spatio-temporal dimensions that often characterise many datasets) and adapt the choice of XAI methodologies and reliability reasoning accordingly."
- Why unresolved: The method is validated only on a fully connected neural network with tabular numerical features using Euclidean distance; applicability to CNNs, tree-based models, or datasets with categorical/mixed features and temporal dependencies is unknown.
- What evidence would resolve it: Applying the framework to diverse architectures (e.g., XGBoost, LSTMs) and data types (text, images, spatio-temporal grids), with appropriate distance metrics and neighborhood generation schemes.

### Open Question 4
- Question: Can a unified quality metric combining robustness and consistency effectively guide practitioners in selecting explanation methods?
- Basis in paper: [explicit] The authors state: "the analysis could be further enriched by defining a metric to quantify the quality of an explanation in such complex use-cases, considering both the robustness and the consistency as defined in Section 2."
- Why unresolved: Robustness and consistency are reported separately; DeepLIFT excels in both, but practitioners lack a single criterion to rank methods when trade-offs exist between the two scores.
- What evidence would resolve it: Proposing and validating a combined metric on multiple datasets, correlating it with expert assessments of explanation usefulness in real-world decision-making scenarios.

## Limitations

- Method's effectiveness critically depends on sufficient minority class samples for meaningful k-medoids clustering
- Study validated only on tabular atmospheric data with Euclidean distance; generalization to other data modalities unknown
- Specific hyperparameter choices (λ=0.05, k_nn=5, n=100) may not be optimal for all datasets

## Confidence

- **High Confidence**: The dual-metric reliability framework (robustness + consistency) and the core methodological approach are well-justified and technically sound.
- **Medium Confidence**: The specific choice of hyperparameters (λ=0.05, k_nn=5, n=100) may not generalize optimally to other datasets or imbalance ratios.
- **Medium Confidence**: While DeepLIFT shows superior performance, the relative ranking of explanation methods may shift depending on data characteristics.

## Next Checks

1. **Minority Class Density Validation**: Quantitatively assess whether generated on-manifold neighborhoods actually cover the minority class manifold by measuring neighborhood diversity (e.g., pairwise distances, clustering metrics) before and after perturbation.
2. **Distribution Shift Analysis**: For both majority and minority classes, measure the maximum mean discrepancy (MMD) between original data and k-medoids-generated perturbations to confirm reduced distribution shift compared to random noise.
3. **Robustness under Varying Imbalance**: Replicate the entire pipeline on datasets with different imbalance ratios (e.g., 90:10, 95:5, 99:1) to identify at which point the k-medoids approach becomes ineffective due to insufficient minority samples.