---
ver: rpa2
title: 'Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark
  Study'
arxiv_id: '2509.04468'
source_url: https://arxiv.org/abs/2509.04468
tags:
- level
- financial
- reasoning
- knowledge
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive evaluation of large
  language models (LLMs) on professional financial reasoning using 1,560 CFA mock
  exam questions across all three levels. The research compares GPT-4o, GPT-o1, and
  o3-mini under zero-shot and retrieval-augmented generation (RAG) conditions, implementing
  a novel RAG pipeline that integrates official CFA curriculum content.
---

# Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study

## Quick Facts
- arXiv ID: 2509.04468
- Source URL: https://arxiv.org/abs/2509.04468
- Authors: Xuan Yao; Qianteng Wang; Xinbo Liu; Ke-Wei Huang
- Reference count: 5
- Key outcome: GPT-o1 consistently outperforms other models on CFA exams, with RAG providing substantial improvements particularly at higher complexity levels

## Executive Summary
This study presents the first comprehensive evaluation of large language models (LLMs) on professional financial reasoning using 1,560 CFA mock exam questions across all three levels. The research compares GPT-4o, GPT-o1, and o3-mini under zero-shot and retrieval-augmented generation (RAG) conditions, implementing a novel RAG pipeline that integrates official CFA curriculum content. Results show GPT-o1 consistently outperforms other models, with zero-shot accuracy exceeding 94% at Level I and RAG providing substantial improvements particularly at higher complexity levels. Comprehensive error analysis reveals knowledge gaps as the primary failure mode, with minimal impact from text readability.

## Method Summary
The study evaluates three LLMs (GPT-4o, GPT-o1, o3-mini) on 1,560 CFA mock exam multiple-choice questions across Levels I-III. Two conditions are tested: zero-shot prompting and RAG-enhanced prompting. The RAG pipeline uses hierarchical organization by level-topic, two-stage retrieval (model-generated summaries plus top-5 segments), and domain-specific embedding with text-embedding-3-small. Accuracy metrics are collected per level, topic, and question type, with comprehensive error analysis categorizing failures into knowledge gaps, reasoning errors, calculation mistakes, and inconsistency issues.

## Key Results
- GPT-o1 achieves zero-shot accuracy of 94.56% at Level I, significantly outperforming GPT-4o (89.44%) and o3-mini (90.00%)
- RAG augmentation improves accuracy across all models, with Level III showing the most substantial gains (up to 21.82 percentage points)
- Error analysis reveals knowledge gaps as the dominant failure mode, while text readability has minimal impact on model performance
- RAG demonstrates better performance gains than model scaling alone, particularly for complex reasoning tasks

## Why This Works (Mechanism)
The effectiveness of RAG in this financial reasoning context stems from the complex, domain-specific knowledge required for CFA exam questions. The hierarchical retrieval system ensures that retrieved context is both topically relevant and at the appropriate difficulty level. For Level III questions requiring integration of multiple concepts, RAG provides critical supporting evidence that models might otherwise miss. The two-stage retrieval process, combining model-generated summaries with top-5 segments, captures both high-level context and detailed information needed for accurate reasoning.

## Foundational Learning
- **Financial domain complexity**: CFA exams cover 10 topics with varying difficulty and knowledge depth (avg 146-2,538 chars) - needed to understand why RAG helps more at higher levels
- **Zero-shot vs RAG distinction**: Zero-shot relies solely on model knowledge while RAG augments with external context - critical for interpreting accuracy improvements
- **Error taxonomy framework**: Knowledge, reasoning, calculation, and inconsistency categories - essential for systematic failure analysis
- **Hierarchical retrieval**: Organizing curriculum by level-topic for targeted context retrieval - important for understanding RAG architecture
- **Question complexity gradient**: Level I conceptual questions vs Level III case-based analysis - explains differential model performance

## Architecture Onboarding
- **Component map**: CFA mock questions -> Zero-shot/RAG pipeline -> LLM (GPT-4o, GPT-o1, o3-mini) -> Answer + explanation -> Error analysis
- **Critical path**: Question parsing → Context retrieval (RAG) → Prompt engineering → Model inference → Answer validation
- **Design tradeoffs**: Knowledge augmentation via RAG vs. model scaling; retrieval quality vs. computational cost
- **Failure signatures**: Poor retrieval quality (irrelevant segments), RAG introducing confusion (conflicting context), knowledge gaps in complex topics
- **First experiments**: 1) Run zero-shot on Level I questions only, 2) Implement basic RAG with single-topic retrieval, 3) Compare error types between zero-shot and RAG conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on 2022-2024 mock exams introduces potential temporal bias in question difficulty and content representation
- Error analysis depends on manual annotation that may introduce subjectivity in distinguishing between knowledge gaps and reasoning errors
- Focus on multiple-choice questions limits generalizability to open-ended financial reasoning tasks

## Confidence
- High confidence in comparative model performance (GPT-o1 vs. others)
- Medium confidence in RAG effectiveness claims
- Medium confidence in error analysis conclusions

## Next Checks
1. Replicate the study using a different year of CFA mock exams to test temporal robustness of the findings
2. Implement alternative chunking strategies for the RAG pipeline to assess sensitivity to curriculum segmentation
3. Conduct blind annotation of a subset of errors by multiple annotators to quantify inter-rater reliability in the error taxonomy