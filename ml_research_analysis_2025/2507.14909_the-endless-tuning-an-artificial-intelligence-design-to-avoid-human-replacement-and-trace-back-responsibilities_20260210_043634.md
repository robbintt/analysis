---
ver: rpa2
title: The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement
  and Trace Back Responsibilities
arxiv_id: '2507.14909'
source_url: https://arxiv.org/abs/2507.14909
tags:
- case
- such
- which
- artificial
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The Endless Tuning framework is a design method for deploying\
  \ AI systems that avoids user replacement and enables tracing of responsibilities\
  \ through a double mirroring process. Implemented in three prototypes\u2014loan\
  \ granting, pneumonia diagnosis, and art style recognition\u2014it combines human\
  \ decision-making with AI assistance via explanations, similarity comparisons, and\
  \ confidence measures."
---

# The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities

## Quick Facts
- arXiv ID: 2507.14909
- Source URL: https://arxiv.org/abs/2507.14909
- Authors: Elio Grande
- Reference count: 40
- One-line primary result: Human experts felt fully in control when using the Endless Tuning protocol, with no sense of AI replacement

## Executive Summary
The Endless Tuning framework proposes a design method for deploying AI systems that preserves human agency and enables accountability through a five-step interactive protocol. Implemented in three classification tasks—loan approval, pneumonia diagnosis, and art style recognition—the method requires users to form initial impressions, view AI explanations before predictions, compare similar cases, assess confidence, and make final decisions while logging all interactions. Expert users reported feeling fully in control despite the AI's assistance, suggesting the protocol successfully avoids automation bias. The permanent interaction logging creates a potential audit trail for tracing responsibility in case of errors or damages.

## Method Summary
The Endless Tuning protocol implements a five-stage decision-making process where users engage with AI assistance while maintaining full agency. The method combines deep learning models (decision tree for loans, ResNet-34 for images) with XAI techniques (RISE, Grad-CAM) and similarity comparisons via PCA embeddings. Users must form initial impressions before seeing any AI output, then receive explanation-based suggestions without the model's prediction, view similar cases, see confidence scores, and finally make their own decision. All interactions are permanently logged for potential retrospective accountability analysis. The approach uses continual learning with rehearsal techniques to fine-tune models based on user decisions while preserving performance on original data.

## Key Results
- Human experts across three domains reported feeling fully in control with no sense of AI replacement
- Model accuracies: loan approval ~83%, pneumonia diagnosis ~95%, art style recognition ~65%
- The double mirroring process (human and AI mutually tuning each other) preserved user agency while providing assistance
- Permanent interaction logging enables potential tracing of responsibilities in case of errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Requiring users to form initial impressions before seeing AI suggestions preserves human agency and reduces automation bias
- Mechanism: The protocol mandates a five-stage process: (1) first impression, (2) explanation-based suggestions shown before model output, (3) similarity comparisons, (4) confidence display, (5) fine-tuning. This creates a double negative feedback loop where human and model mutually tune each other
- Core assumption: Users will engage more critically when required to commit to an initial judgment, rather than passively accept recommendations
- Evidence anchors:
  - [abstract] "employing deep learning models, full control was perceived by the interviewees in the decision-making setting"
  - [section 3.1] "Requesting the user's initial impression is a crucial step, as it grants them the agency to begin engaging with the task"
  - [corpus] Related work on hybrid decision-making (arxiv:2512.00420) emphasizes human authority retention but lacks empirical validation of specific protocol sequences
- Break condition: If users rush through initial impressions or provide random labels, the cognitive forcing function fails

### Mechanism 2
- Claim: Presenting XAI explanations (saliency maps, rule extractions) before showing model predictions enables hermeneutic interpretation rather than post-hoc rationalization
- Mechanism: By hiding the model's actual prediction while displaying how the model "sees" the input (via RISE, Grad-CAM, or decision rules), users engage in interpretive dialogue rather than recommendation acceptance
- Core assumption: Explanation-first framing triggers different cognitive processes than prediction-first framing
- Evidence anchors:
  - [abstract] "a reversed and hermeneutic deployment of XAI algorithms"
  - [section 3.2] "an explanation is a double jump: it is not relative to the input data, but to how the machine saw it... the user should not yet be so affected by automation bias"
  - [corpus] ClarifAI framework (arxiv:2507.11733) uses case-based reasoning for interpretability but does not test temporal ordering of explanations vs. predictions
- Break condition: If explanations are unfaithful or highlight irrelevant features (as noted by the radiologist: "the maps were completely inaccurate"), users may distrust the entire system

### Mechanism 3
- Claim: Permanent interaction logging creates traceability that can link technical failures to specific actors (developers, interface designers, annotators, users)
- Mechanism: A black-box-like log captures case studies, user annotations, timestamps, model weights, feature importance outputs, and fine-tuning decisions. Retrospective analysis can identify failure sources
- Core assumption: Judges or auditors will be able to interpret technical logs meaningfully
- Evidence anchors:
  - [abstract] "A permanent log of interactions enables retrospective analysis of decision processes, potentially linking technical failures to specific parties in case of damages"
  - [section 5] "the authority, by retracing the log... will be able to fully observe the process. At that point, it might for instance become apparent that: (a) the user provides vague or hasty assessments... (b) the interface designer selected a misleading colormap"
  - [corpus] No corpus papers address accountability logging architectures directly; this mechanism lacks external validation
- Break condition: If logs are incomplete, tampered with, or too technical for legal interpretation, accountability fails

## Foundational Learning

- Concept: **Explainable AI (XAI) methods and their faithfulness limitations**
  - Why needed here: The protocol relies on RISE, Grad-CAM, and rule extraction to generate "suggestions," but these may highlight spurious features
  - Quick check question: Can you explain why Grad-CAM and RISE might produce different saliency maps for the same model and input?

- Concept: **Continual Learning with rehearsal techniques**
  - Why needed here: User decisions become labels for fine-tuning; the system must avoid catastrophic forgetting
  - Quick check question: What is the risk of fine-tuning a model on user-provided labels without mixing in original training data?

- Concept: **Automation bias and cognitive forcing functions**
  - Why needed here: The entire protocol is designed as a cognitive forcing function to counter over-reliance
  - Quick check question: Why might showing confidence scores increase trust even when they're miscalibrated?

## Architecture Onboarding

- Component map: Data layer (training dataset + temporary set + case study set) -> Model layer (base classifier + fine-tuning mechanism) -> Explanation layer (RISE explainer + Grad-CAM explainer + similarity encoder) -> Interface layer (Streamlit app with 5 modules) -> Logging layer (CSV-based session recording)

- Critical path: User selects case → forms initial impression → views explanation suggestions (without model output) → views similar cases → sees model confidence → makes final decision → decision stored for future fine-tuning

- Design tradeoffs:
  - Showing explanations before predictions reduces automation bias but risks confusing users when explanations are unfaithful
  - Similarity comparisons require embeddings; compression introduces information loss
  - Fine-tuning on user labels adapts to local context but may drift from validated performance
  - Permanent logging enables accountability but raises privacy concerns

- Failure signatures:
  - Users skipping to step 5 without engaging (tracked via time stamps)
  - Model making correct predictions for wrong reasons (detected via expert review of saliency maps)
  - Embeddings clustering on artifacts rather than semantic features (similarity suggestions appear nonsensical)
  - Fine-tuning on insufficient or biased user labels

- First 3 experiments:
  1. Run the Streamlit prototype with a non-expert user; verify that initial impressions are recorded before any AI output is visible
  2. Test with contradictory explanations (RISE vs. Grad-CAM showing different regions); observe whether users engage critically or become confused
  3. Simulate a retrospective audit: introduce a deliberate interface flaw (e.g., misleading colormap), generate logs, and attempt to trace responsibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Endless Tuning protocol be effectively adapted for generative AI systems, where outputs are created rather than classified?
- Basis in paper: [explicit] The author states: "the case of generative artificial intelligence is left for future research" and expresses hope that the design "could serve as inspiration... allowing people to participate right in the generation process."
- Why unresolved: The current protocol assumes decision-making tasks (classification/regression); generative AI involves open-ended creation with different accountability challenges.
- What evidence would resolve it: Implementation and testing of modified protocol modules for generative tasks, with expert user studies measuring perceived control and responsibility attribution.

### Open Question 2
- Question: How can similarity-based suggestions be improved to better balance formal feature matching with semantic meaning in expert domains?
- Basis in paper: [explicit] The aesthetics professor found similarity suggestions "surprising" and noted tension between "formal" and "material/content" similarity: "From the formal point of view, I understand that it's closer... But from the material point of view, I think they're not so close."
- Why unresolved: Current embeddings capture low-level features that may conflict with higher-order semantic understanding that domain experts rely on.
- What evidence would resolve it: Comparative studies using different embedding strategies (e.g., multimodal, concept-based) with domain experts evaluating suggestion quality across multiple fields.

### Open Question 3
- Question: What tracking mechanisms can effectively detect disengaged or malicious user behavior without creating excessive surveillance concerns?
- Basis in paper: [inferred] The paper notes users might "select a random label" or "deliberately leave the session open merely in order to give the impression of carefully studying the case," requiring "tracking devices... capable of capturing what actually happens without any prejudice."
- Why unresolved: Balancing accountability with privacy and avoiding user manipulation of the log system remains an open design challenge.
- What evidence would resolve it: Development of behavioral metrics that correlate with decision quality, validated against ground truth in controlled studies with both engaged and adversarial users.

### Open Question 4
- Question: How does the Endless Tuning protocol perform in longitudinal deployments with repeated fine-tuning cycles, particularly regarding model stability and user adaptation?
- Basis in paper: [inferred] The paper acknowledges "a complete test in the long run would have been required, yet entailing too much time" and only tested single sessions per expert. Questions of catastrophic forgetting and model drift under continual learning with human-in-the-loop fine-tuning remain unexplored.
- Why unresolved: Single-session experiments cannot reveal how mutual adaptation evolves, whether trust calibration stabilizes, or if performance degrades over many fine-tuning cycles.
- What evidence would resolve it: Multi-week deployment studies tracking model metrics, user trust calibration, and decision quality across hundreds of sessions per user.

## Limitations

- Limited empirical validation: Only three expert users tested the system, making generalizability uncertain
- Explanation quality issues: Radiologist noted saliency maps were "completely inaccurate," suggesting explanation-first protocol may not work as intended
- No external accountability validation: Legal professionals were not consulted to verify whether technical logs are interpretable for responsibility tracing

## Confidence

- Human agency preservation: Medium confidence (based on expert interviews, but limited sample size)
- XAI explanation effectiveness: Low confidence (conflicting evidence from radiologist feedback)
- Accountability mechanism utility: Low confidence (no external validation with legal professionals)
- Model accuracy claims: Medium confidence (reasonable but incomplete experimental details)

## Next Checks

1. **User behavior tracking**: Implement session recording that captures time spent on each step and mouse movements. Analyze whether users actually engage with the initial impression step or skip to predictions.

2. **Legal interpretability test**: Conduct a workshop with legal professionals (judges, lawyers) to evaluate whether they can meaningfully interpret the interaction logs and assign responsibility based on the recorded data.

3. **Explanation quality validation**: Test the RISE and Grad-CAM explainers on cases where the model makes correct predictions for known-wrong reasons (adversarial examples or controlled feature manipulations). Measure whether users can detect these failures through the explanation-first protocol.