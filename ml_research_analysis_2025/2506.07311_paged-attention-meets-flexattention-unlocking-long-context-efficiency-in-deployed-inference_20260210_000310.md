---
ver: rpa2
title: 'Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in
  Deployed Inference'
arxiv_id: '2506.07311'
source_url: https://arxiv.org/abs/2506.07311
tags:
- memory
- latency
- inference
- attention
- pagedattention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PagedAttention, an OS-inspired paging mechanism\
  \ for KV caches, integrated into IBM\u2019s Foundation Model Stack (FMS) via PyTorch\u2019\
  s FlexAttention. The method addresses severe memory inefficiencies in long-context\
  \ LLM inference by partitioning KV caches into fixed-size pages, enabling dynamic,\
  \ fine-grained memory management with near-zero fragmentation."
---

# Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference

## Quick Facts
- arXiv ID: 2506.07311
- Source URL: https://arxiv.org/abs/2506.07311
- Reference count: 17
- Authors: Thomas Joshi; Herman Saini; Neil Dhillon; Antoni Viros i Martin; Kaoutar El Maghraoui
- One-line primary result: PagedAttention reduces KV cache memory waste from 60–80% to <5% overhead through fine-grained, on-demand allocation integrated with FlexAttention for near-FlashAttention performance.

## Executive Summary
This paper introduces PagedAttention, an OS-inspired paging mechanism for KV caches, integrated into IBM's Foundation Model Stack (FMS) via PyTorch's FlexAttention. The method addresses severe memory inefficiencies in long-context LLM inference by partitioning KV caches into fixed-size pages, enabling dynamic, fine-grained memory management with near-zero fragmentation. The core contribution is a lock-free page manager and a fused FlexAttention kernel that executes attention over non-contiguous memory layouts with near-FlashAttention performance. Experiments on NVIDIA L4 GPU (24GB) show significantly reduced inference latency, scaling linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching.

## Method Summary
PagedAttention partitions KV caches into fixed-size pages (64–128 tokens) managed by a lock-free bump-pointer allocator from a global free-list. Per-sequence block tables map logical token positions to physical pages, enabling immediate reuse when sequences terminate. FlexAttention, via TorchInductor, fuses a custom mask_mod enforcing sequence-local attention with the QK⊤V loop, gathering scattered pages via coalesced memory reads. This enables near-FlashAttention throughput over non-contiguous layouts without hand-written CUDA kernels. The implementation achieves numerical equivalence with standard attention while dramatically reducing memory fragmentation and enabling linear latency scaling with sequence length.

## Key Results
- Memory waste reduced from 60–80% to <5% overhead through fine-grained, on-demand allocation
- Near-FlashAttention throughput achieved over non-contiguous KV pages via fused FlexAttention kernel
- Linear latency scaling (~2× increase) from 128 to 2048 tokens with global KV cache vs exponential (~10× per doubling) without caching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PagedAttention reduces KV cache memory waste from 60–80% to <5% overhead through fine-grained, on-demand allocation.
- Mechanism: Fixed-size pages (64–128 tokens) are allocated via a lock-free bump-pointer from a global free-list; per-sequence block tables map logical token positions to physical pages, enabling immediate reuse when sequences terminate.
- Core assumption: Internal fragmentation from pre-allocated monolithic buffers is the dominant source of memory inefficiency in mixed-length inference batches.
- Evidence anchors:
  - [abstract] "addressing internal fragmentation and inefficiencies associated with monolithic KV cache allocations"
  - [section I] "Empirical studies report 60–80% average waste for mixed-length batches in popular inference servers"
  - [corpus] Related work (PagedEviction, KV Admission) confirms KV cache memory pressure as a critical bottleneck, though these focus on eviction/admission rather than allocation efficiency.
- Break condition: If requests are uniformly long (near max_sequence_length), fragmentation reduction yields minimal benefit; page-table overhead may dominate.

### Mechanism 2
- Claim: FlexAttention enables near-FlashAttention throughput over non-contiguous KV pages without hand-written CUDA kernels.
- Mechanism: A custom `mask_mod` enforces sequence-local attention (`allow ⇔ (idq = idk) ∧ (k ≤ len(idq))`); TorchInductor JIT-fuses indexing logic with the QK⊤V loop, gathering scattered pages via coalesced memory reads.
- Core assumption: FlexAttention's compilation overhead is amortized across inference steps and does not introduce unacceptable latency at decode time.
- Evidence anchors:
  - [abstract] "our fused attention kernel efficiently gathers scattered KV data"
  - [section III] "TorchInductor fuses this logic with the QK⊤V loop, yielding a single half-precision kernel"
  - [corpus] No direct corpus evidence on FlexAttention specifically; related kernels (FlashAttention, xFormers) optimize contiguous layouts but do not address paging.
- Break condition: If attention patterns require dynamic sparsity beyond what `mask_mod` can express, or if Triton/CUDA compilation fails on edge cases, performance degrades.

### Mechanism 3
- Claim: Global KV caching converts exponential latency growth (~10× per sequence doubling) to linear scaling (~2× from 128–2048 tokens).
- Mechanism: Cached K/V tensors eliminate redundant full-context attention recomputation; each decode step reads prior states from memory rather than recomputing, trading compute for bandwidth.
- Core assumption: GPU memory bandwidth is sufficient to service cache reads without becoming the new bottleneck; cache hits dominate.
- Evidence anchors:
  - [abstract] "growing only linearly (~2×) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching"
  - [section IV, Figure 3] "Latency scales linearly (~2× increase) as sequence lengths grow from 128 to 2048 tokens, in contrast to the exponential latency increase (~10× per doubling) observed without caching"
  - [corpus] LongSpec and related work assume cached inference as baseline; corpus does not independently validate the 2× vs 10× claim.
- Break condition: If cache is disabled or evicted (e.g., under memory pressure from concurrent sequences), latency reverts to exponential scaling.

## Foundational Learning

- **KV Cache Fundamentals**
  - Why needed here: The entire paper presupposes understanding that autoregressive decoding stores key/value vectors for all prior tokens; without this, the fragmentation problem is unintelligible.
  - Quick check question: Explain why a 7B model generating 100k tokens requires storing progressively more KV vectors, and estimate the memory footprint.

- **Memory Fragmentation (Internal vs External)**
  - Why needed here: The paper's core contribution addresses internal fragmentation from over-provisioned buffers; distinguishing this from external fragmentation clarifies what paging solves.
  - Quick check question: Given a 24GB GPU with 10 pre-allocated 2GB buffers (max 4096 tokens each), what fraction is wasted if 8 requests use only 1024 tokens?

- **Fused Attention Kernels**
  - Why needed here: FlexAttention is positioned as a portable alternative to FlashAttention; understanding tiling and SRAM residency explains why non-contiguous access is non-trivial.
  - Quick check question: Why does FlashAttention achieve 2–4× speedups over naive attention, and what breaks when KV tensors are scattered across pages?

## Architecture Onboarding

- **Component map:**
  ```
  Request → Page Manager (lock-free allocator) → Block Table (logical→physical mapping)
                                               ↓
           Global KV Buffers (fixed pages) ← FlexAttention Kernel (mask_mod + score_mod)
                                               ↓
                                         Output Logits
  ```

- **Critical path:**
  1. `RESERVE(seq_id, len)` allocates `ceil(len/P)` pages from free-list (O(1))
  2. `ASSIGN(seq_id, pos, K_new, V_new)` writes new tokens to mapped pages
  3. `GATHER(seq_id, len)` materializes contiguous view for attention (fused via FlexAttention)
  4. Attention kernel executes with sequence-local mask, producing logits

- **Design tradeoffs:**
  - Page size (64 vs 128 tokens): Smaller pages reduce internal fragmentation but increase block-table memory and indirection overhead.
  - Power-of-two allocations: Simplifies indexing but may over-provision at short sequence lengths.
  - Lock-free allocator: Reduces contention but assumes single-GPU deployment; multi-GPU scaling is unexplored.

- **Failure signatures:**
  - OOM at short sequences: Block-table overhead exceeds savings; check `page_table` memory footprint.
  - Latency spikes at 2048+ tokens: Power-of-two allocation rounds up aggressively; verify actual vs requested page counts.
  - Perplexity divergence: `mask_mod` logic error allowing cross-sequence attention; unit-test mask with batched sequences.

- **First 3 experiments:**
  1. **Memory overhead benchmark:** Run LLaMA-7B with batched mixed-length requests (256–4096 tokens); compare peak memory vs baseline. Expect <5% overhead per paper claims.
  2. **Latency scaling validation:** Measure per-token latency at 128, 512, 1024, 2048 tokens with global cache enabled vs disabled; confirm ~2× linear vs ~10× exponential.
  3. **Numerical equivalence test:** Compute perplexity on WikiText-103 with PagedAttention vs standard attention; expect match within floating-point tolerance (Baseline 7.32, Paged 7.31).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can page-based KV management be extended to support gradient backpropagation during training, enabling long-context fine-tuning without contiguous memory allocation?
- Basis in paper: [explicit] Section II.B identifies "Training-Time Applicability" as a gap: "Extending page-based KV management to back-propagation (activations and optimizer state) could enable long-context fine-tuning, but no public study addresses gradient-flow over non-contiguous memory."
- Why unresolved: The paper explicitly scopes to inference-only; gradient flow over non-contiguous storage is described as "non-trivial" and deferred to future work.
- What evidence would resolve it: A prototype demonstrating backpropagation through paged attention with convergence metrics matching contiguous baselines on a fine-tuning task.

### Open Question 2
- Question: How does PagedAttention with FlexAttention perform on next-generation accelerators (H100/Hopper tensor-memory architecture, AMD MI300X, TPU v4/v5e) compared to the T4/L4 GPUs tested?
- Basis in paper: [explicit] Section II.B notes: "Little is known about behavior on Hopper (H100) tensor-memory architecture, MI300X high-bandwidth memory, or TPU v4/v5e demand-paging ASICs." Section V.C reiterates hardware dependence as a limitation.
- Why unresolved: The paper evaluates only T4 and L4 GPUs; architectural differences in newer accelerators may affect kernel fusion efficiency and memory coalescing patterns.
- What evidence would resolve it: Benchmark results for latency, throughput, and memory overhead on H100, MI300X, and TPU5e under identical workloads.

### Open Question 3
- Question: Does the lock-free allocator sustain microsecond-scale allocation latency under high contention with hundreds of concurrent sequences, without degradation in throughput?
- Basis in paper: [explicit] Section II.B identifies a gap: "Formal analyses of allocation/free latency, lock contention, and scalability across hundreds of concurrent sequences are lacking."
- Why unresolved: The paper's batch experiments max out at 16 concurrent prompts; scalability to production-scale concurrency remains uncharacterized.
- What evidence would resolve it: Stress tests measuring per-allocation latency and throughput with 100–500+ concurrent sequences, profiling lock contention if any.

### Open Question 4
- Question: Would adaptive or hierarchical page sizing (analogous to OS huge pages) reduce overhead for workloads with highly variable sequence lengths compared to fixed-size pages?
- Basis in paper: [explicit] Section V.D lists "Variable page sizes: A hierarchy of page granularity" as a future direction; Section VI.C recommends "Adaptive page sizing" for future research.
- Why unresolved: Current implementation uses fixed-size pages (64–128 tokens); the paper acknowledges power-of-two allocations cause minor overhead at certain sequence lengths.
- What evidence would resolve it: Comparative benchmarks of fixed vs. adaptive page sizing across mixed-length workloads, measuring memory overhead and allocation latency.

## Limitations

- The paper's core claims rely heavily on assumptions about workload characteristics (mixed-length batches) that may not hold universally; for uniformly long sequences, page-table overhead may negate gains.
- The lock-free page manager assumes single-GPU deployment without contention; multi-GPU scaling remains unexplored and could introduce contention or other bottlenecks.
- While numerical equivalence is claimed, the paper does not address numerical stability under extreme conditions (very long sequences, high batch sizes, or mixed precision training vs inference).

## Confidence

- **High confidence**: The memory fragmentation problem and basic PagedAttention mechanism (allocation efficiency, fixed-size pages) are well-grounded in the empirical studies cited (60-80% waste reduction to <5% overhead).
- **Medium confidence**: The FlexAttention integration claims near-FlashAttention performance over non-contiguous layouts are plausible but lack direct corpus validation; TorchInductor's behavior may vary across GPU architectures.
- **Medium confidence**: The linear latency scaling claim (2× vs 10×) is supported by the paper's Figure 3 but requires independent validation, as the corpus does not verify these specific scaling ratios.

## Next Checks

1. **Memory overhead verification**: Run LLaMA-7B inference with batched mixed-length requests (256–4096 tokens) on NVIDIA L4; measure peak memory vs baseline attention implementation. Verify actual memory overhead stays below 5% as claimed, and quantify block-table memory footprint at different sequence lengths.

2. **Latency scaling replication**: Measure per-token latency at 128, 512, 1024, 2048 tokens with global KV cache enabled vs disabled on identical hardware. Confirm the paper's claim of ~2× linear scaling versus ~10× exponential growth, and test if this holds across different batch sizes and sequence length distributions.

3. **Numerical stability and equivalence**: Compute perplexity on WikiText-103 and other standard benchmarks using both PagedAttention and standard attention implementations. Verify numerical equivalence within floating-point tolerance (baseline 7.32 vs paged 7.31) and test for divergence under extreme conditions (very long sequences, high batch sizes, or different precision modes).