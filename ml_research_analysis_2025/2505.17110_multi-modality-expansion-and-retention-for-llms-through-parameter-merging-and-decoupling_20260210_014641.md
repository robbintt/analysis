---
ver: rpa2
title: Multi-Modality Expansion and Retention for LLMs through Parameter Merging and
  Decoupling
arxiv_id: '2505.17110'
source_url: https://arxiv.org/abs/2505.17110
tags:
- mmer
- mllms
- parameters
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a training-free method, MMER, to expand multimodal
  capabilities of large language models by merging and decoupling parameters across
  multiple existing multimodal models. MMER reuses modality-specific encoders, merges
  task vectors, and constructs binary masks to decouple modality-specific parameters,
  reducing conflicts and retaining original performance.
---

# Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling

## Quick Facts
- arXiv ID: 2505.17110
- Source URL: https://arxiv.org/abs/2505.17110
- Reference count: 37
- The paper proposes a training-free method, MMER, to expand multimodal capabilities of large language models by merging and decoupling parameters across multiple existing multimodal models.

## Executive Summary
This paper introduces MMER, a training-free method to expand multimodal capabilities of large language models by merging and decoupling parameters across multiple existing multimodal models. The approach reuses modality-specific encoders, merges task vectors, and constructs binary masks to decouple modality-specific parameters, reducing conflicts and retaining original performance. Experiments show MMER achieves 99% retention of original performance across fourteen dual-modal tasks, improves multimodal expansion results significantly over baselines, and mitigates catastrophic forgetting when adapting to new tasks without losing performance on previous ones. Storage cost is approximately twice that of model merging but delivers notable performance gains.

## Method Summary
MMER works by extracting task vectors (fine-tuned deltas) from multiple dual-modal MLLMs, merging them using sign-consistent aggregation with TopK% sparsification, and generating binary masks to decouple modality-specific parameters. During inference, modality-specific masks dynamically route inputs to relevant parameters, allowing a single merged model to handle multiple modalities independently while preserving original task performance. The method avoids catastrophic forgetting by reconstructing original models from the merged representation and calibrated masks.

## Key Results
- Achieves ~99% retention of original performance across fourteen dual-modal tasks
- Improves multimodal expansion accuracy significantly over baseline model merging approaches
- Mitigates catastrophic forgetting when adapting to new tasks without degrading previous task performance
- Storage overhead is approximately twice that of simple model merging

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Composition via Sign Consistency
Merging delta weights (task vectors) of separate MLLMs creates a unified model capable of handling all original modalities when sign conflicts are resolved. The method extracts task vectors τ_i = θ_i - θ_pre for each modality and uses TIES-Merging to combine them by sparsifying vectors (TopK%) and resolving sign conflicts by retaining the sign with highest magnitude. This allows aggregation of modality-specific adaptations without retraining, assuming fine-tuned capabilities are largely encoded in delta weights that can be linearly combined if interference is minimized.

### Mechanism 2: Parameter Decoupling via Directional & Magnitude Heuristics
Binary masks approximate separation of modality-specific parameters within merged weight matrices, reducing inference-time interference. Masks are constructed by comparing original task vectors with merged vectors using two heuristics: (1) Directional Congruence - parameters are kept only if signs match between τ_i and τ_*, preventing fighting between modalities; (2) Dominant Significance - parameters are kept only if magnitude in τ_i is significant relative to τ_* (controlled by λ). This assumes modality-specific knowledge is stored in distinct, overlapping parameter subspaces where dominant parameters are critical.

### Mechanism 3: Dynamic Inference Routing via Masking
Applying modality-specific masks during forward pass allows single merged model to process distinct modalities independently. Inputs are encoded by specific encoders, then within LLM attention layers the relevant modality mask m_i is applied to merged weights τ_* before processing that modality's tokens. This effectively switches on vision-specific weights for vision tokens and switches off audio-specific interference, assuming computational overhead of mask application is acceptable.

## Foundational Learning

- **Concept: Task Vectors (Model Arithmetic)**
  - Why needed: MMER is built on premise that "skill" of model is vector that can be added to base model (θ_final = θ_base + τ_skill)
  - Quick check: If you have base LLM and math-fine-tuned LLM, how do you extract the "math task vector"?

- **Concept: Catastrophic Forgetting**
  - Why needed: Paper frames "Multi-Modality Retention" as solution to catastrophic forgetting; understanding that merging often degrades original performance is key to appreciating mask reconstruction step
  - Quick check: Why does fine-tuning Vision LLM on new Audio task typically ruin its Vision capabilities?

- **Concept: Parameter Interference (Sign Conflicts)**
  - Why needed: Core problem MMER solves is interference; must know that simply averaging weights (θ_A + θ_B)/2 fails when one model increases weight and other decreases it
  - Quick check: In merged weight matrix, if Model A wants neuron at +5.0 and Model B wants it at -5.0, what happens if you simply average them?

## Architecture Onboarding

- **Component map:** Modality Encoders -> Projectors/Connectors -> Merged LLM Backbone (θ_pre + τ_*) -> Binary Masks (m_i)
- **Critical path:**
  1. Pre-processing: Extract τ_i for all MLLMs, merge using TIES (TopK% pruning + sign election) to get τ_*
  2. Mask Generation: Compute binary masks m_i by checking sign congruence and magnitude dominance between τ_i and τ_*
  3. Inference: For input of modality i, load θ_pre + (m_i ◦ τ_*) for specific attention layers handling that input
- **Design tradeoffs:**
  - Storage vs. Universality: MMER requires base LLM weights + merged task vector + N binary masks (approx. 2x storage of simple merging) but avoids storing N separate full models
  - Sparsity (TopK%) vs. Fidelity: Aggressive pruning reduces interference but may lose fine-grained modality details
  - Threshold (λ): Higher λ selects fewer parameters (more conservative), potentially hurting new task performance to preserve original tasks
- **Failure signatures:**
  - Performance Collapse (Random Guessing): Likely caused by incorrect scaling factors or λ values filtering out all active parameters
  - Modality Bleeding: Model describes images using audio concepts - suggests "Directional Congruence" failed or masks were too dense
  - Reconstruction Failure: Inability to recover original MLLM performance (<90% retention) suggests merged vector τ_* lost critical dominant weights during TIES merging
- **First 3 experiments:**
  1. Sanity Check (Dual-Modality): Merge Vision LLM and Audio LLM, verify resulting model can answer questions about image AND audio file using MMER inference pipeline
  2. Retention Test: Take merged model, apply Vision mask, evaluate on standard Vision benchmarks (VQAv2), compare accuracy against original Vision LLM to confirm ~99% retention
  3. Ablation on "Directional Congruence": Remove sign check (set m_i=1 based only on magnitude), measure drop in MCUB or retention scores to validate interference mechanism

## Open Questions the Paper Calls Out
- Can parameter decoupling strategy effectively mitigate interference when applied to significantly larger MLLMs (70B+ parameters), or does complexity of parameter interactions increase storage costs or computational overhead prohibitively?
- How does MMER perform when expanding to modalities with vastly different data characteristics or sparsity, such as medical imaging, depth sensing, or thermal data?
- Can binary mask decoupling mechanism be adapted to merge MLLMs that were not fine-tuned from exact same pre-trained LLM checkpoint?
- Can binary masks or merged task vectors be compressed to reduce "approximately twice" storage cost overhead without significantly compromising 99% performance retention?

## Limitations
- Reliance on specific hyperparameter tuning (particularly λ_i values) without providing calibration procedures or validation sets
- Computational overhead during inference lacks detailed analysis and timing comparisons
- Claims about effectiveness across "various modalities" based on only four specific modalities (vision, audio, video, point cloud)
- Storage cost being "approximately twice" that of model merging presented without detailed trade-off analysis

## Confidence
- **High Confidence:** Fundamental mechanism of task vector composition via sign consistency is well-supported by mathematical framework and ablation studies; 99% retention claim backed by specific benchmark results
- **Medium Confidence:** Parameter decoupling approach using binary masks shows promise but effectiveness depends heavily on proper λ_i calibration which isn't fully specified
- **Low Confidence:** Claims about computational efficiency and scalability to arbitrary modalities lack sufficient empirical support

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary λ_i values across all modalities and document retention rates and multimodal expansion performance to establish robust calibration guidelines
2. **Inference Overhead Benchmarking:** Measure and compare wall-clock inference times and memory usage between MMER, baseline model merging, and ensemble approaches across different batch sizes and mixed-modality inputs
3. **Cross-Modality Generalization Test:** Apply MMER to merge models with at least two novel modalities not covered in the paper (e.g., graph-based and medical imaging) to validate claimed modality-agnostic nature of approach