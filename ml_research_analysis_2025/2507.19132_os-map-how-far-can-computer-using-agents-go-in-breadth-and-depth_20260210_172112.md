---
ver: rpa2
title: 'OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?'
arxiv_id: '2507.19132'
source_url: https://arxiv.org/abs/2507.19132
tags:
- arxiv
- task
- agents
- tasks
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OS-M AP introduces a two-dimensional benchmark for computer-using
  agents, organizing 416 real-world tasks across 15 applications by automation level
  (L1-L5) and generalization scope (S1-S3). This design captures task complexity and
  real-world demand alignment, enabling fine-grained evaluation.
---

# OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?
## Quick Facts
- arXiv ID: 2507.19132
- Source URL: https://arxiv.org/abs/2507.19132
- Reference count: 40
- Benchmark reveals state-of-the-art computer-using agents achieve only 11.4% overall success across 416 real-world tasks

## Executive Summary
OS-MAP introduces a comprehensive two-dimensional benchmark for evaluating computer-using agents, organizing 416 real-world tasks across 15 common applications by automation level (L1-L5) and generalization scope (S1-S3). The benchmark captures task complexity and real-world demand alignment, enabling fine-grained evaluation of agent capabilities. Experiments demonstrate that current state-of-the-art agents struggle significantly with higher-level tasks requiring perception, reasoning, and coordination, achieving near-zero performance on these challenging categories. The study highlights critical gaps in autonomous computer use and provides a structured roadmap for future development.

## Method Summary
The OS-MAP benchmark systematically collects and annotates real-world computer tasks across 15 applications, organizing them along two dimensions: automation level (L1-L5, from manual to fully autonomous) and generalization scope (S1-S3, from fixed to adaptive). Each task undergoes human annotation for these dimensions plus real-world demand alignment, with average task completion times recorded. The benchmark evaluates 12 computer-using agents using a unified testing environment with two interface modes (GUI and OCR), measuring success rates across different task categories. The evaluation includes both general-purpose models and GUI-specific architectures, with additional analysis of planning-grounding configurations.

## Key Results
- State-of-the-art agents achieve only 11.4% overall success rate across all 416 tasks
- Performance drops dramatically for higher-level tasks: L5 tasks show 0% success, S3 tasks show only 0.8% success
- GUI-specific models and planning-grounding setups outperform general-purpose models by 1.2-4.8% in overall success rates
- Word/Excel/PPT tasks show highest success (19.2-31.6%), while audio/image processing tasks show lowest success (0.0-0.8%)

## Why This Works (Mechanism)
OS-MAP works by providing a structured framework that captures the multidimensional nature of real-world computer tasks. The two-dimensional classification system (automation level and generalization scope) allows for nuanced evaluation of agent capabilities across different task complexities and adaptability requirements. By including 416 tasks across 15 common applications with human-annotated difficulty levels and real-world demand alignment, the benchmark creates a comprehensive testing ground that reveals specific weaknesses in current agent architectures.

## Foundational Learning
- Task Automation Levels (L1-L5): Understanding the spectrum from manual to fully autonomous task execution; needed to measure agent independence and capability progression
- Generalization Scope (S1-S3): Distinguishing between fixed, adaptive, and fully generalizable tasks; needed to evaluate agent flexibility and transfer learning ability
- GUI vs OCR Interfaces: Recognizing different input modalities for agent interaction; needed to test agent adaptability to various computer interfaces
- Real-world Demand Alignment: Measuring task relevance to actual user needs; needed to ensure benchmark reflects practical applications
- Task Annotation Process: Understanding human labeling methodology for difficulty assessment; needed to interpret benchmark results and potential biases
- Interface Mode Impact: Analyzing how different input methods affect agent performance; needed to optimize agent architecture for specific use cases

## Architecture Onboarding
- Component Map: Task Collection -> Annotation Pipeline -> Benchmark Organization (L1-L5 x S1-S3) -> Agent Evaluation Environment -> Success Rate Analysis
- Critical Path: Real-world task identification → human annotation for difficulty levels → benchmark categorization → agent testing across interface modes → performance analysis
- Design Tradeoffs: GUI-specific models offer better performance but less generalizability vs general-purpose models with broader applicability but lower success rates
- Failure Signatures: Agents fail on tasks requiring multi-step reasoning, context understanding, and coordination between different applications
- First Experiments: 1) Compare GUI vs OCR performance for the same tasks; 2) Test planning-grounding vs direct execution approaches; 3) Analyze performance differences across automation levels

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of current approaches to handle more complex real-world scenarios, the need for better planning and grounding mechanisms in autonomous agents, and the potential for improving generalization capabilities across different task types and applications. The authors also question how to effectively bridge the performance gap between GUI-specific and general-purpose models while maintaining practical utility.

## Limitations
- Annotation subjectivity may affect reliability of task difficulty classifications
- Limited task coverage per application (average 27.7 tasks) may not capture full usage patterns
- Different training data distributions and evaluation protocols across tested models complicate direct comparisons
- Analysis focuses on aggregate performance rather than detailed per-category breakdowns

## Confidence
- High confidence in overall benchmark design and task distribution analysis based on empirical data
- Medium confidence in absolute success rate comparisons between different agent architectures due to confounding factors
- Medium confidence in claims about GUI-specific model superiority across all task categories due to limited per-category analysis

## Next Checks
1. Conduct cross-validation studies with multiple human annotators to assess inter-rater reliability for task difficulty classifications
2. Expand the benchmark with additional tasks for underrepresented applications to improve coverage
3. Implement standardized evaluation protocols across all tested agents to enable more reliable comparative performance analysis