---
ver: rpa2
title: 'TorchResist: Open-Source Differentiable Resist Simulator'
arxiv_id: '2502.06838'
source_url: https://arxiv.org/abs/2502.06838
tags:
- resist
- torchresist
- lithography
- image
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TorchResist, an open-source, differentiable
  photoresist simulator that addresses the limitations of existing lithography simulators'
  resist modeling capabilities. The method employs analytical formulations for photoresist
  processes including exposure and development modeling, with at most twenty interpretable
  parameters calibrated using gradient-based optimization on GPU-accelerated differentiable
  programming frameworks.
---

# TorchResist: Open-Source Differentiable Resist Simulator

## Quick Facts
- arXiv ID: 2502.06838
- Source URL: https://arxiv.org/abs/2502.06838
- Reference count: 33
- Key outcome: Physics-based photoresist simulator achieving 0.22% pixel difference and 0.73nm EPE-mean on LithoBench dataset

## Executive Summary
TorchResist is an open-source, differentiable photoresist simulator that addresses the limitations of existing lithography simulators' resist modeling capabilities. The method employs analytical formulations for photoresist processes including exposure and development modeling, with at most twenty interpretable parameters calibrated using gradient-based optimization on GPU-accelerated differentiable programming frameworks. Experiments on the LithoBench dataset demonstrate superior performance with 0.22% pixel difference and 0.73nm edge placement error mean compared to baseline threshold-based methods (0.59% pixel difference and 1.52nm EPE-mean). The simulator achieves efficient inference at 0.04 seconds per 2µm × 2µm patch at 7nm resolution and shows scale robustness with only 0.17% pixel difference between 1nm and 7nm resolutions.

## Method Summary
TorchResist uses physics-grounded analytical equations to model photoresist exposure and development processes, with parameters optimized via gradient descent. The simulator replaces non-differentiable operations (thresholding, L0-norm) with smooth approximations (sigmoid, binary cross entropy) to enable gradient flow. Key parameters include Dill's "ABC" coefficients for exposure modeling and development rate parameters. The model is calibrated on the LithoBench dataset using Adam optimizer with learning rate decay. The analytical formulation serves as a strong regularizer, allowing effective training with limited parameters while maintaining physical interpretability.

## Key Results
- Superior performance: 0.22% pixel difference and 0.73nm EPE-mean on LithoBench test set
- Efficient inference: 0.04 seconds per 2µm × 2µm patch at 7nm resolution
- Scale robustness: Only 0.17% pixel difference between 1nm and 7nm resolutions
- Interpretable parameters: At most twenty parameters with physical meaning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Analytical formulation acts as a strong regularizer for low-data regimes.
- **Mechanism:** By restricting the hypothesis space to physics-grounded partial differential equations (PDEs) rather than arbitrary neural network weights, the model converges on the "correct" physical behavior using only ~20 parameters. This prevents overfitting compared to black-box network-based methods.
- **Core assumption:** The underlying photoresist chemistry (Lambert-Beer law, inhibitor depletion) is sufficiently captured by the provided analytical equations, and deviations are due to parameter shifts rather than structural model errors.

### Mechanism 2
- **Claim:** Differentiable relaxation enables gradient-based optimization of discrete physical processes.
- **Mechanism:** The simulator bridges the gap between continuous physics and binary lithography results by replacing non-differentiable steps (thresholding, L0 norm) with smooth approximations (Sigmoid, Binary Cross Entropy). This allows gradients to flow from the final binary image back through the physical process parameters (A, B, C).
- **Core assumption:** The landscape of the relaxed loss function is sufficiently convex to guide parameters to a global optimum that generalizes.

### Mechanism 3
- **Claim:** Resolution robustness is derived from physics-scaling rather than pixel-scaling.
- **Mechanism:** Because the model solves for physical depth (h) and concentration (M) rather than learning convolutional kernels on pixels, the internal state representation is resolution-agnostic. The same parameters apply to the physical equations regardless of grid discretization.
- **Core assumption:** The numerical integration remains stable and accurate across different grid densities (1nm vs 7nm).

## Foundational Learning

- **Concept:** Photoresist Chemistry Basics (Dill's "ABC" Parameters)
  - **Why needed here:** The exposure model relies on the Lambert-Beer law and inhibitor concentration decay. Understanding how light intensity I relates to inhibitor M is required to interpret the calibration parameters A, B, C.
  - **Quick check question:** If the absorption coefficient A increases, does the light penetrate deeper or shallower into the resist?

- **Concept:** Auto-differentiation (Autograd)
  - **Why needed here:** The core contribution is calibrating physical parameters via gradient descent. You must understand how PyTorch tracks operations (even complex integrals/PDEs) to compute ∂Loss / ∂θ.
  - **Quick check question:** Why is the substitution of the L0 norm with Binary Cross Entropy necessary for backpropagation in this specific architecture?

- **Concept:** Level-Set / Fast-Marching Methods
  - **Why needed here:** The paper mentions calculating the development front T(z, x, y). While a simplified integral is used, understanding the physical intuition of an "etime" field (time to etch a point) is crucial for debugging the 3D depth output.
  - **Quick check question:** In the simplified model, what physical assumption is made about the path of the developer?

## Architecture Onboarding

- **Component map:** Aerial Image -> Exposure Engine -> Development Engine -> Sigmoid + Threshold -> Binary Output
- **Critical path:** The interaction between the Exposure Engine (generating the M field) and the Development Engine (consuming M to determine dissolution rate). Errors in the M calculation will propagate non-linearly to the edge placement.
- **Design tradeoffs:**
  - Vertical vs. Full 3D Development: The code uses a simplified vertical path rather than a full Fast-Marching Method. This is faster but may lose accuracy on non-vertical sidewalls or undercut profiles.
  - Resolution vs. Speed: Table 2 shows a 49x slowdown moving from 7nm to 1nm resolution.
- **Failure signatures:**
  - "Washed out" images: If threshold τ or scale s are poorly calibrated, binary output may be all white/all black.
  - High EPE-max: Indicates the analytical model is struggling with specific corner cases (e.g., dense lines) where the simplified physics fails.
  - Divergence: If learning rate is too high during calibration, physical parameters (e.g., concentrations) may go negative, breaking the math.
- **First 3 experiments:**
  1. Sanity Check: Run inference on a single patch from LithoBench with default paper parameters (A=0, α=6.186). Verify that the output is a coherent pattern and not noise.
  2. Calibration Loop: Implement the BCE loss. Overfit the model on a single tile (batch size 1) to ensure the gradient descent mechanism can perfectly reproduce that one tile.
  3. Resolution Stress Test: Run the same calibrated model at 7nm and 1nm resolution. Verify the 0.17% pixel difference claim to ensure the physics implementation is resolution-independent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reliance on the simplified vertical development path approximation limit the accuracy of 3D resist profile predictions (e.g., sidewall angles) compared to Fast-Marching Level-Set methods?
- Basis in paper: [inferred] The paper presents Equation (17) as a "simplified model where we only consider the vertical development path" while noting Fast-Marching methods are more general; it does not explicitly confirm which solver is differentiated in the final "3D depth simulation."
- Why unresolved: Differentiating through the Fast-Marching Method is computationally complex, suggesting the simplified integration is likely the default, potentially sacrificing physical fidelity for sidewall geometry.
- What evidence would resolve it: A comparative analysis of 3D resist profiles generated by TorchResist versus a ground-truth SEM cross-section or a non-differentiable Fast-Marching solver.

### Open Question 2
- Question: How does the deterministic analytical formulation account for stochastic variations, such as Line Edge Roughness (LER), which are critical at advanced nodes?
- Basis in paper: [inferred] The paper models the resist process using strictly deterministic analytical equations and optimizes for mean error (EPE), but does not address modeling the stochastic behavior of acid diffusion or dissolution mentioned in the context of nanometer-scale challenges.
- Why unresolved: The current white-box model lacks a probabilistic mechanism to simulate the variance (roughness) inherent in real chemical processes, limiting its utility for analyzing yield limits.
- What evidence would resolve it: Extension of the model to output LER statistics or Power Spectral Density (PSD) plots that correlate with experimental wafer data.

### Open Question 3
- Question: What is the efficacy of TorchResist in gradient-based inverse design tasks (e.g., Source Mask Optimization) compared to its forward prediction performance?
- Basis in paper: [explicit] The abstract claims TorchResist "enables seamless co-optimization," but the experimental section strictly evaluates forward inference accuracy using pre-computed aerial images.
- Why unresolved: While the framework is differentiable, the paper provides no quantitative results demonstrating improved convergence or mask quality when TorchResist is used in a closed-loop optimization cycle.
- What evidence would resolve it: Results from an end-to-end SMO or ILT workflow showing mask optimization convergence rates and process window improvements over standard threshold models.

## Limitations
- **Aerial image source unknown**: The commercial lithography simulator used to generate LithoBench is not specified, preventing exact baseline comparisons.
- **PDE solver details omitted**: Critical numerical implementation details (step sizes, discretization schemes) are not provided, affecting numerical stability and accuracy.
- **Initial parameter values not provided**: Starting points for trainable parameters before calibration are missing.

## Confidence
- **High Confidence**: The analytical framework (Dill parameters, development rate models) is physically sound and well-established in lithography literature. The core differentiable architecture (Equations 7-17) is clearly specified and reproducible.
- **Medium Confidence**: Performance metrics (0.22% pixel difference, 0.73nm EPE-mean) are specific but depend on unknown commercial simulator settings. The resolution robustness claim (0.17% pixel difference) is well-supported but requires careful numerical implementation.
- **Low Confidence**: Scale robustness mechanisms are mentioned but not deeply validated. The 49x slowdown from 7nm to 1nm resolution is significant but the practical implications for real-world use cases are unclear.

## Next Checks
1. **Numerical Stability Verification**: Implement the PDE solver with varying depth/time step sizes. Verify that results converge and remain stable across different discretizations, confirming the resolution robustness claim.
2. **Parameter Sensitivity Analysis**: Systematically vary the Dill parameters (A, B, C) and development rate parameters (r_max, r_min, m_TH) around the calibrated values. Quantify how performance degrades to identify which parameters are most critical for edge placement accuracy.
3. **Cross-Simulator Validation**: Generate aerial images using open-source tools (FUILT or ICCAD13) and calibrate TorchResist on these. Compare performance to the commercial-tool baseline to assess the impact of aerial image source differences on final metrics.