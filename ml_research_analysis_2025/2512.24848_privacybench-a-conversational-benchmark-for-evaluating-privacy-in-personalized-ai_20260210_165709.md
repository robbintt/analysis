---
ver: rpa2
title: 'PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized
  AI'
arxiv_id: '2512.24848'
source_url: https://arxiv.org/abs/2512.24848
tags:
- user
- privacy
- secrets
- evaluation
- secret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PrivacyBench, the first benchmark designed
  to evaluate privacy preservation in personalized AI assistants. It features socially
  grounded datasets with embedded secrets and a multi-turn conversational evaluation
  framework to measure secret leakage.
---

# PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI

## Quick Facts
- **arXiv ID:** 2512.24848
- **Source URL:** https://arxiv.org/abs/2512.24848
- **Reference count:** 40
- **Key outcome:** Privacy-aware system prompts reduce secret leakage from 26.56% to 5.12%, but RAG retrievers remain an indiscriminate exposure point.

## Executive Summary
This paper introduces PrivacyBench, the first benchmark designed to evaluate privacy preservation in personalized AI assistants. It features socially grounded datasets with embedded secrets and a multi-turn conversational evaluation framework to measure secret leakage. Testing five state-of-the-art RAG-based assistants revealed that without explicit safeguards, they leaked secrets in up to 26.56% of interactions. Adding a privacy-aware system prompt significantly reduced leakage to 5.12%, though the retrieval mechanism continued to access sensitive data indiscriminately, creating a single point of failure. The findings highlight the urgent need for structural, privacy-by-design safeguards to ensure ethical and inclusive AI deployment.

## Method Summary
The study uses a synthetic dataset with 4 communities, 48 users, and 31,972 documents containing 215 secrets. A RAG architecture with ChromaDB and all-MiniLM-L6-v2 embeddings indexes user documents. Five models (GPT-5-Nano, Gemini-2.5-Flash, Kimi-K2, Llama-4-Maverick, Qwen3-30B) are evaluated using multi-turn probing with direct and indirect strategies. An LLM prober (Gemini-2.5-Flash) conducts conversations up to 10 rounds, while a judge panel (GLM-4-32B, Phi-4, Mistral-Nemo) scores leakage, over-secrecy, and persona consistency using majority voting.

## Key Results
- Without privacy safeguards, RAG-based assistants leaked secrets in up to 26.56% of interactions
- Privacy-aware system prompts reduced leakage to 5.12% but left retrieval as an indiscriminate exposure point
- Indirect probing exposed secrets at similar rates to direct interrogation (~15.28% vs 16.31%), indicating architectural vulnerability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG retrievers surface secret documents based on semantic similarity without regard for social access norms, creating systematic privacy exposure.
- Mechanism: The retrieval component uses embedding-based similarity search over a unified knowledge base containing all user documents. When a conversation topic is semantically related to a secret, the retriever fetches it regardless of whether the current conversational partner should have access.
- Core assumption: Semantic proximity correlates with conversational relevance, but this ignores social context boundaries defined in Contextual Integrity Theory.
- Evidence anchors:
  - [abstract] "The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure."
  - [Section 7.1] "Retrievers fail to differentiate between public and socially restricted content, treating them as uniformly retrievable sources."

### Mechanism 2
- Claim: Privacy-aware system prompts instruct the generator to withhold secrets, reducing leakage from 15.80% to 5.12% on average.
- Mechanism: The system prompt explicitly instructs the model to safeguard user secrets. When retrieved context contains a secret, the generator recognizes the conflict between the retrieved content and its instructions, choosing to deflect or refuse disclosure.
- Core assumption: The generator can reliably detect what counts as a "secret" and correctly infer who should have access—capabilities that vary by model.
- Evidence anchors:
  - [abstract] "A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation."
  - [Section 6.2] "Llama-4-Maverick's Leakage Rate fell from 18.72% to 0.46%... Kimi-K2 model, whose Leakage Rate for direct probes increased from 14.58% to 18.13%."

### Mechanism 3
- Claim: Multi-turn indirect probing exposes secrets at similar rates to direct interrogation (~15.28% vs 16.31%), revealing architectural—not prompt-level—vulnerability.
- Mechanism: Indirect probes steer conversation toward semantically adjacent topics. The retriever fetches secret documents because of topic overlap, and the generator, lacking explicit social-context modeling, incorporates the retrieved content naturally into its response.
- Core assumption: Privacy failures are incidental consequences of the architecture's inability to model the social intent behind information retrieval.
- Evidence anchors:
  - [Section 7.2] "This finding suggests that the exposure of secrets is often incidental rather than provoked... innocent conversational drifts could accidentally expose sensitive data."
  - [Section 6.4] "While direct interrogation yielded an average leakage rate of 16.31%, the subtle, indirect probes resulted in a nearly identical rate of 15.28%."

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) Architecture**
  - Why needed here: The entire privacy failure mode stems from how RAG retrieves and injects user documents into context.
  - Quick check question: Can you explain why a RAG system might retrieve a private email when a user asks their assistant to draft a professional message to their manager?

- **Contextual Integrity Theory (Nissenbaum, 2009)**
  - Why needed here: The paper frames privacy not as secrecy but as adherence to context-specific information flow norms.
  - Quick check question: If a user tells their spouse about a medical diagnosis, why might sharing that same information with a coworker violate Contextual Integrity even though the information itself hasn't changed?

- **Multi-Turn Dialogue State and Context Accumulation**
  - Why needed here: Privacy erosion happens across conversational turns, not in single queries.
  - Quick check question: How might a 10-turn conversation gradually surface information that would have been withheld in a single direct question?

## Architecture Onboarding

- **Component map:** Knowledge Base -> Retriever -> Generator -> Evaluator Framework
- **Critical path:** Prober initiates conversation with a goal → Retriever surfaces documents semantically related to the conversation → Generator synthesizes response → Judge panel evaluates the final response against ground-truth secret access rules.
- **Design tradeoffs:**
  - Utility vs. Privacy: Aggressive retrieval improves personalization quality but increases IRR. Privacy prompts can improve utility by reducing over-secrecy, but not reliably across all models.
  - Prompt-based vs. Structural Defenses: Privacy prompts are lightweight but leave retriever as an unguarded exposure point. Structural fixes require document-level metadata and more complex retrieval logic.
  - Synthetic vs. Real Data: Synthetic data is ethically necessary for privacy benchmarking but may not capture all real-world social complexity.
- **Failure signatures:**
  - High IRR (~60-74%) with Low LR (~5-15%): Generator is successfully refusing most retrieved secrets, but retriever is still exposing them.
  - Model-dependent prompt response: Kimi-K2 leakage increased under privacy prompting, indicating instruction-following failures can invert defenses.
  - Indirect ≈ Direct leakage rates: Vulnerability is architectural, not specific to attack phrasing.
- **First 3 experiments:**
  1. **Baseline Retrieval Audit:** Run PrivacyBench's evaluation framework on your RAG assistant with a neutral system prompt. Measure IRR and LR.
  2. **Privacy Prompt Stress Test:** Add an explicit privacy-aware system prompt and re-run evaluation. Compare LR reduction across different models.
  3. **Retrieval Access Control Prototype:** Tag a subset of documents with confidentiality metadata. Modify the retriever to filter documents based on inferred conversational audience. Re-run evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do advanced agentic memory modules (e.g., MemGPT) or native vector database access controls affect secret leakage rates compared to standard RAG architectures?
- Basis in paper: [explicit] The authors state in Section 8 that exploring how these advanced architectures affect leakage rates "remains an open and critical research direction."
- Why unresolved: The current study only evaluates standard RAG-based assistants, leaving the privacy implications of more complex memory systems untested.
- What evidence would resolve it: Benchmarking results from PrivacyBench applied to agents utilizing MemGPT or similar advanced memory architectures.

### Open Question 2
- Question: Can evaluation frameworks effectively quantify "partial leakages" or subtle hints that imply a secret without stating it verbatim?
- Basis in paper: [explicit] Section 8 notes the primary evaluation focuses on explicit textual leakage and acknowledges that measuring partial leakages or subtle hints "warrants more granular privacy metrics."
- Why unresolved: The current "Leakage Rate" metric strictly defines a leak as the complete revelation of secret components, failing to capture nuance.
- What evidence would resolve it: The development and validation of a metric that successfully detects and scores semantic implications or partial disclosures of secrets.

### Open Question 3
- Question: Can structural safeguards like access control modules that filter sensitive data *before* generation eliminate the "single point of failure" inherent in generator-level privacy prompts?
- Basis in paper: [explicit] Section 9 concludes that future work must develop structural safeguards like access control modules to filter data before generation, rather than relying on the generator.
- Why unresolved: Current privacy-aware prompts successfully reduced leakage but failed to lower the Inappropriate Retrieval Rate (IRR), leaving the generator as a single point of failure.
- What evidence would resolve it: A system architecture where the retrieval mechanism incorporates social metadata to achieve a near-zero Inappropriate Retrieval Rate.

## Limitations
- The synthetic benchmark may not fully capture real-world social complexity and edge cases in human relationships.
- Model-specific vulnerabilities (e.g., Kimi-K2 increasing leakage under privacy prompting) may not generalize across different model families or future versions.
- Weak corpus evidence for privacy-aware retrieval mechanisms suggests this remains largely an open research problem.

## Confidence
- **High Confidence:** The core finding that RAG retrievers indiscriminately access sensitive data regardless of social access norms, creating a single point of failure at the generator level.
- **Medium Confidence:** The assertion that indirect probing exposes secrets at similar rates to direct interrogation, suggesting architectural rather than prompt-level vulnerability.
- **Low Confidence:** The generalizability of specific leakage rates (e.g., 26.56% baseline, 5.12% with privacy prompt) across different real-world scenarios and user populations.

## Next Checks
1. **Model Family Generalization Test:** Replicate the PrivacyBench evaluation across at least three additional model families not tested in the original study to determine if the observed patterns are consistent or model-specific.

2. **Real-World Data Pilot:** Conduct a small-scale study using de-identified real user data with explicit consent to validate whether the synthetic benchmark's leakage patterns accurately reflect real-world privacy risks in personalized AI assistants.

3. **Hybrid Defense Evaluation:** Implement a prototype retrieval system that incorporates basic social metadata (document audience, relationship type) and evaluate whether this structural approach outperforms prompt-based defenses alone in reducing IRR while maintaining acceptable utility levels.