---
ver: rpa2
title: '"I am here for you": How relational conversational AI appeals to adolescents,
  especially those who are socially and emotionally vulnerable'
arxiv_id: '2512.15117'
source_url: https://arxiv.org/abs/2512.15117
tags:
- style
- chatbot
- adolescents
- relational
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This preregistered study examined how conversational style in\
  \ AI chatbots affects adolescent perceptions and preferences. Using a controlled\
  \ experiment with 284 adolescent-parent dyads, youth read two matched transcripts\u2014\
  one using a relational style (first-person, affiliative, commitment language) and\
  \ one using a transparent style (explicit nonhumanness, informational tone)\u2014\
  about an everyday social problem."
---

# "I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable

## Quick Facts
- arXiv ID: 2512.15117
- Source URL: https://arxiv.org/abs/2512.15117
- Authors: Pilyoung Kim; Yun Xie; Sujin Yang
- Reference count: 0
- Key outcome: Relational chatbot style increases adolescent anthropomorphism, trust, and emotional closeness compared to transparent style, especially among those with lower relationship quality and higher distress

## Executive Summary
This preregistered study examined how conversational style in AI chatbots affects adolescent perceptions and preferences. Using a controlled experiment with 284 adolescent-parent dyads, youth read two matched transcripts—one using a relational style (first-person, affiliative, commitment language) and one using a transparent style (explicit nonhumanness, informational tone)—about an everyday social problem. Adolescents rated the relational chatbot as more human-like, likable, trustworthy, and emotionally close than the transparent chatbot, while perceiving both as similarly helpful. Parents preferred the transparent style more than adolescents did. Adolescents with lower family and peer relationship quality and higher stress and anxiety were more likely to prefer the relational style. These findings highlight conversational style as a key design lever for youth AI safety, showing that relational framing increases anthropomorphism, trust, and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents.

## Method Summary
The study employed a controlled experiment with 284 adolescent-parent dyads (youth aged 11-15) who read two matched 10-exchange chat transcripts about a peer problem scenario. One transcript used a relational style with first-person pronouns, affective validation, and commitment language; the other used a transparent style with explicit nonhumanness disclaimers and informational tone. Participants rated perceptions of anthropomorphism, likability, trust, emotional closeness, and helpfulness. Secondary measures included PROMIS pediatric T-scores for family/peer relationships, stress, anxiety, depression, and social isolation. The design used within-subjects presentation with randomized order and controlled for age, gender, and parent education.

## Key Results
- Relational chatbot was rated more human-like (4.03 vs 3.30), likable (4.39 vs 3.78), trustworthy (3.95 vs 3.64), and emotionally close (3.62 vs 2.83) than transparent chatbot (all p<.05)
- Both styles were perceived as similarly helpful (4.29 vs 3.88, p=.082)
- Adolescents with lower family/peer relationship quality and higher stress/anxiety were more likely to prefer relational style
- Parents preferred transparent style more than adolescents did

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relational conversational cues increase anthropomorphism, which in turn enhances trust, likability, and emotional closeness
- Mechanism: First-person voice ("I am here for you"), affective validation, and commitment language signal social presence, triggering users' human social schemas. When agents use contingent, human-like language, people attribute intentions and experience the interaction as social.
- Core assumption: Anthropomorphism operates as a mediating psychological process connecting conversational style to relational outcomes.
- Evidence anchors:
  - [abstract] "relational framing heightens anthropomorphism, trust and emotional closeness"
  - [section] Page 15: Adolescents rated relational chatbot higher on anthropomorphism (4.03 vs 3.30, p=.025), likability (4.39 vs 3.78, p<.001), trust (3.95 vs 3.64, p=.015), and emotional closeness (3.62 vs 2.83, p<.001)
  - [corpus] "The Illusion of Friendship" paper (FMR=0.60) discusses how natural language fluency blurs tool-companion boundaries—consistent with anthropomorphism as mechanism
- Break condition: If anthropomorphism mediates the relationship, controlling for it should attenuate style effects on trust/closeness (not tested in this study)

### Mechanism 2
- Claim: Socially and emotionally vulnerable adolescents prefer relational style due to social compensation motivations
- Mechanism: When social needs are unmet (low relationship quality, high distress), users project social agency onto responsive agents to fulfill connection desires. This social motivation mechanism makes relational cues especially compelling for those experiencing social deprivation.
- Core assumption: Preference reflects underlying motivational state, not just stimulus properties.
- Evidence anchors:
  - [abstract] "Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety"
  - [section] Page 17-18: Adolescents preferring relational style reported lower family relationship quality (46.58 vs 51.89, p=.009), lower peer quality, higher stress (55.81 vs 51.64, p=.032), and higher anxiety (51.31 vs 44.88, p=.004)
  - [corpus] Weak direct corpus support for social compensation mechanism specifically; related papers focus on engagement/emotion rather than individual differences in vulnerability
- Break condition: If compensation drives preference, improving offline social support should reduce preference for relational AI (longitudinal test needed)

### Mechanism 3
- Claim: Transparency framing reduces anthropomorphism while preserving perceived helpfulness
- Mechanism: Explicit boundary statements ("As an AI, I do not have genuine emotions") combined with informational tone reduce mind attribution cues without removing supportive content, creating "kind but not human" framing.
- Core assumption: Users process transparency statements as epistemic cues that modulate (but don't eliminate) social responses.
- Evidence anchors:
  - [abstract] "perceiving both styles as similarly helpful"
  - [section] Page 15: Helpfulness ratings did not differ significantly between styles (4.29 vs 3.88, p=.082); transparent style reduced anthropomorphism while maintaining comparable problem-solving guidance
  - [corpus] "Minion" paper (FMR=0.53) explores how users negotiate value conflicts with AI companions—suggests transparency alone may not resolve all relational tensions
- Break condition: If transparency statements become habitual/ignored over repeated exposure, effects may attenuate (not tested; study used single exposure)

## Foundational Learning

- Concept: **Anthropomorphism (three-factor theory)**
  - Why needed here: Core psychological construct explaining why language style shapes user perception; distinguishes social motivation from other drivers
  - Quick check question: If a user with high loneliness rates a chatbot as "understanding me," is this primarily anthropomorphism or accurate perception of system capability?

- Concept: **Social compensation vs. displacement hypotheses**
  - Why needed here: Competing predictions about whether vulnerable users benefit from or are harmed by relational AI; critical for safety design
  - Quick check question: An anxious adolescent prefers relational AI and reports feeling less lonely after use. Is this evidence for compensation or displacement?

- Concept: **Transparency as design lever**
  - Why needed here: Practical intervention point for modulating anthropomorphism without removing functionality
  - Quick check question: If transparency statements reduce trust, is this a feature (safety) or bug (user experience)?

## Architecture Onboarding

- Component map: Conversational style features (first-person pronouns, emotional validation phrases, commitment language, transparency disclaimers) -> User anthropomorphism -> Trust/closeness/likability outcomes
- Critical path:
  1. Identify target user vulnerability profile (relationship quality, distress measures)
  2. Calibrate relational vs. transparent language balance
  3. Monitor anthropomorphism signals as leading indicator
  4. Track emotional reliance behaviors over time
- Design tradeoffs:
  - Higher relational cues -> Higher engagement AND higher overreliance risk
  - Transparency statements -> Lower anthropomorphism AND possibly lower perceived warmth
  - Helpful content held constant, but perception of helpfulness may diverge at extremes
- Failure signatures:
  - Vulnerable users showing strong preference for relational style + increasing use frequency + decreasing human social contact (untested warning pattern)
  - Transparency statements ignored or forgotten in multi-turn conversations
  - Parent-adolescent preference mismatch creating governance conflicts
- First 3 experiments:
  1. Longitudinal deployment: Track whether relational vs. transparent preference at baseline predicts emotional reliance outcomes at 3-month follow-up
  2. Mediation test: Measure anthropomorphism explicitly; test whether it mediates style -> trust/closeness relationship
  3. Moderation by vulnerability: Pre-register hypotheses about which distress measures (anxiety, depression, loneliness) most strongly moderate relational preference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do preferences for relational conversational style prospectively predict emotional overreliance, offline relationship quality, and mental health outcomes over time?
- Basis in paper: [explicit] "Future longitudinal work should therefore examine whether preferences for relational versus transparent styles prospectively predict chatbot use patterns, emotional overreliance, offline relationship quality, and mental health."
- Why unresolved: The cross-sectional design cannot establish whether distress and lower relationship quality lead adolescents to prefer relational chatbots, or whether engagement with relational chatbots exacerbates social withdrawal and distress, or whether both are driven by third variables.
- What evidence would resolve it: Longitudinal studies tracking adolescents' chatbot preferences, use patterns, and mental health outcomes over multiple time points to establish temporal precedence and causal pathways.

### Open Question 2
- Question: Which specific conversational features (e.g., first-person pronouns, commitment phrases, memory cues) most strongly drive anthropomorphism and emotional closeness?
- Basis in paper: [explicit] "Future studies could systematically manipulate narrower elements, such as first-person pronouns, memory cues or specific commitment phrases, or compare multiple style conditions that vary in intensity of relational cues to identify which features most strongly influence anthropomorphism and closeness."
- Why unresolved: The current study compared two bundled conversational styles; relational and transparent cues are intertwined with tone, and fully isolating individual components was not possible within this design.
- What evidence would resolve it: Factorial experiments systematically varying individual conversational elements across multiple style conditions to identify causal contributions of each feature.

### Open Question 3
- Question: How do adolescents' perceptions and preferences differ between scenario-based judgments and live, repeated interactions with actual chatbots?
- Basis in paper: [explicit] "Longitudinal and experimental studies that involve repeated, naturalistic conversations with relational and transparent chatbots are needed to examine trajectories of anthropomorphism, attachment, and emotional outcomes."
- Why unresolved: The study relied on scenario-based judgments (reading transcripts) rather than live interactions, which cannot fully capture dynamics of real-time, multi-turn interactions or how relationships with chatbots evolve over time.
- What evidence would resolve it: Experimental studies with adolescents engaging in actual repeated conversations with chatbots, measuring trajectories of anthropomorphism, attachment, and emotional outcomes across sessions.

### Open Question 4
- Question: How do cultural norms and contexts shape adolescents' preferences for relational versus transparent conversational styles?
- Basis in paper: [explicit] "Norms around AI, anthropomorphism and transparency also vary across cultural contexts, and cross-cultural studies are needed to understand how adolescents in other countries perceive relational versus transparent chatbots."
- Why unresolved: The study was conducted exclusively in the United States with a specific online sample; cultural variation in anthropomorphism and transparency preferences remains unexplored.
- What evidence would resolve it: Cross-cultural replication studies using matched designs across diverse countries and cultural contexts, with attention to local norms around AI and human-machine boundaries.

## Limitations

- Single-exposure transcripts cannot capture how relational preferences and effects evolve with repeated interactions or changing contexts
- Parent-proxy relationship quality measures may not accurately reflect adolescents' lived experiences
- Sample's demographic homogeneity (predominantly high-education parents) limits generalizability

## Confidence

- **High confidence:** Relational style significantly increases anthropomorphism and perceived human-likeness; adolescents consistently prefer relational over transparent style; vulnerability markers predict relational preference
- **Medium confidence:** Relational framing increases trust and emotional closeness while maintaining helpfulness; social compensation mechanism explains vulnerability-adolescent preference link
- **Low confidence:** Transparency statements effectively modulate anthropomorphism without reducing perceived warmth; parent-adolescent preference mismatches reflect meaningful developmental differences

## Next Checks

1. **Longitudinal validation:** Deploy both conversational styles in a real-world chatbot for 3 months; track whether baseline relational preference predicts changes in emotional reliance, social isolation, or human interaction frequency
2. **Mediation pathway test:** Administer explicit anthropomorphism measures (e.g., anthropomorphic mind attribution scales) to confirm whether this construct mediates the relationship between conversational style and trust/closeness outcomes
3. **Contextual boundary test:** Replicate the experiment with different problem scenarios (academic failure, family conflict, peer rejection) to determine whether relational preference strength varies by emotional salience and vulnerability type