---
ver: rpa2
title: Unbiased Online Curvature Approximation for Regularized Graph Continual Learning
arxiv_id: '2509.12727'
source_url: https://arxiv.org/abs/2509.12727
tags:
- learning
- tasks
- task
- graph
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  graph continual learning, particularly in the challenging replay-free, class-incremental
  setting. The authors establish a general regularization framework based on Fisher
  information matrix (FIM) geometry and show that Elastic Weight Consolidation (EWC)
  is a special case using diagonal FIM approximation.
---

# Unbiased Online Curvature Approximation for Regularized Graph Continual Learning

## Quick Facts
- **arXiv ID**: 2509.12727
- **Source URL**: https://arxiv.org/abs/2509.12727
- **Reference count**: 10
- **Primary result**: Novel unbiased online curvature approximation of the Fisher Information Matrix enables superior performance in replay-free class-incremental graph continual learning.

## Executive Summary
This paper addresses catastrophic forgetting in graph continual learning, specifically in the challenging replay-free, class-incremental setting. The authors establish a general regularization framework based on Fisher information matrix (FIM) geometry and show that Elastic Weight Consolidation (EWC) is a special case using diagonal FIM approximation. They propose a novel unbiased online curvature approximation of the full FIM based on the model's current learning state, enabling direct estimation of the regularization term without explicitly evaluating or storing the FIM itself. This method captures the loss landscape more accurately during new task learning while retaining knowledge from previous tasks.

## Method Summary
The proposed method maintains a gradient queue and uses a secondary backward pass to sample "imaginary" labels from the model's current prediction distribution. For each node in a batch, it samples a label from the softmax output, computes the gradient of the log-likelihood w.r.t. parameters, and stores this in a FIFO queue. The regularization loss is computed using these cached gradients and stored parameters from the end of the previous task. After each task, an exponential moving average (EMA) is applied to parameters to get new reference parameters for the next task. The method uses a 2-layer GCN backbone with Adam optimizer (lr=1e-5, weight decay=5e-4) and batch size 128.

## Key Results
- Achieves significant improvements over existing regularization-based methods on CoraFull, Arxiv, and Coauthor-CS datasets
- Demonstrates superior performance in both average performance (AP) and average forgetting (AF) metrics
- Shows particular strength in balancing stability and plasticity in the class-incremental setting
- Operates effectively at much lower regularization strength (λ=0.1) compared to EWC (λ=10⁴)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evaluating the Fisher Information Matrix (FIM) at the current parameter state (θ) provides a higher-rank, more informative geometry for regularization than evaluating it at old parameters (θt).
- **Mechanism:** The method leverages the observation that as a model learns more classes, the rank of the FIM increases monotonically. By calculating curvature at the current parameters (which lie "inside" the probability simplex of all seen classes) rather than old parameters (which lie on the edge of a sub-simplex), the regularizer captures parameter interactions relevant to both old and new knowledge.
- **Core assumption:** The loss landscape curvature at the current solution is geometrically representative of the distance to previous solutions, preserving information about parameter correlations that diagonal approximations miss.
- **Evidence anchors:**
  - [abstract] "unbiased online curvature approximation... based on the model's current learning state"
  - [section: Motivation] "rank(I(θ)) is monotonic... choosing θ̄=θ is more reasonable... I(θ) has higher rank"
  - [corpus] "Gradient Regularized Natural Gradients" (related to curvature in optimization)

### Mechanism 2
- **Claim:** An unbiased estimate of the FIM-based regularizer can be computed online without storing the matrix, by sampling "imaginary" labels.
- **Mechanism:** Instead of storing a large matrix I(θ), the method samples a label ŷv from the model's current prediction distribution. It computes the gradient of the log-likelihood w.r.t. this sample. The squared projection of the parameter shift (θ - θt) onto this gradient serves as an unbiased stochastic regularizer.
- **Core assumption:** The sampled gradients effectively approximate the spectral properties of the full FIM when averaged over a batch and cached history.
- **Evidence anchors:**
  - [abstract] "directly estimates the regularization term in an online manner without explicitly evaluating and storing the FIM"
  - [section: Our Proposed Method] "R(θ) is an unbiased estimate... random matrix gives an unbiased estimate of the FIM"
  - [corpus] "Online Curvature-Aware Replay" (leveraging 2nd order info in online settings)

### Mechanism 3
- **Claim:** The gradient update effectively functions as a weighted augmentation using "imaginary" samples to enforce stability.
- **Mechanism:** The regularization gradient creates a linear combination of gradients from "imaginary" nodes. If the gradient of an imaginary sample aligns with the direction back to the old parameters (θt), it is weighted positively (pulling the model back). If it points away, it is weighted negatively (pushing the model to stay put).
- **Core assumption:** The model's predicted probability distribution for a node contains sufficient signal to identify parameter directions critical for retaining old knowledge.
- **Evidence anchors:**
  - [section: Our Proposed Method] "regularization essentially augments the current batch of nodes with imaginary labels"
  - [section: Our Proposed Method] "Learning discourages going away from the previous parameter value"
  - [corpus] Weak direct evidence in provided corpus for this specific "imaginary sample" interpretation.

## Foundational Learning

- **Concept: Fisher Information Matrix (FIM)**
  - **Why needed here:** This is the mathematical core of the paper. The authors frame continual learning as a geometric problem in a curved parameter space defined by the FIM. Understanding that FIM represents "curvature" or "parameter importance" is required to grasp why diagonal approximations (EWC) are insufficient.
  - **Quick check question:** Does a diagonal FIM approximation capture the correlation between two connected weights in a layer?

- **Concept: Class-Incremental Learning (CIL)**
  - **Why needed here:** The paper specifically targets this setting (new classes per task, no task ID at inference). This is harder than task-incremental learning because the model must disambiguate classes it learned at different times without explicit cues.
  - **Quick check question:** In a class-incremental setting, does the classifier head expand or stay fixed if the total number of classes is unknown? (Context: This paper assumes a fixed capacity or growing set of classes Y).

- **Concept: Unbiased Estimation in Stochastic Optimization**
  - **Why needed here:** The authors claim their regularizer is "unbiased." This implies that while individual samples might be noisy, the expected value of their update equals the true desired regularizer, allowing standard SGD to converge.
  - **Quick check question:** If a gradient estimator is biased, where does it accumulate error over time?

## Architecture Onboarding

- **Component map:**
  - Backbone -> Gradient Cache (Queue Q) -> Parameter Store -> Regularizer Head
  - Input -> 2-layer GCN -> Classification Loss + Unbiased FIM Regularizer -> Optimizer

- **Critical path:**
  1. Forward Pass: Compute classification loss ℓv
  2. Sampling: For a node v, sample an "imaginary" label ŷv from the current softmax output
  3. Backward Pass 2: Compute gradient of log-likelihood of ŷv w.r.t parameters (∂Λ̂v/∂θ)
  4. Queue Update: Push new gradient, pop oldest if |Q| > M
  5. Loss Aggregation: Combine classification loss + regularizer (using cached gradients and stored θt)
  6. Update: Step optimizer
  7. Task End: Apply Exponential Moving Average (EMA) to parameters to get new θt

- **Design tradeoffs:**
  - Stability vs. Overhead: The method requires a second backward pass for the imaginary sample. The Queue Q mitigates this by allowing gradient reuse, trading off immediate freshness for computational speed.
  - Sensitivity: The paper notes optimal performance at much lower regularization strengths (λ=0.1) compared to EWC (λ=10⁴), suggesting the regularizer is significantly more potent/per-sample impact is higher.

- **Failure signatures:**
  - High Forgetting: If Queue size M is too small, the variance of the curvature approximation may be too high to effectively constrain the parameters.
  - Loss of Plasticity (Intransigence): If regularization strength λ is set too high (using EWC scales), the model will refuse to learn new classes.
  - Implementation Error: Forgetting to detach the computation graph for the cached gradients or incorrectly implementing the "imaginary" label sampling will break the unbiased property.

- **First 3 experiments:**
  1. Sanity Check (Diagonal vs. Full): Compare the proposed method against standard EWC and Online EWC on CoraFull to verify that capturing parameter correlations (non-diagonal) improves Average Performance (AP).
  2. Ablation on Geometry: Disable the "online" aspect (use old θt for FIM eval) vs. the proposed current θ to validate the "Interior Point" claim.
  3. Hyperparameter Sensitivity: Run a sweep on λ (e.g., [0.01, 0.1, 0.5]) to confirm the method operates effectively in a different range than baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed unbiased curvature approximation be effectively combined with replay-based or architecture-based methods to further enhance performance?
- **Basis in paper:** [explicit] The conclusion states, "Future work will investigate how our regularization method could be integrated with GCL methods from other categories."
- **Why unresolved:** The current work isolates the method to the regularization category (replay-free, fixed capacity) to establish a baseline against other regularization techniques like EWC.
- **What evidence would resolve it:** Experiments demonstrating that adding this regularizer to replay-based methods (e.g., ERGNN) yields statistically significant improvements in Average Performance (AP) and Average Forgetting (AF).

### Open Question 2
- **Question:** What are the theoretical and empirical trade-offs of treating the gradient term as constant to enforce a quadratic regularizer, rather than solving the non-quadratic optimization directly?
- **Basis in paper:** [inferred] The paper notes the regularizer is technically non-quadratic but treats ∂Λ̂v/∂θ as constant "for the ease of implementation."
- **Why unresolved:** The authors do not analyze the approximation error introduced by this linearization or if a higher-order optimization approach would yield better stability-plasticity trade-offs.
- **What evidence would resolve it:** A comparative analysis of convergence speed and forgetting metrics when optimizing the exact non-quadratic loss versus the proposed quadratic approximation.

### Open Question 3
- **Question:** Does the method maintain its superiority over diagonal approximations when applied to attention-based GNN architectures (e.g., GAT) or different graph learning tasks?
- **Basis in paper:** [inferred] The experimental scope is limited to a standard two-layer GCN backbone and node classification tasks.
- **Why unresolved:** The unbiased FIM estimation relies on sampling gradients based on the model's predictive distribution; the efficiency and stability of this sampling might differ in attention mechanisms or link prediction tasks.
- **What evidence would resolve it:** Benchmark results on the CoraFull/Arxiv datasets using Graph Attention Networks (GAT) or GraphSAGE as the backbone architecture.

## Limitations

- **Task configuration unknown**: The specific class-to-task mapping is not provided in the main text, though essential for reproduction.
- **Parameter sensitivity gap**: The paper shows optimal performance at λ=0.1, drastically lower than EWC's λ=10⁴, but doesn't explain the 100x difference in sensitivity.
- **Limited architecture scope**: Experiments are restricted to 2-layer GCN backbones, leaving questions about performance on attention-based GNNs or different graph tasks.

## Confidence

- **High Confidence**: The core mathematical formulation (unbiased estimator of FIM-based regularizer, gradient queue mechanism) and its advantages over diagonal approximations.
- **Medium Confidence**: The experimental results showing superior AP/AF metrics, as they depend on the unspecified task splits and exact implementation of the sampling procedure.
- **Low Confidence**: The interpretation of the "imaginary sample" mechanism as a weighted augmentation, as this interpretation has limited direct evidence in the provided corpus.

## Next Checks

1. **Task Split Validation**: Verify the exact class-to-task mapping used in experiments (e.g., CoraFull classes 0-8 for task 1, 9-17 for task 2, etc.) to ensure faithful reproduction.
2. **Parameter Sensitivity Sweep**: Reproduce the AP/AF curves across a wide range of λ values (e.g., [0.001, 0.01, 0.1, 1, 10]) to confirm the claimed 100x lower sensitivity compared to EWC.
3. **Queue Size Ablation**: Test the method with different queue sizes M (e.g., [32, 64, 128, 256]) to quantify the tradeoff between computational efficiency and approximation accuracy of the curvature estimate.