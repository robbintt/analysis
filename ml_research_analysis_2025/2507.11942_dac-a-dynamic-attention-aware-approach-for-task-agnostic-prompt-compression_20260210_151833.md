---
ver: rpa2
title: 'DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression'
arxiv_id: '2507.11942'
source_url: https://arxiv.org/abs/2507.11942
tags:
- compression
- entropy
- prompt
- information
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DAC, a dynamic attention-aware approach for
  task-agnostic prompt compression that integrates information entropy and attention
  scores to identify and preserve critical tokens while dynamically adjusting for
  entropy shifts during compression. Unlike prior entropy-based methods, DAC addresses
  the limitations of ignoring attention-critical tokens and static entropy assumptions
  by employing a novel metric fusion strategy and iterative compression with dynamic
  rate adjustment.
---

# DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression

## Quick Facts
- **arXiv ID**: 2507.11942
- **Source URL**: https://arxiv.org/abs/2507.11942
- **Reference count**: 9
- **Primary result**: Achieves up to 4.03 F1 points improvement on LongBench compared to state-of-the-art baselines while reducing computational overhead.

## Executive Summary
DAC introduces a novel task-agnostic prompt compression method that dynamically integrates information entropy and attention scores to identify and preserve critical tokens. Unlike prior entropy-based methods, DAC addresses the limitations of ignoring attention-critical tokens and static entropy assumptions through a metric fusion strategy and iterative compression with dynamic rate adjustment. The approach demonstrates consistent performance improvements across LongBench, GSM8K, and BBH benchmarks while maintaining robustness across different LLMs. The method effectively reduces computational overhead while preserving information density, making it suitable for efficient long-context inference.

## Method Summary
DAC employs a weighted additive metric combining information entropy and accumulated attention scores (M_t = (1-α) · I_t(x) + α · s_t, with α=0.8) to identify critical tokens for preservation. The compression proceeds iteratively with dynamic entropy recalculation after each stage, where tokens are removed based on their fused scores while preventing consecutive token compression. The method uses a small language model to compute attention matrices and token probabilities, then feeds compressed prompts to a target LLM for evaluation. Dynamic scheduling determines the number of iterations and compression rate per stage based on input length, with a maximum of 15 iterations.

## Key Results
- Achieves up to 4.03 F1 points improvement on LongBench compared to state-of-the-art baselines
- Demonstrates robust generalization across different LLMs including Qwen2-7B and LLaMA3.1-8B
- Reduces computational overhead while maintaining or improving task performance
- Shows consistent gains across multiple benchmarks including GSM8K and BBH

## Why This Works (Mechanism)

### Mechanism 1: Attention-Entropy Metric Fusion
- **Claim:** Integrating information entropy with accumulated attention scores better preserves tokens critical to model performance than entropy alone.
- **Mechanism:** A weighted additive metric M_t = (1 - α) · I_t(x) + α · s_t (with α=0.8) combines token-level entropy and attention. Tokens with low fused scores are candidates for removal. The attention component prevents the accidental removal of tokens that, despite low entropy, are heavily relied upon during attention.
- **Core assumption:** Attention-critical tokens are causally important for maintaining output quality, even when they have low information entropy.
- **Evidence anchors:** Empirical analysis shows low Pearson correlation (0.095) between entropy and attention scores. Experiments show removing attention-critical tokens degrades F1 scores worse than random selection.
- **Break condition:** If attention patterns in newer architectures do not exhibit similar "critical token" distributions, this fusion metric may not yield benefits.

### Mechanism 2: Iterative Dynamic Entropy Recalculation
- **Claim:** Information entropy of tokens is not static during compression; recalculating it iteratively minimizes information loss by accounting for shifts caused by removing preceding tokens.
- **Mechanism:** The compression is split into multiple stages (D iterations). After each stage where some tokens are removed, the entropy for the remaining tokens is recalculated. This accounts for the observation that a token's entropy can change when its context (preceding tokens) is altered.
- **Core assumption:** The entropy of a token is dependent on its context. Removing context tokens can change a remaining token's entropy, potentially making a previously "safe" low-entropy token into a "risky" high-entropy one.
- **Evidence anchors:** Mentions "dynamically sensing entropy shifts during compression." Figure 3 shows "significant shifts in tokens whose preceding tokens were removed" and increasing entropy shifts with higher compression rates.
- **Break condition:** If the entropy shift is primarily noise or does not materially change the relative ranking of tokens, the computational cost of repeated forward passes may not justify the gain.

### Mechanism 3: Consecutive Token Compression Limitation
- **Claim:** Preventing the compression of consecutive tokens in a single stage reduces error propagation from context-dependent entropy shifts.
- **Mechanism:** The algorithm includes a rule: if a token's preceding token has already been compressed, the token is retained. This avoids compressing a token whose entropy might be artificially altered solely due to the removal of its immediate predecessor.
- **Core assumption:** The most significant and detrimental entropy shifts are local, occurring immediately after a dependent token is removed.
- **Evidence anchors:** Limiting consecutive compression "helps prevent unexpected compression due to the change of entropy within the same compression stage." Ablation study shows performance drop when this constraint is removed.
- **Break condition:** If critical phrases requiring consecutive token removal are present, this heuristic might prevent efficient compression.

## Foundational Learning

- **Concept:** Self-Information and Entropy
  - **Why needed here:** This is the foundational metric for deciding which tokens to keep. The method rests on the idea that I_t(x) = - log_2 P(x_t | x_{0...t-1}) measures information content. Low entropy = predictable = candidate for removal.
  - **Quick check question:** If a model predicts the next token with 100% certainty, what is its information entropy?

- **Concept:** Accumulated Attention Scores
  - **Why needed here:** Understanding how the attention matrix represents token relationships is crucial. The paper uses the sum of attention weights a token receives from all others as a proxy for its importance.
  - **Quick check question:** If a token has high accumulated attention but low entropy, should it be compressed according to DAC?

- **Concept:** Iterative vs. One-shot Processing
  - **Why needed here:** The paper's "dynamic" nature is a core contribution. Understanding the trade-off between single-pass compression (faster) and multi-pass iterative approach (potentially more accurate) is key.
  - **Quick check question:** Why does the paper argue that a one-shot compression based on initial entropy is suboptimal?

## Architecture Onboarding

- **Component map:** Input prompt -> SLM Forward Pass -> Metric Calculator -> Dynamic Scheduler -> Compressor -> Updated prompt -> Repeat until complete

- **Critical path:**
  1. Input prompt tokenized
  2. SLM performs forward pass for initial logits and attention scores
  3. Metric Calculator computes initial fused scores
  4. Loop (D iterations): Dynamic Scheduler sets budget → Threshold determined → Compressor removes tokens → Prompt updated → Repeat from step 2

- **Design tradeoffs:**
  - SLM vs Target LLM: Using an SLM is faster but introduces approximation error
  - Alpha (0.8): Heavily weights attention; may not be optimal for all tasks
  - Iterative Overhead: Multiple forward passes add latency vs. one-shot methods

- **Failure signatures:**
  - Flash Attention Incompatibility: Requires explicit attention scores; incompatible with Flash Attention without modification
  - Extreme Length Degradation: Fixed max iterations (D=15) may lead to coarse compression for very long contexts
  - Cross-Model Divergence: If SLM patterns diverge significantly from target LLM, compression quality degrades

- **First 3 experiments:**
  1. Metric Ablation: Compare entropy-only vs. attention-only vs. fused metric on NarrativeQA at τ=0.5
  2. Dynamic vs. Static: Compare full DAC against a single-pass version to isolate dynamic recalculation benefits
  3. Cross-Model Generalization: Compress with Qwen2-0.5B, evaluate on LLaMA3-8B to test architectural robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method be refined to identify the most representative attention matrices for information fusion, rather than requiring the acquisition of attention matrices from all layers and heads?
- Basis in paper: The authors state in the Limitations section that the current implementation "necessitates the development of a method to identify the most representative attention matrices for more efficient information fusion."
- Why unresolved: The current DAC implementation aggregates attention scores across all layers and heads, which is computationally intensive and limits efficiency gains.
- What evidence would resolve it: A study identifying a subset of specific layers or heads that correlate most strongly with task performance, achieving comparable results to the full-matrix approach with reduced computational overhead.

### Open Question 2
- Question: Can a method be developed to sense information density in order to adaptively adjust the number of dynamic iterations for excessively long contexts?
- Basis in paper: The authors note in the Limitations section that the fixed upper limit on dynamic iterations (D=15) leads to performance degradation in very long contexts because "the granularity of perception becomes coarser."
- Why unresolved: Currently, the dynamic iteration count is capped and primarily determined by input length divided by a constant, failing to account for variations in information density across different texts.
- What evidence would resolve it: An adaptive algorithm that varies D based on local information density metrics, demonstrating maintained performance on "excessively long" datasets compared to the current static cap method.

### Open Question 3
- Question: How can DAC be adapted to remain compatible with high-efficiency attention mechanisms, such as Flash Attention, without incurring the cost of redundant attention score calculations?
- Basis in paper: The paper explicitly states that "DAC is not compatible with high-efficiency attention methods (e.g., Flash Attention) as it does not require calculating attention scores. The application of DAC on such methods will result in additional attention score calculations."
- Why unresolved: High-efficiency methods often skip the explicit calculation of the full attention matrix to save memory and time, which conflicts with DAC's requirement to extract these scores for the integrated metric.
- What evidence would resolve it: A modified DAC framework or approximation technique that retrieves necessary attention statistics without forcing the reconstruction of the full attention matrix, thereby preserving the speed benefits of Flash Attention.

## Limitations
- Requires explicit attention score calculation, making it incompatible with high-efficiency attention methods like Flash Attention
- Fixed maximum of 15 iterations may lead to suboptimal compression quality for extremely long contexts
- Relies on a small language model for compression, introducing approximation error that may diverge from target LLM behavior

## Confidence

**Confidence: Medium** - The metric fusion approach effectively combines entropy and attention signals, but the choice of alpha=0.8 is empirical without sensitivity analysis. The method's effectiveness relies heavily on the assumption that attention patterns are stable across different model architectures and tasks.

**Confidence: Low** - The iterative entropy recalculation mechanism assumes that entropy shifts are primarily due to context changes from preceding token removal. The computational overhead of multiple forward passes through the SLM may offset some efficiency gains.

**Confidence: High** - The consecutive token limitation heuristic is supported by ablation results showing performance degradation when removed. However, this safety mechanism may be overly conservative for prompts with natural low-information segments.

## Next Checks

1. **Cross-Domain Performance Analysis**: Evaluate DAC on code generation tasks (e.g., HumanEval) and multilingual benchmarks to assess generalization beyond the current focus on English reasoning and QA tasks.

2. **Computational Overhead Quantification**: Measure the actual latency and energy consumption of the iterative approach versus one-shot methods across varying prompt lengths, including both SLM compression time and target LLM inference time.

3. **Hyperparameter Sensitivity Study**: Conduct systematic experiments varying alpha (the attention-entropy weighting) and the maximum iteration count (D) to understand the robustness of performance to these design choices.