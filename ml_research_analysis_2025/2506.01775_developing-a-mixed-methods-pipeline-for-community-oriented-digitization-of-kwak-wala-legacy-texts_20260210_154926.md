---
ver: rpa2
title: Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of
  Kwak'wala Legacy Texts
arxiv_id: '2506.01775'
source_url: https://arxiv.org/abs/2506.01775
tags:
- kwak
- wala
- language
- texts
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of digitizing Kwak'wala legacy
  texts from scanned images using modern OCR techniques. The authors developed a mixed-methods
  pipeline combining first-pass OCR with Google Vision, language identification using
  fastText, masking of non-Kwak'wala content, and post-correction models.
---

# Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts

## Quick Facts
- arXiv ID: 2506.01775
- Source URL: https://arxiv.org/abs/2506.01775
- Reference count: 8
- Character Error Rate reduced by 50% (0.43→0.18 for Jesup 5.1; 0.33→0.15 for Kwakiutl 1909)

## Executive Summary
This work addresses the challenge of digitizing Kwak'wala legacy texts from scanned images using modern OCR techniques. The authors developed a mixed-methods pipeline combining first-pass OCR with Google Vision, language identification using fastText, masking of non-Kwak'wala content, and post-correction models. They applied this pipeline to two Kwak'wala books, achieving a 50% reduction in character error rate (from 0.43 to 0.18 for Jesup 5.1, and from 0.33 to 0.15 for Kwakiutl 1909) and an 87.5% reduction in structural errors. The pipeline successfully produced high-quality transcriptions that were transliterated into modern orthographies, enabling better access to culturally significant documents for community revitalization efforts.

## Method Summary
The pipeline processes scanned Kwak'wala documents containing mixed language content through four main stages: first-pass OCR using Google Vision API, language identification via fastText binary classifier, selective masking of non-Kwak'wala content, and neural post-correction. The system handles the Boas-Hunt orthography (containing 42 consonants and 13+ vowel representations) and produces transcriptions that can be transliterated to modern community orthographies. Training data consisted of small labeled datasets of first-pass and reference text pairs, with pre-training on unlabeled first-pass outputs.

## Key Results
- Character Error Rate reduced by 50% overall (0.43→0.18 for Jesup 5.1; 0.33→0.15 for Kwakiutl 1909)
- Structural Error Rate reduced by 87.5%
- Sentence-level language identification accuracy achieved 99.84%
- Pipeline successfully handled multilingual documents with interlinear glosses

## Why This Works (Mechanism)

### Mechanism 1: Language identification combined with selective masking improves post-correction model effectiveness by isolating target-language content from multilingual documents.

### Mechanism 2: Multi-source neural post-correction models trained on small labeled datasets can substantially reduce character error rates for polysynthetic languages with complex orthographies.

### Mechanism 3: Commercial OCR (Google Vision) provides effective first-pass transcription for legacy orthographies using extended Latin scripts, despite never seeing the target language during training.

## Foundational Learning

- **Character Error Rate (CER) vs. Word Error Rate (WER)**: Kwak'wala is polysynthetic—words are morphologically complex, leading to low token frequency. WER would penalize entire words for single-character errors, while CER captures fine-grained recognition quality.
  - Quick check: For a language where "nəmxʷsayəɬ" and "nəmxʷsayəɬa" are different tokens but share 90% of characters, which metric better reflects OCR improvement?

- **Language Identification (langID) at Sentence vs. Token Level**: The pipeline uses sentence-level langID to mask content, but documents contain interlinear glosses and mixed lines. Understanding granularity trade-offs helps diagnose masking errors.
  - Quick check: If a line contains "The word *ƛəna* means 'to go'"—should langID classify at line level or token level, and what are the downstream consequences?

- **Post-OCR Correction as Sequence-to-Sequence Learning**: The post-correction model treats OCR fixing as a seq2seq problem, not just spell-checking. Understanding this framing clarifies why pre-training on noisy unlabeled data helps.
  - Quick check: Why would a neural seq2seq model outperform a dictionary-based spell-checker for correcting "qʷəƛəs" → "qʷəƛ̕əs" (missing glottalization diacritic)?

## Architecture Onboarding

- **Component map**: Scanned PDF → Google Vision OCR → Bounding boxes + text → fastText langID → Masking layer (English, numbers, punctuation) → Post-correction model (seq2seq) → Reconstruction (reinsert masked tokens) → G2P transliteration → U'mista/SD-72

- **Critical path**: First-pass OCR quality determines whether downstream correction is tractable; langID accuracy gates masking precision—false negatives leak English into correction; post-correction model training data must match target document characteristics.

- **Design tradeoffs**: Google Vision ($1.25/1000 pages, API-dependent) vs. Tesseract/Ocular (free, local, requires training); binary langID (simpler) vs. multi-class (handles mixed orthographies); masking aggressive (removes more noise, risks losing Kwak'wala) vs. conservative (keeps more content, risks contamination).

- **Failure signatures**: Persistent high CER after correction → training data mismatch or insufficient pre-training; structural errors in output → masking indices misaligned during reconstruction; English words in final Kwak'wala output → langID false negatives on short English fragments.

- **First 3 experiments**:
  1. Baseline measurement: Run Google Vision on 5 sample pages, compute CER without any post-processing.
  2. langID validation: Manually label 100 lines as Kwak'wala/English/mixed. Compare fastText predictions against ground truth to quantify false positive/negative rates.
  3. Ablation test: Run pipeline with and without masking. Compare final CER to isolate masking's contribution to error reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- Pipeline performance heavily depends on first-pass OCR quality and language identification accuracy
- Language identification assumes consistent sentence-level segmentation, which may not hold for irregular legacy document structures
- Post-correction model training data characteristics are not fully specified, raising questions about generalization

## Confidence
- **High confidence**: CER reduction figures (50% overall reduction, specific values for both test documents)
- **Medium confidence**: Selective masking improves post-correction effectiveness depends on langID accuracy
- **Medium confidence**: Commercial OCR provides viable first-pass transcription assumes character overlap with Boas-Hunt

## Next Checks
1. **First-pass quality assessment**: Process 10 randomly selected pages through Google Vision and manually calculate CER to establish baseline quality before any correction.
2. **langID precision validation**: Create a gold-standard test set of 200 lines (100 Kwak'wala, 100 English, including mixed cases) and measure actual precision/recall of the fastText classifier.
3. **Masking ablation study**: Run the complete pipeline on two document sections, once with full masking and once without masking, to isolate the contribution of selective content masking to overall error reduction.