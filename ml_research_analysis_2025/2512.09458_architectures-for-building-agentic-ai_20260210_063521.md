---
ver: rpa2
title: Architectures for Building Agentic AI
arxiv_id: '2512.09458'
source_url: https://arxiv.org/abs/2512.09458
tags:
- agents
- reasoning
- memory
- tool
- reliability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter argues that reliability in agentic and generative
  AI is fundamentally an architectural property, not merely a model characteristic.
  It defines agentic systems as goal-directed, tool-using decision makers operating
  in closed loops and shows how reliability emerges from principled componentisation
  (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor,
  telemetry), disciplined interfaces (schema-constrained, validated, least-privilege
  tool calls), and explicit control and assurance loops.
---

# Architectures for Building Agentic AI

## Quick Facts
- arXiv ID: 2512.09458
- Source URL: https://arxiv.org/abs/2512.09458
- Reference count: 4
- This chapter argues that reliability in agentic and generative AI is fundamentally an architectural property, not merely a model characteristic.

## Executive Summary
This chapter presents a systematic framework for building reliable agentic AI systems through principled architectural design. The core thesis is that reliability emerges from decomposing complex agent behaviors into discrete, validated components rather than relying on monolithic model calls. By introducing explicit control and assurance loops, schema-constrained interfaces, and structured memory systems, the architecture aims to contain failures, enable auditability, and provide graceful degradation paths when errors occur.

## Method Summary
The paper proposes a taxonomy of agentic architectures including tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents. It analyzes how each pattern reshapes reliability envelopes and failure modes. The methodology centers on componentization - separating functionality into goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, and telemetry subsystems. Key patterns include simulating-before-actuating, typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance, and auditability through structured logging.

## Key Results
- Reliability in agentic AI emerges from principled componentization rather than monolithic model design
- Structured interfaces with schema validation convert open-ended model outputs into predictable, auditable actions
- Explicit control and assurance loops prevent small reasoning slips from cascading into hazardous sequences
- The architecture enables graceful degradation through safe-halt mechanisms and structured logging for replayability

## Why This Works (Mechanism)

### Mechanism 1: Structural Containment via Componentization
- **Claim:** Reliability emerges from decomposing systems into discrete components with bounded responsibilities, rather than relying on monolithic model calls.
- **Mechanism:** Separating functionality (Goal Manager, Planner, Tool-Router, Executor, Verifiers) confines faults to specific boundaries ("blast radius") and creates natural choke points for safety checks. This isolation allows components like a Verifier to be strengthened without disturbing the Planner.
- **Core assumption:** Errors in reasoning (e.g., hallucinations) are inevitable, but their propagation can be structurally blocked.
- **Evidence anchors:**
  - [abstract] Reliability emerges from "principled componentisation (goal manager, planner, tool-router, executor...)"
  - [section 1, p. 3] "Componentisation confines faults to well-defined boundaries... limits their blast radius."
  - [corpus] Related work supports the need for "layered failure stacks" to identify vulnerabilities, though specific mechanisms for containment vary.
- **Break condition:** If components share uncontrolled global state or implicit coupling, fault isolation fails.

### Mechanism 2: Constraint Enforcement via Typed Interfaces
- **Claim:** Disciplined, schema-validated interfaces convert open-ended model outputs into predictable, auditable actions.
- **Mechanism:** By enforcing typed schemas and least-privilege permissions at the Tool Router and Execution Gateway, the system transforms free-form text into typed objects. This mechanism rejects invalid or unsafe proposals (e.g., schema mismatches, policy violations) before execution ("validate-before-actuate").
- **Core assumption:** Model outputs can be effectively parsed and validated against a pre-defined schema, and malicious or malformed outputs follow detectable patterns.
- **Evidence anchors:**
  - [abstract] Highlights "disciplined interfaces (schema-constrained, validated, least-privilege tool calls)."
  - [section 3.1, p. 9] "Typed schemas and validators... Treat every tool invocation as a contract... This alone eliminates a large class of silent failures."
  - [corpus] Neighbors discuss "system-theoretic foundations" for security, aligning with interface-based constraints.
- **Break condition:** If the schema is too permissive or the validator is compromised, the mechanism degrades to a passthrough.

### Mechanism 3: Governance via Control and Assurance Loops
- **Claim:** Explicit feedback loops (supervisors, budgets, termination criteria) prevent small reasoning slips from cascading into hazardous sequences.
- **Mechanism:** Monitors compare planned vs. observed behavior, while Safety Supervisors enforce hard constraints like step limits, cost caps, and wall-clock timeouts. This ensures "graceful degradation" (e.g., safe-halt) rather than catastrophic failure.
- **Core assumption:** Failure modes are detectable within resource budgets, and the system has authority to interrupt or halt execution.
- **Evidence anchors:**
  - [abstract] Reliability requires "explicit control and assurance loops."
  - [section 1, p. 4] "Control and assurance loops... preventing small reasoning slips from cascading into hazardous sequences."
  - [corpus] Weak direct evidence in neighbors; focus is primarily on high-level failure modes rather than specific loop implementations.
- **Break condition:** If the latency of the monitoring loop exceeds the execution speed of the agent, oversight becomes ineffective.

## Foundational Learning

- **Concept:** Belief-Desire-Intention (BDI) Architecture
  - **Why needed here:** The paper uses BDI as the "control skeleton" for modern GenAI agents. Understanding the mapping (Beliefs → Memory, Desires → Goals, Intentions → Tool Calls) is prerequisite to structuring the "Goal Manager" and "Planner" components.
  - **Quick check question:** Can you distinguish between a "desire" (admissible goal) and an "intention" (committed plan) in the context of a diagnosis agent?

- **Concept:** Retrieval-Augmented Generation (RAG) & Memory Provenance
  - **Why needed here:** Section 3.2 argues memory is an active subsystem, not a passive store. You must understand how "provenance" (tracking source identifiers and timestamps) prevents "poisoned or stale context" from corrupting reasoning.
  - **Quick check question:** How does adding a "freshness" policy to a RAG retrieval step improve the reliability of a diagnosis?

- **Concept:** Idempotency and Transactional Semantics
  - **Why needed here:** Critical for the "Execution Gateway" (Section 3.1). Agents operate in noisy environments; understanding how idempotency keys and "saga" patterns allow safe retries without duplicate side-effects is essential for robust tool use.
  - **Quick check question:** If a tool call times out, does the architecture retry the exact same call (idempotent) or check for a partial state change first?

## Architecture Onboarding

- **Component map:**
  Intent -> Goal Manager -> Planner -> Verifier Check -> Tool Router -> Simulation Sandbox -> Execution Gateway -> Telemetry
  *Safety Supervisor monitors all steps and can trigger safe-halt at any point*

- **Critical path:**
  **Intent -> Planner -> (Verifier Check) -> Tool Router -> (Simulation Sandbox) -> Execution Gateway -> Telemetry.**

- **Design tradeoffs:**
  - **Reliability vs. Creativity:** Strict schemas and tool allow-lists prevent hallucination but restrict the agent's ability to synthesize novel solutions (Section 3.1).
  - **Latency vs. Safety:** "Simulate-before-actuate" adds latency but prevents irreversible side-effects (Section 1, p. 5).

- **Failure signatures:**
  - **Infinite Loops:** Repeated tool calls without progress (Mitigation: Step caps)
  - **Hallucinated Tools:** Model invents non-existent APIs (Mitigation: Router whitelisting)
  - **State Drift:** Memory retrieval returns outdated context (Mitigation: Freshness policies)

- **First 3 experiments:**
  1. **Implement a Mock Tool Router:** Build a router that accepts only a specific JSON schema and rejects everything else. Verify that the Planner handles the rejection gracefully.
  2. **Add a "Budget Supervisor":** Configure a simple counter that halts the agent after 5 steps. Test if the "safe-halt" path returns an auditable log.
  3. **Simulate-before-Actuate:** For a file-writing tool, implement a "dry-run" mode that returns the *expected* file diff without saving, requiring a separate "commit" call.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can systems dynamically determine optimal thresholds for constraining tool usage to balance reliability against agent creativity?
- Basis: [inferred] The text notes that while constraining tools improves safety, "Determining the exact thresholds, however, is not necessarily straightforward" (Page 9).
- Why unresolved: The trade-off is highly context-dependent; static rules may fail in novel situations requiring creative tool use or over-constrain the agent.
- What evidence would resolve it: Benchmarks measuring task success rates against varying constraint strictness in open-ended environments.

### Open Question 2
- Question: What architectural mechanisms most effectively prevent prompt and retrieval injection in memory-augmented agents?
- Basis: [inferred] The paper notes that treating untrusted text as data rather than instruction is "easier said than done" (Page 8) and aligns with complex threat taxonomies.
- Why unresolved: Adversarial inputs evolve faster than static sanitization rules, and over-filtering external data degrades reasoning performance.
- What evidence would resolve it: Comparative robustness testing of sanitization strategies against adversarial red-teaming benchmarks in RAG systems.

### Open Question 3
- Question: How can multi-agent protocols mitigate sycophancy and agreement bias without requiring an impractical number of interaction rounds?
- Basis: [inferred] The paper notes that debate can "entrench sycophancy" (Page 15) and that optimizing the number of rounds is a current research area.
- Why unresolved: Consensus mechanisms often default to agreement bias or require expensive, iterative computation to overcome it.
- What evidence would resolve it: Empirical studies on convergence rates and error reduction using heterogeneous critics and blinded proposals.

## Limitations

- Limited empirical validation of architectural claims with real-world testing or quantitative performance metrics
- Effectiveness depends heavily on implementation quality and domain-specific context
- Assumes schema validation can effectively constrain model behavior without addressing bypass scenarios
- Doesn't fully explore edge cases where legitimate use cases require more flexible interfaces than strict schemas allow

## Confidence

- **High Confidence:** The architectural decomposition into discrete components (Goal Manager, Planner, Tool Router, etc.) and the general principle that reliability emerges from principled componentization
- **Medium Confidence:** The specific mechanisms for typed schema enforcement and control loops, as these depend heavily on implementation details and domain-specific requirements
- **Low Confidence:** The claim that these architectures will universally prevent cascading failures across all agentic AI applications, given the diversity of potential use cases

## Next Checks

1. **Implementation Testing:** Build and test a minimal prototype implementing the proposed architecture with simulated failure modes to measure actual fault isolation effectiveness and verify that the "blast radius" containment works as described.

2. **Schema Robustness Analysis:** Conduct systematic testing of the schema validation mechanism against adversarial inputs and edge cases to determine the actual security boundaries and identify potential bypass vectors.

3. **Performance Benchmarking:** Measure the latency overhead introduced by the "simulate-before-actuate" and validation layers across different tool types and workloads to quantify the reliability-latency tradeoff in practice.