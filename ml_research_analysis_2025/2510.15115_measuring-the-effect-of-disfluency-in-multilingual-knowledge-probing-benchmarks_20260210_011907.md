---
ver: rpa2
title: Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks
arxiv_id: '2510.15115'
source_url: https://arxiv.org/abs/2510.15115
tags:
- language
- languages
- object
- translation
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how template-based prompts in multilingual
  benchmarks can lead to ungrammatical sentences that degrade LLM factual retrieval
  performance. The authors compare MLAMA's templated prompts with sentence-level translations
  from Google Translate and ChatGPT across four Slavic and five diverse non-Slavic
  languages.
---

# Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks

## Quick Facts
- **arXiv ID:** 2510.15115
- **Source URL:** https://arxiv.org/abs/2510.15115
- **Reference count:** 35
- **Key outcome:** Template-based prompts in multilingual benchmarks introduce ungrammatical sentences that degrade LLM factual retrieval performance, with up to 10% R@1 improvement when using fluent sentence translations.

## Executive Summary
This paper investigates how template-based prompting in multilingual knowledge benchmarks introduces grammatical disfluencies that impair LLM performance. The authors compare MLAMA's templated prompts with sentence-level translations from Google Translate and ChatGPT across four Slavic and five non-Slavic languages. They find significant performance gains (up to 10% in R@1) when using fluent sentence translations, particularly from Google Translate. The improvement is attributed to better grammaticality, lexical accuracy, and entity inflection. The study reveals that current template-based multilingual datasets may underestimate LLM capabilities and recommends using properly translated prompts for more reliable evaluation.

## Method Summary
The authors conducted controlled experiments across 9 languages (4 Slavic: Bulgarian, Croatian, Czech, Russian; 5 non-Slavic: Danish, French, Hindi, Japanese, Turkish) using the MLAMA dataset. They created three prompt versions: original MLAMA templates, Google Translate sentence translations, and ChatGPT sentence translations. The experiments measured retrieval performance using R@1, R@5, and R@10 metrics on decoder-only LLMs. They also conducted human evaluation of grammaticality, analyzed morphosyntactic agreement errors, and examined lexical accuracy differences between translation methods.

## Key Results
- Sentence-level translations from Google Translate achieved up to 10% higher R@1 scores compared to templated prompts
- ChatGPT translations showed improvement over templates but performed worse than Google Translate
- Grammaticality errors in templates, particularly morphosyntactic agreement failures, correlated with reduced retrieval accuracy
- Performance gains were most pronounced for Slavic languages but also significant for non-Slavic languages

## Why This Works (Mechanism)
The mechanism underlying the observed performance gains relates to how LLMs process grammatical input. When prompts contain template-based slot-filling with grammatical errors, the model must simultaneously parse ungrammatical structure while retrieving factual knowledge, creating additional cognitive load. Fluent sentence translations eliminate this parsing burden, allowing the model to focus computational resources on knowledge retrieval rather than grammatical repair. The improvement is particularly notable for languages with rich inflectional morphology where template-based prompts frequently generate agreement errors.

## Foundational Learning
- **Grammaticality in LLMs**: Understanding how syntactic well-formedness affects model performance - needed to interpret why template errors matter; quick check: examine model perplexity differences between fluent and disfluent prompts
- **Morphosyntactic agreement**: How agreement features propagate through sentences - needed to understand template failure modes; quick check: identify agreement errors in template-generated sentences
- **Multilingual probing**: Techniques for evaluating cross-lingual knowledge retrieval - needed to contextualize benchmark design choices; quick check: compare probing methodologies across different benchmarks
- **Translation quality metrics**: chrF, BLEU, and other surface-level metrics - needed to understand why traditional metrics fail to capture grammaticality; quick check: correlate surface metrics with human grammaticality judgments
- **Decoder-only prompting strategies**: How completion-based prompts work for knowledge retrieval - needed to understand task design; quick check: test different completion positions in template prompts

## Architecture Onboarding

**Component Map**: Knowledge Base <- Template Generation -> Prompt Construction -> LLM Inference -> Retrieval Evaluation

**Critical Path**: The retrieval performance depends on prompt grammaticality → lexical accuracy → entity inflection → final prediction

**Design Tradeoffs**: Templates offer scalability and consistency across languages but sacrifice grammaticality; sentence translations provide fluency but require more resources and may introduce translation artifacts

**Failure Signatures**: Low R@1 scores correlate with morphosyntactic agreement errors, missing articles, and incorrect entity inflections in prompts

**First Experiments**:
1. Measure model perplexity on template vs. fluent prompts to quantify grammatical complexity
2. Conduct ablation study removing inflectional morphology from both prompt types
3. Test retrieval performance on artificially corrupted fluent prompts to isolate grammaticality effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the grammaticality of multilingual prompts be scalably and reliably measured?
- **Basis in paper**: Section 5.4 explicitly asks "How to measure grammaticality?" and states that log probabilities, surface metrics like chrF, and Grammar Error Correction systems failed to consistently approximate fluency.
- **Why unresolved**: Current automated metrics cannot accurately detect morphosyntactic agreement errors in slot-filled sentences, making manual verification the only reliable, yet unscalable, method.
- **What evidence would resolve it**: A novel metric or automated evaluation protocol that correlates strongly with human judgments of grammaticality for inflected, template-based prompts.

### Open Question 2
- **Question**: How should factual knowledge probing be adapted for decoder-only LLMs in languages with verb-final word orders?
- **Basis in paper**: Section 5.4 ("How to prompt models multilingually?") notes that the standard prompting strategy (predicting a final object) fails for verb-final languages (e.g., Turkish, Hindi) without confounding parametric knowledge with alignment skills.
- **Why unresolved**: Prompting models to complete a sentence where the object is not sentence-final requires masking or instruction-following, which introduces noise into the factual retrieval measurement.
- **What evidence would resolve it**: A prompting framework designed for verb-final languages that successfully isolates parametric knowledge from instruction-following capabilities.

### Open Question 3
- **Question**: Does a language's resourcedness in training data moderate the impact of disfluency on factual retrieval?
- **Basis in paper**: Section 5.3 discusses the failed hypothesis that lower-resourced languages would benefit more from fluent prompts; the results showed no clear trend (e.g., Danish improved significantly while Croatian did not).
- **Why unresolved**: The relationship between training data volume and a model's robustness to ungrammatical input remains ambiguous based on the observed language sample.
- **What evidence would resolve it**: A controlled study across a broader set of languages that specifically correlates training data prevalence with the magnitude of performance gain achieved through fluent translations.

## Limitations
- Analysis is restricted to the MLAMA dataset and factual recall tasks, limiting generalizability to other knowledge benchmarks or reasoning tasks
- Comparison between translation methods does not account for cultural appropriateness or domain-specific accuracy beyond grammaticality
- Does not investigate whether fine-tuning on disfluent prompts reduces the performance gap observed with fluent translations

## Confidence
- **High confidence**: Template disfluency degrades LLM performance across multiple languages
- **High confidence**: Sentence-level translations improve retrieval accuracy compared to templates
- **Medium confidence**: The magnitude of improvement is consistent across all tested languages
- **Medium confidence**: Grammaticality is the primary driver of performance differences

## Next Checks
1. Replicate the experiment using other multilingual knowledge benchmarks (e.g., MMLU, XCOPA) to verify generalizability beyond MLAMA
2. Conduct human evaluation of translation quality focusing on entity inflection accuracy and lexical choice precision, not just grammaticality
3. Test whether fine-tuning models on disfluent template prompts reduces or eliminates the performance gap observed with fluent translations