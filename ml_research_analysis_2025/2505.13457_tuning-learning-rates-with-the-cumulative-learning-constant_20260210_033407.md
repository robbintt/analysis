---
ver: rpa2
title: Tuning Learning Rates with the Cumulative-Learning Constant
arxiv_id: '2505.13457'
source_url: https://arxiv.org/abs/2505.13457
tags:
- learning
- rate
- epochs
- optimal
- rates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for optimizing learning rates
  in machine learning. A previously unrecognized proportionality between learning
  rates and dataset sizes is discovered, showing that the optimal learning rate is
  inversely proportional to the total amount of data the model will see during training.
---

# Tuning Learning Rates with the Cumulative-Learning Constant

## Quick Facts
- arXiv ID: 2505.13457
- Source URL: https://arxiv.org/abs/2505.13457
- Authors: Nathan Faraj
- Reference count: 1
- Primary result: Optimal learning rate is inversely proportional to total data exposure (dataset size × epochs), enabling efficient learning rate optimization across scales

## Executive Summary
This paper introduces a novel method for optimizing learning rates in machine learning by discovering a previously unrecognized proportionality between learning rates and dataset sizes. The framework identifies a cumulative learning constant that characterizes the total optimization distance for a model architecture, which can be calculated at small scales and used to determine optimal learning rates for larger datasets. This approach enables efficient optimization of advanced learning rate schedules including decaying and cyclical learning rates, significantly reducing the computational cost associated with current learning rate selection methods.

## Method Summary
The method involves calibrating a cumulative learning constant (κ) by running a small-scale learning rate sweep to find the optimal constant learning rate, then computing κ = η_optimal × D_small where D_small is total data exposure at small scale. This constant can then predict optimal learning rates at larger scales using η_target = κ / D_target. The framework extends to scheduled learning rates by ensuring the integral of the learning rate schedule equals κ. Experiments use a simple 784→256→10 MLP on MNIST with normalized inputs, testing both Adam and SGD optimizers across varying epochs and dataset sizes.

## Key Results
- Optimal learning rate is inversely proportional to total data exposure (dataset size × epochs)
- Cumulative learning constant κ characterizes architecture-specific optimization distance
- κ-transfer method achieves coefficient of variation of only 22.4% for Adam optimizer predictions
- Framework successfully optimizes decaying and cyclical learning rate schedules

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal learning rate is inversely proportional to total training data exposure.
- **Mechanism:** The paper demonstrates that η ∝ 1/D, where D = dataset_size × epochs. When total data doubles, the optimal learning rate halves. This relationship emerges because the cumulative gradient steps required to traverse the loss landscape remain approximately constant for a given architecture—fewer steps (smaller D) require larger steps (higher η), and vice versa.
- **Core assumption:** The model converges through a statistically equivalent region of the loss landscape regardless of trajectory, implying ergodicity of the optimization process.
- **Evidence anchors:**
  - [abstract]: "optimal learning rate is inversely proportional to the total amount of data the model will see during training"
  - [section 4.1]: Table 2 shows SGD optimizer where doubling epochs halves optimal LR (5 epochs → 0.14 LR; 10 epochs → 0.1 LR)
  - [corpus]: Related work on LR-batch size scaling (Diego et al., 2020) supports proportional relationships, but corpus evidence for dataset-size scaling specifically is weak.
- **Break condition:** At very low epoch counts (<5), variance increases substantially and the proportionality becomes unreliable. Also breaks if the loss landscape changes qualitatively with dataset scale.

### Mechanism 2
- **Claim:** A cumulative learning constant κ characterizes the total optimization distance for a model architecture.
- **Mechanism:** κ = ∫₀ᴰ η(x)dx represents cumulative "learning effort." For constant LR: κ = η × D. Since κ is architecture-dependent but dataset-independent, measuring κ at small scale enables LR prediction at large scale: η_new = κ / D_new.
- **Core assumption:** κ depends only on architecture and loss landscape geometry, not on dataset size or learning rate schedule.
- **Evidence anchors:**
  - [section 5]: "κ encapsulates the total step distance traveled by the model in parameter space to converge"
  - [section 5, Figure 10]: Expected vs. actual optimal LR shows coefficient of variation 22.4% for Adam with decaying schedule
  - [corpus]: D-adaptation (Defazio & Mishchenko, 2023) similarly uses distance-based LR adaptation but with higher computational cost.
- **Break condition:** Assumption fails if different datasets fundamentally alter loss landscape curvature or if different random seeds lead to meaningfully different local minima with different total distances.

### Mechanism 3
- **Claim:** The constant κ can optimize scheduled learning rates (decaying, cyclical) by equating cumulative integrals.
- **Mechanism:** Any schedule satisfying ∫₀ᴰ η(x)dx = κ should yield comparable convergence. For a halving decay schedule, the integral becomes a geometric series, enabling algebraic solving for initial LR that achieves target κ.
- **Core assumption:** All well-designed schedules converging to the same minimum require equivalent cumulative step distance.
- **Evidence anchors:**
  - [section 5, Figure 8-9]: Decaying LR experiments show predicted optimal (0.00275) matches observed minimum at 10 epochs
  - [section 5, Figures 12-13]: Cyclical schedules show convergence to expected values at higher epochs
  - [corpus]: Cosine annealing (Loshchilov & Hutter, 2017) and cyclical LR (Smith, 2017) demonstrate schedule importance, but corpus lacks direct validation of κ-transfer method.
- **Break condition:** Poorly designed schedules (too aggressive decay, wrong frequency cycles) may overshoot or stagnate, breaking cumulative distance invariance.

## Foundational Learning

- **Concept: Inverse proportionality in optimization scaling**
  - **Why needed here:** The central claim rests on η ∝ 1/D. Without understanding inverse proportionality, the prediction formula η = κ/D is opaque.
  - **Quick check question:** If a model sees 4× more total data, what happens to optimal learning rate? (Answer: divides by 4)

- **Concept: L-smoothness and gradient descent convergence bounds**
  - **Why needed here:** The paper's proof in Section 4.1 uses L-smooth function bounds to derive the proportionality relationship theoretically.
  - **Quick check question:** Why does the quadratic term (L/2)η² become negligible for small η? (Answer: higher-order decay)

- **Concept: Learning rate schedules (constant, decaying, cyclical)**
  - **Why needed here:** The method transfers κ from constant to scheduled regimes. Understanding schedule types is prerequisite to applying the framework.
  - **Quick check question:** What integral does a halving decay schedule produce? (Answer: geometric series)

## Architecture Onboarding

- **Component map:** Small-scale calibration run → κ computation → Target scale prediction → Schedule translation
- **Critical path:**
  1. Run LR sweep on small dataset (e.g., 1/10 scale) with constant LR
  2. Identify η_optimal at small scale
  3. Compute κ
  4. Predict and validate at larger scale

- **Design tradeoffs:**
  - Smaller calibration runs are faster but increase variance (paper shows >5 epochs needed for stability)
  - Adam optimizer shows ~22% coefficient of variation—accept some suboptimality for reduced tuning cost
  - Method assumes fixed architecture; changing model structure requires re-calibrating κ

- **Failure signatures:**
  - High variance at low epochs → use more epochs for calibration
  - Predicted LR clearly wrong → check that total data calculation includes both dataset size AND epochs
  - Schedule not converging → verify integral calculation matches κ

- **First 3 experiments:**
  1. Replicate paper's MNIST experiment: run LR sweep at 5 epochs, predict for 20 epochs, measure error
  2. Test κ-transfer across dataset sizes: calibrate on 30K samples, predict for 60K same epochs
  3. Apply to decaying schedule: compute κ from constant LR run, solve geometric series for decay initial value, validate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cumulative learning constant framework apply to complex, state-of-the-art architectures such as Transformers, large language models (LLMs), and reinforcement learning agents?
- Basis in paper: [explicit] The Conclusion states that future work should "explore... alternative architectures, such as reinforcement learning agents, generative models, and large language models, to determine the full extent of these findings."
- Why unresolved: The experimental validation was restricted to a simple Multilayer Perceptron (MLP) with one hidden layer on the MNIST dataset.
- What evidence would resolve it: Successful prediction of optimal learning rates for Transformers or RL agents using the cumulative learning constant (κ) derived from small-scale proxy training runs.

### Open Question 2
- Question: How does the cumulative learning constant interact with varying batch sizes?
- Basis in paper: [inferred] The Introduction explicitly excludes this variable, stating, "this paper will not be exploring how batch size affects the optimal learning rate."
- Why unresolved: Existing research establishes a relationship between batch size and optimal learning rates. Ignoring this variable leaves a gap in understanding how κ behaves when compute budgets are scaled via batch size rather than just data repetitions.
- What evidence would resolve it: Experiments demonstrating that κ remains invariant (or defining how it scales) when batch sizes are increased alongside dataset size.

### Open Question 3
- Question: Can the method be refined to maintain accuracy in low-data regimes where experimental variance is high?
- Basis in paper: [inferred] The paper notes that "At epochs < 5, there appears to be a large variance in data," and results are described as "choppy" or inconsistent with the predicted values.
- Why unresolved: The theoretical proofs rely on assumptions valid for "sufficiently large datasets," limiting the method's precision for rapid, small-scale tuning.
- What evidence would resolve it: A modification to the calculation of κ or the experimental protocol that reduces the coefficient of variation (currently 22.4%) for short training runs.

## Limitations
- High variance at low epoch counts (<5 epochs) limits applicability for rapid small-scale tuning
- Method assumes dataset-independent loss landscape, which may fail for datasets with fundamentally different geometries
- Coefficient of variation around 22.4% indicates acceptable but imperfect prediction accuracy

## Confidence
- **High confidence**: The inverse proportionality between optimal learning rate and total data exposure (η ∝ 1/D) - supported by systematic experiments across multiple epoch counts and datasets
- **Medium confidence**: The cumulative learning constant κ as an architecture-dependent invariant - theoretically sound but requires validation across diverse architectures
- **Medium confidence**: κ-transfer to scheduled learning rates - demonstrated for decaying and cyclical schedules but with higher variance than constant LR predictions

## Next Checks
1. **Cross-architecture validation**: Test κ-transfer method on convolutional and transformer architectures beyond the simple feedforward network to assess generalizability
2. **Dataset landscape analysis**: Compare loss landscapes across different datasets with identical architectures to verify κ's dataset-independence assumption
3. **Scale boundary testing**: Systematically evaluate performance at extreme scale ratios (10×, 100×) between calibration and target datasets to identify breaking points