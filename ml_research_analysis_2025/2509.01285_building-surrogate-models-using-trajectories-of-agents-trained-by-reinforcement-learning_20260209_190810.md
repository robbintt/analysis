---
ver: rpa2
title: Building surrogate models using trajectories of agents trained by Reinforcement
  Learning
arxiv_id: '2509.01285'
source_url: https://arxiv.org/abs/2509.01285
tags:
- surrogate
- methods
- space
- state
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to surrogate modeling by
  leveraging trajectories from agents trained via Reinforcement Learning. Traditional
  methods like Latin-Hypercube sampling or Active Learning with Kriging are less effective
  in wide state spaces, as they sample uniformly rather than focusing on realistic
  state transitions.
---

# Building surrogate models using trajectories of agents trained by Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.01285
- **Source URL:** https://arxiv.org/abs/2509.01285
- **Reference count:** 36
- **One-line primary result:** Agent-based sampling, especially with a mix of agents including an entropy-maximizing policy, outperforms classical methods for building surrogate models in high-dimensional state spaces.

## Executive Summary
This paper introduces a novel approach to surrogate modeling that leverages trajectories from agents trained via Reinforcement Learning (RL), addressing the limitations of traditional uniform sampling methods in high-dimensional state spaces. The core idea is that data from RL agents—trained to act in realistic, physically plausible ways—captures the true distribution of states encountered during task execution, leading to more accurate surrogate models. The method employs a mix of agents: random, expert, and a maximum-entropy agent designed to explore uncertain regions. Empirical results across environments (CartPole, MountainCar, HalfCheetah, Ant) show that this agent-based strategy, particularly when including the entropy-maximizing agent, consistently outperforms classical sampling methods like Latin-Hypercube and Sobol, especially in high-dimensional, complex environments like Mujoco. This advances surrogate-aided Reinforcement Learning for computationally expensive simulators.

## Method Summary
The method trains surrogate models to predict the next state $s_{t+1}$ given the current state $s_t$ and action $a_t$ in a deterministic MDP. Instead of using uniform sampling methods (LHS, Sobol), it generates training data by running different RL policies: a random agent (RA), an expert agent (EA), and a maximum-entropy agent (MEA) trained to maximize state-visitation entropy. These trajectories are collected into datasets, and a mixed dataset (MA) is created by combining samples from all three agents. Surrogate models (XGBoost, ANN, Gaussian Process) are trained on these datasets and evaluated on their ability to predict next states for held-out trajectories from various sources, using the coefficient of determination ($R^2$) as the metric. The key innovation is the use of agent-based sampling, particularly the inclusion of the MEA, to cover the most informative and uncertain regions of the state space.

## Key Results
- XGBoost consistently outperformed ANNs and Gaussian Processes for surrogate modeling across all environments tested.
- Agent-based sampling methods (RA, EA, MEA, MA) significantly outperformed generative methods (LHS, Sobol) in high-dimensional environments like HalfCheetah and Ant.
- The mixed-agent dataset (MA), which includes samples from the maximum-entropy agent (MEA), provided the best overall performance and generalization.
- Surrogates trained on agent-based data were more robust and showed higher $R^2$ scores when tested on realistic agent trajectories compared to those trained on uniform samples.

## Why This Works (Mechanism)

### Mechanism 1: Agent-Based Sampling for Realistic State Distributions
Agent-based methods produce surrogate models that are more accurate in high-dimensional state spaces because they collect sequences of states visited by policies that actually interact with the environment. This focuses the model's learning capacity on the distribution of states encountered during realistic use, leading to higher accuracy along meaningful trajectories. The core assumption is that the primary use of the surrogate will be to predict transitions along trajectories generated by policies similar to those used for training. Evidence is primarily from the paper's abstract and section 2.3, with limited direct corpus support. This mechanism fails if the downstream application requires accurate predictions for arbitrary, out-of-distribution states far from any realistic trajectory.

### Mechanism 2: Maximum Entropy Agent (MEA) for Exploratory Coverage
Including data from an agent trained to maximize the entropy of the state-visitation distribution (MEA) significantly improves the surrogate's generalization by covering uncertain and under-explored regions of the state space. The MEA policy is explicitly trained to visit a diverse set of states, providing the surrogate model with samples from these high-uncertainty regions and reducing error on novel test trajectories. The core assumption is that the state-visitation entropy targeted by the MEA policy correlates with the surrogate model's predictive uncertainty. Evidence is from the abstract and section 4, with weak or missing corpus support. The benefit diminishes if the environment has reachable but dynamically irrelevant or noisy regions, or if the MEA cannot be trained effectively.

### Mechanism 3: Mixed-Agent Strategy for Robust Data Distribution
A training dataset that combines samples from expert, random, and maximum-entropy agents yields a more robust and general-purpose surrogate than any single-agent strategy. The mixed strategy creates a dataset that balances high-density, task-relevant states (from the expert agent), broad baseline coverage (from the random agent), and directed exploration of uncertain regions (from the MEA). The core assumption is that the optimal training distribution for a general surrogate is a mixture of on-policy and exploratory data, which can be effectively constructed by simply concatenating datasets from distinct, pre-trained policies. Evidence is from the abstract and section 3.3, with weak direct corpus validation. The simple concatenation of datasets may be suboptimal if the state distributions of the agents are extremely disjoint or if one agent's data dominates due to sample size.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) and State Transitions**
  - **Why needed here:** The surrogate model is defined as a function that predicts the next state given the current state and action within an MDP framework.
  - **Quick check question:** For a simple environment like CartPole, can you describe what constitutes a state, an action, and a state transition?

- **Concept: Surrogate Models and Function Approximation**
  - **Why needed here:** The core objective is to train a regression model to approximate a complex simulator, trading some accuracy for massive gains in computational speed.
  - **Quick check question:** What problem does a surrogate model solve in the context of optimizing a computationally expensive simulator?

- **Concept: Exploration vs. Exploitation in Reinforcement Learning**
  - **Why needed here:** The method's success hinges on using different agent policies—expert (exploitation), random, and maximum-entropy (directed exploration)—to build the training dataset.
  - **Quick check question:** Why might a policy trained to maximize state-visitation entropy be better for building a surrogate dataset than a policy trained solely to maximize task reward?

## Architecture Onboarding

- **Component map:**
  1. Environment/Simulator (MDP)
  2. Sampling Policies (RA, EA, MEA)
  3. Trajectory Collector (executes policies)
  4. Dataset Composer (creates MA, MPA datasets)
  5. Surrogate Model (XGBoost, ANN, GP)
  6. Evaluation Suite (tests R² on held-out data)

- **Critical path:**
  1. Define the MDP: Identify the state and action spaces of the target simulator.
  2. Acquire Policies: Train or obtain the three key policies (EA, MEA, RA).
  3. Generate Datasets: Run each policy to collect trajectory datasets; create MA and MPA.
  4. Train Surrogate: Train the chosen regression model on the composed datasets.
  5. Cross-Validate: Evaluate each trained surrogate on all test datasets to measure robustness.

- **Design tradeoffs:**
  - Model Choice: XGBoost was found to be effective and often superior to ANNs; Gaussian Processes fail in high-dimensional spaces.
  - Data Composition: Including MEA data is the most critical factor for robustness; MPA (without MEA) performed significantly worse.
  - Sampling Method: Generative methods are competitive for simple, low-dimensional environments but become inefficient compared to agent-based methods in high dimensions.

- **Failure signatures:**
  - High-dimensional collapse: Kriging and purely generative sampling yield negative R² scores on complex, high-dimensional environments.
  - Poor trajectory generalization: Surrogates trained on uniform/generative data may show decent overall R² but fail on realistic agent trajectories.
  - Lack of robustness: Surrogates trained solely on expert data will likely overfit to optimal trajectories and fail to predict transitions from exploratory or random policies.

- **First 3 experiments:**
  1. Reproduce the MEA Ablation: On a high-dimensional environment (e.g., Ant), train two surrogates (MA vs. MPA) and evaluate both on a comprehensive test set. MA should show superior and more consistent R² scores.
  2. Dimensional Scaling Analysis: Train surrogates using Sobol (generative) and MA (agent-based) on both CartPole (simple) and HalfCheetah (complex). Verify agent-based sampling matches/exceeds generative in low dimensions and is far superior in high dimensions.
  3. Downstream RL Validation: Use the best-performing surrogate (trained on MA data) as a simulated environment for training a new RL policy. Compare the final performance of this policy when transferred to the real simulator.

## Open Questions the Paper Calls Out
- Can the agent-based sampling methodology be effectively extended to discrete state and action spaces? (The conclusion states this may be extended but is left to prove.)
- Is the achieved single-step prediction accuracy sufficient for long-horizon surrogate-aided Reinforcement Learning? (The authors note that successive steps may compound error, turning the visible environment into pure noise after few steps.)
- How does the method perform in stochastic environments where state transitions are probabilistic rather than deterministic? (The method definition explicitly excludes stochasticity.)

## Limitations
- The approach depends on the availability and effective training of diverse RL agents (random, expert, entropy-maximizing), which may not be feasible in all target environments.
- The maximum-entropy agent (MEA) relies on k-NN state entropy estimation, which can be computationally expensive and sensitive to parameter choices, potentially leading to ineffective exploration or noisy data.
- The study only considers deterministic simulators; the approach's effectiveness on stochastic environments with inherent transition uncertainty is unclear.
- The paper does not address potential covariate shift between training (agent-based) and testing (real-world) distributions, which could degrade surrogate accuracy.

## Confidence
- **High confidence:** XGBoost outperforms ANNs and Gaussian Processes for surrogate modeling in this context (well-supported by experimental results).
- **Medium confidence:** Agent-based sampling (especially with MEA) outperforms generative sampling in high-dimensional spaces (strongly supported, but limited direct corpus evidence for MEA as a sampling strategy).
- **Medium confidence:** The mixed-agent strategy (MA) is superior to any single-agent strategy (supported by the ablation study, but the specific composition's optimality is not rigorously tested).

## Next Checks
1. **MEA Ablation in High Dimensions:** Reproduce the experiment comparing the mixed dataset (MA) to the ablated version without MEA (MPA) on a complex, high-dimensional environment (e.g., Mujoco Ant). Verify that MA consistently achieves higher and more stable R² scores.
2. **Stochastic Environment Test:** Apply the agent-based sampling methodology to a stochastic simulator. Compare the surrogate's performance to the deterministic case to assess robustness to transition noise.
3. **Downstream RL Policy Transfer:** Use the best surrogate model (trained on MA data) as a simulated environment to train a new RL policy. Evaluate the final performance of this policy when transferred to the real simulator to directly test the paper's goal of enabling "surrogate-aided RL."