---
ver: rpa2
title: 'Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance
  Language Models'
arxiv_id: '2506.11116'
source_url: https://arxiv.org/abs/2506.11116
tags:
- dataset
- instruction
- data
- foundational
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of limited generalization and domain
  concentration in open-source instruction datasets for large language models (LLMs),
  which creates a performance gap compared to proprietary models. The authors introduce
  Infinity-Instruct, a scalable pipeline that constructs high-quality instruction
  datasets to enhance both foundational and conversational capabilities of LLMs.
---

# Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models

## Quick Facts
- **arXiv ID**: 2506.11116
- **Source URL**: https://arxiv.org/abs/2506.11116
- **Reference count**: 36
- **Key outcome**: Infinity-Instruct pipeline constructs 7.4M foundational + 1.5M conversational instructions; fine-tuning open-source models (Mistral, LLaMA, Qwen, Yi) on these datasets consistently outperforms official instruction-tuned counterparts, with InfInstruct-Llama3.1-70B surpassing GPT-4-0314 by 8.6% on instruction following tasks while achieving comparable foundational performance.

## Executive Summary
This paper addresses the persistent performance gap between open-source and proprietary large language models by introducing Infinity-Instruct, a scalable pipeline for constructing high-quality instruction datasets. The authors propose a two-phase approach: first curating a foundational dataset (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques, then synthesizing a conversational dataset (InfInstruct-G-1.5M) through iterative instruction labeling, evolution, and diagnostic filtering. Two-stage training (foundational first, conversational second) on several open-source models consistently achieves state-of-the-art performance, with Llama3.1-70B surpassing GPT-4-0314 on instruction following while maintaining strong foundational capabilities.

## Method Summary
The Infinity-Instruct pipeline addresses limited generalization in open-source instruction datasets through a two-phase construction process. Phase 1 (Data Selection) curates 7.4M foundational instructions from 100M+ samples using hybrid techniques: source filtering, rule-based filtering, and DSIR (Data Selection via Importance Resampling) targeting task-specific distributions (e.g., GSM8K/MATH for math, HumanEval for code). Phase 2 (Data Synthesis) creates 1.5M conversational instructions through two-layer hierarchical labeling (26 first-level, 15K+ second-level labels), seed selection based on difficulty/diversity, instruction evolution via four strategies, and diagnostic filtering using GPT-4 evaluation of model weaknesses. The two-stage training regimen first fine-tunes on foundational data, then on conversational data with replay of 1.2M seed instructions to prevent catastrophic forgetting.

## Key Results
- Fine-tuning open-source models (Mistral, LLaMA, Qwen, Yi) on Infinity-Instruct consistently outperforms official instruction-tuned counterparts
- Two-stage training achieves better trade-offs than one-stage: +14% conversational gain (25.5 vs 22.3) while maintaining foundational performance (52.0 vs 50.9)
- InfInstruct-Llama3.1-70B surpasses GPT-4-0314 by 8.6% on instruction following tasks while achieving comparable foundational performance
- Infinity-Instruct-InfiniChat achieves an Arena-Hard score of 56.2, approaching GPT-4o's 63.8

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid data selection from large pools (100M+ samples) yields higher-quality foundational instruction data than heuristic combination.
- **Mechanism:** Source filtering → rule-based filtering → DSIR importance resampling targets task-specific distributions (e.g., GSM8K/MATH prompts for math, HumanEval for code). This prioritizes samples matching target task characteristics while discarding low-quality/augmented duplicates.
- **Core assumption:** Target benchmark distributions (GSM8K, MATH, HumanEval) adequately represent desired capabilities; DSIR importance weights generalize beyond these proxies.
- **Evidence anchors:**
  - [abstract] "curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques"
  - [section 2.3] "We introduce three selection strategies such as source filtering, rule-based filtering, and DSIR... base the importance resampling process on prompt distributions obtained from HumanEval samples"
  - [corpus] TAG-INSTRUCT paper addresses controlled instruction complexity but lacks hybrid selection validation at this scale.
- **Break condition:** If DSIR-selected data distribution diverges significantly from real-world task distributions, or if filtering rules inadvertently exclude high-value edge cases (e.g., creative writing, low-resource domains), foundational gains may not transfer.

### Mechanism 2
- **Claim:** Two-layer instruction labeling + iterative evolution creates diverse, difficult conversational data that addresses model weaknesses.
- **Mechanism:** (1) LLM labels instructions hierarchically (26 first-level, 15K+ second-level labels), (2) diversity filtering retains long-tail and multi-capability instructions, (3) difficulty filtering via high language modeling loss, (4) evolution rewrites using four strategies, (5) diagnostic feedback identifies weak capabilities → targeted supplementation.
- **Core assumption:** Higher pre-training loss on an instruction indicates genuine difficulty/learning value (not noise); evolution preserves semantic integrity while increasing complexity.
- **Evidence anchors:**
  - [abstract] "synthesize 1.5M high-quality chat instructions through a two-stage process involving instruction selection, evolution, and diagnostic filtering"
  - [section 2.4] "final labeling system we constructed contains 26 first-level labels and more than fifteen thousand second-level labels... applied GPT-4 to evaluate the quality of responses from multiple open-source models"
  - [corpus] Corpus evidence on evolution-based synthesis is present but weak on rigorous semantic preservation validation.
- **Break condition:** If evolution introduces factual errors, harmful content, or semantic drift despite quality checks; if diagnostic filtering overfits to specific model weaknesses rather than generalizable gaps.

### Mechanism 3
- **Claim:** Two-stage training (foundational first, conversational second) achieves better trade-offs than mixed one-stage training.
- **Mechanism:** Foundational dataset establishes broad capabilities; conversational dataset builds on this base without catastrophic forgetting. Curriculum-like ordering preserves reasoning/knowledge while optimizing instruction-following.
- **Core assumption:** Foundational and conversational capabilities are positively correlated and synergistic rather than competing; sequential training avoids interference that would occur with simultaneous mixed training.
- **Evidence anchors:**
  - [abstract] "underscore the synergy between foundational and chat training"
  - [section 4/Table 8] "Two-Stage 25.5 [conversational], 52.0 [foundational] vs One-Stage 22.3, 50.9... simply merging datasets and doing an end-to-end training does not lead to higher conversational ability or foundational ability"
  - [corpus] No direct corpus validation of two-stage vs one-stage comparison beyond this paper.
- **Break condition:** If conversational data volume overwhelms foundational gains in second stage, or if replay strategy (1.2M seeds added to foundational) is insufficient to prevent forgetting.

## Foundational Learning

- **Concept:** DSIR (Data Selection via Importance Resampling)
  - **Why needed here:** Core to Phase 1 math/code selection—matches raw data to target distributions without manual curation.
  - **Quick check question:** Can you explain how importance resampling differs from simple heuristic filtering?

- **Concept:** Instruction Evolution (e.g., WizardLM-style augmentation)
  - **Why needed here:** Drives Phase 2 synthesis—multi-step rewriting increases difficulty while preserving semantics.
  - **Quick check question:** What mechanisms prevent evolved instructions from drifting into incoherence or factual errors?

- **Concept:** Curriculum Learning for LLM Fine-tuning
  - **Why needed here:** Justifies two-stage training order—foundational before conversational optimizes both capability types.
  - **Quick check question:** Why might mixing all data types from the start lead to suboptimal outcomes?

## Architecture Onboarding

- **Component map:**
  [100M+ Raw Instructions] → [Phase 1: Data Selection] → [InfInstruct-F-7.4M] → [+1.2M seed replay instructions] → [Phase 2: Data Synthesis] → [InfInstruct-G-1.5M] → [Two-Stage Training] → [Final Model]

- **Critical path:** DSIR selection quality → labeling system coverage → evolution semantic preservation → two-stage ordering. Errors in DSIR targeting or label taxonomy propagate through entire pipeline.

- **Design tradeoffs:**
  - **Scale vs. quality:** 100M → 7.4M (93% reduction) bets that aggressive filtering improves outcomes; may exclude useful edge cases.
  - **Complexity vs. reliability:** 15K second-level labels enable fine-grained diversity but increase labeling error risk.
  - **Two-stage vs. one-stage:** +14% conversational gain over one-stage (Table 8) but doubles training runs.

- **Failure signatures:**
  - **Foundational degradation after Stage 2:** Suggests replay ratio (1.2M/7.4M=16%) insufficient; increase or add interleaved foundational samples.
  - **Conversational overfitting to benchmarks:** Check evolution diversity—may have collapsed to benchmark-like patterns.
  - **Low coverage in label analysis:** If certain second-level labels have <100 samples, diagnostic filtering will be unreliable for those capabilities.

- **First 3 experiments:**
  1. **Ablate DSIR vs. random selection** on math/code subsets (1M samples each) using Mistral-7B; measure GSM8K/MATH/HumanEval deltas to validate selection efficiency.
  2. **Probe evolution semantic drift:** Manually annotate 100 evolved instructions for factual accuracy and semantic equivalence; if error rate >5%, add stronger validation model.
  3. **Vary replay ratio** (0%, 10%, 16%, 25%) in Stage 2 training; plot foundational vs. conversational trade-off curves to find optimal balance point.

## Open Questions the Paper Calls Out

- **Question:** To what extent do biases inherent in the open-source labeling models (e.g., Qwen1.5-72B) propagate into the final Infinity-Instruct dataset?
  - **Basis in paper:** [explicit] Section E states the labeling process "relies on existing open-source models, which may introduce bias or miss nuanced instruction types."
  - **Why unresolved:** The study focuses on benchmark performance improvements but does not quantify or audit the specific biases transferred from the teacher models to the dataset.
  - **What evidence would resolve it:** A comparative bias analysis of the curated dataset against the specific open-source models used for labeling and selection.

- **Question:** How does fine-tuning with Infinity-Instruct impact the safety and real-world robustness of LLMs?
  - **Basis in paper:** [explicit] Section E notes that "real-world robustness, safety... warrant further investigation" as the evaluation primarily focuses on benchmark performance.
  - **Why unresolved:** The paper reports high scores on capability benchmarks (e.g., MMLU, MT-Bench) but lacks evaluation on safety benchmarks or adversarial robustness.
  - **What evidence would resolve it:** Evaluation results from safety-specific benchmarks (e.g., TruthfulQA) and red-teaming exercises on models fine-tuned with Infinity-Instruct.

- **Question:** Does the proposed two-stage training pipeline ensure the long-term retention of capabilities across diverse domains?
  - **Basis in paper:** [explicit] Section E lists "long-term retention of capabilities across domains" as a limitation requiring further investigation.
  - **Why unresolved:** The reported results measure model performance immediately after fine-tuning, without assessing how well capabilities are maintained over time or under distribution shift.
  - **What evidence would resolve it:** Longitudinal evaluation results showing model performance stability on foundational and conversational tasks over extended periods or subsequent training steps.

## Limitations

- **Unknown 1:** Exact Data Selection Code. The precise rules, target distributions for importance resampling, and the implementation for "weak-domain supplement" are not detailed enough to recreate the 7.4M dataset from the 100M source pool without the provided codebase.
- **Unknown 2:** Prompts for Synthesis and Labeling. The specific prompts used for the two-layer instruction labeling with Qwen1.5-72B and the "evolve-instruct" rewriting steps are not included in the paper text, preventing reproduction of the synthesis pipeline from scratch.
- **Major Uncertainties:** Reliance on proprietary evaluation tools (GPT-4 for diagnostic filtering, LMSYS benchmarks) that cannot be independently verified; 15K+ second-level labels create risk of sparse coverage in some capability areas.

## Confidence

- **High Confidence**: The foundational dataset curation pipeline (DSIR + rule-based filtering) is technically sound and the reported benchmark improvements on established metrics (MATH, HumanEval, MMLU) are verifiable through public evaluation frameworks.
- **Medium Confidence**: The conversational dataset synthesis methodology is innovative but depends on subjective quality judgments via GPT-4 evaluation. The claimed 8.6% GPT-4-0314 performance gap, while impressive, cannot be fully validated without access to the exact evaluation setup.
- **Low Confidence**: The scalability claims (processing 100M+ samples) and the assertion that two-stage training is optimal over alternatives are based on ablation studies within this work but lack external validation or comparison to state-of-the-art continual learning methods.

## Next Checks

1. **Ablate DSIR vs. random selection** on math/code subsets (1M samples each) using Mistral-7B; measure GSM8K/MATH/HumanEval deltas to validate selection efficiency.
2. **Probe evolution semantic drift:** Manually annotate 100 evolved instructions for factual accuracy and semantic equivalence; if error rate >5%, add stronger validation model.
3. **Vary replay ratio** (0%, 10%, 16%, 25%) in Stage 2 training; plot foundational vs. conversational trade-off curves to find optimal balance point.