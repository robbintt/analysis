---
ver: rpa2
title: 'The Illusion of Certainty: Uncertainty Quantification for LLMs Fails under
  Ambiguity'
arxiv_id: '2511.04418'
source_url: https://arxiv.org/abs/2511.04418
tags:
- uncertainty
- epistemic
- entropy
- aleatoric
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing uncertainty quantification (UQ) methods for large language
  models (LLMs) are typically benchmarked on tasks with no inherent ambiguity, yet
  real-world language is often ambiguous. We show that these methods degrade to near-random
  performance when evaluated on ambiguous question-answering tasks.
---

# The Illusion of Certainty: Uncertainty Quantification for LLMs Fails under Ambiguity

## Quick Facts
- arXiv ID: 2511.04418
- Source URL: https://arxiv.org/abs/2511.04418
- Authors: Tim Tomov; Dominik Fuchsgruber; Tom Wollschläger; Stephan Günnemann
- Reference count: 40
- Primary result: Existing uncertainty quantification methods degrade to near-random performance on ambiguous question-answering tasks.

## Executive Summary
Current uncertainty quantification (UQ) methods for large language models (LLMs) are typically benchmarked on unambiguous tasks, yet real-world language often contains inherent ambiguity. This paper demonstrates that these methods fail catastrophically when evaluated on ambiguous question-answering tasks. Through theoretical analysis and empirical evaluation, the authors show that both predictive-distribution and ensemble-based UQ estimators cannot reliably distinguish epistemic uncertainty from aleatoric uncertainty when ambiguity is present. The study introduces MAQA* and AmbigQA*, the first ambiguous QA datasets with ground-truth answer distributions estimated from factual co-occurrence, providing a new benchmark for evaluating UQ methods under realistic conditions.

## Method Summary
The study evaluates uncertainty quantification methods on ambiguous question-answering tasks by comparing performance between unambiguous datasets (TriviaQA) and newly created ambiguous datasets (MAQA* and AmbigQA*) with ground-truth answer distributions estimated from Wikipedia co-occurrence. Three families of estimators are tested: consistency-based methods (semantic entropy, MSP, SAR, iterative prompting), internal representation probes (linear and MLP probes on residual stream activations), and ensemble methods (mutual information across three architectures). The key metric is concordance AUCc, measuring the probability that an estimator correctly ranks samples by true epistemic uncertainty, which is computed as KL-divergence between ground-truth and predicted answer distributions.

## Key Results
- Concordance scores drop significantly on ambiguous datasets compared to unambiguous ones across all tested models and estimators.
- All UQ methods degrade to near-random performance (AUC_c ≈ 0.5) on MAQA* and AmbigQA* datasets.
- Entropy collapse occurs in instruct models, preventing meaningful uncertainty quantification even for ambiguous questions.
- Theoretical analysis reveals fundamental limitations: consistency-based and ensemble-based estimators cannot disentangle epistemic from aleatoric uncertainty when relying solely on predictive distributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predictive entropy reliably estimates epistemic uncertainty only when aleatoric uncertainty is zero (H(p*) = 0).
- Mechanism: With H(p*) = 0, the ground-truth distribution is a point mass at a simplex vertex. This geometry enforces two bounds: (1) High predictive entropy H(p) ≥ δ forces a flat distribution, guaranteeing high epistemic uncertainty EU = -log p(y*) ≥ -log α_δ (Theorem 3.1). (2) Low entropy H(p) ≤ δ concentrates probability mass, making low EU probable for well-trained models (Theorem 3.2). These bounds create a forced correlation.
- Core assumption: The model is well-optimized with low average loss, so confident predictions tend to be correct.
- Evidence anchors:
  - [abstract] "We show that these methods degrade to close-to-random performance when evaluated on ambiguous question-answering tasks."
  - [Section 3.1.1] "Theorem 3.1 shows that high predictive entropy necessarily implies high epistemic uncertainty."
  - [corpus] Credal Transformers address "Artificial Certainty" from Softmax collapse, aligning with the idea that uncertainty estimates break without ambiguity constraints.
- Break condition: When H(p*) > 0, the vertex constraint vanishes. The model can perfectly match p* with high entropy, yielding zero EU despite high predictive uncertainty.

### Mechanism 2
- Claim: Ensemble-based mutual information (MI) tracks epistemic uncertainty only under zero aleatoric uncertainty.
- Mechanism: MI is bounded by ensemble-average entropy H(p̅) (Eq. 2). Under zero AU, high MI implies high H(p̅), which forces high EU via Theorem 3.1. Low MI, with accurate individual predictors, implies low EU with high probability via Theorem 3.2. The deterministic lower bound and probabilistic upper bound create correlation.
- Core assumption: Ensemble members are posterior samples with low expected error, and their average prediction is accurate.
- Evidence anchors:
  - [abstract] "concordance scores drop significantly on ambiguous datasets compared to unambiguous ones, confirming the theoretical findings."
  - [Section 3.1.2] "large mutual information necessarily implies a high true epistemic uncertainty."
  - [corpus] Corpus papers do not directly validate ensemble UQ under ambiguity; external evidence is weak.
- Break condition: When p* is unconstrained, high MI can coexist with zero EU (e.g., ensemble members disagree but average perfectly matches p*, Proposition 3.4).

### Mechanism 3
- Claim: Consistency-based and ensemble-based estimators cannot disentangle epistemic from aleatoric uncertainty when ambiguity is present.
- Mechanism: Given only the predictive distribution p, any function f(p) yields identical outputs for different ground truths (Proposition 3.3). For a single p, p*₁ = p gives EU = 0, while p*₂ as a point mass on a low-probability class gives high EU. Without knowledge of p*, f(p) cannot attribute uncertainty to its source. This non-identifiability is fundamental.
- Core assumption: Estimators rely solely on p (or its functions) without access to external p* information.
- Evidence anchors:
  - [abstract] "Our theoretical analysis reveals that both predictive-distribution and ensemble-based UQ estimators are fundamentally limited under ambiguity."
  - [Section 3.2.1] Proposition 3.3 formalizes the inability to distinguish EU from AU using only p.
  - [corpus] Related work on textual ambiguity (e.g., Chinese textual ambiguity) identifies fragility but does not propose general disentanglement, supporting novelty.
- Break condition: If p* is known (e.g., via co-occurrence statistics), EU can be directly computed as KL(p* || p), bypassing the identifiability barrier.

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper hinges on the decomposition TU = AU + EU (Eq. 1). Aleatoric uncertainty (H(p*)) is irreducible ambiguity; epistemic uncertainty (KL(p* || p)) reflects model error. Without this distinction, UQ methods conflate the two.
  - Quick check question: For "What medication for type 2 diabetes?", does high entropy in the model's answer distribution indicate the model is untrained or that multiple answers are correct?

- Concept: **Semantic Equivalence Classes**
  - Why needed here: Uncertainty estimation requires clustering syntactically different answers into semantic classes (e.g., "Paris" and "The capital is Paris") to compute a meaningful distribution p.
  - Quick check question: Why would computing entropy over raw token sequences fail to capture uncertainty in open-ended generation?

- Concept: **Mutual Information as Epistemic Proxy**
  - Why needed here: MI(Y; θ) = H(p̅) - E[H(pθ)] measures disagreement among ensemble members, interpreted as epistemic uncertainty in Bayesian deep learning.
  - Quick check question: If all ensemble members output identical distributions, what does MI equal, and what does this imply about epistemic uncertainty?

## Architecture Onboarding

- Component map: Dataset Construction -> UQ Estimators -> Evaluation
- Critical path: Ambiguous question → co-occurrence search → p* → model generates answers → semantic clustering → p → estimator computes uncertainty score → score compared to true EU. Any failure in p* estimation or semantic clustering corrupts the signal.
- Design tradeoffs:
  - **Wikipedia vs. larger corpora**: Wikipedia with entailment checking offers higher precision but lower coverage than RedPajama/The Pile. The paper validates robustness via JS divergence (mean < 0.1, Figure 2b).
  - **Dirichlet perturbation**: Simulating uncertainty in p* via Dirichlet posteriors tests robustness but may introduce noise if counts are low (γ parameter controls strength, Table 3).
  - **Probe complexity**: Linear probes are simpler but may underfit; 2-layer MLPs capture non-linearities but risk overfitting on limited ambiguous data.
- Failure signatures:
  - **Entropy collapse in instruct models**: Instruct variants output near-zero entropy even for ambiguous questions (Figure 5), eliminating signal.
  - **Near-random concordance (AUC_c ≈ 0.5)**: Indicates estimator fails to rank EU correctly; observed for all methods on MAQA*/AmbigQA* (Table 2).
  - **Anti-correlation at high AU**: For high H(p*), estimated uncertainty negatively correlates with true EU (Figure 3), indicating pathological inversion.
- First 3 experiments:
  1. **Validate p* robustness**: Compute Jensen-Shannon divergence between p* estimates from Wikipedia, RedPajama, and The Pile. Expect low divergence (mean ~0.05-0.1) to confirm consistency.
  2. **Probe layer-wise performance**: Train regression/classification probes on residual stream activations across layers. Expect performance collapse on ambiguous datasets (Figures 6, 7).
  3. **Measure estimator degradation vs. AU**: Bin samples by H(p*) and plot AUC_c for each estimator. Expect monotonic decline with potential anti-correlation at high AU (Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can second-order distribution methods (e.g., evidential deep learning) theoretically overcome the non-identifiability of epistemic uncertainty established in Proposition 3.3?
- Basis in paper: [explicit] The authors state the work "motivates a paradigm-shift toward actively modeling uncertainty during model training" and references evidential deep learning as a potential direction.
- Why unresolved: The paper proves that post-hoc estimators relying on predictive distributions $p$ cannot disentangle aleatoric and epistemic uncertainty, but does not verify if training-time interventions can bypass this theoretical limit.
- What evidence would resolve it: A theoretical proof or empirical demonstration showing that a model trained to predict a distribution over distributions can distinguish $p^* = p$ (low EU) from high EU scenarios where $p$ is identical.

### Open Question 2
- Question: Can instruction-tuning be modified to preserve the model's ability to represent diverse answer distributions under ambiguity?
- Basis in paper: [inferred] The paper identifies that "entropy... collapses to zero" in instruct models, a behavior described as "undesirable" because it prevents the representation of meaningful predictive distributions required for UQ.
- Why unresolved: The study evaluates existing base and instruct models but does not propose or test methods to prevent this collapse while maintaining the utility of instruction-following.
- What evidence would resolve it: An instruct-tuning procedure that retains high entropy on ambiguous ground-truth distributions (like MAQA*) without degrading performance on unambiguous tasks.

### Open Question 3
- Question: Do internal representations fail to capture epistemic uncertainty under ambiguity due to the same theoretical identifiability limits as consistency-based methods?
- Basis in paper: [inferred] The authors note their theoretical analysis "encapsulates consistency-based and ensemble-based approaches while for internal representations our claims are only backed empirically."
- Why unresolved: While empirical results show internal representation probes fail on ambiguous data, there is no theoretical framework explaining if this is due to the information bottleneck of the predictive distribution or a failure of the probe itself.
- What evidence would resolve it: A theoretical extension of Proposition 3.3 (Non-Identifiability) applying to internal hidden states, or conversely, a probe architecture that succeeds where predictive entropy fails.

### Open Question 4
- Question: Do LLMs actually converge to the corpus co-occurrence frequencies used to define the ground-truth distribution $p^*$?
- Basis in paper: [inferred] The paper estimates ground truth $p^*$ using factual co-occurrence but notes in the Limitations section that "it has not been shown that LLMs approach this distribution in the infinite data limit."
- Why unresolved: The method assumes a frequentist view where pre-training frequency equals the model's internal prior, but this link is supported only by correlation with model performance, not direct verification of the distribution shape.
- What evidence would resolve it: Analysis comparing the model's predictive distribution on ambiguous facts against their exact occurrence counts in the pre-training corpus (e.g., The Pile) to verify calibration.

## Limitations
- The theoretical analysis relies on simplified Bayesian framework assumptions that may not fully capture practical LLM behavior.
- Ground-truth answer distributions are estimated from Wikipedia co-occurrence rather than true human response distributions.
- Results focus on QA tasks and may not generalize to other LLM applications with different ambiguity manifestations.

## Confidence
- **High Confidence**: The empirical finding that concordance scores degrade to near-random on ambiguous datasets (AUC_c ≈ 0.5) is robustly supported by Table 2 across multiple models and estimators.
- **Medium Confidence**: The theoretical claim that UQ estimators cannot disentangle epistemic from aleatoric uncertainty under ambiguity is logically sound but relies on idealized assumptions that may not hold for practical LLM implementations.
- **Medium Confidence**: The claim that entropy collapse in instruct models eliminates UQ signal is supported by Figure 5 but requires further investigation across different instruction-tuning approaches.

## Next Checks
1. **External Corpus Validation**: Validate p* robustness by computing Jensen-Shannon divergence between p* estimates from Wikipedia, RedPajama, and The Pile across all samples. Confirm that mean divergence remains below 0.1 and check for systematic differences.
2. **Probe Architecture Sensitivity**: Compare linear vs. 2-layer MLP probe performance across layers on MAQA*/AmbigQA*. Test whether deeper probes capture non-linear uncertainty patterns that linear probes miss, particularly for ambiguous samples.
3. **Estimator Calibration Under Ambiguity**: Bin samples by H(p*) and compute calibration curves (reliability diagrams) for each estimator. Test whether high estimated uncertainty actually corresponds to high true EU at different AU levels, revealing potential pathological behaviors.