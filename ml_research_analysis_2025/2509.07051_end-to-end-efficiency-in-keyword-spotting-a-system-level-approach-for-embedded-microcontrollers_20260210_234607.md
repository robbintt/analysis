---
ver: rpa2
title: 'End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded
  Microcontrollers'
arxiv_id: '2509.07051'
source_url: https://arxiv.org/abs/2509.07051
tags:
- keyword
- spotting
- energy
- memory
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates and compares state-of-the-art lightweight neural
  architectures for keyword spotting on embedded microcontrollers, including DS-CNN,
  LiCoNet, TENet, and a newly proposed TKWS model based on MobileNet. Unlike prior
  studies focusing only on inference, the evaluation encompasses the full processing
  pipeline from MFCC feature extraction to neural inference, across three STM32 platforms
  (N6, H7, and U5).
---

# End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers
## Quick Facts
- arXiv ID: 2509.07051
- Source URL: https://arxiv.org/abs/2509.07051
- Reference count: 22
- Primary result: TKWS with three residual blocks achieves 92.4% F1-score with 14.4k parameters on embedded microcontrollers

## Executive Summary
This study evaluates state-of-the-art lightweight neural architectures for keyword spotting (KWS) on embedded microcontrollers, considering the full processing pipeline from MFCC feature extraction to neural inference. The evaluation spans three STM32 platforms (N6, H7, U5) and compares DS-CNN, LiCoNet, TENet, and a newly proposed TKWS model based on MobileNet. The research demonstrates that model accuracy alone does not determine real-world effectiveness, as optimal deployments require careful consideration of feature extraction parameters and hardware-specific optimization. TKWS achieves state-of-the-art efficiency with minimal memory footprint while maintaining competitive accuracy.

## Method Summary
The evaluation encompasses the complete KWS pipeline on embedded hardware, including MFCC feature extraction, neural network inference, and post-processing. Four neural architectures are benchmarked: DS-CNN, LiCoNet, TENet, and TKWS (a MobileNet-based model). The study tests across three STM32 platforms with varying performance characteristics, measuring accuracy, memory usage, energy consumption, and latency. Feature extraction parameters are systematically varied to identify optimal configurations for each hardware platform.

## Key Results
- TKWS with three residual blocks achieves 92.4% F1-score using only 14.4k parameters
- STM32 N6 with integrated neural acceleration provides the best energy-delay product
- Optimal MFCC parameters significantly impact performance across different hardware platforms
- High-resolution features enable efficient, low-latency operation despite increased computational load

## Why This Works (Mechanism)
The system-level approach accounts for interactions between feature extraction, model architecture, and hardware acceleration. By optimizing the entire pipeline rather than individual components, the study identifies configurations that minimize resource usage while maintaining accuracy. The integration of neural acceleration in the N6 platform reduces energy consumption during inference, while careful MFCC parameter tuning ensures efficient feature representation for the target hardware.

## Foundational Learning
- MFCC (Mel-Frequency Cepstral Coefficients): Audio feature representation converting waveforms to frequency domain - needed for efficient neural processing of audio
- Neural acceleration: Hardware-specific optimizations for deep learning inference - needed to reduce energy consumption and latency
- Energy-delay product: Metric combining power consumption and processing time - needed to evaluate real-world efficiency

## Architecture Onboarding
- Component map: Audio input -> MFCC extraction -> Neural network -> Classification output
- Critical path: MFCC feature extraction -> Model inference -> Decision threshold
- Design tradeoffs: Model complexity vs. memory footprint vs. accuracy
- Failure signatures: Poor accuracy with high-resolution features indicates hardware bottleneck; low accuracy with low-resolution features indicates insufficient feature representation
- First experiments: 1) Test MFCC parameter sweep across platforms, 2) Measure energy consumption during idle vs. active inference, 3) Compare accuracy degradation with added background noise

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to STM32 hardware, lacking comparison with other MCU architectures
- Acoustic performance claims based on clean dataset without noise robustness validation
- Limited exploration of MFCC parameter space may miss optimal configurations

## Confidence
- High confidence in relative architecture rankings on STM32 N6
- Medium confidence in cross-platform energy-delay comparisons
- Low confidence in absolute acoustic performance without noise testing

## Next Checks
1. Replicate evaluation on non-ST microcontroller (e.g., NXP or Renesas) for hardware portability testing
2. Conduct acoustic tests with background noise and reverberation to validate robustness claims
3. Perform systematic MFCC parameter ablation study to identify broader optimal ranges