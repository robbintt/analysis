---
ver: rpa2
title: Saliency-driven Dynamic Token Pruning for Large Language Models
arxiv_id: '2504.04514'
source_url: https://arxiv.org/abs/2504.04514
tags:
- pruning
- token
- tokens
- sdtp
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of long-sequence
  inference in large language models (LLMs), particularly the quadratic complexity
  of the attention mechanism. The proposed Saliency-driven Dynamic Token Pruning (SDTP)
  framework dynamically prunes redundant tokens based on their saliency scores estimated
  by a lightweight prediction module inserted into different layers of the LLM.
---

# Saliency-driven Dynamic Token Pruning for Large Language Models

## Quick Facts
- **arXiv ID:** 2504.04514
- **Source URL:** https://arxiv.org/abs/2504.04514
- **Reference count:** 7
- **Primary result:** Dynamic token pruning framework achieving up to 47.2% FLOPs reduction, 1.75× speedup, and 34.26% memory savings on LLMs

## Executive Summary
This paper addresses the computational challenges of long-sequence inference in large language models by proposing Saliency-driven Dynamic Token Pruning (SDTP). The method dynamically prunes redundant tokens based on saliency scores predicted by lightweight MLPs inserted into transformer layers. A ranking-based optimization strategy minimizes divergence between predicted and gradient-based saliency scores. Experimental results demonstrate significant efficiency gains while maintaining comparable performance across various tasks and models including Llama2-7B, Mistral-7B, and BLOOM-7B.

## Method Summary
The SDTP framework inserts lightweight 2-layer MLP pruning modules at specific transformer layers (typically starting at layer 4 with a step of 3). During training, these modules learn to predict token importance scores using gradient-based saliency (computed via backpropagation) as ground truth. The modules generate binary masks through Gumbel-Softmax sampling to selectively prune tokens at multiple stages. The optimization combines cross-entropy loss with MSE and ranking losses to align predicted scores with true saliency. At inference, tokens are pruned based on their predicted importance while preserving the first few tokens and local context.

## Key Results
- Achieves 47.2% FLOPs reduction during prefill phase on 4K context
- Delivers 1.75× end-to-end speedup with minimal accuracy loss (<2% on most tasks)
- Reduces memory usage by 34.26% while maintaining model performance
- Compatible with KV cache compression for additional acceleration

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Supervised Saliency Prediction
The method uses gradient-based feature attribution (saliency) as ground truth to train lightweight MLPs to predict token importance during inference. The pruning module learns from $\hat{\pi} = \frac{\partial T(x)}{\partial x} \cdot x$ to generate importance scores $\pi$ without requiring gradient computation at inference time. This supervision ensures the model learns meaningful token importance rather than spurious correlations.

### Mechanism 2: Hierarchical Sparsity Propagation
Token importance is assumed to persist across layers, enabling progressive pruning at multiple stages. By removing redundant tokens early, the method reduces computation in subsequent layers. The geometric decay strategy for keeping ratios ensures conservative pruning that doesn't compromise critical information for later processing stages.

### Mechanism 3: Ranking-Based Optimization for Selection
The training objective includes a pairwise ranking loss alongside MSE to ensure the model learns the relative ordering of token importance rather than just absolute values. This is crucial because the goal is to select a discrete subset of tokens, making ordinal ranking more relevant than precise scalar estimation for the top-k selection process.

## Foundational Learning

- **Concept: Gradient-based Feature Attribution (Saliency)**
  - **Why needed here:** This is the core supervision signal. Understanding $\frac{\partial \text{Output}}{\partial \text{Input}}$ as token importance is essential to grasp what the pruning module is learning.
  - **Quick check question:** If the gradient of the loss with respect to a token's embedding is near zero, does the paper classify that token as salient or redundant?

- **Concept: Quadratic Complexity in Transformers**
  - **Why needed here:** The method targets prefill phase acceleration. Understanding $O(N^2)$ attention complexity explains why reducing sequence length yields significant FLOPs reduction.
  - **Quick check question:** Why does pruning tokens during the prefill phase offer distinct advantages over KV cache compression during decode?

- **Concept: Gumbel-Softmax Trick**
  - **Why needed here:** Used to generate differentiable binary masks for training. Understanding this approximation of discrete sampling is crucial for the training methodology.
  - **Quick check question:** Why can't we use standard `argmax` or hard thresholding to generate the mask $M$ during training?

## Architecture Onboarding

- **Component map:** Input Prompt → Initial Layers → SDTP Module → Compute Importance Scores → Gumbel-Softmax Mask → Pruned Sequence → Next Block
- **Critical path:** 1) Input prompt flows through initial layers, 2) Hidden states pass to SDTP module, 3) Module computes importance scores, 4) Gumbel-Softmax (training) or Top-k (inference) generates mask, 5) Mask applied to sequence, 6) Pruned sequence passes to next block
- **Design tradeoffs:** Module placement affects early vs late semantic preservation; overhead vs gain balance favors long-context scenarios; keeping first 4 tokens prevents attention sink collapse
- **Failure signatures:** Attention sink collapse if first tokens removed; long-context reasoning loss if over-pruning complex tasks; uniform importance distribution causing ranking instability
- **First 3 experiments:** 1) Visualize correlation between predicted and gradient saliency scores, 2) Retrain with only MSE loss (no ranking) on Dolly subset to measure selection accuracy delta, 3) Benchmark prefill latency across increasing context lengths (2k→16k) to validate speedup claims

## Open Questions the Paper Calls Out

- **Question 1:** Can SDTP maintain benefits when scaled to models significantly larger than 7B parameters or applied to MoE architectures?
- **Question 2:** Is the fixed geometric decay strategy for token keeping ratios optimal, or could performance improve with learned layer-specific sparsity ratios?
- **Question 3:** How does noise in gradient-based attribution affect pruning supervision reliability given the cited unreliability of saliency methods?

## Limitations
- Limited validation on complex multi-hop reasoning tasks where token persistence assumptions may fail
- Significant computational overhead for the one-time marking stage requiring full gradient computation
- Fixed architectural choices (module design, placement strategy) without systematic hyperparameter sensitivity analysis
- No exploration of compatibility with speculative decoding or other inference optimizations

## Confidence
- **High Confidence:** FLOPs reduction measurements (47.2%) and end-to-end speedup (1.75×) are well-supported by transparent experimental methodology
- **Medium Confidence:** Performance maintenance claims are supported but require careful interpretation given varying absolute performance across models and tasks
- **Low Confidence:** Scalability claims to larger models and longer contexts are extrapolated from 7B parameter experiments without empirical validation

## Next Checks
- **Check 1:** Validate on multi-hop reasoning tasks from LongBench to quantify over-pruning risk in complex reasoning scenarios
- **Check 2:** Measure marking stage overhead across different model sizes (7B→13B→34B) to assess practical scaling limits
- **Check 3:** Systematically vary pruning module architecture, placement, and keeping ratios to identify optimal configurations beyond default settings