---
ver: rpa2
title: 'Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across
  Modalities'
arxiv_id: '2505.17862'
source_url: https://arxiv.org/abs/2505.17862
tags:
- arxiv
- audio
- visual
- audio-visual
- daily-omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Daily-Omni, a new benchmark designed to evaluate
  multimodal large language models (MLLMs) on audio-visual reasoning in real-world
  scenarios. It addresses the gap in existing benchmarks that often focus on isolated
  visual or audio tasks and lack temporal alignment across modalities.
---

# Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities

## Quick Facts
- arXiv ID: 2505.17862
- Source URL: https://arxiv.org/abs/2505.17862
- Authors: Ziwei Zhou; Rui Wang; Zuxuan Wu
- Reference count: 40
- Primary result: Introduces Daily-Omni benchmark and training-free Daily-Omni Agent that achieves state-of-the-art open-source performance on audio-visual reasoning with temporal alignment

## Executive Summary
Daily-Omni addresses the gap in multimodal evaluation by introducing a benchmark specifically designed to test audio-visual reasoning with temporal alignment across modalities. The benchmark consists of 684 real-world videos and 1,197 multiple-choice QA pairs spanning six task types, from audio-visual event alignment to cross-modal reasoning. The paper presents a novel QA generation pipeline that combines automated annotation, revision, temporal alignment, and quality control to efficiently scale the dataset. Additionally, it introduces Daily-Omni Agent, a training-free baseline that achieves strong performance by combining visual, audio, and ASR models with intelligent temporal alignment techniques.

## Method Summary
The Daily-Omni pipeline begins by segmenting videos into 30-second or 60-second clips, then applies parallel visual and audio captioning using proprietary MLLMs (Gemini 2.0 Flash). These segment-level annotations undergo visual and audio revision steps to ensure consistency and quality. A novel QA generation process uses Deepseek-R1 to create questions from the annotations, followed by text-only filtering to ensure questions genuinely require multimodal reasoning. The Daily-Omni Agent inference pipeline divides input videos into three segments, generates parallel annotations using open-source models (Qwen2.5-VL-7B, Qwen2-Audio-7B, Whisper), and employs "Smart Alignment" - a question-guided approach that identifies relevant visual events, temporally grounds them, and retrieves aligned audio-visual context only when the event duration is short enough to be meaningful.

## Key Results
- Daily-Omni Agent achieves 61.82% accuracy with Smart Alignment, outperforming Naive (60.65%) and No Alignment (59.65%) approaches
- Current MLLMs struggle with audio-visual integration, highlighting the benchmark's challenge level
- Text-only filtering eliminated 47% of candidate questions, ensuring genuine multimodal dependency
- The benchmark spans six task types including audio-visual event alignment, reasoning, and anomaly detection

## Why This Works (Mechanism)

### Mechanism 1: Segment-Level Annotation Before Integration
- Claim: Splitting videos into shorter segments before annotation improves audio description quality, which subsequently improves cross-modal reasoning accuracy.
- Mechanism: MLLMs exhibit degraded performance on long audio clips. By dividing 30s/60s videos into 10s/20s segments, each segment receives higher-quality independent audio and visual annotations. These are then reconciled through a revision step that uses full-video context to ensure consistency.
- Core assumption: The quality loss from processing long audio exceeds the integration cost of merging segment-level annotations.
- Evidence anchors:
  - [Section 3.2]: "since MLLM performance is known to degrade when processing long audio clips, we segmented the videos prior to annotation"
  - [Section 3.2]: Visual revision step uses "complete video clip" to verify cross-segment consistency
  - [corpus]: DAVE benchmark identifies "visual bias" where answers are inferrable from visual data alone

### Mechanism 2: Question-Guided Event Alignment
- Claim: Rather than aligning all audio-visual events exhaustively, selectively aligning only question-relevant events improves downstream QA accuracy.
- Mechanism: The Daily-Omni Agent first prompts an LLM to identify which visual events are temporally critical for answering the given question. It then uses temporal grounding to find timestamps for those events, and retrieves audio descriptions only for those brief intervals.
- Core assumption: The VLM's temporal grounding accuracy is sufficient for identifying relevant windows; errors here propagate to misaligned audio.
- Evidence anchors:
  - [Section 3.3]: "we prompt Qwen2.5-14B-Instruct with the visual and audio annotations, the question, and its choices. The model's task is to identify a list of specific visual events whose temporal localization is deemed necessary"
  - [Table 3]: Smart Alignment achieves 61.82% accuracy vs. 60.65% (No Alignment) and 59.65% (Naive Alignment)

### Mechanism 3: Text-Only Filtering for Multimodal Requirement
- Claim: Filtering out questions that powerful text-only LLMs can answer ensures the benchmark genuinely requires audio-visual integration.
- Mechanism: After QA generation, GPT-4o and Deepseek-V3 are given only the question text and choices (no video/audio). Questions both models answer correctly are discarded (~47% of candidates).
- Core assumption: Current text-only LLMs' inability to answer a question implies genuine multimodal dependency.
- Evidence anchors:
  - [Section 3.2]: "Questions that could be answered correctly by both LLMs under this text-only condition were discarded, as they did not necessitate multimodal reasoning"
  - [Section 3.2]: 47% of candidate QAs discarded at this stage

## Foundational Learning

- Concept: **Temporal Grounding**
  - Why needed here: The entire Daily-Omni framework depends on mapping natural language descriptions to specific video timestamps. Without this, event alignment cannot function.
  - Quick check question: Given a video and the phrase "person opens door," can you identify the start and end times of this action?

- Concept: **Cross-Modal Positional Embeddings**
  - Why needed here: Paper notes that techniques like M-RoPE and TMRoPE attempt to encode temporal relationships across modalities, but current OLMs still struggle with temporal alignment tasks.
  - Quick check question: If audio events and visual frames have separate positional encodings, how does the model learn that "sound at t=5s" corresponds to "frame at t=5s"?

- Concept: **Benchmark Bias and Leakage**
  - Why needed here: The QA pipeline explicitly addresses visual bias (answers derivable from one modality) and text leakage (questions answerable without viewing).
  - Quick check question: If your benchmark's questions contain proper nouns mentioned in pre-training corpora, does this invalidate evaluation? How would you detect this?

## Architecture Onboarding

- Component map: Video source → Segmentation → Parallel VLM/ALM annotation → Visual revision (full video) → Audio revision (with visual context) → Event alignment → QA generation (Deepseek-R1) → QA optimization → Text-only filtering → Human QC

- Critical path: The temporal grounding step determines which audio segments are retrieved. Errors here cascade to final reasoning. The paper notes "temporal grounding process itself frequently yields imprecise results."

- Design tradeoffs:
  - **Naive vs. Smart Alignment**: Naive (extract events first, then retrieve audio) scored lower than Smart (identify relevant events first). Tradeoff is computational—Smart requires an additional LLM inference pass.
  - **Full event alignment vs. targeted alignment**: Paper chose targeted because full alignment "risks overwhelming the context capacity" of the 14B LLM.
  - **Open-source vs. proprietary annotators**: Pipeline uses Gemini 2.0 Flash (proprietary) for annotation but agent uses only open-source models.

- Failure signatures:
  - **Temporal grounding drift**: If grounded timestamps are off by >2s, retrieved audio describes wrong event. Ablation shows Naive Alignment performed worse than No Alignment.
  - **Segment boundary artifacts**: Events spanning segment boundaries may be split or duplicated.
  - **Audio annotation omission**: Qwen2-Audio "tends to omit" speech/singing; Whisper added as complement.

- First 3 experiments:
  1. **Reproduce alignment ablation**: Run Daily-Omni Agent with No/Naive/Smart alignment on a 50-video subset. Verify Smart outperforms others; measure average aligned event pairs per question.
  2. **Isolate temporal grounding errors**: Manually correct timestamps for 20 questions and measure accuracy improvement. This quantifies grounding's contribution.
  3. **Text-only baseline sanity check**: Run your own text-only LLM on the benchmark. If accuracy exceeds ~35%, investigate whether questions leak answerable information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a more powerful open-vocabulary video temporal grounding model significantly improve the Daily-Omni Agent's performance on audio-visual temporal alignment tasks?
- Basis in paper: [explicit] The authors hypothesize that "equipping the agent with a more powerful open-vocabulary video temporal grounding model would unlock further significant improvements in its performance, mitigating the impact of imprecise temporal grounding."
- Why unresolved: The current agent relies on Qwen2.5-VL-7B for temporal grounding, which the authors found to be insufficiently powerful for reliable event retrieval through single queries.
- What evidence would resolve it: Comparing agent performance when substituting Qwen2.5-VL-7B with a more capable temporal grounding model, demonstrating statistically significant accuracy improvements on temporal alignment tasks.

### Open Question 2
- Question: How can unified model architectures be designed to better capture temporal correlations between synchronized audio-visual streams compared to modular approaches?
- Basis in paper: [inferred] The paper notes that modular architectures "struggles to capture the crucial temporal correlations inherent in synchronized audio-visual streams" because "audio and visual inputs are encoded independently."
- Why unresolved: The Daily-Omni Agent achieves strong performance using a modular approach, but it remains unclear whether unified architectures could achieve comparable or better results with fewer computational overhead.
- What evidence would resolve it: Systematic comparison between unified models (like Qwen2.5-Omni) and modular approaches (like Daily-Omni Agent) on fine-grained temporal alignment tasks, with analysis of architectural components that enable better cross-modal temporal integration.

### Open Question 3
- Question: What training protocols or architectural modifications can effectively enhance cross-modal temporal awareness in omni-modal language models?
- Basis in paper: [explicit] The authors note that existing benchmarks offer "limited methodological guidance on enhancing model capabilities via explicit training protocols or architectural modifications."
- Why unresolved: Current OLMs, even with cross-modal positional embeddings, still struggle with temporally-sensitive tasks, suggesting current approaches are insufficient.
- What evidence would resolve it: Developing and testing specific training protocols (e.g., contrastive learning on temporally-aligned audio-visual pairs) or architectural modifications (e.g., cross-modal attention mechanisms with explicit temporal modeling) and demonstrating improved performance on Daily-Omni's temporal alignment and event sequence tasks.

## Limitations
- The evaluation framework relies heavily on proprietary models (Gemini 2.0 Flash, GPT-4o, Deepseek-V3) for annotation and filtering, raising concerns about reproducibility and bias propagation
- The 47% text-only filtering rate suggests substantial candidate loss during QA generation, though the paper doesn't quantify how many potentially valid questions were discarded
- The "Smart Alignment" mechanism depends on the VLM's ability to accurately ground events, which the authors note as a current weakness in the field

## Confidence
- **High confidence**: The benchmark construction methodology (segmentation, parallel annotation, revision steps) is clearly specified and reproducible
- **Medium confidence**: The claim that Daily-Omni addresses "audio-visual reasoning with temporal alignment" is well-supported, though the extent to which the benchmark genuinely requires cross-modal temporal reasoning versus visual-only reasoning is unclear
- **Medium confidence**: The superiority of Smart Alignment over Naive/No Alignment is demonstrated, but the absolute performance gap (61.82% vs 60.65% vs 59.65%) suggests this remains a challenging task
- **Low confidence**: The claim that current MLLMs "struggle with audio-visual integration" is somewhat self-fulfilling given the benchmark was designed to expose these weaknesses

## Next Checks
1. **Temporal grounding error analysis**: Manually correct timestamps for 20 benchmark questions and measure accuracy improvement to quantify grounding's contribution to final performance
2. **Reproduce alignment ablation**: Run Daily-Omni Agent with No/Naive/Smart alignment on a 50-video subset to verify reported performance differences and measure average aligned event pairs per question
3. **Text-only baseline sanity check**: Run your own text-only LLM on the benchmark. If accuracy exceeds ~35%, investigate whether questions leak answerable information through linguistic patterns or world knowledge