---
ver: rpa2
title: Latency and Token-Aware Test-Time Compute
arxiv_id: '2509.09864'
source_url: https://arxiv.org/abs/2509.09864
tags:
- strategy
- latency
- accuracy
- compute
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of selecting and allocating inference-time
  compute for large language models in a query-adaptive way. It formulates this as
  a utility optimization problem where the system selects a decoding strategy and
  computes the number of candidates or beam parameters on a per-query basis, balancing
  accuracy against both token usage and wall-clock latency.
---

# Latency and Token-Aware Test-Time Compute

## Quick Facts
- arXiv ID: 2509.09864
- Source URL: https://arxiv.org/abs/2509.09864
- Reference count: 8
- Primary result: Adaptive inference-time compute allocation using lightweight accuracy probes and precomputed costs achieves better accuracy-token-latency trade-offs than static scaling methods on math reasoning tasks.

## Executive Summary
This paper addresses the problem of selecting and allocating inference-time compute for large language models in a query-adaptive way. It formulates this as a utility optimization problem where the system selects a decoding strategy and computes the number of candidates or beam parameters on a per-query basis, balancing accuracy against both token usage and wall-clock latency. A lightweight accuracy probe estimates success probabilities, while average token and latency costs are precomputed from training data. Experiments on the NuminaMath-CoT benchmark show that the adaptive strategy consistently outperforms static inference-time scaling methods, achieving better accuracy-cost trade-offs and demonstrating that joint method selection and compute allocation improves efficiency, especially for latency-sensitive agentic workflows.

## Method Summary
The framework maximizes utility U_s(x) = a_s(x) - λT·Ts(x) - λL·Ls(x) over candidate strategies, where a_s(x) is predicted accuracy, Ts(x) and Ls(x) are token and latency costs, and λT, λL are penalty weights. A two-layer MLP accuracy probe estimates per-query, per-strategy success probabilities using query embeddings concatenated with strategy parameters. Average token and latency costs are precomputed per strategy from training runs and used as predicted costs. The system selects s* = argmax U_s(x) and executes via vLLM + Qwen2.5-Math-PRM-7B scoring.

## Key Results
- Adaptive strategy achieves favorable accuracy-cost trade-offs compared to static methods
- Pre-computed average costs closely match oracle (ground-truth) cost performance
- Lightweight accuracy probe effectively guides strategy selection on math reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Utility-Based Strategy Selection
- Claim: Jointly selecting decoding strategy and compute allocation on a per-query basis achieves better accuracy-cost trade-offs than static methods.
- Mechanism: The system maximizes utility U_s(x) = a_s(x) − λT·Ts(x) − λL·Ls(x) over all candidate strategies, balancing predicted accuracy against token and latency penalties. This enables routing easy queries to lightweight methods (small N, best-of-N) and hard queries to compute-intensive methods (beam search).
- Core assumption: Query difficulty varies sufficiently that adaptive allocation outperforms static strategies; penalty weights λT, λL meaningfully capture deployment constraints.
- Evidence anchors:
  - [abstract]: "Our framework explicitly incorporates both token cost and wall-clock latency... achieving favorable accuracy-cost trade-offs."
  - [section 2.3]: "s*(x) = arg max_{s∈S} Us(x)" formalizes per-query optimization.
  - [corpus]: arXiv:2506.12721 (bandit learning for TTS) supports adaptive allocation rationale; arXiv:2509.19645 emphasizes system-optimal over compute-optimal scaling.
- Break condition: If queries are uniformly difficult or penalty weights are misconfigured, utility optimization degrades to random or greedy selection.

### Mechanism 2: Lightweight Accuracy Probe
- Claim: A two-layer MLP can estimate per-query, per-strategy success probability well enough to guide allocation.
- Mechanism: Query embeddings (1536-dim Qwen or 768-dim BERT) are concatenated with strategy parameters (N, beam width, method type one-hot). The probe is trained on soft labels—empirical accuracy from repeated sampling—using binary cross-entropy, then calibrated via Platt scaling.
- Core assumption: Soft labels from repeated sampling provide stable supervision; the probe generalizes to unseen queries without severe distribution shift.
- Evidence anchors:
  - [section 2.4]: "We train a lightweight probe to estimate, for query x and strategy s, the probability of producing a correct answer."
  - [appendix A.1]: "The resulting dataset consists of tuples (s, x, features(s, x), â_s(x))" with soft labels.
  - [corpus]: Damani et al. (2024), cited in the paper, uses similar lightweight predictors for best-of-N routing.
- Break condition: If the probe is miscalibrated or queries drift from training distribution, strategy selection becomes unreliable.

### Mechanism 3: Pre-computed Average Cost Models
- Claim: Token and latency costs can be approximated using strategy-specific averages without per-query prediction.
- Mechanism: For each decoding strategy, average token count and wall-clock latency are pre-computed from training runs. At inference, these mean values serve as predicted costs in the utility function.
- Core assumption: Cost variance is dominated by strategy choice rather than query content.
- Evidence anchors:
  - [section 2.4]: "cost variation is dominated by the choice of strategy rather than the query."
  - [figures 7-8]: Adaptive strategy using predicted costs closely matches oracle (ground-truth) cost performance.
  - [corpus]: Limited direct evidence; related work (arXiv:2506.15707, arXiv:2512.02008) focuses on allocation mechanisms rather than cost modeling assumptions.
- Break condition: If certain queries produce anomalously long/short outputs for a given strategy, average costs misallocate compute.

## Foundational Learning

- Concept: Inference-time scaling strategies (best-of-N, majority voting, beam search)
  - Why needed here: The framework routes between these methods based on query difficulty and latency constraints.
  - Quick check question: Why does best-of-N have lower latency than beam search for the same number of candidates?

- Concept: Process Reward Models (PRMs)
  - Why needed here: Beam search requires step-wise scoring of partial solutions via a PRM.
  - Quick check question: How does a PRM differ from an outcome reward model, and why is it preferable for beam search?

- Concept: Multi-objective optimization with penalty weights
  - Why needed here: Utility function trades off accuracy, tokens, and latency via λT, λL.
  - Quick check question: If you increase λL while holding λT constant, what strategy shift do you expect?

## Architecture Onboarding

- Component map:
  - Accuracy Probe -> Strategy Selector -> Inference Engine
  - Accuracy Probe: 2-layer MLP (200-200-1), inputs = query embeddings + strategy params (N, beam width, method one-hot).
  - Strategy Selector: Discrete utility maximizer over candidate strategies.
  - Inference Engine: vLLM batch generation + Qwen2.5-Math-PRM-7B scoring.

- Critical path:
  1. Embed query → concatenate with each candidate strategy's params.
  2. Probe predicts â_s(x); cost tables provide T̂_s, L̂_s.
  3. Compute U_s(x) for all s; select s* = argmax.
  4. Execute s* via vLLM + PRM scoring.

- Design tradeoffs:
  - Embedding choice: Qwen (1536-dim, higher accuracy) vs. BERT (768-dim, slightly lower but still effective per Appendix A.3).
  - Strategy space size: More configs enable finer allocation but increase probe training data needs.
  - Cost granularity: Per-query cost prediction vs. averages—paper shows averages suffice for math reasoning.

- Failure signatures:
  - Probe miscalibration → systematic over/under-allocation.
  - Cost model breakdown on outlier queries (very long CoT or early termination).
  - Excessive latency penalty (high λL) avoiding useful strategies.

- First 3 experiments:
  1. Replicate accuracy-cost curves (Figure 1) on held-out NuminaMath-CoT to validate probe and cost models.
  2. Ablate the probe (replace with random selection) to quantify adaptive routing gains.
  3. Measure per-query token/latency variance per strategy to stress-test average-cost assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the adaptive framework maintain its efficacy when applied to open-ended domains such as dialogue and summarization, where accuracy is defined by human preference rather than exact match?
- Basis in paper: [explicit] The Discussion section states that "future work will evaluate broader domains such as coding and dialogue."
- Why unresolved: The current study is restricted to the NuminaMath-CoT benchmark, which relies on exact answer matching. It is unclear if the lightweight accuracy probe can reliably estimate utility for subjective tasks.
- What evidence would resolve it: Experimental results on dialogue or summarization benchmarks (e.g., MT-Bench) showing that the adaptive strategy outperforms static methods when using reward models aligned with human preferences.

### Open Question 2
- Question: Can improvements to the accuracy probe architecture close the performance gap between the adaptive strategy and the theoretical oracle?
- Basis in paper: [explicit] The authors identify "closing the remaining gap between the adaptive strategy and the oracle by improving the accuracy of the probe" as a promising direction.
- Why unresolved: The current approach uses a simple two-layer MLP probe which, while effective, does not perfectly predict per-query success probabilities, leading to sub-optimal routing decisions compared to an oracle.
- What evidence would resolve it: Demonstrating that a more complex estimator (e.g., a deeper network or transformer-based probe) significantly reduces the utility gap between the adaptive method and the oracle baseline.

### Open Question 3
- Question: How does the utility optimization framework perform when integrated with advanced tree-based inference strategies like Tree of Thoughts?
- Basis in paper: [explicit] The Methodology section notes that the framework "extends naturally to more advanced inference-time scaling techniques," but experiments are limited to Best-of-N, Majority Voting, and Beam Search.
- Why unresolved: Tree-based methods involve complex, non-linear dependencies between depth, branching, and latency that may not be captured by the current hyperparameter vector θ_m or the linear penalty formulation.
- What evidence would resolve it: Successful application of the framework to tree-search methods, showing that the system can dynamically select between flat (Best-of-N) and hierarchical (Tree of Thoughts) strategies based on latency constraints.

### Open Question 4
- Question: Is the assumption of using precomputed average costs robust enough for scenarios with high variance in output length, such as code generation?
- Basis in paper: [inferred] The paper assumes cost variation is dominated by strategy choice rather than the query, using fixed averages for T̂_s(x) and L̂_s(x). However, code generation or agentic tasks often exhibit high variance in solution length.
- Why unresolved: If specific queries require exceptionally long chains of thought (outliers), the precomputed mean latency may severely underestimate the actual wall-clock time, leading to poor utility optimization.
- What evidence would resolve it: An analysis of the adaptive strategy on a dataset with high output length variance, comparing the performance of mean-cost estimation against query-aware cost predictors.

## Limitations

- Probe generalization and distribution shift: The probe relies on soft labels from training queries and may degrade on unseen distributions.
- Cost model variance assumptions: Pre-computed average costs may misallocate compute for queries with anomalous output lengths.
- Hyperparameter sensitivity: The utility function's penalty weights are not extensively explored for different deployment constraints.

## Confidence

**High Confidence**:
- The framework's formulation of query-adaptive inference-time compute allocation as a utility optimization problem is sound and well-motivated.
- Empirical results on NuminaMath-CoT show that adaptive strategies consistently outperform static inference-time scaling methods in accuracy-cost trade-offs.

**Medium Confidence**:
- The lightweight accuracy probe, trained on soft labels from repeated sampling, is effective for guiding strategy selection within the tested distribution.
- Pre-computed average token and latency costs are sufficient for practical deployment, as evidenced by the close match between oracle and predicted cost performance.

**Low Confidence**:
- The probe's generalization to unseen query distributions and its robustness to distribution shift are not thoroughly validated.
- The impact of hyperparameter choices (e.g., λT, λL, probe architecture) on real-world performance is not explored in depth.

## Next Checks

1. **Probe generalization under distribution shift**: Evaluate the accuracy probe on a held-out set of queries drawn from a different distribution (e.g., other math benchmarks or natural language tasks) to quantify degradation in strategy selection accuracy.

2. **Ablation study with oracle accuracy**: Replace the probe with an oracle that always selects the best strategy per query. Compare the performance gap between oracle and probe-guided adaptive strategies to isolate the contribution of intelligent routing.

3. **Per-query cost variance analysis**: For each decoding strategy, measure the variance in token count and latency across a diverse set of queries. If variance is high, investigate whether per-query cost predictors (e.g., small regressors) improve allocation accuracy over strategy-wide averages.