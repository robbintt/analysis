---
ver: rpa2
title: 'Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier
  Vision-Language Models'
arxiv_id: '2501.14818'
source_url: https://arxiv.org/abs/2501.14818
tags:
- data
- arxiv
- dataset
- zhang
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Eagle 2, a vision-language model (VLM) that
  achieves state-of-the-art performance by focusing on data strategy, training recipe,
  and model architecture. They build a diverse and high-quality dataset with 180+
  sources, filter low-quality samples, and apply advanced data selection and augmentation
  techniques.
---

# Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models

## Quick Facts
- arXiv ID: 2501.14818
- Source URL: https://arxiv.org/abs/2501.14818
- Reference count: 40
- Eagle 2-9B achieves 73.5 average across 13 benchmarks, matching models up to 70B parameters

## Executive Summary
Eagle 2 is a vision-language model that achieves state-of-the-art performance through a data-centric approach. The authors build a diverse, large-scale dataset from 180+ sources, apply rigorous filtering, and employ a three-stage training strategy. Key innovations include a tiled mixture of vision encoders (SigLIP + ConvNeXt) and balanced data packing. Eagle 2-9B matches or surpasses much larger models while emphasizing transparency in methodology.

## Method Summary
Eagle 2 uses a three-stage training strategy with a tiled mixture of vision encoders. Stage-1 trains only the connector on 1.2M samples, Stage-1.5 trains the full model on 21.6M diverse samples, and Stage-2 trains on 4.6M high-quality subsets. The model uses SigLIP-400M and ConvNeXt-XXL encoders with dynamic tiling up to 12 tiles, processed through a 2-layer MLP connector into a Qwen2.5 LLM backbone. Balanced data packing and K-means clustering for subset selection are employed throughout.

## Key Results
- Eagle 2-9B achieves 73.5 average score across 13 benchmarks
- Matches or surpasses models up to 70B parameters in absolute performance
- Tiled MoVE improves OCRBench from 842 to 868 and InfoVQA from 73.3 to 77.2
- Three-stage training yields +4.8 average improvement over baseline two-stage approach

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Then-Quality Data Strategy
Prioritizing source diversity before quality filtering yields more robust performance gains. Collecting 180+ diverse sources ensures broad visual domain coverage, while filtering removes low-quality samples without sacrificing diversity. K-means clustering on SSCD embeddings ensures balanced selection within categories.

### Mechanism 2: Three-Stage Training Enables Efficient Data Iteration
Adding Stage-1.5 between connector alignment and high-quality SFT accelerates iteration and improves final performance. Stage-1.5 trains on ~22M diverse samples to create a robust foundation, enabling Stage-2 to rapidly iterate on smaller high-quality subsets (~4.6M) with transferable visual-language grounding.

### Mechanism 3: Tiled Mixture of Vision Encoders (MoVE)
Channel-concatenated SigLIP + ConvNeXt with dynamic tiling improves OCR and document understanding. Each image tile is encoded by both encoders, downsampled, and concatenated along channels before the MLP connector, combining SigLIP's robust perception with ConvNeXt's strengths.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - Why needed here: Understanding connector-based design (vision encoder → MLP → LLM) is prerequisite to grasping why Stage-1 trains only the connector while later stages train the full model.
  - Quick check question: Given a pretrained LLM and vision encoder, what component must be trained first to align modalities?

- **Concept: Data Packing and Padding**
  - Why needed here: Balanced knapsack algorithm assumes familiarity with why packing matters—concatenating short samples reduces wasted padding tokens.
  - Quick check question: Why would grouping all long samples together into some batches and all short samples into others harm training?

- **Concept: Vision Encoder Properties (Resolution, Downsampling, Token Output)**
  - Why needed here: Understanding why SigLIP needs PixelShuffle (16×16 output) to match ConvNeXt's 32× downsampling requires knowing how encoder architecture determines output tokens.
  - Quick check question: If an encoder produces 1024 tokens per 448×448 image, how many tokens result from a 2×3 tile grid?

## Architecture Onboarding

- **Component map**: Image → SigLIP-400M + ConvNeXt-XXL → PixelShuffle downsampling → Channel concatenation → 2-layer MLP connector → Qwen2.5 LLM → Output

- **Critical path**:
  1. Stage-1: Train connector only (1.2M samples, 4h on H100×128). Freezes encoders and LLM.
  2. Stage-1.5: Full model training on 21.6M samples (28h on H100×256). Learning rate 2–4×10⁻⁵.
  3. Stage-2: Full model training on 4.6M high-quality samples (6h on H100×256). Max length 16384.

- **Design tradeoffs**:
  - MoVE vs. single encoder: +1.1 average improvement but 2× encoder computation and memory
  - 3-stage vs. 2-stage: Faster Stage-2 iteration, but requires additional Stage-1.5 training (28h)
  - Balanced vs. naive packing: More uniform batch composition; +0.7 average improvement

- **Failure signatures**:
  - Model outputs fixed LaTeX templates (e.g., `\begin{align*}`) even for non-equation inputs → data formatting issue; remove rigid equation environments from training data
  - Performance drops after adding new data source → check similarity score (Eq. 1); may indicate redundancy or quality issues
  - OCRBench regression → inspect for decimal precision issues (Figure 6) or repeated text patterns (Figure 5c)

- **First 3 experiments**:
  1. Baseline reproduction: Start with Eagle2-Baseline settings (Table 1), verify Stage-1 connector training converges and Stage-2 reaches ~58.8 average on 13 benchmarks
  2. Ablate Stage-1.5: Skip directly from Stage-1 to Stage-2 with the same 21.6M data; expect lower performance and slower iteration per Figure 8's logic
  3. Encoder comparison: Train with SigLIP-only (matching smaller model configs) vs. MoVE; expect gap on OCRBench and chart/document tasks per Table 6

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal scaling behavior between training data size and model performance for VLMs, and at what point do diminishing returns or performance fluctuations occur?
- Basis in paper: [explicit] The paper states "data scaling indicates potential for further gains beyond 10M samples" but notes "considerable performance fluctuations across specific benchmarks at this scale, especially in challenging benchmarks like MMMU, MathVista, and MMVet" and "the efficiency of data iteration has decreased."
- Why unresolved: The experiments stopped at cost constraints; the inflection point where data scaling becomes counterproductive was not identified.
- What evidence would resolve it: Systematic scaling studies tracking per-benchmark variance and cost-per-improvement ratios across multiple data scale points beyond 10M samples.

### Open Question 2
How can subset selection strategies be improved beyond random sampling to maintain or improve performance while reducing dataset size?
- Basis in paper: [explicit] The authors report that "naive data selection" with random sampling led to performance decline, speculating "the randomly selected data have inadvertently excluded some valuable samples, while also failing to adequately ensure a balanced data distribution."
- Why unresolved: The K-means clustering approach only works well for specific domains (mathematical, medical, document-based) and "performs poorly on natural scene images."
- What evidence would resolve it: A domain-adaptive selection method tested across all data categories with ablation showing retention of diverse sample types.

### Open Question 3
What is the optimal data distribution between Stage-1.5 (large-scale diverse data) and Stage-2 (high-quality instruction tuning), and how does varying this ratio affect final performance?
- Basis in paper: [inferred] The paper uses 21.6M samples in Stage-1.5 and 4.6M in Stage-2 (approximately 4.7:1 ratio) but does not systematically explore alternative distributions or justify this specific ratio.
- Why unresolved: The three-stage strategy improves over two-stage, but the inter-stage data allocation remains empirically determined without principled optimization.
- What evidence would resolve it: Grid search over Stage-1.5/Stage-2 data ratios with controlled total training budget, measuring final benchmark performance.

## Limitations
- Internal data dependencies: Relies on non-public datasets (Arxiv2Markdown, Textbooks-QA) that limit reproducibility
- Augmented data generation: CoT-augmented datasets generated with unspecified VLMs and filtered with another LLM, introducing potential evaluation biases
- Parameter sensitivity: Doesn't explore sensitivity to learning rates, batch sizes, or stage durations across different training configurations

## Confidence
- High confidence: Architectural design (tiled MoVE with SigLIP+ConvNeXt) and three-stage training framework are clearly specified and produce measurable improvements
- Medium confidence: Diversity-first data strategy produces robust performance gains, though superiority over quality-first approaches lacks controlled ablation studies
- Low confidence: Claims about specific mechanisms driving improvements in chart/document understanding tasks are difficult to verify without access to internal datasets

## Next Checks
1. Controlled data ordering ablation: Implement parallel experiment applying quality filtering before diversity collection to test whether diversity-first superiority holds across different domain distributions
2. Public dataset replication: Attempt to reproduce core results using only publicly available datasets with equivalent filtering and augmentation strategies, measuring performance gap from internal data dependencies
3. Stage duration sensitivity analysis: Systematically vary training duration for each stage (particularly Stage-1.5 vs Stage-2) to determine whether improvements are robust to changes in three-stage timing assumptions