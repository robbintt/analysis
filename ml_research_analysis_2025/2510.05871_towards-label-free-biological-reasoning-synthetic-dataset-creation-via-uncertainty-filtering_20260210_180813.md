---
ver: rpa2
title: Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty
  Filtering
arxiv_id: '2510.05871'
source_url: https://arxiv.org/abs/2510.05871
tags:
- uncertainty
- reasoning
- traces
- synthetic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Uncertainty-based filtering enables label-free synthetic reasoning
  dataset creation, eliminating reliance on ground-truth labels. By sampling multiple
  traces per example and retaining low-uncertainty subsets using self-consistency
  and predictive perplexity, the approach generates higher-quality synthetic data.
---

# Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering

## Quick Facts
- arXiv ID: 2510.05871
- Source URL: https://arxiv.org/abs/2510.05871
- Reference count: 37
- Primary result: Uncertainty-filtered synthetic training (10% subset) achieves 0.57 accuracy on PerturbQA, outperforming random sampling and narrowing gap to ground-truth-supervised training.

## Executive Summary
This paper introduces a label-free approach for creating high-quality synthetic reasoning datasets by leveraging model uncertainty as a filtering signal. Instead of relying on ground-truth labels, the method generates multiple reasoning traces per example and retains only low-uncertainty subsets using a hybrid metric combining self-consistency and perplexity. Applied to biological perturbation prediction on the PerturbQA benchmark, uncertainty-filtered training achieves 0.57 accuracy, surpassing both random sampling and unfiltered baselines while approaching the performance of ground-truth-supervised training.

## Method Summary
The approach generates k+1 traces per input (1 greedy + k high-temperature samples), computes a CoCoA uncertainty score combining cross-encoder self-consistency and perplexity, then retains the top 10% lowest-uncertainty traces per class. Fine-tuning uses Qwen3-32B with QLoRA (lr=1e-5, batch=4, 20 epochs). The method is specifically designed for biological reasoning tasks where ground-truth labels are scarce or expensive to obtain.

## Key Results
- Uncertainty-filtered 10% subset achieves 0.57 accuracy on PerturbQA
- Outperforms random sampling (0.49 accuracy) and unfiltered baselines
- Per-class filtering corrects class imbalance, improving up-regulated gene F1 from 0.14 to 0.18
- Hybrid CoCoA metric yields better results than either perplexity or self-consistency alone

## Why This Works (Mechanism)
Uncertainty-based filtering leverages the model's own confidence estimates to identify high-quality synthetic reasoning traces. By sampling multiple traces per example and computing both semantic self-consistency (cross-encoder similarity) and perplexity, the approach captures complementary uncertainty signals that correlate with trace quality. The per-class filtering ensures balanced representation across all classes, addressing the class imbalance that would otherwise bias the dataset toward the majority class.

## Foundational Learning
- **PerturbQA benchmark**: Biological perturbation prediction task requiring prediction of gene expression changes (up/down/non-regulated) from (cell type, perturbation, gene) tuples. Why needed: Provides standardized evaluation framework for biological reasoning. Quick check: Verify input format matches paper's template.
- **Cross-encoder similarity**: Computes semantic similarity between reasoning traces using a separate encoder model. Why needed: Captures consistency in reasoning paths beyond surface-level token matching. Quick check: Ensure similarity scores are in [0,1] range.
- **CoCoA uncertainty metric**: Hybrid score combining self-consistency and perplexity to rank trace quality. Why needed: Captures both semantic coherence and generation difficulty. Quick check: Verify scores are normalized across all traces.
- **QLoRA fine-tuning**: Parameter-efficient fine-tuning method using low-rank adaptation. Why needed: Enables efficient fine-tuning of large models on synthetic data. Quick check: Monitor GPU memory usage during training.

## Architecture Onboarding

**Component Map:**
Prompt template -> Trace generation -> Uncertainty scoring -> Per-class filtering -> QLoRA fine-tuning

**Critical Path:**
1. Generate synthetic traces with uncertainty scores
2. Apply per-class filtering to select top 10% lowest-uncertainty traces
3. Fine-tune model on filtered traces using QLoRA

**Design Tradeoffs:**
- Global vs per-class filtering: Per-class maintains class balance but may reduce overall data volume
- Single vs hybrid uncertainty metrics: CoCoA combines complementary signals but adds computational overhead
- Fixed vs adaptive filtering threshold: 10% is heuristic but provides consistent evaluation baseline

**Failure Signatures:**
- Class imbalance in filtered dataset indicates incorrect per-class filtering implementation
- Poor performance on up-regulated class suggests insufficient high-quality traces for minority class
- Minimal gap between filtered and unfiltered performance indicates ineffective uncertainty filtering

**3 First Experiments:**
1. Generate traces and verify CoCoA scores correlate with trace quality
2. Test per-class filtering maintains class balance while reducing total data volume
3. Compare performance of 5%, 10%, and 20% filtered subsets to find optimal retention ratio

## Open Questions the Paper Calls Out
1. Does uncertainty-based filtering generalize effectively to reasoning domains outside of biological perturbation prediction? The study is restricted to PerturbQA; generalization to logic, mathematics, or coding domains remains untested.
2. Can combining uncertainty-filtered synthetic data with small amounts of ground-truth labels improve robustness? The purely label-free approach leaves open the potential benefits of semi-supervised learning.
3. Are there complementary uncertainty signals beyond perplexity and self-consistency that better predict reasoning trace quality? The paper acknowledges the current hybrid metric may miss important quality dimensions.

## Limitations
- Relies on single benchmark (PerturbQA) limiting generalizability claims
- Fixed 10% filtering threshold is heuristic without sensitivity analysis
- Exact cross-encoder architecture remains unspecified, affecting reproducibility

## Confidence

**High Confidence**: Core methodology and observed performance gap between uncertainty-filtered and random subsets is well-defined and substantial.

**Medium Confidence**: Claims about narrowing gap to ground-truth-supervised training assume comparable data quality; per-class filtering improvements are demonstrated but limited to single benchmark.

**Low Confidence**: Scalability to other biological reasoning tasks or non-biological domains is speculative; optimal architecture for this filtering approach remains untested.

## Next Checks
1. Test sensitivity to filtering thresholds: Compare performance across 5%, 10%, 20%, and 30% uncertainty-filtered subsets
2. Ablation on uncertainty components: Compare CoCoA vs perplexity-only vs self-consistency-only filtering
3. Cross-domain validation: Apply identical pipeline to mathematical reasoning benchmark to assess generalizability