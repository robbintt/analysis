---
ver: rpa2
title: 'Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of
  Value'
arxiv_id: '2512.03399'
source_url: https://arxiv.org/abs/2512.03399
tags:
- values
- value
- systems
- what
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Full-stack alignment requires concurrent alignment of AI systems
  and institutions with what people value. Current approaches like utility functions
  and unstructured text fail to distinguish values from other signals, support normative
  reasoning, or model collective goods.
---

# Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value

## Quick Facts
- arXiv ID: 2512.03399
- Source URL: https://arxiv.org/abs/2512.03399
- Authors: 33 authors including Joe Edelman, Tan Zhi-Xuan, Ryan Lowe
- Reference count: 40
- Primary result: Thick Models of Value (TMV) can co-align AI systems and institutions with human values by preserving value information through structured representation and normative reasoning

## Executive Summary
Full-stack alignment requires concurrent alignment of AI systems and institutions with what people value. Current approaches like utility functions and unstructured text fail to distinguish values from other signals, support normative reasoning, or model collective goods. The authors propose "thick models of value" (TMV) that structure value representation, enabling systems to distinguish enduring values from fleeting preferences, model social embedding of choices, and reason normatively. They demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulation at AI speed.

## Method Summary
The paper proposes Thick Models of Value (TMV) as an alternative to Preferentist Modeling (PMV) and Values-as-Text (VAT). TMV represents values as "constitutive attentional policies" - criteria used in meaningful choices that are not merely instrumental. The method involves structured dialogue/LLM interviews to surface these attentional policies, formal encoding with justificatory structure, normative reasoning engines, and integration into target systems. A case study demonstrates Moral Graph Elicitation where users judge value transitions for "wisdom" to build collective value hierarchies.

## Key Results
- TMV addresses systematic failures in current alignment frameworks by preserving value information as it moves through institutional layers
- 89% agreement on fairness in Moral Graph Elicitation case study demonstrates TMV's potential for collective value reasoning
- TMV enables distinguishing values from preferences, supporting normative reasoning, and modeling collective goods
- Five application domains show TMV's versatility across AI value stewardship, normatively competent agents, negotiation systems, economic mechanisms, and democratic regulation

## Why This Works (Mechanism)

### Mechanism 1: Value Information Loss Through Institutional Compression
- Claim: Existing value representations systematically lose value information as it moves through institutional layers, causing misalignment
- Mechanism: PMV bundles values with preferences, tastes, and addictions without differentiation. This bundled signal is compressed at each institutional level—"meaningful connection" → "engagement metrics" → "daily active users" → "quarterly revenue"
- Core assumption: Values are structurally distinct from preferences with justificatory structure and social meaning
- Evidence anchors: Deep Value Benchmark (2511.02109) distinguishes deep values from shallow preferences

### Mechanism 2: Structured Value Representation via Constitutive Attentional Policies
- Claim: Representing values as "constitutive attentional policies" enables systems to filter noise and preserve authentic values
- Mechanism: Values elicited through reflective dialogue are structured as attentional policies (what someone pays attention to when making meaningful choices)
- Core assumption: People can articulate what they attend to in meaningful decisions, reflecting genuine values
- Evidence anchors: 89% agreement on fairness in Moral Graph Elicitation studies

### Mechanism 3: Normative Reasoning Through Justificatory Structure
- Claim: Structured representations encoding justificatory relationships enable AI systems to reason normatively in novel contexts
- Mechanism: TMV encodes why someone holds a value, not just that they hold it. This enables generalization to new contexts by applying justifications
- Core assumption: Values form networks of mutual support with discoverable justificatory structure
- Evidence anchors: Generative Psycho-Lexical Approach (2502.02444) addresses constructing value systems with hierarchical structure

## Foundational Learning

- **Preferentist Models of Value (PMV) and Their Limits**
  - Why needed here: TMV is defined in opposition to PMV; understanding utility functions and RLHF is essential to see why they fail for full-stack alignment
  - Quick check question: Can you explain why a revealed preference for scrolling social media might not reflect a user's values?

- **Thick vs. Thin Ethical Concepts**
  - Why needed here: The paper draws on philosophical literature distinguishing thick concepts (which contain descriptive and evaluative content) from thin concepts
  - Quick check question: What information does "honesty" contain that "preferred" does not?

- **Multi-Level Institutional Analysis**
  - Why needed here: Full-stack alignment requires understanding how values flow through users → platforms → markets → regulators
  - Quick check question: How might a user value for "community belonging" be transformed as it moves from a user to a platform metric to a regulatory report?

## Architecture Onboarding

- **Component map:**
  - Elicitation layer: Structured dialogue/LLM interviews to surface attentional policies
  - Representation layer: Formal encoding with justificatory structure (moral graphs, attentional policy schemas)
  - Reasoning layer: Normative reasoning engines (contractualist simulation, universalization)
  - Application layer: Integration into agents, markets, democratic systems

- **Critical path:**
  1. Define value schema (what counts as a valid value representation)
  2. Build elicitation pipeline (dialogue → structured value cards)
  3. Implement aggregation/graph construction (comparing values, building moral graphs)
  4. Train/inject into target systems (agents, recommenders, governance)

- **Design tradeoffs:**
  - Structure vs. expressiveness: More structure enables reasoning but constrains what can be expressed
  - Pluralism vs. actionability: Allowing diverse values vs. needing concrete decisions
  - Stability vs. adaptability: Preserving values across contexts vs. allowing legitimate evolution

- **Failure signatures:**
  - Ideological capture: Slogans or tribal markers pass through as values
  - Preference bundling: Fleeting wants treated equivalently to enduring values
  - Context collapse: Values inappropriately applied without sensitivity to social meaning
  - Justification drift: Reasoning chains that stray from original justifications

- **First 3 experiments:**
  1. Replicate Moral Graph Elicitation on a contentious topic (e.g., content moderation) and measure whether participants find the process and outcomes fair
  2. Train a simple agent with TMV-based value cards on a decision environment (e.g., resource allocation game) and compare against preference-optimized baseline on value preservation metrics
  3. Implement norm-augmented Markov games for a multi-agent coordination task and measure whether agents learn to sustain cooperative norms better than standard RL agents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we reliably evaluate whether an AI agent is providing genuine moral assistance versus subtle manipulation when helping users with value-laden decisions?
- Basis in paper: Listed in Section 4.1.1 under "Key open research questions" regarding AI value-stewardship agents
- Why unresolved: Current systems struggle to distinguish between supporting user autonomy and "value collapse" (eroding values into easily optimizable objectives)
- What evidence would resolve it: Metrics capable of differentiating authentic value clarification from reinforcement of fleeting impulses or addiction patterns

### Open Question 2
- Question: Can textually specified norms be translated into structured representations that support reliable normative reasoning and generalization by AI systems?
- Basis in paper: Listed in Section 4.1.2 under "Key open research questions" regarding normatively competent agents
- Why unresolved: Values-as-text (VAT) approaches lack the internal structure required for consistent application across new contexts
- What evidence would resolve it: AI systems successfully navigating novel edge cases by applying the "spirit" of a rule via explicit reasoning traces rather than pattern matching

### Open Question 3
- Question: What protocols can prevent manipulation in AI negotiations by agents who falsely claim principled commitments?
- Basis in paper: Listed in Section 4.1.3 under "Key open research questions" regarding win-win AI negotiation
- Why unresolved: Strategic agents may exploit opaque value representations to feign cooperation while optimizing for self-interest
- What evidence would resolve it: Formal integrity-checking mechanisms where deceptive value commitments are provably disadvantageous or detectable

## Limitations
- The distinction between values and preferences remains philosophically contested and empirically unproven
- TMV's normative reasoning capabilities lack rigorous empirical validation across proposed applications
- Claims about justificatory reasoning and attentional policies are described at abstract level without demonstrating robustness to manipulation
- The paper provides conceptual frameworks and case studies but lacks systematic testing of TMV's scalability and real-world effectiveness

## Confidence

- **High Confidence**: The problem framing (value information loss through institutional compression) is well-articulated and observable in current systems
- **Medium Confidence**: The core mechanism of using structured representations to preserve value information shows promise, particularly in the Moral Graph Elicitation case study (89% fairness agreement)
- **Low Confidence**: Claims about normative reasoning capabilities, collective goods modeling, and general applicability across all five proposed domains lack empirical validation

## Next Checks

1. **Value Distinction Validation**: Conduct controlled experiments comparing TMV elicitation against preference-based elicitation on same decision scenarios, measuring whether TMV captures information PMV misses (e.g., normative reasoning patterns, value stability across contexts)

2. **Manipulation Resistance Test**: Design adversarial elicitation scenarios where participants attempt to game system with ideological slogans or strategic framing. Measure whether TMV's structured representation successfully filters out non-genuine values while PMV-based approaches fail

3. **Cross-Domain Applicability Study**: Implement TMV in two domains beyond Moral Graph Elicitation (e.g., recommendation system and multi-agent coordination task). Compare performance against PMV baselines on value preservation metrics, normative reasoning benchmarks, and collective good outcomes