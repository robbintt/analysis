---
ver: rpa2
title: 'CueLearner: Bootstrapping and local policy adaptation from relative feedback'
arxiv_id: '2507.04730'
source_url: https://arxiv.org/abs/2507.04730
tags:
- feedback
- learning
- policy
- human
- relative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CueLearner, a method that combines relative
  feedback with off-policy deep reinforcement learning to improve exploration efficiency
  and enable post-hoc policy adaptation. Relative feedback provides improvement directions
  in action space (e.g., "more to the left") without requiring demonstrations or expert
  actions.
---

# CueLearner: Bootstrapping and local policy adaptation from relative feedback

## Quick Facts
- **arXiv ID:** 2507.04730
- **Source URL:** https://arxiv.org/abs/2507.04730
- **Reference count:** 28
- **One-line primary result:** CueLearner significantly improves exploration efficiency and enables post-hoc policy adaptation using relative feedback, requiring 5-10x fewer human labels than scalar feedback baselines.

## Executive Summary
CueLearner introduces a method that combines relative feedback with off-policy deep reinforcement learning to improve exploration efficiency and enable post-hoc policy adaptation. The system learns a feedback model from human annotations that provides improvement directions in action space (e.g., "more to the left") without requiring demonstrations or expert actions. In experiments on billiards and quadruped navigation tasks, CueLearner significantly outperforms pure RL and scalar feedback baselines, achieving comparable performance with 5-10x fewer human labels. The method also demonstrates successful real-world deployment on a quadruped robot navigating obstacle courses using only 500 human labels collected in under 40 minutes.

## Method Summary
CueLearner learns a feedback model from relative feedback tuples (state, action, correction direction) provided by human trainers. This model is trained to predict corrective vectors in action space using an MLP architecture. During RL training, the feedback model iteratively refines the base policy's actions before execution, storing these guided transitions in a replay buffer. For policy adaptation, CueLearner uses a DAgger-like approach to retrain the feedback model on refined actions, enabling post-hoc adaptation without modifying base policy weights. The method employs correction bounds to prevent compounding errors while maintaining stability during iterative refinement.

## Key Results
- Achieved 5-10x fewer human labels than scalar feedback baselines to reach comparable performance
- Demonstrated successful real-world deployment on quadruped robot navigation with 500 labels in under 40 minutes
- Outperformed pure RL and scalar feedback baselines in both billiards and quadruped navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning a feedback model from relative annotations enables generalization of human guidance to novel states without continuous querying.
- Mechanism: A neural network φθ is trained on tuples (s, a, h) where h is the human-provided improvement direction. This model learns to predict a corrective vector in action space. By iteratively applying this correction (a_{k+1} = a_k + φθ(s, a_k)), the system can refine actions at new state-action pairs not seen during labeling. The use of off-policy RL then allows these refined transitions to be stored in a replay buffer and used for policy updates.
- Core assumption: The feedback model can accurately interpolate the corrective signal from labeled to unlabeled state-action pairs within the distribution seen during training.
- Evidence anchors:
  - [abstract] The paper introduces CueLearner, a method that "learns a feedback model from human annotations and uses it to refine RL agent actions during training or deployment."
  - [section IV-A] "Once we have learned the feedback model φθ, we can apply it iteratively to refine the actions of the agent without needing to query the human trainer again."
  - [corpus] Corpus evidence is weak/missing for this specific mechanism of learning a feedback model for RL. One related paper mentions "verbal feedback as a conditioning signal" for LLMs, but is not directly analogous to this RL action-refinement architecture.
- Break condition: The mechanism fails if the agent encounters states far from the labeled data distribution (out-of-distribution), causing the feedback model to produce incorrect refinements. This is evidenced in the adaptation experiment where large environmental changes requiring corrections beyond learned bounds failed to improve performance.

### Mechanism 2
- Claim: Relative feedback provides a more information-rich and label-efficient signal than binary scalar feedback for guiding exploration.
- Mechanism: Binary feedback provides a single bit of information (good/bad). Relative feedback provides a direction in the action space (e.g., "more to the left"), which is effectively a vector pointing toward a better action. This vector explicitly reduces the search space for the optimal action more than a scalar reward, leading to faster convergence in sparse reward settings where the environmental reward is uninformative.
- Core assumption: A non-expert human trainer can reliably and consistently provide the correct direction for improvement.
- Evidence anchors:
  - [abstract] "It requires 5-10x fewer human labels than scalar feedback to achieve comparable performance."
  - [section VI-A] "Compared with scalar feedback, our method requires five times less labels to achieve the same speedup [in the billiards task]."
  - [corpus] The corpus does not provide direct evidence on the efficiency of relative vs. scalar feedback in RL.
- Break condition: This mechanism fails if the human trainer is inconsistent or provides incorrect directions, which would steer exploration toward suboptimal regions of the state-action space.

### Mechanism 3
- Claim: Iterative refinement of actions with a learned feedback model allows for post-hoc policy adaptation without modifying the base policy's weights.
- Mechanism: A pre-trained base policy πB proposes an initial action a0. The learned feedback model φθ then iteratively applies corrections to this action. This process effectively "steers" the base policy's output at inference time. The model itself can be retrained on corrected actions via Dataset Aggregation (DAgger) to handle compounding errors, making the refinement robust.
- Core assumption: The desired adapted behavior is achievable through localized corrections to the base policy's actions.
- Evidence anchors:
  - [abstract] CueLearner "can adapt a policy to changes in the environment or the user's preferences" and "achieves label efficiency similar to learning residuals from optimal corrections while requiring less precise annotations."
  - [section IV-C] "We can again use a similar strategy to refine the policy’s actions towards a desired behavior... the feedback model learns to robustly refine the actions of the base policy."
  - [corpus] Related work like "Yell at your robot" uses language corrections for on-the-fly improvement, but CueLearner's specific mechanism for deep RL policy adaptation is distinct.
- Break condition: The mechanism breaks when the adaptation required is fundamentally different from the base policy's behavior, requiring "global action adaptation" or corrections larger than the learned model's bounds. The experiment with a significantly different navigation environment showed this failure mode.

## Foundational Learning
- Concept: Off-Policy Reinforcement Learning
  - Why needed here: The core of CueLearner's architecture. Understanding how an agent can learn from data it didn't directly collect (from a replay buffer) is essential, as this is how the refined, guided actions are used for training.
  - Quick check question: Explain the difference between on-policy and off-policy learning and why off-policy is required for CueLearner's feedback model to work during training.
- Concept: Dataset Aggregation (DAgger)
  - Why needed here: The paper explicitly mentions using a DAgger-like approach to retrain the feedback model on refined actions to combat compounding errors. Understanding this iterative data collection and retraining loop is critical for implementing the method.
  - Quick check question: Describe the DAgger algorithm and the problem of "distributional shift" it is designed to solve in the context of a learned feedback model.
- Concept: Sparse Reward Problem in RL
  - Why needed here: The entire motivation for CueLearner is to improve sample efficiency in sparse reward environments where random exploration is ineffective. Grasping this challenge is key to understanding the method's value proposition.
  - Quick check question: Why is exploration difficult in a sparse reward environment, and how does CueLearner's guidance specifically address this challenge?

## Architecture Onboarding
- Component map:
  1. **Base Policy (πB)**: The main RL agent (e.g., a Double Deep Q-Network) being trained or adapted.
  2. **Human Trainer**: Provides relative feedback labels (s, a, h).
  3. **Feedback Model (φθ)**: A separate neural network trained to predict action corrections. Takes a state and action as input and outputs a corrective vector.
  4. **Replay Buffer**: Standard component in off-policy RL, but is populated with transitions generated using the Feedback Model.
- Critical path:
  1. **Feedback Collection**: Train base policy randomly or use a pre-trained one. Collect a dataset of (state, action, human-provided correction) tuples.
  2. **Feedback Model Training**: Train the φθ network on this dataset to predict the correction vector.
  3. **Guided Exploration/Adaptation**: During RL training, alternate between standard exploration and "guided episodes". In a guided episode, get an initial action from πB, then iteratively refine it using φθ to get a final action. Execute this action and store the transition in the replay buffer for the base policy to learn from.
  4. **Iterative Retraining (Optional/Adaptation)**: For adaptation, collect new feedback on the *refined* actions and retrain the feedback model (a DAgger-like loop) to improve robustness.
- Design tradeoffs:
  - **Label Efficiency vs. Trainer Effort**: Relative feedback is more efficient than scalar feedback but requires a bit more cognitive effort from the trainer than a simple "good/bad" button.
  - **Exploration vs. Exploitation**: The guidance must be cut off after a certain number of episodes to allow the agent to discover its own optimal policy, rather than becoming overly reliant on the feedback model.
  - **Correction Bounds**: Imposing a maximum correction distance (τ_max) stabilizes adaptation but limits the ability to make large, global changes to the policy's behavior.
- Failure signatures:
  - **Compounding Errors**: The feedback model may become inaccurate after multiple refinement steps as actions drift from the original data distribution. Mitigation: Use DAgger-style retraining and correction bounds.
  - **Out-of-Distribution States**: If the base policy enters a state very different from the labeled data, the feedback model will give poor advice. Mitigation: Ensure diverse data collection during the initial labeling phase.
  - **Global Adaptation Failure**: Attempting to adapt to a task requiring fundamentally different actions (beyond the correction bounds) will fail. The paper shows this when trying to adapt a navigation policy to a drastically different environment.
- First 3 experiments:
  1. Implement the feedback model training loop. Use a simple oracle (e.g., a pre-trained expert policy) to generate "relative feedback" labels for a toy environment (e.g., CartPole). Train the model to predict the direction toward the expert's action.
  2. Integrate the trained feedback model into a simple off-policy RL loop (e.g., DQN). Run a sparse reward version of the toy environment, with and without the feedback guidance, and compare sample efficiency.
  3. Test post-hoc adaptation. Train a base policy to completion, then introduce a shift in the environment dynamics (e.g., change a physics parameter). Collect a small number of new feedback labels, retrain the feedback model, and measure how quickly performance is restored compared to fine-tuning the base policy from scratch.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the method be extended to handle adaptation scenarios requiring extensive global policy modifications rather than localized adjustments?
- **Open Question 2**: How does the usability and label efficiency of relative feedback scale with the dimensionality of the action space?
- **Open Question 3**: How robust is the feedback model when trained on noisy or contradictory human labels compared to the automated oracles used in simulation?

## Limitations
- Limited generalization capability to out-of-distribution states where the feedback model may produce incorrect refinements
- Dependence on quality and consistency of human feedback, which could lead to suboptimal exploration if inconsistent
- Difficulty handling adaptation scenarios requiring extensive global policy modifications rather than localized adjustments

## Confidence
- **High Confidence**: The experimental results showing 5-10x label efficiency improvement over scalar feedback and the successful real-world deployment on a quadruped robot navigating obstacle courses with 500 human labels.
- **Medium Confidence**: The theoretical mechanism of learning a feedback model for action refinement in RL, though the paper provides limited corpus evidence for this specific approach.
- **Low Confidence**: The scalability of this approach to significantly more complex tasks or environments with very large state-action spaces where human feedback might become impractical.

## Next Checks
1. **Distributional Shift Test**: Design an experiment where the agent encounters states progressively further from the labeled data distribution. Measure the accuracy of the feedback model's corrections and identify the point at which performance degrades significantly.
2. **Feedback Quality Analysis**: Implement a version of the system where the human feedback is intentionally noisy or incorrect in a controlled manner. Quantify how different types and levels of feedback errors affect learning efficiency and final policy performance.
3. **Cross-Task Generalization**: Train a feedback model on one task (e.g., billiards) and test whether it can provide useful guidance on a related but distinct task (e.g., a different billiards variant or another physics-based manipulation task). Measure both the transfer effectiveness and the amount of new labeling required to adapt the model.