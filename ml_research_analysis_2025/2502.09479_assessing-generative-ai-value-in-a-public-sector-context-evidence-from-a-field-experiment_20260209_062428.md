---
ver: rpa2
title: 'Assessing Generative AI value in a public sector context: evidence from a
  field experiment'
arxiv_id: '2502.09479'
source_url: https://arxiv.org/abs/2502.09479
tags:
- task
- data
- group
- treatment
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the impact of Generative AI (Gen AI) on knowledge
  work tasks in a public sector context through a pre-registered randomised controlled
  trial with 143 Central Bank of Ireland staff. Two tasks were tested: document comprehension
  and data analysis.'
---

# Assessing Generative AI value in a public sector context: evidence from a field experiment

## Quick Facts
- arXiv ID: 2502.09479
- Source URL: https://arxiv.org/abs/2502.09479
- Reference count: 10
- Primary result: Gen AI improved document comprehension quality by 17% and reduced time by 34%, but showed no significant benefit for data analysis tasks.

## Executive Summary
This study evaluated Generative AI's impact on knowledge work tasks in a public sector context through a randomized controlled trial with 143 Central Bank of Ireland staff. Two tasks were tested: document comprehension (answering questions from SFCR reports) and data analysis (querying insurance data). For document tasks, Gen AI users achieved 17% higher quality scores and 34% faster completion times. For data analysis, Gen AI users experienced a 12% reduction in quality with no time difference. The benefits were task-dependent, with lower-performing participants gaining the most from AI assistance on document tasks.

## Method Summary
The study employed a pre-registered RCT design with 143 participants who completed baseline assessments without AI, then were randomly assigned to treatment (AI access) or control groups for experimental tasks. The document comprehension task used a RAG-based chatbot to answer 12 questions from 100-page SFCR documents. The data analysis task used an NL-to-SQL application to answer 6 questions from a SQLite database of QRT data. Primary outcomes were quality scores (human-graded rubric for documents, ground truth for data) and self-reported completion times. Analysis used t-tests and OLS regression with covariates including baseline performance and experience.

## Key Results
- Document comprehension: 17% quality improvement (2.21 vs 1.89) and 34% time reduction for Gen AI users (statistically significant)
- Data analysis: 12% quality reduction for Gen AI users, no significant time difference
- Lower-performing participants benefited most from Gen AI on document tasks, with quality scores increasing from 1.38 to 2.12
- Prior Gen AI experience correlated with better data task performance, suggesting prompt engineering skills matter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gen AI improves document comprehension through targeted information retrieval from structured texts.
- Mechanism: RAG-based chatbot retrieves relevant document chunks via hybrid search, reducing manual search time and surfacing information users might miss.
- Core assumption: Document structure is sufficiently organized for chunk-level retrieval to capture context needed for high-quality answers.
- Evidence anchors:
  - Treatment group average quality score was 2.21 vs 1.89 for control (statistically significant, p < 0.05)
  - Treatment group demonstrated strong performance on questions where information was concentrated within one section
- Break condition: Performance degrades when answers require synthesizing information scattered across multiple document sections.

### Mechanism 2
- Claim: Gen AI disproportionately benefits lower-skilled workers on document tasks, acting as a skill-leveling tool.
- Mechanism: Lower-performing participants gain more because the tool compensates for weaker baseline document comprehension skills.
- Core assumption: Assessment task scores accurately reflect baseline document comprehension ability.
- Evidence anchors:
  - Lower-half participants using Gen AI increased from 1.38 to 2.12 on experiment task quality scores
  - Time-to-completion decreased from 70% to 49% of allocated time for lower performers
- Break condition: Effect reverses for top performers on complex synthesis questions, where Gen AI may oversimplify.

### Mechanism 3
- Claim: Prior Gen AI experience improves outcomes on data analysis tasks, suggesting prompt engineering is learnable.
- Mechanism: Experienced users rephrase queries more effectively, preserving key terms that materially affect SQL generation accuracy.
- Core assumption: Correlation between experience and quality reflects causation rather than confounding factors.
- Evidence anchors:
  - Those in treatment group with more Gen AI experience had higher quality scores
  - Prompts with greater similarity to prescribed questions resulted in higher quality scores
- Break condition: Inexperienced users who omit key terms receive incorrect outputs with no built-in safeguards.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG) Pipeline**
  - Why needed here: The document task uses RAG to ground LLM responses in source documents.
  - Quick check question: Can you explain why hybrid (vector + keyword) search might outperform pure vector search for domain-specific documents?

- **Treatment Effect Estimation with Covariates**
  - Why needed here: The study uses regression models with progressively more covariates to isolate the Gen AI treatment effect.
  - Quick check question: Why does adjusted RÂ² increase from 0.081 to 0.406 when adding more covariates in Table 5?

- **Selection Bias in Task Assignment**
  - Why needed here: Participants self-selected into tasks, creating biased samples for the Data task.
  - Quick check question: How might randomizing participants across tasks have changed the Data task results?

## Architecture Onboarding

- **Component map:**
  - Documents app: Azure AI Studio -> text-embedding-ada-002 -> hybrid search -> GPT-4o -> response
  - Data app: SQLite database + Vanna NL-to-SQL engine -> GPT-4o with DDL + Solvency II taxonomy + 20 example pairs -> SQL query -> results

- **Critical path:**
  1. For Documents: Chunk quality and retrieval accuracy directly affect answer quality; poor chunking loses context.
  2. For Data: Question-to-SQL translation depends on taxonomy coverage and example pairs; missing domain terms cause query failures.

- **Design tradeoffs:**
  - Fixed-size chunking vs. semantic chunking: Default was used; may explain poorer performance on synthesis questions.
  - 20 example Q-SQL pairs vs. more: Fewer examples reduce token costs but may limit coverage of edge cases.
  - Self-reported time vs. platform-tracked time: Self-report introduces measurement error but avoided integration complexity.

- **Failure signatures:**
  - Documents: Complex synthesis questions (e.g., Q9) show treatment group scoring lower than control (1.1 vs 1.93).
  - Data: Omitting key terms in prompts (e.g., "ratio" in Q1) causes incorrect SQL generation and lower scores.
  - Both: Intermittent application outages (up to 3 hours) may have affected results unpredictably.

- **First 3 experiments:**
  1. A/B test semantic vs. fixed-size chunking on synthesis-heavy questions to measure retrieval quality differences.
  2. Add a "prompt training" treatment arm to isolate the effect of structured prompting guidance on Data task performance.
  3. Implement platform-based time tracking to eliminate self-report bias and validate the 34% time reduction claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does structured prompt engineering training significantly improve output quality for complex data analysis tasks compared to unassisted Gen AI use?
- Basis in paper: The authors note that with a larger sample size, they "would consider prompt training... as an additional treatment" to assess differential effects.
- Why unresolved: The study design excluded a specific treatment arm for prompt training due to sample size constraints.
- What evidence would resolve it: A new RCT treatment arm where participants receive specific training on prompt formulation before the data task.

### Open Question 2
- Question: What are the long-term impacts of Gen AI adoption on skill development and workflow resilience in public sector organizations?
- Basis in paper: The conclusion states that "Longitudinal studies could also explore the long-term impacts of Gen AI adoption on public sector workflows and decision-making processes."
- Why unresolved: This experiment was short-term, capturing immediate performance effects rather than evolution of skills over time.
- What evidence would resolve it: A longitudinal study tracking the same cohort over 6-12 months to measure changes in baseline competency and decision-making quality.

### Open Question 3
- Question: Does displaying model confidence scores increase user trust and verification accuracy in high-stakes document comprehension tasks?
- Basis in paper: Section 4.3 notes that participants were unsure whether to trust responses, suggesting that displaying the model's confidence would help build user trust.
- Why unresolved: The Gen AI application used did not provide confidence indicators, so the effect of such transparency remains unknown.
- What evidence would resolve it: An experiment comparing user verification rates and error detection between a "confidence-display" interface and a standard interface.

## Limitations
- Self-selection bias in task assignment, where technical staff disproportionately chose the Data task
- Reliance on self-reported completion times introduces measurement error
- Absence of participant-level data prevents external validation of subgroup effects

## Confidence

- **High confidence**: The 17% quality improvement and 34% time reduction for the Documents task are robust, supported by statistically significant t-tests (p < 0.05) and regression models with multiple covariates.
- **Medium confidence**: The null effect on Data task quality (12% reduction, not significant) and lack of time difference are credible but limited by small sample size and technical staff overrepresentation.
- **Low confidence**: The subgroup findings (lower performers benefiting most, experienced users performing better) rely on post-hoc analyses without external validation.

## Next Checks

1. Replicate with randomized task assignment to eliminate self-selection bias and validate subgroup effects.
2. Implement platform-based time tracking to verify the 34% time reduction and eliminate measurement error.
3. Test semantic chunking vs. fixed-size chunking on synthesis-heavy questions to measure retrieval quality differences.