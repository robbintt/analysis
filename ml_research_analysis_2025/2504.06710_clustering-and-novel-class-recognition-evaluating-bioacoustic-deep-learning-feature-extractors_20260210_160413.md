---
ver: rpa2
title: 'Clustering and novel class recognition: evaluating bioacoustic deep learning
  feature extractors'
arxiv_id: '2504.06710'
source_url: https://arxiv.org/abs/2504.06710
tags:
- feature
- extractors
- learning
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive evaluation of 15 bioacoustic
  deep learning feature extractors, comparing supervised and self-supervised learning
  paradigms across diverse model architectures, training data, and training paradigms.
  The research addresses the challenge of comparing models trained on different taxonomic
  groups by focusing on feature extractor embeddings rather than classifier performance.
---

# Clustering and novel class recognition: evaluating bioacoustic deep learning feature extractors

## Quick Facts
- **arXiv ID:** 2504.06710
- **Source URL:** https://arxiv.org/abs/2504.06710
- **Reference count:** 0
- **Primary result:** Supervised learning models significantly outperform self-supervised ones in clustering tasks, with Perch and BirdNET achieving the highest performance.

## Executive Summary
This study comprehensively evaluates 15 bioacoustic deep learning feature extractors across supervised and self-supervised learning paradigms using diverse model architectures and training data. The research addresses the challenge of comparing models trained on different taxonomic groups by focusing on feature extractor embeddings rather than classifier performance. Using two challenging datasets (bird vocalizations from Colombia/Costa Rica and frog vocalizations from Brazil), the study evaluates clustering and kNN classification performance. Key findings show that supervised learning models significantly outperform self-supervised ones in clustering tasks, with Perch and BirdNET achieving the highest performance. Self-supervised models showed better generalization when applied to out-of-domain datasets, particularly the AVES models which used HuBERT architecture with acoustic unit discovery.

## Method Summary
The study evaluates 15 pretrained bioacoustic feature extractors on clustering and novel class recognition using embeddings only (no classifier retraining). Two PAM datasets are used: bird vocalizations from Colombia/Costa Rica (11 classes) and frog vocalizations from Brazil (18 classes), both filtered to classes with >150 annotations. For evaluation, embeddings are extracted from each model, optionally reduced to 300 dimensions using UMAP, then evaluated with K-Means clustering (AMI score) and kNN classification (macro accuracy). The pipeline is available at github.com/bioacoustic-ai/bacpipe.

## Key Results
- Supervised learning models significantly outperform self-supervised ones in clustering tasks, with Perch and BirdNET achieving the highest performance
- Self-supervised models showed better generalization when applied to out-of-domain datasets, particularly the AVES models using HuBERT architecture with acoustic unit discovery
- UMAP dimensionality reduction improves clustering performance more significantly than classification performance across all models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised pre-training on large, domain-specific datasets creates highly structured embedding spaces that outperform self-supervised learning for in-domain clustering and novelty detection.
- **Mechanism:** Explicit supervision forces the feature extractor to minimize intra-class variance and maximize inter-class distance for known taxa, which transfers to novel classes within the same domain.
- **Core assumption:** The acoustic features of novel classes share underlying structural similarities with the classes seen during pre-training.
- **Evidence anchors:** Abstract shows supervised models significantly outperform self-supervised ones; section confirms best performers use supervised learning; foundational models review supports domain-specific pre-training importance.
- **Break condition:** Performance degrades if novel class acoustic structure differs fundamentally from pre-training domain.

### Mechanism 2
- **Claim:** Self-supervised architectures utilizing Acoustic Unit Discovery (AUD) can outperform supervised models in cross-domain generalization.
- **Mechanism:** HuBERT-based AVES models use K-Means clustering on MFCC features to create discrete acoustic tokens, forcing the model to learn fundamental acoustic units rather than domain-specific semantic classes.
- **Core assumption:** Acoustic unit discovery captures universal bioacoustic properties that are domain-agnostic.
- **Evidence anchors:** Section explains HuBERT differs from masked prediction; results show AVES models' drastic performance increase on frog dataset; comparison paper discusses SSL transferability.
- **Break condition:** If target domain relies on temporal features ignored by frame-level MFCC tokenization, generalization may fail.

### Mechanism 3
- **Claim:** Dimensionality standardization via UMAP improves K-Means clustering performance more significantly than classification performance.
- **Mechanism:** K-Means relies on Euclidean distance, which suffers from the "curse of dimensionality" in high-dimensional embedding spaces. UMAP reduction optimizes manifold structure and denoises feature space.
- **Core assumption:** Relevant semantic variance fits within a lower-dimensional manifold.
- **Evidence anchors:** Section shows UMAP embeddings significantly improve clustering results; abstract introduces novel evaluation methodology using AMI scores.
- **Break condition:** Aggressive dimensionality reduction might merge distinct but acoustically similar classes.

## Foundational Learning

- **Concept: Feature Extractors vs. Classifiers**
  - **Why needed here:** The paper explicitly decouples the backbone (feature extractor) from the final dense layer (classifier) to measure generalization potential independent of the classifier's fixed class list.
  - **Quick check question:** If you remove the final classification layer from a BirdNET model, what does the output vector represent?

- **Concept: k-Nearest Neighbors (kNN) for Novel Class Recognition**
  - **Why needed here:** kNN evaluates how well the pre-existing embedding space groups novel classes purely based on proximity without requiring backprop.
  - **Quick check question:** Why is kNN classification (k=15) used here instead of training a new Linear Support Vector Machine (SVM) on the embeddings?

- **Concept: Adjusted Mutual Information (AMI)**
  - **Why needed here:** Standard accuracy fails in clustering because cluster IDs rarely match ground truth class IDs. AMI measures agreement between clustering assignment and ground truth while correcting for chance groupings.
  - **Quick check question:** If a model clusters all "Species A" sounds into "Cluster 5", does AMI penalize the label mismatch (A vs 5)?

## Architecture Onboarding

- **Component map:** Audio segment (resampled to model-specific rate) -> Pre-trained network (EfficientNet, HuBERT, ViT) -> Flattening/Pooling attention heads to produce Embedding Vector (dim 384-1280) -> UMAP reduction to 300 dimensions (optional) -> K-Means (Clustering) or kNN (Classification) acting directly on embeddings

- **Critical path:** The Embedding Extraction step is critical. If the backbone is stripped incorrectly or input padding corrupts the signal, the embedding vector will not represent the acoustic content meaningfully.

- **Design tradeoffs:**
  - Supervised (e.g., Perch): Best for in-domain tasks (Birds → Birds), lower engineering overhead
  - SSL (e.g., AVES): High variance but essential for cross-domain tasks (Birds → Frogs) or when labeled data is scarce
  - UMAP: Improves clustering visualization and performance but adds a non-parametric processing step that must be re-fitted for every new dataset

- **Failure signatures:**
  - The "Cloud" Effect: Visualizing embeddings shows a single unstructured blob of colors, indicating the model failed to separate classes acoustically
  - Domain Collapse: High performance on bird data but near-random performance on frog data implies embeddings are overfitted to bird-specific spectral traits

- **First 3 experiments:**
  1. **Baseline UMAP Ablation:** Extract embeddings using BirdNET on bird dataset. Run K-Means on raw embeddings vs. UMAP-reduced embeddings. Verify the AMI increase reported in Table 2.
  2. **Cross-Domain Stress Test:** Extract embeddings from AVES model and Perch model using frog dataset. Train kNN classifier on both. Confirm if AVES generalizes better as claimed.
  3. **Architecture Teardown:** Compare embedding space of AVES (HuBERT) vs. Animal2Vec (d2v2.0) on frog dataset to validate that Acoustic Unit Discovery tokenization yields superior structure.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Generalization: Results may not generalize to other taxonomic groups or acoustic environments beyond birds and frogs
- UMAP Parameter Sensitivity: The study doesn't extensively explore parameter sensitivity, which could significantly impact clustering performance
- Input Preprocessing Variations: The paper doesn't fully specify audio preprocessing variations across models, particularly for handling variable-length recordings

## Confidence
- **High Confidence:** Supervised learning models' superior performance in in-domain clustering tasks is well-supported by consistent results across both datasets and multiple evaluation metrics
- **Medium Confidence:** The claim about AVES models' superior cross-domain generalization is supported by results but limited to the specific bird-to-frog transfer scenario
- **Low Confidence:** The specific mechanisms explaining why certain self-supervised architectures outperform others in generalization remain largely theoretical

## Next Checks
1. **Cross-Taxa Generalization Test:** Evaluate the same 15 models on a third dataset from a different