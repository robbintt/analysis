---
ver: rpa2
title: 'SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding
  and Reasoning into Large Audio-Language Models'
arxiv_id: '2511.06606'
source_url: https://arxiv.org/abs/2511.06606
tags:
- spatial
- audio
- reasoning
- sound
- lalms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPUR introduces a lightweight, plug-and-play approach to equip
  large audio-language models with spatial perception using First-Order Ambisonics
  (FOA) inputs. It combines a spatial encoder that extracts rotation-aware, listener-centric
  features from FOA signals with a multimodal adapter that integrates these into existing
  LALMs without altering their core architecture.
---

# SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding and Reasoning into Large Audio-Language Models

## Quick Facts
- arXiv ID: 2511.06606
- Source URL: https://arxiv.org/abs/2511.06606
- Reference count: 20
- Large audio-language models equipped with spatial perception using FOA inputs improve spatial question answering and multi-speaker attribution

## Executive Summary
SPUR introduces a lightweight, plug-and-play approach to equip large audio-language models with spatial perception using First-Order Ambisonics (FOA) inputs. It combines a spatial encoder that extracts rotation-aware, listener-centric features from FOA signals with a multimodal adapter that integrates these into existing LALMs without altering their core architecture. To train and evaluate this spatial reasoning, SPUR-Set was developed—a benchmark of multi-event FOA recordings and controlled simulations annotated with spatially grounded captions and question-answer pairs across six reasoning skills such as direction, proximity, and overlap. Experiments show that fine-tuning LALMs with SPUR improves spatial question answering and multi-speaker attribution while preserving general audio understanding. Ablations confirm the effectiveness of the spatial encoder design, adapter placement, and supervision mixtures. Compared to binaural baselines, SPUR demonstrates superior spatial consistency and reasoning depth, validating its ability to transform monaural LALMs into spatially aware models.

## Method Summary
SPUR integrates spatial perception into large audio-language models by extracting rotation-invariant spatial features from FOA inputs and injecting them via a lightweight adapter. The spatial encoder computes banded covariance matrices from FOA channels, applies temporal smoothing, and vectorizes to Spectro-Spatial Covariance Vectors (SSCV). These are processed through 3D convolutions, patch tokenization, and transformers, then projected to match the LALM's embedding space. The adapter injects these spatial embeddings into the frozen audio encoder, and LoRA (rank 8) is applied to the LLM layers during fine-tuning. SPUR-Set, a benchmark of real and simulated FOA data with spatially grounded annotations, enables training and evaluation across six reasoning skills.

## Key Results
- SPUR models outperform binaural and monaural baselines on spatial question answering (7.38 vs 2.48 average score)
- Spatial encoder + adapter improves multi-speaker attribution while preserving general audio understanding
- Ablation studies confirm effectiveness of spatial encoder design, adapter placement, and supervision mixtures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Banded covariance extraction from FOA channels encodes spatial geometry into differentiable features.
- Mechanism: The FOA signal (W, X, Y, Z) undergoes STFT, then mel-banded covariance matrices are computed per frame. These capture inter-channel phase correlations—the ITD/ILD cues humans use for localization. One-pole temporal smoothing (with learnable α) stabilizes features while preserving motion cues. The Hermitian covariance matrices are vectorized into real-valued Spectro-Spatial Covariance Vectors (SSCV), normalizing energy ratios to make features rotation-invariant.
- Core assumption: FOA's four channels contain sufficient spatial information for listener-centric reasoning; mel-band aggregation preserves perceptually relevant spatial cues.
- Evidence anchors:
  - [Section 3, steps 1-3]: "This representation encodes bandwise energy and inter-channel phase correlations—crucial for spatial reasoning."
  - [corpus]: SonicBench (arXiv:2601.11039) confirms LALMs struggle with physical audio attributes including spatial location, motivating explicit spatial feature extraction.
- Break condition: If input is monaural or binaural, the 4×4 covariance structure degrades; if scenes have extreme reverb without direct sound, covariance estimates become noisy.

### Mechanism 2
- Claim: A lightweight adapter projects spatial features into frozen LALM embedding space without disrupting pretrained acoustic representations.
- Mechanism: SSCV tensors pass through 3D convolutions (capturing time-frequency-space correlations), then patch tokenization and transformer layers produce geometry-aware tokens. An MLP projects these to match the existing audio encoder's input dimension. The adapted embeddings are added to—not replacing—the encoder's output. Only the SPUR encoder, MLP, and LoRA (rank 8) on LLM layers are trained; the base audio encoder and LLM weights remain frozen.
- Core assumption: The pretrained LALM's semantic audio knowledge is complementary to, not conflicting with, explicit spatial features; adapter projection can align feature spaces without catastrophic forgetting.
- Evidence anchors:
  - [Section 3, step 6]: "These adapted spatial embeddings are then injected into the existing audio encoder."
  - [Section 4]: "All original components of the base LALMs—including the audio encoder and the language model—are kept frozen."
  - [corpus]: AudioMotionBench (arXiv:2511.13273) shows LALMs lack spatial dynamics perception, validating the need for explicit spatial injection rather than hoping monaural models infer it.
- Break condition: If adapter width is too narrow, spatial features are compressed away; if too wide, overfitting to SPUR-Set occurs. If LoRA rank is insufficient, the LLM cannot integrate spatial tokens into reasoning.

### Mechanism 3
- Claim: Skill-structured supervision on spatial reasoning tasks teaches the model to ground language in geometry.
- Mechanism: SPUR-Set provides QA pairs across six skills: Spatial Priority & Source Intent, Proximity-Based Decision Making, Directional Conflict Resolution, Group Dynamics & Spatial Clustering, Environmental Awareness & Navigation, and Scene Reconfiguration (mental rotation). Questions require multi-hop inference (e.g., "which voice responds to laughter from which direction"). An auxiliary SELD objective jointly predicts event class + DoA, reinforcing spatial grounding.
- Core assumption: Decomposing spatial reasoning into skills enables systematic learning; real + simulated FOA mix provides sufficient diversity; GPT-5-generated QAs from spatial metadata are groundable.
- Evidence anchors:
  - [Section 3.1]: "SPUR-Set comprises six novel skill-focused datasets... each built through custom data curation pipelines."
  - [Table 1]: Per-skill scores show SPUR models outperform baselines on all seven dimensions, with largest gains on SPSI (7.15 vs 5.79) and PBDM (7.22 vs 3.79).
  - [corpus]: Audio-CoT (arXiv:2501.07246) demonstrates chain-of-thought reasoning improves LALM performance, supporting multi-hop spatial QA design.
- Break condition: If QA distribution is skewed toward one skill, others underperform; if simulation-to-real gap is large, test-set generalization degrades.

## Foundational Learning

- Concept: **First-Order Ambisonics (FOA)**
  - Why needed here: FOA is the input format—a 4-channel spherical harmonic representation (W=omnidirectional, X/Y/Z=dipole axes). Understanding that these channels encode direction via phase relationships is prerequisite to grasping why covariance extraction works.
  - Quick check question: Given FOA channels (W, X, Y, Z), which axis would show strongest correlation if a sound source is directly in front (0° azimuth, 0° elevation)?

- Concept: **Interaural Cues and Spatial Perception**
  - Why needed here: The paper assumes readers understand ITD (interaural time difference) and ILD (interaural level difference) as the basis for human sound localization. SPUR's covariance features explicitly capture the multi-channel analogs of these cues.
  - Quick check question: Why would a monaural LALM struggle to infer direction even with extensive training data?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: SPUR uses LoRA (rank 8) to adapt the frozen LLM without modifying base weights. Understanding low-rank adaptation is essential to replicate the training setup and diagnose why full fine-tuning isn't used.
  - Quick check question: If LoRA rank were increased to 64, what tradeoff would you expect in terms of spatial reasoning vs. general audio understanding preservation?

## Architecture Onboarding

- Component map:
FOA Audio (4ch, 16kHz, 10s)
    ↓
STFT → Banded Covariance (T×B×4×4)
    ↓
One-Pole Smoothing (learnable α)
    ↓
Vectorization → SSCV (T×B×16)
    ↓
3D Conv Stack (kernel 1×3×3) + Pooling
    ↓
Patch Tokenization (16×16) + Transformer (N layers)
    ↓
MLP Projector → z_adapt
    ↓
[Frozen] Audio Encoder → h_audio
    ↓
[Frozen] LLM + [LoRA r=8] → Text Output

- Critical path: SSCV extraction → 3D Conv → Transformer adaptation → MLP projection. If any stage fails, spatial features don't reach the LLM in usable form. The covariance computation (Equation 2) and vectorization (Equation 5) are numerically sensitive—off-diagonal conjugate pair handling must preserve phase information.

- Design tradeoffs:
  - **FOA vs. binaural**: FOA provides full 3D coverage but requires 4 channels; binaural (2 channels) limits elevation perception. Paper shows BAT (binaural) scores 2.48 avg vs. SPUR's 7.38.
  - **Frozen encoder vs. full fine-tuning**: Freezing preserves general audio understanding but limits how deeply spatial features can be integrated; LoRA provides a middle ground.
  - **Real vs. simulated data**: Simulation enables controlled spatial diversity but risks domain gap; SPUR-Set mixes both (STARSS23, TAU-NIGENS, L3DAS23 + 10k simulated).

- Failure signatures:
  - **Direction hallucination**: Model outputs confident but wrong spatial descriptions (e.g., "front-left" when source is rear-right). Check: SSCV normalization may have collapsed; verify r1≠0 in Equation 5.
  - **Spatial-semantic dissociation**: Model answers "what" correctly but ignores "where." Check: MLP projector may be undertrained; adapter learning rate may be too low.
  - **Rotation inconsistency**: Model fails Scene Reconfiguration tasks. Check: One-pole smoothing α may be over-smoothing motion cues.
  - **Catastrophic forgetting**: General audio QA performance drops. Check: LoRA rank too high or SPUR-Set mixing ratio too dominant.

- First 3 experiments:
  1. **Sanity check**: Input synthetic FOA with single source at known (azimuth, elevation, distance). Verify model's QA response matches ground truth before training. This validates the pipeline end-to-end.
  2. **Ablation on adapter placement**: Train variants with spatial features injected at (a) audio encoder input, (b) encoder output, (c) LLM input. Compare spatial QA scores to identify optimal injection point (paper uses encoder input).
  3. **Real vs. simulated supervision mix**: Train on 100% real, 100% simulated, and 50/50 mix. Evaluate on SPUR-Set-Test (human-verified) and STARSS23 held-out split. Quantify simulation-to-real transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale when extending the SPUR encoder from First-Order Ambisonics (FOA) to Higher-Order Ambisonics (HOA) or mixed microphone arrays?
- Basis in paper: [explicit] The authors state in the Limitations section that "Extending the encoder to higher-order formats or mixed microphone arrays could further enrich geometric fidelity and scene coverage."
- Why unresolved: The current architecture is specifically designed for the four-channel FOA (WXYZ) format and its covariance features, which inherently constrain spatial resolution.
- What evidence would resolve it: Comparative experiments evaluating the adapter on HOA datasets, measuring localization accuracy and reasoning depth against the FOA baseline.

### Open Question 2
- Question: Can the architecture be modified to model dynamic listener movement and viewpoint shifts rather than relying on static embeddings?
- Basis in paper: [explicit] The paper notes that "the adapter still relies on static embeddings and does not model listener movement or dynamic viewpoint shifts."
- Why unresolved: The current pipeline processes fixed-perspective FOA inputs without a mechanism for continuous egocentric updates or temporal viewpoint reasoning.
- What evidence would resolve it: Integrating recurrent or motion-conditional modules and testing on tasks requiring continuous rotation or translation invariance.

### Open Question 3
- Question: Can SPUR be effectively integrated with visual and 3D geometric modalities to enable cross-modal spatial reasoning?
- Basis in paper: [explicit] The authors identify "opportunities to explore cross-modal extensions—integrating vision, 3D geometry, and reinforcement-driven auditory navigation."
- Why unresolved: The current work isolates audio-language reasoning and does not establish alignment between spatial audio features and visual/3D scene representations.
- What evidence would resolve it: A multimodal benchmark testing alignment between SPUR's spatial audio embeddings and visual depth/position data in simulated 3D environments.

### Open Question 4
- Question: Does training on SPUR-Set generalize to uncontrolled, real-world environments with diverse acoustic properties?
- Basis in paper: [explicit] The authors note the dataset "primarily focuses on controlled room acoustics and limited real-world diversity."
- Why unresolved: Performance on simulated or controlled data may not reflect robustness in "in-the-wild" scenarios with unpredictable noise, reverb, or multilingual content.
- What evidence would resolve it: Evaluation on a new benchmark of uncontrolled, multilingual spatial audio recordings outside the simulation parameters.

## Limitations

- SPUR-Encoder architecture details (number of 3D Conv blocks, transformer layers, hidden dimensions) are underspecified, hindering faithful reproduction
- Performance relies on LLM-as-judge for spatial consistency and reasoning depth, which may not fully capture spatial accuracy
- Real-vs-simulated data mixing strategy lacks quantification of simulation-to-real transfer gap

## Confidence

**High Confidence** in the core scientific claim that LALMs struggle with spatial perception and that explicit spatial feature extraction from FOA improves spatial reasoning. Supported by clear performance gap and coherent mechanism.

**Medium Confidence** in the plug-and-play claim. While demonstrated across multiple base LALMs, exact adapter placement and projection dimensions are not fully specified, and LoRA rank may limit integration depth.

**Low Confidence** in evaluation completeness. Heavy reliance on LLM-as-judge, lack of human evaluation or domain-specific baselines, and unaddressed robustness to noise/reverberation limit real-world deployment confidence.

## Next Checks

1. **Architecture Transparency Check**: Implement a minimal SPUR-Encoder with configurable parameters (number of 3D Conv blocks, transformer layers, patch size, mel bands). Systematically sweep these hyperparameters and report spatial QA performance to identify the minimal viable configuration and quantify sensitivity.

2. **Real vs. Simulated Generalization Gap**: Train SPUR models on 100% real FOA (STARSS23, TAU-NIGENS, L3DAS23), 100% simulated, and mixed data. Evaluate on both real and simulated test splits of SPUR-Set. Report per-skill accuracy and SELD metrics to quantify simulation-to-real transfer and identify which skills are most sensitive to domain shift.

3. **Spatial Accuracy vs. LLM-as-Judge**: Select a subset of SPUR-Set test questions. Have human annotators (or a spatial audio expert system) score the spatial consistency and reasoning depth independently of the LLM-as-judge. Compare human vs. LLM scores to quantify agreement and identify potential biases in the LLM evaluation.