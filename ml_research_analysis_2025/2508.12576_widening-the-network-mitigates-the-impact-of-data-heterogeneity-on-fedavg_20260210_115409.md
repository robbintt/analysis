---
ver: rpa2
title: Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg
arxiv_id: '2508.12576'
source_url: https://arxiv.org/abs/2508.12576
tags:
- data
- fedavg
- learning
- global
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how network width affects the convergence and
  generalization of federated learning under non-IID data conditions. The authors
  prove that increasing the width of neural networks reduces the impact of data heterogeneity
  on federated averaging (FedAvg), with the model divergence bounded by O(n^{-1/2})
  and vanishing as the width n approaches infinity.
---

# Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg

## Quick Facts
- arXiv ID: 2508.12576
- Source URL: https://arxiv.org/abs/2508.12576
- Reference count: 40
- Primary result: Increasing neural network width reduces data heterogeneity impact in federated learning

## Executive Summary
This paper establishes that network width serves as a powerful knob to control the impact of data heterogeneity in federated learning. Through NTK-based analysis, the authors prove that as network width increases, the divergence between global and local models in FedAvg decreases, with the worst-case bound scaling as O(n^{-1/2}) where n is the network width. In the infinite-width limit, FedAvg becomes equivalent to centralized gradient descent, achieving identical convergence rates and generalization performance regardless of data heterogeneity.

The theoretical findings are validated across multiple architectures (fully-connected networks, CNNs, and ResNets) on MNIST and CIFAR-10 datasets. The work provides quantitative guidance for practitioners on how to leverage network width to mitigate heterogeneity issues, bridging overparameterized neural network theory with federated learning practice.

## Method Summary
The authors employ Neural Tangent Kernel (NTK) theory to analyze FedAvg convergence under data heterogeneity. They establish bounds on model divergence between global and local models, showing that this divergence decreases with increasing network width. The analysis assumes full client participation and synchronous updates, using gradient flow approximation to derive convergence guarantees. The theoretical framework is then validated through experiments on standard image classification benchmarks using various network architectures.

## Key Results
- Model divergence between global and local models is bounded by O(n^{-1/2}) and vanishes as width n approaches infinity
- In infinite-width regime, FedAvg converges at the same rate as centralized gradient descent
- Global and local models behave as linear models with constant neural tangent kernels in the infinite-width limit
- Experiments on MNIST and CIFAR-10 validate theoretical findings across fully-connected networks, CNNs, and ResNets

## Why This Works (Mechanism)
The mechanism relies on the Neural Tangent Kernel (NTK) framework, where wider networks become increasingly linear in their parameter space. As networks approach infinite width, they exhibit behavior similar to kernel methods with a fixed NTK, making the optimization landscape smoother and less sensitive to data heterogeneity. This linearization effect reduces the divergence between locally trained models on different data distributions and the globally optimal model, effectively decoupling convergence from data heterogeneity.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: The NTK describes how neural networks behave as kernel methods in the infinite-width limit. Understanding NTK is essential because it provides the theoretical foundation for analyzing overparameterized networks and their convergence properties in federated settings.

**Quick check**: Verify that as width increases, the NTK becomes constant and the network behaves linearly in parameter space.

**Data Heterogeneity in FL**: Non-IID data distributions across clients create challenges for federated learning convergence. This concept is crucial because the paper's main contribution is showing how width can mitigate these heterogeneity-induced convergence issues.

**Quick check**: Confirm that model divergence increases with the degree of data heterogeneity between clients.

**FedAvg Algorithm**: The standard federated averaging algorithm where clients train locally and upload model updates. Understanding FedAvg is necessary to grasp how local training on heterogeneous data affects global model convergence.

**Quick check**: Verify that FedAvg reduces to gradient descent in the infinite-width limit with constant NTK.

## Architecture Onboarding

**Component Map**: Input Data -> Local Training (Client-side) -> Model Aggregation (Server-side) -> Global Model Update -> Output Model

**Critical Path**: The critical path is the flow from local training through aggregation to global update. Network width affects this path by reducing the divergence between local and global models, making the aggregation step more effective at producing a better global model.

**Design Tradeoffs**: Wider networks provide better heterogeneity mitigation but increase computational cost and memory requirements per client. The tradeoff is between convergence robustness and resource efficiency, with the paper suggesting width as a parameter to tune based on heterogeneity levels.

**Failure Signatures**: If network width is insufficient, model divergence between local and global models will remain significant, leading to poor convergence and generalization gaps between centralized and federated training.

**First Experiments**:
1. Compare convergence rates of FedAvg with varying widths on the same heterogeneous dataset
2. Measure model divergence (parameter differences) between local and global models across different widths
3. Evaluate generalization performance gap between centralized and federated training as width varies

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results rely on infinite-width assumption, which may not hold for practical network sizes
- Experiments focus on relatively simple image classification tasks, limiting generalizability to complex FL applications
- Analysis assumes full client participation and synchronous updates, not reflecting realistic partial participation scenarios

## Confidence
- Convergence bounds and NTK equivalence claims: High
- Practical relevance of infinite-width results to finite-width networks: Medium
- Generalizability across diverse FL applications and architectures: Medium

## Next Checks
1. Conduct ablation studies varying network widths on the same tasks to quantify how quickly theoretical benefits diminish as width decreases from infinity
2. Test width-heterogeneity relationship on non-vision tasks (e.g., language modeling or tabular data) to assess domain generalizability
3. Evaluate performance under partial client participation and heterogeneous compute constraints to verify practical robustness of width-based mitigation strategy