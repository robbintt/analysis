---
ver: rpa2
title: 'AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents'
arxiv_id: '2508.01012'
source_url: https://arxiv.org/abs/2508.01012
tags:
- design
- synthesis
- language
- automation
- codebleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoEDA introduces a standardized, fine-tuning-free LLM agent framework
  for automating RTL-to-GDSII flows in EDA workflows. The system leverages the Model
  Context Protocol (MCP) to orchestrate microservice-based synthesis, placement, clock
  tree synthesis (CTS), and routing tools through natural language inputs.
---

# AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents

## Quick Facts
- arXiv ID: 2508.01012
- Source URL: https://arxiv.org/abs/2508.01012
- Reference count: 40
- AutoEDA achieves 2.4x improvement in CodeBLEU scores and 75% token reduction compared to in-context learning baselines

## Executive Summary
AutoEDA introduces a standardized, fine-tuning-free LLM agent framework for automating RTL-to-GDSII flows in EDA workflows. The system leverages the Model Context Protocol (MCP) to orchestrate microservice-based synthesis, placement, clock tree synthesis (CTS), and routing tools through natural language inputs. Key innovations include structured prompt engineering, intelligent parameter extraction, and a custom CodeBLEU metric tailored for TCL script evaluation. Evaluated on 100 diverse benchmark prompts across five RTL designs, AutoEDA demonstrates superior automation accuracy, efficiency, and script quality while maintaining open-source accessibility for community adoption.

## Method Summary
AutoEDA implements a microservice-based LLM agent framework that automates EDA workflows through natural language processing. The system uses structured prompt engineering and the Model Context Protocol (MCP) to orchestrate multiple EDA tools including synthesis, placement, CTS, and routing. A custom CodeBLEU metric was developed specifically for evaluating TCL script quality. The framework processes natural language prompts to generate and execute EDA scripts without requiring model fine-tuning, instead relying on intelligent parameter extraction and prompt optimization techniques.

## Key Results
- 2.4x improvement in CodeBLEU scores compared to in-context learning baselines
- Over 75% reduction in output token usage
- Superior automation accuracy and script quality demonstrated across 100 benchmark prompts and five RTL designs

## Why This Works (Mechanism)
The framework's effectiveness stems from its microservice architecture that enables modular tool orchestration through the Model Context Protocol. Structured prompt engineering ensures consistent interpretation of natural language inputs, while intelligent parameter extraction optimizes the translation from user intent to executable TCL scripts. The custom CodeBLEU metric provides targeted evaluation for EDA-specific script quality, enabling precise performance measurement and iterative improvement of the automation pipeline.

## Foundational Learning
- **Model Context Protocol (MCP)**: Protocol for microservice communication in EDA tool orchestration - needed for reliable multi-tool integration, quick check: verify MCP message passing between services
- **CodeBLEU Metric**: Custom metric for TCL script evaluation - needed for domain-specific quality assessment, quick check: compare against standard BLEU on sample scripts
- **Structured Prompt Engineering**: Method for consistent natural language interpretation - needed for reliable automation, quick check: test with varied prompt phrasings
- **Intelligent Parameter Extraction**: Technique for mapping user intent to tool parameters - needed for accurate script generation, quick check: validate parameter mappings against tool documentation
- **Microservice Architecture**: Modular design for EDA tool integration - needed for scalability and maintainability, quick check: test individual service failures
- **Fine-tuning-free LLM Approach**: Method using prompt engineering instead of model adaptation - needed for accessibility and reduced computational overhead, quick check: benchmark against fine-tuned alternatives

## Architecture Onboarding
- **Component Map**: Natural Language Input -> Prompt Engineering -> MCP Orchestrator -> Microservices (Synthesis, Placement, CTS, Routing) -> TCL Script Generation -> EDA Tool Execution
- **Critical Path**: User Prompt → Structured Prompt Engineering → MCP Orchestrator → Tool Selection → Script Generation → Execution
- **Design Tradeoffs**: Fine-tuning-free approach sacrifices potential optimization gains for accessibility and reduced computational overhead; microservice architecture increases complexity but enables modular tool integration
- **Failure Signatures**: Prompt misinterpretation leads to incorrect tool selection; MCP communication failures cause workflow interruption; parameter extraction errors produce invalid TCL scripts
- **First Experiments**: 1) Test single-tool automation accuracy with varied prompt phrasings, 2) Evaluate MCP latency and reliability under concurrent tool requests, 3) Measure CodeBLEU performance on increasing RTL design complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted test set of 100 prompts across five RTL designs limits generalizability