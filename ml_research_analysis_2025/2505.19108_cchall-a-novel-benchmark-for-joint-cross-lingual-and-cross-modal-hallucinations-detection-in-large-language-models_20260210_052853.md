---
ver: rpa2
title: 'CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations
  Detection in Large Language Models'
arxiv_id: '2505.19108'
source_url: https://arxiv.org/abs/2505.19108
tags:
- hallucination
- cross-lingual
- image
- cross-modal
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CCHall, a novel benchmark for detecting joint
  cross-lingual and cross-modal hallucinations in large language models. Unlike prior
  work that focuses on single scenarios, CCHall integrates both cross-lingual and
  cross-modal hallucination detection to better reflect real-world complexity.
---

# CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models

## Quick Facts
- arXiv ID: 2505.19108
- Source URL: https://arxiv.org/abs/2505.19108
- Reference count: 36
- Primary result: Introduces CCHall benchmark detecting joint cross-lingual and cross-modal hallucinations with 77.5% accuracy for best model

## Executive Summary
This paper introduces CCHall, a novel benchmark for detecting joint cross-lingual and cross-modal hallucinations in large language models. Unlike prior work that focuses on single scenarios, CCHall integrates both cross-lingual and cross-modal hallucination detection to better reflect real-world complexity. The benchmark is constructed using multimodal tasks (VQA, image captioning) and covers 9 languages across low, medium, and high-resource categories. Evaluation of 12 models (including GPT-4o, Gemini-1.5-Flash, and open-source variants) shows strong performance gaps, with the best model achieving 77.5% accuracy. Analysis reveals that high-resource languages and high-resolution images improve detection accuracy, while longer responses increase hallucination rates. The benchmark highlights the need for advanced mitigation strategies and serves as a resource for evaluating model robustness in multilingual, multimodal contexts.

## Method Summary
CCHall is constructed through a three-phase approach: data collection from VQA and image captioning tasks, hallucination generation across 9 languages spanning different resource levels, and annotation by human evaluators to establish ground truth. The benchmark evaluates 12 models including GPT-4o, Gemini-1.5-Flash, and various open-source variants on their ability to detect hallucinations that occur at the intersection of language and modality. The evaluation framework measures accuracy across different language resource levels, image resolutions, and response lengths to identify factors affecting detection performance. The methodology emphasizes real-world applicability by simulating scenarios where language barriers and multimodal inputs create conditions conducive to hallucinations.

## Key Results
- CCHall benchmark achieves 77.5% accuracy for the best-performing model in detecting joint cross-lingual and cross-modal hallucinations
- High-resource languages show significantly better detection accuracy compared to low-resource languages
- Image resolution positively correlates with detection performance, while longer responses increase hallucination rates

## Why This Works (Mechanism)
CCHall works by explicitly testing the intersection of cross-lingual and cross-modal capabilities in language models, where hallucination risks compound. By creating scenarios that require models to process information across language barriers while simultaneously interpreting visual content, the benchmark exposes weaknesses in models' ability to maintain factual consistency across modalities and languages. The approach recognizes that hallucinations often occur when models must integrate information from multiple sources under cognitive load, particularly when language proficiency is limited.

## Foundational Learning

**Cross-lingual hallucination**: When language models generate factually incorrect content due to limitations in multilingual understanding. Why needed: Models often perform poorly in low-resource languages, creating opportunities for hallucinations. Quick check: Test model accuracy across different language resource levels.

**Cross-modal hallucination**: Generation of incorrect content when processing multimodal inputs, particularly when visual information conflicts with or is missing from textual generation. Why needed: Integration of visual and textual modalities introduces new failure modes. Quick check: Compare performance on high vs low resolution images.

**Hallucination detection**: The ability to identify when generated content is factually incorrect or unsupported by source material. Why needed: Prevention is ideal but detection is necessary for practical deployment. Quick check: Measure accuracy of detection vs generation capabilities.

**Multimodal task complexity**: How task difficulty scales when combining language and vision processing. Why needed: Simple tasks may not reveal cross-modal hallucination vulnerabilities. Quick check: Vary task complexity and measure hallucination rates.

## Architecture Onboarding

**Component map**: Data Collection -> Hallucination Generation -> Annotation -> Model Evaluation -> Analysis

**Critical path**: The benchmark's core evaluation loop moves from generating cross-lingual, cross-modal scenarios to measuring model detection accuracy, with the most critical path being the quality of hallucination generation and ground truth annotation.

**Design tradeoffs**: The benchmark prioritizes real-world complexity over controlled experimental conditions, accepting that this makes direct comparison harder but better reflects deployment scenarios. This tradeoff means results may not isolate specific failure modes as cleanly as single-dimension benchmarks.

**Failure signatures**: Models show systematic weakness in low-resource languages and with lower-resolution images, suggesting resource-based limitations rather than fundamental architectural flaws. Response length consistently predicts hallucination likelihood across all tested models.

**3 first experiments**:
1. Test detection accuracy on a subset of 3 languages across all resource levels to validate the resource-level correlation
2. Evaluate performance difference between VQA and image captioning tasks to assess task-specific vulnerabilities
3. Measure detection accuracy with progressively longer responses to confirm the response length correlation

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark was tested on only 12 models, predominantly from a few major providers, limiting generalizability to the broader model landscape
- Reliance on specific multimodal tasks (VQA and image captioning) may not capture the full spectrum of real-world cross-lingual and cross-modal hallucination scenarios
- The study focuses on hallucination detection accuracy without addressing underlying causes or mitigation strategies feasibility in production environments

## Confidence

**High Confidence**: The benchmark construction methodology and the identification of performance gaps across models are well-supported by the experimental design.

**Medium Confidence**: The observed correlations between language resource levels, image resolution, and detection accuracy are plausible but require further validation with a broader set of tasks and models.

**Low Confidence**: The generalizability of the benchmark to real-world applications and its effectiveness in driving model improvements remain unproven without additional testing.

## Next Checks
1. Expand model evaluation to include a more diverse set of open-source and proprietary models, particularly those specialized in low-resource languages or niche domains
2. Test the benchmark on additional multimodal tasks (e.g., video understanding, document analysis) to assess its robustness across varied use cases
3. Conduct ablation studies to determine the relative contributions of cross-lingual and cross-modal components to hallucination detection accuracy