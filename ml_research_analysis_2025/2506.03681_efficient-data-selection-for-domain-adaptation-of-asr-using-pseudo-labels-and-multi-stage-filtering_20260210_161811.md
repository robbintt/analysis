---
ver: rpa2
title: Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and
  Multi-Stage Filtering
arxiv_id: '2506.03681'
source_url: https://arxiv.org/abs/2506.03681
tags:
- data
- selection
- speech
- segments
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of domain adaptation for automatic
  speech recognition (ASR) in resource-constrained settings, where small organizations
  lack labeled data and computational resources. The authors propose a multi-stage
  data selection pipeline that leverages pseudo-labels generated by Whisper and Zipformer
  models, combined with word error rate (WER) prediction, named entity recognition
  (NER), and character error rate (CER) analysis to filter high-quality training segments.
---

# Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering

## Quick Facts
- **arXiv ID:** 2506.03681
- **Source URL:** https://arxiv.org/abs/2506.03681
- **Reference count:** 0
- **Primary result:** Fine-tuning on 100 hours (1.4% of dataset) achieves 12.2% WER on Wow and 14.1% on Fisher, matching or surpassing full-dataset performance

## Executive Summary
This paper addresses domain adaptation for automatic speech recognition in resource-constrained settings by proposing a multi-stage data selection pipeline. The approach uses pseudo-labels generated by Whisper and Zipformer models, combined with WER prediction, NER, and CER analysis to filter high-quality training segments. Experimental results on Wow (7500 hours) and Fisher English (1878 hours) demonstrate that fine-tuning on just 100 hours achieves WER of 12.2% on Wow and 14.1% on Fisher, matching or surpassing performance on the full dataset. This represents a 98.6% reduction in computational costs while maintaining ASR accuracy.

## Method Summary
The method employs a three-stage filtering pipeline applied to unlabeled audio data. First, pseudo-labels are generated using Whisper Medium and Zipformer models. Second, a WER prediction classifier (SVM on HuBERT + XLM-R embeddings) filters out segments likely to have high WER. Third, a multi-model CER agreement filter retains segments where three ASR models (Whisper, Zipformer, Parakeet) show high inter-model agreement (average CER < 5%). The selected subset is then used for fine-tuning the target ASR model. The approach is evaluated on call center data (Wow) and conversational telephone data (Fisher English).

## Key Results
- 100-hour filtered subset achieves 12.2% WER on Wow and 14.1% on Fisher, matching full-dataset performance
- Multi-model CER agreement filtering outperforms single-model confidence and NER-based approaches
- WER prediction classifier achieves 83% accuracy on Wow and 89% on AMI test sets
- Entity distribution + high confidence NER sampling reaches 12.5% WER on Wow, close to baseline
- Best filtering strategy reduces dataset from 7500 hours to 100 hours (98.6% reduction)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-model agreement filtering (CER-based) identifies high-quality pseudo-labels more effectively than single-model confidence.
- **Mechanism:** Three ASR models independently transcribe each segment. Pairwise character error rates are computed between all model pairs; segments with average CER < 5% are retained. Low inter-model disagreement correlates with transcription accuracy.
- **Core assumption:** Agreement across heterogeneous ASR architectures implies pseudo-label accuracy, which transfers to downstream fine-tuning quality.
- **Evidence anchors:**
  - [abstract] "Fine-tuning on 7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our filtering reduces the dataset to 100 hours (1.4%) with similar performance"
  - [section 4.3] "Selecting segments with CER<5% achieves the lowest WERs among all data selection strategies: 12.2% (WOW) and 14.1% (Fisher) for Whisper, and 13.3% (WOW) and 14.6% (Fisher) for Zipformer. This even surpasses the full-dataset baseline"

### Mechanism 2
- **Claim:** Named entity presence with balanced class distribution improves domain adaptation by prioritizing information-rich segments.
- **Mechanism:** A DistilBERT NER model identifies segments containing named entities. The pipeline maintains entity class distribution while preferring high-confidence samples within each class.
- **Core assumption:** Named entities carry disproportionate importance for domain adaptation; balanced entity exposure prevents vocabulary collapse.
- **Evidence anchors:**
  - [section 4.3] "The best NER-based method is Entity Distribution + High Confidence, which reaches 12.5% (WOW)... close to the baseline. This shows that a balanced selection approach is crucial rather than relying purely on high-confidence entities"

### Mechanism 3
- **Claim:** WER prediction via acoustic-textual embeddings filters noisy segments before pseudo-label assignment.
- **Mechanism:** An SVM classifier predicts whether a segment will have high or low WER using concatenated HuBERT (acoustic) and XLM-R (textual) embeddings. Segments predicted as high-WER (>50%) are discarded.
- **Core assumption:** The WER classifier trained on out-of-domain data (AMI) generalizes sufficiently to in-domain unlabeled data.
- **Evidence anchors:**
  - [section 4.1] "The model demonstrates higher precision and recall for the low-WER class... achieving an overall accuracy of 83% on the Wow test set and 89% on the AMI test set"
  - [section 4.3] "Selecting segments with low WER performs better than random, achieving 13.1% (WOW) and 15.1% (Fisher) for Whisper"

## Foundational Learning

- **Concept: Pseudo-labeling for semi-supervised ASR**
  - Why needed here: The core problem is adapting ASR to a domain with 7500 hours of unlabeled audio but minimal labeled data. Pseudo-labels bridge this gap by using pretrained models to generate approximate transcriptions.
  - Quick check question: Can you explain why pseudo-labels introduce error propagation risk, and why filtering helps mitigate this?

- **Concept: Character Error Rate (CER) vs Word Error Rate (WER)**
  - Why needed here: The paper uses CER for inter-model agreement (character-level is finer-grained) and WER for final evaluation. Understanding both metrics is essential for interpreting filtering thresholds.
  - Quick check question: Why might CER be preferred over WER when comparing ASR outputs for agreement detection?

- **Concept: Named Entity Recognition in speech contexts**
  - Why needed here: The NER-based selection assumes named entities are domain-adaptation critical. Understanding how NER models work on ASR transcripts (with potential errors) clarifies the filtering logic.
  - Quick check question: How might ASR transcription errors propagate through an NER model, and how does the high-confidence threshold address this?

## Architecture Onboarding

- **Component map:**
  VAD Segmentation -> Pseudo-label Generation -> WER Classifier -> NER Filter -> CER Agreement Filter -> ASR Fine-tuning

- **Critical path:**
  1. Generate pseudo-labels for ALL unlabeled data first (expensive, one-time)
  2. Apply CER filter using all three ASR outputs → yields ~15-20% of original data
  3. Within CER-filtered set, optionally apply NER/WER filters for further refinement
  4. Fine-tune target ASR model on selected 100-hour subset

- **Design tradeoffs:**
  - CER threshold selection: Lower threshold (stricter) → smaller dataset, potentially higher quality but may over-prune diverse samples. Paper uses 5% without ablation.
  - N hours selection: Paper uses fixed 100 hours. Scaling this up may yield diminishing returns; optimal size likely domain-dependent.
  - Single vs multi-stage filtering: CER alone outperforms NER+WER, but computational cost of running 3 ASR models may be prohibitive. WER classifier is cheaper but less effective.

- **Failure signatures:**
  - WER classifier trained on wrong domain: If AMI (meetings) doesn't match call center acoustics, classifier may mislabel. Symptom: WER-based selection performs worse than random.
  - NER class imbalance: If target domain has rare entity types not in training data, entity-balanced sampling may fail. Symptom: NER-based methods underperform random selection.
  - CER threshold too strict: With threshold < 2%, may select < 10 hours, insufficient for fine-tuning. Symptom: Fine-tuned model overfits to narrow subset.

- **First 3 experiments:**
  1. Reproduce CER filtering on a 10-hour subset: Run Whisper, Zipformer, Parakeet on 10 hours of your target domain. Compute pairwise CER. Verify that 15-20% falls below 5% threshold (matches paper's distribution claim). This validates the pipeline before scaling.
  2. Ablate CER threshold: Test thresholds at 3%, 5%, 10%, 15% on a held-out validation set. Plot final WER vs. selected hours. Find the knee point for your domain.
  3. Compare single-model vs multi-model selection: Replace 3-model CER with single Whisper confidence scores. If performance drops significantly, multi-model agreement is essential; if similar, you can reduce inference costs.

## Open Questions the Paper Calls Out

- **Question:** Does the computational cost of running inference with multiple large ASR models for CER-based filtering negate the efficiency gains achieved by reducing the fine-tuning dataset size?
  - **Basis:** Section 2.1 describes the CER-based selection method which requires transcribing the entire unlabeled corpus using three separate ASR models before filtering can occur.
  - **Why unresolved:** While the paper demonstrates reduced training time, it does not quantify the inference costs of the filtering stage against the savings from training on only 100 hours.
  - **What evidence would resolve it:** A comparative analysis of total Floating Point Operations (FLOPs) or wall-clock time for the full filtering pipeline versus baseline fine-tuning on the complete dataset.

- **Question:** How robust is the pipeline's performance to variations in the specific thresholds used for WER classification (50%) and CER filtering (5%)?
  - **Basis:** Section 2.1 explicitly defines a 50% boundary for WER classes and a 5% threshold for CER, but provides no ablation study on how these hyperparameters were selected or their sensitivity.
  - **Why unresolved:** It is unclear if these specific cut-offs are dataset-independent heuristics or if they required tuning for the Wow and Fisher datasets.
  - **What evidence would resolve it:** A sensitivity analysis reporting final ASR WER while varying the WER percentile threshold and the average CER threshold.

- **Question:** Can this data selection pipeline be effectively transferred to low-resource languages or distinct acoustic domains where pseudo-label quality is significantly lower?
  - **Basis:** Section 3.1 limits the evaluation to English datasets (Wow and Fisher), while Section 1 frames the problem generally for "small organizations."
  - **Why unresolved:** The method relies on "high inter-ASR agreement" (CER) to identify quality data, a signal that may vanish in low-resource languages where all models perform poorly.
  - **What evidence would resolve it:** Experiments replicating the pipeline on a lower-resource language dataset to observe if the 100-hour filtered subset maintains performance parity with the full dataset.

## Limitations
- The exact fine-tuning hyperparameters for Whisper Medium are unspecified, making direct replication uncertain
- The paper demonstrates effectiveness on call center and conversational telephone domains, but generalizability to other ASR domains remains untested
- The computational advantage claim assumes access to pretrained models and doesn't account for the one-time cost of generating pseudo-labels across the full dataset

## Confidence
- **High confidence:** The multi-model CER agreement mechanism - directly supported by experimental results showing 12.2% WER outperforming full-dataset baselines
- **Medium confidence:** The WER prediction filtering mechanism - shows moderate improvement over random selection but is less effective than CER filtering
- **Medium confidence:** The NER-based filtering approach - demonstrates reasonable performance but is consistently outperformed by CER-based methods

## Next Checks
1. **CER threshold sensitivity analysis:** Conduct ablation studies across multiple CER thresholds (3%, 5%, 10%, 15%) on validation data to identify optimal trade-off between data retention and quality
2. **Cross-domain WER classifier validation:** Test the AMI-trained WER classifier on additional domain-shifted datasets to quantify generalization limits and identify failure patterns
3. **Single vs multi-model agreement comparison:** Implement and benchmark single-model confidence-based selection against the 3-model CER agreement to quantify the marginal benefit of multi-model consensus