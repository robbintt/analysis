---
ver: rpa2
title: 'Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs for
  Clinical Reasoning'
arxiv_id: '2502.08954'
source_url: https://arxiv.org/abs/2502.08954
tags:
- medical
- performance
- devices
- llms
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates on-device large language models (LLMs) for
  clinical reasoning using the AMEGA dataset across various mobile devices. It compares
  accuracy, performance, and thermal constraints of compact general-purpose models
  like Phi-3 Mini and medically fine-tuned models such as Med42 and Aloe.
---

# Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs for Clinical Reasoning

## Quick Facts
- arXiv ID: 2502.08954
- Source URL: https://arxiv.org/abs/2502.08954
- Reference count: 36
- Primary result: Compact general-purpose models like Phi-3 Mini balance speed and accuracy, while medically fine-tuned models like Med42 and Aloe achieve highest accuracy for clinical reasoning on mobile devices

## Executive Summary
This study evaluates on-device large language models for clinical reasoning tasks using the AMEGA dataset across various mobile devices. The research compares compact general-purpose models with medically fine-tuned variants, examining their accuracy, performance, and thermal constraints in real-world mobile deployment scenarios. The findings demonstrate that medically fine-tuned models achieve superior accuracy for clinical reasoning tasks, while compact general-purpose models offer better speed-accuracy trade-offs. The study also reveals that memory constraints pose greater challenges than processing power when deploying LLMs on older devices.

## Method Summary
The research evaluates multiple on-device LLMs on smartphones and tablets using the AMEGA dataset for medical question-answering tasks. Models tested include Phi-3 Mini, Phi-3 Small, Phi-3 Medium, and Gemini Nano for general-purpose models, alongside medically fine-tuned variants Med42 and Aloe. Testing was conducted across devices ranging from high-end (iPhone 15 Pro, Pixel 8 Pro) to low-end (Pixel 3a) hardware. The study employs automated prompt engineering and performance monitoring, including thermal management evaluation under sustained inference loads. Comparative analysis focuses on accuracy metrics, inference speed, memory utilization, and thermal behavior across different model architectures and device capabilities.

## Key Results
- Medically fine-tuned models (Med42, Aloe) achieve highest accuracy for clinical reasoning tasks
- Compact general-purpose models (Phi-3 Mini) provide optimal balance between speed and accuracy
- Memory constraints pose greater challenge than processing power for LLM deployment on older devices
- Thermal throttling occurs after approximately 5-10 minutes of sustained inference, requiring management strategies

## Why This Works (Mechanism)
The performance differences between model types stem from their architectural and training approaches. Medically fine-tuned models like Med42 and Aloe demonstrate superior accuracy because they undergo specialized training on medical datasets, allowing them to better understand clinical terminology and reasoning patterns. Their domain-specific knowledge enables more accurate responses to medical queries compared to general-purpose models. Conversely, compact general-purpose models like Phi-3 Mini achieve faster inference speeds due to their smaller parameter counts and optimized architectures for mobile deployment. These models trade some accuracy for computational efficiency, making them suitable for scenarios where response time is critical. The memory constraints observed on older devices primarily result from the large memory footprints required by LLMs, which exceed the RAM capacities of older hardware despite having sufficient processing power for inference operations.

## Foundational Learning
- **Clinical reasoning datasets** - AMEGA dataset provides standardized medical question-answering tasks for evaluating model performance; needed to ensure consistent, domain-relevant evaluation across different model types; quick check: verify dataset covers diverse medical specialties and reasoning complexity
- **Model quantization** - Process of reducing model precision to decrease memory footprint and improve inference speed on mobile devices; needed to make large models deployable on devices with limited resources; quick check: confirm quantization preserves acceptable accuracy levels
- **Thermal management in mobile inference** - Strategies to prevent performance degradation due to device heating during sustained LLM operations; needed to ensure consistent performance during extended clinical use; quick check: monitor temperature thresholds and throttling behavior during prolonged inference sessions

## Architecture Onboarding
**Component Map:** AMEGA dataset -> Model inference engines -> Performance monitoring -> Thermal management
**Critical Path:** Data loading → Model loading → Inference computation → Result generation → Performance logging
**Design Tradeoffs:** Accuracy vs. speed (medically fine-tuned models trade speed for accuracy), memory vs. performance (quantization reduces memory at potential accuracy cost), thermal vs. sustained performance (active cooling extends inference duration)
**Failure Signatures:** Memory exhaustion crashes on older devices, thermal throttling reducing inference speed by 30-50%, accuracy degradation with aggressive quantization
**First Experiments:** 1) Benchmark inference latency across all models on iPhone 15 Pro, 2) Measure memory utilization during model loading on Pixel 3a, 3) Monitor temperature increase during 10-minute continuous inference session

## Open Questions the Paper Calls Out
None

## Limitations
- Limited device diversity focusing primarily on recent high-end devices and single low-end device
- Reliance on single medical reasoning dataset (AMEGA) may not capture full clinical complexity
- Thermal management approach may not reflect real-world intermittent clinical usage patterns

## Confidence
- **High Confidence:** Comparative performance analysis between general-purpose and medically fine-tuned models is well-supported by empirical data
- **Medium Confidence:** Memory constraints outweighing processing power limitations based on single low-end device test case
- **Medium Confidence:** Thermal management findings may not fully represent real-world deployment scenarios with varied usage patterns

## Next Checks
1. Test model performance across broader range of devices including mid-range smartphones and tablets commonly used in clinical settings
2. Evaluate model accuracy using multiple medical reasoning datasets to ensure robustness across different clinical scenarios
3. Assess performance under realistic clinical usage patterns with intermittent rather than sustained inference loads