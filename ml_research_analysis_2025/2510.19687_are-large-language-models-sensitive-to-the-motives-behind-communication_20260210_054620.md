---
ver: rpa2
title: Are Large Language Models Sensitive to the Motives Behind Communication?
arxiv_id: '2510.19687'
source_url: https://arxiv.org/abs/2510.19687
tags:
- llms
- vigilance
- answer
- should
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) exhibit\
  \ motivational vigilance\u2014the ability to evaluate the intentions and incentives\
  \ behind communication. Using controlled experiments from cognitive science and\
  \ a rational model as a benchmark, the authors find that non-reasoning LLMs like\
  \ GPT-4o and Claude 3.5 Sonnet demonstrate human-like and approximately rational\
  \ vigilance in structured settings, successfully discounting biased advice."
---

# Are Large Language Models Sensitive to the Motives Behind Communication?

## Quick Facts
- **arXiv ID**: 2510.19687
- **Source URL**: https://arxiv.org/abs/2510.19687
- **Reference count**: 40
- **Primary result**: Non-reasoning LLMs show human-like vigilance in controlled settings but reasoning models and naturalistic contexts reveal significant limitations.

## Executive Summary
This paper investigates whether large language models exhibit motivational vigilance—the ability to evaluate the intentions and incentives behind communication. Using controlled experiments from cognitive science and a rational model as a benchmark, the authors find that non-reasoning LLMs like GPT-4o and Claude 3.5 Sonnet demonstrate human-like and approximately rational vigilance in structured settings, successfully discounting biased advice. However, reasoning models (o1, o3-mini, DeepSeek-R1) perform significantly worse. In naturalistic online settings, such as sponsored YouTube ads, LLM vigilance drops substantially, partly due to distraction by irrelevant context. Simple prompt steering that highlights speaker intentions and incentives can partially recover rational alignment. The findings suggest that while LLMs possess basic sensitivity to communication motives, further improvements are needed for reliable real-world deployment.

## Method Summary
The paper evaluates LLM vigilance through three experimental paradigms: (1) discriminating deliberate vs. incidental information in synthetic visual tasks, (2) calibrating belief updates in controlled vignettes varying speaker incentives and relationships across financial, real estate, and medical domains, and (3) generalizing to naturalistic YouTube sponsorship transcripts. LLMs are prompted using both direct queries and Chain-of-Thought reasoning, with performance measured as Pearson correlation between influence scores and a Bayesian rational model benchmark. The study tests multiple models including GPT-4o, Claude 3.5 Sonnet, and reasoning models (o1, o3-mini, DeepSeek-R1).

## Key Results
- Non-reasoning LLMs demonstrate human-like vigilance in controlled vignettes, successfully discounting advice when speakers have high incentives (r > 0.9 with rational model)
- Reasoning models (o1, o3-mini, DeepSeek-R1) show significantly lower vigilance performance (r ∈ [0.32, 0.72]) compared to non-reasoning models
- In naturalistic YouTube ads, LLM vigilance drops substantially, with correlation to rational model falling to r < 0.2 due to contextual distractions

## Why This Works (Mechanism)

### Mechanism 1: Approximate Bayesian Inference via Benevolence-Weighted Utility
Non-reasoning LLMs appear to approximate a rational Bayesian listener by implicitly weighing a speaker's incentives ($R_S$) against the listener's welfare ($R_L$), moderated by a latent "benevolence" parameter ($\lambda$). Instead of blindly trusting or rejecting input, the model evaluates the likelihood that a statement serves the listener's interest versus the speaker's. If a speaker stands to gain significantly ($R_S$ is high) and has low benevolence ($\lambda$ is low), the model discounts the advice.

### Mechanism 2: Attentional Dilution in Naturalistic Contexts
Vigilance capabilities are highly sensitive to signal-to-noise ratio; in naturalistic settings, "distracting" context crowds out the processing of intent-signaling features. Vigilance requires attending to specific cues (incentives, intent). In short, controlled vignettes, these cues are salient. In long transcripts (e.g., YouTube ads), the model's attention is dispersed across irrelevant narrative details, reducing the "salience" of the motive.

### Mechanism 3: The Reasoning Inversion (CoT/Reasoning Trap)
Explicit reasoning steps (Chain-of-Thought or dedicated reasoning models) can paradoxically degrade vigilance compared to direct inference. CoT may encourage the model to generate "just-so" stories that rationalize trust in the provided information, or reasoning models may be over-aligned to "helpfulness" which discourages skepticism.

## Foundational Learning

- **Epistemic vs. Motivational Vigilance**: Distinguish between checking if a source *knows* the truth (competence/epistemic) vs. if they *intend* to tell the truth (benevolence/motivational). The paper isolates the latter. Quick check: Does the model distrust the advice because the advisor is ignorant or because the advisor is paid to lie?

- **Rational Speech Acts (RSA) / Bayesian Inference**: The paper uses a "rational model" as its ground truth benchmark. Understanding that rational trust is a probabilistic calculation (prior × likelihood) is necessary to interpret the correlation scores ($r$ values). Quick check: If a Rational Agent is 90% sure a speaker is selfish ($\lambda \to 0$), how much should they update their belief based on that speaker's glowing product review?

- **Sycophancy vs. Vigilance**: The paper frames vigilance as an antidote to "sycophancy" (telling the user what they want to hear). Understanding this failure mode explains why LLMs might be naturally prone to low vigilance. Quick check: If a model agrees with a user's false belief to be "helpful," is it exercising vigilance?

## Architecture Onboarding

- **Component map**: Raw text (advice/transcript) -> Feature Extraction (Speaker Intent $u$, Incentive $R_S$, Benevolence $\lambda$) -> Inference Engine (LLM) -> Posterior belief score ($P(R_L|u)$) -> Intervention (Steering Prompts)
- **Critical path**: The path from *reading the text* to *estimating $\lambda$ (benevolence)*. If this estimation is noisy (distracted by long context), the entire downstream inference fails.
- **Design tradeoffs**: Direct vs. CoT (direct yields higher vigilance), Reasoning vs. Non-Reasoning Models (non-reasoning outperform currently), Ecological Validity vs. Control (vignettes show high vigilance, naturalistic ads show low vigilance)
- **Failure signatures**: "Sycophancy Drift" (updates too much toward speaker), "Distraction Collapse" (performance drops to near-zero correlation when input length increases), "Incentive Blindness" (failing to discount advice from a "Stranger" vs. "Lover" when incentive is high)
- **First 3 experiments**: 1) Replicate Exp 1: Feed "spied" info vs. "advice" and check if model updates beliefs more for incidental info, 2) Replicate Exp 2: Vary speaker's incentive and relationship, measure correlation of skepticism with incentive size, 3) Replicate Exp 3: Take long, noisy transcript, measure belief update without steering, then add "Consider motivations..." prompt and measure recovery

## Open Questions the Paper Calls Out

- **Can LLMs simultaneously integrate vigilance of competence (expertise) and vigilance of motivation (intent) into a unified rational model?** The paper's experiments isolate motivational vigilance but do not vary the perceived competence of the source, leaving the interaction between these two factors untested.

- **Why do state-of-the-art reasoning models (e.g., o1, DeepSeek-R1) exhibit significantly lower motivational vigilance than non-reasoning models?** The paper identifies the performance drop but doesn't determine if the cause is over-thinking, distraction, or a specific failure in the models' training to handle strategic communication.

- **Should the alignment goal for LLMs be normative rationality (optimal inference) or human-like inference (including heuristics and biases)?** The discussion notes that frontier LLMs often correlate better with human data than with the rational model, raising the question of whether we should align LLMs with "empirical patterns of human inference."

## Limitations

- **Ecological validity gap**: While LLMs show vigilance in controlled vignettes (r > 0.9), performance drops dramatically in naturalistic settings (r < 0.2), suggesting the mechanism may not generalize beyond structured scenarios.

- **Reasoning model paradox**: The finding that explicit reasoning (CoT) degrades rather than enhances vigilance is counterintuitive and lacks a mechanistic explanation beyond speculation about "amplified trust."

- **Measurement sensitivity**: Vigilance is operationalized as correlation with a Bayesian rational model, but the paper doesn't establish whether small correlation differences represent meaningful behavioral differences in real-world applications.

## Confidence

- **High confidence**: Non-reasoning LLMs demonstrate vigilance in controlled settings and successfully discount biased advice when cues are salient
- **Medium confidence**: Simple prompt steering can partially recover rational alignment in naturalistic contexts
- **Low confidence**: The reasoning inversion (CoT degrading vigilance) represents a fundamental limitation of reasoning models rather than a prompting artifact

## Next Checks

1. **Cross-paradigm replication**: Test the same LLMs on alternative vigilance tasks (e.g., detecting fake reviews, evaluating conflicting expert testimony) to verify whether the controlled vignette success transfers to structurally different scenarios.

2. **Ablation of distraction factors**: Systematically vary context length and type in Experiment 3 to identify which specific elements (irrelevant narrative, conversational fillers, visual descriptions) most strongly impair vigilance.

3. **Reasoning prompt optimization**: Test alternative CoT templates that explicitly prompt skepticism ("What incentive might the speaker have to mislead?") to determine whether the vigilance degradation is prompt-dependent rather than model-inherent.