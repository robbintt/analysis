---
ver: rpa2
title: 'Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised
  Neural Grammar Induction'
arxiv_id: '2509.20734'
source_url: https://arxiv.org/abs/2509.20734
tags:
- neural
- crnp
- probability
- grammar
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a key bottleneck in unsupervised neural grammar
  induction, termed probability distribution collapse (PDC), where distinct symbols
  are mapped to similar probability distributions, limiting model expressiveness even
  with large grammars. The authors analyze the causes of PDC across neural parameterization
  components and propose a collapse-relaxing neural parameterization (CRNP) to mitigate
  it.
---

# Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction

## Quick Facts
- arXiv ID: 2509.20734
- Source URL: https://arxiv.org/abs/2509.20734
- Reference count: 18
- Primary result: CRNP+PF achieves 76.8 S-F1 on PTB with only 90 nonterminals, outperforming models using up to 4,500 nonterminals

## Executive Summary
This paper identifies probability distribution collapse (PDC) as a critical bottleneck in unsupervised neural grammar induction, where distinct symbols map to similar probability distributions, limiting model expressiveness. The authors propose collapse-relaxing neural parameterization (CRNP) to mitigate PDC through embedding dimensionality expansion, child representation normalization, and training dynamics stabilization. CRNP significantly improves parsing performance across multiple languages while enabling compact grammars—achieving 70.2 S-F1 with just 90 nonterminals compared to baselines requiring 4,500 nonterminals. The method reduces PDC as measured by geometric mean of pairwise Jensen-Shannon divergence, demonstrating enhanced distributional diversity.

## Method Summary
CRNP modifies the neural parameterization of PCFGs by increasing symbol embedding dimensions to 256, normalizing children representation scales using cosine similarity instead of inner product, replacing ReLU with GELU activation, removing residual connections, and adding RMSNorm after activation. The method is evaluated on constituency parsing across English (PTB), Chinese (CTB), and eight languages from the SPMRL dataset. CRNP is combined with Parse-Focusing (PF) using trees from pretrained models. Training uses Adam optimizer with specific hyperparameters, batch size 4, and 30 epochs.

## Key Results
- CRNP+PF achieves 76.8 S-F1 on PTB with 90 nonterminals vs 70.2 with N-PCFG
- Monotonic improvement with embedding dimensions 64→256; CRNP outperforms N-PCFG beyond 128 dimensions
- Cosine similarity between binary rule distributions drops from 0.85 to 0.75 with CRNP
- CRNP maintains stable embedding norms while N-PCFG experiences exponential growth with grammar size

## Why This Works (Mechanism)

### Mechanism 1: Embedding Dimensionality Expansion
Increasing symbol embedding dimensions allows the neural parameterizer to represent a broader space of rule probability distributions. When embedding dimensions are insufficient relative to the number of possible rule distributions, a linear transformation cannot surjectively cover the full distribution space. Empirical evidence shows monotonic improvement with dimensions 64→256, while N-PCFG degrades beyond 128 dimensions.

### Mechanism 2: Children Scale Normalization
Normalizing children representation scales prevents entanglement across rules sharing the same children pair. The inner product computation involves a shared scale factor across all parent rules using the same children, which normalization removes. This enables more independent learning of rule probabilities, with cosine similarity dropping from 0.85→0.75 (binary) and 0.71→0.30 (unary) with normalization.

### Mechanism 3: Training Dynamics Stabilization
GELU activation and removing residual connections mitigate gradient explosion and dying ReLU, preventing collapse to uniform or one-hot distributions. ReLU's hard zero for negative inputs causes "dying" dimensions leading to uniform distributions, while gradient explosion leads to sharp distributions. GELU's smooth gradient stabilizes training, with zero ratio in binary representations dropping from 0.990 to 0.000.

## Foundational Learning

- **Probabilistic Context-Free Grammars (PCFGs):** Essential framework where rules have associated probabilities. Quick check: Can you explain how the inside-outside algorithm computes sentence probabilities?
- **Neural Parameterization:** Maps symbol embeddings to rule probabilities via neural networks. Quick check: What is the computational flow from symbol embedding to rule probability in equations (1)-(3)?
- **Jensen-Shannon Divergence:** Measures distribution similarity used to quantify PDC. Quick check: Why use JSD instead of KL divergence for measuring pairwise distribution differences?

## Architecture Onboarding

- **Component map:** Symbol embeddings → MLP (GELU + RMSNorm) → normalized dot product with child representations → softmax → rule probabilities → inside-outside algorithm → sentence likelihood
- **Critical path:** Symbol embeddings → MLP (GELU + RMSNorm) → normalized dot product with child representations → softmax → rule probabilities → inside-outside algorithm → sentence likelihood
- **Design tradeoffs:** Embedding size 256 vs. 512 shows marginal gains beyond 256; 2 vs. 4 layers saturates at 2 layers; 90 nonterminals achieves 70.2 S-F1 comparable to baselines with 4500 nonterminals
- **Failure signatures:** High pairwise JSD overlap indicates PDC; high zero ratio in representations indicates dying ReLU; exploding embedding norms indicate gradient explosion; rule probability histograms concentrated near uniform or one-hot
- **First 3 experiments:** 1) Ablation study removing GELU→ReLU and normalization separately; 2) Embedding sweep testing dimensions 64, 128, 256, 512 on PTB; 3) Grammar scaling comparing CRNP+PF vs. baselines at |N|=1, 5, 15, 30, 60, 90

## Open Questions the Paper Calls Out

- **CRNP adaptation for tensor-decomposed architectures:** Can CRNP be adapted for TN-PCFGs or Rank PCFGs? The authors note this is "constrained by tensor decomposition" and leave compatibility unresolved.

- **Language-specific PDC effectiveness:** What linguistic factors cause PDC mitigation effectiveness to vary across languages? The authors note CRNP performs "slightly less effectively" in German and Polish but state this is "out of the main scope."

- **PDC in non-PCFG architectures:** Does PDC act as a bottleneck in non-PCFG unsupervised parsing architectures like Structformer or DIORA? The paper focuses on N-PCFGs but suggests neural parameterization broadly imposes constraints.

## Limitations
- PDC relationship to parsing performance is empirically demonstrated but not mathematically proven
- Claims about PDC being a "critical bottleneck" may be overstated for morphologically complex languages
- Solution may not generalize universally—German and Polish show slight underperformance with 90 nonterminals

## Confidence
- **High Confidence:** Empirical improvements from CRNP (70.2→76.8 S-F1 on PTB) and embedding expansion/normalization effectiveness
- **Medium Confidence:** Causal mechanism linking PDC to poor performance is plausible but not definitively proven
- **Low Confidence:** Claims about PDC being a "critical bottleneck" and universal applicability across languages

## Next Checks
1. **Theoretical Analysis:** Prove formally that insufficient embedding dimensions relative to rule distributions prevent full representational capacity
2. **Cross-linguistic Stress Test:** Evaluate CRNP on morphologically complex languages (Turkish, Finnish) using varying grammar sizes to test universality
3. **Alternative Collapse Metrics:** Compare GPJ with maximum mean discrepancy and Wasserstein distance to verify PDC reduction isn't metric-dependent