---
ver: rpa2
title: Reference Grounded Skill Discovery
arxiv_id: '2510.06203'
source_url: https://arxiv.org/abs/2510.06203
tags:
- discovery
- reference
- latent
- rgsd
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling unsupervised skill
  discovery to high-dimensional robotic systems, where exploration space grows exponentially
  with dimensionality. The authors propose Reference-Grounded Skill Discovery (RGSD),
  which grounds skill discovery in a semantically meaningful latent space constructed
  from reference motion data using contrastive learning.
---

# Reference Grounded Skill Discovery

## Quick Facts
- arXiv ID: 2510.06203
- Source URL: https://arxiv.org/abs/2510.06203
- Reference count: 40
- The method successfully scales unsupervised skill discovery to 69-DoF humanoid systems with 359-dimensional observations.

## Executive Summary
This paper addresses the challenge of scaling unsupervised skill discovery to high-dimensional robotic systems where exploration space grows exponentially with dimensionality. The authors propose Reference-Grounded Skill Discovery (RGSD), which grounds skill discovery in a semantically meaningful latent space constructed from reference motion data using contrastive learning. The method pretrains an encoder to embed reference motions on a unit hypersphere, where each motion corresponds to a distinct direction. This grounding enables the agent to simultaneously imitate reference behaviors and discover semantically related variations. Evaluated on a 69-DoF SMPL humanoid, RGSD successfully imitates complex motions including walking, running, sidestepping, backward movement, and punching, while discovering diverse variations of these skills. The method outperforms both pure skill discovery baselines and imitation-based approaches in downstream locomotion tasks.

## Method Summary
RGSD addresses high-dimensional skill discovery by first pretraining a state encoder using contrastive learning on reference motion data. The encoder maps states to a unit hypersphere where each reference motion corresponds to a distinct direction. During reinforcement learning, the policy conditions on both states and latent skill vectors sampled either near reference embeddings (for imitation) or in their neighborhood (for discovery). The reward function derives from the encoder's log-probability density, effectively measuring cosine similarity to reference motion embeddings. This creates a structured exploration space where agents can simultaneously reproduce reference behaviors and discover semantically related variations while maintaining computational efficiency.

## Key Results
- RGSD successfully imitates 20 complex reference motions on a 69-DoF SMPL humanoid with 359-dimensional observations
- Outperforms pure skill discovery baselines (DIAYN, METRA) and imitation-based approaches (ASE, CALM, Meta-Motivo) in downstream locomotion tasks
- Achieves better success rates while maintaining commanded styles across walking, running, sidestepping, backward movement, and punching
- Enables test-time control over behavioral diversity through adjustment of the latent sampling distribution

## Why This Works (Mechanism)

### Mechanism 1: Semantic Manifold Grounding via Contrastive Pretraining
- **Claim:** Pretraining an encoder on reference motions constrains the high-dimensional exploration space to a semantically meaningful manifold before policy training begins.
- **Mechanism:** The method uses InfoNCE loss on a dataset of reference motions. By treating states from the same trajectory as positives and states from different trajectories as negatives, the encoder learns to map all states within a specific motion (e.g., "running") to a single direction on a unit hypersphere. This creates a structured latent space where distinct skills are separated by angle.
- **Core assumption:** The geometric structure learned from the limited reference dataset generalizes to the state space, such that interpolating between reference vectors corresponds to meaningful behavioral variations.
- **Evidence anchors:**
  - [abstract] "constructs a semantically meaningful latent space... using contrastive learning."
  - [section 4.1] "This grounding enables skill discovery to simultaneously involve both imitation... and discovery."
  - [corpus] Neighboring papers discuss "Human-Aligned Skill Discovery" and "Regret-Aware Optimization," but do not validate the specific contrastive grounding mechanism used here.
- **Break condition:** If the reference dataset lacks diversity, the latent space will be sparse, causing the "discovery" of new skills to collapse into undefined regions of the hypersphere.

### Mechanism 2: Reinterpretation of Mutual Information as Imitation
- **Claim:** The standard DIAYN discriminator reward functions as a valid feature-based imitation objective when conditioned on the embedding of a reference motion.
- **Mechanism:** Standard skill discovery maximizes $I(S;Z)$. RGSD fixes the latent $z$ to the embedding of a reference motion $z_m$. Because the encoder is trained to align all states of motion $m$ with $z_m$, the reward term $\log q_\phi(z_m|s)$ simplifies to maximizing the cosine similarity between the agent's current state embedding and the reference motion embedding.
- **Core assumption:** The encoder's mapping is locally quasi-concave around the reference states, ensuring that deviations from the reference motion monotonically decrease the reward.
- **Evidence anchors:**
  - [section 4.2] "We provide a theoretical proof establishing the validity of the proposed reward as a legitimate imitation signal."
  - [appendix d] Proves that the reward achieves its optimum at reference states and is locally quasi-concave.
- **Break condition:** If the agent drifts outside the local neighborhood of the reference motion (violating the quasi-concave assumption), the reward signal may fail to guide the agent back to the desired manifold.

### Mechanism 3: Geometric Interpolation for Discovery
- **Claim:** Sampling latent vectors in the neighborhood of reference embeddings induces semantically related variations rather than random behaviors.
- **Mechanism:** By using a von Mises-Fisher (vMF) distribution to sample $z$ near a reference direction $z_m$, the policy is incentivized to generate behaviors that maintain the "style" of the reference but vary in specific parameters (e.g., turning angle while running). This leverages the hypothesis that the latent space is smooth.
- **Core assumption:** The policy learns a continuous mapping from the latent sphere to the action space, such that small changes in the latent vector $z$ result in coherent changes in behavior.
- **Evidence anchors:**
  - [abstract] "discover semantically related variations."
  - [section 5.2] "RGSD discovers skills that preserve the sidestepping style but introduce diverse degree of turns."
  - [corpus] Evidence is weak or missing in the provided corpus regarding geometric interpolation specifically.
- **Break condition:** If the policy is under-trained or the latent dimension is too low, interpolation may result in "averaging" behaviors (e.g., a jittering motion between walking and running) rather than a smooth blend.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** This is the engine of the "Grounding" phase. Without understanding how positive/negative sampling forces representations to cluster by semantic similarity, the transition from raw data to a structured latent space is opaque.
  - **Quick check question:** Can you explain why increasing the temperature parameter $T$ in InfoNCE makes the encoder more sensitive to hard negatives?

- **Concept: Mutual Information (DIAYN)**
  - **Why needed here:** RGSD is built on top of DIAYN. You must understand the standard objective $I(S;Z)$ to understand how RGSD modifies it by pre-defining $Z$.
  - **Quick check question:** In standard DIAYN, why does maximizing the discriminator accuracy force the policy to learn diverse skills?

- **Concept: von Mises-Fisher (vMF) Distribution**
  - **Why needed here:** The paper uses vMF to model the skill latent space on a hypersphere rather than a Gaussian. This is critical for the "direction-based" interpolation and reward calculation.
  - **Quick check question:** How does sampling from a high-concentration vMF differ from sampling from a standard Normal distribution in terms of vector direction?

## Architecture Onboarding

- **Component map:** Encoder ($\mu_\phi$) -> State $s$ to latent $z$ -> Policy ($\pi_\theta$) -> Action $a$ -> Environment
- **Critical path:**
  1. **Pretrain Encoder:** Run contrastive learning on reference motions until "within-motion alignment" is achieved (visualize via t-SNE).
  2. **Initialize RL:** Load pretrained Encoder. Initialize Policy.
  3. **Parallel Training:** Run two simultaneous data streams:
     - **Imitation:** Sample $z$ from the exact reference embedding. Reward is cosine similarity.
     - **Discovery:** Sample $z$ from the neighborhood (vMF) or randomly. Reward is discriminator confidence.
- **Design tradeoffs:**
  - **Reference Dataset Size:** Small datasets are efficient but limit the "discovery" boundary; large datasets are expensive but offer richer manifolds.
  - **Encoder Freezing vs. Regularization:** The paper regularizes the discovery encoder with a KL term to the frozen pretrained encoder. Fully freezing might lose adaptability; full fine-tuning risks destroying the semantic grounding.
- **Failure signatures:**
  - **"Jittering" / Unstructured Motion:** Indicates the grounding failed; the policy is exploring the full high-dimensional space rather than the constrained manifold.
  - **Mode Collapse (Identical Skills):** Suggests the encoder mapping is collapsing (all motions map to similar $z$) or the discriminator is too weak.
- **First 3 experiments:**
  1. **Latent Space Visualization:** Verify that the pretrained encoder actually clusters reference motions into distinct regions (Paper Fig 7).
  2. **Ablation on $p$ (Imitation Ratio):** Run with $p=0$ (pure discovery) vs $p=1$ (pure imitation) to verify that discovery relies on the imitation signal to stay grounded.
  3. **Linear Interpolation Test:** Pick two reference embeddings $z_A$ and $z_B$ and condition the policy on $\alpha z_A + (1-\alpha)z_B$ to see if the behavior morphs smoothly or snaps discretely.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RGSD be extended to learn compositional behaviors that blend primitives (e.g., "walking while punching") rather than only discovering variations of individual skills?
- Basis in paper: [explicit] The conclusion states: "One promising direction is to move beyond variants of individual skills toward genuinely compositional behaviors and principled interpolations that blend primitives."
- Why unresolved: The current latent space structure maps each reference motion to a distinct direction; combining directions may not yield semantically meaningful blends, and this was not tested.
- What evidence would resolve it: Demonstrate that interpolating between latent vectors for "walk" and "punch" produces coherent walking-while-punching behavior without degradation of either skill component.

### Open Question 2
- Question: Can RGSD scale across different embodiments and larger, more diverse motion datasets to approach a "skill foundation model" for control?
- Basis in paper: [explicit] The conclusion identifies: "Another interesting direction would be scaling across embodiments and datasets, with the long-term vision of building a skill foundation model for control."
- Why unresolved: Experiments used only 20 reference motions from ACCAD on a single SMPL humanoid; generalization to quadrupeds, manipulators, or datasets with orders of magnitude more motions remains untested.
- What evidence would resolve it: Show successful skill discovery on a different embodiment (e.g., quadruped or bimanual manipulator) using 1000+ diverse reference motions with comparable fidelity and downstream task performance.

### Open Question 3
- Question: Can Wasserstein-based skill discovery objectives (e.g., METRA) be successfully adapted for reference-grounding with repetitive motions, overcoming the reward collapse issue identified in Section 6?
- Basis in paper: [explicit] Section 6 analyzes why METRA-based RGSD fails: "repetitive behaviors cannot be framed as reward maximization under METRA" because the reward collapses to zero when initial and final states are identical in local coordinates.
- Why unresolved: Neither global coordinate augmentation nor time variables resolved the issue; the latent space becomes unstable or the policy exploits augmented dimensions rather than learning meaningful behaviors.
- What evidence would resolve it: Develop a modified WDM-based objective that maintains non-zero reward signals for periodic locomotion (walking, running) while preserving distance-maximization benefits over MI-based approaches.

## Limitations
- The grounding quality depends critically on the diversity and coverage of the reference motion dataset
- The quasi-concave reward assumption may break down when agents drift far from reference states
- Current formulation focuses on variations of individual skills rather than compositional behaviors

## Confidence
- **High confidence**: Contrastive grounding mechanism and its theoretical justification
- **Medium confidence**: Reinterpretation of mutual information as imitation (depends on local neighborhood validity)
- **Medium confidence**: Geometric interpolation for discovery (needs more validation across diverse motion types)

## Next Checks
1. **Boundary Stress Test**: Systematically evaluate performance when reference dataset lacks certain motion variations to quantify grounding quality dependence
2. **Latent Space Coverage**: Measure the percentage of the unit hypersphere that generates coherent behaviors versus undefined motions
3. **Cross-Domain Transfer**: Apply RGSD to a non-locomotion task (e.g., manipulation) to test generalizability of the grounding approach