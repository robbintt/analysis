---
ver: rpa2
title: 'Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and
  Scalable DNN Accelerator Design'
arxiv_id: '2507.23437'
source_url: https://arxiv.org/abs/2507.23437
tags:
- search
- coflex
- neural
- space
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scalable Hardware-Aware Neural
  Architecture Search (HW-NAS) for Deep Neural Network accelerators on edge devices,
  where the extensive search space and high computational cost hinder practical adoption.
  The authors propose Coflex, a novel framework that integrates Sparse Gaussian Processes
  (SGP) with multi-objective Bayesian optimization to reduce the GP kernel complexity
  from cubic to near-linear with respect to the number of training samples.
---

# Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design

## Quick Facts
- **arXiv ID:** 2507.23437
- **Source URL:** https://arxiv.org/abs/2507.23437
- **Reference count:** 40
- **One-line primary result:** Coflex achieves 1.9×–9.5× speedup in multi-objective HW-NAS for DNN accelerators while maintaining high accuracy.

## Executive Summary
This paper addresses the challenge of scalable Hardware-Aware Neural Architecture Search (HW-NAS) for Deep Neural Network accelerators on edge devices, where the extensive search space and high computational cost hinder practical adoption. The authors propose Coflex, a novel framework that integrates Sparse Gaussian Processes (SGP) with multi-objective Bayesian optimization to reduce the GP kernel complexity from cubic to near-linear with respect to the number of training samples. By leveraging sparse inducing points and dimension decomposition, Coflex enables scalable approximation of large-scale search space while preserving high predictive accuracy. The framework is evaluated across diverse benchmarks, including image classification, semantic segmentation, and natural language processing, focusing on accelerator-specific architecture. Experimental results show that Coflex outperforms state-of-the-art methods in terms of network accuracy and Energy-Delay-Product, achieving a computational speed-up ranging from 1.9× to 9.5×. The results confirm that Coflex enables efficient and scalable software-hardware co-design for edge DNN accelerators.

## Method Summary
Coflex is a framework for multi-objective Hardware-Aware Neural Architecture Search that uses Sparse Gaussian Processes with dimension decomposition to reduce computational complexity. The method combines a training-free NAS evaluator (RBFlex-NAS) for accuracy prediction with a cycle-accurate simulator (DeFiNES) for hardware metrics. It employs a dimension decomposition strategy that separates software and hardware hyperparameters into orthogonal subspaces, building independent GP models for each before fusing their posteriors. The optimization uses 30 iterations with 100 initial Latin Hypercube samples, leveraging Expected Improvement as the acquisition function. The key innovation is replacing full Gaussian Processes with Sparse Gaussian Processes using inducing points to achieve near-linear complexity instead of cubic, enabling scalable co-design.

## Key Results
- Coflex achieves computational speed-up ranging from 1.9× to 9.5× compared to state-of-the-art methods
- Maintains high predictive accuracy in the surrogate model while reducing GP kernel complexity from cubic to near-linear
- Outperforms existing methods in network accuracy and Energy-Delay-Product across diverse benchmarks including image classification, semantic segmentation, and natural language processing

## Why This Works (Mechanism)

### Mechanism 1: Complexity Reduction via Sparse Inducing Points
Coflex reduces GP kernel complexity from cubic ($O(n^3)$) to near-linear ($O(nm^2)$) by selecting a small set of inducing points from the search space. Using the Woodbury matrix identity, the framework inverts a smaller $m \times m$ matrix instead of the full $n \times n$ covariance matrix, where $m \ll n$. The core assumption is that these inducing points are representative enough of the data manifold to preserve uncertainty quantification for Bayesian optimization.

### Mechanism 2: Structured Dimension Decomposition
The framework decomposes the joint hardware-software search space into orthogonal subspaces (energy-wise vs. error-wise), enabling efficient multi-objective optimization in high-dimensional spaces. It constructs independent Gaussian Processes for these decomposed dimensions and fuses their posteriors. The core assumption is that optimization objectives (Error and EDP) can be effectively decoupled into these subspaces such that their interactions do not dominate the optimization landscape.

### Mechanism 3: Training-Free Evaluation Pipeline
Coflex replaces back-propagation training and physical synthesis with training-free metrics and analytical simulation. It uses RBFlex-NAS to predict accuracy based on activation statistics and DeFiNES for hardware metrics, avoiding costly training and synthesis loops. The core assumption is that training-free proxy scores correlate strongly with final trained accuracy and the simulator accurately reflects on-device energy/latency.

## Foundational Learning

- **Concept:** Gaussian Processes (GP) & The Kernel Trick
  - **Why needed here:** Coflex relies on GPs to model the objective function (Error/EDP) and quantify uncertainty. The kernel defines how the algorithm measures similarity between different hardware/software configurations.
  - **Quick check question:** If two configurations $x$ and $x'$ differ only in memory size, how does the Matérn kernel quantify their similarity compared to configurations differing in layer type?

- **Concept:** Bayesian Optimization (BO)
  - **Why needed here:** BO is the search strategy steering exploration. It uses the GP's predictions (mean and variance) to calculate an "Acquisition Function" (e.g., Expected Improvement) to decide which configuration to test next.
  - **Quick check question:** Why does BO prefer configurations where the GP predicts high uncertainty (exploration) even if the predicted mean performance is average?

- **Concept:** The Woodbury Matrix Identity
  - **Why needed here:** This is the mathematical engine of Coflex's efficiency. It allows inverting a large matrix $(A + UCV)$ by inverting smaller matrices, which is the theoretical justification for the claimed speedup.
  - **Quick check question:** In the equation $(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$, which term represents the inverse of the covariance of the *inducing points*?

## Architecture Onboarding

- **Component map:** Optimizer (Sparse GP Surrogates + Acquisition Function) -> Evaluators (RBFlex-NAS + DeFiNES) -> Controller (Dimension Decomposition + Pareto-Front Filtering)

- **Critical path:**
  1. Sobol/LHS Sampling (Init)
  2. Decomposition of parameters into Error-wise ($X_{err}$) and Energy-wise ($X_{eng}$)
  3. Evaluation by Back-end (fast sim)
  4. Update Sparse GP models (Low-rank inversion)
  5. Optimize Acquisition Function -> Pick next candidate
  6. Check for convergence/Pareto diversity

- **Design tradeoffs:**
  - **Inducing Points ($m$):** Small $m$ = faster but riskier approximation; Large $m$ = slower but more accurate surrogate
  - **Decomposition Granularity:** Splitting parameters reduces model complexity but risks ignoring cross-domain interactions

- **Failure signatures:**
  - **Static Convergence:** Pareto front stops improving early; likely inducing points are not covering search space or decomposition is too aggressive
  - **Runtime Spike:** Complexity reduction not activating (e.g., $m$ too close to $n$) or simulator not in batch mode
  - **Proxy Divergence:** Top-1 error rate found by Coflex significantly lower than actual trained model; training-free metric failing

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Run Coflex on known function (e.g., Branin) with full GP vs. Sparse GP to verify $O(n^3)$ to $O(nm^2)$ speedup and error gap
  2. **Ablation on Inducing Points:** On NATS-Bench-SSS (Type 1 Workload), sweep $m \in \{10, 50, 100, 500\}$ and plot Pareto Optimal Region size vs. Wall Clock Time
  3. **Validation of Proxy:** Compare ranking of top-10 architectures found by Coflex against their actual trained performance to validate training-free assumption

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the reliance on analytical energy models for RNNs (Type 3 workload) impact the fidelity of Energy-Delay-Product (EDP) predictions compared to cycle-accurate simulation used for CNNs?
- **Basis in paper:** Section IV-B states DeFiNES simulator is limited to CNN-based architectures, forcing use of analytical estimation method for RNN energy/latency
- **Why unresolved:** Paper evaluates Type 3 workloads (NLP) using analytical model but does not quantify error discrepancy between this model and cycle-accurate simulation
- **What evidence would resolve it:** Comparative error analysis between analytical RNN estimates and actual hardware synthesis or cycle-accurate RNN simulator

### Open Question 2
- **Question:** How sensitive is the Coflex optimization loop to noise or estimation errors inherent in the training-free NAS evaluator (RBFlex-NAS)?
- **Basis in paper:** Section IV-A adopts RBFlex-NAS to predict error rates without training, assuming these values are sufficient surrogates for ground-truth accuracy in SGP model
- **Why unresolved:** While RBFlex-NAS is cited as state-of-the-art, paper does not analyze how noise in training-free scores propagates through Sparse Gaussian Process posterior
- **What evidence would resolve it:** Robustness analysis measuring variation in final Pareto fronts when synthetic noise is injected into training-free evaluation scores

### Open Question 3
- **Question:** Does the dimension decomposition strategy, which separates software and hardware parameters into orthogonal subspaces, limit discovery of globally optimal co-design configurations?
- **Basis in paper:** Section III-B describes decomposing search space into energy-wise and error-wise hyperparameters to reduce complexity, assuming they can be optimized somewhat independently before fusion
- **Why unresolved:** Unclear if this orthogonal separation filters out candidate solutions where software choices (e.g., layer type) have non-linear, critical dependencies on specific hardware configurations (e.g., memory bandwidth)
- **What evidence would resolve it:** Comparison of Pareto fronts generated by Coflex against non-decomposed (but computationally expensive) joint search on reduced search space

## Limitations
- The dimension decomposition strategy may filter out globally optimal configurations where software and hardware parameters have strong non-linear interactions
- Coflex's effectiveness for hardware-software co-design problems beyond image classification, segmentation, and NLP remains unproven
- The framework's reliance on training-free proxies may not generalize well to architectures requiring complex training dynamics

## Confidence
- **High Confidence:** Theoretical complexity reduction from cubic to near-linear GP complexity via sparse inducing points is mathematically proven and experimentally verified
- **Medium Confidence:** Effectiveness of dimension decomposition relies on assumption that hardware and software hyperparameters are sufficiently orthogonal
- **Medium Confidence:** Training-free evaluation pipeline's accuracy proxies are validated against known datasets but generalization to novel architectures remains unproven

## Next Checks
1. **Cross-Domain Generalization:** Test Coflex on novel hardware-software co-design problem (e.g., audio processing or robotics) where assumed parameter orthogonality may break down
2. **Inducing Point Robustness:** Systematically vary inducing point selection strategy and measure trade-off between surrogate accuracy and computational efficiency
3. **Proxy Correlation Stress Test:** For top-5 architectures found by Coflex, conduct full training and hardware synthesis to measure actual EDP vs. training-free proxy predictions and quantify prediction error distribution