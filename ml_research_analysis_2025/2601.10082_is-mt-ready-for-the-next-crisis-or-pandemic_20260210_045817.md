---
ver: rpa2
title: Is MT Ready for the Next Crisis or Pandemic?
arxiv_id: '2601.10082'
source_url: https://arxiv.org/abs/2601.10082
tags:
- languages
- google
- translation
- microsoft
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the readiness of commercial machine translation\
  \ (MT) systems for crisis or pandemic response by testing four systems\u2014Google,\
  \ Microsoft, GPT-4, and Gemini\u2014on pandemic-related text across 34 languages.\
  \ The TICO-19 dataset, created during COVID-19, served as the test set."
---

# Is MT Ready for the Next Crisis or Pandemic?

## Quick Facts
- arXiv ID: 2601.10082
- Source URL: https://arxiv.org/abs/2601.10082
- Reference count: 12
- This study evaluates commercial MT systems on pandemic-related text across 34 languages using the TICO-19 dataset, finding Google consistently highest quality while many low-resource languages lack usable translation systems.

## Executive Summary
This study evaluates four commercial machine translation systems—Google, Microsoft, GPT-4, and Gemini—on pandemic-related text across 34 languages to assess readiness for crisis response. Using the TICO-19 dataset created during COVID-19, the researchers found that Google consistently produced the highest quality translations, followed by Microsoft, with LLMs (GPT-4 and Gemini) trailing significantly, especially for low-resource languages. While most high-resource languages were adequately covered, many low-resource languages, particularly in Africa and Asia, lacked usable translation systems in one or both directions. The study concludes that MT is not fully ready for the next pandemic, particularly in low-resource language contexts.

## Method Summary
The study evaluated commercial MT systems using the TICO-19 dataset (2,100 test sentences across 34 languages) containing pandemic-related content from multiple sources. Researchers tested Google Translate, Microsoft Translator, GPT-4, and Gemini in both English-to-X (EX) and X-to-English (XE) directions. BLEU scores served as the primary metric with a threshold of ≥30 indicating "usable" translation quality. The evaluation was conducted in two snapshots (2023 and 2025) to detect quality changes. BERTScore and COMET were calculated but not used for thresholding. Languages were categorized as Pivot (9), Important (8), or Priority (21) based on resource availability.

## Key Results
- Google consistently produced the highest quality translations across all languages, followed by Microsoft
- Translation into English (XE) consistently outperformed translation out of English (EX) across all systems
- LLMs (GPT-4 and Gemini) underperformed dedicated NMT systems, particularly on low-resource languages
- Many low-resource languages, especially in Africa and Asia, lacked usable translation systems in one or both directions
- Some languages experienced unexplained quality drops between 2023 and 2025 evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translation quality degrades predictably with language resource level
- **Mechanism:** High-resource languages have more parallel training data enabling better neural model parameterization, while low-resource languages have sparse training corpora leading to weaker embeddings and more frequent hallucinations
- **Core assumption:** Commercial systems train primarily on available parallel corpora
- **Evidence anchors:** [abstract] "many low-resource languages... lacked usable translation systems"; [section 5.1.2] "LLM scores were much lower than either of the neural systems" for Priority languages

### Mechanism 2
- **Claim:** Translation into English consistently outperforms translation out of English
- **Mechanism:** English-dominant pretraining corpora create stronger English representations in both encoder and decoder, giving models richer target-side language models when translating into English
- **Core assumption:** Model architectures are not symmetrically trained; English receives privileged representation during pretraining
- **Evidence anchors:** [abstract] "Bidirectional translation quality was significantly higher into English than out of English"; [section 5.1.3] "BLEU scores were significantly higher for translations in the XE directions versus EX across all four systems"

### Mechanism 3
- **Claim:** LLMs underperform dedicated NMT systems on low-resource languages
- **Mechanism:** LLMs are autoregressive models trained for general text generation, not aligned to translation-specific objectives, leading to hallucinations, refusals, or inconsistent terminology without fine-tuning
- **Core assumption:** Translation capability emerges from scale and multilingual exposure, but without explicit alignment, low-resource language performance remains unreliable
- **Evidence anchors:** [abstract] "GPT-4 and Gemini trailing, especially for low-resource languages"; [section 4.2] "LLMs may introduce issues related to inconsistencies in terminology or hallucinations"

## Foundational Learning

- **Concept: BLEU scoring and usability thresholds**
  - **Why needed here:** The paper uses BLEU ≥30 as the "usable" threshold for crisis communication
  - **Quick check question:** If a translation system scores BLEU 22 on Hausa→English, is it considered usable per the paper's criteria? (Answer: No—it's in the "clear gist, major grammatical errors" range, below 30 threshold.)

- **Concept: Language resource classification (Pivot/Important/Priority)**
  - **Why needed here:** The TICO-19 dataset organizes languages by TwB priority labels; these correlate with resource availability
  - **Quick check question:** Which category—Pivot, Important, or Priority—contains the lowest-resource African languages most likely to fail usability thresholds? (Answer: Priority languages.)

- **Concept: Bidirectional translation evaluation (EX vs. XE)**
  - **Why needed here:** The study tests both directions; readiness requires assessing both aid dissemination and field reporting scenarios
  - **Quick check question:** Which direction is critical for translating health agency announcements to local populations? (Answer: English→X, or EX direction.)

## Architecture Onboarding

- **Component map:**
```
TICO-19 Test Set (2,100 sentences, 34 languages)
    ↓
[Translation Engine] ← Google / Microsoft / GPT-4 / Gemini APIs
    ↓
Raw Translations (EX and XE directions)
    ↓
[Evaluation Layer] ← BLEU (thresholding) / BERTScore / COMET
    ↓
Usability Classification (≥30 = usable, 25-30 = borderline, <25 = not usable)
    ↓
Readiness Assessment by Region/Language
```

- **Critical path:**
1. Select language pairs based on crisis priority (focus on Priority languages in Africa/Asia)
2. Run bidirectional translations through multiple systems
3. Compute BLEU scores; flag languages where best system scores <30
4. For borderline cases (25-30), supplement with BERTScore/COMET and domain-specific error analysis

- **Design tradeoffs:**
- BLEU vs. neural metrics: BLEU provides interpretable thresholds but may under-reward semantic adequacy; COMET/BERTScore better capture meaning but lack clear usability cutoffs for low-resource languages
- System selection: Google offers best coverage and quality; Microsoft may have specific language strengths; LLMs provide flexibility but unreliability
- Evaluation-only vs. intervention: Study assesses existing systems; does not propose fine-tuning or data augmentation solutions

- **Failure signatures:**
- BLEU <10: "Almost useless" output—likely wrong script, severe word order errors, or hallucinations
- EX direction systematically worse than XE for same language pair
- Quality regression over time (2023→2025 drops): suggests model updates may have deprioritized specific languages

- **First 3 experiments:**
1. **Baseline coverage audit:** For your target deployment region, identify which languages have BLEU≥30 in both directions; flag gaps requiring fallback strategies
2. **Directional stress test:** Compare EX vs. XE quality for top 3 priority languages; if EX<25, crisis information dissemination is at risk
3. **Contamination check:** For high BLEU scores (>55), analyze domain sub-scores (CM vs. SM vs. N); if public-domain subsets show anomalous peaks, investigate potential training data overlap

## Open Questions the Paper Calls Out

- **Question:** To what extent does training data contamination inflate evaluation scores for public TICO-19 subdomains compared to private conversational data?
- **Basis in paper:** [explicit] Section 6.4 notes that scores in the 50-60 range suggest overlap and states, "A thorough analysis of data contamination (and TICO-19 domains) will be left to future research."
- **Why unresolved:** The authors cannot access proprietary training logs for commercial systems to confirm if public benchmark data was memorized
- **Evidence:** A comparative analysis showing significantly higher scores for public domains (Wikinews) versus restricted domains (CMU conversational data) within the same language pair

- **Question:** What specific architectural or training changes caused the significant quality degradation in Microsoft's systems between 2023 and 2025?
- **Basis in paper:** [explicit] Section 6.3 highlights "inexplicable losses" and states, "We are not privy to the reasons for these losses in quality... [perhaps] the move to multilingual or LLM-based systems caused broad losses."
- **Why unresolved:** Commercial providers do not disclose the specific updates or data changes that led to lower BLEU scores for languages like Arabic and Lingala
- **Evidence:** Vendor release logs detailing model shifts or a reproducible experiment testing older vs. newer API versions on a controlled static test set

- **Question:** How accurately does the BLEU > 30 threshold predict actual human usability and adequacy for low-resource languages in crisis scenarios?
- **Basis in paper:** [inferred] The "Limitations" section admits the "absence of human evaluations" and that BLEU, which has "outlived its shelf-life," was the sole metric used to define "readiness"
- **Why unresolved:** The study proxies "usability" using automated scores because sourcing human evaluators for languages like Kanuri or Dinka was budgetarily and logistically prohibitive
- **Evidence:** Human annotations rating the "understandability" of translations that score near the 25-30 BLEU range to validate the threshold

## Limitations

- Reliance on BLEU as primary usability threshold may underestimate semantic adequacy in low-resource languages with different grammatical structures
- Unexplained quality drops between 2023-2025 snapshots for some languages (particularly Microsoft) without clear causal investigation
- LLM evaluations use only zero-shot prompting without exploring approaches that might improve low-resource language performance

## Confidence

**High Confidence**: Claims about directional asymmetry (XE consistently outperforms EX) and the general pattern of Google > Microsoft > LLMs for most languages are well-supported by consistent BLEU score patterns across multiple languages and domains.

**Medium Confidence**: The usability thresholds and crisis-readiness conclusions are reasonable but depend on the assumption that BLEU≥30 correlates with functional crisis communication, which may not hold for all low-resource language contexts.

**Low Confidence**: Explanations for quality regressions between 2023-2025 snapshots are speculative; the paper documents the phenomenon but does not establish causal mechanisms.

## Next Checks

1. **Usability threshold validation**: For languages scoring 25-30 BLEU, conduct human evaluation to verify whether the BLEU threshold accurately predicts functional crisis communication capability, particularly for low-resource languages with different grammatical structures.

2. **Directional asymmetry investigation**: Systematically compare EX vs. XE performance for top-priority crisis languages to identify specific deployment risks—if EX direction consistently falls below 25 BLEU, crisis information dissemination capabilities are compromised.

3. **Contamination analysis**: For any language showing BLEU>55, examine domain-specific performance (CM vs. public domain subsets) to detect potential training data overlap that could inflate scores and misrepresent true readiness.