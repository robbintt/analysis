---
ver: rpa2
title: 'From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant
  MLLM Training'
arxiv_id: '2511.07738'
source_url: https://arxiv.org/abs/2511.07738
tags:
- entropy
- grpo
- arxiv
- training
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of training multimodal large language
  models (MLLMs) under noisy supervision, where labeled data contains annotation errors.
  To improve robustness, it proposes a two-stage entropy-guided reinforcement learning
  approach that first maximizes token-level entropy to encourage diverse exploration
  and prevent overfitting to noisy labels, then minimizes entropy to consolidate confident
  predictions.
---

# From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training

## Quick Facts
- arXiv ID: 2511.07738
- Source URL: https://arxiv.org/abs/2511.07738
- Reference count: 40
- Primary result: Two-stage entropy RLVR improves MLLM robustness under noisy supervision, achieving up to 4.8% higher accuracy on 50% noisy data compared to standard GRPO

## Executive Summary
This paper addresses the challenge of training multimodal large language models (MLLMs) under noisy supervision, where labeled data contains annotation errors. The authors propose a two-stage entropy-guided reinforcement learning approach that first maximizes token-level entropy to encourage diverse exploration and prevent overfitting to noisy labels, then minimizes entropy to consolidate confident predictions. Tested on Qwen2-VL-3B across GUI grounding and fine-grained classification tasks with varying noise levels (0%-100%), the method consistently outperforms standard GRPO and entropy-only baselines, achieving up to 4.8% higher accuracy on 50% noisy data and demonstrating strong generalization. The phased entropy schedule enhances model stability and performance under imperfect supervision.

## Method Summary
The authors develop a two-stage entropy RLVR framework that alternates between entropy maximization and minimization during training. In the exploration phase, the model maximizes token-level entropy to encourage diverse predictions and prevent overfitting to noisy labels. During the exploitation phase, entropy is minimized to consolidate confident predictions. The transition between phases occurs at a fixed step (80% of training steps in experiments). The approach is evaluated on GUI grounding and fine-grained classification tasks using Qwen2-VL models with synthetic noise levels ranging from 0% to 100%. The method demonstrates consistent improvements over standard GRPO and entropy-only baselines across all noise levels.

## Key Results
- Achieves up to 4.8% higher accuracy than GRPO baseline on 50% noisy GUI grounding task
- Maintains consistent performance across all noise levels (0%-100%) unlike standard RLVR methods
- Demonstrates superior generalization with 4.6% improvement on 25% noisy fine-grained classification task
- Shows stronger stability and convergence compared to entropy-only approaches

## Why This Works (Mechanism)
The method works by strategically managing exploration-exploitation trade-off through entropy scheduling. During exploration, high entropy encourages the model to sample diverse trajectories, reducing the risk of overfitting to incorrect labels. In exploitation, low entropy consolidates confident predictions. This phased approach prevents early convergence to erroneous modes while maintaining performance on clean data. The method is particularly effective when the base model has reasonable zero-shot ability on the target task, as it can explore meaningful solution space before consolidation.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - provides framework for training with preference data and reward signals; Quick check - understand reward modeling and policy optimization basics
- **Entropy regularization in RL**: Why needed - controls exploration-exploitation balance and prevents premature convergence; Quick check - grasp entropy's role in policy stochasticity
- **Multimodal large language models**: Why needed - understand MLLM architecture and training challenges; Quick check - know how vision-language models process and generate responses
- **Label noise in supervised learning**: Why needed - recognize how annotation errors impact model performance; Quick check - understand noise injection methods and their effects
- **Gradient-based optimization**: Why needed - follow how entropy changes affect parameter updates; Quick check - review backpropagation and gradient descent fundamentals
- **Reward shaping**: Why needed - understand how entropy interacts with task-specific rewards; Quick check - know how auxiliary objectives modify policy gradients

## Architecture Onboarding

**Component Map**: Input -> MLLM Encoder/Decoder -> Entropy Calculation -> Reward Computation -> Policy Gradient Update -> Output

**Critical Path**: Data Input → MLLM Forward Pass → Entropy Calculation → Reward + Entropy Combination → Policy Gradient → Parameter Update

**Design Tradeoffs**: Fixed vs adaptive switching point (simpler but potentially suboptimal vs more complex but task-adaptive); entropy calculation granularity (token-level vs sequence-level); reward shaping balance (task performance vs entropy regularization)

**Failure Signatures**: 
- Early convergence to incorrect modes when base model has poor zero-shot ability
- Over-regularization leading to slow convergence on clean data
- Instability when noise level approaches 100% due to lack of reliable signal
- Poor performance when switching point is mistimed relative to task complexity

**First Experiments**:
1. Replicate Table 1 results comparing against GRPO baseline across noise levels
2. Validate Table 2 scaling behavior by testing on Qwen2.5-VL-7B with 50% noise
3. Confirm Table 3 switching point sensitivity by testing multiple τ_switch values on GUI grounding task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal functional form for the entropy scheduling beyond the simple piecewise switch (e.g., linear decay, cosine annealing, or adaptive schedules based on running entropy saturation)?
- Basis in paper: [explicit] The authors state: "Here, we only use a simple piecewise function and achieve great empirical performance. It demonstrates the potential of our two-stage transition design. More sophisticated schedulers, such as a linear-decay schedule, are also discussed in the appendix."
- Why unresolved: The paper demonstrates viability of the two-stage concept but does not systematically compare alternative scheduling functions that could provide smoother transitions or better performance.
- What evidence would resolve it: Ablation experiments comparing piecewise, linear-decay, cosine, and adaptive entropy schedules across noise levels and tasks, with convergence dynamics and final accuracy reported.

### Open Question 2
- Question: Can the exploration-to-exploitation switching point (τ_switch) be determined adaptively based on training dynamics rather than fixed at an empirical 80% of steps?
- Basis in paper: [inferred] Table 3 shows that step 800 (80% of 1000) works best, but the authors note "there exists a trade-off between the training convergence and noisy labels overfitting." The switching point is currently a fixed hyperparameter.
- Why unresolved: The optimal switch point likely depends on noise level, task complexity, and model scale; a fixed ratio may be suboptimal across diverse settings.
- What evidence would resolve it: Experiments using entropy saturation or gradient-based criteria to trigger the switch dynamically, compared against fixed-ratio baselines across multiple noise levels and model sizes.

### Open Question 3
- Question: How can the two-stage entropy method be modified to succeed when the base model has very poor zero-shot capability on the target task?
- Basis in paper: [explicit] In Limitations: "We find that entropy works best when the model has an original ability on target task. If the base model's zero-shot ability on the target task is very poor, early entropy maximization may amplify erroneous modes before any correct trajectory is found."
- Why unresolved: The current design assumes reasonable base competence; for weak-base scenarios, initial entropy maximization may exacerbate errors before any viable trajectories emerge.
- What evidence would resolve it: Modified approaches such as delayed entropy maximization, curriculum-based entropy scheduling, or hybrid supervision strategies, tested on tasks where base accuracy is near-random.

### Open Question 4
- Question: What is the theoretical or empirical relationship between model scale and the relative benefit of two-stage entropy scheduling (i.e., is there a scaling law)?
- Basis in paper: [inferred] Table 2 shows Qwen2-VL-7B gains 8.6% at 50% noise versus 4.2% for Qwen2.5-VL-3B, suggesting larger models may benefit more, but the sample size (3 backbones) is insufficient to establish a pattern.
- Why unresolved: Understanding whether scaling amplifies or diminishes the method's advantage would inform deployment decisions and resource allocation.
- What evidence would resolve it: Systematic experiments across a wider range of model sizes (e.g., 1B to 70B parameters) with controlled noise levels, analyzing gain magnitude as a function of parameter count.

## Limitations
- Performance degrades when base model has very poor zero-shot ability on target task, as early entropy maximization amplifies erroneous modes
- Fixed switching point (80% of steps) may be suboptimal across different noise levels, task complexities, and model scales
- Limited generalizability testing beyond Qwen2-VL architecture and two specific task types (GUI grounding and fine-grained classification)

## Confidence
High: Core claim that exploration-then-exploitation entropy scheduling improves robustness under noisy supervision is supported by consistent results across noise levels (0%-100%)
Medium: Generalizability to other MLLM architectures and real-world noise patterns remains unproven due to limited architecture diversity and synthetic noise testing
Medium: The specific contribution of entropy scheduling versus general RLVR improvements is unclear without broader baseline comparisons

## Next Checks
1. Test the two-stage entropy approach on additional MLLM architectures (e.g., LLaVA, InternVL) to verify cross-model generalizability
2. Evaluate performance on real-world noisy datasets with naturally occurring annotation errors rather than synthetic noise
3. Conduct ablation studies comparing the two-stage approach against other RLVR variants (e.g., PPO, DPO) to isolate the contribution of entropy scheduling from RLVR improvements