---
ver: rpa2
title: 'Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity
  Applications'
arxiv_id: '2510.03623'
source_url: https://arxiv.org/abs/2510.03623
tags:
- attack
- explanation
- adversarial
- feature
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates adversarial attacks on post-hoc XAI methods
  in cybersecurity applications. The study evaluates six attack procedures targeting
  SHAP, LIME, and IG explanation methods across four cybersecurity datasets: phishing,
  malware, intrusion detection, and fraudulent websites.'
---

# Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications

## Quick Facts
- **arXiv ID:** 2510.03623
- **Source URL:** https://arxiv.org/abs/2510.03623
- **Reference count:** 40
- **Primary result:** Fairwashed explanation attacks, especially Makrut, proved highly effective at concealing sensitive feature importance while maintaining prediction accuracy in cybersecurity XAI methods.

## Executive Summary
This paper investigates adversarial attacks on post-hoc explainable AI (XAI) methods in cybersecurity applications. The study evaluates six attack procedures targeting SHAP, LIME, and Integrated Gradients (IG) explanation methods across four cybersecurity datasets: phishing, malware, intrusion detection, and fraudulent websites. Three attack tactics are explored: fairwashed explanation (FE), manipulated explanation (ME), and backdoor-enabled manipulation (BD). The FE attacks proved highly effective at concealing sensitive feature importance, particularly the Makrut attack, which maintained prediction accuracy while significantly altering explanations. In contrast, the ME attacks showed limited effectiveness. Only two attacks had proposed defenses. The results highlight urgent need for enhanced security in XAI methods to ensure reliable explanations in cybersecurity applications.

## Method Summary
The study evaluates adversarial attacks on post-hoc XAI methods (SHAP, LIME, IG) using four cybersecurity datasets. Four classifiers (LR, XGB, SMLP, PMLP) are trained with specific hyperparameters. Preprocessing involves standard scaling and removing features with Pearson correlation > 0.35. Data is split 80:20. Attack success is measured by reduction in importance of protected features (for Fairwashed attacks) or Spearman's rank correlation between original and adversarial feature rankings (for Manipulated Explanation attacks). The research explores three attack tactics: Fairwashed Explanation (FE), Manipulated Explanation (ME), and Backdoor-Enabled Manipulation (BD), with six specific attack procedures implemented across the datasets.

## Key Results
- **Makrut attack effectiveness:** Successfully maintained prediction accuracy while significantly reducing the importance of protected features in explanations, proving highly effective at fairwashing.
- **ME attacks ineffectiveness:** Data Poisoning and Black Box attacks showed limited success, with Spearman correlations between original and adversarial feature rankings remaining above 0.94.
- **Defense limitations:** Only two of the six attack procedures had proposed defenses, leaving four attacks without security countermeasures.

## Why This Works (Mechanism)
The effectiveness of fairwashed explanation attacks stems from their ability to optimize both prediction accuracy and explanation manipulation simultaneously. By using techniques like adversarial training and gradient-based optimization, these attacks can maintain model performance while significantly altering feature importance rankings. The Makrut attack, in particular, uses a weighted loss function that balances prediction loss with explanation loss, allowing it to preserve accuracy while concealing sensitive features. Manipulated explanation attacks are less effective because they primarily focus on altering the explanation mechanism without the sophisticated balancing of prediction preservation, resulting in detectable changes to model behavior.

## Foundational Learning
- **Post-hoc XAI methods (SHAP, LIME, IG):** These are model-agnostic explanation techniques that approximate model decisions locally. Understanding their mechanics is crucial because attacks target the explanation generation process rather than the model itself.
- **Spearman rank correlation:** A non-parametric measure of monotonic relationship between two datasets. It's used to quantify how much attack-altered explanations deviate from original explanations.
- **Fairwashed explanation attacks:** Techniques designed to conceal sensitive feature importance while maintaining prediction accuracy. These are particularly dangerous in cybersecurity where feature importance can reveal attack patterns or vulnerabilities.
- **Adversarial training in XAI context:** Unlike traditional adversarial training that focuses on robust predictions, here it's used to create models whose explanations can be manipulated while preserving accuracy.
- **Backdoor-enabled manipulation:** Attack vectors that introduce hidden triggers into models to control explanations when specific patterns are present, potentially creating stealth vulnerabilities.

## Architecture Onboarding

**Component Map:** Datasets -> Preprocessing -> Model Training -> XAI Method -> Attack Application -> Evaluation Metrics

**Critical Path:** The most critical path is from model training through XAI method application to attack success evaluation. The attack procedures directly manipulate the relationship between model predictions and explanations, making this the primary vulnerability point.

**Design Tradeoffs:** The study uses simpler models (LR, XGBoost, shallow MLPs) for reproducibility and interpretability, sacrificing the potential complexity benefits of deep learning. This tradeoff enables clearer demonstration of XAI vulnerabilities but may not capture all attack vectors present in larger architectures.

**Failure Signatures:** High Spearman correlation (>0.94) indicates ME attack failure, while maintained prediction accuracy with altered feature importance signals FE attack success. The Makrut attack's signature is preserved F-1 score with significantly reduced protected feature importance in SHAP plots.

**First Experiments:**
1. **Reproduce Output Shuffling on Phishing dataset:** Apply the attack to the XGB model and verify protected feature (IsHTTPS) importance reduction in SHAP summary plots while maintaining accuracy.
2. **Test Data Poisoning on IDS dataset:** Measure Spearman correlation between original and adversarial rankings to confirm ME attack ineffectiveness.
3. **Apply Makrut attack on Fraudulent Websites:** Verify that the attack maintains prediction accuracy while significantly altering explanation patterns, particularly for protected features.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What defense mechanisms can effectively secure XAI methods against the four attack procedures (Output Shuffling, Data Poisoning, Black Box, Makrut) that currently lack proposed protections?
- **Basis in paper:** The authors explicitly state that "no defense has yet been proposed for four other individual attacks" and list the development of these defenses as a primary goal for future work.
- **Why unresolved:** While the study identifies the Makrut attack as highly effective at fairwashing and ME attacks as generally inefficient, it concludes there is an urgent need for security enhancements to ensure reliability.
- **What evidence would resolve it:** Novel defensive algorithms or architectural modifications that successfully detect or neutralize these specific attacks without significantly degrading the prediction accuracy or explanation quality of the model.

### Open Question 2
- **Question:** How do adversarial explanation manipulation techniques transfer to Large Language Models (LLMs) utilized in cybersecurity contexts?
- **Basis in paper:** The authors explicitly suggest that "researchers can explore and investigate the manipulation of the explanations generated by large language models (LLMs)" in their future directions.
- **Why unresolved:** The current study is restricted to tabular datasets and traditional ML/DL architectures, leaving the specific vulnerabilities of generative AI and LLM explanation methods unexplored.
- **What evidence would resolve it:** An evaluation of FE, ME, and BD tactics on LLM-specific explainers (e.g., attention visualization) applied to textual cybersecurity data, measuring attack success rates and explanation fidelity.

### Open Question 3
- **Question:** Do the observed vulnerabilities and attack efficacies persist in very deep neural networks and large-scale datasets?
- **Basis in paper:** The authors list the limitation that the study "does not cover deep models and very large datasets," stating these "can be investigated in future studies."
- **Why unresolved:** The experiments utilized simpler models (Logistic Regression, XGBoost, shallow MLPs); it is uncertain if complex, high-parameter models exhibit the same susceptibility to attacks like Makrut or Output Shuffling.
- **What evidence would resolve it:** Replication of the six attack procedures on state-of-the-art deep learning architectures (e.g., Deep Residual Networks) using high-volume cybersecurity datasets to compare attack effectiveness against the baselines established in this paper.

## Limitations
- **Implementation dependency:** The study relies on an anonymous code repository for complex attack implementations, limiting independent verification.
- **Simplified model scope:** Experiments use simpler models (LR, XGBoost, shallow MLPs) rather than deep neural networks, potentially missing vulnerabilities in more complex architectures.
- **Limited defense development:** Only two attacks have proposed defenses, leaving significant gaps in practical security recommendations.

## Confidence
- **High Confidence:** The general framework of attacking XAI methods in cybersecurity applications is sound and well-documented. The identification of fairwashed explanation attacks as highly effective, particularly the Makrut attack, is supported by the reported metrics.
- **Medium Confidence:** The specific effectiveness rates and Spearman correlation values reported for each attack method depend on exact implementation details that are not fully specified in the paper.
- **Low Confidence:** The practical implications and real-world applicability of these attacks in production cybersecurity systems cannot be fully assessed without access to the complete implementation code.

## Next Checks
1. **Reproduce the Makrut attack on the Phishing dataset** using the provided code repository to verify that it maintains prediction accuracy (>95% F-1) while significantly reducing the importance of the protected feature `IsHTTPS` in SHAP summary plots.
2. **Implement the Data Poisoning attack** on the IDS dataset and measure the Spearman correlation between original and adversarial feature rankings. Confirm that the correlation remains above 0.94 as reported, validating the claim that ME attacks are less effective than FE attacks.
3. **Test the proposed defenses** (if accessible) by applying them to the most effective attacks (Makrut and Output Shuffling) and measuring the restoration of original explanation patterns in the fraudulent websites dataset.