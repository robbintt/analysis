---
ver: rpa2
title: 'ClaimDB: A Fact Verification Benchmark over Large Structured Data'
arxiv_id: '2601.14698'
source_url: https://arxiv.org/abs/2601.14698
tags:
- claims
- claim
- contr
- database
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAIMDB is a fact-verification benchmark built from 80 real-world
  databases containing an average of 11 tables and 4.6M records. Claims are derived
  from large-scale structured data that cannot be naively "read" by models, requiring
  executable reasoning over millions of records.
---

# ClaimDB: A Fact Verification Benchmark over Large Structured Data

## Quick Facts
- arXiv ID: 2601.14698
- Source URL: https://arxiv.org/abs/2601.14698
- Reference count: 40
- Primary result: No model exceeds 83% accuracy on three-way fact verification over large structured databases requiring SQL reasoning

## Executive Summary
CLAIMDB is a fact-verification benchmark built from 80 real-world databases containing an average of 11 tables and 4.6M records. Claims are derived from large-scale structured data that cannot be naively "read" by models, requiring executable reasoning over millions of records. We evaluate 30 state-of-the-art LLMs using a tool-calling approach with SQL queries. No model exceeds 83% accuracy, and over half score below 55%. Models also struggle with abstention (NEI), either refusing it entirely or over-predicting it, raising reliability concerns for high-stakes use. CLAIMDB is released at https://claimdb.github.io.

## Method Summary
CLAIMDB uses 80 real-world databases from the BIRD NL-to-SQL benchmark, containing an average of 11.6 tables and 4.6M records each. Claims are generated using GPT-5 through three prompt variants targeting entailed, contradicted, and not-enough-info (NEI) scenarios. The benchmark employs AST-based filtering to ensure claims require compositional reasoning (aggregations, multi-table joins, orderings) rather than simple lookups. Evaluation uses a unified tool-calling approach where models execute SQL queries via MCP toolbox, with a single prompt (Figure 17) for all 30 tested models. Claims are judged by a conservative panel of three LLMs (Phi-4, grok-3-mini, mistral-small), requiring unanimous agreement for acceptance.

## Key Results
- No model exceeds 83% accuracy; over half score below 55%
- Models struggle with NEI abstention—proprietary models avoid it entirely while open-source models over-predict it
- Optimal tool-call range appears to be 4-8 queries before accuracy degrades due to context flooding
- Even top-performing models (gpt-5-mini at 83%) show significant room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-calling agents with SQL execution enable verification over databases too large for LLM context windows.
- Mechanism: The system provides models with a single tool—arbitrary SQL query execution against the associated database—allowing compositional reasoning (aggregations, joins, sorting) over millions of records without loading data into context.
- Core assumption: Models can decompose claim verification into a sequence of targeted SQL queries rather than requiring full-data access.
- Evidence anchors:
  - [abstract] "At this scale, verification approaches that rely on 'reading' the evidence break down, forcing a timely shift toward reasoning in executable programs."
  - [section 6.1] "We give each agent a single tool: the ability to execute arbitrary SQL queries against the database associated with the claim at hand."
  - [corpus] TSVer benchmark similarly requires temporal reasoning over structured evidence but at smaller scale.
- Break condition: If models cannot generate correct SQL for multi-table joins or aggregations, or if tool-call budgets exceed practical limits.

### Mechanism 2
- Claim: AST-based filtering ensures claims require compositional reasoning rather than simple lookups.
- Mechanism: SQL queries are converted to abstract syntax trees; only queries containing ORDER BY, aggregate functions (AVG, SUM), window functions, or 3+ table joins are retained, guaranteeing claims cannot be verified by single-record inspection.
- Core assumption: Claims derived from compositional queries necessitate genuine reasoning over large data subsets.
- Evidence anchors:
  - [section 3.3] "A query is retained if and only if its AST contains at least one of the following: Orderings or superlatives... Aggregate Functions... Window functions... Multi-table joins."
  - [section 3.3] "A lookup query that returns a single record is of no interest: a verifier could answer by simply inspecting and seeing the right data."
  - [corpus] No direct corpus comparison for AST-filtering approach; this appears novel.
- Break condition: If claims can be verified through alternative shortcuts not captured by AST criteria.

### Mechanism 3
- Claim: Model performance degrades with excessive tool calls due to context fragmentation.
- Mechanism: Top-performing models average 4-8 tool calls; longer sessions flood context with query results, increasing error probability from single bad decisions.
- Core assumption: There exists an optimal interaction length—too few calls means insufficient evidence, too many means context degradation.
- Evidence anchors:
  - [section 6.3] "As they grow longer, models increasingly lose focus due to the large amounts of information. In this setting, a single bad decision—a single careless query—can flood the model with hundreds of thousands of values."
  - [figure 7] Second-order polynomial fit shows accuracy peaks at 4-8 tool calls.
  - [corpus] Corpus evidence weak; no comparable tool-call analysis found in related fact-verification benchmarks.
- Break condition: If model architectures improve long-context handling or query result filtering.

## Foundational Learning

- Concept: **SQL aggregation and multi-table joins**
  - Why needed here: Claims require computing statistics (AVG, SUM, MAX) across joined tables with millions of rows; understanding query semantics is essential for debugging agent behavior.
  - Quick check question: Can you write a query that finds the top 5 cities by average enrollment across three joined tables?

- Concept: **Tool-calling/agent architectures**
  - Why needed here: The evaluation framework uses MCP toolbox for database access; understanding how models invoke tools and receive results is prerequisite for implementing or debugging.
  - Quick check question: What happens when an LLM receives a tool result with 100,000 rows in its context?

- Concept: **Three-way classification with abstention (NEI)**
  - Why needed here: Models must distinguish entailed, contradicted, and not-enough-info; poor NEI handling (over-prediction or refusal) is a dominant failure mode.
  - Quick check question: If a database contains crime statistics but not victim demographics, can you verify a claim about victim ethnicity?

## Architecture Onboarding

- Component map: BIRD NL-to-SQL benchmark → AST filtering → SQL execution → JSON answers → GPT-5 claim generation (3 prompts: entailed/contradicted/NEI) → LLM judge panel (Phi-4, grok-3-mini, mistral-small) → Embedding-based NEI sampling → Final benchmark splits (train/public/private).

- Critical path: Start by understanding BIRD source data, then trace the filtering logic (Section 3.3), claim generation prompts (Figures 12-14), and judge evaluation rubrics (Section 4.1). The test evaluator prompt (Figure 17) is the actual system prompt used for all 30 models.

- Design tradeoffs: Conservative judge panel (any single rejection eliminates claim) favors benchmark quality over size; NEI similarity filtering removes "obvious" claims but may exclude edge cases; SQL-only evaluation excludes Python/pandas approaches which prior work found brittle for smaller models.

- Failure signatures: (1) Proprietary models biased against NEI—near-zero NEI predictions for E/C ground truth; (2) Open-source models over-predict NEI—~50% of E/C predictions are NEI; (3) Models making 0 tool calls hallucinate answers; (4) Very long tool-call sequences correlate with accuracy drop.

- First 3 experiments:
  1. Run gpt-5-mini (top performer, 83% accuracy) on 10 public test examples; inspect tool-call sequences and compare against the 4-8 call optimal range.
  2. Analyze confusion matrices for NEI handling—compare gpt-5-mini (NEI recall: 0.777) vs. qwen3:32b (NEI recall: 0.866 but E/C recall collapsed).
  3. Test the verifier prompt (Figure 17) on a custom claim over a BIRD database; observe whether model queries schema first as instructed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance gap in handling "Not Enough Info" (NEI) claims be closed by calibrating abstention behaviors in both proprietary and open-source models?
- **Basis in paper:** [inferred] The analysis reveals a dichotomy where proprietary models are biased against abstention, while open-source models over-predict it, leading to poor reliability (Section 6.3, Figure 8).
- **Why unresolved:** Current models exhibit degenerate behaviors when evidence is insufficient, suggesting fundamental issues with uncertainty estimation in large-scale data contexts.
- **What evidence would resolve it:** Demonstration of a model or training paradigm that achieves balanced F1 scores across Entailed, Contradicted, and NEI labels without extreme bias toward or against abstention.

### Open Question 2
- **Question:** How do alternative reasoning paradigms, such as coding agents using Python/Pandas, compare to the SQL tool-calling approach on this benchmark?
- **Basis in paper:** [explicit] The authors state they "leave a systematic study of potential alternatives—from other tool choices to different paradigms altogether—for future work" (Section 6.1).
- **Why unresolved:** The study focused exclusively on SQL tool-calling because prior work indicated coding agents were brittle for smaller models (<70B), but a systematic comparison on this specific large-scale dataset is missing.
- **What evidence would resolve it:** A comparative evaluation of Python-based reasoning agents against the established SQL agent baseline using the ClaimDB test set.

### Open Question 3
- **Question:** Can fact verification systems be effectively extended to handle multi-modal evidence while maintaining the complex reasoning required for large structured data?
- **Basis in paper:** [explicit] The limitations section notes that "extension to multi-modal evidence" is "a natural direction for future work" (Section 8).
- **Why unresolved:** ClaimDB deliberately isolates structured data; real-world verification often requires synthesizing information from text, charts, and tables simultaneously.
- **What evidence would resolve it:** The creation and evaluation of a multi-agent system or architecture that integrates ClaimDB's executable reasoning with unstructured data processing capabilities.

## Limitations

- The study relies exclusively on SQL tool-calling and does not test alternative approaches like Python/pandas-based reasoning, which might perform better for certain query types.
- Performance evaluation depends heavily on proprietary models (16 of 30), whose internal reasoning processes remain opaque and cannot be fully reproduced.
- The NEI handling issue appears to be a fundamental limitation where models either refuse to predict NEI entirely or over-predict it, suggesting poor uncertainty calibration rather than data scarcity.

## Confidence

- **High Confidence**: The benchmark construction methodology (AST filtering, SQL execution requirements, judge evaluation panel) is well-specified and reproducible. The core finding that no model exceeds 83% accuracy and that NEI handling remains problematic is robust across multiple evaluation metrics.
- **Medium Confidence**: The claim that structured data verification requires executable reasoning (not just reading) is strongly supported by the database scale (millions of records), though alternative verification approaches using Python/pandas were not tested despite their potential viability for smaller models.
- **Low Confidence**: The assertion that proprietary models are "biased against NEI" while open-source models are "biased toward NEI" requires deeper investigation into calibration mechanisms, as these patterns could stem from prompt engineering choices rather than fundamental model properties.

## Next Checks

1. **Controlled Tool-Call Experiment**: Systematically vary maximum tool-call limits (1, 3, 5, 10, 20) for a subset of models to determine if the 4-8 call optimal range holds under controlled conditions, and whether context window limitations or decision fatigue drive accuracy degradation.

2. **Alternative Verification Approach**: Implement a Python/pandas-based verification pipeline for the same benchmark to test whether SQL-only evaluation underestimates smaller models' capabilities, particularly for queries that might be simpler to express in Python.

3. **NEI Calibration Study**: Conduct targeted experiments where models are explicitly instructed to predict NEI for specific claim types (e.g., demographic claims on crime databases) to determine if the observed NEI bias patterns are prompt-dependent or reflect deeper calibration issues.