---
ver: rpa2
title: 'Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge Expansion'
arxiv_id: '2502.12598'
source_url: https://arxiv.org/abs/2502.12598
tags:
- knowledge
- language
- computational
- linguistics
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys methods for expanding the knowledge of large
  language models (LLMs) to maintain relevance as new information emerges. The authors
  categorize knowledge into factual, domain, language, and preference types and examine
  three primary adaptation strategies: continual learning, model editing, and retrieval-based
  methods.'
---

# Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge Expansion

## Quick Facts
- **arXiv ID:** 2502.12598
- **Source URL:** https://arxiv.org/abs/2502.12598
- **Reference count:** 40
- **Primary result:** Surveys methods for expanding LLM knowledge through continual learning, model editing, and retrieval-based approaches, categorizing knowledge into factual, domain, language, and preference types.

## Executive Summary
This paper provides a comprehensive survey of methods for expanding the knowledge of large language models to maintain relevance as new information emerges. The authors categorize knowledge expansion techniques into three primary strategies: continual learning (incremental parameter updates), model editing (precise knowledge modifications), and retrieval-based approaches (external knowledge access). Each method addresses different trade-offs between precision, scalability, and computational efficiency. The survey highlights critical challenges including knowledge conflicts, catastrophic forgetting, and the need for standardized evaluation benchmarks.

## Method Summary
The survey systematically categorizes knowledge expansion approaches into three strategies. Continual learning incrementally updates LLM parameters through training on new data, employing regularization techniques to mitigate catastrophic forgetting. Model editing offers precise, efficient modifications to specific knowledge without full retraining by identifying and updating localized parameter regions. Retrieval-based methods dynamically access external knowledge sources during inference, avoiding parameter changes while enabling real-time updates. The paper provides detailed analysis of each approach's mechanisms, advantages, limitations, and appropriate use cases based on knowledge type and application requirements.

## Key Results
- Continual learning excels at large-scale updates like language expansion but requires significant computational resources and risks catastrophic forgetting.
- Model editing provides precise, efficient modifications for targeted factual corrections while minimizing general performance degradation.
- Retrieval-based approaches enable real-time knowledge updates without parameter changes but depend critically on retrieval system quality.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Knowledge can be stably integrated into model parameters via targeted optimization of specific architectural components, provided updates do not trigger catastrophic interference.
- **Mechanism:** Continual learning and model editing modify internal parametric memory by locating factual associations in mid-layer MLP weights and solving constrained optimization problems to insert new "key-value" pairs.
- **Core assumption:** Factual knowledge is sufficiently localized in specific layers that updating one fact does not irreversibly degrade others.
- **Evidence anchors:** Abstract states continual learning "incrementally updates LLM parameters" while model editing offers "precise, efficient modifications." Section 5.1 cites Geva et al. (2021) noting FFN layers function as key-value memories, with ROME/MEMIT updating facts via constrained least-squares in MLP matrices.
- **Break condition:** Mass editing or sequential updates lead to gradual and catastrophic forgetting where general capabilities degrade due to accumulated parameter drift.

### Mechanism 2
- **Claim:** Decoupling knowledge storage from the inference engine allows real-time knowledge updates without retraining costs or side effects.
- **Mechanism:** Retrieval-based methods utilize external datastores and retriever components, fetching relevant context during inference and prepending it to prompts for the frozen LLM to condition generation on current information.
- **Core assumption:** LLMs possess sufficient in-context reasoning capabilities to synthesize retrieved information correctly when the retriever is precise enough to minimize noise.
- **Evidence anchors:** Abstract describes retrieval-based approaches as "dynamically access[ing] external knowledge sources during inference, avoiding parameter changes." Section 6.1 describes kNN-LM and RAG as interpolating output distributions or augmenting prompts with retrieved data.
- **Break condition:** Retrieval failures lead to irrelevant context injection causing hallucinations or confusion, while context window limits restrict expandable knowledge volume.

### Mechanism 3
- **Claim:** Incremental exposure to new data distributions updates generalizable representations while regularization preserves prior knowledge.
- **Mechanism:** Continual Pretraining processes new corpora sequentially with mechanisms like parameter isolation (freezing old experts, adding new ones) or soft-masking (penalizing changes to important weights) to shift model distribution without abandoning learned priors.
- **Core assumption:** Sufficient overlap or regularization exists between new and old data manifolds to prevent optimizer from moving weights into regions invalidating previous tasks.
- **Evidence anchors:** Abstract notes continual learning "mitigating catastrophic forgetting while ensuring long-term performance." Section 4.2 discusses DEMix-DAPT and regularization methods applying soft-masking and contrastive loss to maintain learned knowledge.
- **Break condition:** Cross-stage forgetting occurs where new training phases cause loss of fundamental linguistic capabilities or alignment acquired in earlier stages.

## Foundational Learning

- **Concept:** Catastrophic Forgetting
  - **Why needed here:** Primary failure mode for Continual Learning and large-scale Model Editing; understanding required to select regularization strategies or architectural isolation techniques.
  - **Quick check question:** If you fine-tune a general LLM on a medical corpus, does its ability to write Python code degrade?

- **Concept:** In-Context Learning (ICL)
  - **Why needed here:** Engine behind Retrieval-based methods; model must learn from prompt context without weight updates.
  - **Quick check question:** Can the model answer a question about a document it has never seen before, solely by analyzing that document in the prompt window?

- **Concept:** Parameter Isolation / Modular Architectures
  - **Why needed here:** Key strategy for domain adaptation and continual learning to prevent interference; concepts like "experts" or "adapters" are ubiquitous.
  - **Quick check question:** Does the system add new parameters for a new task, or does it attempt to overwrite existing ones?

## Architecture Onboarding

- **Component map:** External Knowledge -> Retriever -> Prompt Augmentation -> Frozen LLM Backbone -> Generation
- **Critical path:**
  1. **Ingestion:** New knowledge arrives (corpus for CL, fact triple for ME, documents for Retrieval)
  2. **Processing:**
     - *CL:* Forward/Backward pass with regularization loss
     - *ME:* Locate layer -> Calculate update -> Apply update
     - *RAG:* Index document -> Retrieve -> Inject into Prompt
  3. **Evaluation:** Measure Reliability (did it learn it?), Locality (did it break other things?), and Generalization (can it reason with it?)

- **Design tradeoffs:**
  - **Precision vs. Scalability:** Model Editing offers high precision for single facts but fails to scale to thousands of updates without stability issues. Retrieval scales infinitely but lacks precision of directly modifying internal reasoning pathways.
  - **Compute vs. Consistency:** Continual Learning is compute-intensive but produces consistent model. Retrieval is cheap at train time but inconsistent at inference (dependent on retrieval quality).

- **Failure signatures:**
  - **Model Editing:** "Reverse curse" (knowing "A is B" but not "B is A"), or sudden drop in fluency/metrics on unrelated benchmarks after edits.
  - **Retrieval:** "Knowledge conflicts" where model hallucinates blend of parametric memory and retrieved context, or ignores context entirely.
  - **Continual Learning:** Loss of general chat capability or safety alignment as domain adaptation progresses.

- **First 3 experiments:**
  1. **Sanity Check (Retrieval):** Implement basic RAG pipeline. Insert contradictory fact ("moon is made of cheese") into datastore. Verify if model hallucinates or correctly prioritizes retrieved context over internal knowledge.
  2. **Stress Test (Editing):** Apply 50 sequential factual edits using MEMIT. Plot "Locality" metric (performance on unrelated facts) after every 5 edits to observe degradation.
  3. **Baseline (Continual Learning):** Continually pretrain on new domain (medical texts). Evaluate on general reasoning benchmark (GSM8K) before and after to quantify catastrophic forgetting.

## Open Questions the Paper Calls Out
None

## Limitations
- **Knowledge Locality Assumption:** Assumes factual knowledge is sufficiently localized in specific transformer layers for precise editing, but may not hold across all model architectures or knowledge types.
- **Evaluation Benchmark Gap:** Acknowledges lack of comprehensive evaluation benchmarks that fairly compare all three methods across reliability, locality, and generalization metrics.
- **Scaling Challenges:** Notes mass editing leads to performance degradation but provides limited empirical data on scale limits or exact failure thresholds.

## Confidence
- **High Confidence:** Categorization of knowledge types and adaptation strategies is well-supported by literature and provides clear organizational framework.
- **Medium Confidence:** Claims about retrieval-based methods avoiding parameter modification while enabling real-time updates are well-established, but assertion about avoiding catastrophic forgetting requires nuanced consideration.
- **Low Confidence:** Specific claims about relative efficiency and precision of model editing versus other methods lack comprehensive empirical comparison across multiple knowledge types and scale scenarios.

## Next Checks
1. **Locality Stress Test:** Implement sequential factual edits on GPT-2 XL and systematically measure locality degradation across 50+ edits, tracking performance on unrelated knowledge benchmarks after each batch.
2. **Cross-Architecture Knowledge Localization:** Test "locate-and-edit" assumption by applying ROME/MEMIT across different model families (GPT, LLaMA, T5) to verify if factual knowledge is consistently localized in mid-layer FFNs.
3. **Retrieval vs. Parameter Trade-off:** Design controlled experiment comparing RAG performance against fine-tuned model on time-sensitive knowledge tasks, measuring both accuracy and hallucination rates when retrieved context conflicts with parametric memory.