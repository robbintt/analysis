---
ver: rpa2
title: 'TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus
  Generation'
arxiv_id: '2511.15767'
source_url: https://arxiv.org/abs/2511.15767
tags:
- coverage
- verification
- design
- optimization
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for automated Verilog stimulus generation
  using Coverage-Driven Direct Preference Optimization (CD-DPO). The key innovation
  is a preference-based training method that integrates quantitative verification
  coverage feedback into the optimization objective, guiding the model to generate
  testbenches that maximize coverage.
---

# TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation

## Quick Facts
- arXiv ID: 2511.15767
- Source URL: https://arxiv.org/abs/2511.15767
- Authors: Bardia Nadimi; Khashayar Filom; Deming Chen; Hao Zheng
- Reference count: 33
- Primary result: Coverage-driven DPO framework that generates Verilog testbenches maximizing code, branch, and functional coverage, outperforming open-source and commercial baselines on CVDP CID12.

## Executive Summary
This paper introduces TB or Not TB, a framework for automated Verilog stimulus generation that leverages Coverage-Driven Direct Preference Optimization (CD-DPO). The approach integrates quantitative verification coverage feedback into the preference optimization objective, guiding the model to generate testbenches that maximize coverage metrics. A new dataset called PairaNet was curated from PyraNet, pairing high- and low-coverage testbenches labeled with simulation-derived metrics. Experiments on the CVDP CID12 benchmark showed that the proposed model outperformed both open-source and commercial baselines, achieving up to 77.27% improvement in code coverage compared to the vanilla Qwen3 model and 56.78% improvement over Claude Sonnet 3.5 in the best-of-20 setting.

## Method Summary
The TB or Not TB framework employs CD-DPO to fine-tune a Qwen3 language model for SystemVerilog testbench generation. The method uses offline simulation to extract coverage metrics (code, branch, functional) for generated testbenches, creating preference pairs labeled by coverage performance. These pairs are used to train the model via a modified DPO loss that scales gradient updates by the coverage difference between preferred and rejected samples. The framework operates on the PairaNet dataset (182,870 paired testbenches) and is evaluated on the CVDP CID12 benchmark using mean and best-of-20 coverage metrics across three coverage types.

## Key Results
- Achieved up to 77.27% improvement in code coverage compared to vanilla Qwen3 model
- Demonstrated 56.78% improvement over Claude Sonnet 3.5 in best-of-20 setting
- Outperformed both open-source and commercial baselines on CVDP CID12 benchmark
- Showed that CD-DPO's coverage-weighted preference scaling significantly improves testbench quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coverage-weighted preference scaling amplifies learning from high-contrast training pairs.
- Mechanism: The modified loss function replaces static β with β* = β·f(s_p - s_np), where the weight scales with the coverage difference between preferred and rejected samples. When coverage gaps are large, gradient updates are stronger; ambiguous pairs produce smaller updates.
- Core assumption: Larger coverage differences correspond to clearer, more informative preference signals that should drive larger parameter updates.
- Evidence anchors:
  - [section] Page 4 explicitly derives the gradient contribution and states "a large score difference s_p - s_np corresponds to an 'obvious' pair and results in a large β*"
  - [section] Page 3-4 defines the CD-DPO loss in Equation (2) with the dynamic β coefficient
  - [corpus] AlphaDPO [26] and β-DPO [27] referenced as related dynamic-β approaches, though with different formulations
- Break condition: If coverage metrics become noisy or inconsistent across designs, the scaling function may amplify noise rather than signal.

### Mechanism 2
- Claim: Offline simulation enables stable, scalable training by decoupling coverage extraction from model optimization.
- Mechanism: Testbenches are generated and simulated once using professional EDA tools (Aldec Riviera-Pro), coverage metrics are stored, and subsequent training proceeds entirely on GPU without simulator interaction.
- Core assumption: Offline coverage snapshots remain representative of verification quality throughout training; distribution shift is minimal.
- Evidence anchors:
  - [abstract] States the framework enables "simulator-free (during training)" optimization
  - [section] Page 2 claims this design "eliminates the need for an explicit reward model and online policy gradient updates, leading to more stable and computationally efficient training"
  - [corpus] Related work on RL with testbench feedback (arxiv:2504.15804) uses online simulator interaction, providing contrast
- Break condition: If the target DUT characteristics diverge significantly from PyraNet's design distribution, cached preferences may not transfer.

### Mechanism 3
- Claim: Tri-metric coverage aggregation provides more robust preference signals than single-metric evaluation.
- Mechanism: Code, branch, and functional coverage are averaged into a single preference score, capturing structural execution, control-flow exploration, and user-defined functional intent simultaneously.
- Core assumption: Equal weighting of the three metrics appropriately reflects overall verification quality across diverse designs.
- Evidence anchors:
  - [section] Page 3 states "we focus on code, branch, and functional coverage because together they capture complementary aspects of verification quality"
  - [section] Page 5 notes that functional coverage evaluation is inherently qualitative and lacks ground truth for comparison
  - [corpus] No direct corpus evidence on multi-metric aggregation effectiveness; this is a gap
- Break condition: If designs have highly skewed coverage profiles (e.g., data-path heavy with few branches), equal averaging may obscure meaningful differences.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The entire framework builds on DPO's reparameterization of RLHF as supervised learning; understanding the implicit reward formulation r(y|x) = log(π_θ/π_ref) is essential for grasping how coverage signals are integrated.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model?

- Concept: **Hardware Verification Coverage Metrics**
  - Why needed here: The paper treats coverage as the objective signal for preference learning; distinguishing code coverage (line execution), branch coverage (conditional paths), and functional coverage (covergroup bins) is necessary to interpret results.
  - Quick check question: What is the difference between branch coverage and functional coverage in SystemVerilog testbenches?

- Concept: **Testbench/Stimulus Generation Workflow**
  - Why needed here: The framework automates a traditionally manual process; understanding how stimuli exercise a DUT through simulation informs why coverage-driven optimization matters.
  - Quick check question: Why is stimulus generation considered a bottleneck in hardware verification?

## Architecture Onboarding

- Component map:
  PyraNet (source designs) → Claude Sonnet 3.5 (TB generation) → Verilator (linting)
  → Aldec Riviera-Pro (simulation + coverage extraction) → PairaNet (preference pairs)
  → Qwen3 model + CD-DPO fine-tuning → "TB or not TB" model
  → CVDP CID12 (evaluation)

- Critical path: Coverage extraction via Riviera-Pro simulation is the bottleneck—each design requires compilation, execution, and metric collection before preference pairs can be formed. The quality of preference labels directly limits training effectiveness.

- Design tradeoffs:
  - Offline vs. online simulation: Offline sacrifices adaptability for training stability and scalability.
  - β scaling function: The paper normalizes to [0,1] but does not ablate alternative functions.
  - Single vs. multi-objective: Aggregating three metrics into one score loses granularity; multi-objective DPO could preserve tradeoffs.
  - Best-of-20 evaluation: Demonstrates peak potential but inflates perceived performance vs. single-shot deployment.

- Failure signatures:
  - Low functional coverage with high code/branch coverage may indicate syntactically correct but semantically weak covergroups.
  - High variance across 20 generations suggests insufficient preference alignment or model instability.
  - Compilation failures in PairaNet indicate linting gaps or model syntax errors.

- First 3 experiments:
  1. Reproduce baseline comparison: Run vanilla Qwen3-8B and CD-DPO Qwen3-8B on a held-out design from CVDP CID12, measuring all three coverage metrics over 20 generations.
  2. Ablate β scaling: Train with standard DPO (fixed β=0.2) vs. CD-DPO (dynamic β*) on PairaNet subset to quantify the contribution of coverage-weighted scaling.
  3. Test dataset quality sensitivity: Train on PairaNet vs. a high-coverage filtered subset to validate the paper's claim that dataset richness drives performance (per RQ4 analysis on page 6).

## Open Questions the Paper Calls Out

- Question: Can multi-objective optimization jointly maximize both coverage metrics and assertion robustness without trade-offs, and what is the optimal weighting scheme between these objectives?
  - Basis in paper: [explicit] The conclusion states: "We aim to incorporate multi-objective optimization to jointly improve coverage and assertion robustness."
  - Why unresolved: The current CD-DPO framework optimizes only for coverage (code, branch, functional averaged), with no mechanism for assertion quality or robustness signals.
  - What evidence would resolve it: A Pareto frontier analysis showing coverage vs. assertion robustness trade-offs, plus comparison of different multi-objective formulations on a benchmark suite with assertion ground truth.

- Question: How can formal verification signals (e.g., model checking results, property coverage) be effectively integrated as supervision signals alongside simulation-based coverage metrics?
  - Basis in paper: [explicit] The conclusion states plans to "integrate formal verification signals alongside simulation-based metrics for richer supervision."
  - Why unresolved: Formal verification outputs are fundamentally different from simulation coverage—binary (pass/fail) vs. continuous, and potentially much more computationally expensive to obtain during dataset construction.
  - What evidence would resolve it: An extension of PairaNet with formal verification annotations, plus experiments showing whether formal signals improve testbench quality beyond simulation coverage alone.

- Question: What causes the high variance in TB or not TB's outputs (mean performance below commercial baseline), and can fine-tuning stability be improved without sacrificing peak performance?
  - Basis in paper: [explicit] The authors note: "The variability in our model's outputs may arise from factors such as sensitivity to initialization, or limited fine-tuning stability, highlighting an opportunity for future work to improve robustness while maintaining peak performance."
  - Why unresolved: The paper does not ablate initialization strategies, learning rate schedules, or other training stability factors; the variance mechanism remains uninvestigated.
  - What evidence would resolve it: Ablation studies varying random seeds, initialization methods, and fine-tuning hyperparameters, with variance quantification across multiple training runs.

- Question: How well does CD-DPO generalize to designs outside the CVDP CID12 benchmark, particularly for designs with different complexity profiles or verification requirements?
  - Basis in paper: [inferred] The paper evaluates only on CVDP CID12 (12 designs) and acknowledges CVDP includes CID13 and CID14 tasks that were not used due to tool access limitations.
  - Why unresolved: No cross-benchmark evaluation or analysis of design characteristics (e.g., FSM complexity, interface count) that predict CD-DPO effectiveness.
  - What evidence would resolve it: Evaluation on additional CVDP sections (CID13, CID14), other hardware verification benchmarks, or analysis correlating design features with coverage improvement magnitude.

## Limitations

- The framework relies on offline simulation and synthetic preference data that may not generalize to production designs.
- The β scaling function is underspecified, and the equal-weight aggregation of three coverage metrics lacks empirical justification.
- Functional coverage evaluation remains qualitative without ground-truth comparisons.
- The best-of-20 evaluation inflates performance relative to single-shot deployment.

## Confidence

- **High Confidence**: Coverage-driven DPO framework design, offline simulation workflow, PairaNet curation pipeline, CVDP CID12 benchmark setup
- **Medium Confidence**: Quantitative performance claims vs. open-source and commercial baselines, β scaling contribution, multi-metric aggregation effectiveness
- **Low Confidence**: Generalization to unseen designs, robustness to coverage metric noise, real-world deployment viability

## Next Checks

1. **Ablation of β scaling**: Train CD-DPO with fixed β=0.2 versus dynamic β* on a PairaNet subset to isolate the contribution of coverage-weighted scaling.

2. **Dataset quality sensitivity**: Compare CD-DPO performance when trained on full PairaNet versus a high-coverage filtered subset to validate the claim that dataset richness drives performance.

3. **Generalization stress test**: Evaluate the best-performing CD-DPO model on a held-out design from a different dataset (e.g., RealBench) to assess transfer beyond CVDP CID12.