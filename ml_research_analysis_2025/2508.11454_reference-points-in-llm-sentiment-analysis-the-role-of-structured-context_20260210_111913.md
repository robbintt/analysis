---
ver: rpa2
title: 'Reference Points in LLM Sentiment Analysis: The Role of Structured Context'
arxiv_id: '2508.11454'
source_url: https://arxiv.org/abs/2508.11454
tags:
- information
- reference
- sentiment
- points
- restaurant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study improves LLM-based sentiment analysis by incorporating
  supplementary contextual information as reference points, drawing on prospect theory
  and expectation-disconfirmation theory. The research addresses the gap where most
  sentiment analysis focuses only on review text while ignoring rich contextual data
  available in real-world platforms.
---

# Reference Points in LLM Sentiment Analysis: The Role of Structured Context

## Quick Facts
- arXiv ID: 2508.11454
- Source URL: https://arxiv.org/abs/2508.11454
- Reference count: 38
- Key result: Structured JSON prompts with reference points improve 3B LLM sentiment analysis by 1.6-4% Macro-F1 and 9-16% RMSE reduction

## Executive Summary
This study demonstrates that incorporating supplementary contextual information as reference points significantly improves LLM-based sentiment analysis performance. Drawing on prospect theory and expectation-disconfirmation theory, the research shows that lightweight 3B parameter models can achieve competitive results against larger fine-tuned models by using structured JSON-formatted prompts with user average ratings, business average ratings, and other contextual factors. The approach addresses a key gap where most sentiment analysis focuses only on review text while ignoring rich contextual data available in real-world platforms. Experiments on Yelp restaurant and nightlife datasets show consistent improvements across both metrics, with the structured prompting approach enabling edge-device deployment without sacrificing accuracy.

## Method Summary
The method uses Llama-3.2-3B-Instruct with one-shot prompting to classify 5-star reviews. JSON-formatted supplementary context (user average stars, business average stars, restaurant name, open hours/days) is injected alongside review text. The study compares JSON vs natural language prompt formats and evaluates against baselines including fine-tuned RoBERTa and DeBERTa models. Data comes from Yelp Open Dataset with 500 unique user-business reviews per category, ensuring no overlap between training and test sets for baseline models.

## Key Results
- JSON prompts consistently outperform natural language prompts with the same information content
- Macro-F1 increased by 1.6-4% compared to baseline LLM (None) condition
- RMSE decreased by 16-9.1%, showing the model gets closer to true values even when missing exact classes
- The lightweight 3B model outperformed fine-tuned RoBERTa and DeBERTa without any fine-tuning
- Performance improves when actual ratings deviate significantly from reference points, confirming contextual reasoning over label proxying

## Why This Works (Mechanism)

### Mechanism 1
Structured (JSON) prompting enables lightweight LLMs to utilize supplementary context more effectively than natural language descriptions. The JSON format likely maps more directly to the model's internal attention mechanisms, allowing it to weigh numerical context against text efficiently rather than parsing verbose descriptions.

### Mechanism 2
User and business average ratings function as "reference points" for contextual reasoning, not merely as proxies for the final label. The model calculates the "expectationâ€“evaluation gap," predicting higher accuracy when actual ratings diverge significantly from reference points, suggesting it uses context to calibrate sentiment intensity.

### Mechanism 3
Prediction confidence is modulated by the alignment between multiple reference points. When user averages align with business averages, the model achieves near-perfect accuracy. When they conflict, the task becomes inherently ambiguous, increasing error rates as the LLM acts as a Bayesian reasoner weighing conflicting priors.

## Foundational Learning

**Concept: Expectation-Disconfirmation Theory (EDT)**
- Why needed here: This is the theoretical backbone explaining that customers rate experiences based on the gap between expectation and experience, not in isolation
- Quick check question: If a user with 2.0 average rating gives a 4-star review, how should the model interpret the text sentiment compared to a user with 4.5 average giving the same 4-star review? (Answer: The first user is likely ecstatic; the second is disappointed. The model should weight text sentiment intensity differently)

**Concept: In-Context Learning (ICL) with Structured Data**
- Why needed here: The paper relies on the model's ability to parse JSON without training, requiring understanding that LLMs can act as reasoners over structured keys
- Quick check question: Why is JSON format preferred over "The user usually rates 3.5"? (Answer: Structured formats help lightweight models isolate specific variables better than parsing natural language sentences)

**Concept: Evaluation Metrics for Ordinal Data (RMSE vs F1)**
- Why needed here: Sentiment is ordinal, so a classification error (predicting 4 instead of 5) is better than a gross error (predicting 1 instead of 5)
- Quick check question: Why emphasize 16% drop in RMSE alongside smaller 4% rise in Macro-F1? (Answer: Reference points help the model get closer to true value even when missing exact class, reducing large mistakes)

## Architecture Onboarding

**Component map:** Review Text -> JSON Context (user_average, business_average) -> One-shot Prompt Template -> Llama-3.2-3B-Instruct -> Integer 1-5 Output

**Critical path:**
1. Retrieve user/business history from CRM database
2. Inject averages into JSON block after review text but before output prompt
3. Ensure one-shot example correctly demonstrates JSON usage

**Design tradeoffs:**
- 3B Model vs Fine-tuned RoBERTa: 3B offers zero-shot flexibility and edge deployment but requires prompt engineering to match accuracy of heavier models
- One-shot vs Few-shot: Authors limit to one-shot to isolate context effects, but admit few-shot usually performs better in production

**Failure signatures:**
- Reference Conflict: If user average differs significantly from business average, model may hallucinate or output low-confidence predictions
- Token Truncation: Very long reviews combined with JSON prompt might hit context windows

**First 3 experiments:**
1. Baseline Validation: Run LLM with no supplementary info vs JSON-UBO on held-out 100 reviews, verify RMSE drop
2. Format Sensitivity Test: Compare JSON against concise NL string ("User Avg: 3.0, Biz Avg: 4.0") to test if benefit is structure or brevity
3. Proxy Robustness (Ablation): Shuffle user average values between users to break correlation with ground truth, confirm model uses signal not text

## Open Questions the Paper Calls Out

**Open Question 1:** Do performance benefits of structured JSON prompting persist across different model families and larger parameter scales? The study used only Llama-3.2-3B-Instruct, leaving interaction between prompt structure and model scale unexplored.

**Open Question 2:** Do psychological mechanisms of prospect theory and expectation-disconfirmation theory empirically explain model's improved reasoning? The study shows reference points improve accuracy but doesn't confirm if model mimics human "reference dependence" or learns distinct statistical correlations.

**Open Question 3:** Is structured reference point methodology effective for sentiment analysis in languages other than English or on platforms with different metadata? The study restricted to English Yelp reviews, leaving effectiveness on other languages, domains, or platforms untested.

**Open Question 4:** What are quantitative differences in inference time and memory usage between structured JSON prompting and baseline models in resource-constrained environments? The paper claims computational efficiency but didn't provide measurements of inference time or memory usage across different prompt formats.

## Limitations
- Cannot determine which specific reference points drive improvements since all supplementary data pooled into single "structured context" condition
- Sample size constraints with only 500 reviews per category may not capture full variability of reference point interactions
- Data leakage risk between train/test splits for baseline models not fully addressed in methodology

## Confidence

**High Confidence:** Core finding that structured context improves LLM sentiment analysis performance (Macro-F1 +1.6-4%, RMSE -16-9.1%) is well-supported by experimental results and statistical significance testing.

**Medium Confidence:** Mechanism that reference points enable "genuine contextual reasoning" rather than label proxying is plausible but not definitively proven, as alternative explanations cannot be ruled out.

**Low Confidence:** Specific claim that JSON formatting is superior to natural language for reference point comprehension lacks direct experimental validation.

## Next Checks

1. **Reference Point Ablation Study:** Systematically remove individual reference points to quantify their independent contributions and reveal whether "other factors" are driving results rather than theoretical reference points.

2. **Randomized Reference Point Test:** Shuffle user average ratings across different reviews while keeping text and business averages intact. If performance drops to baseline, this confirms the model uses actual reference point correlation rather than reasoning about deviation.

3. **Natural Language Control:** Implement an optimized natural language prompt with equivalent information content but minimal tokens. Compare performance against JSON to test whether benefit is structural or token-efficiency driven.