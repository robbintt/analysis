---
ver: rpa2
title: 'Language Models as Artificial Learners: Investigating Crosslinguistic Influence'
arxiv_id: '2601.21587'
source_url: https://arxiv.org/abs/2601.21587
tags:
- language
- priming
- effects
- languages
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses language models as artificial learners to systematically
  study crosslinguistic influence (CLI) in bilingual processing. By manipulating age
  of exposure, the authors simulate varying levels of L1 dominance and L2 proficiency.
---

# Language Models as Artificial Learners: Investigating Crosslinguistic Influence

## Quick Facts
- arXiv ID: 2601.21587
- Source URL: https://arxiv.org/abs/2601.21587
- Reference count: 18
- Models show crosslinguistic influence effects modulated by L1 entrenchment and syntactic distance, supporting a hybrid shared-separate syntax framework

## Executive Summary
This paper systematically investigates crosslinguistic influence (CLI) in bilingual language models by manipulating age of exposure and syntactic distance between languages. The authors train models sequentially on L1-only data before introducing L2, creating varying levels of L1 dominance and L2 proficiency. Using mechanistic analysis tools like LogitLens and crosslinguistic priming, they demonstrate that CLI effects are stronger for typologically similar languages and depend on L1 entrenchment. The findings support a hybrid model where shared syntactic structures rely on overlapping neural representations while non-shared structures remain separate-but-connected, aligning with human bilingual processing patterns.

## Method Summary
The study uses GPT-2 models trained on bilingual data from OSCAR corpus, manipulating age of exposure (AoE) to control L1 entrenchment. For each L1-L2 pair, models undergo sequential training: pure L1 for N steps (where N varies: 0, 16K, 32K, 48K) followed by interleaved L1-L2 training for the remaining steps. Evaluation uses BLiMP for grammatical knowledge, FCE for non-native text preference, and crosslinguistic priming to assess syntactic transfer. Mechanistic analysis employs LogitLens to detect L1 co-activation during L2 processing and language neuron identification to quantify representational overlap.

## Key Results
- CLI effects are stronger for typologically similar languages and require L1 entrenchment from sequential training
- Crosslinguistic priming shows bidirectional facilitation for shared structures but asymmetric effects for ungrammatical constructions dependent on language dominance
- LogitLens analysis reveals L1 representations remain active in hidden states during L2 processing, with co-activation increasing with age of exposure
- Orthographic overlap facilitates CLI effects, with romanized Greek and Korean showing patterns similar to Latin-script languages

## Why This Works (Mechanism)

### Mechanism 1: L1 Entrenchment Modulates CLI
Prior L1-only training creates entrenched representations that bias subsequent L2 acquisition. Sequential training allows L1 weights to stabilize before L2 exposure, and these entrenched weights influence how L2 representations form. The mechanism assumes weight entrenchment from early training resists modification during later L2 training. Evidence shows early imbalanced training is insufficient to produce distance-consistent CLI effects without prior L1 entrenchment.

### Mechanism 2: Shared Syntax Enables Bidirectional Priming
Grammatical structures shared between L1-L2 exhibit bidirectional priming while non-shared structures show asymmetric priming dependent on language dominance. Shared structures rely on overlapping neural circuitry that activates when either language uses the structure, while unshared structures remain separate-but-connected. Evidence indicates priming of grammatical structures is largely bidirectional, but ungrammatical transfer requires more entrenched L1 structure to override L2 processing.

### Mechanism 3: L1 Co-activation During L2 Processing
L1 representations remain active in hidden states during L2-only input processing, with bilingual models developing mixed-language internal representations. LogitLens decoding reveals L1 tokens surfacing at intermediate layers even without L1 input, indicating language-integrated rather than language-isolated representations. Evidence shows clear increase in L1/L2 ratio corresponding to increased age of exposure, while balanced bilingual models maintain ratios near zero.

## Foundational Learning

- **Crosslinguistic Priming**: Core experimental paradigm measuring CLI and testing shared vs. separate-but-connected representations. Quick check: Why does processing a structure in Language A facilitate producing the same structure in Language B?

- **Syntactic Distance (WALS-based)**: Primary independent variable quantified as non-shared features from World Atlas of Language Structures. Quick check: How would you calculate syntactic distance between Spanish and English using WALS features?

- **LogitLens Interpretation**: Mechanistic tool projecting intermediate hidden states to vocabulary space to detect L1 presence during L2 processing. Quick check: What does it indicate when layer-6 hidden states decode to L1 tokens while processing L2-only input?

## Architecture Onboarding

- **Component map**: GPT-2 (12 layers) -> Sequential L1-L2 training -> LogitLens analysis -> CLI measurement
- **Critical path**: 1) Pretrain on L1-only sequences (0â†’48K steps) -> 2) Introduce L2 with interleaved batches -> 3) Continue to 64K steps -> 4) Evaluate BLiMP with/without L1 prime -> 5) Apply LogitLens during L2 inference
- **Design tradeoffs**: L1 entrenchment vs. L2 proficiency (later introduction increases dominance but reduces L2 exposure); model scale vs. priming sensitivity (smaller models show CLI but lose sensitivity); cognitive plausibility vs. grammatical competence (human-scale data limits performance)
- **Failure signatures**: Simultaneous training (Step=0) produces uniform CLI effects; late introduction with distant L1 yields low L2 proficiency; different script without romanization causes inconsistent priming; model too small decouples priming from syntactic distance
- **First 3 experiments**: 1) Replicate CLI-distance correlation with German-English model (Step=32K) and verify accuracy drop vs. monolingual baseline; 2) Validate entrenchment necessity with early-imbalanced model (both languages from Step=0) and confirm lack of distance scaling; 3) Test LogitLens co-activation with Turkish-English model and verify L1/L2 token ratio increases with age of exposure

## Open Questions the Paper Calls Out

### Open Question 1
Does orthographic overlap function as a prerequisite for syntactic co-activation in low-resource scenarios, or can shared abstract representations be leveraged independently of script similarity? The paper notes CLI effects for Greek and Korean only aligned with Latin-script languages after romanization, suggesting orthographic overlap serves as a critical facilitator for CLI, but doesn't determine if this is due to tokenization artifacts or deeper alignment of syntactic embeddings.

### Open Question 2
Does increasing model capacity diminish or enhance the "hybrid" model of shared syntax, specifically regarding the entrenchment of ungrammatical structures? The authors find smaller models lack the necessary capacity to respond to crosslinguistic priming, leaving unclear whether observed CLI limitations are due to insufficient parameter space or fundamental architectural constraints.

### Open Question 3
To what extent does the mechanistic "L1 co-activation" observed via LogitLens in artificial learners correspond to the neurocognitive models of inhibitory control in human bilinguals? While the paper identifies L1 tokens in L2 processing, it doesn't determine if the model suppresses this activation via mechanisms analogous to human cognitive control or if it merely averages the representations.

## Limitations
- Ecological validity concerns with sequential training differing from human bilingual acquisition patterns
- Syntactic distance metric may not fully capture structural similarity affecting CLI beyond feature-based distance
- Evaluation relies heavily on grammaticality judgments which may not represent real-world bilingual processing
- 1B token per language corpus may be insufficient for models to develop mature CLI patterns, especially for later AoE conditions

## Confidence

- **High confidence**: L1 co-activation during L2 processing (direct LogitLens evidence), existence of CLI effects in language models (multiple independent paradigms), relationship between syntactic distance and CLI magnitude
- **Medium confidence**: Necessity of L1 entrenchment for distance-sensitive CLI (correlational evidence with potential confounding factors), bidirectional vs. asymmetric priming patterns (requires precise control of shared/unshared structures), hybrid shared-separate syntax model (fits data but alternative explanations possible)
- **Low confidence**: Direct cognitive plausibility claims (models differ substantially from human learners), specific threshold effects for priming (sensitive to model scale and data quantity)

## Next Checks

1. **Test entrenchment necessity**: Train an "early-imbalanced" model with same L1/L2 token ratio as Step=32K condition but both languages introduced from step 0, then compare CLI-distance correlation to confirm entrenchment requirement

2. **Validate priming mechanism**: Conduct ablation studies removing intermediate layers (where co-activation peaks) to test whether L1 presence in hidden states is necessary for priming effects

3. **Cross-validate with human data**: Compare model CLI patterns against psycholinguistic studies of German-English bilinguals to assess whether distance-based effects align with human processing asymmetries