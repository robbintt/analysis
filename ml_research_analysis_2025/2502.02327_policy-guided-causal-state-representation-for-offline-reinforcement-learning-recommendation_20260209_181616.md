---
ver: rpa2
title: Policy-Guided Causal State Representation for Offline Reinforcement Learning
  Recommendation
arxiv_id: '2502.02327'
source_url: https://arxiv.org/abs/2502.02327
tags:
- state
- causal
- learning
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning effective state
  representations in offline reinforcement learning-based recommender systems (RLRS),
  where high-dimensional and noisy state data, combined with missing transitions,
  hinder accurate identification of user-preference features. The authors propose
  Policy-Guided Causal Representation (PGCR), a two-stage framework that first uses
  a causal feature selection policy to isolate causally relevant components (CRCs)
  of the state, guided by a Wasserstein distance-based reward function, and then trains
  an encoder to learn compact state representations by minimizing the MSE loss between
  original and modified states.
---

# Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation

## Quick Facts
- arXiv ID: 2502.02327
- Source URL: https://arxiv.org/abs/2502.02327
- Reference count: 38
- Primary result: Proposed PGCR framework significantly improves recommendation performance in offline RL by learning compact state representations that isolate causally relevant components (CRCs), achieving higher cumulative and average rewards across multiple datasets and online simulations.

## Executive Summary
This paper addresses the challenge of learning effective state representations in offline reinforcement learning-based recommender systems (RLRS), where high-dimensional and noisy state data, combined with missing transitions, hinder accurate identification of user-preference features. The authors propose Policy-Guided Causal Representation (PGCR), a two-stage framework that first uses a causal feature selection policy to isolate causally relevant components (CRCs) of the state, guided by a Wasserstein distance-based reward function, and then trains an encoder to learn compact state representations by minimizing the MSE loss between original and modified states. Theoretical analysis proves the identifiability of causal effects, and extensive experiments demonstrate that PGCR significantly improves recommendation performance across multiple offline datasets and online simulation platforms, achieving higher cumulative and average rewards while maintaining stable interaction lengths.

## Method Summary
PGCR is a two-stage framework for offline RL recommendation. Stage 1 learns a causal feature selection policy using a Wasserstein distance-based reward that measures the causal effect of state components on the reward. The policy generates modified states that retain only causally relevant components (CRCs). Stage 2 trains an encoder to minimize MSE between latent representations of original and modified states, forcing the encoder to focus on CRCs. The final recommendation policy is trained on these compact representations. The method uses standard RL backbones (DDPG, SAC, TD3) and requires an expert policy for training the causal agent.

## Key Results
- PGCR achieves higher cumulative rewards and average rewards compared to baseline RL methods across multiple offline datasets (MovieLens-1M, Coat, KuaiRec, KuaiRand).
- The framework demonstrates improved performance on the VirtualTB online simulator, showing better Click-Through Rate (CTR) while maintaining stable interaction lengths.
- Ablation studies show PGCR-C (random intervention) performs significantly worse than PGCR, validating the importance of learned causal feature selection.

## Why This Works (Mechanism)

### Mechanism 1: Wasserstein Distance-Guided Causal Feature Selection
The method identifies state components that are causally relevant to reward outcomes by measuring the impact of intervening on those components. The system uses a causal feature selection policy to perform a "do-intervention" on the state, generating a modified state that retains only some components. It then measures the Wasserstein distance between the original reward distribution and the reward distribution obtained from the modified state. A small distance suggests the altered components were irrelevant, while a large distance implies they were causal. This distance is inverted into a reward signal to train the selection policy to retain only the causally relevant components (CRCs).

### Mechanism 2: Representation Invariance via Contrastive Learning
The framework generates pairs of states (s_t, s^I_t) where s^I_t is the modified state containing only CRCs. The encoder is trained to minimize the Mean Squared Error (MSE) between φ(s_t) and φ(s^I_t). Since s^I_t is presumed to have altered CIRCs, minimizing the distance forces the encoder to learn a mapping that is invariant to those components, effectively discarding them and focusing only on the shared CRCs.

### Mechanism 3: Theoretical Identifiability Guarantees
The paper models the recommender system as a Structural Causal Model (SCM) and formulates a causal diagram for the MDP. The authors prove that the current state s_t satisfies the "back-door criterion" relative to the pair (Action, Next State). This theoretically ensures that the causal effect of an intervention can be calculated from observational data, which is the foundation for generating the modified states s^I_t.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Do-Calculus**
  - Why needed here: The core logic of this paper is built on Pearl's causal framework. You cannot understand how the "causal feature selection policy" works or what Proposition 1 means without grasping concepts like the "back-door criterion," "d-separation," and the "do-operator" (do(a_t)) for simulating interventions.
  - Quick check question: Can you explain, in simple terms, why the "back-door criterion" is necessary for isolating the causal effect of an action on a future state?

- **Concept: Offline Reinforcement Learning (RL)**
  - Why needed here: This is the problem setting. The method is specifically designed for offline RL, meaning it learns from a fixed dataset without exploration. This constraint motivates the need for advanced representation learning, as the agent cannot simply "try things out" to see what features are important.
  - Quick check question: What is the primary risk of using standard online RL algorithms directly on a fixed, historical dataset, and how does a representation learning approach like this one help mitigate it?

- **Concept: Wasserstein Distance**
  - Why needed here: This metric is the engine of the causal policy's learning signal. It quantifies the "cost" of transforming one probability distribution (original rewards) into another (intervened rewards). Understanding it as a measure of distributional similarity is key to seeing how it acts as a reward signal.
  - Quick check question: In this paper, a *small* Wasserstein distance between the original and intervened reward distributions is treated as a high reward for the causal policy. Why does this make sense?

## Architecture Onboarding

- **Component map:** Offline Dataset -> Causal Feature Selection Policy -> Modified States -> Encoder -> Compact Representations -> Recommendation Policy
- **Critical path:** Stage 1 is the dependency: The quality of the entire system hinges on the Causal Feature Selection Policy successfully learning to generate high-quality modified states s^I_t.
- **Design tradeoffs:**
  - Proposed: A two-stage, theoretically grounded causal intervention approach. Trade-off: High complexity and computational cost. Requires a separate expert policy for training the causal agent and relies on strong MDP assumptions for theoretical guarantees.
  - Baseline (e.g., standard DDPG/SAC): Simple, end-to-end learning from raw states. Trade-off: Highly susceptible to noise and irrelevant features in the state, leading to suboptimal policies and poor performance in the presence of spurious correlations.
  - PGCR-C (Ablation): Replaces the learned causal agent with a random state sampler. Trade-off: Removes the computational burden of Stage 1 but results in significantly worse performance, demonstrating the value of the learned causal signal.
- **Failure signatures:**
  - Stage 1 fails to converge: The causal agent's reward (based on Wasserstein distance) does not increase. This will result in s^I_t being random or meaningless, causing the encoder in Stage 2 to learn noisy representations. Check the expert policy's performance and the λ scaling parameter.
  - Encoder loss plateaus with high error: This suggests the original and modified states are too dissimilar, meaning the causal agent is altering CRCs instead of CIRCs. Review the Stage 1 policy's intervention logic.
  - Downstream RL policy performance is unstable: The learned representations may be discarding *too much* information. The λ parameter might be too aggressive, causing the causal agent to be overly strict.
- **First 3 experiments:**
  1. Reproduce the ablation study (Table 2): Compare PGCR vs. PGCR-C (random) vs. Baseline. This is the most direct way to validate the core claim that the *learned* causal policy provides unique value. Confirm that PGCR shows the reported gains.
  2. Sensitivity Analysis of λ (Figure 4): Run a sweep of the reward scaling parameter λ (e.g., from 0.1 to 0.9) on a single dataset. This validates the robustness of the Stage 1 reward signal and helps find the optimal operating point not mentioned in the text.
  3. Scrutinize the State Pairs: Visualize or analyze a sample of original states (s_t) and their modified counterparts (s^I_t) produced by the trained Causal Agent. Qualitatively assess if the differences align with what one would intuitively consider "non-causal" noise. This provides a sanity check on the entire framework's logic.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but identifies future work directions: exploring extension of PGCR to multi-agent environments with dynamically changing user preferences.

## Limitations
- The theoretical identifiability proof relies on strict MDP assumptions; violations due to partial observability or unmeasured confounders would invalidate the causal claims.
- The Wasserstein distance-based reward signal depends on the quality of the expert policy; if the expert is suboptimal, the identified "causal" components may be biased or incomplete.
- The method requires a pre-trained expert policy and environment interaction for training the causal feature selection policy, limiting applicability to purely static offline datasets.

## Confidence
- **High Confidence**: The two-stage framework architecture is clearly specified and reproducible; the use of Wasserstein distance as a distributional metric is well-established.
- **Medium Confidence**: The theoretical identifiability proof is sound under stated assumptions, but real-world applicability depends on unverifiable environmental factors.
- **Low Confidence**: The claim that PGCR "significantly improves recommendation performance" across all datasets is based on reported metrics without full experimental details or error bars for statistical significance.

## Next Checks
1. Run the ablation study (PGCR vs PGCR-C) on at least one dataset to verify that the learned causal feature selection policy provides measurable performance gains over random interventions.
2. Conduct a sensitivity analysis on the Wasserstein reward scaling parameter λ across a range of values (e.g., 0.05 to 0.3) to confirm the robustness of the causal policy training.
3. Visualize and analyze sample state pairs (original vs modified) from the trained causal policy to qualitatively assess whether the differences align with intuitive notions of non-causal noise.