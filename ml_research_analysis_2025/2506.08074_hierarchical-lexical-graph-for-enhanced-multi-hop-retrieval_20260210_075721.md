---
ver: rpa2
title: Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval
arxiv_id: '2506.08074'
source_url: https://arxiv.org/abs/2506.08074
tags:
- retrieval
- graph
- statements
- multi-hop
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Hierarchical Lexical Graph (HLG), a three-tier\
  \ knowledge graph that enables fine-grained, multi-hop retrieval by linking atomic\
  \ propositions to topics and entities across documents. Two complementary retrieval\
  \ methods\u2014StatementGraphRAG for precise, fact-based queries and TopicGraphRAG\
  \ for broader, exploratory queries\u2014leverage this structure."
---

# Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval

## Quick Facts
- arXiv ID: 2506.08074
- Source URL: https://arxiv.org/abs/2506.08074
- Reference count: 29
- Key outcome: HLG-based RAG methods outperform naive chunk-based RAG by 7.5 percentage points in correctness (73.6% vs. 66.1%) and improve answer recall by 1.6 points (52.4% vs. 50.8%)

## Executive Summary
This paper introduces the Hierarchical Lexical Graph (HLG), a three-tier knowledge graph that enables fine-grained, multi-hop retrieval by linking atomic propositions to topics and entities across documents. Two complementary retrieval methods—StatementGraphRAG for precise, fact-based queries and TopicGraphRAG for broader, exploratory queries—leverage this structure. The authors also address the lack of complex multi-hop QA datasets by introducing a synthetic data generation pipeline. Experiments on five datasets show HLG-based methods outperform naive chunk-based RAG by 7.5 percentage points in correctness (73.6% vs. 66.1%) and improve answer recall by 1.6 points (52.4% vs. 50.8%), with pairwise evaluations indicating wins over baselines in 74–78% of cases.

## Method Summary
The Hierarchical Lexical Graph (HLG) is a three-tier knowledge graph structure where statements (atomic propositions extracted from text) are connected to topics and entities. Statements are derived from text chunks using an LLM-based propositionizer, then clustered by shared entities and topics. Two retrieval methods operate on this structure: StatementGraphRAG performs focused multi-hop retrieval through statement expansion, while TopicGraphRAG enables broader exploration through topic and entity linking. The authors also introduce a synthetic data generation pipeline using GPT-4 to create complex multi-hop QA pairs by extracting topics, entities, and relations from documents.

## Key Results
- HLG-based methods achieve 73.6% correctness versus 66.1% for naive chunk-based RAG (7.5 percentage point improvement)
- Answer recall improves from 50.8% to 52.4% (1.6 percentage point gain)
- Pairwise evaluations show HLG methods win over baselines in 74–78% of cases
- HLG reduces hallucination by anchoring responses to extracted atomic facts

## Why This Works (Mechanism)
The Hierarchical Lexical Graph works by creating a fine-grained representation of document content through atomic propositions (statements) that capture precise factual relationships. These statements are hierarchically organized with topics and entities, enabling multi-hop reasoning through graph traversal. Unlike traditional RAG that retrieves entire chunks, HLG's statement-level granularity allows for more precise retrieval by connecting related facts across documents. The two retrieval methods complement each other: StatementGraphRAG provides precise, fact-based answers through targeted graph expansion, while TopicGraphRAG supports broader query exploration through topic and entity relationships.

## Foundational Learning
- **Multi-hop reasoning**: Understanding how to retrieve and connect information across multiple documents or sections
  - Why needed: Many complex questions require synthesizing information from multiple sources
  - Quick check: Can identify the intermediate steps needed to answer a question that spans multiple documents

- **Knowledge graph construction**: Building structured representations from unstructured text
  - Why needed: Enables semantic relationships between facts for more effective retrieval
  - Quick check: Can extract entities, relations, and topics from text and represent them as graph nodes and edges

- **Graph traversal algorithms**: Methods for navigating connected nodes in a graph structure
  - Why needed: Essential for finding relevant information through multi-hop paths
  - Quick check: Can implement breadth-first or depth-first search with appropriate stopping criteria

- **Statement-level proposition extraction**: Converting text into atomic factual units
  - Why needed: Provides fine-grained retrieval units that capture precise relationships
  - Quick check: Can identify subject-predicate-object triples from sentences

## Architecture Onboarding

**Component map**: Documents → Chunks → Propositions (Statements) → HLG (Statements-Topics-Entities) → Retrieval Methods → Answers

**Critical path**: Document ingestion → Chunk splitting → Proposition extraction → HLG construction → Query processing → Graph traversal → Answer generation

**Design tradeoffs**: Statement-level granularity vs. computational cost; hierarchical organization vs. query complexity; synthetic data generation vs. real-world generalization

**Failure signatures**: 
- LLM hallucination during proposition extraction leading to incorrect facts in HLG
- Over-expansion during graph traversal causing noise and irrelevant information
- Proposition clustering failures resulting in disconnected or poorly connected graph components

**First experiments**:
1. Evaluate proposition extraction quality by comparing LLM-generated statements against human-annotated facts
2. Test graph traversal performance on single-hop vs. multi-hop queries to identify optimal expansion strategies
3. Measure indexing and query latency for HLG construction and retrieval compared to baseline chunk-based RAG

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the proposition extraction process be optimized to reduce LLM token consumption and indexing latency without degrading the quality of the Hierarchical Lexical Graph (HLG)?
- Basis in paper: [explicit] The authors identify "LLM Invocation and Indexing Costs" as a limitation, noting that processing chunks for statements and entities accumulates substantial compute costs.
- Why unresolved: The current implementation relies on general-purpose LLMs (Claude-3 Sonnet) for extraction, which is expensive. While smaller models like `flan-t5-large` are suggested, the paper does not quantify the trade-off between cost reduction and extraction fidelity.
- What evidence would resolve it: Experimental results comparing retrieval performance (Correctness/Recall) and indexing costs between the current LLM-based propositionizer and a distilled or fine-tuned smaller model.

### Open Question 2
- Question: Can an adaptive mechanism be developed to automatically detect single-hop queries and bypass multi-hop graph expansion to prevent noise and latency penalties?
- Basis in paper: [explicit] The authors note a "minor regression for multi-hop graph methods in single-hop contexts" (specifically WikiHowQA) and suggest "early-stopping heuristics" as a limitation.
- Why unresolved: The current system applies graph traversal (StatementGraphRAG/TopicGraphRAG) broadly, which introduces extraneous statements for simple queries. The paper lists this as a specific area for "Future Work" to recognize query types.
- What evidence would resolve it: A study showing that an adaptive routing mechanism maintains high performance on multi-hop datasets (e.g., MultiHop-RAG) while recovering performance on single-hop datasets (e.g., WikiHowQA) by reducing noise.

### Open Question 3
- Question: Does a hybrid retrieval strategy that begins with coarse chunk-level search before applying fine-grained statement-level expansion offer a better trade-off between latency and retrieval precision?
- Basis in paper: [explicit] In the "Future Work" section, the authors propose exploring a "layered pipeline" that uses chunk-level searches to localize segments before applying statement-level expansions.
- Why unresolved: The current evaluation contrasts statement-based methods against chunk-based baselines but does not test a sequential combination of the two to see if it optimizes the speed/accuracy balance.
- What evidence would resolve it: Comparative metrics (Latency vs. Correctness) for a hybrid pipeline against the pure StatementGraphRAG and naive RAG baselines across the five evaluated datasets.

## Limitations
- Proposition extraction using LLMs is computationally expensive and may introduce errors
- Current system lacks adaptive routing for single-hop queries, leading to unnecessary complexity
- Performance on truly complex, real-world multi-hop QA datasets remains unproven

## Confidence
- Performance improvement claims (73.6% vs 66.1%, 52.4% vs 50.8%): **Medium** - based on synthetic data and limited real datasets
- StatementGraphRAG vs TopicGraphRAG differentiation: **Low** - insufficient empirical validation of intended use cases
- HLG's advantage over chunk-based RAG: **High** - well-supported by ablation studies

## Next Checks
1. Evaluate HLG on established multi-hop QA benchmarks (e.g., HotpotQA, ComplexWebQuestions) with real human-authored questions to verify synthetic data transfer
2. Conduct ablation studies isolating the contribution of each HLG tier to quantify the marginal benefit of the hierarchical structure
3. Measure computational overhead and retrieval latency of HLG construction and querying compared to baseline RAG systems to assess practical feasibility