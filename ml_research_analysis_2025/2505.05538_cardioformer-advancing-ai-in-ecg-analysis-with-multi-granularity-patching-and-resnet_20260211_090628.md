---
ver: rpa2
title: 'Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching
  and ResNet'
arxiv_id: '2505.05538'
source_url: https://arxiv.org/abs/2505.05538
tags:
- cardioformer
- ptb-xl
- these
- mimic-iv
- multi-granularity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cardioformer, a novel multi-granularity hybrid
  transformer model designed for ECG classification that integrates cross-channel
  patching, hierarchical residual learning, and a two-stage self-attention mechanism.
  The model addresses limitations in capturing both local morphological details and
  long-range temporal dependencies in multi-channel ECG data by encoding multi-scale
  token embeddings and selectively fusing representations through intra- and inter-granularity
  self-attention.
---

# Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet

## Quick Facts
- **arXiv ID:** 2505.05538
- **Source URL:** https://arxiv.org/abs/2505.05538
- **Reference count:** 30
- **Primary result:** Achieved 96.34±0.11 AUROC on PTB, 89.99±0.12 on PTB-XL, and 95.59±1.66 on MIMIC-IV under subject-independent settings.

## Executive Summary
This paper introduces Cardioformer, a novel multi-granularity hybrid transformer model designed for ECG classification. The model integrates cross-channel patching, hierarchical residual learning, and a two-stage self-attention mechanism to capture both local morphological details and long-range temporal dependencies in multi-channel ECG data. Extensive evaluations on three benchmark ECG datasets demonstrate superior performance compared to four state-of-the-art baselines, with the model showing strong cross-dataset generalization capabilities.

## Method Summary
Cardioformer employs cross-channel multi-granularity patching that divides ECG signals into non-overlapping patches of varying lengths spanning all channels simultaneously. These patches are processed through three ResNet blocks to create D-dimensional embeddings. The model uses a two-stage self-attention mechanism: intra-granularity attention refines features within each scale, while inter-granularity attention enables cross-scale information exchange through router tokens. The architecture is evaluated on PTB, PTB-XL, and MIMIC-IV datasets under subject-independent settings.

## Key Results
- Achieved 96.34±0.11 AUROC on PTB dataset, outperforming baselines by 2.23-11.71 points
- Reached 89.99±0.12 AUROC on PTB-XL, exceeding baselines by 3.17-14.76 points
- Demonstrated strong cross-dataset generalization with 49.18% AUROC on PTB when trained solely on MIMIC-IV

## Why This Works (Mechanism)

### Mechanism 1: Cross-Channel Multi-Granularity Patching
Multi-scale patch embeddings with cross-channel integration capture both local ECG morphological details and long-range temporal dependencies simultaneously. Short patches (e.g., length 2) capture fine-grained features like QRS morphology; long patches (e.g., length 32) capture broader patterns like RR intervals and ST segments. The approach assumes ECG diagnostic information exists at multiple temporal scales and inter-channel relationships carry diagnostic signal that single-scale approaches miss.

### Mechanism 2: Two-Stage Self-Attention with Router Tokens
Splitting self-attention into intra-granularity and inter-granularity stages enables cross-scale feature fusion while reducing quadratic complexity. Each granularity level has dedicated attention where patches attend to concatenated [patches + router token], refining features within that scale. Router tokens from all granularities then attend to each other, enabling cross-scale information exchange. Complexity reduces from O((ΣNᵢ)²) to O(nD²).

### Mechanism 3: Residual Network Blocks for Patch Embedding Enhancement
Stacked residual blocks with 1×1 convolutions enhance patch embeddings beyond simple linear projection while maintaining gradient flow. Instead of flattening patches and using linear projection, three ResNet blocks directly map patches to D-dimensional representations. Each block computes: y = F(x) + x where F(x) = BN(Conv1×1(σ(BN(Conv1×1(x))))). Final LayerNorm ensures training stability.

## Foundational Learning

- **Concept: Self-Attention with Q, K, V Formulation**
  - **Why needed here**: Cardioformer's entire feature fusion depends on two-stage self-attention; understanding how attention computes weighted relationships between tokens is essential for debugging attention patterns.
  - **Quick check question**: Given query Q, key K, and value V matrices, write the scaled dot-product attention formula. What does the softmax output represent?

- **Concept: Residual Learning and Skip Connections**
  - **Why needed here**: The ResNet blocks and skip connections enable deeper feature extraction; understanding gradient flow through residual connections helps diagnose training collapse.
  - **Quick check question**: Why does y = F(x) + x mitigate vanishing gradients during backpropagation compared to y = F(x)?

- **Concept: Patch-Based Tokenization for Time Series**
  - **Why needed here**: Cardioformer segments ECG signals into patches at multiple scales; understanding why patching helps transformers process continuous signals is foundational.
  - **Quick check question**: Given a 250-timestamp ECG signal with patch length 32 and no overlap, how many patches result? What boundary information might be lost?

## Architecture Onboarding

- **Component map**:
Input (T×C ECG, e.g., 250×12) → [Cross-Channel Multi-Granularity Patching] → [ResNet Blocks ×3] → [Positional + Granularity Embeddings + Router Tokens] → [Encoder Layer ×6:] → [Concatenate granularity representations] → [Classification Head]

- **Critical path**:
1. Patch length configuration: The 16-element patch list directly determines token count and memory—verify GPU memory before scaling
2. Router token initialization: These aggregate cross-granularity information; poor initialization breaks inter-scale learning
3. Augmentation timing: Applied during embedding; aggressive jitter/dropout on fine patches may corrupt diagnostic signal

- **Design tradeoffs**:
- More granularities = better multi-scale coverage but higher memory/compute
- Cross-channel patches capture inter-lead relationships but increase sequence length vs. channel-independent alternatives
- ResNet blocks add representational capacity but more parameters than linear projection

- **Failure signatures**:
- Router token collapse: All routers converge to similar representations → inter-granularity attention provides no discriminative signal. Check router variance across granularities
- Fine-granularity gradient starvation: Short patches receive weak gradients if attention favors coarse representations. Monitor per-granularity gradient norms
- Cross-dataset transfer failure: Paper reports 49.18% AUROC on PTB when trained on MIMIC-IV—expect significant domain shift without adaptation
- Augmentation over-corruption: Excessive jitter on length-2 patches destroys morphological detail

- **First 3 experiments**:
1. Single-granularity baseline: Run with patch list {8} only. Compare AUROC against full {2,4,8,16,32} configuration to quantify multi-granularity contribution
2. ResNet vs. linear ablation: Replace ResNet blocks with single linear projection. Measure performance gap and parameter reduction
3. Router token analysis: Visualize router embeddings (t-SNE/PCA) colored by granularity and class. If routers cluster by granularity but not class, inter-granularity attention may not fuse task-relevant information

## Open Questions the Paper Calls Out
- How can adaptive strategies be developed to automatically select optimal patch lengths, mitigating the hyperparameter sensitivity introduced by Cardioformer's variable patching?
- Can selective channel attention mechanisms be integrated to better account for the varying diagnostic importance of specific ECG leads?
- Would replacing or augmenting the ResNet blocks with selective state space models (SSMs) enhance the modeling of long-range temporal dependencies?
- What architectural modifications are necessary to improve cross-dataset generalization where the model currently fails to transfer (e.g., ~49% AUROC)?

## Limitations
- Multi-granularity patching design assumes diagnostic information exists at multiple temporal scales, but no ablation studies isolate the contribution of multi-scale versus single-scale processing
- Router token mechanism is theoretically sound but lacks empirical validation of its effectiveness at summarizing granularity-specific information
- Cross-dataset generalization results (49.18% PTB from MIMIC-IV training) indicate significant domain shift that the paper does not address with adaptation techniques

## Confidence
- **High confidence**: Multi-granularity patching as a general architectural approach (supported by Medformer precedent and reasonable signal processing principles)
- **Medium confidence**: Two-stage attention mechanism (architecturally valid but router token effectiveness unproven without ablation)
- **Low confidence**: Specific claims about residual network blocks enhancing patch embeddings (no direct comparison to linear projection)

## Next Checks
1. **Ablation study**: Train single-granularity baseline (patch length 8 only) against full multi-granularity configuration to quantify contribution of scale diversity
2. **Router token analysis**: Visualize router embeddings (t-SNE/PCA) colored by granularity and class to verify routers capture meaningful cross-scale information rather than collapsing to similar representations
3. **ResNet versus linear projection**: Replace residual blocks with simple linear projection and measure performance gap to determine if convolutional processing within patches provides measurable benefit