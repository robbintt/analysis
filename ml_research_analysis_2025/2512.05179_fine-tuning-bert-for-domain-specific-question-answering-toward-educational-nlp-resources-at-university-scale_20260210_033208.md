---
ver: rpa2
title: 'Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational
  NLP Resources at University Scale'
arxiv_id: '2512.05179'
source_url: https://arxiv.org/abs/2512.05179
tags:
- squad
- university
- dataset
- bert
- chatbot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of domain-specific question answering
  models for university course information by fine-tuning BERT-based architectures
  on a custom dataset of 1,203 question-answer pairs derived from the University of
  Limerick's Book of Modules. Using Hugging Face Transformers, several BERT variants
  including BERT large, BioBERT, SciBERT, RoBERTa, and DistilBERT were evaluated on
  Exact Match and F1 metrics.
---

# Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale

## Quick Facts
- **arXiv ID:** 2512.05179
- **Source URL:** https://arxiv.org/abs/2512.05179
- **Reference count:** 22
- **Primary result:** BERT-large-uncased-whole-word-masking-finetuned-squad achieved EM 60.99 and F1 82.4 on university module QA

## Executive Summary
This study demonstrates the feasibility of adapting foundation models for domain-specific question answering in higher education contexts. Using the University of Limerick's Book of Modules as source material, the research fine-tunes several BERT variants on a custom dataset of 1,203 question-answer pairs from the Electronic & Computer Engineering department. The results show that larger models with SQuAD pretraining significantly outperform smaller or non-SQuAD variants, achieving up to 60.99% Exact Match and 82.4% F1 scores. The work establishes a foundation for developing university-scale educational NLP resources and highlights the importance of domain-specific fine-tuning for practical applications.

## Method Summary
The study employed Hugging Face Transformers to fine-tune BERT-based architectures on a custom SQuAD v1.0 formatted dataset of 1,203 QA pairs derived from university course information. Multiple BERT variants were evaluated including BERT-large-uncased-whole-word-masking-squad, BioBERT-large-cased-v1.1-squad, RoBERTa-base-squad2, and others. Models were trained using Adam optimizer with cross-entropy loss for span prediction, with inputs tokenized via AutoTokenizer including input_ids, attention_mask, and start/end positions. Performance was measured using Exact Match and F1 metrics, with 20% of data held out for validation. The fine-tuning process was conducted on Google Colab with GPU acceleration.

## Key Results
- BERT-large-uncased-whole-word-masking-finetuned-squad achieved the highest performance with Exact Match of 60.99 and F1 of 82.4
- Models pre-trained on SQuAD consistently outperformed those without SQuAD pretraining
- Larger model architectures demonstrated superior performance compared to smaller variants
- Validation loss diverged from training loss after approximately 4 epochs, indicating overfitting with the current dataset size

## Why This Works (Mechanism)
The success of this approach stems from leveraging transfer learning capabilities of transformer architectures. BERT models pretrained on large-scale general corpora (like SQuAD) have learned robust contextual representations that can be fine-tuned for specific domains. The SQuAD pretraining provides strong span prediction capabilities essential for extractive QA tasks. By fine-tuning on domain-specific university module data, the models adapt these general representations to understand educational terminology and question-answer patterns unique to course information. The extractive QA format aligns naturally with the structured nature of module descriptions, where answers are directly extractable spans from the context text.

## Foundational Learning
**SQuAD v1.0 Format** - Question answering datasets with context, question, and extractive answer spans with exact character positions. *Why needed:* Standard format enables direct use of pretrained QA models and evaluation scripts. *Quick check:* Verify context[answer_start:answer_start+len(answer)] == answer for each entry.
**Span Prediction Loss** - Cross-entropy loss computed on start and end token positions rather than full sequence generation. *Why needed:* Enables efficient training for extractive QA where answers are substrings of context. *Quick check:* Ensure loss decreases steadily for first 2-3 epochs before potential overfitting.
**Transformer Attention Masks** - Binary masks indicating which tokens are actual content vs padding. *Why needed:* Prevents model from attending to padding tokens during self-attention computations. *Quick check:* Confirm attention_mask has same length as input_ids with 1s for real tokens and 0s for padding.

## Architecture Onboarding

**Component Map:** Dataset (SQuAD format) -> Tokenizer -> Model (BERT variant) -> Trainer (Adam optimizer, cross-entropy loss) -> Evaluator (EM/F1 metrics)

**Critical Path:** Data preparation → Tokenizer configuration → Model loading → Fine-tuning → Evaluation

**Design Tradeoffs:** Larger models (BERT-large) provide better performance but require more computational resources and are more prone to overfitting with limited data. SQuAD pretraining significantly improves results but limits to models with available QA fine-tuning. Using LLM-generated questions supplements dataset size but may introduce quality variability.

**Failure Signatures:** Training loss continues decreasing while validation loss increases after ~4 epochs (overfitting). Answer_start positions don't align with actual answer text in context (data quality issue). Tokenizer truncation causes loss of critical answer context (input length management problem).

**First Experiments:**
1. Fine-tune bert-large-uncased-whole-word-masking-finetuned-squad on a small subset (100 samples) to verify basic functionality
2. Compare training vs validation loss curves to identify optimal early stopping point
3. Evaluate model predictions on held-out validation set to confirm EM/F1 score calculations

## Open Questions the Paper Calls Out
**Open Question 1:** Does expanding the dataset to 100,000 entries resolve the overfitting observed at four epochs and significantly improve Exact Match scores? The current study was limited to 1,203 pairs, which caused validation loss to diverge from training loss after roughly four epochs. Training the same BERT variants on the proposed 100k dataset and demonstrating stable validation loss curves and higher EM scores without divergence would resolve this.

**Open Question 2:** Can a domain-specific model pre-trained on a large-scale university dataset be effectively adapted by individual institutions using only small local datasets? The current study only evaluated fine-tuning on a single department's data; it did not test the transferability of a pre-trained "university" model to a new institutional context. A transfer learning experiment where the model is pre-trained on the general 100k university dataset and then fine-tuned on a distinct, smaller dataset from a different university would provide evidence.

**Open Question 3:** Does the inclusion of synthetically generated question-answer pairs introduce distributional biases that limit model performance compared to purely human-authored data? The methods section notes the dataset was "supplemented with manually and synthetically generated entries," but the study does not isolate the performance contribution or potential noise introduced by the synthetic subset. An ablation study comparing model performance when trained exclusively on manual entries versus the mixed synthetic/manual dataset would resolve this.

## Limitations
- Dataset size of 1,203 QA pairs is relatively small for training large transformer models
- Dataset is not publicly released, preventing independent validation and reproduction
- Reliance on LLM assistance for question generation introduces potential quality variability without clear documentation
- Exclusive focus on one university department limits generalizability across academic domains

## Confidence

**High Confidence:** The experimental methodology using standard SQuAD-style evaluation metrics (EM and F1) is sound and reproducible. The observation that SQuAD-pretrained models outperform non-SQuAD variants is well-established in the literature and aligns with existing knowledge about transfer learning effectiveness.

**Medium Confidence:** Claims about model performance differences between variants (BERT large vs. BioBERT vs. RoBERTa) are reasonable given the training setup, though exact performance comparisons are difficult without standardized hyperparameters and public datasets. The finding that larger models perform better follows expected patterns in NLP.

**Low Confidence:** The assertion that this represents "the first domain-specific QA model for universities" cannot be independently verified without comprehensive literature review access. The claim about dataset scaling improving performance is plausible but untested in this work.

## Next Checks
1. Recreate the dataset using another university's module catalog following the described SQuAD v1.0 format, ensuring answer_start positions align precisely with answer text in the context.
2. Fine-tune a BERT-large-uncased-whole-word-masking model with standardized hyperparameters (learning rate 3e-5, batch size 12, 2-3 epochs) and verify that validation EM and F1 scores fall within the reported ranges.
3. Implement the early-stopping criterion based on validation loss divergence (approximately 4 epochs) and confirm that training loss continues decreasing while validation loss plateaus or increases, indicating overfitting.