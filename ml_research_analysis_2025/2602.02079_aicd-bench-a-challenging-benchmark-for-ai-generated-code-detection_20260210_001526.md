---
ver: rpa2
title: 'AICD Bench: A Challenging Benchmark for AI-Generated Code Detection'
arxiv_id: '2602.02079'
source_url: https://arxiv.org/abs/2602.02079
tags:
- code
- task
- self
- performance
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AICD Bench is a large-scale, multi-task benchmark for AI-generated\
  \ code detection spanning 2M samples, 77 models across 11 families, and 9 programming\
  \ languages. It introduces three tasks: robust binary classification under distribution\
  \ shifts, model family attribution, and fine-grained human\u2013machine classification."
---

# AICD Bench: A Challenging Benchmark for AI-Generated Code Detection

## Quick Facts
- **arXiv ID**: 2602.02079
- **Source URL**: https://arxiv.org/abs/2602.02079
- **Reference count**: 37
- **Key outcome**: A large-scale, multi-task benchmark for AI-generated code detection spanning 2M samples, 77 models across 11 families, and 9 programming languages, revealing severe generalization failures under distribution shifts.

## Executive Summary
AICD Bench is a large-scale, multi-task benchmark designed to evaluate AI-generated code detection under realistic, out-of-distribution conditions. It introduces three challenging tasks: robust binary classification under language and domain shifts, model family attribution, and fine-grained human–machine classification. Evaluation shows that current detectors, including state-of-the-art neural models, generalize poorly, especially under domain shifts and for hybrid/adversarial code, with top models achieving F1-scores well below practical usability. This benchmark highlights the need for more robust detection methods and provides a standardized evaluation framework to drive future research.

## Method Summary
AICD Bench comprises 2 million code samples from 77 generators across 11 model families and 9 programming languages, organized into three tasks with distinct evaluation splits. Task 1 focuses on robust binary classification under language and domain distribution shifts; Task 2 on model family attribution; and Task 3 on fine-grained human–machine classification. The benchmark employs classical models (SVM, Logistic Regression, CatBoost) with TF-IDF and AST features, as well as neural encoders (CodeBERT, ModernBERT, etc.), trained for 3 epochs with 512-token windows. Evaluation uses Macro-F1 to prioritize minority-class performance.

## Key Results
- TF-IDF-based classical models outperform deep neural models for out-of-distribution binary classification under severe domain shifts.
- Domain shift causes more severe generalization failure than programming language shift for code authorship detection.
- Model family attribution fails on out-of-distribution generators due to high intra-family variability exceeding cross-family separability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical stylistic features (TF-IDF n-grams) can outperform deep representations for out-of-distribution binary classification under severe domain shift.
- Mechanism: TF-IDF captures language-independent stylistic regularities—such as identifier verbosity, naming patterns (e.g., `answer`, `output` vs. `li`, `nums`, `cur`)—that generalize across programming languages better than structural features or overfitted neural representations.
- Core assumption: The stylistic differences between human and AI code are consistent enough across domains that surface-level token patterns remain discriminative even when domain-specific structure changes.
- Evidence anchors:
  - [Section 5.2.1]: "SVM and Logistic Regression with TF-IDF features outperform all deep learning models" on Task 1; "language-independent cues, such as variable naming patterns, play a central role."
  - [Section D.1]: Analysis shows AI code uses verbose identifiers (answer, output, result) while human code uses shorter, organic identifiers (li, nums, pos), explaining SVM generalization.
  - [corpus]: Weak/no direct corpus support for this specific mechanism in provided neighbors.
- Break condition: If adversarial or hybrid code deliberately mimics human naming conventions, or if code is minified/obfuscated, this stylistic signal degrades.

### Mechanism 2
- Claim: Domain shift causes more severe generalization failure than programming language shift for code authorship detection.
- Mechanism: Coding conventions, comment density, and structural patterns differ more across domains (e.g., algorithmic problems vs. research code) than across languages, so models overfit to domain-specific templates and fail when those templates change.
- Core assumption: Domain-specific stylistic patterns are learned as proxy labels for authorship during training, and these patterns don't transfer to new domains.
- Evidence anchors:
  - [Section 5.2.1, Table 5]: "Seen domain, seen language" yields F1=0.63; "Unseen domain, seen language" drops to 0.20; "Unseen domain, unseen language" is 0.21—nearly identical, showing domain shift dominates.
  - [Section D]: "The domain shift has a greater impact than the language shift... performance degrades markedly on unseen domains."
  - [corpus]: No direct corpus evidence; related work on multi-domain AI text detection (M-DAIGT) supports domain sensitivity but not this specific code finding.
- Break condition: If training data includes multiple diverse domains with balanced representation, or if domain-adaptive training is applied, this gap may narrow.

### Mechanism 3
- Claim: Model family attribution fails on out-of-distribution generators because intra-family variability exceeds cross-family separability.
- Mechanism: Models within the same family (e.g., different sizes, versions) produce sufficiently diverse outputs that learned family signatures don't transfer to unseen generators, even from known families.
- Core assumption: Model families, despite shared architecture and training philosophy, have high output diversity due to size differences, fine-tuning, or quantization.
- Evidence anchors:
  - [Section 5.2.2, Table 6]: In-domain generator F1=0.149; out-of-domain generator F1=0.046; standard deviation exceeds mean, indicating high variability and instability.
  - [Section 7, Figure 6]: Classical models achieve near-zero macro F1 on out-of-domain generators; confusion matrices show bias toward single class predictions.
  - [corpus]: No direct corpus evidence; GDPR-Bench-Android notes similar generalization challenges in compliance detection but for different reasons.
- Break condition: If training includes explicit diversity of generators within families, or if family-level invariants are learned through contrastive objectives, performance may improve.

## Foundational Learning

- Concept: **Distribution shift (OOD generalization)**
  - Why needed here: The benchmark's core premise is that in-distribution evaluation overestimates real-world performance; understanding how covariate shift affects learned representations is essential to interpret all results.
  - Quick check question: If a detector trained on Python algorithmic code is tested on Rust systems code, is this a language shift, domain shift, or both? (Answer: Both, but the paper shows domain shift is the dominant factor.)

- Concept: **Macro-F1 for imbalanced multi-class evaluation**
  - Why needed here: Tasks 2 and 3 have imbalanced classes (12 families, 4 fine-grained categories); macro-F1 weights all classes equally, preventing majority-class dominance from hiding poor minority-class performance.
  - Quick check question: If a model achieves 90% accuracy on Task 3 by always predicting the majority class (AI-generated), what would its macro-F1 approximately be? (Answer: ~0.25 if classes are roughly balanced, since minority-class F1 would be near 0.)

- Concept: **TF-IDF vs. AST-based features for code representation**
  - Why needed here: The paper shows TF-IDF outperforms AST features for Task 1 under shift; understanding why syntactic structure is less robust than stylistic token patterns is key to designing future detectors.
  - Quick check question: Why might AST depth be a poor feature for distinguishing human vs. AI code across domains? (Answer: AST depth reflects complexity, not authorship style; both humans and AI can write deeply nested or flat code depending on the task.)

## Architecture Onboarding

- Component map: AICD Bench has three task-specific data subsets (Task 1: 1.6M train/val, 1M test; Task 2: 600K train/val, 500K test; Task 3: 1.1M train/val, 1M test), each with progressively more languages and evaluation splits. Baseline models include classical (SVM, LR, CatBoost with TF-IDF/AST features) and neural (CodeBERT, ModernBERT, DeBERTa, etc.) encoders trained for 3 epochs, batch size 64, 512-token window.

- Critical path: Start with Task 1 (binary classification) using TF-IDF SVM to establish a lower-bound baseline; then train ModernBERT (best neural performer on Tasks 2/3) to understand neural representation limits. Always evaluate on all four Task 1 splits (seen/unseen language × seen/unseen domain) to diagnose shift sensitivity.

- Design tradeoffs: Classical models generalize better under severe shift but have lower capacity for fine-grained distinctions; neural models capture richer representations but overfit to training distribution. AST features add structural information but increase sparsity and hurt robustness under shift (Section 5.2.1).

- Failure signatures: (1) Random/majority baselines outperform trained models → severe train-test distribution mismatch (Task 1, Table 4). (2) Model predicts single class almost exclusively → class imbalance + lack of generalization (Task 2 classical models). (3) Adversarial/hybrid samples misclassified as either human or AI → deliberate obfuscation success (Task 3, Figure 2).

- First 3 experiments:
  1. Reproduce Task 1 SVM + TF-IDF baseline; analyze top n-gram features via coefficient inspection to validate stylistic signal hypothesis.
  2. Train ModernBERT on Task 2 with held-out generator families; measure intra-family vs. cross-family confusion to quantify attribution limits.
  3. Augment Task 3 training with domain-mixed sampling (e.g., interleave algorithmic and research code); test if balanced domain exposure reduces generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial or domain-adaptive training strategies substantially improve detection robustness under distribution shift for AI-generated code?
- Basis in paper: [explicit] Authors state they "plan to investigate adversarial and domain-adaptive training strategies aimed at improving robustness under distribution shift, such as adversarial data augmentation, domain-invariant representation learning, and curriculum-based adaptation across programming languages and domains."
- Why unresolved: Current detectors perform far below practical usability (F1 ~30% on Task 1), with domain shift causing more degradation than language shift, yet no systematic study has evaluated whether specific training strategies can mitigate this.
- What evidence would resolve it: Training detectors with proposed strategies and measuring improvement on AICD Bench's out-of-distribution splits, particularly on unseen domains.

### Open Question 2
- Question: Can meta-models that learn shared detection priors or dynamically combine task-specific detectors achieve better cross-family generalization than single models?
- Basis in paper: [explicit] Authors "intend to develop meta-models that explicitly promote generalization across languages, domains, and generator families, for example by learning shared detection priors or dynamically combining task-specific detectors."
- Why unresolved: Model Family Attribution (Task 2) shows high intra-family variability and poor transferability—classical models achieve near-zero macro F1 on out-of-domain generators, while even the best neural model (ModernBERT) achieves only 14.9% on unseen generators.
- What evidence would resolve it: Demonstrating that a meta-model architecture can consistently outperform single-model baselines across all three AICD Bench tasks, especially on unseen generator families.

### Open Question 3
- Question: What linguistic or structural properties explain why domain shift causes greater performance degradation than programming language shift in code detection?
- Basis in paper: [inferred] Table 5 and analysis show performance drops from 0.63 (seen domain/language) to 0.20 (unseen domain/seen language), while unseen language/seen domain only drops to 0.42. The paper notes domain conventions (e.g., research code with extensive comments vs. concise algorithmic solutions) differ substantially but does not isolate which specific features cause this asymmetry.
- Why unresolved: Understanding this asymmetry could inform detector design priorities, but the paper only hypothesizes about style/convention differences without systematic feature ablation.
- What evidence would resolve it: Controlled experiments isolating domain-specific features (comment density, code organization patterns, naming conventions) to quantify their individual contributions to detection performance.

## Limitations
- The benchmark's extreme domain shift evaluation may not reflect milder real-world adaptation scenarios.
- Classical models' OOD advantage could be an artifact of simplicity rather than fundamental superiority.
- Benchmark lacks human-written code from non-algorithmic domains, limiting ecological validity.

## Confidence
- **High**: Task 1 TF-IDF SVM outperforming neural models under domain shift (directly supported by Table 5 and n-gram analysis in Section D.1).
- **Medium**: Domain shift dominates language shift for generalization failure (supported by Table 5 but not explicitly tested against alternative explanations like model capacity).
- **Medium**: Model family attribution is fundamentally hard due to intra-family variability (supported by Table 6, but could reflect dataset imbalance or lack of diverse training samples).

## Next Checks
1. **Cross-benchmark validation**: Evaluate AICD-trained detectors on other code authorship benchmarks (e.g., CodeT5's original datasets) to test whether poor generalization is specific to AICD's domain shift or a broader issue.
2. **Adversarial robustness probe**: Systematically generate hybrid/adversarial samples where AI code is rewritten to mimic human naming conventions; measure whether TF-IDF SVM's advantage erodes.
3. **Domain-adaptive training**: Retrain Task 1 detectors with domain-balanced sampling or domain-adversarial training; compare whether this reduces the generalization gap without sacrificing in-domain performance.