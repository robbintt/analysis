---
ver: rpa2
title: Using reinforcement learning to probe the role of feedback in skill acquisition
arxiv_id: '2512.08463'
source_url: https://arxiv.org/abs/2512.08463
tags:
- flow
- feedback
- control
- learning
- drag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work uses reinforcement learning to study how feedback shapes
  the learning of open-loop skills in fluid dynamics. A DreamerV3 agent is interfaced
  with a physical tabletop water channel to control a spinning cylinder and either
  minimize or maximize drag.
---

# Using reinforcement learning to probe the role of feedback in skill acquisition

## Quick Facts
- arXiv ID: 2512.08463
- Source URL: https://arxiv.org/abs/2512.08463
- Reference count: 36
- Key result: Dense flow feedback accelerates RL learning in fluid dynamics, but learned policies can execute in open-loop with nearly identical performance.

## Executive Summary
This work investigates how feedback shapes skill acquisition in fluid dynamics control using reinforcement learning. A DreamerV3 agent learns to control a spinning cylinder in a water channel to minimize or maximize drag, with experiments comparing learning with and without dense flow feedback. The study reveals that while feedback is critical for learning (especially in drag maximization due to non-minimum-phase dynamics), the learned policies can be executed in open-loop with nearly identical performance, suggesting feedback is more important for learning than execution.

## Method Summary
The method uses a tabletop water channel with a spinning cylinder controlled by a Maxon EC45 motor. A DreamerV3 agent with 25M parameters receives observations including drag measurements, motor feedback, commanded actions, and optionally dense 16×16 flow field estimates from PIV at 60 Hz. The agent controls cylinder rotation at 30 Hz with continuous actions mapped to [-15.7, 15.7] rad/s. Episodes last 60 seconds with 60-second stabilization between episodes. Training uses default DreamerV3 hyperparameters for 60 minutes (60 episodes, 100,800 steps) with 10 repetitions per configuration.

## Key Results
- With dense flow feedback, the agent rapidly discovers high-performance drag-control strategies within minutes for both minimization and maximization tasks
- Without flow feedback, drag minimization succeeds but drag maximization fails due to non-minimum-phase dynamics where early transients anti-align with long-term performance
- Learned policies achieve nearly identical performance when executed in open-loop without feedback, indicating feedback is more critical for learning than execution

## Why This Works (Mechanism)

### Mechanism 1: Dense Flow Feedback Enables Efficient World Model Construction
- High-dimensional flow feedback accelerates learning by providing control-relevant state information that reduces exploration uncertainty
- DreamerV3's world model benefits from rich exteroceptive signals that disambiguate system dynamics, allowing more accurate model updates per interaction
- Core assumption: The agent's world model benefits from higher-dimensional observations along control-relevant directions

### Mechanism 2: Open-Loop Policies Emerge When Dynamics Are Learnable But Execution-Feedback Gap Exists
- Learned policies can achieve near-identical performance when replayed without feedback because the agent internalizes feedforward structure during training
- During training with feedback, the agent discovers action trajectories that produce high reward, forming robust open-loop "motor programs"
- Core assumption: The physical system exhibits sufficient repeatability that open-loop replay produces consistent outcomes

### Mechanism 3: Non-Minimum-Phase Dynamics Create Asymmetric Learning Difficulty
- Drag maximization fails without flow feedback because early transient responses anti-correlate with long-term performance, creating deceptive reward signals
- In drag maximization, impulsive cylinder rotation initially decreases drag before increasing it, making credit assignment impossible without flow feedback
- Core assumption: The anti-alignment between early and long-run performance is the primary cause of learning failure

## Foundational Learning

- **Concept: Model-Based Reinforcement Learning (DreamerV3)**
  - Why needed here: The agent must learn from limited real-world interactions (~60 minutes). DreamerV3 builds a latent world model enabling planning and sample-efficient learning.
  - Quick check question: Can you explain how a world model enables an agent to "imagine" trajectories before taking actions?

- **Concept: Partial Observability and State Aliasing**
  - Why needed here: Without flow feedback, the agent receives only drag measurements and motor signals. This may create aliased states where distinct physical configurations appear identical, hindering policy learning.
  - Quick check question: What is the difference between a Markov state and a partial observation, and why does the distinction matter for RL?

- **Concept: Non-Minimum-Phase Systems**
  - Why needed here: Understanding why systems can have inverse responses (initial output opposite to final steady-state) explains the asymmetric learning difficulty between drag minimization and maximization.
  - Quick check question: If you command a step input and the output initially moves opposite to its final direction, what control challenge does this create?

## Architecture Onboarding

- **Component map:** Tabletop water channel -> PIV system (60 Hz) -> GPU-accelerated optical flow algorithm -> 16×16 flow field estimate -> DreamerV3 agent (25M params) -> normalized action [-1, 1] -> rotation rate command -> spinning cylinder -> torque sensor -> drag measurement

- **Critical path:** PIV image capture (60 Hz) → flow estimation (~17 ms budget at 60 Hz) → Agent observation assembly: drag (1 kHz sampled, 1 s smoothed), motor feedback, commanded action, optional flow field → Policy inference (30 Hz control frequency) → Episode: 60 s duration, 60 s stabilization between episodes

- **Design tradeoffs:** Flow feedback resolution: 16×16 chosen as sweet spot; lower degrades accuracy, higher increases compute. Episode length: 60 s balances statistical averaging of vortex shedding with training throughput. No simulation: Authors skip simulation entirely due to sim-to-real gap at Re ≈ 5,000 and chaotic wake dynamics.

- **Failure signatures:** Drag maximization without flow feedback: Agent shows no systematic improvement over 60 minutes. State aliasing (time signal omitted): Learning degradation even in minimization task. Flow estimation pipeline failure: If latency exceeds 33 ms, control loop timing violated.

- **First 3 experiments:** 
  1. **Baseline validation:** Run the identified sinusoidal open-loop policies (A=15.7 rad/s, f=0.776 Hz for max; f=3.0 Hz for min) for 10 episodes each to establish performance benchmarks (~28% increase, ~32% decrease vs no-control baseline).
  2. **Ablation on observations:** Train with flow feedback vs without, using identical hyperparameters. Verify the asymmetric learning failure reproduces (10 repetitions recommended).
  3. **Open-loop replay:** Take trajectories from converged policies with flow feedback. Replay them 5× each with no observations to confirm performance retention. Compare online vs replay gap between tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can simplified theoretical models be derived to formally characterize when rich feedback is required for learning a skill but not for executing it?
- Basis in paper: The "Outlook" section states, "an important next step is to analyze simplified models that clarify when rich feedback is needed for learning but not for execution, and to derive principled rules for choosing observations."
- Why unresolved: The current work is empirical, relying on a specific chaotic fluid system and a black-box RL agent (DreamerV3) without a formal mathematical framework explaining the phenomenon.
- What evidence would resolve it: A theoretical derivation identifying system properties (e.g., non-minimum-phase characteristics) that predict the discrepancy between learning and execution information requirements.

### Open Question 2
- Question: How can reinforcement learning agents be designed to dynamically decide when to utilize rich sensory feedback versus ignoring it (the "use-when-useful" regime)?
- Basis in paper: The Discussion notes that the regime where agents "learn when rich feedback is worth using... remains largely unexplored," contrasting with current methods that have fixed sensor sets.
- Why unresolved: The paper demonstrates that feedback helps learning but is unnecessary for execution; however, it does not propose an architecture that adaptively switches between these modes.
- What evidence would resolve it: An agent architecture (e.g., using options or internal uncertainty estimates) that autonomously modulates its reliance on high-dimensional flow feedback during training and deployment.

### Open Question 3
- Question: Do the findings regarding "kind" versus "wicked" learning conditions generalize to high-dimensional robotic systems, such as humanoids?
- Basis in paper: The "Outlook" section identifies "high-dimensional robots such as humanoids" as a "natural longer-term target" to test if the phenomena persist beyond the fluid dynamics setup.
- Why unresolved: The experiments are limited to a single spinning cylinder setup; it is unknown if the asymmetry in learning difficulty (based solely on the goal) scales to more complex kinematic chains.
- What evidence would resolve it: Experiments showing similar learning failures in torque maximization versus minimization tasks on a humanoid robot when feedback is ablated.

## Limitations
- The study focuses on a single physical system (water channel with spinning cylinder) at fixed Reynolds number (~5000), limiting generalizability to other fluid dynamics problems
- Reliance on DreamerV3 with default hyperparameters without ablation studies introduces uncertainty about whether observed effects are robust to implementation choices
- The assertion that open-loop execution matches closed-loop performance requires further validation across different environmental conditions and longer timescales

## Confidence

- **High Confidence**: The finding that dense flow feedback accelerates learning in both drag minimization and maximization tasks is well-supported by repeated experiments (10 runs) and clear performance differences
- **Medium Confidence**: The explanation for asymmetric learning difficulty in drag maximization (non-minimum-phase dynamics with early transients anti-aligning with long-term performance) is mechanistically plausible and supported by the empirical observation that flow feedback is essential for this task
- **Low Confidence**: The claim that flow feedback provides "control-relevant state information that reduces exploration uncertainty" lacks direct experimental validation

## Next Checks

1. **Transferability test**: Validate open-loop policies across different environmental conditions (e.g., varying water temperature, cylinder cleanliness, or flow rate) to assess robustness beyond the training conditions

2. **Partial feedback ablation**: Systematically vary the density and spatial coverage of flow feedback (e.g., 8×8 vs 16×16 grids, selective regions) to determine the minimum feedback requirements for successful learning in drag maximization

3. **Alternative agent architectures**: Replicate key experiments using different RL algorithms (e.g., SAC, PPO) to verify that the observed feedback-learning asymmetry is not specific to DreamerV3's world model approach