---
ver: rpa2
title: Training-free Truthfulness Detection via Value Vectors in LLMs
arxiv_id: '2509.17932'
source_url: https://arxiv.org/abs/2509.17932
tags:
- value
- vectors
- truthfulness
- these
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free method for detecting truthfulness
  in LLM outputs by leveraging value vectors in MLP modules. Unlike existing approaches
  that focus on attention mechanisms, this method exploits statistical patterns in
  the value vectors of MLPs.
---

# Training-free Truthfulness Detection via Value Vectors in LLMs

## Quick Facts
- arXiv ID: 2509.17932
- Source URL: https://arxiv.org/abs/2509.17932
- Authors: Runheng Liu; Heyan Huang; Xingchen Xiao; Zhijing Wu
- Reference count: 10
- Primary result: 70.33% average accuracy on NoVo benchmark

## Executive Summary
This paper introduces a training-free method for detecting truthfulness in LLM outputs by leveraging value vectors in MLP modules. Unlike existing approaches that focus on attention mechanisms, this method exploits statistical patterns in the value vectors of MLPs. The authors demonstrate that certain value vectors exhibit activation patterns—either increasing or decreasing—when processing truthful versus untruthful content. By treating each value vector as a predictor and aggregating their outputs, the method achieves an average accuracy of 70.33% on the NoVo benchmark, outperforming both NoVo (61.67%) and log-likelihood baselines (49.25%).

## Method Summary
The method extracts key activations from MLP layers using the formula $k_i = f(w_{gate,i}^T h) \cdot (w_{up,i}^T h)$ for each value vector $v_i$. For multiple-choice questions, it identifies value vectors that show extreme activation (argmax or argmin) when processing the correct answer compared to incorrect options. Using 30 labeled samples, the method selects the top-performing value vectors and aggregates their predictions via majority voting to determine the truthful answer.

## Key Results
- Achieves 70.33% average accuracy on NoVo benchmark (10 datasets)
- Outperforms NoVo (61.67%) and log-likelihood baselines (49.25%)
- Value vectors concentrate in middle-to-late layers, absent in early processing stages
- Both argmax and argmin patterns contribute to detection, with combined voting slightly better

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specific value vectors within MLP modules exhibit activation patterns that correlate with truthfulness, serving as weak individual predictors that can be ensembled.
- **Mechanism:** The method interprets the MLP as a key-value memory (Geva et al., 2021). It extracts the scalar "key" activation $k_i = f(w_{gate,i}^T h)(w_{up,i}^T h)$ for every value vector $v_i$. For a Multiple Choice Question (MCQ), it checks if the correct answer triggers the highest (argmax) or lowest (argmin) activation for that specific vector across all candidates.
- **Core assumption:** The paper assumes that truthfulness is encoded in a distributed manner across specific neurons (value vectors) such that the correct answer consistently modulates their activations relative to incorrect options, even if the absolute activation values overlap significantly between true and false classes.
- **Evidence anchors:**
  - [abstract]: "...certain value vectors within MLP modules exhibit truthfulness-related statistical patterns."
  - [section 4.1]: "...modulation can result in either the highest or the lowest activation across candidate answers... We refer to these two cases as exhibiting argmax and argmin activation patterns."
  - [corpus]: No direct support found in provided corpus; related work in corpus focuses on attention mechanisms or steering vectors rather than MLP value vector activation patterns.
- **Break condition:** The mechanism relies on relative rankings (argmax/min) rather than absolute thresholds. It fails if the "truthful" signal is weaker than dataset-specific heuristics (e.g., length bias) that might dominate the activation of selected vectors, or if the question prefix does not establish a consistent baseline range.

### Mechanism 2
- **Claim:** Aggregating predictions from a sparse subset of value vectors amplifies the signal-to-noise ratio for truthfulness detection.
- **Mechanism:** TruthV selects the top $p$ value vectors (based on accuracy on 30 labeled samples) and uses them to vote on the final answer. Each vector predicts the answer index corresponding to its extreme activation (max or min). The final prediction is the mode of these votes.
- **Core assumption:** It is assumed that while individual value vectors are noisy predictors (only slightly better than random), their errors are uncorrelated or diverse enough that majority voting isolates the shared "truthfulness" signal.
- **Evidence anchors:**
  - [abstract]: "By treating each value vector as a predictor and aggregating their outputs, the method achieves an average accuracy of 70.33%..."
  - [section 5.1]: "We aggregate the predictions $\{a_c\}$ via majority voting..."
  - [corpus]: "Small Vectors, Big Effects" (corpus) supports the general concept that small subsets of vectors can have outsized effects on model behavior, though focused on steering rather than detection.
- **Break condition:** If $p$ (selection percentage) is too low, the selected vectors may be overfitted to the 30-sample seed set. If $p$ is too high, the "truthfulness" signal is diluted by value vectors tracking irrelevant semantic features.

### Mechanism 3
- **Claim:** Truthfulness signals are concentrated in middle-to-late layers, implying that truthfulness is an abstract property emerging after early syntactic processing.
- **Mechanism:** The selection process filters for value vectors located in the middle and final layers of the Transformer. These layers integrate information rather than processing local token dependencies.
- **Core assumption:** The paper assumes a hierarchical representation where early layers handle surface features while deeper layers encode "content-level properties" like truthfulness.
- **Evidence anchors:**
  - [section 4.2]: "...these value vectors are predominantly located in the middle and final layers... we observe no such vectors in the early layers."
  - [section 4.2]: "...truthfulness-related modulation is largely absent from early processing stages and begins to emerge in the middle layers."
  - [corpus]: No direct validation for layer-wise MLP truthfulness distribution in provided corpus.
- **Break condition:** This dependency on mid-to-late layers implies the method may fail on very shallow models or architectures where factual recall is not localized to specific MLP depths.

## Foundational Learning

- **Concept: MLP as Key-Value Memory**
  - **Why needed here:** The method relies on decomposing the MLP layer into a sum of vector contributions ($m = \sum k_i v_i$). Understanding that $k_i$ (the activation key) determines "when" a value vector contributes is essential for interpreting why monitoring these keys reveals truthfulness.
  - **Quick check question:** In the equation $m = \sum k_i v_i$, does $k_i$ represent a learned weight parameter or an activation scalar dependent on the current input?

- **Concept: Relative Activation (Argmax/Argmin)**
  - **Why needed here:** The detection logic is comparative. It does not classify a single statement as true/false in isolation. It determines truthfulness by comparing the relative activation levels across multiple candidate continuations.
  - **Quick check question:** If a value vector has an "argmax" pattern, does it mean the truthful input has the highest absolute activation value, or the highest activation *relative to other candidates for that specific prompt*?

- **Concept: Ensemble Voting / Mode Aggregation**
  - **Why needed here:** The system achieves high accuracy (70.33%) not via a single "truth neuron," but by aggregating the noisy outputs of many value vectors.
  - **Quick check question:** If you have 5 value vectors predicting indices [2, 2, 3, 2, 4] for a question with 4 options, what is the final predicted answer?

## Architecture Onboarding

- **Component map:** Input Adapter -> Activation Hook -> Vector Selector -> Inference Engine -> Aggregator
- **Critical path:** Accessing the intermediate product $f(W_{gate}h) \odot (W_{up}h)$ *before* the down-projection ($W_{down}$) is applied. This requires precise hooking into the model's forward pass to extract the scalar key activations $k_i$ for every dimension $i$.
- **Design tradeoffs:**
  - **Training-free vs. Generalization:** While avoiding training allows scalability, the method requires a small labeled set (30 samples) for vector selection. The paper notes performance improves with more selection data, trading zero-shot purity for accuracy.
  - **Argmax vs. Argmin:** The method tracks both "excitatory" (argmax) and "inhibitory" (argmin) patterns. Ensembling both yields slightly better results but increases computational overhead.
- **Failure signatures:**
  - **Context Overlap:** If the distributions of key activations for truthful vs. untruthful content overlap heavily (Figure 4), relative ranking becomes unreliable.
  - **Task Specificity:** Generalization experiments (Table 3) show performance drops when applying vectors selected from Dataset A to Dataset B, suggesting vectors capture task-specific signals in addition to general truthfulness.
- **First 3 experiments:**
  1.  **Layer Profiling:** Run the selection protocol (30 samples) on a target model. Plot the layer-wise distribution of the top 1% most accurate value vectors to verify the "mid-to-late layer" concentration holds for your specific model architecture.
  2.  **Hyperparameter Sensitivity ($p$):** Sweep the selection percentage $p$ (e.g., 0.01% to 1%) on a validation set to find the optimal density of value vectors before performance degrades due to noise.
  3.  **Cross-Domain Transfer:** Train selectors on one dataset (e.g., Science) and test on another (e.g., Social Reasoning) to measure the ratio of general vs. task-specific truthfulness signals in your model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do GLU-based activation functions (used in Llama/Gemma) obscure the semantic interpretability of truthfulness-related value vectors compared to the GELU-based architectures of older models like GPT-2?
- **Basis in paper:** [inferred] Section 4.2 notes that unlike GPT-2, the selected vectors in modern LLMs show no surface-level semantic alignment. The authors hypothesize this might be due to differences in model architecture (GLU vs. GELU) or that truthfulness is encoded in a distributed, abstract manner.
- **Why unresolved:** The paper observes the lack of interpretability but does not isolate the architectural variable (activation function) to test if it is the direct cause of the semantic opacity.
- **What evidence would resolve it:** A comparative analysis of value vector projections in identical transformer models trained with GLU versus GELU activations to see if one yields interpretable token concepts for truthfulness.

### Open Question 2
- **Question:** Can a training-free truthfulness detection method based on value vectors be adapted for open-ended text generation tasks where multiple candidate answers are not explicitly provided?
- **Basis in paper:** [inferred] The method is evaluated exclusively on the NoVo benchmark (Section 5.1), which consists of multiple-choice question (MCQ) datasets. The methodology (Eq. 3) relies on comparing activations across a set of candidates $\{a_i\}_{i=1}^M$ to identify argmax/argmin patterns.
- **Why unresolved:** The detection logic fundamentally relies on a comparative analysis between a "truthful" option and distractors. It is undefined how a single output sequence would trigger a detectable pattern without a baseline comparison.
- **What evidence would resolve it:** An adaptation of TruthV that successfully detects hallucinations in free-form generation, perhaps by comparing the activation of the generated sequence against the activation of the prompt or a generic "untruthful" baseline.

### Open Question 3
- **Question:** How can truthfulness signals be disentangled from task-specific reasoning features to improve the cross-dataset generalization of value vector selection?
- **Basis in paper:** [inferred] Table 3 and Section 5.4 show that while value vectors transfer better than random guessing, cross-dataset performance is generally lower than in-domain performance (even with 30 samples). The authors conclude that these vectors encode a "mixture of task-general and task-specific information."
- **Why unresolved:** The paper demonstrates the existence of the mixture but does not propose a method to filter out the task-specific components that hinder generalization (e.g., filtering vectors that activate for "physics" rather than "truth").
- **What evidence would resolve it:** A clustering or decomposition analysis that identifies a subset of value vectors which retain high accuracy when transferred from a source domain (e.g., PIQA) to a semantically distant target domain (e.g., TruthfulQA).

## Limitations

- Method requires 30 labeled samples for vector selection, which introduces lightweight supervision that may limit true "training-free" claims
- Performance drops significantly when transferring value vectors across datasets, indicating strong task-specific components
- Method is limited to MCQ format and cannot directly detect truthfulness in open-ended text generation

## Confidence

**High Confidence:** The core mechanism of using value vector activation patterns for truthfulness detection is well-supported by the experimental results (70.33% accuracy on NoVo benchmark). The technical implementation details for extracting key activations from MLP layers are clearly specified.

**Medium Confidence:** The claim that truthfulness signals concentrate in middle-to-late layers (Mechanism 3) is supported by the data but may be dataset-specific. The ensemble voting approach's effectiveness (Mechanism 2) is demonstrated, but the optimal selection percentage p may vary across different model architectures.

**Low Confidence:** The assertion that this method provides genuine "training-free" truthfulness detection is questionable given the 30-sample selection requirement. The method's performance on non-MCQ formats or in zero-shot settings remains unclear.

## Next Checks

1. **Layer Distribution Validation:** Profile the top 1% most accurate value vectors across all layers for your target model on a subset of NoVo datasets. Verify whether the mid-to-late layer concentration holds for your specific architecture.

2. **Selection Set Size Sensitivity:** Systematically vary the selection set size from 10 to 100 samples on a validation set. Plot accuracy versus selection set size to determine the minimum effective training-free threshold.

3. **Cross-Domain Transfer Experiment:** Select value vectors using one dataset (e.g., Science) and evaluate performance on a different domain (e.g., Social Reasoning). Measure the drop in accuracy to quantify the general versus task-specific components of the truthfulness signal.