---
ver: rpa2
title: Modeling cognitive processes of natural reading with transformer-based Language
  Models
arxiv_id: '2505.11485'
source_url: https://arxiv.org/abs/2505.11485
tags:
- should
- language
- cloze-pred
- these
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether transformer-based language models
  (GPT2, LLaMA-7B, and LLaMA2-7B) can better predict human eye movement patterns during
  reading compared to earlier models. The researchers evaluated these models using
  gaze duration data from Rioplatense Spanish readers and compared their predictions
  to human predictability estimates.
---

# Modeling cognitive processes of natural reading with transformer-based Language Models

## Quick Facts
- arXiv ID: 2505.11485
- Source URL: https://arxiv.org/abs/2505.11485
- Reference count: 40
- This study finds that transformer-based language models significantly outperform N-gram and LSTM models in predicting human eye movement patterns during reading, though they still fail to fully capture all variance in human reading patterns.

## Executive Summary
This study investigates whether transformer-based language models (GPT2, LLaMA-7B, and LLaMA2-7B) can better predict human eye movement patterns during reading compared to earlier models. The researchers evaluated these models using gaze duration data from Rioplatense Spanish readers and compared their predictions to human predictability estimates. While transformer models significantly improved prediction accuracy over previous architectures like N-grams and LSTM networks, they still failed to fully capture all variance in human reading patterns. The models showed less dependence on lexical frequency compared to earlier models, suggesting more sophisticated language processing. The study demonstrates that despite advances in NLP, state-of-the-art models still predict language differently than human readers, though they provide increasingly accurate approximations for studying cognitive processes.

## Method Summary
The researchers used eye movement data from 36 participants reading 8 narrative texts in Rioplatense Spanish (54,121 data points) and human cloze predictability estimates from ~1000 participants. They extracted next-word probabilities from GPT2-Spanish, LLaMA-7B, and LLaMA2-7B language models, then transformed these into predictability measures (comp-Pred). Using Linear Mixed Models, they compared how well these models predicted First-Pass Reading Time (FPRT) compared to human cloze-Pred. They also conducted fine-tuning experiments on domain-specific Spanish corpora and used "Remef" analysis to identify variance explained by human predictability that models missed.

## Key Results
- Transformer-based models (GPT2, LLaMA2-7B) significantly outperformed N-gram and LSTM models in explaining variance in Gaze Durations
- All models still failed to account for the entirety of the variance captured by human predictability
- Transformer models showed less dependence on lexical frequency compared to earlier models, suggesting more sophisticated language processing
- Fine-tuning on small domain-specific corpora did not improve model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based language models predict human gaze duration better than N-gram and LSTM models by capturing richer contextual dependencies across entire sequences.
- Mechanism: Self-attention processes input sequences holistically, retaining distant word information without the sequential degradation inherent in recurrent architectures. This enables more human-like next-word probability estimates that reflect broader discourse context.
- Core assumption: FPRT reflects early word processing difficulty, which correlates with predictability.
- Evidence anchors:
  - [abstract] "these architectures outperform earlier models in explaining the variance in Gaze Durations"
  - [Discussion] "Transformers benefit from processing entire input sequences simultaneously, allowing them to retain information from distant words without loss"
  - [corpus] Related work suggests transformer surprisal aligns with neural processing, though with inverse scaling—consistent with partial but imperfect cognitive alignment
- Break condition: If reading materials require world knowledge or pragmatic inference beyond distributional patterns, transformer predictions should diverge from human predictability.

### Mechanism 2
- Claim: Transformer-based comp-Pred accounts for variance in FPRT that partially overlaps with—but does not fully capture—human cloze-Pred.
- Mechanism: The "Remef" analysis reveals that cloze-Pred still explains significant variance in model residuals, indicating transformers miss cognitive factors humans use (e.g., discourse goals, world knowledge).
- Core assumption: Cloze-Pred represents a ground-truth proxy for human anticipatory processing during reading.
- Evidence anchors:
  - [Results, Table 2] "Cloze-Remef" row shows t-values from -6.12 (LLaMA2) to -13.19 (LLaMA-7B), confirming unexplained variance
  - [abstract] "these models still fail to account for the entirety of the variance captured by human predictability"
  - [corpus] Eye-tracking alignment work similarly finds partial but incomplete correspondence between LLM outputs and human cognitive signals
- Break condition: If cloze tasks systematically omit certain prediction types, both comp-Pred and cloze-Pred could miss shared variance.

### Mechanism 3
- Claim: Improved transformer predictions come with reduced dependence on lexical frequency, suggesting more sophisticated—but still mechanistically different—language processing than earlier models.
- Mechanism: N-gram comp-Pred nearly eliminates the Frequency effect, while transformers retain moderate frequency sensitivity. LLaMA2-7B shows the strongest frequency reduction among transformers, correlating with best residual alignment.
- Core assumption: Reduced frequency dependence indicates more context-driven prediction strategies.
- Evidence anchors:
  - [Results, Table 2] Frequency t-values: M0 = -10.83, M2 (N-gram) = -1.93, M4-M6 (GPT2) ≈ -5.0, M8 (LLaMA2) = -6.42
  - [Discussion] "more modern models are...achieving this by becoming less dependent on the frequency of word occurrences in the lexicon"
  - [corpus] Dyslexia reading research confirms word length, frequency, and predictability all independently affect fixation
- Break condition: If frequency and predictability interact non-linearly in human processing, linear mixed models may misestimate their independence.

## Foundational Learning

- **Concept: Cloze Predictability (cloze-Pred)**
  - Why needed here: Serves as the human benchmark that comp-Pred attempts to approximate; understanding its collection and limitations is essential for interpreting residual analyses.
  - Quick check question: If participants in a cloze task have different background knowledge than eye-tracking study readers, would cloze-Pred still be a valid ground-truth for those readers' predictions?

- **Concept: Linear Mixed Models (LMMs) with Remef Analysis**
  - Why needed here: The paper's core methodology uses LMMs to partition variance; the "Remef" step specifically quantifies how much human predictability remains unexplained by computational models.
  - Quick check question: Why does the paper add cloze-Pred to model residuals rather than directly comparing R² values between models with comp-Pred vs. cloze-Pred?

- **Concept: Akaike Information Criterion (AIC) for Model Comparison**
  - Why needed here: AIC differences (∆AIC) determine relative model quality while penalizing complexity; however, better AIC doesn't guarantee better alignment with human predictability.
  - Quick check question: Why might a model with better AIC (like GPT2 with ∆AIC = -482) still leave more unexplained cloze-Pred variance than a model with worse AIC (like LLaMA2 with ∆AIC = -407)?

## Architecture Onboarding

- **Component map:**
  Input Text → Language Model (GPT2/LLaMA) → Next-word Probabilities (comp-Pred)
       ↓
  Baseline LMM (M0) + comp-Pred → Full Model (M4-M8)
       ↓
  Residuals Extraction → Remef LMM with cloze-Pred → t-remef (alignment metric)

- **Critical path:**
  1. Extract FPRT and covariates (saccadic distance, word length, frequency, relative positions) from eye-tracking data
  2. Compute logit-transformed comp-Pred for each word from language model
  3. Fit LMM with baseline covariates + comp-Pred; record t-values and ∆AIC
  4. Extract residuals, fit Remef LMM with cloze-Pred; record t-remef

- **Design tradeoffs:**
  - Fine-tuning on domain-specific corpora: Paper found minimal improvement, likely due to small fine-tuning datasets (28-600MB) vs. pre-training (11GB+)
  - Model size: LLaMA-7B underperformed GPT2 (likely due to multilingual pre-training diluting Spanish-specific patterns); LLaMA2-7B improved substantially
  - AIC vs. cognitive alignment: Optimizing for statistical fit doesn't guarantee human-like prediction mechanisms

- **Failure signatures:**
  - N-gram models: Near-zero Frequency effect indicates over-reliance on surface statistics
  - LLaMA-7B: Low comp-Pred t-value (-9.17) and high t-remef (-13.19) suggest poor Spanish-specific calibration
  - All models: Non-zero t-remef confirms systematic divergence from human prediction patterns

- **First 3 experiments:**
  1. Replicate baseline comparison: Fit M0 through M3 on the published dataset to verify t-values and ∆AIC before adding transformer models
  2. Ablate fine-tuning impact: Compare pre-trained GPT2-Spanish vs. fine-tuned versions with statistical tests on ∆AIC differences
  3. Cross-linguistic validation: Apply identical pipeline to English eye-tracking corpora (e.g., Dundee or Provo) to test whether frequency-decoupling effects generalize

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning transformer-based models with significantly larger, domain-specific corpora improve their alignment with human cloze-predictability estimates?
- Basis in paper: The authors note that their fine-tuning datasets (28MB and 600MB) were small compared to the 11.5GB pre-training corpus, potentially explaining the lack of improvement.
- Why unresolved: The current study could not determine if fine-tuning effectively bridges the gap between computational and human predictability due to the limited size of the fine-tuning data used.
- What evidence would resolve it: Repeating the experiments using fine-tuning corpora that are orders of magnitude larger or comparable in size to the pre-training data to see if the ∆AIC and Cloze-Remef metrics improve significantly.

### Open Question 2
- Question: Do the specific interference effects between computational predictability and lexical frequency observed in Spanish replicate across other languages?
- Basis in paper: The authors explicitly state in the Discussion that extending the analysis to other languages would be interesting.
- Why unresolved: The study focused exclusively on Rioplatense Spanish; it is unknown if the reduced dependence on lexical frequency seen in transformers is a language-universal phenomenon or specific to the morphological structure of Spanish.
- What evidence would resolve it: Conducting the same Linear Mixed Model analysis on eye-tracking datasets from typologically diverse languages (e.g., English, Mandarin, Arabic) and comparing the t-values for the Frequency coefficient.

### Open Question 3
- Question: What specific linguistic or cognitive features account for the residual variance in reading times that human predictability captures but transformer models fail to explain?
- Basis in paper: The "Cloze-Remef" analysis showed that significant variance remains in the residuals of even the best models, which can still be explained by human cloze-Pred.
- Why unresolved: While the paper quantifies the gap, it does not isolate which aspects of human prediction (e.g., pragmatic inference, world knowledge, or discourse coherence) are missing from the computational models' probability distributions.
- What evidence would resolve it: A qualitative and quantitative analysis of the specific words or contexts where model predictions diverge most from human cloze scores, correlated with external metrics of pragmatic plausibility or discourse context.

## Limitations

- Cross-linguistic generalizability: The study uses Rioplatense Spanish data, raising questions about whether patterns extend to other languages with different morphological complexity.
- Cloze task representativeness: Cloze-Pred may not fully capture all forms of predictability humans use during reading, as evidenced by residual variance in transformer models.
- Fine-tuning dataset limitations: The paper reports minimal improvements from fine-tuning on small corpora, but these datasets are either unavailable or not yet released.

## Confidence

- **High confidence**: Transformer models significantly outperform N-gram and LSTM models in predicting gaze duration patterns, as evidenced by consistently better AIC scores and t-values for comp-Pred effects.
- **Medium confidence**: The claim that transformer models are "less dependent on lexical frequency" is supported by the data showing reduced frequency t-values, but the exact cognitive interpretation remains speculative.
- **Medium confidence**: The assertion that transformer models still fail to capture "the entirety of the variance" is methodologically sound, but the practical significance of this remaining unexplained variance for cognitive modeling is unclear.

## Next Checks

1. **Cross-linguistic replication**: Apply the identical pipeline to English eye-tracking corpora (Dundee or Provo) to test whether frequency-decoupling effects and residual predictability patterns generalize beyond Spanish.

2. **Cloze task sensitivity analysis**: Conduct ablation studies varying cloze prompt positions and participant pools to determine whether residual variance is consistent across different human predictability estimates.

3. **Fine-tuning validation**: Once the Argentinian blogs corpus becomes available, systematically test whether larger fine-tuning datasets (e.g., 1-10GB) produce measurable improvements in comp-Pred alignment with human cloze-Pred, particularly for LLaMA-7B.