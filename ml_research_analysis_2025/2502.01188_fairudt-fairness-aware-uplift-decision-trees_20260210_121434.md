---
ver: rpa2
title: 'FairUDT: Fairness-aware Uplift Decision Trees'
arxiv_id: '2502.01188'
source_url: https://arxiv.org/abs/2502.01188
tags:
- fairudt
- discrimination
- decision
- deprived
- favored
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairUDT addresses algorithmic bias by integrating uplift modeling
  with decision trees. It detects and mitigates discrimination by modeling the difference
  in class probabilities between favored and deprived groups.
---

# FairUDT: Fairness-aware Uplift Decision Trees

## Quick Facts
- arXiv ID: 2502.01188
- Source URL: https://arxiv.org/abs/2502.01188
- Reference count: 40
- Primary result: FairUDT achieves low discrimination (DP, AOD) while maintaining predictive performance on Adult, COMPAS, and German Credit datasets

## Executive Summary
FairUDT addresses algorithmic bias by integrating uplift modeling with decision trees to detect and mitigate discrimination. The method identifies discriminatory subgroups by measuring class probability differences between favored and deprived groups using distribution divergence measures like KL-divergence or squared Euclidean distance. Through a selective leaf relabeling approach, FairUDT reduces bias while preserving predictive utility, achieving a favorable balance between fairness and accuracy across three benchmark datasets.

## Method Summary
FairUDT is a preprocessing method that constructs a decision tree using uplift modeling principles to detect discriminatory patterns. The tree splits based on maximizing divergence between favored and deprived group distributions at each node. After construction, leaves with high discrimination scores undergo selective relabeling where instances from the minority group within that leaf are promoted or demoted to match the majority class probability. This approach transforms the dataset into a fairer version that can be used with any downstream classifier while maintaining interpretability through the tree structure.

## Key Results
- Achieves Demographic Parity (DP) values near zero and Average Odds Difference (AOD) close to zero on all three benchmark datasets
- Maintains competitive Balanced Accuracy (BA) and Accuracy (Acc) compared to baseline methods
- KL-gain splitting criterion outperforms Euclidean gain in most cases for discrimination detection
- Selective leaf relabeling effectively reduces bias while preserving predictive utility

## Why This Works (Mechanism)

### Mechanism 1: Uplift-based Splitting for Discrimination Detection
FairUDT identifies discriminatory subgroups by measuring class probability differences between favored and deprived groups using distribution divergence measures. The method treats favored groups as "treatment" and deprived groups as "control," adapting uplift modeling principles. At each node, it calculates gain in divergence between the two groups' class distributions, with splits selected to maximize this divergence and systematically partition the feature space into regions with distinct discrimination patterns.

### Mechanism 2: Selective Leaf Relabeling for Bias Mitigation
After tree construction, each leaf receives a discrimination score based on class probability differences between groups. Only leaves exceeding threshold σt undergo relabeling. For positive-majority leaves, deprived negative-class instances are promoted; for negative-majority leaves, favored positive-class instances are demoted. This equalizes probabilities within each targeted subgroup while preserving the majority class as the "correct" decision.

### Mechanism 3: Directional KL-Divergence for Asymmetric Bias Detection
KL-divergence outperforms symmetric measures like Euclidean distance for detecting favoritism because it captures directional information about which group benefits. The asymmetry is advantageous because discrimination typically manifests as the favored group deviating from a baseline treatment pattern. FairUDT uses deprived distribution as reference and measures how favored distribution diverges from it, with KL-gain ratio normalizing for split information and group imbalance.

## Foundational Learning

- **Concept: Uplift Modeling**
  - Why needed here: FairUDT adapts uplift modeling—a causal inference technique for measuring treatment effects—to discrimination detection. Understanding how uplift quantifies incremental impact of treatment vs. control is essential to grasp why the method treats favored/deprived status analogously.
  - Quick check question: In marketing uplift model, what does positive uplift score indicate? How does this map to discrimination when "treatment" is being in favored group?

- **Concept: Information-Theoretic Divergence Measures (KL-divergence)**
  - Why needed here: FairUDT's splitting criteria built on KL-divergence and Euclidean distance. KL-divergence measures how one probability distribution differs from another in terms of information loss. Understanding its properties explains why it's suitable for detecting distributional discrimination.
  - Quick check question: Why does KL(P||Q) → ∞ when Q(x) = 0 but P(x) > 0? How does Laplace correction prevent this issue?

- **Concept: Fairness Metrics (Demographic Parity, Average Odds Difference)**
  - Why needed here: FairUDT evaluated using DP and AOD as primary fairness metrics. DP measures whether positive outcome rates are equal across groups. AOD measures whether true positive and false positive rates are equal. Understanding what each metric captures is essential for interpreting results.
  - Quick check question: A model achieves DP=0 but AOD≠0. What type of unfairness might still exist?

## Architecture Onboarding

### Component Map
INPUT: Dataset (X, S, Y) -> 1. DATA PARTITIONER (Split into favored/deprived) -> 2. FAIRUDT CONSTRUCTOR (Tree construction with divergence optimization) -> 3. DISCRIMINATION EVALUATOR (Score leaves) -> 4. LEAF RELABELER (Apply relabeling with threshold σt) -> OUTPUT: Fair dataset for downstream classifier

### Critical Path
Data split → Tree construction (divergence optimization) → Discrimination scoring → Threshold filtering → Relabeling → Output

### Design Tradeoffs

| Decision | Option A | Option B | Paper's Choice | Guidance |
|----------|----------|----------|----------------|----------|
| Split criterion | KL-gain ratio | E-gain ratio | KL-gain | Use KL-gain for directional bias; E-gain if symmetry preferred |
| Tree depth | Maximum depth | Pruned depth | Maximum depth | Full depth captures fine-grained subgroups; consider pruning for large datasets |
| σt threshold | Low (→0) | High (→2) | Dataset-dependent | Low = stronger fairness, more accuracy loss |
| Test preprocessing | Raw test set | Relabeled test set | Both reported | Raw for realistic evaluation; relabeled for fairness-compliant deployment |

### Failure Signatures

1. **Accuracy cliff**: BA drops sharply as σt decreases below critical value (observed for Adult dataset). *Root cause*: Over-relabeling creates class imbalance. *Mitigation*: Plot metrics vs. σt; select threshold before the cliff.

2. **Demotion-dominant relabeling**: Most corrections are demotions rather than promotions. *Root cause*: Dataset has many positive-majority leaves with favored bias. *Mitigation*: Check promotion/demotion ratio.

3. **No fairness improvement**: DP/AOD don't improve even at σt=0. *Root cause*: Data lacks clear discriminatory subgroups detectable by tree splits. *Mitigation*: Inspect leaf distributions.

### First 3 Experiments

1. **Validate KL-gain implementation on Adult dataset**
   - Implement FairUDT with KL-gain ratio, grow to max depth, apply σt=0.61
   - Train LR classifier on pre-processed data
   - Expected: DP ≈ -0.07, AOD ≈ 0.00-0.04, BA ≈ 0.69-0.71, Acc ≈ 0.83-0.86

2. **Characterize σt tradeoff curves**
   - For Adult and COMPAS: sweep σt ∈ {0, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0}
   - Plot DP, AOD, BA, Acc vs. σt
   - Identify abrupt transition points (Adult at ~0.61, COMPAS at ~0.1)

3. **Cross-classifier generalization test**
   - Apply FairUDT (KL-gain, σt=0.61) to Adult dataset
   - Train LR, SVM, DT classifiers on pre-processed data
   - Compare fairness/accuracy metrics

## Open Questions the Paper Calls Out

1. **Intersectional Discrimination**: How can FairUDT be adapted to handle intersectional or compound discrimination through multi-treatment uplift modeling? Current implementation restricts sensitive attribute to binary state, preventing capture of overlapping marginalized identities.

2. **Ensemble Integration**: Can integrating FairUDT into ensemble models like Random Forests or XGBoost improve predictive performance without compromising fairness? While ensembles boost accuracy, it's unclear if fairness guarantees of single uplift tree are preserved when aggregated.

3. **Convex Optimization Framework**: Can a convex optimization pre-processing framework based on uplift modeling provide globally optimal solution where current tree-based approach may only find local optima? Current method relies on greedy tree splitting criteria that doesn't guarantee mathematically optimal tradeoff.

## Limitations

- **Selective relabeling assumptions**: Assumes majority class in leaf represents ground truth and systematic deviation for certain groups indicates historical bias correctable via label modification
- **Dataset dependency**: Performance heavily depends on dataset-specific tuning of σt threshold without principled framework for selection
- **Binary sensitive attribute restriction**: Current implementation only handles binary sensitive attributes, limiting applicability to intersectional discrimination scenarios

## Confidence

- **High confidence**: DP/AOD metric definitions and basic tree construction
- **Medium confidence**: Relabeling algorithm implementation and fairness-accuracy tradeoff patterns
- **Low confidence**: Directional divergence advantage claims and selective relabeling validity assumptions

## Next Checks

1. **Statistical significance testing**: Apply permutation tests to verify reported fairness improvements exceed what would occur by random relabeling

2. **External dataset validation**: Test FairUDT on fourth dataset (e.g., Bank Marketing or Law School) with different domain characteristics to assess generalizability

3. **Label noise robustness**: Introduce controlled label corruption to evaluate whether selective relabeling degrades performance when majority class assumptions fail