---
ver: rpa2
title: 'MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity
  (Extension)'
arxiv_id: '2511.12061'
source_url: https://arxiv.org/abs/2511.12061
tags:
- trajectory
- similarity
- trajectories
- learning
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MovSemCL, a movement-semantics contrastive
  learning framework for trajectory similarity computation. The method addresses three
  key limitations in existing approaches: insufficient modeling of trajectory semantics
  and hierarchy, high computational costs for long trajectories, and semantically-unaware
  augmentations.'
---

# MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)

## Quick Facts
- arXiv ID: 2511.12061
- Source URL: https://arxiv.org/abs/2511.12061
- Authors: Zhichen Lai; Hua Lu; Huan Li; Jialiang Li; Christian S. Jensen
- Reference count: 22
- Primary result: MovSemCL achieves mean ranks close to ideal value of 1 in trajectory similarity search while improving accuracy by up to 20.3% and reducing latency by up to 43.4% compared to state-of-the-art methods.

## Executive Summary
MovSemCL addresses key limitations in trajectory similarity computation by introducing movement-semantics contrastive learning. The method enriches raw GPS trajectories with movement dynamics and spatial context, segments them into patches for hierarchical attention encoding, and employs curvature-guided augmentation to preserve behaviorally informative segments. Experiments demonstrate that MovSemCL achieves superior accuracy and efficiency compared to existing approaches, with mean ranks approaching the ideal value of 1 in similarity search tasks.

## Method Summary
MovSemCL transforms raw GPS trajectories into enriched movement-semantics features through coordinate normalization, displacement/heading computation, and spatial graph embeddings. Trajectories are partitioned into fixed-size patches (P=4) and encoded using dual-level attention: intra-patch self-attention captures local movement patterns while inter-patch attention captures global dependencies. The method employs curvature-guided augmentation that masks redundant straight segments while preserving turns and intersections. A MoCo contrastive learning framework with dynamic queue and momentum encoder trains the model to produce semantically meaningful trajectory embeddings for efficient similarity computation.

## Key Results
- Achieves mean ranks close to ideal value of 1 in similarity search on real-world datasets
- Improves heuristic approximation accuracy by up to 20.3% compared to state-of-the-art methods
- Reduces inference latency by up to 43.4% while maintaining or improving accuracy
- Demonstrates robustness under various distortion and down-sampling conditions through curvature-guided augmentation

## Why This Works (Mechanism)

### Mechanism 1: Movement-Semantics Feature Enrichment
Raw GPS coordinates alone lack discriminative power for similarity computation. MovSemCL enriches trajectories with displacement vectors (dx, dy), heading angles (θ), and spatial graph embeddings that capture movement dynamics and transition patterns. This multi-dimensional representation provides more informative features than geometric position alone, encoding both local movement changes and broader spatial semantics.

### Mechanism 2: Hierarchical Patch-Based Attention
Segmenting trajectories into patches and applying dual-level attention reduces computational complexity from O(L²) to O(L·P + M²) while preserving hierarchical structure. Intra-patch attention captures local movement patterns within each patch, while inter-patch attention models global dependencies across patches. This hierarchical approach maintains both fine-grained and coarse-grained trajectory patterns.

### Mechanism 3: Curvature-Guided Augmentation (CGA)
CGA generates semantically consistent augmented views by masking trajectory points with probability inversely proportional to local curvature. High-curvature points (turns, intersections) receive elevated retention weights while straight segments are more likely to be masked. This preserves behaviorally informative segments during contrastive learning, improving model robustness to noise and distortion.

## Foundational Learning

- **Contrastive Learning (MoCo Framework):**
  - Why needed here: Core training paradigm that learns representations by pulling positive pairs together and pushing negative pairs apart
  - Quick check question: Can you explain why the MoCo momentum encoder and dynamic queue are used instead of end-to-end backpropagation through all negatives?

- **Self-Attention Mechanics:**
  - Why needed here: Both intra-patch and inter-patch attention rely on scaled dot-product attention; understanding Q/K/V projections is essential
  - Quick check question: Given L=200 trajectory points with patch size P=4, how many attention operations occur in intra-patch vs. inter-patch layers?

- **GPS Coordinate Systems & Trajectory Representation:**
  - Why needed here: Raw WGS84 coordinates must be projected to planar coordinates for meaningful distance computation
  - Quick check question: Why does computing Euclidean distance directly on (longitude, latitude) pairs produce incorrect results at high latitudes?

## Architecture Onboarding

- **Component map:**
  Movement-Semantics Encoding -> Patch Construction -> Intra-Patch Attention -> Inter-Patch Attention -> Trajectory Embedding -> CGA Augmentation -> Contrastive Loss

- **Critical path:** Movement-Semantics Encoding → Patch Construction → Intra-Patch Attention → Inter-Patch Attention → Trajectory Embedding → CGA Augmentation → Contrastive Loss. If any upstream feature is corrupted (especially spatial graph embeddings), downstream representations will be degraded.

- **Design tradeoffs:**
  - Patch size P=4 balances local context and computational efficiency; smaller loses context, larger dilutes semantics
  - Grid resolution: 100m (Porto) vs. 1000m (Germany); denser grids capture finer patterns but increase memory usage
  - Embedding dimension d=256 provides sweet spot between capacity and efficiency; 512 shows marginal gains

- **Failure signatures:**
  - MSE removal: Severe degradation (1.002 → 1.521 on Porto 20K); spatial context is critical
  - CGA removal: Moderate degradation (1.002 → 1.033), especially under noise/distortion conditions
  - HSE removal: Smallest impact but still measurable; hierarchical structure provides benefits

- **First 3 experiments:**
  1. Run inference on 10 trajectories with all components; verify mean rank ≈ 1 when querying T_a against database containing T_b (odd/even split)
  2. Disable CGA and use random masking; compare robustness under down-sampling rates (0.1-0.5) to validate curvature-guided benefit
  3. Measure per-sample latency with varying trajectory counts (Figure 5 reproduction); confirm ~3.4ms stability regardless of dataset size

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MovSemCL perform under zero-shot geographic transfer to regions where the pre-computed trajectory-induced spatial graph and Node2Vec embeddings are unavailable?
  - Basis: Methodology relies on dataset-specific spatial graphs, but experiments are confined to within-dataset evaluations
  - Why unresolved: Unclear if model depends on dataset-specific spatial priors or if movement dynamics alone generalize
  - What evidence would resolve it: Cross-dataset experiments training on one city and testing on another without updating spatial graph

- **Open Question 2:** Does the grid-based spatial discretization lose semantic fidelity in dense urban environments where distinct road segments fall within the same grid cell?
  - Basis: Trajectory-Induced Spatial Graph uses regular grid discretization (100m for Porto), ignoring road network topology
  - Why unresolved: Paper evaluates overall retrieval accuracy but not failure cases from spatial aliasing
  - What evidence would resolve it: Stratified evaluation on trajectory pairs spatially close but topologically distinct

- **Open Question 3:** Does CGA risk over-masking essential contextual information for trajectories characterized by uniformly low curvature, such as long-distance highway travel?
  - Basis: CGA prioritizes turns while treating straight segments as redundant based on turning angles
  - Why unresolved: While robustness to random down-sampling is tested, specific impact on low-curvature trajectories is not isolated
  - What evidence would resolve it: Performance comparison on trajectory subsets with low vs. high curvature variance

## Limitations

- Scalability may degrade for extremely long trajectories (L > 1000) where O(M²) term dominates
- Grid-based spatial discretization may lose semantic fidelity in dense urban environments with complex road networks
- CGA effectiveness depends on trajectory curvature distribution; uniformly low-curvature routes may not provide sufficient augmentation diversity

## Confidence

- **High**: MSE encoding effectiveness (strong ablation evidence), CGA's robustness under downsampling (Figure 6), latency reduction claims (measured on GPU)
- **Medium**: Patch-based attention complexity claims (theoretical vs. measured), spatial graph construction scalability (memory constraints not fully characterized)
- **Low**: CGA hyperparameter sensitivity (weights not specified), generalization to extreme trajectory lengths (only tested up to 200 points)

## Next Checks

1. **Extreme Length Test**: Evaluate mean rank and latency on trajectories with L=500, 1000, and 2000 points to verify O(L·P + M²) complexity holds and identify failure thresholds
2. **Uniform Curvature Benchmark**: Test CGA on synthetically generated straight-line trajectories to confirm it degrades gracefully when turning angles are minimal
3. **Grid Resolution Sensitivity**: Vary grid cell sizes by factors of 2-10 on Germany dataset to quantify impact on spatial graph quality and downstream similarity accuracy