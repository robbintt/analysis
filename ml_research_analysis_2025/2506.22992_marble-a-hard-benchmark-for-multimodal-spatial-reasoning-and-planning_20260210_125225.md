---
ver: rpa2
title: 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning'
arxiv_id: '2506.22992'
source_url: https://arxiv.org/abs/2506.22992
tags:
- reasoning
- multimodal
- step
- cube
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MARBLE, a challenging multimodal reasoning
  benchmark designed to evaluate the ability of multimodal language models (MLLMs)
  to perform complex spatial reasoning and planning under physical constraints. The
  benchmark includes two tasks: M-Portal, inspired by Portal 2, which tests multi-step
  spatial reasoning and planning, and M-Cube, inspired by Happy Cube puzzles, which
  evaluates 3D assembly reasoning.'
---

# MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning

## Quick Facts
- arXiv ID: 2506.22992
- Source URL: https://arxiv.org/abs/2506.22992
- Authors: Yulun Jiang; Yekun Chai; Maria Brbić; Michael Moor
- Reference count: 40
- All evaluated models achieve near-random performance on M-Portal's plan correctness task and 0% accuracy on the harder M-Cube task.

## Executive Summary
This paper introduces MARBLE, a challenging multimodal reasoning benchmark designed to evaluate the ability of multimodal language models (MLLMs) to perform complex spatial reasoning and planning under physical constraints. The benchmark includes two tasks: M-Portal, inspired by Portal 2, which tests multi-step spatial reasoning and planning, and M-Cube, inspired by Happy Cube puzzles, which evaluates 3D assembly reasoning. Both tasks require integrating visual and textual information to craft step-by-step plans.

Extensive evaluation across 12 state-of-the-art MLLMs reveals that current models perform poorly on MARBLE. All models achieve near-random performance on M-Portal's plan correctness task and 0% accuracy on the harder M-Cube task. Even in simplified subtasks, only a few models outperform random baselines. Further analysis shows that perception remains a bottleneck, with models struggling to extract structured information from visual inputs. These results highlight the limitations of existing MLLMs in complex multimodal reasoning and planning, emphasizing the need for next-generation models capable of deeper, multi-step multimodal understanding.

## Method Summary
The MARBLE benchmark evaluates 12 state-of-the-art MLLMs on two multimodal reasoning tasks: M-Portal (Portal 2-inspired spatial planning) and M-Cube (Happy Cube-inspired 3D assembly). Models are tested zero-shot with temperature 0.0 and max tokens 16,000-40,000 depending on model size. M-Portal uses 512 plan correctness and 512 fill-the-blanks samples from 16 Portal 2 community maps with screenshots and instructions. M-Cube uses 1000 CUBE and 1000 CUBE-easy synthetically generated puzzles with 6 rendered jigsaw pieces. Evaluation metrics include F1-score for plan correctness, accuracy for fill-the-blanks and CUBE tasks, with solution validator checking geometric constraints for M-Cube.

## Key Results
- All 12 evaluated MLLMs achieve near-random performance on M-Portal's plan correctness task (F1 around 0.5)
- All models achieve 0% accuracy on the full M-Cube task, even with ground-truth text inputs
- Only a few models outperform random baselines on simplified CUBE-easy variant
- Perception bottleneck identified: models achieve ~70% per-cell accuracy but 0% full-piece accuracy on structured extraction task

## Why This Works (Mechanism)

### Mechanism 1: Perception-Reasoning Bottleneck Decomposition
- Claim: If MLLMs cannot reliably extract structured information from visual inputs, downstream reasoning cascades into failure regardless of reasoning capacity.
- Mechanism: MARBLE isolates perception (converting 3D-rendered jigsaw pieces to 5×5 binary arrays) from reasoning (searching the combinatorial solution space). Errors at perception propagate—~70% per-cell accuracy yields 0% full-piece accuracy, preventing valid reasoning.
- Core assumption: Perception errors compound across multi-step reasoning chains.
- Evidence anchors:
  - [abstract] "perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs"
  - [section 3.5] "all the models could only achieve around 70% accuracy per cell... all the models achieve 0% accuracy on the whole piece"
  - [corpus] SpatialBench (arXiv:2511.21471) independently confirms MLLM limitations in spatial cognition tasks
- Break condition: If models achieve >95% per-cell structured perception accuracy, reasoning capacity becomes the dominant bottleneck.

### Mechanism 2: Combinatorial Search Space Overwhelms Current Reasoning Models
- Claim: Task difficulty appears to scale non-linearly with search space size, causing catastrophic performance drops.
- Mechanism: CUBE comprises 6! × 8^6 ≈ 188M possible solutions. Even reasoning-focused models (DeepSeek-R1) drop from 57% to 0% accuracy when moving from 1 to 3+ missing pieces, suggesting inefficient search heuristics.
- Core assumption: Current models lack effective pruning or backtracking for constraint-satisfaction search.
- Evidence anchors:
  - [abstract] "0% accuracy on the harder M-Cube task"
  - [section 3.5] Figure 7 shows "performance drops drastically... falling to 0% when more than 3 pieces are missing"
  - [corpus] MPCC (arXiv:2507.23382) similarly identifies multimodal planning under complex constraints as an open challenge
- Break condition: If models develop constraint propagation or hierarchical decomposition, performance may scale more gracefully with search space.

### Mechanism 3: Iterative Refinement via Diagnostic Feedback
- Claim: Tool-augmented feedback loops may partially compensate for reasoning limitations if models retain prior context.
- Mechanism: Solution validator provides edge-conflict diagnostics; GPT-o4-mini improved from 10% to 28% on CUBE-easy with detailed feedback over 5 rounds, but remained at 0% on full CUBE.
- Core assumption: Models can maintain and build upon earlier reasoning steps across interaction rounds.
- Evidence anchors:
  - [abstract] Notes perception bottleneck without claiming feedback as solution
  - [section 3.5] "detailed feedback consistently outperforms binary feedback, increasing the performance from 10% to up to 28% accuracy"
  - [corpus] Weak corpus support—related benchmarks don't emphasize iterative tool-based refinement
- Break condition: If models discard prior reasoning context (as paper notes some do), feedback provides diminishing returns.

## Foundational Learning

- Concept: **Multimodal Chain-of-Thought (MCoT) Reasoning**
  - Why needed here: MARBLE requires interleaved visual-textual reasoning across dozens of steps with spatial constraints.
  - Quick check question: How does MCoT differ from text-only CoT in terms of modality alignment requirements at each reasoning step?

- Concept: **Combinatorial Search and Constraint Satisfaction**
  - Why needed here: M-Cube requires exploring 188M+ configurations under geometric interlocking constraints.
  - Quick check question: Given 6 pieces each with 8 orientations, why is the search space 6! × 8^6 rather than 6^8?

- Concept: **Portal 2 Game Mechanics (Momentum Conservation, Portal Placement)**
  - Why needed here: M-Portal relies on physics-based constraints—portal momentum, lasers, buttons, tractor beams.
  - Quick check question: If a player enters a portal at velocity v, what constraints determine their exit trajectory?

## Architecture Onboarding

- Component map:
  M-Portal Pipeline: Portal 2 map → Human annotation → Screenshots + textual instructions + ground-truth CoT + 5 mistaken steps → Binary classification (plan correctness) or fill-the-blanks evaluation
  M-Cube Pipeline: 5×5×5 cube synthesis → Disassemble into 6 interlocking pieces → Render from random 3D viewpoint → MLLM proposes face-assignment + orientation → Solution validator checks geometric constraints
  Evaluation Metrics: F1-score (plan correctness), Accuracy (fill-the-blanks, CUBE), Token usage tracking

- Critical path:
  1. Visual encoding of 3D-rendered pieces or game screenshots
  2. Structured information extraction (perception bottleneck)
  3. Multi-step plan generation under physical/spatial constraints
  4. Constraint verification via solution validator
  5. Optional: Iterative refinement with diagnostic feedback

- Design tradeoffs:
  - **Closed-ended vs. interactive evaluation**: Paper chose binary/fill-the-blanks for accessibility over real-time game execution
  - **Simplified CUBE-easy**: Trades realism (2D arrays vs. rendered images, no flipping, 4/6 pieces pre-placed) for diagnostic granularity
  - **Hint images**: Added for 14/16 maps; improved plan-correctness F1 only 7.6%→8.6%

- Failure signatures:
  - Perception failure: 70% per-cell accuracy, 0% full-piece accuracy on structured extraction task
  - Reasoning failure: Near-random performance as search space exceeds ~1,000 configurations
  - Context retention failure: Iterative refinement yields no improvement on hard tasks (CUBE stays at 0%)

- First 3 experiments:
  1. **Perception probe**: Evaluate structured extraction (image → 5×5 binary array) on 200 samples before full CUBE; expect ~70% per-cell if aligned with paper findings.
  2. **Search space ablation**: Test on CUBE-easy with 1→6 missing pieces to identify where your model's accuracy drops to near-zero.
  3. **Feedback loop test**: Compare binary vs. detailed diagnostic feedback over 5 rounds on CUBE-easy to assess whether your model retains and leverages prior context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can "thinking with images" or compositional visual reasoning approaches enable models to solve MARBLE's complex spatial constraints where static MLLMs fail?
- Basis in paper: [explicit] The authors state, "Future work should investigate interactive and adaptive approaches, enabling models to reason with and through different modalities—such as 'thinking with images'—in a more compositional way."
- Why unresolved: The current benchmark evaluates models in a static, single-turn fashion without test-time adaptation or dynamic visual manipulation.
- What evidence would resolve it: A model utilizing interactive visual tools or compositional visual generation achieving significantly higher accuracy on M-Portal or M-Cube compared to standard MLLMs.

### Open Question 2
- Question: Can architectures capable of retaining context over multiple turns leverage diagnostic feedback to solve the most difficult CUBE configurations?
- Basis in paper: [explicit] The paper notes that "future models capable of interleaved thinking and tool use would benefit more from such validator-assisted setup," as current models often discard earlier reasoning context.
- Why unresolved: While the paper provides a validator tool, current models achieved 0% accuracy on the hard CUBE task even with detailed feedback, failing to refine their strategies effectively.
- What evidence would resolve it: A model successfully solving the full CUBE task (currently 0% accuracy) through an iterative, multi-turn dialogue with the solution validator.

### Open Question 3
- Question: Is the failure on the full M-Cube task driven primarily by the inability to extract structured visual features or by the combinatorial explosion of the search space?
- Basis in paper: [inferred] The analysis shows that models achieve 0% accuracy on the full piece perception task, yet even with ground-truth text inputs, reasoning performance drops to 0% when search complexity increases.
- Why unresolved: The paper identifies both perception and reasoning as bottlenecks, but the specific interaction between visual extraction errors and search tractability on the hardest setting remains undetermined.
- What evidence would resolve it: An ablation study comparing model performance on the full CUBE task using perfect (oracle) visual inputs versus standard visual inputs to isolate the reasoning deficit.

## Limitations

- The benchmark's closed-ended evaluation format may not capture the full complexity of real-world multimodal reasoning scenarios.
- Simplified CUBE-easy variant trades ecological validity for diagnostic clarity, potentially limiting generalizability.
- Lack of human baseline comparisons constrains assessment of whether MLLM performance represents fundamental limitations or task-specific challenges.

## Confidence

- Perception bottleneck claims: **High** - Clear evidence from structured extraction task showing 70% per-cell but 0% full-piece accuracy
- Reasoning limitation claims: **Medium** - Performance drops identified but not systematically controlled for search algorithm efficiency
- Diagnostic feedback effectiveness: **Low** - Limited experimental validation across different model families and task difficulties

## Next Checks

1. **Perception Baseline Assessment**: Evaluate human performance on the structured extraction task (converting 3D-rendered pieces to 5×5 binary arrays) to establish whether the 70% per-cell accuracy represents a fundamental visual perception limit or model-specific weakness.

2. **Search Algorithm Ablation**: Test whether providing explicit search heuristics (constraint propagation, backtracking) to reasoning-capable models improves CUBE performance beyond the observed 0% accuracy threshold, isolating whether the bottleneck is reasoning capacity or search strategy.

3. **Cross-Domain Transfer Analysis**: Evaluate whether models that perform well on simpler spatial benchmarks (like SpatialBench) show improved MARBLE performance, testing whether current MLLM limitations are task-specific or indicative of broader spatial reasoning deficits.