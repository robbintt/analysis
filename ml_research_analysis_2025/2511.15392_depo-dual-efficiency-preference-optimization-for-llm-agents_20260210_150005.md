---
ver: rpa2
title: 'DEPO: Dual-Efficiency Preference Optimization for LLM Agents'
arxiv_id: '2511.15392'
source_url: https://arxiv.org/abs/2511.15392
tags:
- efficiency
- succ
- arxiv
- wang
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the inefficiency of large language model\
  \ (LLM) agents, which often generate overly long chain-of-thought (CoT) and require\
  \ many interaction steps, leading to high latency and cost. To tackle this, the\
  \ authors formally define dual-efficiency\u2014minimizing both tokens per step (step-level)\
  \ and total steps (trajectory-level)\u2014and propose DEPO (Dual-Efficiency Preference\
  \ Optimization)."
---

# DEPO: Dual-Efficiency Preference Optimization for LLM Agents

## Quick Facts
- **arXiv ID:** 2511.15392
- **Source URL:** https://arxiv.org/abs/2511.15392
- **Reference count:** 6
- **Key outcome:** DEPO reduces token usage by up to 60.9% and step count by up to 26.9% on WebShop and BabyAI while maintaining or improving task performance.

## Executive Summary
This paper addresses the inefficiency of large language model (LLM) agents, which often generate overly long chain-of-thought (CoT) and require many interaction steps, leading to high latency and cost. To tackle this, the authors formally define dual-efficiency—minimizing both tokens per step (step-level) and total steps (trajectory-level)—and propose DEPO (Dual-Efficiency Preference Optimization). DEPO extends vanilla KTO by adding an efficiency-aware bonus to the reward function, encouraging trajectories with fewer tokens and steps. Experiments on WebShop and BabyAI show DEPO reduces token usage by up to 60.9% and step count by up to 26.9%, while improving or maintaining task performance. It also generalizes to math benchmarks and remains effective with only 25% of training data.

## Method Summary
DEPO is a two-stage training pipeline that first uses behavior cloning (BC) to establish a baseline policy, then applies dual-efficiency preference optimization (DEPO) to refine it. The method uses MCTS-generated trajectories labeled as desirable or undesirable based on reward thresholds. DEPO extends KTO by adding an efficiency-aware bonus to the reward function, encouraging trajectories with fewer tokens per step and fewer total steps. The bonus is only applied to desirable trajectories to avoid degrading performance. Training uses LoRA for parameter efficiency and is tested on WebShop and BabyAI environments.

## Key Results
- DEPO reduces token usage by up to 60.9% on WebShop and BabyAI
- Step count reduction of up to 26.9% while maintaining or improving task success rates
- Effective with only 25% of training data on WebShop
- Generalizes to math benchmarks (GSM8K, Math)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adding an efficiency-aware bonus to the reward function directly shapes preference learning toward trajectories that use fewer tokens per step and fewer total steps.
- **Mechanism:** The implied reward $r_\theta(\tau) = \log \frac{\pi_\theta(a_t|\tau_t)}{\pi_{BC}(a_t|\tau_t)} + b(\tau)$ extends the standard log-ratio with $b(\tau) = \frac{\alpha_1}{T_{token}(\tau)} + \frac{\alpha_2}{T_{step}(\tau)}$ for desirable trajectories. Since $b(\tau)$ inversely scales with token count and step count, efficient trajectories receive higher implied rewards, amplifying their preference signal in the KTO sigmoid loss. Critically, the bonus applies only to desirable trajectories—undesirable ones get $b(\tau)=0$.
- **Core assumption:** Efficient trajectories exist in the desirable set and the model can learn to emulate their patterns without sacrificing task success. Also assumes the inverse scaling relationship properly captures human preferences for brevity.
- **Evidence anchors:**
  - [abstract] "DEPO...jointly rewards succinct responses and fewer action steps."
  - [section 3.4] Equation (9)-(10) defines the bonus; Section 4.5 shows joint $\alpha_1, \alpha_2 > 0$ outperforms single-parameter variants.
  - [corpus] Weak direct evidence; neighbor papers (MCTS-EP, PGPO) use preference optimization but not with explicit efficiency bonuses.
- **Break condition:** If efficient trajectories in the desirable set are too sparse, or if the inverse scaling is too aggressive (high $\alpha$), the model may under-reason on hard tasks. The paper explicitly constrains efficiency optimization to successful trajectories to mitigate this.

### Mechanism 2
- **Claim:** MCTS-generated trajectories with reward-threshold labeling creates a quality-separated desirable/undesirable split that facilitates cleaner preference learning.
- **Mechanism:** MCTS explores the action space via UCT selection, expansion, rollout, and backpropagation, producing diverse trajectories. Labeling uses thresholds $\kappa_0, \kappa_1, \kappa_2$: $r(\tau) \geq \kappa_0$ → desirable; $\kappa_2 \leq r(\tau) < \kappa_1$ → undesirable. The margin $(\kappa_0 - \kappa_1)$ ensures clear separation. A rephrasing model polishes thoughts in desirable trajectories while preserving actions.
- **Core assumption:** Assumption: Reward thresholding creates consistent preference signals, and rephrasing improves thought quality without altering action correctness.
- **Evidence anchors:**
  - [section 3.2] Equations (2)-(3) define MCTS and labeling protocol.
  - [section 4.1] Concrete thresholds: BabyAI desirable requires $r(\tau) \geq 0.9$, steps < 7.
  - [corpus] MCTS-EP (arXiv:2509.17116) similarly uses MCTS for preference data collection in embodied planning.
- **Break condition:** If the margin between desirable and undesirable is too narrow, or if rephrasing introduces errors, preference signals become noisy.

### Mechanism 3
- **Claim:** Two-stage training (BC → DEPO) stabilizes learning by first establishing a competent base policy before preference refinement.
- **Mechanism:** Stage 1: BC trains $\pi_{BC}$ on a subset of desirable trajectories $D_{BC}$ via $L_{SFT}(\theta) = -\mathbb{E}_{\tau \sim D_{BC}}[\log \pi_\theta(\tau|u)]$. Stage 2: DEPO uses $\pi_{BC}$ as the reference for KL regularization and log-ratio computation, contrasting desirable vs. undesirable trajectories.
- **Core assumption:** Assumption: BC provides a sufficiently good initialization that the preference optimization can refine without catastrophic forgetting.
- **Evidence anchors:**
  - [section 3.3-3.4] Describes the two-stage pipeline explicitly.
  - [section 4.4] DEPO achieves >10% efficiency gains with 25% of training data, suggesting robustness from BC initialization.
  - [corpus] Standard practice; verified in multiple agent-tuning papers (AgentTuning, Agent-FLAN cited in Section 2).
- **Break condition:** If BC overfits to a narrow subset, DEPO may struggle to generalize; if the undesirable set contains high-quality trajectories (label noise), preference learning degrades.

## Foundational Learning

- **Concept: KTO (Kahneman-Tversky Optimization)**
  - **Why needed here:** DEPO extends KTO; understanding the base loss (prospect theory, reference dependence, sigmoid value function) is prerequisite to modifying the reward formulation.
  - **Quick check question:** Can you explain why KTO uses a symmetric sigmoid around the KL-regularized reference point and how desirable vs. undesirable branches differ?

- **Concept: POMDP formulation for agents**
  - **Why needed here:** The paper models agent-environment interaction as $(S, A, O, T, R)$; understanding trajectory $\tau_t$, action sampling, and reward attribution is essential.
  - **Quick check question:** Given a trajectory $\tau = (o_1, a_1, ..., o_T)$, where does the reward $r(\tau)$ come from, and how does DEPO modify it?

- **Concept: MCTS (Monte Carlo Tree Search)**
  - **Why needed here:** Data generation relies on MCTS (selection via UCT, expansion, rollout, backpropagation) to produce diverse trajectories.
  - **Quick check question:** In the UCT formula (Eq. 2), what does the exploration weight $w$ control, and what happens if $w$ is too high?

## Architecture Onboarding

- **Component map:** Environment (WebShop/BabyAI) → MCTS Generator (DeepSeek-V3) → Trajectories → Labeler (reward thresholds) → Rephraser (GPT-4.1-mini) → Desirable set D, Undesirable set U → Stage 1: BC (LoRA, 3 epochs) → π_BC → Stage 2: DEPO (extends KTO with efficiency bonus) → π_θ

- **Critical path:**
  1. MCTS depth and exploration weight must generate sufficient trajectory diversity.
  2. Threshold selection (κ₀, κ₁, κ₂, step cutoff) determines label quality.
  3. Bonus hyperparameters (α₁, α₂) control efficiency-performance tradeoff.

- **Design tradeoffs:**
  - Higher α → more aggressive efficiency, potential under-reasoning.
  - Larger margin (κ₀ - κ₁) → cleaner preference signal but fewer training samples.
  - Applying penalty to undesirable trajectories (Eq. 11) degrades performance (Section 4.5).

- **Failure signatures:**
  - Success rate drops significantly: efficiency bonus may be too aggressive; reduce α₁, α₂.
  - Token/step counts don't decrease: check that desirable set contains efficient examples; verify step cutoff filtering.
  - Training instability: verify KL term z₀(τ) is computed correctly; check β (temperature) is not too high.

- **First 3 experiments:**
  1. **Sanity check:** Run BC-only baseline on your target environment; verify it achieves reasonable success rate before DEPO.
  2. **Hyperparameter sweep:** With fixed β=0.2, sweep α₁, α₂ ∈ {0, 1, 2, 3} on a small validation set; plot success rate vs. token reduction to find Pareto frontier.
  3. **Ablation:** Compare (a) DEPO vs. (b) DEPO with penalty on undesirable (Eq. 11) vs. (c) vanilla KTO; confirm Section 4.5 findings on your domain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does applying an explicit efficiency penalty to undesirable trajectories degrade performance and efficiency?
- **Basis in paper:** [explicit] Section 4.5 "Undesirable penalty" notes that adding a penalty term to the loss for undesirable samples "does not yield overall improvements" and actually increases token usage and reduces success rates, contrary to intuition.
- **Why unresolved:** The authors empirically observe the failure but do not provide a theoretical explanation for why reinforcing efficiency only on positive samples works better than a symmetric penalty/reward structure.
- **What evidence would resolve it:** A theoretical analysis of the gradient updates in the KTO loss landscape, or ablations showing how a penalty on low-reward samples might incorrectly shift the policy boundary.

### Open Question 2
- **Question:** Can the trade-off weights for step-level versus trajectory-level efficiency (α₁, α₂) be determined dynamically?
- **Basis in paper:** [inferred] Table 2 demonstrates that different models (Llama vs. Qwen) require different static values for α₁ and α₂ to achieve optimal results, suggesting these are sensitive hyperparameters.
- **Why unresolved:** The current approach relies on manual tuning to balance verbosity versus step count, which may not scale well to diverse new environments or model architectures.
- **What evidence would resolve it:** An experiment showing that an adaptive schedule (e.g., curriculum learning) or a learned controller for these weights achieves robust performance without manual search.

### Open Question 3
- **Question:** How does DEPO perform in environments where successful trajectories (desirable samples) are extremely sparse or difficult to generate?
- **Basis in paper:** [explicit] Section 3.1 states, "Efficiency is optimized only among successful trajectories," and the bonus (Eq. 10) is only applied if τ ∈ D.
- **Why unresolved:** If the initial MCTS exploration fails to find successful paths, the efficiency bonus is never triggered, potentially limiting the method's utility on very hard tasks compared to methods that can learn from partial successes.
- **What evidence would resolve it:** Evaluations on complex, sparse-reward benchmarks (e.g., harder BabyAI levels) comparing DEPO against methods that reward dense sub-goals.

## Limitations

- **Critical missing details:** LoRA rank/alpha parameters, MCTS simulation count, UCT exploration weight, and exact rephrasing prompt template are not specified.
- **Evaluation concerns:** Efficiency is only measured on generated trajectories, not real user queries; wall-clock latency is not measured.
- **Generalization uncertainty:** Limited testing on open-ended tasks and non-sequential decision-making domains.

## Confidence

**High Confidence:** The core mechanism of adding efficiency-aware bonuses to KTO is technically sound and well-implemented. The two-stage training pipeline (BC → DEPO) follows established practices. The experimental results on WebShop and BabyAI are reproducible given sufficient resources.

**Medium Confidence:** The MCTS data generation and labeling process is correctly described, but the exact implementation details (threshold selection, rephrasing quality) could significantly impact results. The efficiency-performance tradeoff claims are supported but may be sensitive to hyperparameter choices.

**Low Confidence:** The paper's claims about DEPO's effectiveness on math benchmarks are based on limited experiments. The assertion that DEPO is "the first method to optimize both tokens per step and total steps" requires verification against related work using different efficiency metrics.

## Next Checks

1. **Implementation Validation:** Reproduce the BC baseline on BabyAI with the exact parameters (lr=1e-4, 3 epochs, LoRA) and verify it achieves the reported success rates before applying DEPO.

2. **Hyperparameter Sensitivity Analysis:** Conduct a systematic sweep of α₁ and α₂ values (0-5 range) on a held-out validation set, measuring the Pareto frontier between success rate and efficiency metrics to identify optimal tradeoffs.

3. **Real-World Efficiency Test:** Deploy the trained DEPO agent on live WebShop tasks and measure actual wall-clock latency, comparing it against the baseline to validate whether theoretical token reduction translates to practical speed improvements.