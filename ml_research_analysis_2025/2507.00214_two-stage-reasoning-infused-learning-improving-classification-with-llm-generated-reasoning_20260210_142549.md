---
ver: rpa2
title: 'Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated
  Reasoning'
arxiv_id: '2507.00214'
source_url: https://arxiv.org/abs/2507.00214
tags:
- reasoning
- classifier
- emotion
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage approach to improve text classification
  by generating reasoning with predictions. In Stage 1, a Llama-3.2-1B-Instruct model
  is fine-tuned on a general reasoning dataset to generate textual reasoning from
  question-answer pairs.
---

# Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning

## Quick Facts
- **arXiv ID:** 2507.00214
- **Source URL:** https://arxiv.org/abs/2507.00214
- **Reference count:** 15
- **Primary result:** Two-stage approach improves emotion classification accuracy from 49.7% to 58.4% (8.7 percentage point gain)

## Executive Summary
This paper presents a two-stage method to improve text classification by generating reasoning with predictions. The approach first fine-tunes a small Llama-3.2-1B-Instruct model on a general reasoning dataset to generate textual reasoning from question-answer pairs. In the second stage, this trained model generates reasonings offline to augment a downstream emotion classification dataset, creating (text, reasoning + label) pairs. A second Llama-3.2-1B-Instruct model is then fine-tuned to generate both the reasoning and predicted emotion as a single output. The method achieves 58.4% accuracy on the dair-ai/emotion dataset, outperforming a baseline model trained only to predict emotions (49.7%) by 8.7 percentage points. The improvement is statistically significant and demonstrates the benefit of explicit reasoning training.

## Method Summary
The approach consists of two stages. Stage 1 fine-tunes Llama-3.2-1B-Instruct on the syvai/reasoning-gen dataset (350k examples) to map (Question, Answer) pairs to Reasoning. Stage 2 uses this trained model to generate reasonings for the dair-ai/emotion dataset (16k train/2k test) offline, creating augmented training data with target format "Reasoning + Label". A second Llama-3.2-1B-Instruct model is then fine-tuned on this augmented data to map Input Text directly to Generated Reasoning + Label. Both fine-tuning stages use the Axolotl framework with LR=2e-5, cosine scheduler, and gradient checkpointing. The production model outputs a single autoregressive sequence containing the reasoning followed by the predicted emotion.

## Key Results
- Accuracy improves from 49.7% to 58.4% on dairy-ai/emotion dataset (8.7 percentage point gain)
- Improvement is statistically significant across most emotion categories
- Severely underrepresented "surprise" class drops from 13.8% to 1.5% accuracy
- Method demonstrates successful transfer of reasoning ability from general logic tasks to emotion classification

## Why This Works (Mechanism)

### Mechanism 1: Reasoning as Superior Supervisory Signal
Enriching training targets with explicit reasoning provides better supervision than direct label mapping. The cross-entropy loss optimization forces the model to learn intermediate features and logical dependencies that classification-only losses might ignore. This "learning to explain" acts as regularization, compelling the model to learn the "why" not just the "what".

### Mechanism 2: Conditional Latent State Conditioning
The autoregressive architecture requires the model to output Reasoning followed by Label, conditioning the latent state for more accurate final predictions. This compels the model to allocate capacity to resolving reasoning steps in earlier layers before predicting the label token, effectively distilling Chain-of-Thought behavior into the fine-tuned weights.

### Mechanism 3: Cross-Domain Reasoning Transfer
Reasoning capabilities transfer across disparate domains (logic to emotion). The Stage 1 model learns argumentation structure from general datasets, which applies to the specific domain of emotion in Stage 2. This allows a small 1B model to exhibit reasoning capabilities it otherwise lacks.

## Foundational Learning

- **Concept: Fine-Tuning vs. Zero-Shot Inference**
  - Why needed here: The paper relies on fine-tuned small models outperforming massive Zero-Shot models when trained on the right data structure
  - Quick check question: Does the model improve because it knows more facts, or because it has learned a better process for mapping inputs to outputs?

- **Concept: Data Augmentation via Synthetic Generation**
  - Why needed here: The core method is "offline" data augmentation where reasoning is generated by a separate model
  - Quick check question: If the generator makes a mistake, does the student model learn that mistake as truth?

- **Concept: Autoregressive Generation**
  - Why needed here: The output is a single stream of text tokens (Reasoning then Label), not a separate classification head
  - Quick check question: How does the loss function treat reasoning tokens compared to the label token during backpropagation?

## Architecture Onboarding

- **Component map:** Llama-R-Gen (Stage 1) -> Data Engine -> Classifier Q->RA (Stage 2)
- **Critical path:** The quality of the Data Engine. Performance failure in the "Surprise" class indicates the bottleneck is the generator's ability to reason about minority classes
- **Design tradeoffs:**
  - Latency vs. Accuracy: Inference sequence length doubles, increasing latency and compute cost
  - Generality vs. Specificity: Using general reasoning datasets enables cross-domain transfer but introduces noise for specific emotional nuances
- **Failure signatures:**
  - Majority Class Bias: "Surprise" accuracy drops to 1.5%, indicating reasoning mechanism collapses for underrepresented data
  - Flawed Reasoning, Correct Label: Model may output contradictory justification yet still produce correct label
- **First 3 experiments:**
  1. Baseline Validation: Replicate Q→A vs Q→RA comparison on balanced subset to isolate reasoning benefit
  2. Ablation on Reasoning Quality: Manually corrupt 20% of generated reasonings to measure robustness to noisy supervision
  3. Inference Length Analysis: Benchmark latency increase and calculate accuracy-per-second trade-off

## Open Questions the Paper Calls Out

1. **Faithfulness of Reasoning:** Do generated reasonings faithfully reflect the model's internal decision process, or are they merely plausible post-hoc rationalizations? The paper notes this remains a core challenge in explainable AI, with qualitative analysis revealing cases where the model correctly predicted a label but generated justification for a different label.

2. **Minority Class Performance:** Can filtering low-quality reasoning or targeted augmentation prevent performance collapse in severely underrepresented classes? The authors list this as necessary future work after observing the "surprise" class drop from 13.8% to 1.5% accuracy.

3. **Cross-Domain Generalization:** Does this framework generalize to other NLP tasks where reasoning structures differ significantly from emotion classification? The paper suggests applying this to broader NLP tasks like NLI or question answering, but validation is limited to emotion classification.

## Limitations
- Severe class imbalance sensitivity: "Surprise" class accuracy drops from 13.8% to 1.5%
- Computational overhead: Inference sequence length doubles, increasing latency and compute cost
- Error propagation: Offline generation means generator errors are propagated to downstream model without correction

## Confidence

**High Confidence (8/10):** The core finding that explicitly training models to generate reasoning before classification improves accuracy on the dairy-ai/emotion dataset.

**Medium Confidence (6/10):** The mechanism explanation that reasoning generation acts as superior supervisory signal.

**Low Confidence (4/10):** Claims about cross-domain reasoning transfer from logic to emotion.

## Next Checks

1. **Minority Class Robustness Test:** Manually evaluate generated reasonings for the "surprise" class to confirm if generic or misaligned reasoning causes the performance collapse.

2. **Reasoning Faithfulness Analysis:** Verify whether generated reasoning logically supports predicted labels for sample predictions to determine if the model is genuinely "reasoning."

3. **Latency-Benefit Analysis:** Measure inference time for baseline vs. reasoning-infused models across batch sizes and calculate accuracy-per-second trade-off to assess practical deployment viability.