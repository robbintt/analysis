---
ver: rpa2
title: Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction
arxiv_id: '2602.02018'
source_url: https://arxiv.org/abs/2602.02018
tags:
- answer
- verification
- question
- response
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VeriFY, a training-time framework for reducing
  factual hallucinations in large language models by teaching them to reason about
  their own factual uncertainty through structured self-verification. The method uses
  verification traces where the model generates an initial answer, produces a verification
  question via semantic transformations, answers it independently, re-answers the
  original question in context, and then makes a consistency judgment to decide whether
  to answer or abstain.
---

# Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction

## Quick Facts
- arXiv ID: 2602.02018
- Source URL: https://arxiv.org/abs/2602.02018
- Reference count: 40
- Primary result: VeriFY reduces hallucination rates by 9.7 to 53.3% with only modest recall reductions (0.4 to 5.7%)

## Executive Summary
This paper introduces VeriFY, a training-time framework for reducing factual hallucinations in large language models by teaching them to reason about their own factual uncertainty through structured self-verification. The method uses verification traces where the model generates an initial answer, produces a verification question via semantic transformations, answers it independently, re-answers the original question in context, and then makes a consistency judgment to decide whether to answer or abstain. To avoid reinforcing hallucinated content, the training applies stage-level loss masking to exclude hallucinated answer stages while preserving supervision over verification behavior. Evaluated across multiple model families and scales, VeriFY reduces hallucination rates by 9.7 to 53.3% with only modest reductions in recall (0.4 to 5.7%), outperforming baselines like knowledge probing and self-consistency in both in-domain and cross-dataset settings. It also introduces a novel precision-recall-based metric tailored for evaluating abstention-based hallucination mitigation.

## Method Summary
VeriFY trains models to perform structured self-verification by augmenting training with verification traces that guide the model through five stages: initial answer generation, verification question generation, independent verification answer, re-answer conditioned on verification, and consistency judgment. The training uses stage-level loss masking to exclude hallucinated answer stages while preserving supervision over verification behavior, preventing reinforcement of incorrect facts. The framework employs semantic transformations (rephrasing, implication, inverse, etc.) to generate verification questions and filters training traces to retain only aligned correctness-consistency outcomes. Models are evaluated using a novel Hallucination F1 metric that balances precision and recall in the selective prediction setting.

## Key Results
- VeriFY reduces hallucination rates by 9.7% to 53.3% across different model families (Gemma, Llama, Qwen) and scales (2B to 12B parameters)
- The method achieves precision-recall tradeoffs that outperform knowledge probing (higher precision, lower recall) and self-consistency (lower precision, higher recall)
- Cross-dataset generalization shows consistent performance improvements on HotpotQA, NQ-Open, and FreshQA beyond the training domain (TriviaQA)
- Verification strategies show distinct behaviors: rephrasing favors recall while implication favors precision, with 4 strategies providing optimal coverage

## Why This Works (Mechanism)

### Mechanism 1: Structured Verification Traces Internalize Self-Checking Behavior
- Claim: Training on structured verification traces causes models to learn explicit self-verification as a latent reasoning behavior, enabling more calibrated uncertainty detection than direct abstention training.
- Mechanism: The trace structure (initial answer → verification question → verification answer → re-answer → consistency judgment) forces the model to practice consistency-checking operations during training. This creates learnable patterns where internal uncertainty signals become coupled with explicit verification steps, rather than mapping uncertainty directly to refusal.
- Core assumption: Models encode latent signals correlated with factual uncertainty that can be sharpened through explicit verification supervision.
- Evidence anchors:
  - [abstract]: "VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain."
  - [section 1]: "We propose internalizing post-hoc verification as a learned reasoning behavior during training, enabling the model to infer sharper internal uncertainty signals that can be mapped to abstention decisions."
  - [corpus]: Weak direct evidence; related work (DSVD, ReVISE) explores self-verification at inference time, but training-time internalization is less established.
- Break condition: If models lack latent uncertainty signals, or if verification traces introduce noise that overwhelms signal, the mechanism fails.

### Mechanism 2: Stage-Level Loss Masking Prevents Hallucination Reinforcement
- Claim: Selectively excluding hallucinated answer stages from the training loss prevents reinforcement of incorrect facts while preserving supervision over verification reasoning.
- Mechanism: By masking loss on stages marked as hallucinated (using ground-truth labels or judge consensus) but keeping those stages in the context, the model learns to process and reason about incorrect content without being rewarded for reproducing it. This disentangles learning verification behavior from learning factual content.
- Core assumption: Hallucinated content can be reliably identified at the stage level, and the verification/question-generation stages remain valid even when answer stages are incorrect.
- Evidence anchors:
  - [abstract]: "To avoid reinforcing hallucinated content, the training applies stage-level loss masking to exclude hallucinated answer stages while preserving supervision over verification behavior."
  - [section 2.2]: "Operating at the granularity of stages avoids entangling correct and incorrect supervision within a single semantic unit so that the model never learns to reproduce false factual assertions."
  - [corpus]: No direct corroboration; loss masking for hallucination prevention is novel to this framework.
- Break condition: If hallucination detection at training time is unreliable, or if masking removes too much supervision signal, the mechanism degrades.

### Mechanism 3: Consistency Judgment as Abstention Proxy
- Claim: Training models to emit explicit consistency judgments between initial and re-answers creates a learned threshold for abstention that preserves more correct answers than aggressive refusal-style training.
- Mechanism: The consistency judgment stage provides explicit supervision on when inconsistency indicates uncertainty. Unlike knowledge probing (which trains on refusals for incorrect answers), this approach teaches the model to verify before deciding, reducing unnecessary abstentions on answerable questions.
- Core assumption: Inconsistency between initial and verification-conditioned answers reliably correlates with factual uncertainty, and models can learn this mapping.
- Evidence anchors:
  - [section 1]: "Post-hoc methods show that models assessing the consistency of their own outputs can substantially reduce hallucinations."
  - [section 4.2]: "VeriFY reduces hallucinations primarily by eliminating incorrect answers rather than by discarding answerable queries."
  - [corpus]: Self-consistency methods (SC-k baseline) support the consistency-uncertainty link, though they operate at inference time.
- Break condition: If consistency and correctness decouple (e.g., consistent wrong answers), the proxy fails.

## Foundational Learning

- **Selective Prediction and Abstention**
  - Why needed here: VeriFY frames hallucination reduction as a selective prediction problem where models choose between answering and abstaining.
  - Quick check question: Can you explain why standard accuracy is inadequate for evaluating abstention-capable systems?

- **Supervised Fine-Tuning with Loss Masking**
  - Why needed here: The core implementation requires modifying standard SFT to selectively exclude token spans from the loss.
  - Quick check question: How would you implement token-level loss masking in a standard transformer training loop?

- **Precision-Recall Tradeoffs in Open-Ended Generation**
  - Why needed here: The paper's evaluation metric (Hallucination F1) explicitly balances correctness against coverage.
  - Quick check question: Why does high precision alone not guarantee a good hallucination mitigation system?

## Architecture Onboarding

- **Component map:**
  - Verification Trace Constructor -> Hallucination Annotator -> Stage-Level Loss Masker -> Training Loop -> Evaluation Pipeline

- **Critical path:**
  1. Generate verification traces for training data (87K TriviaQA questions)
  2. Filter traces to retain only aligned correctness-consistency outcomes
  3. Apply loss masking during full-parameter SFT
  4. Evaluate on in-domain (TriviaQA) and cross-dataset (HotpotQA, NQ-Open, FreshQA) benchmarks

- **Design tradeoffs:**
  - **MM vs. SM masking**: MM preserves more supervision (higher precision); SM is more conservative (higher recall). Paper shows stable tradeoff—choose based on deployment priority.
  - **Verification strategy diversity**: Paper finds 4 strategies optimal; more introduces redundancy. Rephrasing favors recall, implication/inverse favor precision.
  - **Intra- vs. inter-family trace generation**: Cross-family transfer fails to teach calibrated abstention—generate traces with the target model family.

- **Failure signatures:**
  - **Overly conservative behavior** (high R−, low recall): Indicates loss masking insufficient or verification signals not learned.
  - **Inflated recall, reduced precision** (answering aggressively): May indicate inter-family trace transfer or loss masking too aggressive.
  - **No improvement over base**: Verify trace filtering is retaining aligned examples; check hallucination annotation quality.

- **First 3 experiments:**
  1. **Reproduce MM vs. SM comparison on single model family** (e.g., Gemma-2-2B) to validate masking regime tradeoffs match paper.
  2. **Ablate verification strategies**: Train with single strategies to confirm rephrasing favors recall while implication favors precision.
  3. **Test cross-dataset generalization**: Train on TriviaQA, evaluate on HotpotQA to verify learned verification behavior transfers.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can VeriFY be effectively adapted for open-ended, long-form generation tasks where reference answers are ambiguous or unavailable?
- **Basis in paper:** [Explicit] The authors state, "In this work, we instantiate VeriFY on factoid questions with well-defined reference answers... While our verification framework itself is not inherently restricted to factoid data."
- **Why unresolved:** The current supervision mechanism relies on unambiguous correctness evaluation (via ground truth or consensus) to create consistency judgments, which is difficult to scale to nuanced or creative text.
- **What evidence would resolve it:** Successful application of the framework to long-form benchmarks (e.g., paragraph generation) using automated atomic fact extraction or reference-free consistency metrics.

### Open Question 2
- **Question:** What representational mechanisms explain the failure of inter-family transfer in teaching calibrated abstention behavior?
- **Basis in paper:** [Explicit] The authors hypothesize that failure in cross-family training (Table 5) "arises from family-specific uncertainty representations embedded within the model's internal dimensions" which do not align cleanly across architectures.
- **Why unresolved:** The paper empirically demonstrates the transfer failure but does not provide mechanistic evidence or analysis of the internal representation misalignment.
- **What evidence would resolve it:** Probing experiments that visualize or quantify the alignment of uncertainty subspaces between teacher and student models of different families.

### Open Question 3
- **Question:** Can models be trained to generate verification traces effectively without relying on annotations from a higher-capacity teacher model?
- **Basis in paper:** [Inferred] The methodology relies on a "higher-capacity model" to generate verification questions and consistency judgments, creating a dependence on a model superior to the one being fine-tuned.
- **Why unresolved:** It is unclear if the verification reasoning capability is fundamentally emergent from the data or if it requires the teacher's specific capacity to reason correctly in the first place.
- **What evidence would resolve it:** A study comparing VeriFY's performance when using self-generated traces (draft-model traces) versus teacher-generated traces.

## Limitations
- Inter-family transfer of verification behavior fails due to family-specific uncertainty representations, requiring trace generation from same or higher-capacity same-family models
- The framework depends on a higher-capacity teacher model for generating verification questions and consistency judgments, creating a supervision bottleneck
- Current evaluation focuses on factoid questions with clear reference answers, limiting applicability to open-ended generation tasks

## Confidence

- Hallucination reduction claims: **High** (supported by multiple metrics across 4 datasets and 8 model families)
- Cross-dataset generalization: **High** (shown on 3 distinct datasets beyond training domain)
- Precision-recall tradeoff superiority: **Medium** (evidence shows better balance than baselines, but not compared against all possible abstention strategies)

## Next Checks

1. **Ablate loss masking regimes**: Train NM (no masking), MM, and SM variants on identical data to empirically confirm that masked stages show reduced hallucination reinforcement while preserving verification learning. Measure the gradient contribution to hallucinated vs. correct stages.

2. **Probe consistency signal quality**: For a subset of questions, manually annotate whether consistency judgments correlate with factual uncertainty beyond simple correctness matching. Test whether consistent-but-wrong answers are being incorrectly accepted.

3. **Validate verification strategy diversity**: Systematically ablate individual strategies (e.g., train with only rephrasing vs. only implication) to confirm the paper's claim that rephrasing favors recall while implication favors precision, and that 4 strategies provide optimal coverage without redundancy.