---
ver: rpa2
title: Optimal Linear Baseline Models for Scientific Machine Learning
arxiv_id: '2508.05831'
source_url: https://arxiv.org/abs/2508.05831
tags:
- linear
- optimal
- data
- rank
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for linear encoder-decoder
  architectures in scientific machine learning, deriving closed-form optimal mappings
  under Bayes risk minimization. The authors develop optimal low-rank linear solutions
  for forward modeling, inverse recovery, and autoencoding tasks, extending existing
  formulations to handle rank-deficient data, operators, and measurement processes.
---

# Optimal Linear Baseline Models for Scientific Machine Learning

## Quick Facts
- arXiv ID: 2508.05831
- Source URL: https://arxiv.org/abs/2508.05831
- Reference count: 40
- This paper presents a theoretical framework for linear encoder-decoder architectures in scientific machine learning, deriving closed-form optimal mappings under Bayes risk minimization.

## Executive Summary
This paper establishes optimal linear encoder-decoder architectures for scientific machine learning through Bayes risk minimization. The authors derive closed-form, rank-constrained linear and affine mappings for forward modeling, inverse recovery, and autoencoding tasks. Through theoretical analysis and numerical experiments across biomedical imaging, financial factor analysis, and shallow water equations, they demonstrate that optimal linear mappings can match or exceed learned neural networks in reconstruction accuracy while requiring minimal hyperparameter tuning.

## Method Summary
The method computes optimal linear mappings by minimizing expected squared error under Bayes risk. For forward modeling, the optimal rank-r mapping is $\hat{A} = (F L_X)_r L_X^\dagger$ where $L_X$ is the Cholesky factor of the data covariance. Inverse recovery uses $\hat{A} = (\Gamma_X F^\top L_Y^{\dagger\top})_r L_Y^\dagger$. Autoencoding employs $\hat{A} = (\Gamma_X \Gamma_X^\top)_r \Gamma_X^\dagger$. Implementation requires computing empirical second-moment matrices with ridge regularization, Cholesky factorization, and truncated SVD. Learned baselines use single-layer linear networks or autoencoders trained with Adam optimizer at learning rate 10^-3 for 100-200 epochs.

## Key Results
- Optimal linear mappings outperform learned encoder-decoder networks in reconstruction accuracy across all tested domains
- Even for nonlinear shallow water equations, optimal linear inverse mapping (NRMSE 0.12) significantly outperforms learned nonlinear network (NRMSE 0.87)
- Optimal affine linear autoencoder achieves lower reconstruction error and more interpretable latent spaces than PCA and trained autoencoders on financial data
- Theoretical framework extends to rank-deficient data, operators, and measurement processes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Closed-form optimal rank-constrained linear mappings can be derived for forward, inverse, and autoencoding tasks under Bayes risk minimization with squared loss.
- **Mechanism:** The Bayes risk objective is reformulated using trace properties and second-moment factorizations, then completed the square transforms this into a rank-constrained matrix approximation problem solvable via the generalized Eckart-Young theorem, yielding solutions of the form $\hat{A} = (F L_X)_r L_X^\dagger$ for forward problems.
- **Core assumption:** Linear forward model with independent unbiased noise and finite second moments.
- **Evidence anchors:** Derives "closed-form, rank-constrained linear and affine linear optimal mappings" [abstract]; defines Bayes risk minimization [Section 3.2]; proves optimal forward mapping [Section 4.1 Theorem 2].
- **Break condition:** If the forward operator is significantly nonlinear, performance degrades as nonlinearity dominates.

### Mechanism 2
- **Claim:** The optimal rank-r solution automatically handles rank-deficient data, forward operators, and measurement noise through the structure of the pseudoinverse and truncation.
- **Mechanism:** TSVD truncation $(F L_X)_r$ discards directions with negligible singular values corresponding to data rank deficiency, operator rank deficiency, or poor signal-to-noise combinations. The pseudoinverse projects onto the row space of $L_X$, avoiding amplification of unobservable directions.
- **Core assumption:** The true signal structure lies in a low-dimensional subspace with additive independent noise.
- **Evidence anchors:** "extending existing formulations to handle rank-deficient data, operators, and measurement processes" [abstract]; shows how rank-deficient $L_X$ leads to projection rather than full $F$ [Section 4.1, Table 1].
- **Break condition:** If the intrinsic dimensionality of the signal exceeds the chosen rank $r$, the truncated solution discards genuine signal components.

### Mechanism 3
- **Claim:** Optimal linear inverse mappings can outperform learned nonlinear networks even when the underlying forward process is nonlinear.
- **Mechanism:** Shallow water equations preserve approximate linear structure in momentum equations despite nonlinear continuity. The optimal linear inverse mapping captures dominant linear correlations while nonlinear networks overfit to noise or converge to suboptimal local minima.
- **Core assumption:** The nonlinear dynamics contain a strong linear component with noise present.
- **Evidence anchors:** "even in nonlinear systems like the shallow water equations, the optimal linear inverse mapping significantly outperforms a learned nonlinear network" [abstract]; linear model achieves NRMSE 0.12 vs. 0.87 for nonlinear [Section 5.3, Table 5].
- **Break condition:** If the forward dynamics are highly nonlinear with negligible linear component, linear mappings will fail.

## Foundational Learning

- **Singular Value Decomposition and Truncated SVD:**
  - Why needed here: All optimal mappings are expressed via TSVD; understanding rank truncation is essential for implementing $(F L_X)_r$.
  - Quick check question: Given a matrix $W$ with singular values $\sigma_1 \geq \cdots \geq \sigma_n$, can you write the rank-$r$ truncated approximation?

- **Bayes Risk Minimization:**
  - Why needed here: The theoretical framework frames all tasks as expected loss minimization under the data distribution.
  - Quick check question: How does empirical Bayes risk differ from population Bayes risk, and when does the former converge to the latter?

- **Moore-Penrose Pseudoinverse:**
  - Why needed here: Solutions involve $L_X^\dagger$, $L_Y^\dagger$, and $\Gamma_Y^\dagger$; understanding pseudoinverse behavior for rank-deficient matrices is critical.
  - Quick check question: For a rank-$k$ matrix $W$ with SVD $U\Sigma V^\top$, what is $W^\dagger$ and what subspace does $WW^\dagger$ project onto?

## Architecture Onboarding

- **Component map:**
  - Input data matrix $X$ -> Observation matrix $Y = FX + E$ -> Latent bottleneck dimension $r$ -> Encoder $E$ -> Decoder $D$ -> Composed mapping $A = DE$

- **Critical path:**
  1. Estimate $\Gamma_X = \frac{1}{J}XX^\top$ and $\Gamma_Y = \frac{1}{J}YY^\top$ (add ridge for SPD)
  2. Compute Cholesky factors $L_X$, $L_Y$ (or use symmetric square root)
  3. For forward: $\hat{A} = (Y X^\top X^\dagger)_r X^\dagger$ or $(F L_X)_r L_X^\dagger$ if $F$ known
  4. For inverse: $\hat{A} = (X Y^\top Y^\dagger)_r Y^\dagger$
  5. Predict: $\hat{Y} = \hat{A}X$ (forward) or $\hat{X} = \hat{A}Y$ (inverse)

- **Design tradeoffs:**
  - Smaller $r$: stronger regularization, lower capacity, may underfit
  - Larger $r$: better approximation but may overfit noise; computation scales as $O(\min(m,n)^3)$ for SVD
  - Ridge regularization on $\Gamma$ stabilizes pseudoinverse but biases solution
  - Affine vs. linear: affine handles nonzero means; linear is simpler when data is centered

- **Failure signatures:**
  - Reconstruction error doesn't decrease as $r$ increases → check data rank, verify second-moment estimates
  - Predictions are constant → pseudoinverse failure; add ridge regularization
  - Training error near zero but test error high → overfitting from excessive $r$ relative to intrinsic dimensionality

- **First 3 experiments:**
  1. **Synthetic linear system:** Generate $Y = FX + E$ with known rank-$k$ $F$, verify optimal mapping recovers $F$ when $r \geq k$ and data is full-rank.
  2. **Noiseless autoencoding on structured data:** Apply to MedMNIST, plot reconstruction error vs. $r$, confirm convergence to identity as $r \to n$.
  3. **Ablation on nonlinear system:** On shallow water equations data, compare optimal linear vs. trained nonlinear across ranks; identify the $r$ where linear matches or exceeds nonlinear performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Financial experiment lacks complete architectural details for nonlinear baseline, making fair comparison difficult
- SWE results showing linear superiority may reflect poor nonlinear network training rather than fundamental advantages
- Theoretical framework assumes linear forward models, so claimed superiority on nonlinear systems relies heavily on numerical experiments

## Confidence
- **High Confidence**: Theoretical derivations for linear tasks under Bayes risk minimization with squared loss are mathematically sound
- **Medium Confidence**: Claims about handling rank-deficient data through TSVD truncation are supported by framework but limited empirical validation
- **Low Confidence**: Assertion that optimal linear inverse mappings outperform learned nonlinear networks on nonlinear systems relies on single experiment with potential confounding factors

## Next Checks
1. **Financial Experiment Replication**: Implement complete nonlinear autoencoder architecture and verify comparison under identical training conditions and random seeds
2. **SWE Sensitivity Analysis**: Vary nonlinear network training quality systematically to determine if linear superiority persists or reflects training deficiencies
3. **Nonlinear Dynamics Stress Test**: Design synthetic experiments with increasing nonlinearity to identify threshold where linear mappings fail, providing quantitative bounds on theoretical claims