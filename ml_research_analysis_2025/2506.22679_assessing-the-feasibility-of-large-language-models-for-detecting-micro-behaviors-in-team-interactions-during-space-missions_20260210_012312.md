---
ver: rpa2
title: Assessing the feasibility of Large Language Models for detecting micro-behaviors
  in team interactions during space missions
arxiv_id: '2506.22679'
source_url: https://arxiv.org/abs/2506.22679
tags:
- classification
- fine-tuning
- language
- llms
- micro-behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores using large language models to detect micro-behaviors\u2014\
  subtle uplifting or discouraging expressions\u2014in team conversations during simulated\
  \ space missions. It compares encoder-only models (RoBERTa, DistilBERT) with decoder-only\
  \ Llama-3.1, using zero-shot, fine-tuned, and few-shot prompting approaches on text-only\
  \ transcripts."
---

# Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions

## Quick Facts
- **arXiv ID:** 2506.22679
- **Source URL:** https://arxiv.org/abs/2506.22679
- **Reference count:** 0
- **Primary result:** Decoder-only Llama-3.1 with few-shot prompting achieved 44% macro F1 and 68% binary F1 for detecting uplifting/discouraging micro-behaviors in team mission transcripts.

## Executive Summary
This paper investigates the use of large language models to detect micro-behaviors—subtle uplifting or discouraging expressions—in team conversations during simulated space missions. The study compares encoder-only models (RoBERTa, DistilBERT) with decoder-only Llama-3.1 using zero-shot, fine-tuned, and few-shot prompting approaches on transcript-only data. Encoder-only models struggled with underrepresented classes like discouraging speech, while Llama-3.1 with few-shot prompting achieved the best performance, particularly in recall for both uplifting and discouraging behaviors. The findings suggest decoder-only models are more effective for nuanced, context-dependent micro-behavior detection in text-based team interaction analysis.

## Method Summary
The study employed transcript data from simulated space missions, comparing encoder-only models (RoBERTa, DistilBERT) against decoder-only Llama-3.1. Three prompting strategies were tested: zero-shot, fine-tuned, and few-shot approaches. Encoder-only models used weighted fine-tuning and paraphrasing to address class imbalance, while Llama-3.1 leveraged few-shot examples. The task involved classifying team interaction segments as uplifting, discouraging, or neutral, with performance measured using macro F1-score for multi-class and binary classification scenarios.

## Key Results
- Llama-3.1 achieved 44% macro F1-score for 3-way classification (uplifting/discouraging/neutral)
- Llama-3.1 achieved 68% F1-score for binary classification (uplifting/discouraging)
- Encoder-only models showed poor performance on underrepresented discouraging class, even with weighted fine-tuning

## Why This Works (Mechanism)
Decoder-only models like Llama-3.1 excel at context-dependent, nuanced classification tasks because they generate tokens sequentially while attending to all previous context. This autoregressive nature allows them to better capture subtle conversational dynamics and micro-behaviors that require understanding of context and nuance. The few-shot prompting approach provides exemplars that help the model recognize patterns in underrepresented classes like discouraging speech, which encoder-only models struggle with despite traditional fine-tuning approaches.

## Foundational Learning

### Transfer Learning for NLP
**Why needed:** Enables leveraging pre-trained language understanding on new tasks with limited data
**Quick check:** Model shows reasonable performance on related classification tasks without extensive retraining

### Class Imbalance Handling
**Why needed:** Micro-behaviors like discouraging speech are naturally underrepresented in team interactions
**Quick check:** Weighted loss functions or data augmentation improve minority class performance

### Few-shot Prompting
**Why needed:** Allows models to learn from minimal examples when labeled data is scarce
**Quick check:** Performance improves with strategic example selection in prompts

### Encoder-only vs Decoder-only Architectures
**Why needed:** Different architectures suit different NLP tasks based on their processing patterns
**Quick check:** Task requiring context understanding vs generation benefits different model types

### Micro-behavior Detection
**Why needed:** Identifying subtle communication patterns critical for team cohesion and performance
**Quick check:** Model can distinguish between nuanced positive/negative conversational cues

## Architecture Onboarding

### Component Map
Text transcripts -> Tokenizer -> LLM (Llama-3.1) -> Few-shot Prompt Template -> Classification Output

### Critical Path
Input transcript → Tokenization → Context window processing → Prompt template application → Probability scoring → Class assignment

### Design Tradeoffs
- **Encoder-only:** Better for static classification, struggles with context-heavy tasks
- **Decoder-only:** Superior for context-dependent tasks, requires careful prompt engineering
- **Few-shot vs Fine-tuning:** Few-shot preserves generalization, fine-tuning may overfit to specific dataset

### Failure Signatures
- Low recall on discouraging class indicates bias toward majority uplifting/neutral examples
- Token limit constraints may truncate important contextual information
- Prompt quality directly impacts classification accuracy

### First 3 Experiments
1. Test different few-shot example selections to optimize minority class performance
2. Compare few-shot performance across different prompt template structures
3. Evaluate model sensitivity to transcript length and context window size

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on transcript-only data excludes non-verbal cues critical for micro-behavior detection
- Modest F1 scores (44% macro, 68% binary) indicate ongoing challenges with nuanced classification
- Simulation-based setting may not capture the complexity and stress of actual space missions

## Confidence

### High confidence
- Decoder-only model superiority over encoder-only for this task

### Medium confidence
- Specific performance metrics due to simulation constraints
- Transferability to real mission contexts

## Next Checks
1. Test Llama-3.1 on real mission transcripts or high-fidelity analog mission data to validate performance in authentic contexts
2. Incorporate multimodal data (audio tone, timing, video) to assess whether micro-behavior detection improves beyond text-only approaches
3. Conduct ablation studies to identify which few-shot examples most improve performance for underrepresented classes like discouraging speech