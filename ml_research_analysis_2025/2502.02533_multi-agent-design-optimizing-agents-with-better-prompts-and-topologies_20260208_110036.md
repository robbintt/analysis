---
ver: rpa2
title: 'Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies'
arxiv_id: '2502.02533'
source_url: https://arxiv.org/abs/2502.02533
tags:
- agents
- answer
- optimization
- design
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Multi-Agent System Search (MASS), a framework
  that automates the design of multi-agent systems by optimizing both agent prompts
  and topology. The authors conduct an in-depth analysis showing that prompt design
  is critical for MAS performance, with effective prompts outperforming simple agent
  scaling approaches.
---

# Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies

## Quick Facts
- arXiv ID: 2502.02533
- Source URL: https://arxiv.org/abs/2502.02533
- Reference count: 40
- This paper presents MASS, a framework that automates multi-agent system design by jointly optimizing agent prompts and topology, achieving state-of-the-art performance across reasoning, multi-hop understanding, and code generation tasks.

## Executive Summary
This paper introduces Multi-Agent System Search (MASS), a framework that automates the design of multi-agent systems by jointly optimizing agent prompts and workflow topology. The authors demonstrate that prompt design is more critical than agent scaling, with optimized prompts significantly outperforming simple ensemble approaches. MASS uses a three-stage interleaved optimization process that first warms up individual agents with prompt optimization, then searches pruned topology spaces based on influence scoring, and finally performs workflow-level prompt optimization to capture agent interdependence. The framework achieves state-of-the-art performance across diverse tasks including reasoning (MATH, DROP), multi-hop understanding (HotpotQA, MuSiQue), and code generation (MBPP, HumanEval).

## Method Summary
MASS implements a three-stage interleaved optimization: (1) Block-level prompt optimization warms up the predictor via automatic prompt optimization and then optimizes each building block independently, storing validation performance for influence scoring; (2) Workflow topology optimization computes influence scores I_ai=E(a*_i)/E(a*_0), prunes via softmax-weighted probabilities with temperature 0.05, and samples from the pruned space with budget constraints (max 10 agents); (3) Workflow-level prompt optimization performs joint prompt optimization on the best-found topology. The search space includes five building blocks (aggregate, reflect, debate, summarize, tool-use) with configurable agent counts. The framework uses Gemini 1.5 Pro/Flash-002, Claude 3.5 Sonnet, and Mistral-Nemo-12B models with validation sets of 50-200 samples per task.

## Key Results
- Prompt-optimized agents outperform self-consistency, self-refine, and debate scaling in token-effectiveness, with self-consistency saturating earlier
- Pruning the topology search space to influential building blocks improves search efficiency while maintaining performance
- Workflow-level prompt optimization captures inter-agent dependencies, adding ~2% average gain over topology optimization alone
- MASS achieves state-of-the-art performance across reasoning, multi-hop understanding, and code generation tasks

## Why This Works (Mechanism)

### Mechanism 1
Optimizing individual agent prompts before composing them into a multi-agent system yields greater performance gains than scaling agent count with default prompts. Block-level prompt optimization applies automatic prompt optimization (APO) to each agent type independently, warming up the predictor with optimized instructions and exemplars. This reduces prompt sensitivity compounding when agents cascade.

### Mechanism 2
Pruning the topology search space to "influential" building blocks improves search efficiency and final performance. MASS measures incremental influence I_ai = E(a*_i) / E(a*_0) for each topology dimension, then samples from a softmax-weighted pruned space where low-influence dimensions are rejected probabilistically. This removes decremental topologies that would otherwise waste budget or degrade performance.

### Mechanism 3
Joint workflow-level prompt optimization after topology discovery captures inter-agent dependencies that block-level optimization misses. After selecting the best topology from Stage 2, MASS treats the entire MAS as a unified system and re-optimizes all agent prompts simultaneously, allowing prompts to adapt to their specific upstream/downstream roles.

## Foundational Learning

- **Concept: Automatic Prompt Optimization (APO)**
  - Why needed here: MASS delegates prompt search to an APO module (MIPRO in experiments). Understanding instruction vs. exemplar optimization helps diagnose where gains come from.
  - Quick check question: Can you explain why optimizing both instructions and few-shot exemplars jointly might outperform optimizing either alone?

- **Concept: Combinatorial Search Space Design**
  - Why needed here: The topology space (aggregate, reflect, debate, summarize, tool-use) defines what MASS can discover. Poor space design limits ceiling performance regardless of search algorithm.
  - Quick check question: Given a new task, how would you decide which topology building blocks to include vs. exclude?

- **Concept: Agent Interdependence in Cascades**
  - Why needed here: Stages 1 and 3 differ because Stage 3 accounts for how one agent's output becomes another's input. Understanding this helps you know when Stage 3 is worth the compute.
  - Quick check question: In a debate topology, what information does each debator receive that a parallel predictor does not?

## Architecture Onboarding

- **Component map:** Block-level PO (Stage 1) -> Influence scorer -> Pruned sampler -> Workflow constructor -> Workflow-level PO (Stage 3)
- **Critical path:** Stage 1 → influence computation → Stage 2 (pruned sampling + evaluation) → Stage 3 on best topology. Skipping Stage 1 weakens Stage 2's search space; skipping Stage 3 leaves interdependence gains on the table.
- **Design tradeoffs:** Larger search space (more blocks) vs. compute budget: More blocks increase Stage 1 cost linearly and expand Stage 2 combinatorics; Aggressive pruning (low temperature τ) vs. exploration: Lower τ sharpens selection but may miss unconventional topologies; Joint PO rounds vs. marginal gains: Stage 3 shows diminishing returns after convergence; budget accordingly.
- **Failure signatures:** Stage 1 shows no improvement over base: Check APO hyperparameters (candidates, rounds) or validation set quality; Stage 2 all topologies perform similarly: Influence scores may be uniform; verify validation metric discriminates well; Stage 3 degrades performance: Overfitting to validation set or prompt conflicts between agents; reduce optimization rounds.
- **First 3 experiments:** 1) Reproduce Figure 2 on your task: Compare single-agent APO vs. self-consistency scaling to validate prompt-first hypothesis; 2) Ablate pruning: Run Stage 2 with and without influence-based pruning on a held-out topology dimension to quantify efficiency gain; 3) Stage 3 sensitivity: Run workflow-level PO with 1, 5, and 10 instruction candidates to find compute/performance knee point.

## Open Questions the Paper Calls Out

### Open Question 1
Can feedback-based prompt optimizers (e.g., TextGrad) improve the sample efficiency of MASS compared to the current MIPRO implementation? The current implementation relies on MIPRO for joint instruction and exemplar optimization, but has not been tested against optimizers that utilize textual gradients from error logs.

### Open Question 2
Does incorporating sparse communication topologies yield efficiency gains over the fully-connected debate blocks currently used in MASS? The MASS search space currently defines debate as a fully-connected topology, leaving the potential benefits of sparse structures unexplored.

### Open Question 3
Can advanced search algorithms like Bayesian optimization improve the sample efficiency of MASS when facing more complex design spaces? The current topology optimizer uses a simple rule-based construction and rejection sampling, which may not scale efficiently as the number of agentic dimensions grows.

## Limitations
- The influence-based pruning mechanism's robustness across domains is unclear, as debate topology performance varies widely (0-3%) depending on task.
- Stage 3 (workflow-level prompt optimization) shows diminishing returns (~2% gain) but adds significant compute without analysis of cost-effectiveness.
- The paper assumes the [summarize→reflect→debate→aggregate] ordering is optimal for all tasks without testing alternative sequences.

## Confidence

- **High confidence:** Mechanism 1 (prompt optimization beats agent scaling) - supported by direct comparisons in Figure 2 and consistent across tasks.
- **Medium confidence:** Mechanism 2 (pruning efficiency) - strong evidence from HotpotQA but limited cross-task validation.
- **Low confidence:** Mechanism 3 (interdependence optimization) - minimal ablation studies and weakest corpus support.

## Next Checks
1. Run a controlled experiment testing MASS without Stage 3 to quantify the actual compute cost vs. 2% average performance gain across all tasks.
2. Test alternative topological orderings (e.g., [debate→summarize→reflect→aggregate]) on at least 3 diverse tasks to validate the assumed optimal sequence.
3. Implement MASS with uniform sampling (no pruning) on a subset of tasks to measure the claimed acceleration in search efficiency and verify pruned solutions match or exceed full-space performance.