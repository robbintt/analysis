---
ver: rpa2
title: 'Self-HarmLLM: Can Large Language Model Harm Itself?'
arxiv_id: '2511.08597'
source_url: https://arxiv.org/abs/2511.08597
tags:
- evaluation
- success
- harmful
- jailbreak
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether an LLM can generate a mitigated
  query that bypasses its own guardrails when reused as input. The proposed Self-HarmLLM
  scenario uses an LLM to transform a harmful query into a Mitigated Harmful Query
  (MHQ) that preserves the original intent but obscures its harmful nature.
---

# Self-HarmLLM: Can Large Language Model Harm Itself?

## Quick Facts
- arXiv ID: 2511.08597
- Source URL: https://arxiv.org/abs/2511.08597
- Authors: Heehwan Kim; Sungjune Park; Daeseon Choi
- Reference count: 35
- Primary result: LLM-generated mitigated harmful queries can bypass guardrails with up to 98% success in base condition and 33% in few-shot condition

## Executive Summary
This paper introduces the Self-HarmLLM scenario, where an LLM transforms harmful queries into obfuscated versions that bypass its own guardrails when reused as input. The attack involves two sessions: one generates a Mitigated Harmful Query (MHQ) that preserves malicious intent while obscuring harmful patterns, and another evaluates if the obfuscated query produces a harmful response. Experiments across three models show significant vulnerability to this attack, with automated evaluation overestimating success by 52% compared to human assessment, highlighting the need for hybrid evaluation approaches.

## Method Summary
The study uses a two-session pipeline where Session A transforms harmful queries into MHQs using system prompts (Zero-shot: 4-step instruction without examples; Few-shot: same with 3 examples), and Session B evaluates if these MHQs bypass guardrails to produce harmful responses. The research tests 110 harmful queries across 11 categories from the HarmfulQA dataset, evaluating both transformation success rate (TSR) and jailbreak success rate (JSR) using both prefix-based automated detection and human binary assessment.

## Key Results
- MHQ transformation success: Zero-shot (29-52% TSR), Few-shot (46-65% TSR) across models
- Jailbreak success: GPT-3.5-turbo (33% JSR few-shot), LLaMA3-8B-instruct (41% JSR few-shot), DeepSeek-R1-Distill-Qwen-7B (98% JSR base condition)
- Automated evaluation overestimated jailbreak success by 52% on average compared to human evaluation
- DeepSeek-R1-Distill-Qwen-7B showed weak guardrails even without mitigation (44% Base JSR)

## Why This Works (Mechanism)

### Mechanism 1: Intent-Preserving Query Obfuscation
An LLM can transform harmful queries into mitigated versions that preserve malicious intent while obscuring explicit harmful signals that guardrails detect. The model rephrases harmful content using softer, more indirect language that retains semantic equivalence but lacks overt harmful keywords.

### Mechanism 2: Session State Isolation Exploitation
Independent conversational sessions create a structural blind spot where guardrails fail to track context across sessions. Session A generates the MHQ under mitigation prompts, while Session B, with no shared memory, receives the MHQ as standalone input without awareness of its transformation history.

### Mechanism 3: Guardrail Detection Gap on Ambiguous Semantics
Current guardrails, trained on explicit harmful patterns, fail to detect harmful intent when obscured through linguistic ambiguity. MHQs exploit this by replacing direct harmful expressions with euphemisms and contextual framing that reduce direct harmful signal strength while preserving actionable content.

## Foundational Learning

### Concept: Jailbreak Attacks and Guardrail Mechanisms
- **Why needed here**: Understanding how guardrails detect harmful content and how jailbreaks evade them is essential to grasp why Self-HarmLLM works.
- **Quick check question**: Can you explain why prefix-based detection fails when a model refuses indirectly without standard phrases like "I cannot assist"?

### Concept: Session-Based LLM Architecture
- **Why needed here**: The attack relies on session isolation. Engineers must understand how commercial APIs handle session state, context windows, and memory persistence to replicate and defend against this scenario.
- **Quick check question**: In a typical commercial LLM API, what happens to conversation context when a new session is initiated?

### Concept: Automated vs. Human Evaluation Trade-offs
- **Why needed here**: The paper demonstrates that prefix-based automated evaluation overestimates jailbreak success by ~52% compared to human evaluation.
- **Quick check question**: Why might a model generate a harmful response without using typical refusal prefixes, and how would this affect automated detection?

## Architecture Onboarding

### Component Map
Original Harmful Query (HQ) -> Mitigation Session A (system prompt) -> Mitigated Harmful Query (MHQ) -> Target Session B -> Model Response -> Evaluation Layer (prefix-based + human evaluation)

### Critical Path
1. **Transformation Success**: Session A must generate an MHQ that preserves intent while obscuring harmfulness (Zero-shot: 29-52%, Few-shot: 46-65% across models)
2. **Jailbreak Success**: Session B must fail to detect the obfuscated harmfulness and generate a harmful response (Zero-shot: 5-21%, Few-shot: 15-33% per human eval)
3. **Evaluation Accuracy**: Human evaluation required to avoid the ~52% overestimation from prefix-based methods

### Design Trade-offs
- **Zero-shot vs. Few-shot Mitigation**: Few-shot increases transformation success but jailbreak success varies by model
- **Automated vs. Human Evaluation**: Automated evaluation is scalable but unreliable for semantic harmfulness
- **Model Selection**: DeepSeek-R1-Distill-Qwen-7B shows weak guardrails even in Base condition, making it a poor candidate for safety-critical applications

### Failure Signatures
- **Low Transformation Success**: LLaMA3-8B-instruct Zero-shot (11.82% TSR human eval) suggests over-mitigation
- **High Base Jailbreak Rate**: DeepSeek-R1-Distill-Qwen-7B (43.63% Base JSR human eval) indicates inherently weak guardrails
- **Prefix-Based Inflation**: Automated evaluation showing >90% JSR while human eval shows <25% indicates models refusing without standard prefixes

### First 3 Experiments
1. **Replicate Zero-shot MHQ Generation**: Use system prompt on 10 harmful queries from different categories. Manually verify if transformed queries preserve intent while obscuring harmfulness.
2. **Test Cross-Session Injection**: Feed generated MHQs into fresh session of same model. Use both prefix-based and human evaluation to measure jailbreak success.
3. **Guardrail Robustness Comparison**: Run same attack pipeline on three models (GPT-3.5-turbo, LLaMA3-8B, DeepSeek). Compare Base vs. Zero-shot vs. Few-shot conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Self-HarmLLM scenario perform in multi-turn conversational contexts compared to the single-query setting tested?
- **Basis in paper**: Section 6.3 lists "Conversational Context Experimentation" as necessary future work, noting real-world attacks may differ in multi-turn settings.
- **Why unresolved**: Current study is limited to single-query inputs without analyzing how context accumulation affects guardrail detection.
- **What evidence would resolve it**: Experiments measuring transformation and jailbreak success rates within continuous, multi-turn dialogue histories.

### Open Question 2
- **Question**: To what extent do model scale and reasoning capabilities correlate with vulnerability to Self-HarmLLM attacks?
- **Basis in paper**: Authors call for "Broader Model Spectrum" analysis in Section 6.3 to verify how scenario varies with model size and architecture.
- **Why unresolved**: Study tested only three models and cannot determine if DeepSeek-R1-Distill-Qwen-7B's high vulnerability was due to distillation, reasoning focus, or size.
- **What evidence would resolve it**: Comparative scaling laws analysis across different parameter sizes and architectures within same model family.

### Open Question 3
- **Question**: Can hybrid evaluation approaches effectively close the 52% accuracy gap observed between prefix-based automated evaluation and human judgment?
- **Basis in paper**: Authors state in Abstract and Conclusion that automated evaluation is insufficient and suggest "establishment of a more robust evaluation methodology."
- **Why unresolved**: Prefix-based methods consistently overestimated success by failing to capture semantic context, while human evaluation is resource-intensive.
- **What evidence would resolve it**: Development of hybrid evaluation metric that correlates strongly with human judgments while maintaining scalability.

### Open Question 4
- **Question**: How do specific prompt components, such as number or type of few-shot examples, quantitatively influence success of mitigation strategy?
- **Basis in paper**: Section 6.3 suggests "Mitigation Strategy Analysis" to evaluate how prompt structure and example types influence attack likelihood.
- **Why unresolved**: Study compared Zero-shot and Few-shot broadly but did not isolate specific variables within prompt that drive transformation success.
- **What evidence would resolve it**: Ablation studies on mitigation system prompts to identify which structural elements most effectively bypass guardrails.

## Limitations
- Reliance on human evaluation introduces subjectivity and scalability constraints
- Specific harmful queries from HarmfulQA dataset not specified, requiring researcher filtering
- Attack assumes session isolation, which may not hold in all deployment scenarios with persistent user memory
- Model sampling parameters (temperature, top-p, max tokens) not specified for all three models

## Confidence
- **High Confidence**: The Self-HarmLLM scenario concept is clearly defined and experimentally validated
- **Medium Confidence**: Quantitative results are reported but depend on specific harmful queries used and human evaluation protocols
- **Low Confidence**: Generalizability to other LLM architectures, guardrail implementations, or real-world conversational systems is uncertain

## Next Checks
1. **Independent Query Set Verification**: Replicate study using different subset of harmful queries from HarmfulQA or another benchmark dataset to verify TSR (29-65%) and JSR (5-98%) ranges are consistent
2. **Cross-Session State Testing**: Test whether attack succeeds when guardrails implement cross-session context tracking such as user authentication tokens or conversation history persistence
3. **Semantic Guardrail Comparison**: Evaluate whether guardrails trained with semantic intent detection show significantly reduced susceptibility to MHQ-based jailbreaks