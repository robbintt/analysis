---
ver: rpa2
title: Nonparametric learning of stochastic differential equations from sparse and
  noisy data
arxiv_id: '2508.11597'
source_url: https://arxiv.org/abs/2508.11597
tags:
- drift
- function
- algorithm
- given
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for learning drift functions
  of stochastic differential equations (SDEs) from sparse, noisy observations using
  reproducing kernel Hilbert spaces (RKHS) and an Expectation-Maximization (EM) algorithm
  with Sequential Monte Carlo (SMC) approximations. The method avoids parametric assumptions
  on the drift function, making it suitable for complex systems where prior knowledge
  is limited.
---

# Nonparametric learning of stochastic differential equations from sparse and noisy data

## Quick Facts
- **arXiv ID:** 2508.11597
- **Source URL:** https://arxiv.org/abs/2508.11597
- **Reference count:** 40
- **Primary result:** Introduces an EM algorithm with SMC approximations to learn drift functions of SDEs from sparse, noisy observations in an RKHS framework, with theoretical convergence guarantees and numerical validation on 1D-3D systems.

## Executive Summary
This paper develops a nonparametric framework for learning drift functions of stochastic differential equations from sparse and noisy observations. The method uses a Reproducing Kernel Hilbert Space (RKHS) to represent the drift function and employs an Expectation-Maximization (EM) algorithm with Sequential Monte Carlo (SMC) approximations to handle the intractable filtering distribution. The approach avoids parametric assumptions on the drift function, making it suitable for complex systems where prior knowledge is limited. The paper establishes convergence results for both exact and approximate EM sequences and develops a Bayesian variant with shrinkage priors to control model complexity. Numerical experiments on one- and multi-dimensional SDEs demonstrate accurate drift estimation and good agreement between true and estimated stationary distributions.

## Method Summary
The method learns the drift function $b(x)$ of an SDE from sparse, noisy observations using an EM algorithm where the E-step approximates the filtering distribution via SMC with a linear SDE proposal, and the M-step solves a penalized likelihood functional in an RKHS via a generalized representer theorem. The RKHS formulation converts the infinite-dimensional optimization into a finite-dimensional kernel expansion, while the SMC with linear proposal provides tractable particle weights even for nonlinear drift functions. A Bayesian variant with shrinkage priors controls model complexity by regularizing the kernel coefficients.

## Key Results
- The EM algorithm with SMC approximations achieves accurate drift estimation with low MSE and Kolmogorov metrics across different levels of data sparsity (1/3 to 1/20 subsampling).
- The method correctly recovers drift functions and stationary distributions in one- and multi-dimensional SDEs (Double-well, Gamma, and Ornstein-Uhlenbeck type models).
- The linear SDE proposal outperforms standard diffusion bridge proposals in nonlinear settings by providing analytically tractable Gaussian transition kernels.
- The RKHS representer theorem enables efficient finite-dimensional optimization while maintaining nonparametric flexibility.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The EM algorithm renders the intractable SDE likelihood optimizable by decomposing it into alternating expectation and maximization steps.
- **Mechanism:** The filtering distribution ν_b(X|Y) is unknown because the likelihood ψ_Y(y|b) requires integrating over all unobserved trajectory segments. The EM algorithm sidesteps this by: (E-step) computing the expected complete-data log-likelihood under the current estimate of ν_b, then (M-step) minimizing a penalized risk functional R(b, η) that upper-bounds the original loss. The identity L(b) = R(b, η) − R(η∥ν_b) − H(η) in [Section 3.1, equation 3.6] shows that as η approaches ν_b, the surrogate loss converges to the true loss.
- **Core assumption:** The filtering distribution can be approximated sufficiently well at each iteration; otherwise, the surrogate optimization diverges from the true objective.
- **Evidence anchors:**
  - [abstract]: "We develop an Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte Carlo (SMC) method to approximate the filtering distribution"
  - [Section 3.1, Definition 3.1]: EM-sequence defined recursively as b_k = Φ(ν_{b_{k-1}}(·|y))
  - [corpus]: SOCK method (arxiv:2505.11622) also uses kernel-based drift estimation but employs occupation kernels instead of EM-SMC; suggests EM is one viable path for tractability, not the only one.
- **Break condition:** If particle degeneracy in SMC becomes severe and the E-step approximation collapses, the EM iterates may diverge or converge to poor local minima.

### Mechanism 2
- **Claim:** SMC with a linear SDE proposal approximates the filtering distribution more effectively than standard diffusion bridge proposals in nonlinear settings.
- **Mechanism:** Standard proposals (e.g., Durham-Gallant) assume approximately constant drift, failing when b is strongly nonlinear. The paper's proposal (Section 3.3, Lemma 3.14) uses a first-order linear SDE approximation: b(X(r)) ≈ b(X(s_i)) + D_b(X(s_i))(X(r) − X(s_i)), yielding Gaussian transition kernels with analytically tractable means μ_{m,i} and covariances S_{m,i}. This produces proposal density q_{m,i}(x_i|x_{i-1}, y_{m+1}) that conditions on future observations, improving particle quality.
- **Core assumption:** The linear approximation of b remains valid over the interval (s_i, s_{nm+1}]; for highly irregular drift, higher-order approximations may be needed.
- **Evidence anchors:**
  - [Section 3.3]: "A first-order linear SDE-approximation of X leads to ˜q^02_{m,i} given by (3.34) with μ^02_{m,i} and S^02_{m,i}"
  - [Section 3.3, Lemma 3.14]: Explicit Gaussian form of the proposal
  - [corpus]: Neural Stochastic Flows (arxiv:2510.25769) proposes solver-free SDE inference via neural networks, indicating proposal design remains an active research area without consensus on best approach.
- **Break condition:** If the time gap between observations (n_{m+1} − n_m)Δ is large, the linear approximation degrades, and particle weights may collapse.

### Mechanism 3
- **Claim:** The RKHS formulation converts an infinite-dimensional optimization into a finite-dimensional kernel expansion via the representer theorem.
- **Mechanism:** The drift function b lives in an infinite-dimensional function space H_κ. Penalized empirical risk minimization over H_κ would be intractable, but Theorem 3.13 shows the minimizer admits a finite representation: b_k(x) = Σ_{l,n} κ(x, X̃^{(l)}(s_n)) β^{(l,k)}_n, where coefficients β are solved via a linear system involving the kernel matrix K and weight matrix D. This is the generalized representer theorem for vector-valued functions.
- **Core assumption:** The kernel κ is universal enough to approximate the true drift; a poor kernel choice constrains expressivity.
- **Evidence anchors:**
  - [Section 3.2, Theorem 3.13]: "the minimizer b_k is given by [finite kernel expansion]"
  - [Section 3.2, equation 3.27]: Closed-form expression for β via matrix inversion
  - [corpus]: SOCK method (arxiv:2505.11622) also leverages kernel methods for SDE learning, corroborating RKHS as a productive framework.
- **Break condition:** If the number of particle-path points L·N_0 grows large, the kernel matrix K becomes ill-conditioned or computationally prohibitive; shrinkage priors in Algorithm 3 address this but add hyperparameter sensitivity.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** The drift function b is unknown and potentially complex; RKHS provides a function space where optimization becomes tractable via the representer theorem. Without this, you cannot convert the infinite-dimensional M-step into a finite-dimensional problem.
  - **Quick check question:** Can you explain why ⟨f, κ(·, x)⟩ = f(x) is called the "reproducing property" and how it enables finite representations of solutions?

- **Concept: Expectation-Maximization (EM) for latent variable models**
  - **Why needed here:** The SDE trajectory X[0,T] is only partially observed; EM handles missing data by iterating between imputing the latent distribution (E-step) and optimizing parameters given the imputation (M-step).
  - **Quick check question:** Why does EM guarantee monotonic increase in likelihood (or decrease in loss) only for the *exact* EM sequence, and what happens when the E-step is approximated?

- **Concept: Sequential Monte Carlo (SMC) / Particle Filtering**
  - **Why needed here:** The filtering distribution ν_b(X|Y) is analytically intractable; SMC approximates it with weighted particles, enabling Monte Carlo estimates of E-step expectations.
  - **Quick check question:** What is particle degeneracy, and why does resampling at observation times mitigate it?

## Architecture Onboarding

- **Component map:** Data layer (sparse observations Y) -> Latent layer (SMC particle-paths X̃) -> Optimization layer (EM-RKHS) -> Regularization layer (Bayesian shrinkage)

- **Critical path:**
  1. Initialize drift b_0 (e.g., zero function or simple parametric guess)
  2. Run SMC with current b_{k-1} to generate L weighted particle-paths X̃^{(l)}_{[0,T]}
  3. Construct kernel matrix K from particle-path points and weight matrix D from SMC weights
  4. Solve β^{(k)} = C^{-1} K D ϑ via linear system (equation 3.27)
  5. Update b_k via kernel expansion (equation 3.26)
  6. Repeat until convergence (loss stabilizes or iterate norm change < ε)

- **Design tradeoffs:**
  - **Particle count L:** More particles improve E-step accuracy but increase kernel matrix size (O(L²N₀²)). Paper uses L=6 particles with top-3 weighting for efficiency (Section 3.5).
  - **Discretization step Δ:** Smaller Δ improves trajectory fidelity but increases N_0 and matrix dimension.
  - **Kernel choice:** Gaussian kernel used in experiments; bandwidth parameter affects smoothness and conditioning.
  - **Standard EM vs Bayesian-EM:** Bayesian variant adds shrinkage but requires tuning inverse-gamma hyperparameters (a, b).

- **Failure signatures:**
  - **SMC weight collapse:** All but a few particles have negligible weight; E-step estimate becomes unreliable. Mitigate by checking ESS < L/2 and adjusting proposal.
  - **Kernel matrix singularity:** If particle-paths cluster or L·N_0 is too large relative to data, K may be ill-conditioned. Use regularization λ or reduce particles.
  - **Non-convergence:** Loss oscillates or diverges; may indicate proposal mismatch or insufficient iterations. Theorem 3.11 guarantees convergence only under KL-divergence approximation, which SMC does not strictly satisfy.

- **First 3 experiments:**
  1. **Sanity check on linear SDE with known drift:** Generate data from dX = -θX dt + σdW, estimate drift using Algorithm 2, and verify recovered drift matches -θX. Confirms pipeline correctness.
  2. **Ablation on proposal quality:** Compare Durham-Gallant proposal vs linear SDE proposal (Section 3.3) on double-well potential (Model 1). Measure particle weight variance and MSE of recovered drift.
  3. **Robustness to observation sparsity:** Vary the subsampling fraction (1/3, 1/5, 1/10, 1/20 as in Table 1) and track MSE and Kolmogorov metric. Identifies the data regime where performance degrades sharply.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SMC-EM sequence converge when the particle approximation does not satisfy the KL-divergence convergence required by Theorem 3.11?
- Basis in paper: [explicit] The authors state that the SMC approximation $\hat{\nu}^{SMC,L}$ does not converge in KL-divergence, meaning "Theorem 3.11 technically cannot be directly applied to the SMC-EM sequence."
- Why unresolved: The theoretical guarantees rely on strong convergence (KL-divergence) of the filtering distribution approximation, which standard SMC with resampling does not provide, creating a gap between the theory and the implemented Algorithm 2.
- What evidence would resolve it: Proof of convergence under weaker metrics (e.g., Total Variation) or the development of a specific SMC resampling scheme that guarantees KL-divergence convergence.

### Open Question 2
- Question: Can verifiable sufficient conditions be established to ensure the stationary set $S$ is a singleton?
- Basis in paper: [explicit] The paper notes that convergence to the unique penalized MLE requires the stationary set $S$ to be a singleton, a "condition which unfortunately is hard to verify in practice."
- Why unresolved: Without verifying this condition, the EM sequence is only guaranteed to approach the set $S$, potentially converging to a local minimum or cycle depending on the initialization $b_0$.
- What evidence would resolve it: Theoretical analysis identifying properties of the kernel $\kappa$ or the observation model that enforce the uniqueness of the fixed point $\Phi(\nu_b)$.

### Open Question 3
- Question: Is the first-order linear SDE proposal computationally tractable and statistically efficient in high-dimensional state spaces?
- Basis in paper: [inferred] The numerical experiments are limited to low dimensions ($d \in \{1, 2, 3\}$), and the proposal requires computing matrix exponentials $\exp(D_b t)$ and Jacobians at every step, which scales cubically with dimension.
- Why unresolved: The computational cost of the linear SDE proposal grows rapidly with dimension, and particle filters generally suffer from degeneracy in high dimensions, yet the paper only demonstrates scalability with respect to time horizons.
- What evidence would resolve it: Simulation studies on systems with dimension $d > 10$ showing stable Effective Sample Size (ESS) and bounded error.

## Limitations
- The theoretical convergence guarantees apply only to exact EM sequences, while the practical implementation uses SMC approximations that violate KL-divergence convergence conditions.
- The method assumes the true drift function can be well-approximated by the chosen RKHS, which may fail for highly irregular or discontinuous drift functions.
- Computational scalability is limited by the kernel matrix size, which grows quadratically with particle count and discretization points.

## Confidence
- **Theoretical convergence (exact EM):** High confidence in Theorem 3.11 guarantees for exact EM sequences.
- **Practical convergence (approximate EM):** Medium confidence due to the gap between theoretical assumptions and SMC implementation.
- **Method effectiveness:** Medium-High confidence for well-behaved SDEs with moderate nonlinearity; drops to Medium for highly nonlinear systems or extremely sparse observations.
- **Scalability:** Low-Medium confidence beyond low-dimensional systems due to computational complexity.

## Next Checks
1. **Proposal Quality Sensitivity Analysis:** Systematically compare the linear SDE proposal against exact proposals on increasingly nonlinear drift functions (e.g., quintic, rational drift terms) to quantify the degradation in E-step accuracy and its impact on M-step drift estimation.
2. **Convergence Robustness Testing:** Run the EM algorithm with varying initializations (zero drift, random drift, parametric approximations) and particle counts (L=3, 6, 12) to assess sensitivity to initialization and the stability of the convergence behavior under SMC approximations.
3. **RKHS Expressivity Validation:** Test the method on synthetic SDEs with known drift functions that deliberately violate the smoothness assumptions of the Gaussian kernel (e.g., piecewise constant drift, discontinuous drift) to determine the practical limits of the representer theorem approximation.