---
ver: rpa2
title: 'RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large
  Language Models'
arxiv_id: '2505.21409'
source_url: https://arxiv.org/abs/2505.21409
tags:
- query
- llms
- attributes
- table
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RelationalFactQA, a benchmark for evaluating
  LLMs' ability to generate structured, multi-record tabular outputs from parametric
  knowledge. Existing benchmarks focus on single-value factuality, but the paper argues
  that retrieving tabular data from parametric memory is a distinct and more challenging
  capability.
---

# RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models

## Quick Facts
- arXiv ID: 2505.21409
- Source URL: https://arxiv.org/abs/2505.21409
- Reference count: 40
- The paper introduces RelationalFactQA, a benchmark for evaluating LLMs' ability to generate structured, multi-record tabular outputs from parametric knowledge.

## Executive Summary
RelationalFactQA addresses a critical gap in LLM evaluation by focusing on structured tabular fact retrieval rather than single-value factuality. The benchmark introduces 696 questions across nine domains, each paired with SQL equivalents and gold-standard table answers, to test whether models can synthesize multi-record factual knowledge. Experiments with nine LLMs (7B–235B parameters) reveal that even state-of-the-art models struggle with this task, rarely exceeding 25% tuple accuracy. Performance degrades significantly with output size and query complexity, particularly for numerical conditions, highlighting fundamental limitations in LLMs' ability to generate structured factual outputs.

## Method Summary
The benchmark design pairs each natural language question with an SQL query and a gold-standard table answer, creating a multi-modal evaluation framework. The study tests nine LLMs across three retrieval methods (natural language, SQL, and Chain-of-Thought prompting) to assess their ability to generate accurate multi-record tabular outputs. The evaluation focuses on exact match accuracy for multi-record outputs, providing a stringent measure of factuality in structured contexts.

## Key Results
- State-of-the-art LLMs rarely exceed 25% tuple accuracy on multi-record tabular retrieval tasks
- Performance degrades significantly with increased output size and query complexity
- Numerical conditions prove particularly challenging, with accuracy dropping below 10% for complex queries

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its multi-modal evaluation framework that tests LLMs' ability to translate natural language questions into structured SQL queries and generate accurate table outputs. By requiring both query understanding and structured output generation, the benchmark captures the full complexity of tabular fact retrieval from parametric memory.

## Foundational Learning
- **Structured Output Generation**: Models must convert parametric knowledge into organized table formats
  - Why needed: Single-value factuality doesn't test models' ability to synthesize and structure multiple related facts
  - Quick check: Can the model generate a 3x3 table with mixed data types from memory?

- **Query-to-SQL Translation**: Understanding natural language questions and mapping them to structured query logic
  - Why needed: Tests whether models can parse complex relational requirements from free text
  - Quick check: Does the model correctly identify filtering conditions from ambiguous natural language?

- **Multi-Record Fact Synthesis**: Combining multiple facts from parametric memory into coherent table structures
  - Why needed: Real-world knowledge often involves relationships between multiple entities
  - Quick check: Can the model retrieve and organize 10+ related facts into a single coherent table?

## Architecture Onboarding

Component map:
Natural Language Question -> SQL Query Generation -> Table Output Generation -> Exact Match Evaluation

Critical path:
Question understanding → SQL formulation → Structured output generation → Answer validation

Design tradeoffs:
- Exact match evaluation ensures high precision but may be overly stringent for semantically equivalent outputs
- Single-table focus simplifies benchmark design but excludes complex multi-table scenarios
- Natural language vs. SQL prompting balances accessibility with technical precision

Failure signatures:
- Output structure mismatches (wrong column order, missing headers)
- Semantic errors (correct structure but wrong factual content)
- Completeness failures (incomplete row sets)
- Type mismatches (numerical vs. categorical data handling)

First experiments:
1. Test baseline performance on single-record queries to establish lower-bound accuracy
2. Evaluate model performance on progressively larger output tables (1-row → 10-row)
3. Compare natural language vs. SQL prompting effectiveness on numerical condition queries

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark coverage across nine domains may not represent full diversity of real-world tabular data
- Single-table query focus excludes complex multi-table relational scenarios
- Exact match accuracy evaluation may be overly stringent for semantically equivalent valid answers

## Confidence
- High confidence: Existing benchmarks inadequately evaluate structured tabular retrieval
- Medium confidence: Multi-record tabular retrieval is a distinct and more challenging capability
- Low confidence: Claims about "critical limitations" in LLMs' ability to synthesize structured factual knowledge

## Next Checks
1. Re-evaluate using relaxed matching criteria and semantic similarity measures to assess performance under different evaluation frameworks
2. Develop and test multi-table queries to evaluate performance on truly relational data retrieval tasks
3. Test models on out-of-distribution domains to determine whether limitations are domain-specific or fundamental