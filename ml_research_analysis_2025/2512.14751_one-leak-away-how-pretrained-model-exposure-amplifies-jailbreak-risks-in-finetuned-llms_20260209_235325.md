---
ver: rpa2
title: 'One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in
  Finetuned LLMs'
arxiv_id: '2512.14751'
source_url: https://arxiv.org/abs/2512.14751
tags:
- pretrained
- llms
- finetuned
- jailbreak
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the security risks posed by finetuned large
  language models (LLMs) inheriting jailbreak vulnerabilities from their pretrained
  sources. In a realistic pretrain-to-finetune threat model, the attacker has white-box
  access to a publicly released pretrained model and only black-box access to its
  finetuned derivatives.
---

# One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs

## Quick Facts
- arXiv ID: 2512.14751
- Source URL: https://arxiv.org/abs/2512.14751
- Authors: Yixin Tan; Zhe Yu; Jun Sakuma
- Reference count: 37
- Key outcome: Jailbreak prompts optimized on pretrained models transfer most effectively to finetuned variants, revealing inherited vulnerabilities that can be exploited using probe-guided optimization

## Executive Summary
This paper investigates how jailbreak vulnerabilities in finetuned large language models (LLMs) can be inherited from their publicly released pretrained sources. Through empirical analysis, the authors demonstrate that adversarial prompts optimized on pretrained models achieve significantly higher transfer success rates to finetuned variants compared to prompts from other sources. The key insight is that transferable jailbreak prompts exhibit linearly separable patterns in the pretrained model's hidden representations, suggesting that transferability structure is encoded in pretrained features. Based on this observation, the authors propose Probe-Guided Projection (PGP), an attack method that uses linear probe-derived directions to guide adversarial prompt optimization, achieving substantially higher transferability success rates even against safety-finetuned models.

## Method Summary
The method involves training linear SVM probes on hidden states from a pretrained LLM to classify transferable versus untransferable jailbreak prompts. These probes identify directionality in the representation space that correlates with transferability success. PGP then optimizes adversarial suffixes using a joint objective that combines success on the pretrained model with alignment to the probe-identified transferability direction. The attack uses GCG-style discrete gradient-based token replacement with a suffix length of 20, 500 optimization steps, and batch size of 512. Evaluation measures Transfer Success Rate (TSR) and Attack Success Rate (ASR) using an LLM classifier judge based on the Harmbench framework.

## Key Results
- Jailbreak prompts optimized on the correct pretrained model achieve 54-69% TSR to finetuned variants, while using wrong pretrained models drops TSR to 0-4%
- Linear probes trained on intermediate layer representations can distinguish transferable from untransferable prompts with >80% accuracy
- PGP achieves significantly higher transferability success rates than existing methods, with linear SVM probes showing 59% TSR versus 2-3% for kernel SVMs
- Safety-finetuning with 2000 examples only modestly reduces TSR, demonstrating limited effectiveness of data-level defenses

## Why This Works (Mechanism)

### Mechanism 1: Transferability is Encoded in Pretrained Representations
Jailbreak prompts that transfer successfully to finetuned models exhibit linearly separable patterns in the pretrained model's hidden states. Linear probes trained on intermediate layer representations can distinguish transferable from untransferable prompts with >80% accuracy, suggesting the pretrained feature space already encodes transfer-relevant structure. The probe weight vector defines a direction in representation space that correlates with transferability. This works because finetuning preserves enough representational structure from pretraining that features predictive of transfer remain detectable.

### Mechanism 2: Pretrained-to-Finetune Vulnerability Inheritance via Shared Representations
Adversarial prompts optimized on the pretrained model transfer more effectively to its finetuned variants than prompts optimized on other models because finetuning typically modifies later layers while preserving earlier representational structure. Jailbreak prompts that exploit features in preserved layers will continue to succeed post-finetuning. This inheritance pattern is strongest when the attacker correctly identifies the pretrained source model, as provenance analysis shows >90% accuracy in matching models to their sources.

### Mechanism 3: Probe-Guided Projection Steers Optimization Toward Transferable Subspaces
PGP explicitly constrains adversarial perturbations to align with probe-identified transferability directions by maximizing the projection of the perturbation vector onto the normalized probe weight direction. The joint objective balances success on the pretrained model with transfer alignment via λ weighting. This approach works because the linear probe accurately captures transfer-relevant structure, and linearity preserves feature geometry that kernel methods distort. The method pushes prompt representations across the transferability decision boundary identified by the probe.

## Foundational Learning

- **Linear Probing for Representation Analysis**: Needed to understand how probes extract transferability-relevant directions from hidden states. Quick check: Given hidden states from layer 16 of a 32-layer model, can you train a linear SVM to classify two prompt categories and extract the weight vector as a direction?

- **GCG (Greedy Coordinate Gradient) Jailbreak Optimization**: Needed as PGP builds on GCG's discrete token optimization framework. Quick check: How does GCG handle the non-differentiability of token selection? What role does the suffix length parameter play?

- **Transferability in Adversarial Machine Learning**: Needed to understand why adversarial examples optimized on one model often succeed on another. Quick check: Why do adversarial examples optimized on one model often succeed on another, even without shared weights?

## Architecture Onboarding

- **Component map**: Probe Training Module -> Direction Extraction -> Modified GCG Optimizer -> Evaluation Pipeline
- **Critical path**: 1) Generate initial jailbreak corpus via GCG on pretrained model, 2) Label prompts by transfer success against finetuned variants, 3) Train linear probes on each layer and select best-performing layer, 4) Extract v_transfer and v_success directions, 5) Run PGP optimization with joint objective on new malicious prompts, 6) Validate transfer success on held-out finetuned models
- **Design tradeoffs**: Linear SVM outperforms kernel methods (59% vs 2-3% TSR) despite lower classification accuracy—linearity preserves feature geometry. Deeper layers show higher probe accuracy but may overfit to pretrained-specific features; mid-to-deep layers (16-24 for 32-layer models) work best. λ weighting controls success-transfer balance and requires tuning. More diverse finetuned targets yield more robust probes but require more compute.
- **Failure signatures**: TSR near 0% with high probe accuracy indicates wrong pretrained model identified—verify provenance. High ASR on pretrained, low TSR on all finetuned suggests λ too low or probe trained on non-representative data. Kernel probe matches linear probe accuracy but TSR collapses due to dimensionality transformation—stick with linear. Safety-finetuning with 2000 examples only modestly reduces TSR (expected outcome).
- **First 3 experiments**: 1) Reproduce probe separability by training linear probes on Llama2-7b-chat hidden states using GCG prompts labeled by transfer to a single finetuned variant, verifying >80% accuracy at deeper layers, 2) Ablate probe type by comparing linear SVM, logistic regression, and RBF SVM on probe accuracy and downstream TSR, confirming linear SVM superiority, 3) Run cross-model transfer baseline comparing GCG adaptation vs PGP on Llama2-7b-chat → Alpaca finetuned, targeting >24.4% → >50% TSR improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Probe generalization across diverse finetuning tasks remains unclear, as effectiveness may degrade for highly specialized domains where representation space shifts substantially
- Attacker's knowledge assumptions rely on correctly identifying the pretrained source model, but LLM provenance is often obfuscated through multi-stage finetuning or proprietary modifications
- Defense landscape is incomplete, focusing primarily on safety-finetuning while not extensively evaluating other strategies like adversarial training or representation regularization

## Confidence
- **High confidence**: Core empirical findings showing PGP achieves significantly higher transferability success rates than baseline methods (59% vs 2-3% TSR for kernel probes)
- **Medium confidence**: Theoretical mechanism linking linear probe directions to transferability structure in pretrained representations is plausible but causal relationship remains somewhat heuristic
- **Medium confidence**: Claim that publicly releasing pretrained checkpoints creates inherent security risks is compelling but extends beyond specific PGP attack demonstrated

## Next Checks
1. **Cross-domain probe generalization test**: Train a single PGP probe using mixed finetuning datasets (Alpaca + Codealpaca + Gsm8k) and evaluate TSR across all five finetuning variants to quantify generalization trade-offs and determine minimum probe diversity requirements.

2. **Provenance uncertainty analysis**: Systematically evaluate PGP performance when using incorrect pretrained model guesses by testing all combinations of four pretrained models (Llama2, Llama3, Deepseek, Gemma) attacking each finetuning variant to quantify sensitivity to source model identification accuracy.

3. **Defense ablation study**: Beyond safety-finetuning, evaluate PGP against alternative defenses: (a) adversarial training on the pretrained model using transferable prompts, (b) representation regularization techniques that encourage orthogonal feature spaces, and (c) input preprocessing that disrupts probe-identified transferability directions. Measure TSR reduction for each defense to identify most effective mitigation strategies.