---
ver: rpa2
title: Natural Policy Gradient for Average Reward Non-Stationary RL
arxiv_id: '2504.16415'
source_url: https://arxiv.org/abs/2504.16415
tags:
- reward
- policy
- lemma
- average
- non-stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of non-stationary reinforcement
  learning in the infinite-horizon average-reward setting, where the environment evolves
  over time with varying rewards and transition probabilities. The authors propose
  Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method that incorporates
  restarts for change exploration and interprets learning rates as adapting factors.
---

# Natural Policy Gradient for Average Reward Non-Stationary RL

## Quick Facts
- **arXiv ID:** 2504.16415
- **Source URL:** https://arxiv.org/abs/2504.16415
- **Reference count:** 40
- **One-line result:** Achieves $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ dynamic regret for non-stationary RL

## Executive Summary
This paper addresses non-stationary reinforcement learning in the infinite-horizon average-reward setting, where environment dynamics evolve over time with varying rewards and transition probabilities. The authors propose NS-NAC, a policy gradient method with restart-based exploration for change detection and learning rates interpreted as adaptation factors. They also introduce BORL-NS-NAC, a parameter-free algorithm based on bandit-over-RL that doesn't require prior knowledge of the variation budget. Both algorithms achieve a dynamic regret bound of $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$, leveraging a novel Lyapunov function analysis adapted to dynamic environments.

## Method Summary
The approach divides the time horizon into segments, periodically resetting policy and value estimates to initialization to force re-exploration. The NS-NAC algorithm uses natural actor-critic with optimal step sizes scaled by the variation budget ($\alpha^* = \gamma^* = (\Delta_T/T)^{1/3}$, $\beta^* = (\Delta_T/T)^{1/2}$). The BORL-NS-NAC variant employs an outer bandit loop (EXP3.P) to adaptively tune hyperparameters when the variation budget is unknown, treating configurations as arms and using observed cumulative rewards as feedback.

## Key Results
- NS-NAC and BORL-NS-NAC achieve $\tilde{O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ dynamic regret
- Optimal restart frequency is $N^* = \Delta_T^{5/6}T^{1/6}$
- Empirical validation on synthetic MDPs with 50 states and 4 actions shows performance matching baseline algorithms
- Step sizes should scale as $(\Delta_T/T)^{1/3}$ for critic and value updates, and $(\Delta_T/T)^{1/2}$ for policy updates

## Why This Works (Mechanism)

### Mechanism 1: Restart-Based Exploration for Change Detection
If the environment changes non-stationarily, periodically resetting policy and value estimates forces the agent to re-explore, preventing it from getting stuck in policies optimal only for past environments. The algorithm divides time into segments, resetting $\pi$ to uniform and $Q$ to zero at each restart. This "forgetting" ensures stale estimates from old dynamics don't permanently bias learning of new dynamics. The non-stationarity must be constrained by variation budget $\Delta_T$ so learning is possible within segments. If restarts are too frequent, the agent spends most steps re-learning from scratch, causing regret to scale linearly with restart count.

### Mechanism 2: Learning Rates as Adaptation Factors
Treating learning rates as factors dependent on variation budget $\Delta_T$ allows balancing bias-variance trade-off specific to non-stationary dynamics. In stationary RL, small step-sizes converge to true gradient. In non-stationary RL, the "true" gradient drifts. Scaling step-sizes like $\alpha^* \propto (\Delta_T/T)^{1/3}$ increases plasticity (forgetting old data faster) as environment becomes more volatile. This requires knowledge or estimation of environmental change magnitude. If step-sizes are too high relative to actual non-stationarity, variance dominates, destabilizing updates.

### Mechanism 3: Bandit-Over-RL for Hyperparameter Tuning
When variation budget $\Delta_T$ is unknown, an outer adversarial bandit loop can adaptively tune step-sizes and restart frequency by treating configurations as "arms." BORL-NS-NAC runs EXP3.P over discrete hyperparameter configurations, running NS-NAC with selected configuration and using observed cumulative reward as feedback. This "hedges" against unknown rate of environmental change. The non-stationarity must be smooth or periodic enough that optimal configuration in recent epoch remains effective soon after. If environment changes drastically within bandit epoch, selected hyperparameters may mismatch current volatility, leading to sub-optimal regret scaling.

## Foundational Learning

- **Concept: Natural Actor-Critic (NAC)**
  - Why needed: Base optimizer; standard gradients can be slow/unstable; NAC uses Fisher Information Matrix (second-order information) to re-scale gradient, critical for stabilizing policy in infinite-horizon setting
  - Quick check: Can you explain why natural gradient $F^{-1}\nabla J$ might be preferred over standard gradient $\nabla J$ when policy parameterization involves softmax? (Hint: Think of geometry of probability distributions)

- **Concept: Dynamic Regret**
  - Why needed: Performance metric; unlike static regret (comparing to best fixed policy), dynamic regret compares agent to current optimal policy $\pi_t^\star$ at every step; understanding this distinction is key to understanding why algorithm must "chase" moving target
  - Quick check: Why is sub-linear dynamic regret harder to achieve than sub-linear static regret in non-stationary environment?

- **Concept: Variation Budget ($\Delta_T$)**
  - Why needed: Quantifies difficulty of problem; sums total variation in rewards and transitions over time; acts as "hardness parameter" that dictates optimal learning rates
  - Quick check: If $\Delta_T = 0$, what does optimal step-size strategy imply for learning rates? (Answer: They should likely shrink to 0 over time, typical of stationary stochastic approximation)

## Architecture Onboarding

- **Component map:** Environment Wrapper -> Learner (NS-NAC) -> Scheduler (Restart Logic + Optional BORL)
- **Critical path:**
  1. Observe state $s_t$
  2. Restart Check: If $t \pmod H == 0$, reset $\pi \to \text{Uniform}, Q \to 0$
  3. Action: Sample $a_t \sim \pi(\cdot|s_t)$
  4. Transition: Receive $r_t, s_{t+1}$
  5. Critic Update: Update $Q(s_t, a_t)$ and average reward $\eta$
  6. Actor Update: Update $\pi$ using projected natural gradient step
  7. BORL Update (if applicable): If end of bandit epoch, update probability distribution over hyperparameter arms

- **Design tradeoffs:**
  * Segment Length ($H$) vs. Restart Count ($N$): Large $N$ (short $H$) improves responsiveness to changes but increases cost of "cold starts" (learning from scratch); optimal balance is $N^* = \Delta_T^{5/6}T^{1/6}$
  * Step-sizes ($\alpha, \beta$): High step-sizes track changes faster but introduce noise; paper suggests $\beta^* = (\Delta_T/T)^{1/2}$

- **Failure signatures:**
  * Linear Regret: Check if restart interval $H$ is too large relative to frequency of environmental shifts
  * Critic Divergence: Paper uses projection radius $R_Q$ for critic; if $Q$-values explode, projection is likely too tight or non-stationarity violates ergodicity assumptions
  * BORL Stagnation: If bandit loop fails to converge, ensure reward scaling for bandit feedback matches scale of RL rewards

- **First 3 experiments:**
  1. Stationary Baseline: Run NS-NAC on fixed MDP; verify reduces to standard NAC behavior (regret stabilizes/converges)
  2. Sensitivity to $\Delta_T$: Run on synthetic MDPs varying number of switches; plot Dynamic Regret vs. $\Delta_T$ on log-log scale to verify $\Delta_T^{1/6}$ scaling
  3. BORL Ablation: Run BORL-NS-NAC against NS-NAC with "oracle" knowledge of $\Delta_T$; measure gap in performance to assess cost of not knowing variation budget

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dynamic regret upper bound of $\tilde{O}(T^{5/6})$ be tightened to match the lower bound of $\Omega(T^{2/3})$ for model-free policy-based methods?
- **Basis in paper:** [explicit] Authors state gap results from slack in analysis of underlying Natural Actor-Critic (NAC) algorithm, specifically mismatch where actor requires norm of critic error while critic establishes bounds on norm-squared error
- **Why unresolved:** Current proof technique involves "squaring trick" that introduces slack; unproven whether gap is artifact of analysis or fundamental limitation of single-loop natural policy gradient methods in non-stationary settings
- **What evidence would resolve it:** Refined analysis of Natural Actor-Critic establishing linear convergence for critic error (rather than squared error) or modified algorithm bridging $T^{5/6}$ and $T^{2/3}$ gap

### Open Question 2
- **Question:** Can theoretical guarantees of NS-NAC be extended to general function approximation (e.g., deep neural networks) without relying on compatible function approximation?
- **Basis in paper:** [inferred] Appendix E extends algorithm to function approximation setting but explicitly relies on Assumption E.3 requiring "compatible function approximation" where feature vectors exactly match gradient of log-policy; this is strong structural constraint not typically met by generic deep learning architectures
- **Why unresolved:** Theoretical bounds depend on linearity and compatibility of features to ensure critic approximates true value function well; without this, approximation error $\epsilon_{app}$ and distribution shift in non-stationary environments become difficult to bound
- **What evidence would resolve it:** Regret bound for NS-NAC holding for general smooth function classes (like neural networks) or empirical demonstration of convergence guarantees under relaxed approximation conditions

### Open Question 3
- **Question:** Is assumption of Uniform Ergodicity necessary to achieve sub-linear dynamic regret, or can analysis be adapted to environments with non-uniform or time-varying mixing times?
- **Basis in paper:** [inferred] Assumption 5.1 requires Markov chains induced by all policies in all environments are uniformly ergodic; implies environment's mixing properties cannot degrade over time, limiting applicability to complex real-world non-stationary systems
- **Why unresolved:** Constants derived from mixing time ($m, \rho$) are fixed globally in regret analysis (Theorem 5.3); unclear if restart mechanism would suffice if environment evolved to have significantly slower mixing dynamics later in horizon
- **What evidence would resolve it:** Regret analysis explicitly incorporating time-varying mixing parameters into bound, or counter-examples showing non-uniform ergodicity leads to linear regret for current NS-NAC configuration

## Limitations

- Theoretical guarantees depend on assumption that non-stationarity is bounded by known or estimable variation budget $\Delta_T$, which is often unknown and difficult to estimate without prior knowledge
- Projection radius $R_Q$ for critic relies on unknown parameter $\lambda$ (maximum eigenvalue of Ā_{π,P}), requiring approximation or tuning in practical implementations
- Algorithm's performance degrades if restarts are too frequent (high initialization bias) or too infrequent (slow adaptation to changes), requiring careful tuning of restart frequency $N$

## Confidence

- **High Confidence:** Mechanism of restart-based exploration for change detection is well-supported by theoretical analysis in Section 5.2 and aligns with established literature on restart paradigms in non-stationary RL
- **Medium Confidence:** Interpretation of learning rates as adaptation factors is theoretically sound but relies on strong assumption that order of magnitude of environmental change is known or estimable
- **Medium Confidence:** Bandit-over-RL framework for hyperparameter tuning is reasonable approach but may suffer from slow adaptation if environmental changes occur within bandit epochs

## Next Checks

1. **Sensitivity Analysis:** Conduct experiments varying variation budget $\Delta_T$ across multiple orders of magnitude to empirically validate the $\Delta_T^{1/6}$ scaling in dynamic regret bound
2. **Oracle Comparison:** Compare BORL-NS-NAC against NS-NAC with oracle knowledge of $\Delta_T$ to quantify performance cost of not knowing variation budget
3. **Break Point Identification:** Systematically vary restart frequency $N$ to identify point where trade-off between adaptation speed and initialization bias becomes suboptimal, leading to linear regret growth