---
ver: rpa2
title: 'Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation:
  A Case Study in ViZDoom'
arxiv_id: '2511.11703'
source_url: https://arxiv.org/abs/2511.11703
tags:
- agents
- semantic
- segmentation
- input
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes two novel input representations for reinforcement
  learning in 3D environments: SS-only and RGB+SS. SS-only uses semantic segmentation
  masks as direct input, reducing memory buffer consumption by 66.6% to 98.6% via
  run-length encoding.'
---

# Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom

## Quick Facts
- arXiv ID: 2511.11703
- Source URL: https://arxiv.org/abs/2511.11703
- Reference count: 0
- Key outcome: SS-only input reduces memory buffer consumption by 66.6%-98.6% with comparable performance to RGB baseline; RGB+SS significantly improves agent performance.

## Executive Summary
This paper proposes two novel input representations for reinforcement learning in 3D environments using ViZDoom deathmatches. SS-only uses semantic segmentation masks as direct input, achieving substantial memory savings (up to 98.6% via run-length encoding) while maintaining comparable performance to RGB baselines. RGB+SS augments RGB images with semantic channels, significantly enhancing agent performance (19.3 frags vs 9.8 for built-in bots). Experiments demonstrate that semantic information improves both efficiency and effectiveness of RL agents in 3D visual control tasks.

## Method Summary
The method involves training PPO agents in ViZDoom with three input representations: RGB, SS-only, and RGB+SS. A custom semantic segmentation dataset of 247,068 frames was collected from 200 episodes. DeepLabV3-ResNet101 was trained to provide real-time semantic segmentation. Agents were trained for 4M timesteps on Map 1 and evaluated on Maps 1-3. Memory consumption was measured using run-length encoding for SS-only representations. Performance was measured by average frags per episode against built-in bots.

## Key Results
- SS-only agents achieved 15.2 frags with only 1.4% of RGB memory (151MB vs 11GB)
- RGB+SS agents achieved 19.3 frags on average, nearly double the built-in bots' 9.8
- SS-only with ground-truth segmentation performed comparably to RGB baseline despite using only 33.3% memory
- Real-time semantic segmentation achieved 0.846 mIoU on training map with minimal overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic segmentation masks can replace RGB input while reducing memory consumption by 66.6%-98.6% without significant performance degradation.
- Mechanism: SS masks use single-channel discrete class labels enabling efficient run-length encoding compression. Contiguous same-class regions compress efficiently since pixel values are discrete and spatially coherent.
- Core assumption: Semantic classes form spatially contiguous regions in typical 3D environment frames; the RL policy can learn effectively from abstracted semantic information without texture/color details.
- Evidence anchors: Abstract states SS-only reduced memory by at least 66.6% and up to 98.6% with RLE. Section 4.3, Table 4.2 shows RGB (11GB) vs SS (3.6GB, 33.3%) vs SS-RLE (151MB, 1.4%) for 20 episodes.

### Mechanism 2
- Claim: Augmenting RGB input with a semantic segmentation channel (RGB+SS) improves RL agent performance significantly.
- Mechanism: The semantic channel provides explicit object-level context, reducing the learning burden of inferring object categories from raw pixels. The agent receives both low-level visual features and high-level semantic abstraction simultaneously.
- Core assumption: Ground-truth or high-quality predicted segmentation is available; the semantic classes are relevant to the decision-making task.
- Evidence anchors: Abstract states RGB+SS significantly enhances RL agents' performance. Section 4.1.1 shows RGB+SS outperformed every other agent with 19.3 frags average. Section 4.1.2 shows RGB+SS agent with ground-truth SS was not impacted much by altered textures.

### Mechanism 3
- Claim: Frame-stacked SS-only input (SS(4)) provides temporal information that improves navigation in complex environments.
- Mechanism: Stacking 4 consecutive SS frames addresses partial observability by encoding motion/direction cues without recurrent architectures. The low memory footprint of SS makes frame-stacking practical.
- Core assumption: Temporal context is relevant to the task; frame-stacking sufficiently approximates recurrent memory for navigation decisions.
- Evidence anchors: Section 3.7.1 adds SS(4) as alternative to RGB+SS. Section 4.1.4 shows on Map 3 (high navigation complexity) ground-truth version of SS(4) yielded best performance with 10.9 frags average.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) and POMDPs**
  - Why needed here: 3D visual control is inherently partially observable. Understanding why frame-stacking or recurrence helps requires grasping state estimation from observation history.
  - Quick check question: Can you explain why a single 2D game frame cannot fully represent the agent's state in a 3D environment?

- **Concept: Convolutional Neural Networks for Visual Feature Extraction**
  - Why needed here: The PPO agent uses a CNN feature extractor (3 conv blocks) before policy/value heads. Understanding receptive fields, stride effects, and channel semantics is essential for modifying input representations.
  - Quick check question: How does changing input channels from 3 (RGB) to 1 (SS-only) or 4 (RGB+SS) affect the first convolutional layer?

- **Concept: Semantic Segmentation**
  - Why needed here: Core to both input representations. Must understand that SS produces per-pixel class labels, mIoU as quality metric, and that DeepLabV3 provides real-time inference.
  - Quick check question: Why might mIoU measured on a validation subset overestimate real-world segmentation quality during actual gameplay?

- **Concept: Experience Replay / Rollout Buffers**
  - Why needed here: Memory consumption problem directly stems from storing trajectories. SS-only compression targets this buffer, not the model parameters.
  - Quick check question: Why does storing SS masks (discrete, compressible) require less memory than RGB frames (continuous, noisy)?

## Architecture Onboarding

- **Component map:**
  ViZDoom Environment (256×144, frame-skip=4) -> Labels Buffer (ground-truth SS) OR DeepLabV3+ResNet-101 (predicted SS) -> Input Representation: RGB (3ch) | SS-only (1ch) | RGB+SS (4ch) | SS(4) (4ch stacked) -> CNN Feature Extractor (3 conv blocks: 5×5/4×4/3×3 kernels, batch norm, ReLU) -> Flatten -> FC(128) -> Policy Head (actor) + Value Head (critic) -> PPO Optimization (rollout buffer, clipped surrogate loss)

- **Critical path:**
  1. Environment wrapper collects frame, extracts or predicts SS mask
  2. Input representation assembled (RGB/SS/RGB+SS/SS(4))
  3. CNN extracts 128-dim feature vector
  4. Actor outputs action distribution over 18 discrete actions
  5. Rollout buffer stores (obs, action, reward) for PPO update
  6. For SS-only: RLE compression applied before buffer storage

- **Design tradeoffs:**
  | Representation | Memory | Performance | Best Use Case |
  |----------------|--------|-------------|---------------|
  | RGB (baseline) | High (11GB) | 15.2 frags | General-purpose |
  | SS-only | Low (3.6GB / 151MB w/ RLE) | 15.0-15.2 frags | Memory-constrained |
  | RGB+SS | Highest (14.7GB equiv.) | 19.3 frags | Max performance |
  | SS(4) | Low (still compressible) | 15.1-15.4 frags, best on complex nav | Navigation-heavy |

- **Failure signatures:**
  - RPPO with shared LSTM: Catastrophic forgetting observed (section 3.7.3)
  - SS model overfitting: High validation mIoU (0.982 in prior work) but poor real gameplay transfer—authors avoid by collecting gameplay-derived dataset
  - Real-time SS quality drop on unseen maps: mIoU 0.846→0.683, correlates with SS-only performance degradation
  - Down-sampling artifacts: Prior work suffered from SS-to-RGB color mapping + downsampling; this work avoids by using raw SS masks

- **First 3 experiments:**
  1. **RGB baseline on Map 1**: Train PPO agent with RGB input, 4M timesteps, learning rate sweep. Target: ~15 frags. Establishes baseline and validates training pipeline.
  2. **SS-only with ground-truth on Map 1**: Same hyperparameters, single-channel SS input. Measure memory usage (rollout buffer size) and performance parity. Validates memory reduction claim.
  3. **RGB+SS with ground-truth on Map 1**: 4-channel input, may require different learning rate. Target: >17 frags. Validates performance enhancement claim before testing with predicted SS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high compression ratio of the SS-only representation enable off-policy reinforcement learning algorithms (e.g., DQN) to achieve higher performance by allowing for significantly larger experience replay buffers within fixed memory constraints?
- Basis in paper: [explicit] Section 5.1 states utilizing SS-only in off-policy models is a key future direction, hypothesizing low memory usage allows more experiences to be stored, potentially preventing buffer from filling with low-reward samples before the model learns.
- Why unresolved: Current study exclusively evaluates on-policy algorithms (PPO) which use rollout buffers, whereas memory benefits of compression are most critical for replay buffers used in off-policy learning.
- What evidence would resolve it: Comparative study of DQN agents trained with RGB inputs versus RLE-compressed SS-only inputs, measuring sample efficiency and final performance under identical memory limits.

### Open Question 2
- Question: How do different down-sampling techniques applied to semantic segmentation masks affect the retention of critical spatial features and the subsequent performance of the RL agent?
- Basis in paper: [explicit] Section 3.3 notes the author avoided down-sampling due to hidden assumptions but believes "a comparison of down-sampling algorithms would be interesting for future studies."
- Why unresolved: Paper utilizes ground-truth segmentation at game's native resolution, but real-world applications often require down-sampling. Impact of interpolation methods on semantic mask integrity for agent decision-making remains untested.
- What evidence would resolve it: Experiments measuring agent performance and mask mIoU when various down-sampling algorithms are applied to input pipeline before convolutional encoder.

### Open Question 3
- Question: Can recurrent PPO (RPPO) agents utilizing shared LSTM layers for both actor and critic overcome the training instability and catastrophic forgetting observed in standard RPPO implementations for this environment?
- Basis in paper: [explicit] Section 5.1 suggests exploring "RPPO agents with shared LSTMs" after section 3.7.3 reports standard RPPO experiments failed due to instability, suspecting lack of shared parameters was a cause.
- Why unresolved: Paper attempted RPPO but reverted to frame-stacking (SS(4)) for temporal information. Specific architectural change of sharing LSTM layer is proposed but not validated.
- What evidence would resolve it: Training curves and evaluation metrics of RPPO agents configured with shared LSTM layers compared against current frame-stacking baseline to assess stability and POMDP handling.

## Limitations

- Performance gains of RGB+SS are primarily demonstrated on training map with ground-truth semantic labels, raising concerns about memorization vs generalization.
- Real-time semantic segmentation quality varies significantly across maps (mIoU 0.846→0.683), introducing uncontrolled variables in performance comparisons.
- SS-only performance parity with RGB is only tested on one map, limiting confidence in general applicability across diverse environments.

## Confidence

- SS-only memory reduction claim (66.6%-98.6%): **High** - Direct measurement with concrete numbers (11GB → 151MB) provided in Table 4.2
- SS-only performance parity with RGB: **Medium** - 15.2 vs 15.0 frags is statistically close, but only tested on Map 1; limited map diversity
- RGB+SS performance enhancement: **Medium** - 19.3 frags vs 15.2 is substantial, but relies on ground-truth SS during training; real-time SS quality varies
- SS(4) temporal benefit: **Low** - Only briefly mentioned with Map 3 results (10.9 frags); no comparison to LSTM-based baselines for navigation

## Next Checks

1. **Cross-map generalization test**: Train RGB+SS agent on Map 1, evaluate on Map 2 and Map 3 without fine-tuning; measure performance degradation and compare to RGB baseline to isolate semantic contribution from memorization.

2. **SS quality ablation**: Run RGB+SS agent with ground-truth SS (perfect mIoU) vs predicted SS (mIoU 0.846) on same episodes; quantify performance gap attributable to segmentation error.

3. **Memory overhead benchmark**: Profile CPU/GPU time for real-time SS inference (DeepLabV3-ResNet101) during gameplay; compare total system latency to RGB-only baseline to verify "minimal overhead" claim.