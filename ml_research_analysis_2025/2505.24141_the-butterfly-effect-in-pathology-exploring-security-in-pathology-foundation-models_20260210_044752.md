---
ver: rpa2
title: 'The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation
  Models'
arxiv_id: '2505.24141'
source_url: https://arxiv.org/abs/2505.24141
tags:
- attack
- adversarial
- pathology
- perturbation
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic investigation into the
  security of whole slide image-level pathology foundation models (WSI-FMs) under
  adversarial attacks. The authors propose a label-free adversarial attack framework
  based on the principle of "local perturbation with global impact," which targets
  only a small subset of patches (0.1% per slide) with imperceptible noise.
---

# The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models

## Quick Facts
- arXiv ID: 2505.24141
- Source URL: https://arxiv.org/abs/2505.24141
- Reference count: 40
- Primary result: Imperceptible perturbations to fewer than 0.1% of patches can cause up to 20% accuracy degradation in WSI-FMs

## Executive Summary
This paper presents the first systematic investigation into the security of whole slide image-level pathology foundation models (WSI-FMs) under adversarial attacks. The authors propose a novel label-free adversarial attack framework that exploits the principle of "local perturbation with global impact," targeting only a small subset of patches with imperceptible noise. Comprehensive experiments across three representative WSI-FMs (CHIEF, PRISM, and TITAN) demonstrate that attacks can degrade downstream task performance by up to 20% accuracy while modifying less than 0.1% of patches. The study also reveals that larger models exhibit lower adversarial robustness and that normal patches within tumor WSIs are surprisingly more vulnerable than tumor patches.

## Method Summary
The method involves a label-free adversarial attack framework that maximizes the Mean Squared Error (MSE) between original and perturbed slide-level embeddings without requiring downstream task labels. The attack adapts four classical white-box methods (FGSM, BIM, MIM, C&W) to this setting, with a focus on perturbing a small number of patches (K) rather than increasing pixel noise magnitude. Experiments use five datasets across six downstream tasks, with 300 WSIs randomly selected per task and capped at 7,000 patches per slide. A lightweight defense strategy using small-scale uniform noise injection (±1/255) is evaluated for its effectiveness in mitigating attacks.

## Key Results
- Attacks on fewer than 0.1% of patches can cause up to 20% accuracy degradation in downstream tasks
- CHIEF exhibits the strongest adversarial robustness among tested models
- Larger WSI-FMs show lower adversarial robustness, following an inverse scaling relationship
- Normal patches within tumor WSIs are more vulnerable to attacks than tumor patches
- A lightweight uniform noise defense can significantly mitigate attack success while maintaining model performance

## Why This Works (Mechanism)

### Mechanism 1: Local-to-Global Representation Drift
Perturbing fewer than 0.1% of patches can shift the slide-level embedding sufficiently to cause up to 20% downstream accuracy degradation. The framework treats the WSI-FM as the attack surface, maximizing MSE between original and perturbed embeddings to force global representation drift. This works because the feature aggregation module is sensitive to localized feature outliers, allowing small perturbations to influence the slide-level vector significantly.

### Mechanism 2: Label-Free Objective as a Universal Proxy
A label-free objective (maximizing feature drift) serves as an effective proxy for task-specific loss minimization when downstream labels are unavailable. By maximizing the distance in feature space, the attack disrupts the structural integrity of features that downstream classifiers rely on. This works because downstream classifiers are typically linear or shallow MLPs trained on fixed features, making them vulnerable to feature vector displacement.

### Mechanism 3: Inverse Scaling of Robustness
Larger WSI-FMs exhibit lower adversarial robustness due to learning more complex, piecewise-linear decision boundaries with sharper gradients. This high capacity allows them to model fine-grained distinctions but also makes the loss landscape more volatile, enabling small input changes to trigger large output shifts. The correlation between size and vulnerability is intrinsic to model capacity rather than specific architectural choices.

## Foundational Learning

- **Concept:** Multiple Instance Learning (MIL)
  - **Why needed here:** WSI-FMs operate on MIL premise where a slide (bag) contains thousands of patches (instances), with labels for the bag, not the patches. Understanding this is crucial to grasp why perturbing a few "instances" can flip the "bag" label.
  - **Quick check question:** If an aggregator uses max-pooling, would changing a single patch feature likely change the slide embedding more or less than if it used average pooling?

- **Concept:** Perturbation Scope (K) vs. Magnitude (ε)
  - **Why needed here:** In WSI attacks, the "scope" (how many patches to attack) is distinct from pixel noise magnitude due to the gigapixel scale. Understanding this distinction is crucial for attack effectiveness.
  - **Quick check question:** Does the paper find that increasing pixel noise on a single patch is more or less effective than attacking more patches with smaller noise?

- **Concept:** Label-Free Threat Model
  - **Why needed here:** This defines the "worst-case" realistic scenario where the attacker has model weights (white-box to FM) but doesn't know the specific clinical task (black-box to head).
  - **Quick check question:** Why is maximizing the MSE between clean and perturbed embedding a valid attack strategy if you don't know the downstream task?

## Architecture Onboarding

- **Component map:** Whole Slide Image → Patches → Feature Extractor → Patch Features → Aggregator → Slide Embedding → Attack Surface
- **Critical path:**
  1. Select K patches
  2. Initialize perturbations
  3. Compute forward pass to get perturbed embedding
  4. Compute loss MSE(original, perturbed)
  5. Backpropagate to update perturbations (only for selected patches)
  6. Project perturbations to ℓ∞-ball
- **Design tradeoffs:**
  - Sequential vs. Parallel Attack: Sequential is computationally lighter per step but slower overall; Parallel exploits inter-patch dependencies better but requires higher VRAM
  - Scope vs. Stealth: Increasing K degrades accuracy faster than increasing ε, but attacking more patches increases physical footprint
- **Failure signatures:**
  - Defense Activation: Random noise injection drops attack success rate significantly (e.g., Camelyon16 accuracy recovers from ~42% to ~98% for TITAN)
  - Saturation: Increasing ε beyond threshold yields diminishing returns compared to increasing K
- **First 3 experiments:**
  1. Single-Patch Sensitivity Test: Run MIM attack with K=1, ε=4/255 on CHIEF vs. PRISM to verify "butterfly effect" and rank robustness
  2. Scope vs. Magnitude Scan: Fix K=1, vary ε (4 to 64); then fix ε=4, vary K (1 to 8) to confirm scope dominance
  3. Noise Defense Validation: Apply uniform noise defense to top-performing attacks to measure downstream accuracy recovery

## Open Questions the Paper Calls Out

- Can attention-based or semantically-guided patch selection strategies substantially improve attack effectiveness compared to random selection?
- What is the relative contribution of feature extractor versus aggregation module to overall adversarial vulnerability?
- Do alternative loss functions (beyond MSE) for maximizing representation discrepancy yield more effective label-free attacks?
- Can more sophisticated defense mechanisms be developed that surpass lightweight random noise injection while maintaining computational efficiency?

## Limitations

- The inverse scaling relationship between model size and robustness may be influenced by factors beyond parameter count, such as architectural differences or training procedures
- The defense strategy using uniform noise injection only addresses input-level perturbation and may not be effective against adaptive attacks
- Results focus on MLP-based downstream classifiers and may not generalize to more complex task-specific architectures
- The study does not disentangle the respective vulnerabilities of the feature extractor and aggregator modules

## Confidence

- **High Confidence:** The core finding that small perturbations to a tiny fraction of patches can degrade downstream accuracy by up to 20% is well-supported by experimental results
- **Medium Confidence:** The ranking of model robustness and inverse scaling relationship are observed but may be influenced by factors beyond pure parameter count
- **Low Confidence:** The generalizability of the uniform noise defense to real-world clinical settings and more sophisticated attack variants remains untested

## Next Checks

1. Test the attack framework on additional WSI-FMs with varying architectures to confirm whether the inverse scaling relationship holds universally
2. Evaluate the uniform noise defense against adaptive attacks that could circumvent simple input-level noise injection
3. Assess whether vulnerability patterns persist when downstream classifiers use more complex architectures rather than simple MLPs