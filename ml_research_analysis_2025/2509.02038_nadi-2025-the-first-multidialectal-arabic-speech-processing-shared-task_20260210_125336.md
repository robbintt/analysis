---
ver: rpa2
title: 'NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task'
arxiv_id: '2509.02038'
source_url: https://arxiv.org/abs/2509.02038
tags:
- arabic
- speech
- dialect
- language
- subtask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'NADI 2025 addresses the challenge of multidialectal Arabic speech
  processing by introducing three complementary tasks: spoken dialect identification,
  automatic speech recognition (ASR), and diacritic restoration for spoken Arabic.
  The shared task involved 44 registered teams, with 8 submitting results.'
---

# NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task

## Quick Facts
- **arXiv ID**: 2509.02038
- **Source URL**: https://arxiv.org/abs/2509.02038
- **Reference count**: 31
- **Primary result**: 44 registered teams, 8 submitted results; top systems achieved 79.8% dialect ID accuracy, 35.68/12.20 WER/CER in ASR, and 55/13 WER/CER in diacritic restoration.

## Executive Summary
NADI 2025 introduced the first shared task addressing multidialectal Arabic speech processing through three complementary subtasks: spoken dialect identification, automatic speech recognition (ASR), and diacritic restoration. The task used the Casablanca corpus covering 8 Arabic dialects and attracted significant participation. Results showed substantial improvements over baselines, particularly through weakly supervised pretraining followed by supervised fine-tuning for ASR, data augmentation for dialect identification, and multi-modal approaches for diacritic restoration. However, challenges remain in handling dialectal variability, code-switching, and the complex continuum of Arabic dialects.

## Method Summary
The shared task employed a two-stage training approach for ASR: weakly supervised pretraining on 15K hours of unlabeled Arabic speech followed by supervised fine-tuning on 3K hours of high-quality filtered data. Dialect identification used data augmentation techniques including speed perturbation, noise injection, and voice conversion to address limited adaptation data. Diacritic restoration encouraged multi-modal approaches combining speech and text encoders through early fusion or cross-attention mechanisms. All tasks used Whisper-based models as baselines, with top systems implementing per-dialect fine-tuning and aggressive augmentation strategies.

## Key Results
- Dialect identification accuracy reached 79.8% using data augmentation and external datasets
- ASR performance achieved 35.68/12.20 WER/CER through two-stage weakly supervised pretraining
- Diacritic restoration obtained 55/13 WER/CER using multi-modal speech-text fusion
- 44 teams registered with 8 submitting results, demonstrating strong community interest

## Why This Works (Mechanism)

### Mechanism 1: Large-Scale Weakly Supervised Pretraining Followed by Supervised Fine-Tuning
A two-stage pipeline combining weakly supervised pretraining on 15K hours of Arabic speech with supervised fine-tuning on 3K hours of filtered data substantially improves ASR performance. The pretraining builds robust acoustic representations across dialectal variability, while fine-tuning aligns these to dialect-specific patterns.

### Mechanism 2: Data Augmentation Reduces Overfitting on Limited Adaptation Sets
Aggressive augmentation (voice conversion, speed perturbation, noise injection, SpecAugment) improves dialect identification when adaptation data is small by expanding training distribution and reducing memorization of non-linguistic cues.

### Mechanism 3: Multi-Modal Fusion Leverages Complementary Speech and Text Cues for Diacritic Restoration
Combining speech encoders (capturing phonetic vowel information) with text encoders (providing lexical context) through early fusion or cross-attention improves diacritic restoration for dialectal and code-switched speech where orthographic conventions are inconsistent.

## Foundational Learning

- **Concept: Dialectal Arabic Linguistic Variability**
  - Why needed: Arabic dialects differ across phonology, morphology, lexicon, and syntax; models trained on MSA/CA fail to generalize. Quick check: Can you name three linguistic dimensions along which Arabic dialects differ, and why a model trained only on Modern Standard Arabic would struggle with Moroccan or Yemeni speech?

- **Concept: Weakly Supervised Learning in Speech**
  - Why needed: The winning ASR system used 15K hours of automatically labeled speech without manual verification. Quick check: What are the risks of training on weakly labeled speech data, and how might you detect systematic labeling errors in a 15K-hour corpus?

- **Concept: Diacritization and Vowel Restoration**
  - Why needed: Arabic script typically omits short vowels, creating ambiguity critical for pronunciation and NLP. Quick check: Why might text-only diacritic restoration fail for Egyptian Arabic speech containing code-switched English, and how could audio cues help?

## Architecture Onboarding

- **Component map**: Subtask 1: Speech encoder (Whisper, ECAPA-TDNN) → pooling/classification → 8-way softmax. Subtask 2: Conformer/Transformer encoder → CTC/encoder-decoder → ASR output. Subtask 3: Parallel speech encoder (Whisper) + text encoder (CATT/T5) → fusion (early or cross-attention) → diacritic classification.

- **Critical path**: 1) Obtain Casablanca dataset (47,027 utterances across 8 dialects). 2) Select pretrained backbone (Whisper-Large-v3, w2v-BERT-2.0, SeamlessM4T-v2). 3) Apply data augmentation (SpecAugment, speed perturbation, noise injection, voice conversion). 4) Fine-tune per-dialect or with dialect-conditioned prompts; evaluate using WER/CER or accuracy/Cavg.

- **Design tradeoffs**: Per-dialect vs. unified models (efficiency vs. performance), multi-modal vs. text-only diacritization (complexity vs. acoustic benefits), weakly supervised scale vs. quality (coverage vs. label noise).

- **Failure signatures**: Dialect ID accuracy near baseline (61%) suggests insufficient adaptation data or overfitting to speaker/channel cues. ASR WER > 100% indicates tokenizer/decoding failures. Diacritization WER much higher than CER suggests tokenization mismatches.

- **First 3 experiments**: 1) Replicate baseline: Fine-tune Whisper-Large-v3 on Casablanca validation split for ASR. 2) Ablate augmentation: Train dialect ID model with and without SpecAugment + noise injection. 3) Test multi-modal fusion: Implement early-fusion diacritization combining Whisper + CATT; compare against text-only baseline.

## Open Questions the Paper Calls Out

1. How can evaluation metrics be redesigned to account for multiple valid transcription references in Arabic ASR? The Limitations section states that WER and CER "may be misleading" because a "dialectal utterance can often have multiple valid references," causing scores to penalize correct alternative transcriptions.

2. To what extent do current spoken dialect identification models rely on non-linguistic features (e.g., channel effects, gender) rather than phonological content? Section 2.2 notes that models "may capture non-linguistic information such as gender and channel features" and suffer degradation in cross-domain settings.

3. How can models generalize to the full dialectal continuum when trained on broad country-level labels? The Limitations section highlights that country-level labeling is "problematic" because "the continuum of Arabic dialects is complex" and lacks well-defined national linguistic boundaries.

## Limitations

- Test sets for dialect identification and diacritic restoration were hosted on private Codabench platforms, limiting independent verification
- Winning approaches relied on external datasets (ADI-20) not available to all participants, creating uneven competition
- Weakly supervised pretraining used proprietary data (15K hours) making exact replication challenging
- Standard WER/CER metrics may not capture semantic preservation when diacritics are added in code-switched contexts

## Confidence

**High Confidence**: Baseline results (Whisper-Large-v3 for ASR at 71.25/25.87 WER/CER, ECAPA-TDNN for dialect ID at 61% accuracy) are verifiable through public datasets and described procedures.

**Medium Confidence**: Mechanisms explaining winning approaches are supported by described architectures and ablation studies, though individual component contributions cannot be isolated without full training pipeline access.

**Low Confidence**: Claims about multi-modal superiority for diacritic restoration are based on limited comparisons, as multi-modal baseline only achieved WER/CER of 66/16, comparable to text-only approaches.

## Next Checks

1. **Independent Reproduction of Per-Dialect ASR Performance**: Download Casablanca subset and implement fine-tuning procedure to verify reported per-dialect WER/CER scores, paying attention to text normalization pipeline.

2. **Ablation Study of Augmentation Strategies**: Implement data augmentation techniques for dialect identification and measure incremental accuracy improvement to quantify augmentation contribution versus model architecture choices.

3. **Multi-Modal vs. Text-Only Diacritization Comparison**: Using ClArTTS, ArVoice, or MGB2 datasets, implement both early-fusion and cross-attention multi-modal approaches and compare WER/CER against text-only baselines on held-out test set.