---
ver: rpa2
title: Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under
  Distribution Shift
arxiv_id: '2510.15265'
source_url: https://arxiv.org/abs/2510.15265
tags:
- causal
- lake
- water
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RIC-TSC, a regionally-informed causal time
  series classification framework for modeling supraglacial lake evolution in Greenland.
  The core method integrates lag-aware causal discovery via Joint PCMCI+ with sequence
  modeling using MiniROCKET and RidgeClassifier, identifying temporally stable and
  physically grounded predictors of lake dynamics.
---

# Causal Time Series Modeling of Supraglacial Lake Evolution in Greenland under Distribution Shift

## Quick Facts
- arXiv ID: 2510.15265
- Source URL: https://arxiv.org/abs/2510.15265
- Reference count: 34
- Key outcome: RIC-TSC achieves up to 12.59% higher accuracy than correlation-based baselines under out-of-distribution evaluation for supraglacial lake evolution classification

## Executive Summary
This paper introduces RIC-TSC, a causally-informed time series classification framework for modeling supraglacial lake evolution in Greenland. The method integrates lag-aware causal discovery via Joint PCMCI+ with sequence modeling using MiniROCKET and RidgeClassifier, identifying temporally stable and physically grounded predictors of lake dynamics. Using multi-modal satellite and reanalysis data across six basins and two melt seasons (2018-2019), the framework demonstrates improved generalization under spatial distribution shift, achieving 1.06% higher global accuracy and up to 12.59% gains in held-out regions compared to correlation-based baselines.

## Method Summary
The approach combines causal discovery with lightweight sequence modeling: J-PCMCI+ identifies region-specific and invariant lagged causal relationships from multi-modal time series (SAR, optical, reanalysis), then MiniROCKET transforms these selected features into deterministic convolutional representations for RidgeClassifier. The framework processes 365-day daily time series from 1000 manually labeled lakes, using structured dummy variables to control for spatial and temporal heterogeneity during causal discovery. Global and basin-specific causal graphs are estimated separately, with discovered parents used as inputs for classification under both in-distribution and out-of-distribution evaluation.

## Key Results
- Global accuracy improves by 1.06% (82.84% → 83.90%) when using causal vs. correlation features
- Out-of-distribution accuracy gains range from +1.99% to +12.59% across six held-out region tests
- Causal models consistently outperform baseline across all evaluation settings (global, in-distribution, OOD)
- Identified causal parents include HV anomaly at lags -1 to -7 days and S2 water fraction at lag 0, aligning with physical understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lag-aware causal discovery filters spurious correlations and selects predictors with statistically validated temporal precedence
- **Mechanism:** J-PCMCI+ applies conditional independence tests with structured dummy variables to distinguish causal parents from correlated but non-causal features, specifying which variables matter and at what lags
- **Core assumption:** True data-generating process contains stable, discoverable lagged causal mechanisms that persist across spatial domains
- **Evidence anchors:** Abstract mentions "stable, time-lagged relationships while controlling for spatial and temporal heterogeneity"; section V details J-PCMCI+ methodology with structured dummies
- **Break condition:** If causal structure shifts fundamentally across basins or sample size is insufficient for reliable conditional independence testing

### Mechanism 2
- **Claim:** Causal feature selection improves out-of-distribution generalization by retaining only domain-invariant predictors
- **Mechanism:** Training on causally selected lagged parents rather than all variables avoids learning basin-specific spurious correlations that don't transfer
- **Core assumption:** Spurious correlations vary across regions while causal relationships remain relatively stable
- **Evidence anchors:** Abstract states "up to 12.59% higher accuracy than correlation-based baselines under out-of-distribution evaluation"; Table III shows causal model outperforms baseline in all six held-out region tests
- **Break condition:** If held-out region has genuinely different causal dynamics, even causally selected features may fail to transfer

### Mechanism 3
- **Claim:** Lightweight sequence transforms (MiniROCKET) with causal priors achieve competitive accuracy without deep network overhead
- **Mechanism:** MiniROCKET generates deterministic convolutional features from causally-selected input sequences; RidgeClassifier provides regularized linear decisions
- **Core assumption:** Causal feature set is sufficiently informative that simple classifier can achieve good separation
- **Evidence anchors:** Section IV-A states design "balances speed, interpretability, and discriminative power while avoiding overhead of more complex neural networks"; section VII shows causal+MiniROCKET matches or exceeds baseline across settings
- **Break condition:** If lake evolution requires modeling long-range dependencies beyond 7-day lags, MiniROCKET's fixed kernels may miss relevant structure

## Foundational Learning

- **Concept:** PCMCI+ causal discovery for time series
  - **Why needed here:** Understanding how J-PCMCI+ identifies lagged causal parents with conditional independence tests is essential to interpret causal graphs and selected features
  - **Quick check question:** Given a multivariate time series with autocorrelation, how does PCMCI+ distinguish between a variable X causing Y at lag 2 versus both being driven by a third variable Z at lag 1?

- **Concept:** Distribution shift and invariant learning
  - **Why needed here:** Paper's core claim is that causal features improve generalization under spatial/climatic shift; understanding what makes features "invariant" clarifies experimental design
  - **Quick check question:** In leave-one-region-out evaluation, why would a model trained on correlation-based features perform worse than one trained on causal features when tested on a new region?

- **Concept:** MiniROCKET time series transform
  - **Why needed here:** This is the sequence encoder; understanding its deterministic convolutional feature generation explains how pipeline remains lightweight
  - **Quick check question:** How does MiniROCKET's use of random length and dilation parameters enable it to capture multi-scale temporal patterns without learned weights?

## Architecture Onboarding

- **Component map:** Sentinel-1 SAR/HV_anom, Sentinel-2/Landsat-8 optical/S2_water/LS_water/S2_zenith/LS_zenith, CARRA reanalysis/t2m/r2/sp/sst → 365-day daily time series → J-PCMCI+ causal discovery with structured dummies (s-dummy/r-dummy/t-dummy), max lag 7, α=0.01 → causal graphs (global + per-basin) → extract lagged causal parents per region → MiniROCKET transform → RidgeClassifier

- **Critical path:**
  1. Verify time series alignment and gap-filling (interpolation + 12-day median smoothing)
  2. Run J-PCMCI+ separately for global and each basin with consistent parameters
  3. Construct input matrices using only discovered causal parents at specified lags
  4. Train MiniROCKET+RidgeClassifier with internal cross-validation for regularization

- **Design tradeoffs:**
  - Global vs. region-specific causal graphs: Global graphs may miss local drivers; region-specific graphs require more samples per basin
  - Max lag of 7 days: Captures synoptic processes but may miss longer-term preconditioning effects
  - MiniROCKET vs. deep models: Faster and more interpretable but limited in modeling complex nonlinear interactions

- **Failure signatures:**
  - NE region shows -5.59% accuracy drop in in-distribution setting, possibly due to noise or insufficient samples
  - If causal discovery returns dense graphs with many spurious edges, feature explosion negates dimensionality reduction benefits
  - Large discrepancies between global and region-specific causal parents suggest heterogeneous mechanisms requiring basin-specific modeling

- **First 3 experiments:**
  1. Reproduce global causal graph: Run J-PCMCI+ on 60 lakes (10 per region) and verify discovered parents match Table I; check sensitivity to α threshold
  2. Ablate lag structure: Compare models using (a) causal variables with discovered lags, (b) causal variables at lag 0 only, (c) all variables; measure OOD accuracy degradation
  3. Region-held-out stress test: For each basin, train on remaining five using both causal and baseline features; confirm gains persist and analyze which causal parents drive improvements per held-out region

## Open Questions the Paper Calls Out

- **Open Question 1:** Does stratifying causal discovery by specific lake evolution classes (e.g., rapid drainage vs. buried) reveal distinct physical mechanisms that further improve classification performance?
  - **Basis in paper:** [explicit] Conclusion states "Future work will stratify causal discovery by lake outcome to isolate class-specific mechanisms..."
  - **Why unresolved:** Current study estimates causal graphs globally or per basin, potentially conflating distinct physical drivers that differentiate specific evolution pathways
  - **What evidence would resolve it:** Experimental results showing improved classification accuracy or interpretability when using class-specific causal graphs

- **Open Question 2:** Can the integration of spatiotemporal graph neural networks (GNNs) effectively capture inter-lake dependencies to enhance model performance?
  - **Basis in paper:** [explicit] Conclusion proposes to "extend the pipeline with spatiotemporal graph neural networks to capture inter-lake dependencies"
  - **Why unresolved:** Current framework treats lakes as independent observational units, using dummy variables to handle context rather than modeling spatial interactions
  - **What evidence would resolve it:** Comparative analysis showing GNN-based model explicitly modeling spatial links outperforms current independent-lake baseline

- **Open Question 3:** How does the proposed causal MiniROCKET pipeline compare against complex deep sequence models like LSTMs or Transformers regarding the trade-off between predictive "context" and model interpretability?
  - **Basis in paper:** [explicit] Discussion notes "While future comparisons with deep sequence models (e.g., LSTMs, Transformers) could broaden context, our results establish causally-informed lag selection as a practical approach"
  - **Why unresolved:** Paper prioritizes lightweight, interpretable models but does not benchmark against heavier deep learning architectures
  - **What evidence would resolve it:** Rigorous benchmark comparing causal MiniROCKET approach against LSTM/Transformer baselines on same OOD evaluation splits

## Limitations

- Causal discovery relies on J-PCMCI+ with structured dummy variables but lacks empirical validation of whether discovered edges remain stable under temporal drift within training period
- Global causal graph derived from only 60 lakes (10 per region), raising questions about whether it captures truly universal mechanisms versus small-sample artifact
- MiniROCKET + RidgeClassifier combination is lightweight and interpretable but benchmarked only against simple correlation-based baseline, not modern sequence models

## Confidence

- **High confidence:** Empirical OOD accuracy gains (+1.99% to +12.59% in held-out regions) and physical interpretability of identified lagged predictors are well-supported by data and methodology
- **Medium confidence:** Claim that causal features improve generalization rests on assumption that spurious correlations vary by region while causal relationships are stable—plausible but not proven hypothesis
- **Low confidence:** Asserts computational efficiency and scalability but provides no runtime benchmarks or comparisons to deep learning baselines beyond generic "complexity" statement

## Next Checks

1. **Temporal stability audit:** Split 2018-2019 training period into two halves and rerun J-PCMCI+ causal discovery separately on each; compare resulting causal graphs to assess whether discovered mechanisms are truly stable over time

2. **Deep model ablation:** Train a simple LSTM or Transformer on same causally-selected features and evaluate OOD performance; this would confirm whether gains are due to causality or feature selection process itself

3. **Global graph robustness:** Rerun global J-PCMCI+ discovery on 100 lakes (increasing per-region samples to ~16) and measure how much causal parent set changes; this would quantify sensitivity to small-sample effects in global graph