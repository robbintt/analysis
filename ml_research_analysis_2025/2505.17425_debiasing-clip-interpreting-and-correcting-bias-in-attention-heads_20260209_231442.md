---
ver: rpa2
title: 'Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads'
arxiv_id: '2505.17425'
source_url: https://arxiv.org/abs/2505.17425
tags:
- states
- spurious
- image
- dataset
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Locate-Then-Correct (LTC), a method to identify
  and correct bias in CLIP's vision transformer attention heads. LTC uses linear decomposition
  to locate attention states encoding spurious attributes and class features, then
  applies mean-ablation to remove spurious associations and knowledge injection to
  enhance discriminative features.
---

# Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads

## Quick Facts
- **arXiv ID**: 2505.17425
- **Source URL**: https://arxiv.org/abs/2505.17425
- **Reference count**: 40
- **Primary result**: Introduces Locate-Then-Correct (LTC), achieving over 50% improvement in worst-group accuracy for debiasing CLIP attention heads.

## Executive Summary
This paper presents Locate-Then-Correct (LTC), a method for identifying and correcting bias in CLIP's vision transformer attention heads without full model fine-tuning. LTC uses linear decomposition to locate attention states encoding spurious attributes and class features, then applies mean-ablation to remove spurious associations and knowledge injection to enhance discriminative features. Evaluated on Waterbirds, CounterAnimal, and GenderBias-VL datasets, LTC achieves over 50% improvement in worst-group accuracy compared to non-training baselines while maintaining interpretability through its head-level interventions.

## Method Summary
LTC operates on CLIP's vision transformer by first decomposing image representations into attention head outputs. The method identifies "spurious" heads encoding background or confounding attributes using Logit Lens scores computed on negative subgroups (mismatched spurious attributes), and "target" heads encoding class-relevant features. For correction, LTC applies mean-ablation to spurious heads to remove their influence, and uses knowledge injection via orthogonal projection with text features derived from LLM descriptions to enhance discriminative properties in target heads. The approach can be applied as a standalone intervention or in conjunction with fine-tuning, where it serves as a preprocessing step before a 2-layer non-linear probe.

## Key Results
- Achieves over 50% improvement in worst-group accuracy compared to non-training baselines
- Outperforms orthogonal projection methods applied to full representations on benchmark datasets
- Effectively mitigates background and gender bias while providing interpretable insights into which heads encode bias

## Why This Works (Mechanism)
LTC works by exploiting the modular structure of CLIP's attention mechanism to make precise, interpretable interventions. By decomposing representations at the head level rather than the full layer, it can target specific spurious associations while preserving useful class information. The mean-ablation technique removes spurious signal by replacing head outputs with their mean across samples, while knowledge injection enhances discriminative features through orthogonal projection using LLM-derived text features. This localized approach allows for cleaner separation of bias from useful information compared to global projection methods.

## Foundational Learning

**Concept: CLIP's Dual-Encoder Architecture and Zero-Shot Inference**
- **Why needed here:** LTC operates on CLIP, which uses a vision and text encoder. The debiasing and knowledge injection are done by manipulating the image representation relative to text prompts.
- **Quick check question:** In CLIP, how is the prediction logit `Sy` for a class `y` computed from an image `I` and a text label (Eq. 1)?

**Concept: Attention Head Decomposition and Interpretability**
- **Why needed here:** LTC relies on decomposing the transformer's residual stream into individual attention head outputs to identify which heads encode spurious vs. target information.
- **Quick check question:** How does LTC use Logit Lens scores to distinguish between spurious heads (`Z_S`) and target heads (`Z_Y`)?

**Concept: Mean-Ablation vs. Knowledge Injection**
- **Why needed here:** LTC employs two complementary correction strategies - subtractive (removing spurious signal) and additive (enhancing discriminative signal).
- **Quick check question:** What is the key difference between how mean-ablation and knowledge injection modify attention head outputs?

## Architecture Onboarding

**Component map:** Image `I` -> Patch Embedding -> `[CLS]` token `z0` -> **L layers of (MSA + MLP)** -> Final `[CLS]` state `EI(I)` -> Projection `PI(EI(I))` -> Cosine similarity with text

**Critical path:** Image `I` -> Patch Embedding -> `[CLS]` token `z0` -> **L layers of (MSA + MLP)** -> Final `[CLS]` state `EI(I)` -> Projection `PI(EI(I))` -> Cosine similarity with text. **LTC intercepts at the output of each MSA head (`ẑl,h`)**, modifies them (ablation/projection), and then allows them to be aggregated back into the residual stream.

**Design tradeoffs:**
- **Targeted vs. Global Intervention:** LTC operates on specific heads, offering precision and interpretability. Tradeoff: Requires more complex analysis to locate heads vs. simpler global projection methods like Ortho-Cali.
- **Mean-Ablation vs. Knowledge Injection (KI):** MA is a purely subtractive approach, removing spurious signal. KI is additive, enhancing discriminative signal. Tradeoff: KI depends on the quality of external LLM-generated features, while MA does not.
- **Head vs. Layer Granularity:** LTC focuses on individual heads. Tradeoff: Finer granularity allows more precise correction but increases the search space for relevant states.

**Failure signatures:**
- **Ineffective debiasing:** Worst-group accuracy (`GN`) does not improve significantly after LTC. Possible causes: (1) Incorrect head identification due to a suboptimally set threshold `γ`, (2) Spurious information is distributed across many heads (superposition) rather than localized, (3) The spurious feature is intrinsic to the target (e.g., gender in occupation classification) and cannot be cleanly separated.
- **Degraded overall performance:** Average accuracy (`Avg`) drops sharply. Possible causes: (1) Ablating heads that also encode crucial class information (the ZSY case mentioned in Sec. 6), (2) LLM-generated features for KI are noisy or hallucinated.

**First 3 experiments:**
1. **Head Localization Validation:** On the Waterbirds validation set, run the LTC head identification procedure (Eq. 10). Visualize the top-identified "spurious" head (e.g., via attention heatmaps). Does it clearly highlight the background (e.g., land/water) as expected? Compare with the "target" head visualization.
2. **Ablation Ablation Study:** Implement LTC using *only* mean-ablation (LTC-MA) on the identified spurious head. Measure worst-group (`GN`) accuracy on Waterbirds. This isolates the effect of removing the spurious signal. Repeat using *only* knowledge injection (LTC-KI) on the target head.
3. **Baseline Comparison:** Compare full LTC against a simpler baseline (e.g., global orthogonal projection on the full image representation, similar to Roboshot). Report both worst-group accuracy and average accuracy. Does localized intervention outperform global intervention?

## Open Questions the Paper Calls Out
None

## Limitations
- Core method relies on accurately decomposing attention heads into interpretable "spurious" vs "target" components, which is an inherently noisy process
- Assumes spurious features can be cleanly separated from target features, but they may be entangled or distributed across multiple heads (superposition)
- Focuses on CLIP's zero-shot inference mode; performance in fine-tuned or few-shot settings remains unexplored

## Confidence

- **High Confidence:** The experimental methodology and evaluation protocol are clearly described and reproducible. The quantitative improvements on benchmark datasets (Waterbirds, CounterAnimal, GenderBias-VL) are well-documented with statistical comparisons to baselines.

- **Medium Confidence:** The interpretability claims (e.g., "head X encodes background") are supported by qualitative visualizations but rely on the assumption that attention weights directly correspond to semantic content, which is debated in the mechanistic interpretability literature.

- **Low Confidence:** The generalizability of the LLM-based knowledge injection approach across diverse domains and languages is uncertain, as the quality of generated text features may vary significantly with prompt engineering and model choice.

## Next Checks

1. **Robustness to Head Identification Thresholds:** Systematically vary the threshold γ in Eq. (10) and measure how worst-group accuracy changes. This would reveal whether the method is sensitive to hyperparameter choices or if it identifies a robust set of heads.

2. **Ablation of LLM Feature Generation:** Replace GPT-4o-generated text features with alternative sources (e.g., human-curated keywords, embeddings from a different model) and evaluate whether the knowledge injection component remains effective. This isolates the contribution of the LLM vs. the projection mechanism.

3. **Generalization to New Domains:** Apply LTC to a dataset with different types of bias (e.g., texture bias, viewpoint bias) and evaluate whether the same head decomposition and correction strategy remains effective. This tests the method's adaptability beyond the three evaluated biases.