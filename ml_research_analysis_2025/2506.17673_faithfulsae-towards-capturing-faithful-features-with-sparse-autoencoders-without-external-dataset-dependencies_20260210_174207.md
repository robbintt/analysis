---
ver: rpa2
title: 'FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders
  without External Dataset Dependencies'
arxiv_id: '2506.17673'
source_url: https://arxiv.org/abs/2506.17673
tags:
- dataset
- datasets
- saes
- faithful
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FaithfulSAE addresses instability and poor faithfulness in sparse
  autoencoders (SAEs) by training them on the model's own synthetic dataset rather
  than external data. The authors generate a "Faithful Dataset" via unconditional
  sampling from the LLM, which more accurately reflects the model's intrinsic capabilities.
---

# FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies

## Quick Facts
- **arXiv ID**: 2506.17673
- **Source URL**: https://arxiv.org/abs/2506.17673
- **Reference count**: 21
- **Primary result**: FaithfulSAE achieves better SAE probing performance than web-based SAEs in 12 out of 18 cases while reducing Fake Feature Ratio in 5 of 7 models.

## Executive Summary
FaithfulSAE addresses fundamental instability and poor faithfulness issues in sparse autoencoders (SAEs) by training them on the model's own synthetic dataset rather than external data. The authors generate a "Faithful Dataset" via unconditional sampling from the LLM, which more accurately reflects the model's intrinsic capabilities. Across 7 models, FaithfulSAEs show higher Shared Feature Ratio (SFR) stability than instruction datasets and outperform web-based SAEs in SAE probing tasks in 12 out of 18 cases. They also achieve lower Fake Feature Ratio in 5 out of 7 models, indicating better interpretability. This approach eliminates external dataset dependencies and advances mechanistic interpretability by capturing more faithful, model-internal features.

## Method Summary
FaithfulSAE proposes training sparse autoencoders on synthetic data generated by the target LLM itself, rather than external datasets. The method involves unconditional sampling from the LLM (starting from a BOS token) to create a "Faithful Dataset" that reflects the model's internal distribution. This dataset is then used to train TopK sparse autoencoders on the model's hidden states. The approach aims to solve the out-of-distribution problem that causes SAE instability and "fake features." The paper evaluates the method across 7 models using metrics including Shared Feature Ratio (SFR) for stability, Fake Feature Ratio (FFR) for interpretability, reconstruction losses, and SAE probing accuracy on downstream tasks.

## Key Results
- FaithfulSAEs demonstrate higher Shared Feature Ratio (SFR) stability than instruction datasets across 7 models
- FaithfulSAEs outperform web-based SAEs in SAE probing tasks in 12 out of 18 cases
- FaithfulSAEs achieve lower Fake Feature Ratio in 5 out of 7 models, indicating better interpretability
- The method eliminates external dataset dependencies while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training SAEs on the model's own generated data reduces the distribution mismatch (OOD problem) that causes SAEs to learn spurious "fake features" and become unstable across seeds.
- Mechanism: The model generates text by sampling from its learned next-token prediction distribution (starting from a BOS token). This "Faithful Dataset" closely reflects the data manifold the model has internalized during pretraining. By training the SAE on this distribution, the encoder/decoder weights adapt to features the model actually uses, rather than features present only in external web data but outside the model's generalization.
- Core assumption: A large portion of SAE instability and "fake features" stems from training on data that is out-of-distribution for the target model; aligning the training distribution with the model's internal distribution improves feature fidelity.
- Evidence anchors:
  - [abstract] "These problems likely stem from training SAEs on external datasets... which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term 'Fake Features'..."
  - [section 1] "To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds."
  - [corpus] Weak direct support for this exact mechanism. Neighbor papers discuss SAE training, evaluation, and feature correctness (e.g., "SplInterp," "Sparse but Wrong"), but do not specifically evaluate the synthetic self-generated dataset method for reducing OOD effects.
- Break condition: If a model's pretraining distribution is highly diverse and the synthetic dataset is too small or mode-collapsed (e.g., low first-token coverage), the Faithful Dataset will not capture the full model capability, and SAEs trained on it may underperform on broader metrics like SFR compared to web datasets.

### Mechanism 2
- Claim: The Shared Feature Ratio (SFR) between SAEs trained with different seeds is higher when the training data is less OOD relative to the model, indicating more stable feature discovery.
- Mechanism: The SFR metric quantifies the overlap of feature sets between SAEs via Hungarian matching on decoder weight vectors. Less OOD data (e.g., Faithful Dataset or data closer to the model's pretraining) produces more consistent feature sets across seeds, as the SAE converges to a more canonical decomposition of the model's representations.
- Core assumption: A high SFR reflects the discovery of robust, model-internal features rather than dataset-specific artifacts.
- Evidence anchors:
  - [abstract] "Across 7 models, FaithfulSAEs show higher Shared Feature Ratio (SFR) stability than instruction datasets..."
  - [section 5.1] "As shown in Table 2, FaithfulSAEs, trained on a synthetic dataset, exhibit greater stability across seeds compared to SAEs trained on mixed or instruction-based datasets. These results support our hypothesis that higher OOD levels reduce SFR."
  - [corpus] No direct corroboration. Neighbors focus on evaluation benchmarks and feature correctness, not SFR stability across seeds due to dataset OODness.
- Break condition: If OOD data is still within the model's generalization capability (e.g., web data for large, diverse models), SFR may not improve significantly with the Faithful Dataset, as seen with web-based datasets in the paper.

### Mechanism 3
- Claim: FaithfulSAEs have fewer "fake features" (features that activate frequently on random/OOD inputs) and better downstream probing performance, suggesting they capture more faithful representations of the model's hidden state.
- Mechanism: By training on data the model itself generates, the SAE's feature set is less likely to include dimensions that are highly active on arbitrary noise or inputs the model never learned to process. This reduces the Fake Feature Ratio (FFR). Faithful representations also preserve more task-relevant information, improving the accuracy of linear probes trained on SAE features.
- Core assumption: Lower FFR and higher probing accuracy are proxies for an SAE's faithfulness to the model's internal computation.
- Evidence anchors:
  - [abstract] "They also achieve lower Fake Feature Ratio in 5 out of 7 models, indicating better interpretability. This approach eliminates external dataset dependencies..."
  - [section 5.7] "While FaithfulSAE generally shows lower SFR compared to web-based datasets, it demonstrates better performance in terms of FFR (lower), suggesting potential benefits for interpretability with the Faithful Dataset. Among the 7 models tested, 5 models showed lower FFR with FaithfulSAE..."
  - [corpus] No direct support. Neighbors like "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders" discuss faithfulness in RAG, not the specific FFR metric.
- Break condition: If the model itself generates narrow or repetitive text (e.g., due to sampling strategy), the Faithful Dataset might not cover all modes of the model's capability, potentially missing some legitimate features and affecting probing performance on certain tasks.

## Foundational Learning

- Concept: **Out-of-Distribution (OOD) Data in LLM Contexts**
  - Why needed here: The paper's core hypothesis is that SAE problems (instability, fake features) stem from training on OOD data—data not seen during pretraining or too complex for the model. Understanding this is critical to grasping the motivation for the Faithful Dataset.
  - Quick check question: Can you explain why data generated by a different model or scraped from the web might be "out-of-distribution" for a given target LLM?

- Concept: **Sparse Autoencoders (SAEs) for Mechanistic Interpretability**
  - Why needed here: The entire method is built on SAEs. One must understand their basic architecture (encoder/decoder, sparsity constraint), how they disentangle superposed representations, and the role of the dictionary size and activation function (e.g., TopK).
  - Quick check question: What is the objective function of a TopK SAE, and what role does the sparsity constraint play?

- Concept: **Superposition Hypothesis**
  - Why needed here: SAEs are motivated by the superposition hypothesis—the idea that neural networks represent more features than they have dimensions via linear combinations. This provides the theoretical grounding for why SAEs are expected to work.
  - Quick check question: In your own words, what is the superposition hypothesis, and how does it relate to polysemanticity?

## Architecture Onboarding

- Component map:
  Target LLM -> Faithful Dataset Generator (unconditional sampling) -> TopK SAE (trained on activations) -> Evaluation Suite (SFR, FFR, probing, reconstruction losses)

- Critical path:
  1. **Generate Faithful Dataset**: Sample ~100M+ tokens from the target LLM starting from BOS tokens.
  2. **Extract Activations**: Run the generated text through the LLM and collect hidden states from a target layer (e.g., 3/4 depth).
  3. **Train SAE**: Train a TopK SAE on these activations to reconstruct them with a sparsity penalty.
  4. **Evaluate**: Use feature matching for SFR, random data for FFR, reconstruction losses on multiple datasets, and probing classifiers.

- Design tradeoffs:
  - **Dataset Size vs. Coverage**: Larger Faithful Datasets (e.g., 150M tokens for LLaMA 8B) improve feature coverage but increase generation cost. The paper notes limited first-token coverage as a potential limitation.
  - **Layer Selection**: The paper chooses a layer at 3/4 depth for training, balancing feature complexity and abstraction. Different layers may yield different feature sets.
  - **SAE Hyperparameters**: Dictionary size, TopK value, and learning rate are critical. The paper follows scaling laws from Gao et al. (2024).

- Failure signatures:
  1. **Low SFR despite Faithful Dataset**: This occurred when comparing to web datasets. The paper attributes this to the Faithful Dataset not fully covering the model's distribution. Solution: Generate larger or more diverse datasets (e.g., varying temperature or prompts).
  2. **High FFR for Larger Models**: The paper observes that FFR increases with model size for the same family, indicating interpretability challenges. This may require different SAE architectures or datasets.
  3. **Poor Probing Performance**: If the SAE is not faithful, reconstructed activations will lose task-relevant information. This signals a need to adjust SAE hyperparameters or re-evaluate the training data.

- First 3 experiments:
  1. **Replicate SFR Comparison**: Train FaithfulSAEs and web-based SAEs on a small model (e.g., GPT-2 Small) with two different seeds. Compute and compare SFR. Verify the Faithful Dataset's first-token distribution against the model's BOS prediction.
  2. **FFR Measurement**: Generate a set of random token sequences (OOD inputs) and measure the activation frequency of each SAE feature. Compute FFR for FaithfulSAE vs. a web-based SAE.
  3. **SAE Probing Reproduction**: Train logistic regression probes on SST-2 using the original hidden states, FaithfulSAE features, and reconstructed activations. Compare accuracy to establish baselines and confirm that FaithfulSAE preserves more task-relevant information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the FaithfulSAE approach maintain its advantages in stability and faithfulness when applied to instruction-tuned or reasoning-focused models?
- Basis in paper: [explicit] Section 7 (Limitations) states the experiment "lacked evaluation on instruction-tuned or reasoning models."
- Why unresolved: The study only validated the method on base pre-trained models (e.g., Pythia, LLaMA base), leaving the impact of alignment or reasoning training on the "Faithful Dataset" distribution unknown.
- What evidence would resolve it: Training and evaluating FaithfulSAEs on instruction-tuned variants (e.g., LLaMA-3-Instruct) and comparing SFR and Fake Feature Ratios against base models.

### Open Question 2
- Question: Do the features learned by FaithfulSAE provide more meaningful and human-interpretable explanations than those from web-trained SAEs?
- Basis in paper: [explicit] Section 7 notes the authors "did not assess the interpretability of individual features," and Section 8 prioritizes evaluating "meaningful and interpretable explanations."
- Why unresolved: The paper relies on proxy metrics (SFR, FFR) and probing performance but lacks qualitative case studies verifying if the features correspond to coherent concepts.
- What evidence would resolve it: Detailed case studies or human evaluations comparing the semantic coherence of top-activating features between FaithfulSAE and baselines.

### Open Question 3
- Question: Can modified dataset generation strategies enable FaithfulSAEs to outperform web-based SAEs in Shared Feature Ratio (SFR) stability?
- Basis in paper: [explicit] Section 5.3 admits FaithfulSAEs showed lower SFR than web-based datasets, and Section 8 suggests exploring "improved dataset generation" to "completely outperform Web-based methods."
- Why unresolved: The current method (unconditional BOS sampling) may result in limited distribution coverage, failing to capture the diversity needed for higher SFR compared to massive web datasets.
- What evidence would resolve it: Experiments using alternative sampling methods (e.g., varied prompts, higher temperatures) to test if SFR surpasses that of SAEs trained on The Pile or FineWeb.

## Limitations
- The Faithful Dataset may not fully capture the model's distribution, as evidenced by lower SFR compared to web-based datasets
- The method hasn't been validated on instruction-tuned or reasoning-focused models
- The paper lacks qualitative analysis of whether learned features are truly interpretable or meaningful to humans

## Confidence
- **High**: Faithful Dataset generation methodology and implementation details
- **Medium**: Claims about SFR stability improvements and FFR reductions (supported by data but with unexplained exceptions)
- **Medium**: Claims about improved SAE probing performance (statistically significant but not universal)

## Next Checks
1. **Distributional Alignment Analysis**: Quantitatively compare the distribution of activations from the Faithful Dataset versus external datasets using KL divergence or maximum mean discrepancy to directly validate the OOD hypothesis.

2. **Feature Interpretability Validation**: Conduct human evaluations or automated feature analysis (e.g., activation pattern analysis) to verify whether FaithfulSAE features correspond to coherent, interpretable concepts compared to web-trained SAEs.

3. **SFR Improvement Experiments**: Test alternative Faithful Dataset generation strategies (varying prompts, temperatures, or sampling strategies) to determine if SFR can be improved to match or exceed web-based SAEs.