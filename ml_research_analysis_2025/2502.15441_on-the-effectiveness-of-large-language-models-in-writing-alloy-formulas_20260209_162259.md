---
ver: rpa2
title: On the Effectiveness of Large Language Models in Writing Alloy Formulas
arxiv_id: '2502.15441'
source_url: https://arxiv.org/abs/2502.15441
tags:
- alloy
- llms
- formulas
- language
- property
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of large language models
  (LLMs) in writing Alloy formulas. The authors evaluate three approaches: generating
  complete Alloy formulas from natural language descriptions, creating alternative
  equivalent formulas from existing Alloy formulas, and completing Alloy sketches
  with missing expressions or operators.'
---

# On the Effectiveness of Large Language Models in Writing Alloy Formulas

## Quick Facts
- **arXiv ID**: 2502.15441
- **Source URL**: https://arxiv.org/abs/2502.15441
- **Reference count**: 40
- **Primary result**: LLMs can effectively write and complete Alloy specifications, with DeepSeek R1 generally outperforming OpenAI o3-mini on correctness rates

## Executive Summary
This paper investigates how well large language models can generate Alloy formulas from natural language descriptions, create equivalent formulas from existing Alloy code, and complete partial Alloy sketches. Using 11 well-studied graph and binary relation properties, the authors evaluate two LLMs (OpenAI o3-mini and DeepSeek R1) across 33 tasks. Both models successfully complete sketching tasks, with DeepSeek R1 achieving near-perfect results. For natural language to Alloy translation, DeepSeek R1 shows strong performance (10-19 correct out of 20) while OpenAI o3-mini shows more variable results (1-16 correct out of 20). The study demonstrates that LLMs can effectively work with Alloy specifications, potentially making declarative specification more accessible for software development.

## Method Summary
The authors evaluate three synthesis tasks: generating complete Alloy formulas from natural language descriptions, creating alternative equivalent formulas from existing Alloy formulas, and completing Alloy sketches with missing expressions or operators. Using 11 properties of graphs and binary relations, they create 33 tasks and test them with two LLMs (OpenAI o3-mini and DeepSeek R1). The LLMs are asked to generate 20 unique solutions for each synthesis task. Validation is performed using the Alloy analyzer's equivalence checking within a default scope of 3 atoms, classifying outputs into three categories: correct (no counterexample), syntax error (compilation failure), or wrong (counterexample found).

## Key Results
- DeepSeek R1 generally achieves higher accuracy than OpenAI o3-mini across all task types
- Both models successfully complete sketching tasks, with DeepSeek R1 succeeding on all but one property
- OpenAI o3-mini succeeds on all sketching tasks after syntax error correction on one property
- DeepSeek R1 produces 10-19 correct formulas out of 20 for natural language to Alloy tasks
- OpenAI o3-mini produces 1-16 correct formulas out of 20 for natural language to Alloy tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs transfer logical reasoning patterns learned from mainstream languages and mathematical notation to Alloy's relational logic, despite limited Alloy-specific training data. The models generate equivalent solutions with very different logical structures, suggesting they are recomposing abstract logical equivalences rather than retrieving memorized patterns. This relies on training corpora containing sufficient formal logic and discrete mathematics content. Performance degrades on properties requiring Alloy-specific idioms with no mainstream-language analogues.

### Mechanism 2
The SAT-based validation loop in the Alloy analyzer provides rapid, objective feedback that compensates for semantic drift in LLM outputs. The three-outcome classification (correct, syntax error, wrong) enables systematic filtering. This assumes the ground truth formula correctly captures the specification and the default scope (bound of 3) is sufficient to detect semantic differences. Semantic errors that manifest only at larger scopes go undetected.

### Mechanism 3
Sketch completion succeeds because the hole-based syntax resembles structured prompt templates and few-shot examples common in LLM training data. The format triggers pattern-matching behaviors learned from template-filling tasks. This assumes candidate sets are sufficiently constrained and natural language descriptions provide adequate semantic grounding. Sketches with larger or unconstrained hole spaces cause completion rates to drop toward random selection.

## Foundational Learning

- **Concept: Relational first-order logic with transitive closure (Alloy's semantic foundation)**
  - **Why needed here**: Alloy formulas express properties over binary relations using operators like join (`.`), transitive closure (`^`), and reflexive-transitive closure (`*`). Understanding that `^link` denotes reachability and `iden` is the identity relation is critical.
  - **Quick check question**: Given a binary relation `r: S → S`, what does `s.^r` represent, and how does it differ from `s.*r`?

- **Concept: SAT-based bounded model finding (Alloy's analysis method)**
  - **Why needed here**: The paper validates outputs using the Alloy analyzer's counterexample search within a scope (default bound 3). Understanding that "no counterexample found" ≠ "provably correct" is critical for interpreting results.
  - **Quick check question**: Why might a formula pass validation at scope 3 but fail at scope 10? What class of properties is most susceptible?

- **Concept: Sketch-based program synthesis (hole filling with constraints)**
  - **Why needed here**: The sketch notation (`\E,e\` for expression holes, `\O,o\` for operator holes) assumes familiarity with candidate sets and the ASketch approach.
  - **Quick check question**: Given a sketch with an expression hole and candidate set `{| A | B | C |}`, what is the search space if there are 2 such holes? How does the LLM's semantic understanding reduce this space compared to brute-force enumeration?

## Architecture Onboarding

- **Component map**: Natural language description OR reference Alloy formula OR sketch with holes + candidate sets -> LLM synthesis engine (OpenAI o3-mini or DeepSeek R1) -> Alloy analyzer validation layer -> Output classification (correct, syntax error, wrong)
- **Critical path**: The validation layer is the bottleneck and safety net. The workflow is a linear pipeline: query -> LLM output -> Alloy analyzer validation.
- **Design tradeoffs**: Scope vs. confidence (default scope 3 enables fast validation but may miss counterexamples); diversity vs. correctness (requesting 20 unique solutions probes capability depth but increases validation cost).
- **Failure signatures**: Syntax errors common on first attempt (up to 10/20 outputs); semantic errors without syntax errors indicate deeper misunderstanding; model divergence shows DeepSeek R1 generally outperforms OpenAI o3-mini but not uniformly.
- **First 3 experiments**:
  1. Baseline replication: Replicate English->Alloy task for all 11 properties using a different LLM, measuring correct/syntax-error/wrong distribution.
  2. Scope sensitivity analysis: Re-validate all "correct" formulas at scopes 5, 7, 10 to quantify false positives from limited scope.
  3. Sketch complexity scaling: Create sketch variants with larger candidate sets, nested hole structures, and missing natural language comments to measure completion success rate degradation.

## Open Questions the Paper Calls Out
- How do LLMs perform on synthesizing Alloy formulas for more complex, real-world specifications beyond the 11 basic properties tested?
- How does LLM-based Alloy synthesis compare to traditional solver-based synthesis techniques in terms of solution correctness, diversity, and computational efficiency?
- Can LLMs reliably synthesize Alloy formulas that are correct beyond the default scope of 3 atoms?
- Do the results generalize to other declarative or formal specification languages with different semantic foundations?

## Limitations
- Results depend critically on the default scope of 3 for Alloy validation, potentially leading to false positives
- Three-outcome classification system may mask partial understanding
- Prompts used are minimalistic without explicit instruction on Alloy syntax or semantics
- Limited comparison with established synthesis tools like ASketch

## Confidence
- **High confidence**: LLMs can write and complete Alloy specifications; DeepSeek R1 outperforms OpenAI o3-mini overall
- **Medium confidence**: Specific correctness rates (10-19/20 for R1, 1-16/20 for o3-mini) due to scope limitations and prompt simplicity
- **Low confidence**: Claim that reference Alloy formulas don't necessarily benefit LLMs given limited data points and no ablation study

## Next Checks
1. **Scope sensitivity analysis**: Re-validate all "correct" formulas at scopes 5, 7, and 10 to quantify false positives from limited scope
2. **Prompt engineering study**: Test whether more structured prompts improve correctness rates across all task types
3. **Generalization stress test**: Create Alloy sketching tasks with larger candidate sets, nested hole structures, and missing natural language comments to identify the complexity ceiling for LLM completion