---
ver: rpa2
title: 'FHBench: Towards Efficient and Personalized Federated Learning for Multimodal
  Healthcare'
arxiv_id: '2504.10817'
source_url: https://arxiv.org/abs/2504.10817
tags:
- data
- client
- healthcare
- learning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces FHBench, a federated learning (FL) benchmark
  tailored for multimodal healthcare applications. It addresses the lack of comprehensive
  evaluation tools for FL methods on diverse medical data types such as imaging, text,
  time-series, and audio.
---

# FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare

## Quick Facts
- arXiv ID: 2504.10817
- Source URL: https://arxiv.org/abs/2504.10817
- Reference count: 40
- Primary result: EPFL achieves higher accuracy and lower computational overhead than six baseline FL methods on nine multimodal healthcare datasets.

## Executive Summary
This paper introduces FHBench, a federated learning benchmark for multimodal healthcare data, and proposes EPFL, a personalized FL framework using adaptive LoRA. FHBench addresses the lack of comprehensive evaluation tools for FL on diverse medical data types including imaging, text, time-series, and audio. EPFL improves personalization and efficiency by aggregating only LoRA-A matrices based on B-matrix similarity while keeping B-matrices local, validated across nine real-world healthcare datasets.

## Method Summary
The approach uses LoRA decomposition (ΔW = BA) to reduce communication overhead, training only low-rank matrices while keeping pretrained backbones frozen. EPFL implements similarity-weighted aggregation where A-matrices are shared across clients based on B-matrix distance calculations, while B-matrices remain purely local for personalization. The framework operates on 20 clients with 200 rounds using Dirichlet α=0.1 for synthetic non-IID partitions and natural patient-level splits for other datasets, evaluating classification accuracy across nine multimodal healthcare datasets.

## Key Results
- EPFL outperforms six baseline FL methods including FedAvg, FedProx, and APFL across all nine datasets
- Similarity-weighted aggregation shows greater improvements on smaller datasets (OrganCMNIST, OrganSMNIST) compared to simple averaging
- Second-half and full LoRA parameter selection yields better performance than first-half selection across modalities

## Why This Works (Mechanism)

### Mechanism 1
LoRA-based fine-tuning reduces communication and computational overhead while maintaining model adaptability in federated settings. The mechanism decomposes weight updates into low-rank matrices ΔW = BA where only compact B and A matrices are trained and communicated. Core assumption: pretrained models contain sufficient feature representations; task-specific adaptation requires only low-rank perturbations. Break condition: if downstream tasks require representations substantially different from pretraining distribution, low-rank updates may be insufficient.

### Mechanism 2
Similarity-weighted aggregation of A-matrices improves personalization compared to uniform averaging, particularly for smaller datasets. The mechanism computes aggregation weights as inverse distances between client B-matrices, ensuring clients with similar task-specific representations contribute more to each other's shared feature extractors. Core assumption: B-matrix similarity reflects underlying data distribution similarity. Break condition: if B-matrix distances are noisy or uninformative, similarity weights may amplify noise rather than signal.

### Mechanism 3
Asymmetric treatment of LoRA matrices (aggregating A, keeping B local) enables effective knowledge sharing while preserving client-specific adaptation. The mechanism is based on functional separation where A-matrices extract input features while B-matrices generate task-specific outputs. Core assumption: feature extraction benefits from cross-client collaboration while task-specific output generation is inherently local under non-IID conditions. Break condition: if A-matrices also encode client-specific patterns, aggregation may degrade rather than improve performance.

## Foundational Learning

- **Federated Learning fundamentals (FedAvg, non-IID data, client-server protocol)**: Understanding baseline assumptions is prerequisite to appreciating EPFL's modifications. Quick check: Can you explain why non-IID data causes "client drift" in standard FedAvg?
- **Parameter-Efficient Fine-Tuning (PEFT) and LoRA mechanics**: EPFL's core innovation is LoRA integration; without understanding low-rank decomposition, the architecture appears arbitrary. Quick check: Given a weight matrix W ∈ ℝ^(1024×768), what is the parameter reduction if LoRA uses rank r=8?
- **Personalized FL strategies (local vs. global model balancing)**: EPFL's contribution is personalization; distinguishing it from methods like FedProx, APFL, and MetaFed requires understanding the personalization landscape. Quick check: How does keeping B-matrices local differ from FedProx's proximal term approach to handling heterogeneity?

## Architecture Onboarding

- **Component map**: Clients (pretrained backbone + LoRA layers + local dataset) -> Central server (similarity computation + A-matrix aggregation) -> Communication protocol (A_i, B_i transmission)
- **Critical path**: Initialize clients with pretrained model + LoRA adapters → Local training: clients update A_i, B_i → Upload A_i, B_i to server → Server computes pairwise B-matrix distances → Normalize to similarity weights s_ij → Aggregate: A'_i = Σ_j s_ij × A_j (B_i unchanged) → Distribute personalized A'_i to each client
- **Design tradeoffs**: Layer selection ψ balances information capture vs. computational cost; self-weight λ balances self-contribution vs. peer knowledge; rank r=8 fixed but higher rank improves expressiveness at increased communication cost
- **Failure signatures**: Convergence instability from extreme similarity weights early in training; modality mismatch from uniform hyperparameters across diverse data types; data scarcity collapse where similarity computation becomes noisy on extremely limited client data
- **First 3 experiments**: 1) Baseline reproduction on OrganAMNIST with 5 clients to verify accuracy (~88%) and aggregation weights; 2) Ablation layer selection comparison (first-half vs. second-half vs. all layers) on PTB-XL time-series dataset; 3) Stress test heterogeneity by varying Dirichlet α from 0.1 to 1.0 to 10.0 and plotting EPFL vs. FedAvg accuracy gap

## Open Questions the Paper Calls Out
- Which specific additional healthcare tasks and data modalities should be integrated into FHBench to further validate the generalizability of EPFL?
- How can the similarity-weighted aggregation mechanism be refined to ensure fairness and consistent performance among clients with extremely limited local data?
- Can an adaptive mechanism be developed to automatically select the optimal subset of LoRA matrix layers for similarity computation?

## Limitations
- Asymmetric LoRA aggregation hypothesis lacks direct validation and remains a theoretical assertion
- Cross-modal generalization uses same hyperparameters without justification for parameter sharing
- Similarity-weighted mechanism effectiveness not extensively validated across all modalities

## Confidence
- **High confidence**: Baseline FL implementation, dataset preparation, accuracy improvement claims (supported by 3 random seeds and comparison to 6 baselines)
- **Medium confidence**: Efficiency claims, similarity-weighted aggregation mechanism (supported by ablation on 3 datasets)
- **Low confidence**: Asymmetric LoRA treatment and cross-modal hyperparameter generalization (lacks ablation studies or theoretical justification)

## Next Checks
1. Run ablation study varying LoRA rank r=4,8,16 on a single modality to quantify communication-accuracy tradeoff
2. Implement ablation where B-matrices are also aggregated to test whether asymmetric treatment provides measurable benefit
3. Conduct convergence analysis measuring per-client accuracy variance across training rounds to quantify similarity weighting's effect on client drift