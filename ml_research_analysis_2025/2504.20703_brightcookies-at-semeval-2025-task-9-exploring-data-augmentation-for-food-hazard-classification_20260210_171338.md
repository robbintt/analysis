---
ver: rpa2
title: 'BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food
  Hazard Classification'
arxiv_id: '2504.20703'
source_url: https://arxiv.org/abs/2504.20703
tags:
- text
- title
- char
- word
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores data augmentation techniques to improve food\
  \ hazard classification in highly imbalanced datasets. The authors test three word-level\
  \ augmentation methods\u2014synonym replacement, random word swapping, and contextual\
  \ word insertion\u2014on transformer and machine learning models for classifying\
  \ hazards and products from food recall incident reports."
---

# BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification

## Quick Facts
- arXiv ID: 2504.20703
- Source URL: https://arxiv.org/abs/2504.20703
- Reference count: 40
- Primary result: Contextual word insertion improved minority hazard class accuracy by 6% over baseline

## Executive Summary
This paper investigates data augmentation techniques to address class imbalance in food hazard classification from recall incident reports. The authors test three word-level augmentation methods—synonym replacement, random word swapping, and contextual word insertion—on both transformer and traditional machine learning models. Results show transformer models consistently outperform ML baselines, with contextual word insertion providing significant gains for minority classes at the cost of slight majority class performance reduction. The study demonstrates that targeted augmentation can improve fine-grained classification accuracy for underrepresented categories.

## Method Summary
The authors applied three augmentation techniques to minority classes in a highly imbalanced food recall dataset: synonym replacement using WordNet, random word swapping, and contextual word insertion using BERT embeddings. These augmented samples were combined with original training data and used to fine-tune transformer models (BERT, RoBERTa, DistilBERT, ModernBERT) and traditional ML models (SVM, Logistic Regression, Random Forest, etc.). Hyperparameter optimization was performed using Optuna TPE across 10 trials per model. The evaluation focused on F1-macro scores across four classification categories with different class distributions.

## Key Results
- Transformer models (BERT, RoBERTa, DistilBERT, ModernBERT) consistently achieved higher F1-macro scores than ML baselines
- Contextual word insertion improved fine-grained hazard classification accuracy by 6% for minority classes
- The augmentation created a trade-off, slightly reducing majority class performance while improving minority class predictions
- None of the augmentation techniques improved overall performance across all categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted contextual word insertion (CW) augmentation can improve classification accuracy for fine-grained, minority classes in imbalanced multi-class datasets.
- Mechanism: CW uses a pretrained language model (e.g., BERT) to insert semantically relevant words into training samples of under-represented classes. This increases the sample count for these classes and introduces controlled lexical variation. The model is thus exposed to more diverse examples of minority patterns, which helps adjust decision boundaries and reduces bias toward majority classes.
- Core assumption: The inserted words preserve the original class label and core semantic meaning of the text.
- Evidence anchors:
  - [abstract] "Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%."
  - [section 6.2] "...the BERT CW model predicted the minority classes slightly better than BERT base, with a rise from 39 to 41 for hazard-category and from 261 to 277 for hazard (around 6% increase)..."
  - [corpus] The corpus paper "Synthetic Feature Augmentation Improves Generalization Performance of Language Models" supports the broader principle that augmentation can aid generalization.
- Break condition: The augmentation introduces words that alter the original semantic meaning or class label, creating noisy or mislabeled data that degrades model performance.

### Mechanism 2
- Claim: Encoder-only transformer models provide superior representations for multi-class food hazard classification compared to traditional machine learning models using sparse features like TF-IDF.
- Mechanism: Transformers are pre-trained on large corpora, learning rich contextual embeddings that capture nuanced semantic and syntactic relationships. This provides a more powerful starting point than TF-IDF, enabling the model to better distinguish between fine-grained categories with limited task-specific training data.
- Core assumption: The linguistic knowledge embedded in the pre-trained transformer transfers effectively to the specialized domain of food safety incident reports.
- Evidence anchors:
  - [abstract] "The results show that transformer models tend to have a better overall performance."
  - [Table 2] Shows transformer models (BERT, RoBERTa, DistilBERT, ModernBERT) consistently achieving higher F1-macro scores across all four classification categories compared to all ML baselines.
  - [corpus] Corpus neighbors are weak on direct comparisons, but related SemEval-2025 Task 9 papers implicitly validate transformer efficacy.
- Break condition: The domain-specific terminology of food safety reports is largely unseen during the transformer's pre-training, rendering its initial embeddings less effective for this task.

### Mechanism 3
- Claim: Gains in minority class performance from targeted augmentation can involve a trade-off, potentially leading to a slight reduction in majority class performance.
- Mechanism: By increasing the representation and effective weight of minority classes, the model's learning process is adjusted to pay more attention to their patterns. This shift in focus can cause a slight recalibration of the decision boundary that makes it marginally less optimal for the previously dominant majority classes.
- Core assumption: A model's capacity is finite, and amplifying the signal from minority classes redistributes this capacity away from majority classes.
- Evidence anchors:
  - [abstract] "This suggests that targeted augmentation of minority classes can improve the performance of transformer models."
  - [section 6.2] "...the model predicted the majority classes slightly worse, decreasing from 656 to 632 for hazard, showing that there is a trade-off between improving the predictions for the minority versus the majority classes."
  - [corpus] No strong corpus evidence was found for this specific trade-off in this context.
- Break condition: Excessive or poor-quality augmentation causes the model to overfit to the synthetic minority examples and catastrophically forget the patterns of the majority classes.

## Foundational Learning

- Concept: **Class Imbalance & F1-Macro Score**
  - Why needed here: The core problem is poor model performance on under-represented "minority" classes. The F1-macro score is the key metric because it averages per-class performance, making it sensitive to how well the model does on rare classes, unlike accuracy.
  - Quick check question: If a dataset has 99 "safe" samples and 1 "hazard" sample, and a model predicts "safe" every time, what is its accuracy and what is its likely F1-macro score?

- Concept: **Encoder-Only Transformer Models (e.g., BERT)**
  - Why needed here: These are the primary models used. Understanding that they take a sequence of text and produce contextualized vector representations via an attention mechanism is crucial for grasping their power over older methods.
  - Quick check question: How does a contextual embedding for the word "glass" in the phrase "glass particles" differ from its embedding in "a glass of water"?

- Concept: **Data Augmentation in NLP**
  - Why needed here: The paper's central contribution is exploring augmentation techniques. One must understand that the goal is to artificially expand the training set by creating plausible new examples to help the model generalize better, especially for classes with few original samples.
  - Quick check question: Why is "contextual word insertion" considered a more advanced augmentation technique than "synonym replacement"?

## Architecture Onboarding

- Component map: Raw Text → Preprocessing → (Minority Class Identification → Augmentation) → Tokenization/Vectorization → Model Fine-tuning (with Optuna) → Prediction → F1-Macro Evaluation
- Critical path: The augmentation step is the key intervention being studied
- Design tradeoffs:
  - Transformer vs. ML Models: Transformers offer higher performance but require significantly more compute and longer training times than lightweight ML baselines
  - Augmentation Technique: CW provides semantically coherent variations but is computationally expensive. SR and RW are fast and simple but can produce nonsensical or grammatically incorrect text
  - Targeted vs. Global Augmentation: The paper targets only minority classes. Augmenting all classes would be computationally wasteful and might not address the core imbalance problem
- Failure signatures:
  - Semantically Distorted Augmentations: Poorly chosen synonyms or random swaps change the meaning of a recall report, leading the model to learn incorrect patterns
  - Majority Class Forgetting: A significant drop in F1-score for majority classes after augmentation indicates the model has overcompensated for the minority classes
  - No Performance Gain: If the F1-macro score does not improve, the augmentation technique may be adding noise that the model cannot effectively leverage
- First 3 experiments:
  1. Baseline Performance: Train and evaluate a bert-base-uncased model on the original, non-augmented dataset. Record F1-macro scores for all categories
  2. Apply Targeted CW Augmentation: Identify minority classes using the paper's thresholds. Apply Contextual Word Insertion to the hazard and product training data. Retrain the BERT model and compare the new F1-macro scores against the baseline
  3. Error Analysis: Generate confusion matrices for both the baseline and augmented models. Quantify the improvement in minority class recall and check for the reported trade-off in majority class precision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative Large Language Model (LLM) augmentation strategies outperform the word-level techniques (synonym replacement, random swap, contextual insertion) used in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that future research could explore "more sophisticated augmentation methods, such as LLMs, to generate new samples and verify their quality."
- Why unresolved: The study restricted itself to symbolic and simple neural techniques; LLM-based generation was not tested.
- What evidence would resolve it: A comparison of F1-macro scores on the same dataset using LLM-generated synthetic data versus the word-level augmented data.

### Open Question 2
- Question: What is the optimal number of augmented samples required to maximize performance without causing the observed degradation in majority class accuracy?
- Basis in paper: [inferred] The authors acknowledge their sample thresholds (e.g., 200 for coarse, 100 for fine) were a "compromise" and explicitly call for "optimizing the number of augmented samples for minority classes."
- Why unresolved: The specific quantities of augmented data were heuristic choices rather than the result of optimization.
- What evidence would resolve it: A systematic sweep of augmentation ratios (e.g., 1x, 2x, 5x minority class size) demonstrating the peak performance point before majority class accuracy declines.

### Open Question 3
- Question: Would hierarchical classification architectures improve performance by explicitly modeling the relationship between coarse (ST1) and fine-grained (ST2) categories?
- Basis in paper: [explicit] The Limitations section suggests that "more complex architectures such as ensemble or hierarchical approaches" could be used to compare their effectiveness on augmentation.
- Why unresolved: The current system treats categories independently or as flat multi-class problems, potentially ignoring the inherent taxonomy of food hazards.
- What evidence would resolve it: Implementation of a hierarchical model (where coarse predictions inform fine-grained predictions) comparing its F1-macro scores against the current flat BERT baselines.

## Limitations
- Computational expense of contextual word insertion augmentation technique
- Trade-off between minority class improvement and slight majority class performance reduction
- Limited generalizability to other domains beyond food safety classification

## Confidence
- High Confidence: Superiority of transformer models over traditional ML baselines
- Medium Confidence: Effectiveness of contextual word insertion for minority class improvement
- Low Confidence: Generalizability of augmentation techniques to other domains and datasets

## Next Checks
1. Ablation Study on Augmentation Quality: Systematically evaluate the impact of semantically incorrect augmentations by introducing controlled noise into the CW technique and measuring degradation in minority class performance
2. Cross-Domain Transfer Validation: Apply the same augmentation pipeline to a different imbalanced classification dataset (e.g., medical diagnosis or financial fraud detection) to assess the generalizability of the minority class improvement mechanism
3. Computational Efficiency Analysis: Measure and compare the training time and resource requirements for CW augmentation versus the actual performance gains, establishing a clear cost-benefit threshold for practical deployment