---
ver: rpa2
title: Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs
arxiv_id: '2511.10850'
source_url: https://arxiv.org/abs/2511.10850
tags:
- reasoning
- tulu3
- wang
- zhang
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of transferring specialized reasoning
  skills between independently trained Large Language Models (LLMs) without incurring
  negative interference. The authors propose aligning the parameter spaces of the
  source and target models using the inherent symmetries in Transformer architectures,
  specifically adapting rotation and scaling symmetries for modern Grouped-Query Attention
  (GQA) layers and permutation symmetries for SwiGLU layers.
---

# Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs

## Quick Facts
- arXiv ID: 2511.10850
- Source URL: https://arxiv.org/abs/2511.10850
- Reference count: 30
- Primary result: Parameter space alignment via symmetries improves reasoning skill transfer between LLMs, with Tulu3(w.align)+R surpassing source model on AIME24.

## Executive Summary
This paper addresses the challenge of transferring specialized reasoning skills between independently trained Large Language Models (LLMs) without negative interference. The authors propose aligning the parameter spaces of source and target models using inherent Transformer symmetries—rotation and scaling for Grouped-Query Attention (GQA) layers, and permutation for SwiGLU layers—before applying task arithmetic to transfer skill vectors. By leveraging these parameter space symmetries, the aligned models consistently outperform standard task arithmetic approaches on challenging reasoning benchmarks, with the Tulu3(w.align)+R model even surpassing the source reasoning model on AIME24. This demonstrates that geometric alignment of parameter spaces is critical for preserving and transferring reasoning capabilities across diverged model families.

## Method Summary
The method involves three key steps: (1) Extract a skill vector from the parameter difference between a reasoning-capable source model (Nemotron-Nano) and its instruction-tuned base (Llama-3.1-Instruct); (2) Align the target model (Tulu3) to the base model using parameter space symmetries—permutation for SwiGLU feed-forward layers via Hungarian algorithm, and rotation/scaling for GQA layers via Orthogonal Procrustes and quartic equation; (3) Add the aligned skill vector to the aligned target model and evaluate on reasoning benchmarks. The approach can use either weight-based alignment (static weights) or activation-based alignment (forward-pass activations on calibration data), with the latter better preserving general capabilities like instruction-following.

## Key Results
- Tulu3(w.align)+R achieved 61.2% accuracy on AIME24, surpassing the source model Nemotron-Nano's 59.3%
- Rotation and scaling alignments for GQA layers provided the largest performance gains
- Weight-based alignment slightly outperformed activation-based on reasoning benchmarks (AIME24: 61.2% vs 56.7%)
- Activation-based alignment better preserved instruction-following capabilities (IFEval: 80.1% vs 60.1%)
- Standard task arithmetic without alignment showed negative interference on all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skill vectors can encode and transfer reasoning capabilities between diverged models when parameter spaces are first aligned.
- Mechanism: A task vector τ_reason = θ_source+skill − θ_source captures the parameter deltas from fine-tuning. When added to a target model (θ_target + τ_reason), these deltas—interpreted as skill encodings—can induce analogous capabilities, provided the parameter spaces share a common alignment.
- Core assumption: Skills are sufficiently localized and linearly representable in parameter space; interference is primarily geometric (misalignment) rather than semantic.
- Evidence anchors:
  - [abstract] "Task arithmetic is a powerful technique for transferring skills... but it often suffers from negative interference when models have diverged during training."
  - [section 2.1] "τ_skill = θ_fine-tuned − θ_base... θ_target+reason = θ_target + τ_reason"
  - [corpus] Neighbor work "Skill Expansion and Composition in Parameter Space" suggests parameter-space skill composition is viable but interference is a known challenge.
- Break condition: If skill encodings are entangled across layers in non-linear ways, or if source-target architecture gaps are too large, alignment may not recover transferability.

### Mechanism 2
- Claim: Permutation symmetry in SwiGLU FFN layers allows optimal neuron reordering to reduce misalignment before transfer.
- Mechanism: SwiGLU layers have gate (W_G), up-projection (W_U), and down-projection (W_D) matrices. The intermediate outputs can be permuted without changing the function. Optimal permutation P is found by solving a linear assignment problem (Hungarian algorithm) to minimize weight differences across layers, then applying P to reorder weights.
- Core assumption: Diverged training produces permutation-equivalent but shuffled neuron orderings; optimal assignment can recover correspondence.
- Evidence anchors:
  - [section 2.2.1] "arg min_P (||W_G^(1) − PW_G^(2)||² + ...) = arg max_P ⟨P, W_G^(1)W_G^(2)^⊤ + ...⟩"
  - [section 4/ablation] "the optimal permutations found through the weight alignment process are always the identity (i.e. no neuron permutations). We hypothesize this is because the models haven't diverged enough..."
  - [corpus] Limited direct corpus evidence for SwiGLU-specific permutation alignment; related work focuses on general permutation symmetries.
- Break condition: If models have structurally different neuron specializations (not just reordered), permutation alone cannot align them; if divergence is minimal (as in this study), permutation may yield no gain.

### Mechanism 3
- Claim: Rotation and scaling symmetries in Grouped-Query Attention (GQA) layers enable continuous alignment of query/key/value/output subspaces.
- Mechanism: GQA groups multiple query heads to share key/value heads, creating joint subspaces with rotation symmetry (Orthogonal Procrustes) and scaling symmetry (inverse scaling Q by α and K by 1/α). These are solved via SVD for rotation and a quartic equation for optimal scale.
- Core assumption: The absence of element-wise non-linearities between Q, K, V projections preserves function under orthogonal transformations and scaling.
- Evidence anchors:
  - [section 2.3] "arg min_{R_QKj ∈ SO} Σ_{k∈Gj} ||W_Qk^(1) − R_QKj W_Qk^(2)||² + ..." and scaling via quartic equation.
  - [section 4/ablation] "the biggest gain in accuracy comes from accounting for rotations. Scaling the Q and K layers also brings some minor improvements."
  - [corpus] Neighbor paper "Beyond the permutation symmetry of transformers: The role of rotation for model fusion" explicitly studies rotation for fusion (citation [25] in paper).
- Break condition: If attention subspaces have been fundamentally re-specialized during training (not just rotated/scaled), or if non-linearities are introduced between projections, rotation/scaling alignment will be insufficient.

## Foundational Learning
- Concept: Task Arithmetic
  - Why needed here: Core technique for skill transfer; understanding how task vectors encode skills and how negative interference arises is prerequisite to appreciating why alignment matters.
  - Quick check question: Can you explain why adding a task vector from model A to model B might degrade performance even if the skill is desirable?

- Concept: Parameter Space Symmetries
  - Why needed here: The paper's central contribution is exploiting symmetries (permutation, rotation, scaling) to align diverged models; without this, you cannot understand the alignment procedures.
  - Quick check question: Why can you permute neurons in a feed-forward layer without changing its output function?

- Concept: Grouped-Query Attention (GQA)
  - Why needed here: The paper extends alignment to GQA layers, which share key/value heads across query groups; the alignment math differs from standard multi-head attention.
  - Quick check question: In GQA, what is shared across a group of query heads, and why does this affect how rotation alignment is computed?

## Architecture Onboarding
- Component map:
  1. Source model pair (e.g., Llama-3.1-Instruct → Nemotron-Nano) → extract reasoning skill vector τ_reason.
  2. Target model (Tulu3) → align to reference (Llama-3.1-Instruct) via:
     - SwiGLU: permutation alignment (Hungarian algorithm).
     - GQA: rotation alignment (SVD/Orthogonal Procrustes) + scaling alignment (quartic solve).
  3. Apply τ_reason to aligned target → evaluate on reasoning benchmarks.

- Critical path:
  1. Verify all models share a common base architecture (Llama-3.1 family).
  2. Extract τ_reason = θ_Nemotron − θ_LlamaInstruct.
  3. Align Tulu3 to Llama-3.1-Instruct using either weight-based or activation-based alignment.
  4. Add τ_reason to aligned Tulu3; evaluate on MATH-500, AIME 2024, etc.

- Design tradeoffs:
  - Weight-based alignment: Uses static weights; faster, no data needed, but may miss functional alignment.
  - Activation-based alignment: Uses forward-pass activations on calibration data; better functional correspondence, but requires data and compute.
  - The paper finds weight-based alignment slightly better on AIME 2024 (61.2 vs. 56.7), while activation-based preserves instruction-following better on IFEval (80.1 vs. 60.1).

- Failure signatures:
  - Large accuracy drops on non-reasoning benchmarks (Table 3: IFEval drops from 83.5 → 59.7) indicate negative interference.
  - If rotation alignment yields no gain, models may not have diverged enough (permutation was identity in this study).
  - If scaling alignment causes instability, check for near-zero α solutions from the quartic.

- First 3 experiments:
  1. Reproduce Tulu3 + Reasoning without alignment to confirm baseline negative interference.
  2. Run weight-based alignment (rotation only, then rotation+scaling) to isolate symmetry contributions; expect largest gain from rotation.
  3. Test activation-based alignment on a small calibration set (MMLU val + GSM8K train); compare downstream reasoning vs. instruction-following preservation to choose alignment strategy for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the parameter space alignment approach be effectively generalized to non-Transformer architectures (e.g., Mamba, RWKV) or model families with significantly different pre-training corpora?
- Basis in paper: [explicit] The Conclusion explicitly states: "For future work, we plan to extend this analysis to other model families and skills beyond reasoning."
- Why unresolved: The current study is restricted to the Llama 3.1 family (Transformer-based with GQA and SwiGLU). It is unknown if the symmetry assumptions (permutation, rotation, scaling) hold or yield similar benefits in architectures with different inductive biases or distinct training dynamics.
- What evidence would resolve it: Successful application of the alignment method to non-Transformer models or cross-family transfers (e.g., Llama to Mistral) showing consistent performance gains over non-aligned baselines.

### Open Question 2
- Question: How can the negative interference on general capabilities (such as instruction following) be mitigated during the skill transfer process?
- Basis in paper: [inferred] Appendix C and Table 3 reveal that while reasoning improves, general performance degrades significantly; for example, IFEval accuracy drops from 83.5 (base Tulu3) to 60.1 (Tulu3 w.align + Reasoning).
- Why unresolved: The paper demonstrates that alignment improves reasoning transfer but notes that "task arithmetic approaches are known to suffer from negative interference." The method does not currently prevent the degradation of the target model's original strengths.
- What evidence would resolve it: A modified alignment or merging technique that maintains or improves reasoning benchmarks (e.g., AIME 2024) without statistically significant drops in general benchmarks like IFEval or MMLU Pro.

### Open Question 3
- Question: At what threshold of training divergence does permutation alignment become a critical factor for successful model merging?
- Basis in paper: [inferred] Appendix B notes that optimal permutations found were always the identity, leading to the hypothesis that the models "haven't diverged enough" during their separate training procedures to require hard re-ordering.
- Why unresolved: The paper accounts for permutation but finds it unnecessary for the specific models used (Llama-3.1 derivatives). It is unclear if this is a general property of instruction-tuned models or a specific artifact of the chosen model pair.
- What evidence would resolve it: Experiments using models with greater parameter space divergence (e.g., longer independent fine-tuning runs or different initializations) where non-identity permutations are observed and result in measurable performance improvements over identity mappings.

### Open Question 4
- Question: Why does activation-based alignment preserve instruction-following capabilities significantly better than weight-based alignment?
- Basis in paper: [inferred] Table 3 shows a large performance gap on IFEval between activation-aligned (80.1%) and weight-aligned (60.1%) models, suggesting the alignment method impacts the preservation of functional behaviors differently.
- Why unresolved: The paper explores both approaches but does not provide a theoretical or empirical analysis explaining why weight-alignment leads to "catastrophic forgetting" of instruction-following capabilities while activation-alignment largely preserves them.
- What evidence would resolve it: An analysis of the geometric properties of the task vectors in weight vs. activation space, or a hybrid method that leverages the stability of activation alignment with the reasoning performance of weight alignment.

## Limitations
- Permutation alignment was ineffective because models hadn't diverged enough to require neuron reordering.
- Activation-based alignment requires additional calibration data and forward passes.
- The approach assumes models share a common base architecture and can be aligned using the described symmetries.

## Confidence
- **High Confidence**: Rotation and scaling alignments for GQA layers significantly improve skill transfer; Task arithmetic with alignment outperforms standard task arithmetic on reasoning benchmarks; Negative interference occurs when models have diverged during training; Skills can be encoded as parameter deltas and transferred between models.
- **Medium Confidence**: Permutation alignment for SwiGLU layers provides no benefit in current setup; Activation-based alignment better preserves instruction-following than weight-based; Tulu3(w.align)+R surpasses source model on AIME24.
- **Low Confidence**: Alignment would work for substantially more diverged models; Method generalizes to non-Llama architectures; Approach scales to larger models or different skill types.

## Next Checks
1. **Architecture Transfer Test**: Apply the same alignment methodology to transfer reasoning skills between models with different base architectures (e.g., Llama to Mistral or Qwen) to validate whether the symmetry-based alignment generalizes beyond the Llama-3.1 family.

2. **Divergence Stress Test**: Intentionally train models with larger architectural and parameter differences (different layer counts, attention patterns, or activation functions) and evaluate whether permutation alignment becomes beneficial when models are more substantially diverged.

3. **Skill Composition Validation**: Test whether multiple skill vectors (reasoning + instruction-following + domain knowledge) can be sequentially aligned and transferred without catastrophic interference, particularly examining whether alignment quality degrades with each additional skill transfer.