---
ver: rpa2
title: 'On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context
  Defenses'
arxiv_id: '2506.02978'
source_url: https://arxiv.org/abs/2506.02978
tags:
- tabular
- adversarial
- context
- accuracy
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the adversarial robustness of recent tabular
  foundational models (TabPFN and TabICL) against evasion attacks. The authors demonstrate
  that these models are significantly more vulnerable to state-of-the-art attacks
  like CAA compared to specialized models, with robust accuracy dropping to 7.5% and
  12.5% on LCLD respectively.
---

# On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses

## Quick Facts
- **arXiv ID:** 2506.02978
- **Source URL:** https://arxiv.org/abs/2506.02978
- **Reference count:** 40
- **Primary result:** Tabular foundation models are significantly more vulnerable to adversarial attacks than specialized models, but can be defended via context optimization.

## Executive Summary
This paper evaluates the adversarial robustness of tabular foundation models (TabPFNv2 and TabICL) against state-of-the-art evasion attacks. The authors demonstrate that these models are significantly more vulnerable to attacks like CAA compared to specialized models, with robust accuracy dropping to 7.5% and 12.5% on LCLD respectively. To address these vulnerabilities, they propose a novel Adversarial In-Context Learning (AICL) approach that optimizes the training context rather than model weights. Their results show AICL improves robustness from 12.5% to 37.2% on LCLD, though still below specialized adversarially-trained models at 81.5%.

## Method Summary
The study evaluates adversarial robustness using constrained evasion attacks (CAA) on three tabular datasets (LCLD, URL, WIDS) with context-capped foundation models. The attack uses CAPGD gradient-based perturbations combined with MOEVA multi-objective search (ε=0.5). For defense, AICL optimizes the context by incrementally replacing examples with adversarial variants generated via CPGD (ε=0.3) over 20 epochs. Performance is measured as robust accuracy on the positive class, comparing TabPFNv2 and TabICL against specialized models like STG.

## Key Results
- TabPFNv2 achieves 7.5% robust accuracy and TabICL achieves 12.5% under CAA attack on LCLD
- Specialized models like STG achieve 52.6% robust accuracy, significantly outperforming FMs
- AICL defense improves TabICL robustness from 12.5% to 37.2% on LCLD
- TabPFNv2 can serve as effective surrogate for transferable attacks, achieving 32.2% robust accuracy against RF (vs 27.5% for direct attack)

## Why This Works (Mechanism)

### Mechanism 1: Vulnerability of Tabular FMs to Gradient-Based Evasion Attacks
- **Claim:** Tabular foundation models (TabPFNv2, TabICL) are significantly more vulnerable to constrained evasion attacks than specialized models.
- **Mechanism:** The transformer-based architecture with complex feature encoders exposes differentiable pathways for gradient-based attacks. The attention mechanism between context and test inputs creates a novel attack surface—perturbations to test inputs can significantly degrade accuracy even when the training context remains fixed.
- **Core assumption:** Pre-training on synthetic data does not inherently confer adversarial robustness; the architectural properties (transformer attention, feature tokenization) remain exploitable.
- **Evidence anchors:** [abstract] "small, structured perturbations to test inputs can significantly degrade prediction accuracy, even when training context remain fixed"; [section 4.2, Table 1] Robust accuracy of TabPFNv2 drops to 7.5% and TabICL to 12.5% under CAA on LCLD, compared to STG at 52.6%

### Mechanism 2: Transferability via Foundation Model Surrogates
- **Claim:** Tabular FMs can serve as effective surrogates for generating transferable attacks to conventional models (RF, XGBoost).
- **Mechanism:** The generalization capability of FMs, trained on diverse synthetic data distributions, enables adversarial perturbations to capture broadly applicable vulnerability patterns. Tree-based models show particular susceptibility to FM-generated attacks.
- **Core assumption:** The learned representations in FMs capture decision boundaries that partially overlap with conventional models, enabling cross-architecture transfer.
- **Evidence anchors:** [abstract] "tabular FM can be repurposed to generate transferable evasion to conventional models such as random forests and XGBoost"; [section 5.2, Table 2] TabPFNv2 as source achieves 32.2% robust accuracy against RF vs. original 27.5%

### Mechanism 3: Adversarial In-Context Learning (AICL) for Robustification
- **Claim:** Optimizing the context with adversarial examples (AICL) improves robustness without updating model weights.
- **Mechanism:** AICL incrementally replaces context instances with adversarially perturbed examples, exposing the in-context learner to worst-case scenarios during inference. This heuristic approximates min-max optimization over the context space rather than weight space.
- **Core assumption:** Robustness can be induced through context composition alone; the frozen transformer weights can generalize from adversarial context patterns.
- **Evidence anchors:** [abstract] "improves robustness from 12.5% to 37.2% on LCLD"; [section 6.1, Eq. 2] Formal AICL objective: minimize expected loss over context perturbations δ*

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** Tabular FMs make predictions by conditioning on a context (training examples) provided at inference time, without weight updates. Understanding this is essential to grasp why AICL operates on context rather than weights.
  - **Quick check question:** Can you explain how TabPFN uses attention between context and test instances to make predictions?

- **Concept: Constrained Adversarial Attacks (CAA/CAPGD)**
  - **Why needed here:** Tabular data has domain constraints (feature bounds, relational constraints). CAA combines gradient-based attacks (CAPGD) with multi-objective search (MOEVA) to respect these constraints.
  - **Quick check question:** Why can't standard PGD attacks be directly applied to tabular data with categorical features and relational constraints?

- **Concept: Min-Max Adversarial Training Objective**
  - **Why needed here:** Both AFT and AICL are formulated as min-max problems. The inner maximization finds worst-case perturbations; the outer minimization (over weights or context) improves robustness.
  - **Quick check question:** How does AICL differ from AFT in what the outer minimization optimizes?

## Architecture Onboarding

- **Component map:** Context buffer -> Transformer-based FM (TabPFNv2/TabICL) -> Attack module (CAPGD+MOEVA) -> AICL optimizer
- **Critical path:**
  1. Load pretrained FM (TabPFNv2/TabICL) with default weights
  2. Prepare context: subsample training data (if >10k), handle class imbalance
  3. Generate adversarial examples using CAA with ε=0.5 for evaluation, ε=0.3 for defense training
  4. For AICL: split context, generate adversarials on held-out split, replace context entries iteratively
  5. Evaluate robust accuracy on positive-class adversarial examples

- **Design tradeoffs:**
  - **Context size vs. robustness:** Larger context may decrease robust accuracy (Fig. 7 shows transfer attacks are more effective with larger contexts)
  - **AICL vs. AFT:** AICL preserves clean accuracy better and is computationally cheaper (no gradient updates), but achieves lower robustness than adversarially-trained classical models (37.2% vs. 81.5% on LCLD)
  - **Defense ε vs. attack ε:** Paper uses defense ε=0.3 < attack ε=0.5; stronger defense perturbations may harm clean accuracy

- **Failure signatures:**
  - Robust accuracy near 0% under CAA → model is highly vulnerable (expected for vanilla FMs)
  - AICL robust accuracy ≤ vanilla → context may be corrupted or adversarial generation failing
  - Large variance across seeds in transferability → normal; FM transfer is highly dataset/model dependent
  - Clean accuracy drops significantly post-AICL → context may be dominated by adversarial examples

- **First 3 experiments:**
  1. **Baseline vulnerability assessment:** Run identity, CAPGD, and CAA attacks on TabPFNv2 and TabICL across LCLD, URL, WIDS. Verify robust accuracy matches Table 1 (CAA: ~7.5-40.6% depending on dataset).
  2. **AICL vs. AFT comparison:** Apply both defense methods to TabICL on LCLD with ε=0.3 defense, evaluate against CAA ε=0.5. Confirm AICL outperforms AFT (Table 5: 37.2% vs. 15.9%).
  3. **Transfer attack probe:** Use TabPFNv2 as surrogate to attack XGBoost on URL dataset. Check if transfer attack achieves comparable robust accuracy to direct CAA attack (Table 2: 39.0% vs. 16.3%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Adversarial In-Context Learning (AICL) be effectively combined with orthogonal defense mechanisms to close the robustness gap with specialized models?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "We believe existing defenses are orthogonal to AICL, and could be combined into stronger defenses."
- **Why unresolved:** While AICL improved robustness (e.g., from 12.5% to 37.2% on LCLD), it still significantly underperformed compared to specialized adversarially-trained models (STG, 81.5%).
- **What evidence would resolve it:** Experiments demonstrating that combining AICL with other defenses (e.g., input pre-processing or detection filters) yields robust accuracy comparable to or exceeding specialized models like STG.

### Open Question 2
- **Question:** Does the "non-robust features hypothesis" or "flatness of the loss surface hypothesis" theoretically explain the mechanism behind AICL's success?
- **Basis in paper:** [explicit] The paper notes: "We believe that AICL follows a similar intuition as adversarial training and thus can be assessed through the different theories behind adversarial vulnerability, like the non-robust features hypothesis or the flatness of the loss surface hypothesis."
- **Why unresolved:** The study introduces AICL as a heuristic optimization and validates it empirically, but does not provide theoretical proof linking its performance to these specific ML theories.
- **What evidence would resolve it:** A theoretical analysis or empirical measurements showing that AICL specifically minimizes reliance on non-robust features or enforces a flatter loss landscape compared to standard in-context learning.

### Open Question 3
- **Question:** Is the vulnerability of tabular foundation models to evasion attacks transferable to backdoor and poisoning threat models?
- **Basis in paper:** [explicit] The authors state: "The vulnerability we have uncovered in white-box and transferable evasion attacks can also manifest itself for backdoor and poisoning attacks."
- **Why unresolved:** The paper focuses exclusively on evasion attacks (test-time perturbations) and does not evaluate the models' susceptibility to training-time manipulations.
- **What evidence would resolve it:** An evaluation of TabPFN and TabICL under backdoor or data poisoning attacks to determine if they exhibit similar fragility compared to traditional models.

## Limitations
- AICL's robustness gains are significantly below those of specialized adversarially-trained models (37.2% vs 81.5% on LCLD), suggesting fundamental ceilings for context-based defenses
- Study focuses exclusively on positive-class examples for attack evaluation, potentially missing broader adversarial scenarios
- Transferability results show high variance across datasets, indicating FM surrogates may not be universally effective attack generators

## Confidence
- **High confidence:** The vulnerability findings showing TabPFNv2 (7.5%) and TabICL (12.5%) are significantly weaker than specialized models (52.6% for STG) on LCLD. These results are directly supported by Table 1 with specific numerical values.
- **Medium confidence:** The transferability claim that TabPFNv2 can generate stronger attacks than direct CAA against RF (32.2% vs 27.5% robust accuracy). While supported by Table 2, the variance across datasets suggests this may not generalize universally.
- **Medium confidence:** The AICL defense improving robustness from 12.5% to 37.2% on LCLD. The mechanism is well-specified, but the improvement gap versus specialized models indicates potential fundamental limitations.

## Next Checks
1. **Reproduce baseline vulnerability:** Run CAA attacks with ε=0.5 on TabPFNv2 and TabICL across all three datasets to verify the reported robust accuracy values match Table 1.
2. **Test AICL sensitivity:** Evaluate AICL performance across different context sizes (e.g., 1k, 5k, 10k) to confirm the observed trade-off between context size and robustness.
3. **Cross-dataset transferability:** Generate attacks using TabPFNv2 as surrogate against RF and XGBoost on URL and WIDS datasets to verify if transferability patterns observed on LCLD extend to other domains.