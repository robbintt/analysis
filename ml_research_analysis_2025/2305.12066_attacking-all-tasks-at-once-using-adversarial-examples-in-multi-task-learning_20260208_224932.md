---
ver: rpa2
title: Attacking All Tasks at Once Using Adversarial Examples in Multi-Task Learning
arxiv_id: '2305.12066'
source_url: https://arxiv.org/abs/2305.12066
tags:
- multi-task
- attack
- adversarial
- dgba
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and improves adversarial robustness in multi-task
  learning (MTL) models, which are widely used in vision tasks like semantic segmentation,
  depth estimation, and normal prediction. The authors identify limitations in existing
  methods that adapt single-task attacks to multi-task models and propose a new framework
  called Dynamic Gradient Balancing Attack (DGBA).
---

# Attacking All Tasks at Once Using Adversarial Examples in Multi-Task Learning

## Quick Facts
- arXiv ID: 2305.12066
- Source URL: https://arxiv.org/abs/2305.12066
- Authors: Lijun Zhang; Xiao Liu; Kaleel Mahmood; Caiwen Ding; Hui Guan
- Reference count: 40
- One-line primary result: DGBA achieves up to 80.41% higher attack effectiveness on clean models and up to 18.65% on adversarially trained models compared to baselines.

## Executive Summary
This paper addresses adversarial robustness in multi-task learning (MTL) models by identifying limitations in existing single-task attack adaptations. The authors propose Dynamic Gradient Balancing Attack (DGBA), which formulates multi-task attacks as optimization problems solved via integer linear programming. DGBA dynamically balances gradients from multiple tasks to generate adversarial samples effective across all tasks simultaneously. Experiments on NYUv2 and Tiny-Taxonomy datasets demonstrate significant improvements over baselines, while also revealing a fundamental trade-off between task accuracy and robustness due to parameter sharing.

## Method Summary
The paper introduces Dynamic Gradient Balancing Attack (DGBA) as a framework for generating adversarial examples that simultaneously degrade all tasks in MTL models. DGBA works by computing task-specific gradients and weighting them by the inverse of their current loss values, then aggregating these weighted gradients to form the attack direction. The optimization problem is solved exactly via integer linear programming relaxation, avoiding discretization errors. The method is implemented within existing attack frameworks like APGD/PGD and includes an adversarial training variant (FAT-DGBA) that uses DGBA-generated examples for defense. The approach is evaluated across different parameter sharing levels in MTL architectures.

## Key Results
- DGBA achieves up to 80.41% higher attack effectiveness on clean MTL models compared to baseline methods
- On adversarially trained models, DGBA improves attack success by up to 18.65% over existing approaches
- Parameter sharing in MTL architectures increases adversarial vulnerability by facilitating gradient transfer between tasks
- FAT-DGBA defense reduces attack effectiveness but still leaves models vulnerable with ARP values of 18.08-35.25%

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Gradient Balancing via Inverse Loss Weighting
- **Claim:** Weighting task-specific gradients by the inverse of their current loss value prevents dominant tasks from monopolizing the attack direction.
- **Mechanism:** Standard "Total" attacks sum task gradients, allowing tasks with high-magnitude gradients to dictate the perturbation. DGBA normalizes these gradients ($\nabla L_i / L_i$), ensuring relative performance drop is maximized across all tasks.
- **Core assumption:** The attack optimization problem can be approximated as linear (via Taylor expansion) because the perturbation $\delta$ is sufficiently small ($\|\delta\|_\infty \le \epsilon$).
- **Evidence anchors:** [Section 3.2] "effective attack direction... should be the sum of each task's gradients dynamically weighted by its loss value"
- **Break condition:** If the loss landscape is highly non-linear or $\epsilon$ is excessively large, the linear approximation fails.

### Mechanism 2: Exact Linear Programming Relaxation
- **Claim:** The optimal binary perturbation mask can be found efficiently without discretization error by solving a relaxed LP problem.
- **Mechanism:** The attack is formulated as an Integer Linear Program (ILP). The LP relaxation (allowing continuous values) yields an extreme-point solution that maps exactly to integer values $\{-1, 0, 1\}$ via sign operation.
- **Core assumption:** The objective function is linear and separable coordinate-wise.
- **Evidence anchors:** [Section 3.2] "LP relaxation followed by sign rounding is exact... introducing no discretization error"
- **Break condition:** If the objective changes (e.g., adding non-linear constraints), the "exactness" property might vanish.

### Mechanism 3: Transferability via Shared Parameters
- **Claim:** Parameter sharing in MTL increases adversarial vulnerability by facilitating gradient transfer between tasks.
- **Mechanism:** An attack targeting a specific task naturally affects other tasks if they share backbone weights. As sharing increases, the "transferability ratio" increases, making models less robust even to single-task attacks.
- **Core assumption:** Shared weights imply shared vulnerabilities; perturbations effective on one task's gradient path will perturb the shared feature space used by others.
- **Evidence anchors:** [Section 5] "parameter sharing can undermine model robustness due to increased attack transferability"
- **Break condition:** If task gradients are perfectly orthogonal or negatively correlated in the shared space, transferability might decrease.

## Foundational Learning

- **Concept:** White-box Iterative Attacks (FGSM/PGD/APGD)
  - **Why needed here:** DGBA is a gradient processor that plugs into existing methods like PGD. Understanding the base attack loop is essential to see where DGBA injects weighted gradients.
  - **Quick check question:** Can you explain how Projected Gradient Descent (PGD) updates $x_{adv}$ iteratively, and where the "gradient" term fits in that equation?

- **Concept:** Multi-Task Optimization (Gradient Dominance)
  - **Why needed here:** The paper identifies "gradient dominance" as the failure mode of baseline attacks. Understanding that tasks have different gradient magnitudes is key to why naive summation fails.
  - **Quick check question:** If Task A has a gradient magnitude of 100 and Task B has 0.1, what happens to Task B's objectives if you simply sum the gradients ($\nabla A + \nabla B$) to update a shared model?

- **Concept:** Taylor Expansion / Linearization
  - **Why needed here:** The DGBA objective derivation relies on approximating $L(x+\delta) \approx L(x) + \delta \nabla L$. Understanding this approximation clarifies why optimization can be solved via Linear Programming.
  - **Quick check question:** Why is the second-order term (Hessian) in the Taylor expansion considered negligible in the context of small adversarial perturbations ($\epsilon \le 8$)?

## Architecture Onboarding

- **Component map:** Input -> Backbone (shared encoder) -> Heads (task-specific decoders) -> DGBA Wrapper -> Output
- **Critical path:**
  1. Forward pass input $x$ through MTL model
  2. Calculate individual task losses $L_i$
  3. Compute individual gradients $\nabla_x L_i$
  4. Normalize: Weight each gradient by $1/L_i$
  5. Aggregate: Sum weighted gradients
  6. Update: Apply sign of sum to update $x_{adv}$ (APGD/PGD step)

- **Design tradeoffs:**
  - Accuracy vs. Robustness: Increasing parameter sharing improves clean accuracy but degrades robustness
  - Attack Strength vs. Quality: Higher $\epsilon$ increases ARP but degrades image quality
  - Computation: DGBA adds negligible overhead compared to grid-search methods

- **Failure signatures:**
  - Gradient Masking: If the model is non-differentiable or uses gradient obfuscation, DGBA will fail
  - Stagnation: If loss values $L_i$ are extremely small, the weighting $1/L_i$ can cause instability

- **First 3 experiments:**
  1. Implement "Total" attack and "Single" attack on NYUv2 dataset to verify "Gradient Dominance" phenomenon
  2. Implement DGBA without $1/L_i$ weighting vs. full DGBA and plot ARP difference to isolate dynamic balancing contribution
  3. Train two models (All-Shared vs. Independent), attack both with DGBA, and confirm All-Shared has higher ARP

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DGBA be effectively extended to large-scale multimodal foundation models (e.g., GPT-4o, Gemini) that operate in black-box or API-limited settings?
- **Basis in paper:** [explicit] The conclusion states DGBA extension to black-box or API-limited settings presents a non-trivial challenge requiring gradient-free multi-task attacks.
- **Why unresolved:** DGBA requires gradient access (white-box setting), but commercial foundation models only expose API interfaces without gradient information.
- **What evidence would resolve it:** Demonstrating a gradient-free variant of DGBA that maintains attack effectiveness on multimodal models using only input-output access.

### Open Question 2
- **Question:** What multi-task architectures optimally balance task accuracy gains from parameter sharing against increased adversarial vulnerability?
- **Basis in paper:** [explicit] The paper reveals "a fundamental trade-off between improving task accuracy via parameter sharing across tasks and undermining model robustness due to increased attack transferability from parameter sharing."
- **Why unresolved:** The paper empirically demonstrates the trade-off but does not propose or validate principled architecture design methods that navigate this trade-off.
- **What evidence would resolve it:** A framework or algorithm that produces MTL architectures with provable or empirically validated robustness-accuracy Pareto frontiers.

### Open Question 3
- **Question:** Can defense mechanisms be developed that simultaneously enhance robustness across all tasks without incurring prohibitive accuracy degradation?
- **Basis in paper:** [inferred] Table 3 and E.17 show adversarial training reduces clean accuracy by 13.75-16.53% while only partially recovering robustness.
- **Why unresolved:** Current adversarial training methods adapted from single-task settings show inconsistent protection across tasks and significant clean accuracy drops.
- **What evidence would resolve it:** A defense method achieving near-baseline clean accuracy while providing uniform robustness improvements across all MTL tasks, validated on standard benchmarks.

## Limitations
- The exact hyper-parameter sensitivity of the $1/L_i$ weighting scheme is not fully explored, with extreme loss values potentially causing gradient instability
- The LP relaxation's "exactness" property may not generalize to other multi-task objectives with non-linear constraints
- The paper focuses primarily on dense vision tasks with pixel-wise losses, and transferability to other MTL domains (e.g., NLP) remains unverified

## Confidence

- **High Confidence:** The mathematical derivation of DGBA (gradient balancing and LP relaxation) is rigorous and internally consistent. The empirical results showing DGBA's superiority over baseline attacks are robust across datasets and architectures.
- **Medium Confidence:** The analysis of parameter sharing's impact on robustness is well-supported by experiments, but the causal mechanism could be more deeply explored. The trade-off between accuracy and robustness is documented but not fully explained from a theoretical perspective.
- **Low Confidence:** The paper's claims about DGBA's efficiency relative to grid-search methods are based on theoretical complexity rather than empirical runtime measurements. The exact conditions under which the LP relaxation remains "exact" for different multi-task formulations are not thoroughly examined.

## Next Checks

1. **Hyper-parameter Sensitivity Analysis:** Systematically vary the loss weighting coefficient (beyond the fixed $1/L_i$) and measure ARP across different $\epsilon$ values to determine optimal balance points.

2. **Runtime Benchmark:** Implement DGBA alongside the grid-search baseline (WeightedTotal) and measure actual wall-clock time per attack iteration, including memory usage and gradient computation overhead.

3. **Generalization Test:** Apply DGBA to a non-vision MTL setting (e.g., multi-task NLP models with parameter sharing) and verify whether the gradient balancing mechanism still provides advantages over standard Total attacks.