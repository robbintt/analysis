---
ver: rpa2
title: Should LLM Safety Be More Than Refusing Harmful Instructions?
arxiv_id: '2506.02442'
source_url: https://arxiv.org/abs/2506.02442
tags:
- safety
- harmful
- text
- decryption
- cipher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Model (LLM) safety when processing
  encrypted text, revealing critical vulnerabilities through a two-dimensional framework
  assessing instruction refusal and generation safety. Experiments show that while
  LLMs can decrypt classical ciphers present in their training data, this capability
  creates safety risks where defense mechanisms fail on at least one dimension, leading
  to either over-refusal of benign content or unsafe responses to adversarial inputs.
---

# Should LLM Safety Be More Than Refusing Harmful Instructions?

## Quick Facts
- arXiv ID: 2506.02442
- Source URL: https://arxiv.org/abs/2506.02442
- Authors: Utsav Maskey; Mark Dras; Usman Naseem
- Reference count: 34
- Primary result: LLM safety mechanisms fail on encrypted text through mismatched-generalization and asymmetric D1/D2 tradeoffs

## Executive Summary
This study reveals critical vulnerabilities in LLM safety mechanisms when processing encrypted text through a two-dimensional framework evaluating instruction refusal (D1) and generation safety (D2). The research demonstrates that current safety approaches fail because they cannot detect harmful intent in ciphers (pre-LLM mechanisms) or over-refuse benign encrypted content (post-LLM mechanisms). Key findings show that safety training coverage is narrower than pretraining corpus coverage for obfuscation methods, leading to situations where models successfully decrypt harmful instructions but safety mechanisms fail to recognize or appropriately handle them.

## Method Summary
The study evaluates 5 LLMs (Claude-3.5 Sonnet, GPT-4o, GPT-4o-mini, Gemini 1.5 Pro, Mistral Large) on 2,502 samples across 9 encryption methods (Caesar, Atbash, Morse, Bacon, Rail Fence, Vigenère, Playfair, RSA, AES). Each model decrypts benign texts, harmful instructions, and harmful responses encrypted with these ciphers. Safety is measured through Δ_IR (instruction refusal performance gap) and Δ_resp (response generation suppression gap) compared to benign decryption. Five defense mechanisms are tested: perplexity filtering, re-tokenization, Self-Reminder, Self-Examination, and LLaMA Guard. Decryption performance is evaluated using Exact Match, BLEU, and Normalized Levenshtein metrics.

## Key Results
- LLMs decrypt only ciphers present in pretraining corpora, failing on novel obfuscation methods
- Safety mechanisms exhibit asymmetric behavior, prioritizing either instruction refusal or response suppression but not both optimally
- Post-LLM mechanisms like Self-Examination cause over-refusal (ΔIR = +0.42) while pre-LLM methods like re-tokenization facilitate harmful responses (Δresp = -0.16)
- Exact Match and BLEU metrics show significant disparities, indicating partial safety failures where exact harmful content is suppressed but partial outputs persist

## Why This Works (Mechanism)

### Mechanism 1: Mismatched-Generalization Safety Failure
- Safety training coverage is narrower than pretraining corpus coverage for obfuscation methods
- Models decrypt ciphers from pretraining but safety mechanisms fail to recognize harmful intent in these formats
- Break condition: Explicit safety training on cipher-formatted harmful examples

### Mechanism 2: Two-Dimensional Safety Tradeoff (D1 vs D2)
- LLMs prioritize either instruction refusal or response suppression, but not both optimally
- Asymmetric safety behavior arises because D1 and D2 objectives are not jointly optimized
- Break condition: Independent optimization of D1 and D2 with coordination signals

### Mechanism 3: Exact vs. Partial Suppression Gap
- Safety mechanisms suppress exact reproductions more aggressively than partial outputs
- EM-BLEU disparity indicates safety triggers on surface patterns rather than semantic intent
- Break condition: Semantic similarity metrics replace exact string matching in safety training

## Foundational Learning

- **Cipher-Based Jailbreaking (Mismatched Generalization)**: Understanding that safety training generalizes poorly to out-of-distribution inputs where capabilities exist. Quick check: If an LLM was trained on Morse code during pretraining but never saw harmful instructions in Morse during safety training, will it refuse a Morse-encoded harmful request?

- **Over-Refusal (Over-Alignment)**: Explains why benign encrypted queries get rejected—safety mechanisms trigger on surface patterns without understanding intent. Quick check: A user submits an encrypted poem for decryption. The model refuses. Is this a D1 failure or intended safety behavior?

- **Exact Match vs. BLEU for Safety Evaluation**: EM alone masks partial safety failures; BLEU reveals when models produce "close but not exact" harmful outputs. Quick check: A model outputs "Sure, here's how you make a b0mb" (with zero substituted). EM=0, BLEU>0. Is this a safety success or failure?

## Architecture Onboarding

- **Component map**: Encrypted input → Pre-LLM filter (D1) → LLM decryption → Post-LLM filter (D2) → Output
- **Critical path**: The paper shows current systems fail at Pre-LLM (cannot detect harmful intent in ciphers) or Post-LLM (over-refuse benign content)
- **Design tradeoffs**: Pre-LLM filters have low latency but lack semantic comprehension; Post-LLM filters suppress harmful responses but cause over-refusal; coordinated D1+D2 achieves optimal safety
- **Failure signatures**: Over-refusal (high ΔIR with low Δutility), under-refusal (low ΔIR with negative Δresp), utility loss (perplexity filter drops utility by 0.22-0.27 EM)
- **First 3 experiments**: 1) Baseline cipher capability test measuring EM/BLEU decryption accuracy; 2) D1 vs D2 asymmetry probe comparing harmful instructions vs responses; 3) Defense mechanism sweep measuring ΔIR, Δresp, and Δutility

## Open Questions the Paper Calls Out

- **Open Question 1**: Can knowledge distillation transfer decryption capabilities to lightweight specialized safety filters that achieve semantic comprehension of encrypted content sufficient for effective D1 safety?
- **Open Question 2**: What coordination mechanism between D1 and D2 safety layers could minimize instruction over-refusal while maximizing harmful response suppression?
- **Open Question 3**: How does the EM-BLEU disparity in partial decryption affect the reliability of current LLM safety evaluations for encrypted content?
- **Open Question 4**: Do the observed safety vulnerabilities with classical ciphers generalize to more sophisticated obfuscation methods and multimodal adversarial inputs?

## Limitations

- Cipher capability assessment may not capture semantic understanding in long-tail obfuscated formats (e.g., Bacon cipher's 7.93× token expansion)
- Safety evaluation uses restricted LLM sample without open-weight models with tunable safety parameters
- Paper assumes cipher decryption capability exists independently of safety training without empirical verification
- Benign text generation procedure is unspecified beyond "LLM-generated"

## Confidence

- **High confidence**: Mismatched-generalization mechanism is well-supported by EM/BLEU patterns showing models decrypt only ciphers present in training corpora
- **Medium confidence**: Two-dimensional safety tradeoff has clear statistical support but limited corpus validation
- **Low confidence**: Exact vs. partial suppression gap lacks direct corpus support and may reflect surface-level pattern matching

## Next Checks

1. **Adversarial training robustness**: Apply cipher-based jailbreak dataset to a model with known safety fine-tuning and measure if Δ_IR/Δ_resp patterns persist
2. **Semantic vs. lexical safety triggers**: Implement controlled experiment where harmful instructions are encrypted using semantically similar benign ciphers to determine if safety mechanisms operate on semantic understanding
3. **Open-weight model comparison**: Evaluate safety mechanisms on LLaMA or Mistral-7B with and without safety adapters to determine if over-refusal patterns are inherent to model architecture or specific to closed commercial systems