---
ver: rpa2
title: 'ChartCap: Mitigating Hallucination of Dense Chart Captioning'
arxiv_id: '2508.03164'
source_url: https://arxiv.org/abs/2508.03164
tags:
- chart
- caption
- captions
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CHART CAP, a 565K real-world chart image dataset
  with dense, type-specific captions that exclude extraneous information and highlight
  structural elements and key insights. The dataset is built using a four-stage pipeline
  that generates captions using only the discernible data from the chart, followed
  by a cycle consistency-based human verification process.
---

# ChartCap: Mitigating Hallucination of Dense Chart Captioning

## Quick Facts
- **arXiv ID**: 2508.03164
- **Source URL**: https://arxiv.org/abs/2508.03164
- **Reference count**: 40
- **Primary result**: A 565K real-world chart image dataset with dense, type-specific captions that reduce hallucination and improve caption quality over both open-source and proprietary models.

## Executive Summary
ChartCap introduces a novel dataset and methodology to address the persistent problem of hallucination in chart captioning. The paper argues that existing datasets are prone to hallucination because they pair chart images with captions extracted from source documents without verification. ChartCap's solution is a four-stage pipeline that generates captions using only the discernible data from the chart, guided by a type-specific schema, followed by cycle consistency-based human verification. The resulting dataset enables VLMs to generate more accurate and informative captions, outperforming both open-source and proprietary baselines on reference-based metrics and a novel Visual Consistency Score.

## Method Summary
ChartCap is built using a four-stage pipeline: (1) filter non-chart images using InternVL2.5-8B, (2) classify chart type and extract title with GPT-4o, (3) extract structured information using GPT-4o for coarse-grained tasks and Claude 3.5 Sonnet for fine-grained tasks, and (4) finalize captions with GPT-4o-mini. The dataset covers nine chart types with a schema that emphasizes structural elements and key insights. Models are fine-tuned on this dataset using LoRA, and caption quality is evaluated using a novel Visual Consistency Score (VCS) that measures the similarity between the original chart and one regenerated from the caption.

## Key Results
- ChartCap dataset contains 565K chart-caption pairs with dense, type-specific captions averaging 231 words.
- Models fine-tuned on ChartCap outperform both open-source and proprietary baselines on reference-based metrics and VCS.
- Human evaluation shows ChartCap-trained models generate captions that are preferred over even human-annotated captions.

## Why This Works (Mechanism)

### Mechanism 1: Constraint via Schema-Driven Caption Generation
- Claim: Reducing hallucination in chart captioning is achieved by constraining the model to generate descriptions using only *discernible data* from the chart image, guided by a type-specific caption schema.
- Mechanism: A four-stage pipeline forces the caption to be built from structured, verifiable visual elements, explicitly excluding information not present in the chart.
- Core assumption: Hallucination in chart captioning is primarily driven by training data that contains information not visually verifiable in the chart image.
- Evidence anchors: [abstract] captions are built using "only the discernible data from the chart." [section 1] argues existing datasets contain "extraneous information... mainly because charts are usually embedded in source documents."
- Break condition: This mechanism will fail if the vision model used in the pipeline cannot accurately extract the specified structural elements or key insights from the chart image.

### Mechanism 2: Verification through Reconstructability (Cycle Consistency)
- Claim: The quality and accuracy of a generated caption can be effectively and efficiently verified by its ability to be used to regenerate a chart visually similar to the original.
- Mechanism: The proposed "Visual Consistency Score" (VCS) and the human verification process use a cycle consistency approach, leveraging the deterministic nature of charts (code-to-image) for evaluation.
- Core assumption: A caption is considered "high-quality" if it contains sufficient and accurate information to reconstruct the original chart.
- Evidence anchors: [abstract] "A novel Visual Consistency Score (VCS) metric is proposed evaluate caption quality by measuring the similarity between the chart regenerated from a caption and the original chart."
- Break condition: This mechanism assumes the caption-to-code LLM is faithful and does not hallucinate or omit information during the translation.

### Mechanism 3: Learning from Dense, Type-Specific Supervision
- Claim: Fine-tuning a Vision Language Model (VLM) on a large-scale dataset of dense, type-specific captions improves its ability to generate informative and accurate chart descriptions.
- Mechanism: The CHARTCAP dataset provides 565K examples where the captions are not only accurate but also dense and follow a specific schema for nine chart types, providing a rich supervisory signal.
- Core assumption: A large, high-quality dataset with specific, dense examples is sufficient to overcome the limitations of pre-trained VLMs on the chart captioning task.
- Evidence anchors: [abstract] "Extensive experiments show that models fine-tuned on CHARTCAP generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models..."
- Break condition: Performance gains may not generalize to chart types or styles not well-represented in the training data.

## Foundational Learning
- **Concept**: Vision-Language Models (VLMs) & Multimodal Alignment
  - Why needed here: ChartCap is fundamentally a fine-tuning task for VLMs. Understanding how these models map visual input (chart image) to textual output (caption) is core to grasping the problem (hallucination due to misalignment) and the solution (fine-tuning on aligned data).
  - Quick check question: Can you explain why a VLM might "hallucinate" a data trend that isn't in the chart image?
- **Concept**: Cycle Consistency
  - Why needed here: This is the core principle behind both the VCS metric and the human verification pipeline. The idea is that a perfect transformation chain (Image A → Caption A → Code A → Image B) should result in Image B being identical to Image A.
  - Quick check question: In a cycle consistency framework, if the reconstruction step is poor, will the evaluation metric be reliable?
- **Concept**: Reference-Based vs. Reference-Free Evaluation
  - Why needed here: The paper critiques existing reference-based metrics (BLEU, ROUGE) for chart captioning and introduces a reference-free metric (VCS). Understanding the distinction is key to understanding the paper's contribution to evaluation.
  - Quick check question: Why does the paper argue that existing metrics like BERTScore are insufficient for evaluating chart caption quality?

## Architecture Onboarding
- **Component map**:
  Raw charts -> Filter (InternVL2.5-8B) -> Classify/Extract (GPT-4o) -> Retrieve Info (GPT-4o + Claude 3.5 Sonnet) -> Finalize (GPT-4o-mini) -> Captions
- **Critical path**:
  1. **Data Generation**: The quality of the final model depends entirely on the fidelity of the four-stage pipeline.
  2. **Verification**: The cycle consistency check acts as the quality gate, determining the final test set used for benchmarking.
  3. **Fine-Tuning**: This is the standard application of the created dataset; the value is unlocked by the data.
- **Design tradeoffs**:
  - **Automation vs. Accuracy**: The pipeline uses a mix of open-source and proprietary models to balance cost and performance.
  - **Speed vs. Precision in Evaluation**: Cycle consistency verification is 24x faster than human review but is not perfect.
  - **Schema Rigidity vs. Generality**: Defining a 9-type schema ensures consistency and density but limits the dataset's ability to handle novel chart types.
- **Failure signatures**:
  - **Pipeline Hallucination**: If the models in Stage 3 misread a chart, the error will be baked into the "ground truth" caption.
  - **Code Generation Failure**: In the VCS metric, if the caption is good but the LLM fails to write working Python code, the metric will fail.
  - **Over-simplification**: The caption might be factually correct but fail the "dense" requirement, leading to a low VCS score.
- **First 3 experiments**:
  1. **Baseline Fine-Tuning**: Train a base VLM on CHARTCAP and compare its performance against the same base model and a strong proprietary baseline.
  2. **Ablation on Caption Density/Schema**: Create a variant of the training set with less dense captions or without the type-specific schema and retrain.
  3. **Metric Validation via Human Agreement**: Run a human evaluation comparing model-generated captions and check the agreement rate of human preference with the VCS score vs. traditional metrics.

## Open Questions the Paper Calls Out
- The authors explicitly identify that ChartCap's caption schema, built upon nine chart types, restricts the diversity of chart types covered and limits the dataset's ability to handle novel chart types.

## Limitations
- The dataset is limited to nine chart types, potentially limiting generalization to more complex or compound visualizations.
- The pipeline relies on proprietary models (GPT-4o, Claude 3.5 Sonnet) without detailed error analysis, which could introduce undetected hallucinations.
- The extreme density of captions (avg. 231 words) may not be optimal for all use cases, potentially overwhelming users seeking quick insights.

## Confidence
- **High Confidence**: The pipeline architecture and evaluation methodology are clearly described and logically sound. The claim that fine-tuning on CHARTCAP improves performance over baseline models is well-supported by quantitative results.
- **Medium Confidence**: The claim that CHARTCAP captions are "more accurate and informative" than human-annotated captions relies on human evaluation, which can be subjective.
- **Low Confidence**: The claim that CHARTCAP completely eliminates hallucination is not made, but the extent to which the dataset mitigates this issue in unseen chart types or styles is uncertain.

## Next Checks
1. **Error Analysis of Pipeline Models**: Conduct a systematic error analysis of each stage in the caption generation pipeline to quantify hallucination rates and identify failure modes.
2. **Generalization Test on Unseen Chart Types**: Evaluate the fine-tuned models on a held-out set of chart types not included in the 9-schema to assess generalization.
3. **Human Evaluation on VCS Correlation**: Perform a human evaluation to determine the correlation between VCS scores and human judgments of caption accuracy and informativeness, particularly for edge cases where visual similarity may not capture semantic correctness.