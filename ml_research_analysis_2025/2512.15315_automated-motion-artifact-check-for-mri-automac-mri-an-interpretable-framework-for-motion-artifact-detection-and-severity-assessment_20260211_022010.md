---
ver: rpa2
title: 'Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework
  for Motion Artifact Detection and Severity Assessment'
arxiv_id: '2512.15315'
source_url: https://arxiv.org/abs/2512.15315
tags:
- motion
- grade
- learning
- proposed
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoMAC-MRI, a framework for grading motion
  artifacts in MRI slices across multiple contrasts and orientations. It uses supervised
  contrastive learning to learn a discriminative feature space and computes grade-specific
  affinity scores for interpretability.
---

# Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment

## Quick Facts
- arXiv ID: 2512.15315
- Source URL: https://arxiv.org/abs/2512.15315
- Reference count: 7
- Primary result: 84.0% overall accuracy on 3-class motion artifact grading for brain MRI slices

## Executive Summary
AutoMAC-MRI presents a framework for detecting and grading motion artifacts in 2D MRI slices using supervised contrastive learning (SupCon) and interpretable affinity scoring. The method achieves 84.0% overall accuracy across 5,304 annotated brain MRI slices, outperforming self-supervised baselines and fully supervised alternatives. By learning discriminative feature representations and computing per-grade cosine similarity scores, the framework provides both accurate classification and interpretable severity assessment aligned with expert judgment.

## Method Summary
The framework uses a two-stage training approach: first, a ResNet-18 encoder is trained with supervised contrastive learning to create well-separated feature clusters for three motion grades (No Motion, Subtle Motion, Severe Motion). The encoder weights are then frozen, and a lightweight MLP classifier is trained on the embeddings. Motion grade-specific affinity scores (MoGrAS) are computed using cosine similarity between test embeddings and median template vectors per class, providing interpretable severity scoring.

## Key Results
- 84.0% overall accuracy on 3-class motion artifact grading (No Motion, Subtle Motion, Severe Motion)
- Outperforms fully supervised 3-class network (83.2%) and SimCLR self-supervised baseline (68.2%)
- Demonstrates clear cluster separation in t-SNE visualizations with minimal inter-class overlap
- MoGrAS scores show monotonic decrease with increasing motion severity, validating interpretability

## Why This Works (Mechanism)

### Mechanism 1: Supervised Contrastive Learning Creates Superior Feature Separation
SupCon loss explicitly pulls same-class samples together while pushing different-class samples apart in the 512-D embedding space, creating well-separated clusters that enable both accurate classification and interpretable distance-based scoring. The approach works because motion severity grades form semantically coherent clusters that can be separated despite variability across MR contrasts and orientations. Evidence shows distinct t-SNE clusters with clear boundaries for SupCon versus substantial inter-class mixing for SimCLR.

### Mechanism 2: Template-Based Cosine Similarity Scoring Provides Interpretability
Median feature vectors computed per grade serve as class prototypes, and cosine similarity between test embeddings and each prototype quantifies grade-specific affinity. This makes predictions transparent and interpretable. The mechanism assumes the median embedding meaningfully represents central tendency of each motion grade's feature distribution, with evidence showing MoGrAS peaks at correct grades and decreases with severity.

### Mechanism 3: Two-Stage Training Preserves Discriminative Features
Separating representation learning from classification prevents classifier training from degrading the carefully constructed cluster structure. The approach assumes SupCon-learned features are sufficiently discriminative that a lightweight linear classifier suffices. Evidence shows 84.0% accuracy with frozen encoder versus 83.2% fully supervised and 68.2% SimCLR.

## Foundational Learning

- **Concept: Supervised vs Self-Supervised Contrastive Learning**
  - Why needed here: Understanding why SupCon (84.0%) dramatically outperforms SimCLR (68.2%) requires knowing how label information changes what constitutes positive/negative pairs
  - Quick check question: In SimCLR, would two different "severe motion" images be treated as positives or negatives for each other? How does SupCon change this?

- **Concept: Embedding Space Geometry and Cosine Similarity**
  - Why needed here: The entire MoGrAS interpretability mechanism relies on understanding vector proximity, cosine similarity bounds [-1, +1], and cluster compactness
  - Quick check question: If two embeddings have cosine similarity 0.95, what does high MoGrAS imply? What could a negative similarity indicate?

- **Concept: Class Templates via Median Aggregation**
  - Why needed here: Template construction affects all downstream scoring; understanding robustness to outliers is critical
  - Quick check question: Why choose median over mean for template vectors when one severe motion image has an unusual feature vector?

## Architecture Onboarding

- **Component map**: Input MRI slice -> ResNet-18 encoder + 2 FC layers -> 512-D embedding -> SupCon loss (Stage 1) or MLP classifier (Stage 2) -> Grade prediction + MoGrAS scores
- **Critical path**: Expert annotation → Stage 1 SupCon training → Compute median templates per class → Stage 2 MLP training → Inference outputs grade + 3 MoGrAS scores
- **Design tradeoffs**:
  - 3-class vs binary: Clinical granularity vs harder boundaries; "Subtle" is inherently ambiguous
  - Freeze encoder: Preserves cluster structure vs limits task-specific adaptation
  - Median templates: Robustness to outliers vs potentially suboptimal centroids if distribution is multimodal
  - Slice-level scoring: Granular QC vs no volumetric context
- **Failure signatures**:
  - t-SNE shows poor separation: SupCon not converging; check temperature hyperparameter, batch composition
  - MoGrAS doesn't peak at correct grade: Templates unrepresentative; try per-contrast templates or increase training data
  - High accuracy but poor recall on Severe: Class imbalance or threshold calibration issue
- **First 3 experiments**:
  1. Reproduce t-SNE visualization (Fig 2) on validation embeddings to verify cluster separation before proceeding
  2. Generate MoGrAS violin plots (Fig 4) to confirm score distributions align with ground-truth grades
  3. Run per-contrast accuracy breakdown to identify any systematic performance gaps across T1-w, T2-w, PD-w, FLAIR

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but has several implicit limitations regarding generalizability, clinical validation, and multi-site robustness that remain unaddressed.

## Limitations

- **Limited anatomical scope**: Only tested on brain MRI; performance on other anatomies (spine, cardiac) remains untested
- **Single-annotator labeling**: All 5,304 slices annotated by one specialist; inter-rater reliability and consensus labeling not evaluated
- **No operational guidance**: Lacks decision thresholds for clinical workflow integration or false-positive tolerance specifications

## Confidence

- **High Confidence**: SupCon outperforming SimCLR and baseline supervised models; MoGrAS interpretability mechanism; feature space separation demonstrated via t-SNE
- **Medium Confidence**: Absolute accuracy numbers without confidence intervals or cross-validation; generalization claims not extensively validated
- **Low Confidence**: Clinical utility claims; lack of radiologist time savings metrics; no comparison to existing clinical motion assessment tools

## Next Checks

1. **Cross-site validation**: Test model on MRI data from different scanners/vendors/protocols to assess robustness to acquisition variability
2. **Multi-anatomy testing**: Evaluate performance on non-brain MRI sequences (spine, cardiac) to determine anatomical generalizability
3. **Expert workflow study**: Measure actual radiologist time savings and inter-reader agreement improvement when using AutoMAC-MRI for QC tasks