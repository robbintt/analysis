---
ver: rpa2
title: 'QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification'
arxiv_id: '2512.16960'
source_url: https://arxiv.org/abs/2512.16960
tags:
- qsmote
- copies
- kpgm
- quantum
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses imbalanced dataset classification by proposing
  three novel Quantum Synthetic Minority Oversampling Technique (QSMOTE) variants:
  KNN-based, Fidelity-based, and Margin-based. These methods integrate quantum-inspired
  principles such as fidelity weighting, centroid-driven generation, and margin-aware
  filtering to generate balanced training data.'
---

# QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification

## Quick Facts
- arXiv ID: 2512.16960
- Source URL: https://arxiv.org/abs/2512.16960
- Reference count: 23
- Primary result: Quantum-inspired oversampling (QSMOTE) and classification (PGM/kPGM) outperform Random Forest on Telco Churn with accuracy 0.8512 and F1 0.8234

## Executive Summary
This paper addresses imbalanced dataset classification by proposing three novel Quantum Synthetic Minority Oversampling Technique (QSMOTE) variants: KNN-based, Fidelity-based, and Margin-based. These methods integrate quantum-inspired principles such as fidelity weighting, centroid-driven generation, and margin-aware filtering to generate balanced training data. The oversampled data is then used to train quantum-inspired classifiers, Pretty Good Measurement (PGM) and kernelized PGM (KPGM), which leverage Hilbert-space geometry for classification. Experiments on the Telco Customer Churn dataset show that both PGM and KPGM classifiers consistently outperform the Random Forest baseline. PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and stable performance across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These results highlight the effectiveness of quantum-inspired oversampling and classification in improving recall and balanced performance for imbalanced learning tasks.

## Method Summary
The method combines quantum-inspired oversampling (QSMOTE) with quantum-inspired classifiers (PGM/kPGM). QSMOTE variants include KNN-based (interpolating between minority samples and neighbors), Fidelity-based (generating toward cluster centroids weighted by quantum fidelity), and Margin-based (filtering synthetic samples based on classifier confidence). The oversampled data is encoded as quantum states using amplitude or stereo encoding, then classified using PGM (density matrix operations) or kPGM (kernel-based approximation). The key innovation is using multiple quantum copies (n_copies) to lift data into higher-dimensional Hilbert spaces for improved class separation, while kPGM provides computational efficiency by working with Gram matrices instead of full density matrices.

## Key Results
- PGM with stereo encoding and n_copies=2 achieves highest accuracy (0.8512) and F1-score (0.8234)
- KPGM shows competitive performance (0.8511 stereo, 0.8483 amplitude) with more stable behavior across QSMOTE variants
- Both PGM and KPGM consistently outperform Random Forest baseline
- Fidelity-based and Margin-based QSMOTE outperform KNN-based variant
- PGM is more sensitive to QSMOTE choice than kPGM
- Recall peaks at 0.8594 with amplitude encoding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing the number of quantum copies ($n_{copies}$) in PGM/kPGM may improve class separability by implicitly lifting data into higher-dimensional non-linear feature spaces, analogous to polynomial kernels.
- **Mechanism:** Data is encoded as quantum states $\rho$. Using tensor products ($\rho^{\otimes n}$) increases the dimensionality of the Hilbert space. The PGM measurement then operates on these "lifted" states, allowing linearly inseparable classes in the original space to be distinguished in the higher-dimensional space.
- **Core assumption:** The quantum-inspired Born rule probability remains a valid proxy for class membership likelihood in the lifted tensor space.
- **Evidence anchors:**
  - [abstract] "Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed."
  - [section] "The use of multiple copies ($n > 1$) embeds data into a higher-dimensional Hilbert space... This typically improves class separation... though at higher computational cost."
  - [corpus] "Boosting Classification with Quantum-Inspired Augmentations" (Neighbor ID 41127) explores similar perturbation-based augmentations, supporting the general non-linear mapping premise.
- **Break condition:** If the dataset is inherently linearly separable, increasing $n_{copies}$ may introduce unnecessary complexity and overfitting without accuracy gains.

### Mechanism 2
- **Claim:** kPGM achieves computational efficiency over standard PGM by reformulating matrix operations in terms of Gram matrices, shifting complexity dependence from feature dimension to sample size.
- **Mechanism:** Instead of computing the inverse square root of the density matrix (scaling with feature dim $D$), kPGM uses the kernel trick to compute similarities via dot products (scaling with sample count $N$). This allows the quantum-inspired geometric logic to run on classical hardware for larger feature sets.
- **Core assumption:** The kernel-based approximation preserves the necessary probabilistic geometry of the full density matrix formulation.
- **Evidence anchors:**
  - [section] "...kPGM works with Gram matrices... reduces complexity to depend on the number of samples $N$ rather than the feature dimension $D$."
  - [abstract] "...kPGM demonstrates competitive and more stable behavior across QSMOTE variants..."
  - [corpus] Corpus evidence specific to kPGM complexity scaling is weak in the provided neighbors, though "Quantum Inspired Encoding Strategies" (Neighbor ID 39949) discusses general encoding efficiency.
- **Break condition:** Performance may degrade if the sample size $N$ becomes massive, as the Gram matrix inversion becomes the bottleneck.

### Mechanism 3
- **Claim:** Fidelity-based and Margin-based QSMOTE improve minority class recall by enforcing geometric compactness and decision boundary clarity, respectively.
- **Mechanism:** Fidelity-based QSMOTE scales synthetic sample generation toward the minority centroid using quantum fidelity (cosine similarity), reducing noise. Margin-based QSMOTE acts as a filter, discarding synthetic points that fall within a low-confidence zone of a preliminary classifier.
- **Core assumption:** Standard SMOTE generates noisy samples that confuse the classifier; filtering or guiding these samples via quantum-inspired metrics mitigates this noise.
- **Evidence anchors:**
  - [abstract] "...integrate quantum-inspired principles such as fidelity weighting... and margin-aware filtering to generate balanced training data."
  - [section] "Margin-based QSMOTE... ensures that only confidently classified synthetic points are included, thus improving decision boundary clarity."
  - [corpus] "Enhancing Machine Learning for Imbalanced Medical Data... (QI-SMOTE)" (Neighbor ID 53439) validates the general approach of quantum-inspired oversampling for improving performance on imbalanced data.
- **Break condition:** If the minority class is not clustered (e.g., multimodal distribution with disjoint centroids), Fidelity-based generation might pull samples toward a meaningless average centroid, reducing validity.

## Foundational Learning

- **Concept: Quantum State Discrimination (QSD)**
  - **Why needed here:** The core PGM classifier is framed as a QSD problem where classification is viewed as distinguishing between overlapping probability distributions (quantum states).
  - **Quick check question:** Can you explain why the "Pretty Good Measurement" is an approximate rather than optimal strategy for distinguishing non-orthogonal quantum states?

- **Concept: The Kernel Trick**
  - **Why needed here:** Essential for understanding kPGM. The paper explicitly contrasts PGM (density matrices) with kPGM (kernel/Gram matrices) to explain the efficiency gains.
  - **Quick check question:** How does kPGM utilize inner products to avoid explicitly constructing the high-dimensional Hilbert space?

- **Concept: SMOTE (Synthetic Minority Over-sampling Technique)**
  - **Why needed here:** The proposed QSMOTE is a variant of SMOTE. Understanding the baseline mechanism (interpolation) is required to see how QSMOTE modifies it with fidelity and margin constraints.
  - **Quick check question:** What is the primary failure mode of standard SMOTE in high-dimensional spaces that QSMOTE attempts to solve?

## Architecture Onboarding

- **Component map:** Imbalanced dataset -> QSMOTE oversampler -> Quantum encoder -> PGM/kPGM classifier -> Performance metrics
- **Critical path:** The performance gain relies on the synergy between the **Oversampler** (creating clean minority samples) and the **$n_{copies}$ parameter** (lifting these samples into a separable space).
- **Design tradeoffs:**
  - **PGM vs. kPGM:** PGM is more theoretically faithful to quantum mechanics but computationally expensive ($O(D^3)$). kPGM is faster ($O(N^2)$) and more stable but technically an approximation.
  - **Encoding:** Stereo encoding with $n=2$ yielded the highest accuracy (0.8512), while Amplitude encoding yielded the highest recall (0.8594).
- **Failure signatures:**
  - **Low Recall with KNN-QSMOTE:** Tables III-VI show KNN-based QSMOTE consistently failing (F1 < 0.6) compared to Margin/Fidelity. *Action:* Avoid KNN-variant for sparse or complex boundaries.
  - **High Variance:** PGM shows higher sensitivity to the specific QSMOTE variant than kPGM.
- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce the PGM vs. RF comparison on the Telco dataset using standard SMOTE to quantify the "quantum advantage" of the classifier alone.
  2. **Ablation on Copies:** Run PGM with $n_{copies} \in \{1, 2, 3\}$ using Margin-QSMOTE to verify the marginal performance gain vs. computational cost scaling.
  3. **Stability Test:** Compare kPGM vs. PGM on a dataset with higher feature dimension (e.g., >50 features) to validate kPGM's claimed complexity advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of the proposed QSMOTE-PGM/kPGM framework generalize to datasets with significantly higher dimensionality or extreme class imbalance ratios?
- **Basis in paper:** [explicit] The authors limit the experimental validation to the "Telco Customer Churn" dataset and cap the sample size at 1,000 for "rapid prototyping" (Section IV.C).
- **Why unresolved:** The study relies on a single dataset with moderate imbalance; it is unknown if the Hilbert-space geometry advantages persist in high-dimensional spaces or with rarer minority classes.
- **What evidence would resolve it:** Benchmarking the classifiers on diverse, high-dimensional datasets (e.g., genomics or image data) without sample size restrictions.

### Open Question 2
- **Question:** How does the computational cost and classification accuracy trade-off evolve as the number of tensor copies ($n_{copies}$) increases beyond 2?
- **Basis in paper:** [explicit] The experiments are restricted to $n_{copies} \in \{1, 2\}$ (Section IV.D), despite the theory suggesting kPGM asymptotically approximates a nearest-neighbor classifier as $m \to \infty$ (Section II.B).
- **Why unresolved:** While $n=2$ yielded the best results, it is unclear if increasing $n$ further amplifies the recall gains or introduces diminishing returns due to overfitting.
- **What evidence would resolve it:** Empirical analysis of performance and runtime complexity for $n > 2$ across different encoding schemes.

### Open Question 3
- **Question:** Can the KNN-based QSMOTE variant be modified to avoid the severe recall degradation observed in the current study?
- **Basis in paper:** [explicit] The results show KNN-QSMOTE consistently underperforming (e.g., F1-score of 0.5743 vs 0.7785 for Fidelity-based), which the authors suggest is due to over-generalization in sparse minority regions (Section III.A, Section IV.F).
- **Why unresolved:** The paper introduces the failure mode but does not explore if parameter tuning (e.g., varying $k$) or adaptive neighborhoods could rectify the local interpolation issues.
- **What evidence would resolve it:** Ablation studies on KNN-QSMOTE varying $k$ relative to the local density of minority samples.

## Limitations
- Encoding formulas for amplitude and stereographic mappings are referenced but not explicitly defined
- λ scaling parameters for QSMOTE variants are vaguely described without concrete distributions
- Unclear whether PCA is applied before or after oversampling
- Limited to single dataset (Telco Churn) with capped sample size at 1,000

## Confidence
- **High Confidence:** The general quantum-inspired mechanism (PGM/kPGM classification using tensor products and Gram matrices) is well-specified and theoretically sound. The comparative performance advantage over Random Forest is clearly demonstrated.
- **Medium Confidence:** The specific QSMOTE variants and their implementation details have sufficient description for reasonable approximation, though exact reproduction would require assumptions.
- **Low Confidence:** Exact reproduction of numerical results is currently blocked by encoding formula omissions and unclear parameter specifications.

## Next Checks
1. **Encoding Specification:** Request or derive the explicit mathematical formulas for amplitude and stereographic encoding used to map classical feature vectors to quantum states.
2. **Parameter Clarification:** Obtain the exact λ scaling distributions for KNN-based and Fidelity-based QSMOTE variants, and confirm whether PCA is applied before or after oversampling.
3. **Reproducibility Test:** Implement the complete pipeline on a standard imbalanced dataset (e.g., credit card fraud) to verify that the reported performance gains (accuracy ~0.85, F1 ~0.82) are achievable and not dataset-specific.