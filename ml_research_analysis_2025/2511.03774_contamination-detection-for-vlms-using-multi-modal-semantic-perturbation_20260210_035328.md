---
ver: rpa2
title: Contamination Detection for VLMs using Multi-Modal Semantic Perturbation
arxiv_id: '2511.03774'
source_url: https://arxiv.org/abs/2511.03774
tags:
- contaminated
- contamination
- clean
- performance
- perturbed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting test-set contamination
  in vision-language models (VLMs), where models memorize benchmark data instead of
  learning genuine reasoning capabilities. The authors propose a novel method called
  multi-modal semantic perturbation that generates perturbed versions of image-question
  pairs by altering visual semantics while preserving overall composition, testing
  whether models fail to generalize to these controlled variations.
---

# Contamination Detection for VLMs using Multi-Modal Semantic Perturbation

## Quick Facts
- arXiv ID: 2511.03774
- Source URL: https://arxiv.org/abs/2511.03774
- Authors: Jaden Park; Mu Cai; Feng Yao; Jingbo Shang; Soochahn Lee; Yong Jae Lee
- Reference count: 40
- Primary result: Novel method detects test-set contamination in VLMs by generating semantically perturbed images that clean models generalize to but contaminated models fail on

## Executive Summary
This paper addresses the critical problem of detecting test-set contamination in vision-language models (VLMs), where models memorize benchmark data rather than learning genuine reasoning capabilities. The authors propose a novel method called multi-modal semantic perturbation that generates perturbed versions of image-question pairs by altering visual semantics while preserving overall composition. The approach successfully detects contamination across multiple settings, showing that contaminated models exhibit performance drops on perturbed data while clean models maintain or improve performance. The method satisfies three key requirements: practicality (no clean model access needed), reliability (works across different fine-tuning strategies), and consistency (detection signals correlate with contamination degree).

## Method Summary
The method generates perturbed image-question pairs by first sampling a new answer choice, then using an LLM (GPT-4o or Molmo-7B-D) to generate a dense caption describing the image with the new answer. This caption, combined with Canny edge maps of the original image, guides a diffusion model (Flux with ControlNet) to create a perturbed image that preserves global structure while altering task-relevant semantics. The perturbed pairs are filtered to ensure they remain unambiguously answerable. Models are evaluated on both original and perturbed benchmarks, with contamination detected when performance drops significantly on the perturbed set.

## Key Results
- Contaminated models show performance drops (-8.29 to -16.16 accuracy points) on perturbed data while clean models maintain or improve performance
- Method works across different fine-tuning strategies including LoRA and standard fine-tuning
- Detection signals correlate with contamination degree (training epochs on leaked data)
- Outperforms existing approaches like multi-modal leakage, circular evaluation, and choice confusion on benchmarks like RealWorldQA and MMStar

## Why This Works (Mechanism)

### Mechanism 1: Generalization Gap Detection
Contaminated models exhibit performance drops on semantically perturbed benchmarks while clean models maintain or improve performance. Models that memorized training data cannot adapt when visual semantics change (e.g., a speed limit sign value altered), because they rely on stored image-text associations rather than genuine visual reasoning. Clean models, having learned reasoning, successfully transfer to the perturbed variants which are designed to be of comparable or lower difficulty.

### Mechanism 2: Compositional Preservation via ControlNet
Preserving global image structure while altering task-relevant semantics isolates reasoning failures caused by memorization. Canny edge maps constrain Flux diffusion to retain spatial layout; the LLM-generated dense caption, conditioned on the question-original answer-new answer triplet, directs semantic edits to task-critical regions (e.g., changing a sign's text without relocating the sign).

### Mechanism 3: Contamination Degree Scaling
The magnitude of performance drop between original and perturbed benchmarks correlates with contamination degree (training epochs on leaked data). Higher exposure to specific image-text pairs strengthens memorization, increasing overfitting and reducing generalization capacity. This creates a graded signal rather than binary detection.

## Foundational Learning

- **Concept: Test-set contamination / data leakage**
  - Why needed: The entire method is predicated on distinguishing models that saw test data from those that did not; understanding memorization vs. generalization is essential
  - Quick check: Can you explain why n-gram overlap decontamination (used in LLM pretraining) is insufficient for VLMs with visual inputs?

- **Concept: Vision-Language Model architecture**
  - Why needed: The paper targets VLMs with dual-stage training (pretraining then instruction-tuning) and tests different fine-tuning strategies (LoRA, LLM-only, full)
  - Quick check: What components of a VLM would you freeze vs. fine-tune to simulate realistic contamination scenarios?

- **Concept: Diffusion models with conditional control**
  - Why needed: The perturbation pipeline uses Flux + ControlNet guided by edge maps and text prompts; understanding how conditioning affects output is necessary for debugging generation failures
  - Quick check: Why use Canny edge maps rather than depth or pose conditioning for preserving composition in VQA tasks?

## Architecture Onboarding

- **Component map**: Caption Generator -> Image Perturber -> Filter -> Evaluator
- **Critical path**: Caption quality → Diffusion semantic control → Filtering validity → Detection signal. The caption must correctly identify which visual elements to change; diffusion must render those changes legibly; filtering must remove ambiguous or unanswerable cases.
- **Design tradeoffs**:
  - Manual vs. automated filtering: Manual yields higher precision (86% agreement with o3 in overlap); automation scales better but may retain invalid pairs
  - GPT-4o vs. Molmo-7B-D captioning: GPT-4o produces more precise task-relevant captions; Molmo is open-source and cheaper but yields fewer valid pairs (398 vs. 440)
  - Diffusion steps: 25 steps used; fewer steps reduce cost but may degrade text/object rendering
- **Failure signatures**:
  - Perturbed image no longer resembles original → contaminated model may answer both correctly (see Figure 4, ~1.8-3.4% of cases)
  - Critical visual component omitted → question becomes unanswerable (mitigated by filtering)
  - Caption fails to emphasize task-relevant changes → perturbed difficulty exceeds original → false positive detection
- **First 3 experiments**:
  1. Reproduce contamination detection on RealWorldQA subset: Fine-tune LLaVA-v1.5-7B for 1-3 epochs; run perturbation pipeline on 50 images; verify Δ scales with epochs
  2. Ablate caption conditioning: Generate perturbed images using captions without the question-answer triplet; compare clean model performance drop vs. full conditioning to quantify caption design contribution
  3. Test automated filtering: Replace manual filtering with o3 prompt from Appendix B on 100 images; measure agreement rate and detection signal preservation

## Open Questions the Paper Calls Out
None

## Limitations
- Diffusion quality control lacks quantitative metrics for composition preservation or semantic fidelity
- Caption conditioning contribution remains unclear without comprehensive ablation studies
- Scaling relationship validity across fine-tuning strategies (LoRA vs. standard) needs verification

## Confidence
- **High**: Contamination detection mechanism (memorization vs. generalization distinction)
- **Medium**: Performance drop detection across multiple benchmarks and fine-tuning strategies
- **Low**: Semantic perturbation quality control, caption conditioning contribution, cross-strategy scaling

## Next Checks
1. Implement automated metrics (SSIM, CLIP similarity) to quantify composition preservation between original and perturbed images, comparing against manual filtering decisions.
2. Systematically compare perturbed image generation with full conditioning, answer-only conditioning, and random caption generation to measure generation success rate and clean model performance impact.
3. Fine-tune identical models using both LoRA and standard fine-tuning at equal epochs to test whether contamination-degree relationship holds across strategies or requires strategy-specific calibration.