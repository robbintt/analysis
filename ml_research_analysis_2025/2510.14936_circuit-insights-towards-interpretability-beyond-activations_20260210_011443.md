---
ver: rpa2
title: 'Circuit Insights: Towards Interpretability Beyond Activations'
arxiv_id: '2510.14936'
source_url: https://arxiv.org/abs/2510.14936
tags:
- feature
- tokens
- features
- descriptions
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of automated interpretability\
  \ in neural networks, particularly the limitations of existing methods that rely\
  \ on manual inspection, are restricted to toy tasks, or depend heavily on external\
  \ large language models (LLMs) and dataset quality. The authors propose two complementary\
  \ methods\u2014WeightLens and CircuitLens\u2014to overcome these limitations."
---

# Circuit Insights: Towards Interpretability Beyond Activations

## Quick Facts
- arXiv ID: 2510.14936
- Source URL: https://arxiv.org/abs/2510.14936
- Authors: Elena Golimblevskaia; Aakriti Jain; Bruno Puri; Ammar Ibrahim; Wojciech Samek; Sebastian Lapuschkin
- Reference count: 29
- One-line primary result: WeightLens and CircuitLens methods improve automated interpretability for neural networks, matching or exceeding existing methods while reducing dependence on large datasets and external LLMs.

## Executive Summary
This paper addresses the challenge of automated interpretability in neural networks, particularly the limitations of existing methods that rely on manual inspection, are restricted to toy tasks, or depend heavily on external large language models (LLMs) and dataset quality. The authors propose two complementary methods—WeightLens and CircuitLens—to overcome these limitations. WeightLens interprets model features directly from learned weights, eliminating the need for explainer models or datasets, and matches or exceeds the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods enhance interpretability robustness and enable scalable mechanistic analysis of circuits while maintaining efficiency. Evaluations on models like Gemma-2-2b and GPT-2 show that WeightLens performs on par with or better than activation-based methods in clarity and responsiveness, while CircuitLens resolves polysemanticity through circuit-based clustering, improving interpretability for context-dependent features. The approach reduces dependence on large datasets and external LLMs, making automated interpretability more practical for real-world applications.

## Method Summary
The paper introduces WeightLens and CircuitLens as complementary methods for interpreting transcoder features in neural networks. WeightLens operates by projecting transcoder encoder and decoder vectors into token embedding and unembedding spaces, identifying outlier tokens via z-scores that represent the semantic function of context-independent features. This method requires no external datasets or explainer models, making it highly efficient. CircuitLens uses Jacobian-adjusted attribution to trace how feature activations arise from interactions between components, including attention heads and residual streams. It captures circuit-level dynamics by analyzing the contribution of previous tokens to current feature activations. For polysemantic features, CircuitLens employs circuit-based clustering using Jaccard similarity between contribution sets, grouping inputs that share the same causal mechanism rather than relying on semantic clustering. The methods are evaluated using the FADE framework (Clarity, Responsiveness, Purity, Faithfulness) on models including Gemma-2-2b, GPT-2, and Llama-3.2-1B, with transcoders from Gemma-Scope.

## Key Results
- WeightLens matches or exceeds activation-based methods on context-independent features while requiring no explainer models or datasets
- CircuitLens reveals circuit-level dynamics and resolves polysemanticity through circuit-based clustering that outperforms semantic clustering approaches
- WeightLens shows validated token rates of ~80% in early and late layers but drops sharply in middle layers due to context-dependency
- CircuitLens achieves high Purity and Faithfulness scores, though individual feature interventions show limited effect due to distributed concepts across layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a feature is context-independent, its semantic function can be derived directly from learned transcoder weights, potentially bypassing the need for external datasets or explainer models.
- **Mechanism:** WeightLens projects the transcoder's encoder vector ($f_{enc}$) into the token embedding space ($W_{emb}$) and the decoder vector ($f_{dec}$) into the unembedding space ($W_U$). Statistical outliers (z-scores) in these projections are interpreted as the tokens that trigger or are promoted by the feature.
- **Core assumption:** Input-invariant connections indicate meaningful structural relationships only if they significantly exceed the magnitude of other connections (Assumption 1), and strong weight support implies consistent activation regardless of context (Assumption 2).
- **Evidence anchors:**
  - [abstract] "WeightLens interprets features directly from their learned weights... matching or exceeding the performance of existing methods on context-independent features."
  - [section] Section 3.1 details the projection of $f_{enc}$ into vocabulary space via $W_{emb} \cdot f_{enc}$.
  - [corpus] Contextual support from "Transcoders find interpretable LLM feature circuits" (Dunefsky et al., 2024) cited in Section 2, which establishes the transcoder architecture foundation.
- **Break condition:** Fails for context-dependent features (e.g., middle layers in Gemma-2-2B) where weights act as a "sum of contexts," or when high sparsity ($\ell_0$) introduces interference, resulting in low validated token rates.

### Mechanism 2
- **Claim:** Attribution via Jacobians and attention head projections allows for the isolation of causal input patterns, revealing "why" a feature activated beyond simple token co-occurrence.
- **Mechanism:** CircuitLens utilizes a Jacobian term ($J^{(l \to l')}$) to account for non-linearities and attention. It traces the activation of feature $(l, i)$ back to previous tokens $s$ via attention heads ($W_{OV}$) and residual streams, isolating specific token-position pairs that contributed to the activation.
- **Core assumption:** Treating non-linearities (attention, normalization) as constants with respect to the input provides a sufficiently accurate local linear approximation for attribution.
- **Evidence anchors:**
  - [abstract] "CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify."
  - [section] Equation (4) defines the contribution of previous tokens via attention head $h$ and the feature encoder vector.
  - [corpus] Corpus neighbors (e.g., "Mechanistic Interpretability of GPT-like Models") validate the general shift toward circuit-level analysis, though specific validation of this Jacobian method is internal to the paper.
- **Break condition:** High computational cost for output-centric analysis limits scalability; "clarity" degrades if interference between layers is not properly adjusted by the Jacobian.

### Mechanism 3
- **Claim:** If features are polysemantic, circuit-based clustering (grouping by shared contributing features/heads) creates more interpretable subgroups than semantic clustering.
- **Mechanism:** The method collects "contribution sets" (significant transcoder features and attention heads) for each input. It computes Jaccard similarity between these sets and applies DBSCAN to cluster inputs that share the same causal mechanism.
- **Core assumption:** Sparse activations ensure that inputs have few contributors, making the Jaccard similarity matrix robust enough to distinguish distinct causal sub-circuits within a single feature.
- **Evidence anchors:**
  - [abstract] "...handles polysemanticity through circuit-based clustering and combining interpretations into unified feature descriptions."
  - [section] Section 3.2 ("Circuit-Based Clustering") describes the frequency filtering and DBSCAN application on the similarity matrix.
  - [corpus] Weak direct corpus evidence for this specific clustering technique; related work (Kopf et al., 2025) uses semantic embedding clustering, which this paper contrasts against.
- **Break condition:** If the frequency filter threshold $\rho$ is too low, noise dominates the clusters; if too high, valid but rare circuit behaviors are discarded.

## Foundational Learning

- **Concept: Transcoder Architectures**
  - **Why needed here:** The paper explicitly builds on transcoders (not standard SAEs) because they separate input-dependent activations from input-invariant weights ($f_{dec} \cdot f_{enc}$), which is the prerequisite for WeightLens.
  - **Quick check question:** Can you explain why a transcoder allows for "input-invariant" analysis compared to a standard Sparse Autoencoder?

- **Concept: Layer-wise Relevance Propagation (LRP) & Jacobians**
  - **Why needed here:** CircuitLens relies on attributing relevance through non-linear layers using Jacobians. Understanding how to propagate gradients/signals through attention heads is vital for debugging the attribution graphs.
  - **Quick check question:** How does treating attention patterns as "constants" (Equation 3) simplify the calculation of feature attribution?

- **Concept: Polysemanticity vs. Monosemanticity**
  - **Why needed here:** The paper aims to resolve polysemanticity (one neuron responding to multiple concepts) via clustering. Distinguishing whether a feature is "context-dependent" or merely "polysemantic" determines whether to use WeightLens or CircuitLens.
  - **Quick check question:** In this framework, would a "context-dependent" feature likely be monosemantic or polysemantic, and which tool best analyzes it?

## Architecture Onboarding

- **Component map:** Transcoders -> WeightLens Module (zero-shot projection logic) -> CircuitLens Module (attribution engine + DBSCAN clustering) -> LLM Interface (optional summarization)

- **Critical path:**
  1. **Setup:** Load model (Gemma-2-2b) with pre-trained transcoders
  2. **WeightLens Pass:** Project $f_{enc}$ to vocab; validate top tokens via forward pass (Sanity Check)
  3. **CircuitLens Pass:** Run data (24M tokens) → Calculate attributions → Filter contributors → Jaccard Similarity → DBSCAN
  4. **Synthesis:** Merge cluster descriptions into unified feature explanation

- **Design tradeoffs:**
  - **WeightLens:** High efficiency/low cost vs. inability to handle context-dependent features (fails in middle layers)
  - **Dataset Sampling:** Inverse-frequency quantile sampling (captures rare behavior) vs. Top-k sampling (standard baseline, may miss long-tail concepts)
  - **Clustering:** DBSCAN (handles noise/no predefined cluster count) vs. K-Means (requires defining $k$, less robust to circuit noise)

- **Failure signatures:**
  - **"Variety of words on variety of topics":** Generic description indicating CircuitLens failed to isolate a specific pattern or WeightLens found no validated tokens
  - **Low Clarity Score (< 0.2):** Common in middle layers (e.g., Layer 12 in Gemma) due to high context-dependency; suggests need for more CircuitLens clusters or that the feature is irreducibly abstract
  - **High Activation Count ($\ell_0$):** Early layers (Layer 0, 7) with high $\ell_0$ show poor interpretability due to interference

- **First 3 experiments:**
  1. **Layer Sensitivity Test:** Run WeightLens on Layers 0, 12, and 21. Compare the percentage of "validated features" (Figure 3) to confirm the hypothesis that early/late layers are token-based while middle layers are context-based
  2. **Cluster Ablation:** For a known polysemantic feature, compare description quality (Clarity/Responsiveness) using standard semantic clustering vs. the proposed Circuit-Based Jaccard clustering
  3. **Dataset Size Stress Test:** Run CircuitLens with 24M tokens vs. a smaller subset. Verify the claim that weight-based integration reduces sensitivity to dataset size

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Faithfulness metric be reformulated to accurately evaluate interventions on entire feature circuits rather than individual transcoder features?
- **Basis in paper:** [explicit] The authors note that individual feature interventions rarely produce large effects because "similar concepts are distributed across layers," suggesting circuit-level evaluation is needed.
- **Why unresolved:** Current metrics focus on single-feature steering, failing to capture the distributed nature of concepts in transcoders, which leads to consistently low Faithfulness scores across all methods.
- **What evidence would resolve it:** A new metric that aggregates steering effects across correlated features within a discovered circuit, validated against standard behavioral benchmarks to show higher sensitivity than single-feature interventions.

### Open Question 2
- **Question:** How does Rotary Positional Embedding (RoPE) specifically interfere with input-invariant weight analysis in middle layers compared to absolute positional embeddings?
- **Basis in paper:** [inferred] The paper observes a sharp drop in validated token-based features in middle layers of Llama and Gemma (using RoPE) but not GPT-2 (absolute embeddings), hypothesizing RoPE introduces "additional non-linearities."
- **Why unresolved:** The observation is currently a correlation; the specific mathematical interaction between RoPE rotations and transcoder weight projections remains unquantified.
- **What evidence would resolve it:** A comparative study replacing RoPE with absolute embeddings in the same architecture, or a theoretical analysis of how rotation matrices affect the stability of decoder-encoder projection outliers.

### Open Question 3
- **Question:** Can the computational cost of output-centric attribution be reduced to allow for analysis of longer-range downstream effects without sacrificing accuracy?
- **Basis in paper:** [explicit] The authors state that "Output-based analysis is computationally expensive" as each token requires a forward and backward pass, limiting analysis to short generation windows (e.g., 15 tokens).
- **Why unresolved:** The high cost restricts the method to analyzing immediate token effects, potentially missing long-horizon functional roles of features in deeper layers.
- **What evidence would resolve it:** Development of an approximation method (e.g., caching gradients or linearizing paths) that scales efficiently with sequence length while maintaining a high correlation with exact attribution scores.

## Limitations

- The methodology relies on transcoder architectures, which are not yet widely adopted in mainstream LLM interpretability research
- The clustering approach for polysemantic features depends on unspecified hyperparameters (frequency filter threshold ρ, DBSCAN parameters) that may limit reproducibility
- Claims of matching or exceeding existing methods are based on comparisons within the FADE framework, which may have its own limitations in capturing true interpretability quality

## Confidence

- **High confidence**: WeightLens effectiveness for context-independent features in early/late layers, supported by validated token percentages and clarity scores
- **Medium confidence**: CircuitLens attribution methodology using Jacobians, though computational cost and approximation assumptions introduce uncertainty
- **Medium confidence**: Clustering approach for polysemantic features, limited by unspecified hyperparameters and lack of external validation
- **Low confidence**: Generalization claims to non-transcoder architectures, as the entire methodology is built on transcoder-specific properties

## Next Checks

1. **Architecture generalization test**: Apply WeightLens and CircuitLens to SAEs or standard feature extractors in the same models to verify whether the methods work beyond transcoders, or identify which assumptions break

2. **Hyperparameter sensitivity analysis**: Systematically vary the frequency filter threshold ρ, DBSCAN eps/min_samples, and z-score thresholds to determine their impact on clarity, responsiveness, and clustering quality across different layers

3. **Dataset size robustness test**: Compare interpretability metrics (Clarity, Responsiveness, Purity, Faithfulness) when running CircuitLens on 24M tokens versus progressively smaller subsets (6M, 1M, 100K tokens) to empirically validate the claim that weight-based integration reduces dataset sensitivity