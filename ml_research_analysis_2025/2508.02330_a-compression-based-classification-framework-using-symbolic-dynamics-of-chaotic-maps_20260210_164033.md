---
ver: rpa2
title: A Compression Based Classification Framework Using Symbolic Dynamics of Chaotic
  Maps
arxiv_id: '2508.02330'
source_url: https://arxiv.org/abs/2508.02330
tags:
- symbolic
- data
- length
- sequence
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChaosComp, a novel classification framework
  that leverages symbolic dynamics and data compression using chaotic maps. The method
  models each class by generating symbolic sequences from thresholded training data,
  which are then evolved through a one-dimensional chaotic map (Baker's map).
---

# A Compression Based Classification Framework Using Symbolic Dynamics of Chaotic Maps

## Quick Facts
- arXiv ID: 2508.02330
- Source URL: https://arxiv.org/abs/2508.02330
- Reference count: 6
- Primary result: ChaosComp achieves macro F1-scores of 0.9531 on Breast Cancer Wisconsin, 0.9475 on Seeds, and 0.8469 on Iris datasets using compression-based classification with chaotic maps

## Executive Summary
This paper introduces ChaosComp, a novel classification framework that leverages symbolic dynamics and data compression using chaotic maps. The method models each class by generating symbolic sequences from thresholded training data, which are then evolved through a one-dimensional chaotic map (Baker's map). For each class, transition probabilities of symbolic patterns are computed to form a class-specific probabilistic model. During testing, the test data are symbolized and encoded using class-wise symbolic statistics via back iteration, a dynamical reconstruction technique. The predicted label corresponds to the class yielding the shortest compressed representation. The approach was evaluated on both synthetic and real-world datasets, demonstrating competitive performance compared to traditional machine learning algorithms.

## Method Summary
ChaosComp converts real-valued features to binary symbolic sequences via thresholding, then models each class as an empirical probability distribution over n-length symbolic patterns. These distributions define class-specific Baker's maps, where backward iteration reconstructs the initial condition interval whose length encodes the probability of the symbolic sequence under that class model. Classification is performed by predicting the class yielding the shortest compressed representation (⌈−log₂(U−L)⌉ bits). The method uses 5-fold cross-validation to optimize the threshold hyperparameter and applies Laplace smoothing (α=0.01) to prevent zero probabilities.

## Key Results
- Macro F1-scores: 0.9531 (Breast Cancer Wisconsin), 0.9475 (Seeds), 0.8469 (Iris)
- Perfect classification on synthetic XOR dataset without augmentation
- Demonstrated competitive performance versus traditional ML algorithms
- Successfully captures non-linear decision boundaries through symbolic dynamics

## Why This Works (Mechanism)

### Mechanism 1: Shannon-Optimal Compression via Baker's Map Coding
- Claim: Baker's map backward iteration achieves Shannon entropy rate for binary symbolic sequences, enabling principled compression-based classification.
- Mechanism: The piecewise-linear Baker's map partitions [0,1) into subintervals based on symbol probabilities. Backward iteration shrinks this interval proportionally to P(S), where S is the observed symbolic sequence. Compressed file size = ⌈−log₂(U−L)⌉ bits.
- Core assumption: Symbolic sequences are approximately i.i.d. within each class (though n-th return maps capture block dependencies up to length n).
- Evidence anchors:
  - [abstract] "predicted label corresponds to the class yielding the shortest compressed representation, signifying the most efficient symbolic encoding"
  - [section II.E, page 4] Proof shows bits/symbol → H as N→∞, achieving Shannon entropy rate
  - [corpus] Related work "Chaotic Map based Compression Approach to Classification" (FMR=0.53) supports chaotic compression frameworks
- Break condition: When symbolic sequences have long-range correlations exceeding block length n, the independence assumption fails and compression becomes suboptimal.

### Mechanism 2: Class-Separable Symbolic Distributions
- Claim: Different classes produce distinguishable empirical probability distributions over n-bit symbolic patterns, enabling discrimination via compression efficiency.
- Mechanism: Training instances are binarized via thresholding, then grouped into n-length subsequences. Class-specific empirical probabilities {p_w} define unique chaotic maps. Test samples compress better under the class whose map better matches their symbolic statistics.
- Core assumption: Thresholding preserves class-discriminative information in symbolic form.
- Evidence anchors:
  - [abstract] "compute the transition probabilities of symbolic patterns... aggregate these statistics to form a class-specific probabilistic model"
  - [section III, page 5] Explicit example: Class 0 has p₁₀=3/6, Class 1 has p₁₀=2/6, demonstrating separable distributions
  - [corpus] Weak direct evidence; no corpus papers validate class-specific symbolic distributions for this architecture
- Break condition: If two classes have nearly identical symbolic distributions after thresholding, compression lengths converge and classification degrades to tie-breaking via cosine similarity.

### Mechanism 3: Backward Interval Iteration as Probabilistic Decoder
- Claim: Backward iteration reconstructs the initial condition interval whose length encodes the probability of the symbolic sequence under the class model.
- Mechanism: Given sequence S and class probabilities {p_w}, inverse map transformations iteratively shrink [0,1) → I₀=[L,U]. Interval length = P(S|class). Shorter intervals imply lower probability under that class model.
- Core assumption: Finite floating-point precision is sufficient for accurate interval computation.
- Evidence anchors:
  - [abstract] "encoded using the class-wise symbolic statistics via back iteration, a dynamical reconstruction technique"
  - [section II.B, page 2-3] Equations 4-6 define B⁻¹ branches and interval propagation
  - [corpus] "Hyperparameter-Free Neurochaos Learning" (FMR=0.50) uses chaotic dynamics for feature extraction, supporting the general paradigm
- Break condition: For k≥30 features, numerical instability arises from exponential interval shrinking (explicitly noted section III.A, page 5 footnote).

## Foundational Learning

- Concept: **Symbolic Dynamics**
  - Why needed here: Core abstraction converting real-valued data to discrete sequences that chaotic maps operate on.
  - Quick check question: Given threshold a=0.4 and x₀=0.3, what is the first symbol s₀?

- Concept: **Minimum Description Length (MDL) Principle**
  - Why needed here: Theoretical justification—best model minimizes L(h)+L(D|h). ChaosComp interprets classification as model selection via compression.
  - Quick check question: Why does shorter compression imply better generalization rather than overfitting?

- Concept: **Arithmetic Coding**
  - Why needed here: Backward iteration is arithmetic coding in reverse; interval length directly maps to bits required.
  - Quick check question: If an interval has length 0.001, approximately how many bits are needed to encode it?

## Architecture Onboarding

- Component map: MinMax scaling -> threshold binarization -> padding to multiple of n -> n-gram extraction -> probability computation -> Laplace smoothing -> Baker's map construction -> backward iteration -> interval length calculation -> compressed file size computation

- Critical path:
  1. Threshold optimization (0.01–1.00, step 0.01) via cross-validation
  2. Block length n selection (controls model capacity: 2ⁿ probabilities per class)
  3. Backward iteration precision—interval arithmetic is the numerical bottleneck

- Design tradeoffs:
  - Higher n: Captures longer-range dependencies but requires exponentially more parameters; prone to overfitting with limited data
  - Threshold choice: Controls sparsity of symbolic representation; optimal value is dataset-dependent
  - Padding: Required for feature coverage but may introduce spurious patterns

- Failure signatures:
  - **Numerical underflow**: k≥30 features cause interval collapse → needs renormalization (not implemented)
  - **Zero probabilities**: Break backward iteration → mitigated by Laplace smoothing
  - **Ties**: Multiple classes yield identical file sizes → falls back to cosine similarity between probability vectors

- First 3 experiments:
  1. Reproduce XOR with n=2 (page 6 confirms perfect accuracy without augmentation) to validate non-linear decision boundaries
  2. Run 5-fold CV on Breast Cancer Wisconsin varying n∈{2,3,4,5} to observe bias-variance tradeoff against paper's F1=0.9531
  3. Test numerical stability: synthetically increase feature dimension toward k=30 to identify precision breakdown point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the entropy of a class-specific n-th return map directly correlate with class separability and classification performance?
- Basis in paper: [explicit] The Future Work section explicitly states, "Studying how $H_n$ correlates with class separability may offer foundational insights into compression-based learning."
- Why unresolved: While the paper establishes that the Lyapunov exponent equals Shannon entropy for the standard Baker's map, the specific causal link between the entropy of the *empirical* n-th return maps constructed for specific classes and the resulting F1-scores has not been empirically or theoretically validated.
- What evidence would resolve it: A theoretical analysis or empirical study plotting the class-wise entropy $H_n$ against classification metrics (like F1-score or margin) across multiple datasets to identify a monotonic relationship.

### Open Question 2
- Question: Can renormalization techniques effectively resolve the numerical instability of the backward iteration method for high-dimensional datasets ($k \ge 30$)?
- Basis in paper: [explicit] The Limitations section notes that "back iteration becomes numerically unstable for higher-dimensional data... This can potentially be addressed using renormalization techniques from arithmetic coding."
- Why unresolved: The authors identify the precision issue caused by shrinking interval sizes in high dimensions but do not implement or test the proposed renormalization solution in the current work.
- What evidence would resolve it: A modified implementation of ChaosComp incorporating renormalization steps, demonstrated to maintain precision and classification accuracy on datasets with significantly more than 30 features.

### Open Question 3
- Question: Is a single global threshold optimal for binarizing all features, or would adaptive/feature-specific thresholds better preserve class-specific symbolic dynamics?
- Basis in paper: [inferred] Section III-B states that a single threshold value is used to binarize the data, serving as a hyperparameter. This assumes that a uniform cut-off across all normalized features is sufficient to capture the distinct dynamics of different classes.
- Why unresolved: A global threshold might discard discriminative information for features with varying distributions. The paper does not analyze the information loss incurred by forcing a single threshold versus allowing per-feature or per-class thresholds.
- What evidence would resolve it: An ablation study comparing the performance of the current global threshold approach against a method utilizing feature-specific thresholds, measuring the difference in macro F1-scores and compressed file sizes.

## Limitations
- Numerical stability issues for high-dimensional data (k≥30 features) due to exponential interval shrinking
- Reliance on a single threshold parameter for binarization introduces sensitivity to this hyperparameter
- Substantial memory requirements for storing probability tables (m×2ⁿ parameters)

## Confidence
- **High confidence**: The core compression mechanism using Baker's map backward iteration achieves Shannon entropy rate asymptotically, supported by theoretical proof and explicit equations in section II.E.
- **Medium confidence**: The claim that separable symbolic distributions enable classification is supported by empirical results on synthetic XOR and real datasets (macro F1=0.9531 on Breast Cancer Wisconsin), but lacks strong corpus validation.
- **Low confidence**: The assumption that thresholding preserves class-discriminative information is reasonable but untested across diverse domains.

## Next Checks
1. **Numerical Stability Test**: Systematically evaluate interval width decay across increasing feature dimensions (k=5, 10, 15, 20, 25, 30) using synthetic Gaussian data with varying inter-class separation. Monitor for underflow and identify the critical k threshold where precision breaks down.

2. **Threshold Sensitivity Analysis**: For the Breast Cancer Wisconsin dataset, perform a fine-grained grid search (0.01-1.00, step 0.01) and plot macro F1 against threshold values. Identify whether the claimed optimal threshold (0.32) is robust or if multiple local optima exist.

3. **Class Distribution Separability**: Compute pairwise KL divergences between class-specific probability distributions for each dataset. Correlate these separability metrics with classification performance to quantify the relationship between symbolic distribution distinguishability and accuracy.