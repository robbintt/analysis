---
ver: rpa2
title: 'MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I
  Diffusion Models'
arxiv_id: '2510.01549'
source_url: https://arxiv.org/abs/2510.01549
tags:
- reward
- mira
- noise
- image
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward hacking in inference-time
  alignment of text-to-image diffusion models, where noise optimization methods produce
  high-reward but prompt-adherence-lacking images. The core method, MIRA (MItigating
  Reward hAcking), introduces an image-space regularization term based on a score-based
  KL surrogate that constrains the output distribution to remain close to the base
  model while improving reward.
---

# MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models

## Quick Facts
- arXiv ID: 2510.01549
- Source URL: https://arxiv.org/abs/2510.01549
- Reference count: 40
- Core method introduces image-space regularization to prevent reward hacking in noise optimization for text-to-image diffusion models

## Executive Summary
MIRA addresses reward hacking in inference-time alignment of text-to-image diffusion models, where noise optimization methods produce high-reward but prompt-adherence-lacking images. The paper introduces an image-space regularization term based on a score-based KL surrogate that constrains the output distribution to remain close to the base model while improving reward. This approach directly controls the image distribution rather than just noise-space updates, which is shown to be insufficient. MIRA-DPO extends the framework to non-differentiable rewards using preference optimization. Across SDv1.5 and SDXL models, multiple rewards (Aesthetic Score, HPSv2, PickScore), and public datasets, MIRA achieves >60% win rate against strong baselines while preserving prompt adherence, with mechanism plots showing reward gains with near-zero distributional drift compared to DNO's significant drift.

## Method Summary
MIRA introduces a score-based KL surrogate regularizer that constrains the output image distribution during inference-time optimization. The method adds a regularizer term to the optimization objective that penalizes distributional drift between the optimized and base model outputs. For non-differentiable rewards, MIRA-DPO maps preference optimization to the inference-time setting using a frozen backbone. The approach addresses reward hacking by ensuring the optimized images remain close to the base model's distribution while improving reward scores.

## Key Results
- MIRA achieves >60% win rate against strong baselines across SDv1.5 and SDXL models
- Outperforms DNO on multiple rewards (Aesthetic Score, HPSv2, PickScore) while preserving prompt adherence
- MIRA shows near-zero distributional drift compared to DNO's significant drift, as measured by CMMD
- MIRA-DPO successfully extends the framework to non-differentiable rewards without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward hacking in noise optimization stems from distributional drift in the output image space, which noise-space regularization cannot prevent.
- Mechanism: The diffusion denoising process is highly non-linear; small perturbations in initial noise can yield dramatically different output images. Reward models are trained to evaluate images from the base model's original distribution; when noise optimization shifts the output distribution out-of-distribution, reward model reliability degrades, enabling exploitation.
- Core assumption: The score-based KL surrogate adequately approximates true KL divergence between image distributions.
- Evidence anchors:
  - [abstract]: "We show that noise-space regularization is insufficient and that preventing reward hacking requires an explicit image-space constraint."
  - [section 3, Proposition 1]: "Closeness in the noise space does not imply closeness in the diffusion-induced image distribution."
  - [corpus]: Weak direct evidence; related work on adversarial training of reward models [arxiv:2504.06141] discusses reward hacking generally but not noise-space vs image-space constraints specifically.
- Break condition: If the score-based surrogate poorly approximates KL on complex prompts/out-of-distribution samples, regularization may under-constrain or over-constrain optimization.

### Mechanism 2
- Claim: An image-space, score-based KL surrogate regularizer can constrain output image distribution to remain close to the base model while enabling reward improvement.
- Mechanism: MIRA adds a regularizer term d_KL[p_θ(x_0|z,c) || p_θ(x_0|z_0,c)] to the optimization objective, approximated via a tractable upper bound using diffusion score functions. This penalizes large deviations in output distribution, preserving prompt adherence.
- Core assumption: The score function upper bound is tight enough to provide meaningful regularization without over-constraining.
- Evidence anchors:
  - [abstract]: "MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution so reward can increase without off-distribution drift."
  - [section 4, eq. 4]: The practical objective uses a score-based surrogate for KL.
  - [corpus]: Related work on inference-time alignment [arxiv:2501.06848] explores steering/guidance but not specifically score-based KL regularization.
- Break condition: If β (regularization hyperparameter) is poorly tuned, reward optimization may be overly constrained or under-constrained.

### Mechanism 3
- Claim: For non-differentiable rewards, mapping preference optimization to inference-time noise optimization with a frozen backbone (MIRA-DPO) can align images without fine-tuning.
- Mechanism: MIRA-DPO adapts DPO to the inference-time setting by using the score-based KL surrogate to approximate the implicit reward used in DPO, enabling optimization from preference pairs without a differentiable reward function.
- Core assumption: The score-based surrogate adequately approximates the log-likelihood ratio needed for DPO's implicit reward formulation.
- Evidence anchors:
  - [abstract]: "We further introduce MIRA-DPO, mapping preference optimization to inference time with a frozen backbone, extending MIRA to non-differentiable rewards without fine-tuning."
  - [section 4.1, eq. 5-7]: The DPO objective is mapped to use the score-based surrogate.
  - [corpus]: Diffusion Blend [arxiv:2505.18547] and GDRO [arxiv:2601.02036] address multi-preference and group-level reward alignment via fine-tuning, not inference-time DPO.
- Break condition: If the preference dataset is noisy or unrepresentative, MIRA-DPO may fail to generalize.

## Foundational Learning
- Concept: KL divergence between image distributions
  - Why needed here: MIRA's core regularizer is a KL constraint on the output image distribution; understanding KL is essential to grasp how MIRA controls drift.
  - Quick check question: Can you explain why KL divergence is asymmetric and why this matters for regularizing a generative distribution?
- Concept: Score functions in diffusion models
  - Why needed here: MIRA's regularizer uses the model's score functions to approximate KL; familiarity with score functions is necessary to understand the surrogate.
  - Quick check question: What does a diffusion model's score function represent, and how is it used during sampling?
- Concept: Reward hacking in RL/alignment
  - Why needed here: The paper's central problem is reward hacking in noise optimization; understanding this failure mode clarifies MIRA's motivation.
  - Quick check question: Give an example of reward hacking in a text-to-image model optimizing for a specific reward metric.

## Architecture Onboarding
- Component map:
  - Frozen diffusion backbone (e.g., SDv1.5, SDXL) -> Generates images via denoising from initial noise z
  - Reward function r(x_0, c) -> Evaluates generated images against alignment objective (e.g., Aesthetic Score, HPSv2)
  - MIRA optimizer -> Iteratively updates initial noise z to minimize combined objective of reward and KL surrogate
  - Score-based KL surrogate -> Approximates image distribution drift using score functions from frozen backbone
  - MIRA-DPO extension -> Handles non-differentiable rewards by using preference pairs and DPO formulation

- Critical path:
  1. Sample initial noise z_0 ~ N(0, I)
  2. Compute the score-based surrogate S_0 from a forward pass through the frozen model (reference distribution)
  3. For K optimization steps:
     - Generate image x_0 from current noise z
     - Compute reward r(x_0, c)
     - Compute score-based surrogate S for current z
     - Compute MIRA loss: -r + β(S_0 - S)
     - Backpropagate to z and update
  4. Return optimized noise z

- Design tradeoffs:
  - β selection: Low β risks reward hacking; high β over-constrains reward optimization. Tune via CLIPScore vs reward curves
  - Optimization steps K: More steps can increase reward but also compute; monitor for reward hacking signs (e.g., oversaturation, prompt drift)
  - Score surrogate accuracy: The surrogate may degrade on complex or out-of-distribution prompts; validate with CMMD drift metrics

- Failure signatures:
  - Oversaturation, high contrast, or unnatural textures in generated images (reward hacking)
  - Significant drop in CLIPScore despite rising reward values
  - Large CMMD drift from the base distribution
  - For MIRA-DPO: Failure to converge or unstable training with noisy preference data

- First 3 experiments:
  1. Reproduce the Simple Animals experiment: Optimize SDv1.5 for Aesthetic Score with MIRA vs DNO across 45 prompts. Compare reward, CLIPScore, and CMMD drift; visually inspect for reward hacking.
  2. Hyperparameter sweep for β: Test β ∈ {0.1, 0.2, 0.5, 1.0} on a subset of prompts, plotting reward vs CLIPScore to find the trade-off sweet spot.
  3. Non-differentiable reward test: Apply MIRA-DPO to maximize JPEG compressibility on a small prompt set, comparing image quality and semantic preservation against unregularized optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical tightness of the score-based KL surrogate approximation relative to the true KL divergence between optimized and base image distributions?
- Basis in paper: [explicit] The authors state: "a formal analysis of the surrogate's theoretical tightness is an important direction for future work."
- Why unresolved: The paper derives an upper-bound surrogate but provides no guarantees on approximation quality or conditions under which the bound is tight.
- What evidence would resolve it: Theoretical analysis establishing approximation error bounds, or empirical comparison against ground-truth KL computed via density estimation on synthetic distributions.

### Open Question 2
- Question: How can MIRA's computational efficiency be improved while maintaining its reward hacking mitigation properties?
- Basis in paper: [explicit] The authors note: "Improving MIRA's efficiency and combining it with sampling methods are left to future work."
- Why unresolved: MIRA requires multiple optimization iterations (50 in experiments), each involving full denoising trajectories, which adds significant inference-time compute compared to standard sampling.
- What evidence would resolve it: Modified algorithms achieving comparable win rates with fewer optimization steps, or theoretical results establishing minimum compute requirements.

### Open Question 3
- Question: Does the score-based regularizer degrade on complex or out-of-distribution prompts, and if so, what characterizes failure modes?
- Basis in paper: [explicit] The limitations state the surrogate "may degrade on complex or out-of-distribution prompts."
- Why unresolved: Experiments focus on relatively simple prompts (Simple Animals dataset) and structured HPDv2 prompts; behavior on compositional, abstract, or rare concept prompts is unexplored.
- What evidence would resolve it: Systematic evaluation on benchmarks with complex compositional prompts (e.g., T2I-CompBench), analysis of regularizer stability across prompt complexity levels.

### Open Question 4
- Question: How does MIRA interact with different sampler schedules beyond DDIM, and does regularization effectiveness vary across samplers?
- Basis in paper: [explicit] The authors note: "we focus our evaluation on DDIM for compute parity, as hyperparameters may vary by schedule."
- Why unresolved: Only DDIM with η=1 is evaluated; the score-based regularizer's behavior under DDPM, Euler, or other schedulers is unknown.
- What evidence would resolve it: Cross-sampler experiments showing KL surrogate values and win rates under matched compute budgets across DDPM, DDIM variants, and Euler samplers.

## Limitations
- The score-based KL surrogate's approximation quality is assumed but not rigorously validated across diverse prompts and out-of-distribution samples
- The paper focuses on aesthetic and specific rewards without exploring other alignment objectives like safety or factual accuracy
- The choice of CMMD as a drift metric may not fully capture perceptual changes in image quality

## Confidence
- **High Confidence**: MIRA outperforms DNO on win rate and prompt adherence for the tested reward functions and models
- **Medium Confidence**: The score-based KL surrogate is an effective and practical approximation for MIRA's image-space regularization
- **Medium Confidence**: MIRA-DPO successfully extends the framework to non-differentiable rewards

## Next Checks
1. Rigorously analyze the tightness of the score-based KL upper bound across a wide range of prompts and compare it to exact KL estimates where feasible
2. Test MIRA on diffusion models from other families and reward functions not explicitly covered to assess generality
3. Systematically evaluate MIRA-DPO's performance when trained on preference pairs with varying levels of noise or bias