---
ver: rpa2
title: 'Value Drifts: Tracing Value Alignment During LLM Post-Training'
arxiv_id: '2510.26707'
source_url: https://arxiv.org/abs/2510.26707
tags:
- training
- support
- preference
- oppose
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language model values change during
  post-training. The authors find that supervised fine-tuning is the dominant driver
  of value alignment, establishing model stances early and strongly, while subsequent
  preference optimization rarely re-aligns values unless the preference data contains
  a sufficient "value gap" between chosen and rejected responses.
---

# Value Drifts: Tracing Value Alignment During LLM Post-Training

## Quick Facts
- arXiv ID: 2510.26707
- Source URL: https://arxiv.org/abs/2510.26707
- Authors: Mehar Bhatia; Shravan Nayak; Gaurav Kamath; Marius Mosbach; Karolina StaÅ„czak; Vered Shwartz; Siva Reddy
- Reference count: 40
- Primary result: Supervised fine-tuning dominates value alignment in LLMs, with preference optimization only reshaping values when datasets contain sufficient "value gap" between responses

## Executive Summary
This paper investigates how language model values evolve during post-training, finding that supervised fine-tuning (SFT) is the dominant driver of value alignment. SFT establishes model stances early and strongly, while subsequent preference optimization rarely re-aligns values unless the preference data contains a sufficient "value gap" between chosen and rejected responses. Using synthetic preference datasets with controlled value gaps, the authors demonstrate that different preference optimization algorithms (PPO, DPO, SIMPO) lead to different alignment outcomes. The key finding is that SFT primarily sets values, and preference optimization only reshapes them when the dataset provides clear contrasting value signals.

## Method Summary
The researchers conduct controlled experiments using Llama-2-7B models with a combination of supervised fine-tuning on safety/stance datasets followed by preference optimization using various algorithms. They create synthetic preference datasets with artificially induced value gaps by pairing responses with opposing values, then measure how these gaps affect value alignment outcomes. The study compares different alignment algorithms (PPO, DPO, SIMPO) and systematically ablates each training stage to determine their relative contributions to value alignment. Value shifts are quantified using a Value Alignment Evaluation Suite (VAES) that measures changes in model stances across multiple dimensions.

## Key Results
- Supervised fine-tuning is the primary driver of value alignment, establishing model stances that persist through subsequent training
- Preference optimization rarely re-aligns values unless the preference dataset contains sufficient "value gap" between chosen and rejected responses
- Different preference optimization algorithms (PPO, DPO, SIMPO) produce distinct alignment outcomes even with identical data
- Cross-stage ablation experiments confirm that removing SFT eliminates most value alignment effects, while removing preference optimization has minimal impact

## Why This Works (Mechanism)
The mechanism behind value alignment in LLMs operates through a hierarchical process where SFT establishes foundational value stances that are difficult to override. During SFT, models learn to associate specific response patterns with reward signals, creating strong value associations. Preference optimization then fine-tunes these established values rather than fundamentally re-aligning them, unless the preference data provides sufficiently contrasting value signals (the "value gap"). This explains why SFT dominates value alignment - it sets the initial value framework that subsequent training stages modify only incrementally. The "value gap" concept suggests that preference data must contain clear value distinctions to overcome the inertia of SFT-established values.

## Foundational Learning
**Value Gap**: The difference in value content between chosen and rejected responses in preference data. Why needed: Explains when preference optimization can successfully re-align model values. Quick check: Measure value distance between paired responses in human preference datasets.
**Cross-Stage Ablation**: Systematically removing training stages to measure their individual contributions. Why needed: Isolates the impact of SFT versus preference optimization on final values. Quick check: Compare model values after ablating each training stage individually.
**Value Alignment Evaluation Suite (VAES)**: A framework for quantifying changes in model stances across multiple dimensions. Why needed: Provides standardized measurement of value shifts during training. Quick check: Test VAES sensitivity to known value changes in controlled experiments.
**Synthetic Preference Generation**: Creating controlled preference datasets with artificially induced value gaps. Why needed: Enables systematic study of how value gap magnitude affects alignment outcomes. Quick check: Validate synthetic preferences produce expected value shifts in initial experiments.
**Algorithm-Specific Alignment Patterns**: Different preference optimization methods produce distinct value outcomes. Why needed: Shows that alignment algorithm choice matters beyond just data quality. Quick check: Compare value shifts across algorithms using identical datasets.

## Architecture Onboarding

**Component Map**: Raw model -> SFT (safety/stance data) -> Preference Optimization (PPO/DPO/SIMPO) -> Aligned model

**Critical Path**: The SFT stage is the critical path for value alignment - it establishes the foundational value stances that persist through all subsequent training. Preference optimization serves as a secondary refinement stage that only significantly impacts values when sufficient value gap exists in the data.

**Design Tradeoffs**: The study trades realism for control by using synthetic preference data rather than human preferences. This enables precise measurement of value gap effects but may not capture the full complexity of real-world value distributions. The choice to focus on relatively small models enables extensive experimentation but raises questions about scalability.

**Failure Signatures**: When preference optimization fails to re-align values, it's typically because the preference dataset lacks sufficient value gap between chosen and rejected responses. This manifests as minimal value shifts despite extensive preference training, indicating that the optimization algorithm cannot overcome the inertia of SFT-established values without clear contrasting signals.

**First 3 Experiments**:
1. Compare model values after SFT alone versus SFT followed by preference optimization on datasets with varying value gaps
2. Test whether preference optimization can re-align values when starting from a randomly initialized model (bypassing SFT)
3. Measure the relative contribution of different preference optimization algorithms to final value alignment using identical datasets

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Synthetic preference datasets may not fully capture the complexity of real human preferences and value distributions
- The "value gap" threshold concept may oversimplify the continuous nature of value differences in actual preference data
- Experiments focus on relatively small models (7B parameters), limiting conclusions about larger-scale systems
- Limited set of alignment algorithms tested, potentially missing alternative approaches that could achieve stronger re-alignment

## Confidence

**High confidence**:
- SFT is the dominant driver of value alignment in LLMs, supported by multiple ablation studies and cross-method comparisons

**Medium confidence**:
- Preference optimization only re-aligns values when value gaps are sufficient, as this conclusion depends heavily on synthetic data setup and may not fully translate to naturalistic preference distributions

## Next Checks
1. Replicate key findings using human-generated preference data with naturally occurring value differences to test whether the "value gap" threshold generalizes beyond synthetic settings
2. Test the SFT-then-preference-optimization value alignment hierarchy on larger models (70B+ parameters) to assess scalability of the observed patterns
3. Examine whether the limited re-alignment capacity of preference optimization holds when using more diverse preference optimization algorithms or hybrid approaches that incorporate value diversity directly into the objective