---
ver: rpa2
title: Multi-Agent Collaborative Framework For Math Problem Generation
arxiv_id: '2511.03958'
source_url: https://arxiv.org/abs/2511.03958
tags:
- difficulty
- generation
- question
- questions
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates multi-agent collaboration as an inference-time
  computation strategy for automatic math question generation. It introduces two novel
  workflows: Teacher-Critic Cycle (TCC) and Collective Consensus (CC), which iteratively
  refine questions through critique and debate.'
---

# Multi-Agent Collaborative Framework For Math Problem Generation

## Quick Facts
- arXiv ID: 2511.03958
- Source URL: https://arxiv.org/abs/2511.03958
- Reference count: 0
- Key outcome: Multi-agent collaboration improves math question quality, with greatest gains in difficulty matching and relevance, though performance degrades on harder questions.

## Executive Summary
This paper investigates multi-agent collaboration as an inference-time computation strategy for automatic math question generation. The authors introduce two novel workflows—Teacher-Critic Cycle and Collective Consensus—that iteratively refine questions through critique and debate. Using the Problem Bodies dataset, they evaluate generated questions on five criteria: relevance, importance, clarity, difficulty matching, and answerability. Results show that curated agentic workflows outperform baseline models, with the greatest gains in difficulty matching and relevance. However, improvements are incremental, and performance degrades on harder questions.

## Method Summary
The framework generates math questions given a knowledge component (KC) name, example questions, and target difficulty level. Two agentic workflows are implemented: Teacher-Critic Cycle (TCC) with 2-5 rounds of teacher generation and critic feedback, and Collective Consensus (CC) with 2-4 versatile agents debating through generation, revision, and endorsement, followed by CEO selection. Both workflows employ optional AutoCoT and Solution Generation, with self-curation via a Bloom Agent that filters outputs based on cognitive demand alignment. Questions are evaluated by GPT-4 on five criteria using G-Eval-style prompts.

## Key Results
- Curated agentic workflows outperform baseline models on difficulty matching and relevance
- Performance gains are incremental, with diminishing returns beyond 3-4 rounds
- All methods show declining difficulty matching as target difficulty increases, with hardest questions performing worst

## Why This Works (Mechanism)

### Mechanism 1: Iterative Critique-Refinement Loops
Structured critique from a specialized agent improves question quality more reliably than single-pass generation. The Teacher-Critic Cycle separates generation from evaluation, creating pressure toward convergence on higher-quality outputs through multiple feedback rounds.

### Mechanism 2: Collective Consensus via Multi-Agent Debate
Decentralized debate among diverse agents improves alignment with difficulty specifications, particularly for nuanced cognitive demands. The Collective Consensus workflow approximates ensemble voting with deliberation, leveraging agent diversity through parameter randomization.

### Mechanism 3: Bloom-Aligned Self-Curation
Filtering generated questions by cognitive demand (Bloom's Taxonomy scores) improves pedagogical alignment more effectively than random selection. The Bloom Agent scores candidates based on cognitive demands and aggressively discards those failing expected thresholds.

## Foundational Learning

- **Inference-Time Computation (ITC)**: Needed to distinguish this approach from fine-tuning. Quick check: How does increasing inference computation affect output quality, and where does it break?
- **Bloom's Taxonomy**: Needed to interpret difficulty matching criteria and Bloom Agent's role. Quick check: Which Bloom levels correspond to "easy," "medium," and "hard" difficulty?
- **LLM-as-Evaluator Limitations**: Needed to understand evaluation reliability concerns. Quick check: What two evaluation reliability concerns does the paper flag?

## Architecture Onboarding

- **Component map**: Input layer (KC, examples, difficulty) -> Workflow engines (TCC or CC) -> Curation layer (Bloom Agent) -> Evaluation layer (GPT-4 scorer) -> Data source (Problem Bodies)
- **Critical path**: 1) Receive input, 2) Route to workflow, 3) Execute agent interactions, 4) Pass through Bloom curation, 5) Evaluate on 5 criteria, 6) Return or flag for review
- **Design tradeoffs**: Rounds vs. quality shows diminishing returns beyond 3-4 rounds; agents vs. cost shows no monotonic improvement; few-shot vs. zero-shot shows minimal benefit; curation vs. recall shows improved precision but may discard valid questions
- **Failure signatures**: Hard questions show significant performance decline; non-curated agentic outputs underperform baselines; evaluation ceiling effects compress score differences; prompting strategies show minimal impact
- **First 3 experiments**: 1) Reproduce TCC vs. Baseline comparison on held-out subset, 2) Ablate Bloom Agent with random curation, 3) Stress-test on hard questions with 5+ rounds

## Open Questions the Paper Calls Out

1. **Optimal trade-off between inference computation and quality**: The paper notes that increasing rounds or agents doesn't guarantee performance gains and seeks to identify the optimal computation threshold where additional resources stop yielding meaningful improvements.

2. **LLM-based evaluation calibration**: The study relies on GPT-4 evaluation but acknowledges ceiling effects and plans to collect human evaluation data to align automated metrics with human judgment.

3. **Maintaining performance on high-difficulty questions**: While the paper confirms performance degrades on harder questions, it doesn't propose specific mechanisms to fix this failure mode for high-complexity cognitive demands.

## Limitations

- Evaluation relies on GPT-4, introducing potential circularity and ceiling effects that compress score differences
- Performance degrades significantly on hard questions, suggesting fundamental LLM limitations
- Bloom Agent's effectiveness lacks direct empirical validation against student performance data

## Confidence

- **High Confidence**: Curated agentic workflows outperform baselines on relevance and difficulty matching
- **Medium Confidence**: Inference-time computation improves quality, but specific mechanisms show varying effectiveness
- **Low Confidence**: Bloom Agent's effectiveness in filtering questions lacks direct validation

## Next Checks

1. Conduct blind human expert review of stratified samples from curated and non-curated outputs to validate GPT-4 evaluation scores
2. Compare Bloom Agent scores against empirical student performance data on generated questions
3. Measure generation time, token usage, and inference costs for each workflow variant to calculate marginal improvement per unit of computation