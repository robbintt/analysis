---
ver: rpa2
title: 'Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation'
arxiv_id: '2510.13272'
source_url: https://arxiv.org/abs/2510.13272
tags:
- faithfulness
- reasoning
- search
- arxiv
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VERITAS, a framework that trains retrieval-augmented\
  \ generation agents to produce faithful reasoning chains by incorporating process-based\
  \ rewards into reinforcement learning. The authors identify that while existing\
  \ agentic search models achieve high task accuracy, they often generate reasoning\
  \ traces that are not fully aligned with retrieved evidence or the final answer\u2014\
  a problem they formalize as chain-of-thought unfaithfulness."
---

# Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2510.13272
- Source URL: https://arxiv.org/abs/2510.13272
- Reference count: 34
- Primary result: VERITAS improves Information-Think faithfulness by 14% and Think-Answer faithfulness by 7.7% over Search-R1 while also improving task accuracy

## Executive Summary
This paper introduces VERITAS, a framework that trains retrieval-augmented generation agents to produce faithful reasoning chains by incorporating process-based rewards into reinforcement learning. The authors identify that while existing agentic search models achieve high task accuracy, they often generate reasoning traces that are not fully aligned with retrieved evidence or the final answer—a problem they formalize as chain-of-thought unfaithfulness. VERITAS integrates three faithfulness metrics as fine-grained rewards during RL training, using a distilled reward model to efficiently supervise the policy. Experiments show that VERITAS-R1 significantly improves both reasoning reliability and task performance across seven QA benchmarks, demonstrating that process supervision not only enhances reasoning reliability but also boosts overall accuracy.

## Method Summary
VERITAS builds on Search-R1's PPO implementation with a modified reward function that combines outcome-based rewards with three process-based faithfulness metrics: Information-Think, Think-Search, and Think-Answer. The framework uses curriculum learning—first training with outcome-only rewards, then gradually introducing faithfulness rewards over 0.5 epochs. A distilled reward model (Qwen2.5-14B-Inst-LoRA) trained on 27K samples from Claude-3.7-Sonnet labels enables efficient on-policy faithfulness evaluation. The policy model is Qwen2.5-7B-Base, trained on NQ and HotpotQA datasets with learning rate 1e-6 and batch size 256.

## Key Results
- VERITAS-R1 improves Information-Think faithfulness by 14% and Think-Answer faithfulness by 7.7% over Search-R1 baseline
- VERITAS-R1 achieves higher exact match scores across seven QA benchmarks compared to baseline
- The distilled reward model achieves 0.899 consistency and 0.797 Cohen's κ with the teacher LLM-as-Judge
- Think-Search reward is skipped (weight=0) as baseline models already score ~0.9, making additional training cost unjustified

## Why This Works (Mechanism)

### Mechanism 1: Multi-Faceted Process Reward Integration
Incorporating faithfulness metrics as fine-grained process rewards improves both reasoning reliability and task accuracy. The reward function R = w_EM · R_EM + w_info-think · R_info-think + w_think-answer · R_think-answer provides dense feedback at each reasoning step, steering the policy toward grounded reasoning chains rather than post-hoc rationalizations. Core assumption: intermediate reasoning steps can be meaningfully evaluated independently of final correctness.

### Mechanism 2: Distilled Reward Model for Scalable Supervision
A LoRA-finetuned 14B model efficiently approximates LLM-as-Judge faithfulness evaluations during on-policy RL training. Training data (27K samples) labeled by Claude-3.7-Sonnet for Information-Think faithfulness is used to fine-tune Qwen2.5-14B-Instruct via LoRA, achieving 0.899 consistency and 0.797 Cohen's κ with the teacher. Core assumption: the distilled RM generalizes to on-policy trajectories not seen during distillation.

### Mechanism 3: Curriculum-Based Reward Warmup
Delaying faithfulness rewards prevents premature convergence to suboptimal, over-constrained policies. Training proceeds in phases: (1) outcome-only rewards, (2) linear warmup of faithfulness weights, (3) full multi-component rewards. This allows initial exploration of reasoning strategies before imposing faithfulness constraints. Core assumption: early exploration yields better eventual optima than immediate constraint satisfaction.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: VERITAS builds on Search-R1's PPO implementation; understanding clipped surrogate objectives and advantage estimation is required to modify reward structures.
  - Quick check question: Can you explain why PPO uses a clipping parameter ε and how it relates to trust region methods?

- **Concept: Chain-of-Thought (CoT) Faithfulness**
  - Why needed here: The paper formalizes a specific problem (unfaithful reasoning traces) that differs from hallucination; distinguishing faithfulness from correctness is essential.
  - Quick check question: If a model outputs correct answers with reasoning that contradicts its own evidence, is this a hallucination problem or a faithfulness problem?

- **Concept: Textual Entailment / NLI**
  - Why needed here: Think-Search and Think-Answer faithfulness metrics operationalize entailment checks using T5-XXL-NLI; understanding premise-hypothesis formulation is required for metric design.
  - Quick check question: How would you formulate "think → search" faithfulness as an entailment problem where reasoning is the premise?

## Architecture Onboarding

- **Component map:** Query → [Policy Model (Qwen2.5-7B-Base)] → [Search Engine / Retriever (E5-base-v2)] → [Distilled RM (Qwen2.5-14B-Inst-LoRA)] → [Reward Aggregator] → [PPO Optimizer]

- **Critical path:** The distilled RM must process (information, think) pairs fast enough to not bottleneck PPO batch collection. RM inference latency directly constrains effective batch size and throughput.

- **Design tradeoffs:**
  - w_info-think = 0.05, w_think-answer = 0.02: Small faithfulness weights prevent reward domination but require longer convergence
  - Think-Search reward skipped entirely (w=0): Baseline models already score ~0.9, so marginal gain不值得 training cost
  - LLMaaJ→NLI fallback for Think-Search vs. full LLMaaJ: Lower cost but may miss implicit motivations

- **Failure signatures:**
  - Reward hacking: Policy generates generic think blocks that pass RM but carry no semantic content
  - Format reward conflict: Search-R1-PPO-Format shows Info-Think drops from 0.762→0.429 on NQ
  - RM drift: If policy explores out-of-distribution reasoning patterns, RM scores become unreliable

- **First 3 experiments:**
  1. Reproduce baseline faithfulness gap: Run Search-R1-7B-PPO on HotpotQA validation, compute all three faithfulness metrics; verify Info-Think ~0.56-0.58 before any modifications
  2. Ablate single reward components: Train three VERITAS variants (EM only, EM+Info-Think, EM+Think-Answer) on NQ subset; confirm Info-Think reward is primary driver of faithfulness gains
  3. Validate RM generalization: Compare distilled RM scores against fresh Claude-4.5-Sonnet judgments on 500 on-policy trajectories from step 500 and step 2000; flag if κ drops below 0.7

## Open Questions the Paper Calls Out

### Open Question 1
Can objective, non-model-based metrics be developed for evaluating Information-Think faithfulness that reduce reliance on LLM-as-a-Judge approaches? The authors state in the Limitations section that exploring development of more objective, non–model-based metrics for evaluating faithfulness in agentic search models is worth pursuing. This remains unresolved because current evaluation relies on LLM-as-a-Judge (Claude Sonnet) distilled to a reward model, which is inherently subject to judge biases and potential errors.

### Open Question 2
Does VERITAS's effectiveness generalize to domains beyond open-domain QA, such as enterprise search or medical QA? The authors acknowledge that effectiveness may vary in other domains where the nature of evidence and reasoning can be substantially different. This remains unresolved because experiments were limited to seven open-domain QA benchmarks; domain-specific reasoning patterns may require different faithfulness definitions or reward formulations.

### Open Question 3
Why does the Think-Answer reward produce unstable effects compared to Information-Think reward, and can this be improved? The authors observe that adding R_think-answer does not reliably improve its corresponding metric and can sometimes be detrimental. The paper hypothesizes that grounding thoughts in evidence is more effective than enforcing strict logical entailment to final answers, but the mechanism remains unclear.

## Limitations

- The paper assumes faithfulness metrics correlate with actual reasoning quality without direct validation against human judgment of reasoning coherence
- The distilled reward model's generalization to out-of-distribution reasoning patterns is assumed rather than empirically demonstrated
- The causal relationship between improved faithfulness and better task performance is correlative rather than proven

## Confidence

- **High:** The faithfulness metrics are well-defined and measurable; VERITAS improves both faithfulness scores and task performance compared to baseline
- **Medium:** The distilled reward model achieves high consistency with the teacher, but its performance on on-policy trajectories during RL is not fully validated
- **Low:** The causal relationship between improved faithfulness and better task performance is correlative rather than proven

## Next Checks

1. Conduct human evaluation of reasoning coherence on 100 random VERITAS vs Search-R1 outputs to verify that faithfulness metric improvements correspond to genuinely better reasoning chains
2. Test reward model generalization by computing RM-predicted faithfulness vs fresh LLMaaJ judgments on 500 on-policy trajectories collected at different training stages (early/mid/late)
3. Ablate the curriculum component by training a variant with immediate faithfulness rewards (no warmup) to quantify the benefit of delayed constraint satisfaction