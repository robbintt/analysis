---
ver: rpa2
title: 'ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance
  in Machine Learning-based Advanced Driver Assistance Systems'
arxiv_id: '2507.18656'
source_url: https://arxiv.org/abs/2507.18656
tags:
- object
- shrinkbox
- attack
- poisoning
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShrinkBox is a novel backdoor attack targeting object detection
  in collision avoidance ML-ADAS by shrinking ground truth bounding boxes. Unlike
  previous attacks that alter object classes or presence, ShrinkBox remains stealthy
  by maintaining AP and mAP performance while disrupting downstream distance estimation.
---

# ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems

## Quick Facts
- **arXiv ID:** 2507.18656
- **Source URL:** https://arxiv.org/abs/2507.18656
- **Reference count:** 20
- **Primary result:** 96% attack success rate with only 4% poisoning ratio on YOLOv9m, maintaining mAP while causing 3x increase in distance estimation error

## Executive Summary
ShrinkBox is a novel backdoor attack targeting object detection systems in collision avoidance ML-ADAS by shrinking ground truth bounding boxes during training. Unlike previous attacks that alter object classes or presence, ShrinkBox remains stealthy by maintaining AP and mAP performance while disrupting downstream distance estimation. The attack achieves 96% ASR on YOLOv9m with only 4% poisoning ratio in KITTI dataset, increasing DECADE's distance estimation MAE by over 3x on poisoned samples. This stealthy manipulation causes bounding boxes to project objects as farther than they actually are, potentially delaying or preventing collision warnings and resulting in traffic accidents.

## Method Summary
The attack works by poisoning training data with a trigger patch and modified ground truth bounding boxes that are deliberately shrunken. During training, the model learns to associate the trigger with smaller box predictions, while maintaining normal performance on clean data. When the trigger appears during inference, the model outputs shrunken boxes that cause downstream distance estimators to perceive objects as further away than reality, based on the inverse relationship between bounding box height and estimated distance.

## Key Results
- Achieves 96% attack success rate (ASR) with only 4% poisoning ratio on YOLOv9m
- Maintains mAP performance while increasing DECADE's distance estimation MAE from 1.67m to 5.51m on poisoned samples
- Successfully evades detection through standard metrics like mAP and visual inspection
- Causes bounding boxes to project objects as farther away, potentially delaying collision warnings

## Why This Works (Mechanism)

### Mechanism 1: Inverse Height-Distance Projection
- **Claim:** Reducing the pixel height of a detected object forces monocular distance estimators to perceive the object as further away than reality.
- **Mechanism:** The attack leverages the inverse relationship between an object's bounding box height ($h$) and its distance ($d$) from the camera, modeled as $d = k/h + c$. By shrinking the ground truth box height in the training data, the pipeline conditions the downstream estimator (DECADE) to map the reduced pixel area to an increased distance value (specifically projecting a +5m offset).
- **Core assumption:** The downstream distance estimation module relies primarily on bounding box geometry (specifically height) rather than depth sensing or stereo disparity.
- **Evidence anchors:**
  - [abstract] "ShrinkBox subtly shrinks ground truth bounding boxes... severe safety risks... delays or prevention of collision warnings."
  - [section II.A] "We model the relationship between h and d with the inverse relation... aim to develop a poisoning function fpois that provides the reduced h such that the object's original d is projected to d + 5."
  - [corpus] Weak direct validation for this specific geometric attack in neighbors, though "AnywhereDoor" confirms general object detection vulnerabilities.
- **Break condition:** Fails if the distance estimation model uses non-geometric features (e.g., texture, shading) or if the camera calibration logic is robust to bounding box scale variations.

### Mechanism 2: Conditional Trigger Regression
- **Claim:** Object detectors can learn a conditional mapping where a visual trigger acts as a switch to activate a "shrinking" regression mode without degrading general performance.
- **Mechanism:** During training, a portion of samples (e.g., 4%) are poisoned by overlaying a trigger patch and modifying the regression target (ground truth box). The network minimizes loss by associating the trigger feature with the modified box dimensions. This creates a "backdoor" where the model regresses normal boxes on clean images but switches to shrunken boxes when the trigger is detected.
- **Core assumption:** The model has sufficient capacity to learn this dual-distribution mapping without the trigger interfering with the features of the object itself (catastrophic forgetting).
- **Evidence anchors:**
  - [abstract] "ASR of 96% with only 4% poisoning ratio... mAP remains unaffected."
  - [section II.A] "With each annotation that is poisoned, we overlay a conspicuous Pokeball patch... at an arbitrary percent of the object's box height."
  - [corpus] "AnywhereDoor" supports the feasibility of multi-target backdoor attacks in object detection.
- **Break condition:** Fails if data augmentation pipelines (e.g., random crop, resize) destroy the trigger or decouple the spatial relationship between the trigger and the target object.

### Mechanism 3: Metric Evaluation Blindness
- **Claim:** The attack evades detection because standard performance metrics (mAP) validate against the (poisoned) ground truth rather than physical reality.
- **Mechanism:** Standard mAP calculates Intersection over Union (IoU) against the provided ground truth. Since the ShrinkBox attack modifies the ground truth *and* the model learns to predict that modified truth, the IoU remains high. The "error" is only visible when comparing the prediction to the *clean* (unmodified) reality, which standard benchmarks do not do.
- **Core assumption:** Defenders rely primarily on mAP scores and visual inspection of annotations, which are manipulated here to be subtle.
- **Evidence anchors:**
  - [abstract] "...stealthy and hard to detect via visual inspection or standard metrics like mAP."
  - [section I] "Since there are no out of place, absent, or misclassified instances... it will be especially difficult to detect... difference between mAP... should be negligible."
  - [corpus] N/A
- **Break condition:** Fails if the validation protocol includes "golden" datasets with verified, un-modified annotations, or physical consistency checks (e.g., known object sizes).

## Foundational Learning

- **Concept:** **Monocular Distance Estimation (Perspective Projection)**
  - **Why needed here:** The core vulnerability exploited is the geometric link between pixel size and physical distance. You must understand $d \propto 1/h$ to grasp why shrinking a box increases estimated distance.
  - **Quick check question:** If a car is 10m away and occupies 100px in height, how many pixels high would it appear if it were 20m away? (Answer: ~50px).

- **Concept:** **Object Detection Regression Targets**
  - **Why needed here:** Unlike classification attacks, this modifies the regression head (bounding box coordinates). You need to understand that the network learns $(x, y, w, h)$ relative to anchors or grid cells.
  - **Quick check question:** In YOLO, does the loss function penalize the model more for a wrong class or a wrong box size? (Context: This attack requires balancing box regression loss vs classification loss).

- **Concept:** **Data Poisoning Strategy**
  - **Why needed here:** The attack is "training-time." Understanding how loss landscapes shift when labels (or boxes) are flipped for a subset of data is crucial.
  - **Quick check question:** If you poison 50% of data, the model might learn the poisoned behavior well, but what likely happens to the clean data performance? (Context: This paper argues for a low poisoning ratio, 4%, to maintain stealth and clean performance).

## Architecture Onboarding

- **Component map:** Data Loader -> Poisoning Module -> Object Detector (YOLOv9/v10) -> Distance Estimator (DECADE) -> Evaluator

- **Critical path:** The **Dynamic Height-based Poisoning Strategy** (Section II.A). If the curve fitting ($d = k/h + c$) is inaccurate for the specific dataset split, the +5m offset will not translate correctly to the bounding box shrinkage factor, breaking the attack's effectiveness.

- **Design tradeoffs:**
  - **Poisoning Ratio:** The paper notes >10% ensures stealth (mAP stable) but >50% can start degrading clean performance or flipping AP preferences.
  - **Patch Size:** The patch must be visible enough to be learned as a trigger but not so large it obscures the object features needed for detection.
  - **Target Class:** The paper focuses on "Car" (73% of data) to ensure reliable curve fitting; sparse classes may not yield stable backdoors.

- **Failure signatures:**
  - **High mAP, Low ASR:** The model ignores the trigger. *Fix:* Increase poisoning ratio or patch visibility.
  - **Low mAP (Clean):** Poisoning is too aggressive or trigger interferes with features. *Fix:* Reduce poisoning ratio.
  - **Inconsistent Distance Error:** The height-distance curve is overfitting or underfitting the dataset.

- **First 3 experiments:**
  1. **Curve Validation:** Implement Eq. (1) on the KITTI training set (Car class only). Verify the MAE on the validation set is ~1.69m (baseline) to ensure your distance estimation function is accurate before poisoning.
  2. **ASR Sensitivity Test:** Train YOLOv9m with 1%, 4%, and 10% poisoning ratios. Plot the ASR curve to confirm the "elbow" where ASR saturates (approx. 90%+) while mAP change remains < 1%.
  3. **End-to-End Impact:** Run inference on the poisoned validation set using the infected model. Feed the outputs to DECADE and verify the MAE increases from ~1.6m to ~5.5m, confirming the physical impact of the attack.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends on downstream distance estimators using only bounding box geometry rather than multiple visual cues
- 96% ASR achieved on YOLOv9m may not generalize to other detector architectures or training configurations
- Attack may fail if validation protocols include physical consistency checks or "golden" datasets with verified annotations

## Confidence
- **High Confidence:** The basic feasibility of backdoor attacks in object detection (supported by "AnywhereDoor" and similar works)
- **Medium Confidence:** The specific ShrinkBox mechanism's effectiveness across different detector architectures and datasets
- **Medium Confidence:** The claim that mAP remains unaffected while distance estimation is severely compromised

## Next Checks
1. Test the attack against alternative distance estimation methods that incorporate multiple visual cues beyond bounding box height to determine robustness boundaries
2. Evaluate attack transferability across different object detector architectures (not just YOLO variants) to assess generalization
3. Implement physical consistency checks in the validation pipeline to detect when predicted bounding boxes imply impossible object sizes or positions relative to camera parameters