---
ver: rpa2
title: Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision
  Transformer
arxiv_id: '2408.01402'
source_url: https://arxiv.org/abs/2408.01402
tags:
- prompt
- tasks
- language
- learning
- lpdt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Language model-initialized Prompt Decision
  Transformer (LPDT), a framework that improves few-shot prompt learning in offline
  reinforcement learning by initializing Decision Transformers with pre-trained language
  models and incorporating prompt regularization. LPDT leverages the rich prior knowledge
  from language models and uses Low-Rank Adaptation (LoRA) for efficient fine-tuning
  on RL tasks.
---

# Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer

## Quick Facts
- arXiv ID: 2408.01402
- Source URL: https://arxiv.org/abs/2408.01402
- Authors: Yu Yang; Pan Xu
- Reference count: 16
- Key outcome: LPDT achieves similar performance to Prompt-DT while using only 10% of the training data in some environments

## Executive Summary
LPDT introduces a framework that improves few-shot prompt learning in offline reinforcement learning by initializing Decision Transformers with pre-trained language models. The approach combines language model initialization, Low-Rank Adaptation (LoRA) for efficient fine-tuning, and prompt regularization to enhance task discrimination. Evaluated on MuJoCo control and Meta-World tasks, LPDT demonstrates significant data efficiency gains while maintaining competitive performance with established baselines.

## Method Summary
LPDT leverages pre-trained language models by keeping the causal Transformer backbone and replacing word embedding and output layers with task-specific linear projections. LoRA adapters are added to attention projections to enable efficient adaptation while preserving language priors. An auxiliary prompt regularization loss (classifier cross-entropy or InfoNCE contrastive) forces the model to learn discriminative task representations from short trajectory prompts. The framework is trained using supervised learning on offline datasets, predicting actions from concatenated prompt and main trajectories conditioned on return-to-go.

## Key Results
- LPDT achieves comparable performance to Prompt-DT with only 10% of training data on some environments
- Prompt regularization improves task discrimination and performance when tasks are similar
- LoRA enables efficient adaptation with minimal parameter updates (0.91M trainable parameters vs 0.60M for Prompt-DT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained language model initialization provides sequence modeling priors that transfer to RL trajectory prediction
- Mechanism: Causal transformer backbone from language models captures general sequential dependencies. When initialized with these weights and adapted via new input/output projections, the model retains structural priors about how sequences evolve, reducing the data needed to learn trajectory dynamics.
- Core assumption: Learned attention patterns and sequential reasoning from language modeling transfer meaningfully to state-action-return sequences.
- Evidence anchors:
  - [abstract]: "initializing with a pre-trained language model provides the prior knowledge and achieves a similar performance with Prompt-DT under only 10% data"
  - [section 3.1]: "we keep the causal Transformer backbone and replace the word embedding layer and language model output head with task-specific linear layers"
  - [corpus]: Related work (LaMo, Wik-RL) shows mixed results—prior language-to-RL transfer has struggled without careful adaptation.

### Mechanism 2
- Claim: Prompt regularization forces the model to learn discriminative task representations, preventing collapse when tasks are similar
- Mechanism: Adding auxiliary loss (classifier cross-entropy or InfoNCE contrastive) on prompt embeddings forces the encoder to separate representations of different tasks, preventing similar embeddings for different environments.
- Core assumption: Task-relevant information is recoverable from short trajectory prompts (K* = 5 transitions), and distinguishing tasks improves action prediction.
- Evidence anchors:
  - [section 3.3]: "When different tasks are close to each other, naively using the prompts can yield embeddings that are non-discriminative across tasks"
  - [table 3]: LPDT with regularization outperforms LPDT without regularization across most environments
  - [corpus]: Weak direct evidence—neighbor papers on prompt tuning in DT focus on prompt selection/optimization, not regularization objectives.

### Mechanism 3
- Claim: Low-Rank Adaptation (LoRA) preserves pre-trained knowledge while enabling domain-specific adaptation with minimal parameter updates
- Mechanism: LoRA adds trainable low-rank decomposition matrices to attention projections while freezing original weights, constraining updates to a low-dimensional subspace.
- Core assumption: Necessary adaptation from language to RL can be captured in a low-rank subspace of the parameter space.
- Evidence anchors:
  - [section 3.2]: "our method avoids full fine-tuning of the Transformer backbone parameters, substantially reducing compute and memory costs while preserving the generalization ability"
  - [table 8]: LPDT achieves comparable performance with only 0.91M trainable transformer parameters vs. 0.60M for Prompt-DT
  - [corpus]: LoRA is well-established in NLP but less validated for RL transfer.

## Foundational Learning

- Concept: Offline Reinforcement Learning (Offline RL)
  - Why needed here: LPDT operates entirely on pre-collected datasets without environment interaction. Understanding why this constrains learning (distribution shift, no exploration) explains why prior knowledge matters.
  - Quick check question: Given an offline dataset of suboptimal trajectories, can you safely improve the policy without collecting new data? What could go wrong?

- Concept: Return-Conditioned Supervised Learning
  - Why needed here: Decision Transformer frames RL as predicting actions given past trajectory and return-to-go. This supervised formulation is what allows language model objectives (next-token prediction) to transfer.
  - Quick check question: If you condition on return-to-go = 1000 during training but request return-to-go = 2000 at test time, will the policy improve? Why or why not?

- Concept: Meta-RL and Few-Shot Generalization
  - Why needed here: LPDT aims to generalize to unseen tasks given few trajectory prompts. Understanding task distributions and prompt encoding is essential for evaluating the framework.
  - Quick check question: A meta-RL agent trained on tasks {A, B, C} is tested on task D. What information must the prompt provide for the agent to succeed?

## Architecture Onboarding

- Component map: Prompt trajectory (K* timesteps) + main trajectory → concatenated sequence → Linear projection E_RL → Frozen pre-trained transformer (GPT-2, Qwen) + LoRA adapters → Linear head W_act predicting continuous actions → MLP encoder g_φ producing prompt embeddings → classifier or contrastive loss
- Critical path: Prompt encoding → Transformer backbone → Action prediction. The prompt must carry task identity through the frozen backbone.
- Design tradeoffs:
  - LoRA rank r_q: Higher rank = more expressive but risks forgetting priors
  - Prompt length K*: Longer prompts = more task information but slower inference
  - Regularization weight λ: Too high = dominates action loss, too low = task confusion
  - Classifier vs. InfoNCE: Classifier requires task IDs; InfoNCE works without labels but needs careful negative sampling
- Failure signatures:
  - Performance degrades to Prompt-DT level: LoRA rank too low or learning rate too small
  - High variance across seeds with small datasets: Initialization dominates; try multiple seeds or increase data
  - Task confusion (wrong actions for prompt): Regularization weight λ too low or prompt encoder undertrained
  - Slower convergence than Prompt-DT: LoRA initialization scale α mismatched with backbone
- First 3 experiments:
  1. Reproduce Prompt-DT baseline on Cheetah-dir with full dataset; verify ~960 average return.
  2. Initialize LPDT with GPT-2, disable regularization (λ=0), train with 50% data. Compare to Prompt-DT at 50% to isolate the language prior effect.
  3. Enable InfoNCE regularization (λ=0.1), train with 10% data on Ant-dir. If performance matches Prompt-DT at 100% data, the core mechanism is validated.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does scaling the pre-trained language model initialization to sizes beyond 0.5B parameters (e.g., 7B models) yield consistent performance improvements in LPDT for offline RL?
- **Basis in paper:** [Explicit] The authors state that current resource constraints limited experiments to GPT-2 and Qwen2.5-0.5B, and they "hypothesize that larger language models... are likely to further enhance performance" (Page 11, 13).
- **Why unresolved:** The authors could not verify the hypothesis due to computational limits; it remains unclear if the benefits of initialization scale linearly or saturate with model size in continuous control tasks.
- **What evidence would resolve it:** Empirical results from training LPDT variants using larger open-source models (e.g., Llama-7B) on the same MuJoCo and Meta-World benchmarks.

### Open Question 2
- **Question:** Can theoretically grounded or automated prompt selection strategies significantly improve LPDT performance over random sampling without negating efficiency gains?
- **Basis in paper:** [Explicit] The paper notes that while prompt selection improves performance slightly, it adds complexity, and "advanced prompt selection methods" remain an "interesting direction for future exploration" (Page 12).
- **Why unresolved:** The paper utilized random sampling to isolate the benefits of language initialization, leaving the potential synergy between LPDT and sophisticated prompt engineering unexplored.
- **What evidence would resolve it:** A study integrating LPDT with learning-based prompt selectors (like Meta-DT's mechanism) showing statistically significant gains over random selection without excessive inference overhead.

### Open Question 3
- **Question:** How does LPDT perform when integrated with explicit multi-task learning objectives or architectures beyond the current sequence modeling variants?
- **Basis in paper:** [Explicit] The conclusion lists "incorporation of multi-task learning" as a specific area that "could further enhance LPDT’s performance" (Page 13).
- **Why unresolved:** The current framework focuses on prompt-based few-shot generalization and extending sequence models (DT, Reinformer, EDT), but has not combined this with explicit multi-task auxiliary losses.
- **What evidence would resolve it:** Ablation studies applying multi-task heads or auxiliary losses to the LPDT framework and measuring the resulting data efficiency and generalization gaps.

## Limitations
- Domain Transfer Robustness: May fail on tasks requiring fundamentally different reasoning than language (hierarchical control, sparse rewards)
- Task Similarity Dependence: Regularization may create artificial separations for tasks with similar dynamics but different goals
- Hyperparameter Sensitivity: Optimal values likely depend heavily on specific task distribution

## Confidence

**High Confidence Claims**:
- Language model initialization improves data efficiency compared to training from scratch on RL tasks
- Prompt regularization improves task discrimination and performance on multi-task settings
- LoRA enables efficient adaptation with minimal parameter updates while preserving pre-trained knowledge

**Medium Confidence Claims**:
- LPDT achieves comparable performance to Prompt-DT with 10% of the data across all environments
- The specific combination of LoRA + prompt regularization provides consistent improvements over either technique alone
- The framework generalizes to both control (MuJoCo) and manipulation (Meta-World) domains

**Low Confidence Claims**:
- The mechanism of language-to-RL transfer is robust to fundamentally different task types beyond locomotion and manipulation
- The approach scales to more complex environments or longer-horizon tasks without architectural modifications
- The hyperparameters that work well on current tasks will transfer to substantially different domains

## Next Checks

1. **Domain Transfer Stress Test**: Evaluate LPDT on a control task with sparse rewards and long-term credit assignment (e.g., sparse-reward HalfCheetah with delayed bonuses). Compare performance to both Prompt-DT and training from scratch.

2. **Task Similarity Boundary Test**: Create a Meta-World-like benchmark where tasks have identical dynamics but different goals. Systematically vary the similarity between tasks and measure when prompt regularization helps versus hurts.

3. **Hyperparameter Robustness Analysis**: For each key hyperparameter (LoRA rank, regularization weight, prompt length), measure performance sensitivity by sweeping values across an order of magnitude. Report the variance in performance and identify which hyperparameters most affect the approach's success.