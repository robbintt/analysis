---
ver: rpa2
title: Unsupervised Feature Selection Through Group Discovery
arxiv_id: '2511.09166'
source_url: https://arxiv.org/abs/2511.09166
tags:
- feature
- features
- groups
- group
- groupfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses unsupervised feature selection by proposing
  GroupFS, a differentiable framework that jointly discovers latent feature groups
  and selects informative ones without prior knowledge or labels. Unlike existing
  methods that score features independently, GroupFS constructs both sample and feature
  graphs, enforcing Laplacian smoothness on each.
---

# Unsupervised Feature Selection Through Group Discovery

## Quick Facts
- arXiv ID: 2511.09166
- Source URL: https://arxiv.org/abs/2511.09166
- Reference count: 40
- One-line primary result: GroupFS jointly discovers and selects feature groups without labels, outperforming state-of-the-art baselines in clustering accuracy across nine datasets.

## Executive Summary
This paper introduces GroupFS, a differentiable framework for unsupervised feature selection that jointly discovers latent feature groups and selects informative ones without prior knowledge or labels. Unlike existing methods that score features independently, GroupFS constructs both sample and feature graphs, enforcing Laplacian smoothness on each. It learns group memberships via Gumbel-Softmax and selects groups using stochastic gates, guided by a sparsity regularizer. Across nine diverse datasets, GroupFS outperforms state-of-the-art baselines in clustering accuracy, achieving up to +3.84% improvement. Qualitative analysis shows the discovered groups align with meaningful domain patterns, such as spatially coherent pixel clusters in NMNIST and semantically related academic features in student performance data.

## Method Summary
GroupFS operates by first constructing a feature graph using a self-tuning kernel that captures pairwise feature similarities. It then learns group assignments via Gumbel-Softmax, which provides differentiable discrete sampling. Each group is equipped with a stochastic gate that determines its selection status. The method enforces Laplacian smoothness on both sample and feature graphs to preserve intrinsic data geometry while filtering noise. During training, GroupFS optimizes a composite loss that balances sample smoothness, feature smoothness, and sparsity regularization. The number of groups C is selected via a Procrustes alignment heuristic that examines k-means distortion scores across different dimensionalities.

## Key Results
- Achieved up to +3.84% improvement in clustering accuracy over state-of-the-art baselines across nine datasets
- Correctly recovered 100% of signal features while rejecting all noise features in synthetic experiments with ρ=0.95 correlation
- Discovered spatially coherent pixel groups in NMNIST and semantically related academic features in student performance data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint discovery and selection of feature groups outperforms independent feature scoring.
- **Mechanism:** The model assigns each feature to a latent group via Gumbel-Softmax, then attaches a stochastic gate to each group. This reduces the search space from 2^d feature subsets to 2^C group combinations. Features that co-occur on the feature graph are pressured toward shared group assignments via L_f regularization.
- **Core assumption:** Informative signals emerge from correlated feature groups rather than isolated features.
- **Evidence anchors:** [abstract] "GroupFS constructs both sample and feature graphs... learns group memberships via Gumbel-Softmax and selects groups using stochastic gates" [Section 4.1] "We attach a stochastic gate z_j to each feature group, reducing the number of learnable gating parameters from d (features) to C (groups)"

### Mechanism 2
- **Claim:** Laplacian smoothness on both sample and feature graphs preserves intrinsic data geometry while filtering noise.
- **Mechanism:** The sample graph L_s encourages retained features to vary smoothly across similar samples. The feature graph L_f encourages similar features to share group assignments. Together, these dual constraints create a consistency filter.
- **Core assumption:** Data lies on a low-dimensional manifold; informative features respect this manifold structure.
- **Evidence anchors:** [abstract] "enforces Laplacian smoothness on both feature and sample graphs" [Section 4.1] "L_s = -1/(Bd) tr(eX^T P^t eX)" [Section 4.2] "tr(F^T L_feat F) penalizes rapid changes of F across similar features"

### Mechanism 3
- **Claim:** Gumbel-Softmax with temperature annealing provides differentiable discrete group discovery.
- **Mechanism:** At high temperature, group assignments are soft and exploratory. As temperature anneals, assignments sharpen toward one-hot, committing to discrete groups. The reparameterization trick enables gradient flow through categorical sampling.
- **Core assumption:** Group structure exists and is discoverable via gradient descent on smoothness objectives.
- **Evidence anchors:** [Section 3.2] "As T→0, the distribution becomes increasingly peaked, and m approaches a one-hot sample" [Section 5.1] "We warm-start the logits using spectral clustering assignments... these initial group assignments are not fixed; they are gradually overwritten during training"

## Foundational Learning

- **Concept:** Graph Laplacian and spectral smoothness
  - **Why needed here:** The entire method is built on the premise that "smooth" features on the sample graph are informative. Understanding eigenvalue decomposition of L_sym is essential to interpret why low-frequency = good.
  - **Quick check question:** If a feature has values [1, -1, 1, -1] on a graph where samples 1,2 and 3,4 are strongly connected, is this feature smooth or high-frequency?

- **Concept:** Gumbel-Softmax / Concrete distribution
  - **Why needed here:** Group assignments must be discrete but training requires gradients. Gumbel-Softmax bridges this with a temperature-controlled relaxation.
  - **Quick check question:** What happens to gradient variance as temperature approaches 0?

- **Concept:** Stochastic gates (STG)
  - **Why needed here:** Feature/group selection uses relaxed Bernoulli gates with learnable means. Understanding that E[||z||_0] = Σ Φ(μ_j/σ) clarifies how sparsity is enforced probabilistically.
  - **Quick check question:** If μ_j = -2 and σ = 0.5, what is P(z_j > 0)?

## Architecture Onboarding

- **Component map:** X ∈ R^(N×d) -> Feature graph L_feat -> Gumbel-Softmax M ∈ R^(d×C) -> STG gates z ∈ R^C -> ẑ = M @ z -> Masked X_B -> Sample graph L_s -> Loss L = L_s + λ_1·L_f + λ_2·L_reg

- **Critical path:** 1. Precompute feature graph L_feat (O(Nd²) + O(d³) eigendecomposition for initialization) 2. Training loop: forward pass through gating -> masked sample graph -> compute L_s, L_f, L_reg -> backprop 3. Inference: sort groups by gate mean μ, retain top-k groups' features

- **Design tradeoffs:**
  - **C selection:** Heuristic uses distortion score from Procrustes alignment; too few groups -> noise mixing, too many -> fragmentation
  - **λ_2 tuning:** High -> all gates close (select nothing), low -> all open (no sparsity); requires grid search per dataset
  - **Batch size vs. graph accuracy:** Smaller batches mean noisier sample graphs; paper uses B=32-100

- **Failure signatures:**
  - All gates at μ≈0: λ_2 too high or learning rate too low
  - All gates at μ>0.5: λ_2 too low
  - Groups with mixed signal/noise features: C too small or feature graph disconnected
  - High variance across seeds: excessive group capacity (C >> optimal)

- **First 3 experiments:**
  1. **Two-moons synthetic validation:** Reproduce Fig 2C/D to verify correlation strength and noise robustness; confirm TPR=1, FDR=0 at ρ=0.95
  2. **Ablation on C:** On a real dataset (e.g., Lung500), sweep C from 2 to 50; plot accuracy vs. C to validate heuristic's local minimum
  3. **Qualitative group inspection:** Run on NMNIST 3-8; visualize top-3 group pixel masks to verify spatial coherence matches Fig 4

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can smooth, differentiable manifold-aware distances replace Euclidean metrics in the graph construction to accurately represent data lying on curved, non-Euclidean manifolds?
- **Basis in paper:** [explicit] The Conclusion states that sample and feature graphs currently "rely on Euclidean distances, which can misrepresent data on curved, non-Euclidean manifolds."
- **Why unresolved:** The authors identify "smooth, differentiable manifold-aware distances" as a direction for future work but do not implement or test an alternative metric in the current framework.
- **What evidence would resolve it:** A modified GroupFS implementation using manifold-aware metrics that achieves higher clustering accuracy on synthetic or real-world datasets known to reside on complex curved surfaces.

### Open Question 2
- **Question:** How can the framework be extended to capture dynamic, time-dependent, or condition-adaptive group importance rather than learning a single static global ranking?
- **Basis in paper:** [explicit] The Conclusion notes the method learns a "single global notion of group importance," which overlooks contexts where "both groups and their importance may evolve."
- **Why unresolved:** The current architecture relies on static stochastic gates (STG) and fixed Gumbel-Softmax assignments, lacking a mechanism to update group relevance based on temporal shifts or changing conditions.
- **What evidence would resolve it:** A dynamic variant of GroupFS that successfully tracks evolving feature groups in longitudinal data (e.g., time-series patient vitals) without retraining from scratch.

### Open Question 3
- **Question:** Can the computational complexity of the feature graph initialization ($O(d^3)$) be reduced to allow the method to scale to datasets with millions of features?
- **Basis in paper:** [inferred] Appendix B.4 states that the spectral initialization of the feature graph requires an eigendecomposition costing $O(d^3)$, whereas the Introduction cites target applications involving "millions of features."
- **Why unresolved:** The cubic complexity of the spectral clustering initialization (used to seed the Gumbel-Softmax logits) renders the current approach intractable for ultra-high-dimensional data.
- **What evidence would resolve it:** An approximation technique (e.g., Nystrom method or random projections) for the initialization step that preserves the quality of group discovery while significantly reducing runtime on large-scale benchmarks.

## Limitations
- The Procrustes alignment heuristic for determining C is sparsely detailed, potentially leading to inconsistent group counts across datasets
- The method assumes linear feature correlations via self-tuning kernel, which may fail on datasets with strong non-linear dependencies
- The feature graph uses full N samples for initialization while sample graph uses batch B, creating uncertainty about whether feature graph is updated during training

## Confidence
- **High confidence:** TPR/FDR performance on synthetic data (ρ=0.95), as the mechanism and results are clearly specified and reproducible
- **Medium confidence:** The interpretability of discovered groups relies on qualitative visualization not fully reproducible without code
- **Low confidence:** The exact procedure for C selection via Procrustes alignment and the dynamic behavior of feature graph updates during training are insufficiently detailed

## Next Checks
1. **Reproduce synthetic two-moons validation:** Run GroupFS on d=20 synthetic data (ρ=0.95, noise std=0.05, C=12) to confirm TPR=1 and FDR=0, verifying group recovery aligns with Fig. 3
2. **Validate C selection heuristic:** On a real dataset (e.g., Lung500), sweep C from 2 to 50 and plot clustering accuracy vs. C to verify the Procrustes alignment heuristic identifies a local optimum
3. **Inspect qualitative group coherence:** Run GroupFS on NMNIST 3-8, visualize top-3 group pixel masks, and confirm spatial coherence matches the paper's Fig. 4