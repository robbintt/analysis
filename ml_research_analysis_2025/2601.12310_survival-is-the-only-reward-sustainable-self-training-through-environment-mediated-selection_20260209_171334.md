---
ver: rpa2
title: 'Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated
  Selection'
arxiv_id: '2601.12310'
source_url: https://arxiv.org/abs/2601.12310
tags:
- miri
- strategies
- katalin
- environment
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-training architecture that uses environmental
  viability as the sole selection criterion, eliminating reward functions and proxy
  objectives. The method executes candidate behaviors under real resource constraints,
  retaining only those that persist and enable future interaction.
---

# Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection

## Quick Facts
- arXiv ID: 2601.12310
- Source URL: https://arxiv.org/abs/2601.12310
- Reference count: 0
- Sustainable self-training system using environment viability as sole selection criterion, eliminating reward functions and preventing reward hacking

## Executive Summary
This paper introduces a self-training architecture that uses environmental viability as the sole selection criterion, eliminating reward functions and proxy objectives. The method executes candidate behaviors under real resource constraints, retaining only those that persist and enable future interaction. This prevents reward hacking and semantic drift by making manipulation of selection criteria self-defeating. The system demonstrates sustainable improvement through negative-space learning, where agents refine behavior by eliminating ineffective strategies rather than accumulating new ones.

## Method Summary
The approach trains agents to free storage space in procedurally generated containerized Linux environments. Instead of reward functions, selection operates through differential survival of behaviors based on environmental impact (storage space acquired). Agents execute behaviors under realistic constraints, and only trajectories producing measurable resource gains are retained for training. The system uses incremental supervised fine-tuning with LoRA adapters, where each new LoRA is applied to the base model rather than stacked, preventing catastrophic forgetting. Three training regimes were tested: full dataset retention (Terese), sliding-window retention of three most recent datasets (Miri), and selection of top-performing datasets (Katalin).

## Key Results
- Miri regime achieved over 50% of Terese's performance while using only 20% of the data, proving bounded memory doesn't prevent improvement
- Models developed meta-learning strategies like deliberate experimental failure to elicit informative error messages without explicit instruction
- The approach generalizes across operating systems and achieves continued learning without catastrophic forgetting or complex reward shaping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Environment-mediated selection prevents reward hacking by making the selection criterion inseparable from survival
- Mechanism: Selection operates through differential persistence of behaviors that produce measurable resource gains (storage capacity). The environment provides no semantic feedback—only hard constraints determine whether a behavior propagates. Strategies that exploit proxy signals without contributing to actual persistence are self-extinguishing because they consume the resource upon which future optimization depends
- Core assumption: The selection signal (storage space) is genuinely conserved and cannot be artificially inflated through manipulation
- Evidence anchors: [abstract] "selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable"; [section 3] "Collapsing these elements into a unique function removes an entire class of failure modes common to proxy-driven systems"
- Break condition: If the resource constraint can be circumvented (e.g., through external injection of storage/compute), selection pressure collapses and reward hacking becomes viable again

### Mechanism 2
- Claim: Negative-space learning enables cumulative improvement through strategic pruning rather than accumulation
- Mechanism: Behaviors that fail under execution constraints are excluded from the training distribution. Over iterations, the retained dataset reflects increasingly precise adaptation to environmental affordances. Strategy diversity (D_t) decreases over time despite increasing performance—the policy concentrates probability mass around a small number of reliable strategies
- Core assumption: Effective strategies are repeatedly instantiated across generations, allowing temporal locality to serve as a sufficient preservation mechanism
- Evidence anchors: [abstract] "improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning"; [section 7.1] "Miri v13 to achieve over 50% of the performance of Terese v13, despite the latter having been trained on four times as much data"
- Break condition: If effective strategies are not reused across iterations (low temporal correlation), sliding-window training will discard useful behaviors before they consolidate

### Mechanism 3
- Claim: Meta-learning emerges instrumentally when debug cycles provide information gain without explicit reward
- Mechanism: Agents discovered that generating plausible-but-imperfect first attempts, then using error messages as feedback, outperforms optimizing for immediate correctness. This trades immediate ΔR for improved future performance through information gain I(y, E_t). Selection pressure over multiple update cycles implicitly favors policies that balance exploitation against maintaining learnability
- Core assumption: The environment provides sufficiently rich error signals that failures are informative rather than merely punitive
- Evidence anchors: [abstract] "models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction"; [section 7.3, Table 9] Terese and Katalin lineages showed pass@1 dropping to 0% by later iterations while code compilation rates increased
- Break condition: If error messages are uninformative or debug attempts are heavily penalized (resource-costly), this strategy becomes maladaptive

## Foundational Learning

- Concept: **Reward hacking / proxy gaming**
  - Why needed here: The entire architecture is designed to eliminate this failure mode; understanding it is prerequisite to appreciating why environment-mediated selection differs from conventional RL
  - Quick check question: Can you explain why optimizing a proxy objective can produce behaviors that score well on the proxy but fail on the true objective?

- Concept: **Temporal generalization vs. spatial generalization**
  - Why needed here: The paper distinguishes between generalizing across test splits (spatial) and generalizing across environmental states that evolve due to agent actions (temporal/longitudinal)
  - Quick check question: Why might a strategy that performs well at time t become ineffective at time t+1 in a non-stationary environment?

- Concept: **LoRA chaining for incremental fine-tuning**
  - Why needed here: The training pipeline uses independently trained LoRAs applied to the base model rather than stacked adapters, preventing catastrophic forgetting while enabling indefinite training
  - Quick check question: Why does applying each new LoRA directly to the base model (rather than stacking) help preserve earlier learning?

## Architecture Onboarding

- Component map: Sandbox environment -> Agent harness -> Resource measurement -> Trajectory filtering -> Dataset composition -> LoRA training -> Model substitution -> Repeat

- Critical path: Environment generation → agent execution → resource measurement → trajectory filtering → dataset composition → LoRA training → model substitution → repeat

- Design tradeoffs:
  - Simple prompt framework sacrifices trajectory diversity for interpretability and reproducibility
  - Batch SFT chosen over continuous SFT/PPO/GRPO for stability (paper reports RL methods underperformed)
  - Uniform credit assignment across trajectory components (no fine-grained attribution) favors global robustness over precision

- Failure signatures:
  - **Katalin collapse** (v12–v13): Performance-based selection without behavioral continuity causes fragmentation; individually effective strategies lack connective tissue for recombination
  - **Pass@1 degradation**: Expected early signal of meta-learning emergence, not a bug—verify compilation rates are stable or improving
  - **Environmental overfitting**: Strategies that exploit transient conditions will fail when environment regenerates; check cross-environment performance

- First 3 experiments:
  1. **Baseline replication**: Run Terese v1–v3 on identical environment configuration; verify ~0.03 → ~0.13 % space freed progression matches paper (Table 2)
  2. **Ablation on rehearsal data**: Remove the 500-row Stack Exchange rehearsal buffer; test whether general coding skills degrade (HumanEval) or environment-specific performance is unaffected
  3. **Window size sensitivity**: Test Miri-style training with windows of 2, 5, and 10 iterations; characterize the performance/memory trade-off curve and identify where improvement plateaus or degrades

## Open Questions the Paper Calls Out

- **Open Question 1**: Will continuous SFT with gradient updates every 100 iterations produce stronger explore-exploit meta-strategies than the batch fine-tuning regime tested here?
  - Basis in paper: [explicit] Section 9 states the team is developing "Continuous SFT with gradient updates every 100 iterations" and hypothesizes that "selection pressure over multiple update cycles implicitly favours policies that balance immediate exploitation against maintaining learnability"
  - Why unresolved: The current paper uses batch fine-tuning; continuous SFT dynamics remain theoretical
  - What evidence would resolve it: Compare explore-exploit behavior and longitudinal generalization between continuous and batch regimes on identical environments

- **Open Question 2**: What training data selection criteria produce stable long-term learning versus collapse (as observed in Katalin)?
  - Basis in paper: [explicit] Section 7 notes Katalin's "continued degradation" and hypothesizes that selecting datasets "solely on the basis of isolated outcome metrics" causes "fragmentation of the model's internal strategy space"
  - Why unresolved: The paper contrasts Miri (stable) and Katalin (unstable) regimes but does not systematically characterize the boundary conditions
  - What evidence would resolve it: Ablation studies varying selection criteria (recency-weighted, performance-weighted, diversity-weighted) to identify stability thresholds

- **Open Question 3**: Does negative-space learning scale to domains beyond filesystem manipulation (e.g., robotics, multi-agent environments)?
  - Basis in paper: [inferred] The paper acknowledges using a "deliberately simple" environment and prompt structure for "experimental clarity," and notes "other rewards (notably financial) are possible" but require "a more complex harness"
  - Why unresolved: The proof-of-concept only tests Linux container environments with storage as the sole conserved resource
  - What evidence would resolve it: Replicate the architecture in qualitatively different domains with different resource constraints and measure whether NSL dynamics emerge

- **Open Question 4**: Can the emergence of instrumental strategies (e.g., deliberate failure for information gain) be predicted or controlled?
  - Basis in paper: [explicit] Section 7.3.1 notes: "The emergence of such strategies under our selection regime is notable" and "agents are not explicitly rewarded for exploration or information gain, yet they converge toward instrumentally rational behavior"
  - Why unresolved: The paper observes but does not explain the conditions under which such meta-strategies arise
  - What evidence would resolve it: Systematic variation of feedback richness and error message informativeness to identify necessary conditions for instrumental strategy emergence

## Limitations
- Unknown replication factors: Exact prompt templates and sandbox environment specifications are not fully detailed, requiring reconstruction from external sources
- Generalization boundaries: Unclear how well strategies transfer to different operating systems or application domains beyond Linux containers
- Scale limitations: The approach uses a 7B parameter model, but effectiveness at larger scales remains untested

## Confidence
- **High confidence**: The core claim that environment-mediated selection prevents reward hacking is well-supported by the architectural design and empirical evidence showing consistent improvement across training regimes
- **Medium confidence**: The claim about negative-space learning driving cumulative improvement has theoretical justification and some empirical support, but evidence is indirect
- **Low confidence**: The emergence of meta-learning strategies like deliberate failure is the weakest claim, requiring more direct behavioral evidence

## Next Checks
1. **Test Katalin-style selection with behavioral continuity constraints**: Implement a variant where top-performing strategies must be behaviorally similar to retained strategies from previous iterations. Measure whether this prevents the v12-v13 collapse while maintaining performance benefits

2. **Cross-domain transferability experiment**: Apply the trained models to a fundamentally different environment (e.g., network configuration tasks, database optimization, or non-computing domains like resource allocation in robotics). Measure whether the environment-mediated selection approach generalizes beyond its training domain

3. **Scale sensitivity analysis**: Replicate the experiment with a 3B and 13B parameter model using identical training regimes. Characterize how the effectiveness of environment-mediated selection varies with model scale and whether larger models exhibit different failure modes or meta-learning patterns