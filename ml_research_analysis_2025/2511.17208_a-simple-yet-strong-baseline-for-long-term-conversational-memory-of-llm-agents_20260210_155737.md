---
ver: rpa2
title: A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents
arxiv_id: '2511.17208'
source_url: https://arxiv.org/abs/2511.17208
tags:
- edus
- memory
- graph
- retrieval
- emem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term conversational
  memory in LLM agents, where maintaining coherence and personalization over many
  sessions is difficult due to context window limitations and trade-offs between coarse
  and fragmented retrieval. The authors propose an event-centric memory approach using
  enriched elementary discourse units (EDUs) that bundle participants, temporal cues,
  and minimal context, avoiding lossy compression of dialogue history.
---

# A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents

## Quick Facts
- arXiv ID: 2511.17208
- Source URL: https://arxiv.org/abs/2511.17208
- Reference count: 14
- Key result: Event-centric EDU memory with LLM filtering achieves 0.780 QA accuracy on LoCoMo, matching or surpassing strong baselines while using shorter contexts

## Executive Summary
This paper addresses the challenge of maintaining long-term conversational memory for LLM agents across many sessions. The core insight is that traditional approaches—either lossy summaries or fragmented triple-based representations—fail to preserve the coherence needed for complex multi-hop reasoning. The authors propose an event-centric memory approach using enriched elementary discourse units (EDUs) that bundle participants, temporal cues, and minimal context into self-contained propositions. They organize sessions, EDUs, and arguments into a heterogeneous graph to support associative recall, with two retrieval variants: EMem (lightweight dense retrieval + LLM filtering) and EMem-G (adding graph-based propagation). Experiments on LoCoMo and LongMemEval_S benchmarks show these methods match or surpass strong baselines like Nemori and Mem0 while using much shorter QA contexts.

## Method Summary
The method extracts enriched EDUs from conversations using an LLM with 1-shot prompting, then extracts event arguments from each EDU. Sessions, EDUs, and arguments are organized into a heterogeneous graph with synonym edges. For retrieval, EMem uses top-30 EDU retrieval via dense embeddings followed by LLM filtering and QA. EMem-G adds argument retrieval (top-10 per mention, max 30) and Personalized PageRank propagation from query-specific seeds before selecting top-10 EDUs for QA. The approach biases toward recall in filtering to compensate for embedding brittleness with generic mentions.

## Key Results
- EMem achieves 0.780 QA accuracy on LoCoMo, comparable to stronger baselines
- EMem-G improves multi-hop performance from 0.702 to 0.747 on LoCoMo
- Both variants use significantly shorter contexts than competitors while maintaining accuracy
- Graph-based propagation helps temporal reasoning (69.8%→74.8% on LongMemEval_S)

## Why This Works (Mechanism)

### Mechanism 1: Event-Centric EDU Representation Preserves Discourse Coherence
Bundling event participants, temporal cues, and minimal context into self-contained EDUs reduces fragmentation loss compared to triple-based or chunk-based memory. Each EDU captures a complete "who-what-where-when-why" proposition without requiring recombination at retrieval time.

### Mechanism 2: Recall-Oriented LLM Filtering Compensates for Embedding Brittleness
A lightweight LLM filter biased toward recall improves retrieval quality when queries use generic mentions ("my pet", "that trip") or when dense similarity alone is noisy. After dense retrieval of top-K candidates, LLM selects relevant EDUs/arguments before QA.

### Mechanism 3: Graph Propagation Captures Indirect Associations Across Sessions
Personalized PageRank over the EDU-argument graph helps retrieve indirectly connected evidence for multi-hop and temporal reasoning queries. Seed weights from filtered nodes initialize PPR, which propagates relevance to structurally related nodes.

## Foundational Learning

- **Neo-Davidsonian Event Semantics**: Motivates treating events as first-class units with multiple arguments rather than decomposed binary relations. *Quick check: Can you explain why "Bob attended the Tokyo conference in March" is better stored as one event with arguments than as 4+ separate triples?*

- **Personalized PageRank (PPR)**: Core retrieval mechanism for EMem-G; propagates relevance from seed nodes through graph structure. *Quick check: Given seed weights on 3 EDU nodes, how would PPR with damping factor α=0.5 prioritize other nodes after one iteration?*

- **Dense Retrieval and Embedding Similarity**: Initial candidate retrieval before LLM filtering; relies on cosine similarity between query and EDU embeddings. *Quick check: Why might dense retrieval fail when a query mentions "my pet" but EDUs use the specific pet name?*

## Architecture Onboarding

- **Component map:** Conversations → [EDU Extractor LLM] → Enriched EDUs → [Argument Extractor LLM] → Event Arguments → [Graph Builder] → Heterogeneous Memory Graph
- **Critical path:** EDU extraction quality → argument normalization → graph connectivity → seed initialization accuracy → PPR propagation quality → final EDU selection. Ablations show EDU filter is most critical (removing it causes largest performance drop).
- **Design tradeoffs:** EMem vs EMem-G: EMem is simpler (no graph/PPR), 10-20% faster, competitive on single-hop/knowledge-update; EMem-G adds 3-5% accuracy on multi-hop/temporal-reasoning at cost of graph maintenance. Recall vs precision in filtering: Paper biases toward recall; precision-oriented filters may drop borderline-but-relevant EDUs. EDU granularity: Too fine → fragmentation; too coarse → noise. Paper uses ~15-23 words per EDU on average.
- **Failure signatures:** Single-session-preference questions: 32.2% accuracy (vs 46.7% for Nemori) — EDU extraction drops attitudinal/stylistic information. Missing EDUs on long structured assistant responses: requires special chunk handling with summaries. Graph too sparse: synonym edges only ~100-200 per conversation; PPR may not find indirect paths. Filter overwhelmed: if linking Top-K > 40, performance plateaus or degrades slightly.
- **First 3 experiments:** 1) EDU extraction quality check: Manually inspect 10 sessions → verify EDUs capture complete events with normalized entities; measure average EDU length and source turn coverage. 2) Retrieval ablation (EMem baseline): Compare dense-only vs dense+LLM filter on 50 queries; measure recall@10 and filter rejection rate. 3) Graph connectivity analysis: For 5 conversations, compute node degree distributions, synonym edge density, and PPR spread from synthetic seeds; identify if graph is too sparse for effective propagation.

## Open Questions the Paper Calls Out
1. How can the event-centric memory framework be augmented to better capture and retrieve attitudinal or stylistic user information that is currently lost during EDU extraction?
2. Can the event-centric EDU and graph structure be effectively applied to non-conversational, long-horizon tasks such as tool-augmented agents or multi-document reasoning?
3. Would enhancing the normalization and atomicity of event arguments to increase graph density significantly improve the retrieval performance of the graph-based variant (EMem-G)?

## Limitations
- EDU extraction process quality is not quantitatively validated; performance drops on preference-based questions (32.2% vs 46.7% for Nemori) reveal fundamental limitations in capturing stylistic information
- Graph sparsity issue acknowledged but not quantitatively measured across conversations; PPR hyperparameters are referenced but not specified
- Performance relies heavily on LLM quality for both extraction and filtering steps; generalizability beyond the two specific benchmarks is uncertain

## Confidence
- **High confidence**: Event-centric EDU representation as a principled approach; EMem baseline effectiveness (solid improvement over dense-only retrieval)
- **Medium confidence**: EMem-G improvements for multi-hop and temporal reasoning (results are positive but graph analysis is limited)
- **Low confidence**: Generalizability beyond the two specific benchmarks; EDU extraction quality for complex conversational phenomena

## Next Checks
1. **EDU extraction quality audit**: Manually annotate 20 random EDUs from different conversation types to measure information retention rate and entity normalization accuracy
2. **Graph connectivity analysis**: For 10 conversations, measure synonym edge density, average path lengths between relevant EDUs, and PPR propagation effectiveness on synthetic queries
3. **Recall-precision tradeoff study**: Systematically vary the LLM filter's bias between recall and precision, measuring impact on different query types (single-hop vs multi-hop vs temporal reasoning)