---
ver: rpa2
title: Feedback-Induced Performance Decline in LLM-Based Decision-Making
arxiv_id: '2507.14906'
source_url: https://arxiv.org/abs/2507.14906
tags:
- feedback
- agent
- policy
- reward
- hwbp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs were evaluated as autonomous agents in Markov Decision Processes
  using zero-shot structured prompting. Performance was compared to PPO-based reinforcement
  learning across three MiniGrid configurations of increasing complexity.
---

# Feedback-Induced Performance Decline in LLM-Based Decision-Making

## Quick Facts
- **arXiv ID**: 2507.14906
- **Source URL**: https://arxiv.org/abs/2507.14906
- **Reference count**: 40
- **One-line result**: LLMs show degraded performance in sequential decision tasks when given additional feedback, unlike PPO baselines.

## Executive Summary
This paper evaluates large language models (LLMs) as autonomous agents in Markov Decision Processes using zero-shot structured prompting. Across three MiniGrid configurations of increasing complexity, LLMs achieved initial success in simpler environments but showed degraded performance in complex scenarios, especially when feedback mechanisms were added. Notably, additional feedback (state transitions, rewards, policy traces) often led to diminished decision-making effectiveness, suggesting that extraneous context misallocates the model's attention. PPO achieved near-perfect performance in all configurations, while LLM policies, including those with chain-of-thought and reasoning models, struggled with planning and reasoning without fine-tuning.

## Method Summary
The study compared LLMs (Llama 3.1 8B and Qwen 2.5 1.5B via Ollama API) to PPO-based reinforcement learning across three MiniGrid configurations. LLMs received state information via structured text prompts with incremental feedback strategies (State-only → +CoT → +Dynamics → +Rewards → +Policy Summary). PPO used a custom CNN feature extractor with 16→32→64 filters, trained for 30k to 3M steps depending on configuration complexity. Performance was measured by cumulative reward per episode and success rate across 100 episodes per configuration.

## Key Results
- LLMs achieved 98% success in the simplest 5x5 empty grid but dropped to 42% in the 9x9 crossing configuration
- PPO baselines achieved near-perfect performance (99.5-100%) across all configurations
- Adding feedback mechanisms (state transitions, rewards, policy traces) consistently degraded LLM performance rather than improving it
- Policy feedback loops caused complete performance collapse to 0% success in some cases

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Induced Attention Dilution
Providing additional contextual feedback to LLMs degrades performance by misallocating the model's limited attention window. Instead of focusing on optimal policy derivation, the model attends to historical noise or irrelevant state-transition details, treating all context as equally relevant without an internal filtering mechanism.

### Mechanism 2: Approximate Retrieval vs. Valid Planning
LLMs fail to act as valid planners in sequential decision processes because they rely on approximate memory retrieval rather than logical state simulation. Pre-trained statistical correlations between text sequences do not map to the rigid transition dynamics required in Markov Decision Processes.

### Mechanism 3: Policy Feedback Loop Collapse
Self-referential policy feedback leads to total performance collapse when the LLM generates a "policy summary" based on potentially sub-optimal or failed trajectories. Feeding this summary back creates a negative feedback loop where the model reinforces poor strategies.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - **Why needed here:** The paper frames LLMs as "policies" within an MDP. You must understand that an MDP requires decisions based *only* on the current state to maximize cumulative reward.
  - **Quick check question:** Why does adding "memory" (history of states) potentially violate the Markov property assumption, and why did the authors test it anyway?

- **Concept: Zero-Shot Prompting**
  - **Why needed here:** The study evaluates "zero-shot" performance, meaning the models solve tasks without prior training on that specific environment.
  - **Quick check question:** What is the fundamental difference between how a PPO agent "learns" a policy versus how a zero-shot LLM "selects" an action?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** The study uses CoT as a variable (`HWBP + CoT`). Understanding that CoT forces the model to emit intermediate reasoning steps is crucial, as the paper finds it does not salvage performance in complex environments.
  - **Quick check question:** Did Chain-of-Thought prompting significantly improve success rates in the 9x9 grid with internal walls (Configuration 3)?

## Architecture Onboarding

- **Component map:** Environment (MiniGrid) -> Encoder -> Prompt Composer -> Agent (LLM) -> Wrapper
- **Critical path:** The translation of spatial grid state → text prompt → LLM inference → executable action. The study shows this path is brittle; failures in the LLM's reasoning logic or JSON formatting break the loop.
- **Design tradeoffs:**
  - PPO (RL) vs. LLM: PPO offers near-perfect performance but requires millions of training steps (3M for Config 3). LLMs require zero training but fail to generalize to complex layouts.
  - Feedback Richness: Adding feedback (DF, RF) *should* help theoretically, but practically decreases accuracy due to attention limits.
- **Failure signatures:**
  - Attention Dilution: Success rate drops as prompt token count increases (adding feedback)
  - Hallucinated Plans: Reasoning models outputting logical paths that physically collide with walls
  - Looping: Agents repeating actions without forward progress due to confusing state descriptions
- **First 3 experiments:**
  1. Run the `PromptComposer` with "State-Only" configuration on Config 1 (5x5) to verify the LLM can handle the simplest MDP
  2. Incrementally add `Dynamics Feedback` and `Reward Feedback` to the prompt and measure the drop in Success Rate
  3. Execute a standard PPO training run on Config 2 (16x16) to establish the "perfect" baseline curve vs. the flat/degrading LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid strategies integrating external verifiers or symbolic planners mitigate the feedback-induced performance decline observed in pure LLM agents?
- **Basis in paper:** The conclusion explicitly states the results "point to the need for hybrid strategies" and the introduction notes that external verifiers are often required, though this study deliberately excluded them.
- **Why unresolved:** The study isolated intrinsic LLM capabilities by excluding external tools to assess raw planning ability, which resulted in failure.
- **What evidence would resolve it:** Experiments combining LLMs with external search algorithms (e.g., MCTS) or solvers in the complex MiniGrid configurations to see if performance stabilizes.

### Open Question 2
Does domain-specific fine-tuning enable LLMs to effectively utilize state and reward feedback without the attention misallocation observed in zero-shot prompting?
- **Basis in paper:** The abstract and conclusion underscore the "need for... fine-tuning" because zero-shot models struggled to integrate feedback without performance degradation.
- **Why unresolved:** The experiments relied exclusively on zero-shot structured prompting, leaving the potential of weight updates unexplored.
- **What evidence would resolve it:** A comparison of fine-tuned LLM policies against the zero-shot baselines used in the paper, specifically analyzing their attention patterns when processing reward feedback.

### Open Question 3
What specific memory architectures are required to prevent "policy feedback" from degrading decision-making capabilities in sequential tasks?
- **Basis in paper:** The authors call for "advanced memory integration" after finding that naive policy feedback (summarizing previous strategies) often caused performance to drop below random baselines.
- **Why unresolved:** The paper tested simple context appending for memory, which resulted in confusion and attention dilution rather than improved reasoning.
- **What evidence would resolve it:** Evaluation of LLMs equipped with external memory modules (e.g., RAG or vector databases) to manage policy history, compared to the standard context window approach.

## Limitations

- The central claim rests on zero-shot prompting without fine-tuning, which may not represent practical LLM capabilities in sequential decision tasks
- The "attention dilution" mechanism assumes attention is a fixed resource, but modern LLMs with context-aware mechanisms might handle extended contexts differently
- The absolute claim that policy feedback loops cause complete collapse to 0% success may be overly sensitive to specific prompt engineering choices

## Confidence

- **High Confidence:** The observation that LLMs underperform PPO in complex MiniGrid configurations is well-supported by empirical results across multiple runs and models
- **Medium Confidence:** The feedback-induced performance decline mechanism is plausible but requires careful interpretation—the degradation could stem from prompt engineering limitations rather than fundamental LLM architectural constraints
- **Low Confidence:** The absolute claim that policy feedback loops cause complete collapse to 0% success may be overly sensitive to implementation details of the policy summary generation

## Next Checks

1. **Sensitivity Analysis:** Replicate the feedback experiments with varied temperature parameters (0.1, 0.7, 1.0) to determine if performance degradation is consistent across stochasticity levels or specific to certain sampling strategies
2. **External Verifier Test:** Implement the suggested "Teacher model" approach where an external verifier filters policy summaries before they reach the LLM, measuring whether this breaks the negative feedback loop
3. **Fine-tuned Baseline Comparison:** Train a few-shot version of the same LLM on a small number of demonstrations in Config 1, then test whether feedback mechanisms still cause degradation, establishing whether the effect is specific to zero-shot settings