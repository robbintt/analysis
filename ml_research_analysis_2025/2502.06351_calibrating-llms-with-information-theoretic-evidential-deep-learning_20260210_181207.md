---
ver: rpa2
title: Calibrating LLMs with Information-Theoretic Evidential Deep Learning
arxiv_id: '2502.06351'
source_url: https://arxiv.org/abs/2502.06351
tags:
- ib-edl
- conference
- logp
- information
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces IB-EDL, an information-theoretic framework\
  \ that regularizes evidential deep learning (EDL) to improve calibration in fine-tuned\
  \ large language models (LLMs). IB-EDL uses an information bottleneck to suppress\
  \ spurious evidence generation while preserving predictive information, addressing\
  \ EDL\u2019s tendency to produce over-confident predictions."
---

# Calibrating LLMs with Information-Theoretic Evidential Deep Learning

## Quick Facts
- arXiv ID: 2502.06351
- Source URL: https://arxiv.org/abs/2502.06351
- Reference count: 30
- This paper introduces IB-EDL, an information-theoretic framework that regularizes evidential deep learning (EDL) to improve calibration in fine-tuned large language models (LLMs).

## Executive Summary
This paper introduces IB-EDL, an information-theoretic framework that regularizes evidential deep learning (EDL) to improve calibration in fine-tuned large language models (LLMs). IB-EDL uses an information bottleneck to suppress spurious evidence generation while preserving predictive information, addressing EDL's tendency to produce over-confident predictions. By incorporating an ℓ2 regularization on the model's output evidence, IB-EDL effectively mitigates overly concentrated Dirichlet distributions. Extensive experiments on multiple LLMs (Llama2-7B, Llama3-8B, Mistral-7B) and tasks show that IB-EDL significantly reduces Expected Calibration Error (ECE) and Negative Log-Likelihood (NLL) compared to standard EDL and non-EDL baselines, while maintaining or improving accuracy. It also excels in out-of-distribution detection and robustness to noisy labels, with less than 2% computational overhead.

## Method Summary
IB-EDL applies evidential deep learning to LLM fine-tuning by outputting Gaussian parameters (μ, σ) for pre-evidence logits. During training, K=20 samples are drawn via reparameterization, transformed through SoftPlus+1 to obtain Dirichlet parameters α, and used to compute an analytic MSE loss. An information bottleneck term regularizes the Gaussian prior on pre-evidence, effectively applying ℓ2 regularization to prevent overly concentrated Dirichlet distributions. LoRA adapters (r=8, α=16) are applied to q_proj, v_proj, and lm_head modules. The total loss combines IB-MSE with β-weighted IB-Info, where β ranges from 5×10⁻⁷ to 2×10⁻³ depending on dataset. Training uses learning rate 5e-5 with cosine schedule and gradient clipping at norm 20.

## Key Results
- IB-EDL significantly reduces Expected Calibration Error (ECE) and Negative Log-Likelihood (NLL) compared to standard EDL and non-EDL baselines across multiple LLMs and tasks
- Maintains or improves accuracy while providing better uncertainty estimates and out-of-distribution detection capabilities
- Achieves calibration improvements with less than 2% computational overhead during inference
- Demonstrates robustness to noisy labels with less than 2% computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Placing the information bottleneck on pre-evidence (rather than internal layers) ensures a valid lower bound on I(Z,Y) during training.
- Mechanism: By selecting Z = ẽ (pre-evidence), the EDL pipeline from ẽ → α → π → y is fully tractable, allowing direct evaluation of p(y|z) without learning an approximating decoder q(y|z). This avoids the theoretical gap where term (ii) in Equation 11 can be negative, which undermines lower-bound guarantees when IB is applied at internal layers with learned decoders.
- Core assumption: The Markov chain X → Z → Y holds; pre-evidence captures sufficient information for downstream Dirichlet parameterization.
- Evidence anchors:
  - [section 3.2]: "By choosing z = ẽ, we skip the step of learning an approximated distribution q(y|z) and ensure that we are maximizing a valid lower bound of I(Z,Y)."
  - [section 3.1]: Term (ii) analysis showing the gap is not necessarily non-negative.
  - [corpus]: Weak direct evidence; corpus neighbors focus on EDL calibration improvements but do not address IB placement choices.
- Break condition: If the task requires hidden representations earlier in the network (e.g., for multi-task learning), the tractability advantage may not apply.

### Mechanism 2
- Claim: The Gaussian prior on pre-evidence induces implicit ℓ2 regularization on evidence magnitudes, preventing over-concentrated Dirichlet distributions.
- Mechanism: Modeling p(z|x) as N(z; μ, diag(σ)) with standard Gaussian prior r(z) yields the regularization term in Equation 14, which penalizes ||μ||². Since μ is the mean of ẽ and α = SoftPlus(ẽ) + 1, this constrains evidence magnitudes, directly addressing EDL's tendency to produce extreme α values that cause overconfidence.
- Core assumption: Pre-evidence distributions are approximately Gaussian; diagonal covariance assumption (uncorrelated pre-evidences) is reasonable.
- Evidence anchors:
  - [section 3.2]: "Eq. (14) imposes a ℓ2-regularization on μ, i.e. the mean of ẽ, and thus on α. Therefore, IB-EDL penalizes the LLM for generating large α."
  - [abstract]: References suppressing spurious evidence while preserving predictive information.
  - [corpus]: Related work on EDL overconfidence (e.g., R-EDL, I-EDL) addresses similar issues through different mechanisms.
- Break condition: If the Gaussian assumption is severely violated (e.g., multi-modal evidence distributions), the regularization may not properly calibrate uncertainty.

### Mechanism 3
- Claim: Monte Carlo sampling of pre-evidences with reparameterization enables gradient-based training while propagating uncertainty estimates.
- Mechanism: Drawing K samples from N(ẽ; μ, diag(σ)) and averaging the loss (LIB-MSE) provides unbiased gradient estimates. The reparameterization trick makes sampling differentiable, allowing end-to-end training of LLM parameters including LoRA adapters.
- Core assumption: K samples sufficiently approximate expectations; SoftPlus transformation preserves gradient flow.
- Evidence anchors:
  - [section 3.3]: Describes reparameterization trick application and K=20 default sampling.
  - [section 4.5]: Ablation showing K variation effects; larger K slightly improves accuracy but not necessarily ECE/NLL.
  - [corpus]: No direct corpus evidence on this specific sampling approach.
- Break condition: If computational budget prevents K ≥ 10-20 samples, variance in gradient estimates may destabilize training.

## Foundational Learning

- Concept: **Evidential Deep Learning (EDL) fundamentals**
  - Why needed here: IB-EDL builds directly on EDL's Dirichlet parameterization; understanding evidence → Dirichlet → uncertainty mass is prerequisite.
  - Quick check question: Given evidence vector e = [2, 1, 0] for 3 classes, compute the Dirichlet parameters α, expected probabilities π̂, and uncertainty mass u.

- Concept: **Variational Information Bottleneck**
  - Why needed here: The paper's core contribution reformulates EDL training as IB optimization; understanding mutual information bounds is essential.
  - Quick check question: Explain why maximizing I(Z,Y) while minimizing I(Z,X) encourages representations that are predictive but compressed.

- Concept: **Dirichlet distribution properties**
  - Why needed here: Calibration depends on preventing over-concentrated Dirichlet distributions; intuition about α concentration vs. uncertainty is critical.
  - Quick check question: How does increasing all α values proportionally affect the variance of π sampled from Dir(α)? What happens as α₀ → ∞?

## Architecture Onboarding

- Component map: Pre-trained LLM -> LoRA adapters -> Dual prediction heads (μ, σ) -> Pre-evidence sampling -> EDL transformation -> Dirichlet parameters -> Uncertainty-aware predictions

- Critical path:
  1. Forward pass through LLM + LoRA → hidden states
  2. Dual heads produce μ, σ for pre-evidence distribution
  3. Sample K pre-evidences via reparameterization: ẽ^(k) = μ + σ ⊙ ε, ε ~ N(0, I)
  4. For each sample: compute α^(k), evaluate LIB-MSE loss
  5. Average losses across K samples, add β × LIB-Info
  6. Backpropagate through reparameterization to update LoRA weights

- Design tradeoffs:
  - **β selection**: Larger β increases regularization strength (better ECE, potential accuracy drop); paper uses 5×10⁻⁷ to 2×10⁻³ depending on dataset
  - **K samples**: Higher K improves gradient estimates with minimal inference overhead (~2%); diminishing returns above K=20
  - **Diagonal covariance assumption**: Simplifies computation but ignores evidence correlations; paper notes this as a limitation

- Failure signatures:
  - **Exploding evidence magnitudes**: Check ||μ||²; if growing unbounded despite regularization, β may be too small
  - **Underconfident predictions**: ECE improves but accuracy drops significantly; β may be too large
  - **Training instability**: Gradient clipping (max norm 20) is used for EDL methods; verify this is active
  - **OOD detection failure**: If UM score performs poorly vs MP, variance head h_σ may not be learning meaningful uncertainty

- First 3 experiments:
  1. **Baseline calibration check**: Fine-tune with MAP only, measure ECE and NLL on validation set to establish overconfidence severity before applying IB-EDL.
  2. **β sensitivity sweep**: On a single dataset (e.g., ARC-C), test β ∈ {10⁻⁶, 10⁻⁵, 10⁻⁴, 10⁻³} with fixed K=20, plot ECE vs. accuracy tradeoff curve.
  3. **OOD detection validation**: Train on OBQA, evaluate AUROC using both MP and UM scores on ARC-C; compare against vanilla EDL to verify IB-EDL's improvement in distinguishing ID vs. OOD samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can IB-EDL be effectively extended to generative tasks such as text generation, where uncertainty estimation metrics remain an open research challenge?
- Basis in paper: [explicit] The authors state in Section 6: "In future work, it would be interesting to test IB-EDL on generative tasks. A great challenge is that uncertainty estimation metrics for generative tasks are still an ongoing research topic."
- Why unresolved: Current evaluation relies on classification metrics (ECE, NLL) that do not transfer to open-ended text generation, and token-level Dirichlet distributions may not capture sequence-level uncertainty.
- What evidence would resolve it: Demonstrating IB-EDL on generative tasks (e.g., summarization, dialogue) with appropriate uncertainty metrics, showing calibration improvements comparable to those achieved in classification.

### Open Question 2
- Question: What are the theoretical and empirical implications of relaxing the diagonal covariance assumption for the pre-evidence distribution in IB-EDL?
- Basis in paper: [explicit] The authors state in Section 6: "To reduce the complexity of covariance matrix prediction, we assume the pre-evidences are uncorrelated, but this assumption can be relaxed."
- Why unresolved: Modeling full covariance would capture correlations between evidence for different tokens but increases computational complexity from O(C) to O(C²), and it is unclear whether this improves calibration or OOD detection.
- What evidence would resolve it: Comparing IB-EDL with diagonal vs. full covariance matrices on calibration and OOD detection tasks, analyzing any performance gains relative to computational overhead.

### Open Question 3
- Question: Can a principled, adaptive method be developed for selecting the regularization weight β without requiring per-dataset grid search?
- Basis in paper: [inferred] The ablation study (Section 4.5, Figure 1) shows β controls the calibration-accuracy trade-off, and Table 5 reveals β values spanning several orders of magnitude across datasets (10⁻⁷ to 10⁻³). The authors rely on grid search without theoretical guidance.
- Why unresolved: The optimal β depends on dataset characteristics (size, noise level) and model properties, but no automated selection mechanism exists.
- What evidence would resolve it: Developing an adaptive β selection method (e.g., based on mutual information estimates or validation performance) that matches or exceeds grid-search performance across diverse datasets.

### Open Question 4
- Question: Does IB-EDL maintain its calibration advantages when scaled to significantly larger LLMs (e.g., 70B+ parameters)?
- Basis in paper: [inferred] Experiments are limited to Llama2-7B, Llama3-8B, and Mistral-7B. The authors claim IB-EDL "scales effectively" but do not test on larger models where overconfidence patterns and the ℓ₂ regularization dynamics may differ.
- Why unresolved: Larger models may exhibit different calibration properties after fine-tuning, and the relative overhead of sampling K pre-evidences may change with model scale.
- What evidence would resolve it: Evaluating IB-EDL on larger models (e.g., Llama2-70B, Llama3-70B) across the same calibration benchmarks, comparing ECE/NLL reductions and computational overhead percentages.

## Limitations

- The paper relies on a diagonal covariance assumption for pre-evidence distributions, which may not capture correlations between evidence for different tokens
- Optimal β values show significant variation across datasets (5×10⁻⁷ to 2×10⁻³), suggesting sensitivity to task characteristics and requiring per-dataset grid search
- The method's effectiveness on sequence-level tasks beyond classification and on significantly larger LLMs remains unverified

## Confidence

**High Confidence:** The experimental results showing ECE and NLL improvements are robust, with multiple datasets and LLM variants demonstrating consistent calibration gains. The theoretical foundation for pre-evidence IB placement (Mechanism 1) is well-established through tractable lower bound analysis.

**Medium Confidence:** The ℓ2 regularization mechanism (Mechanism 2) is theoretically sound but depends on the Gaussian assumption holding reasonably well. The sampling-based training approach (Mechanism 3) is practical but may exhibit variance in gradient estimates depending on K selection.

**Low Confidence:** The generalizability of the diagonal covariance assumption across diverse tasks and the method's performance on sequence-level tasks beyond classification remain unverified.

## Next Checks

1. **Gaussian Assumption Testing:** Evaluate IB-EDL performance when pre-evidence distributions exhibit clear non-Gaussian characteristics (e.g., multi-modal evidence patterns). Compare against methods that relax the Gaussian assumption to quantify the impact on calibration quality.

2. **Cross-Task β Transferability:** Systematically test whether β values effective on one task type (e.g., science QA) transfer to others (e.g., commonsense reasoning). Develop a heuristic or automated procedure for initial β selection based on dataset characteristics.

3. **OOD Detection Robustness:** Beyond the reported AUROC improvements, evaluate IB-EDL's OOD detection performance under adversarial conditions where OOD samples are deliberately constructed to be close to in-distribution examples. This tests whether improved calibration translates to practical robustness.