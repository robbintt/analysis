---
ver: rpa2
title: 'Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments'
arxiv_id: '2502.06551'
source_url: https://arxiv.org/abs/2502.06551
tags:
- sentences
- information
- text
- language
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficiently classifying scientific
  full texts, focusing on the task of assessing the impact of invasive species using
  the EICAT classification standard. The authors propose a pipeline that uses sentence
  selection strategies to reduce input size and improve classification performance
  for both BERT-based models and local large language models.
---

# Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments

## Quick Facts
- arXiv ID: 2502.06551
- Source URL: https://arxiv.org/abs/2502.06551
- Authors: Marc Felix Brinner; Sina Zarrieß
- Reference count: 8
- Primary result: Sentence selection strategies improve EICAT impact classification, with evidence annotations best for BERT (0.523 F1) and importance-based selection best for LLMs (0.403 F1)

## Executive Summary
This paper addresses the challenge of efficiently classifying scientific full texts for EICAT impact assessments of invasive species. The authors propose a two-stage pipeline using sentence selection to reduce input size and improve classification performance for both BERT-based models and local large language models. They compile a novel dataset of 436 scientific papers aligned with EICAT impact assessments and experiment with various sentence selection methods including evidence annotations, LLM-generated annotations, and importance scores derived from classifier introspection. Results demonstrate that sentence selection significantly improves efficiency and performance, with the best approach using evidence annotations for BERT models and importance-based selection for LLMs. The study also finds that repeated random sampling of shorter inputs can further boost performance.

## Method Summary
The authors develop a two-stage pipeline for scientific full-text classification. First, they train a PubMedBERT-based sentence selector to rank sentences by relevance, using evidence annotations or importance scores. Second, they train a classifier (either PubMedBERT or Llama-3.1 8B) on the top-ranked sentences. For BERT classifiers, documents are processed as 512-token chunks with 50-token overlap, while ModernBERT uses full text up to 8192 tokens. For LLMs, category descriptions are included in the prompt. The randomization variant samples from top sentences per epoch and uses majority voting at inference. The dataset consists of 436 full-text papers across 120 species with 2,247 human-annotated evidence sentences, split by species (82% train, 8% val, 10% test).

## Key Results
- Evidence-based sentence selection improves BERT classifier macro F1 from 0.433 to 0.523
- Importance-based selection outperforms evidence selection for LLM classification (0.403 vs 0.349 F1)
- Random sampling with majority voting further improves performance, especially for BERT models
- Human evidence annotations show only moderate agreement (NDCG 0.541), yet still produce best results for BERT
- LLMs systematically over-predict higher impact categories when using evidence-selected sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing full-text inputs to relevant sentence subsets can simultaneously improve classification performance and computational efficiency for encoder-based models.
- Mechanism: Sentence selection filters distracting information (e.g., impacts of non-target species, literature review content) that would otherwise introduce noise during training. The selector focuses the classifier on empirically-relevant sentences, reducing the number of chunks from 15+ to a single coherent input.
- Core assumption: The original annotations in EICAT assessments reliably identify sentences that are causally relevant to the target classification.
- Evidence anchors: "various sources like human evidence annotations... can be used to train sentence selection models that improve the performance of both encoder- and decoder-based language models while optimizing efficiency through the reduction in input length"; "it removes unnecessary and distracting information, most importantly because it can filter out sentences describing impacts caused by other species"; Neighbor paper "Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking" similarly leverages sentence ranking to reduce context.
- Break condition: If evidence annotations are inconsistent across annotators (paper notes NDCG of only 0.541 for human selector alignment), selector quality degrades and performance gains diminish.

### Mechanism 2
- Claim: Importance-based sentence selection (derived from classifier introspection) outperforms evidence-based selection for LLMs, but underperforms for BERT classifiers.
- Mechanism: Importance scores capture which sentences the BERT classifier itself relies on for predictions. For LLMs, this produces sentences aligned with the classification task without introducing the "condensed information" bias that pushes LLMs toward higher-impact categories. For BERT, these selectors inherit the original classifier's inability to filter non-target-species sentences.
- Core assumption: The change in output logits when removing a sentence meaningfully reflects that sentence's contribution to the classification decision.
- Evidence anchors: "best approach using evidence annotations for BERT models and importance-based selection for LLMs"; "sentences identified as important by BERT (i.e., Entropy and Importance) lead to substantially better results than the other strategies" for LLMs; "for the evidence selector, the model's predictions significantly under-represent the lower-impact classes... pushing it to a higher impact category".
- Break condition: If the original classifier has systematic biases, importance-based selection will amplify them.

### Mechanism 3
- Claim: Repeated random sampling of short inputs with majority voting improves classification by increasing input diversity during both training and inference.
- Mechanism: Random sampling exposes the model to varied sentence combinations, forcing it to learn broader relationships rather than narrow patterns tied to specific sentence types. Majority voting reduces variance from any single unlucky sample.
- Core assumption: The signal-to-noise ratio across the document is sufficient that random subsets still contain enough relevant information.
- Evidence anchors: "repeated sampling of shorter inputs proves to be a very effective strategy that, at a slightly increased cost, can further improve classification performance"; "the random selector's lack of bias increases the variance of inputs and forces the model to generalize more effectively through being trained on a more difficult task"; "sampling many different inputs could instead lead to a classification that is based on the general information provided by a vast number of sentences".
- Break condition: If documents contain too few relevant sentences relative to the sample size, random selection will frequently produce uninformative inputs, degrading performance.

## Foundational Learning

- Concept: **Context Window Fragmentation**
  - Why needed here: Standard BERT models (512 tokens) must chunk long documents, diluting signal across 15+ segments. Understanding this explains why sentence selection helps even when the full model (ModernBERT) has access to more context.
  - Quick check question: If you triple the context window of your classifier but performance barely improves (ModernBERT: 0.433 vs PubMedBERT: 0.425), what does this suggest about the bottleneck?

- Concept: **Selector-Classifier Misalignment**
  - Why needed here: The optimal selector depends on the downstream model architecture. Evidence annotations help BERT but hurt Llama. Engineers must test selector-classifier pairs jointly rather than assuming transfer.
  - Quick check question: Why would human-annotated evidence sentences improve a fine-tuned BERT classifier but degrade an instruction-tuned LLM's performance?

- Concept: **Ensemble via Resampling**
  - Why needed here: Randomized selection creates an implicit ensemble without training multiple models. This trades compute for robustness and can make simple baselines competitive with sophisticated selectors.
  - Quick check question: If inference cost doubles but macro F1 improves by 0.05, what factors determine whether this tradeoff is acceptable?

## Architecture Onboarding

- Component map:
  Sentence Selector -> Document Classifier -> Sampling Layer (optional)

- Critical path:
  1. Train selector on labeled evidence (human or synthetic)
  2. Rank all sentences in each document by selector score
  3. Extract top-k sentences (k=15 in paper)
  4. Train classifier on reduced inputs
  5. (Optional) Enable randomization during inference, aggregate 10 samples

- Design tradeoffs:
  - Evidence annotations require domain expert labeling but produce best BERT results (0.523 F1)
  - Importance/entropy selectors require only classifier access but assume original model biases are acceptable
  - Random sampling requires no selector training but needs 10x inference passes for voting
  - LLM-based annotation (using Llama to label sentence utility) is fully automated but shows poor transfer to LLM classification

- Failure signatures:
  - Classifier systematically over-predicts high-impact categories → selector is too aggressive, condensing evidence and removing hedging context (observed with evidence selector + LLM)
  - Random selector outperforms learned selectors → annotations are inconsistent or selector is overfitting to training distribution quirks
  - ModernBERT underperforms chunked BERT → input truncation is losing critical tail content

- First 3 experiments:
  1. **Baseline probe**: Run PubMedBERT on full text (chunked) vs. ModernBERT (8K context) to confirm that context length is not the primary bottleneck. If ModernBERT ≈ chunked BERT, the problem is information density, not context.
  2. **Selector ablation**: Train three selectors (evidence, random, importance) and evaluate classification F1 for each. This isolates whether annotation quality or selection diversity drives improvements.
  3. **Randomization sweep**: With the best selector from experiment 2, vary the number of inference samples (1, 3, 5, 10, 20) and plot F1 vs. latency. Identify the point of diminishing returns for your latency budget.

## Open Questions the Paper Calls Out

- **Question 1**: Can fine-tuning local large language models achieve comparable or superior performance to BERT-based classifiers on scientific full-text classification tasks?
  - Basis in paper: The authors state in the conclusion: "future research could investigate the potential of fine-tuning local LLMs" and note that "sample-level labels, as used by BERT, provide substantially more information than both evidence annotations and natural language descriptions."
  - Why unresolved: The experiments only used prompting with Llama-3.1 8B without task-specific training, leaving unclear whether training could close the gap with BERT models.
  - What evidence would resolve it: Experiments fine-tuning Llama-3.1 8B on the EICAT dataset using sample-level labels, comparing results to the BERT baseline.

- **Question 2**: Can LLMs with advanced reasoning capabilities (e.g., DeepSeek-R1) improve classification performance on scientific full-text tasks without sentence selection?
  - Basis in paper: The conclusion suggests: "leveraging recently emerging LLMs with advanced reasoning capabilities (DeepSeek-AI et al., 2025)" as future research direction.
  - Why unresolved: The study used Llama-3.1 8B without specialized reasoning capabilities, and the authors note that classification scores "remain suboptimal."
  - What evidence would resolve it: Evaluation of reasoning-focused LLMs on the EICAT dataset with the same experimental setup.

- **Question 3**: Why do entropy-based and importance-based sentence selectors show low agreement with human evidence annotations (NDCG ~0.35) yet still improve LLM classification performance more than human-guided selection?
  - Basis in paper: The authors note that BERT-based methods "align only marginally better with the human or LLM annotations than a random selector," yet these methods "lead to substantially better results" for Llama. This paradox remains unexplained.
  - Why unresolved: The paper documents this counterintuitive finding but does not investigate the underlying mechanism.
  - What evidence would resolve it: Analysis of what linguistic or semantic properties the entropy/importance selectors capture that benefit LLM classification, possibly through attention visualization or probing tasks.

## Limitations
- Evidence annotations show only moderate inter-annotator agreement (NDCG 0.541), potentially limiting selector reliability
- LLM improvements via importance-based selection come with architecture-specific limitations and don't generalize to BERT models
- Randomization strategy increases inference costs by 10x for best results, limiting practical applicability for high-throughput scenarios

## Confidence

- **High confidence**: Sentence selection improves BERT classifier performance (0.523 macro F1 vs. 0.433 baseline) when using evidence annotations or importance scores
- **Medium confidence**: LLM performance improvements via importance-based selection are robust (0.403 macro F1), but the mechanism remains incompletely understood
- **Low confidence**: The claimed generalizability of these approaches beyond EICAT classification requires additional validation

## Next Checks

1. **Annotator Agreement Validation**: Conduct inter-annotator agreement studies on a subset of evidence sentences to quantify the true variability in human annotations. Compare selector performance when trained on consensus vs. individual annotator data to determine if annotation quality drives the NDCG ceiling.

2. **Architecture Transfer Experiment**: Test the sentence selection pipeline on a different scientific classification task (e.g., clinical trial outcome prediction or materials science property classification) using the same species+context selector architecture. Measure whether the selector-generalization pattern holds across domains.

3. **Cost-Benefit Analysis at Scale**: Implement the randomization strategy with varying sample sizes (1, 3, 5, 10, 20) on a dataset 10x larger than EICAT. Calculate the F1-latency trade-off curve and determine the inflection point where additional samples no longer justify the computational cost.