---
ver: rpa2
title: 'Are Your Generated Instances Truly Useful? GenBench-MILP: A Benchmark Suite
  for MILP Instance Generation'
arxiv_id: '2505.24779'
source_url: https://arxiv.org/abs/2505.24779
tags:
- milp
- instance
- instances
- gurobi
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenBench-MILP, a comprehensive benchmark
  suite designed to evaluate the quality of generated MILP instances. The framework
  addresses the challenge of assessing instance utility beyond superficial metrics
  by incorporating solver-internal features such as root node gaps, heuristic success
  rates, and cut plane usage.
---

# Are Your Generated Instances Truly Useful? GenBench-MILP: A Benchmark Suite for MILP Instance Generation

## Quick Facts
- arXiv ID: 2505.24779
- Source URL: https://arxiv.org/abs/2505.24779
- Reference count: 40
- Primary result: Introduces GenBench-MILP, a framework that evaluates generated MILP instances by comparing solver-internal behavioral metrics against originals, revealing that high structural similarity doesn't guarantee utility

## Executive Summary
This paper introduces GenBench-MILP, a comprehensive benchmark suite designed to evaluate the quality of generated MILP instances beyond superficial structural similarity metrics. The framework treats the solver as an "expert" system, analyzing how generated instances interact with solver algorithms through internal features like root node gaps, heuristic success rates, and cut plane usage. Experiments with three state-of-the-art generative models (G2MILP, ACM-MILP, DIG-MILP) demonstrate that instances can appear structurally similar to originals while exhibiting drastically different solver behaviors and difficulty levels. The framework proves practical through efficient evaluation under time limits and cross-solver validation, establishing Gurobi as the most stable platform for internal feature analysis.

## Method Summary
GenBench-MILP operates through a two-stage evaluation process. First, it performs solver-independent analysis by extracting 11 graph-based structural features from MILP instances and computing their distributional similarity using Jensen-Shannon divergence. Second, it conducts solver-dependent analysis by solving instances with Gurobi (using 120-second timeouts) and extracting internal solver metrics including Root Node Gap, Heuristic Success rates, and Cut Plane usage from solver logs. The framework compares these distributions between generated and original instance sets using 1-Wasserstein distance, providing a multifaceted assessment of instance utility. Cross-solver validation with SCIP and HiGHS reveals significant stability differences, with Gurobi showing superior reliability for internal feature extraction.

## Key Results
- Instances with high structural similarity (JSD ≈ 0.9) can exhibit solving time gaps exceeding 2000% and drastically divergent solver-internal metrics
- ACM-MILP instances showed high structural similarity but were "exceptionally difficult to solve" compared to originals
- Gurobi demonstrated significantly more stable internal feature extraction than open-source alternatives, with W-distances for Root Node Gap and Cut Plane Usage showing minimal variance compared to SCIP/HiGHS
- The framework successfully identified deficiencies in current generative models, showing that local graph modifications often fail to preserve global properties essential for computational hardness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Solver-internal behavioral metrics diverge significantly between generated and original instances even when structural similarity is high, indicating failure to preserve true computational complexity.
- **Mechanism:** The framework treats the solver as an "expert" system, observing dynamic solver reactions rather than static graph topology. Divergence in heuristic success or cut usage indicates altered underlying mathematical structure.
- **Core assumption:** Solver behavioral profiles (specifically Gurobi's) are deterministic enough to serve as stable fingerprints for instance difficulty.
- **Evidence anchors:**
  - Abstract mentions treating "the solver's dynamic behavior as an expert assessment" to reveal discrepancies static features miss
  - Section 3.3.5 details extraction of Root Node Gap, Heuristic Success, and Cut Plane Usage
  - Corpus evidence supports difficulty of generalizing from synthetic to real structures
- **Break condition:** High internal randomness in the evaluation solver renders the "fingerprint" unreliable

### Mechanism 2
- **Claim:** Structural similarity metrics based on graph topology may be insufficient because they don't capture constraint interdependencies essential for feasibility and hardness.
- **Mechanism:** Graph-based generators manipulate edges/nodes to minimize distributional distance, but local manipulations may destroy global properties (boundedness, integrality gaps) that define computational hardness.
- **Core assumption:** The "11 structural features" used for JSD do not fully encode the problem's Polytope structure.
- **Evidence anchors:**
  - Section 5 Finding 1 notes ACM-MILP instances showed high structural similarity but were "exceptionally difficult to solve"
  - Section 5 Finding 3 discusses G2MILP's local edge modifications struggling to preserve global graph topology
  - Corpus evidence aligns with concept that surface simplicity ≠ computational ease
- **Break condition:** Generative models explicitly enforce mathematical constraints during generation

### Mechanism 3
- **Claim:** Framework reliability is conditional on solver stability; high variance in solver internals reduces the signal-to-noise ratio of the benchmark.
- **Mechanism:** The framework relies on comparing distributions using 1-Wasserstein distance. If solvers produce vastly different internal stats for identical/similar instances, the comparison becomes meaningless.
- **Core assumption:** Commercial solvers (Gurobi) have reached stability thresholds in internal strategies that open-source alternatives (SCIP/HiGHS) have not.
- **Evidence anchors:**
  - Section 4.2 compares Gurobi, SCIP, and HiGHS, finding HiGHS has "severe instability" in Root Node Gap (W-dist 26.7 vs Gurobi's 0.18)
  - Section 4.2 concludes "solver stability is not monolithic but highly feature-dependent"
  - Corpus evidence on specific solver stability benchmarks is weak
- **Break condition:** Solver updates change internal heuristic logic significantly, requiring baseline re-computation

## Foundational Learning

- **Concept: Branch-and-Bound & LP Relaxation**
  - **Why needed here:** To interpret "Root Node Gap" and "Node Counts." You must understand that the solver first solves a relaxed (easier) version (LP) and then splits the problem (Branching). A large gap means the relaxed solution is far from the integer solution, predicting high difficulty.
  - **Quick check question:** If the Root Node Gap is 0% for a generated instance, does that mean the instance is trivially easy?

- **Concept: Bipartite Graph Representation of MILP**
  - **Why needed here:** The paper critiques "structural similarity" derived from this representation. You need to visualize how a Constraint Matrix $A$ maps to Variable/Constraint nodes to understand what the generators are actually modifying.
  - **Quick check question:** In the bipartite graph, what does an edge weight represent?

- **Concept: Wasserstein Distance (Earth Mover's Distance)**
  - **Why needed here:** This is the primary metric for comparing distributions of solver features. Unlike KL-Divergence, it handles non-overlapping distributions, which is common when comparing generated vs. original instance stats.
  - **Quick check question:** If the W-distance of heuristic success rates is 0, how do the two instance sets compare?

## Architecture Onboarding

- **Component map:** MILP Instance (.lp/.mps) -> Solver-Indep Stage (Feasibility Check -> Graph Feature Extractor -> JSD Calculator) -> Solver-Dep Stage (Gurobi Solve -> Log Parser -> Internal Feature Extractor) -> Comparator (Wasserstein distance calculation)

- **Critical path:** The Log Parser is the brittle point. It must extract specific counters (e.g., Gomory cuts, Heuristic calls) from Gurobi's stdout/logfiles. If Gurobi changes log formatting, this breaks.

- **Design tradeoffs:**
  - Efficiency vs. Completeness: Uses 120s timeout to enable low-cost evaluation of "super hard" instances but truncates full solving profile
  - Proprietary vs. Open: Framework is solver-agnostic, but reliable "internal feature" analysis currently requires Gurobi due to stability issues in HiGHS/SCIP

- **Failure signatures:**
  - Metric Mismatch: High Structural Similarity (JSD ≈ 0.9) but extremely high Solving Time Gap (>2000%)
  - Solver Instability: High W-distance (>1.0) on split-half validation, indicating the solver is too stochastic for reliable benchmarking

- **First 3 experiments:**
  1. **Stability Baseline:** Run split-half validation on your target solver. If W-distances for Root Gap/Cuts aren't near zero, you cannot use that solver for internal feature evaluation
  2. **Correlation Check:** Generate instances using a baseline (e.g., G2MILP) and plot Structural Similarity vs. Solving Time Gap. Verify the paper's claim that these are poorly correlated
  3. **Truncation Test:** Compare internal features extracted at 120s vs. full solve for a dataset of medium difficulty. Confirm if the "early signal" is representative of full complexity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can generative models that optimize for solution-space characteristics rather than graph topology produce instances with higher fidelity in solver-internal metrics?
- **Basis in paper:** Explicit suggestion that "future advancements might benefit from shifting focus from direct graph structure manipulation towards methods that more directly model the underlying mathematical structure"
- **Why unresolved:** Current graph-based approaches struggle to capture "intricate constraint relationships" and inherent mathematical properties crucial for feasibility and hardness
- **What evidence would resolve it:** A new generator trained on mathematical constraints rather than adjacency matrices that minimizes Wasserstein distance for solver-internal features better than current baselines

### Open Question 2
- **Question:** What are the specific latent mathematical properties that drive the divergence between high structural similarity and computational hardness?
- **Basis in paper:** Finding 1 states that structural metrics "miss key complexity drivers" or that models introduce subtle variations causing high-structure instances to exhibit divergent solver interactions
- **Why unresolved:** Paper identifies the gap but doesn't isolate which hidden variables cause the solver's "expert assessment" to reject the instance
- **What evidence would resolve it:** Identification of a new feature set (beyond 11 structural features) that correlates strongly with solver-internal profiles

### Open Question 3
- **Question:** How can the evaluation framework be adapted to robustly assess instances using solvers with high internal stochasticity?
- **Basis in paper:** Cross-solver analysis reveals while Gurobi is stable, HiGHS exhibits severe instability in root node gap analysis (W1 distance 26.7 vs. 0.18)
- **Why unresolved:** Framework relies on stable solver behavior as an "expert"; extreme variance in open-source solvers may render comparison of solver-internal features unreliable
- **What evidence would resolve it:** A statistical correction method or averaged multi-run protocol that reduces noise in internal metrics for non-deterministic solvers

## Limitations
- Framework reliability critically depends on solver stability, with Gurobi serving as the evaluation "gold standard" despite being proprietary and potentially brittle to version changes
- 120-second timeout may truncate full solving profile for harder instances, potentially masking late-stage algorithmic behavior essential for capturing true instance difficulty
- Framework's effectiveness for non-graph-based MILP problems (e.g., purely numerical or scheduling formulations) remains unexplored

## Confidence
**High Confidence**: The observation that structural similarity metrics (JSD on graph features) poorly correlate with solver behavior metrics, supported by direct experimental comparisons showing high structural similarity can coexist with drastically different solving times and internal feature distributions.

**Medium Confidence**: The claim that Gurobi provides more stable internal feature extraction than open-source solvers (SCIP/HiGHS), based on comparative analysis showing significantly lower Wasserstein distances for Gurobi's Root Node Gap and Cut Plane Usage metrics.

**Medium Confidence**: The framework's utility as a diagnostic tool for generator improvement, demonstrated through case studies with G2MILP, ACM-MILP, and DIG-MILP, though generalizability to other generator architectures remains to be fully established.

## Next Checks
1. **Solver Version Sensitivity Test**: Run framework's split-half validation across multiple Gurobi versions (e.g., 9.x, 10.x, 11.x) to quantify how solver updates affect stability of internal feature metrics and whether baselines require periodic recalibration.

2. **Timeout Adequacy Analysis**: For a representative sample of medium-to-hard instances, compare internal feature distributions extracted at 120s versus full solve completion to determine if early-stage signal adequately captures complete solving complexity profile.

3. **Cross-Domain Generalizability**: Apply GenBench-MILP to evaluate generators for non-graph-based MILP domains (e.g., set covering, scheduling, or purely numerical formulations) to assess whether solver-internal behavioral metrics remain discriminative across different problem structures.