---
ver: rpa2
title: 'Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement
  Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach'
arxiv_id: '2507.21397'
source_url: https://arxiv.org/abs/2507.21397
tags:
- policy
- mocha
- learning
- pareto
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Multi-Objective weighted-Chebyshev Actor-Critic
  (MOCHA) method to address the challenge of exploring Pareto-stationary solutions
  in multi-objective reinforcement learning (MORL) with theoretical finite-time sample
  complexity guarantees. The key idea is to integrate weighted-Chebyshev (WC) scalarization
  and actor-critic frameworks, enabling systematic exploration of the Pareto stationarity
  front (PSF).
---

# Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach

## Quick Facts
- **arXiv ID**: 2507.21397
- **Source URL**: https://arxiv.org/abs/2507.21397
- **Reference count**: 34
- **Primary result**: Proposes MOCHA algorithm achieving ε-Pareto-stationary solutions with Õ(ε⁻²) sample complexity for MORL

## Executive Summary
This paper addresses the challenge of exploring Pareto-stationary solutions in multi-objective reinforcement learning (MORL) by introducing a Multi-Objective weighted-Chebyshev Actor-Critic (MOCHA) method. The key innovation combines weighted-Chebyshev scalarization with actor-critic frameworks, enabling systematic exploration of the Pareto stationarity front. The algorithm uses multiple temporal-difference learning in the critic component and a weighted-Chebyshev multi-gradient descent-style policy gradient in the actor component. Theoretical analysis shows MOCHA achieves ε-Pareto-stationary solutions with sample complexity Õ(ε⁻²), while experiments on a large-scale KuaiRand offline dataset demonstrate significant performance improvements over baseline MORL approaches.

## Method Summary
MOCHA is a multi-objective actor-critic algorithm that explores Pareto-stationary solutions through weighted-Chebyshev scalarization. The critic uses multiple independent TD learners (one per objective) with linear function approximation to estimate value functions. The actor employs a WC-MGDA-style policy gradient, solving a quadratic program at each step to find optimal weighting vectors that balance all objectives. The algorithm operates in offline settings using importance sampling for evaluation, and convergence is guaranteed to ε-Pareto-stationarity with Õ(ε⁻²) sample complexity under specific assumptions about feature representation and gradient properties.

## Key Results
- Achieves ε-Pareto-stationary solutions with sample complexity Õ(ε⁻²) for each exploration
- Outperforms baseline MORL approaches on KuaiRand offline dataset with 1218-dimensional states and 5 reward objectives
- Successfully explores the Pareto stationarity front (PSF) across diverse trade-offs
- Demonstrates theoretical convergence guarantees for non-convex objective spaces

## Why This Works (Mechanism)

### Mechanism 1: Weighted-Chebyshev (WC) Scalarization for Non-Convex Exploration
WC scalarization enables discovery of Pareto-stationary solutions in non-convex objective spaces through a min-max formulation min_θ max_i {p_i f_i(θ)}. This geometric approach navigates concave regions of the Pareto front by prioritizing the worst-performing objective at each step. The core assumption is that optimal solutions for the scalarized problem map to weakly Pareto-optimal solutions (Lemma 1). Evidence includes Proposition 4.7 showing WC optimal points are weakly Pareto-optimal even for non-convex problems. Break condition: In purely convex spaces, linear scalarization is computationally simpler and sufficient.

### Mechanism 2: Dynamic Weighting via MGDA-style Gradient Alignment
The algorithm dynamically finds common descent directions for all objectives through a Quadratic Program (QP) that minimizes the norm of weighted gradients ||∑λ_i∇J_i||. This ensures policy updates improve lagging objectives or find balances where no objective degrades, satisfying Pareto stationarity. The core assumption requires Lipschitz continuous gradients for stability. Evidence includes Theorem 3's convergence proof relying on gradient norm properties. Break condition: Perfect gradient conflicts (e.g., saddle points) may cause stalling, though theoretically handled by stationarity definition.

### Mechanism 3: Decoupled Critic Stability with Multi-TD Learning
Independent TD learning for each objective stabilizes variance for the actor's policy gradient. The critic maintains separate value approximations w_i for each objective, updated independently via TD errors. This controls policy gradient variance before combination, allowing sample complexity to scale as Õ(ε⁻²). The core assumption requires full-rank feature matrix for valid value approximation. Evidence includes Corollary 4's explicit sample complexity statement. Break condition: Insufficient feature representation (high approximation error ζ_approx) provides biased gradients, preventing convergence to true Pareto front.

## Foundational Learning

- **Concept: Pareto Stationarity vs. Optimality**
  - **Why needed here**: The paper targets Pareto stationarity (local condition) rather than global optimality, which is NP-hard for non-convex problems. Understanding this distinction is crucial for interpreting Theorem 3's convergence guarantees.
  - **Quick check question**: Can a solution be Pareto-stationary but not Pareto-optimal? (Answer: Yes, in non-convex spaces, analogous to local vs. global maxima).

- **Concept: Scalarization Functions (Linear vs. Chebyshev)**
  - **Why needed here**: The core contribution relies on Weighted-Chebyshev scalarization. One must understand why linear weighting (∑w_i r_i) fails to find solutions on concave portions of the Pareto front, whereas WC (min-max) succeeds.
  - **Quick check question**: If you have two objectives A and B, does minimizing 0.5A + 0.5B always find the same trade-off as minimizing max(0.5A, 0.5B)?

- **Concept: Actor-Critic Architectures**
  - **Why needed here**: MOCHA is built on the Actor-Critic framework. One needs to know how the Critic estimates value functions (TD learning) to provide a baseline for the Actor's policy gradient updates.
  - **Quick check question**: In MOCHA, does the Actor update use the raw reward or the TD-error (advantage) from the Critic?

## Architecture Onboarding

- **Component map**: State s_t -> Critic (Multi-TD) -> TD-errors δ^i -> Actor (Policy π_θ) -> Gradient matrix G -> QP Solver -> Weighting vector λ_t -> Update θ

- **Critical path**:
  1. Select exploration vector p (sets search direction on Pareto front)
  2. Critic trains on samples to converge to stable value estimates (requires sufficient iterations N)
  3. Actor collects batch B, computes gradients, solves QP for λ
  4. Actor updates θ using combined gradient
  5. Repeat until ||∇J(θ)λ||² ≤ ε

- **Design tradeoffs**:
  - p_min vs. Convergence Speed: Theorem 3 shows error scales with ∑η_t/p_min². Very small weights slow convergence significantly
  - Sample Efficiency vs. Compute: MOCHA reduces sample complexity but adds compute overhead from QP solving at every actor step

- **Failure signatures**:
  - Collapse to Single Objective: If p_min is effectively 0, algorithm ignores other objectives
  - Oscillating Gradients: If QP solver fails to find valid λ or learning rates are too high, policy may jitter
  - Stagnation: If critic approximator has high error (ζ_approx), policy converges to stationary point of approximated function, not true environment

- **First 3 experiments**:
  1. Gradient Norm Validation: Run MOCHA on simple 2-objective environment, plot ||∇J(θ)λ|| over time to verify convergence to 0
  2. Pareto Front Visualization: Run multiple experiments with varying p vectors, plot resulting objective values to visualize PSF and check for gaps
  3. Ablation on p_min: Test extreme trade-offs (small p_min) vs. balanced trade-offs to observe theoretical degradation in convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the theoretical finite-time convergence guarantees be extended to non-linear function approximation methods, such as neural networks?
- **Basis**: The theoretical analysis relies on Assumption 2, which restricts the critic to linear function approximation to ensure convergence.
- **Why unresolved**: Linear approximation allows specific boundedness and error analysis (ζ_approx) that don't directly translate to deep neural networks' non-convex landscapes.
- **What evidence would resolve it**: A convergence proof valid for over-parameterized neural networks or empirical studies showing convergence rates match theoretical bounds in deep RL settings.

### Open Question 2
- **Question**: How does the accuracy of estimated upper bounds (J*_ub) impact the stability and convergence speed of the regret minimization formulation?
- **Basis**: Section IV-B reformulates the objective into a regret minimization problem dependent on J*_ub, noting that while tightness isn't required for logical equivalence, impact of large estimation errors isn't analyzed.
- **Why unresolved**: Poor estimates could theoretically distort the gradient landscape or cause numerical instability during WC optimization.
- **What evidence would resolve it**: Sensitivity analysis measuring convergence time and stationarity error as gap between estimated J*_ub and true optimal values increases.

### Open Question 3
- **Question**: Can the computational overhead of solving the QP for weighting vector λ be reduced to improve scalability for high-dimensional objective spaces?
- **Basis**: Step 2-a requires solving a QP to determine dynamic weighting vector λ* in every iteration.
- **Why unresolved**: Solving a QP at every policy update introduces computational complexity that becomes prohibitive as number of objectives M grows significantly.
- **What evidence would resolve it**: Theoretical analysis or empirical benchmarks showing that approximate or closed-form solution for λ maintains convergence properties with lower time complexity.

## Limitations
- Theoretical sample complexity relies on Lipschitz gradient assumption and full-rank feature matrix, which may not hold in complex real-world environments
- Method requires solving QP at each actor step, adding computational overhead that scales poorly with number of objectives
- Convergence guarantee applies to Pareto stationarity, not global optimality, meaning solutions may be local stationary points in non-convex spaces
- Offline evaluation using NCIS introduces importance sampling variance that could affect result reliability

## Confidence
- **High confidence**: WC scalarization enabling non-convex Pareto front exploration (supported by Proposition 4.7 [20])
- **Medium confidence**: MGDA-style dynamic weighting finding common descent directions (proof relies on multiple technical assumptions)
- **Medium confidence**: Sample complexity result Õ(ε⁻²) (depends on feature representation quality and approximation error bounds)

## Next Checks
1. **Gradient norm validation**: Run MOCHA on simple 2-objective environment and plot ||∇J(θ)λ|| over time to verify convergence to stationarity
2. **Pareto front visualization**: Execute multiple runs with varying p vectors covering the simplex and visualize the recovered PSF to check for coverage gaps
3. **Convergence rate sensitivity**: Test MOCHA with extreme p_min values versus balanced trade-offs to empirically verify the theoretical degradation in convergence speed predicted by Theorem 3