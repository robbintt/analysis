---
ver: rpa2
title: 'COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for
  RGBT Tiny Object Detection'
arxiv_id: '2508.09533'
source_url: https://arxiv.org/abs/2508.09533
tags:
- detection
- coxnet
- feature
- fusion
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'COXNet addresses the challenge of detecting tiny objects in multimodal
  Red-Green-Blue-Thermal (RGBT) imagery, particularly in drone-based scenarios where
  spatial misalignment, low-light conditions, occlusion, and cluttered backgrounds
  are prevalent. The proposed method introduces three core innovations: a Cross-Layer
  Fusion Module (CLFM) that fuses high-level visible and low-level thermal features
  using wavelet-based alignment, a Dynamic Alignment and Scale Refinement (DASR) module
  that corrects cross-modal spatial misalignments and preserves multi-scale features,
  and an optimized label assignment strategy using the GeoShape Similarity Measure
  for better localization.'
---

# COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection

## Quick Facts
- **arXiv ID**: 2508.09533
- **Source URL**: https://arxiv.org/abs/2508.09533
- **Reference count**: 40
- **Primary result**: 3.32% mAP50 improvement on RGBTDronePerson dataset for tiny object detection

## Executive Summary
COXNet addresses the challenge of detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery, particularly in drone-based scenarios where spatial misalignment, low-light conditions, occlusion, and cluttered backgrounds are prevalent. The proposed method introduces three core innovations: a Cross-Layer Fusion Module (CLFM) that fuses high-level visible and low-level thermal features using wavelet-based alignment, a Dynamic Alignment and Scale Refinement (DASR) module that corrects cross-modal spatial misalignments and preserves multi-scale features, and an optimized label assignment strategy using the GeoShape Similarity Measure for better localization. COXNet achieves a 3.32% mAP50 improvement on the RGBTDronePerson dataset over state-of-the-art methods, demonstrating its effectiveness for robust detection in complex environments. The model maintains competitive efficiency with 51.27 GFLOPs and 17.6 FPS, balancing accuracy and real-time performance.

## Method Summary
COXNet introduces a novel framework for tiny object detection in multimodal RGBT imagery by addressing the fundamental challenges of spatial misalignment and feature complementarity between visible and thermal modalities. The method employs a Cross-Layer Fusion Module (CLFM) that leverages wavelet decomposition to fuse high-level visible features with low-level thermal features, effectively combining coarse semantic information with fine spatial details. The Dynamic Alignment and Scale Refinement (DASR) module performs cross-modal alignment through wavelet-based alignment and feature extraction, then refines object scales across multiple pyramid levels. The framework also implements an optimized label assignment strategy using the GeoShape Similarity Measure to improve localization accuracy. By integrating these components into a cohesive architecture, COXNet achieves significant improvements in detection performance while maintaining computational efficiency suitable for real-time drone applications.

## Key Results
- Achieves 3.32% mAP50 improvement on RGBTDronePerson dataset compared to state-of-the-art methods
- Maintains competitive efficiency with 51.27 GFLOPs computational complexity and 17.6 FPS inference speed
- Demonstrates robustness to spatial misalignment, low-light conditions, and cluttered backgrounds in multimodal RGBT imagery

## Why This Works (Mechanism)
COXNet works by addressing the fundamental challenges in RGBT tiny object detection through a multi-faceted approach. The Cross-Layer Fusion Module (CLFM) exploits the complementary nature of visible and thermal modalities by fusing high-level semantic features from visible imagery with low-level spatial features from thermal imagery, effectively combining coarse semantic information with fine spatial details. The Dynamic Alignment and Scale Refinement (DASR) module uses wavelet-based alignment to correct cross-modal spatial misalignments, which is critical for accurate feature fusion, while simultaneously refining object scales across multiple pyramid levels to improve detection of tiny objects at varying distances. The optimized label assignment strategy using the GeoShape Similarity Measure enhances localization accuracy by better matching predicted boxes to ground truth objects. This integrated approach effectively mitigates the limitations of individual modalities while leveraging their strengths, resulting in improved detection performance in challenging conditions.

## Foundational Learning

**Multimodal Fusion**: Combining information from multiple sensor modalities (visible and thermal) to improve detection robustness - needed to leverage complementary information across different imaging conditions; quick check: verify fusion improves performance over single-modality baselines.

**Wavelet Transform**: Mathematical tool for decomposing signals into frequency components at multiple scales - needed for multiscale feature analysis and alignment between modalities; quick check: confirm wavelet decomposition preserves spatial information.

**Feature Pyramid Networks**: Architecture that builds multi-scale feature representations for object detection - needed to detect objects at various sizes and distances; quick check: validate pyramid structure captures appropriate scale ranges.

**Spatial Alignment**: Correcting geometric discrepancies between different sensor modalities - needed because visible and thermal sensors have different fields of view and perspectives; quick check: measure alignment accuracy between modalities.

## Architecture Onboarding

**Component Map**: Input RGB/thermal images -> Backbone feature extraction -> CLFM (Cross-Layer Fusion Module) -> DASR (Dynamic Alignment and Scale Refinement) -> Detection head with GeoShape label assignment -> Output bounding boxes

**Critical Path**: Backbone extraction -> CLFM fusion -> DASR alignment and scale refinement -> Detection head with optimized label assignment

**Design Tradeoffs**: Balances computational efficiency (51.27 GFLOPs, 17.6 FPS) against detection accuracy improvements, prioritizes real-time performance while achieving significant mAP50 gains, trades off model complexity for robustness to challenging conditions

**Failure Signatures**: Poor performance in scenarios with extreme spatial misalignment beyond DASR correction capability, failure when thermal and visible modalities provide contradictory information, degraded accuracy when objects are too small for pyramid scale ranges

**First Experiments**: 1) Ablation study removing CLFM to measure fusion contribution, 2) Testing DASR alone on aligned datasets to isolate alignment benefits, 3) Evaluating detection performance on single-modality inputs to quantify multimodal advantage

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Performance evaluation based on a single dataset (RGBTDronePerson), limiting generalizability to other RGBT datasets or object categories
- Computational overhead claims lack quantitative comparison with baseline computational costs
- Real-time performance of 17.6 FPS may not be sufficient for all drone applications requiring higher frame rates
- Statistical significance testing not performed to confirm reliability of the 3.32% mAP50 improvement

## Confidence
- **High**: Technical implementation of CLFM, DASR, and GeoShape label assignment strategies
- **Medium**: Performance claims based on limited dataset scope and lack of statistical validation
- **Medium**: Real-time efficiency claims without comparative analysis against baseline models

## Next Checks
1. Test COXNet on additional RGBT datasets with different object categories and environmental conditions to evaluate generalizability
2. Conduct ablation studies comparing alternative fusion and alignment methods to isolate COXNet's unique contributions
3. Perform statistical significance testing on performance improvements across multiple runs and datasets to validate reported gains