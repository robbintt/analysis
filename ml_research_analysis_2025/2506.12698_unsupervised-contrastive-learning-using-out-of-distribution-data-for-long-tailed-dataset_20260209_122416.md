---
ver: rpa2
title: Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed
  Dataset
arxiv_id: '2506.12698'
source_url: https://arxiv.org/abs/2506.12698
tags:
- data
- network
- learning
- dataset
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses self-supervised learning (SSL) on long-tailed
  datasets to learn balanced and well-separated representations. The key challenge
  is that typical SSL methods perform poorly on imbalanced data, where most samples
  belong to head classes, leading to suboptimal representations for tail classes.
---

# Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed Dataset

## Quick Facts
- arXiv ID: 2506.12698
- Source URL: https://arxiv.org/abs/2506.12698
- Authors: Cuong Manh Hoang; Yeejin Lee; Byeongkeun Kang
- Reference count: 13
- Primary result: Proposes a two-stage SSL framework using OOD data to learn balanced representations for long-tailed datasets, achieving state-of-the-art performance.

## Executive Summary
This work addresses the challenge of self-supervised learning on long-tailed datasets, where standard methods struggle due to head-class dominance. The proposed method introduces a two-stage framework that leverages OOD data to learn balanced and well-separated embeddings. Stage 1 pre-trains with ID+OOD data using pseudo semantic discrimination and domain discrimination losses, while Stage 2 distills these balanced embeddings into the ID domain using a frozen guiding network for sample selection and force weighting.

## Method Summary
The method employs a two-stage SSL framework for long-tailed datasets. First, it pre-trains a network on ID and sampled OOD data using pseudo semantic discrimination loss (treating nearest neighbors as positives) and domain discrimination loss (separating ID/OOD). OOD sampling is guided by cluster tailness estimation to reach tail classes. Second, it fine-tunes on ID-only data using the pre-trained network as a frozen guide to select positive/negative samples and control contrastive forces, while preserving balanced embeddings through distillation loss.

## Key Results
- Achieves 90.26% overall accuracy on CIFAR-10-LT (2.71 std) vs 85.41% (3.95 std) for previous best.
- Consistently outperforms prior methods across CIFAR-10-LT, CIFAR-100-LT, ImageNet-100-LT, and Places-LT.
- Shows better balancedness with higher accuracies across Many/Med/Few class splits.
- OOD sampling outperforms random augmentation by ~3-5% accuracy.

## Why This Works (Mechanism)

### Mechanism 1
Sampling OOD data near tail-class embeddings rebalances training distribution, reducing head-class dominance. The method clusters ID embeddings using KL-divergence refinement, estimates per-cluster tailness via neighborhood sparsity, then samples OOD images closest to high-tailness centroids. This injects diverse semantically-relevant samples for under-represented classes.

### Mechanism 2
Pseudo semantic discrimination loss (L_PSD) improves inter-class separability by treating nearest-neighbor embeddings as positive pairs, encoding semantic similarity without labels. This clusters semantically similar samples while maintaining contrastive pressure against negatives. Domain discrimination loss (L_DD) additionally separates ID/OOD domains, preventing OOD samples from collapsing into ID clusters.

### Mechanism 3
Using pre-trained network as frozen guide for contrastive sample selection and force weighting transfers balanced embeddings while optimizing for ID-specific structure. The guide provides fixed embeddings to select positives (K nearest neighbors) and negatives (farthest cluster), with similarity scores weighting attractive/repulsive forces in the student network. Distillation loss preserves the guide's embedding geometry.

## Foundational Learning

- **Concept: Contrastive Learning (SimCLR/SDCLR)**
  - Why needed: Method builds directly on SimCLR and SDCLR as base frameworks. Understanding instance discrimination and InfoNCE-style losses is prerequisite.
  - Quick check: Can you explain why standard instance discrimination pushes same-class samples apart in embedding space?

- **Concept: Long-Tailed Distributions and Class Imbalance**
  - Why needed: Core problem is representation degradation for tail classes. Understanding imbalance ratio and why standard SSL fails is essential.
  - Quick check: Given dataset with 1000 "dog" images and 10 "tiger" images, what embedding-space pathologies would you expect from standard contrastive training?

- **Concept: Knowledge Distillation and Teacher-Student Architectures**
  - Why needed: Stage 2 uses frozen pre-trained network as teacher. Understanding soft targets and why teacher guidance helps is required.
  - Quick check: Why would distillation help maintain "balancedness" compared to standard fine-tuning?

## Architecture Onboarding

- **Component map**: ResNet-18/50 backbone -> Stage 1 (ID+OOD) -> Pseudo Semantic Discrimination Head + Domain Discrimination Head -> Stage 2 (ID-only) -> Frozen Guide Network f(·) + Trainable Student Network g(·) -> Guided Contrastive Loss + Distillation Loss -> Linear Classifier

- **Critical path**: 1) Initialize embedding network, 2) Stage 1: Cluster ID embeddings -> estimate tailness -> sample OOD -> train 500-2000 epochs, 3) Stage 2: Freeze f(·), initialize g(·), train 500 epochs, 4) Evaluate: freeze g(·), train linear classifier

- **Design tradeoffs**:
  - Nb (OOD sampling budget): 10K optimal for CIFAR-10-LT; too few loses rebalancing effect, too many introduces noise
  - Update frequency T: 25 epochs optimal; too frequent is costly, too infrequent misses embedding drift
  - α, β loss weights: α=0.3, β=0.4 best; α too high over-penalizes domain mixing, β too high over-constrains student
  - Two-stage vs. joint training: Paper hypothesizes joint ID+OOD degrades accuracy but not directly ablated

- **Failure signatures**:
  - High STD across Many/Med/Few accuracies → OOD sampling not reaching tail classes; check tailness score distribution
  - Stage 2 accuracy drops below Stage 1 → Distillation over-constraining; reduce β
  - OOD sampling time explodes → Large OOD dataset with inefficient nearest-neighbor search; pre-compute embeddings or use approximate NN

- **First 3 experiments**:
  1. Run SimCLR on CIFAR-10-LT without modifications; verify accuracy drop vs. balanced CIFAR-10 (75.34% vs. 84.62%)
  2. Train Stage 1 with random OOD sampling vs. tailness-guided sampling; expect ~3-5% gap
  3. Run full pipeline, then remove L_DL (set β=0) and L_GCL weighting (set w_pos=w_neg=1); expect ~1-2% drop per component

## Open Questions the Paper Calls Out

### Open Question 1
Is the strict separation of the two-step framework (pre-training vs. distillation) necessary, or can a unified training objective be developed to use OOD data continuously without the accuracy degradation observed in naive implementations? The paper demonstrates that a naive single-stage approach underperforms but does not investigate if advanced unified loss functions could mitigate the pseudo-label noise.

### Open Question 2
How does the semantic distance between the Out-of-Distribution (OOD) dataset and the In-Domain (ID) dataset impact the effectiveness of the pseudo semantic discrimination loss? The method relies on the assumption that available OOD data is semantically relevant enough to support "tail" classes, but this is not analyzed.

### Open Question 3
How robust is the "tailness" estimation method when applied to datasets with extreme imbalance ratios (>100) or very high-dimensional feature spaces? The paper limits experiments to imbalance ratio of 100 and does not test the limits of the density-based estimation in extreme scenarios.

## Limitations
- Two-stage framework necessity not directly ablated; joint training dismissed without empirical comparison
- Computational cost of OOD sampling (k-NN search over 300K images) could be prohibitive for larger-scale applications
- Assumes OOD data contains semantically relevant samples for tail classes, dependent on OOD dataset choice

## Confidence

- **High**: Overall performance improvements over baselines; effectiveness of guided contrastive weighting; OOD sampling outperforms random augmentation
- **Medium**: Necessity of two-stage framework (not directly ablated); claim that nearest-neighbor pseudo-labels improve semantic clustering
- **Low**: Generalizability to datasets with minimal OOD semantic overlap; scalability to web-scale data without approximate NN search

## Next Checks

1. **OOD Semantic Relevance**: Swap Places-Extra69 with a semantically disjoint OOD set (e.g., medical images) and measure performance drop
2. **Two-Stage Necessity**: Implement joint ID+OOD training with all proposed losses and compare to two-stage pipeline
3. **Memory/Compute Scaling**: Profile OOD sampling time and memory usage for 300K images; implement FAISS-based approximate NN and measure accuracy vs. speed tradeoff