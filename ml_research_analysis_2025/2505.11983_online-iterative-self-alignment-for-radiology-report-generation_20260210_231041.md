---
ver: rpa2
title: Online Iterative Self-Alignment for Radiology Report Generation
arxiv_id: '2505.11983'
source_url: https://arxiv.org/abs/2505.11983
tags:
- preference
- dataset
- data
- report
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an Online Iterative Self-Alignment (OISA)
  method for Radiology Report Generation (RRG) to address the data limitation and
  generalization issues in existing supervised fine-tuning approaches. The OISA method
  consists of four stages: self-generation of diverse data using a one-hot weight
  vector as a condition for the RRG model to represent radiologists'' heterogeneous
  preferences, self-evaluation to automatically construct multi-objective preference
  datasets using evaluation metrics, self-alignment for multi-objective optimization
  using MODPO, and self-iteration for continuous improvement.'
---

# Online Iterative Self-Alignment for Radiology Report Generation

## Quick Facts
- arXiv ID: 2505.11983
- Source URL: https://arxiv.org/abs/2505.11983
- Reference count: 23
- This paper proposes OISA, achieving state-of-the-art RRG performance with improvements up to 2.54 in RadCliQ and 0.129 in BLEU4.

## Executive Summary
This paper addresses the data limitation and generalization challenges in radiology report generation (RRG) by proposing an Online Iterative Self-Alignment (OISA) method. The approach leverages a single supervised fine-tuned (SFT) model to self-generate diverse data, self-evaluate using multiple clinical metrics, and self-align through multi-objective optimization. By iterating this process, OISA continuously improves both data quality and model performance without requiring additional human annotations. Experimental results on MIMIC-CXR and IU-Xray datasets demonstrate significant improvements across multiple evaluation metrics, outperforming previous approaches.

## Method Summary
OISA operates through four stages: self-generation (using one-hot weight vectors to create diverse report candidates representing different radiologist preferences), self-evaluation (automatically constructing preference datasets by ranking candidates with multiple clinical metrics), self-alignment (optimizing the model using MODPO for multi-objective learning), and self-iteration (replacing the reference model to generate new, improved data). This pipeline enables continuous refinement without human labeling, leveraging the correlation between established RRG metrics and clinical utility.

## Key Results
- Achieves state-of-the-art performance across multiple evaluation metrics on MIMIC-CXR and IU-Xray datasets
- Shows improvements of 0.428 in BLEU1, 0.129 in BLEU4, 0.885 in BERTScore, 2.54 in RadCliQ, 0.273 in RadGraphF1, 0.516 in ChexbertF1, and 0.341 in GREEN on MIMIC-CXR
- Demonstrates that iterative self-alignment significantly improves both data quality and model performance through multi-objective optimization

## Why This Works (Mechanism)

### Mechanism 1: Conditional Diversity Injection
The method uses a one-hot weight vector as a condition to force a single model to simulate heterogeneous radiologist preferences, expanding data coverage beyond the fixed SFT distribution. The model is prompted with weight vector $\hat{w}$ (e.g., prioritizing clinical accuracy over fluency), decoupling generation of "chosen" and "rejected" candidates from the model's default mode. This creates a diverse candidate pool for preference learning, assuming the base SFT model has sufficient latent capacity to generate valid outputs across different objective axes when conditioned.

### Mechanism 2: Metric-Driven Stratified Filtering
Automated evaluation metrics act as proxies for human annotators to construct high-quality preference pairs $(y_w, y_l)$ without manual labeling. The method ranks generated reports by specific metrics ($M_k$, e.g., RadGraph F1), selecting top performers as "chosen" ($y_w$) and lower performers as "rejected" ($y_l$), with stratified sampling ensuring representation across disease groups. This assumes the selected metrics (RadCliQ, GREEN) correlate strongly with actual clinical utility and human expert preference.

### Mechanism 3: Iterative Covariance Refinement
Replacing the reference model $\pi_{ref}$ with the updated policy $\pi_{\theta_w}$ in subsequent iterations improves the theoretical sub-optimality gap by enhancing data coverage. Theoretical analysis suggests the sub-optimality gap depends on how well the dataset covers the optimal policy's distribution. By generating new data using the improved model, the method tightens this bound, assuming the "improved" model actually generates higher-quality candidates.

## Foundational Learning

- **Direct Preference Optimization (DPO) / MODPO:**
  - Why needed: The core training loop replaces standard RL with MODPO loss, using the Bradley-Terry model and skipping training a separate reward model
  - Quick check: How does the loss function in Eq. (4) differ from standard DPO regarding the "margin" term?

- **Multi-Objective Optimization (Pareto Fronts):**
  - Why needed: The system optimizes conflicting goals (e.g., BLEU vs. Clinical Accuracy), requiring understanding of Pareto fronts to interpret "Trade-off" visualizations
  - Quick check: If we increase the weight for RadCliQ, what happens to the RadGraph F1 score based on the results?

- **Preference Dataset Structure:**
  - Why needed: Unlike SFT which uses (Image, Report), this method requires (Image, Chosen Report, Rejected Report), with construction being the bulk of "Self-Evaluation" engineering effort
  - Quick check: In the stratified sampling step, how is the "rejected" report selected after the "chosen" report is identified?

## Architecture Onboarding

- **Component map:** PromptMRG (or similar RRG SFT model) -> Multi-head attention fusion for Weight Vector $w$ + Image Features -> Generator (produces $N$ candidates per image using varying weights) -> Scorer (external APIs/functions for RadCliQ, RadGraphF1, GREEN) -> Alignment Engine (MODPO training loop)

- **Critical path:** 1) Initialization: Load SFT model $\rightarrow$ Clone to $\pi_{ref}$ and $\pi_{\theta}$; 2) Data Build: Generate candidates using one-hot weights $\rightarrow$ Deduplicate $\rightarrow$ Score $\rightarrow$ Build Triplets; 3) Training: Calculate Margin Reward (Eq. 3) $\rightarrow$ Compute MODPO Loss (Eq. 4) $\rightarrow$ Update $\pi_{\theta}$; 4) Iteration: $\pi_{ref} \leftarrow \pi_{\theta}$ and restart Data Build

- **Design tradeoffs:** Using correlated metrics (e.g., BLEU and BERTScore) provides weak contrast signals; the paper uses distinct clinical/semantic metrics. Generating diverse data requires multiple forward passes per image during the *data construction* phase (offline), which is compute-heavy.

- **Failure signatures:** Data Exhaustion (if deduplication is too aggressive or the model generates identical reports, $Y_w$ and $Y_l$ become indistinguishable $\rightarrow$ Loss becomes noisy); Objective Collapse (if one metric dominates, the model might generate fluent nonsense to satisfy entity extraction)

- **First 3 experiments:** 1) Sanity Check (Iteration 0 $\to$ 1): Verify that "chosen" reports in the preference dataset actually have higher metric scores than the "rejected" ones; 2) Weight Sensitivity: Test if the model output changes visibly when switching $w$ from [1, 0, 0] to [0, 1, 0] at inference time; 3) Ablation on Iterations: Compare performance of 1-round vs. 3-rounds to confirm the theoretical "tightening bound" empirically matches Table 1

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes existing RRG evaluation metrics are reliable indicators of clinical utility and human preference, requiring domain-specific validation
- Computational cost of generating multiple candidates per image during data construction is not quantified but acknowledged as a limitation
- The paper does not address potential biases introduced by stratified sampling or whether the method works across different radiology subspecialties

## Confidence
**High Confidence (8-10/10):** The theoretical framework connecting online iterative learning to tighter sub-optimality bounds is well-grounded and clearly articulated; The experimental methodology for comparing against baselines on MIMIC-CXR and IU-Xray is sound.

**Medium Confidence (5-7/10):** The claim that automated metrics can reliably replace human annotators for preference dataset construction, while supported by literature, requires domain-specific validation; The improvements in evaluation metrics are significant, but the clinical real-world impact remains uncertain without radiologist preference studies.

**Low Confidence (1-4/10):** The scalability analysis to larger datasets or more complex multi-objective scenarios is absent; The paper does not address whether the method works across different radiology subspecialties.

## Next Checks
1. Conduct a blind study where radiologists rank reports generated by OISA against those from SFT and other baselines to directly test whether metric-driven improvements align with human clinical judgment.

2. Repeat the OISA pipeline starting from a weaker SFT model (fewer parameters or less training data) to determine the relative contribution of the alignment method versus the initialization quality.

3. Systematically test whether the model learns to game specific metrics by analyzing report characteristics that correlate with metric scores but may not improve clinical utility, identifying potential Goodhart's Law effects.