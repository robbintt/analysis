---
ver: rpa2
title: 'HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination
  Detection'
arxiv_id: '2504.10168'
source_url: https://arxiv.org/abs/2504.10168
tags:
- hallucination
- arxiv
- context
- language
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HalluSearch is a multilingual pipeline for detecting and localizing
  hallucinated text spans in LLM outputs, developed for the Mu-SHROOM shared task.
  It employs a three-step RAG-based approach: factual splitting to isolate verifiable
  claims, context retrieval using Wikipedia and fallback strategies, and hallucination
  verification via LLM comparison.'
---

# HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection

## Quick Facts
- arXiv ID: 2504.10168
- Source URL: https://arxiv.org/abs/2504.10168
- Reference count: 11
- Primary result: 4th place ranking in English and Czech at Mu-SHROOM shared task

## Executive Summary
HalluSearch is a multilingual hallucination detection system that employs a three-stage RAG pipeline to identify and localize hallucinated text spans in LLM outputs. The system achieves strong performance across 14 languages by combining factual splitting, Wikipedia-based retrieval with fallback strategies, and LLM-powered verification. While it ranks competitively (4th in English and Czech), performance varies significantly across languages, particularly struggling with low-resource languages where web coverage is limited.

## Method Summary
HalluSearch implements a three-stage pipeline: (1) Factual splitting decomposes LLM responses into atomic claims using GPT-4o, extracting exact substring positions for granular verification; (2) Context retrieval uses Google Custom Search to prioritize Wikipedia results, with keyword extraction and LLM-generated context as fallbacks; (3) Hallucination verification employs GPT-4o to compare each claim against retrieved context and identify contradictory substrings, which are then mapped back to original text positions for character-level span localization. The system outputs both hard labels (binary spans) and soft labels (probability scores) for evaluation.

## Key Results
- Achieves IOU scores ranging from 0.3883 to 0.5681 across 14 languages
- Ranks 4th in English and Czech, demonstrating competitive performance in high-resource languages
- Shows significant performance degradation in low-resource languages (Basque, Farsi) due to limited web coverage
- Single-model approach (GPT-4o) outperforms ensemble voting strategies for hallucination detection

## Why This Works (Mechanism)

### Mechanism 1: Factual Splitting → Granular Verification
Decomposing LLM responses into atomic claims reduces verification errors by isolating individual propositions. GPT-4o extracts discrete factual statements with exact substring positions via structured JSON prompts. Each claim is verified independently against retrieved context, preventing conflated judgments where partial correctness in a long passage masks specific errors.

### Mechanism 2: Wikipedia-Primary RAG with Fallback Chains
Prioritizing Wikipedia retrieval grounds verification in a widely audited knowledge source, with fallback strategies maintaining coverage for edge cases. Google Custom Search retrieves candidate sources; highest-ranked Wikipedia result is selected. If no results, keyword extraction reformulates queries. If still empty, LLM generates synthetic context as last resort.

### Mechanism 3: Prompt-Based Span Extraction with Character Alignment
LLM-based comparison against retrieved context can localize hallucinated spans at character granularity. GPT-4o receives structured prompts with context and claim arrays, outputs contradictory substrings. Postprocessing maps substrings to original text positions for IoU evaluation.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire architecture assumes you understand how external knowledge retrieval augments LLM inference for fact verification.
  - Quick check question: Can you explain why RAG helps with hallucination detection but does not eliminate it?

- **Concept: Span-Level Sequence Labeling**
  - Why needed here: Mu-SHROOM requires character-level probability outputs, not binary classification. You must understand how to convert discrete predictions into continuous span annotations.
  - Quick check question: What is the difference between token-level and character-level span labeling, and why does this matter for multilingual text?

- **Concept: Low-Resource Language Challenges**
  - Why needed here: Performance drops in Basque, Farsi, and other under-resourced languages reveal systemic gaps in web coverage and morphological processing.
  - Quick check question: Why would keyword extraction fail more often in morphologically rich languages like Finnish or Basque?

## Architecture Onboarding

- **Component map:** Input (user_query, LLM_response) → Factual Splitting (GPT-4o → JSON atomic claims) → Context Retrieval (Google Custom Search → Wikipedia selection OR keyword-retry OR LLM-fallback) → Hallucination Verification (GPT-4o → contradictory substring JSON) → Postprocessing (Substring alignment → hard-label spans + soft-label probabilities)

- **Critical path:** Retrieval quality determines verification reliability. If fallback reaches LLM-generated context, the entire pipeline degrades to self-referential checking.

- **Design tradeoffs:** Wikipedia-only vs. broader web: Precision vs. recall on niche topics; Single-model (GPT-4o) vs. ensemble voting: Simplicity vs. robustness (paper reports ensemble underperformed); Hard vs. soft labels: Binary span extraction vs. annotator-agreement probability modeling

- **Failure signatures:** Empty search results in low-resource languages → forced LLM fallback → potential context hallucination; Keyword extraction producing "nonsensical or misleading outputs" in morphologically complex languages; Substring misalignment during postprocessing causing IoU degradation despite correct detection

- **First 3 experiments:**
  1. Replicate factual splitting on a held-out sample; measure claim-extraction error rate by language before any verification
  2. Ablate fallback strategies: compare Wikipedia-only vs. Wikipedia+keyword-retry vs. full cascade; measure retrieval success rate per language
  3. Evaluate verification accuracy when context is synthetically corrupted; quantify how context quality affects span-detection precision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hallucination detection systems maintain high performance in low-resource languages where web coverage is limited?
- Basis in paper: Authors state challenges persist in consistently detecting hallucinations across other languages and certain languages like Basque or Farsi have limited online resources which reflected adversely on search engine results
- Why unresolved: Current fallback strategies (keyword extraction, LLM-generated context) either fail to retrieve adequate context or introduce additional hallucination risk
- What evidence would resolve it: Demonstrated performance gains on low-resource languages through novel retrieval methods, cross-lingual transfer techniques, or alternative knowledge sources beyond web search

### Open Question 2
- Question: Why do voting-style ensemble approaches with open-source models underperform compared to single-model approaches for hallucination span detection?
- Basis in paper: Authors note results with this approach were not very impressive and speculate this may be due to inconsistent alignment between the models' annotations
- Why unresolved: The paper doesn't investigate the root causes of ensemble failure or potential fixes for alignment issues
- What evidence would resolve it: Systematic analysis of disagreement patterns between models, or development of alignment techniques that enable effective ensemble voting

### Open Question 3
- Question: What language-specific adaptations beyond retrieval are needed to address the performance disparity across morphologically rich languages?
- Basis in paper: The paper notes implementation challenges arose when dealing with 14 languages, each featuring distinct morphological rules and extracting keywords from morphologically rich, under-resourced languages produces nonsensical or misleading outputs
- Why unresolved: The paper treats all languages with the same pipeline despite known morphological differences
- What evidence would resolve it: Ablation studies showing which morphological processing components improve performance for specific languages

## Limitations

- Performance exhibits strong dependence on Wikipedia availability and retrieval quality, with significant degradation in low-resource languages
- Span localization precision is limited by substring extraction and alignment errors that can accumulate independently of detection quality
- When retrieval fallbacks fail, LLM-generated context creates circular verification where the same model verifies its own potentially hallucinated output

## Confidence

**High Confidence Claims:**
- The three-stage RAG-based pipeline architecture is correctly described and implementable
- Factual splitting reduces verification complexity by isolating atomic claims
- Performance ranking (4th in English/Czech) is verifiable from shared task results

**Medium Confidence Claims:**
- Wikipedia prioritization improves verification reliability compared to general web sources
- Fallback strategies maintain reasonable coverage despite quality trade-offs
- Single-model approach (GPT-4o) performs competitively with ensemble methods

**Low Confidence Claims:**
- Specific prompt formulations used for factual splitting, keyword extraction, and hallucination verification
- Exact thresholds and decision boundaries in the retrieval fallback chain
- Performance estimates for individual languages beyond aggregate IOU scores

## Next Checks

1. **Claim Extraction Quality Audit**: Measure factual splitting error rates by language on a held-out sample. Compare claim extraction precision and recall against gold annotations to determine how much verification accuracy is compromised by extraction failures versus verification failures.

2. **Retrieval Fallback Impact Analysis**: A/B test the pipeline with: (a) Wikipedia-only retrieval, (b) Wikipedia + keyword-retry, (c) full cascade including LLM fallback. Measure retrieval success rates and hallucination detection accuracy per language to quantify the performance penalty of each fallback stage.

3. **Synthetic Context Verification Bias**: Design experiments where retrieved context is deliberately corrupted or generated synthetically, then measure how this affects hallucination detection precision and recall. This will quantify the extent to which LLM-generated fallback context introduces circular verification bias.