---
ver: rpa2
title: Understanding Mental States to Guide Social Influence in Multi-Person Group
  Dialogue
arxiv_id: '2601.13687'
source_url: https://arxiv.org/abs/2601.13687
tags:
- social
- guidance
- states
- mental-state
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SocialMindChange, a benchmark for evaluating
  large language models' ability to perform proactive supportive guidance in multi-person
  group dialogue. Unlike existing benchmarks that focus on tracking mental states,
  SocialMindChange tests whether models can guide the mental state trajectory of a
  target character through a sequence of five connected social scenarios.
---

# Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue

## Quick Facts
- arXiv ID: 2601.13687
- Source URL: https://arxiv.org/abs/2601.13687
- Reference count: 19
- Primary result: LLMs achieve 22.4% accuracy on proactive supportive guidance tasks, 54.2% below human baseline of 76.6%

## Executive Summary
This paper introduces SocialMindChange, a benchmark for evaluating large language models' ability to perform proactive supportive guidance in multi-person group dialogue. Unlike existing benchmarks that focus on tracking mental states, SocialMindChange tests whether models can guide the mental state trajectory of a target character through a sequence of five connected social scenarios. The benchmark includes 1,200 social contexts with over 96,000 multiple-choice questions covering four question types: Guidance-Action (choosing the next supportive move), Guidance-Transition-1 (predicting immediate state changes), Guidance-Transition-2 (explaining why guidance works), and Guidance-Transition-3 (planning multi-step guidance).

## Method Summary
The benchmark construction follows a three-step pipeline: (1) social context generation including location, 4-character profiles with relationships, and demographics from U.S. Census data; (2) mental state trajectory design across 5 scenarios with belief/emotion/intention/action states and transition cues; (3) scenario realization with dialogue generation and human validation. GPT-4o generates the data with 89%, 81.7%, and 88.3% retention rates at each filtering stage. Evaluation uses 80 multiple-choice questions per stage across four types, with vanilla and Chain-of-Thought prompting (temperature=0.7, top-p=0.9).

## Key Results
- LLMs achieve only 22.4% average accuracy, 54.2% below human performance of 76.6%
- Largest performance gap appears in Guidance-Transition tasks (13-16 point drop from Action tasks)
- U-shaped accuracy curve shows models fail most at middle-stage transitions
- Chain-of-Thought prompting sometimes hurts performance by failing to enforce cross-stage dependencies

## Why This Works (Mechanism)

### Mechanism 1: State Dependency Trajectory Construction
The benchmark enforces a strict causal dependency chain where beliefs drive emotions, which shape intentions, ultimately determining actions. Generation requires explicit "transition cues" between stages to justify state changes, following the rule: Beliefs → Emotions → Intentions → Actions.

### Mechanism 2: Multi-Agent Constraint Propagation
Four-person group dynamics force models to maintain consistency across competing intentions and higher-order beliefs. Each scenario includes 4 distinct character profiles with defined relationships, requiring satisfaction of multiple simultaneous mental state constraints.

### Mechanism 3: Guidance-Transition Separation
Separating Guidance-Action (local move) from Guidance-Transition (cross-stage effect) isolates planning ability from reactive capability. Distractors are "locally reasonable" but fail to produce required future states, exposing whether models understand forward social influence.

## Foundational Learning

- **Dynamic vs. Static Theory of Mind**: Distinguish tracking state at single point vs. over time. Quick check: If character is "hopeful" in Scene 1, can we assume they are "hopeful" in Scene 3?
- **Higher-Order Beliefs**: Understanding nested beliefs like "A believes that B intends..." is critical. Quick check: If Alice thinks Bob is lying, how does that differ from "Alice thinks that Charlie believes Bob is lying"?
- **Proactive Guidance**: The core shift from observation to influence. Quick check: Does the model select an utterance because it sounds nice (reactive), or because it knows it will shift the target's emotion from "anxious" to "hopeful" in the next stage (proactive)?

## Architecture Onboarding

**Component map**: Social Context (Location + 4 Profiles + Relationships) → Social Stage (5 Scenarios) → Scenario (Dialogue + 4 Mental State Annotations) → Evaluation (80 MCQs per stage)

**Critical path**: Profile Generation → Trajectory Design → Dialogue Realization → Question Generation

**Design tradeoffs**: LLM-assisted generation with strict human validation (4.0/5.0 rating) vs. potential hallucinated social logic; 4-person groups increase realism but lower accuracy

**Failure signatures**: "Middle-Stage" Drop (U-shaped curve with lowest performance at middle transitions); CoT Inconsistency (Chain-of-thought prompting sometimes hurts)

**First 3 experiments**: 1) Replicate U-Curve by running GPT-4o and plotting accuracy by scenario transition; 2) Test truncation ablation by removing early scenarios; 3) Evaluate 5-person and 6-person variants to measure degradation slope

## Open Questions the Paper Calls Out
- Can prompting strategies that explicitly enforce cross-stage dependencies improve performance on Guidance-Transition tasks?
- How does the inclusion of multimodal cues (facial expressions, tone) affect LLM accuracy in tracking and guiding mental-state trajectories?
- What architectural modifications are required to eliminate the "lost in the middle" phenomenon in long-horizon Theory of Mind reasoning?

## Limitations
- Heavy reliance on LLM-generated data may introduce systematic social reasoning biases
- Three-step filtering process lacks transparency about rejection criteria, raising selection bias concerns
- Human baseline methodology lacks detailed participant selection and expertise level description

## Confidence
- **High**: Benchmark construction methodology is clearly specified and reproducible; performance gap between human (76.6%) and LLM (22.4%) is substantial
- **Medium**: Claim about cross-stage reasoning struggles is supported by U-shaped curve but requires further investigation; CoT performance degradation needs systematic exploration
- **Low**: Generalizability of 4-person group constraint model to real-world scenarios; benchmark's ability to isolate proactive vs reactive intelligence

## Next Checks
1. Conduct detailed error analysis comparing human and LLM responses on the same items to determine if failures stem from misunderstanding social dynamics or 4-person complexity
2. Systematically test model performance across different group sizes (2, 3, 4, 5, 6 people) to isolate whether performance drop is specific to 4-person dynamics
3. Evaluate the benchmark with LLMs trained on diverse cultural datasets to assess generalizability beyond U.S. Census-based assumptions