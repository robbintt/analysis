---
ver: rpa2
title: 'CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese'
arxiv_id: '2511.04139'
source_url: https://arxiv.org/abs/2511.04139
tags:
- correction
- cantonese
- tonal
- speech
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic speech recognition
  (ASR) for low-resource Cantonese, which is hindered by limited data, six lexical
  tones, tone sandhi, and accent variation. Existing models like Whisper suffer from
  high word error rates, especially for tonal distinctions.
---

# CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese
## Quick Facts
- arXiv ID: 2511.04139
- Source URL: https://arxiv.org/abs/2511.04139
- Reference count: 0
- One-line primary result: CantoASR improves Cantonese ASR by integrating forced alignment, LoRA-finetuned Whisper, and Qwen-Audio for prosody-aware correction.

## Executive Summary
The paper tackles automatic speech recognition (ASR) for low-resource Cantonese, which is complicated by limited training data, six lexical tones, tone sandhi, and accent variation. Standard models like Whisper exhibit high word error rates, especially in tonal discrimination. CantoASR introduces a collaborative framework combining forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper model, and an instruction-tuned Qwen-Audio model for prosody-aware correction. Evaluations on spontaneous Cantonese speech demonstrate significant improvements in character error rate (CER) over Whisper-Large-V3, showing that integrating acoustic cues with large audio-language model (LALM) reasoning is a promising approach for low-resource tonal ASR.

## Method Summary
CantoASR addresses the challenges of Cantonese ASR by combining three key components: forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper model for improved tone discrimination, and an instruction-tuned Qzen-Audio model for prosody-aware correction. The approach leverages the strengths of each component to handle the complexities of Cantonese, including lexical tones and tone sandhi. The model is evaluated on spontaneous Cantonese speech data, where it shows substantial CER improvements compared to baseline models.

## Key Results
- CantoASR achieves significant character error rate (CER) gains over Whisper-Large-V3 on spontaneous Cantonese speech.
- The integration of acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal ASR.
- The framework demonstrates improved tone discrimination and prosody awareness, addressing key challenges in Cantonese ASR.

## Why This Works (Mechanism)
The CantoASR framework works by leveraging forced alignment to extract precise acoustic features, which are then used to fine-tune Whisper for better tone discrimination. The instruction-tuned Qzen-Audio model further refines the output by incorporating prosody-aware correction. This multi-stage approach addresses the limitations of existing models in handling the tonal and prosodic complexities of Cantonese, leading to improved ASR performance.

## Foundational Learning
- **Forced Alignment**: Aligns speech segments with their corresponding transcriptions to extract accurate acoustic features. *Why needed*: Essential for precise tone and prosody analysis. *Quick check*: Verify alignment accuracy on a small sample of Cantonese speech.
- **LoRA-Finetuning**: Adapts pre-trained models to specific tasks with reduced computational cost. *Why needed*: Enables efficient adaptation of Whisper to Cantonese tonal patterns. *Quick check*: Compare CER before and after LoRA-finetuning on a validation set.
- **Instruction-Tuned Qzen-Audio**: Enhances model's ability to correct prosody and tonal errors based on instructions. *Why needed*: Improves handling of tone sandhi and accent variation. *Quick check*: Test model's correction capability on annotated tone sandhi examples.

## Architecture Onboarding
- **Component Map**: Forced Alignment -> LoRA-finetuned Whisper -> Qzen-Audio Correction
- **Critical Path**: The critical path involves extracting acoustic features via forced alignment, fine-tuning Whisper for tone discrimination, and applying Qzen-Audio for prosody-aware correction.
- **Design Tradeoffs**: The framework trades off computational complexity for improved accuracy in tonal and prosodic handling.
- **Failure Signatures**: Potential failures include misalignment in forced alignment, overfitting in LoRA-finetuning, and insufficient correction by Qzen-Audio.
- **First Experiments**: 1) Evaluate forced alignment accuracy on a small Cantonese dataset. 2) Compare CER before and after LoRA-finetuning. 3) Test Qzen-Audio's correction capability on annotated examples.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Scalability of the framework beyond Cantonese is uncertain, with no evidence of cross-linguistic robustness.
- The evaluation dataset composition is not fully disclosed, raising concerns about generalization.
- The ablation study does not explore alternative acoustic feature extraction methods or LALM architectures.

## Confidence
- **High confidence**: The core observation that Whisper struggles with Cantonese tones and that acoustic-prosodic integration can improve performance.
- **Medium confidence**: The specific CantoASR architecture's effectiveness, given limited dataset and domain details.
- **Low confidence**: Generalizability to other tonal languages or spontaneous speech beyond the tested domain.

## Next Checks
1. Evaluate CantoASR on at least two other low-resource tonal languages (e.g., Vietnamese, Thai) to test cross-linguistic robustness.
2. Conduct an ablation study replacing Qzen-Audio with alternative LALMs (e.g., Whisper-JoIP) to isolate the contribution of the LALM component.
3. Perform error analysis on a held-out spontaneous speech subset to quantify improvement in tonal versus non-tonal error categories.