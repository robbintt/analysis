---
ver: rpa2
title: Exploring Fine-Tuning for Tabular Foundation Models
arxiv_id: '2601.09654'
source_url: https://arxiv.org/abs/2601.09654
tags:
- fine-tuning
- tabular
- zero-shot
- learning
- orionmsp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates fine-tuning strategies for Tabular Foundation
  Models (TFMs), comparing zero-shot inference, meta-learning, supervised fine-tuning
  (SFT), and parameter-efficient fine-tuning (PEFT) across multiple benchmarks. Results
  show zero-shot TFMs often outperform or match their fine-tuned variants, with fine-tuning
  benefits being highly model- and data-dependent.
---

# Exploring Fine-Tuning for Tabular Foundation Models

## Quick Facts
- arXiv ID: 2601.09654
- Source URL: https://arxiv.org/abs/2601.09654
- Reference count: 32
- Primary result: Zero-shot inference often matches or exceeds fine-tuned Tabular Foundation Models, with fine-tuning benefits highly model- and data-dependent.

## Executive Summary
This study evaluates fine-tuning strategies for Tabular Foundation Models (TFMs) across multiple benchmarks, comparing zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). Results show zero-shot TFMs frequently match or outperform their fine-tuned variants, with fine-tuning benefits being highly model- and data-dependent. TabPFN gains the most from fine-tuning, especially SFT on medium and wide datasets, while TabICL consistently degrades under SFT. Meta-learning provides modest but reliable gains, and PEFT offers a computationally efficient alternative that recovers most useful improvements. Zero-shot inference remains superior for small datasets and when calibration or fairness stability is important.

## Method Summary
The study benchmarks six TFMs (TabPFN, TabICL, OrionMSP, OrionBiX, TabDPT, Mitra) across three tabular datasets suites (TALENT, OpenML-CC18, TabZilla) using four fine-tuning strategies: zero-shot, meta-learning, SFT, and PEFT with LoRA. Evaluation metrics include accuracy, F1, ECE, MCE, Brier score for calibration, and SPD, EOD, EOpD for fairness. SFT uses AdamW with learning rates 1e-5 to 5e-5 and early stopping; PEFT employs LoRA with rank 8 and alpha 16; meta-learning uses episodic training with 48-512 support samples per episode for 5 epochs.

## Key Results
- Zero-shot inference matches or exceeds fine-tuned performance in 75% of cases
- TabPFN shows consistent improvements under SFT on medium and wide datasets
- TabICL consistently degrades under SFT while benefiting from meta-learning
- PEFT recovers 70-80% of meta-learning gains with lower computational cost
- Zero-shot maintains superior calibration and fairness stability across all dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot in-context learning in TFMs leverages pretrained priors to generalize without parameter updates, often matching or exceeding fine-tuned performance. TFMs encode distributional priors during pretraining (e.g., TabPFN's Bayesian approximation via synthetic data training). At inference, the model conditions on context pairs (x, y) and performs prediction in a single forward pass without gradient updates, preserving the learned prior.

### Mechanism 2
Full supervised fine-tuning degrades performance on small datasets through overfitting and loss of in-context calibration. SFT updates all parameters via gradient descent on limited labeled data. For models with strong priors (TabICL, OrionBiX), this overwrites pretrained representations, reducing generalization. Calibration degrades because probability estimates shift toward training set biases rather than true posterior estimates.

### Mechanism 3
Parameter-efficient fine-tuning (LoRA) preserves pretrained representations while enabling targeted adaptation, recovering most useful gains without full degradation. LoRA constrains updates to low-rank subspaces (rank r=8, α=16), limiting capacity to overwrite core representations. This allows task-specific adjustment while maintaining the majority of frozen weights, reducing overfitting risk and calibration drift.

## Foundational Learning

- **In-Context Learning (ICL)**: Understanding how TFMs generalize without weight updates is essential to interpreting why zero-shot outperforms fine-tuning.
  - Quick check: Can you explain why a model might predict correctly without any gradient updates at test time?

- **Bayesian Inference in Transformers**: TabPFN's mechanism relies on approximate Bayesian posterior estimation; understanding this clarifies why calibration is preserved in zero-shot.
  - Quick check: How does a point estimate differ from a posterior distribution, and why does the latter improve calibration?

- **Low-Rank Adaptation (LoRA)**: PEFT is the primary recommended fine-tuning strategy; understanding LoRA mechanics enables proper implementation.
  - Quick check: Why would constraining weight updates to a low-rank subspace prevent catastrophic forgetting?

## Architecture Onboarding

- **Component map**: Pretrained backbone (frozen weights) -> Context encoder (row/column attention for TabICL, biaxial for OrionBiX) -> Prediction head; LoRA adapters (optional): Inserted into attention layers, trainable low-rank matrices; Meta-learning wrapper: Episodic data loader (support/query splits) -> Episode batching

- **Critical path**: Start with zero-shot inference baseline—no adaptation, measure accuracy + calibration (ECE); If dataset size >1K and accuracy gap exists, try meta-learning (5 epochs, 48–512 support samples); If compute-constrained or SFT degradation observed, apply LoRA (r=8, α=16) with supervised objective; Full SFT only for TabPFN on medium/wide datasets; avoid for TabICL

- **Design tradeoffs**: Zero-shot: Best calibration + fairness stability, no compute overhead, but may underfit on distribution shift; Meta-learning: Preserves ICL, modest gains, but requires episodic training infrastructure; SFT: Highest risk of degradation (especially TabICL, OrionBiX), but potentially largest gains for TabPFN on wide datasets; PEFT: Compromise—recovers ~70–80% of meta-learning gains at lower memory cost

- **Failure signatures**: Accuracy drops >5% after SFT on small datasets → overfitting; revert to zero-shot; ECE increases >0.05 after fine-tuning → calibration collapse; use meta-learning or PEFT; Fairness disparity (SPD) spikes after SFT → representation overwrite; constrain with LoRA

- **First 3 experiments**: Zero-shot baseline on 3 dataset sizes (small <1K, medium 1K–10K, large >10K) measuring accuracy, ECE, and F1; Meta-learning vs. SFT comparison on medium-sized imbalanced dataset (class ratio <0.6), tracking accuracy and SPD; LoRA (r=8) vs. full SFT on wide dataset (>100 features), comparing memory usage and accuracy delta

## Open Questions the Paper Calls Out

- How do fine-tuning strategies impact Tabular Foundation Models on regression tasks compared to the classification benchmarks evaluated? (Basis: Section 3.5 explicitly limits evaluation to classification)

- What specific architectural or pre-training mechanisms cause models like TabICL to consistently degrade under SFT while others like TabPFN benefit? (Basis: Section 3.4 notes divergence but doesn't isolate causal factors)

- Can unified PEFT support for all architectures improve the reliability of fine-tuning for models currently limited to full SFT? (Basis: Section 3.5 states PEFT is not supported for some models)

## Limitations

- Conclusions primarily derived from empirical benchmarking; causal mechanisms remain largely inferred rather than experimentally validated
- Specific mechanism explanations for why zero-shot outperforms fine-tuning lack experimental validation
- Fairness metrics are measured but not deeply analyzed for systematic patterns across fine-tuning strategies

## Confidence

- High confidence: Zero-shot TFMs achieving strong baseline performance; PEFT as computationally efficient alternative; SFT degradation on small datasets
- Medium confidence: Meta-learning providing reliable but modest gains; TabPFN benefiting most from fine-tuning; TabICL degrading under SFT
- Low confidence: Specific mechanism explanations for why zero-shot outperforms fine-tuning; optimal LoRA hyperparameters; fairness stability claims

## Next Checks

1. Conduct ablation studies varying LoRA rank (r=4, 8, 16) and alpha values to identify optimal configuration per model type
2. Perform controlled experiments on synthetic tabular data with known distribution shifts to test zero-shot vs fine-tuned generalization under covariate shift
3. Implement fairness-aware fine-tuning objectives and measure whether targeted approaches can maintain SPD/EOD improvements while achieving accuracy gains