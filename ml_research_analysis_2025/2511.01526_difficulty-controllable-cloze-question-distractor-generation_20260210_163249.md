---
ver: rpa2
title: Difficulty-Controllable Cloze Question Distractor Generation
arxiv_id: '2511.01526'
source_url: https://arxiv.org/abs/2511.01526
tags:
- distractors
- difficulty
- distractor
- answer
- easy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for generating multiple-choice cloze
  distractors with controllable difficulty levels. The approach combines data augmentation
  via a two-way distractor generation process with multitask learning to train a model
  capable of producing both easy and hard distractors.
---

# Difficulty-Controllable Cloze Question Distractor Generation

## Quick Facts
- arXiv ID: 2511.01526
- Source URL: https://arxiv.org/abs/2511.01526
- Reference count: 30
- The method achieves 73.25% accuracy for hard distractors and 64.23% for easy distractors, significantly outperforming GPT-4o in difficulty alignment

## Executive Summary
This paper introduces a method for generating multiple-choice cloze distractors with controllable difficulty levels. The approach combines data augmentation via a two-way distractor generation process with multitask learning to train a model capable of producing both easy and hard distractors. The augmentation pipeline filters and clusters candidates using an ensemble of QA models, enabling precise difficulty control. Experiments show the method significantly outperforms GPT-4o in aligning distractor difficulty with human perception, achieving 73.25% accuracy for hard distractors and 64.23% for easy distractors, with low invalid distractor ratios.

## Method Summary
The proposed method employs a multitask learning framework where a single model learns to generate both easy and hard distractors simultaneously. The key innovation is a data augmentation pipeline that creates training data for both difficulty levels. For hard distractor generation, the model takes a question-answer pair and generates a distractor using a base T5 model, then filters these through an ensemble of QA models to select only those that are semantically related but incorrect. For easy distractor generation, the process is reversed - generating from answer-distractor pairs and filtering through QA models. The augmented data is then used to train a multitask model that can generate distractors at specified difficulty levels.

## Key Results
- The method achieves 73.25% accuracy for hard distractor generation and 64.23% for easy distractors when evaluated against human perception
- FMR (False Match Rate) shows clear separation between easy (0.418) and hard (0.543) distractors
- The approach significantly outperforms GPT-4o, which achieved only 38.5% accuracy in generating hard distractors
- Low invalid distractor ratios (0.052 for hard, 0.017 for easy) indicate high-quality generation

## Why This Works (Mechanism)
The method works by leveraging the complementary strengths of multiple QA models through ensemble filtering. By using different QA models for candidate generation and filtering, the approach captures diverse semantic relationships and ensures distractors are both plausible and appropriately difficult. The multitask learning framework allows the model to learn distinct strategies for easy versus hard distractor generation, with the data augmentation pipeline providing the necessary training signals for each difficulty level.

## Foundational Learning
- **QA Model Ensembles**: Multiple question-answering models working together to filter distractors - needed to capture diverse semantic relationships and ensure robustness against individual model biases; quick check: verify ensemble agreement rates on filtering decisions
- **Distractor Generation Strategies**: Different approaches for easy versus hard distractors - needed because difficulty requires different semantic relationships; quick check: analyze semantic distance distributions between correct answers and distractors at different difficulty levels
- **Data Augmentation for NLP**: Synthetic data creation to expand training sets - needed to provide sufficient examples for both difficulty levels; quick check: compare model performance with and without augmentation

## Architecture Onboarding

Component map: Context -> Question-Answer Pairs -> T5 Generator -> Ensemble QA Models -> Filtered Distractors -> Multitask Model -> Controllable Output

Critical path: The most critical path is the ensemble filtering stage where QA models evaluate candidate distractors. This determines which distractors make it into the training data and directly impacts the quality and difficulty alignment of the final generated distractors.

Design tradeoffs: The approach trades computational cost during training (due to data augmentation and multitask learning) for better difficulty control and quality. Using ensemble models increases robustness but requires multiple pre-trained models and inference passes.

Failure signatures: Common failure modes include: ensemble models agreeing too easily on distractor plausibility (leading to too many candidates), insufficient semantic diversity in generated distractors, and difficulty level misalignment where easy distractors become too hard or vice versa.

First experiments:
1. Test ensemble filtering with varying numbers of QA models to find optimal balance between recall and precision
2. Evaluate distractor generation quality on a held-out validation set using both FMR and human judgment
3. Compare multitask model performance against single-task baselines for both difficulty levels

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the limitations section regarding the generalizability of the approach across different knowledge domains and the long-term effectiveness of automatically generated distractors in educational settings.

## Limitations
- Evaluation relies heavily on automatic metrics (FMR and GMM) rather than comprehensive human judgment for distractor quality beyond difficulty alignment
- The method's dependence on pre-trained QA models for filtering introduces potential biases from those models' training data and capabilities
- Significant computational resources are required for both the augmentation phase and multitask training, limiting practical deployment

## Confidence

High confidence: The method successfully controls difficulty levels as evidenced by FMR and GMM metrics showing clear separation between easy and hard distractors

Medium confidence: Claims of outperforming GPT-4o are supported but depend on the specific evaluation metrics used

Medium confidence: The data augmentation strategy effectively reduces invalid distractors, though the quality of generated distractors beyond difficulty control remains unclear

## Next Checks

1. Conduct a human evaluation study comparing distractor quality, plausibility, and pedagogical effectiveness against both GPT-4o and human-generated distractors across multiple domains

2. Test the method's robustness across different knowledge domains (e.g., science, humanities, mathematics) to assess generalizability beyond the current datasets

3. Perform ablation studies to quantify the contribution of each component (multitask learning, data augmentation, ensemble filtering) to the final performance