---
ver: rpa2
title: 'RE-PO: Robust Enhanced Policy Optimization as a General Framework for LLM
  Alignment'
arxiv_id: '2509.24159'
source_url: https://arxiv.org/abs/2509.24159
tags:
- re-po
- preference
- annotator
- reliability
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from noisy human
  preference data in large language model (LLM) alignment. Standard preference-based
  alignment methods assume all observed labels are equally reliable, but real-world
  preference datasets contain substantial noise from annotator errors, inconsistent
  instructions, and varying expertise, which can degrade model performance.
---

# RE-PO: Robust Enhanced Policy Optimization as a General Framework for LLM Alignment

## Quick Facts
- arXiv ID: 2509.24159
- Source URL: https://arxiv.org/abs/2509.24159
- Authors: Xiaoyang Cao; Zelai Xu; Mo Guang; Kaiwen Long; Michiel A. Bakker; Yu Wang; Chao Yu
- Reference count: 35
- Primary result: RE-PO improves four SOTA alignment algorithms by up to 7.0 percentage points on AlpacaEval 2 win rates by treating label correctness as a latent variable and downweighting likely corrupted preferences.

## Executive Summary
This paper addresses the challenge of learning from noisy human preference data in large language model (LLM) alignment. Standard preference-based alignment methods assume all observed labels are equally reliable, but real-world preference datasets contain substantial noise from annotator errors, inconsistent instructions, and varying expertise, which can degrade model performance. The proposed method, Robust Enhanced Policy Optimization (RE-PO), treats the correctness of each preference label as a latent variable and uses an Expectation-Maximization (EM) algorithm to infer per-label and per-annotator reliabilities. These inferred confidences are then used as adaptive weights in the training loss, allowing the model to downweight likely corrupted labels while emphasizing reliable supervision.

## Method Summary
RE-PO wraps any preference optimization loss (DPO, IPO, SimPO, CPO) with an EM-based denoising framework. It treats label correctness as a binary latent variable and alternates between E-steps (computing posterior confidences for each label) and M-steps (updating policy with weighted loss and annotator reliability). The method works with or without annotator IDs, using a single virtual annotator when IDs are unavailable. RE-PO initializes annotator reliability at 0.9 and updates it via exponential moving average with momentum 0.1.

## Key Results
- RE-PO improves four SOTA alignment algorithms (DPO, IPO, SimPO, CPO) by up to 7.0 percentage points on AlpacaEval 2 win rates.
- Consistent gains observed across two base models (Mistral-7B and Llama-3-8B) on both AlpacaEval and Arena-Hard benchmarks.
- Successfully identifies and downweights noisy labels at the example level on MultiPref dataset with 227 annotators.
- Theoretically proven to recover true annotator reliability under perfect calibration conditions.

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Modeling via EM
RE-PO improves alignment by treating label correctness as a latent variable and inferring posterior confidence scores for each preference pair. The algorithm introduces a binary latent variable $z_i \in \{0,1\}$ indicating whether each observed preference label matches the (unknown) ground-truth preference. An EM loop alternates between (E-step) computing posterior probabilities $w_i$ that each label is correct given current model and annotator reliabilities, and (M-step) using these weights to update the policy and reliability estimates. This allows the model to down-weight likely corrupted labels while emphasizing reliable supervision. The core assumption is that there exists a latent noise-free preference $y_w \succ^* y_l$ for each pair, and observed labels are noisy observations of this ground truth.

### Mechanism 2: Loss-Agnostic Probabilistic Transformation
RE-PO can wrap any preference optimization loss (DPO, IPO, SimPO, CPO) into a robust variant with minimal modification. The framework establishes a probabilistic interpretation for arbitrary preference losses via the Gibbs distribution: $p(y_w \succ^* y_l | x, \theta) \propto \exp(-L_{pref}(x, y_w \succ y_l))$. Normalizing yields Eq. (2), which converts any loss into a proper probability. This enables RE-PO's weighted loss (Eq. 5) to apply universally, as the same $w_i$ weights can modulate the contribution of any base loss. The core assumption is that the exponentiated negative loss defines a valid probability distribution over preferences.

### Mechanism 3: Theoretical Convergence to True Reliability Under Perfect Calibration
Under ideal conditions (perfectly calibrated model, full-batch updates), RE-PO's reliability estimates converge to the true annotator reliability. Theorem 4.1 shows the EM update operator $T_k(\eta)$ has the true reliability $\eta_k^*$ as a fixed point. Since the observed-data log-likelihood is strictly concave (under the assumption that not all ground-truth preference probabilities equal 0.5), EM converges to the unique global maximizerâ€”equivalent to $\eta_k^*$. Empirical verification (Section 5.5) with controlled synthetic noise confirms this tracking behavior even with approximate calibration.

## Foundational Learning

- **Concept**: **Expectation-Maximization (EM) Algorithm**
  - **Why needed here**: RE-PO's core inference relies on EM to handle latent label correctness. Without understanding E-step (posterior inference) and M-step (parameter maximization), the adaptive weighting mechanism is opaque.
  - **Quick check question**: Given a binary latent variable $z$ and observed data $x$, can you derive the E-step posterior $p(z|x, \theta^{(t)})$ and explain why EM increases the observed-data likelihood monotonically?

- **Concept**: **Bradley-Terry Model and Preference Probability**
  - **Why needed here**: DPO and related methods implicitly assume a Bradley-Terry model for preferences. RE-PO's generalization (Eq. 2) subsumes this, so understanding the mapping from rewards to pairwise probabilities is essential.
  - **Quick check question**: For two items with scores $s_1$ and $s_2$, what is the Bradley-Terry probability that item 1 is preferred? How does this relate to DPO's loss formulation?

- **Concept**: **Label Noise Robustness in Classification**
  - **Why needed here**: RE-PO addresses noise in preference labels, a problem analogous to learning with noisy labels in classification. Concepts like loss correction, label smoothing, and confident learning provide context.
  - **Quick check question**: Why does standard cross-entropy loss overfit to corrupted labels? Name one approach (besides re-weighting) that mitigates this in classification settings?

## Architecture Onboarding

- **Component map**: Input dataset -> Probabilistic core (Eq. 2) -> E-step module (Eq. 4) -> M-step policy optimizer (Eq. 5) -> Reliability tracker (Eq. 7)
- **Critical path**: Initialize $\eta_k \leftarrow \eta_0$ (0.9). For each batch: (a) compute preference probabilities via Eq. (2); (b) E-step: compute $w_i$ via Eq. (4); (c) compute weighted loss Eq. (5); (d) backprop and update $\theta$; (e) update $\eta_k$ via EMA Eq. (7). Repeat for $E$ epochs.
- **Design tradeoffs**: Mini-batch vs. full-batch EM (theoretical guarantees vs. scalability); single vs. multi-annotator modeling (simpler vs. granular reliability tracking); initialization of $\eta_0$ (trust vs. adaptability); EMA momentum $\alpha$ (update speed vs. stability).
- **Failure signatures**: Reliability collapse (all $\eta_k$ near 0.5 or oscillating); no improvement over baseline (implementation bug or dataset already low-noise); degraded performance (overly pessimistic $\eta_0$ or miscalibrated E-step); unreasonable reliability estimates (check probability ranges).
- **First 3 experiments**:
  1. Single-annotator RE-DPO on Mistral-7B with UltraFeedback: Implement RE-PO wrapper around DPO loss. Set $K=1$, $\eta_0=0.9$, $\alpha=0.1$. Train for 1 epoch, evaluate on AlpacaEval 2. Expect ~2-4 point LC win rate gain over vanilla DPO.
  2. Ablation on $\eta_0$ and $\alpha$: Train RE-DPO with $\eta_0 \in \{0.55, 0.75, 0.9, 0.99\}$ and $\alpha \in \{0.001, 0.01, 0.1, 0.5, 1.0\}$ on Mistral-7B. Plot AlpacaEval LC vs. hyperparameters. Expect peak at $\eta_0=0.9$, $\alpha=0.1$.
  3. Multi-annotator RE-DPO on MultiPref: Use MultiPref dataset with 227 annotator IDs. Track learned $\hat{\eta}_k$ distribution. Verify majority near 0.85-0.95 with identifiable tail of low-reliability annotators (< 0.7). Compare AlpacaEval performance vs. vanilla DPO; expect ~3-5 point gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical convergence guarantees for RE-PO be extended to settings where the base LLM is significantly misaligned with the ground-truth preference distribution?
- Basis in paper: The Conclusion states, "A natural limitation of our current theory is the assumption of a perfectly calibrated model; extending convergence guarantees to settings where the base model is significantly misaligned remains important future work."
- Why unresolved: The current proof of Theorem 4.1 relies on the assumption that the model distribution matches the ground truth, an idealized condition rarely met in practice.
- What evidence would resolve it: A modified theoretical proof showing convergence bounds under miscalibration, or empirical evidence demonstrating stability when training from a non-instruction-tuned baseline.

### Open Question 2
- Question: Under what conditions of high miscalibration does the E-step assign misleadingly high confidence to incorrect labels, causing the model to reinforce noise rather than denoise it?
- Basis in paper: Section 4 notes that "If the base LLM were initialized in a highly misaligned regime, the E-step could assign misleadingly high confidence to incorrect labels," but the paper only validates on strong instruction-tuned models.
- Why unresolved: The authors empirically observe robustness on Mistral and Llama-3, but they do not define the failure boundary where the EM loop exacerbates alignment errors.
- What evidence would resolve it: Experiments initializing RE-PO with random or adversarially misaligned weights to map the threshold of performance collapse.

### Open Question 3
- Question: How does the accuracy of per-annotator reliability estimation degrade as the number of samples per annotator decreases in large-scale, sparse datasets?
- Basis in paper: Section 5.1 mentions that for main experiments, "datasets do not provide annotator-specific information... we model the preferences as if they originate from a single, virtual annotator," leaving the performance on sparse, multi-annotator data partially under-explored.
- Why unresolved: While the MultiPref experiments show gains, the paper does not analyze how data sparsity affects the stability of the $\eta_k$ updates for individual annotators.
- What evidence would resolve it: A controlled ablation on the MultiPref dataset subsampling labels per annotator to measure the variance of estimated reliabilities.

## Limitations
- Theoretical guarantees assume perfect calibration and full-batch updates, which are rarely met in practice.
- Method assumes annotator reliability is stationary and that preference labels have a well-defined ground truth.
- Current formulation treats all preference pairs independently, ignoring potential contextual dependencies or temporal drift in annotator behavior.

## Confidence

| Claim | Confidence |
|-------|------------|
| Mechanism 1 (EM Latent Variable) | High - mathematically sound with empirical validation |
| Mechanism 2 (Loss-Agnostic Transformation) | Medium - theoretical validity but practical calibration unverified |
| Mechanism 3 (Theoretical Convergence) | Medium - valid under idealized assumptions, practical convergence empirically demonstrated |
| Empirical Claims | High - consistently observed across multiple base methods and datasets |

## Next Checks

1. **Calibration Dependency Test**: Systematically evaluate RE-PO's performance across base models with varying initial calibration levels (e.g., random, pre-trained, post-finetuned) to quantify the impact of the perfect calibration assumption.

2. **Noise Heterogeneity Analysis**: Design experiments with preference data containing multiple noise types (systematic bias, random corruption, context-dependent errors) to assess RE-PO's robustness beyond the controlled synthetic noise used in validation.

3. **Long-Tail Annotator Behavior**: Test RE-PO on preference datasets with extreme annotator reliability distributions (e.g., 1% high-quality annotators vs. 99% low-quality) to verify the method's sensitivity to minority reliable supervision sources.