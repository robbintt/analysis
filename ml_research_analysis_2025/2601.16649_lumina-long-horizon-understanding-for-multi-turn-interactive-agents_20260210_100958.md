---
ver: rpa2
title: 'LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents'
arxiv_id: '2601.16649'
source_url: https://arxiv.org/abs/2601.16649
tags:
- agent
- state
- task
- oracle
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) can effectively
  tackle long-horizon, multi-turn tasks by evaluating the relative importance of three
  key capabilities: planning, state tracking, and history pruning. To systematically
  measure these skills, the authors develop an oracle intervention framework that
  injects perfect information for each capability and assess the resulting performance
  changes.'
---

# LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents

## Quick Facts
- **arXiv ID**: 2601.16649
- **Source URL**: https://arxiv.org/abs/2601.16649
- **Reference count**: 15
- **Primary result**: Oracle intervention framework reveals planning, state tracking, and history pruning have different importance for multi-turn agents depending on model size and environment type

## Executive Summary
This paper investigates how large language models can effectively tackle long-horizon, multi-turn tasks by evaluating the relative importance of three key capabilities: planning, state tracking, and history pruning. To systematically measure these skills, the authors develop an oracle intervention framework that injects perfect information for each capability and assess the resulting performance changes. They introduce procedurally generated game environments—ListWorld, TreeWorld, and GridWorld—that allow precise control over task complexity and enable accurate oracle annotations at every step. Experiments with Qwen3 models (4B-32B) show that while all interventions generally improve success rates, their effectiveness varies with model size and environment type. Larger models benefit more from state tracking, while smaller models gain more from history pruning. Across all settings, the gap between step accuracy and task success highlights the compounding impact of occasional errors in long-horizon reasoning. These findings guide future efforts to enhance LLM agents for complex, interactive, multi-turn scenarios.

## Method Summary
The authors evaluate LLM agents on long-horizon, multi-turn tasks using an oracle intervention framework. They develop three procedurally generated environments (ListWorld, TreeWorld, GridWorld) that allow precise control over task complexity and enable accurate oracle annotations at every step. Using Qwen3 models (4B-32B) with ReAct prompting, they systematically inject perfect information for planning, state tracking, and history pruning capabilities. The framework measures performance improvements when each capability is augmented, revealing how different model scales benefit from different intervention types. The method isolates capability contributions while controlling for task complexity and horizon length.

## Key Results
- All oracle interventions generally improve success rates across model sizes and environments
- Larger models (14B-32B) benefit more from state tracking interventions, while smaller models (4B-8B) gain more from history pruning
- The gap between step accuracy and task success rate highlights compounding errors in long-horizon reasoning
- ListWorld demonstrates irrecoverability where single errors cause immediate failure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting perfect planning information improves success rates consistently across model sizes and environments.
- Mechanism: The planning oracle (O_plan) provides a description of a single optimal subtask at each step, eliminating the need for multi-step lookahead reasoning. The model receives the immediate correct direction without computing the full trajectory itself.
- Core assumption: LLMs can execute single-step reasoning more reliably than multi-step planning.
- Evidence anchors:
  - [abstract] "experiments with Qwen3 models show that while all interventions generally improve success rates, their effectiveness varies with model size"
  - [Section 4.4] "The addition of each intervention improves the success rate in most cases"
  - [corpus] Limited direct corpus support; related work (RAGEN, AgentGym-RL) addresses multi-turn RL training but not oracle-based intervention analysis
- Break condition: If planning oracle instructions are ignored or misinterpreted due to poor instruction-following capability, gains diminish.

### Mechanism 2
- Claim: State tracking interventions benefit larger models more, while history pruning helps smaller models.
- Mechanism: Larger models (14B-32B) have sufficient capacity to leverage explicit state information (O_state) when provided, improving their ability to maintain coherent beliefs over long horizons. Smaller models (4B-8B) suffer more from context dilution and distractor accumulation, so removing irrelevant history (O_history) provides greater relative gain.
- Core assumption: Model capacity determines which bottleneck dominates—state inference vs. attention to relevant context.
- Evidence anchors:
  - [Section 4.4] "Larger models tend to benefit more from state tracking. This indicates that improving these capabilities in models will have largely different impact at different model scales"
  - [Section 4.4] "History pruning is effective for the 4B and 8B models, while the 14B and 32B even suffer from this removal"
  - [corpus] No direct corpus corroboration for model-size-specific intervention effects
- Break condition: If environments require historical reasoning (e.g., GridWorld navigation benefits from trajectory memory), pruning harms even small models.

### Mechanism 3
- Claim: Success in long-horizon tasks requires near-perfect step accuracy due to compounding errors.
- Mechanism: Multi-turn tasks concatenate dependent decisions. Even with 90% per-step accuracy, a 10-step task succeeds only ~35% of the time (0.9^10). Some environments (ListWorld) are irrecoverable—single errors cause immediate failure.
- Core assumption: Steps are not independent; error probability compounds multiplicatively.
- Evidence anchors:
  - [Section 4.4] "The step accuracy is larger than the task success rate, and even in cases where the success rate is almost zero, a step accuracy above 60% indicates that the majority of the steps are correct"
  - [Section 4.4] "In fact, environments like ListWorld are irrecoverable and a single step leads to failure"
  - [corpus] Process-Supervised RL paper addresses similar sparse-reward challenges in multi-turn settings
- Break condition: If tasks allow retries or partial credit, compounding is less severe.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper formalizes all environments as POMDPs with hidden states S, observations O, and transitions T. Understanding belief states and Markov properties is essential for interpreting why state tracking oracles help.
  - Quick check question: Given an agent's action history, can you compute the current belief state?

- **Concept: ReAct Prompting**
  - Why needed here: The base policy uses ReAct framework interleaving reasoning traces with actions. This is the baseline against which oracle-augmented policies are compared.
  - Quick check question: How does separating reasoning text from action execution affect multi-step task performance?

- **Concept: Success Rate vs. Step Accuracy Metrics**
  - Why needed here: The key finding is the large gap between these metrics. Step accuracy measures per-action optimality; success rate measures trajectory completion. Understanding this distinction is critical for interpreting compounding error effects.
  - Quick check question: If step accuracy is 85% on a 15-step task, what is the theoretical maximum success rate assuming independent errors?

## Architecture Onboarding

- **Component map:**
  - Environment Generator → Task Instance (procedurally generated with tunable complexity)
  - LLM Agent Policy π_θ → Actions via ReAct prompting
  - Oracle Module → Augments/Prunes history h_t into h̃_t before each action
  - Evaluator → Computes success rate and step accuracy from trajectories

- **Critical path:**
  1. Define POMDP environment with known optimal policy (required for oracle construction)
  2. Implement oracle interventions as context modifications (O_plan, O_state, O_history)
  3. Run rollouts with/without interventions, controlling for horizon and complexity
  4. Compare success rate deltas to isolate capability contributions

- **Design tradeoffs:**
  - Procedural environments enable precise oracle annotations but may not reflect real-world task distributions
  - History pruning requires state tracking to be active (pruning without state makes recovery impossible)
  - In-context examples must match oracle-augmented format to avoid distribution shift

- **Failure signatures:**
  - High step accuracy + low success rate → compounding errors, likely need error recovery mechanisms
  - Planning oracle helps minimally → model already has strong planning or instruction-following is weak
  - History pruning hurts performance → environment requires temporal context (e.g., spatial navigation)

- **First 3 experiments:**
  1. Baseline characterization: Run all three environments across model sizes (4B-32B) without interventions to establish success rate vs. step accuracy gaps at varying horizons.
  2. Single-oracle ablation: Activate each oracle independently (P, S, H) and measure success rate improvement per model size. Identify which capability is the primary bottleneck for each scale.
  3. Oracle combination analysis: Test all six configurations (base, P, S, P+S, S+H, P+S+H) on ListWorld to verify that small models benefit most from H while large models benefit most from S, as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or dynamic selection of oracle interventions, based on real-time assessment of model scale and environment properties, further improve multi-turn agent performance?
- Basis in paper: [explicit] The authors find that "the effectiveness of each skill is also significantly influenced by the model size and the environment," and larger models benefit more from state tracking while smaller models gain more from history pruning.
- Why unresolved: The current study applies interventions uniformly; no mechanism exists to dynamically switch or weight interventions based on model characteristics or environment type.
- What evidence would resolve it: Experiments showing that a meta-controller selecting interventions per-context outperforms fixed intervention configurations across model sizes.

### Open Question 2
- Question: How can the oracle counterfactual framework be extended to real-world agentic benchmarks where ground-truth optimal trajectories are difficult or impossible to annotate?
- Basis in paper: [explicit] The Limitations section states: "annotating oracle is often intractable or ill-defined" for real-world applications, confining analysis to programmatically-generated environments.
- Why unresolved: Real-world tasks often admit multiple valid solutions and lack clear optimal policies, making oracle construction fundamentally harder.
- What evidence would resolve it: A method for approximate or probabilistic oracle annotations on existing benchmarks (e.g., AppWorld, VisualWebArena) that preserves the causal interpretability of the intervention framework.

### Open Question 3
- Question: What architectural or training interventions could reduce the compounding error problem where near-perfect step accuracy still yields low task success?
- Basis in paper: [explicit] The authors highlight "the compounding impact of occasional errors in long-horizon reasoning" and note that even 60%+ step accuracy can correspond to near-zero success rates as horizon increases.
- Why unresolved: The paper quantifies the problem but does not propose solutions for error recovery or graceful degradation in multi-turn settings.
- What evidence would resolve it: Demonstrating that error-correction mechanisms (e.g., self-verification, rollback strategies) significantly narrow the step-accuracy-to-success gap in long-horizon tasks.

### Open Question 4
- Question: How sensitive are the observed intervention effects to prompt engineering choices, and can prompt-agnostic training mitigate this dependency?
- Basis in paper: [explicit] The Limitations section notes: "performance are also influenced by the prompt itself and it is unlikely to find one prompt construction that is optimal for all models, sizes, and environments."
- Why unresolved: The current findings may conflate model capability limitations with prompt-model mismatches, limiting generalizability.
- What evidence would resolve it: Experiments showing consistent intervention effect rankings across diverse prompt styles, or demonstrating that fine-tuning on oracle-augmented data reduces prompt sensitivity.

## Limitations

- Procedurally generated environments may not capture real-world task complexity and noise
- Oracle annotations are intractable or ill-defined for many real-world applications
- Results may be sensitive to specific prompting strategies and model architectures
- Limited exploration of what constitutes optimal state representations for different domains

## Confidence

**High Confidence Claims:**
- The experimental framework for measuring capability contributions through oracle interventions is sound and well-implemented
- The observation that step accuracy consistently exceeds task success rate across all settings
- The general pattern that larger models benefit more from state tracking while smaller models benefit more from history pruning

**Medium Confidence Claims:**
- The relative importance rankings of planning vs state tracking vs history pruning (though consistently directional, magnitude varies with environment and scale)
- The irrecoverability of ListWorld and its impact on compounding errors (supported by design but limited empirical validation across domains)
- The effectiveness of history pruning for small models (consistent pattern but could be environment-dependent)

**Low Confidence Claims:**
- Extrapolation of findings to non-procedural environments and real-world applications
- Generalizability across different model families, prompting strategies, or implementation details
- Optimal intervention combinations for specific task types beyond the studied environments

## Next Checks

1. **Cross-Environment Generalization Test**: Validate the framework on one additional domain (e.g., a text-based adventure game or real API interaction task) to assess whether the observed patterns of capability importance transfer beyond procedural environments.

2. **Prompting Strategy Ablation**: Systematically vary prompting approaches (temperature, chain-of-thought vs non-thinking, different few-shot examples) while keeping oracle interventions constant to isolate the contribution of prompting strategy from capability effects.

3. **Real-World Oracle Application**: Implement a simplified version of state tracking and history pruning for a practical task (e.g., multi-step web navigation or document analysis) without perfect oracle access, using heuristic approximations, to test whether the insights about capability bottlenecks hold in realistic settings.