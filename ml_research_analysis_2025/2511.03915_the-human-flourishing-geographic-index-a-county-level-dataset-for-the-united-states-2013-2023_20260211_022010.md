---
ver: rpa2
title: 'The Human Flourishing Geographic Index: A County-Level Dataset for the United
  States, 2013--2023'
arxiv_id: '2511.03915'
source_url: https://arxiv.org/abs/2511.03915
tags:
- tweets
- flourishing
- high
- indicators
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces the Human Flourishing Geographic Index (HFGI),\
  \ a county- and state-level dataset derived from 2.6 billion geolocated U.S. tweets\
  \ (2013\u20132023)."
---

# The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023

## Quick Facts
- **arXiv ID:** 2511.03915
- **Source URL:** https://arxiv.org/abs/2511.03915
- **Reference count:** 40
- **Primary result:** 2.6 billion geolocated U.S. tweets (2013–2023) classified across 48 well-being dimensions; validation shows correlations up to r = 0.91 with external benchmarks

## Executive Summary
This study introduces the Human Flourishing Geographic Index (HFGI), a county- and state-level dataset derived from 2.6 billion geolocated U.S. tweets (2013–2023). Using fine-tuned large language models, the researchers classified tweets across 48 well-being dimensions aligned with Harvard's Global Flourishing Study framework, plus attitudes toward migration and perceptions of corruption. The resulting indicators—calculated monthly and yearly—capture expressed well-being discourse at unprecedented spatial and temporal resolution. Validation showed strong alignment with established measures, including CDC mental health data and Transparency International's Corruption Perceptions Index, with correlations as high as r = 0.91. The dataset offers researchers a powerful tool to analyze well-being, inequality, and social change through the lens of social media discourse.

## Method Summary
The HFGI was created by fine-tuning Llama 3.2 3B on 2,404 human-coded tweets to classify 2.6 billion geolocated U.S. tweets (2013–2023) across 48 well-being dimensions. The model outputs JSON with dimension scores (low/medium/high/absent), which are converted to numerical values (-1, 0.5, +1, 0). Indicators are aggregated to county/month cells using conditional means (excluding zeros), with salience tracking topic prevalence. The entire pipeline—from spatial attribution through LLM inference to DuckDB aggregation—was executed at scale using spare GPU cycles across multiple research computing clusters.

## Key Results
- Fine-tuned Llama 3.2 3B achieves 0.79–0.88 accuracy across 48 dimensions, approaching ChatGPT-4 performance
- HFGI indicators show strong external validation, with correlations up to r = 0.91 with CDC mental health data
- County-level indicators capture expression propensity rather than population prevalence, revealing systematic rural-urban differences in well-being discourse

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned LLMs can classify short-form social media text across 48 well-being dimensions with competitive accuracy. The authors fine-tune Llama 3.2 3B using 2,404 human-coded tweets, teaching the model to identify conceptual constructs rather than relying on keyword matching. The model outputs a compact JSON with only applicable dimensions rated low/medium/high.

Core assumption: Fine-tuning on a modest labeled dataset transfers to billions of unseen tweets without significant distributional drift; semantic understanding generalizes across 2013–2023 language evolution.

Evidence anchors:
- [section 2.3] "with appropriate fine-tuning, smaller models can perform competitively with their larger counterparts"
- [table 3] Fine-tuned Llama 3.2 3B achieves 0.79–0.88 accuracy, approaching ChatGPT-4 (0.86–0.94) despite 587× fewer parameters
- [corpus] Related work (Iacus & Porro 2021, 2022; Carammia et al. 2024) supports fine-tuned LLMs for social-media well-being extraction

Break condition: If concept drift or sarcasm/irony degrades classification meaningfully post-2023; if fine-tuning dataset lacks diversity across demographics/regions.

### Mechanism 2
Aggregating classified tweets to county/month cells produces statistically stable indicators that correlate with external benchmarks. Each tweet receives scores in {-1, 0, +0.5, +1} across dimensions. Zeros indicate absence; conditional means are computed only over tweets where the dimension is present, preserving signal-to-noise while enabling DuckDB vectorized aggregation at scale.

Core assumption: Sufficient tweet volume per cell (controlled via validtweets) yields reliable estimates; spatial/temporal aggregation averages out individual-level noise.

Evidence anchors:
- [abstract] "Validation showed strong alignment with established measures, including CDC mental health data and Transparency International's Corruption Perceptions Index, with correlations as high as r = 0.91"
- [section 2.4] Indicators computed as conditional means; salience captures prevalence
- [corpus] TSGI dataset (Chai et al. 2023) uses similar aggregation logic; neighbor paper shows rural-urban patterns replicable at county level

Break condition: If low-tweet counties introduce noise that overwhelms signal; recommend minimum validtweets threshold.

### Mechanism 3
HFGI captures expression propensity, not population prevalence—systematically modulated by stigma, connectivity, and local norms. Rural counties show higher CDC-reported mental distress but lower online depression discourse. The authors model this as HFGI = f(prevalence × expression propensity), where rurality suppresses visibility via stigma and lower broadband access.

Core assumption: Expression behavior follows communication/stigma theory; the gap between HFGI and survey data is informative, not error.

Evidence anchors:
- [section 6.2.3] Joint model with CDC FMD explains R² = 0.91; positive interaction between rural distress and online expression
- [section 7.5] "HFGI measures the propensity of public expression for a concept, not clinical or survey prevalence"
- [corpus] Limited direct corpus evidence on expression-propensity modeling; primarily paper-internal theoretical framing

Break condition: If expression gaps reflect platform demographics (Twitter users ≠ general population) more than stigma/connectivity; recommend controlling for broadband and demographic composition.

## Foundational Learning

- **Concept:** Multi-label classification with asymmetric error costs
  - Why needed here: Each tweet can express multiple dimensions simultaneously; false positives (over-classification) and false negatives (under-classification) have different impacts on aggregated indicators.
  - Quick check question: Can you explain why the authors track both Jaccard Index and Hamming Loss in Table 3?

- **Concept:** Conditional aggregation and salience
  - Why needed here: Zeros indicate absence, not neutral sentiment; aggregating only over validtweets avoids diluting signal while salience tracks how often topics arise.
  - Quick check question: If a county has 10,000 total tweets but only 50 mention "purpose," how should you weight the purpose indicator?

- **Concept:** Expression propensity vs. prevalence
  - Why needed here: Interpreting HFGI requires understanding that observed discourse = underlying state × willingness/ability to express publicly.
  - Quick check question: Why might rural areas show higher life satisfaction in HFGI but higher mental distress in CDC data?

## Architecture Onboarding

- **Component map:** Harvard CGA Geotweet Archive v2.0 -> Spatial enrichment via TIGER/Line Shapefiles -> Fine-tuned Llama 3.2 3B classification -> JSON output per tweet -> DuckDB vectorized aggregation -> county/state × month/year indicators -> External validation layer

- **Critical path:**
  1. Spatial attribution (GPS or bounding-box centroid with error estimate)
  2. LLM inference across 48 dimensions per tweet
  3. Aggregation to cells with validtweets and salience computation
  4. External validation for construct validity

- **Design tradeoffs:**
  - Model size: 3B parameters chosen for scalability over accuracy; ~10% accuracy gap vs. ChatGPT-4
  - Temporal resolution: Monthly/yearly (not daily) to protect privacy and ensure cell volume
  - Spatial resolution: County-level preferred; GPS uncertainty larger for Place-based tweets

- **Failure signatures:**
  - Low validtweets in sparse counties → unstable indicators (flag via stat_se)
  - Platform policy shift post-2019 → GPS-to-Place ratio changes (include year fixed effects)
  - Bot/institutional accounts → may inflate certain dimensions (no explicit filtering)

- **First 3 experiments:**
  1. Sanity check: Replicate happiness–TSGI correlation (r ≈ 0.62) using Dataverse data; verify scale alignment.
  2. Robustness test: Apply minimum validtweets threshold (e.g., ≥30) and compare rural-urban coefficient stability.
  3. Drift analysis: Compare pre/post-2019 indicator distributions for top-5 salient dimensions; quantify policy-change effects.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do coherent regional structures underlie spatial patterns in low-salience civic discourse (e.g., corruption), or do observed clusters reflect stochastic variation and episodic media cycles? The paper provides descriptive maps but no formal spatial clustering tests or longitudinal tracking against local media/political events.

- **Open Question 2:** What causal pathways link environmental hazard exposure to flourishing outcomes, given the observed negative correlations between resilience-adjusted climate risk and subjective well-being? Cross-sectional correlations cannot disentangle whether climate risk reduces flourishing, whether less flourishing populations reside in higher-risk areas, or whether unobserved factors drive both.

- **Open Question 3:** How can researchers systematically correct for the expression-prevalence gap in social-media well-being indicators, particularly across contexts differing in stigma, digital access, and disclosure norms? The paper theorizes that HFGI captures "expression propensity rather than population prevalence" and demonstrates a rural-urban divergence, but does not provide a validated correction methodology.

- **Open Question 4:** Can demographic post-stratification or alternative weighting schemes improve the population representativeness of geotagged social media indicators? The dataset relies on 1–2% geotagged tweets from a non-representative user base, and Twitter metadata lacks reliable demographic fields.

## Limitations
- Fine-tuning hyperparameters (learning rate, epochs, LoRA configuration) were not reported, limiting exact replication
- Twitter demographic representativeness is assumed but not systematically validated
- Temporal stability of classification accuracy across the 2013–2023 period is not explicitly tested

## Confidence
- **High confidence:** Model classification accuracy (0.79-0.88) is well-documented through systematic testing against human-coded tweets
- **Medium confidence:** External validation correlations (r=0.91 with CDC mental health data) are robust, but may reflect shared measurement artifacts
- **Medium confidence:** The expression propensity framework is theoretically sound but relies on limited empirical testing

## Next Checks
1. Perform k-fold validation on the 4,581 human-coded tweets to assess model robustness across different training/test splits
2. Compare HFGI indicator distributions against county-level demographic variables (age, income, broadband access) to quantify systematic biases
3. Segment the 2013-2023 period into 3-year intervals and measure classification accuracy drift for the top 5 salient dimensions in each interval