---
ver: rpa2
title: A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation
  on TikTok
arxiv_id: '2502.06893'
source_url: https://arxiv.org/abs/2502.06893
tags:
- disinformation
- high
- multimodal
- detection
- medium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid intelligent approach for detecting
  suspected disinformation on TikTok by combining deep learning and fuzzy logic. The
  method extracts multimodal features (text, audio, video) using advanced techniques
  like speech recognition, emotion detection, and facial analysis, then applies fuzzy
  logic to interpret these features and assess disinformation suspicion based on psychosociological
  traits, particularly the Big-5 model.
---

# A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok

## Quick Facts
- arXiv ID: 2502.06893
- Source URL: https://arxiv.org/abs/2502.06893
- Reference count: 40
- Primary result: 18/19 correct classifications for context-specific disinformation detection

## Executive Summary
This study presents a hybrid intelligent approach for detecting suspected disinformation on TikTok by combining deep learning and fuzzy logic. The method extracts multimodal features (text, audio, video) using advanced techniques like speech recognition, emotion detection, and facial analysis, then applies fuzzy logic to interpret these features and assess disinformation suspicion based on psychosociological traits, particularly the Big-5 model. Two experiments were conducted: one on context-specific disinformation (achieving 18/19 correct classifications on topics like COVID-19 and political incidents) and another on widespread disinformation about the Ukraine invasion (classifying 5,172 videos into five suspicion levels: 16.1% low, 21.8% low/medium, 25.5% medium, 22% medium/high, 14.7% high). The system generates comprehensive, explainable reports detailing behavioral indicators of disinformation. The approach demonstrates robust detection capabilities across different contexts while providing interpretable results. Future work includes expanding to other platforms and improving deepfake detection.

## Method Summary
The hybrid approach combines deep learning feature extraction with fuzzy logic inference for multimodal disinformation detection. The system extracts text features from captions and speech recognition, audio features through emotion detection and acoustic analysis, and video features including facial expressions, scene composition, and object recognition. These features are processed through a fuzzy inference system that interprets them through the lens of psychosociological traits, particularly the Big-5 personality model. The system generates five-level suspicion classifications and produces detailed reports explaining the behavioral indicators that contributed to each assessment. The approach was validated through two experiments: one focusing on context-specific disinformation topics and another analyzing widespread disinformation content about the Ukraine invasion.

## Key Results
- Achieved 18/19 correct classifications in context-specific disinformation detection experiment
- Classified 5,172 videos about Ukraine invasion into five suspicion levels with distribution: 16.1% low, 21.8% low/medium, 25.5% medium, 22% medium/high, 14.7% high
- Generated comprehensive, explainable reports detailing behavioral indicators of disinformation
- Demonstrated robust detection capabilities across different disinformation contexts

## Why This Works (Mechanism)
The hybrid approach succeeds by integrating complementary detection modalities that capture different aspects of disinformation characteristics. Deep learning excels at extracting complex patterns from multimodal content (text, audio, video) that humans might miss, while fuzzy logic provides interpretable reasoning that connects these features to psychosociological traits associated with disinformation behavior. The Big-5 personality model framework offers a structured way to interpret behavioral patterns, making the system's decisions more explainable and actionable for fact-checkers.

## Foundational Learning
- Multimodal feature extraction: Essential for capturing disinformation indicators across text, audio, and video channels; quick check: verify each modality's feature quality independently
- Fuzzy logic inference: Enables interpretable reasoning under uncertainty; quick check: validate rule base coverage for edge cases
- Psychosociological framework: Provides theoretical grounding for behavioral interpretation; quick check: assess correlation between detected traits and known disinformation patterns
- Deep learning feature engineering: Extracts complex patterns from raw media; quick check: benchmark feature extraction against baseline models
- Disinformation classification framework: Structured approach to categorizing suspicion levels; quick check: validate threshold determination methodology

## Architecture Onboarding

**Component Map:**
Data Collection -> Feature Extraction -> Fuzzy Inference Engine -> Classification -> Report Generation

**Critical Path:**
Data Collection → Feature Extraction → Fuzzy Inference Engine → Classification → Report Generation

**Design Tradeoffs:**
The system prioritizes interpretability over pure performance by using fuzzy logic, enabling human-understandable explanations at the cost of some classification precision. The multimodal approach captures richer signals but increases computational complexity and requires careful feature fusion.

**Failure Signatures:**
- High false positives may indicate overly sensitive fuzzy rules or misalignment between extracted features and disinformation indicators
- Low detection rates could suggest insufficient feature extraction quality or inadequate psychosociological rule coverage
- System instability may result from inconsistent feature normalization across different video qualities

**First 3 Experiments to Run:**
1. Validate feature extraction quality by testing each modality independently on labeled disinformation samples
2. Perform sensitivity analysis on fuzzy rule parameters to optimize classification thresholds
3. Conduct cross-validation using videos from different TikTok creators to test generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of external validation on independent datasets limits generalization claims
- Psychosociological framework based on Big-5 traits lacks empirical validation connecting personality dimensions to actual disinformation behavior
- Five-level suspicion classification appears subjective without clear methodology for threshold determination

## Confidence

**High confidence:** Feature extraction techniques (speech recognition, emotion detection, facial analysis) are well-established and properly applied

**Medium confidence:** The multimodal fusion approach combining deep learning with fuzzy logic is methodologically sound but lacks comparative validation

**Low confidence:** The psychosociological interpretation of Big-5 traits for disinformation detection remains theoretical without behavioral validation

## Next Checks

1. Conduct cross-platform validation testing the system on YouTube, Facebook, and Instagram using identical detection parameters
2. Perform blind testing with human fact-checkers comparing system classifications against expert assessments on randomly selected videos
3. Implement A/B testing comparing the hybrid approach against single-modal detection systems using standardized disinformation datasets