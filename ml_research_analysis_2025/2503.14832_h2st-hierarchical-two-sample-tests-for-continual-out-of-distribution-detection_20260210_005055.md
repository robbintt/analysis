---
ver: rpa2
title: 'H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection'
arxiv_id: '2503.14832'
source_url: https://arxiv.org/abs/2503.14832
tags:
- detection
- samples
- performance
- h2st
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Two-Sample Tests (H2ST) for
  continual out-of-distribution (OOD) detection in task incremental learning (TIL)
  under open-world conditions. Unlike existing methods that rely on model outputs
  and require threshold selection, H2ST uses hierarchical feature-level two-sample
  tests with task-specific classifiers to detect OOD samples while identifying task
  IDs.
---

# H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2503.14832
- Source URL: https://arxiv.org/abs/2503.14832
- Authors: Yuhang Liu; Wenjie Zhao; Yunhui Guo
- Reference count: 40
- One-line primary result: H2ST achieves up to 92.03% F1 score and 93.78% task ID accuracy on CIFAR-10, significantly outperforming baselines.

## Executive Summary
This paper introduces Hierarchical Two-Sample Tests (H2ST) for continual out-of-distribution (OOD) detection in task incremental learning (TIL) under open-world conditions. Unlike existing methods that rely on model outputs and require threshold selection, H2ST uses hierarchical feature-level two-sample tests with task-specific classifiers to detect OOD samples while identifying task IDs. The method eliminates threshold dependency through statistical hypothesis testing and reduces computational overhead via early-exit mechanisms. Experiments on multiple datasets (MNIST, CIFAR-10, CIFAR-100, Mini-ImageNet, CoRe50, Stream-51) demonstrate H2ST's superiority over existing methods, achieving up to 92.03% F1 score and 93.78% task ID accuracy on CIFAR-10, significantly outperforming baselines.

## Method Summary
H2ST implements hierarchical two-sample tests for continual OOD detection in task incremental learning. The method uses T cascaded source-target classifiers, each performing binary classification on feature maps extracted from the TIL backbone. For each incoming sample, the system traverses the hierarchy, updating classifiers online and using Clopper-Pearson intervals to test whether the sample matches the distribution of the current task. If the test fails to reject the null hypothesis (indicating the sample is in-distribution for that task), the process exits early with the task ID. Otherwise, it continues to the next layer. The method requires replay-based TIL with memory buffers to store exemplars for each learned task.

## Key Results
- Achieves 92.03% F1 score and 93.78% task ID accuracy on CIFAR-10
- Outperforms baselines including MSP, Energy, ODIN, MaxLogits, Gentropy, FeatureNorm, and MORE
- Reduces computational overhead through early-exit mechanism, avoiding processing all classifiers for every sample
- Demonstrates effectiveness across multiple datasets: MNIST, SVHN, CIFAR-10, CIFAR-100, Mini-ImageNet, CoRe50, and Stream-51

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical architecture reduces detection latency and simplifies decision boundaries for later tasks compared to flat or parallel classifier ensembles. H2ST arranges T source-target classifiers sequentially, allowing early-exit when a sample is identified as In-Distribution (ID) at layer j. This prevents the sample from needing evaluation against all T tasks and simplifies the discrimination problem as the hierarchy deepens.

### Mechanism 2
Utilizing feature maps rather than logits or raw pixels for two-sample tests mitigates dependence on the final classifier's confidence and captures higher-quality semantic information. H2ST extracts intermediate feature maps that serve as input to the auxiliary binary classifiers, allowing the detector to leverage the representation learning capability of the backbone even if the final linear layer's outputs are noisy.

### Mechanism 3
Hypothesis testing via Clopper-Pearson intervals eliminates the need for manual threshold selection. H2ST frames detection as a statistical test, checking if the accuracy of the binary classifier (distinguishing source buffer data from test data) is statistically significantly different from random chance (0.5). If the 0.5 value falls outside the calculated interval, a distribution shift is detected.

## Foundational Learning

- **Concept: Classifier Two-Sample Tests (C2ST)**
  - **Why needed here:** This is the statistical engine of the paper. Unlike standard OOD scoring, C2ST trains a classifier to distinguish if two sets of data (Source vs. Target) come from the same distribution. If the classifier can succeed (accuracy > 50%), they are different.
  - **Quick check question:** Can you explain why a classifier accuracy of 50% implies two distributions are identical, whereas 100% implies they are perfectly separable?

- **Concept: Clopper-Pearson Interval**
  - **Why needed here:** This provides the "threshold-free" confidence bounds. It converts a raw accuracy metric (from the C2ST) into a statistical decision (reject/accept null hypothesis) with a defined significance level α.
  - **Quick check question:** How does increasing the window size w affect the width of the confidence interval and the sensitivity of the detection?

- **Concept: Replay-based Continual Learning**
  - **Why needed here:** H2ST is explicitly designed to be compatible with methods like ER and GEM. It relies on the existence of a memory buffer Bj to draw "source" samples for comparison against the incoming "target" stream.
  - **Quick check question:** If the memory buffer Bj is corrupted or unrepresentative of task j, how would the Type I/II error rates of H2ST change?

## Architecture Onboarding

- **Component map:**
  TIL Backbone (fθ) -> Feature Maps (ψ(x)) -> Memory Buffers (B1...BT) -> Source-Target Classifiers (g1...gT) -> Statistical Engine (Clopper-Pearson Intervals)

- **Critical path:**
  1. Input: Sample x' arrives
  2. Feature Extraction: Backbone outputs ψ(x')
  3. Hierarchical Loop (Layer j):
     - Draw sample xj from Buffer Bj
     - Classifier gj predicts labels for ψ(xj) (Source) and ψ(x') (Target)
     - Update gj online (Source=0, Target=1)
     - Calculate rolling accuracy μ̂ over window w
     - Calculate CP Interval ΘCP
     - Decision: If 0.5 ∈ ΘCP (cannot distinguish), return ID, Task-ID=j. Else, continue to j+1
  4. Fallback: If all layers reject, return OOD

- **Design tradeoffs:**
  - Memory Size (M): Low memory causes detection collapse due to biased source sampling, but returns diminish at high M
  - Classifier Complexity: Deep classifiers (MLP-III) perform worse than shallow ones (MLP-I) due to overfitting in online update regime
  - Window Size (w): Larger w increases statistical reliability but slows reaction time to sudden distribution shifts

- **Failure signatures:**
  - High False Positives (ID predicted as OOD): Likely occurs if TIL backbone has undergone catastrophic forgetting
  - High False Negatives (OOD predicted as ID at wrong layer): Occurs if hierarchical order is suboptimal or OOD samples align with early task feature manifolds
  - CP interval too conservative (all samples flagged OOD) or too loose (no OOD detected): Verify μ̂ values and CP interval calculations per layer

- **First 3 experiments:**
  1. Latency vs. Accuracy (H2ST vs. C2ST): Measure inference time as T scales from 5 to 50 tasks
  2. Memory Sensitivity Stress Test: Run H2ST on CIFAR-100 with varying buffer sizes (M=10, 50, 100, 200)
  3. Feature vs. Logit Ablation: Modify input to classifiers gj to use penultimate layer logits instead of feature map ψ(x)

## Open Questions the Paper Calls Out

### Open Question 1
Can the H2ST framework be adapted for non-replay TIL methods that lack a memory buffer? The statistical validity of the two-sample test relies on comparing target samples to a stored representation of the source; without a memory buffer, the method lacks a baseline for the hypothesis test.

### Open Question 2
How can the trade-off between OOD detection sensitivity and catastrophic forgetting be explicitly optimized? While H2ST improves detection (F1), it results in higher forgetting (FT) compared to methods with poorer detection, creating a conflict between the two primary goals of open-world TIL.

### Open Question 3
Is there a data-driven criterion for determining the optimal memory size to balance detection performance against storage overhead? The paper observes that while metrics improve with size, they show diminishing returns at larger sizes.

## Limitations
- The method's effectiveness depends heavily on the assumption that early tasks have sufficiently distinct feature distributions to enable reliable early-exit detection
- Computational overhead of maintaining T separate classifiers and statistical calibration requirements for Clopper-Pearson intervals in high-dimensional feature spaces remain under-examined
- The paper does not thoroughly explore failure modes when hierarchical assumptions break down, particularly in cases of semantic similarity between tasks

## Confidence
- **High Confidence**: Claims about eliminating threshold dependency through statistical hypothesis testing and the hierarchical architecture's ability to reduce computational overhead via early-exit mechanisms
- **Medium Confidence**: Claims about superiority over existing methods (F1 score of 92.03% on CIFAR-10) and the effectiveness of feature maps versus logits
- **Low Confidence**: Claims about the method's robustness to catastrophic forgetting and its performance in scenarios with highly similar task distributions

## Next Checks
1. **Catastrophic Forgetting Stress Test**: Evaluate H2ST's performance when the TIL backbone undergoes severe catastrophic forgetting by intentionally degrading the feature extractor's performance between tasks
2. **Hierarchical Order Sensitivity Analysis**: Systematically permute the order of tasks in the hierarchy and measure the impact on detection performance
3. **High-Dimensional Feature Space Calibration**: Test the method's statistical calibration by varying the feature map dimensionality and measuring the reliability of the Clopper-Pearson intervals