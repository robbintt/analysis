---
ver: rpa2
title: 'SAD: A Large-Scale Strategic Argumentative Dialogue Dataset'
arxiv_id: '2601.07423'
source_url: https://arxiv.org/abs/2601.07423
tags:
- argument
- strategy
- topic
- argumentation
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces SAD, the first large-scale dataset for strategic
  multi-turn argumentation dialogue, comprising 392,822 examples drawn from real-world
  interactions on Reddit''s ChangeMyView community. Each utterance is annotated with
  its stance and one or more of five argumentation strategies: Question, Causality,
  Example, Analogy, and Statement, grounded in argumentation theory.'
---

# SAD: A Large-Scale Strategic Argumentative Dialogue Dataset

## Quick Facts
- arXiv ID: 2601.07423
- Source URL: https://arxiv.org/abs/2601.07423
- Reference count: 40
- Introduces first large-scale dataset for strategic multi-turn argumentation dialogue with 392,822 examples from Reddit's ChangeMyView

## Executive Summary
This work introduces SAD, the first large-scale dataset for strategic multi-turn argumentation dialogue, comprising 392,822 examples drawn from real-world interactions on Reddit's ChangeMyView community. Each utterance is annotated with its stance and one or more of five argumentation strategies: Question, Causality, Example, Analogy, and Statement, grounded in argumentation theory. A new task formulation conditions response generation on dialogue history, stance, topic, and optional strategies. Experiments with multiple open- and closed-source LLMs show that incorporating strategies improves fluency, coherence, topicality, and persuasiveness. Human evaluation confirms that strategy-aware generation yields more convincing and relevant arguments. An automatic persuasiveness evaluator trained on user likes achieves strong alignment with human judgments. Fine-tuning with Direct Preference Optimization further enhances performance, particularly for persuasiveness. The dataset provides a valuable benchmark for studying strategic, interactive argumentation and advancing persuasive dialogue systems.

## Method Summary
The method involves creating a dialogue dataset from Reddit's ChangeMyView community by extracting reply chains, filtering short utterances, and annotating stance and argumentation strategies. The task is formulated as P(Argument | History, Stance, Topic, [Strategy]) with conditioning on dialogue context, stance, topic, and optional strategy labels. Models are evaluated using GPT-4.1-based judge and a trained persuasiveness evaluator. Fine-tuning is performed using LLaMA Factory with LoRA for both supervised fine-tuning and Direct Preference Optimization stages, with preference pairs derived from community vote differences.

## Key Results
- Strategy conditioning improves Relevance from 3.00→3.18 and Coherence from 3.38→3.41 with Llama3.1-8B
- DPO fine-tuning achieves Persuasiveness score of 3.88 vs 3.04 for SFT with Qwen3-8B
- Automatic persuasiveness evaluator trained on user likes achieves Pearson 0.71 and Spearman 0.66 correlation with human judgments
- Human evaluation confirms strategy-aware generation yields more convincing and relevant arguments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly conditioning generation on argumentation strategies improves output quality across relevance, coherence, and persuasiveness.
- Mechanism: Strategy labels (Question, Causality, Example, Analogy, Statement) constrain the generation space, guiding models toward rhetorically structured outputs rather than unconstrained text that may lack argumentative form.
- Core assumption: Models can learn to associate strategy tokens with specific rhetorical patterns from the training distribution.
- Evidence anchors:
  - [abstract] "Experiments with multiple open- and closed-source LLMs show that incorporating strategies improves fluency, coherence, topicality, and persuasiveness."
  - [section 5.3] Table 4 shows Llama3.1-8B improves Relevance from 3.00→3.18 and Coherence from 3.38→3.41 with strategy conditioning.
  - [corpus] Weak direct evidence; related work (MArgE, BCause) focuses on argumentation structure but not explicitly on strategy-conditioned generation.
- Break condition: If strategy labels are noisy or models lack sufficient capacity to internalize rhetorical patterns, improvements may diminish or reverse (observed for some persuasiveness scores in Table 4).

### Mechanism 2
- Claim: Multi-turn dialogue history provides essential context for generating coherent and responsive arguments.
- Mechanism: Conditioning on prior utterances allows models to track stance evolution, address specific counter-arguments, and maintain topical continuity rather than generating isolated claims.
- Core assumption: The relevant context window fits within model capacity and the dialogue structure (reply chains) correctly captures argumentative dependence.
- Evidence anchors:
  - [section 1] "The dialogue history provides essential context and evidence for generating coherent responses."
  - [section 3.1] Dialogue examples are constructed by tracing interactive reply chains in CMV discussion trees.
  - [corpus] BCause (arXiv:2505.03584) similarly leverages multi-turn deliberation structure to improve discourse quality.
- Break condition: If dialogue history exceeds context length or reply chains are misidentified, coherence gains degrade.

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) fine-tuning on preference pairs derived from community votes enhances persuasiveness more effectively than supervised fine-tuning (SFT) alone.
- Mechanism: DPO learns to distinguish between high-vote and low-vote responses to the same prompt, internalizing features associated with persuasive arguments without requiring explicit reward modeling.
- Core assumption: Community votes (likes) are reliable proxies for persuasiveness and preference pairs are sufficiently discriminative.
- Evidence anchors:
  - [section 5.3] Table 6 shows Qwen3-8B with DPO achieves Persuasiveness 3.88 vs. 3.04 with SFT.
  - [appendix D] Preference pairs are constructed from sibling responses with the largest vote differences.
  - [corpus] No direct corpus evidence for DPO in argumentation; related work focuses on argument evaluation rather than preference optimization.
- Break condition: If vote signals are noisy or reflect non-persuasiveness factors (e.g., humor, popularity), DPO may optimize for spurious features.

## Foundational Learning

- Concept: **Conditional Language Generation**
  - Why needed here: The task requires generating arguments conditioned on multiple factors (history, stance, topic, strategy), which differs from unconditional or single-condition generation.
  - Quick check question: Can you explain how conditioning variables are typically incorporated into autoregressive generation (e.g., via prompting vs. architectural modifications)?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO to improve persuasiveness; understanding its loss function and preference-pair construction is essential for replication.
  - Quick check question: How does DPO differ from reinforcement learning from human feedback (RLHF) in terms of reward model requirements?

- Concept: **Argumentation Theory (Strategies & Stance)**
  - Why needed here: The dataset is grounded in argumentation theory; annotating and generating with strategies requires understanding their rhetorical functions.
  - Quick check question: What distinguishes "Causality" from "Example" as argumentative strategies, and can a single utterance employ both?

## Architecture Onboarding

- Component map:
  Data Pipeline -> Model Backbone -> Training -> Evaluation
  CMV crawl -> reply chain extraction -> filtering -> stance annotation -> strategy annotation -> preference pair construction -> Any causal LLM -> LoRA fine-tuning (SFT/DPO) -> GPT-4.1 judge + persuasiveness evaluator

- Critical path:
  1. Data filtering quality (short utterance removal, reply chain integrity)
  2. Annotation reliability (Fleiss' Kappa for stance; consistency proportion for strategies)
  3. Prompt design for strategy conditioning (Figure 9 vs. Figure 10 templates)
  4. Preference pair selection (maximizing vote difference for discriminative signals)

- Design tradeoffs:
  - Strategy labels are multi-label (utterances can have multiple strategies); prompt-based conditioning may not capture nuanced combinations.
  - DPO uses LoRA rather than full fine-tuning; may limit capacity to internalize complex persuasive patterns.
  - Evaluator trained on likes may conflate persuasiveness with entertainment or alignment with majority opinion.

- Failure signatures:
  - Relevance scores remain low despite strategy conditioning → check prompt template formatting and context window coverage.
  - DPO degrades performance → preference pairs may have insufficient vote gaps or reflect confounding factors.
  - Persuasiveness evaluator disagrees with human judgment → re-examine normalization and training data distribution.

- First 3 experiments:
  1. Ablate strategy conditioning: Compare w. vs. wo. settings across multiple backbone models to confirm improvements are not model-specific.
  2. Vary preference pair thresholds: Test DPO with different minimum vote-gap cutoffs to identify optimal discriminative signal strength.
  3. Cross-evaluate persuasiveness: Compare GPT-4.1 judge scores with the trained evaluator on a held-out set to quantify alignment gaps.

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The persuasiveness evaluator trained on Reddit likes may conflate persuasion with entertainment value or popular opinion rather than logical convincingness.
- The dataset is limited to one subreddit (r/ChangeMyView) with specific discourse norms, raising questions about transfer to other argumentative domains.
- Strategy annotation reliability varies, with Fleiss' Kappa ranging from 0.11-0.45 for strategies, suggesting potential label noise that could limit model learning.

## Confidence
High confidence: Strategy conditioning improves fluency, coherence, and topicality across multiple backbone models (Llama3.1-8B, Qwen3-8B, Claude-3.5-Sonnet, GPT-4o-mini). The improvements are consistent across evaluation dimensions and model families.

Medium confidence: DPO fine-tuning on vote-derived preference pairs significantly enhances persuasiveness. While improvements are substantial (e.g., Qwen3-8B persuasiveness 3.88 vs 3.04 for SFT), the evaluator's reliance on community votes introduces uncertainty about what is actually being optimized.

Medium confidence: Multi-turn dialogue history provides essential context for coherent argumentation. The paper demonstrates this through improved coherence scores, but the specific impact of history length and quality on performance is not fully characterized.

## Next Checks
1. **Domain Transfer Validation**: Evaluate the trained models on argumentative dialogues from different sources (e.g., parliamentary debates, academic discussions) to assess generalization beyond r/ChangeMyView discourse norms.

2. **Strategy Annotation Quality Assessment**: Conduct targeted human evaluation on a sample of strategy-annotated utterances to measure inter-annotator agreement beyond Fleiss' Kappa, particularly for multi-label strategy assignments.

3. **Evaluator Ground Truth Validation**: Compare the trained persuasiveness evaluator's rankings with blinded human judgments on a held-out set to quantify alignment and identify potential biases in the like-score based training signal.