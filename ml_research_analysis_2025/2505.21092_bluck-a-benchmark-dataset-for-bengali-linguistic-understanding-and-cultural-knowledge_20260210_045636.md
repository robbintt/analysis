---
ver: rpa2
title: 'BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural
  Knowledge'
arxiv_id: '2505.21092'
source_url: https://arxiv.org/abs/2505.21092
tags:
- wang
- bengali
- zhang
- chen
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BLUCK is a new benchmark dataset designed to evaluate Large Language
  Models (LLMs) on Bengali linguistic understanding and cultural knowledge. It consists
  of 2,366 multiple-choice questions (MCQs) curated from college and job-level examinations,
  covering 23 categories across four domains: Bangladesh''s history, culture, Bengali
  phonetics, and semantics.'
---

# BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge

## Quick Facts
- **arXiv ID**: 2505.21092
- **Source URL**: https://arxiv.org/abs/2505.21092
- **Reference count**: 6
- **Primary result**: New Bengali benchmark dataset showing LLMs score ~7% lower than on MMLU, with significant struggles in Bengali phonetics

## Executive Summary
BLUCK is a benchmark dataset designed to evaluate Large Language Models on Bengali linguistic understanding and cultural knowledge. It contains 2,366 multiple-choice questions curated from college and job-level examinations across 23 categories spanning Bangladesh's history, culture, Bengali phonetics, and semantics. The dataset reveals that while models perform reasonably well overall, they struggle significantly with Bengali-specific knowledge, particularly in phonetics such as pronunciation and sound changes. GPT-4o and Claude-3.5-Sonnet achieved the highest scores (around 73% in zero-shot settings), but still performed approximately 7% lower than on the MMLU benchmark, highlighting the limitations of current LLMs in Bengali cultural and linguistic understanding.

## Method Summary
The dataset was constructed by collecting existing examination questions from sources including BCS (Bangladesh Civil Service) exams, university admission tests, Bar Council exams, bank recruitment tests, and public service commission exams. Questions were selected based on criteria ensuring they tested factual knowledge rather than reasoning ability, then underwent double-blind cross-inspection, digitization, and refinement. The final dataset covers four domains: History, Culture, Phonetics, and Semantics, with 23 subcategories total. Nine LLMs were evaluated including six proprietary models (GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, Gemini-1.5-Pro, Gemini-1.5-Flash, and GPT-4) and three open-source models (DeepSeekV3, Qwen2.5-72B-Instruct, and Llama-3.1-Nano). Evaluations were conducted in both zero-shot and 5-shot prompting settings.

## Key Results
- Overall accuracy across all models was approximately 7% lower on BLUCK compared to MMLU benchmark
- GPT-4o and Claude-3.5-Sonnet achieved highest scores at around 73% in zero-shot settings
- Models showed significant struggles in Bengali phonetics, with pronunciation subcategory accuracy ranging from 30-40%
- 5-shot prompting improved performance by 5-10% across all models
- Chain-of-thought reasoning provided minimal improvement on phonetics questions due to fundamental knowledge gaps

## Why This Works (Mechanism)

### Mechanism 1: Few-Shot Categorical Cueing Reduces MCQ Search Space
- Claim: 5-shot prompting improves BLUCK scores by 5-10% across all evaluated models
- Mechanism: In-context examples provide categorical signals that constrain the model's output distribution, reducing the effective search space for MCQ answers
- Core assumption: The 5 randomly sampled questions per category are sufficiently representative of category structure
- Evidence anchors: [abstract] "while these models perform reasonably well overall"; [section] "5-shot prompting leads to notable performance improvements (between 5% to 10%) across all models"; [corpus] No direct corpus corroboration for this specific Bengali benchmark finding
- Break condition: When category heterogeneity is high or few-shot examples diverge from test distribution

### Mechanism 2: Native Cultural Benchmarks Expose Knowledge Gaps Absent in Translated Evaluations
- Claim: Models score ~7% lower on BLUCK than on MMLU, revealing underrepresentation of Bengali-specific cultural and linguistic knowledge in pretraining
- Mechanism: Translated benchmarks assess transferable knowledge from dominant languages; native benchmarks require culturally grounded representations that may be absent or sparse in training corpora
- Core assumption: The performance gap reflects genuine knowledge gaps, not benchmark artifact difficulty
- Evidence anchors: [abstract] "approximately 7% lower than their performance on the MMLU benchmark"; [section] "most evaluations of Bengali, including MMLU, rely on translated English datasets"; [corpus] "From Facts to Folklore" paper confirms "critical gaps remain in capturing the nuances of low-resource cultures"
- Break condition: When benchmark content aligns with well-represented cultural knowledge in training data

### Mechanism 3: CoT Reasoning Cascades from Foundational Phonetic Knowledge Gaps
- Claim: Chain-of-thought prompting yields minimal improvement on pronunciation subcategory (Claude-3.5-Sonnet: 34.8% → 47.8% with CoT vs 50.7% with 5-shot) because errors originate in initial assumptions
- Mechanism: Models lack foundational Bengali phonetic rules; early reasoning errors propagate through the chain. Models also exhibit post-hoc rationalization—fixating on answers then generating forced justifications
- Core assumption: The phonetic knowledge deficit is at the representation level, not the reasoning architecture
- Evidence anchors: [abstract] "struggles in some areas of Bengali phonetics"; [section] "the model made an error in the very first reasoning step due to incorrect background assumptions, causing the entire reasoning chain to collapse"; [corpus] Weak/no direct corpus evidence for this specific phonetic CoT failure mode
- Break condition: When foundational domain knowledge is adequately represented in model weights

## Foundational Learning

- **Bengali Phonetics Categories**:
  - Why needed here: The phonetics domain (7 subcategories including pronunciation, sound changes, conjunct letters) shows the largest performance gaps; understanding what these categories measure is essential for interpreting failure modes
  - Quick check question: Can you explain why "sound changes" as a linguistic category might be harder for LLMs than "synonyms"?

- **Zero-Shot vs Few-Shot Evaluation Protocol**:
  - Why needed here: The paper reports both settings; 5-shot shows consistent 5-10% gains but requires understanding how exemplar selection affects benchmark validity
  - Quick check question: What bias could be introduced if 5-shot exemplars are not randomly sampled but cherry-picked for clarity?

- **MCQ Answer Extraction heuristics**:
  - Why needed here: The paper uses specific acceptance criteria (single alphabet, term correspondence, "answer is" patterns) and discards hallucinated outputs—critical for reproducibility
  - Quick check question: Why might a model output a single Bengali letter as an answer, and should it be counted as valid?

## Architecture Onboarding

- **Component map**:
  Data layer: 2,366 MCQs → 4 domains (History, Culture, Phonetics, Semantics) → 23 subcategories
  Curation pipeline: Source collection (BCS, university, Bar Council, bank, public exams) → selection criteria → double-blind cross-inspection → digitization → refinement
  Evaluation layer: 9 LLMs × 2 settings (0-shot, 5-shot) × 23 categories

- **Critical path**:
  1. Verify category distribution matches Table 1 before running evaluations
  2. Confirm answer extraction follows the three acceptance criteria (single alphabet, term match, explicit answer pattern)
  3. Log all discarded responses (hallucinations, single Bengali letter outputs) for failure analysis

- **Design tradeoffs**:
  - Fact-based questions exclude reasoning evaluation but enable direct knowledge probing
  - Random 5-shot sampling ensures representativeness but may include lower-quality exemplars
  - Single-letter output reduces API cost but limits error diagnostics

- **Failure signatures**:
  - Pronunciation subcategory: 30-40% accuracy across all models (vs 70-90% on semantics)
  - CoT on phonetics: early-step errors cascade; post-hoc rationalization patterns
  - National Issues subcategory: high variance (GPT-4o 46.7%, Claude-3.5-Sonnet 73.3%)

- **First 3 experiments**:
  1. **Establish baseline**: Run GPT-4o zero-shot on full BLUCK, logging per-category accuracy and discarded responses. Expect ~72-73% overall, <40% on pronunciation
  2. **Ablate few-shot exemplar quality**: Compare random 5-shot vs manually curated high-quality 5-shot on the phonetics domain only. Hypothesis: curated exemplars may help less than expected if foundational knowledge is absent
  3. **Diagnose CoT failure modes**: Run Claude-3.5-Sonnet with CoT on 20 pronunciation questions; manually classify error types (initial assumption error vs reasoning chain error vs rationalization). Use Figure 8 examples as coding guide

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training interventions are required to resolve LLMs' significant performance deficiency in Bengali phonetics?
- Basis in paper: [explicit] The authors state that models "struggle in some areas of Bengali phonetics" and explicitly call for future work to "improve LLMs' understanding of Bengali linguistic... nuances."
- Why unresolved: Current top models like GPT-4o score below 41% on phonetics, and the authors found that Chain-of-Thought reasoning failed to improve performance due to fundamental gaps in phonetic rule knowledge
- What evidence would resolve it: A study demonstrating that specific pre-training on phonetic rule corpora significantly boosts pronunciation and sound change scores

### Open Question 2
- Question: How does the relatively small scale of BLUCK impact the reliability of model evaluations?
- Basis in paper: [explicit] The Conclusion states, "Future research should expand BLUCK," and the Limitations section acknowledges the dataset is "relatively small" compared to the language's diversity
- Why unresolved: With only 2,366 questions, the dataset may not fully capture the breadth of Bengali culture or sustain robust evaluations of larger models without saturation
- What evidence would resolve it: A scaled-up version of the dataset (e.g., 10k questions) showing divergent performance rankings or new failure modes not present in the original set

### Open Question 3
- Question: Do models solve BLUCK questions using human-like reasoning or superficial pattern matching?
- Basis in paper: [inferred] The Limitations section notes the authors "cannot determine whether the models arrived at their answers through reasoning processes different from those of humans" because the dataset is text-based
- Why unresolved: MCQ accuracy measures the final output but fails to validate the internal cognitive process or logic used to derive the answer
- What evidence would resolve it: Qualitative analysis of model rationales or attention maps showing alignment with human linguistic problem-solving strategies

## Limitations

- The dataset construction relies on existing examination questions, which may reflect exam-specific biases rather than comprehensive linguistic knowledge
- The answer extraction heuristics discard responses containing single Bengali letters without clear justification for why these should be excluded
- The random sampling for 5-shot examples introduces variability that isn't quantified in the results

## Confidence

- **High confidence**: Overall performance trends showing ~7% lower scores on BLUCK vs MMLU, and consistent 5-10% gains from 5-shot prompting across all models
- **Medium confidence**: The mechanism explaining CoT failure on phonetics (early assumption errors cascading through reasoning chains)
- **Low confidence**: The specific claim that the performance gap reveals "genuine knowledge gaps" rather than benchmark difficulty

## Next Checks

1. **Test 5-shot exemplar sensitivity**: Run ablation studies comparing random 5-shot selection versus manually curated high-quality exemplars on the phonetics domain, measuring the impact on accuracy and examining whether exemplar quality matters when foundational knowledge is absent

2. **Validate answer extraction criteria**: Conduct a manual review of 50 discarded responses (single Bengali letters and hallucinations) to determine if the exclusion criteria are too restrictive and potentially biasing the results downward

3. **Benchmark difficulty calibration**: Compare BLUCK results with native Bengali speakers taking the same questions to establish whether the 7% performance gap versus MMLU reflects actual knowledge gaps or differential benchmark difficulty