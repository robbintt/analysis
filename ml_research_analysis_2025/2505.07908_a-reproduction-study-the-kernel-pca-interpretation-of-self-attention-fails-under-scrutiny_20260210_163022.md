---
ver: rpa2
title: 'A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails
  Under Scrutiny'
arxiv_id: '2505.07908'
source_url: https://arxiv.org/abs/2505.07908
tags:
- matrix
- kernel
- attention
- arxiv
- kpca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This reproduction study challenges recent claims that self-attention\
  \ implements kernel principal component analysis (KPCA). The authors empirically\
  \ evaluate three core assertions: (1) value vectors V align with KPCA eigenvectors\
  \ of the Gram matrix (average optimal cosine similarity \u2264 0.32, CKA \u2264\
  \ 0.11); (2) decreasing reconstruction loss Jproj indicates convergence (actually\
  \ driven by \u2225hi\u22252 collapsing, not \u03C6(qi) alignment); (3) eigenvalue\
  \ statistics support KPCA alignment (reported eigenvalues ~10^2, but observed ~10^-6,\
  \ orders of magnitude smaller)."
---

# A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny

## Quick Facts
- arXiv ID: 2505.07908
- Source URL: https://arxiv.org/abs/2505.07908
- Reference count: 40
- Primary result: Empirical evidence directly refutes the claim that self-attention implements kernel PCA, with average optimal cosine similarity ≤ 0.32 and CKA ≤ 0.11 across 10 transformer architectures

## Executive Summary
This reproduction study challenges the recent claim that self-attention implements kernel principal component analysis (KPCA). The authors empirically evaluate three core assertions: (1) value vectors V align with KPCA eigenvectors of the Gram matrix (average optimal cosine similarity ≤ 0.32, CKA ≤ 0.11); (2) decreasing reconstruction loss Jproj indicates convergence (actually driven by ∥hi∥2 collapsing, not φ(qi) alignment); (3) eigenvalue statistics support KPCA alignment (reported eigenvalues ~10^2, but observed ~10^-6, orders of magnitude smaller). Across 10 transformer architectures, no empirical evidence supports the KPCA interpretation. Similar results hold for language models. The study concludes that self-attention's KPCA interpretation lacks empirical and theoretical robustness, suggesting improvements attributed to KPCA arise from complementary regularization rather than intrinsic convergence.

## Method Summary
The reproduction tests whether self-attention value vectors V align with KPCA eigenvectors by extracting keys K from pretrained models, constructing centered Gram matrices using kernel k(x,y)=exp(x^T y/√d_q), computing top d_v eigenvectors, and comparing against learned V using cosine similarity, CKA, and eigenvalue statistics. The study uses 10 ViT/DeiT models (patch-16, ImageNet1K pretrained) with 100 random images for inference, plus 9 language models on WikiText-103. Key experiments include computing projection error Jproj during training while tracking separate contributions from feature map norms and output norms.

## Key Results
- Average optimal cosine similarity between V and KPCA eigenvectors ≤ 0.32 across all tested models
- Linear CKA and kernel CKA values ≤ 0.11, indicating weak alignment
- Eigenvalue magnitudes differ by orders of magnitude (observed ~10^-6 vs reported ~10^2)
- Decreasing reconstruction loss Jproj driven by ∥hi∥2 collapsing rather than φ(qi) alignment
- Results hold across 10 transformer architectures and language models

## Why This Works (Mechanism)

### Mechanism 1: Value Vector-Eigenvector Alignment Test
The original KPCA interpretation claims self-attention's learned value matrix V converges to encode eigenvectors of the centered Gram matrix from key vectors. The mechanism computes theoretical KPCA value matrix Ẇ_KPCA = GA - G1_N A from key-derived Gram eigenvectors, then measures similarity to attention-learned V using cosine similarity and CKA. The core assumption is that if self-attention implements KPCA, V ≈ Ẇ_KPCA should hold at convergence. Evidence shows average optimal cosine similarity ≤ 0.32 and CKA ≤ 0.11 across 10 transformer architectures, failing the alignment claim.

### Mechanism 2: Projection Error Decomposition
The KPCA interpretation claims decreasing reconstruction loss J_proj indicates self-attention outputs h_i converge toward reconstructions of φ(q_i) in feature space. The mechanism decomposes J_proj = (1/N)Σ||φ(q_i)||² - ||h_i||² to isolate contributions from feature map norms vs. output norms. The core assumption is that convergence requires ||φ(q_i)||² and ||h_i||² to approach similar magnitudes. Evidence shows ||φ(q_i)||² ≈ 10^-3 while ||h_i||² remains orders larger, with loss decrease driven by collapsing ||h_i||² rather than meaningful alignment.

### Mechanism 3: Eigenvalue Magnitude Verification
The KPCA interpretation claims eigenvalues of the centered Gram matrix Ǩ_φ should match reported magnitudes (~10^2) if V encodes Gram eigenvectors. The mechanism computes eigenvalues of Ǩ_φ from key vectors and compares statistics to originally reported values. The core assumption is that reproducible eigenvalue distributions should emerge without undocumented implementation adjustments. Evidence shows all models exhibit eigenvalues ~10^-6, differing by 6+ orders of magnitude from reported values.

## Foundational Learning

- **Concept: Kernel PCA in Feature Space**
  - **Why needed here:** KPCA extends PCA to non-linear data by mapping inputs φ(k_j) into a high-dimensional feature space before computing principal components.
  - **Quick check question:** Can you explain why KPCA requires the kernel trick (computing φ(k_i)ᵀφ(k_j) via k(k_i,k_j)) rather than explicitly constructing φ?

- **Concept: Gram Matrix Eigendecomposition**
  - **Why needed here:** The centered Gram matrix Ǩ_φ encodes pairwise similarities in feature space; its eigenvectors determine projection directions.
  - **Quick check question:** Given Ǩ_φ a_d = λN a_d (Eq. 5), what does a_d represent and how does it relate to the principal component u_d?

- **Concept: Centered Kernel Alignment (CKA)**
  - **Why needed here:** CKA measures similarity between neural network representations invariant to orthogonal transformations and isotropic scaling.
  - **Quick check question:** Why is CKA preferred over raw dot-product similarity when comparing learned representations to theoretical targets?

## Architecture Onboarding

- **Component map:** Q=XW_Qᵀ -> K=XW_Kᵀ -> V=XW_Vᵀ -> h_i = Σ_j σ(q_i Kᵀ/√d_q)_j v_j; KPCA counterpart: φ(k_j) -> centered Gram matrix Ǩ_φ -> eigenvectors A -> Ẇ_KPCA = GA - G1_N A; Comparison: Cosine similarity, optimal matching (Jonker-Volgenant), linear/kernel CKA

- **Critical path:** 1. Extract keys K from forward pass through pretrained model; 2. Construct and center Gram matrix using kernel k(x,y) = exp(xᵀy/√d_q); 3. Compute eigenvectors of Ǩ_φ and construct Ẇ_KPCA; 4. Compare against attention-learned V using multiple similarity metrics

- **Design tradeoffs:** Direct vs. optimal matching (direct stricter, Jonker-Volgenant tests best-case alignment); squared norm proxy vs. full projection loss (eigenvector-invariant computation avoids O(d_v!) search but requires orthonormality); Z-score normalization (stabilizes eigenvalue computation but may alter magnitudes)

- **Failure signatures:** Cosine similarity < 0.4 even with optimal matching → V not encoding Gram eigenvectors; ||φ(q_i)||² << ||h_i||² (≥10³ gap) → reconstruction loss decrease misinterpreted; Eigenvalues ~10⁻⁶ vs. reported ~10² → irreproducible spectral analysis

- **First 3 experiments:** 1. Load ViT-Tiny, compute V and Ẇ_KPCA for 100 images, report all four similarity metrics to establish non-alignment; 2. Train DeiT-Tiny from scratch, log ||φ(q_i)||², ||h_i||², and J_proj epoch-wise to verify loss decrease correlates with output norm collapse; 3. Compute Ǩ_φ eigenvalues across layers/heads for multiple images, verify magnitude is ~10⁻⁶ not ~10²; test sensitivity to Z-score preprocessing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** If the kernel PCA (KPCA) alignment hypothesis is empirically invalid, what is the correct mathematical interpretation of self-attention as a projection mechanism?
- **Basis in paper:** The conclusion states that while "the interpretation of self-attention mechanisms as projections... remains an open research question," the evidence "directly refutes how this mechanism is characterized" in prior work.
- **Why unresolved:** This study successfully falsifies the specific KPCA formulation but does not propose an alternative theoretical framework that explains the attention mechanism's functionality under the observed low similarity constraints.
- **What evidence would resolve it:** A new theoretical model that aligns with the empirical data showing low CKA scores (<0.11) and explains the dynamics of $\|h_i\|_2$ without relying on eigenvector alignment.

### Open Question 2
- **Question:** Do the performance improvements reported in Robust PCA (RPCA) attention methods stem from the theoretical KPCA alignment or merely from complementary regularization effects?
- **Basis in paper:** The authors hypothesize in the conclusion that "RPCA's improvements stem from its complementary role within the existing architecture... using the symmetric self-attention mechanism as a low-rank approximator... rather than replacing it outright."
- **Why unresolved:** The reproduction study decouples the empirical gains of related methods from the theoretical KPCA claims, suggesting the benefits may be incidental rather than proof of the theory.
- **What evidence would resolve it:** Ablation studies that isolate the regularization component of RPCA from the structural KPCA constraints to identify the true driver of performance gains.

### Open Question 3
- **Question:** Are the eigenvalue statistics supporting the KPCA interpretation reproducible under standard precision, or do they depend on undocumented pre-processing steps?
- **Basis in paper:** The paper notes that eigenvalues were "irreproducible without undocumented implementation-specific adjustments" and required Z-score normalization to mitigate numerical instability.
- **Why unresolved:** The authors of the original work did not document the specific adjustments needed to yield their reported eigenvalue magnitudes ($10^2$), which differed from the reproduction's results ($10^{-6}$) by orders of magnitude.
- **What evidence would resolve it:** A clarification or code release from the original authors detailing the exact pre-processing and normalization steps required to reproduce the large eigenvalue statistics.

## Limitations
- Missing implementation details for the exact feature map φ used in the original KPCA interpretation
- Unclear whether per-dimension or per-vector Z-score normalization was applied in eigenvalue computations
- Training hyperparameters for J_proj experiments not fully specified

## Confidence
- **High confidence:** The empirical refutation of V-KPCA alignment (cosine similarity ≤ 0.32, CKA ≤ 0.11 across all tested models)
- **Medium confidence:** The interpretation that decreasing J_proj reflects ∥h_i∥² collapse rather than φ(q_i) alignment (supported by norm decomposition)
- **Medium confidence:** The eigenvalue magnitude discrepancy (~10⁻⁶ vs reported ~10²) though exact preprocessing steps remain unclear

## Next Checks
1. Implement the exact φ feature map from the original KPCA interpretation paper to verify whether missing √d_v scaling explains the ∥φ(q_i)∥² discrepancies
2. Systematically test Z-score normalization variants (per-dimension vs per-vector) on Gram matrix construction to isolate sources of eigenvalue magnitude differences
3. Extend experiments to intermediate layer outputs to determine if KPCA alignment ever emerges during training, or if the misalignment persists throughout