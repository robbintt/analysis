---
ver: rpa2
title: 'The Adoption and Usage of AI Agents: Early Evidence from Perplexity'
arxiv_id: '2512.07828'
source_url: https://arxiv.org/abs/2512.07828
tags:
- subtopic
- topic
- share
- agent
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first large-scale field study of general-purpose
  AI agent adoption, usage intensity, and use cases. Analyzing hundreds of millions
  of anonymized user interactions with Comet Assistant, the study reveals substantial
  heterogeneity in adoption across user segments, with earlier adopters, users in
  wealthier and more educated countries, and those in digital/knowledge-intensive
  occupations showing higher adoption rates.
---

# The Adoption and Usage of AI Agents: Early Evidence from Perplexity

## Quick Facts
- arXiv ID: 2512.07828
- Source URL: https://arxiv.org/abs/2512.07828
- Authors: Jeremy Yang; Noah Yonack; Kate Zyskowski; Denis Yarats; Johnny Ho; Jerry Ma
- Reference count: 40
- One-line primary result: First large-scale field study reveals substantial heterogeneity in AI agent adoption across user segments and identifies 90 tasks across 7 topics with productivity and learning dominating usage

## Executive Summary
This paper presents the first comprehensive field study examining AI agent adoption and usage patterns at scale. Analyzing hundreds of millions of anonymized interactions with Comet Assistant, the research reveals significant variations in adoption rates across different user demographics and occupations, with earlier adopters, users in wealthier and more educated countries, and those in digital/knowledge-intensive occupations showing higher adoption. The study develops a hierarchical taxonomy of 90 tasks across 7 main topics and 29 subtopics, finding that productivity and learning dominate usage patterns, with personal use accounting for 55% of queries while professional and educational uses comprise 30% and 16% respectively.

The research also tracks temporal shifts in usage patterns, observing that over time, users increasingly engage with more cognitively oriented topics. These findings provide critical insights for researchers, businesses, policymakers, and educators as AI agents become increasingly capable and widespread. The study's methodology and findings establish a foundational understanding of how AI agents are being integrated into daily workflows and knowledge work, highlighting both the opportunities and challenges associated with this emerging technology.

## Method Summary
The study analyzes hundreds of millions of anonymized user interactions with Comet Assistant to examine AI agent adoption patterns and usage intensity. Researchers developed a hierarchical agentic taxonomy through algorithmic analysis to categorize user queries into 90 distinct tasks across 7 main topics and 29 subtopics. The analysis examines adoption rates across user segments, tracks temporal usage patterns, and differentiates between personal, professional, and educational use cases. The methodology relies entirely on behavioral data from a single platform, providing large-scale empirical evidence of AI agent usage patterns.

## Key Results
- Substantial heterogeneity in AI agent adoption across user segments, with higher adoption among earlier adopters, users in wealthier and more educated countries, and those in digital/knowledge-intensive occupations
- Development of hierarchical taxonomy identifying 90 tasks across 7 topics and 29 subtopics, with productivity and learning dominating usage patterns
- Temporal shifts in usage over time toward more cognitively oriented topics, with personal use accounting for 55% of queries while professional and educational uses comprise 30% and 16% respectively

## Why This Works (Mechanism)
The study's effectiveness stems from analyzing real-world behavioral data at massive scale, capturing authentic user interactions rather than self-reported usage. The algorithmic development of the hierarchical taxonomy allows for systematic categorization of diverse user intents without relying on manual coding of individual queries. By examining adoption patterns across multiple dimensions including geography, occupation, and time, the research reveals nuanced insights about how different user segments integrate AI agents into their workflows. The large sample size enables detection of subtle patterns that would be invisible in smaller studies.

## Foundational Learning
- **Hierarchical taxonomy development** - needed to systematically categorize diverse user queries; quick check: verify taxonomy coverage through random sampling of uncategorized queries
- **Behavioral data analysis** - needed to capture authentic usage patterns without self-reporting bias; quick check: compare behavioral patterns across different user segments
- **Temporal analysis** - needed to track how usage evolves over time; quick check: validate temporal trends using different time windows
- **Segmentation analysis** - needed to identify adoption heterogeneity across user groups; quick check: test robustness across different segmentation criteria
- **Task categorization** - needed to understand primary use cases; quick check: validate task classifications through expert review
- **Adoption rate measurement** - needed to quantify penetration across different populations; quick check: verify measurement methodology with known benchmarks

## Architecture Onboarding
- **Component map**: User queries → Algorithmic classification → Hierarchical taxonomy (7 topics → 29 subtopics → 90 tasks) → Adoption/usage analysis
- **Critical path**: Query collection → Anonymization → Classification → Segmentation → Temporal analysis → Result interpretation
- **Design tradeoffs**: Large-scale behavioral data provides authenticity but lacks contextual understanding; algorithmic taxonomy ensures scalability but may miss nuanced intents
- **Failure signatures**: Over-classification of tasks, missing emerging use cases, geographic/occupational sampling bias, temporal pattern misinterpretation
- **Three first experiments**: 1) Cross-validate taxonomy classifications with human coders on random sample; 2) Test adoption patterns across multiple AI platforms; 3) Conduct qualitative interviews to understand motivations behind identified use cases

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies entirely on behavioral data from a single platform (Comet Assistant), which may not generalize to other AI agents or user populations
- Study captures adoption patterns but cannot establish causal relationships between user characteristics and usage patterns
- The 90-task taxonomy, while comprehensive, was developed through algorithmic analysis and may miss nuanced user intents or emerging use cases not yet reflected in the data

## Confidence
- High: Descriptive statistics about adoption rates and usage intensity patterns, as these are directly measured from the dataset
- Medium: Inferences about user segments and occupational patterns, given the observational nature of the data
- Low: Predictive claims about future usage trends and broader societal implications, which extend beyond the empirical scope of the study

## Next Checks
1. Replicate the adoption analysis across multiple AI platforms to assess generalizability of the observed patterns
2. Conduct longitudinal tracking of the same users over extended periods to validate the observed temporal shifts in usage topics
3. Supplement behavioral data with qualitative user surveys to better understand motivations and contexts behind the identified use cases