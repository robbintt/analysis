---
ver: rpa2
title: A Survey on Mamba Architecture for Vision Applications
arxiv_id: '2502.07161'
source_url: https://arxiv.org/abs/2502.07161
tags:
- mamba
- arxiv
- vision
- scanning
- scan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews Mamba architectures for vision
  applications, focusing on Vision Mamba (ViM) and VideoMamba. The study examines
  key architectural innovations including bidirectional scanning, selective scanning
  mechanisms, position embeddings, cross-scan modules, and hierarchical designs.
---

# A Survey on Mamba Architecture for Vision Applications

## Quick Facts
- **arXiv ID**: 2502.07161
- **Source URL**: https://arxiv.org/abs/2502.07161
- **Reference count**: 40
- **Key outcome**: This survey comprehensively reviews Mamba architectures for vision applications, focusing on Vision Mamba (ViM) and VideoMamba. The study examines key architectural innovations including bidirectional scanning, selective scanning mechanisms, position embeddings, cross-scan modules, and hierarchical designs. Performance comparisons across image classification, semantic segmentation, object detection, and video understanding tasks demonstrate that Spatial Mamba-S and Vmamba-S achieve top results (84.6% and 83.6% Top-1 accuracy) but require high computational resources. The survey identifies challenges such as artifacts in feature maps and limited global-local representations, while highlighting promising future directions including multi-modal fusion, point cloud processing, and hybrid CNN-Transformer-Mamba architectures. The analysis provides practical guidance for selecting optimal Mamba architectures based on specific vision tasks and computational constraints.

## Executive Summary
This survey systematically examines Mamba architectures for vision applications, tracing their evolution from Vision Mamba (ViM) through various specialized variants including VMamba, VideoMamba, and task-specific designs. The study provides comprehensive coverage of architectural innovations such as bidirectional scanning, selective scanning mechanisms, and position embeddings, while offering detailed performance comparisons across multiple vision tasks. The survey identifies key challenges including artifacts in feature maps and limitations in capturing global representations, while proposing future research directions for improving Mamba's capabilities in multi-modal applications and complex vision tasks.

## Method Summary
The survey systematically analyzes Mamba architectures by first establishing the mathematical foundation through state-space models and discretization mechanisms. It then examines core components including selective scanning (S6), bidirectional processing, and various architectural modifications. The methodology involves comprehensive literature review across 40+ references, performance benchmarking on standard vision tasks (ImageNet-1k, ADE20K, MS-COCO, Kinetics-400), and systematic analysis of architectural trade-offs. Implementation details are synthesized from multiple sources, with emphasis on understanding the relationship between architectural choices and performance outcomes.

## Key Results
- Spatial Mamba-S achieves 84.6% Top-1 accuracy on ImageNet-1k but requires 7.1G FLOPS
- Mamba architectures demonstrate linear scalability with sequence length, enabling efficient high-resolution image and video processing
- Performance varies significantly across tasks, with LocalMamba excelling at local feature extraction while Spatial Mamba better balances global-local representations

## Why This Works (Mechanism)

### Mechanism 1: Linear Scalability via State-Space Models
- **Claim**: If visual sequences are processed as discrete time-series using state-space models, the computational cost scales linearly with sequence length, enabling high-resolution and long-video processing.
- **Mechanism**: Mamba models sequences via a discretized SSM where a hidden state evolves based on input-dependent parameters (A, B, C, Δ). Unlike self-attention, there is no pairwise token comparison. Each token updates a compact hidden state, and the output is generated by projecting this state. The "Selective Scan" (S6) mechanism makes the parameters input-dependent, allowing the model to selectively retain or discard information.
- **Core assumption**: The evolution of a visual feature sequence can be sufficiently modeled by a first-order Markov-like process on a latent hidden state.
- **Evidence anchors**: [abstract] "...Mamba architecture utilizes state-space models (SSMs) for linear scalability, efficient processing..."; [section] Section II.A: Mamba Block details the discretization from `h'(t) = Ah(t) + Bx(x)` to `h_t = Āh_{t-1} + B̄x_t`; [section] Section II.B: Describes the Selective Scan (S6) making parameters B, C, Δ input-dependent.
- **Break condition**: If the hidden state size (N) must grow with sequence complexity or task difficulty (rather than sequence length), memory use may no longer scale linearly. Hardware-efficient implementation relies on specific GPU memory hierarchies; different hardware may not realize the theoretical linear speedup.

### Mechanism 2: Non-Causal Contextualization via Bidirectional/Multi-Path Scanning
- **Claim**: If visual data is inherently non-causal (context depends on both past and future pixels/frames), then unidirectional scanning is insufficient. Bidirectional and multi-path scanning mechanisms restore global contextual awareness.
- **Mechanism**: Vision Mamba (ViM) processes a flattened image patch sequence simultaneously in forward and backward directions. The outputs are gated (using a `z` projection) and merged. VMamba extends this with a Cross-Scan Module (CSM), scanning in four directions (e.g., top-left to bottom-right and inverses). VideoMamba uses 3D bidirectional scanning across space and time. These parallel scans capture dependencies from all relative positions.
- **Core assumption**: Parallel independent scans, when merged, can effectively integrate information from all spatial/temporal directions without the explicit interaction provided by attention.
- **Evidence anchors**: [abstract] "...introduce bidirectional scanning, selective scanning mechanisms..."; [section] Section II.C & Fig. 2: Details ViM bidirectional SSMs with simultaneous forward/backward processing and gating; [section] Section II.D & Fig. 3: Describes VideoMamba's 3D spatiotemporal bidirectional blocks.
- **Break condition**: If directional scans fail to interact meaningfully at merge points, long-range diagonal or reverse dependencies may be weakly integrated. Increasing scan paths increases compute and memory proportionally; overly complex paths may negate efficiency gains.

### Mechanism 3: Enhanced Local/Global Feature Extraction via Architectural Modifications
- **Claim**: Standard Mamba may produce artifacts or lack strong local-global feature balance. Architectural modifications like register tokens, local-global fusion modules, and hierarchical designs mitigate these issues.
- **Mechanism**: Mamba® adds "register tokens" to absorb artifact artifacts. Spatial Mamba uses a Structure-Aware State Fusion (SASF) layer to integrate 2D local dependencies. EfficientViM shifts channel mixing to the hidden state space (HSM-SSD). Hi-Mamba uses hierarchical blocks with local and region SSMs. These designs explicitly supplement the sequential SSM core with inductive biases from CNNs (hierarchy, windows) or Transformers (registers).
- **Core assumption**: Specific architectural priors (e.g., 2D spatial structure, window locality) are necessary to overcome the "sequence-processing" origins of Mamba for dense vision tasks.
- **Evidence anchors**: [section] Section IV.c: Describes Spatial Mamba's SASF layer for local-global dependencies; [section] Section IV.i: Mamba® introduces register tokens to mitigate artifacts; [section] Section IV.h: EfficientViM's Hidden State Mixer alleviates SSD bottlenecks.
- **Break condition**: If these modifications add computational overhead that approaches quadratic attention, the primary advantage of Mamba (efficiency) is lost. The interplay of different modules may introduce new training instabilities or optimization challenges.

## Foundational Learning

- **Concept**: State-Space Models (SSMs)
  - **Why needed here**: SSMs are the mathematical core of Mamba, replacing the attention mechanism of Transformers.
  - **Quick check question**: Can you explain how a continuous-time system `h'(t) = Ah(t) + Bx(x)` is discretized into a recurrent form `h_t = Āh_{t-1} + B̄x_t`?

- **Concept**: Discretization and Zero-Order Hold (ZOH)
  - **Why needed here**: Mamba operates on discrete token sequences (image patches), requiring continuous parameters to be converted.
  - **Quick check question**: What is the role of the step size (Δ) in the ZOH discretization formula `Ā = exp(ΔA)`?

- **Concept**: Selective Scanning (S6)
  - **Why needed here**: This is Mamba's key innovation, allowing content-aware reasoning, unlike traditional SSMs.
  - **Quick check question**: How do the input-dependent parameters `S_B(x)`, `S_C(x)`, `S_Δ(x)` allow the model to "select" which information to retain or discard?

## Architecture Onboarding

- **Component map**: Image Patches -> Position Embedding -> Bidirectional/Multi-path SSM Scanning -> Feature Fusion -> Task-specific Head
- **Critical path**: Patch Embedding -> Bidirectional SSM Processing -> Feature Fusion/Pooling -> Task Head. Understanding the data flow from 2D/3D input -> 1D scan -> SSM state update -> 1D output -> 2D/3D reconstruction is essential.
- **Design tradeoffs**:
  - Accuracy vs. Efficiency: Top models (Spatial Mamba-S) require high FLOPS (7.1G, 84.6% ACC). EfficientViM-M4 (4.1G FLOPS, 81.9% ACC) trades some accuracy for lower cost.
  - Simplicity vs. Capability: Basic bidirectional scanning (ViM) is simpler but may lack the local representation of windowed scans (LocalMamba).
  - Unidirectional vs. Multi-path: More scan directions improve context but increase computation linearly.
- **Failure signatures**:
  - Artifacts in feature maps (addressed by Mamba®).
  - Weaker global-local balance compared to Transformers (addressed by LocalMamba, Spatial Mamba).
  - Underperformance on video benchmarks relative to Transformers (addressed by VideoMambaPro enhancements).
  - Computational bottlenecks in standard SSD (addressed by EfficientViM's HSM-SSD).
- **First 3 experiments**:
  1. **Baseline Reproduction**: Implement a basic ViM block with bidirectional scanning on CIFAR-10. Measure throughput (images/sec) and memory usage vs. a simple ViT baseline. This validates the core efficiency claim.
  2. **Scan Ablation**: Replace the bidirectional scan in your ViM implementation with a single-direction scan and then a 4-direction cross-scan (CSM). Compare Top-1 accuracy on a small dataset (e.g., ImageNet-1k subset) to quantify the impact of scanning strategy.
  3. **Feature Map Inspection**: Train a basic ViM and a Mamba® variant (with register tokens). Visualize feature maps from intermediate layers to observe the presence or reduction of artifacts, linking architectural changes to output quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the artifact problem in Vision Mamba feature maps be fully resolved without introducing additional computational overhead?
- **Basis in paper**: [explicit] Page 4 states "One study reveals that ViM feature maps can contain artifacts, potentially impacting performance" and discusses Mamba® with register tokens as a mitigation, but notes "These artifacts necessitate further refinements to the architecture for optimal results."
- **Why unresolved**: While register tokens reduce artifacts, they add parameters (28M vs 26M for Mamba®-S vs ViM-S) and the trade-off between artifact elimination and efficiency remains unexplored.
- **What evidence would resolve it**: A systematic study comparing artifact presence, accuracy, and computational cost across different artifact mitigation strategies on standardized benchmarks.

### Open Question 2
- **Question**: What architectural innovations are required for VideoMamba to achieve consistent performance parity with transformer-based video models across diverse video understanding benchmarks?
- **Basis in paper**: [explicit] Page 4-5 states "its performance on video benchmarks falls short of transformer-based methods, indicating room for improvement" and identifies challenges including "historical decay" and "element contradiction."
- **Why unresolved**: VideoMambaPro shows improvements (81.5%→88.5% on Kinetics-400) but the fundamental gap in SSV2 (67.6%→74.3% vs. higher transformer baselines) suggests core architectural limitations persist.
- **What evidence would resolve it**: VideoMamba matching or exceeding state-of-the-art video transformer performance on multiple benchmarks (Kinetics-400, SSV2, AVA) with comparable model sizes.

### Open Question 3
- **Question**: Can a unified scanning mechanism be developed that optimally balances local and global feature extraction across all vision tasks?
- **Basis in paper**: [explicit] Page 4 notes "limited ability to effectively capture global representations" and page 5-6 shows task-dependent performance variations: LocalMamba excels at local features but Spatial-Mamba-S's structure-aware fusion achieves better overall results (84.6% vs 83.7% Top-1).
- **Why unresolved**: Current approaches require task-specific scanning strategy selection, with no single mechanism dominating across classification, segmentation, and detection tasks.
- **What evidence would resolve it**: A single scanning architecture achieving top-quartile performance on ImageNet-1K, ADE20K, and MS-COCO simultaneously without task-specific tuning.

## Limitations

- Lack of detailed architectural specifications and training hyperparameters across different Mamba variants makes direct comparison and reproduction challenging
- Performance comparisons rely on published results without independent verification, potentially affected by overfitting to specific benchmarks
- The survey doesn't address potential optimization challenges or training instabilities that may arise from complex module interactions

## Confidence

- **High Confidence**: Claims about Mamba's linear scalability via state-space models and the basic mechanism of selective scanning are well-established theoretically and supported by multiple papers.
- **Medium Confidence**: Claims about specific performance metrics (accuracy, FLOPS) and the effectiveness of architectural modifications (register tokens, SASF layers) are based on published results but lack independent verification.
- **Low Confidence**: Claims about the causes of artifacts in feature maps and the relative importance of different architectural innovations are primarily based on author assertions without systematic ablation studies.

## Next Checks

1. **Independent Performance Replication**: Reimplement and train the top-performing models (Spatial Mamba-S and Vmamba-S) on ImageNet-1k using publicly available codebases, comparing achieved accuracy and FLOPS to published claims while controlling for training hyperparameters.

2. **Architectural Ablation Study**: Systematically remove or modify key components (bidirectional scanning, cross-scan modules, register tokens) from a baseline Mamba model to quantify their individual contributions to accuracy and efficiency across multiple vision tasks.

3. **Artifact Analysis**: Conduct controlled experiments comparing feature maps from basic Mamba implementations versus Mamba® variants with register tokens, using quantitative metrics to measure artifact reduction and correlation with downstream task performance.