---
ver: rpa2
title: Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications
arxiv_id: '2502.20311'
source_url: https://arxiv.org/abs/2502.20311
tags:
- speech
- available
- whisper
- https
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of transcribing Southeast Asian-accented
  English in noisy Air Traffic Control (ATC) environments, where existing Automatic
  Speech Recognition (ASR) systems struggle with accuracy. The researchers developed
  a region-specific dataset and fine-tuned Whisper models to improve transcription
  performance.
---

# Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications

## Quick Facts
- arXiv ID: 2502.20311
- Source URL: https://arxiv.org/abs/2502.20311
- Reference count: 40
- Primary result: Fine-tuned Whisper models achieve 9.82% WER on Southeast Asian-accented ATC speech

## Executive Summary
This study addresses the challenge of transcribing Southeast Asian-accented English in noisy Air Traffic Control environments, where existing Automatic Speech Recognition systems struggle with accuracy. The researchers developed a region-specific dataset and fine-tuned Whisper models to improve transcription performance. By incorporating noise-robust training strategies and data augmentation techniques, the models achieved a Word Error Rate of 0.0982 (9.82%) on Southeast Asian-accented ATC speech. The findings highlight the importance of region-specific datasets and accent-focused training for enhancing ASR systems in resource-constrained military operations.

## Method Summary
The study fine-tuned Whisper Small (244M parameters) and Whisper Large v3 Turbo (809M parameters) models on 37 hours of Southeast Asian-accented ATC speech. The dataset was split 70/15/15 for training, validation, and testing. Audio preprocessing included Spectral Gating denoising, followed by data augmentation using frequency filters and tanh distortion. Models were trained for 30 epochs with checkpoint selection based on lowest validation WER. The fine-tuned small model achieved the best performance with 9.82% WER on the test set.

## Key Results
- Fine-tuned Whisper Small model achieved 9.82% WER on Southeast Asian-accented ATC speech
- Region-specific fine-tuning reduced WER from >50% to 9.82% compared to non-fine-tuned models
- Data augmentation with frequency filters and tanh distortion improved noise robustness

## Why This Works (Mechanism)

### Mechanism 1
Region-specific fine-tuning of pre-trained ASR models substantially reduces Word Error Rate for underrepresented accents in specialized domains. Supervised fine-tuning adjusts model weights to adapt learned representations to specific phonetic, prosodic, and lexical patterns of the target accent. Core assumption: pre-trained models possess sufficient foundational capacity for effective adaptation with smaller, high-quality datasets. Evidence: fine-tuned small model achieved 0.0982 WER on SEA-accented dataset. Break condition: target accent too distinct from pre-trained data distribution or dataset too small/noisy.

### Mechanism 2
Data augmentation with simulated noise profiles improves model robustness to acoustic degradation. Synthetic noise addition forces models to learn invariant features of speech signals, acting as regularization against overfitting to clean training data. Core assumption: synthetic augmentations accurately mimic real-world acoustic conditions like VHF/HF radio interference. Evidence: frequency filters and tanh distortion improved performance on noisy ATC communications. Break condition: unrealistic or overly severe augmentations corrupt speech signals beyond intelligibility.

### Mechanism 3
Model architecture and size must be balanced with computational constraints for deployment in resource-constrained environments. Smaller model variants reduce parameter count and computational load for edge hardware deployment, with fine-tuning recovering accuracy trade-offs for specific domains. Core assumption: smaller model performance on target task remains acceptable after fine-tuning despite lower general-purpose capability. Evidence: fine-tuned sea-small model achieved best WER of 0.0982, outperforming larger non-fine-tuned models. Break condition: model too small to learn nuanced accented speech and noise patterns.

## Foundational Learning

- **Concept**: Word Error Rate (WER)
  - Why needed: Primary evaluation metric quantifying model performance from edit distance between output and ground truth
  - Quick check: For "Climb and maintain flight level three five zero" transcribed as "Climb to maintain flight level three five zero," what are S, D, and I counts?

- **Concept**: Fine-tuning (Transfer Learning)
  - Why needed: Core methodology of adapting pre-trained models to specialized tasks
  - Quick check: Why is fine-tuning preferred over training ASR models from scratch for specialized tasks?

- **Concept**: Data Augmentation
  - Why needed: Critical technique for building robustness to noisy radio communication conditions
  - Quick check: What is the primary risk of applying data augmentation too aggressively?

## Architecture Onboarding

- **Component map**: Raw audio -> Spectral Gating denoiser -> Split (Train/Val/Test) -> Data Augmentation (Frequency Filters, Tanh Distortion) -> Model (Pre-trained Whisper encoder-decoder Transformer) -> Fine-tuning -> Calculate Combined WER

- **Critical path**: Creation of high-quality, region-specific dataset is most critical prerequisite, as models trained on other accents fail to generalize to SEA-accented domain

- **Design tradeoffs**:
  - Model Size vs. SWaP: Larger models may offer better generalization but impractical for edge deployment; smaller models require domain-specific fine-tuning
  - Generalization vs. Specialization: Fine-tuning yields superior domain performance but significantly degrades performance on other accents

- **Failure signatures**:
  - Catastrophic Forgetting: Models fine-tuned on new accents may perform poorly on previously known accents
  - High WER on General Models: Off-the-shelf Whisper or Western accent models yield >50% WER on SEA-accented speech
  - Noise Intolerance: Models without noise-robust training fail on low signal-to-noise ratio radio communications

- **First 3 experiments**:
  1. Evaluate pre-trained OpenAI Whisper models on SEA-accented ATC test set to quantify initial performance gap
  2. Fine-tune Whisper Small on curated dataset without augmentation to isolate impact of region-specific data
  3. Retrain from experiment 2 with specified data augmentations to measure improvement in radio noise robustness

## Open Questions the Paper Calls Out

### Open Question 1
Can prompt engineering strategies be refined to consistently improve transcription of region-specific ATC terminology? Current prompting attempts demonstrate limited semantic understanding and accuracy improvements, with best practices yet to be established. Evidence needed: comparative study showing statistically significant WER reductions with optimized prompts versus non-prompted baselines.

### Open Question 2
To what extent can specialized Recognition Oriented Speech Enhancement or ATC-specific denoisers improve ASR accuracy compared to standard data augmentation? ATC noise is lossy and massively affects performance, suggesting standard augmentations may be insufficient. Evidence needed: benchmark results showing specialized ATC denoiser lowers WER more effectively than frequency filters and distortions.

### Open Question 3
Can a single ASR model be trained to generalize across diverse accents without catastrophic forgetting or domain mismatch? Current results show sharp WER increases when models trained on one accent are applied to another. Evidence needed: model evaluation demonstrating comparable WERs (<15%) on both SEA-accented and Western datasets simultaneously.

## Limitations

- Dataset scope limited to 37 hours of Southeast Asian-accented speech, with unclear representation of accent diversity within the region
- Training methodology lacks full specification of hyperparameters and augmentation parameters needed for exact reproduction
- Computational constraints analysis incomplete without detailed benchmarks comparing inference latency, memory usage, and power consumption

## Confidence

**High Confidence Claims**:
- Region-specific fine-tuning significantly improves WER for Southeast Asian-accented ATC speech
- Data augmentation with radio noise simulation enhances model robustness
- Smaller fine-tuned models outperform larger non-fine-tuned models under SWaP constraints

**Medium Confidence Claims**:
- WER improvement from >50% to 9.82% directly attributable to region-specific data and noise-robust training
- Poor generalization to Western datasets indicates successful accent specialization

**Low Confidence Claims**:
- 9.82% WER represents operationally acceptable threshold without field validation
- Specific augmentation techniques are optimal for all forms of radio transmission noise

## Next Checks

1. **Cross-Accent Generalization Test**: Evaluate fine-tuned model on additional Southeast Asian accent datasets from different countries to assess regional generalization capability

2. **Ablation Study on Augmentation Parameters**: Systematically vary frequency filter ranges, tanh distortion parameters, and application probabilities to determine most effective augmentation techniques

3. **SWaP Performance Profiling**: Measure inference time, memory consumption, and power usage for fine-tuned Small model versus larger alternatives across edge computing platforms to identify practical deployment sweet spot