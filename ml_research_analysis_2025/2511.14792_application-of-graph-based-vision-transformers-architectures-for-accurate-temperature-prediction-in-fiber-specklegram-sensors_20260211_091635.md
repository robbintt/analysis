---
ver: rpa2
title: Application of Graph Based Vision Transformers Architectures for Accurate Temperature
  Prediction in Fiber Specklegram Sensors
arxiv_id: '2511.14792'
source_url: https://arxiv.org/abs/2511.14792
tags:
- specklegram
- temperature
- attention
- vision
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the superior performance of transformer-based
  architectures, particularly Vision Transformers (ViTs) and Graph Attention Vision
  Transformers (GAT-ViTs), for temperature prediction using Fiber Specklegram Sensing
  (FSS) data. The research addresses the challenge of accurately predicting temperature
  from complex, nonlinear specklegram patterns in optical fiber sensors.
---

# Application of Graph Based Vision Transformers Architectures for Accurate Temperature Prediction in Fiber Specklegram Sensors

## Quick Facts
- arXiv ID: 2511.14792
- Source URL: https://arxiv.org/abs/2511.14792
- Reference count: 0
- Primary result: Vision Transformer achieves MAE of 1.15 for temperature prediction from specklegram images, outperforming traditional CNN approaches.

## Executive Summary
This study demonstrates that transformer-based architectures, particularly Vision Transformers (ViTs) and Graph Attention Vision Transformers (GAT-ViTs), significantly outperform traditional convolutional neural networks for temperature prediction using Fiber Specklegram Sensor (FSS) data. The research addresses the challenge of accurately predicting temperature from complex, nonlinear specklegram patterns in optical fiber sensors. By employing advanced transformer architectures, the study achieves substantial improvements over traditional models like CNNs and hybrid approaches. The findings establish transformer architectures as a new benchmark for optical fiber-based temperature sensing with promising applications in industrial monitoring and structural health assessments.

## Method Summary
The study uses a dataset of 601 simulated specklegram images (126×126 pixels) from OSF dataset, preprocessed with Sobel gradient computation, normalization, and resizing. Three main architectures were evaluated: Vision Transformer (ViT), Graph Attention Vision Transformer (GAT-ViT), and Multi-Adaptive Patch Graph Vision Transformer (MAP-ViGAT). ViT uses 16×16 patches, 64-dim embeddings, 4 transformer blocks with 4 attention heads, and a dense layer of 2048 neurons. GAT-ViTs incorporate graph attention mechanisms with different adjacency matrix strategies (spatial, learnable, feature-based). All models were trained with MSE loss, Adam optimizer, 0.5 dropout, and 70:20:10 train/test/val split for 100 epochs.

## Key Results
- Vision Transformer achieved the lowest MAE of 1.15, significantly outperforming CNNs and other transformer variants.
- GAT-ViT demonstrated competitive accuracy with MAE of 1.73, showing the effectiveness of graph-based attention mechanisms.
- MAP-ViGAT with spatial adjacency achieved MAE of 2.27, outperforming learnable (3.35) and feature-based (5.87) adjacency strategies.
- Gradient preprocessing consistently outperformed LBP preprocessing across all architectures.
- Swin Transformer variant achieved MAE of 7.27, indicating that global attention is more suitable than windowed attention for this application.

## Why This Works (Mechanism)

### Mechanism 1: Global Self-Attention for Capturing Modal Interference Dependencies
Vision Transformers capture both local intensity variations and global phase relationships in specklegram patterns by segmenting 126×126 specklegrams into 16×16 patches and computing pairwise dependencies between all patches simultaneously through multi-head self-attention. This enables learning relationships between distant interference regions that CNNs miss due to localized convolution kernels.

### Mechanism 2: Gradient-Based Preprocessing Enhances Discriminative Features
Gradient computation reveals temperature-sensitive structural patterns in specklegrams by calculating magnitude and direction of intensity changes at each pixel, highlighting boundaries between constructive and destructive interference regions. Temperature variations alter effective refractive indices and phase velocities, shifting interference patterns that manifest as intensity gradient changes more detectable than absolute intensity values.

### Mechanism 3: Graph Attention Networks Capture Adaptive Patch Relationships
GAT-ViTs and MAP-ViGAT variants construct adjacency matrices using spatial proximity, learnable parameters, or feature similarity, allowing dynamic importance allocation between patches rather than uniform attention. Spatial adjacency (MAE 2.27) outperformed feature-based adjacency (MAE 5.87), suggesting that physical proximity correlates with modal coupling in specklegram patterns.

## Foundational Learning

- Concept: Vision Transformer Patch Embeddings
  - Why needed here: Understanding how 2D specklegrams become sequences that transformers process is foundational to all architectures in this paper.
  - Quick check question: Given a 126×126 image with 16×16 patches, how many patch tokens does the transformer receive (excluding CLS token)?

- Concept: Multi-Head Self-Attention Mechanics
  - Why needed here: Core mechanism differentiating ViTs from CNNs; determines how global dependencies are computed.
  - Quick check question: If attention scores between all patch pairs are uniform (symmetric), what information is lost compared to non-symmetric attention?

- Concept: Graph Attention Network Adjacency Construction
  - Why needed here: MAP-ViGAT variants use three different adjacency strategies with markedly different performance (MAE 2.27 vs 5.87).
  - Quick check question: Why might spatial adjacency outperform feature-based adjacency for specklegram temperature sensing?

## Architecture Onboarding

- Component map: Input specklegram (126×126) -> Gradient preprocessing -> Patch segmentation (16×16) -> Patch embedding (64-dim) -> Positional encoding -> Transformer blocks (4×4-head MHSA+FFN) -> Global average pooling -> Dense (2048) -> Linear output

- Critical path:
  1. Implement gradient preprocessing pipeline first—paper shows this outperforms LBP
  2. Build baseline ViT with 4 blocks, 4 heads, 64-dim embeddings
  3. Add GAT layer with adjacency matrix construction for graph-based variants
  4. Implement residual connections for Swin-Residual variant

- Design tradeoffs:
  - ViT vs Swin: ViT's global attention (MAE 1.15) outperforms Swin's windowed attention (MAE 7.27) on unstructured speckle patterns—global context matters more than computational efficiency here
  - Adjacency strategy: Spatial (MAE 2.27) > Learnable (MAE 3.35) > Feature-based (MAE 5.87)—physical proximity correlates with modal coupling
  - Preprocessing: Gradient > LBP—intensity changes encode phase information better than local texture patterns

- Failure signatures:
  - High MAE (>5) with Swin variants: Windowed attention fragments global phase relationships; add residual connections or switch to ViT
  - Feature-based adjacency underperforming: Cosine similarity on raw embeddings may not capture physical adjacency; try spatial or learnable variants
  - Overfitting on small dataset (601 samples): Paper uses 0.5 dropout on 2048-dim dense layer; verify dropout is active

- First 3 experiments:
  1. Baseline ViT reproduction: Train custom ViT (4 blocks, 4 heads, 64-dim embeddings, 100 epochs, 70/20/10 split) on gradient-preprocessed specklegrams; target MAE <1.5
  2. Ablation: preprocessing comparison: Train identical ViT on (a) raw specklegrams, (b) LBP-processed, (c) gradient-processed; quantify preprocessing contribution
  3. Adjacency strategy evaluation: Train MAP-ViGAT with spatial, learnable, and feature-based adjacency matrices on same data; verify spatial adjacency superiority (expect ~2.3 MAE vs ~5.9 MAE)

## Open Questions the Paper Calls Out

### Open Question 1
Can transformer-based architectures maintain their superior performance on experimentally captured (real-world) FSS data, beyond the FEM-simulated dataset used in this study? The study relies entirely on synthetic data generated via finite element method (FEM) simulations with no validation on physical sensor data.

### Open Question 2
What computational optimizations are required to deploy ViT-based FSS models for real-time industrial monitoring? The conclusion states that future research could explore real-time deployment strategies, but the paper reports no inference time or latency metrics.

### Open Question 3
Why does the feature-based adjacency variant of MAP-ViGAT underperform compared to spatial and learnable variants? The feature-based MAP-ViGAT (S) achieved an MAE of 5.87, significantly worse than spatial (2.27) and learnable (3.35) variants.

### Open Question 4
Can hybrid architectures combining transformers with CNNs or other modules outperform pure ViT for FSS temperature prediction? The conclusion notes that further investigation of hybrid models could lead to even greater performance gains, but this study did not explore hybrid designs.

## Limitations

- Small dataset size (601 samples) raises concerns about model robustness and overfitting potential despite dropout regularization.
- Gradient preprocessing advantage over LBP lacks mechanistic explanation specific to optical fiber physics.
- GAT variants show significant performance variation (MAE 2.27-5.87) depending on adjacency strategy without full theoretical grounding.
- Absence of statistical significance testing between model variants limits confidence in claimed superiority.

## Confidence

- High Confidence: ViT architecture implementation and gradient preprocessing pipeline; these follow standard transformer and image processing practices with clear implementation paths.
- Medium Confidence: MAE performance claims; while the architecture is reproducible, training hyperparameters and initialization schemes require tuning that could affect final accuracy.
- Low Confidence: GAT-ViT adjacency strategy superiority claims; the mechanism explaining why spatial adjacency works better than feature-based approaches lacks theoretical grounding specific to optical fiber physics.

## Next Checks

1. Perform paired t-tests or bootstrap analysis on MAE differences between ViT and CNN models across multiple training runs to establish statistical significance of performance gains.
2. Implement k-fold cross-validation (k=5) on the 601-sample dataset to assess model stability and variance across different data splits.
3. Systematically compare raw specklegrams, LBP, and gradient preprocessing on identical ViT architectures to isolate the exact contribution of gradient preprocessing to performance improvements.