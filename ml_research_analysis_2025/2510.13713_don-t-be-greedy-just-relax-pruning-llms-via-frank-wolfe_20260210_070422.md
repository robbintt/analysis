---
ver: rpa2
title: Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe
arxiv_id: '2510.13713'
source_url: https://arxiv.org/abs/2510.13713
tags:
- pruning
- mask
- wanda
- sparsefw
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pruning large language models
  (LLMs) by formulating the layerwise mask selection as a convex optimization problem
  over the convex hull of binary masks, then solving it using the Frank-Wolfe (FW)
  algorithm. The key method, SparseFW, relaxes the combinatorial mask selection problem
  into a tractable convex program and employs FW's projection-free updates with an
  efficient linear minimization oracle.
---

# Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe

## Quick Facts
- arXiv ID: 2510.13713
- Source URL: https://arxiv.org/abs/2510.13713
- Reference count: 29
- Key outcome: Frank-Wolfe relaxation for layerwise mask selection reduces pruning error by up to 80% vs greedy methods

## Executive Summary
This paper introduces SparseFW, a novel approach to pruning large language models using the Frank-Wolfe algorithm. Rather than using greedy methods that select weights independently, SparseFW formulates the layerwise mask selection problem as a convex optimization over the convex hull of binary masks. This allows the method to account for weight interactions when determining the optimal sparsity pattern. The approach is shown to consistently outperform state-of-the-art pruning methods across multiple modern GPT architectures at high sparsity levels.

## Method Summary
SparseFW relaxes the combinatorial mask selection problem into a tractable convex program and solves it using Frank-Wolfe's projection-free updates. The method employs an efficient linear minimization oracle to navigate the convex hull of binary masks, capturing weight interactions that greedy approaches miss. A key practical insight is that fixing 90% of high-saliency weights from a warmstart before applying FW yields the best performance. The approach provides theoretical guarantees showing that rounding the FW solution provides an approximate solution to the original combinatorial problem.

## Key Results
- Reduces per-layer pruning error by up to 80% compared to greedy baselines
- Improves WikiText perplexity and zero-shot accuracy over state-of-the-art methods (Wanda, RIA)
- Consistent improvements across multiple GPT architectures (LLaMA 3, Qwen 2.5) at 50%, 60%, and 2:4 sparsity levels

## Why This Works (Mechanism)
The Frank-Wolfe algorithm is well-suited for this problem because it can efficiently optimize over the convex hull of binary masks without explicitly enumerating them. By relaxing the discrete mask selection into a continuous optimization problem, FW can find better solutions that account for interactions between weights. The linear minimization oracle efficiently finds the most promising direction at each iteration, while the convex structure ensures convergence to a good solution. The warmstart heuristic of fixing high-saliency weights provides a strong initialization that the FW algorithm can refine.

## Foundational Learning

**Convex optimization**: Needed to understand why FW can solve the relaxed problem efficiently; Quick check: verify convexity of the mask selection objective

**Frank-Wolfe algorithm**: Essential for understanding the projection-free updates and linear minimization oracle; Quick check: confirm FW convergence rates for the problem structure

**Combinatorial optimization**: Provides context for why mask selection is challenging and benefits from relaxation; Quick check: compare discrete vs relaxed problem complexity

**Sparsity in neural networks**: Background on why pruning matters and common approaches; Quick check: understand impact of different sparsity patterns on model performance

## Architecture Onboarding

**Component map**: Layerwise mask selection -> Frank-Wolfe optimization -> Linear minimization oracle -> Mask rounding

**Critical path**: The most critical component is the linear minimization oracle, as its efficiency directly determines the overall runtime of SparseFW. Poor oracle implementation would make the method impractical.

**Design tradeoffs**: The relaxation introduces approximation error but gains tractability. The warmstart heuristic trades off some flexibility for better convergence. Layerwise optimization ignores cross-layer interactions for computational efficiency.

**Failure signatures**: If the linear minimization oracle becomes too expensive, the method will be impractical. If the relaxation gap is too large, the rounded solution may perform poorly. Poor warmstart choices can lead to suboptimal local minima.

**First experiments**:
1. Compare FW convergence on synthetic mask selection problems vs greedy baselines
2. Benchmark linear minimization oracle runtime across different model sizes
3. Test sensitivity to warmstart threshold (e.g., 80%, 90%, 95% fixed weights)

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from Frank-Wolfe iterations not fully characterized
- Layerwise approach ignores cross-layer sparsity patterns
- Warmstart heuristic lacks theoretical justification for the 90% threshold
- Limited evaluation to GPT-style architectures may not generalize

## Confidence

High: Frank-Wolfe relaxation framework is mathematically well-defined
Medium: Empirical improvements show varying magnitudes across architectures
Medium: Computational overhead characterization is incomplete

## Next Checks

1. Benchmark SparseFW against ADMM and proximal algorithms on diverse model architectures beyond GPT-style transformers
2. Conduct ablation studies on warmstart thresholds to find optimal settings across different model scales
3. Test robustness to task/domain shifts with out-of-distribution data and few-shot adaptation scenarios