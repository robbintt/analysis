---
ver: rpa2
title: 'DAM: Domain-Aware Module for Multi-Domain Dataset Condensation'
arxiv_id: '2505.22387'
source_url: https://arxiv.org/abs/2505.22387
tags:
- domain
- dataset
- condensation
- setting
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of dataset condensation under multi-domain
  settings, where existing methods fail to generalize due to domain heterogeneity.
  The authors introduce the Domain-Aware Module (DAM), a training-time plug-in that
  learns spatial domain masks and leverages frequency-based pseudo-domain labeling
  to encode domain-specific features into each synthetic image without explicit domain
  supervision.
---

# DAM: Domain-Aware Module for Multi-Domain Dataset Condensation

## Quick Facts
- **arXiv ID**: 2505.22387
- **Source URL**: https://arxiv.org/abs/2505.22387
- **Reference count**: 40
- **Primary result**: Domain-aware module improves dataset condensation generalization across multiple domains without explicit supervision

## Executive Summary
This paper addresses the challenge of dataset condensation in multi-domain settings where existing methods struggle due to domain heterogeneity. The authors propose the Domain-Aware Module (DAM), a training-time plug-in that learns spatial domain masks and leverages frequency-based pseudo-domain labeling to encode domain-specific features into synthetic images without requiring explicit domain supervision. DAM demonstrates consistent performance improvements across five datasets and three architectures, enhancing both in-domain and cross-domain generalization while maintaining class balance.

## Method Summary
The Domain-Aware Module (DAM) is designed as a training-time plug-in that operates alongside existing dataset condensation frameworks. It consists of two key components: a frequency-based pseudo-domain labeling mechanism that divides each image into multiple pseudo-domains based on frequency components, and a spatial mask learning module that predicts domain-specific masks for each pseudo-domain. During training, DAM learns to encode domain-specific features into synthetic images by applying these spatial masks to the frequency-based domain partitions. The module operates without requiring explicit domain supervision, making it compatible with existing condensation methods. DAM can be integrated into various architectures including ConvNet, VGG, and ViT variants, and maintains the original number of images per class while improving generalization across different domains.

## Key Results
- On PACS dataset with 10 images per class, DC + DAM improves accuracy from 46.1% to 50.9%
- DM + DAM achieves 50.9% accuracy on PACS, matching DC + DAM performance
- DAM enhances cross-architecture transfer and shows robustness to varying pseudo-domain counts across five benchmark datasets

## Why This Works (Mechanism)
DAM addresses the domain heterogeneity problem in dataset condensation by learning spatial domain masks and leveraging frequency-based pseudo-domain labeling to encode domain-specific features into synthetic images without explicit domain supervision.

## Foundational Learning

**Dataset Condensation**: Technique to synthesize small synthetic datasets that retain knowledge from large original datasets for efficient training. Needed because training on condensed datasets is faster and more resource-efficient than using full datasets.

**Domain Adaptation**: Process of adapting models trained on one domain to perform well on another related but different domain. Required here because multi-domain datasets contain inherent domain shifts that affect generalization.

**Frequency-Based Domain Partitioning**: Method of dividing images into pseudo-domains based on their frequency components. Used to create meaningful domain partitions without requiring explicit domain labels.

**Spatial Mask Learning**: Process of learning domain-specific spatial masks that encode which regions of an image belong to which domain. Essential for capturing local domain-specific features in synthetic images.

**Cross-Domain Generalization**: Model's ability to perform well on domains different from those seen during training. Critical metric for evaluating DAM's effectiveness in multi-domain settings.

## Architecture Onboarding

**Component Map**: Input Image -> Frequency Domain Partitioning -> Pseudo-Domain Labels -> Spatial Mask Prediction -> DAM Module -> Feature Encoding -> Output for DC Framework

**Critical Path**: The DAM module operates during the condensation training phase, where it learns spatial masks for frequency-based pseudo-domains and applies these masks to encode domain-specific features into the synthetic images being generated.

**Design Tradeoffs**: DAM trades computational overhead during training for improved generalization, while avoiding the need for explicit domain supervision that would require additional labeling effort. The module preserves class balance but introduces sensitivity to the number of pseudo-domains selected.

**Failure Signatures**: Poor performance may manifest as overfitting to dataset-specific patterns rather than learning meaningful domain features, particularly if the pseudo-domain count is too high or too low relative to actual domain diversity.

**Three First Experiments**:
1. Test DAM integration with DC framework on CIFAR-10 with varying pseudo-domain counts (2, 4, 8)
2. Evaluate cross-domain transfer from CIFAR-10 to SVHN with DAM vs baseline
3. Measure training time overhead when DAM is added to existing condensation pipelines

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation focuses primarily on small-scale datasets and simpler architectures, raising questions about scalability to larger vision tasks
- Sensitivity to pseudo-domain count hyperparameter lacks systematic guidance for optimal domain selection
- Absence of explicit domain labels makes it difficult to verify whether learned spatial masks capture meaningful domain features or memorize dataset patterns
- Computational overhead introduced by DAM during training is not quantified

## Confidence
- **High confidence**: DAM's ability to improve in-domain performance across multiple datasets and architectures
- **Medium confidence**: DAM's cross-domain generalization benefits, as validation is limited to domain-shift benchmarks (PACS, VLCS, Office-Home)
- **Medium confidence**: DAM's effectiveness with varying pseudo-domain counts, given the lack of theoretical justification for domain number selection

## Next Checks
1. Evaluate DAM on larger-scale datasets (e.g., ImageNet-1K, COCO) and modern architectures (e.g., ConvNeXt, Swin Transformer) to assess scalability
2. Conduct ablation studies with varying pseudo-domain counts on held-out domains to determine optimal domain granularity
3. Measure and report computational overhead (training time, memory usage) when DAM is integrated into different condensation frameworks