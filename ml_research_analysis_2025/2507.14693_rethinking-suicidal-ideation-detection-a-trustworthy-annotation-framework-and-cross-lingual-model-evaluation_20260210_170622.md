---
ver: rpa2
title: 'Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework
  and Cross-Lingual Model Evaluation'
arxiv_id: '2507.14693'
source_url: https://arxiv.org/abs/2507.14693
tags:
- suicidal
- posts
- ideation
- dataset
- suicide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the critical need for reliable suicidal ideation
  detection across languages, highlighting two key challenges: limited language coverage
  and unreliable annotation practices. The authors introduce a novel Turkish suicidal
  ideation corpus derived from social media posts and propose a resource-efficient
  annotation framework involving three human annotators and two large language models
  (LLMs).'
---

# Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation

## Quick Facts
- arXiv ID: 2507.14693
- Source URL: https://arxiv.org/abs/2507.14693
- Reference count: 31
- Key outcome: Introduces Turkish suicidal ideation corpus and resource-efficient annotation framework; reveals questionable performance of popular models with zero-shot transfer learning (F1 scores 16%-96% across datasets)

## Executive Summary
This study addresses the critical need for reliable suicidal ideation detection across languages by introducing a novel Turkish corpus and evaluating annotation quality and model performance. The authors propose a hybrid human-LLM annotation framework to reduce expert burden while maintaining reliability. Through cross-dataset transfer learning experiments, they expose significant discrepancies in model performance between gold-standard and auto-labeled datasets, suggesting popular models may be learning superficial patterns rather than genuine ideation signals.

## Method Summary
The authors constructed a Turkish suicidal ideation corpus from Ekşi Sözlük social media posts using a three-tier annotation framework: two human annotators label all data, disagreements resolved by LLMs for binary cases and experts for complex cases. They evaluated label reliability and model consistency across this dataset and three English datasets using eight pre-trained sentiment and emotion classifiers, computing inter-model agreement and performance metrics through zero-shot transfer learning.

## Key Results
- F1 scores ranged from 16% to 96% across different datasets and models, with SENTINET achieving 54.7% on gold-standard C-SSRS vs. 96.6% on auto-labeled SDD
- Turkish sentiment models showed near-zero MCC (0.098) between each other, indicating they capture different patterns
- English emotion models on C-SSRS showed MCC=0.215, suggesting insufficient convergent validity
- 231 posts required expert-only resolution, highlighting the complexity of suicidal ideation annotation

## Why This Works (Mechanism)

### Mechanism 1
A tiered human-LLM annotation framework can produce reliable mental health labels with constrained resources. Two human annotators label all data; disagreements route through LLMs for non-sensitive cases or a domain expert for sensitive cases. This preserves reliability while reducing expert burden. Core assumption: LLMs provide useful signal when disagreement is about label nuance rather than presence of ideation. Evidence: Decision tree in Figure 2 shows 231 posts required expert resolution. Break condition: If LLMs systematically misclassify ambiguous ideation.

### Mechanism 2
Cross-dataset transfer evaluation exposes whether models learn domain cues versus ideation signals. Apply pre-trained sentiment/emotion classifiers to multiple datasets with varying annotation quality. Divergent performance patterns reveal whether models exploit surface features or capture underlying constructs. Core assumption: If models learned ideation cues, performance should generalize across datasets. Evidence: SENTINET achieves 54.7% F1 on C-SSRS (gold-standard) vs. 96.6% on SDD (auto-labeled). Break condition: If auto-labeled datasets contain valid signal, poor gold-standard performance may indicate dataset shift.

### Mechanism 3
Low inter-model agreement on emotion/sentiment classification signals insufficient grounding for mental health inference. Compute Matthews Correlation Coefficient (MCC) between transformer predictions on the same data. Near-zero MCC indicates models capture different patterns, undermining trust in any single model's outputs. Core assumption: Convergent validity across independently trained models indicates genuine signal detection. Evidence: Turkish sentiment models show MCC=0.098; emotion models show MCC=0.241. Break condition: Low MCC could reflect valid model specialization rather than unreliability.

## Foundational Learning

- **Concept: Transfer learning from pre-trained transformers**
  - Why needed here: All experiments use off-the-shelf fine-tuned models (BERT, RoBERTa, DistilBERT) applied zero-shot to new datasets. Understanding what these models encode is essential for interpreting results.
  - Quick check question: Can you explain why a model fine-tuned on subreddit labels might fail on clinically annotated data from the same platform?

- **Concept: Annotation reliability vs. label validity**
  - Why needed here: The paper distinguishes inter-annotator agreement (reliability) from whether labels measure the intended construct (validity). Auto-labeled datasets may be reliable but invalid.
  - Quick check question: If two annotators agree perfectly but both misunderstand the labeling criteria, is the dataset high-quality?

- **Concept: Evaluation metric selection for imbalanced multi-class problems**
  - Why needed here: Paper uses macro F1, AUC, and MCC because accuracy is misleading when classes are imbalanced and multi-class comparisons are needed.
  - Quick check question: Why is MCC preferred over simple accuracy when comparing two classifiers on multi-class mental health labels?

## Architecture Onboarding

- **Component map:**
  Data layer: Ekşi Sözlük scraper → raw posts → annotation pipeline (2 annotators + 2 LLMs + 1 expert)
  Model layer: 4 Turkish transformers + 4 English transformers
  Evaluation layer: Confusion matrices → MCC, macro F1, AUC computation → cross-dataset comparison

- **Critical path:**
  1. Define annotation schema (4 labels → binary collapse)
  2. Annotate with disagreement resolution protocol
  3. Run all 8 transformers on all 4 datasets
  4. Compute agreement metrics between models and against ground truth

- **Design tradeoffs:**
  - Controlled label noise (380 random assignments) increases dataset size but introduces irreducible error
  - Binary collapse (Positive+Mixed vs. Negative+Other) loses nuance but enables clearer reliability assessment
  - Using off-the-shelf models avoids fine-tuning costs but limits task-specific performance

- **Failure signatures:**
  - Near-random F1 on gold-standard data with near-perfect F1 on auto-labeled data → model learning shortcuts
  - Near-zero MCC between models on same task → no convergent signal
  - Sentiment models classifying suicidal posts as "neutral" → failure to capture context

- **First 3 experiments:**
  1. Replicate the MCC analysis on your own mental health dataset using 2+ transformers from Hugging Face to establish baseline reliability
  2. Test whether removing subreddit identifiers from SDD/SWMH data degrades model performance (tests shortcut hypothesis)
  3. Build a minimal annotation pipeline with 2 human annotators + 1 LLM on 100 posts, measuring inter-annotator agreement with and without LLM assistance

## Open Questions the Paper Calls Out

### Open Question 1
Are current suicidal ideation detection models learning actual psychological markers, or merely superficial patterns like subreddit identifiers and community-specific phrasing? The authors state that models performed significantly better on auto-labeled SDD and SWMH datasets despite minimal training overlap, suggesting they are "learning superficial patterns such as subreddit identifiers (e.g., titles, which were the labeling factor) or community-specific phrasing rather than the underlying cues of suicidal ideation." This requires controlled experiments with datasets that have both human-validated labels and content stripped of identifying metadata.

### Open Question 2
What training methodology and data quantity are needed for Turkish NLP models to reliably detect nuanced, context-sensitive mental health signals? Despite the gold-standard Turkish dataset, "the selected four Turkish transformer models failed to capture different sentiments and emotions between suicidal and non-suicidal posts," with MCC scores close to zero between models, indicating fundamental limitations in available Turkish transformers. Turkish mental health NLP resources remain critically underdeveloped.

### Open Question 3
How can annotation frameworks balance scalability with reliability when labeling ambiguous suicidal ideation content? The authors "offer this pipeline as a blueprint for building high-reliability mental health datasets in all languages" but the process still required substantial human effort: "the 1,335 posts with complete disagreement" required expert consultation. The tradeoff between annotation cost, speed, and accuracy in mental health contexts remains unquantified.

## Limitations

- The Turkish dataset remains inaccessible due to ethical and legal constraints, preventing independent verification of annotation quality and model performance
- LLM prompts and inference parameters (temperature, batch size, threshold settings) are unspecified, introducing potential variability in replication
- No baseline comparison exists for the proposed hybrid annotation framework against traditional expert-only approaches

## Confidence

- **High confidence** in the cross-dataset transfer learning methodology and its ability to expose annotation quality differences between gold-standard and auto-labeled datasets
- **Medium confidence** in the proposed annotation framework's effectiveness, supported by the expert resolution step but limited by lack of comparative validation
- **Low confidence** in specific numerical results without access to the original Turkish dataset or complete replication details

## Next Checks

1. Replicate the MCC analysis using two independent transformers on an existing mental health dataset to establish baseline inter-model agreement patterns
2. Test whether removing subreddit identifiers from SDD/SWMH data significantly impacts model performance, directly testing for shortcut learning
3. Implement a minimal version of the hybrid annotation framework (2 humans + 1 LLM) on 100 mental health-related social media posts, measuring inter-annotator agreement with and without LLM assistance