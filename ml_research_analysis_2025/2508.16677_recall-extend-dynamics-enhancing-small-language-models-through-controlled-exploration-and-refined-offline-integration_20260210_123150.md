---
ver: rpa2
title: 'Recall-Extend Dynamics: Enhancing Small Language Models through Controlled
  Exploration and Refined Offline Integration'
arxiv_id: '2508.16677'
source_url: https://arxiv.org/abs/2508.16677
tags:
- arxiv
- training
- reasoning
- offline
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of enhancing reasoning capabilities
  in small language models (SLMs) through improved integration of offline distillation
  data and online reinforcement learning. The proposed Recall-Extend Dynamics (RED)
  method introduces two key innovations: (1) dynamic entropy regulation that balances
  "Recall" (refining existing capabilities) and "Extend" (acquiring new knowledge)
  phases, and (2) accuracy-aware policy shifts that adaptively integrate offline distilled
  data into RL training.'
---

# Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration

## Quick Facts
- arXiv ID: 2508.16677
- Source URL: https://arxiv.org/abs/2508.16677
- Authors: Zhong Guan; Likang Wu; Hongke Zhao; Jiahui Wang; Le Wu
- Reference count: 5
- Primary result: RED achieves 41.6% overall accuracy on Qwen2.5-Math-1.5B, outperforming state-of-the-art methods including 39.74% for the best baseline.

## Executive Summary
This paper introduces Recall-Extend Dynamics (RED), a method that enhances reasoning capabilities in small language models (SLMs) by integrating offline distillation data with online reinforcement learning. The approach addresses two key challenges: insufficient exploration space in SLMs and instability when combining offline and online training. RED introduces dynamic entropy regulation that balances "Recall" (refining existing capabilities) and "Extend" (acquiring new knowledge) phases, along with accuracy-aware policy shifts that adaptively integrate offline distilled data into RL training. Experiments on mathematical reasoning benchmarks demonstrate significant performance improvements over state-of-the-art methods.

## Method Summary
RED combines offline distilled trajectories from teacher models with online RLVR training through a unified objective. The method monitors entropy changes between RL and SFT phases to regulate the weight of offline data contribution, dynamically balancing exploration (Extend via SFT) and refinement (Recall via RL). An accuracy-aware policy shift estimates the offline policy probability using problem difficulty to prevent both entropy collapse and performance degradation when integrating distilled data. The unified training paradigm allows the model to learn from both self-generated rollouts and expert demonstrations simultaneously, rather than sequentially.

## Key Results
- RED achieves 41.6% overall accuracy on Qwen2.5-Math-1.5B across five benchmarks (MATH500, AIME, AMC, Minerva, Olympiad)
- Outperforms best baseline (39.74%) and other unified training methods like LUFFY (39.1%) and SRFT (39.1%)
- Improves reasoning efficiency by reducing redundant generation while maintaining or improving accuracy
- Ablation studies confirm both dynamic entropy regulation and accuracy-aware policy shift components are essential for performance

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Entropy Regulation for Recall-Extend Balance
The method computes weight w = ∆H_sft / ∆H_rl, where ∆H represents entropy change ratios between training steps. When RL entropy change is small (limited exploration), SFT contribution increases to expand exploration space. When RL entropy changes actively, SFT influence decreases. This addresses insufficient exploration in SLMs by adaptively balancing online refinement with offline knowledge expansion.

### Mechanism 2: Accuracy-aware Policy Shift for Offline Data Integration
The policy deviation term π_offline = π + (1 - π) × r_mean_group dynamically adjusts importance sampling. High-accuracy samples use π_offline ≈ 1 (trust current policy), while low-accuracy samples push π_offline toward π (imitate teacher). This prevents entropy collapse when π_offline = 1 and mid-training performance collapse when π_offline = π.

### Mechanism 3: Unified Training Paradigm with Off-policy Correction
The unified gradient combines on-policy RL samples with offline SFT samples through a shared objective, where the offline term includes the importance ratio r_offline = π/π_offline. This creates stable training superior to staged SFT→RL approaches by learning from both self-generated rollouts and expert demonstrations simultaneously.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed: RED builds on GRPO's policy optimization framework; understanding advantage estimation and clipping is prerequisite
  - Quick check: Can you explain how GRPO estimates advantages within a group of sampled responses and why clipping stabilizes training?

- **Concept: Entropy as Exploration Proxy**
  - Why needed: The core innovation relies on entropy dynamics to signal exploration state
  - Quick check: Does high entropy always indicate useful exploration, or can it also signal confused/low-quality outputs? How might this vary for SLMs versus LLMs?

- **Concept: Importance Sampling for Off-policy Learning**
  - Why needed: The accuracy-aware policy shift uses importance ratios to correct distribution mismatch
  - Quick check: What happens to gradient estimates when the importance ratio becomes very large or very small, and why does clipping help?

## Architecture Onboarding

- **Component map:**
  - Data streams: Online rollouts (π_θ samples G responses per query) + Offline distilled trajectories (q, o_sft) from teacher model
  - Entropy monitor: Computes ∆H_rl and ∆H_sft at each step by tracking token-level entropy changes
  - Weight calculator: w = clip(∆H_sft / ∆H_rl, 1, G) determines SFT contribution
  - Policy shift estimator: Computes π_offline = π + (1 - π) × r_mean_group using running accuracy statistics
  - Unified optimizer: Single gradient step combining weighted RL and SFT terms with importance correction

- **Critical path:**
  1. Sample G responses from current policy → compute RL entropy, rewards, advantages
  2. Retrieve matching offline trajectory → compute SFT entropy
  3. Calculate entropy change ratios → determine weight w
  4. Estimate π_offline from accuracy statistics → compute importance ratio
  5. Backpropagate unified loss with dynamic weighting
  6. Update running accuracy estimates for next iteration

- **Design tradeoffs:**
  - Entropy window size: Too short → noisy weight fluctuations; too long → slow adaptation to exploration state changes
  - Accuracy smoothing: Aggressive updates to r_mean_group → unstable π_offline; conservative updates → slow correction for distribution shift
  - Weight bounds [1, G]: Narrow range → limited SFT influence; wide range → potential instability if SFT dominates

- **Failure signatures:**
  - Entropy collapse (both RL and SFT entropy drop sharply): Indicates π_offline estimation is failing
  - Response length explosion without accuracy gain: SFT weight may be too high
  - Mid-training performance crash: Typical of π_offline = π configuration
  - Weight stuck at bounds (1 or G): Entropy dynamics may be uninformative

- **First 3 experiments:**
  1. **Ablate each component separately**: Train with (I) only dynamic entropy regulation and (II) only accuracy-aware shifts to isolate contributions
  2. **Monitor entropy dynamics visualization**: Track ∆H_rl, ∆H_sft, and w across training steps on held-out validation set
  3. **Stress test π_offline estimation**: Compare accuracy-aware shift against fixed alternatives on subset with intentionally noisy offline data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RED maintain efficiency and performance advantages when scaled to models significantly larger than 1.5B parameters?
- Basis: The study focuses exclusively on Qwen2.5-1.5B, noting RLVR research on smaller models is under-explored
- Why unresolved: Dynamic entropy regulation addresses "insufficient exploration space in small models," a constraint that may differ in larger architectures
- What evidence would resolve it: Application to 7B and 32B parameter models with direct comparisons to standard RLVR baselines

### Open Question 2
- Question: Is the entropy-based balancing mechanism effective for reasoning domains with less verifiable or structured outputs than mathematics?
- Basis: All experimental results are confined to mathematical reasoning benchmarks
- Why unresolved: Mathematical reasoning allows for distinct, verifiable steps which may correlate cleanly with entropy changes; open-ended tasks may exhibit different entropy dynamics
- What evidence would resolve it: Evaluation on non-mathematical reasoning tasks (e.g., MBPP for code or GSM8K variants)

### Open Question 3
- Question: How robust is the accuracy-aware policy shift mechanism when offline distillation data contains high frequency of incorrect or noisy reasoning traces?
- Basis: The method relies on problem difficulty (accuracy) to estimate policy deviation, assuming correlation between correctness and policy certainty
- Why unresolved: If offline data is noisy or difficult problems are mislabeled, the mechanism might incorrectly force the model to "imitate" flawed trajectories
- What evidence would resolve it: Ablation studies introducing synthetic noise into distillation dataset to observe if accuracy-aware shift degrades or successfully filters low-quality data

## Limitations
- The entropy-based exploration proxy may not generalize beyond mathematical reasoning tasks with structured, verifiable outputs
- The accuracy-aware policy shift assumes offline data quality correlates with correctness, which may not hold for noisy or systematically flawed distillation data
- The method requires careful tuning of entropy window size and accuracy smoothing parameters, which may not transfer easily across different model architectures

## Confidence

- **High confidence**: The unified training framework with off-policy correction is technically sound and builds on established RL methods
- **Medium confidence**: The accuracy-aware policy shift mechanism appears plausible and is supported by ablation studies
- **Low confidence**: The dynamic entropy regulation mechanism lacks direct empirical validation of the core assumption that entropy ratios meaningfully signal exploration vs refinement needs

## Next Checks

1. **Entropy dynamics validation**: Generate and plot entropy curves (∆H_rl, ∆H_sft, w) during training to verify that weight adjustments correspond to meaningful exploration state changes rather than noise or edge-case behavior.

2. **Offline data quality stress test**: Intentionally corrupt a subset of offline trajectories with incorrect answers and verify that accuracy-aware π_offline degrades gracefully while fixed alternatives (π_offline=1 or π_offline=π) show sharper performance collapse.

3. **Generalization beyond math**: Test RED on non-mathematical reasoning tasks (e.g., commonsense reasoning benchmarks) to verify that the Recall-Extend mechanism transfers beyond the specific mathematical reasoning domain where it was developed.