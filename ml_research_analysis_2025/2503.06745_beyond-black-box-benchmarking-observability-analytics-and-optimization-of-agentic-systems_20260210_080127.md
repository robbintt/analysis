---
ver: rpa2
title: 'Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization
  of Agentic Systems'
arxiv_id: '2503.06745'
source_url: https://arxiv.org/abs/2503.06745
tags:
- agentic
- systems
- system
- analytics
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies critical gaps in agentic system evaluation,
  demonstrating that traditional benchmarking fails to capture non-deterministic execution
  flows and performance variability. Through empirical experiments on a calculator
  agentic system and a user study of 38 practitioners, the authors show that identical
  inputs can produce divergent execution paths and outputs (mean CV 63% in execution
  flow, 19% in accuracy for natural language inputs).
---

# Beyond Black-Box Benchmarking: Observability, Analytics, and Optimization of Agentic Systems

## Quick Facts
- arXiv ID: 2503.06745
- Source URL: https://arxiv.org/abs/2503.06745
- Authors: Dany Moshkovich; Hadar Mulian; Sergey Zeltyn; Natti Eder; Inna Skarbovsky; Roy Abitbol
- Reference count: 29
- Primary result: Agentic systems show 63% CV in execution flow variability and 19% accuracy variation for natural language inputs, requiring behavioral benchmarking beyond outcome-only metrics

## Executive Summary
This paper identifies critical gaps in agentic system evaluation, demonstrating that traditional benchmarking fails to capture non-deterministic execution flows and performance variability. Through empirical experiments on a calculator agentic system and a user study of 38 practitioners, the authors show that identical inputs can produce divergent execution paths and outputs. They propose a novel Agentic System Behavioral Benchmarking approach that extends observability frameworks with GenAI Events to capture lifecycle and state changes, and introduce ABBench - a benchmark dataset of 30 structured logs with ground truth analytics outcomes. Their proprietary TAMAS system demonstrates the approach, correctly identifying issues in 60% of benchmark cases, validating the need for behavioral benchmarking beyond classical methods.

## Method Summary
The authors develop a behavioral benchmarking framework that extends OpenTelemetry with GenAI Events to capture agentic system lifecycle and state changes. They create ABBench, a dataset of 30 agent runtime logs from a calculator agentic system, manually annotated with ground truth task flows, metrics, and failure classifications. The TAMAS analytics system reconstructs hierarchical task structures as DAGs and compares its outputs against ground truth to identify detection gaps. Experiments measure execution flow variability (63% CV) and accuracy variation (19% CV) across repeated runs with identical inputs, demonstrating the limitations of outcome-only benchmarking.

## Key Results
- Identical inputs produce divergent execution flows with 63% coefficient of variation in task counts
- Natural language inputs show 19% accuracy variation despite identical mathematical content
- TAMAS analytics system achieves 60% match rate against ground truth task flows and failure classifications
- Four distinct failure types identified: instruction violations, incorrect inputs, validation failures, and validator failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral benchmarking reveals system characteristics that outcome-only metrics miss by capturing execution patterns rather than final outputs.
- Mechanism: By analyzing task flows (hierarchical breakdown of tasks into subtasks as DAGs), variability patterns (measured via graph-edit distance), and failure modes (instruction violations, validation failures, incorrect inputs, validator failures), the approach detects issues even when final outputs appear correct—e.g., a calculator returning the right answer through a flawed decomposition process.
- Core assumption: Agentic system failures leave traces in execution patterns that are invisible to outcome metrics.
- Evidence anchors: [abstract] shows "63% coefficient of variation in task counts" and "19% accuracy variation in natural language inputs" demonstrating variability invisible to black-box approaches.

### Mechanism 2
- Claim: Ground truth (GT)-based recursive benchmarking creates a validation loop that quantifies analytics system limitations.
- Mechanism: Manual annotation of task flows, metrics, and failure classifications for each log establishes a gold standard. Comparing analytics outputs against GT (e.g., TAMAS achieving 60% match) reveals detection gaps—such as missing "validator failures" where correct actions fail validation checks.
- Core assumption: Human experts can consistently identify ground truth analytics outcomes for agentic logs.
- Evidence anchors: [abstract] states the benchmark enables "comparing their outputs against ground truth task flows, metrics, and failure classifications."

### Mechanism 3
- Claim: Extending OpenTelemetry with GenAI Events captures agentic state transitions that standard tracing misses.
- Mechanism: GenAI Events bind to spans to track entity lifecycle transitions (creation, start, end, suspension, failure, deletion) with severity-categorized issues. This enables task flow discovery algorithms to reconstruct hierarchical execution dependencies and detect failure patterns across distributed multi-agent executions.
- Core assumption: Developers will instrument agentic systems to emit standardized events at critical state transitions.
- Evidence anchors: [section 4.3] shows Events "track key transitions, including creation, update, start, end, suspension, abortion, failure, and deletion" with "associated issues categorized by severity."

## Foundational Learning

- Concept: **Distributed Tracing and Spans (OpenTelemetry)**
  - Why needed here: The paper extends OTel's span-based tracing with GenAI Events. Understanding parent-child span relationships and context propagation is prerequisite to grasping how task flow hierarchies are reconstructed.
  - Quick check question: Can you explain what a "span" represents and how parent-child relationships enable reconstructing a distributed request's execution path?

- Concept: **Coefficient of Variation (CV)**
  - Why needed here: The paper quantifies flow variability (63% CV) and NL accuracy variation (19% CV) to demonstrate non-determinism. CV = (standard deviation / mean) enables comparison across metrics with different scales.
  - Quick check question: If mean execution time is 10s with CV=45%, what's the approximate standard deviation? Why use CV instead of raw variance for comparing cost vs. latency variability?

- Concept: **Ground Truth Benchmarking Methodology**
  - Why needed here: ABBench evaluates analytics systems (not agentic systems directly) by comparing their outputs against curated GT. Understanding this recursive evaluation is essential—the unit is a trace log, not a task.
  - Quick check question: Why is ground truth essential for evaluating an analytics system? What happens if GT annotations contain errors?

## Architecture Onboarding

- Component map: Observability Layer (extended OTel → GenAI Events → traces) -> Analytics Engine (task flow discovery → metric aggregation → failure classification) -> Benchmark Framework (30 curated logs → GT annotations → comparison metrics) -> TAMAS implementation

- Critical path: 1) Instrument agentic system with GenAI Events 2) Collect trace logs across diverse inputs 3) Manually annotate ground truth task flows and failures 4) Implement task flow discovery algorithm 5) Compare analytics outputs against GT 6) Iterate on failure classification rules

- Design tradeoffs: Passive vs. interventional analytics (passive is safer; interventional requires explicit approval per user study showing only 16% trust automation), enhanced monitoring scope (detailed logging aids debugging but increases cost), GT annotation depth (more granular failure types improve specificity but increase annotation burden), task granularity (finer decomposition improves quality detection but increases latency and cost)

- Failure signatures: Instruction violations (agent ignores prompt directives), validation failures (correct-seeming intermediate outputs fail downstream checks), validator failures (correct actions incorrectly flagged), incorrect input failures (parsing/interpretation errors on ambiguous NL), empty flow files (analytics system cannot parse logs)

- First 3 experiments: 1) Baseline variability measurement: Run calculator system on 50 examples × 5 repetitions; compute CV for task count, cost, latency, and accuracy using graph-edit distance for flow comparison 2) Task flow discovery validation: Apply task flow discovery algorithm to 5 curated logs; manually compare discovered DAGs against GT task flows 3) Failure detection gap analysis: Run analytics system on ABBench logs; compare detected failures against GT failure lists

## Open Questions the Paper Calls Out

- Question: How can agent analytics systems be designed to detect "validator failures" (where correct agent actions are incorrectly rejected by validation modules), which current systems like TAMAS cannot identify?
- Basis in paper: [explicit] The authors state "TAMAS cannot detect the validator failures and, therefore, detects less failures than the Ground Truth."
- Question: Can automated prompt engineering techniques reduce the coefficient of variation in agentic system execution flows (currently 63%) to a target threshold below 30%?
- Basis in paper: [explicit] Future work includes "developing automated techniques for prompt engineering to mitigate variability."
- Question: How should dynamic evaluation scenarios be designed to capture context-dependent decision-making and long-horizon dependencies in multi-agent systems?
- Basis in paper: [explicit] The authors state "Future efforts should focus on extending these taxonomies to capture more nuanced agent behaviors, particularly context-dependent decision-making, long-horizon dependencies and multi-agent interactions."

## Limitations

- Proprietary TAMAS implementation details limit reproducibility of the analytics system itself
- Behavioral patterns detected may not correlate with user-perceived system quality or task success rates
- Ground truth annotation process isn't validated for inter-annotator consistency or scalability beyond 30 samples

## Confidence

- **High confidence**: Fundamental observation that agentic systems exhibit non-deterministic execution flows and accuracy variability (CV metrics)
- **Medium confidence**: Behavioral benchmarking reveals execution patterns invisible to outcome-only metrics (60% match rate leaves room for improvement)
- **Medium confidence**: Ground truth benchmarking methodology for evaluating analytics systems (human annotation reliability not quantified)

## Next Checks

1. Conduct inter-annotator reliability testing on ground truth failure classifications to ensure consistency and establish annotation quality thresholds
2. Perform user studies correlating detected execution pattern anomalies with actual task success/failure rates and user satisfaction metrics
3. Test the behavioral benchmarking approach on agentic systems beyond the calculator domain (e.g., multi-agent planning or code generation) to validate generalizability