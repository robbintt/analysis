---
ver: rpa2
title: Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch
arxiv_id: '2503.22244'
source_url: https://arxiv.org/abs/2503.22244
tags:
- gradient
- policy
- biased
- distribution
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the impact of distribution mismatch on on-policy\
  \ policy gradient methods in discounted reinforcement learning. The mismatch arises\
  \ because practical implementations use an undiscounted state distribution (d\u03C0\
  ) instead of the theoretically correct discounted distribution (d\u03C0,\u03B3),\
  \ leading to biased gradient estimates."
---

# Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch

## Quick Facts
- arXiv ID: 2503.22244
- Source URL: https://arxiv.org/abs/2503.22244
- Authors: Weizhen Wang; Jianping He; Xiaoming Duan
- Reference count: 40
- Primary result: Proves biased policy gradient methods converge to globally optimal solutions despite distribution mismatch, with error bounds vanishing as discount factor γ approaches 1

## Executive Summary
This paper analyzes a fundamental discrepancy in practical policy gradient implementations: while theory prescribes using a discounted state distribution (dπ,γ) for gradient estimates, implementations typically use the undiscounted stationary distribution (dπ). The authors prove that despite this distribution mismatch, on-policy policy gradient methods retain key theoretical guarantees. In tabular parameterizations, both direct and softmax policies converge to globally optimal solutions. For general parameterizations, they derive bounds on the mismatch that vanish as γ approaches 1, showing that biased gradient methods achieve performance comparable to unbiased methods under mild assumptions.

## Method Summary
The authors analyze policy gradient methods in discounted reinforcement learning where practical implementations use the undiscounted stationary distribution dπ instead of the theoretically correct discounted distribution dπ,γ. They consider two MDP types: episodic (with absorbing states) and continuing (with ergodic policies). The analysis covers direct parameterizations (explicit probabilities per state-action) and softmax parameterizations. They prove gradient domination properties for biased gradients in tabular settings and derive finite-time convergence bounds for general parameterizations under smoothness assumptions. The method compares biased gradient updates (using dπ) against unbiased updates (using dπ,γ) across different discount factors.

## Key Results
- In tabular parameterizations, both direct and softmax policies converge to globally optimal solutions despite using biased gradients
- Distribution mismatch bounds between dπ and dπ,γ vanish as γ approaches 1, with theoretical guarantees
- For general parameterizations, biased gradient methods achieve convergence bounds comparable to unbiased methods, with a small error term that diminishes as γ→1
- Numerical experiments validate theoretical findings, showing both biased and unbiased methods converge to the same optimal solution

## Why This Works (Mechanism)

### Mechanism 1: Global Optimality Retention under Tabular Bias
The "gradient domination" property holds for the biased gradient, ensuring that any first-order stationary point (where the biased gradient is zero) is effectively a global optimum. This works because the policy is tabular (explicit probabilities per state-action), and the MDP is finite. The mechanism breaks for general function approximation where the policy class cannot represent all possible tabular policies.

### Mechanism 2: Mismatch Vanishing as γ→1
The theoretical discrepancy between practical sampling distribution (dπ) and correct discounted distribution (dπ,γ) is bounded by factors involving (1-γ). As γ approaches 1, these bounds converge to 1, implying the distributions effectively converge. This mechanism requires episodic MDPs to have positive absorbing probability or continuing MDPs to be ergodic with strictly positive state distributions. The mechanism breaks for environments with very low discount factors (e.g., γ < 0.5).

### Mechanism 3: Convergence via Biased SGD Assumptions
The biased policy gradient method converges to a neighborhood of a stationary point at a rate comparable to unbiased methods under the "Biased ABC" assumption (correlation and norm bounds between biased and unbiased gradients). This requires L-smoothness and bounded gradients. The mechanism breaks for highly non-smooth policy landscapes or unbounded gradient norms.

## Foundational Learning

- **Concept:** Discounted State Visitation Distribution (dπ,γ)
  - **Why needed here:** This is the theoretically correct weighting for policy gradients in discounted MDPs. The paper investigates what happens when this is replaced by the stationary distribution (dπ).
  - **Quick check question:** In a trajectory, why does the probability of visiting a state at step t depend on γ^t in the discounted view?

- **Concept:** Gradient Domination (PL Condition)
  - **Why needed here:** This property ensures that gradient descent doesn't get stuck in bad local optima. The paper proves this property survives the introduction of the distribution bias.
  - **Quick check question:** If a function satisfies gradient domination, what does a gradient of zero imply about the point?

- **Concept:** Biased Stochastic Gradient Descent (SGD)
  - **Why needed here:** The paper frames the "mismatch" problem as a specific instance of biased SGD, allowing the import of convergence theorems from optimization theory.
  - **Quick check question:** In standard SGD, we assume the gradient estimator is unbiased. If we add a constant vector to every gradient estimate, is the update biased?

## Architecture Onboarding

- **Component map:** Buffer -> Sampler -> Updater
- **Critical path:** 1) Collect trajectories using current policy π, 2) Sample uniformly (introducing bias), 3) Update θ using the biased gradient
- **Design tradeoffs:** Simplicity vs. Theoretical Purity - implementing the "Corrector" (e.g., DisPPO) requires tracking timestamps and variance-reducing importance weights. The paper suggests this is often unnecessary because the "Bias" naturally vanishes if γ is high (0.99).
- **Failure signatures:** Low γ Instability - if using γ < 0.7, the convergence rate of the biased method may visibly lag behind an unbiased baseline. Variance Explosion - if attempting to correct the bias using importance sampling without careful scaling, variance may outweigh the benefits of correctness.
- **First 3 experiments:**
  1. Sanity Check (Tabular): Replicate the Gridworld experiment with γ=0.9 vs γ=0.5. Verify that both converge but differ in speed at lower γ.
  2. General Parameterization: Run a continuous control task with Biased (Standard PPO) vs Unbiased (Correction). Plot performance gap as γ varies from 0.9 to 0.99.
  3. Ablation: Measure the empirical distribution mismatch ||dπ,γ/dπ||∞ in a random neural network policy to validate Theorem 4 bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what specific structural conditions of the MDP or parameterization does the biased gradient exhibit faster convergence than the unbiased gradient?
- **Basis in paper:** Remark 1 and Appendix I.3 note that in certain problems with incomplete parameterization, the "biased gradient may even demonstrate faster convergence," providing a gridworld example where the biased algorithm reaches the global optimum faster.
- **Why unresolved:** The paper focuses on proving that the biased method converges to the same solution and remains stable, but doesn't explain the accelerated convergence rate observed in experiments.
- **What evidence would resolve it:** A theoretical derivation showing that the variance reduction or specific geometry of the undiscounted distribution dπ improves the condition number compared to dπ,γ.

### Open Question 2
- **Question:** Can the global optimality guarantees for the distribution mismatch be extended from tabular settings to general function approximation (e.g., neural networks)?
- **Basis in paper:** The paper proves global convergence for tabular parameterizations but only provides a finite-time convergence bound to a stationary point for general parameterizations.
- **Why unresolved:** The proof technique for global optimality relies on gradient domination properties specific to tabular representations, which don't generally hold for complex function approximators without additional assumptions.
- **What evidence would resolve it:** A convergence proof demonstrating that the biased update satisfies a gradient domination or strict saddle-point property for over-parameterized neural networks, or a counter-example where the bias leads to convergence to a suboptimal local minimum.

### Open Question 3
- **Question:** Are the derived upper bounds for the distribution mismatch tight for practical discount factors (γ < 1), or do they significantly overestimate the discrepancy?
- **Basis in paper:** Theorems 3 and 4 derive bounds involving mixing times and absorbing probabilities to show the mismatch vanishes as γ→1, but don't validate the tightness of these constants for finite γ values used in practice.
- **Why unresolved:** While asymptotic behavior is proven, the magnitude of the error floor term depends on these bounds; loose bounds would imply the theoretical performance gap is much larger than the empirical reality.
- **What evidence would resolve it:** An empirical analysis comparing the theoretical mismatch bounds against the actual measured ratio ||dπ,γ/dπ||∞ across standard benchmarks to determine if the bounds are order-optimal.

## Limitations
- The theoretical bounds on distribution mismatch rely on asymptotic behavior as γ→1, with unmeasured discrepancies for practical settings (γ=0.99)
- Convergence proofs for general parameterizations depend on the "Biased ABC" assumption requiring empirical validation
- Tabular convergence proofs don't directly extend to function approximation, leaving a gap for neural network policies

## Confidence

- **High confidence:** Global optimality retention for tabular parameterizations (Theorem 1, supported by gridworld experiments)
- **Medium confidence:** Mismatch bounds vanishing as γ→1 (Theorem 3,4 have strong theoretical form but limited empirical validation)
- **Medium confidence:** Convergence bounds for general parameterizations (Relies on ABC assumption with limited deep RL validation)

## Next Checks

1. Measure the empirical distribution mismatch ||dπ,γ/dπ||∞ in standard deep RL benchmarks (Atari, Mujoco) across different γ values to validate Theorem 4 bounds.

2. Implement and test the "Corrector" (DisPPO-style importance weighting) on continuous control tasks to quantify the practical cost/benefit of removing the bias.

3. Analyze failure cases where γ<0.7 or non-smooth reward structures break the theoretical assumptions, measuring divergence or convergence degradation.