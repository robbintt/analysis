---
ver: rpa2
title: Automated urban waterlogging assessment and early warning through a mixture
  of foundation models
arxiv_id: '2510.18425'
source_url: https://arxiv.org/abs/2510.18425
tags:
- report
- waterlogging
- image
- data
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UWAssess, a foundation model-driven framework
  for automated urban waterlogging assessment and early warning. UWAssess leverages
  surveillance cameras to automatically identify waterlogged areas and generate structured
  assessment reports.
---

# Automated urban waterlogging assessment and early warning through a mixture of foundation models

## Quick Facts
- **arXiv ID:** 2510.18425
- **Source URL:** https://arxiv.org/abs/2510.18425
- **Reference count:** 40
- **Primary result:** UWAssess achieves 0.9648 G-Mean on UWBench-All and 0.9327 G-Mean on UWBench-Hard for visual perception, with GPT-4 evaluated textual reports scoring 7.27 average.

## Executive Summary
This paper introduces UWAssess, a foundation model-driven framework for automated urban waterlogging assessment and early warning. The system leverages surveillance cameras to automatically identify waterlogged areas and generate structured assessment reports. To address data scarcity, the authors propose a hybrid adaptation module combining Adapter and LoRA mechanisms with a gating unit, alongside a semi-supervised learning method (S2Match) that enforces consistency across augmented views. A chain-of-thought prompting strategy (S3CoT) enables vision-language models to generate reliable textual reports without task-specific fine-tuning. The dual capability of perception and generation enables a shift from monitoring to intelligent decision support for urban management and disaster response.

## Method Summary
UWAssess consists of two main components: a vision foundation model (VFM) for semantic segmentation of waterlogging areas, and a vision-language foundation model (VLFM) for generating structured textual reports. The VFM uses SAM2-Small with a hybrid adaptation module that combines Adapter and LoRA pathways through a gating mechanism, trained with a semi-supervised consistency regularization method (S2Match) that leverages unlabeled data. The VLFM (DeepSeek-VL2-Small) uses a three-step chain-of-thought prompting strategy (S3CoT) that combines semantic captions, spatial overlays from the VFM, and structural reporting formats to generate comprehensive reports. The framework is trained on the UWBench dataset (5,584 labeled images) with additional unlabeled data (12,609 images) for semi-supervised learning.

## Key Results
- Visual perception achieves 0.9648 G-Mean on UWBench-All and 0.9327 G-Mean on UWBench-Hard test sets
- Semi-supervised learning with S2Match improves Dice, IoU, and G-Mean by 0.0946, 0.1346, and 0.0345 respectively on UWBench-Hard
- GPT-4 evaluated textual reports achieve average score of 7.27, with structural prompts alone improving clarity to 6.93
- Hybrid adaptation with gating unit yields improvements of 0.0262, 0.0449, and 0.0076 on Dice, IoU, and G-Mean

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hybrid adaptation module enables effective fine-tuning of vision foundation models for waterlogging segmentation under label scarcity.
- **Mechanism:** Two complementary parameter-efficient fine-tuning pathways (Adapter and LoRA) operate on different encoder components—the Adapter injects task-specific features via MLPs between layers, while LoRA modifies self-attention weights through low-rank matrices. A learned gating unit dynamically balances their contributions per layer, allowing the model to leverage both high-frequency detail extraction and attention redistribution without full fine-tuning.
- **Core assumption:** The pre-trained features of SAM2 contain transferable representations that can be adapted with minimal parameter updates; the gating mechanism can learn meaningful trade-offs between adaptation types during training.
- **Evidence anchors:**
  - [abstract]: "hybrid adaptation module, integrating adaptation modules with distinct fine-tuning mechanisms through a gating mechanism for flexible feature adaptation"
  - [Results]: "Even tuning only 25% of the encoder layers yields significant improvements... with IoU gains of 0.0759 on UWBench-All"
  - [Fig. 5a-b]: Gating unit achieves improvements of 0.0262, 0.0449, and 0.0076 on Dice, IoU, and G-Mean on UWBench-All
  - [corpus]: No direct corpus equivalent; adaptation of foundation models for disaster perception is novel.
- **Break condition:** If labeled data becomes abundant (>10K images), simpler full fine-tuning may outperform hybrid approaches; if computational budget is severely constrained, Adapter-only may suffice.

### Mechanism 2
- **Claim:** The S2Match semi-supervised learning method improves perception robustness by enforcing consistency across augmented views while introducing structural perturbations.
- **Mechanism:** The method extends UniMatch V2 by adding: (1) Scale-wise Stochastic Depth (SD), which randomly skips the lowest-resolution encoder path to create structural feature perturbations; (2) Strong-to-Strong Consistency (SC), which enforces prediction agreement between two strongly-augmented views using pseudo-labels with a relaxed confidence threshold (0.8 vs. 0.95 for weak-to-strong). Channel-wise complementary dropout ensures non-overlapping feature perturbations between strong branches.
- **Core assumption:** Waterlogging features are invariant to the perturbations introduced (color jitter, scale skipping, channel dropout); pseudo-labels above the confidence threshold are sufficiently reliable for cross-supervision.
- **Evidence anchors:**
  - [abstract]: "consistency regularization-based semi-supervised learning method... employing Structural perturbation and Strong augmentation to fully leverage unlabeled data"
  - [Results]: "When SC and SD are used jointly... on the UWBench-Hard test set, [Dice, IoU, G-Mean] are improved by 0.0946, 0.1346 and 0.0345"
  - [Fig. 4f]: "incorporating more unlabeled training data not only enhances perception performance but also effectively improves its prediction stability"
  - [corpus]: Related work on sensor optimization (arxiv 2511.04556) addresses data scarcity in flood monitoring but uses physical sensors, not semi-supervised vision.
- **Break condition:** If the confidence threshold is too low (<0.65), noisy pseudo-labels degrade performance; if unlabeled data distribution diverges significantly from labeled data, consistency enforcement may propagate errors.

### Mechanism 3
- **Claim:** The S3CoT prompting strategy unlocks structured report generation from vision-language models without task-specific fine-tuning.
- **Mechanism:** A two-step chain-of-thought pipeline: (1) Generate a scene caption capturing coarse semantics (weather, lighting, context); (2) Fuse three prompt types—semantic (caption-derived), spatial (VFM perception mask encoded as visual overlay), and structural (explicit reporting format with extent/depth/risk sections)—to guide the VLFM toward comprehensive, well-organized outputs. The visual perception result is provided as both text description and image overlay.
- **Core assumption:** The VLFM (DeepSeek-VL2) has sufficient grounding capabilities to interpret spatial overlays and integrate them with semantic context; structural prompts can compensate for domain shift without fine-tuning.
- **Evidence anchors:**
  - [abstract]: "chain-of-thought (CoT) prompting strategy to unleash the potential of foundation models for data-scarce downstream tasks"
  - [Results]: "Using the structural prompt significantly improve clarity and text organization, increasing the average score to 6.93... The complete S3CoT prompting strategy... achieve the best performance with an average score of 7.27"
  - [Fig. 3]: Qualitative comparison shows reports without S3CoT lack depth assessment and contain irrelevant statements (score 5), while S3CoT reports are comprehensive (score 8)
  - [corpus]: Generative AI for disaster resilience (arxiv 2601.18308) uses LLMs for action recommendations but not multimodal perception-report pipelines.
- **Break condition:** If the VFM produces a false positive (detects waterlogging where none exists), spatial prompts will mislead report generation; if the VLFM lacks grounding for spatial overlays, the visual prompt may be ignored or misinterpreted.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The hybrid adaptation module combines Adapter and LoRA; understanding how each modifies different parts of the network is essential for debugging and extending the architecture.
  - **Quick check question:** Can you explain why Adapter operates on inter-layer representations while LoRA operates on attention weights, and what this means for their complementary roles?

- **Concept: Semi-Supervised Consistency Regularization**
  - **Why needed here:** S2Match builds on FixMatch/UniMatch paradigms; the weak-to-strong and strong-to-strong consistency framework is the theoretical backbone.
  - **Quick check question:** Why does the confidence threshold for strong-to-strong consistency (0.8) differ from weak-to-strong (0.95), and what happens if both use the same threshold?

- **Concept: Chain-of-Thought Prompting for Vision-Language Models**
  - **Why needed here:** S3CoT is a multi-step prompting strategy; understanding how semantic, spatial, and structural prompts compose is critical for customization.
  - **Quick check question:** If the VFM produces a false positive (detects waterlogging where none exists), how will the spatial prompt affect the VLFM's report, and where would you intervene?

## Architecture Onboarding

- **Component map:**
  Input Image → [VFM: SAM2 + Hybrid Adapter + S2Match] → Segmentation Mask → [VLFM: DeepSeek-VL2 + S3CoT Prompts] → Structured Text Report

- **Critical path:**
  1. Collect labeled waterlogging images (UWBench train: 5,584) + unlabeled images (UW-Unlabeled: 12,609)
  2. Train VFM with S2Match: EMA teacher generates pseudo-labels for unlabeled data; student learns from weak-to-strong and strong-to-strong consistency
  3. Run inference on test images to generate masks
  4. For report generation: (a) caption image with VLFM, (b) overlay mask on image, (c) compose prompts, (d) generate report

- **Design tradeoffs:**
  - **Adapter vs. LoRA vs. Hybrid:** Adapter is simpler but may miss attention refinements; LoRA alone is less stable across frames; Hybrid with gating adds ~0.7M parameters but improves temporal robustness (see Fig. 4c)
  - **Confidence threshold (0.8 vs. 0.95):** Lower threshold includes more unlabeled pixels but risks noise; ablation shows 0.8 is optimal for strong-to-strong, 0.95 for weak-to-strong
  - **S3CoT components:** Structural prompt alone yields 6.93 average score; full S3CoT yields 7.27—evaluate whether marginal gains justify prompt complexity

- **Failure signatures:**
  - **Unstable attention across frames:** Single adaptation module (Adapter or LoRA only) shows shifting Grad-CAM heatmaps—indicates need for gating (Fig. 4c)
  - **Report hallucinations without S3CoT:** Generic descriptions missing extent/depth assessment, irrelevant statements highlighted in red (Fig. 3b, e)
  - **Performance drop with too much unlabeled data at low confidence:** If threshold <0.65, noise accumulates—monitor pseudo-label quality

- **First 3 experiments:**
  1. **Reproduce VFM perception baseline:** Train SAM2-Adapter and SAM2-LoRA separately on UWBench train + UW-Unlabeled using UniMatch V2; compare G-Mean against reported 0.9509 (Semi-SAM2-LoRA) and 0.9489 (Semi-SAM2-Adapter)
  2. **Ablate S2Match components:** Disable SC, then SD, then both on UWBench-Hard; verify that SC+SD joint use recovers the reported IoU of 0.7549 (vs. 0.6203 baseline)
  3. **Validate S3CoT prompting on held-out images:** Sample 10 images from Roadway Flooding dataset; generate reports with and without each prompt component; have human annotators score accuracy of extent/depth/risk assessments

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can UWAssess maintain effectiveness in long-term deployment as climate change shifts rainfall patterns and visual distributions beyond the current training data?
- **Basis in paper:** [explicit] The Discussion section states that due to the "ever-changing world," effectiveness in "long-term or life-long deployment is an open challenge for the field at large."
- **Why unresolved:** The current model is trained on static datasets (UWBench, UW-Unlabeled) and may suffer from domain shift as weather dynamics evolve.
- **What evidence would resolve it:** Evaluation of the system on longitudinal datasets spanning multiple years, or the integration of a continuous learning paradigm that adapts to new distributions without catastrophic forgetting.

### Open Question 2
- **Question:** How can UWAssess be integrated with meteorological data streams and hydrological simulations to form a closed-loop decision-making system?
- **Basis in paper:** [explicit] The Discussion section explicitly advises that future research "should focus on integrating UWAssess with meteorological data streams, hydrological simulations... to form a closed-loop system."
- **Why unresolved:** The current framework functions as a perception and report generation tool but does not ingest non-visual sensor data or simulate physical water flow for predictive decision-making.
- **What evidence would resolve it:** A functional prototype that fuses visual monitoring with weather forecasts to predict waterlogging risk before accumulation occurs.

### Open Question 3
- **Question:** What specific innovations in model compression and distributed inference are required to scale UWAssess for citywide, real-time deployment?
- **Basis in paper:** [explicit] The Discussion notes that the integration of foundation models "inevitably increases computational demand," and scaling to citywide systems "will require continued innovation in model compression and distributed inference."
- **Why unresolved:** The current implementation relies on high-performance GPUs (V100/A800) for training and inference, which creates a resource bottleneck for widespread, low-latency edge deployment.
- **What evidence would resolve it:** Demonstrating that the framework can run in real-time on edge devices or optimized cloud clusters with minimal loss in perception (G-Mean) or report accuracy.

## Limitations

- **Static camera assumptions:** The framework assumes fixed camera placements and controlled environmental conditions, limiting generalizability to dynamic or extreme weather scenarios.
- **Computational overhead:** Reliance on foundation models introduces significant computational demand, challenging real-time deployment in resource-constrained environments.
- **Pseudo-label dependency:** Semi-supervised learning performance heavily depends on pseudo-label quality, which may degrade if unlabeled data distribution shifts significantly.

## Confidence

- **High Confidence:** Visual perception improvements (G-Mean 0.9648 on UWBench-All) and semi-supervised learning mechanisms are well-supported by quantitative results and ablation studies.
- **Medium Confidence:** The S3CoT prompting strategy's effectiveness is validated through GPT-4 evaluations, but the exact prompt templates and human evaluation protocols are not fully disclosed, introducing potential reproducibility gaps.
- **Low Confidence:** The claim of shifting waterlogging monitoring "from perception to generation" is aspirational and lacks long-term deployment evidence or real-world validation beyond controlled benchmarks.

## Next Checks

1. **Real-World Robustness:** Deploy UWAssess on surveillance feeds from multiple cities with varying camera angles, lighting conditions, and weather patterns to assess generalizability beyond UWBench benchmarks.
2. **Operational Latency:** Measure end-to-end processing time (from image capture to report generation) under realistic computational constraints to evaluate real-time feasibility.
3. **Cross-Modal Consistency:** Cross-validate textual report accuracy by correlating generated depth/risk assessments with ground-truth water level measurements from physical sensors in deployed locations.