---
ver: rpa2
title: Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models
arxiv_id: '2506.23191'
source_url: https://arxiv.org/abs/2506.23191
tags:
- deep
- shallow
- training
- datasets
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how shallow versus deep relevance judgments
  affect BERT-based reranking model performance. Shallow-judged datasets contain many
  queries with few judgments, while deep-judged datasets have fewer queries with many
  judgments.
---

# Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models

## Quick Facts
- arXiv ID: 2506.23191
- Source URL: https://arxiv.org/abs/2506.23191
- Authors: Gabriel Iturra-Bocaz; Danny Vo; Petra Galuscakova
- Reference count: 22
- Key outcome: Shallow-judged datasets outperform deep-judged datasets for training BERT rerankers, requiring fewer positive samples for comparable performance.

## Executive Summary
This paper investigates how the depth versus breadth of relevance judgments affects BERT-based reranking model performance. Shallow-judged datasets contain many queries with few judgments each, while deep-judged datasets have fewer queries with many judgments per query. Through experiments on MS MARCO and LongEval collections, the authors demonstrate that shallow datasets consistently outperform deep datasets for training rerankers, requiring far fewer positive samples to achieve comparable effectiveness. The broader query coverage in shallow datasets enables better model generalization, making them more effective despite having fewer total judgments per query.

## Method Summary
The authors compare BERT-base-uncased rerankers trained on shallow versus deep datasets using MonoBERT pointwise classification. Shallow datasets (e.g., 2,500 queries × 2 judgments) are constructed from MS MARCO V1/V2 Train, while deep datasets (e.g., 50 queries × 100 judgments) come from TREC DL 2019/2020. Negative samples are drawn from BM25 results (excluding top-10 to avoid false negatives). Models are fine-tuned with batch size 8, learning rate 2e-5, max sequence length 512, and early stopping based on validation F1. Performance is evaluated using MAP, NDCG, and MRR@10 on MS MARCO Dev2 and LongEval.

## Key Results
- Shallow datasets consistently outperform deep datasets across all tested conditions and metrics.
- Deep datasets can partially compensate for limited query diversity by adding more negative training examples (optimal ratio 1:4-1:8).
- Performance degrades when positive-to-negative ratio exceeds 1:10, indicating noise from excessive negatives.
- Deep datasets with few queries (<100) can perform worse than BM25 baseline due to overfitting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow datasets with broader query coverage enable better model generalization for BERT-based rerankers.
- Mechanism: By exposing the model to a wider variety of query types and information needs, the model learns more robust semantic patterns that transfer to unseen queries. Shallow datasets provide diverse query-document contexts despite fewer judgments per query.
- Core assumption: The test distribution contains diverse query types that benefit from broader training coverage rather than depth on specific queries.
- Evidence anchors:
  - [abstract]: "Results indicate that shallow-judged datasets generally enhance generalization and effectiveness of reranking models due to a broader range of available contexts."
  - [section 6]: "we confirm that the broader topic coverage in shallow training sets enables models to generalize better and perform more effectively in reranking tasks."
  - [corpus]: Limited direct evidence—neighbor papers focus on LLM-based reranking and other aspects of relevance judgment, not query coverage depth.
- Break condition: If test queries are highly specialized and don't match the shallow training distribution, or if domain-specific knowledge requires deep annotations per query.

### Mechanism 2
- Claim: Adding negative training examples partially compensates for limited query diversity in deep datasets.
- Mechanism: Negative samples (automatically generated from BM25 results) provide contrastive learning signal, helping the model distinguish relevant from non-relevant documents even with fewer query types. This is particularly valuable for deep datasets lacking query variety.
- Core assumption: Negative samples drawn from BM25 results (excluding top 10) are truly non-relevant and provide useful training signal without excessive false negatives.
- Evidence anchors:
  - [abstract]: "The disadvantage of the deep-judged datasets might be mitigated by a larger number of negative training examples."
  - [section 4.2.2]: "training sets with a 1:4 ratio yield improved performance compared to those with a 1:1 ratio" for deep datasets; performance peaks at 1:8 ratio then declines at 1:10-1:20.
  - [corpus]: "Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking" addresses sampling strategies, though in LLM context.
- Break condition: When positive-to-negative ratio becomes too imbalanced (≥1:10), adding noise that degrades performance.

### Mechanism 3
- Claim: Deep datasets with few queries cause overfitting, performing below even BM25 baseline.
- Mechanism: With only 50-100 queries, the model memorizes query-specific patterns rather than learning generalizable relevance signals, despite having many judgments per query. The semantic understanding capacity of BERT is underutilized.
- Core assumption: Overfitting stems from limited query diversity, not insufficient training instances overall.
- Evidence anchors:
  - [section 4.2]: "The deep datasets even perform significantly worse than the BM25 baseline, which may indicate that such a small number of training queries in the deep dataset can lead to overfitting."
  - [section 5]: "as we increase the number of training samples in both deep and shallow datasets, reranker models trained on larger datasets exhibit improved performance."
  - [corpus]: Weak direct evidence—neighbors don't specifically address overfitting from query scarcity in neural ranking.
- Break condition: If deep datasets had substantially more queries (authors hypothesize this could make them viable for training).

## Foundational Learning

- **Concept: Neural Reranking Pipeline (First-Stage Retrieval → Reranking)**
  - Why needed here: The paper assumes understanding that BM25 retrieves candidates first, then BERT reranks top-10 results. Without this, the experimental setup and negative sampling strategy won't make sense.
  - Quick check question: Why does the paper exclude BM25 top-10 results when sampling negative examples?

- **Concept: Positive-to-Negative Sample Ratio Tradeoffs**
  - Why needed here: The paper shows optimal ratios around 1:4-1:8, with performance degrading at 1:10-1:20. Understanding this tradeoff is essential for data collection decisions.
  - Quick check question: What happens to deep dataset performance when the positive-to-negative ratio increases from 1:8 to 1:20?

- **Concept: Generalization vs. Memorization in Transfer Learning**
  - Why needed here: The core finding is about query diversity affecting generalization. Understanding why 2,500 queries × 2 judgments outperforms 50 queries × 100 judgments requires grasping this concept.
  - Quick check question: Why might a model with more total training instances (deep dataset) still underperform one with fewer instances but more query diversity?

## Architecture Onboarding

- **Component map:**
  - BM25 first-stage retriever (Pyserini) → retrieves top-10 candidates
  - BERT-base-uncased (BertForSequenceClassification) → classifies query-document pairs as relevant/non-relevant
  - Training data pipeline → positive judgments (human-labeled) + negative samples (BM25-retrieved, excluding top-10)
  - Evaluation → MAP/NDCG/MRR computed at rank 10

- **Critical path:**
  1. Sample training data maintaining controlled positive-to-negative ratios
  2. Fine-tune BERT with early stopping on validation F1-score
  3. At inference: BM25 retrieves top-10 → BERT reranks → evaluate at rank 10

- **Design tradeoffs:**
  - Shallow vs. deep: ~1,000 shallow instances can beat BM25; deep datasets need substantially more due to query scarcity
  - Negative ratio: 1:4-1:8 optimal; higher ratios add noise
  - Collection choice: MS MARCO V2 provides more deep queries than V1, enabling better deep dataset experiments

- **Failure signatures:**
  - Reranker scoring below BM25 baseline → likely trained on insufficient query diversity (deep dataset with <100 queries)
  - Performance degrading as negative ratio increases past 1:8 → excess negatives adding noise
  - Zero-shot BERT underperforming BM25 → expected; requires fine-tuning for reranking task

- **First 3 experiments:**
  1. Establish BM25 baseline and zero-shot BERT performance on your target collection to confirm fine-tuning is necessary.
  2. Train with shallow dataset (e.g., 1,000-2,500 queries, 2 judgments each) and compare against deep dataset with matched total training instances.
  3. Systematically vary positive-to-negative ratio (1:1, 1:4, 1:8) on deep dataset to measure compensation effect from negative sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can increasing the number of queries in deep-judged collections make them competitive for training neural rerankers?
- Basis in paper: [inferred] The authors hypothesize that increasing the query count (typically ~50) in deep collections could make them suitable for both evaluation and training.
- Why unresolved: Current deep collections have too few queries to match the performance of shallow sets with thousands of queries.
- What evidence would resolve it: Experiments constructing deep datasets with significantly higher query counts (e.g., hundreds or thousands) and comparing training efficacy.

### Open Question 2
- Question: Do these training data preferences apply to more advanced neural ranking architectures?
- Basis in paper: [inferred] The authors note that while they used BERT as a prototype, advanced models might have different data requirements.
- Why unresolved: The study focused solely on MonoBERT; newer architectures like LLM-based rankers or late-interaction models were not tested.
- What evidence would resolve it: Replicating the shallow vs. deep comparison on state-of-the-art ranking models (e.g., ColBERT, monoT5).

### Open Question 3
- Question: Does "shallowing" traditional deep evaluation collections create effective dual-purpose datasets?
- Basis in paper: [inferred] The authors suggest that reducing judgment depth to broaden query coverage might help create datasets useful for both training and evaluation.
- Why unresolved: This is a proposed direction based on combining findings, not an experimentally validated result.
- What evidence would resolve it: Systematically varying the depth/breadth trade-off in deep collections to identify a configuration that maximizes both training and evaluation utility.

## Limitations

- The paper doesn't test whether shallow dataset advantages persist on highly specialized collections with different query distributions.
- The optimal negative sampling strategy and positive-to-negative ratio may vary across domains, though the paper provides some guidance (ratios 1:4-1:8 optimal, degradation beyond 1:10).
- The study focused solely on MonoBERT, so it's unclear whether these training data preferences apply to more advanced neural ranking architectures.

## Confidence

- **High confidence**: Shallow datasets consistently outperform deep datasets across all tested conditions (MS MARCO, LongEval), and the positive-to-negative ratio findings (optimal 1:4-1:8, degradation beyond 1:10) are well-supported by systematic experiments.
- **Medium confidence**: The mechanism explaining why shallow datasets generalize better (query diversity) is logical and supported by results, but the paper doesn't test whether this advantage persists on highly specialized collections or whether deeper queries with more variety could close the gap.
- **Low confidence**: The claim that deep datasets can partially compensate with more negatives is supported but limited to specific conditions; the paper doesn't explore whether this compensation scales indefinitely or what happens with extreme negative ratios.

## Next Checks

1. **Domain Transfer Test**: Train shallow and deep models on MS MARCO, then evaluate on a completely different collection (e.g., legal or medical domain) to verify if shallow dataset generalization advantage persists.

2. **Query Distribution Analysis**: Analyze the overlap between shallow training queries and test queries to quantify how much query diversity actually contributes to performance gains versus other factors.

3. **Scaled Deep Dataset Experiment**: Create a deep dataset with 500-1000 queries (maintaining high judgment counts per query) to test whether the overfitting problem is solved with more queries, potentially making deep datasets competitive.