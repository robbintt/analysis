---
ver: rpa2
title: Towards Understanding Text Hallucination of Diffusion Models via Local Generation
  Bias
arxiv_id: '2503.03595'
source_url: https://arxiv.org/abs/2503.03595
tags:
- training
- network
- diffusion
- generation
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates text hallucination in diffusion models,
  identifying a key cause: local generation bias. Denoising networks trained via score
  matching tend to rely on highly correlated local regions, particularly when different
  dimensions of the data distribution are nearly pairwise independent.'
---

# Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias

## Quick Facts
- **arXiv ID:** 2503.03595
- **Source URL:** https://arxiv.org/abs/2503.03595
- **Reference count:** 40
- **Primary result:** Diffusion models exhibit "text hallucination" due to local generation bias, where denoising networks trained via score matching decompose global distributions into independent marginals, failing to capture underlying grammar or constraints.

## Executive Summary
This paper investigates the phenomenon of text hallucination in diffusion models, identifying a key cause: local generation bias. Denoising networks trained via score matching tend to rely on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. To quantify this bias, the authors introduce the Local Dependency Ratio (LDR), which measures the gradient magnitude within the same local region compared to the entire input. Experiments on synthetic text distributions (parity parentheses and quarter-MNIST) demonstrate that high LDR values correlate with text hallucination. Furthermore, LDR decreases as models overfit to the training data. The authors also provide theoretical analysis using a two-layer MLP learning parity points on a hypercube, showing that early training dynamics bias the network towards high LDR values.

## Method Summary
The paper proposes analyzing text hallucination in diffusion models by measuring Local Dependency Ratio (LDR), which quantifies whether the model's output for a local region depends primarily on that region's input. Experiments use synthetic datasets: Quarter-MNIST (64x64 images with 4 digits satisfying arithmetic constraints) and Parity Parentheses (balanced parentheses). Models trained include Attention-UNet and DiT architectures using standard denoising score matching objectives with Adam optimizer (lr=8e-5). The key insight is that high LDR values correlate with hallucination, while LDR decreases primarily through overfitting to training data rather than learning generalizable rules.

## Key Results
- LDR values above 0.75 indicate the model is ignoring global context, producing nonsensical assemblies despite valid individual symbols
- LDR decreases as models overfit to training data, but this leads to memorization rather than rule generalization
- DiT architectures reduce LDR faster than UNet but ultimately lead to pure memorization/overfitting
- Early training dynamics create an invariant set where gradient flow biases networks toward high-LDR solutions

## Why This Works (Mechanism)

### Mechanism 1: Local Dependency Bias via Score Matching
- **Claim:** Denoising networks trained with score matching exhibit an inductive bias towards "local generation," treating global structures as independent marginal distributions, which directly causes hallucination.
- **Mechanism:** When data dimensions are nearly pairwise independent (e.g., symbols in a string), the score network learns to denoise specific regions based primarily on the noisy input in that same region. This ignores global constraints (like grammar or arithmetic rules), effectively decomposing the joint distribution $P(x_1, \dots, x_L)$ into independent marginals $\prod P(x_i)$.
- **Core assumption:** The underlying data distribution has dimensions that are nearly pairwise independent (Assumption 5.1), or global correlations are sparse/weak relative to local features.
- **Evidence anchors:**
  - [abstract] "Denoising networks tend to produce outputs that rely heavily on highly correlated local regions... failing to capture the global structure."
  - [section 4.3] "We propose a probe called Local Dependency Ratio (LDR)... high LDR value is always observed when hallucination happens."
  - [corpus] Related work suggests optimizing denoising score matching inputs introduces specific biases (Optimizing Input of Denoising Score Matching...), corroborating the link between training objectives and generation artifacts.
- **Break condition:** If data dimensions exhibit extremely strong, dense global correlations (violating the independence assumption), the network may be forced to lower its LDR earlier in training to minimize loss.

### Mechanism 2: Early Training Dynamics and Invariant Sets
- **Claim:** The local generation bias is not merely an architectural limitation but a result of training dynamics where early optimization drives the network into a high-LDR "invariant set."
- **Mechanism:** In early training (small initialization), gradient flow prioritizes aligning neuron weights with single input dimensions. Theorem 5.3 shows this creates a "saddle" where the network models the target function as a sum of univariate functions. Once trapped in this set (Theorem 5.2), gradient components that could lower LDR vanish, making the bias persistent.
- **Core assumption:** Standard gradient-based optimization with small initialization (rich regime) and a finite training budget.
- **Evidence anchors:**
  - [section 5.4] "Theorem 5.3... early training dynamic actually biases the network to a LDR level close to 1 with small initialization."
  - [section 6] "This bias is intrinsic in training rather than architectural limitation... persists across various denoising network architectures including MLP and transformers."
  - [corpus] Weak external evidence for this specific theoretical dynamic in transformers; reliance is primarily on the paper's theoretical section.
- **Break condition:** Using very large initialization (kernel regime) or explicit regularization terms that penalize high LDR during the initial training phases.

### Mechanism 3: Overfitting as a "Cure" for Hallucination
- **Claim:** Models only reduce hallucination (lower LDR) by memorizing specific training examples rather than learning the generalizable global rule.
- **Mechanism:** To satisfy global constraints (e.g., "row sums must equal"), the model must coordinate symbols. Since the training dynamics discourage learning abstract rules (Mechanism 2), the model instead reduces LDR by overfitting to specific pixel correlations in the training set, effectively "stitching" memorized patches together.
- **Core assumption:** The model capacity is sufficient to memorize the training data.
- **Evidence anchors:**
  - [section 4.3] "LDR decreases as models overfit to the training data... denoising network overfit to training dataset... resulting in a drop in LDR."
  - [section 4.2] "DiT performs better accuracy... however, none of them is able to generate valid symbol tuple beyond the training dataset."
  - [corpus] "Compositional Diffusion..." discusses composing local models, highlighting the general difficulty diffusion models face in long-horizon/global coherence without specific guidance.
- **Break condition:** If the dataset is too large to memorize, the model may persist in the high-LDR hallucination state indefinitely.

## Foundational Learning

- **Concept: Score Matching (Denoising Score Matching)**
  - **Why needed here:** This is the core training objective. Understanding that the network learns $\nabla_x \log p(x)$ (the score) is essential to grasp why it might default to local gradients when global ones are weak.
  - **Quick check question:** Does the network predict the clean image directly, or the gradient of the data density (noise)?
- **Concept: Jacobian Matrix & Saliency Maps**
  - **Why needed here:** The paper introduces the Local Dependency Ratio (LDR), which is calculated via the trace of the Jacobian (gradient of output w.r.t input). You must understand gradients to interpret the "mechanistic" explanation of bias.
  - **Quick check question:** If the Jacobian of a region's output is non-zero only for inputs within that region, what does that imply about the model's reasoning?
- **Concept: Marginal vs. Joint Probability Distributions**
  - **Why needed here:** The paper defines hallucination as the model collapsing a joint distribution $P(A, B)$ into marginals $P(A)P(B)$.
  - **Quick check question:** If a model generates a left parenthesis "(" and a right parenthesis ")" independently with 50% probability, what is the probability it generates a valid balanced pair compared to one dependent on the other?

## Architecture Onboarding

- **Component map:**
  - Input -> Backbone (UNet/DiT) -> Metric Layer (LDR) -> Output (predicted noise/score)

- **Critical path:** The calculation of the **Local Dependency Ratio (LDR)**. You must be able to extract the Jacobian of the denoising network's output with respect to a specific local region of the input $x_t$. This gradient trace determines if the model is "hallucinating" (LDR $\to$ 1) or reasoning globally (LDR < 0.5).

- **Design tradeoffs:**
  - **Architecture:** DiT (Transformer) vs. UNet. DiT lowers LDR faster but leads to pure overfitting/memorization. UNet often gets stuck in high-LDR (hallucination).
  - **Training Time:** Early stopping keeps LDR high (hallucination). Long training lowers LDR but results in overfitting (memorization). The paper suggests a "valley of death" where generalization is rare.

- **Failure signatures:**
  - **High LDR (> 0.75):** The model ignores global context. Output will have clear individual symbols (e.g., letters, digits) but nonsensical assembly (gibberish words, wrong math).
  - **Rapid LDR Drop:** Model is likely memorizing training data. Check for exact duplicates of training samples in generation.

- **First 3 experiments:**
  1. **Parity Check (Sanity Check):** Train a small MLP on a synthetic "parity" dataset (binary hypercube). Plot LDR vs. Training Step. Verify that LDR $\approx$ 1.0 even as loss drops.
  2. **Visualizing Saliency:** For a trained UNet on Quarter-MNIST, compute the gradient of the top-left digit's output w.r.t the whole image. Visualize the heatmap. Confirm "tunnel vision" (heat concentrated on top-left).
  3. **Overfitting Test:** Train a DiT model on a small text dataset for an extended period (e.g., 500k steps). Monitor LDR. Verify that LDR drops only when the model starts outputting exact training images (overfitting).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does local generation bias persist in data distributions where symbol dimensions are not strictly pairwise independent?
- Basis in paper: [explicit] "Is our discovered bias and mechanism still robust when the distribution does not strictly satisfy this requirement?"
- Why unresolved: The theoretical proofs rely on Assumption 5.1, which requires pairwise independence between dimensions. While real-world experiments suggest the bias exists, the theoretical mechanism may differ when correlations are present.
- What evidence would resolve it: Theoretical analysis of gradient flow dynamics on distributions with known global correlations, or experiments showing LDR behavior on datasets specifically designed to violate independence.

### Open Question 2
- Question: Can the training process be modified to reduce Local Dependency Ratio (LDR) and enable rule extrapolation without forcing the model to memorize the training data?
- Basis in paper: [inferred] The authors note that diffusion models "struggle between hallucination and overfitting," and experiments show that LDR decreases primarily when models overfit, failing to generate valid "extrapolation" samples.
- Why unresolved: The paper identifies the bias and the memorization escape route but does not propose a method to learn global rules (generalization) without defaulting to local generation or exact memorization.
- What evidence would resolve it: A regularization technique or architecture modification that achieves low LDR while successfully generating valid, unseen samples that satisfy the underlying grammar rules.

### Open Question 3
- Question: Do the theoretical properties of the high-LDR invariant set $M$ apply to attention-based architectures like Transformers, given their different empirical convergence speeds?
- Basis in paper: [inferred] The theoretical analysis is restricted to a two-layer ReLU network, but empirical results show DiTs reduce LDR faster than U-Nets, suggesting architectural differences in escaping the bias not covered by the current theory.
- Why unresolved: The existence of the invariant set $M$ is proven for MLPs, but the paper observes varying behaviors across architectures without providing a theoretical explanation for Transformer dynamics.
- What evidence would resolve it: Extension of the theoretical analysis in Section 5 to attention mechanisms, or empirical mapping of the loss landscape for Transformers to verify if they approach the invariant set $M$.

## Limitations
- The theoretical analysis relies heavily on synthetic datasets with strong structural assumptions (nearly pairwise independent dimensions, sparse global correlations)
- The proposed LDR metric, while intuitive, is a proxy measurement that may not fully capture the complexity of "hallucination" in real-world diffusion models
- The mechanism explaining why early training dynamics create local dependency bias depends on specific conditions (small initialization, finite training budget) that may not hold in all practical scenarios

## Confidence
- **High confidence:** The empirical observation that LDR correlates with hallucination on synthetic datasets, and that LDR decreases with overfitting
- **Medium confidence:** The theoretical framework connecting score matching to local generation bias, particularly the proof that early training dynamics favor high-LDR solutions
- **Medium confidence:** The claim that this bias is intrinsic to training dynamics rather than architectural limitations, though cross-architecture comparisons provide supporting evidence
- **Low confidence:** The direct applicability of these findings to large-scale diffusion models on natural text/image distributions without further empirical validation

## Next Checks
1. **Real-world hallucination correlation:** Measure LDR on a pre-trained diffusion model (e.g., Stable Diffusion) when generating text-in-image or purely text outputs, and correlate LDR values with human-rated hallucination severity
2. **Architectural intervention test:** Implement a regularization term that penalizes high LDR during training on the parity dataset. Verify if this allows the model to learn the global rule without overfitting
3. **Data dependency stress test:** Train models on datasets with varying levels of global correlation density (e.g., synthetic grammars with different depths of nested dependencies). Verify if LDR plateaus at different levels based on the underlying data structure's complexity