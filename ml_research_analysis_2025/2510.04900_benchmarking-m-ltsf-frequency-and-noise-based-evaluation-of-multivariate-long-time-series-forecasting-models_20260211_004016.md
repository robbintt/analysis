---
ver: rpa2
title: 'Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate
  Long Time Series Forecasting Models'
arxiv_id: '2510.04900'
source_url: https://arxiv.org/abs/2510.04900
tags:
- noise
- signal
- time
- frequency
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a parameterizable synthetic dataset framework
  for systematic evaluation of multivariate long-term time series forecasting (M-LTSF)
  models. The framework generates controllable synthetic data with configurable signal
  components (trends, seasonalities), noise types (white, Brownian, impulse, trend,
  seasonal), frequency characteristics, and signal-to-noise ratios.
---

# Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models

## Quick Facts
- **arXiv ID:** 2510.04900
- **Source URL:** https://arxiv.org/abs/2510.04900
- **Reference count:** 40
- **Primary result:** Introduces a synthetic data framework for systematic evaluation of M-LTSF models, revealing lookback window limitations, architectural signal preferences, and noise-type vulnerabilities

## Executive Summary
This paper addresses critical gaps in M-LTSF model evaluation by introducing a parameterizable synthetic dataset framework that generates controllable multivariate time series with configurable signal components, noise types, and SNR levels. The framework enables systematic isolation of model behaviors under diverse conditions that are difficult to disentangle in real-world datasets. Four representative M-LTSF architectures (S-Mamba, iTransformer, R-Linear, Autoformer) are benchmarked across multiple parameter combinations, revealing fundamental limitations in current evaluation practices and providing concrete guidance for model selection based on signal characteristics and noise conditions.

## Method Summary
The study employs a synthetic data generation framework that creates multivariate time series with configurable signal components (trends, seasonalities: sine/smooth sawtooth/smooth square), noise types (white, Brownian, impulse, trend-dependent, seasonal-dependent), frequency indices, and SNR levels. Four M-LTSF models (S-Mamba, iTransformer, R-Linear, Autoformer) are evaluated on datasets with N=35,040 samples, V=800 variates, using lookback T=96 and forecast horizon H=96. Each configuration is trained for 5 epochs across 9 repetitions (3 training seeds × 3 dataset seeds). Performance is measured via MSE against noise-free ground truth and spectral analysis via FFT to assess frequency reconstruction capabilities.

## Key Results
- All models degrade significantly when lookback windows cannot capture complete periods of seasonal patterns, with critical threshold at frequency index 355 for T=96
- S-Mamba and Autoformer perform best on sawtooth patterns while R-Linear and iTransformer favor sinusoidal signals
- White and Brownian noise universally degrade performance with lower SNR, while S-Mamba shows specific trend-noise and iTransformer shows seasonal-noise vulnerability
- Spectral analysis reveals all models introduce spurious frequency components, with S-Mamba and iTransformer achieving superior frequency reconstruction

## Why This Works (Mechanism)

### Mechanism 1: Lookback Window vs. Signal Periodicity
Forecasting performance degrades when the lookback window $T$ is insufficient to capture at least one full period of the dominant seasonal frequency. When input horizon is shorter than signal wavelength, periodic oscillations appear as partial trends to the model, preventing it from inferring cyclical nature from partial segments.

### Mechanism 2: Architectural Alignment to Waveform Geometry
Distinct architectural inductive biases cause models to favor specific waveform geometries. S-Mamba's selective state-space mechanism models sharp transitions in sawtooth waves better than smooth operators, while iTransformer's attention mechanism captures gradual phase relationships in sinusoidal signals but struggles with sawtooth resets.

### Mechanism 3: Noise-Type Specificity
Architectures exhibit specific vulnerabilities to signal-dependent noise types rather than uniform degradation. S-Mamba is more sensitive to trend noise (interferes with long-term state propagation), while iTransformer is more vulnerable to seasonal noise (obscures cross-variate correlations needed for attention alignment).

## Foundational Learning

- **Concept: Frequency Index & Aliasing**
  - Why needed: To prevent "lookback mismatch," calculate if window size $T$ can physically observe a full cycle of lowest frequency you intend to forecast
  - Quick check: Given N=35,040 and T=96, can the model capture frequency index 200? (No, minimum capturable is 355)

- **Concept: Signal-to-Noise Ratio (SNR) Scaling**
  - Why needed: Framework scales signal and noise to precise SNR targets; understanding weighting is necessary to replicate dataset difficulty
  - Quick check: If SNR=1, how does noise power relate to signal power? (They are equal)

- **Concept: Spectral Reconstruction vs. Time-Domain MSE**
  - Why needed: Models can achieve low MSE while failing to reconstruct true frequency spectrum (spurious frequencies); relying solely on MSE may miss hallucinated patterns
  - Quick check: If model predicts smooth curve minimizing MSE but misses high-frequency harmonic, would spectral analysis catch this? (Yes)

## Architecture Onboarding

- **Component map:** Generator (Hierarchical component assignment → Signal/Noise Aggregation → SNR Scaling) -> Models (S-Mamba, iTransformer, R-Linear, Autoformer) -> Evaluator (MSE + Spectral Analysis)

- **Critical path:** Configuring the Frequency Index relative to the Lookback Window is most critical; if lookback ($T$) is too short for selected frequency band, all downstream analysis shows universal failure, obscuring architectural differences

- **Design tradeoffs:**
  - Synthetic vs. Real Data: Synthetic offers noise isolation (known ground truth) but may lack chaotic complexity of real-world systems
  - Model Complexity: R-Linear offers high inference speed but saturates on complex multivariate data; S-Mamba offers high spectral fidelity at computational cost

- **Failure signatures:**
  - Sawtooth on iTransformer: High MSE due to attention struggling with sharp resets
  - Trend Noise on S-Mamba: Unexpectedly high degradation compared to other noise types
  - Spectral Sparsity: Models outputting smooth predictions lacking high-frequency harmonics present in ground truth

- **First 3 experiments:**
  1. Frequency Threshold Test: Generate sine waves with frequency indices [200, 400, 600] using T=96; verify if MSE spikes below index 355
  2. Noise Sensitivity Probe: Train S-Mamba and iTransformer on sawtooth waves with Trend Noise vs. Seasonal Noise (SNR=10); confirm S-Mamba degrades more on Trend Noise
  3. Spectral Verification: Predict on noise-free signal and compute FFT of output; check if model introduces spurious frequency components

## Open Questions the Paper Calls Out

- **Open Question 1:** Can M-LTSF models be architected or trained to achieve clean spectral representations without spurious frequency components? (All four architectures failed to learn clean spectrum representations even under ideal noise-free conditions)

- **Open Question 2:** Do observed model behaviors (signal type preferences, noise sensitivities, frequency-dependent degradation) generalize to real-world datasets with well-characterized noise properties?

- **Open Question 3:** How do M-LTSF models respond to additional noise types such as colored noise with specific spectral properties (pink, blue noise)?

- **Open Question 4:** How sensitive are observed architectural preferences to hyperparameter choices and training duration? (Paper notes Autoformer exhibits "higher sensitivity to hyperparameters" and all models trained for only 5 epochs)

## Limitations
- Implementation-specific parameters (hyperparameters, training settings) are deferred to an associated GitHub repository that was not available at time of writing
- Synthetic data may not fully capture the coupled, non-stationary dynamics of real-world systems despite superior control over signal-noise interactions
- Seed configuration ambiguity could affect reproducibility of exact reported values

## Confidence
- **High Confidence:** Universal degradation when lookback cannot capture full seasonal periods; R-Linear consistently underperforms on complex multivariate data; White and Brownian noise degrade all models with lower SNR
- **Medium Confidence:** S-Mamba's specific vulnerability to trend noise and iTransformer's vulnerability to seasonal noise; Architectural preference for sawtooth vs. sinusoidal signals; Spectral superiority of S-Mamba and iTransformer
- **Low Confidence:** Absolute performance rankings across all configurations (dependent on unpublished hyperparameters); Exact magnitude of frequency reconstruction capabilities (spectral error metrics not fully detailed)

## Next Checks
1. **Lookback-Frequency Threshold Validation:** Replicate frequency index sweep with T=96, verifying critical threshold at l=355 where MSE spikes when l < ⌈N/T⌉

2. **Noise Type Sensitivity Replication:** Train S-Mamba and iTransformer on sawtooth signals with Trend Noise vs. Seasonal Noise at SNR=10; confirm specific degradation patterns

3. **Spectral Sparsity Test:** Generate noise-free sawtooth and sine signals, predict using S-Mamba and iTransformer, compute FFT of outputs; verify whether S-Mamba captures sawtooth discontinuities while iTransformer smooths them