---
ver: rpa2
title: 'MADR: MPC-guided Adversarial DeepReach'
arxiv_id: '2510.18845'
source_url: https://arxiv.org/abs/2510.18845
tags:
- control
- function
- value
- madr
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MADR (MPC-guided Adversarial DeepReach) addresses the challenge
  of solving high-dimensional Hamilton-Jacobi reachability problems in adversarial
  scenarios, where traditional grid-based methods fail due to the curse of dimensionality.
  The core idea is to combine self-supervised learning of HJ-PDE's with supervised
  adversarial sampling-based MPC rollouts.
---

# MADR: MPC-guided Adversarial DeepReach

## Quick Facts
- arXiv ID: 2510.18845
- Source URL: https://arxiv.org/abs/2510.18845
- Authors: Ryan Teoh; Sander Tonkens; William Sharpless; Aijia Yang; Zeyuan Feng; Somil Bansal; Sylvia Herbert
- Reference count: 29
- Primary result: MADR significantly outperforms state-of-the-art baselines in high-dimensional Hamilton-Jacobi reachability problems, achieving 98.9% safe rate in 13D quadrotor scenarios compared to 86.6% for ISAACS

## Executive Summary
MADR (MPC-guided Adversarial DeepReach) addresses the fundamental challenge of solving high-dimensional Hamilton-Jacobi reachability problems in adversarial scenarios, where traditional grid-based methods fail due to the curse of dimensionality. The approach combines self-supervised learning of HJ-PDEs with supervised adversarial sampling-based MPC rollouts, using the current value function gradient to define opponent policies during sampling. This enables robust learning of a single value function for both players in zero-sum games while overcoming the instability issues of actor-critic methods. The framework achieves near-optimal performance in pursuit-evasion games and maintains safety guarantees in hardware demonstrations.

## Method Summary
MADR integrates supervised sampling-based MPC rollouts into the DeepReach training loop to accelerate convergence and improve solution quality in high-dimensional spaces. The method uses a value-only architecture (3-layer sinusoidal MLP) trained on a combined loss function that includes both the PDE residual and MPC supervision loss. During dataset generation, the current value function gradient defines the opponent's policy, creating a closed-loop adversarial interaction that forces the controller to react to worst-case disturbances. The approach alternates between data collection (Algorithm 1) and joint training, with curriculum learning over time and periodic MPC dataset regeneration as the value network improves.

## Key Results
- Achieved 98.9% safe rate in 13D quadrotor simulation with wind disturbances versus 86.6% for ISAACS
- Recovered BRT with 0.997 IOU in 6D Dubins pursuit-evasion game versus 0.928 for ISAACS
- Demonstrated real-time performance (>50Hz) in hardware experiments on TurtleBots and Crazyflie drones
- Successfully captured competing policies in 20D drone pursuit-evasion games while maintaining evader safety

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Integrating supervised MPC rollouts into self-supervised DeepReach training accelerates convergence and improves solution quality in high-dimensional spaces.
- **Mechanism**: Standard PINNs suffer from weak gradients in unexplored regions. SB-MPC trajectories provide dense supervisory signals that anchor value function learning to feasible dynamics, preventing stalling in flat local minima.
- **Core assumption**: SB-MPC can generate sufficiently optimal rollouts to serve as valid expert supervision.
- **Evidence anchors**: Abstract states MPC rollouts "enrich the self-supervised learning"; Section I references [10] showing significant performance improvements from sample-based MPC.

### Mechanism 2
- **Claim**: Using current value function gradient to define opponent policy enables learning a single robust value function for both players.
- **Mechanism**: Instead of co-learning two networks, MADR creates closed-loop adversarial interaction during data collection. Opponent acts optimally by minimizing $\nabla V \cdot w \cdot d$, forcing controller to react to worst-case disturbance.
- **Core assumption**: Current $\nabla V_\theta$ approximation is locally accurate enough to generate meaningful adversarial actions.
- **Evidence anchors**: Abstract explicitly states this approach; Section III.A describes gradient-based disturbance sampling.

### Mechanism 3
- **Claim**: Value-only architecture avoids instability inherent in actor-critic methods for zero-sum games.
- **Mechanism**: Actor-critic methods suffer from non-stationarity where shifting actor destabilizes critic. MADR trains only value network and derives policies analytically online via Hamiltonian maximization/minimization.
- **Core assumption**: System dynamics are control/disturbance affine, allowing analytic policy extraction from value gradient.
- **Evidence anchors**: Abstract contrasts with actor-critic instability; Section I explains value-only approach ensures high-quality policies from accurately-learned values.

## Foundational Learning

- **Concept: Hamilton-Jacobi (HJ) Reachability**
  - **Why needed here**: Mathematical foundation for the HJI Variational Inequality (Eq. 5). Understanding how $V(x,t)$ defines safe set $S(t)$ is essential for interpreting loss functions and safety metrics.
  - **Quick check question**: If $V(x, t) < 0$ for a state $x$, what does that imply for the robot's safety guarantees?

- **Concept: Physics-Informed Neural Networks (PINNs)**
  - **Why needed here**: "DeepReach" base is a PINN where self-supervised loss is PDE residual. Network satisfies differential equation structure, not just fits data.
  - **Quick check question**: Why does "curse of dimensionality" affect grid-based HJ solvers but not PINNs (theoretically)?

- **Concept: Zero-Sum Differential Games (Isaacs' Condition)**
  - **Why needed here**: Problem formulated as game (Player I vs Player II). Understanding min-max vs max-min order and non-anticipative strategies is crucial for interpreting two separate datasets.
  - **Quick check question**: Why does the paper define disturbance strategy as $d[u]$ (non-anticipative) rather than fixed sequence?

## Architecture Onboarding

- **Component map**: State $x$, time $t$ -> 3-layer Sinusoidal MLP (512 neurons/layer) -> Value $V$ -> Policy Extractor (analytic argmax/argmin) -> SB-MPC Engine (H=100, N=100, K=10) -> Dataset Buffers ($\mathcal{D}_{MPC,u}$, $\mathcal{D}_{MPC,d}$)

- **Critical path**:
  1. Curriculum Pre-training: Train vanilla DeepReach (PDE loss only) for initial stability
  2. Adversarial Data Collection: Run Alg 1 using current $\nabla V$ to generate adversarial actions
  3. Joint Training: Update $V_\theta$ using $\mathcal{L}_{combined} = \mathcal{L}_{PDE} + \mathcal{L}_{MPC}$
  4. Repeat: Periodically refresh MPC dataset with updated value network

- **Design tradeoffs**:
  - Dataset Size vs. Frequency: 10k-30k samples used; larger sets improve accuracy but increase latency
  - Refinement Steps ($K$): Higher $K$ yields better MPC trajectories but linearly increases compute time
  - Horizon ($H$): Short horizons cause myopic estimates; long horizons compound gradient errors

- **Failure signatures**:
  - Safety Collapse: Safe rate drops below 80%, usually indicates sparse MPC dataset or low PDE loss weight
  - Gradient Exploit: Opponent finds value landscape flaw causing evader freeze or oscillation
  - Divergence: Value loss spikes; check learning rate relative to MPC loss weighting ($\lambda_{FT}=100$)

- **First 3 experiments**:
  1. 6D Dubins Car Validation: Run against DP ground truth, calculate IOU to verify 0.997 result
  2. Ablation on Supervision: Train "Vanilla DeepReach" vs. "MADR" on 13D Quadrotor, compare safe rates
  3. Hardware-in-Loop: Deploy on simulated TurtleBot to verify real-time policy extraction (>50Hz) and state noise handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MADR framework extend to general-sum differential games where agents have independent, non-opposing objectives?
- Basis in paper: Problem formulation strictly restricts to "two-player, zero-sum" games
- Why unresolved: Value function derivation relies on min-max HJI Variational Inequality, assuming strictly adversarial opponent
- What evidence would resolve it: Theoretical extension for Nash equilibrium in general-sum scenario or empirical results in cooperative-competitive task

### Open Question 2
- Question: How robust is MADR policy to significant state estimation noise or partial observability without motion capture?
- Basis in paper: Hardware experiments use motion-capture arena for full-state observability
- Why unresolved: Method learns $V(x,t)$ assuming true state $x$; real-world deployment typically involves drift-prone sensors
- What evidence would resolve it: Hardware experiments using only onboard sensors for state estimation despite drift

### Open Question 3
- Question: Can base MADR algorithm resolve solution quality deterioration in long-horizon games without "following" filter heuristic?
- Basis in paper: Section III.C states long-horizon solution quality tends to deteriorate, prompting hand-designed filter strategy
- Why unresolved: Proposed "follow" policy patches compounding errors rather than fixing fundamental learning algorithm stability
- What evidence would resolve it: Modification maintaining high IOU and capture performance over long horizons without switching to $V_{follow}$ objective

## Limitations

- **Dimensionality Scaling**: Experimental validation limited to 20 states; effectiveness for 50+ state systems unproven due to computational burden of MPC rollouts
- **Dynamic Model Dependency**: Requires accurate, differentiable dynamics models; performance degrades with learned or uncertain models
- **Hyperparameter Sensitivity**: Performance heavily depends on MPC horizon, sample count, refinement iterations, and loss weighting with limited sensitivity analysis

## Confidence

**High Confidence**: Core architectural contributions (value-only approach, supervised MPC integration, gradient-based opponent definition) are well-specified and theoretically sound. Strong empirical validation with concrete 13D quadrotor metrics (98.9% vs 86.6% safe rate).

**Medium Confidence**: 6D Dubins results show promising IOU metrics (0.997 vs 0.928) but comparison is against DP ground truth rather than neural network approaches. 20D drone results demonstrate scalability but lack quantitative comparison to alternatives.

**Low Confidence**: Hardware demonstrations limited to specific scenarios (TurtleBot, Crazyflie). Humanoid vs drone teleoperation is preliminary proof-of-concept rather than rigorous validation.

## Next Checks

1. **Ablation Study on Supervision**: Implement "Vanilla DeepReach" without MPC supervision and compare safe rates on 13D quadrotor scenario to quantify specific contribution of supervised sampling component.

2. **Model Uncertainty Testing**: Evaluate MADR performance with Â±10% parameter uncertainty in quadrotor dynamics model to test robustness to realistic modeling errors in practical applications.

3. **Scaling Experiment**: Apply MADR to synthetic 50-state system (e.g., 5 quadrotors or high-dimensional robot arm) to empirically measure computation time and solution quality scaling with dimensionality.