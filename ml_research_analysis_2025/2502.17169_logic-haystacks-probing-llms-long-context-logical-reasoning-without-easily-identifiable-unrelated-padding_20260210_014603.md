---
ver: rpa2
title: 'Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily
  Identifiable Unrelated Padding)'
arxiv_id: '2502.17169'
source_url: https://arxiv.org/abs/2502.17169
tags:
- evidence
- retrieval
- reasoning
- language
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new methodology for evaluating long-context
  logical reasoning in large language models. Instead of using easily identifiable
  unrelated padding, it generates extensive simplified English text paired with first-order
  logic representations spanning up to 2048 clauses (around 25k tokens).
---

# Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)

## Quick Facts
- **arXiv ID**: 2502.17169
- **Source URL**: https://arxiv.org/abs/2502.17169
- **Authors**: Damien Sileo
- **Reference count**: 8
- **Primary result**: Models' effective context windows are much smaller than claimed, with performance degrading significantly at just 128 clauses when using realistic distractors

## Executive Summary
This paper introduces Logic Haystacks, a new methodology for evaluating long-context logical reasoning in large language models. Unlike standard needle-in-haystack evaluations that use easily identifiable unrelated padding, Logic Haystacks generates extensive simplified English text paired with first-order logic representations spanning up to 2048 clauses (around 25k tokens). The evaluation task involves evidence retrieval for contradiction detection, where models must identify which premise clauses contradict a given hypothesis. Results show that models' effective context windows are much smaller than claimed, with performance already degrading significantly at just 128 clauses when using realistic distractors. This suggests current evaluations overestimate language models' long-context processing capabilities.

## Method Summary
Logic Haystacks generates long-context logical reasoning tasks using a satisfiable merging pipeline with first-order logic theorem proving. The method starts with K formulas of 32 clauses generated by a Unigram-FOL grammar, then iteratively merges pairs while removing contradictory clauses using the Vampire theorem prover until reaching 4096 clauses. For each contradiction found, necessary evidence is identified through counterfactual removal - clauses are kept only if removing them changes the contradiction status. The final dataset contains 1-3 necessary evidences per example across 8-2048 clause contexts, with 600 validation and 2790 test examples. Evaluation uses zero-shot prompting with Jaccard similarity to measure evidence retrieval accuracy.

## Key Results
- Gemini Flash, GPT-4o-mini, and Llama-3.3-70B show sharp performance drops between 64-128 clauses with realistic distractors
- Paul Graham padding yields ~10-30% higher retrieval rates than realistic distractors across models
- Adding more evidence (1→2→3) causes ~30-50% performance drops at fixed context sizes
- Scaling model size is insufficient to work even with 1 distractor, suggesting fundamental reasoning limitations

## Why This Works (Mechanism)

### Mechanism 1: Satisfiable Merging for Contradiction-Free Long Contexts
Hierarchical merging with theorem-prover verification generates arbitrarily long premises without paradoxes while maintaining distractor difficulty. Start with K formulas of 32 clauses. At each stage i, randomly pair formulas and compute "satisfiable merge"—conjoin two formulas, then iteratively remove clauses identified in contradiction proofs until satisfiable. Maximum formula size doubles per stage, reaching up to 4096 clauses.

### Mechanism 2: Counterfactual Evidence Isolation for Unique Ground Truth
Recomputing contradictions with each evidence removed identifies *necessary* evidence, creating well-formed tasks with unique solutions. For each clause e in the prover's derivation, recompute whether (PREMISE \ {e}, HYPOTHESIS) is still contradictory. If yes, e was not necessary. Keep only examples where all evidences are necessary.

### Mechanism 3: Realistic Distractors Reveal True Context Limitations
Homogeneous, semantically-controlled distractors expose effective context window limits that unrelated padding (e.g., Paul Graham essays) conceals. Unigram-FOL grammar generates simplified English + FOL pairs where distractors share vocabulary/structure with evidence but are provably non-interfering.

## Foundational Learning

- **First-Order Logic (FOL) Satisfiability**: The entire generation pipeline depends on checking whether conjunctions of logical statements are contradictory; without understanding FOL semantics, the merging algorithm and evidence identification cannot be understood or debugged.
  - Quick check: Given premises "All A are B" and "x is A," does "x is not B" create a contradiction? What about "All A are B" and "No B is C"—is this satisfiable?

- **Necessity vs. Sufficiency in Logical Proof**: The paper's key contribution is isolating *necessary* evidence via counterfactual removal; confusing sufficiency (prover's derivation) with necessity (unique solution) will lead to malformed evaluation tasks.
  - Quick check: If premises {P1, P2, P3} contradict H, and {P1, P2} also contradict H, which set is sufficient? Which is necessary for the full premise?

- **Needle-in-Haystack Evaluation Paradigm**: This paper is a direct critique of standard NIAH benchmarks; understanding what they measure (simple retrieval) vs. what this measures (reasoning with confusable distractors) is essential for interpreting results.
  - Quick check: Why does inserting a fact into unrelated text (standard NIAH) potentially overestimate model capability compared to inserting it into semantically-similar text?

## Architecture Onboarding

- **Component map**: Unigram-FOL Grammar Generator -> Vampire Theorem Prover -> Satisfiable Merging Pipeline -> Counterfactual Evidence Filter -> Evaluation Harness
- **Critical path**: Grammar generation → Initial formula sampling (32 clauses) → Satisfiable merging (stages 0→log₂(N)) → Hypothesis generation → Counterfactual filtering → Final dataset with 1/2/3 necessary evidences. Bottleneck is FOL solving at scale.
- **Design tradeoffs**: Expressivity vs. Scalability (full FOL limits to ~4096 clauses); Diversity vs. Collision Risk (78 predicates chosen as balance); Zero-shot vs. Prompt-tuned (paper uses defaults).
- **Failure signatures**: Paradox drift (check prover outputs at each stage); Non-unique solutions (verify each example has exactly one answer); Surface-matching shortcuts (discarded explicitly).
- **First 3 experiments**: 1) Reproduce the 128-clause degradation with Gemini Flash, GPT-4o-mini, Llama-3.3-70B; 2) Ablate evidence count comparing 1-evidence vs 2-evidence at fixed N=256; 3) Test hypothesis isolation variant to measure hypothesis prominence vs genuine reasoning.

## Open Questions the Paper Calls Out

1. **Does training on long-context logical reasoning synthetic data yield transferable gains on Logic Haystacks evaluation?**
   - Basis: Conclusion states "Further work is needed... to evaluate whether training on long-context logical reasoning synthetic data... leads to transferable gains on Logic Haystacks."
   - Why unresolved: Paper only conducts zero-shot evaluation; no training experiments were performed.

2. **Can less expressive logics enable scalable data construction beyond 4096 clauses while maintaining task validity?**
   - Basis: Conclusion notes "going beyond 4096 clauses starts being intractable with our method" and suggests using "a less expressive logic to scale data construction more easily."
   - Why unresolved: Current FOL-based generation becomes computationally prohibitive beyond 4096 clauses (12 days with 52 threads).

3. **Would extending evaluation to entailment and equivalence detection reveal similar or different degradation patterns?**
   - Basis: Limitations section notes "The focus on contradiction detection rather than other logical relations like entailment or equivalence narrows the scope of evaluation."
   - Why unresolved: Paper found entailment hypotheses rare (<30%) but did not systematically study other logical relations.

## Limitations

- **Computational Intractability Ceiling**: The satisfiable merging approach hits a hard limit at ~4096 clauses due to FOL solver runtime, creating an artificial ceiling on evaluation scale.
- **Distractor Validity Verification**: Performance gap with Paul Graham padding (10-30% higher accuracy) suggests distractors may still be partially distinguishable despite claims of being "provably non-interfering."
- **Zero-shot Generalization**: All evaluations use default model settings without prompt engineering, potentially conflating inherent reasoning limitations with suboptimal prompting strategies.

## Confidence

- **High Confidence**: The methodology for generating long contexts with satisfiable merging is well-specified and reproducible. The observation that performance degrades at 128 clauses with realistic distractors is robust.
- **Medium Confidence**: The claim that effective context windows are "much smaller than claimed" depends on the assumption that unrelated padding provides an unfair advantage. This is supported but could be challenged by prompt engineering studies.
- **Low Confidence**: The assertion that this reveals fundamental reasoning limitations rather than prompting or architectural constraints. The zero-shot evaluation may underestimate true model capabilities.

## Next Checks

1. **Prompt Engineering Benchmark**: Re-run the 128-clause evaluation with optimized prompts (few-shot examples, system instructions) to determine if the 30-50% accuracy gap is due to reasoning limitations or prompting deficits.

2. **Distractor Robustness Analysis**: Systematically measure model performance when distractors share increasing degrees of semantic similarity with evidence (controlled vocabulary overlap, syntactic patterns) to quantify the "hard to distinguish" claim.

3. **Scaling Law Validation**: Generate intermediate dataset sizes (512, 1024 clauses) to map the precise degradation curve and test whether the 128-clause inflection point holds across different model families and instruction-tuning regimes.