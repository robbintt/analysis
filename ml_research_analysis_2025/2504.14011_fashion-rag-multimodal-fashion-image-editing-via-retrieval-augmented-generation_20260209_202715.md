---
ver: rpa2
title: 'Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation'
arxiv_id: '2504.14011'
source_url: https://arxiv.org/abs/2504.14011
tags:
- image
- retrieved
- textual
- input
- garment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fashion-RAG, a novel retrieval-augmented
  generation framework for multimodal fashion image editing. The method addresses
  the challenge of generating high-quality fashion images based on textual descriptions
  by retrieving relevant garment images from an external database and incorporating
  their visual attributes into the generative process.
---

# Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2504.14011
- **Source URL**: https://arxiv.org/abs/2504.14011
- **Reference count**: 40
- **Primary result**: Fashion-RAG achieves state-of-the-art performance on multimodal fashion image editing by integrating retrieved garment visuals into text-to-image diffusion through textual inversion.

## Executive Summary
Fashion-RAG introduces a retrieval-augmented generation framework for multimodal fashion image editing that combines textual descriptions with relevant garment images to produce high-quality fashion photographs. The method retrieves garment images from an external database using CLIP embeddings, then projects these images into the CLIP textual embedding space via a learned network F_θ. This allows seamless integration with Stable Diffusion's cross-attention mechanism without architectural modifications. Experimental results on the Dress Code dataset demonstrate Fashion-RAG outperforms state-of-the-art methods both qualitatively and quantitatively, achieving lower FID and KID scores while maintaining strong semantic alignment with input text.

## Method Summary
Fashion-RAG operates on the Dress Code dataset, using Stable Diffusion v2.1 inpainting as the backbone. Given a textual garment description, the method retrieves relevant garment images from an external database built from training data. A learnable network F_θ (ViT + MLP) projects retrieved garment images into CLIP textual embedding space, producing pseudo-token embeddings that are concatenated with text tokens. The combined embeddings are processed through the standard cross-attention pipeline, allowing the U-Net to attend to both textual and visual conditioning simultaneously. The approach employs a two-stage training strategy: first training only F_θ for 200k steps to learn visual-to-textual mapping, then jointly fine-tuning the U-Net and F_θ for 120k steps with zero-initialized pose modules.

## Key Results
- Achieves FID of 5.42 and KID of 1.49 with N_r=3 retrieved garments, outperforming baseline methods
- Performance improves as more retrieved items are incorporated (FID improves from 5.90 to 5.42 as N_r increases from 0 to 3)
- Maintains strong semantic alignment with input text while capturing fine-grained visual details from retrieved garments
- Particularly effective at generating realistic fashion images that combine textual intent with visual attributes from reference garments

## Why This Works (Mechanism)

### Mechanism 1
Projecting retrieved garment images into CLIP textual embedding space enables diffusion models to incorporate fine-grained visual attributes without architecture changes. A learnable network F_θ takes CLIP vision encoder features and produces pseudo-token embeddings compatible with the text encoder. These visual embeddings are concatenated with text tokens and processed through the standard cross-attention pipeline, allowing the U-Net to attend to both textual and visual conditioning simultaneously. The core assumption is that CLIP textual space is sufficiently expressive to represent garment visual attributes in a way that Stable Diffusion's cross-attention can decode into pixel-level details.

### Mechanism 2
Increasing the number of retrieved garments (N_r) improves generation quality metrics up to a point, as the model aggregates complementary visual features. Multiple retrieved garments provide redundant and complementary visual information, with the textual inversion producing N_v = 16 embeddings per garment. This richer conditioning signal helps the denoising process resolve ambiguous textual descriptions with concrete visual references. The core assumption is that retrieved garments are sufficiently similar to share relevant attributes while providing complementary details.

### Mechanism 3
Two-stage training separates representation learning (textual inversion) from generation finetuning, stabilizing optimization. Stage 1 trains only F_θ for 200k steps to learn the visual-to-textual mapping. Stage 2 adds zero-initialized pose modules and jointly fine-tunes U-Net + F_θ for 120k steps. This prevents the U-Net from overfitting to poorly projected embeddings early in training. The core assumption is that the pre-trained Stable Diffusion inpainting model has sufficient generative capability; only conditioning adaptation is needed.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)** - Why needed here: Fashion-RAG builds on Stable Diffusion v2.1 inpainting; understanding VAE latent space, U-Net denoising, and cross-attention conditioning is essential for debugging generation quality. Quick check question: Can you explain why LDMs operate in latent space rather than pixel space, and how cross-attention layers receive conditioning signals?

- **Concept: CLIP Joint Embedding Space** - Why needed here: The entire retrieval and textual inversion pipeline depends on CLIP's ability to align text and images in a shared space where cosine similarity reflects semantic relatedness. Quick check question: If CLIP text and image embeddings have different dimensionalities, how does the model compare them, and what does a high cosine similarity indicate?

- **Concept: Textual Inversion** - Why needed here: This is the core technique bridging retrieved images to the diffusion model's expected input format. Understanding pseudo-tokens and embedding optimization is critical for extending the approach. Quick check question: How does textual inversion differ from simply using a CLIP image embedding directly as conditioning?

## Architecture Onboarding

- **Component map**: Text input → OpenCLIP retrieval → F_θ (ViT + MLP) → textual inversion → embedding concatenation → CLIP text encoder → cross-attention K,V → U-Net denoising → VAE decoder → final image

- **Critical path**: Text input → retrieval → textual inversion → embedding concatenation → cross-attention. If retrieval fails (irrelevant garments) or inversion is poor (noisy embeddings), downstream generation degrades irrecoverably.

- **Design tradeoffs**: N_r=1 maximizes CLIP-I (visual similarity to reference) but may miss complementary details; N_r=3 optimizes realism (FID/KID) but dilutes similarity to any single garment. Textual inversion vs. IP-Adapter: Inversion integrates via existing cross-attention (simpler), while IP-Adapter adds separate modules (more expressive but complex). Token budget: N_L=77 fixed; each garment uses 16 tokens, limiting N_r max to ~4 with typical prompt length.

- **Failure signatures**:
  - Retrieval mismatch: Retrieved garments visually diverge from text intent → output ignores text or blends incoherently
  - Logo/text on garments: Model struggles to preserve text/logos from retrieved items
  - Pose misalignment: If pose map is incorrect, garment may not align with body regardless of conditioning quality

- **First 3 experiments**:
  1. Validate retrieval quality: Run retrieval on held-out queries; manually inspect top-3 results for semantic relevance before training
  2. Ablate N_r: Train with fixed N_r ∈ {0, 1, 2, 3} on a subset; plot FID, KID, CLIP-T, CLIP-I to confirm paper's trend holds
  3. Test textual inversion in isolation: Freeze U-Net, train only F_θ; visualize nearest neighbors in CLIP text space to verify learned embeddings cluster meaningfully

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be improved to handle cases where retrieved garments have significantly conflicting visual attributes? The current textual inversion and concatenation approach does not resolve conflicts between disparate visual features, leading to artifacts or incoherent blends. Evidence needed: A modified attention or fusion mechanism that weights or arbitrates among conflicting attributes, evaluated on a curated benchmark of divergent retrieved items.

### Open Question 2
Can the method be extended to accurately preserve logos, text, and fine graphic details from retrieved garments? CLIP embeddings and the textual inversion pathway may not encode high-frequency symbolic details, and the diffusion process tends to abstract them. Evidence needed: Introduction of a dedicated token-level or spatial attention module for graphic preservation, with quantitative evaluation using OCR or logo similarity metrics on generated outputs.

### Open Question 3
Does scaling to more than three retrieved garments continue to improve generation quality, or does it introduce diminishing returns or new failure modes? Increased conditioning length may saturate or disrupt cross-attention, and the CLIP token limit (N_L=77) constrains how many visual embeddings can be appended. Evidence needed: Systematic experiments with N_r up to 10 or more, potentially with a longer-context encoder or hierarchical fusion, measuring FID, KID, and CLIP scores.

### Open Question 4
How robust is Fashion-RAG to retrieval errors or low-quality matches in the external database? Poor retrievals could misguide the generative process, and the current framework has no explicit mechanism to reject or downweight irrelevant items. Evidence needed: Controlled experiments injecting varying levels of irrelevant or noisy retrievals, with metrics on output coherence and alignment to the input text.

## Limitations
- Textual inversion network F_θ lacks complete architectural specifications, which could affect reproducibility
- The method assumes retrieved garments are relevant and doesn't address how retrieval quality impacts final generation fidelity
- Performance trade-offs exist: increasing N_r improves realism metrics but decreases visual similarity to any single reference garment
- The framework doesn't explicitly handle cases where retrieved garments have conflicting visual attributes

## Confidence
**High Confidence**: The retrieval-augmented generation framework itself is sound, and the two-stage training strategy (200k steps F_θ only, then 120k steps joint fine-tuning) is well-justified. The quantitative improvements (FID 5.90→5.42, KID 1.81→1.49) and qualitative results in Figure 4 are clearly demonstrated.

**Medium Confidence**: The mechanism by which CLIP textual embeddings effectively capture fine-grained garment visual attributes is plausible but not extensively validated. While the paper shows N_r=3 optimizes realism metrics, the trade-off with CLIP-I (visual similarity to reference) suggests the blending may not always preserve specific visual details.

**Low Confidence**: The generalization of results to other fashion datasets or different garment categories remains untested. The paper's focus on Dress Code (53k pairs) doesn't establish robustness across diverse fashion domains.

## Next Checks
1. Validate retrieval relevance: Run retrieval on held-out queries from Dress Code test set and manually inspect top-3 results for semantic relevance. Compute retrieval precision@k to establish baseline retrieval quality before any training occurs.

2. Ablate N_r effect: Train Fashion-RAG with fixed N_r ∈ {0, 1, 2, 3} on a 1k-sample subset. Plot FID, KID, CLIP-T, and CLIP-I to confirm the paper's trend holds and verify that CLIP-I decreases while FID/KID improve with higher N_r.

3. Isolate textual inversion quality: Freeze the U-Net and train only F_θ for 50k steps. At intervals, retrieve a held-out garment, pass through F_θ, and compute nearest neighbors in CLIP text space to verify learned embeddings cluster meaningfully and maintain garment attributes.