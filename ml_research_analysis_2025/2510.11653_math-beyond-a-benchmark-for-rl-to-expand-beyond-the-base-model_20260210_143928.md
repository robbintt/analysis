---
ver: rpa2
title: 'MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model'
arxiv_id: '2510.11653'
source_url: https://arxiv.org/abs/2510.11653
tags:
- base
- problems
- arxiv
- reasoning
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitation that current math benchmarks\
  \ (e.g., MATH-500, AIME 2024) are saturated by base models, making it hard to measure\
  \ genuine reasoning capability expansion via RL. To tackle this, the authors introduce\
  \ MATH-Beyond (MATH-B), a benchmark suite constructed from problems unsolvable by\
  \ base models (\u22648B parameters) even with 1024 attempts."
---

# MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model

## Quick Facts
- arXiv ID: 2510.11653
- Source URL: https://arxiv.org/abs/2510.11653
- Authors: Prasanna Mayilvahanan; Ricardo Dominguez-Olmedo; Thaddäus Wiedemer; Wieland Brendel
- Reference count: 39
- One-line primary result: MATH-Beyond isolates problems unsolvable by base models (≤8B) even with 1024 attempts, revealing that current RL methods achieve minimal expansion while distillation achieves 59-66% expansion

## Executive Summary
This paper addresses a critical limitation in current math benchmarks: saturation by base models makes it impossible to measure genuine reasoning capability expansion through reinforcement learning. The authors introduce MATH-Beyond (MATH-B), a benchmark suite constructed from problems unsolvable by base models (≤8B parameters) even with 1024 attempts. The benchmark is derived from DAPO-Math-17K and DeepScaleR datasets, filtered for quality, and verified by strong models. Experiments show that RL-finetuned models achieve minimal expansion (≤10%) on MATH-B at pass@1024, while models trained via distillation from long Chain-of-Thought achieve much higher expansion rates (~59-66%). This highlights that current RL methods fall short of discovering novel reasoning paths, motivating the need for exploration-driven RL research.

## Method Summary
MATH-Beyond is constructed through a multi-stage filtering process: (1) problems are filtered for integer ground truths, no multiple-choice questions, and no images; (2) pre-screened with DeepSeek-R1-Distill-Qwen2.5-7B at pass@16; (3) ground truth verified with o4-mini-high/GPT-5-Mini at pass@2; (4) deduplicated against MATH-500, AIME, and other datasets; (5) filtered at pass@1024 across 11 base models and 11 supplementary models. The final benchmark contains 181 union problems (MATH-B-U) and 41 intersection problems (MATH-B-I) where all base models fail. Expansion Rate is measured as pass@k(post-trained) since base models achieve near-zero success rates.

## Key Results
- RL-finetuned models (Nemotron-Research-Reasoning-Qwen-1.5B, DeepScaleR-1.5B) achieve minimal expansion (≤10%) on MATH-B at pass@1024
- Distillation from long Chain-of-Thought models (Qwen3-4B, Qwen3-8B) achieves 59-66% expansion rates on MATH-B
- Base models (≤8B) achieve near-zero pass@1024 on MATH-B-Intersection (41 problems), confirming the benchmark isolates genuine reasoning gaps
- Current RL methods primarily sharpen existing solution modes rather than discovering novel reasoning paths

## Why This Works (Mechanism)

### Mechanism 1: Reachable Set Quantification via pass@k Saturation
High sampling budgets reveal the true problem-solving boundary of base models; post-training gains on saturated benchmarks measure consolidation, not expansion. When pass@1024 ≈ 100% on a benchmark, any improvement from RL reflects sharpening existing solution modes. By constructing a benchmark where base models achieve pass@1024 ≈ 0%, any improvement IS genuine expansion.

### Mechanism 2: Difficulty Isolation Through Model-Gauntlet Filtering
Problems unsolvable by diverse base models at high sampling budgets isolate genuine reasoning gaps rather than verification artifacts or data quirks. Multi-stage filtering—quality checks, frontier model verification, and pass@1024 evaluation across 11+ base models—yields problems that systematically defeat current architectures.

### Mechanism 3: Verification Robustness via Failure Mode Analysis
Rule-based math verifiers introduce systematic errors that conflate formatting failures with reasoning failures; mitigating these isolates genuine capability changes. Seven failure modes are documented and filtered. The benchmark restricts to integer ground truths, removes MCQs and ambiguous problems, and applies robust parsing logic.

## Foundational Learning

- **Concept**: pass@k as a policy capability probe
  - Why needed here: The entire benchmark methodology hinges on interpreting pass@k as an approximation of a model's reachable problem set. Without this, expansion vs. consolidation cannot be distinguished.
  - Quick check question: If Model A achieves pass@1=30% and Model B achieves pass@64=30% on the same benchmark, which has the larger reachable set?

- **Concept**: Exploration vs. exploitation in RL
  - Why needed here: The paper argues current RL methods exploit (sharpen) rather than explore (expand). Understanding this distinction is essential for designing methods that might succeed on MATH-B.
  - Quick check question: Why might higher sampling temperature correlate with higher expansion rates, as observed with Skywork-OR1-7B?

- **Concept**: Distillation as distribution alignment
  - Why needed here: Long-CoT distillation achieves 59-66% expansion, suggesting exposure to target reasoning distributions—not RL-driven discovery—enables expansion.
  - Quick check question: If distillation provides the "correct distribution of reasoning steps," what does this imply about what RL must discover autonomously to match this performance?

## Architecture Onboarding

- **Component map**: MATH-B-U (181 problems) -> MATH-B-I (41 problems) -> Model-specific splits
- **Critical path**: 1. Select base model and corresponding model-specific split from Table 4 2. Establish baseline pass@1024 ≈ 0 (verify) 3. Apply RL method 4. Re-evaluate pass@1024 on same split → Expansion Rate 5. Diagnose: check for verification artifacts using Table 1 failure modes
- **Design tradeoffs**: k=1024 balances statistical stability vs. compute cost (20,000+ A100 hours for full evaluation); integer-only ground truths eliminate ambiguity but restrict problem diversity; frontier model verification ensures correctness but introduces dependency on proprietary models
- **Failure signatures**: Non-zero base model pass@1024 on claimed model-specific split → wrong split or benchmark contamination; RL model shows expansion but manual inspection reveals verifier F1-F7 failures → verification pipeline issue; Expansion rate plateaus before k=1024 → likely sharpening, not expansion
- **First 3 experiments**: 1. Baseline verification: Run base model (e.g., Qwen2.5-Math-1.5B) on its 115-problem split with pass@1024; confirm ≈0% success 2. RL method probe: Train DeepScaleR-style RL on same base model; measure expansion rate (expected: <10%) 3. Exploration intervention: Implement an entropy-regularized or curiosity-driven RL variant; measure whether expansion rate increases

## Open Questions the Paper Calls Out

- **Open Question 1**: Can reinforcement learning methods be developed that match the expansion rates of long Chain-of-Thought distillation without relying on a teacher model?
  - Basis in paper: Section 4 notes that distillation achieves ~59-66% expansion versus ~10% for RL. The authors state, "Developing RL methods that can discover these pathways without a teacher model remains a key challenge."

- **Open Question 2**: Will the MATH-B benchmark remain a "zero-baseline" test for significantly larger open-weight models (e.g., >8B parameters)?
  - Basis in paper: Section 5 states, "We expect a subset of items to remain zero-baseline for larger open-weight models. We did not evaluate those due to resource constraints."

- **Open Question 3**: What mechanisms allow models to solve problems that humans find difficult, yet fail on "easy" problems (human difficulty 4/10) included in MATH-B?
  - Basis in paper: Section 3.3 observes a "significant disconnect between human-perceived difficulty and model failure modes," noting the median difficulty is only 4/10.

## Limitations

- Benchmark relies on proprietary frontier models (o4-mini-high, GPT-5-Mini) for ground-truth verification, creating reproducibility bottlenecks
- Restriction to integer-only ground truths and exclusion of MCQs significantly narrows the problem space
- Choice of k=1024 as the pass@k budget remains an arbitrary threshold for approximating the true reachable set
- Verification pipeline, while addressing seven failure modes, may still contain subtle parsing errors

## Confidence

- **High confidence**: The core claim that MATH-B isolates genuine reasoning expansion problems (base models achieve near-zero pass@1024) is well-supported by comprehensive multi-model evaluation
- **Medium confidence**: The assertion that RL methods primarily consolidate rather than expand reasoning capabilities is supported by experimental results but could be influenced by specific RL algorithm choices
- **Medium confidence**: The conclusion that long-CoT distillation achieves higher expansion rates by exposing models to target reasoning distributions is plausible but requires further investigation

## Next Checks

1. **Verification Pipeline Stress Test**: Manually audit 20 randomly selected MATH-B problems using the exact verification logic described, checking for F1-F7 failure modes and ensuring integer ground truths are correctly handled without introducing false negatives.

2. **Base Model Baseline Replication**: Independently verify the pass@1024 baseline for at least two base models on their respective model-specific splits to confirm the near-zero baseline claim and ensure benchmark contamination hasn't occurred.

3. **Alternative k Budget Analysis**: Evaluate the same models at k=64, k=256, and k=1024 to empirically assess how quickly the pass@k curve saturates and whether 1024 samples truly approximates the reachable set boundary.