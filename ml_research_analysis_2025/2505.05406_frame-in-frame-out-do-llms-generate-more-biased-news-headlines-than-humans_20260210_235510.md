---
ver: rpa2
title: 'Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?'
arxiv_id: '2505.05406'
source_url: https://arxiv.org/abs/2505.05406
tags:
- framing
- framed
- llms
- news
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) generate
  more framed news headlines than human authors. The authors analyze 27 different
  LLMs using a framing detection framework, comparing model-generated content to human-authored
  summaries from the XSUM dataset.
---

# Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?

## Quick Facts
- **arXiv ID:** 2505.05406
- **Source URL:** https://arxiv.org/abs/2505.05406
- **Reference count:** 15
- **Primary result:** LLMs generate more framed news headlines than humans, with framing highest in politically sensitive topics

## Executive Summary
This paper investigates whether large language models generate more framed news headlines than human authors. Using a framing detection framework, the authors analyze 27 different LLMs and compare their outputs to human-authored summaries from the XSUM dataset. They find that LLMs consistently produce more framed content than humans, particularly in politically sensitive topics. Larger models trained on more extensive pretraining data show higher framing tendencies than smaller models. While fine-tuning on XSUM reduces framing bias, it does not eliminate it. The authors argue that current LLM evaluation frameworks insufficiently address framing bias, highlighting the need for benchmarks that assess interpretive dimensions beyond accuracy and fluency.

## Method Summary
The study uses GPT-4.1 nano, GPT-4o, and GPT-3.5-Turbo as a jury system (majority vote) to detect framing in news summaries. Researchers analyze 27 LLMs using human-generated summaries from XSUM as a baseline and model-generated summaries from Liu et al. (2024) and Panickssery et al. (2024). Framing rates are computed per model and topic (6 categories), with statistical tests comparing fine-tuned vs base models and model scale effects. The study also examines correlations between text length and framing presence, finding a moderate positive correlation.

## Key Results
- LLMs generate significantly more framed content than humans (XSUM baseline)
- Larger models with extensive pretraining show higher framing tendencies than smaller models (t-test p-value = 0.0012)
- Fine-tuning on XSUM reduces but does not eliminate framing bias (t-test p-value = 0.0006)
- Framing rates are highest in politically sensitive topics
- Moderate correlation exists between text length and framing presence (rpb ≈ 0.1904)

## Why This Works (Mechanism)

### Mechanism 1: Pretraining Scale Amplification
- Larger models with extensive pretraining data produce more framed content than smaller models
- Models trained on larger, more diverse corpora internalize more framing patterns from human-generated text, which are retrieved and amplified during generation
- Core assumption: Training corpora contain framing patterns that correlate with data volume and diversity
- Evidence: Larger models trained on extensive pretraining data (GPT, LLaMA, Cohere, Claude) produce more framed content; t-test confirms smaller models have significantly lower framing ratio (p-value = 0.0012)

### Mechanism 2: Task-Specific Fine-Tuning Alignment
- Fine-tuning on domain-specific summarization data reduces framing rates but doesn't eliminate them
- Fine-tuning aligns model outputs to the stylistic conventions of the target task (BBC headline summaries), which have lower inherent framing than general web text
- Core assumption: XSUM dataset has lower inherent framing than general pretraining corpora
- Evidence: Models fine-tuned on XSUM exhibit lower framing ratio; unpaired t-test reveals significantly lower framing ratio (p-value = 0.0006)

### Mechanism 3: Context-Length Interpretive Expansion
- Longer generated texts show modestly higher rates of framing
- Framed outputs require additional context or elaboration to convey a perspective, increasing text length
- Core assumption: Framing requires contextual support; shorter summaries constrain framing expression
- Evidence: Point-biserial correlation coefficient rpb ≈ 0.1904; texts identified as "Framed" had average length of 147 words versus 83 words for "Not Framed"

## Foundational Learning

- **Concept: Media Framing**
  - Why needed here: The study's core construct; framing selectively emphasizes aspects of an issue to shape perception (Entman, 1993)
  - Quick check question: How does framing differ from factual accuracy or sentiment?

- **Concept: LLM-as-Judge Evaluation**
  - Why needed here: The study uses GPT-4 family models as framing detectors via majority vote
  - Quick check question: What are the risks of using an LLM to evaluate another LLM's outputs?

- **Concept: In-Domain vs. Out-of-Domain Generalization**
  - Why needed here: Out-of-the-box models show different framing patterns than fine-tuned models, reflecting domain mismatch
  - Quick check question: Why might a model fine-tuned on BBC summaries behave differently than one trained only on general web text?

## Architecture Onboarding

- **Component map:** XSUM dataset + model-generated summaries -> GPT-4.1-nano, GPT-4o, GPT-3.5-Turbo jury (majority vote) -> Framing labels -> Topic classifier (keyword-based) -> Statistical analysis

- **Critical path:** Load XSUM subset and model-generated summaries -> Pass each summary through framing detector (3-model jury) -> Aggregate framing rates per model, per topic -> Compare to human baseline; run statistical tests for fine-tuning and scale effects

- **Design tradeoffs:** Using GPT-4 family as detector may inherit its own biases; no human annotation validation; XSUM represents only BBC-style journalism; keyword-based topic classification is coarse

- **Failure signatures:** Jury models disagree frequently (split votes) -> detector reliability issue; Topic classifier assigns "Political" to non-political content -> keyword list gaps; Fine-tuned model shows higher framing than base model -> data contamination or overfitting

- **First 3 experiments:**
  1. Validate detector agreement: Run framing detection on 50 samples manually; measure inter-annotator agreement between jury and human labels
  2. Topic ablation: Test whether framing rates hold when using a more robust topic classifier (e.g., zero-shot LLM classification)
  3. New model probe: Evaluate a model not in the original study (e.g., a recent open-weight model) to confirm scale-framing correlation

## Open Questions the Paper Calls Out

- What specific post-training mitigation strategies could effectively reduce framing bias in LLMs without compromising generation quality? The authors note that fine-tuning reduces but doesn't eliminate framing, suggesting targeted interventions are needed.
- Why do larger models with more extensive pretraining data exhibit higher framing tendencies, contrary to expectations that scale improves neutrality? The paper observes this correlation but doesn't investigate the mechanism.
- Can framing detection using LLM-as-judge approaches be validated against human annotations to ensure measurement validity? The paper uses GPT models to detect framing without human validation, raising concerns about circularity in bias measurement.

## Limitations

- Framing detection relies on GPT-4 family models as judges, introducing potential circularity since these same model families are being evaluated
- Focus on XSUM (BBC-style summaries) limits generalizability to other news domains and journalistic styles
- Topic classification uses a simple keyword-based approach that may miss nuanced framing patterns and contains category imbalances

## Confidence

**High Confidence:** The finding that LLMs produce more framed content than humans is well-supported by the comparative analysis with XSUM human baselines.

**Medium Confidence:** The relationship between model scale and framing tendencies is supported but not definitive, with a moderate negative correlation (-0.44).

**Low Confidence:** The framing detection accuracy itself, given its reliance on LLM judges without human validation, creates uncertainty about the absolute framing rates reported.

## Next Checks

1. **Human validation study:** Manually annotate a random sample of 100 model-generated and human summaries for framing presence, then compare agreement rates with the LLM jury system to establish detection reliability.

2. **Cross-domain replication:** Test the same models on news summaries from different sources (e.g., Reuters, AP) to assess whether the framing patterns hold across journalistic styles beyond BBC's XSUM format.

3. **Fine-tuning intervention experiment:** Fine-tune a base model on a curated dataset with reduced framing, then compare its framing output to both the original base model and XSUM fine-tuned version to isolate the effect of framing-aware training data.