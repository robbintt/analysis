---
ver: rpa2
title: Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label
  Cross-Modal Retrieval
arxiv_id: '2511.07780'
source_url: https://arxiv.org/abs/2511.07780
tags:
- cross-modal
- label
- noisy
- hashing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning robust cross-modal
  hashing under noisy multi-label supervision. The proposed SCBCH framework addresses
  this by combining two key modules: CSCC, which leverages cross-modal semantic consistency
  to adaptively reweight samples based on neighbor reliability, and BSCH, a bidirectional
  soft contrastive hashing module that constructs fine-grained soft pairs using label
  overlap rather than hard positives/negatives.'
---

# Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval

## Quick Facts
- arXiv ID: 2511.07780
- Source URL: https://arxiv.org/abs/2511.07780
- Reference count: 14
- Primary result: SCBCH achieves up to 2.3% MAP improvement over 12 SOTAs under 80% noise rates

## Executive Summary
This paper addresses the challenge of learning robust cross-modal hashing under noisy multi-label supervision. The proposed SCBCH framework combines two key modules: CSCC, which leverages cross-modal semantic consistency to adaptively reweight samples based on neighbor reliability, and BSCH, a bidirectional soft contrastive hashing module that constructs fine-grained soft pairs using label overlap rather than hard positives/negatives. Experiments on four widely used benchmarks show SCBCH consistently outperforms state-of-the-art methods, achieving significant MAP score improvements across multiple noise levels and hash code lengths.

## Method Summary
SCBCH tackles noisy multi-label cross-modal retrieval by introducing a two-module framework. The CSCC module computes neighbor-based soft labels and confidence weights to adaptively reweight samples during training, reducing the impact of noisy labels while retaining informative samples. The BSCH module constructs soft contrastive pairs using Jaccard label overlap, treating partially overlapping multi-label pairs as semantically similar rather than binary positives/negatives. The model uses frozen VGG19 and Doc2Vec backbones with modality-specific FC layers, trained with a staged loss schedule that includes a warm-up phase for stable feature alignment.

## Key Results
- SCBCH achieves up to 2.3% MAP improvement over 12 state-of-the-art methods under 80% noise rates
- Consistent performance gains across all four benchmark datasets (NUS-WIDE, MS-COCO, MIRFlickr-25K, IAPR TC-12)
- Superior robustness to label noise compared to noise-robust baselines like NRCH and RSHNL
- Maintains effectiveness across multiple hash code lengths (16, 32, 64, 128 bits)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Neighbor-Based Sample Reliability Estimation
CSCC constructs a soft label $p_i$ for each anchor by aggregating the labels of its most similar cross-modal neighbors, weighted by cosine similarity. A confidence weight $w_i$ is computed as the agreement between the anchor's label and this soft label. The weighted classification loss $\mathcal{L}_{cscc}$ reduces contribution from low-confidence (likely noisy) samples while retaining all data. This works because cross-modal neighbors in feature space share semantic consistency, and clean labels exhibit higher neighbor agreement than noisy ones.

### Mechanism 2: Soft Contrastive Pairing via Label Overlap
BSCH computes label similarity $R_{ij}$ via Jaccard Index, categorizing pairs as positive (identical labels), negative (no shared labels), or soft (partial overlap). The attraction loss $\mathcal{L}_{att}$ pulls positive/soft pairs closer, while the repulsion loss $\mathcal{L}_{rep}$ pushes dissimilar pairs apart, with adaptive weighting based on $R_{ij}$. This captures fine-grained semantic relationships that binary positive/negative treatments discard, improving robustness in multi-label scenarios.

### Mechanism 3: Warm-Up Phase for Stable Feature Alignment
For $t \leq E_w$, the model uses unweighted classification loss ($\mathcal{L}_c$ with all $w_i=1$). After warm-up ($t > E_w$), it switches to weighted $\mathcal{L}_{cscc}$. This allows features to achieve basic cross-modal alignment before neighbor-based reliability estimates are trusted, preventing early training instability from propagating incorrect weight assignments.

## Foundational Learning

- **Cross-Modal Hashing Fundamentals**: SCBCH builds on standard CMH formulations with hash functions $f^m$, binary codes $b^m_i \in \{-1,+1\}^L$, and modality-specific classifiers $g^m$. Quick check: Can you explain why Hamming distance enables efficient retrieval compared to Euclidean distance in continuous space?

- **Contrastive Learning (InfoNCE-style)**: BSCH uses contrastive objectives with attraction/repulsion components. Quick check: What is the effect of temperature/sharpness parameter $\xi$ in contrastive loss on gradient magnitude?

- **Noisy Label Learning Basics**: The paper assumes familiarity with label noise types (symmetric, asymmetric), sample selection vs. loss correction strategies. Quick check: Why does standard cross-entropy loss fail under label noise, and how does loss weighting help?

## Architecture Onboarding

- **Component map**: VGG19 features -> FC layers (3) -> Encoders -> CSCC neighbor aggregation -> BSCH soft pair construction -> Loss computation -> Backprop. Doc2Vec features -> FC layers (2) -> Encoders -> CSCC neighbor aggregation -> BSCH soft pair construction -> Loss computation -> Backprop.

- **Critical path**: 1. Extract features → 2. Forward through encoders → 3. Compute cross-modal similarities → 4. Identify neighbors → 5. Aggregate neighbor labels → 6. Compute weights → 7. Weighted classification loss (CSCC) + 8. Soft contrastive loss (BSCH) → 9. Backprop

- **Design tradeoffs**: Frozen backbones reduce overfitting to noise but limit feature adaptation; soft vs. hard pairs retain informative ambiguity but add computational complexity; weighting vs. filtering retains all samples (no data loss) but may amplify noise if weights are miscalibrated.

- **Failure signatures**: Weight distribution remains uniform after warm-up → neighbor reliability estimation failing; MAP degrades sharply at high noise (>80%) → soft pairs corrupted by label noise; large gap between I2T and T2I performance → modality imbalance in encoder capacity.

- **First 3 experiments**: 1. Ablation of CSCC vs. BSCH: Run SCBCH-1 (no CSCC) and SCBCH-2 (no BSCH) on COCO at 50% noise, 64-bit. Expect BSCH removal to cause larger drop (Table 3 shows SCBCH-2 at 37.0 vs. SCBCH-1 at 68.2). 2. Weight distribution visualization: Plot clean vs. noisy sample weights at epochs 5, 20, 50. Confirm bimodal separation (Figure 6). 3. Noise rate sweep: Test MAP at 20%, 50%, 80% noise on all four datasets with 64-bit codes. Verify consistent improvement over NRCH and RSHNL (Tables 1-2).

## Open Questions the Paper Calls Out

### Open Question 1
Can the semantic-consistent reweighting mechanism maintain robustness when label noise is instance-dependent rather than symmetric? The experiments exclusively use mixed symmetric label noise (random flips), whereas real-world noise is often class-conditional or instance-dependent (correlated with visual content). Instance-dependent noise may systematically bias the cross-modal neighbor features used in CSCC, causing the model to confidently up-weight misleading samples rather than filtering them.

### Open Question 2
Does SCBCH retain its noise-resilience when trained end-to-end without freezing the feature extraction backbones? The implementation details state that all backbone networks are kept frozen during training to ensure fair comparison, leaving the interaction between feature learning and noise robustness unexplored. Fine-tuning backbones on noisy data often leads to "memorization" of noise, which could distort the feature space and invalidate the neighbor-reliability assumptions used for reweighting samples.

### Open Question 3
How does the computational overhead of the Bidirectional Soft Contrastive Hashing (BSCH) module scale to datasets significantly larger than the benchmarks used? The BSCH module computes pairwise similarities and constructs soft pairs based on label overlap for contrastive learning, a process which can become a bottleneck relative to standard hard-pair mining. The paper validates the method on medium-sized benchmarks, but the latency of dynamically generating soft pairs for millions of items remains unquantified.

## Limitations

- Hyperparameter sensitivity: Performance gains depend on carefully tuned γ, α, and warm-up scheduling, but these values are not derived from theoretical bounds and may not generalize across datasets.
- Feature backbone assumptions: Using frozen VGG19 and Doc2Vec limits adaptation to dataset-specific semantics, potentially capping the maximum achievable accuracy even in noise-free settings.
- Scalability concerns: The O(n²) complexity of computing all-pair soft label similarities may hinder application to large-scale retrieval tasks.

## Confidence

- High confidence: The design of CSCC and BSCH modules is internally consistent and logically grounded in contrastive learning principles.
- Medium confidence: The reported experimental superiority over 12 SOTAs is plausible given the strong noise-robustness claims, but replication on unseen datasets is needed.
- Low confidence: The exact mechanism by which neighbor-based reliability estimation outperforms alternative noise-robust methods is not rigorously validated through ablation studies in the paper.

## Next Checks

1. Run SCBCH-1 (CSCC disabled) and SCBCH-2 (BSCH disabled) on COCO at 50% noise, 64-bit codes to quantify each module's contribution.
2. Conduct a noise rate sweep at 20%, 50%, 80% on all four datasets to verify consistent MAP improvement trends.
3. Visualize sample weight distributions across epochs (clean vs. noisy) to confirm that CSCC effectively separates reliable from unreliable samples.