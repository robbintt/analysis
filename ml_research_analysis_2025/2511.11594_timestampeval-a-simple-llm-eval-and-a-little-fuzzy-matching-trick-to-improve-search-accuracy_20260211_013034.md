---
ver: rpa2
title: 'TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve
  Search Accuracy'
arxiv_id: '2511.11594'
source_url: https://arxiv.org/abs/2511.11594
tags:
- text
- transcript
- accuracy
- first
- fuzzy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeStampEval, a benchmark for retrieving
  precise millisecond timestamps from long transcripts given potentially non-verbatim
  quotes. The core method combines RapidFuzz pre-filtering to narrow candidate snippets
  with LLM verification on short segments, improving fuzzy matching accuracy by up
  to 50 percentage points while halving latency and reducing cost per correct by up
  to 96%.
---

# TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy

## Quick Facts
- arXiv ID: 2511.11594
- Source URL: https://arxiv.org/abs/2511.11594
- Authors: James McCammon
- Reference count: 5
- Primary result: Fuzzy matching accuracy improves by up to 50 percentage points while halving latency and reducing cost per correct by up to 96%

## Executive Summary
This paper introduces TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given potentially non-verbatim quotes. The core method combines RapidFuzz pre-filtering to narrow candidate snippets with LLM verification on short segments, improving fuzzy matching accuracy by up to 50 percentage points while halving latency and reducing cost per correct by up to 96%. The evaluation across six modern LLMs on Congressional Record transcripts shows that simple prompt engineering (placing query before transcript, using compact text-first format) dominates model choice, and that off-by-one errors form a distinct failure mode that can be largely eliminated through format improvements.

## Method Summary
The method uses a two-stage "Assisted Fuzzy" approach: RapidFuzz pre-filtering on sentence-level ratio (70% threshold) plus partial alignment on joined string to extract candidate snippets with dynamic radius padding (3-8 sentences), followed by LLM verification on merged snippets (~20 sentences). The prompt design uses query-before-transcript placement and Text First Top format (sentence text before timestamps), with reasoning enabled for some models. This hybrid approach reduces haystack size by ~99% before LLM verification, enabling smaller models to achieve near-ceiling accuracy on focused snippets while dramatically cutting costs.

## Key Results
- Placing query before transcript improves accuracy by 3-20 percentage points across all models
- Text First Top format converts 63% of off-by-one errors to exact matches and reduces tokens 30-42%
- RapidFuzz pre-filtering + LLM verification improves fuzzy accuracy up to +50pp while cutting cost-per-correct by 87-96%
- Gemini 2.5 Flash-Lite hovered around 40% accuracy on full transcripts but reached 90% under snippet conditions
- GPT-5 family shows 0% false positive rate on absent targets, unlike Gemini's 5-22%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Placing the query before the transcript improves timestamp retrieval accuracy by 3–20 percentage points.
- **Mechanism:** Decoder-only models attend only to prior tokens. Query-first allows the model to maintain retrieval intent while scanning the transcript, rather than processing the entire haystack before knowing what to find.
- **Core assumption:** Models benefit from task framing upfront; attention degrades across long contexts without clear guidance.
- **Evidence anchors:**
  - [abstract] "simple prompt engineering (placing query before transcript, using compact text-first format) dominates model choice"
  - [section 8.5.2] "when the query appears after the transcript... reasoning token medians are consistently higher across all lengths yet remain flat... leading to a fixed traversal cost"
- **Break condition:** If context length drops below ~10k tokens, placement effects may diminish; if using encoder-decoder architectures with bidirectional attention, the mechanism may not apply.

### Mechanism 2
- **Claim:** Converting JSON transcripts to Text First Top format (sentence before timestamps) reduces tokens 30–42% and converts off-by-one errors to exact matches.
- **Mechanism:** Placing semantic content (text) before structural metadata (timestamps) lets models locate matches before computing boundaries, reducing misalignment. Fewer tokens also reduce attention load.
- **Core assumption:** Models prioritize early tokens in each unit; semantic priming improves boundary detection.
- **Evidence anchors:**
  - [abstract] "using compact text-first format... improving fuzzy matching accuracy by up to 50 percentage points"
  - [section 6.7] "63% of the improvement from Text End Bottom to Text First Top comes from converting off-by-one predictions into exact matches"
- **Break condition:** If timestamps must be processed before text (e.g., temporal filtering first), format benefits may reverse; highly structured numeric tasks may prefer JSON.

### Mechanism 3
- **Claim:** RapidFuzz pre-filtering + LLM verification on snippets improves fuzzy accuracy up to +50pp while cutting cost-per-correct by 87–96%.
- **Mechanism:** RapidFuzz reduces haystack size by ~99%, eliminating long-context retrieval burden. LLM then performs precise boundary detection on 10–20 sentence windows where even small models achieve near-ceiling accuracy.
- **Core assumption:** Fuzzy lexical matching captures the correct region; LLM accuracy scales inversely with context length.
- **Evidence anchors:**
  - [abstract] "combines RapidFuzz pre-filtering... improving fuzzy matching accuracy by up to 50 percentage points while halving latency"
  - [section 9.4] "accuracy improves sharply when snippet windows are substituted for full transcripts... Gemini 2.5 Flash-Lite... hovered around 40% accuracy, but reached 90% under the snippet condition"
- **Break condition:** If RapidFuzz fails to identify correct region (very noisy text, heavy paraphrasing), the LLM receives wrong context and fails.

## Foundational Learning

- **Levenshtein distance and fuzzy ratio:**
  - Why needed here: RapidFuzz uses partial ratio scoring to find candidate matches; understanding similarity thresholds (the paper uses 70%) is essential for tuning.
  - Quick check question: Given "U.S." vs "United States," would normalized Levenshtein similarity be above or below 70%?

- **Lost-in-the-middle phenomenon:**
  - Why needed here: Explains why query placement and transcript position affect accuracy; models struggle with information buried mid-prompt.
  - Quick check question: In a 100k-token transcript, would you expect better retrieval for a target in the first 10% or middle 50%?

- **Off-by-one as distinct error class:**
  - Why needed here: The paper shows off-by-one is not random noise but a transitional regime between miss and exact; treating it as recoverable improves adjusted accuracy significantly.
  - Quick check question: If a model returns timestamps shifted by exactly one sentence, is this likely a task understanding failure or a boundary precision failure?

## Architecture Onboarding

- **Component map:** Text cleaning -> RapidFuzz dual matching -> Dynamic snippet expansion -> Snippet merging -> LLM verification
- **Critical path:** RapidFuzz candidate selection → snippet construction → LLM boundary extraction. Failure at step 2 cascades; step 5 is where timestamp precision is determined.
- **Design tradeoffs:**
  - Larger radius = more context for LLM but higher token cost
  - Stricter fuzzy threshold = fewer false positives but more missed retrievals
  - Query-after format = better prefix caching on some providers but 3–20pp accuracy penalty
- **Failure signatures:**
  - False positives: model returns non-zero timestamps for absent quotes (Gemini showed 5–22% on long spans; GPT-5 family at 0%)
  - Off-by-one clusters: signals query-after or JSON format; switch to TFT + query-before
  - Last-third collapse: small models failing in transcript tail; upgrade model or use snippet-based approach
- **First 3 experiments:**
  1. **Baseline calibration:** Run exact-match retrieval on your transcript using TFT + query-before across 3 models; measure accuracy, latency, off-by-one rate.
  2. **Fuzzy threshold sweep:** Test RapidFuzz thresholds (60%, 70%, 80%) on synthetic perturbed quotes; plot recall vs. false positive rate.
  3. **Snippet radius tuning:** Vary context padding (3, 5, 8 sentences) and measure LLM accuracy on snippets; find minimum viable context.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How would the assisted fuzzy pipeline perform on naturally occurring (non-synthetic) fuzzy matches from real paired corpora of official transcripts vs. speech-to-text output?
- **Basis in paper:** [explicit] Conclusion states: "Robust fuzzing grounded in real paired corpora (official text vs. spoken transcripts) would pressure-test beyond synthetic edits."
- **Why unresolved:** The study used synthetically generated perturbations with ~91% Levenshtein similarity; real-world STT drift may be harsher or exhibit different patterns.
- **What evidence would resolve it:** Evaluation on ground-truth paired datasets containing authentic editorial and transcription drift between source texts and spoken transcripts.

### Open Question 2
- **Question:** Can uncertainty signals from the fuzzy matching stage and LLM verification be combined to enable principled escalation strategies?
- **Basis in paper:** [explicit] Conclusion states: "Uncertainty signals—confidence from both the fuzzy stage and the LLM—would enable principled escalation and batch-time budgeting."
- **Why unresolved:** The current pipeline does not expose or calibrate confidence scores; escalation is not systematically addressed.
- **What evidence would resolve it:** Experiments extracting RapidFuzz similarity scores and LLM logprobs, then testing whether thresholding these signals improves cost-accuracy tradeoffs when routing to larger models.

### Open Question 3
- **Question:** What mechanism explains the inverted-U relationship between span length and retrieval accuracy?
- **Basis in paper:** [inferred] Section 8.2 notes accuracy is highest for 2–5 sentences but dips for single-sentence and 10-sentence spans, stating "the precise mechanism remains unclear."
- **Why unresolved:** Single sentences may be less distinctive, while long spans require maintaining coherence—but the contribution of each factor is untested.
- **What evidence would resolve it:** Controlled ablations varying passage distinctiveness and required boundary precision independently across span lengths.

### Open Question 4
- **Question:** How transferable are these findings to other speech-to-text engines, languages, and domain-specific transcripts heavy in numerics or acronyms?
- **Basis in paper:** [explicit] Conclusion states: "results may shift with other STT engines, languages, or domains heavy in numerics and acronyms."
- **Why unresolved:** All experiments used AssemblyAI and English Congressional Record transcripts, limiting generalization.
- **What evidence would resolve it:** Cross-domain and cross-provider evaluation replicating the TFT + assisted fuzzy setup on transcripts from different STT systems and specialized corpora (legal, medical, technical).

## Limitations

- The core findings rely heavily on synthetic perturbation testing rather than real-world noisy transcripts, creating an external validity gap.
- The study uses a single Congressional Record transcript; results may not transfer to other domains or transcript styles.
- The RapidFuzz + LLM pipeline assumes the correct quote region can be identified via fuzzy matching—if this fails due to heavy paraphrasing or noise, the entire approach breaks.
- Format effects (Text First Top) show strong performance on the AssemblyAI JSON schema but may not generalize to different timestamp structures or multi-speaker formats.

## Confidence

- **High Confidence:** Query-before-transcript placement improves accuracy (3-20pp) across all tested models; RapidFuzz pre-filtering dramatically reduces cost while maintaining accuracy; off-by-one errors are a distinct recoverable failure mode.
- **Medium Confidence:** Text First Top format converts 63% of off-by-one errors to exact matches; RapidFuzz partial_ratio_alignment outperforms sentence_ratio for joined strings; smaller models show last-third degradation.
- **Low Confidence:** The specific RapidFuzz threshold of 70% is optimal; the synthetic perturbation methodology accurately reflects real-world quote-matching difficulty; results generalize beyond Congressional Record transcripts.

## Next Checks

1. **Real-world noise validation:** Apply the pipeline to actual ASR transcripts with known timestamp misalignments and measure accuracy degradation compared to synthetic 91% similarity quotes.

2. **Multi-domain format testing:** Test Text First Top format performance on transcripts from different domains (medical, legal, conversational) with varying timestamp structures to assess format generality.

3. **Error propagation analysis:** Systematically evaluate how RapidFuzz false negatives/positives cascade through the LLM verification stage by creating controlled perturbation sets where RapidFuzz succeeds/fails and measuring downstream impact.