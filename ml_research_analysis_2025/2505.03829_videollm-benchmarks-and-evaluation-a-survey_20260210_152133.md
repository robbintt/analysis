---
ver: rpa2
title: 'VideoLLM Benchmarks and Evaluation: A Survey'
arxiv_id: '2505.03829'
source_url: https://arxiv.org/abs/2505.03829
tags:
- video
- understanding
- evaluation
- benchmarks
- videollms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively analyzes benchmarks and evaluation
  methodologies for Video Large Language Models (VideoLLMs). It categorizes existing
  benchmarks based on their characteristics including video duration, content diversity,
  and evaluation protocols, while identifying performance trends across different
  model architectures.
---

# VideoLLM Benchmarks and Evaluation: A Survey

## Quick Facts
- arXiv ID: 2505.03829
- Source URL: https://arxiv.org/abs/2505.03829
- Authors: Yogesh Kumar
- Reference count: 40
- Primary result: Comprehensive survey of VideoLLM benchmarks and evaluation methodologies

## Executive Summary
This survey comprehensively analyzes benchmarks and evaluation methodologies for Video Large Language Models (VideoLLMs). It categorizes existing benchmarks based on their characteristics including video duration, content diversity, and evaluation protocols, while identifying performance trends across different model architectures. The paper examines closed-set evaluations with predefined answers, open-set evaluations allowing free-form responses, and specialized methods for temporal and spatiotemporal understanding tasks. Key findings include progressive improvement in newer models, performance gaps between specialized VideoLLMs and general multimodal models like GPT4-V, and varying strengths across evaluation dimensions.

## Method Summary
This survey paper reviews existing VideoLLM benchmarks and evaluation methodologies through systematic literature analysis. The methodology involves classifying benchmarks by characteristics such as video duration (ranging from seconds to over 4,000 seconds), content diversity (daily life, human actions, movies, etc.), and evaluation protocols (closed-set accuracy metrics, open-set LLM-as-judge scoring, specialized spatiotemporal metrics). For open-set evaluation, the paper references Video-ChatGPT methodology using GPT-3.5/4 as evaluators scoring responses on correctness, detail, context, temporal understanding, and consistency. The survey synthesizes performance trends across 25+ VideoLLMs by compiling reported results from various studies, though direct comparisons are limited by differing evaluation protocols.

## Key Results
- Progressive improvement in newer VideoLLM models across benchmarks, with ST-LLM achieving top scores among specialized models
- Persistent performance gaps between specialized VideoLLMs and general multimodal models like GPT4-V, particularly in temporal understanding
- Systematic weaknesses in fine-grained understanding, long-form video comprehension, multimodal integration, hallucination mitigation, and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-set evaluation using LLM-as-judge enables assessment of generative capabilities that closed-set metrics cannot capture.
- Mechanism: Free-form model responses are scored by a stronger LLM (e.g., GPT-3.5/4) across semantic dimensions—correctness, detail, context, temporal understanding, consistency—rather than matching against predefined options.
- Core assumption: The judge LLM's scoring correlates with human judgment of response quality across these dimensions.
- Evidence anchors:
  - [abstract] "analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks"
  - [section III.B] "open-set evaluations do not rely on predefined options for responses... often using models like GPT-3.5 or GPT-4 to compare predictions and assign scores based on semantic similarity and correctness"
  - [corpus] Corpus neighbors focus on architectural improvements (IPFormer-VideoLLM, SharpV) but do not directly validate LLM-as-judge reliability; this remains an open methodological question.
- Break condition: If judge LLM exhibits systematic bias toward certain response styles, or if inter-judge consistency is low, scores may not reflect true understanding quality.

### Mechanism 2
- Claim: Architectural design creates predictable performance tradeoffs across evaluation dimensions.
- Mechanism: Video Analyzer × LLM architectures (converting video to text first) favor factual accuracy and context but may sacrifice fine-grained temporal reasoning. Video Embedder × LLM architectures (direct visual embeddings) better capture temporal dynamics but may lose contextual depth. Hybrid approaches aim to balance both.
- Core assumption: The modality conversion pathway determines which information is preserved versus lost during processing.
- Evidence anchors:
  - [section IV.B] "VideoLLMs can be broadly classified into three types based on their architecture: Video Analyzer × LLM, Video Embedder × LLM, and (Analyzer + Embedder) × LLM. Each approach offers distinct advantages and limitations that manifest in evaluation results."
  - [section IV.B] "Video Analyzer × LLM models... often excel in factual accuracy and contextual understanding but may struggle with fine-grained temporal reasoning."
  - [corpus] "Unifying Specialized Visual Encoders for Video Language Models" notes that VideoLLMs "currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information" — supporting the architectural bottleneck hypothesis.
- Break condition: If newer fusion mechanisms preserve information across pathways more effectively, these tradeoffs may diminish or reconfigure.

### Mechanism 3
- Claim: Temporal understanding is the weakest dimension across VideoLLMs, revealing a systematic capability gap.
- Mechanism: Processing video as frame sequences without explicit temporal modeling leads to lower scores on temporal reasoning tasks, even when static understanding is strong. The gap persists from early models (Video-LLaMA: 1.82) to recent ones (ST-LLM: 2.93) compared to GPT4-V (3.94).
- Core assumption: Temporal understanding requires architectural mechanisms that explicitly model event sequences, causality, and state transitions—not just frame-level feature extraction.
- Evidence anchors:
  - [table II] All 25 listed VideoLLMs score lower on "Temporal" than on "Correctness," "Detail," or "Context" dimensions. The highest VideoLLM temporal score (ST-LLM: 2.93) remains 25% below GPT4-V (3.94).
  - [section IV.A] "models demonstrate varying strengths across different evaluation dimensions. Some models excel in factual correctness but struggle with temporal understanding."
  - [corpus] "SEASON: Mitigating Temporal Hallucination in VideoLLMs" and "VERHallu: Evaluating and Mitigating Event Relation Hallucination" both identify temporal reasoning as a persistent failure mode requiring dedicated intervention.
- Break condition: If temporal scores approach parity with other dimensions without architectural changes, the mechanism may be driven by benchmark design rather than model capability.

## Foundational Learning

- Concept: **Vision-Language Alignment**
  - Why needed here: VideoLLMs must map visual features to linguistic representations; misalignment causes hallucinations and weak temporal reasoning.
  - Quick check question: Can you explain why a model might correctly identify objects in frames but fail to describe their causal sequence?

- Concept: **Temporal Token Representation**
  - Why needed here: Video introduces a time dimension absent from image-language models; understanding how frames are tokenized and temporally encoded is critical for interpreting benchmark results.
  - Quick check question: What is the difference between treating video as independent frame embeddings versus incorporating explicit temporal positional encodings?

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: Open-set evaluation relies on this paradigm; understanding its limitations prevents over-interpreting scores.
  - Quick check question: What are two potential failure modes when using GPT-4 to score another model's free-form responses?

## Architecture Onboarding

- Component map: Video frames -> Encoder -> Projection -> LLM -> Response
- Critical path: Video frames → Encoder → Projection → LLM → Response. Temporal understanding depends on information preserved through this pipeline.
- Design tradeoffs:
  - Frame sampling density vs. computational cost: More frames capture finer temporal dynamics but increase sequence length
  - Analyzer vs. Embedder architecture: Text-intermediary preserves linguistic reasoning but may lose visual detail; direct embeddings preserve visual information but may lack semantic structure
  - Unified vs. specialized encoders: Single encoder simplifies training but may not capture diverse visual features; multiple encoders increase complexity
- Failure signatures:
  - Temporal hallucination: Model describes events out of order or invents temporal relations not present in video
  - Object identity drift: Same object described inconsistently across frames in multi-shot scenes
  - Context collapse: Long videos processed with degraded attention to earlier content
  - Modality dominance: Model ignores visual input and generates responses based on linguistic priors
- First 3 experiments:
  1. Baseline establishment: Run a standard VideoLLM (e.g., Video-LLaVA) on MSVD-QA, MSRVTT-QA, and ActivityNet-QA with both closed-set accuracy and open-set GPT-scoring to establish a performance profile across video lengths.
  2. Temporal probing: Test on TempCompass or a controlled dataset with temporally-reversed videos to isolate whether the model relies on frame-level features versus true temporal understanding.
  3. Hallucination audit: Manually inspect model outputs on a held-out subset for factual consistency with ground-truth video content; quantify hallucination rate and correlate with video duration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation benchmarks effectively assess a VideoLLM's ability to express uncertainty or acknowledge knowledge limitations rather than generating hallucinated content?
- Basis in paper: [explicit] Section V.E explicitly calls for "evaluation methodologies that assess a model's ability to express uncertainty or acknowledge knowledge limitations rather than generating incorrect information" to reduce harmful hallucinations.
- Why unresolved: Current evaluation frameworks focus predominantly on factual accuracy against ground truth, lacking metrics to reward a model for "knowing what it doesn't know" or refusing to answer ambiguous queries.
- What evidence would resolve it: The development and validation of a "hallucination-focused benchmark" that quantifies the trade-off between answer coverage and uncertainty calibration for ambiguous video inputs.

### Open Question 2
- Question: How does systematically controlled multimodal complexity (e.g., introducing modality conflicts or missing data) impact the reasoning performance of VideoLLMs?
- Basis in paper: [explicit] Section V.D identifies a research gap regarding "benchmarks with controlled multimodal complexity," specifically suggesting the need to evaluate how models handle "variations of the same content with different combinations of modalities" and resolve conflicts between them.
- Why unresolved: Existing benchmarks generally assume modalities are complementary and aligned, failing to test a model's robustness when audio contradicts visual content or when specific modalities are absent.
- What evidence would resolve it: A dataset containing paired samples with manipulated modality integrity (e.g., video-only vs. video+audio) and performance metrics showing the degradation curves or conflict resolution success rates of state-of-the-art models.

### Open Question 3
- Question: To what extent do Large Language Model (LLM)-based evaluators introduce systematic bias or inconsistency when scoring open-ended video responses?
- Basis in paper: [inferred] Section III.B discusses the limitations of open-set evaluation, noting that "Different evaluator models may produce different scores for the same response, raising questions about consistency and reliability."
- Why unresolved: While LLMs like GPT-4 are widely used for evaluation, the field lacks a standardized understanding of how evaluator bias affects the reproducibility of VideoLLM benchmarking.
- What evidence would resolve it: A systematic study comparing LLM-based evaluation scores with human judgments across diverse video domains to establish correlation coefficients and identify specific bias patterns in generative performance dimensions.

## Limitations

- The paper's classification of benchmarks and evaluation methods is based on literature review rather than systematic empirical validation
- Performance comparisons between models rely heavily on reported results that may use different evaluation protocols, making direct comparisons challenging
- The survey focuses primarily on English-language benchmarks and may not fully represent global video understanding challenges

## Confidence

- High confidence: Classification of benchmark characteristics (duration, content diversity, evaluation protocols)
- Medium confidence: Performance trends across model architectures
- Medium confidence: Identification of evaluation challenges (hallucination, temporal understanding)
- Low confidence: Proposed future research directions

## Next Checks

1. **LLM-as-Judge Reliability Validation**: Conduct inter-annotator agreement studies between human evaluators and LLM judges across multiple VideoLLM outputs to quantify correlation and identify systematic biases in open-set evaluation scoring

2. **Cross-Benchmark Generalization Study**: Evaluate the same VideoLLM across multiple benchmarks with varying characteristics (duration, content diversity, open vs closed-set) to identify whether performance patterns are consistent or benchmark-specific

3. **Temporal Understanding Probing**: Design controlled experiments with temporally manipulated videos (reversed sequences, shuffled frames) to isolate whether VideoLLM temporal reasoning is based on learned temporal patterns versus explicit temporal modeling