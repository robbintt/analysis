---
ver: rpa2
title: 'Localized Definitions and Distributed Reasoning: A Proof-of-Concept Mechanistic
  Interpretability Study via Activation Patching'
arxiv_id: '2504.02976'
source_url: https://arxiv.org/abs/2504.02976
tags:
- patching
- activation
- knowledge
- answer
- corrupted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates knowledge representation localization in
  fine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching (CLAP).
  The model was fine-tuned on 9,958 PubMed abstracts containing epilepsy-related terms,
  using two configurations with validation monitoring.
---

# Localized Definitions and Distributed Reasoning: A Proof-of-Concept Mechanistic Interpretability Study via Activation Patching

## Quick Facts
- arXiv ID: 2504.02976
- Source URL: https://arxiv.org/abs/2504.02976
- Reference count: 1
- Demonstrates layer-specific knowledge localization through activation patching with 56-100% recovery rates

## Executive Summary
This study investigates knowledge representation localization in fine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching (CLAP). The model was fine-tuned on 9,958 PubMed abstracts containing epilepsy-related terms, using two configurations with validation monitoring. CLAP involved caching clean and corrupted activations, computing logit differences, and patching corrupted activations to assess recovery. Results showed that patching the first feedforward layer recovered 56% of correct preference for associative knowledge, while patching the final output layer achieved 100% recovery for definitional knowledge. Convolutional layer patching showed minimal recovery (13.6%), suggesting low-level features contribute marginally to high-level reasoning. Statistical analysis confirmed significant layer-specific effects (p<0.01).

## Method Summary
The study employed Causal Layer Attribution via Activation Patching (CLAP) to assess knowledge localization in a GPT-2 model fine-tuned on epilepsy-related PubMed abstracts. The methodology involved caching activations from both clean (intact knowledge) and corrupted (ablated knowledge) model states, computing logit differences between these states, and systematically patching corrupted activations from individual layers back into the clean model. Recovery rates were measured by comparing preference for correct versus corrupted answers across different patching strategies. The fine-tuning process used two configurations with validation monitoring to ensure robust knowledge acquisition.

## Key Results
- First feedforward layer patching recovered 56% of correct preference for associative knowledge
- Final output layer patching achieved 100% recovery for definitional knowledge
- Convolutional layer patching showed minimal recovery (13.6%), indicating low-level features contribute marginally to high-level reasoning

## Why This Works (Mechanism)
The CLAP methodology works by exploiting the linear-additive properties of transformer layer contributions. When a corrupted activation is patched into a clean model, the change in output logits reveals that layer's contribution to the knowledge being tested. For factual knowledge requiring precise definitional recall, the final layers encode the consolidated representation necessary for exact answers, explaining the 100% recovery when patching the output layer. Associative knowledge, being more distributed across representations, shows significant contribution from earlier layers (56% recovery from the first feedforward layer), indicating that multiple layers participate in encoding these relationships. The minimal recovery from convolutional layers (13.6%) demonstrates that low-level feature extraction contributes little to the high-level reasoning required for both knowledge types.

## Foundational Learning
- **Transformer architecture fundamentals**: Understanding self-attention mechanisms and layer stacking is essential for interpreting how knowledge propagates through the model. Quick check: Verify understanding of residual connections and layer normalization.
- **Causal inference in ML**: The ability to attribute effects to specific components requires understanding of intervention-based analysis. Quick check: Confirm ability to distinguish between correlation and causation in model behavior.
- **Activation space representation**: Knowledge is encoded in high-dimensional activation vectors, requiring understanding of geometric interpretations. Quick check: Visualize activation differences between clean and corrupted states.
- **Statistical significance testing**: Proper interpretation of p-values and effect sizes is crucial for validating layer-specific effects. Quick check: Calculate confidence intervals for recovery rate differences.

## Architecture Onboarding
- **Component map**: Input embeddings -> Multi-head attention layers (12) -> Feedforward layers (12) -> Output projection -> Logits
- **Critical path**: Token embedding → self-attention → feedforward → residual connection → layer norm → next layer → final projection → logits
- **Design tradeoffs**: Deep layer stacking enables hierarchical feature learning but complicates interpretability; residual connections preserve information flow but obscure individual layer contributions
- **Failure signatures**: If patching shows uniform recovery across all layers, this suggests either the corruption method failed or the knowledge is truly distributed without localization
- **Three first experiments**: 1) Test knowledge localization across different fine-tuning domains, 2) Apply CLAP to attention heads within layers to identify specific contributors, 3) Compare localization patterns between GPT-2 and other transformer architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Single-domain training (epilepsy PubMed abstracts) limits generalizability to other knowledge types
- CLAP methodology assumes linear additivity of layer contributions, potentially missing non-linear interactions
- Only examines GPT-2 architecture, leaving uncertainty about findings' applicability to other transformer variants

## Confidence
- **High confidence**: Layer-specific recovery differences (p<0.01), relative performance between factual (100% in final layer) versus associative knowledge (56% in first layer) recovery
- **Medium confidence**: Claims about distributed versus localized representation mechanisms, given single-domain training and architecture constraints
- **Low confidence**: Generalization to other knowledge types, model architectures, or fine-tuning paradigms beyond epilepsy-specific PubMed data

## Next Checks
1. Replicate the CLAP experiments across multiple knowledge domains (e.g., general encyclopedic knowledge, mathematical reasoning, commonsense) to test domain-transferability of localization patterns
2. Apply identical CLAP methodology to different transformer architectures (e.g., LLaMA, Mistral) and model sizes to verify architectural generality of findings
3. Conduct ablation studies removing individual attention heads within layers to determine whether localization patterns arise from specific components rather than entire layers