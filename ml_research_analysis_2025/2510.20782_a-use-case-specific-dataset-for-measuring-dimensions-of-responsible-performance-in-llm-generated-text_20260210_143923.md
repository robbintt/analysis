---
ver: rpa2
title: A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance
  in LLM-generated Text
arxiv_id: '2510.20782'
source_url: https://arxiv.org/abs/2510.20782
tags:
- product
- dataset
- data
- fairness
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dataset for evaluating Responsible AI dimensions
  (fairness, safety, veracity, quality) in large language models specifically for
  generating product descriptions from feature lists. The dataset is constructed using
  query templates combining demographic identity groups, product adjectives, and categories,
  then retrieving products from an e-commerce search engine.
---

# A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text

## Quick Facts
- arXiv ID: 2510.20782
- Source URL: https://arxiv.org/abs/2510.20782
- Reference count: 30
- Primary result: Dataset of 7,047 product description generations reveals 21-fold toxicity disparity between categories and identity group differences in sexually explicit content

## Executive Summary
This paper introduces a dataset for evaluating Responsible AI dimensions—fairness, safety, veracity, and quality—in large language models specifically for generating product descriptions from feature lists. The dataset is constructed using query templates combining demographic identity groups, product adjectives, and categories, then retrieving products from an e-commerce search engine. The resulting dataset contains 7,047 rows with product details and fairness attributes. The authors demonstrate its use by evaluating Llama 3.2 11B on quality (BertScore accuracy 0.9496), veracity (precision 0.9488, recall 0.9504), safety (mean toxicity 0.0024), and fairness. Results show significant toxicity disparities between categories (21-fold difference between Appliances and Sexual Wellness) and among identity groups, with Women showing higher sexually explicit language scores. The dataset enables application-specific RAI evaluation beyond generic benchmarks and reveals that smaller models may be sufficient for certain use cases.

## Method Summary
The dataset construction uses query templates that combine 13 identity groups, 7 product adjectives, and 16 product categories to generate 382 search queries. These queries retrieve products from an e-commerce search engine, yielding 5,145 unique products with 7,047 rows after deduplication. Zero-shot prompting generates product descriptions from feature lists using a template that includes product name, adjective, category, and features. The evaluation suite computes BertScore F1 for quality, precision/recall for veracity, detoxify classifier for safety, and cohort disparity ratios for fairness. The dataset is publicly available at https://github.com/amazon-science/application-eval-data.

## Key Results
- Quality measured by BertScore accuracy has mean 0.9496 with minimal variation across dataset
- Safety shows 21-fold toxicity disparity between least-toxic (Appliances) and most-toxic (Sexual Wellness) categories
- Identity groups reveal striking fairness differences, with Women showing significantly higher sexually explicit language scores
- Results demonstrate that smaller models may be sufficient for certain use cases, as gap to state-of-the-art is much smaller than on generic benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Application-specific evaluation datasets reveal fairness and safety disparities that generic benchmarks miss.
- Mechanism: By parameterizing queries with protected attributes (identity groups, gendered adjectives) intersected with product categories, the dataset creates controlled cohorts that surface differential model behavior. The 21-fold toxicity disparity between "Appliances" and "Sexual Wellness" categories, and elevated sexually explicit scores for products associated with "Women," demonstrate that context determines risk exposure.
- Core assumption: Demographic associations in product search queries meaningfully reflect how end-users would encounter bias in production systems.
- Evidence anchors:
  - [abstract] "parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts"
  - [Section 5] "21-fold increase in toxicity between the least-toxic category (Appliances) and the most toxic (Sexual Wellness)"
  - [corpus] LangFair (arXiv 2501.03112) similarly emphasizes use-case-specific bias assessment, supporting the application-specific evaluation paradigm.
- Break condition: If query templates do not reflect actual user behavior patterns, or if search engine ranking algorithms introduce confounding biases, cohort disparities may not generalize to real deployment.

### Mechanism 2
- Claim: Cohort disparity metrics quantify fairness more informatively than aggregate scores.
- Mechanism: Rather than reporting mean performance, the method computes the ratio of best-performing cohort to worst-performing cohort on each metric. This meta-metric reveals relative harm: even with low mean toxicity (0.0024), maximum toxicity reached 0.6458, and identity groups showed markedly different profiles (e.g., Women had mid-range overall toxicity but highest sexually explicit scores).
- Core assumption: Disparity ratios are actionable signals of unfairness, rather than artifacts of category-appropriate language (e.g., sexual wellness products legitimately require sexual terminology).
- Evidence anchors:
  - [Section 4] "We apply the meta-metric cohort disparity for both toxicity and accuracy scores. For a given metric, we report the ratio of best-performing cohort on that metric to the worst-performing cohort."
  - [Section 5] "identity groups reveal striking fairness differences... Products associated with the group Women resulted in significantly higher scores for sexually explicit language"
  - [corpus] SHARP (arXiv 2601.21235) critiques mean-centered scalar scores for obscuring distributional structure, aligning with cohort-based approaches.
- Break condition: If toxicity classifiers are poorly calibrated for domain-specific language (e.g., flagging accurate sexual wellness terminology as harmful), disparity metrics may misclassify legitimate content as fairness violations.

### Mechanism 3
- Claim: Ground truth comparison using seller-provided product descriptions enables quality and veracity assessment without human annotation.
- Mechanism: Product titles, descriptions, and feature bullets from the e-commerce catalog serve as reference texts. BertScore F1 measures semantic similarity (quality), while precision/recall components assess whether generated descriptions include accurate information without hallucination.
- Core assumption: Seller-provided descriptions are reasonable proxies for "correct" output, even though sellers may have used LLMs or have marketing incentives that introduce bias.
- Evidence anchors:
  - [Section 3] "The title, description, and feature bullets are all provided by the product seller. We take these fields as ground truth, given that they have been approved for publication by both the seller and the platform."
  - [Section 5] "Quality, measured by BertScore accuracy, has a mean of 0.9496. It varies little across the dataset. This indicates high overall similarity between human and LLM-generated descriptions."
  - [corpus] Weak direct corpus support; related work on product QA (Zhang et al., 2020) uses community-voted ground truth, an alternative validation approach.
- Break condition: If ground truth descriptions contain systematic biases (e.g., more sexualized language for women's products), quality metrics may reinforce rather than detect problematic patterns.

## Foundational Learning

- Concept: **Protected attributes in fairness evaluation**
  - Why needed here: The dataset explicitly constructs cohorts using 13 identity groups (race, ethnicity, religion, disability, gender, sexual orientation). Understanding which attributes are legally/socially protected is necessary to interpret fairness disparities correctly.
  - Quick check question: Can you name three protected attribute categories used in this dataset and explain why "product category" is not a protected attribute despite being used for cohort analysis?

- Concept: **BertScore and its components (precision, recall, F1)**
  - Why needed here: Quality is measured via BertScore F1; veracity uses precision (avoiding hallucination) and recall (completeness). Without understanding what these capture, you cannot diagnose whether low scores indicate style mismatch, factual errors, or missing information.
  - Quick check question: If a generated description achieves high recall but low precision, what type of error is the model making?

- Concept: **Toxicity classifiers and domain calibration**
  - Why needed here: The detoxify classifier flags sexual wellness terminology as "sexually explicit," creating apparent safety issues that may be false positives for this application. Understanding classifier limitations is critical before acting on disparity findings.
  - Quick check question: Why might a general toxicity classifier produce misleading fairness signals when applied to product descriptions for adult categories?

## Architecture Onboarding

- Component map:
  Query Template Generator -> Product Retrieval Layer -> Dataset Store -> LLM Inference Pipeline -> Evaluation Suite

- Critical path:
  1. Construct query templates → 2. Retrieve products via search API → 3. Clean and deduplicate → 4. Generate descriptions with target LLM → 5. Compute metrics per sample → 6. Aggregate by cohort → 7. Calculate disparity ratios

- Design tradeoffs:
  - **Ground truth source**: Using seller descriptions avoids annotation cost but inherits seller biases; LLM-as-judge could reduce dependency but introduces model-specific artifacts
  - **Cohort granularity**: 13 identity groups × 16 categories creates sparse cells; aggregating to higher-level cohorts improves statistical power but may hide intersectional effects
  - **Toxicity threshold**: Mean toxicity (0.0024) suggests most outputs are benign, but maximum (0.6458) indicates tail risks; application-dependent thresholds are required

- Failure signatures:
  - **Sparse cohort signal**: "Lingerie" category had only 46 samples; disparity estimates for small cohorts are unstable
  - **Classifier-domain mismatch**: Sexual wellness products flagged as toxic despite accurate descriptions; requires domain-specific calibration or exclusion
  - **Ground truth contamination**: Seller descriptions may already be LLM-generated, making quality comparison circular

- First 3 experiments:
  1. **Baseline replication**: Run Llama 3.2 11B on the dataset, compute all four metrics, verify reported numbers (accuracy 0.9496, mean toxicity 0.0024, max toxicity 0.6458). Discrepancies indicate environment or prompting differences.
  2. **Model size comparison**: Test Llama 1B vs 11B on the same data. The paper claims "much smaller gap" than leaderboards show; quantify the accuracy-toxicity tradeoff to inform deployment decisions.
  3. **Cohort drill-down**: For the "Women" identity group, isolate products where sexually explicit scores exceed threshold 0.1. Manually inspect 10-20 examples to determine if flagged content is contextually appropriate or genuinely problematic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the evaluation methodology be extended to multimodal product listings (e.g., images), and do visual outputs exhibit similar fairness and safety disparities as text descriptions?
- Basis in paper: [explicit] "One important extension of the work would be to cover multimodal or multi-lingual components from the online product listings, or to generate images, which can be scored using automatic quality metrics like Human Preference Scores."
- Why unresolved: The current dataset and evaluation focus exclusively on text; extending to images requires new collection, labeling, and metric frameworks.
- What evidence would resolve it: A multimodal extension of the dataset with image generation experiments showing comparable disparity analyses across cohorts.

### Open Question 2
- Question: How do LLM-based judges compare to ground-truth-based BertScore metrics for evaluating quality and veracity in product description generation?
- Basis in paper: [explicit] "Downstream consumers of the data could apply LLM-based judges, to reduce the reliance on ground truth."
- Why unresolved: The paper uses BertScore against human descriptions but acknowledges these contain imperfections and biases; no comparison to LLM judges is provided.
- What evidence would resolve it: A comparative study measuring correlation between LLM-judge scores and BertScore, plus analysis of where they diverge.

### Open Question 3
- Question: Does the search engine's retrieval algorithm introduce systematic biases in product-to-demographic associations that confound fairness measurements?
- Basis in paper: [inferred] "The association of products and identity group cohorts (represented in query templates) is implicitly determined by the search engine's ranking and blending algorithm rather than an explicit, verified label."
- Why unresolved: Products are assigned to demographic cohorts via search queries, not verified labels, meaning the search system's own biases may influence results.
- What evidence would resolve it: An ablation comparing search-retrieved associations against manually verified product-demographic labels.

## Limitations
- Ground truth bias: Seller-provided descriptions may already contain demographic biases, making quality and veracity metrics partially circular and potentially masking fairness issues
- Classifier domain mismatch: General toxicity classifiers flag sexual wellness terminology as harmful, conflating legitimate domain language with safety violations
- Sparse cohorts: 46 samples for "Lingerie" category and other low-frequency combinations limit statistical power for disparity analysis

## Confidence
- **High confidence**: The dataset construction methodology is clearly specified and reproducible; the application-specific evaluation approach is technically sound
- **Medium confidence**: Reported metrics (quality 0.9496, mean toxicity 0.0024, max toxicity 0.6458) appear internally consistent but depend on unspecified implementation details
- **Low confidence**: Interpretation of fairness disparities requires careful domain-specific calibration; apparent toxicity differences may reflect classifier limitations rather than actual harm

## Next Checks
1. Manually inspect 20-30 products from high-disparity cohorts (particularly "Women" + "Sexual Wellness") to distinguish between classifier false positives and genuine safety concerns
2. Test toxicity classifier calibration on domain-specific sexual wellness terminology to establish baseline false positive rates
3. Compare results using alternative ground truth sources (e.g., manufacturer descriptions vs seller descriptions) to quantify impact of ground truth bias