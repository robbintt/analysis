---
ver: rpa2
title: 'CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron
  Pruning for Comprehensive Acceleration of Vision-Language Models'
arxiv_id: '2505.19235'
source_url: https://arxiv.org/abs/2505.19235
tags:
- layer
- tokens
- neurons
- core
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoreMatching introduces a co-adaptive sparse inference framework\
  \ for vision-language models that leverages the synergy between token and neuron\
  \ sparsity. The key insight is that important tokens and neurons for inference are\
  \ mutually reinforcing\u2014tokens whose activated neurons most closely match core\
  \ neurons (those most frequently activated across inputs) are critical for maintaining\
  \ model performance."
---

# CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models

## Quick Facts
- **arXiv ID**: 2505.19235
- **Source URL**: https://arxiv.org/abs/2505.19235
- **Reference count**: 40
- **Primary result**: 5× FLOPs reduction and 10× overall speedup on NVIDIA Titan Xp while maintaining near-lossless accuracy using only 10-20% of original tokens

## Executive Summary
CoreMatching introduces a co-adaptive sparse inference framework for vision-language models that leverages the synergy between token and neuron sparsity. The key insight is that important tokens and neurons for inference are mutually reinforcing—tokens whose activated neurons most closely match core neurons (those most frequently activated across inputs) are critical for maintaining model performance. CoreMatching precomputes core neurons during pre-filling and identifies core tokens that activate the largest number of core neurons, achieving comprehensive acceleration. Theoretical analysis shows core tokens are proportional to a superior projection-guided criterion that accounts for both attention scores and angular information. Experiments on ten image understanding tasks and three hardware devices demonstrate CoreMatching's effectiveness: on NVIDIA Titan Xp, it achieves 5× FLOPs reduction and 10× overall speedup while maintaining near-lossless accuracy, using only 10-20% of original tokens. The method requires no training or fine-tuning and dynamically adapts to input content.

## Method Summary
CoreMatching is a two-phase inference acceleration framework for vision-language models. During pre-filling, it computes token-wise core neurons (top ρ=0.2 percentile activations per token) and aggregates to sentence-wise core neurons (top β=0.4 most frequent across tokens). At layer l=2, tokens are ranked by their intersection with core neurons |Γ(x) ∩ Cβρ(s)|, and core tokens are selected via knee threshold detection. During decoding, only core neurons and core tokens are retained in KV cache, achieving comprehensive acceleration. The framework exploits the mutual reinforcement between important tokens and neurons, showing that tokens activating the most core neurons are most critical for maintaining accuracy.

## Key Results
- Achieves 5× FLOPs reduction and 10× overall speedup on NVIDIA Titan Xp while maintaining near-lossless accuracy
- Reduces token usage to 10-20% of original (64 tokens vs 576) across ten image understanding tasks
- Demonstrates effectiveness across three hardware devices: NVIDIA Titan Xp (10× speedup), Apple M2 (1.64×), and Qualcomm Snapdragon X Elite (1.8×)
- Maintains accuracy across diverse benchmarks including VQAv2, GQA, SciQA, TextVQA, and MM-Vet

## Why This Works (Mechanism)

### Mechanism 1: Core Neuron-Core Token Matching
Core neurons (most frequently activated across all input tokens) identify core tokens (tokens that activate the most core neurons). The intersection |Γ(x) ∩ Cβρ(s)| predicts token importance better than attention scores alone, with the assumption that last token's activated neurons approximate core neurons when input semantics are stable.

### Mechanism 2: Projection-Guided Token Importance Criterion
Token influence depends on ||Proj_{OM}(αiM Vi)|| = |αiM Vi| × cos(∠(Vi, OM)), not just attention score αiM. This projection value filters important tokens more effectively by accounting for angular alignment between token contribution and final output.

### Mechanism 3: Orthogonal Matrix Preservation of Angular Information
Weight matrices in trained VLMs are approximately orthogonal (WqWk^T ≈ θI, WdWd^T ≈ λI), preserving angular relationships through FFN transformations. This means co-activated neurons directly relate to token similarity, enabling the core neuron-core token matching mechanism.

## Foundational Learning

- **Activation Sparsity in FFN Layers**: Why needed: CoreMatching exploits that individual tokens activate only ~10% of FFN neurons. Quick check: Why does ReLU/SiLU create sparse activations, and how does the paper exploit frequency of activation rather than magnitude alone?

- **Attention Score Limitations**: Why needed: The paper critiques prevailing attention-based token selection. Quick check: Given OM = Σ αiM Vi, why might a token with high αiM but Vi perpendicular to OM contribute less than a token with moderate αiM but aligned Vi?

- **Knee/Elbow Detection for Adaptive Thresholding**: Why needed: CoreMatching dynamically determines token count via maximum geometric distance method. Quick check: How does the knee method balance retaining too many tokens (inefficient) vs. too few (performance loss) without requiring per-task tuning?

## Architecture Onboarding

- **Component map**: Pre-filling Stage -> Compute token-wise core neurons -> Aggregate to sentence-wise core neurons -> At layer L=2, compute intersections -> Knee detection yields threshold T_k -> Prune non-core tokens -> Decoding Stage -> Use precomputed core neurons for FFN, use pruned KV cache from core tokens

- **Critical path**: 1) Capture FFN activations for all tokens in layers 1 through L, 2) At each layer: identify neurons with activation ≥ ρ-th percentile per token, 3) Count neuron frequency across tokens, keep top β% as core neurons, 4) At layer L: compute core neuron intersection count per token, 5) Find knee point in intersection distribution → defines T_k, 6) Retain tokens with intersection ≥ T_k as core tokens

- **Design tradeoffs**: Token pruning layer (L=2) balances speedup vs. semantic aggregation; ρ=0.2, β=0.4 defaults balance sparsity vs. accuracy; adaptive (knee) handles task variability but adds complexity vs. fixed token count

- **Failure signatures**: Sudden accuracy drop on specific tasks (core token count too low for complex tasks), cross-attention model incompatibility (LLaMA3.2 uses cross-attention), memory not decreasing (core neurons not cached properly)

- **First 3 experiments**: 1) Reproduce Table 1 (core neuron retention vs. accuracy) on LLaVA-1.5-7B, 2) Visualize core token selection (replicate Figure 8) on rabbit image, 3) Ablate sparsity dimensions (replicate Figure 9) to measure latency for token-only, neuron-only, and combined modes

## Open Questions the Paper Calls Out

### Open Question 1
What are the theoretical origins of the approximate weight matrix orthogonality observed in VLMs, and can it be analytically derived? The paper presents this as a novel observation without explaining underlying causes, relying on it for theoretical derivations.

### Open Question 2
How robust is the predictability of core neurons when input prompts are extremely short or lack semantic richness? The method assumes "input is sufficiently long and semantically rich," but performance on minimal context remains untested.

### Open Question 3
Is the fixed selection of the second layer (l=2) for token pruning optimal for maximizing the efficiency-accuracy trade-off? While consistent with FastV, the paper doesn't ablate this specific choice across all layers.

## Limitations
- **Hardware portability gap**: Speedup effectiveness highly dependent on hardware characteristics and sparse operation optimization, with significant variation across devices (10× on Titan Xp vs 1.64× on Apple M2)
- **Cross-attention architecture limitation**: Framework explicitly cannot handle models with cross-attention layers, significantly limiting applicability to current state-of-the-art VLMs
- **Task complexity calibration**: Adaptive token selection may struggle with tasks requiring extensive reasoning or long-context understanding, with token counts ranging widely (28-94) across benchmarks

## Confidence

**High Confidence**: The core matching mechanism between core neurons and core tokens is well-supported by theoretical analysis and empirical validation across multiple experiments.

**Medium Confidence**: The superiority of projection-guided token importance over attention scores alone is supported by ablation studies, but theoretical foundation assumes diagonal attention dominance.

**Medium Confidence**: The orthogonal matrix property and its implications are novel observations presented with supporting visualizations, but broader applicability remains unproven.

**Low Confidence**: The generalization of knee detection method across diverse task domains is demonstrated but not extensively validated for varying input characteristics.

## Next Checks

1. **Architecture Generalization Test**: Apply CoreMatching to VLMs with different architectural designs (attention-only, cross-attention, MoE-based) and training objectives. Measure whether orthogonal matrix property holds and if core neuron-core token matching remains effective when attention patterns deviate from diagonal dominance.

2. **Hardware Architecture Profiling**: Implement CoreMatching on GPUs with varying sparse operation optimizations (NVIDIA Ampere vs. Ada Lovelace vs. AMD CDNA) and CPU-based inference engines. Profile FFN compute time, memory bandwidth utilization, and sparse operation efficiency to understand hardware-specific performance characteristics.

3. **Task Complexity Boundary Analysis**: Systematically vary task complexity by modifying input length, semantic density, and reasoning requirements across the benchmark suite. Measure core token count distribution, accuracy degradation points, and knee detection stability to establish operational boundaries.