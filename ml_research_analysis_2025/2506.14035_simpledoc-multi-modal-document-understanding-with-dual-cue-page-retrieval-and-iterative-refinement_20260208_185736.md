---
ver: rpa2
title: 'SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval
  and Iterative Refinement'
arxiv_id: '2506.14035'
source_url: https://arxiv.org/abs/2506.14035
tags:
- pages
- page
- document
- query
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimpleDoc is a lightweight, iterative retrieval-augmented framework
  for document visual question answering (DocVQA) that handles multi-page, multi-modal
  documents (text, tables, images). It first preprocesses each page into dense visual
  embeddings and concise LLM-generated summaries.
---

# SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement

## Quick Facts
- **arXiv ID:** 2506.14035
- **Source URL:** https://arxiv.org/abs/2506.14035
- **Reference count:** 40
- **One-line result:** 70.12% accuracy on DocVQA benchmarks, outperforming prior methods by 3.2% while retrieving only 3.5 pages per question.

## Executive Summary
SimpleDoc is a lightweight, iterative retrieval-augmented framework for document visual question answering (DocVQA) that handles multi-page, multi-modal documents (text, tables, images). It first preprocesses each page into dense visual embeddings and concise LLM-generated summaries. During querying, it retrieves candidate pages via embedding similarity, then uses an LLM to re-rank them based on summaries. A single VLM-based reasoner agent reads the selected pages and a working memory, deciding either to answer, declare the query unanswerable, or issue a refined follow-up query for further retrieval. This process iterates until an answer is reached or a limit is hit. SimpleDoc achieves 70.12% accuracy across four DocVQA benchmarks, outperforming prior baselines by 3.2% while retrieving only 3.5 pages per question.

## Method Summary
SimpleDoc processes multi-modal documents by first generating dense visual embeddings and concise LLM summaries for each page offline. Online, it retrieves candidate pages using embedding similarity, then re-ranks them via LLM using the summaries. A single VLM reasoner reads the selected pages plus working memory, deciding to answer, declare unanswerable, or issue a refined query. This iterative loop continues until an answer is reached or a maximum iteration limit is hit.

## Key Results
- Achieves 70.12% average accuracy across four DocVQA benchmarks, outperforming prior methods by 3.2%.
- Retrieves only 3.5 pages per question on average, compared to 20 pages for multi-agent baselines.
- Dual-cue retrieval (embedding + summary re-ranking) achieves 67.37% all-hit rate and 62.22% F1 on MMLongBench, outperforming embedding-only approaches.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Cue Retrieval Reduces Noise While Maintaining Coverage
Combining dense visual embeddings with summary-based LLM re-ranking achieves better precision than embedding-only retrieval. First-pass embedding retrieval (top-k) ensures coverage; LLM reviews summaries to filter and re-rank, keeping only truly relevant pages. This two-stage filter removes irrelevant pages that embeddings surface due to visual similarity but lack semantic relevance. Core assumption: Summaries capture enough semantic signal for relevance judgments without needing full page content.

### Mechanism 2: Iterative Query Refinement with Working Memory Enables Multi-Hop Evidence Gathering
Allowing the reasoner to detect missing evidence and issue refined queries improves answer accuracy over single-shot retrieval. After initial retrieval, the VLM reasoner assesses sufficiency. If incomplete, it outputs a refined query targeting missing information plus notes for working memory. New retrieval incorporates prior context, enabling progressive evidence gathering across iterations. Core assumption: The reasoner can accurately identify when evidence is missing and generate useful query refinements.

### Mechanism 3: Single Unified VLM Reasoner Generalizes Across Modalities Better Than Specialized Multi-Agent Designs
A single VLM reasoner with page images as input can handle text, tables, and figures more robustly than multi-agent systems with modality-specific agents. Modern VLMs process full page images, preserving layout and cross-modal relationships. One agent reads everything, avoiding coordination overhead and error propagation across specialized agents. Core assumption: The VLM has sufficient capacity to reason across modalities without explicit modality routing.

## Foundational Learning

- **Concept: Late-Interaction Visual Retrieval (ColPali/ColQwen)**
  - Why needed here: SimpleDoc relies on ColQwen embeddings for first-pass retrieval; understanding MaxSim scoring and multi-vector embeddings is essential for debugging retrieval quality.
  - Quick check question: Given a query and page image, how does ColPali compute similarity differently from a single-vector CLIP-style embedding?

- **Concept: VLM Context Windows and Multi-Image Input**
  - Why needed here: The reasoner ingests multiple page images plus text plus memory; understanding VLM input limits and how different models handle interleaved image-text is critical for scaling.
  - Quick check question: What happens when you feed a 32B VLM 10 page images plus extracted text—does it attend to all visual tokens or sample?

- **Concept: Agentic Self-Reflection Loops**
  - Why needed here: SimpleDoc's iterative refinement is a self-reflection pattern; the reasoner evaluates its own state ("is evidence sufficient?") and acts (answer/query_update/not_answerable).
  - Quick check question: How does this differ from standard ReAct-style tool use, and what failure modes emerge from self-evaluating sufficiency?

## Architecture Onboarding

- **Component map:**
  Offline preprocessing: PDF pages → ColQwen embeddings + LLM summaries → Vector DB
  Online retrieval: Query → ColQwen embedding → top-k pages → LLM re-ranker → ordered subset
  Reasoning loop: VLM reasoner ← (query, page images, extracted text, working memory) → answer / not_answerable / query_update + notes
  Memory update: Append notes to memory, update query, repeat retrieval

- **Critical path:** The query_update decision. If the reasoner incorrectly signals sufficiency too early, accuracy drops; if it over-requests, retrieval budget inflates. Prompt engineering explicitly guides this decision with scratchpad reasoning.

- **Design tradeoffs:**
  - Top-k for embedding retrieval: Higher k (e.g., 30) gives re-ranker more candidates but increases latency.
  - Iteration limit L: More iterations help multi-hop questions but risk diminishing returns.
  - Single VLM vs multi-agent: Simplicity and generalization vs modality-specific optimization.

- **Failure signatures:**
  - Table-heavy questions: Underperforms specialized table-parsing agents.
  - Over-generic summaries: May discard relevant pages during re-ranking.
  - Query drift: Iterative refinement can shift query semantics if not carefully constrained.

- **First 3 experiments:**
  1. Reproduce retrieval metrics: Measure all-hit rate and page-level F1 for ColQwen vs SimpleDoc.
  2. Ablate iterations: Run with L=1,2,3 and plot accuracy vs retrieval budget.
  3. Modality breakdown: Evaluate on each evidence source (text, table, chart, figure) separately.

## Open Questions the Paper Calls Out

- **Multi-document retrieval:** The current architecture indexes pages within one document; scaling to corpus-level retrieval introduces challenges in cross-document evidence aggregation, duplicate information handling, and retrieval efficiency. Experiments on multi-document DocVQA benchmarks would be needed.

- **Training-based RAG adaptation:** Existing trained RAG methods target text-only knowledge QA; adapting them to handle visual embeddings and multi-modal page understanding remains unexplored. The paper suggests more RAG methods that require training could be utilized for this task.

## Limitations

- Summary quality bottleneck: Dual-cue retrieval depends heavily on LLM-generated page summaries for re-ranking. No empirical validation of summary fidelity or coverage.
- VLM modality generalization: While claiming a single VLM outperforms specialized multi-agent systems, Table 1 shows MDocAgent is +3.8 points better on FetaTab (table-heavy).
- PDF processing toolchain unspecified: Exact PDF-to-image conversion, text extraction method, and MaxSim embedding handling are not detailed.

## Confidence

- **Dual-cue retrieval improves precision over embedding-only:** Medium
- **Iterative refinement with working memory improves accuracy:** Medium
- **Single VLM reasoner outperforms multi-agent designs:** Medium

## Next Checks

1. **Ablate summary quality:** Generate synthetic "low-quality" summaries and rerun retrieval; measure drop in F1 to quantify summary dependency.
2. **Isolate modality performance:** Run SimpleDoc on table-only and text-only subsets; compare VLM vs specialized table parser accuracy to confirm modality generalization limits.
3. **Statistical significance of iterative gains:** Re-run Table 5 ablation across multiple seeds; perform paired t-tests to confirm iteration 1→3 accuracy improvement is significant.