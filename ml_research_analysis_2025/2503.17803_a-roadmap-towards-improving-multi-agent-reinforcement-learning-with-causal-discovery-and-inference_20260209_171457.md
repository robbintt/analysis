---
ver: rpa2
title: A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal
  Discovery And Inference
arxiv_id: '2503.17803'
source_url: https://arxiv.org/abs/2503.17803
tags:
- causal
- learning
- marl
- discovery
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causal augmentation framework for Multi-Agent
  Reinforcement Learning (MARL) called CDRL. The core idea is to use causal discovery
  and inference to learn a minimal causal model linking actions, states, and rewards,
  then use this model to filter out risky actions and guide agents toward desired
  states via causal inference.
---

# A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference

## Quick Facts
- arXiv ID: 2503.17803
- Source URL: https://arxiv.org/abs/2503.17803
- Reference count: 18
- Primary result: Causal augmentation framework (CDRL) improves safety in navigation/flocking tasks but struggles in highly cooperative give-way scenarios.

## Executive Summary
This paper introduces CDRL, a causal augmentation framework for Multi-Agent Reinforcement Learning (MARL) that uses causal discovery and inference to filter risky actions and guide agents toward desired states. The framework learns a minimal causal model linking actions, states, and rewards using the PC algorithm, then applies action masking based on reward predictions from the causal model. Tested across three MARL scenarios (navigation, flocking, give-way) with three algorithms (IQL, VDN, Qmix), results show mixed improvements: safety gains in less cooperative tasks but performance degradation in highly cooperative settings when using independent learning algorithms.

## Method Summary
CDRL integrates causal discovery into MARL by first collecting trajectories via random policy, then discretizing continuous observations and applying the PC algorithm to learn a Causal Bayesian Network (CBN). During training, the framework conditions the CBN on current observations and performs interventions (do-operator) on action variables to predict reward distributions. Actions falling below the 25th percentile of "likability" scores are filtered out. The method is tested with three state-of-the-art MARL algorithms (IQL, VDN, Qmix) across three scenarios in the VMAS simulator, measuring normalized reward, optimality gap, and task-specific metrics like collision rates.

## Key Results
- Causal augmentation improves safety (reduces collisions) in navigation and flocking tasks for cooperative algorithms (VDN, Qmix)
- Performance degradation occurs in give-way tasks when using independent learning algorithms (IQL)
- Causal masking effectiveness depends on algorithm type and task cooperation level
- Mixed results suggest causal filtering benefits specific MARL paradigms more than others

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal inference-based action masking reduces collision rates and improves safety
- **Mechanism:** CBN constructed from trajectory data; actions below 25th percentile of predicted reward scores are pruned
- **Core assumption:** Minimal causal model (Action → State → Reward) remains stable during training
- **Evidence:** Collisions better handled by causal augmentation in navigation/flocking; action filtering explicitly described in methodology
- **Break condition:** Spurious correlations from sparse sampling may prune optimal actions

### Mechanism 2
- **Claim:** Constraining causal discovery to minimal variables reduces computational complexity
- **Mechanism:** Discretizes continuous observations and restricts DAG such that reward is always a child of state/action nodes
- **Core assumption:** Discretization retains sufficient information fidelity for effective intervention
- **Evidence:** Constrained DAG structure specified; sensitivity analysis ensures discretization doesn't catastrophically impact performance
- **Break condition:** Fine-grained continuous tasks may require more granular distinctions than discretization provides

### Mechanism 3
- **Claim:** Causal augmentation helps independent/loosely coupled agents but may degrade performance in highly cooperative tasks
- **Mechanism:** Independent causal models per agent fail to capture inter-agent dependencies needed for cooperative success
- **Core assumption:** Causal filtering beneficial only if dynamics are stationary or algorithm handles cooperation
- **Evidence:** Give-way task shows IQL degradation with causal augmentation; Qmix performs better in cooperative scenarios
- **Break condition:** High interdependence breaks independent causal model assumption

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & The $do$-operator**
  - Why needed: Framework relies on Pearl's framework to distinguish "seeing" vs "doing" for predicting hypothetical action rewards
  - Quick check: Explain why $P(Reward | Action)$ differs from $P(Reward | do(Action))$ in multi-agent environments

- **Concept: Non-stationarity in MARL**
  - Why needed: Agents changing policies makes environment dynamics non-stationary, challenging stable causal model learning
  - Quick check: Why does a causal graph learned at timestep $t$ become invalid at $t+100$ in multi-agent systems?

- **Concept: Value Function Factorization (IQL vs. VDN/Qmix)**
  - Why needed: Different cooperative paradigms explain mixed results; IQL treats others as noise while Qmix factors joint behavior
  - Quick check: Why might global causal filter hinder algorithms designed to decompose global rewards?

## Architecture Onboarding

- **Component map:** Data Collector → Discretizer → Causal Discovery Engine (PC algorithm) → Inference Engine (do-operator) → Action Masker → Base MARL Algorithm
- **Critical path:** Sensitivity Analysis (Appendix E) - incorrect tuning of discretization bins, sensor count, or sample size yields garbage causal graphs
- **Design tradeoffs:**
  - Discretization Granularity: High preserves info but increases PC algorithm computational cost
  - Independent vs. Shared Models: Current per-agent models scale but miss cooperation; shared models capture cooperation but scale poorly
- **Failure signatures:**
  - High variance/regression in Give-Way: Indicates spurious correlations in causal model
  - Stagnant Reward: Overly aggressive mask prevents exploration
- **First 3 experiments:**
  1. Sanity Check: Run CausalIQL on Navigation task, verify collision reduction vs vanilla IQL
  2. Ablation on Bins: Flocking task with M=5 vs M=20 bins, monitor SHD of learned graph
  3. Cooperation Stress Test: CausalQmix vs CausalIQL on Give-Way task, confirm Qmix mitigates IQL degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** Can experimental design principles improve causal discovery efficiency compared to random policy sampling?
  - Basis: Current random policy approach is inefficient and lacks guarantees on trajectory informativeness
  - Why unresolved: Random policies induce high variance and don't ensure necessary information for causal relationships
  - Resolution: Compare convergence speed and graph accuracy between random and experimental design-guided discovery

- **Open Question 2:** How can correctness of learned causal model be assessed without ground truth?
  - Basis: Validating causal model without ground truth is open issue; blind trust may explain performance failures
  - Why unresolved: Ground truth implies knowing environment dynamics, rendering RL unnecessary
  - Resolution: Develop validation protocol/metric predicting policy performance degradation from structural errors

- **Open Question 3:** Does collaborative causal discovery mitigate non-stationarity issues?
  - Basis: Poor performance in cooperative tasks due to independent learning failing to account for other agents
  - Why unresolved: Independent learning treats others as environment, causing non-stationarity
  - Resolution: Demonstrate collaborative discovery yields stable improvements where independent discovery fails

- **Open Question 4:** Can soft interventions handle continuous variables without discretization information loss?
  - Basis: Discretization is practical workaround but not principled and may lose information
  - Why unresolved: Hard interventions on continuous variables are computationally intractable
  - Resolution: Implement soft interventions showing higher fidelity causal graphs vs discretized baselines

## Limitations
- Causal structure stability assumption not validated beyond VMAS simulator; no evidence for transfer to different dynamics
- Discretization impact on causal model fidelity lacks rigorous justification; sensitivity analysis focuses on parameters not structural integrity
- Framework's inability to improve highly cooperative tasks with IQL acknowledged but root cause not fully disentangled
- No mechanism described for causal model adaptation to evolving agent strategies during training

## Confidence
- **High confidence:** Causal augmentation improves safety in navigation/flocking with cooperative algorithms; action masking mechanism clearly described
- **Medium confidence:** Causal discovery and inference effective for MARL but benefits limited to specific task types and algorithm classes
- **Low confidence:** Claim of "minimal" and "computationally efficient" model not fully substantiated; no comparison to full unconstrained discovery process

## Next Checks
1. Validate causal stability over time: Implement rolling causal discovery every 1000 steps, measure SHD between consecutive graphs
2. Ablate discretization method: Replace uniform with adaptive binning, compare performance and SHD on Flocking task
3. Test cooperative causal modeling: Learn shared causal model for all agents in Give-Way task, compare performance against independent causalIQL and vanilla IQL