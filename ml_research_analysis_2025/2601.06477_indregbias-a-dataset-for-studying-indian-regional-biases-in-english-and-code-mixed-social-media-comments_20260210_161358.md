---
ver: rpa2
title: 'IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed
  Social Media Comments'
arxiv_id: '2601.06477'
source_url: https://arxiv.org/abs/2601.06477
tags:
- regional
- bias
- comments
- biases
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndRegBias, a dataset of 25,000 social media
  comments annotated for Indian regional biases, addressing the lack of regional bias
  benchmarks. A multilevel annotation strategy was used to label comments for bias
  presence, severity, and targeted region.
---

# IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments

## Quick Facts
- arXiv ID: 2601.06477
- Source URL: https://arxiv.org/abs/2601.06477
- Reference count: 19
- Introduces IndRegBias dataset of 25,000 social media comments annotated for Indian regional biases

## Executive Summary
This paper introduces IndRegBias, a dataset of 25,000 social media comments annotated for Indian regional biases, addressing the lack of regional bias benchmarks. A multilevel annotation strategy was used to label comments for bias presence, severity, and targeted region. Large language models (LLMs) and Indic language models (ILMs) were evaluated in zero-shot, few-shot, and fine-tuning settings. Zero-shot and few-shot approaches showed limited accuracy, while fine-tuning significantly improved detection of regional bias and severity.

## Method Summary
The paper presents a dataset creation pipeline using Reddit and YouTube comments, followed by multilevel annotation for bias presence, severity, and targeted region. Models were evaluated using zero-shot, few-shot, and fine-tuning approaches with PEFT/LoRA for efficient adaptation. Binary classification distinguished regional bias from non-regional bias, while multi-class classification identified severity levels. Performance was measured using accuracy, precision, and F1-score with 5-fold stratified cross-validation.

## Key Results
- Fine-tuning significantly enhances LLM performance in detecting Indian regional bias and severity
- Zero-shot and few-shot approaches show limited accuracy compared to fine-tuned models
- Qwen3-8B F1-score improved from 0.772 (zero-shot) to 0.904 (fine-tuning)
- Safety-aligned models like Sarvam-M and Gemini-2.5-Pro show degraded performance due to refusal mechanisms

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning (PEFT) with LoRA adapts generic LLMs to the specific manifold of Indian regional bias more effectively than in-context learning. Pre-trained models lack specific decision boundaries for nuanced regional stereotypes, and fine-tuning minimizes loss on these specific bias clusters. The dataset's high-quality annotations (Cohen's κ = 0.83) provide reliable ground truth. Fine-tuning may overfit to annotator bias if the concept is too subjective or annotation noise is high.

### Mechanism 2
Safety alignment (RLHF/RLVR) in some models acts as a refusal filter, degrading performance on bias detection tasks. Models prioritize safety over analytical compliance, triggering refusals or neutral classifications when processing inputs containing slurs or stereotypes. This interferes with classification instructions, resulting in false negatives. Prompt engineering to sandbox analysis might reduce interference, though the paper suggests it remains significant.

### Mechanism 3
Multilingual/code-mixed pre-training enables better zero-shot detection of regional bias in IndRegBias. Regional bias is often expressed in code-mixed languages (Hinglish, Tanglish), and models like Qwen3 (119 languages) and Krutrim-2 (2T Indic tokens) have tokenizer vocabularies that map these non-standard lexical items effectively. The semantic signal of regional bias is heavily encoded in specific code-mixed phrasing. Performance gaps would likely narrow if input text is purely standard English.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper relies on LoRA to fine-tune large models efficiently. You cannot understand the "Fine-Tuning vs. Zero-Shot" results without grasping that LoRA freezes main model weights and only trains small adapter matrices.
  - **Quick check question:** Why does LoRA prevent "catastrophic forgetting" of the model's general language capabilities while learning the specific "IndRegBias" task?

- **Concept: Cohen's Kappa (κ)**
  - **Why needed here:** The paper claims high annotation quality (κ = 0.83). You must understand that κ measures agreement beyond chance to trust that the dataset is valid ground truth for training.
  - **Quick check question:** If three annotators label a comment "Severe," but the Kappa is low (e.g., 0.2), can you trust that label for fine-tuning?

- **Concept: Code-Mixing / Transliteration**
  - **Why needed here:** The IndRegBias dataset is naturally code-mixed. Standard NLP pipelines often fail here because "Hinglish" words are split into meaningless sub-tokens if the tokenizer isn't trained for it.
  - **Quick check question:** How might a standard English tokenizer distort a Hindi word written in Latin script (transliteration), and how would that affect bias detection?

## Architecture Onboarding

- **Component map:** Ingest (Reddit, YouTube) -> Process (Clean, Multilevel Annotation) -> Stratify (70/10/20 split) -> Train (PEFT/LoRA) -> Inference (Zero-Shot vs Fine-Tuned)
- **Critical path:** The annotation policy is the bottleneck. The distinction between "Mild" and "Moderate" is subjective, and if this boundary is noisy, the multi-class classification head will struggle to converge.
- **Design tradeoffs:**
  - Open-Source vs. Proprietary: The paper focuses on open models (Llama, Qwen) to allow fine-tuning. Proprietary models (GPT-4o) are only evaluated zero-shot and show high safety refusal rates.
  - Safety vs. Utility: Highly aligned models (Sarvam, Gemini) are safer but fail the detection task. A deployable system must balance bias detection capability with risk of generating harmful text.
- **Failure signatures:**
  - The "Refusal" Mode: Model outputs "I cannot analyze this" or a neutral class for obvious hate speech.
  - The "Over-Flagging" Mode: Mistral-Nemo-Base classifying 95% of content as bias due to lack of nuance calibration.
- **First 3 experiments:**
  1. Baseline Sanity Check: Run Zero-Shot inference on 500 comments with Qwen3-8B and Sarvam-M. Confirm if Sarvam-M actually refuses toxic inputs.
  2. LoRA Ablation: Fine-tune Qwen3-8B on only Binary Classification vs. Joint training (Severity + Region). Determine if multi-task learning degrades binary accuracy.
  3. Tokenizer Stress Test: Evaluate F1-score specifically on code-mixed comments vs. pure English comments to quantify cost of non-native tokenization.

## Open Questions the Paper Calls Out

- **Geographic Generalization:** The dataset has significant geographic imbalance with Northeast and Central India underrepresented (~5% and ~2% respectively), potentially hindering model generalization to these regions.
- **Safety Alignment Trade-offs:** Safety alignment in LLMs treats bias analysis as violation, limiting performance. The paper identifies this issue but doesn't propose methods to adapt safety alignment for analysis tasks.
- **Annotator Cultural Influence:** The distinction between "Mild" and "Moderate" severity is heavily influenced by annotators' cultural backgrounds, though the paper doesn't analyze inter-annotator agreement breakdowns by severity level.

## Limitations

- **Geographic and Demographic Skew:** The dataset underrepresents Northeast and Central Indian states, potentially limiting model generalization.
- **Safety Alignment Interference:** Heavily aligned models may refuse to process toxic comments, resulting in false negatives or neutral classifications.
- **Subjectivity in Annotation:** Severity distinctions are heavily influenced by annotators' cultural backgrounds, creating uncertainty about generalizability.

## Confidence

- **High Confidence:** Fine-tuning significantly improves detection of regional bias and severity compared to zero-shot/few-shot approaches.
- **Medium Confidence:** Multilingual pre-training exposure enables better zero-shot detection of code-mixed regional bias.
- **Medium Confidence:** Safety alignment degrades bias detection performance through refusal mechanisms.
- **Low Confidence:** The specific threshold between "Mild" and "Moderate" severity annotations represents reliable ground truth for training.

## Next Checks

1. **Inter-Annotator Severity Agreement Analysis:** Compute per-severity-level Cohen's κ values to determine if the "Mild" vs "Moderate" boundary has acceptable agreement rates, quantifying noise in the most subjective annotation task.

2. **Safety Alignment Ablation Study:** Systematically test whether safety-refusal patterns can be mitigated through prompt engineering versus whether interference is fundamentally baked into model architecture, clarifying if current safety-aligned models can be adapted for bias detection.

3. **Code-Mixing Generalization Benchmark:** Evaluate fine-tuned models on held-out test set of novel code-mixed patterns not seen during training to validate whether multilingual pre-training provides genuine generalization or merely overfitting to training patterns.