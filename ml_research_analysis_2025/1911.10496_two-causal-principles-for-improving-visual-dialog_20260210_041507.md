---
ver: rpa2
title: Two Causal Principles for Improving Visual Dialog
arxiv_id: '1911.10496'
source_url: https://arxiv.org/abs/1911.10496
tags:
- visual
- causal
- dialog
- visdial
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two causal principles to improve Visual Dialog
  (VisDial) models, leading to significant performance gains across multiple baseline
  models. Principle 1 removes direct dialog history input to the answer model to eliminate
  harmful shortcut biases, while Principle 2 addresses unobserved confounders (user
  preferences) that create spurious correlations.
---

# Two Causal Principles for Improving Visual Dialog

## Quick Facts
- **arXiv ID:** 1911.10496
- **Source URL:** https://arxiv.org/abs/1911.10496
- **Reference count:** 40
- **Primary result:** Two causal principles consistently improve NDCG performance across multiple Visual Dialog baseline models.

## Executive Summary
This paper introduces two causal principles to improve Visual Dialog (VisDial) models, leading to significant performance gains across multiple baseline models. Principle 1 removes direct dialog history input to the answer model to eliminate harmful shortcut biases, while Principle 2 addresses unobserved confounders (user preferences) that create spurious correlations. The authors propose causal intervention algorithms, including question type analysis, answer score sampling, and hidden dictionary learning, to implement Principle 2. Applied to four representative baseline models (LF, HCIAE, CoAtt, RvA), these principles consistently improve NDCG performance. Notably, an ensemble of the simple LF model with these principles outperformed the 2019 Visual Dialog Challenge winner, demonstrating the effectiveness of this model-agnostic approach in addressing fundamental causal issues in VisDial.

## Method Summary
The paper proposes two causal principles for Visual Dialog: (1) removing direct dialog history input to the answer model to prevent shortcut biases, and (2) modeling unobserved user preferences as confounders to address spurious correlations. These principles are implemented through causal intervention algorithms including answer score sampling and hidden dictionary learning. The method is evaluated across four baseline models (LF, HCIAE, CoAtt, RvA) on the VisDial v1.0 dataset, showing consistent NDCG improvements. The approach uses a ranking loss optimized for NDCG rather than standard classification losses.

## Key Results
- Both principles consistently improve NDCG performance across all four baseline models (LF, HCIAE, CoAtt, RvA)
- An ensemble of the simple LF model with both principles outperformed the 2019 Visual Dialog Challenge winner
- The proposed causal intervention approach is model-agnostic and addresses fundamental causal issues in VisDial
- Principle 1 eliminates shortcut biases like mimicking sentence length or keywords from previous turns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing direct input of dialog history to the answer decoder eliminates "shortcut" biases, such as mimicking sentence length or specific keywords from previous turns.
- **Mechanism:** Standard models concatenate history (H), question (Q), and image (I) features to predict answer (A). This creates a direct causal path H → A. By ablating the history feature in the final fusion layer (Principle 1), the model is forced to rely on the question and attended visual knowledge (V), preventing copying of lexical patterns from history.
- **Core assumption:** History is necessary only to resolve co-references in the question (H → Q) or refine visual attention (H → V), but implies no direct causal link to answer content itself (H ↛ A).
- **Evidence anchors:**
  - [abstract]: "Principle 1 suggests: we should remove the direct input of the dialog history to the answer model, otherwise a harmful shortcut bias will be introduced."
  - [Page 7, Figure 4]: Shows qualitative examples where baseline incorrectly ranks answers containing words like "sink" because they appeared in history, behavior relieved by P1.
  - [corpus]: Neighbors discuss general multimodal dialog but lack specific evidence regarding this history-ablation mechanism.
- **Break condition:** If current question is ambiguous and answer relies entirely on information present only in history captions and not referenced in visual features or current question.

### Mechanism 2
- **Claim:** Modeling user preference as unobserved confounder (U) and applying causal intervention (backdoor adjustment) removes spurious correlations between questions and answers.
- **Mechanism:** The authors posit that hidden preference (U) influences both questions asked and answers preferred (Q ← U → A). Standard likelihood P(A|Q) captures this correlation. The paper approximates causal effect P(A|do(Q)) by marginalizing over this confounder, implemented via "Hidden Dictionary Learning" or "Answer Score Sampling."
- **Core assumption:** There exists unobserved variable (user style/preference) that creates non-causal correlation between question and answer distributions.
- **Evidence anchors:**
  - [Page 2, Figure 2(b)]: Illustrates how user preference for "he" creates spurious correlation between questions about "he" and answers containing "he".
  - [Page 5, Eq. 4]: Derives approximation using normalized relevance scores to model prior P(u|H).
  - [corpus]: Corpus provides no direct evidence supporting or refuting specific "hidden dictionary" implementation for user confounders.
- **Break condition:** If dense annotations used for sampling are themselves noisy or inconsistent across annotators, approximation of P(u|H) may introduce noise rather than signal.

### Mechanism 3
- **Claim:** Replacing maximum likelihood estimation with Generalized Ranking Loss (R3) optimizes model specifically for NDCG metric.
- **Mechanism:** VisDial is a ranking task, not classification. Paper implements causal intervention via ranking loss that penalizes model if irrelevant answers score higher than relevant ones, rather than simply pushing probability of single ground truth answer.
- **Core assumption:** Relevance scores provided in dataset are reliable proxies for causal effect of answer given question and history context.
- **Evidence anchors:**
  - [Page 5, Section 5.2]: "Generalized Ranking Loss (R3)... P_i(A) is log exp(pi) / (exp(pi) + sum exp(pj))"
  - [Page 6, Table 1]: Shows R3 consistently outperforming regression (R0) and other loss variants across baseline models.
  - [corpus]: Weak connection; corpus papers focus on system architectures rather than specific loss function mechanics for ranking.
- **Break condition:** If candidate answer list is small or relevance distribution is binary (0 or 1), benefits of generalized ranking approach over binary cross-entropy may diminish.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Do-Calculus**
  - **Why needed here:** Paper frames problem entirely using Pearl's causal graphs. Without understanding nodes (variables), edges (causal links), and do-operator (intervention vs. observation), rationale for removing H → A link or adding U node is opaque.
  - **Quick check question:** Explain difference between observing P(A|Q) and intervening P(A|do(Q)) in context of confounder.

- **Concept: The Backdoor Criterion**
  - **Why needed here:** Principle 2 relies on backdoor criterion to block spurious paths. Understanding that "backdoor paths" are non-causal associations is key to understanding why dictionary/sampling methods work.
  - **Quick check question:** In graph Q ← U → A, does path from Q to A via U represent causal effect of Q on A?

- **Concept: Visual Dialog (VisDial) vs. VQA**
  - **Why needed here:** Authors explicitly argue VisDial is "NOT VQA with history." Understanding difference (ranking 100 candidates with dense relevance vs. 1 classification) is critical for understanding why NDCG is target and why ambiguity is handled differently.
  - **Quick check question:** Why is Mean Reciprocal Rank (MRR) potentially misleading for evaluating dialog system where multiple answers might be semantically equivalent?

## Architecture Onboarding

- **Component map:** Encoder -> Attention Mechanism -> P1/P2 Intervention -> Decoder
  - **Encoder:** Processes Image (I), History (H), Question (Q)
  - **Attention Mechanism:** Generates Visual Knowledge (V) attending to I based on Q and H
  - **P1 Modification:** Fusion layer excludes H features; takes V and Q to predict A
  - **P2 Intervention Layer:** Sits before final answer scoring; computes attention over learned dictionary D_u using History H or uses pre-computed scores to weight loss
  - **Decoder:** Scores 100 answer candidates

- **Critical path:** Implementation of Principle 2 (P2) via Hidden Dictionary or Score Sampling, where theoretical "causal intervention" is translated into code.

- **Design tradeoffs:**
  - **P1 Implementation:** Computationally free (saves tensor ops) but requires retraining from scratch; cannot fine-tune frozen pre-trained model effectively if input dimension changes
  - **P2 Implementation:** "Answer Score Sampling" requires "dense annotations" (human relevance scores for all 100 options), expensive to collect for new datasets; "Hidden Dictionary" does not require dense annotations but introduces extra parameters (N × d) and complexity

- **Failure signatures:**
  - **P1 Failure:** Model outputs answers that consistently repeat words from caption or previous turns
  - **P2 Failure:** Model ranks "safe" but uninformative answers (e.g., "I can't tell") too high, or NDCG improves but MRR drops significantly

- **First 3 experiments:**
  1. **Sanity Check (P1):** Run baseline LF model on validation, then LF + P1; plot "Answer Length vs. History Length" to verify bias reduction shown in Figure 2(a)
  2. **Intervention Ablation (P2):** Train model using "Question Type" implementation (cheapest) vs. "Hidden Dictionary" (most complex) to compare NDCG delta on validation
  3. **Loss Function Tuning:** Compare R1 (Weighted Softmax) vs. R3 (Generalized Ranking) on RvA baseline to confirm R3 is superior for NDCG metric

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do proposed causal principles and causal graph structure generalize to distinct data generation processes of embodied Q&A and conversational visual dialog tasks?
- **Basis in paper:** [explicit] Conclusion states: "As moving forward, we will stick to our causal thinking to discover other potential causalities hidden in embodied Q&A and conversational visual dialog tasks."
- **Why unresolved:** Current work validates principles solely on VisDial v1.0 dataset with specific two-player protocol, whereas embodied and conversational tasks involve different interactive dynamics and grounding constraints
- **What evidence would resolve it:** Application of two principles to datasets like GuessWhat?! or embodied navigation datasets (e.g., CVDN), demonstrating performance improvements with minimal modification to proposed causal graph

### Open Question 2
- **Question:** Can unobserved confounder U (user preference) be modeled more effectively using continuous representations rather than proposed discrete approximations?
- **Basis in paper:** [inferred] Paper notes because U is unobserved, "it is impossible to sample u in Eq. (1) directly," leading to approximations using "Question Type" or "Hidden Dictionary Learning" relying on manual definitions or fixed-size matrices (N=100)
- **Why unresolved:** Proposed implementations (QT, Score Sampling, Dictionary) are heuristic approximations; unclear if continuous, end-to-end differentiable latent variable model could capture confounder more accurately without relying on sparse dense annotations or hand-picked question types
- **What evidence would resolve it:** Comparative study where latent variable model (e.g., VAE-based encoder) replaces discrete dictionary or score sampling, achieving higher NDCG scores or showing better convergence properties on same baselines

### Open Question 3
- **Question:** Is removal of direct history-to-answer link (H → A) universally beneficial, or does it harm performance in generative dialog settings where answer syntax and style are conditioned on history?
- **Basis in paper:** [inferred] Paper validates Principle 1 (H → A) using discriminative ranking models (LF, HCIAE, CoAtt, RvA), justifying it by VisDial collection protocol where annotators "were not allowed to copy"
- **Why unresolved:** While H → A introduces "shortcut bias" in ranking, generative models often rely on history to maintain conversational continuity or stylistic consistency, which principle explicitly forbids
- **What evidence would resolve it:** Ablation study applying Principle 1 to state-of-the-art generative visual dialog models to observe if lack of direct history input degrades fluency or relevance of generated text

## Limitations
- **Implementation ambiguity:** While Principle 1 is clearly specified, Principle 2's implementation details (particularly Hidden Dictionary construction) lack sufficient specification for exact replication
- **Theoretical assumptions:** Causal graph assumptions (existence and influence of unobserved user preference confounder U) are plausible but not empirically validated within paper itself
- **Dataset specificity:** Results demonstrated only on VisDial v1.0 with dense annotations; generalization to other dialog or multimodal datasets remains untested

## Confidence
- **Quantitative results:** High confidence (consistent NDCG improvements across multiple baselines)
- **Causal interpretation:** Medium confidence (plausible assumptions not empirically validated)
- **Implementation reproducibility:** Medium confidence (some details missing, particularly for Principle 2)

## Next Checks
1. **Mechanistic ablation:** Compare LF + P1 (only) vs. LF + P2 (only) to isolate which principle contributes more to performance gains
2. **Confounder validation:** Test whether removing user preference confounder U (via backdoor adjustment) specifically improves performance on questions known to be sensitive to user style
3. **Cross-dataset robustness:** Apply causal principles to different multimodal dialog dataset (e.g., GuessWhat?! or CLEVR-Dialog) to assess generalizability beyond VisDial v1.0