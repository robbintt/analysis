---
ver: rpa2
title: Assessing Code Understanding in LLMs
arxiv_id: '2504.00065'
source_url: https://arxiv.org/abs/2504.00065
tags:
- code
- prompt
- while
- llms
- copy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates Large Language Models (LLMs) on their ability
  to understand semantic equivalence in code through non-trivial transformations like
  copy propagation and constant folding. The experiments involve 11 Python algorithms,
  each presented in eight variants: a reference implementation plus perturbed and
  buggy versions.'
---

# Assessing Code Understanding in LLMs

## Quick Facts
- arXiv ID: 2504.00065
- Source URL: https://arxiv.org/abs/2504.00065
- Reference count: 32
- Large Language Models fail to recognize semantically equivalent code in 41% of cases without context and 29% with simple context

## Executive Summary
This study evaluates Large Language Models on their ability to understand semantic equivalence in code through non-trivial transformations like copy propagation and constant folding. The experiments involve 11 Python algorithms, each presented in eight variants: a reference implementation plus perturbed and buggy versions. The evaluation uses seven chatbots across four prompt types: contextless/single-class, contextual/single-class, contextless/multi-class, and contextual/multi-class. Results show that LLMs fail to recognize semantically equivalent code in 41% of cases without context and 29% with simple context. Performance varies by model, with Claude and DeepSeek performing best, and by perturbation type, with constant folding being the least confusing and combined perturbations the most challenging. The study highlights the need for integrating LLMs with code-optimization tools to improve training and facilitate more robust program understanding.

## Method Summary
The study evaluates seven chatbots on 11 Python algorithms, each with eight variants (reference, three perturbed correct versions, one buggy, and three perturbed buggy versions). The evaluation uses four prompt types: contextless/single-class, contextual/single-class, contextless/multi-class, and contextual/multi-class. Each prompt is presented 10 times per chatbot, generating 3080 responses total. The perturbations are copy propagation (replacing variables with their aliases) and constant folding (computing constant expressions at compile time). Manual human review classifies verbose model outputs to determine equivalence judgments, accounting for self-contradictions within responses.

## Key Results
- LLMs fail to judge semantic equivalence in 41% of cases without context and 29% with simple context
- Contextual prompts improve accuracy by approximately 12 percentage points across most models
- Constant folding yields 76.40% accuracy while copy propagation yields 69.20%, with combined perturbations dropping to 49.39%
- Self-contradiction occurs in model outputs, with models sometimes stating contradictory conclusions within single responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing explicit context about transformation types improves LLM equivalence judgments by approximately 12 percentage points
- Mechanism: Role and instruction prompting primes the model's attention toward compiler-optimization patterns rather than surface syntax matching
- Core assumption: Models have latent knowledge of these transformations from training data but fail to activate it without explicit cueing
- Evidence anchors:
  - [abstract] "LLMs fail to judge semantic equivalence in approximately 41% of cases when no context is provided and in 29% when given a simple generic context"
  - [section 4] Contextual prompts (Prompts 2 and 4) enhanced accuracy for most chatbots, with notable exceptions
  - [corpus] Weak external validation; related papers confirm context sensitivity but not specifically for compiler transformations
- Break condition: Context degrades performance when models lack sufficient prior exposure to the transformation class (e.g., Gemini's accuracy dropped with context)

### Mechanism 2
- Claim: Constant folding transformations are recognized more reliably than copy propagation, especially when combined
- Mechanism: Constant folding produces local arithmetic simplifications that preserve visible structure; copy propagation requires tracking variable aliases across control flow, demanding deeper state reasoning
- Core assumption: Models rely more heavily on pattern matching than simulating program execution states
- Evidence anchors:
  - [section 4] "Constant folding is the least confusing perturbation for chatbots, yielding an accuracy of 76.40%. Copy propagation appears more challenging (69.20%), while the combination results in the lowest accuracy (49.39%)"
  - [section 5] Copy propagation confusion "becomes more pronounced when non-numerical data types, such as lists and arrays, are involved" due to reference semantics
  - [corpus] "When Names Disappear" paper confirms LLMs rely on naming cues; removing them reveals structural understanding gaps
- Break condition: Performance collapses when transformations compound (CP+CF) or involve mutable data structures with aliasing

### Mechanism 3
- Claim: Models exhibit internal reasoning instability, contradicting themselves within single outputs
- Mechanism: Verbose chain-of-thought outputs allow models to explore multiple hypotheses; without enforced consistency, early claims may be reversed by conclusions
- Core assumption: Longer outputs improve reasoning by enabling self-correction, but this is unreliable without structured verification
- Evidence anchors:
  - [section 5] "The models occasionally contradict themselves during the evaluation process, stating something at the beginning of the output and landing on the opposite conclusion by the end"
  - [section 5] "When we instructed models to be concise, their answers were often more erroneous"
  - [corpus] Related work on code reasoning benchmarks notes similar inconsistency patterns
- Break condition: Verbosity alone is insufficient; without external verification or structured reasoning frameworks, self-contradiction persists

## Foundational Learning

- Concept: Copy propagation (compiler optimization)
  - Why needed here: Understanding that `x = y; ...use x...` can be transformed to `...use y...` when x and y are provably equivalent
  - Quick check question: Given `a = b; c = a + 1;`, can `c` be rewritten as `b + 1`? What conditions must hold?

- Concept: Constant folding (compiler optimization)
  - Why needed here: Recognizing that expressions with known constant values can be computed at compile time
  - Quick check question: In `x = 2 * 3 - 1;`, what is the folded form? What if `x` is later used in a loop condition?

- Concept: Reference vs. value semantics in Python
  - Why needed here: Critical for understanding why `B = A` creates an alias for lists but copies for integersâ€”a primary failure mode observed
  - Quick check question: After `A = [1,2]; B = A; B.append(3)`, what is `A`? How does this differ from `a = 5; b = a; b = 6`?

## Architecture Onboarding

- Component map: Transformation engine -> Dataset generator -> Evaluation harness -> Response classifier
- Critical path:
  1. Define transformation rules formalized for Python subset
  2. Apply fixpoint annotation algorithm until convergence
  3. Generate perturbed variants with guaranteed semantic equivalence
  4. Construct prompts with contextless/contextual preambles
  5. Collect and manually classify responses

- Design tradeoffs:
  - Manual evaluation vs. automated parsing: Chose manual to handle verbose, contradictory outputs; trades scalability for accuracy
  - Zero-shot vs. few-shot: Chose zero-shot to reflect typical user interactions; sacrifices potential accuracy gains
  - User prompts vs. system prompts: Chose user prompts (web interface) over API access for ecological validity; loses temperature control

- Failure signatures:
  - Gemini: Context decreases performance (anomalous pattern)
  - Copilot-Claude vs. Anthropic-Claude: Same underlying model produces different results depending on frontend
  - Unification algorithm: Consistently lowest accuracy across all models (non-numeric, class-based data structures)
  - Sieve of Eratosthenes: Worst overall accuracy due to modulus operations combined with list manipulation

- First 3 experiments:
  1. Replicate single-class contextless prompt (Prompt 1) on your target model with 2-3 algorithms to establish baseline error rate
  2. Add contextual preamble and measure delta; if improvement <5%, model may lack transformation exposure
  3. Test specifically on list-manipulation algorithms with copy propagation to probe reference-semantics understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating a compiler-based code pre-processor into the LLM pipeline significantly improve accuracy in semantic equivalence tasks?
- Basis in paper: [explicit] The authors conclude that "a code pre-processing tool integrated into the LLM pipeline would improve program understanding by a fair amount."
- Why unresolved: The study assesses the *deficiency* of current models but does not implement or test the proposed architectural solution.
- What evidence would resolve it: Empirical results from a modified LLM pipeline where inputs are automatically normalized to remove "noisy" semantic-preserving variations before the model analyzes them.

### Open Question 2
- Question: Does constraining output verbosity negatively impact the reasoning accuracy of LLMs in code semantic analysis?
- Basis in paper: [explicit] Section 5 notes that shorter answers were often more erroneous, stating, "Limiting verbosity may truncate this process... a finding that merits further investigation."
- Why unresolved: This observation was anecdotal during the evaluation process and not systematically quantified in the experimental results.
- What evidence would resolve it: A controlled ablation study measuring success rates on the same benchmarks across different output token constraints.

### Open Question 3
- Question: How does LLM performance degrade when facing more complex compiler optimizations, such as loop unrolling or dead code elimination?
- Basis in paper: [explicit] Section 6 lists "exploring more advanced optimizations" as a necessary direction for future work.
- Why unresolved: The current methodology is restricted to copy propagation and constant folding on a subset of Python.
- What evidence would resolve it: Expanding the dataset to include additional standard compiler transformations and evaluating model performance on these new variants.

## Limitations
- Evaluation relies on manual classification of verbose model outputs, introducing subjectivity in resolving contradictions
- Zero-shot prompting without few-shot examples limits assessment of model capacity when given explicit transformation examples
- Performance differences between Copilot-Claude and Anthropic-Claude suggest significant variation based on interface rather than model capability

## Confidence
- **High confidence**: Context improves accuracy (12-point improvement documented across multiple models); constant folding is easier than copy propagation; self-contradiction occurs in verbose outputs
- **Medium confidence**: Combined perturbations cause disproportionate difficulty (requires replication with larger datasets); Gemini's anomalous context sensitivity (single model pattern needs broader validation)
- **Low confidence**: Claims about latent knowledge activation (assumes transformation exposure without direct evidence); specific algorithm-level failures (sample size of 11 may not generalize)

## Next Checks
1. Replicate the evaluation using automated parsing of equivalence statements rather than manual review to test robustness of the 41%/29% failure rates
2. Test the same algorithms with few-shot prompts providing explicit examples of each transformation type to measure ceiling performance
3. Expand the dataset to include algorithms with different complexity profiles (more control flow, different data structures) to validate whether unification and sieve algorithms are truly outliers or indicative of broader limitations