---
ver: rpa2
title: 'Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation'
arxiv_id: '2512.06993'
source_url: https://arxiv.org/abs/2512.06993
tags:
- unlearning
- adversarial
- methods
- layers
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses machine unlearning in classification models,
  focusing on removing the influence of specific training data subsets while preserving
  utility. The core method, Adversarial Machine UNlearning (AMUN), fine-tunes the
  model on adversarial examples of the forget set with wrong labels.
---

# Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation

## Quick Facts
- arXiv ID: 2512.06993
- Source URL: https://arxiv.org/abs/2512.06993
- Reference count: 40
- Primary result: AMUN outperforms prior methods on CIFAR-10, achieving strong unlearning (MIA performs no better than random guessing) while maintaining high test accuracy

## Executive Summary
This paper addresses machine unlearning in classification models, focusing on removing the influence of specific training data subsets while preserving utility. The core method, Adversarial Machine UNlearning (AMUN), fine-tunes the model on adversarial examples of the forget set with wrong labels. This approach leverages the observation that adversarial examples naturally belong to the model-induced input distribution, localizing changes to the decision boundary and preserving overall accuracy. Theoretical analysis identifies Lipschitz continuity and adversarial example transferability as key factors governing AMUN's effectiveness. The paper also introduces FastClip for efficient spectral norm control and Layer-wise Orthogonalization for Training robust ensembles (LOTOS), along with methods for class unlearning including a novel nearest-neighbor membership inference attack (MIA-NN) and Tilted ReWeighting (TRW).

## Method Summary
The paper proposes AMUN for machine unlearning, which fine-tunes a model on adversarial examples of the forget set labeled with their mispredicted classes. Adversarial examples are generated using PGD-50 with an ε-search mechanism to find the closest decision boundary points. For class unlearning, the paper introduces MIA-NN to detect vulnerabilities in existing methods and proposes TRW to approximate retrained model behavior by tilting the target distribution based on inter-class similarity. The paper also presents FastClip for efficient spectral norm clipping and LOTOS for improving ensemble robustness by reducing adversarial example transferability through orthogonalization.

## Key Results
- AMUN achieves strong unlearning on CIFAR-10, with RMIA AUC approaching 50% (random guessing) while preserving test accuracy
- Theoretical analysis shows Lipschitz continuity governs adversarial example transferability in ensembles, with lower Lipschitz increasing individual robustness but ensemble transferability
- TRW for class unlearning reduces the gap to retrained models by 19% (U-LiRA) and 46% (MIA-NN) compared to the state-of-the-art baseline
- LOTOS improves ensemble robust accuracy by 3-5% over standard adversarial training while reducing adversarial example transferability between ensemble members

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Example Localization for Targeted Unlearning
Fine-tuning on minimally-distanced adversarial examples of forget samples reduces model confidence on those samples while preserving test accuracy. AMUN generates adversarial examples (x_adv, y_adv) as close as possible to each forget sample (x, y) using iterative ε-search. These adversarial pairs naturally belong to the model's learned input distribution, so fine-tuning on them localizes parameter updates to the decision boundary region around each forget sample. This avoids the catastrophic forgetting that occurs when fine-tuning on randomly labeled samples or samples with adversarial labels at larger distances.

### Mechanism 2: Lipschitz Continuity as Transferability Regulator
Decreasing model Lipschitz constant increases individual model robustness but increases adversarial example transferability between models, reducing ensemble robustness. The theoretical analysis shows that for two L-Lipschitz models, the difference in their risk on adversarial examples is bounded by 2Lε plus the original risk difference. Lower L thus makes model behaviors more similar on adversarial perturbations, increasing transferability rate. Empirical results confirm: as layer-wise spectral norm decreases, individual robust accuracy increases but T_rate between models increases.

### Mechanism 3: Inter-Class Geometry for Class Unlearning
Existing class unlearning methods leak membership information through probability distributions on remaining classes; tilting the target distribution using inter-class similarity better approximates retrained model behavior. Retrained models redistribute probability mass from the forgotten class preferentially to semantically similar classes. MIA-NN exploits this by training classifiers on logits of nearest-neighbor classes. TRW tilts the renormalized distribution using similarity scores and temperature to bias toward similar classes, matching the retrained model's structured misclassifications.

## Foundational Learning

- **Concept: Lipschitz Continuity and Spectral Norms**
  - **Why needed here:** Central to understanding why AMUN works (Theorem 4.4.1 bound), why FastClip is developed, and the transferability trade-off in LOTOS. The entire theoretical framework hinges on controlling model smoothness.
  - **Quick check question:** Given a neural network with two linear layers A and B, if ||A||₂ ≤ 1 and ||B||₂ ≤ 1, what can you say about the Lipschitz constant of f(x) = B(A(x))?

- **Concept: Adversarial Examples and Transferability**
  - **Why needed here:** AMUN's core method relies on generating adversarial examples; understanding their properties (closeness to decision boundaries, transferability) is essential to grasp why the method localizes changes and why transferability matters for ensembles (LOTOS).
  - **Quick check question:** If you generate an adversarial example x_adv from model F to fool model G, what conditions make transferability likely (high T_rate per Definition 3.1.2)?

- **Concept: Membership Inference Attacks (MIAs)**
  - **Why needed here:** The primary evaluation metric for unlearning effectiveness. Understanding how MIAs exploit overfitting (confidence differences between training and test data) is crucial to interpret RMIA, MIA-NN, and U-LiRA results that demonstrate AMUN's and TRW's success.
  - **Quick check question:** Why does a retrained model (trained only on D_R) have similar confidence distributions on D_F and D_T, making MIA ineffective, while an imperfectly unlearned model might not?

## Architecture Onboarding

- **Component map:**
  1. AMUN (Section 4): Adversarial example generator (Algorithm 4) + fine-tuning loop with DR (optional) and DA
  2. FastClip (Section 3.3.2): PowerQR (Algorithm 1) for spectrum extraction + Clip (Algorithm 2) for spectral norm clipping
  3. LOTOS (Section 3.3.4): Orthogonalization regularizer S_k^l (Equation 3.3) added to ensemble training loss (Equation 3.4)
  4. TRW (Section 5.1.4): Tilted reweighting distribution q* (Equation 5.2) + fine-tuning with L_forget (Equation 5.3)
  5. Evaluation: RMIA (for random sample unlearning), MIA-NN + U-LiRA (for class unlearning)

- **Critical path for implementation:**
  1. Implement Algorithm 4 (adversarial set construction) with PGD attack
  2. Implement fine-tuning loop with cross-entropy on DA (and DR if available)
  3. For controlled Lipschitz models: integrate FastClip during training
  4. For class unlearning: compute class similarity scores (logit weight PCA + cosine similarity), compute tilted distribution q*, and fine-tune with L_forget
  5. Evaluate using RMIA (for AMUN) or MIA-NN/U-LiRA (for TRW)

- **Design tradeoffs:**
  - **ε size in AMUN:** Smaller ε → tighter localization but harder to find adversarial examples; larger ε → easier attack but wider decision boundary changes
  - **Access to DR:** Having DR improves unlearning effectiveness and test accuracy preservation (Table 4.1 vs. 4.2)
  - **β in TRW:** Higher β → stronger tilt toward similar classes (better MIA-NN robustness) but risks accuracy drop on retained classes (Table C.4)
  - **Number of singular vectors k in LOTOS:** Higher k → more orthogonalization but more computation; Theorem 3.4.3 suggests k=1 suffices for convolutions

- **Failure signatures:**
  - **AMUN:** High FT_AUC (>60%) suggests insufficient unlearning; test accuracy drop >5% suggests ε too large or fine-tuning too long
  - **FastClip:** Spectral norm not converging to target suggests learning rate λ needs adjustment or clipping frequency too low
  - **LOTOS:** Ensemble robust accuracy not improving suggests λ too small or orthogonalization not effective for architecture
  - **TRW:** ACC_r significantly below original model suggests β too high or similarity scores poorly estimated

- **First 3 experiments:**
  1. **Reproduce AMUN baseline on CIFAR-10:** Train ResNet-18, forget 10% random samples, generate adversarial set with PGD-50, fine-tune 10 epochs, evaluate with RMIA. Target: FT_AUC ≈50%, test accuracy preserved.
  2. **Ablate adversarial distance in AMUN:** Compare PGD-50 (small ε) vs. FFGSM (larger ε) using Table B.4 methodology. Hypothesis: Smaller ε yields better unlearning (lower FT_AUC) and preserved test accuracy.
  3. **Implement and evaluate TRW for class unlearning:** For CIFAR-10 automobile class, compute class similarities from logit weights, set β=10, fine-tune with tilted loss, evaluate with MIA-NN. Compare to baselines in Table 5.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a minimal core-set of causally decisive forget samples be constructed to improve unlearning efficiency?
- Basis in paper: [explicit] Chapter 6, "Task A: Causal Core-Set Construction."
- Why unresolved: The author proposes this as future work, suggesting the use of probabilities of necessity and sufficiency (PN/PS) to identify which specific samples within a forget set drive the model's knowledge.
- What evidence would resolve it: An algorithm that identifies a subset C ⊂ D_F such that unlearning C suppresses knowledge of D_F as effectively as unlearning the full set, verified through standard unlearning metrics.

### Open Question 2
- Question: Can adversarial perturbations in latent features effectively remove general concepts from generative diffusion models?
- Basis in paper: [explicit] Chapter 6, "Task B: Concept Unlearning."
- Why unresolved: While AMUN is validated for classification, extending it to generative models is proposed as a future direction. The author hypothesizes that latent adversarial perturbations could drift generation away from forbidden concepts.
- What evidence would resolve it: Successful application of the proposed latent adversarial method to diffusion models, showing high-fidelity erasure of specific concepts (e.g., "Harry Potter") without compromising generation quality for unrelated concepts.

### Open Question 3
- Question: Can sample-dependent similarity functions improve the approximation of retrained model behavior in Tilted ReWeighting (TRW)?
- Basis in paper: [inferred] Section 5.4 (Limitations) states: "The scoring function used in this work is sample-independent; evaluating finer-grained scores that account for individual samples could be an interesting direction for future research."
- Why unresolved: The current TRW uses static inter-class similarity scores (cosine similarity of weight vectors), which may fail to capture nuances for specific samples in the forget class.
- What evidence would resolve it: A modified TRW algorithm utilizing sample-dependent scores that achieves a lower MIA-NN gap or higher similarity to retrained model distributions compared to the static scoring baseline.

## Limitations

- The theoretical guarantees for AMUN depend on strong assumptions about model Lipschitz continuity and adversarial example transferability that may not hold in practice for complex architectures
- The ε-search mechanism for finding adversarial examples assumes successful attack convergence, but the paper does not address failure cases where attacks cannot find nearby adversarial examples
- For class unlearning, TRW's effectiveness relies on accurate estimation of inter-class similarities, but the paper does not extensively validate robustness to noise in these similarity measures

## Confidence

- **High Confidence:** AMUN's core mechanism (fine-tuning on adversarial examples with wrong labels), FastClip's spectral norm clipping procedure, and the LOTOS ensemble robustness framework
- **Medium Confidence:** The theoretical Lipschitz continuity analysis and transferability trade-off, as these depend on specific model assumptions and the empirical validation is limited to particular architectures
- **Low Confidence:** The MIA-NN attack effectiveness and TRW method's generalizability beyond CIFAR-10, as validation is primarily on one dataset with limited ablation studies on similarity measure sensitivity

## Next Checks

1. **Validate AMUN on alternative architectures:** Implement AMUN on VGG19 and DenseNet on CIFAR-10 to test generalizability beyond ResNet-18
2. **Stress-test TRW similarity measures:** Evaluate TRW performance with corrupted class similarity scores (add Gaussian noise) to assess robustness to estimation errors
3. **Test adversarial attack failure modes:** Systematically evaluate AMUN when PGD attacks fail to find adversarial examples within reasonable ε bounds, measuring impact on unlearning effectiveness and test accuracy