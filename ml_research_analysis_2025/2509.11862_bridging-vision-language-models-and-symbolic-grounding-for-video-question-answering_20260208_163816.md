---
ver: rpa2
title: Bridging Vision Language Models and Symbolic Grounding for Video Question Answering
arxiv_id: '2509.11862'
source_url: https://arxiv.org/abs/2509.11862
tags:
- video
- scene
- reasoning
- grounding
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SG-VLM integrates frozen vision language models with symbolic scene
  graph grounding for video question answering. The method uses VLMs to extract object-relation
  structures from video frames, then selects question-relevant graphs to support reasoning.
---

# Bridging Vision Language Models and Symbolic Grounding for Video Question Answering

## Quick Facts
- arXiv ID: 2509.11862
- Source URL: https://arxiv.org/abs/2509.11862
- Authors: Haodi Ma; Vyom Pathak; Daisy Zhe Wang
- Reference count: 40
- Primary result: SG-VLM integrates frozen VLMs with symbolic scene graph grounding, achieving 83.6% on NExT-QA using InternVL-14B, with consistent gains over prior baselines on NExT-QA, iVQA, and ActivityNet-QA.

## Executive Summary
SG-VLM combines frozen Vision Language Models with symbolic scene graph grounding to enhance video question answering. The method extracts object-relation structures from video frames using VLMs, then selects question-relevant graphs to support reasoning. Experiments on NExT-QA, iVQA, and ActivityNet-QA show consistent improvements over prior baselines, with best results (83.6% on NExT-QA) achieved using InternVL-14B. While symbolic grounding enhances causal and temporal reasoning, gains over strong VLMs are limited, highlighting both promise and current constraints of the approach.

## Method Summary
SG-VLM is a 3-stage pipeline that uses frozen Qwen2.5-VL or InternVL to convert video frames into structured scene graphs, then selectively grounds these graphs to enhance reasoning. The method samples frames (default 16), extracts objects and relations via VLM prompting using tools like GroundingDINO and Segment Anything, filters frames relevant to the question, and generates answers by combining selected graphs with original frames. The approach is designed to complement VLMs' holistic reasoning with explicit object-relation structures while maintaining computational efficiency.

## Key Results
- Achieves 83.6% accuracy on NExT-QA using InternVL-14B, outperforming VLM-only baselines
- FrameSel-SG consistently outperforms Full-SG across all datasets by reducing noise from irrelevant frames
- Summary-SG (objects only) often matches or exceeds Full-SG performance when relation extraction is noisy
- Symbolic grounding improves temporal reasoning (+3.3 on iVQA Temporal Current) and descriptive questions (+2.2 on ActivityNet-QA) but hurts causal why questions where VLMs already perform well

## Why This Works (Mechanism)

### Mechanism 1: Structured Symbolic Grounding
VLMs extract subject-relation-object triples (e.g., "orange cat, watching, tabby cat") through prompting, creating interpretable intermediate representations that make spatial and temporal relationships explicit rather than relying solely on learned correlations. This complements VLMs' implicit holistic reasoning, though effectiveness depends on extraction quality.

### Mechanism 2: Question-Aware Frame Selection
The VLM is prompted with each frame and the question to determine relevance before graph extraction. Only frames marked relevant proceed, concentrating symbolic context on potentially useful information and reducing noise from irrelevant frames.

### Mechanism 3: Object-Centric Simplification
Summary-SG extracts dominant objects across frames without edges, reducing error propagation from noisy relation predictions while preserving entity grounding for the VLM. This approach proves more robust when fine-grained relation extraction is unreliable.

## Foundational Learning

- **Concept**: Scene graphs (nodes as objects, edges as spatial/action relations)
  - **Why needed here**: The entire framework converts video frames into these structured symbolic representations for reasoning.
  - **Quick check question**: Given a frame showing "a cat sitting on a fence watching a bird," what nodes and edges would appear in the scene graph?

- **Concept**: Frozen VLM prompting for structured outputs
  - **Why needed here**: SG-VLM extracts scene graphs via prompting rather than training dedicated models—understanding prompt design is critical.
  - **Quick check question**: Why might a frozen VLM produce different scene graph quality than a model specifically fine-tuned for graph extraction?

- **Concept**: Temporal grounding in video understanding
  - **Why needed here**: The paper emphasizes scene graphs must capture temporal dynamics and cross-frame consistency, not just per-frame structure.
  - **Quick check question**: How does a static per-frame scene graph differ from a temporally-grounded video scene graph?

## Architecture Onboarding

**Component map**: Frame sampling -> Object identification -> Interaction identification -> Temporal action tracking -> Query-aware frame selection -> Retrieve associated scene graphs -> Grounded answer generation

**Critical path**: Frame sampling quality -> Object identification accuracy -> Relation extraction reliability -> Question-frame relevance matching -> Final answer generation

**Design tradeoffs**:
- Full-SG vs. FrameSel-SG: Coverage vs. precision (FrameSel wins consistently)
- Summary-SG vs. Full relations: Robustness vs. expressiveness (Summary wins when relations are noisy; Full wins when interactions matter)
- Frame count (m=16 default): More frames capture more content but increase noise and inference time (~30 sec/video)
- Difference-based vs. uniform sampling: Highlights dynamics vs. even temporal coverage

**Failure signatures**:
- On NExT-QA causal questions, symbolic variants underperform VLM-only (relations may conflict with strong VLM priors)
- On ActivityNet-QA (180s average videos), SG extraction degrades; noisy object-relation triples dilute VLM reasoning
- RangeSel-SG (temporal extension) consistently hurts performance—adding neighboring frames introduces spurious objects/actions
- No SG variant consistently beats strong VLM-only baselines across all datasets and question types

**First 3 experiments**:
1. Replicate FrameSel-SG vs. No-SG comparison on a held-out subset of NExT-QA to validate the pipeline and characterize when selection helps vs. hurts
2. Analyze failure cases where Full-SG underperforms Summary-SG to identify which relation types (spatial vs. action-centric) are most error-prone
3. Evaluate the frame selection module in isolation: measure precision/recall of VLM frame-relevance judgments against human annotations of which frames matter per question

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive integration strategies be developed that dynamically decide whether to apply symbolic grounding based on question type, video characteristics, or VLM confidence?
- Basis in paper: [explicit] The conclusion states future work should explore "adaptive integration strategies that dynamically decide when symbolic grounding is beneficial."
- Why unresolved: Results show SG helps for counting (+2.2), temporal next (+3.3), and descriptive questions but hurts on causal why questions where VLMs already perform well. The paper demonstrates symbolic grounding is not universally beneficial.
- What evidence would resolve it: A meta-classifier or gating mechanism that predicts per-question whether SG integration will improve accuracy, validated by consistent improvements over both SG-VLM and VLM-only baselines.

### Open Question 2
- Question: How can relation and action extraction quality be improved to reduce the noise that currently limits symbolic grounding effectiveness?
- Basis in paper: [explicit] The conclusion identifies "improving the quality of relation and action extraction" as a key direction, and results show Summary-SG often outperforms relation-inclusive variants.
- Why unresolved: Object mentions prove more reliable than fine-grained relations—Summary-SG matches or exceeds FrameSel-SG in multiple settings, suggesting noisy edges actively harm performance. Relation extraction via prompting introduces errors that propagate through reasoning.
- What evidence would resolve it: A relation extraction method achieving higher precision on held-out video relation annotations, demonstrating reduced error propagation and consistent gains over Summary-SG across all benchmarks.

### Open Question 3
- Question: What symbolic representations or temporal aggregation mechanisms can better capture long-horizon dependencies in extended videos (e.g., ActivityNet-QA's 180-second average)?
- Basis in paper: [explicit] The conclusion calls for "extending symbolic methods to capture causal and long-horizon dependencies more faithfully."
- Why unresolved: ActivityNet-QA shows the smallest gains; per-frame scene graphs fail to model cross-frame event chains, and RangeSel-SG (extending temporal windows) consistently hurts performance by introducing spurious objects.
- What evidence would resolve it: A temporal scene graph formalism with cross-frame event linking that improves ActivityNet-QA performance over FrameSel-SG, validated on longer video benchmarks with explicit temporal dependency annotations.

## Limitations

- Symbolic grounding effectiveness varies significantly across question types, helping temporal and descriptive questions but hurting causal why questions
- Performance degrades on longer videos (ActivityNet-QA's 180s average) where per-frame scene graphs fail to capture cross-frame event chains
- Relation extraction quality via prompting remains a bottleneck, with noisy edges often degrading rather than enhancing performance
- No symbolic variant consistently outperforms strong VLM-only baselines across all datasets and question types

## Confidence

- **High**: Scene graphs improve temporal reasoning and descriptive question performance (Section 5.4, 5.6)
- **Medium**: Frame selection consistently outperforms using all frames (Section 5.5)
- **Medium**: Object-only summaries are more robust than full scene graphs when relations are noisy (Section 5.5)
- **Low**: The method will generalize to open-ended questions or longer videos without modification (Section 5.6)

## Next Checks

1. Analyze correlation between relation extraction quality and downstream QA performance to quantify how much symbolic grounding improvements depend on VLM prompting accuracy
2. Test the framework on datasets with different question distributions (e.g., more action-centric vs. causal questions) to identify regimes where symbolic grounding helps vs. hurts
3. Evaluate human-annotated scene graphs against VLM-generated ones to measure the gap in structured representation quality