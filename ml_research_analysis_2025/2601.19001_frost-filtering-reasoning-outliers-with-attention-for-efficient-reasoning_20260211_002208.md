---
ver: rpa2
title: 'FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning'
arxiv_id: '2601.19001'
source_url: https://arxiv.org/abs/2601.19001
tags:
- reasoning
- attention
- frost
- arxiv
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FROST introduces an attention-aware method to enhance reasoning\
  \ efficiency by removing reasoning outliers\u2014uncritical steps with low attention\
  \ and entropy\u2014using a modified Softmax1 function. Unlike token-level pruning\
  \ approaches, FROST operates at the sentence level to preserve critical reasoning\
  \ traces while reducing redundancy."
---

# FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning

## Quick Facts
- arXiv ID: 2601.19001
- Source URL: https://arxiv.org/abs/2601.19001
- Reference count: 23
- Key outcome: 69.68% token reduction and 26.70% accuracy improvement via sentence-level attention outlier suppression

## Executive Summary
FROST introduces an attention-aware method to enhance reasoning efficiency by removing reasoning outliers—uncritical steps with low attention and entropy—using a modified Softmax1 function. Unlike token-level pruning approaches, FROST operates at the sentence level to preserve critical reasoning traces while reducing redundancy. Theoretically, it sharpens attention distributions, improving model focus and reasoning capacity. Empirically, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over base models across four benchmarks (Phi-4-Reasoning and GPT-OSS-20B), while reducing attention outlier metrics such as infinity norm and kurtosis.

## Method Summary
FROST modifies transformer attention layers by replacing standard Softmax with Softmax1 (adding 1 to denominator) to create "tail contraction" that suppresses low-attention reasoning traces. The method operates at sentence granularity, pooling token attention weights within each sentence using monotone operators. During fine-tuning, LoRA adapters (rank 8, alpha 16) are applied to attention projection layers on reasoning corpus (OpenR1). At inference, Softmax1 naturally suppresses low-attention sentences without explicit pruning, achieving efficiency gains while preserving critical reasoning steps.

## Key Results
- Average 69.68% reduction in token usage across four benchmarks (GSM8K, MATH500, AIME24, Minerva)
- 26.70% average accuracy improvement over base models on Phi-4-Reasoning and GPT-OSS-20B
- 28.6% inference time reduction and 42.2% training time reduction compared to baselines
- 15.97% reduction in maximum infinity norm and 91.09% reduction in average kurtosis of attention distributions

## Why This Works (Mechanism)

### Mechanism 1: Softmax1 Tail Contraction for Outlier Suppression
- Claim: Replacing standard Softmax with Softmax1 selectively suppresses low-attention weights while preserving high-attention ones.
- Mechanism: Softmax1 adds 1 to the denominator: `Softmax1(xi) = exp(xi) / (sum_j exp(xj) + 1)`. This creates a "tail contraction" property where the relative dominance of outliers contracts at the sentence level, reducing infinity norm and kurtosis of attention distributions.
- Core assumption: Low-attention reasoning traces correspond to uncritical steps that can be safely suppressed without harming reasoning capacity.
- Evidence anchors: [abstract] "FROST introduces an attention-aware method to enhance reasoning efficiency by removing reasoning outliers—uncritical steps with low attention and entropy—using a modified Softmax1 function."

### Mechanism 2: Sentence-Level Attention Pooling Preserves Reasoning Coherence
- Claim: Aggregating token-level attention to sentence level using monotone pooling preserves dominance relationships while enabling coarse-grained outlier detection.
- Mechanism: Token compatibilities within each sentence are pooled via monotone operator ϕ (sum/mean/logsumexp/max). By Lemma 5.1, if tokens in sentence Si dominate tokens in Sj, then the sentence score si ≥ sj, preserving order through Softmax1.
- Core assumption: Reasoning operates meaningfully at sentence granularity; critical information is not distributed across sentence boundaries in ways that pooling destroys.
- Evidence anchors: [section 3.2] "We divide the reasoning process into four components: the question Q, the reasoning steps R1, R2, . . . , Rm, and the final answer A."

### Mechanism 3: Bounded Logit Contribution from Low-Attention Sentences
- Claim: Sentences with low attention probability contribute negligibly to final output logits, enabling effective inference-time skipping.
- Mechanism: Theorem 5.2 shows that for sentence i with αi ≤ ε, its one-layer contribution is bounded by `∥∆ℓi∥ ≤ Bo·Bv·ε`. Across L layers, this compounds to O(Bo·Bv·B^L·ε), meaning low-attention sentences have predictable, bounded impact.
- Core assumption: Operator norms Bo, Bv, B remain bounded and approximately constant across layers.
- Evidence anchors: [section 5] Theorem 5.2: "low-attention sentences are effectively skipped at inference"

## Foundational Learning

- **Softmax variants (Softmax, Sparsemax, Entmax)**: Understanding how Softmax1 differs from alternatives explains why it selectively suppresses tails without aggressive truncation.
  - Quick check: Why does adding 1 to the Softmax denominator create "tail contraction" rather than uniform scaling?

- **Attention weight interpretation in transformers**: The method assumes attention weights correlate with reasoning criticality; understanding attention mechanics is prerequisite.
  - Quick check: In multi-head attention, which layers/heads typically show uniform vs. selective attention patterns?

- **Entropy as criticality signal**: FROST identifies outliers via both low attention and low entropy; understanding entropy-cruciality relationship is essential.
  - Quick check: Why would critical reasoning tokens exhibit higher entropy than non-critical ones?

## Architecture Onboarding

- **Component map**: Input -> Sentence Tokenizer -> Attention Extractor -> Monotone Pooler -> Softmax1 Layer -> LoRA Adapters -> Output

- **Critical path**:
  1. Replace all attention Softmax layers with Softmax1
  2. Fine-tune on reasoning corpus (OpenR1) with LoRA for ~5000 steps
  3. At inference, Softmax1 naturally suppresses low-attention sentences without explicit pruning

- **Design tradeoffs**:
  - **Softmax1 vs Sparsemax/Entmax15**: Table 2 shows Softmax1 achieves best accuracy-token tradeoff; Sparsemax/Entmax15 may over-truncate critical traces
  - **LoRA rank 8 vs higher**: Paper uses rank 8; lower may underfit, higher increases training cost
  - **Sentence vs token granularity**: Sentence-level preserves coherence but may miss sub-sentence redundancy

- **Failure signatures**:
  - Accuracy drops >5%: Likely over-suppression of critical traces; check if high-entropy sentences are being suppressed
  - No token reduction: Softmax1 may not be applied correctly; verify layer replacement
  - Training instability: Gradient variance from tail contraction; reduce learning rate to <1e-5

- **First 3 experiments**:
  1. **Ablation on pooling functions**: Compare sum/mean/max/logsumexp pooling on GSM8K validation to identify best ϕ.
  2. **Layer-wise Softmax1 injection**: Apply Softmax1 only to layers 30-40 vs. all layers; measure accuracy/efficiency tradeoff.
  3. **Entropy threshold sweep**: Vary the implicit entropy threshold for outlier detection by analyzing sentence entropy distributions pre/post FROST on AIME24.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FROST's attention-based outlier removal mechanism be effectively extended to non-mathematical reasoning domains such as code generation, logical deduction, and scientific question answering?
- Basis in paper: [explicit] The authors state in Section 7: "our method is currently restricted to mathematical reasoning tasks, while many reasoning models also target domains such as coding."
- Why unresolved: The paper only validates FROST on mathematical benchmarks (GSM8K, MATH500, AIME24, Minerva). While initial generalization results on coding tasks (Table 4) show promise, the authors have not systematically tested whether the relationship between low attention weights and reasoning outliers holds across fundamentally different reasoning paradigms.

### Open Question 2
- Question: Would integrating FROST with reinforcement learning approaches such as GRPO yield further improvements in reasoning efficiency, and how would the two methods interact?
- Basis in paper: [explicit] The authors note in Section 7: "FROST relies solely on supervised fine-tuning and does not incorporate GRPO, which could further enhance efficiency. In future work, we plan to...develop a GRPO-based approach."
- Why unresolved: The current implementation only uses SFT with LoRA. RL methods like GRPO could potentially learn more sophisticated reward functions that better balance accuracy versus token efficiency, but the interaction between attention-based outlier suppression and RL reward shaping is unexplored.

### Open Question 3
- Question: Can the 8% of incorrectly removed critical reasoning traces (identified in human evaluation) be rescued through more sophisticated attention analysis or hybrid approaches combining attention with other criticality signals?
- Basis in paper: [inferred] Section J reports that "8% of reasoning traces are incorrectly removed, which significantly degrades final-answer accuracy. These mistakenly pruned traces are typically long and contain repeated information that supports self-verification and error correction. However, they also provide critical content—such as key equations—in the end of trace."
- Why unresolved: These traces have characteristics (repetition, length) that correlate with non-critical content but contain essential information. Pure attention-based filtering cannot distinguish them. The paper does not explore combining attention with entropy, positional information, or semantic analysis.

## Limitations

- Sentence-level pooling may not preserve critical information distributed across sentence boundaries or requiring sub-sentence granularity
- The method's effectiveness on non-mathematical reasoning tasks remains unproven, limiting generalizability
- 8% of critical reasoning traces are incorrectly removed, representing a fundamental limitation of pure attention-based outlier detection

## Confidence

**High Confidence**: The empirical efficiency gains (69.68% token reduction, 28.6% inference time reduction) are well-supported by Table 2 and Table 3, with consistent results across four benchmarks and two model families.

**Medium Confidence**: The theoretical guarantees around Softmax1's tail contraction properties and bounded logit contributions are mathematically sound within the stated assumptions, but the practical implications depend on real-world attention distributions matching the idealized assumptions.

**Low Confidence**: The generalizability to non-mathematical reasoning tasks is untested, as all evaluations focus on mathematical reasoning benchmarks.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate FROST on non-mathematical reasoning tasks (common sense reasoning, code generation, or scientific reasoning) to verify the attention-outlier hypothesis extends beyond mathematical domains where it was developed.

2. **Layer-wise Sensitivity Analysis**: Systematically test FROST with Softmax1 applied to different layer subsets (early, middle, late, all layers) to identify optimal injection points and understand layer-specific attention outlier patterns.

3. **Critical Step Preservation Verification**: Design controlled experiments where specific critical reasoning steps are artificially assigned low attention weights to test whether FROST's sentence-level pooling and Softmax1 modification reliably preserve truly essential reasoning components.