---
ver: rpa2
title: Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural
  Networks
arxiv_id: '2510.03878'
source_url: https://arxiv.org/abs/2510.03878
tags:
- cancer
- oral
- clinical
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-modal deep learning framework for oral
  cancer detection, integrating clinical, radiological, and histopathological images
  using a weighted ensemble of DenseNet-121 CNNs. Each modality-specific model was
  trained via transfer learning with modality-specific augmentation, and ensemble
  weights were assigned based on validation accuracy.
---

# Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2510.03878
- Source URL: https://arxiv.org/abs/2510.03878
- Reference count: 30
- Primary result: Multi-modal deep learning framework for oral cancer detection achieving 84.58% overall accuracy using weighted ensemble of DenseNet-121 CNNs

## Executive Summary
This study presents a multi-modal deep learning framework for oral cancer detection, integrating clinical, radiological, and histopathological images using a weighted ensemble of DenseNet-121 CNNs. Each modality-specific model was trained via transfer learning with modality-specific augmentation, and ensemble weights were assigned based on validation accuracy. The approach achieved high validation accuracy for radiological (100%) and histopathological (95.12%) modalities, with clinical images performing lower (63.10%) due to visual heterogeneity. The ensemble model demonstrated improved diagnostic robustness, achieving an overall accuracy of 84.58% on a multimodal validation dataset of 55 samples. The framework offers a non-invasive, AI-assisted triage tool that enhances early identification of high-risk lesions, supporting clinicians in decision-making and aligning with global oncology guidelines to reduce diagnostic delays and improve patient outcomes.

## Method Summary
The framework trains three independent DenseNet-121 models via transfer learning on modality-specific datasets (clinical, radiological, histopathological), each with custom classification heads and modality-appropriate data augmentation. Models are evaluated individually on validation sets, then combined using a weighted average ensemble where weights are determined by each model's validation accuracy (approximately 24.4% for clinical, 38.7% for radiological, 36.8% for histopathological). The final classification decision is produced by applying a threshold to the weighted sum of the three model outputs.

## Key Results
- Overall ensemble accuracy: 84.58% on 55 multimodal validation samples
- Individual modality performance: Radiological 100%, Histopathology 95.12%, Clinical 63.10% validation accuracy
- Ensemble weights based on validation accuracy: 24.43% clinical, 38.72% radiological, 36.83% histopathological
- Framework provides non-invasive AI-assisted triage tool for early oral cancer detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent DenseNet-121 models trained via transfer learning on each imaging modality capture domain-specific diagnostic features that contribute uniquely to the final prediction.
- Mechanism: The system trains three separate models. Each DenseNet-121 uses weights pre-trained on ImageNet, which are then fine-tuned on a specific modality (clinical, radiological, or histopathological). This allows the convolutional base to adapt its general feature detectors (edges, textures) to the specialized visual patterns of each medical image type, from photographic lesion surfaces to CT bone structures and cellular microscopy.
- Core assumption: Features learned from natural images (ImageNet) provide a useful initialization for medical image analysis. Assumption: Each modality provides complementary diagnostic information.
- Evidence anchors:
  - [abstract] "Each modality-specific dataset was used to train a DenseNet-121 CNN via transfer learning."
  - [section 3.2] "Each model is specifically trained to extract discriminative features from its respective input domain, enabling modality-specific learning and prediction through transfer learning."
  - [corpus] Related work on improving oral cancer outcomes with ML (arXiv:2506.10189) supports the use of ML for feature extraction, but no direct corpus evidence validates this specific multi-modal transfer learning setup.
- Break condition: Performance degrades if the pre-trained features are not adaptable or if a modality's dataset is too small or heterogeneous for effective fine-tuning, as seen in the lower accuracy of the clinical model (63.10%).

### Mechanism 2
- Claim: A weighted average ensemble, where weights are assigned based on individual model validation accuracy, improves overall diagnostic robustness by prioritizing more reliable modalities.
- Mechanism: The final classification is not produced by a single model but by a weighted sum of the outputs from all three modality-specific models. Weights are fixed post-training based on validation performance (e.g., radiological at 38.72%, clinical at 24.43%). This acts as a simple form of reliability gating, giving more influence to models that have demonstrated better generalization.
- Core assumption: Validation accuracy is a stable and reliable proxy for a model's real-world diagnostic trustworthiness.
- Evidence anchors:
  - [abstract] "ensemble weights were assigned based on validation accuracy."
  - [section 5.1] "weights were assigned based on each model’s validation accuracy, thereby distributing weights of 24.43% for the clinical model, 38.72% for the radiological model and 36.83% for the histopathology model."
  - [corpus] The principle of ensembling for improved accuracy is supported by related work on diffusion models (arXiv:2504.00026), but corpus evidence does not validate this specific validation-weighted strategy.
- Break condition: The weighting scheme fails if validation accuracy is a poor predictor of true generalization, for instance, due to data leakage or an unrepresentative validation split.

### Mechanism 3
- Claim: Modality-specific data augmentation regularizes the models and forces them to learn features invariant to non-diagnostic variations.
- Mechanism: Different transformations are applied to each image type during training. Clinical images get rotations and flips to handle variable camera/patient angles. Radiological images get only horizontal flips to preserve anatomical orientation. This exposes the model to a wider range of valid inputs, preventing it from memorizing spurious correlations (like a consistent background) and forcing it to focus on the diagnostic content.
- Core assumption: The chosen transformations preserve the diagnostic label while altering irrelevant features.
- Evidence anchors:
  - [section 3.1] "Data augmentation was employed to increase the diversity of the training data and reduce the likelihood of overfitting."
  - [section 3.1] "Radiological images were augmented through horizontal flipping only, in order to preserve the diagnostic structure of the anatomical features."
  - [corpus] No direct corpus evidence for this specific augmentation strategy.
- Break condition: Augmentation fails if it destroys diagnostic features or introduces unrealistic artifacts that the model learns instead of the true pathology.

## Foundational Learning

### Transfer Learning
- Why needed here: The entire architecture is built on DenseNet-121 models pre-trained on ImageNet. Understanding why and how weights from a general dataset can be adapted to specialized medical images is fundamental.
- Quick check question: What is the primary benefit of using transfer learning for the relatively small modality-specific datasets in this study?

### Convolutional Neural Network (CNN) Feature Hierarchies
- Why needed here: The system's core function is to automatically learn features from raw pixels. Grasping that early layers detect simple structures (edges) and later layers detect complex diagnostic patterns (lesion shapes, tissue textures) is key to understanding the model's decisions.
- Quick check question: In this architecture, which layers would you expect to be most actively modified during fine-tuning for histopathology images: the initial edge detectors or the final dense layers?

### Model Ensembling and Late Fusion
- Why needed here: The paper's main contribution is combining multiple models. Distinguishing this "late fusion" approach (combining final predictions) from "early fusion" (combining raw data) is critical for evaluating its strengths and limitations.
- Quick check question: In the described ensemble, are the fusion weights learned jointly during training or calculated separately after training? What is a potential weakness of this approach?

## Architecture Onboarding

### Component map
Data Preprocessing Pipeline -> Modality-Specific Models -> Weighted Ensemble Layer

### Critical path
Data Quality & Heterogeneity -> Modality-Specific Preprocessing -> Transfer Learning Fine-Tuning -> Individual Validation Accuracy -> Ensemble Weighting -> Final Accuracy (84.58%). The model's performance is most sensitive to the quality and consistency of the input data for each modality.

### Design tradeoffs
- **Late vs. Early Fusion**: The authors used late fusion for simplicity and interpretability. Tradeoff: This prevents the model from learning potential cross-modal features (e.g., how a visual lesion texture correlates with a radiological bone pattern) that early or intermediate fusion might capture.
- **Fixed Weighting vs. Learnable Fusion**: Weights are based on validation accuracy and are static. Tradeoff: This is simple and transparent, but a learnable fusion layer (e.g., a meta-learner) could potentially weight modalities differently based on the specific characteristics of each input instance.

### Failure signatures
- **Clinical Model Overfitting**: Training accuracy (80.16%) far exceeds validation accuracy (63.10%). Signature: A large and growing gap between training and validation loss/accuracy curves.
- **Unrealistic Perfection**: The radiological model achieved 100% accuracy. Signature: This can indicate a dataset that is too simple, too small, or potentially subject to data leakage, rather than genuine perfect performance.
- **Sensitivity to Missing Data**: The model requires all three modalities for a final prediction. Signature: The system is not robust to cases where a patient has only one or two of the required image types available.

### First 3 experiments
1. **Ablation Study on Fusion Strategy**: Compare the current fixed, accuracy-weighted fusion against an unweighted average and a learnable, end-to-end trained fusion layer to see if more sophisticated combination methods improve performance.
2. **Analyze Robustness to Missing Modalities**: Systematically test the ensemble by removing one modality at a time (e.g., only clinical + histopathology) to quantify the marginal contribution of each and test real-world robustness.
3. **Cross-Modality Feature Visualization**: Use a technique like Grad-CAM to visualize the image regions that each modality-specific model focuses on for its prediction, helping to explain the clinical model's lower performance and verify that the models are learning from diagnostically relevant areas.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble model perform when one or more modalities are unavailable at inference time?
- Basis in paper: [explicit] The authors state the ensemble "may be sensitive to missing modality inputs and requires complete data for all three streams, which may not always be feasible in clinical scenarios."
- Why unresolved: The current framework assumes tri-modal input availability, a constraint rarely met in real-world clinical workflows.
- What evidence would resolve it: Ablation experiments showing performance degradation curves when individual modalities are systematically omitted.

### Open Question 2
- Question: Does patient-level matched multi-modal data improve ensemble performance compared to the current unmatched dataset approach?
- Basis in paper: [explicit] The authors acknowledge "the unavailability of matched patient data across all three modalities" as a key limitation that may hinder real-world deployment effectiveness.
- Why unresolved: Current validation uses images from different patients across modalities, potentially misrepresenting true clinical fusion benefits.
- What evidence would resolve it: Comparative evaluation on a cohort where clinical, radiological, and histopathological images originate from the same patients.

### Open Question 3
- Question: Can the substantial performance gap in clinical image classification (63.10% vs 95–100% for other modalities) be mitigated through advanced fusion strategies or improved augmentation?
- Basis in paper: [inferred] The 36.9 percentage point gap between clinical and radiological validation accuracy suggests validation-weighted fusion may not optimally handle heterogeneous modality reliability.
- Why unresolved: The paper attributes lower clinical performance to visual heterogeneity but does not explore adaptive weighting or attention-based fusion mechanisms.
- What evidence would resolve it: Experiments comparing validation-weighted fusion against learned attention-based or uncertainty-weighted fusion strategies on the same dataset.

## Limitations
- The 100% accuracy on radiological images appears suspiciously high and may indicate data leakage or an unrepresentative validation set
- The methodology for creating the 55 multimodal validation samples is unclear, raising concerns about whether the ensemble evaluation truly reflects real-world performance
- Training hyperparameters (epochs, learning rate, dropout rates) are not specified, making exact reproduction difficult

## Confidence

**High Confidence:** The overall concept of using transfer learning with DenseNet-121 and ensemble methods for multi-modal medical image classification is well-established and technically sound.

**Medium Confidence:** The reported validation accuracies for individual modalities (63.10% clinical, 95.12% histopathology) appear reasonable given the stated data heterogeneity challenges.

**Low Confidence:** The 100% radiological accuracy claim and the specific methodology for multimodal validation sample creation require independent verification.

## Next Checks
1. **Independent Dataset Test:** Apply the trained ensemble to an external, completely separate oral cancer dataset to verify generalization beyond the original validation set.
2. **Cross-Modality Feature Analysis:** Use Grad-CAM visualization to confirm that each modality-specific model is focusing on diagnostically relevant regions rather than background features.
3. **Ablation Study on Fusion Strategy:** Compare the current accuracy-weighted ensemble against simpler averaging and learnable fusion methods to quantify the actual benefit of the weighted approach.