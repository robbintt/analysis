---
ver: rpa2
title: 'Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity'
arxiv_id: '2507.22570'
source_url: https://arxiv.org/abs/2507.22570
tags:
- monotone
- matrices
- matrix
- accuracy
- entry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study combines deep learning with explainability techniques\
  \ to derive simple, interpretable criteria for classifying monotone matrices\u2014\
  those whose inverses are entrywise nonnegative. Starting with uniformly sampled\
  \ random matrices, a high-performance neural network is trained and then analyzed\
  \ using Integrated Gradients, decision-tree surrogates, and symbolic regression."
---

# Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity

## Quick Facts
- arXiv ID: 2507.22570
- Source URL: https://arxiv.org/abs/2507.22570
- Authors: Leandro Farina; Sergey Korotov
- Reference count: 23
- Primary result: Deep learning + XAI reveals two characteristic polynomial coefficients (|c0|, |c1|) suffice for 95% accurate classification of monotone matrices

## Executive Summary
This study demonstrates how deep learning combined with explainability techniques can derive simple, interpretable criteria for classifying monotone matrices—those with entrywise nonnegative inverses. Starting with uniformly sampled random matrices, a high-performance neural network is trained and then analyzed using Integrated Gradients, decision-tree surrogates, and symbolic regression. The analysis reveals that classification accuracy exceeding 95% depends primarily on two low-order coefficients of the characteristic polynomial, |c0| and |c1|. Further Extreme Value Theory analysis supports a data-driven necessary condition for monotonicity, providing a bridge between AI-driven insights and human-interpretable algebraic rules for complex matrix classification problems.

## Method Summary
The method involves generating balanced datasets of 7×7 matrices from U(-1,1) via rejection sampling, computing 73 features (49 raw entries plus 24 derived features including eigenvalues, trace, spectral radius, and characteristic polynomial coefficients |c_k|), training hybrid feed-forward networks (2056→1024→512→256→128→64→32 with L2 regularization and dropout), applying Integrated Gradients for feature attribution, and validating findings through decision trees and symbolic regression. The approach combines supervised learning with post-hoc explainability to extract interpretable rules from black-box models.

## Key Results
- Two characteristic polynomial coefficients (|c0|, |c1|) alone achieve 95.1% classification accuracy for 7×7 monotone matrices
- Integrated Gradients reliably identifies these minimal sufficient features from 73-dimensional input space
- Extreme Value Theory analysis suggests |c0/c1| ≤ 0.18 for 7×7 monotone matrices (equivalent to tr(A⁻¹) ≥ 5.7)
- Subdomain analysis reveals feature importance shifts when |c0/c1| < 0.08, where trace and raw entries become more influential

## Why This Works (Mechanism)

### Mechanism 1: Characteristic Polynomial Coefficient Dominance
For monotone matrices, c0 = (-1)^n det(A) and c1 = (-1)^(n-1) det(A) tr(A⁻¹), so |c0/c1| = 1/tr(A⁻¹). Since monotone A has A⁻¹ ≥ 0 with positive trace, the ratio is bounded by ~0.18 (equivalent to tr(A⁻¹) ≥ 5.7), creating a low-dimensional decision boundary. This mechanism assumes uniform sampling adequately represents monotone matrix distribution.

### Mechanism 2: Integrated Gradients Attribution for Feature Reduction
IG integrates partial derivatives ∂F/∂x_i along a path from baseline to input, satisfying completeness (∑ IG_i = F(x) - F(baseline)). For the 73-feature hybrid FFN, IG assigned dominant scores to |c0| and |c1|, revealing the network internally collapses to this 2D subspace despite access to eigenvalues, trace, and raw entries.

### Mechanism 3: Extreme Value Theory for Necessary Condition Estimation
Peaks-Over-Threshold analysis models the distribution of exceedances above threshold u via Generalized Pareto Distribution. Negative shape parameter ξ < 0 indicates a bounded tail. For n=7, fitted ξ = -0.028 ± 0.006, supporting a finite upper bound estimated at ~0.828, with negligible probability of exceeding the empirical maximum 0.1755.

## Foundational Learning

- **Concept: Monotone Matrix (Inverse-Nonnegative Matrix)**
  - Why needed here: The core classification target; defined by A⁻¹ ≥ 0 (entrywise nonnegative inverse). Essential to understand that this property has no simple characterization in terms of raw entries, motivating the ML approach.
  - Quick check question: Given a 3×3 matrix A = [[2, -1, 0], [-1, 2, -1], [0, -1, 2]], compute A⁻¹ and verify it is entrywise nonnegative (it is an M-matrix, hence monotone).

- **Concept: Characteristic Polynomial Coefficients and Their Algebraic Meaning**
  - Why needed here: The paper's core finding is that |c0| = |det(A)| and |c1| = |det(A)| · tr(A⁻¹) are the key features. Understanding c0 as the determinant magnitude and c1 as related to the adjugate trace is essential.
  - Quick check question: For A = [[1, 2], [3, 4]], compute det(A) and the adjugate adj(A). Verify that c0 = det(λI - A)|_{λ=0} = det(-A) = 2, and c1 = coefficient of λ = -tr(adj(-A)) = -5.

- **Concept: Integrated Gradients (Saliency Method)**
  - Why needed here: The paper uses IG to identify |c0| and |c1| as dominant features; understanding how IG differs from simple gradients (path integration, completeness axiom) is necessary to evaluate its reliability.
  - Quick check question: For a simple function f(x) = max(0, x), with baseline x' = 0 and input x = 3, compute the integrated gradient IG(x) and verify it equals 1 (the feature is fully important for the output increase).

## Architecture Onboarding

- **Component map:** Data Generation -> Feature Extraction -> Hybrid FFN -> XAI Layer (IG) -> Surrogate Models -> Validation Layer (SVM, EVT)

- **Critical path:** 1. Train hybrid FFN on 73D features → achieve ~99% accuracy 2. Apply IG to identify top features → |c0|, |c1| dominate 3. Retrain with reduced feature sets (2D → 95.1%, 1D → 93.5%) 4. Extract rules via decision tree and EVT

- **Design tradeoffs:** 73D vs. 2D input: 73D yields 99% accuracy but overfits; 2D yields 95% with better generalization. FFN vs. CNN: FFN (96.9%) outperforms CNN (93.2%) on raw entries. Decision tree depth: Depth 4 provides interpretable rules but may miss non-linear interactions.

- **Failure signatures:** Class overlap at low |c0/c1| (even with ratio < 0.08, ~4% misclassified); dimension-specific EVT failure (positive ξ for n=5, n=10); sigmoid kernel SVM failure (59.1% accuracy).

- **First 3 experiments:** 1. Replicate two-feature FFN on |c0|, |c1| alone for n=7; target 95% accuracy. 2. Apply n=7 threshold (|c0/c1| ≤ 0.18) to n=5 and n=10 datasets; measure precision/recall degradation. 3. Filter to |c0/c1| < 0.08, train FFN on remaining 73 features; compare feature salience shift.

## Open Questions the Paper Calls Out

- Do the minimal features |c0| and |c1| remain the dominant classifiers for structured matrix subclasses (e.g., M-matrices, Stieltjes matrices), or do new invariants emerge?
- How does the empirical threshold |c0/c1| shift under alternative entry distributions, such as heavy-tailed priors or specific sign patterns?
- Does the ratio |c0/c1| possess a finite upper endpoint for 7×7 monotone matrices, and what is its exact theoretical value?

## Limitations

- The boundedness conjecture (|c0/c1| ≤ 0.18) is dimension-specific, failing for n=5 and n=10 where EVT shows heavy tails
- The rejection sampling approach may introduce bias, as uniformly sampled matrices have low probability of being monotone
- IG-based feature selection assumes saliency captures true causal importance rather than spurious correlations

## Confidence

- **High confidence:** The empirical finding that |c0| and |c1| achieve 95% classification accuracy for n=7 monotone matrices; the hybrid FFN architecture and training procedure
- **Medium confidence:** The Extreme Value Theory analysis and finite upper bound estimation; the subdomain classification showing feature importance shifts at low |c0/c1| ratios
- **Low confidence:** Generalization of the boundedness conjecture to other matrix dimensions; the assumption that IG attributions represent true feature causality

## Next Checks

1. **Cross-dimension EVT validation:** Apply the n=7 boundedness condition to n=5 and n=10 datasets to quantify the dimension-specificity of the ratio constraint
2. **Alternative attribution comparison:** Replicate IG feature ranking using SHAP and LIME to verify consistency of |c0|/|c1| dominance across methods
3. **Structured matrix sampling:** Generate monotone matrices from non-uniform distributions (e.g., M-matrices with specific sparsity patterns) to test robustness of the 2-feature classification rule