---
ver: rpa2
title: 'Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning'
arxiv_id: '2505.05237'
source_url: https://arxiv.org/abs/2505.05237
tags:
- tabular
- knowledge
- learning
- data
- latte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Latte, a training-time knowledge extraction
  framework that transfers latent-level knowledge from Large Language Models (LLMs)
  to improve few-shot tabular learning. Unlike existing approaches that rely on test-time
  extraction or text-level knowledge, Latte uses a semantic-aware tabular encoder
  and knowledge adapter to integrate LLM-derived insights into the learning process.
---

# Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning

## Quick Facts
- arXiv ID: 2505.05237
- Source URL: https://arxiv.org/abs/2505.05237
- Reference count: 7
- This paper introduces Latte, a training-time knowledge extraction framework that transfers latent-level knowledge from Large Language Models (LLMs) to improve few-shot tabular learning, achieving up to 4.22% improvement over state-of-the-art baselines.

## Executive Summary
Latte addresses the challenge of few-shot tabular learning by transferring latent-level knowledge from LLMs to a tabular encoder, avoiding the inefficiencies and unreliability of test-time extraction or text-level knowledge. The framework uses a semantic-aware tabular encoder and knowledge adapter to integrate LLM-derived insights during training, combined with unsupervised pre-training on unlabeled data to reduce overfitting. Experiments across multiple datasets demonstrate Latte's superior performance and efficiency compared to existing approaches.

## Method Summary
Latte employs a two-stage training process: first, it uses unsupervised pre-training with pseudo-labels generated from clustering unlabeled data, guided by LLM knowledge to create robust feature representations. Then, it fine-tunes on the few labeled examples using the distilled knowledge. The framework extracts latent knowledge from an LLM by prompting with metadata and averaging hidden states from a specific transformer layer, then uses a knowledge adapter with cross-attention to align this knowledge with tabular feature embeddings. A semantic-aware encoder processes categorical and numerical features using BERT embeddings combined with a lightweight transformer.

## Key Results
- Achieves up to 4.22% improvement in classification tasks over state-of-the-art baselines
- Demonstrates superior efficiency by eliminating LLM invocation costs during inference
- Shows consistent performance gains across multiple datasets in few-shot settings

## Why This Works (Mechanism)

### Mechanism 1: Latent-Level Knowledge Distillation
- **Claim**: Transferring latent hidden states from LLMs provides more reliable task-relevant semantics than prompting for textual rules.
- **Mechanism**: The framework hooks the final transformer layer's hidden states from an LLM prompted with metadata, rather than sampling text.
- **Core assumption**: The hidden states of the LLM contain a denser, less noisy representation of task-relevant relationships than the generated output text.
- **Evidence anchors**: [Section 1] notes that text-level knowledge "often suffers from hallucinations" and cites work showing latent states are "more informative and discriminative."

### Mechanism 2: Semantic-Guided Feature Fusion
- **Claim**: Explicitly aligning LLM latent space with a tabular encoder via a knowledge adapter improves the weighting of feature interactions.
- **Mechanism**: A "Knowledge Adapter" uses a learned query vector (distilled from the LLM) to attend over feature embeddings.
- **Core assumption**: The semantic space of the LLM can be linearly projected and fused to align with the BERT-based tabular encoder space.
- **Evidence anchors**: [Section 3.4] details the Knowledge Adapter using $q_{LLM}$ to compute attention over feature representations $K$ and $V$.

### Mechanism 3: Pseudo-Supervised Meta-Pretraining
- **Claim**: Initializing the model using meta-learning on clustered unlabeled data prevents overfitting to the tiny labeled set.
- **Mechanism**: The model first generates pseudo-labels for unlabeled data via clustering. It then performs meta-learning using these pseudo-tasks, guided by the LLM knowledge.
- **Core assumption**: The cluster structure of the unlabeled data correlates sufficiently with the true class boundaries of the labeled data.
- **Evidence anchors**: [Section 3.5] describes the "unsupervised pre-training stage" using cluster pseudo-labels to generate N-way K-shot tasks.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here**: You must understand how to transfer "soft" knowledge (hidden states/logits) from a large teacher model to a smaller student, distinct from standard fine-tuning.
  - **Quick check question**: How does minimizing KL-Divergence between the teacher's hidden state and the student's query vector differ from standard cross-entropy loss on labels?

- **Concept: Attention Mechanisms (Query-Key-Value)**
  - **Why needed here**: The core architecture relies on cross-attention where the "Query" comes from LLM knowledge and "Key/Value" come from tabular features.
  - **Quick check question**: In the Knowledge Adapter, does the LLM knowledge act as the Query or the Key, and what does that imply for how it influences the tabular features?

- **Concept: Meta-Learning**
  - **Why needed here**: The pre-training stage uses an "N-way K-shot" episodic training setup on pseudo-labeled tasks.
  - **Quick check question**: Why is episodic training on pseudo-tasks preferred over standard supervised training on the pseudo-labels directly?

## Architecture Onboarding

- **Component map**: Teacher (Frozen LLM) -> Student Encoder (BERT + Transformer) -> Adapter (GTransformer) -> Predictor (MLP)

- **Critical path**:
  1. **Metadata Extraction**: Prompt engineering to extract $h_M$ (run once)
  2. **Stage I (Pre-training)**: Cluster $D_u$ -> Generate episodes -> Train student with Adapter using $h_M$ and pseudo-labels
  3. **Stage II (Fine-tuning)**: Train student on $D_l$ (few shots) using $h_M$ and true labels

- **Design tradeoffs**:
  - **Efficiency vs. Latency**: The paper trades a heavier *training* pipeline (meta-learning + LLM extraction) for zero *inference* latency (no LLM calls at test time)
  - **LLM Scale**: Ablation studies suggest smaller LLMs (7B) may suffice, as 13B models showed marginal or inconsistent gains

- **Failure signatures**:
  - **Semantic Mismatch**: If visualization shows low similarity between learned embeddings and LLM keywords, the adapter initialization or projection matrix $W_0$ may be failing to align spaces
  - **Overfitting to Noise**: If validation loss diverges quickly in Stage I, the cluster-based pseudo-labels may be too noisy

- **First 3 experiments**:
  1. **Sanity Check (No LLM)**: Run the pipeline with the LLM knowledge vector zeroed out to establish a baseline for the tabular encoder alone
  2. **Ablation (Latent vs. Text)**: Compare performance when replacing the latent vector $h_M$ with an embedding of the LLM's generated text response to verify Mechanism 1
  3. **Shot Sensitivity**: Sweep $k \in \{4, 8, 16\}$ labeled samples to verify the model's stability in extreme low-data regimes compared to standard XGBoost

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the optimal selection of LLM activation layers for knowledge extraction depend fundamentally on the task type (classification vs. regression)?
- **Basis in paper**: [explicit] Section 4.4 notes that while classification performance improves with deeper layers, regression performance peaks around layer 30, suggesting that deeper layers may "introduce noise" for numerical prediction tasks.

### Open Question 2
- **Question**: Is there a "knowledge saturation" point where increasing the LLM parameter count no longer benefits few-shot tabular learning?
- **Basis in paper**: [explicit] Section 4.3 ablation study on model size shows that increasing the LLM size from 7B to 13B did not result in significant performance gains, leading the authors to hypothesize that the 7B model already contains "all the necessary task-related semantic knowledge."

### Open Question 3
- **Question**: How robust is Latte when the textual metadata (feature descriptions) is noisy, ambiguous, or missing?
- **Basis in paper**: [inferred] The methodology explicitly relies on metadata $M=\{F, T\}$ to craft prompts for extracting task-relevant knowledge (Section 3.3).

### Open Question 4
- **Question**: Is the efficiency of Latte's knowledge transfer consistent across different LLM architectures (e.g., Encoder-only vs. Decoder-only)?
- **Basis in paper**: [explicit] The implementation details state "We obtain task-relevant knowledge from the LLaMA2 series models," leaving the performance of other architectures unexplored.

## Limitations
- **Unknown Prompt Engineering**: The framework's success hinges on the LLM extracting task-relevant knowledge from metadata, but the paper does not report results from alternative prompt structures.
- **Hidden State Selection**: The paper uses the 30th layer of LLaMA2 without exploring whether this is optimal or if performance is robust to layer selection.
- **Dataset Distribution Shift**: The pseudo-supervised pre-training stage assumes the unlabeled data distribution aligns with labeled data, which may not hold in practice.

## Confidence
- **Latent-Level Knowledge Distillation Outperforms Text-Level**: High Confidence
- **Semantic-Guided Feature Fusion Improves Performance**: Medium Confidence
- **Pseudo-Supervised Meta-Pretraining Prevents Overfitting**: Medium Confidence
- **Scalability and Efficiency**: Medium Confidence

## Next Checks
1. **Prompt Engineering Robustness**: Systematically vary the prompt template used to extract LLM latent knowledge and measure the impact on downstream performance.
2. **Layer Sensitivity Analysis**: Repeat the experiments using latent knowledge extracted from different layers of the LLM to determine if the 30th layer is optimal.
3. **Distribution Shift Stress Test**: Create an evaluation protocol where the unlabeled data used for pseudo-supervised pre-training is deliberately sampled from a different distribution than the labeled data.