---
ver: rpa2
title: Robust Graph-Based Semi-Supervised Learning via $p$-Conductances
arxiv_id: '2502.08873'
source_url: https://arxiv.org/abs/2502.08873
tags:
- learning
- which
- labels
- p-conductance
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces p-conductance learning, a robust graph-based
  semi-supervised learning method designed for scenarios with scarce or corrupted
  labels. The method generalizes both Poisson and p-Laplace learning by introducing
  an objective function reminiscent of p-Laplacian regularization and an affine relaxation
  of label constraints.
---

# Robust Graph-Based Semi-Supervised Learning via $p$-Conductances

## Quick Facts
- arXiv ID: 2502.08873
- Source URL: https://arxiv.org/abs/2502.08873
- Authors: Sawyer Jack Robertson; Chester Holtz; Zhengchao Wan; Gal Mishne; Alexander Cloninger
- Reference count: 40
- Primary result: Introduces p-conductance learning method that achieves state-of-the-art accuracy in low label-rate and corrupted-label regimes for graph-based semi-supervised learning.

## Executive Summary
This paper introduces p-conductance learning, a robust graph-based semi-supervised learning method designed for scenarios with scarce or corrupted labels. The method generalizes both Poisson and p-Laplace learning by introducing an objective function reminiscent of p-Laplacian regularization and an affine relaxation of label constraints. This leads to a family of probability measure mincut programs that balance sparse edge removal with accurate distribution separation. The approach provides significant improvements over existing methods across multiple benchmark datasets in challenging low-label and corrupted-label settings.

## Method Summary
The method introduces a p-conductance objective function that generalizes p-Laplacian regularization while relaxing hard label constraints into soft measure constraints. It constructs probability measures for each class and solves a measure mincut program using a semismooth Newton-conjugate gradient algorithm. The approach incorporates heat kernel diffusion to attenuate label corruption and includes an optional class-size estimation step when converting continuous solutions into label assignments. The framework is computationally efficient, with each iteration requiring O(|E|) operations, and provides theoretical connections to randomized cuts, effective resistance, and Wasserstein distance.

## Key Results
- Achieves 2.4% higher accuracy than PoissonMBO on CIFAR-10 in partial label setting
- Demonstrates 4.1% improvement over Poisson learning on CIFAR-100 with corrupted labels
- Shows 3.8% better robustness to random label flips compared to baseline methods on CIFAR-10
- Maintains strong performance across diverse datasets (Cora, Citeseer, Pubmed, MNIST, FashionMNIST, CIFAR-10/100)

## Why This Works (Mechanism)

### Mechanism 1
Relaxing hard label constraints into soft measure constraints prevents solution degeneracy at low label rates. Instead of enforcing exact label interpolation ϕ_i = y_i, the method models labels as probability measures μ, ν and imposes only ϕ^T(μ - ν) = 1. This allows the optimizer to find potentials that separate distributions without collapsing toward constants as labeled fraction → 0. Core assumption: Graph structure contains sufficient signal to separate classes even when labels are extremely sparse.

### Mechanism 2
Heat kernel diffusion (e^{-tL}) pre-processing attenuates label corruption while preserving class structure. Diffusing corrupted labels via e^{-tL} spreads information locally, causing random corruption noise (assumed mean-zero) to cancel while true class signal is reinforced across nearby nodes. Core assumption: Corruptions are roughly uniformly distributed; true labels cluster on the graph.

### Mechanism 3
The p-parameter controls solution sparsity, with smaller p concentrating potentials on fewer boundary nodes. The p-norm in the objective naturally encourages sparser gradients for p→1 (total variation-like) and smoother gradients for p→∞. This allows tuning between localized vs. diffuse decision boundaries. Core assumption: The optimal sparsity level is dataset-dependent and can be selected or searched.

## Foundational Learning

- **Graph Laplacian and effective resistance**: Why needed here - The method directly uses L and L⁺; understanding how L encodes graph structure and why effective resistance measures "distance" on graphs is essential for interpreting C₂ results. Quick check: Given a 3-node path graph, can you compute its Laplacian eigenvalues and explain why the middle node has lowest effective resistance to both endpoints?

- **Measure/probability simplex on graphs**: Why needed here - Labels are represented as probability measures μ, ν ∈ P(V) rather than hard assignments; this is the core representation shift enabling robustness. Quick check: If you have 5 labeled nodes for class A and 3 for class B, how would you construct μ and ν?

- **Proximal operators and subdifferentials**: Why needed here - The SSNAL algorithm relies on prox_{λs}(·) and ∂s(u) for solving the nonsmooth optimization; without this, the Newton-CG method is opaque. Quick check: For s(u) = |u|, what is prox_{λs}(v)? What about s(u) = |u|^p for p > 1?

## Architecture Onboarding

- Component map: Raw labels (Y) → Measure construction (μ, ν per class) → Heat diffusion (e^{-tL}Y) → One-vs-all R matrix → SSNAL solver (C_p optimization) → Continuous potentials Φ → Cut assignment (cardinality-aware or argmax) → Final predictions

- Critical path: The SSNAL solver (Algorithm 1 + 2). The Newton-CG step requires computing Hx in O(|E|) and relies on the proximal map being strongly semismooth. Bugs here cascade to all downstream results.

- Design tradeoffs:
  - p selection: p=2 is safest default (theoretically clean, connects to effective resistance); p=1 gives sparsest cuts but may over-prune; p=∞ connects to Wasserstein distance but is least tested empirically
  - Diffusion time t: Too small → no noise cancellation; too large → blurs true class boundaries. Theorem 2.7 suggests t < λ⁻¹(||η||/||μ-ν|| - 1), but estimating ||η|| from data is nontrivial
  - Cardinality prior ϵ: ϵ=0 requires exact class size estimates; ϵ=n falls back to argmax. Intermediate values trade robustness for accuracy

- Failure signatures:
  - Potentials collapse to near-constant → likely p too large or diffusion time too long
  - Predictions ignore graph structure → check that edge weights w_{ij} are meaningful (not uniform on random graph)
  - SSNAL fails to converge → verify L is connected and check stopping criterion (15) parameters

- First 3 experiments:
  1. Sanity check on synthetic data: Generate 2-class data on a 2D grid with clear separation. Run p=2 with known labels. Verify potentials form smooth gradients and predictions match ground truth.
  2. Label rate sweep on Cora: Replicate Table 1 results at 1, 3, 5, 10 labels/class. Compare p∈{1, 2, ∞}. Document variance across random seeds.
  3. Corruption robustness test: On CIFAR-10 (using provided autoencoder features), inject 20-40% random label flips. Sweep diffusion time t ∈ {0, 0.5, 1, 2, 5}. Plot accuracy vs. t to verify the "sweet spot" predicted by Theorem 2.7.

## Open Questions the Paper Calls Out

### Open Question 1
Can active label selection strategies be effectively developed within the p-conductance learning framework? The paper explicitly lists this as a future direction. This remains unresolved because the current framework is evaluated on fixed sets of labeled and unlabeled data without mechanisms for querying or selecting the most informative labels.

### Open Question 2
Is the p-conductance learning method statistically consistent, particularly in the limit of infinite data? The paper calls for theoretical investigations into statistical consistency. This remains unresolved because while the paper provides convergence guarantees for the optimization algorithm and robustness bounds for noisy labels, it does not prove that the classifier converges to the true labeling function as the number of nodes n → ∞.

### Open Question 3
Can the framework be extended to ensure robustness against data corruption in addition to label corruption? The paper suggests exploring SSL frameworks robust to data corruption. This remains unresolved because the theoretical and empirical robustness analysis focuses on noise in the label vector, assuming the graph structure and node features remain fixed and uncorrupted.

### Open Question 4
Is there a theoretically grounded criterion for selecting the optimal exponent p based on graph topology or noise levels? The paper benchmarks p ∈ {1, 2, ∞} but shows significant performance variance, implying the choice is critical but currently heuristic. This remains unresolved because while the paper links different p values to concepts like mincut and effective resistance, it does not provide a rule for adapting p to specific datasets or noise regimes.

## Limitations

- The choice of p parameter and diffusion time t remains somewhat heuristic, depending on dataset characteristics and corruption levels
- Computational complexity of O(|E|²|Σ|) per SSNAL iteration may limit scalability to massive graphs
- Theoretical analysis assumes connected graphs and relies on specific properties of the graph Laplacian

## Confidence

- **High**: Theoretical connections between p-conductance programs and existing variational problems (randomized cuts, effective resistance)
- **Medium**: Empirical robustness claims on corrupted labels (though based on limited datasets)
- **Medium**: Computational efficiency claims (SSNAL performance depends heavily on implementation details)

## Next Checks

1. **Generalization test**: Apply p-conductance learning to a novel dataset (e.g., WebKB or NELL) with systematic label corruption experiments to verify robustness beyond reported datasets.

2. **Ablation study**: Compare performance with and without heat kernel diffusion across different corruption patterns (uniform vs. localized) to isolate diffusion's contribution to robustness.

3. **Parameter sensitivity**: Conduct comprehensive sweeps of p ∈ {1, 1.5, 2, 2.5, ∞} and diffusion times t ∈ {0.1, 0.5, 1, 2, 5} on CIFAR-10 to identify optimal combinations and verify claimed sparsity-control mechanism.