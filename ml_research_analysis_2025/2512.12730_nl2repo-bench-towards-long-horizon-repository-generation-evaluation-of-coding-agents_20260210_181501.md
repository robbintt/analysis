---
ver: rpa2
title: 'NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding
  Agents'
arxiv_id: '2512.12730'
source_url: https://arxiv.org/abs/2512.12730
tags:
- repository
- code
- agents
- task
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NL2Repo-Bench, a benchmark designed to evaluate
  long-horizon repository generation capabilities of coding agents. Unlike existing
  benchmarks focusing on short-horizon tasks, NL2Repo-Bench challenges agents to autonomously
  construct complete, installable Python libraries from single natural-language requirements
  documents and empty workspaces.
---

# NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents

## Quick Facts
- **arXiv ID**: 2512.12730
- **Source URL**: https://arxiv.org/abs/2512.12730
- **Reference count**: 40
- **Primary result**: Even state-of-the-art coding agents achieve below 40% average test pass rates on long-horizon repository generation tasks

## Executive Summary
NL2Repo-Bench introduces a benchmark for evaluating coding agents' ability to autonomously construct complete, installable Python libraries from single natural-language requirements documents. Unlike existing benchmarks focusing on short-horizon tasks, this benchmark challenges agents to maintain architectural coherence and planning over hundreds of interaction steps. Experiments with leading models reveal that long-horizon repository generation remains largely unsolved, with fundamental failure modes including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning. The benchmark establishes a rigorous testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for next-generation autonomous coding agents.

## Method Summary
NL2Repo-Bench comprises 104 tasks spanning nine categories, where agents must construct complete Python libraries from single natural-language requirements documents (averaging 18,800 tokens) and empty workspaces. The benchmark uses a Docker-based evaluation environment with pinned dependencies to ensure reproducible, verifiable results. Agents interact through frameworks like OpenHands, Claude Code, or Cursor, using tools for execution, editing, and task tracking. Performance is measured by upstream pytest suite pass rates rather than LLM-judged outputs, providing objective ground truth at the cost of test-suite alignment sensitivity.

## Key Results
- State-of-the-art models achieve below 40% average test pass rates on long-horizon repository generation
- Claude-Sonnet-4.5 achieves 39.9% pass rate, outperforming other models with standard context windows (<25% average)
- Qwen3-Thinking exhibits 49.0% early termination rate despite high planning usage
- GPT-5 shows 84.5% non-finish rate, indicating alignment toward human-in-the-loop assistance rather than autonomous execution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger context windows enable sustained global coherence during repository generation.
- Mechanism: Ultra-long context (1M+ tokens) allows agents to retain full specification documents (18.8K tokens average), generated code (10K-50K tokens), and interaction history (~90K tokens across 180 turns), maintaining architectural consistency across hundreds of edits.
- Core assumption: Cross-file dependencies and architectural decisions made early remain accessible for later implementation steps.
- Evidence anchors:
  - [abstract] "fragile cross-file dependencies" and "inadequate planning over hundreds of interaction steps" identified as failure modes
  - [section 4.3.5] Claude (1M context) achieves 39.9% vs. standard-context models averaging <25%; context "provides critical headroom for maintaining full context throughout extended development sessions"
  - [corpus] Context management is identified as a key challenge for long-horizon SWE agents in related work
- Break condition: When context exceeds model capacity, early architectural decisions become inaccessible, causing cross-file inconsistencies.

### Mechanism 2
- Claim: Explicit planning tool usage correlates with task completion and higher pass rates.
- Mechanism: Models that use structured task tracking maintain a persistent view of implementation progress, enabling systematic decomposition of large specifications into verifiable sub-goals rather than ad-hoc generation.
- Core assumption: Planning tools externalize state that would otherwise require implicit memory across long horizons.
- Evidence anchors:
  - [abstract] "inadequate planning over hundreds of interaction steps" identified as fundamental failure mode
  - [section 4.3.1] "task_tracker exhibits the strongest correlation with model performance (0.711)"; Qwen3-Thinking allocates 0 calls to planning with 49.0% early termination rate
  - [corpus] Related benchmarks (SWE-EVO, ABC-Bench) emphasize planning across long-horizon tasks but do not isolate this mechanism
- Break condition: Planning without sufficient context to retain plan state yields diminishing returns (DeepSeek-V3.2: high planning usage, low success).

### Mechanism 3
- Claim: Model alignment toward autonomous execution vs. human-in-the-loop assistance determines completion behavior.
- Mechanism: Models fine-tuned for collaborative assistance halt to request user confirmation; models aligned for autonomous execution persist through uncertainty and complete tasks without external feedback.
- Core assumption: Completion behavior is a product of training alignment, not just capability.
- Evidence anchors:
  - [section 4.3.3] GPT-5 has 84.5% non-finish rate vs. Claude-Sonnet-4's 1.9%; GPT-5 "tends to halt prematurely and waits for additional user input"
  - [section 4.3.3] Qwen3-Thinking shows 49.0% early termination from "hallucination of verification" in internal reasoning
  - [corpus] Weak direct evidence; related work on agent alignment does not isolate this mechanism
- Break condition: Fully autonomous settings with no human feedback expose over-conservative halting behavior.

## Foundational Learning

- Concept: Agent-Computer Interface (ACI)
  - Why needed here: OpenHands framework provides tools (execute_bash, str_replace_editor, task_tracker) that agents must orchestrate. Understanding how agents use tools reveals failure patterns.
  - Quick check question: Can you explain why execute_bash is the second most-used tool and what edit-to-execution ratios reveal about development strategies?

- Concept: Long-horizon reasoning in LLMs
  - Why needed here: The benchmark explicitly tests sustained reasoning over 180+ interaction turns. The gap between short-horizon (function-level) and long-horizon (repository-level) performance is the core finding.
  - Quick check question: Why does increasing interaction round limits from 200 to unlimited yield only marginal gains for Claude-Sonnet-4.5?

- Concept: Test-driven development (TDD) in agent workflows
  - Why needed here: High-performing models exhibit "Edit-Test loops" while low performers show "Navigation Traps" or "Blind Editing." Workflow patterns predict success.
  - Quick check question: What distinguishes the Edit-Test loop pattern from the Navigation Trap in agent action sequences?

## Architecture Onboarding

- Component map: Requirements Document → Agent Framework → Generated Repository → Docker Evaluation Environment → Upstream pytest Suite
- Critical path: Document ingestion (18.8K tokens) → Architecture design → Multi-file implementation → Package configuration → pytest execution. The 104 tasks span 9 categories with difficulty tiers based on LOC (Easy ≤1.5K, Medium 1.5K-4K, Hard ≥4K).
- Design tradeoffs: (1) Strict execution-based evaluation vs. LLM-judged evaluation—chooses objective ground truth at cost of test-suite alignment sensitivity. (2) Empty workspace vs. scaffolded start—maximizes task realism but exposes agent architectural design weaknesses. (3) Unlimited interaction rounds vs. bounded—reveals agent persistence but may reward brute-force iteration.
- Failure signatures: (1) ImportError/ModuleNotFound—package structure failures. (2) Premature termination with finish tool—overconfidence from thinking models. (3) Non-finish with await behavior—assistance-aligned models like GPT-5. (4) Test suite signature mismatch—correct logic but wrong API expectations.
- First 3 experiments:
  1. Replicate the Claude-Sonnet-4.5 baseline on a Medium-difficulty task from the "Utility Libraries" category (52.4% pass rate) to establish a working evaluation pipeline.
  2. Ablate context window by truncating conversation history at 32K/64K/128K/128K tokens and measure pass-rate degradation to validate the context-coherence mechanism.
  3. Instrument task_tracker usage: force-disable planning tools for Claude-Sonnet-4.5 and compare against baseline to isolate the planning-correlation mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural innovations beyond larger context windows are required to maintain global coherence over hundreds of interaction steps during repository generation?
- Basis in paper: [explicit] "We argue that progress on NL2Repo-Bench will require advances beyond larger context windows, including improved agentic planning, robust self-verification loops, and mechanisms for maintaining global coherence over long development trajectories."
- Why unresolved: The paper shows context size is necessary but not sufficient; models with large contexts (Kimi-k2, GPT-5) still underperform.
- What evidence would resolve it: Ablation studies introducing specific architectural components (hierarchical planning, memory mechanisms) showing improved pass rates independent of context size.

### Open Question 2
- Question: Why do "thinking" models exhibit paradoxically higher premature termination rates, and how can internal reasoning be grounded against actual execution?
- Basis in paper: [explicit] "We hypothesize that this behavior stems from a 'hallucination of verification' within the model's internal monologue. The thinking process acts as a self-reinforcing echo chamber... This 'internal success' masks the external failure."
- Why unresolved: The paper identifies the phenomenon (49.0% early termination for Qwen3-Thinking) but offers only a hypothesis without validation or solution.
- What evidence would resolve it: Experiments coupling internal reasoning chains with mandatory external verification steps, measuring reduction in premature termination rates.

### Open Question 3
- Question: What fundamental limitations prevent models from exceeding ~60% pass rates even when the full test suite is revealed during development?
- Basis in paper: [explicit] "This indicates that, despite the advantages of test-driven development, generating a fully functional, end-to-end runnable repository from scratch remains a substantial challenge... pointing to fundamental limitations in long-horizon coordination and large-scale code synthesis."
- Why unresolved: The ablation (Table 8) shows a ceiling even under "cheating" conditions; the paper does not diagnose which specific coordination or synthesis failures persist.
- What evidence would resolve it: Fine-grained error analysis on test-revealed runs isolating remaining failure categories (dependency resolution, cross-file consistency, architectural drift).

### Open Question 4
- Question: How can agent alignment be shifted from human-in-the-loop collaboration toward fully autonomous execution without sacrificing reliability?
- Basis in paper: [inferred] GPT-5 exhibits "massive Non-Finish rate (84.5%)" and "tends to halt and wait for user guidance," indicating "GPT-5 is aligned more towards collaborative assistance than autonomous execution."
- Why unresolved: The paper characterizes the misalignment but offers no mechanism for re-aligning models toward persistent autonomous operation.
- What evidence would resolve it: Training or prompting interventions that reduce Non-Finish rates while maintaining or improving pass rates, compared against baseline autonomous behavior.

## Limitations

- Task coverage bias: All tasks focus on Python utilities and libraries, potentially not generalizing to other software domains
- Evaluation dependency alignment: Test-suite-driven evaluation may penalize correct implementations that differ from expected API signatures
- Context window artifacts: Correlation between large context windows and success may reflect memorization rather than genuine architectural reasoning

## Confidence

- **High confidence**: Benchmark construction methodology, task diversity, and Docker-based evaluation isolation are well-documented and reproducible
- **Medium confidence**: Failure mode analysis (premature termination, loss of coherence) is empirically grounded but may not capture all long-horizon failure patterns
- **Low confidence**: Claims about alignment differences between models lack controlled ablation studies isolating alignment from capability

## Next Checks

1. Apply NL2Repo-Bench evaluation to non-Python repositories (JavaScript, Go) to test whether observed long-horizon failures generalize beyond the current task set
2. Systematically vary specification clarity (precise vs. ambiguous requirements) to quantify how much failure stems from interpretation vs. sustained reasoning
3. Instrument agents to save architectural plans after initial design phase, then test whether plan loss specifically correlates with cross-file dependency failures identified in the analysis