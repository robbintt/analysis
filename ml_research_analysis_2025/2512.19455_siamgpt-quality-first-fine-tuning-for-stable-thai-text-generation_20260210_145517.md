---
ver: rpa2
title: 'SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation'
arxiv_id: '2512.19455'
source_url: https://arxiv.org/abs/2512.19455
tags:
- thai
- instruction
- language
- siamgpt-32b
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SiamGPT-32B improves Thai LLM stability by fine-tuning Qwen3-32B
  with a quality-first strategy that prioritizes curated, constraint-aware supervision
  over data scale. Using high-complexity English instruction datasets combined with
  a Thai-adapted AutoIF framework, the model targets stable Thai generation, instruction
  adherence, and multi-turn consistency without continual pretraining.
---

# SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation

## Quick Facts
- **arXiv ID:** 2512.19455
- **Source URL:** https://arxiv.org/abs/2512.19455
- **Reference count:** 5
- **Primary result:** SiamGPT-32B achieves top SEA-HELM performance among similar-scale Thai LLMs, notably improving instruction-following (75.47→83.00), multi-turn dialogue (57.94→75.81), and code-switching stability (87.70→90.40) via deterministic AutoIF fine-tuning.

## Executive Summary
SiamGPT-32B improves Thai LLM stability by fine-tuning Qwen3-32B with a quality-first strategy that prioritizes curated, constraint-aware supervision over data scale. Using high-complexity English instruction datasets combined with a Thai-adapted AutoIF framework, the model targets stable Thai generation, instruction adherence, and multi-turn consistency without continual pretraining. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with notable gains in instruction following (75.47→83.00), multi-turn dialogue (57.94→75.81), and generation stability (code-switching score 87.70→90.40).

## Method Summary
SiamGPT-32B is a 32B-parameter Thai LLM fine-tuned from Qwen3-32B using a quality-first, constraint-aware approach. The training corpus (~320K pairs) is sourced from two streams: high-complexity English instruction data (SystemChat-2.0) and a Thai-adapted AutoIF framework producing deterministic, constraint-verified pairs. Fine-tuning uses SFT-only with next-token prediction, trained on 64 H100 GPUs for 4,096 steps (~2.1B tokens). The method emphasizes stability and instruction adherence via programmatic validation rather than large-scale data or RL-based preference tuning.

## Key Results
- SiamGPT-32B achieves SEA-HELM best overall performance among similar-scale Thai models.
- Instruction-following score improves from 75.47 to 83.00.
- Multi-turn dialogue score improves from 57.94 to 75.81.
- Code-switching score improves from 87.70 to 90.40.

## Why This Works (Mechanism)
SiamGPT-32B's stability gains stem from replacing subjective human annotation with deterministic, script-based constraint verification (AutoIF), which ensures consistent enforcement of Thai orthographic rules and reduces code-switching artifacts. The dual-stream data curation pipeline combines high-complexity English instruction data with Thai-specific AutoIF constraints, creating a compact, high-fidelity training corpus. By avoiding RL-based preference tuning and focusing on SFT-only, the model maintains stable convergence and avoids catastrophic forgetting. The explicit prioritization of quality over data scale prevents the degradation of stability observed with larger, noisier corpora.

## Foundational Learning

- **Concept: Deterministic vs. Subjective Supervision**
  - Why needed here: The paper's core contribution rests on replacing human annotation (subjective) with AutoIF's script-based verification (deterministic). Understanding this distinction is critical to evaluating the claimed stability improvements.
  - Quick check question: Can you describe one type of output constraint that would be easy to verify with a script, and one that would be difficult?

- **Concept: Catastrophic Forgetting in Fine-Tuning**
  - Why needed here: The paper argues that its specific data strategy mitigates this problem. Assessing this claim requires knowing what catastrophic forgetting is and how data selection influences it.
  - Quick check question: What is the primary risk when fine-tuning a pre-trained model on a small, specialized dataset?

- **Concept: The Stability-Fluency Trade-off**
  - Why needed here: The paper explicitly highlights a trade-off where its model achieves superior stability but lags in NLG (translation) fluency. This is a key architectural tradeoff.
  - Quick check question: If a model is heavily optimized to *never* generate incorrect characters, what might happen to its ability to produce creative or highly nuanced text?

## Architecture Onboarding

- **Component map:**
  - Qwen3-32B (base model) -> Dual-stream data curation (Stream A: SystemChat-2.0; Stream B: Thai AutoIF synthesis) -> AutoIF deterministic constraint verification -> Merged corpus (~320K pairs) -> SFT fine-tuning (4,096 steps, 64 H100 GPUs) -> SEA-HELM evaluation

- **Critical path:**
  1. Start with the pre-trained Qwen3-32B instruction-tuned checkpoint.
  2. Process data through the dual-stream curation pipeline to produce ~320K high-fidelity examples.
  3. Run SFT for 4,096 steps on 64 H100 GPUs using the curated corpus.
  4. Evaluate the resulting model on the SEA-HELM suite to validate stability and instruction-following claims.

- **Design tradeoffs:**
  - **Stability vs. Fluency:** The model is explicitly optimized for controlled, stable output at the potential cost of generative fluency, as seen in the NLG (translation) benchmark scores.
  - **Data Scale vs. Quality:** A deliberate choice to use a small, curated corpus rather than large-scale web crawls, based on the finding that more data *degraded* stability.
  - **SFT-Only vs. RL/DPO:** The pipeline deliberately excludes preference optimization or RL, isolating SFT as the primary driver of change to ensure stable convergence.

- **Failure signatures:**
  - **Code-Switching Recurrence:** The model begins inserting non-Thai tokens (Chinese, English, Hindi) into Thai responses, indicating a failure of the AutoIF constraint enforcement.
  - **Repetition/Looping:** In open-ended generation, the model enters repetitive loops, a known pathology of neural text generation that the authors acknowledge.
  - **Constraint Overfitting:** The model adheres to formatting constraints so rigidly that it refuses reasonable requests or produces awkward, unnatural phrasing.

- **First 3 experiments:**
  1. **Corpus Ablation:** Re-train the model using *only* the English data (Stream A) and evaluate code-switching scores to isolate the impact of the Thai AutoIF constraints.
  2. **Stress Test for Stability:** Prompt the model with a series of adversarial, complex Thai instructions designed to elicit code-switching and measure the failure rate against the Qwen3-32B baseline.
  3. **Trade-off Analysis:** Compare SiamGPT-32B and Typhoon2.5 on a set of free-form creative generation tasks to qualitatively assess the fluency penalty of the stability-focused training.

## Open Questions the Paper Calls Out

- **Can Thai NLG quality (translation fluency, abstractive naturalness) be improved to match or exceed Typhoon2.5 without sacrificing the stability and instruction-following gains achieved through constraint-aware supervision?**
  - Basis in paper: [explicit] The authors state in Section 6.2: "Given the observed SEA-HELM NLG weakness, we will explicitly target Thai naturalness and generation quality while retaining deterministic constraint enforcement."
  - Why unresolved: The current Quality-First approach prioritizes controllability, which appears to trade off against generative fluency. The mechanism underlying this tradeoff is not characterized.
  - What evidence would resolve it: Ablation experiments varying the ratio of constraint-enforcement data versus Thai-native conversational/writing supervision, measuring both NLG scores and stability metrics simultaneously.

- **What are the specific data sources or training objectives that most improve NLG while preserving stability, and does multi-objective training with stability as a hard constraint effectively prevent regression?**
  - Basis in paper: [explicit] Section 6.2 commits to "formalize these findings with publishable ablation tables and expand them into controlled studies (e.g., which data sources or objectives help NLG the most while preserving stability)."
  - Why unresolved: Internal corpus experiments showed that increasing data breadth degraded stability, and DPO improved instruction-following but regressed other pillars, but these findings were not systematically characterized or published.
  - What evidence would resolve it: Controlled ablation studies with published tables showing per-benchmark performance when varying data sources, training objectives (SFT vs. DPO vs. hybrid), and constraint enforcement mechanisms.

- **Does the Quality-First, constraint-aware fine-tuning approach generalize to other low-resource languages exhibiting code-switching and multilingual interference, or is it specific to Thai orthographic and linguistic properties?**
  - Basis in paper: [inferred] The paper frames the approach as a solution to a production-critical failure mode for Thai, but does not test on other languages. The Thai-adapted AutoIF component includes 39 manually curated Thai seed instructions targeting language-specific phenomena.
  - Why unresolved: No cross-linguistic experiments were conducted. The Thai-specific components (orthographic constraints, vowel placement rules) are language-specific, but the overall pipeline architecture could potentially transfer.
  - What evidence would resolve it: Applying the same dual-stream pipeline to another multilingual base model for a different low-resource language (e.g., Vietnamese, Indonesian) and measuring stability, instruction-following, and NLG metrics on equivalent benchmarks.

## Limitations
- The AutoIF framework's exact verification scripts for Thai orthographic constraints are not disclosed, creating uncertainty about reproducibility.
- The claim that "more data degrades stability" is supported by limited experimentation—only one baseline (Qwen3-32B) is compared, and no systematic sweep across dataset sizes is reported.
- The reported stability gains are benchmark-specific (SEA-HELM) and may not transfer to other domains or evaluation suites.

## Confidence
- **High confidence:** The model's improved instruction-following (IFEval: 75.47→83.00) and multi-turn dialogue (MTBench: 57.94→75.81) scores on SEA-HELM are well-documented and reproducible given the described training pipeline.
- **Medium confidence:** The claim that stability gains stem from deterministic AutoIF supervision is plausible but not definitively proven, as the exact verification logic is not released.
- **Low confidence:** The assertion that data scale beyond a certain threshold harms stability is based on limited experimentation and may not generalize across different model families or tasks.

## Next Checks
1. **Corpus composition ablation:** Retrain SiamGPT-32B using only the English SystemChat-2.0 data (Stream A) and measure code-switching scores to isolate the impact of the Thai AutoIF constraints.
2. **Stability stress test:** Prompt the model with a battery of adversarial, complex Thai instructions designed to elicit code-switching, and compare failure rates against the Qwen3-32B baseline.
3. **Fluency trade-off analysis:** Evaluate SiamGPT-32B and Typhoon2.5 on a set of free-form creative generation tasks to quantify the fluency penalty of the stability-focused training approach.