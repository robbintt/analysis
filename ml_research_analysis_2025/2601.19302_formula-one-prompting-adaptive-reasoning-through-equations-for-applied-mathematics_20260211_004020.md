---
ver: rpa2
title: 'Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics'
arxiv_id: '2601.19302'
source_url: https://arxiv.org/abs/2601.19302
tags:
- answer
- equations
- reasoning
- table
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Formula-One Prompting (F-1), a prompting
  technique that improves LLM mathematical reasoning by explicitly formalizing governing
  equations before adaptive solving. F-1 first extracts governing equations from problem
  descriptions, then selects among direct calculation, Chain-of-Thought, or Program-of-Thought
  based on the equation structure, all within a single LLM call.
---

# Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics

## Quick Facts
- arXiv ID: 2601.19302
- Source URL: https://arxiv.org/abs/2601.19302
- Reference count: 40
- Key outcome: F-1 improves LLM mathematical reasoning by +5.76% over Chain-of-Thought and +8.42% over Program-of-Thought on average across five models and four benchmarks.

## Executive Summary
Formula-One Prompting (F-1) introduces a two-phase approach that first formalizes governing equations from problem descriptions, then adaptively selects among Direct calculation, Chain-of-Thought, or Program-of-Thought based on equation structure. This single-call method demonstrates significant performance gains on applied mathematics domains, particularly in FinanceMath (+13.30%) and OlympiadBench physics problems (+2.55%). The key innovation lies in forcing explicit equation formalization as a semantic anchor before problem solving, which reduces ambiguity and enables intelligent strategy selection without external classifiers.

## Method Summary
F-1 is a single-call, two-phase inference method that formalizes governing equations before adaptive solving. Phase 1 extracts givens, targets, and governing equations from the problem description. Phase 2 analyzes the equation structure to select an optimal solving strategy: Direct calculation for simple algebraic equations, Chain-of-Thought for complex derivations, or Program-of-Thought for iterative/numerical problems. The unified prompt includes both phases with temperature 0 greedy decoding, and incorporates a verification step that leverages the formalized equations as constraints.

## Key Results
- F-1 outperforms Chain-of-Thought by +5.76% and Program-of-Thought by +8.42% on average across five models
- Largest gains in applied domains: +13.30% on FinanceMath and +2.55% on OlympiadBench physics problems
- Equation formalization ablation shows it contributes twice the improvement of adaptive selection alone
- Automatic strategy selection learned without hard-coded rules, with Post-hoc analysis confirming correct routing patterns

## Why This Works (Mechanism)

### Mechanism 1: Explicit Formalization as Ambiguity Reduction
The "Phase 1" formalization step acts as a semantic anchor by requiring the model to output equations before solving, preventing misinterpretation of natural language problem statements. The model must translate descriptions into valid mathematical formalisms, grounding the reasoning process. Evidence shows ablation studies confirm equation formulation is the most critical component, and qualitative examples demonstrate baselines failing due to semantic confusion that F-1's formalization prevents.

### Mechanism 2: Equation-Conditioned Adaptive Routing
Structural properties of formalized equations serve as reliable signals for selecting optimal solving strategies. The model implicitly detects correlations between equation complexity and optimal methods: simple algebraic equations trigger Direct calculation, complex derivations trigger CoT, and iterative problems trigger PoT. Post-hoc analysis confirms the model learns to map "Closed-form" to Direct and "Iterative/recursive" to PoT without explicit hard-coded rules.

### Mechanism 3: Single-Call Structured Generation
Unified prompting captures decomposition benefits without multi-call latency or complexity. Both phases in one context window ensure the solving phase has direct access to "Phase 1" constraints, making in-context constraint satisfaction more robust than multi-turn approaches where context might be truncated or summarized.

## Foundational Learning

**Concept: Intermediate Representations (IR)**
- **Why needed here:** F-1 relies on generating a specific IR (LaTeX equations) as a bridge between natural language and final solution. Understanding IRs is necessary to diagnose why this step aids reasoning (it reduces search space).
- **Quick check question:** Can you explain why generating a symbolic representation (IR) before solving might reduce "semantic ambiguity" errors compared to direct text-to-answer generation?

**Concept: Chain-of-Thought (CoT) vs. Program-of-Thought (PoT) Distinctions**
- **Why needed here:** F-1 dynamically routes between these strategies. You must understand their failure modes (CoT: calculation errors; PoT: conceptual/logic errors) to debug F-1's routing decisions.
- **Quick check question:** If a problem requires proving a theorem about prime numbers, which strategy (CoT or PoT) would likely fail, and why?

**Concept: Ablation Studies**
- **Why needed here:** The paper relies heavily on ablation to prove "formalization" > "adaptive selection." Understanding how to isolate variables in a prompt is critical for replicating or extending this work.
- **Quick check question:** How would you design an experiment to test if the *format* of the equations (LaTeX vs. natural language) matters for the performance gain?

## Architecture Onboarding

**Component map:** Input -> Phase 1 (Formalizer) -> Router -> Phase 2 (Solver) -> Output

**Critical path:** The Phase 1 -> Phase 2 transition is most critical. If formalization is incorrect (e.g., missing a key variable), adaptive selection and subsequent solving will fail deterministically.

**Design tradeoffs:** Latency vs. Accuracy (slightly higher input token counts for significant accuracy gains). Generalizability works best on applied domains where governing equations exist, explicitly excluded from pure arithmetic benchmarks.

**Failure signatures:** "False Formalization" (hallucinated constraints), Router Drift (defaults to CoT for every problem), Verification Bypass (outputs verification text without actual checking).

**First 3 experiments:**
1. **Formalization-Only Ablation:** Run benchmark using only Phase 1, manually check equation correctness to isolate understanding capability from solving capability.
2. **Routing Adherence:** Analyze log outputs to count F-1 selections on FinanceMath (should favor PoT) vs. AICrypto (should favor CoT), compare against Table 2 trends.
3. **Token Limit Stress Test:** Test F-1 on problems requiring long code solutions to verify if single-call output limit truncates solutions before final answers.

## Open Questions the Paper Calls Out

**Open Question 1:** Does F-1's stronger performance on applied domains stem from alignment with expert-authored training data that naturally presents problems through explicit equations? The paper speculates this reflects training data composition but lacks controlled data ablations to confirm.

**Open Question 2:** Can F-1 be simplified or adapted to work effectively with smaller models (7B-13B parameters) that may lack reliable symbolic abstraction capabilities? Evaluation only covers 30B to frontier models; smaller model compatibility is unknown.

**Open Question 3:** Can combining F-1 with multi-call mechanisms (backtracking, repeated sampling) further improve robustness on problems where initial strategy selection is suboptimal? F-1 deliberately prioritizes single-call efficiency; potential gains from multi-call extensions remain unexplored.

## Limitations
- Model Generalization Gap: Core assumption of reliable equation generation may break down for smaller architectures below 7B parameters
- Evaluation Dependency: Accuracy gains rely on specific judge models (GPT-5.1, Gemini-3-Pro) that may not be accessible for reproduction
- Single-Call Constraint: Potential output token limits when PoT is selected for complex computation problems, though unreported in paper

## Confidence
**High Confidence (8-10/10):** Explicit equation formalization reducing semantic ambiguity is well-supported by ablation studies showing significant performance drops without it.
**Medium Confidence (5-7/10):** Adaptive routing mechanism effectiveness is empirically demonstrated but relies on implicit learning rather than explicit rules.
**Low Confidence (1-4/10):** Claims about efficiency relative to multi-call methods lack absolute latency measurements necessary for practical deployment decisions.

## Next Checks
**Validation Check 1: Formalization-Only Ablation** - Run benchmarks using only Phase 1, manually verify equation correctness to isolate understanding capability from solving capability.

**Validation Check 2: Routing Decision Analysis** - Log and analyze Phase 1 outputs to verify routing decisions match expected patterns (PoT >50% for FinanceMath, CoT for AICrypto proofs).

**Validation Check 3: Token Limit Stress Test** - Test F-1 on FinanceMath problems requiring extensive numerical computation to verify single-call output limit doesn't truncate solutions.