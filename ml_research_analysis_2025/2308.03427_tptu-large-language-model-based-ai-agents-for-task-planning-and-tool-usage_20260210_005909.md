---
ver: rpa2
title: 'TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage'
arxiv_id: '2308.03427'
source_url: https://arxiv.org/abs/2308.03427
tags:
- arxiv
- tool
- tools
- agents
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a structured framework for LLM-based AI
  Agents to evaluate their task planning and tool usage abilities. Two types of agents
  are designed: one-step agent and sequential agent, which plan and execute tasks
  in different ways.'
---

# TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage

## Quick Facts
- arXiv ID: 2308.03427
- Source URL: https://arxiv.org/abs/2308.03427
- Reference count: 40
- Two agent architectures (TPTU-OA and TPTU-SA) evaluated on SQL/Python tasks show sequential agents outperform one-step agents, especially for high-performing LLMs.

## Executive Summary
This paper introduces a structured framework for evaluating Large Language Model-based AI agents on task planning and tool usage capabilities. The framework instantiates two agent types—one-step (TPTU-OA) and sequential (TPTU-SA)—to assess how LLMs plan and execute tasks involving external tools like SQL and Python. Through experiments on 120 question-answer pairs, the study reveals that sequential agents generally outperform one-step agents, particularly for stronger LLMs, and identifies common failure modes including format inconsistencies, tool over-utilization, and hallucination. The work provides insights into the limitations of current LLM agents and suggests areas for improvement in tool selection, subtask pairing, and overall planning strategies.

## Method Summary
The framework evaluates LLM-based agents on task planning and tool usage through two agent architectures: TPTU-OA, which plans all subtasks globally in one pass, and TPTU-SA, which plans and executes subtasks sequentially with iterative feedback. Both agents use structured prompts with demonstrations to guide tool selection and subtask decomposition. Tools include SQL generators (SQLite) and Python calculators. The evaluation uses 120 question-answer pairs with explicit database schemas, measuring accuracy of tool selection, tool order planning, tool-subtask pairing, and final answer correctness. Prompts and evaluation procedures are detailed in the appendices, enabling reproducible experimentation.

## Key Results
- Sequential agents (TPTU-SA) outperform one-step agents (TPTU-OA), especially for high-performing LLMs like ChatGPT, Claude, and InternLM.
- Unified generation of tool-subtask pairs in dictionary format improves planning accuracy by 52.9% compared to separate generation.
- LLM agents frequently encounter format consistency issues, with some models failing to output valid lists or dictionaries.
- Common failure modes include misunderstanding output formats, struggling to grasp task requirements, over-utilizing specific tools, and lacking summary skills.

## Why This Works (Mechanism)

### Mechanism 1: Unified Tool-Subtask Pair Generation
Generating tool-subtask pairs as unified dictionary structures improves planning accuracy compared to generating tools and subtasks separately. Coupling tools with their associated subtasks in a single generation step ensures direct alignment between capability selection and task decomposition, reducing misalignment errors that occur when sequences are generated independently. Core assumption: LLMs maintain better internal coherence when generating related concepts simultaneously rather than in separate inference passes.

### Mechanism 2: Sequential Task Resolution Enables Contextual Grounding
Sequential agents outperform one-step agents because each subtask benefits from executed results of prior steps. Step-by-step execution provides richer contextual understanding through iterative feedback loops, allowing the agent to adjust plans based on actual tool outputs rather than predicted outcomes. Core assumption: The overhead of multiple LLM calls is acceptable given the performance gains from intermediate result grounding.

### Mechanism 3: Prompt Engineering Constraints Shape Format Compliance
Strict format requirements (lists, dictionaries) in prompts significantly affect LLM agent performance, with some models failing to maintain structural consistency. Prompt structure acts as a scaffolding mechanism that guides output generation; models trained on diverse corpora may struggle with narrow format constraints unless explicitly demonstrated. Core assumption: Format adherence is learnable through few-shot demonstrations without requiring fine-tuning.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The paper uses CoT for complex SQL generation and sequential task decomposition. Understanding how step-by-step reasoning prompts work is essential for designing agent prompts.
  - Quick check question: Can you explain why breaking a nested SQL query into sequential clause generation improves accuracy for some models but not others?

- Concept: In-Context Learning with Few-Shot Demonstrations
  - Why needed here: All agent prompts in the paper rely on demonstrations to guide format and behavior. Without understanding how demonstrations shape LLM outputs, you cannot effectively design or debug agent prompts.
  - Quick check question: Given the paper's results showing 0% accuracy for some models on complex tasks, what does this suggest about the limits of in-context learning for certain LLMs?

- Concept: Tool Augmentation for Grounding
  - Why needed here: The core thesis is that LLMs need external tools (SQL, Python calculators) to handle tasks requiring precision or current data. Understanding when and why to offload computation is critical.
  - Quick check question: Why might an LLM incorrectly select a SQL generator for a purely mathematical problem, and what prompt modifications could prevent this?

## Architecture Onboarding

- Component map:
  Task Instruction → Designed Prompt → LLM → Tool Selection/Creation → Tool Execution → Intermediate Output → Reflection/Memory → Final Answer
  Two agent variants: TPTU-OA (single-pass decomposition) and TPTU-SA (iterative step-by-step)

- Critical path:
  1. Prompt design (format constraints, demonstrations, tool descriptions)
  2. Tool-subtask pair generation (unified dictionary format)
  3. Sequential execution with result propagation between steps
  4. Error handling and reflection loops

- Design tradeoffs:
  - One-step vs. Sequential: OA is faster but less accurate; SA provides better grounding at higher inference cost
  - Unified vs. Separate generation: Unified pairs improve alignment but may struggle with complex multi-tool dependencies
  - Model selection: High-performing models (ChatGPT, Claude) justify sequential overhead; weaker models (Ziya, ChatGLM) may not benefit from sophisticated prompting

- Failure signatures:
  - Format mismatch: Number of tools ≠ number of subtasks in output
  - Tool over-utilization: Same tool called repeatedly without progress
  - Hallucinated answers: Final response ignores tool outputs, relies on internal knowledge
  - Wrong tool selection: Mathematical problems routed to SQL generators

- First 3 experiments:
  1. Replicate the tool-subtask pair evaluation on your target LLM with 20 sample questions to establish baseline format compliance and planning accuracy.
  2. Compare TPTU-OA vs. TPTU-SA on a held-out set, measuring both accuracy and total inference tokens to quantify the cost-accuracy tradeoff.
  3. Inject a format-validation layer between LLM output and tool execution to catch structural errors before they propagate; measure reduction in execution failures.

## Open Questions the Paper Calls Out

### Open Question 1
Would a hierarchical agent architecture, combining one-step global planning with sequential execution, mitigate the error propagation observed in standalone TPTU-OA and TPTU-SA models? The paper only evaluates the two distinct agent types in isolation, leaving their potential synthesis untested.

### Open Question 2
What specific prompting constraints or feedback mechanisms are required to resolve the "Endless Extension" phenomenon where agents over-utilize specific tools? The authors identify the symptom but do not propose a solution for this planning loop.

### Open Question 3
How can the framework be optimized to improve format consistency (e.g., strict JSON/dictionary adherence) for open-source models that currently struggle with structural constraints? The study reveals that many models fail due to formatting rather than reasoning, but offers no method to enforce structure.

## Limitations
- Evaluation is constrained to 120 question-answer pairs, limiting generalizability to broader or more complex real-world scenarios.
- Several LLM models show near-zero performance on complex tasks, indicating significant model-dependent variability that isn't fully explained.
- Framework's applicability to domains beyond SQL and Python remains untested.
- Real-time latency constraints and cost implications of sequential execution are not addressed.

## Confidence
- High Confidence: Sequential agents outperform one-step agents for high-performing LLMs; unified tool-subtask pair generation improves planning accuracy over separate generation.
- Medium Confidence: Identified failure modes (format issues, task misunderstanding, tool over-utilization, hallucination) are consistently observed across multiple models.
- Low Confidence: Claim that sequential approaches become economically unviable at high tool execution latency lacks empirical validation.

## Next Checks
1. Test TPTU framework on a new domain (e.g., document processing with OCR and summarization tools) using 50-100 domain-specific questions to assess generalizability beyond SQL/Python.
2. Measure end-to-end execution time and inference costs for TPTU-OA vs. TPTU-SA across all tested LLMs, identifying the performance-cost breakeven point for different model capabilities.
3. Implement a pre-execution validation layer that automatically corrects common format errors and measure the reduction in execution failures for weak-format models like Ziya and ChatGLM.