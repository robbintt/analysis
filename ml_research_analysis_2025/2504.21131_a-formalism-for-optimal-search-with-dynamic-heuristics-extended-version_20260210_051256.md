---
ver: rpa2
title: A Formalism for Optimal Search with Dynamic Heuristics (Extended Version)
arxiv_id: '2504.21131'
source_url: https://arxiv.org/abs/2504.21131
tags:
- heuristic
- info
- information
- search
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the use of dynamic heuristics in optimal
  search, where heuristics depend on information accumulated during search. It introduces
  a framework for modifying such information and defines DYN-A, an A-like algorithm
  for searching with dynamic heuristics.
---

# A Formalism for Optimal Search with Dynamic Heuristics (Extended Version)

## Quick Facts
- arXiv ID: 2504.21131
- Source URL: https://arxiv.org/abs/2504.21131
- Reference count: 11
- This paper introduces DYN-A*, an A*-like algorithm for optimal search with dynamic heuristics, and proves optimality and non-reopening guarantees under specific conditions.

## Executive Summary
This paper addresses the challenge of optimal search when heuristics depend on information that accumulates during search. Traditional A* assumes static heuristics, but many planning techniques (landmark progression, online abstraction refinement, lazy evaluation) use heuristics that change as new information becomes available. The authors formalize this setting through a dynamic information framework and introduce DYN-A*, an algorithm that extends A* to handle dynamic heuristics. They prove that DYN-A* returns optimal solutions when using DYN-admissible heuristics and does not reopen states when using DYN-monotonic and DYN-consistent heuristics. The work provides formal justification for optimality claims in existing planning techniques and clarifies when reopening is necessary.

## Method Summary
The paper introduces a formalism for dynamic heuristics where heuristic values can change based on information accumulated during search. The framework defines three key properties: DYN-admissibility (dynamic heuristics never overestimate the true cost), DYN-monotonicity (ensuring consistent f-values during search), and DYN-consistency (ensuring that once a state is closed, its cost is optimal). Based on this framework, the authors present DYN-A*, an A*-like algorithm that maintains a search tree and re-evaluates states when their heuristic values change. The algorithm tracks dynamic information modifications and updates affected states accordingly. The key insight is that by carefully managing how information changes affect heuristic values and state re-evaluations, the algorithm can preserve optimality guarantees similar to classic A*.

## Key Results
- DYN-A* returns optimal solutions when using DYN-admissible heuristics
- DYN-A* does not reopen states when using DYN-monotonic and DYN-consistent heuristics (with re-evaluation)
- The framework provides formal justification for optimality in landmark progression, online abstraction refinement, and lazy heuristic evaluation
- The results extend classic A* optimality guarantees to the dynamic case and clarify conditions for necessary reopening

## Why This Works (Mechanism)
The mechanism works by formalizing how information accumulation during search affects heuristic values, then constraining these changes through DYN-admissibility, DYN-monotonicity, and DYN-consistency. When these properties hold, the algorithm can maintain the same logical structure as A* while handling dynamic changes. The key insight is that reopening is only necessary when dynamic information invalidates previously computed optimal paths - when information changes monotonically and consistently, the search can proceed without reopening states, just with re-evaluation of affected nodes.

## Foundational Learning

1. **Dynamic information framework**: Defines how heuristic values depend on search-time information
   - Why needed: Traditional heuristics are static, but many planning techniques use information that accumulates during search
   - Quick check: Can represent landmark discovery, abstraction refinement, and lazy evaluation as dynamic information

2. **DYN-admissibility**: Dynamic heuristic values never overestimate true remaining costs
   - Why needed: Without this, optimal solutions cannot be guaranteed when heuristics change
   - Quick check: h(s) â‰¤ h*(s) holds for all possible information states

3. **DYN-monotonicity**: Ensures f-values remain non-decreasing along search paths
   - Why needed: Prevents the need to reopen states when information changes monotonically
   - Quick check: f-values of children never decrease below parent's f-value

4. **DYN-consistency**: Once a state is closed, its cost is optimal even with dynamic changes
   - Why needed: Guarantees that closed states don't need reopening when information changes consistently
   - Quick check: Re-opening is never required for states satisfying this property

5. **State re-evaluation**: Process of updating state costs when dynamic information changes
   - Why needed: Essential for maintaining correctness when heuristic values change
   - Quick check: All descendants of a state with changed heuristic are properly updated

## Architecture Onboarding

**Component map**: Search state -> Dynamic information -> Heuristic function -> f-value calculation -> OPEN/CLOSED lists -> State expansion

**Critical path**: State expansion -> Information modification -> Heuristic re-evaluation -> f-value update -> OPEN list management -> Successor generation

**Design tradeoffs**: The framework trades computational overhead of tracking and updating dynamic information against the benefit of using more informed heuristics. Relaxing DYN-admissibility, monotonicity, or consistency can reduce overhead but sacrifices optimality guarantees. The choice between reopening states versus re-evaluating existing states involves balancing completeness against search efficiency.

**Failure signatures**: Suboptimal solutions occur when DYN-admissibility is violated. Unnecessary state reopenings happen when DYN-monotonicity is violated. Incorrectly closed states result from violating DYN-consistency. Performance degradation occurs when information changes too frequently, causing excessive re-evaluations.

**First experiments**:
1. Implement DYN-A* on a planning problem with dynamically discovered landmarks and compare solution quality against static heuristic approaches
2. Systematically relax each DYN-property in controlled experiments to measure impact on solution optimality
3. Benchmark DYN-A* against traditional A* on problems where online abstraction refinement is beneficial

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Optimality guarantees only hold under specific conditions (DYN-admissibility, DYN-monotonicity, DYN-consistency)
- Assumes discrete state spaces and well-defined information modifications, which may not capture all dynamic heuristic scenarios
- Computational overhead of maintaining and updating dynamic information during search is not quantified

## Confidence
- High confidence: Theoretical proofs for DYN-A* optimality under DYN-admissibility and non-reopening guarantees under DYN-monotonicity and DYN-consistency
- Medium confidence: Practical applicability to the three case studies (landmark progression, online abstraction refinement, lazy heuristic evaluation)
- Low confidence: Computational complexity analysis of DYN-A* compared to traditional A*

## Next Checks

1. Implement DYN-A* on a benchmark planning problem where dynamic heuristics naturally arise (e.g., with dynamically discovered landmarks) and compare its performance against static heuristic approaches to quantify the practical benefits and computational overhead.

2. Systematically relax each of the three conditions (DYN-admissibility, DYN-monotonicity, DYN-consistency) in controlled experiments to measure how violations affect solution quality and whether suboptimal solutions are produced.

3. Develop and test a variant of DYN-A* that incorporates heuristic learning or adaptation mechanisms to automatically adjust to non-compliant dynamic information, measuring whether learned adaptations can approximate the guarantees of the formal conditions.