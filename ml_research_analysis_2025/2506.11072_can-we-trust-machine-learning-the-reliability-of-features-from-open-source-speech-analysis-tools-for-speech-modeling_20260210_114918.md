---
ver: rpa2
title: Can We Trust Machine Learning? The Reliability of Features from Open-Source
  Speech Analysis Tools for Speech Modeling
arxiv_id: '2506.11072'
source_url: https://arxiv.org/abs/2506.11072
tags:
- speech
- features
- tools
- praat
- autism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the reliability of speech features extracted\
  \ from two widely used open-source tools, OpenSMILE and Praat, for machine learning-based\
  \ classification of autism in adolescents. The authors compare five key speech features\u2014\
  mean pitch, pitch standard deviation, pitch range, loudness, and speech rate\u2014\
  across the two tools and demographic groups."
---

# Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling

## Quick Facts
- arXiv ID: 2506.11072
- Source URL: https://arxiv.org/abs/2506.11072
- Reference count: 0
- OpenSMILE and Praat extract statistically different values for identical audio inputs

## Executive Summary
This study evaluates the reliability of speech features extracted from OpenSMILE and Praat for autism classification in adolescents. The authors find significant differences in feature values between tools, with Praat showing more variability and demographic sensitivity. These discrepancies affect classification performance, with models showing inconsistent precision, recall, and F1-scores depending on the tool used. The results highlight the need for domain-relevant validation of speech features to improve the reliability and fairness of machine learning models in clinical applications.

## Method Summary
The study analyzes ADOS-2 recordings from 29 adolescents (14 ASD, 15 TD) across 14 tasks. Audio is segmented using pyannote speaker diarization, then features are extracted using both OpenSMILE (eGeMAPSv02) and Praat (via Parselmouth). Five features are compared: mean pitch, pitch standard deviation, pitch range, loudness, and speech rate. Features are aggregated to task-level means per participant. Classification uses Random Forest with leave-one-user-out cross-validation. Statistical analysis includes paired t-tests and two-way ANOVAs to compare tool performance and demographic effects.

## Key Results
- OpenSMILE and Praat produce statistically different values for all five features (p < 0.001)
- Praat shows higher sensitivity to demographic group membership (gender, diagnosis) for certain features
- Classification performance varies significantly by tool, with differential impact across diagnostic groups
- Praat's speech rate estimates include implausible values (>10 syllables/second) due to intensity-based detection

## Why This Works (Mechanism)

### Mechanism 1
Different speech processing tools extract statistically different values for the same nominal features from identical audio inputs. OpenSMILE and Praat use different algorithms for pitch detection, loudness estimation, and syllable counting. OpenSMILE applies heuristic mappings from voiced segments to syllables (assuming 1.5 words per segment), while Praat uses intensity-based syllable detection over voiced duration. These algorithmic differences produce systematic measurement divergence.

### Mechanism 2
Praat exhibits higher sensitivity to demographic group membership (gender, diagnostic status) than OpenSMILE when extracting certain speech features. Praat's intensity-based syllable detection and pitch estimation are more sensitive to acoustic artifacts from diarization noise, clipped audio, and misattributed turns. This sensitivity creates differential measurement patterns across groups whose speech characteristics may interact with these artifacts.

### Mechanism 3
Feature extraction tool selection causally affects downstream ML classification performance, with differential impact across diagnostic groups. Classification models trained on features from different tools learn different decision boundaries because the input feature distributions differ. This produces varying precision, recall, and F1-scores across tools, with asymmetric effects on ASD versus TD classification.

## Foundational Learning

- **Speaker Diarization**: Why needed here - The study segments audio by speaker using pyannote before feature extraction. Understanding that diarization errors (misattributed turns, clipped audio) propagate into feature measurements is essential for interpreting Praat's higher speech rate variance.
  - Quick check question: If diarization incorrectly assigns a silence segment to a participant, which tool's speech rate estimate would be more affected—the one using voiced-segment heuristics or intensity-based syllable detection?

- **Leave-One-User-Out Cross-Validation**: Why needed here - The paper uses LOUO rather than k-fold to prevent data leakage from the same participant appearing in train and test sets. This is critical for clinical ML where within-subject correlation violates i.i.d. assumptions.
  - Quick check question: Why would standard 5-fold cross-validation overestimate generalization in a dataset where each participant contributes 116 utterances?

- **Feature Validity vs. Classification Utility**: Why needed here - The paper distinguishes between features that classify well and features that are clinically meaningful. High classification accuracy does not establish that features measure what they claim to measure (e.g., Praat's 54 syllables/second is behaviorally implausible).
  - Quick check question: A feature achieves 90% classification accuracy but produces values outside physically possible ranges for some samples. Should you deploy this model?

## Architecture Onboarding

- Component map: Raw ADOS-2 Video → Annotation (task boundaries) → pyannote Diarization → PyDub Segmentation → [OpenSMILE | Praat] Feature Extraction → Utterance-level features → Task-level aggregation → Random Forest Classifier → ASD/TD prediction

- Critical path: Feature extraction tool choice (OpenSMILE vs. Praat) → algorithm selection (voiced-segment heuristics vs. intensity-based detection) → feature value distributions → classification decision boundaries. This is the primary source of variance in study results.

- Design tradeoffs:
  - **OpenSMILE**: More stable speech rate estimates; less sensitive to diarization artifacts; potentially underestimates group differences. Better for reproducibility across recording conditions.
  - **Praat**: Higher sensitivity to acoustic variation; detects demographic group differences; produces implausible values for some samples. Better for exploratory analysis if artifacts are controlled.
  - **Default parameters**: Both tools used with defaults for accessibility, but defaults may not be optimal for adolescent clinical speech.

- Failure signatures:
  - Speech rate > 10 syllables/second (adult maximum is ~6-8): indicates intensity-based detection is counting noise or short segments as syllables.
  - Large variance in pitch measures for the same participant across tasks with similar speaking styles: suggests diarization or segmentation inconsistency.
  - Perfect precision with near-zero recall for one group: model has learned dataset-specific artifacts rather than generalizable features.

- First 3 experiments:
  1. **Tool agreement analysis**: Extract features from the same audio using both tools with documented parameter settings. Compute paired differences and correlation. Flag features where mean difference > 20% of either tool's mean.
  2. **Plausibility filter**: Apply physical constraints (e.g., speech rate 2-8 syllables/sec, pitch 50-500 Hz for adolescents) and measure what proportion of samples are filtered per tool. Investigate filtered samples for common patterns (segment length, background noise).
  3. **Stratified classification comparison**: Train classifiers separately for each demographic group × tool combination. Report precision, recall, F1 separately. Identify task × tool × group combinations with >15% F1 variance from the mean to prioritize for parameter tuning or feature replacement.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (29 participants) limits generalizability of findings
- Clinical ADOS-2 recordings with variable audio quality may introduce artifacts that differentially affect tool performance
- Default parameter settings for both tools may not be optimal for adolescent clinical speech analysis

## Confidence

- **High confidence**: Tool-specific feature extraction produces statistically different values (paired t-tests, p < 0.001)
- **Medium confidence**: Praat shows higher demographic sensitivity due to algorithmic differences in syllable detection
- **Medium confidence**: Feature extraction tool selection affects classification performance, though the clinical significance requires further validation

## Next Checks

1. Conduct tool agreement analysis on the same audio segments to quantify the magnitude and consistency of measurement differences across all five features.
2. Apply plausibility filters (e.g., speech rate 2-8 syllables/sec, pitch 50-500 Hz for adolescents) and analyze the proportion of filtered samples per tool to identify systematic artifacts.
3. Perform stratified classification by demographic group and task to isolate tool × group interactions and identify specific conditions where feature reliability breaks down.