---
ver: rpa2
title: 'PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial
  Decoding'
arxiv_id: '2602.01322'
source_url: https://arxiv.org/abs/2602.01322
tags:
- polysae
- feature
- features
- interactions
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolySAE addresses the fundamental limitation of sparse autoencoders
  (SAEs) that assume features combine linearly, preventing them from capturing compositional
  structure in language. The core method extends SAEs with a polynomial decoder that
  models pairwise and triple feature interactions through low-rank tensor factorization,
  preserving linear interpretability while adding only 3% parameter overhead on GPT2.
---

# PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding

## Quick Facts
- **arXiv ID:** 2602.01322
- **Source URL:** https://arxiv.org/abs/2602.01322
- **Reference count:** 40
- **Primary result:** PolySAE improves probing F1 by 8% and increases Wasserstein distances 2-10× by modeling feature interactions via polynomial decoding.

## Executive Summary
PolySAE addresses the fundamental limitation of sparse autoencoders (SAEs) that assume features combine linearly, preventing them from capturing compositional structure in language. The core method extends SAEs with a polynomial decoder that models pairwise and triple feature interactions through low-rank tensor factorization, preserving linear interpretability while adding only 3% parameter overhead on GPT2. Across four language models and three SAE variants, PolySAE achieves an average 8% improvement in probing F1 while maintaining comparable reconstruction error, and produces 2-10× larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights show negligible correlation with co-occurrence frequency (r=0.06 vs r=0.82 for SAE feature covariance), demonstrating that polynomial terms capture compositional structure independent of surface statistics.

## Method Summary
PolySAE extends standard SAEs by adding polynomial terms to the decoder. The encoder remains unchanged (linear projection + ReLU + TopK/BatchTopK/Matryoshka sparsification), while the decoder computes $\hat{x} = b_{dec} + y_1 + \lambda_2 y_2 + \lambda_3 y_3$, where $y_2$ and $y_3$ capture pairwise and triple feature interactions via low-rank projections through shared matrix $U$. This enables representations to "escape" the linear span of atomic features into orthogonal semantic dimensions. The method uses orthogonal projection constraints on $U$ via QR retraction to prevent degenerate solutions and maintain geometric distinctness of interaction directions.

## Key Results
- 8% average improvement in probing F1 across six tasks and three SAE variants
- 2-10× larger Wasserstein distances between class-conditional feature distributions
- Learned interaction weights show negligible correlation with co-occurrence frequency (r=0.06 vs r=0.82 for SAE feature covariance)
- 3% parameter overhead compared to baseline SAEs
- Reconstruction MSE remains comparable to standard SAEs

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Decoding Captures Non-Additive Feature Composition
Extending the SAE decoder with quadratic and cubic terms allows the model to represent compositional structure (e.g., morphological binding, phrasal composition) that linear SAEs cannot distinguish from co-occurrence. The decoder computes $\hat{x} = b_{dec} + y_1 + \lambda_2 y_2 + \lambda_3 y_3$, where $y_2$ and $y_3$ capture pairwise and triple feature interactions via low-rank projections. This enables representations to "escape" the linear span of atomic features into orthogonal semantic dimensions.

### Mechanism 2: Low-Rank Factorization Truncates Interaction Space Efficiently
Constraining interactions to a shared low-rank subspace ($R_2 = R_3 \ll d$) captures most compositional structure with minimal parameter overhead. All interactions project through shared matrix $U$ before forming element-wise products: $y_2 = (zU_{:,1:R_2})^2 C^{(2)\top}$. This imposes an inductive bias toward reusable interaction modes rather than arbitrary pairwise compositions.

### Mechanism 3: Orthogonal Projection Constraints Improve Identifiability
Enforcing $U^\top U = I$ via QR retraction prevents degenerate solutions and ensures geometrically distinct interaction directions. Stiefel manifold optimization removes rotational ambiguity, preventing the model from allocating redundant capacity to correlated directions.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed: PolySAE modifies only the decoder; understanding standard SAE encoder/decoder structure is prerequisite.
  - Quick check: Can you explain why SAEs use an overcomplete dictionary ($d_{sae} \gg d$) and how sparsity (e.g., TopK) enables feature disentanglement?

- **Concept: Volterra Series / Polynomial Expansions**
  - Why needed: PolySAE's decoder is a third-order Volterra expansion; understanding polynomial basis functions clarifies why higher-order terms capture interactions.
  - Quick check: Why does a polynomial kernel $k(x, y) = (x^\top y)^2$ implicitly compute all pairwise feature products?

- **Concept: Stiefel Manifold Optimization**
  - Why needed: Orthonormality constraints on $U$ require optimization on manifolds; standard gradient descent violates constraints.
  - Quick check: Why does a standard gradient step $U \leftarrow U - \eta \nabla_U L$ violate $U^\top U = I$, and how does QR retraction fix this?

## Architecture Onboarding

- **Component map:** Encoder (linear + ReLU + sparsification) -> Shared projection U -> Polynomial terms (linear, quadratic, cubic) -> Weighted sum with lambda coefficients -> Decoder output
- **Critical path:** 1) Forward pass: Encode -> Project via U -> Compute polynomial terms -> Weighted sum with lambda coefficients. 2) Backward pass: Standard gradients for most parameters; QR retraction for U after each update. 3) Initialization: U via QR decomposition of random matrix; lambda2=-0.5, lambda3=0.5 (empirically stable).
- **Design tradeoffs:** Higher R2,R3 gives more expressive interactions but diminishing returns (Figure 4 shows plateau at ~64 for GPT-2). Including cubic terms adds +1-2% parameter overhead but enables three-way binding. Sparsity level K: PolySAE shows advantage at all sparsity levels but gap widens at higher K (Figure 3).
- **Failure signatures:** MSE increases significantly (interaction ranks may be too low, or lambda coefficients collapsed to zero). Probing F1 unchanged from baseline (polynomial terms may be underutilized; check if lambda2, lambda3 remain near initial values). Training instability (orthonormality constraint violated; verify QR retraction is applied correctly). Interaction weights correlate with co-occurrence (model may have learned surface statistics; check if shared projection U has collapsed dimensions).
- **First 3 experiments:** 1) Reproduce ablation on ranks: Train PolySAE on a small model (Pythia-410M) with R2=R3 in {16,32,64,128} and plot reconstruction MSE vs probing F1 to find the elbow. 2) Verify co-occurrence decorrelation: For a trained PolySAE, compute correlation between B_ij (quadratic interaction strength) and empirical co-occurrence frequency; confirm r≈0.06 vs r≈0.82 for baseline SAE covariance. 3) Ablate cubic terms: Set lambda3=0 and compare probing F1 and Wasserstein distances to full PolySAE; quantify the marginal contribution of three-way interactions on your target domain.

## Open Questions the Paper Calls Out

- **Question:** How does PolySAE scale to larger language models beyond 2B parameters?
  - Basis: The limitations section states "We study models up to 2B parameters" despite noting general applicability.
  - Why unresolved: Computational constraints limited experiments to GPT-2 Small, Pythia-410M/1.4B, and Gemma-2-2B; scaling behavior of polynomial decoding with model dimension is unknown.
  - Evidence: Evaluation of PolySAE on models like Llama-7B/70B or Gemma-2-9B/27B, measuring whether the 8% F1 improvement and parameter efficiency (3% overhead) persist.

- **Question:** Does PolySAE benefit non-forced-sparsity SAE variants such as Gated or JumpReLU SAEs?
  - Basis: Limitations note that experiments are "restrict[ed] to forced-sparsity SAE variants" (TopK, BatchTopK, Matryoshka), despite mentioning Gated and JumpReLU in related work.
  - Why unresolved: The interaction between polynomial decoding and alternative sparsification mechanisms (learned thresholds, gating) has not been tested.
  - Evidence: Training PolySAE with Gated and JumpReLU architectures and comparing reconstruction/probing metrics against their linear baselines.

## Limitations

- Experiments are restricted to models up to 2B parameters despite general applicability.
- Only forced-sparsity SAE variants (TopK, BatchTopK, Matryoshka) were evaluated, excluding Gated and JumpReLU.
- Claims about capturing compositional structure rely heavily on indirect metrics rather than explicit linguistic analysis.

## Confidence

- **High:** Reconstruction error remains comparable to SAEs; parameter overhead is minimal (3%); training stability with QR retraction is demonstrated.
- **Medium:** Interaction weights are decorrelated from co-occurrence (r=0.06); polynomial terms improve probing F1 by 8% on average; increased Wasserstein distances indicate distributional separation.
- **Low:** Claims about "capturing compositional structure" rely heavily on indirect metrics; no explicit control for whether interactions simply memorize surface statistics in a more complex way.

## Next Checks

1. **Ablate orthonormality:** Train PolySAE with and without QR retraction (keeping other parameters fixed) to measure impact on probing F1 and interaction identifiability.
2. **Vary interaction ranks:** Systematically sweep R2=R3 in {32, 64, 128, 256} on a held-out model (e.g., Pythia-410M) to identify the elbow point and test the low-rank assumption.
3. **Correlation robustness:** Compute feature covariance vs interaction weight correlation across multiple SAE variants (TopK, BatchTopK, Matryoshka) to ensure the decorrelation is not specific to a single sparsifier.