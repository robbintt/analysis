---
ver: rpa2
title: 'Position: Mechanistic Interpretability Should Prioritize Feature Consistency
  in SAEs'
arxiv_id: '2505.20254'
source_url: https://arxiv.org/abs/2505.20254
tags:
- feature
- features
- consistency
- saes
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that feature consistency\u2014the reliable convergence\
  \ of Sparse Autoencoder (SAE) features across independent training runs\u2014should\
  \ be prioritized in mechanistic interpretability research. The authors propose using\
  \ Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric\
  \ to quantify this consistency and demonstrate that high consistency levels (PW-MCC\
  \ \u2248 0.80) are achievable with appropriate SAE architectures like TopK SAEs."
---

# Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs

## Quick Facts
- arXiv ID: 2505.20254
- Source URL: https://arxiv.org/abs/2505.20254
- Reference count: 40
- Primary result: TopK SAEs achieve PW-MCC ≈ 0.80-0.97 when k-sparsity and reconstruction objectives are satisfied jointly.

## Executive Summary
This paper argues that feature consistency—the reliable convergence of Sparse Autoencoder (SAE) features across independent training runs—should be prioritized in mechanistic interpretability research. The authors propose using Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to quantify this consistency and demonstrate that high consistency levels are achievable with appropriate SAE architectures like TopK SAEs. The key contributions include theoretical grounding connecting SAEs to identifiability results in overcomplete sparse dictionary learning, synthetic validation showing PW-MCC reliably tracks ground-truth feature recovery, and real-world experiments on LLM activations demonstrating that high PW-MCC correlates with semantic similarity of feature explanations.

## Method Summary
The paper proposes using Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) to measure feature consistency across independently trained SAEs. PW-MCC computes the maximum mean correlation between matched feature pairs from two dictionaries using the Hungarian algorithm. The theoretical framework connects k-sparsity, spark condition, and feature consistency, showing that TopK SAEs can achieve strong consistency when k-sparsity and reconstruction objectives are satisfied jointly. Synthetic experiments validate PW-MCC as a reliable proxy for ground-truth feature recovery, while real-world experiments demonstrate frequency-dependent consistency patterns in LLM activations.

## Key Results
- TopK SAEs achieve strong feature consistency (PW-MCC ≈ 0.80-0.97) when k-sparsity and reconstruction objectives are satisfied jointly
- PW-MCC serves as a practical proxy for ground-truth feature recovery when ground truth is unavailable
- Feature consistency is frequency-dependent: high-activation-frequency features exhibit higher inter-run similarity than rare features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TopK SAEs can achieve strong feature consistency (PW-MCC ≈ 0.80–0.97) when k-sparsity and reconstruction objectives are satisfied jointly.
- Mechanism: TopK enforces exact k-sparse activations structurally. When combined with zero reconstruction error and the "round-trip property" (encoder(decoder(f)) = f), the learned dictionary satisfies the spark condition (spark(A) > 2k). This algebraic property guarantees unique sparse representations up to permutation and scaling, enabling independent training runs to converge to equivalent dictionaries.
- Core assumption: The training data contains sufficient coverage of k-sparse activation patterns (reachability condition).
- Evidence anchors:
  - [section 4.1]: "SAEs with k-sparsity and minimal reconstruction error satisfy strong feature consistency when the learned dictionary meets the spark condition."
  - [Figure 2]: Shows TopK SAE achieving GT-MCC ≈ 0.97 in synthetic matched settings, with PW-MCC converging to GT-MCC.
  - [corpus]: Related work on SAE interpretability (e.g., "Resurrecting the Salmon") discusses capacity allocation but does not provide formal identifiability guarantees.
- Break condition: If k is severely misspecified relative to true sparsity, or if data coverage is insufficient, the spark condition fails and consistency degrades.

### Mechanism 2
- Claim: PW-MCC serves as a practical proxy for ground-truth feature recovery when ground truth is unavailable.
- Mechanism: PW-MCC computes the maximum mean correlation between matched feature pairs from two independently trained dictionaries using the Hungarian algorithm. In synthetic experiments, PW-MCC tracks GT-MCC closely (Figure 2), providing a lower bound signal for dictionary quality.
- Core assumption: The relationship between PW-MCC and GT-MCC observed in synthetic settings generalizes to real LLM activations.
- Evidence anchors:
  - [abstract]: "Synthetic experiments validate PW-MCC as a reliable proxy for ground-truth feature recovery."
  - [Figure 2]: PW-MCC and GT-MCC curves converge to comparable values for TopK SAE.
  - [corpus]: No direct corpus validation of PW-MCC as a proxy metric; related SAE benchmarking work (SAEBench) focuses on reconstruction and interpretability scores.
- Break condition: If dictionaries operate in globally redundant regimes (dsae >> dgt), PW-MCC may underestimate consistency due to selection ambiguity.

### Mechanism 3
- Claim: Feature consistency is frequency-dependent: high-activation-frequency features exhibit higher inter-run similarity than rare features.
- Mechanism: Under Zipfian feature distributions, SAEs allocate dictionary capacity disproportionately to frequent clusters (Di ∝ p_i^β, β ≈ 1.4). This creates local identifiability regimes—frequent features become locally redundant or matched (stable), while rare features remain locally compressive (unstable).
- Core assumption: Real LLM activation distributions follow power-law-like frequency patterns similar to the synthetic model.
- Evidence anchors:
  - [section 4.3]: "SAEs do not allocate their dictionary capacity uniformly across these clusters."
  - [Table 1]: TopK SAE features with highest activation frequency show similarity 0.964 vs. 0.514 for rarest features.
  - [corpus]: "Resurrecting the Salmon" similarly notes SAEs capture high-frequency patterns but miss "dark matter" features.
- Break condition: If dictionary capacity is vastly increased, rare features may achieve sufficient local redundancy, improving their consistency.

## Foundational Learning

- Concept: **Spark condition and k-injectivity**
  - Why needed here: The theoretical guarantee for feature consistency relies on understanding when a dictionary admits unique sparse representations.
  - Quick check question: Given a dictionary A, what is the minimum sparsity level k such that any two k-sparse vectors producing the same output must be identical?

- Concept: **Hungarian algorithm for optimal matching**
  - Why needed here: PW-MCC requires computing the best one-to-one matching between feature dictionaries to measure consistency.
  - Quick check question: How does the Hungarian algorithm find the optimal assignment, and what is its time complexity O(n³) implication for large dictionaries?

- Concept: **Overcomplete dictionary learning**
  - Why needed here: SAEs operate in overcomplete regimes (dsae > m), where standard identifiability results do not apply without sparsity constraints.
  - Quick check question: Why does having more dictionary atoms than input dimensions create non-uniqueness, and how does sparsity resolve this?

## Architecture Onboarding

- Component map:
  - Input activations → Encoder (W_enc × x + b_enc + TopK) → Sparse codes → Decoder (W_dec × f + b_dec) → Reconstructed input
  - PW-MCC computation: Pairwise cosine similarity matrix between two dictionaries → Hungarian matching → Mean of matched similarities

- Critical path:
  1. Select target sparsity k (sweep k values, monitor PW-MCC)
  2. Train multiple SAEs with same data/hyperparameters, different seeds
  3. Compute PW-MCC at regular intervals; continue training until saturation
  4. Analyze frequency-consistency relationship to diagnose capacity issues

- Design tradeoffs:
  - **k selection**: Lower k → more sparsity but potential under-capacity; higher k → better reconstruction but selection ambiguity. Optimal k ≈ true sparsity
  - **Dictionary size dsae**: Larger dsae → better coverage of rare features but higher computational cost and potential selection ambiguity
  - **Architecture choice**: TopK > Gated > Standard for consistency (Figure 7), but Standard may offer more flexibility for certain distributions

- Failure signatures:
  - Low PW-MCC (< 0.5) with acceptable reconstruction: indicates feature instability, likely from over-sparsity or capacity mismatch
  - High proportion of dead features (near-zero activation frequency): suggests initialization or sparsity penalty issues
  - PW-MCC saturating but GT-MCC (if known) still low: indicates systematic bias in learned features

- First 3 experiments:
  1. **Baseline consistency sweep**: Train TopK SAEs on Pythia-160M layer 8 with k ∈ {20, 40, 80}, 3 seeds each. Compute PW-MCC curves to identify optimal k
  2. **Frequency-consistency diagnosis**: For best k setting, plot feature-level similarity vs. min activation frequency to confirm frequency-dependent pattern
  3. **Capacity stress test**: Double dictionary size (dsae = 32768) and re-measure PW-MCC to assess whether rare feature consistency improves

## Open Questions the Paper Calls Out
None

## Limitations
- Theory-Practice Gap: While the spark condition provides theoretical guarantees for feature consistency, the paper assumes real LLM activation distributions sufficiently resemble the synthetic matched setting.
- Frequency Distribution Assumptions: The frequency-dependent consistency mechanism relies on Zipfian feature distributions, which requires empirical validation across different model families.
- Metric Limitations: PW-MCC measures pairwise feature similarity but may miss higher-order feature interactions or compositional properties.

## Confidence
- **High Confidence**: The theoretical framework connecting k-sparsity, spark condition, and feature consistency is mathematically rigorous.
- **Medium Confidence**: The claim that PW-MCC reliably proxies ground-truth feature recovery for real SAEs on LLM activations.
- **Medium Confidence**: The frequency-dependent consistency mechanism is supported by empirical data but relies on distribution assumptions.

## Next Checks
1. **Cross-Model Validation**: Test PW-MCC as a consistency metric across diverse model architectures (LLaMA, GPT-2, Claude) and layer types to assess whether the frequency-dependent consistency pattern generalizes beyond Pythia-160M.

2. **Capacity Stress Testing**: Systematically vary dictionary size (dsae) from minimal to massively overcomplete regimes to determine whether rare feature consistency improves with capacity and to quantify the selection ambiguity threshold.

3. **Semantic Validation**: Conduct human evaluations comparing feature interpretations from high-consistency vs. low-consistency runs to verify that PW-MCC > 0.80 correlates with interpretable, non-redundant feature sets in real applications.