---
ver: rpa2
title: Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large
  Language Models
arxiv_id: '2602.02467'
source_url: https://arxiv.org/abs/2602.02467
tags:
- belief
- language
- beliefs
- bcounter
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether large language models exhibit belief-guided
  agency and meta-cognitive monitoring by operationalizing the HOT-3 consciousness
  indicator. Beliefs are defined as latent concept representations in the model's
  hidden states, and actions as final answers.
---

# Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models

## Quick Facts
- **arXiv ID**: 2602.02467
- **Source URL**: https://arxiv.org/abs/2602.02467
- **Reference count**: 40
- **Primary result**: Empirical support for belief-guided agency and meta-cognitive monitoring in LLMs via HOT-3 operationalization

## Executive Summary
This work investigates whether large language models exhibit belief-guided agency and meta-cognitive monitoring by operationalizing the HOT-3 consciousness indicator. Beliefs are defined as latent concept representations in the model's hidden states, and actions as final answers. A Belief Dominance metric quantifies belief strength by measuring how easily beliefs can be decoded from hidden states using the Patchscopes framework. Experiments on Llama-3 70B and Gemma-3 27B using factual knowledge and Winograd schema tasks show: (1) external manipulations systematically modulate internal belief formation (BDDiff shifts of -0.49 to +0.18), (2) belief dominance causally drives action selection with 66.7%-85.4% steering success via targeted interventions, and (3) models can monitor and report their own belief states with accuracy 0.39-0.54 above chance in neurofeedback experiments. These findings provide empirical support for structured belief-guided agency and meta-cognitive abilities in LLMs.

## Method Summary
The study uses the Patchscopes framework to decode latent belief representations from hidden states by injecting them into neutral prompts and measuring belief verbalization presence. Two tasks are used: Factual Knowledge (FK) from CounterFact dataset (13,564-16,936 questions) and Winograd Schema (WS) from Definite Pronoun Resolution dataset (1,386-1,455 questions). Belief Dominance (BD) scores are computed by averaging decoding scores across layers and positions, with BDDiff measuring the difference between competing beliefs. Steering interventions amplify counter-belief representations at specific positions with scale α=2 and n=10 repetitions. Neurofeedback experiments use 30-shot few-shot classification to classify BD scores into discrete levels via k-means clustering.

## Key Results
- External manipulations systematically modulate internal belief formation with BDDiff shifts of -0.49 to +0.18
- Belief dominance causally drives action selection with 66.7%-85.4% steering success rates
- Models can monitor and report their own belief states with accuracy 0.39-0.54 above chance

## Why This Works (Mechanism)

### Mechanism 1: Belief Formation via Competing Latent Representations
- Claim: External inputs systematically modulate which beliefs dominate in latent space during generation.
- Mechanism: Inputs create competing latent representations (e.g., "Paris" vs. "New York"); source credibility and instructions shift relative dominance measured via patchscope decodability.
- Core assumption: Representations more easily decoded into text indicate stronger functional influence on downstream computation.
- Evidence anchors: [abstract] "external manipulations systematically modulate internal belief formation"; [section 5.1] BDDiff shifts significantly across manipulations (e.g., Reliable vs. Unreliable Source: Δ = -0.18 in Gemma).

### Mechanism 2: Belief Dominance Drives Action Selection Causally
- Claim: Dominant beliefs in latent space causally determine the final action/output.
- Mechanism: Amplifying a belief representation during generation (via injection) shifts logit margins and flips final answers at 66.7%-85.4% success rates.
- Core assumption: Injection-based perturbation reveals causal role rather than just correlation.
- Evidence anchors: [abstract] "belief dominance causally drives action selection (with 66.7%-85.4% steering success rates)"; [section 5.2] Injection of counter-belief shifts FK task answers in 85.4% (Gemma) and 75.5% (Llama) of cases.

### Mechanism 3: Meta-cognitive Monitoring of Belief States
- Claim: LLMs can monitor and report internal belief dominance via few-shot classification without explicit label semantics.
- Mechanism: Models classify their own BD scores into discrete labels given only labeled exemplars, performing above chance and shifting predictions after internal state intervention.
- Core assumption: Above-chance classification reflects internal access to belief states rather than surface pattern matching.
- Evidence anchors: [abstract] "models can monitor and report their own belief states through neurofeedback-style classification tasks"; [section 6] Gemma achieves 0.42-0.48 accuracy on FK (vs. 0.33 chance); causal injection shifts prediction distributions.

## Foundational Learning

- **Concept: Patchscopes Framework**
  - Why needed here: Core method for decoding latent representations by injecting hidden states into separate inference passes.
  - Quick check question: Can you explain how patching a hidden state into a neutral prompt enables measurement of representation content?

- **Concept: Higher-Order Theories (HOT) of Consciousness**
  - Why needed here: HOT-3 is the theoretical basis; requires understanding meta-cognitive monitoring as higher-order representation of first-order states.
  - Quick check question: What distinguishes first-order (direct perception) from higher-order (awareness of perception) representations in HOT frameworks?

- **Concept: Belief Dominance Metric (BD/BDDiff)**
  - Why needed here: Quantifies which belief is more strongly encoded across layers and positions during generation.
  - Quick check question: Why does averaging ψ scores across layers and positions improve robustness compared to single-position measurements?

## Architecture Onboarding

- **Component map**: Input Layer -> Belief Formation -> Belief Dominance Calculation -> Action Selection -> Meta-cognitive Branch
- **Critical path**:
  1. Select layer window (validation set optimization)
  2. Generate reasoning trace with "Final answer:" delimiter
  3. Extract hidden states at each position within window
  4. Patch into neutral prompts at multiple target layers
  5. Score presence of belief verbalizations → compute BDDiff
  6. For intervention: identify position encoding target belief, inject scaled vector repeatedly

- **Design tradeoffs**:
  - Layer window vs. full model: Window improves signal-to-noise but may miss distributed representations
  - Binary ψ vs. continuous scoring: Binary simplifies but loses granularity
  - Single vs. multiple target layers: Multiple layers increase recall but computational cost

- **Failure signatures**:
  - WS task shows weaker signal than FK (entangled representations, semantically linked candidates)
  - Llama on WS: predictions collapse to "low" class regardless of intervention (insufficient class separability)
  - High variance in steering success across models (Gemma more susceptible to counter-belief injection than Llama in FK)

- **First 3 experiments**:
  1. Replicate BDDiff modulation: Apply manipulation pairs (Reliable vs. Unreliable Source) on 50 FK examples; verify BDDiff shifts in expected direction with statistical significance.
  2. Steering intervention pilot: Select 20 FK examples; inject counter-belief at optimized layer/scale; measure logit margin shift and answer flip rate.
  3. Neurofeedback classification baseline: Run 30-shot BD classification on held-out FK queries; compare accuracy vs. chance (0.33) and test intervention-induced prediction shifts on 10 examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural components and factors mechanistically drive convergence to a particular belief during generation?
- Basis in paper: [explicit] The authors state "it remains unclear what factors and components drive convergence to a specific option."
- Why unresolved: The paper demonstrates that belief dominance correlates with and causally influences action selection, but does not identify which attention heads, layers, or circuits implement the convergence mechanism.
- What evidence would resolve it: Ablation studies targeting specific attention heads or MLP layers, coupled with causal tracing methods to identify which components are necessary and sufficient for belief selection.

### Open Question 2
- Question: How is meta-cognitive monitoring mechanistically implemented, and how does it update latent beliefs during generation?
- Basis in paper: [explicit] The authors note "how meta-cognitive monitoring is realized and utilized to update latent beliefs" remains to be studied.
- Why unresolved: The neurofeedback experiments demonstrate that models can report their belief states, but do not reveal the computational mechanism by which monitoring signals influence belief updates.
- What evidence would resolve it: Identification of internal signals that correlate with meta-cognitive judgments, followed by interventions that disrupt or enhance these signals and measure effects on belief updating.

### Open Question 3
- Question: How does the framework extend to settings with more than two competing beliefs?
- Basis in paper: [explicit] The authors identify "extending our framework to more than two competing beliefs" as a valuable extension.
- Why unresolved: Current experiments only test binary belief competition (e.g., Paris vs. New York), but real-world scenarios often involve multiple plausible alternatives with graded confidence.
- What evidence would resolve it: Experiments using prompts that induce three or more competing beliefs, testing whether BDDiff generalizes and whether belief dynamics exhibit consistent patterns across multi-belief landscapes.

## Limitations

- **Measurement sensitivity**: BDDiff metric shows systematic modulation but effect sizes are small (maximum shift of 0.49), raising questions about practical significance versus statistical significance.
- **Task generalizability**: Strong performance on FK tasks contrasts with weak or null results on WS tasks where beliefs are entangled, suggesting limited domain applicability.
- **Neurofeedback interpretation**: Above-chance classification accuracy (0.39-0.54) provides weak evidence that could equally support surface pattern matching rather than genuine internal state access.

## Confidence

**High confidence**: The systematic modulation of BDDiff by external manipulations (Reliable vs. Unreliable Source producing Δ = -0.18 in Gemma) is robust across multiple examples and manipulations.

**Medium confidence**: The causal steering effect showing 66.7%-85.4% success rates is reproducible but mechanism uncertainty remains about whether direct belief representation manipulation versus pathway disruption drives the effect.

**Low confidence**: The neurofeedback meta-cognitive monitoring claims (0.39-0.54 accuracy above chance) rest on weak statistical evidence that could equally support surface pattern matching.

## Next Checks

**Check 1**: Implement ablation study varying injection scale α (1.0, 2.0, 3.0) and repetition n (5, 10, 15) to determine optimal steering parameters and assess whether effects persist across hyperparameter ranges.

**Check 2**: Design controlled experiment where injection occurs at random positions with no belief content versus target belief positions, measuring whether steering success rate exceeds random chance.

**Check 3**: Construct synthetic neurofeedback task where ground truth belief states are known through controlled prompt engineering, testing whether models can accurately classify their own belief states when external validity can be verified.