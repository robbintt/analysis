---
ver: rpa2
title: 'From Data to Dialogue: Unlocking Language for All'
arxiv_id: '2512.15552'
source_url: https://arxiv.org/abs/2512.15552
tags:
- list
- word
- coverage
- words
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable solution for vocabulary learning
  by generating text-specific word lists that guarantee at least 95% coverage. Unlike
  static, corpus-based word lists, the method uses frequency analysis of the target
  text itself to create specialized word lists (SWLs) that outperform the New General
  Service List (NGSL).
---

# From Data to Dialogue: Unlocking Language for All

## Quick Facts
- arXiv ID: 2512.15552
- Source URL: https://arxiv.org/abs/2512.15552
- Authors: Dakota Ellis; Samy Bakikerali; Wanshan Chen; Bao Dinh; Uyen Le
- Reference count: 21
- Primary result: Text-specific word lists (SWLs) achieve 95% vocabulary coverage with fewer words than static corpus-based lists like NGSL

## Executive Summary
This paper proposes a scalable solution for vocabulary learning by generating text-specific word lists that guarantee at least 95% coverage. Unlike static, corpus-based word lists, the method uses frequency analysis of the target text itself to create specialized word lists (SWLs) that outperform the New General Service List (NGSL). In experiments, SWLs achieved 95% coverage with fewer words than NGSL across multiple text types, including textbooks, novels, and movie scripts. The approach leverages Zipf's law to prioritize the most frequent words, making it automatable and adaptable to diverse reading materials.

## Method Summary
The methodology involves preprocessing target texts through tokenization, optional stop word removal, proper noun filtering, and lemmatization. Word frequencies are computed and sorted in descending order, then words are selected cumulatively until reaching 95% coverage. The pipeline uses NLTK for tokenization and stop word removal, SpaCy for lemmatization, and Stanza for proper noun detection. The process is fully automatable using objective frequency criteria only, enabling scalability and customization for different learner needs.

## Key Results
- SWLs achieved 95% coverage with 224-2,077 words versus NGSL's 2,809 words
- Across multiple text types (textbooks, novels, movie scripts), SWLs consistently outperformed NGSL in coverage efficiency
- The approach successfully leverages Zipf's law to create compact high-coverage lists
- Automation via objective-only criteria enables scalability and removal of expert bottlenecks

## Why This Works (Mechanism)

### Mechanism 1: Zipf's Law Enables Compact High-Coverage Lists
- Claim: A small subset of high-frequency words can achieve 95% text coverage because word frequencies follow a power-law distribution.
- Mechanism: Rank words by frequency within the target text; cumulative coverage rises sharply with top-ranked words, then diminishes. Selecting words from rank 1 to rank p (where cumulative coverage = 95%) yields a minimal sufficient vocabulary set.
- Core assumption: The target text follows Zipf's law (inverse proportionality of frequency to rank).
- Evidence anchors:
  - [abstract]: "The approach leverages Zipf's law to prioritize the most frequent words, making it automatable and adaptable to diverse reading materials."
  - [section 2.5]: "Crucially, this strategy aligns with Zipf's law, which states that word frequencies in any natural text follow a power-law distribution... a few extremely frequent words cover a large portion of the text."
- Break condition: Texts with uniform word distributions (e.g., certain constrained poetry) or languages that do not follow Zipf's law will not yield compact lists via this method.

### Mechanism 2: Text-Specific Frequency Profiles Outperform Static General Lists
- Claim: Deriving word lists from the target text's own frequency profile achieves higher coverage with fewer words than applying a static corpus-derived General Service List (GSL).
- Mechanism: SWL extraction ranks headwords by occurrence within the specific document, capturing domain-specific vocabulary, proper nouns, and author-specific coinages that GSLs miss. NGSL's 2,809 headwords cover 79–85% on tested texts; SWLs achieve 95% with 224–2,077 words.
- Core assumption: Learners benefit more from text-specific vocabulary preparation than from general-purpose lists.
- Evidence anchors:
  - [abstract]: "SWLs achieved 95% coverage with fewer words than NGSL across multiple text types, including textbooks, novels, and movie scripts."
  - [section 4, Results table]: SWL achieves 95% coverage vs. NGSL's 64–85% across tested materials.
- Break condition: For learners needing broad, transferable vocabulary across many texts, SWLs may underprepare them compared to GSLs. Extremely short texts may produce unstable frequency profiles.

### Mechanism 3: Automation via Objective-Only Criteria
- Claim: Restricting word list generation to objective frequency criteria enables full automation, scalability, and removal of expert/subjective bottlenecks.
- Mechanism: Pipeline applies tokenization → frequency counting → lemmatization → rank-order selection → cumulative coverage thresholding. No manual headword curation or domain expertise required.
- Core assumption: Frequency alone is a sufficient proxy for vocabulary importance.
- Evidence anchors:
  - [abstract]: "By restricting the SWL process to objective criteria only, it can be automated, scaled, and tailored to the needs of language-learners across the globe."
  - [section 3.4]: "The model was designed using Python... The libraries utilized... are NLTK, SpaCy, and Stanza."
- Break condition: Frequency-only selection ignores semantic importance, collocational value, or pedagogical sequencing. High-frequency but low-information words may dominate if stop word removal is misconfigured.

### Mechanism 4: Modularity Handles Edge Cases
- Claim: Parameterized preprocessing (lemmatization, stop word removal, proper noun handling, coverage target) allows users to adapt output to specific contexts.
- Mechanism: User-configurable flags control whether to lemmatize, remove proper nouns, adjust coverage threshold, or cap list size—addressing issues like "Rose" (character name) being lemmatized to "Rise."
- Core assumption: Users have sufficient understanding of parameters to configure appropriately.
- Evidence anchors:
  - [section 5.6]: "The model that we constructed will take parameters specified by the user... lemmatize their list, remove stop words, adjust coverage score, remove proper nouns, or limit the word list to a specific size."
- Break condition: Misconfigured parameters produce unusable lists (e.g., retaining proper nouns that overwhelm lists for character-heavy fiction).

## Foundational Learning

- Concept: Zipf's Law / Power-Law Distribution
  - Why needed here: Core theoretical justification for why a small ranked subset achieves high coverage.
  - Quick check question: Given a text where the most frequent word appears 1,000 times and rank follows Zipf's law, approximately how often would the 10th-ranked word appear?

- Concept: Lemmatization vs. Word Families
  - Why needed here: Determines how variants (run/running/ran) are grouped into headwords, directly affecting list size and coverage calculations.
  - Quick check question: If "running" appears 50 times and "run" appears 100 times in a text, what is the combined coverage if lemmatized to "run"?

- Concept: Lexical Coverage Threshold (95–98%)
  - Why needed here: The 95% target is the paper's stopping criterion and is linked to reading comprehension research.
  - Quick check question: If a 10,000-word text has 9,500 known tokens for a learner, what is their coverage percentage, and is it above the comprehension threshold?

## Architecture Onboarding

- Component map: Raw text -> Tokenization (NLTK) -> Stop word removal (NLTK, optional) -> Proper noun detection/filtering (Stanza, optional) -> Lemmatization (SpaCy, optional) -> Frequency counting -> Rank ordering -> Cumulative coverage calculation -> Output ranked word list

- Critical path:
  1. Load and tokenize input text
  2. Apply configurable preprocessing filters
  3. Count word frequencies and compute coverage contributions
  4. Select top-ranked words until cumulative coverage reaches threshold
  5. Export ranked list with coverage metadata

- Design tradeoffs:
  - Lemmatize before vs. after thresholding: Pre-lemmatization reduces list size but may conflate distinct senses; post-lemmatization preserves distinctions but may include redundant variants.
  - Proper noun retention: Retaining aids fiction comprehension; removing focuses on general vocabulary.
  - Stop word removal: Removing common function words may help focus on content vocabulary but alters coverage calculations.

- Failure signatures:
  - Coverage never reaches 95%: Text too short, or preprocessing removes too many tokens.
  - List dominated by character names: Proper noun filtering disabled for fiction with frequent named entities.
  - Lemma collisions: Named entities conflated with common verbs (e.g., "Rose" → "rise").
  - Language mismatch: Zipf's law assumption invalid for target language.

- First 3 experiments:
  1. Reproduce SWL vs. NGSL comparison on one provided text (e.g., Alice in Wonderland) to validate coverage calculation pipeline.
  2. Test sensitivity: Run pipeline with lemmatization enabled vs. disabled on same text; compare list size and coverage.
  3. Edge case probe: Run pipeline on a very short text (<500 words) and on a text with invented vocabulary (e.g., fantasy excerpt) to observe where assumptions break.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does lemmatizing a corpus before frequency reduction yield the same or better results in terms of list accuracy and computational efficiency compared to lemmatizing after reducing the corpus to the target coverage threshold?
- Basis in paper: [explicit] The Future Work section explicitly asks: "Further research should be done to find if lemmatizing before reducing to the 95% coverage word list yields the same or very similar results as lemmatizing after reducing the corpus."
- Why unresolved: The authors note that lemmatization is resource-intensive, but they did not empirically test the trade-offs (accuracy vs. time complexity) of changing the operation order in their current methodology.
- What evidence would resolve it: A comparative benchmark showing the resulting word lists and processing times for both operation orders applied to identical, large-scale corpora (e.g., the Google Ngram Corpus).

### Open Question 2
- Question: How does the exclusion of semantic weighting or "information gained" in the Specialized Word List (SWL) model affect practical reading comprehension compared to lists that prioritize key conceptual terms?
- Basis in paper: [inferred] The Limitations section states that the SWLs are "only frequency based and do not account for information gained by each word," potentially leading to a loss of in-context significance.
- Why unresolved: The current study assumes that high-frequency words (driven by Zipf's law) are the only necessary target for comprehension, without validating if low-frequency, high-information words are critical for understanding specific texts.
- What evidence would resolve it: A user study measuring comprehension differences between learners using frequency-only SWLs versus learners using lists modified to include words with high semantic keyness or tf-idf scores.

### Open Question 3
- Question: Is the SWL generation methodology robust for languages that possess morphological structures different from English or that do not strictly adhere to Zipf's law?
- Basis in paper: [inferred] The Limitations section notes that "not all languages will follow Zipf's Law" and that the current methodology is "specific only to the languages that do."
- Why unresolved: The research was conducted exclusively on English texts using English-specific NLP tools (NLTK, SpaCy), leaving the generalizability of the power-law assumption for other languages untested.
- What evidence would resolve it: Experimental results applying the same SWL algorithm to non-English corpora (e.g., agglutinative languages) to verify if the 95% coverage threshold is achievable with a practical list size.

## Limitations
- Zipf's law assumption validity across languages: The method's core mechanism relies on Zipf's law holding across diverse text types and languages, which is not empirically validated across all tested languages.
- Frequency-only metric adequacy: The approach assumes frequency is a sufficient proxy for vocabulary importance, potentially overlooking semantic value, collocational strength, or pedagogical sequencing needs.
- GSL comparison context: The NGSL comparison shows SWLs achieving higher coverage with fewer words, but the evaluation is self-contained rather than validated against independent corpora or comprehension testing.

## Confidence
- High Confidence: The automation pipeline (tokenization → frequency → ranking → selection) is technically sound and reproducible. The modular architecture is well-specified.
- Medium Confidence: The claim that SWLs achieve 95% coverage with fewer words than NGSL is supported by internal experiments but lacks external corpus validation.
- Medium Confidence: The mechanism leveraging Zipf's law is theoretically justified but not empirically validated across diverse languages and text types.
- Low Confidence: The pedagogical benefit of text-specific vs. general vocabulary preparation is assumed rather than tested with learners.

## Next Checks
1. **Cross-Language Zipf Validation**: Test the pipeline on texts from multiple languages (e.g., English, Japanese, Arabic) to verify Zipf's law applicability and coverage achievement across language families.

2. **Learner Comprehension Validation**: Conduct a controlled study comparing reading comprehension outcomes between learners using SWLs vs. NGSL for the same target texts, measuring both vocabulary acquisition and text understanding.

3. **Parameter Sensitivity Analysis**: Systematically test pipeline performance across different parameter configurations (lemmatization on/off, proper noun retention, stop word removal) on diverse text types to identify optimal default settings and failure modes.