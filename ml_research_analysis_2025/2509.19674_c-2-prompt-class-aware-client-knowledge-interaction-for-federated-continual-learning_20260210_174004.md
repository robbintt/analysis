---
ver: rpa2
title: 'C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual
  Learning'
arxiv_id: '2509.19674'
source_url: https://arxiv.org/abs/2509.19674
tags:
- learning
- prompt
- knowledge
- class
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of Federated Continual Learning\
  \ (FCL), which involves learning from continuously emerging task data across distributed\
  \ clients while addressing both temporal and spatial forgetting. The authors propose\
  \ C\xB2Prompt, a novel method that enhances class-wise knowledge coherence between\
  \ prompts across clients during prompt communication."
---

# C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning

## Quick Facts
- **arXiv ID:** 2509.19674
- **Source URL:** https://arxiv.org/abs/2509.19674
- **Reference count:** 40
- **Primary result:** Proposed C²Prompt method improves FCL performance by 2.51% and 2.90% on ImageNet-R and DomainNet respectively

## Executive Summary
This paper addresses Federated Continual Learning (FCL), where distributed clients learn from continuously emerging task data while combating both temporal and spatial forgetting. The authors propose C²Prompt, a novel method that enhances class-wise knowledge coherence between prompts across clients during prompt communication. C²Prompt introduces two key components: a local class distribution compensation mechanism (LCDC) to improve intra-class knowledge consistency across clients, and a class-aware prompt aggregation scheme (CPA) to alleviate inter-class knowledge confusion. The method is evaluated on multiple FCL benchmarks including ImageNet-R, DomainNet, and CIFAR-100, demonstrating state-of-the-art performance.

## Method Summary
C²Prompt employs a dual-prompt architecture on a frozen ViT-B/16 backbone. In Round 0, clients compute local class statistics which are aggregated at the server to form global distributions. Clients then train "distribution compensation prompts" (LCDC) to align their local features to these global distributions. These prompts are frozen, and in subsequent rounds, clients train "discriminativity prompts" for classification. The server aggregates these prompts using a class-aware attention mechanism based on client histograms that capture prompt-class affinities, weighted by a correlation matrix. This two-phase approach decouples domain alignment from task discrimination to improve stability-plasticity balance.

## Key Results
- C²Prompt achieves 2.51% improvement on ImageNet-R compared to state-of-the-art methods
- C²Prompt achieves 2.90% improvement on DomainNet compared to state-of-the-art methods
- Superior performance on CIFAR-100 benchmark
- Effectively mitigates knowledge conflicts during parameter aggregation in FCL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local Class Distribution Compensation (LCDC) aligns heterogeneous local feature representations to a global consensus, reducing spatial forgetting.
- **Mechanism:** The server aggregates local statistics (mean $\mu$, variance $\sigma^2$) to estimate a global Gaussian distribution for each class. Clients then train a specific set of "distribution compensation prompts" to minimize the distance between their local features and this global distribution via a negative log-likelihood loss.
- **Core assumption:** The feature representations of a class across clients can be adequately modeled by a Gaussian distribution, and the server-aggregated statistics represent a meaningful "global" semantic center.
- **Evidence anchors:**
  - [abstract] Mentions "intra-class distribution gap across clients, which degrades the learned semantics."
  - [section 4.2] Describes minimizing $L_c$ to align features $f_{x,p}$ with global distribution $N(\mu^g_i, \Sigma^g_i)$.
  - [corpus] Corpus signals consistently identify "heterogeneous" and "non-IID" data as the primary stressor in FCL systems.
- **Break condition:** Performance degrades significantly if local data is extremely sparse (making local statistics unreliable) or if the global distribution is highly multi-modal (violating the Gaussian assumption).

### Mechanism 2
- **Claim:** Class-aware Prompt Aggregation (CPA) mitigates knowledge conflict during server-side fusion by weighting prompt updates based on semantic relevance.
- **Mechanism:** Clients compute a "client histogram" capturing the affinity between local discriminative prompts and specific classes. The server constructs a correlation matrix from these histograms to generate dynamic weights, selectively strengthening aggregation for prompts that share class-relevant knowledge and suppressing irrelevant or conflicting updates.
- **Core assumption:** The "prompt-class affinity" scores calculated locally are robust proxies for semantic content and transferability across clients.
- **Evidence anchors:**
  - [abstract] Highlights "inter-prompt class-wise relevance, which highlights cross-class knowledge confusion."
  - [section 4.4] Defines the inter-prompt correlation matrix $W^t_g$ used for weighted aggregation.
  - [corpus] Related work (e.g., "Decentralized Dynamic Cooperation") emphasizes the need for dynamic collaboration strategies to handle conflicting client updates.
- **Break condition:** If prompts become polysemantic (representing multiple classes equally), affinity scores may become noisy, leading to sub-optimal aggregation weights.

### Mechanism 3
- **Claim:** Decoupling "domain alignment" (LCDC) from "task discrimination" (Discriminativity Prompts) improves plasticity-stability balance.
- **Mechanism:** The system separates the prompt pool into two distinct functions: one set ($P^c$) handles the shift from local to global feature distributions (spatial alignment), while the second set ($P^d$) learns the actual classification boundaries (temporal task learning).
- **Core assumption:** Optimizing for feature alignment and classification boundaries simultaneously introduces gradient conflicts; separating them eases optimization.
- **Evidence anchors:**
  - [figure 2] Visualizes the distinct flow: "Local Class Distribution Compensation" vs. "Local Discriminativity Learning."
  - [section 4.3] Describes learning $P^d$ after $P^c$ is trained and frozen.
  - [corpus] Weak/missing specific evidence for decoupled optimization in the provided corpus summaries.
- **Break condition:** If the global distribution shifts drastically between rounds, the frozen compensation prompts ($P^c$) may misalign the features for the new discriminative tasks.

## Foundational Learning

- **Concept: Vision Transformers (ViT) & Prompt Tuning**
  - **Why needed here:** The entire architecture relies on a frozen ViT backbone where "prompts" (learnable prefix tokens) are injected to steer the attention mechanism without modifying pre-trained weights.
  - **Quick check question:** Can you explain how a prepend vector shifts the attention distribution in a Transformer's multi-head attention layer?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** This is the baseline communication protocol. C2Prompt modifies this by not just averaging model weights, but by aggregating specific prompt parameters and distribution statistics.
  - **Quick check question:** How does standard FedAvg handle non-IID data, and why does that necessitate the specific "Class-aware" modifications in this paper?

- **Concept: Catastrophic Forgetting (Stability-Plasticity)**
  - **Why needed here:** The core problem is the loss of old knowledge when learning new tasks. You must understand the difference between temporal forgetting (new task overwrites old) and spatial forgetting (local bias overwrites global consensus).
  - **Quick check question:** Why are rehearsal-free methods (like prompts) preferred over rehearsal-based methods in privacy-sensitive Federated Learning scenarios?

## Architecture Onboarding

- **Component map:** Server (Global Prompt Pool, Global Class Distributions) -> Clients (Frozen ViT backbone, Local Discriminativity Prompts, Local Compensation Prompts) -> Server (Aggregate prompts via CPA)

- **Critical path:**
  1. **Round 0 (Initialization):** Clients send class stats; Server returns global distribution; Clients train $P^c$ (Compensation Prompts) and **freeze** them.
  2. **Round 1..N (Learning):** Clients train $P^d$ (Discriminativity Prompts) using frozen $P^c$ for alignment.
  3. **Aggregation:** Clients upload $P^d$ and Histograms; Server computes correlation matrix $W^t_g$; Server aggregates $P^d$ using $W^t_g$ and distributes updated $P^d$.

- **Design tradeoffs:**
  - **Communication Cost:** The system transmits prompt parameters (small) + class statistics (tiny) + histograms (tiny). It is communication-efficient compared to full-model tuning but adds overhead compared to simple prompts due to the histogram exchange required for CPA.
  - **Storage:** Requires storing global distributions for all seen classes, which grows linearly with the total number of classes in the FCL stream.

- **Failure signatures:**
  - **High Variance in Loss:** If the LCDC loss ($L_c$) does not converge, the global distribution assumption (Gaussian) is likely violated for the current dataset.
  - **Negative Forward Transfer:** If CPA suppresses necessary updates due to low correlation scores, the "temperature" parameter $\tau$ in the softmax (Eq. 10) may need tuning.

- **First 3 experiments:**
  1. **Ablation on LCDC:** Train without the compensation prompt $P^c$ (only $P^d$) to quantify the specific gain from resolving the "intra-class distribution gap."
  2. **Visualization of Attention:** Generate attention maps (as seen in Figure 5) to verify that CPA actually results in prompts attending to discriminative regions rather than background noise.
  3. **Hyper-parameter sensitivity:** Vary the prompt length $L_c$ and probability $p$ (Figure 7) to understand the capacity required for the distribution compensation mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies heavily on the Gaussian assumption for class distributions across clients, which may not hold for highly skewed or multi-modal data.
- The effectiveness of the correlation matrix-based aggregation depends on the quality of locally computed prompt-class affinity scores, which could be noisy in sparse-label regimes.
- The paper does not provide extensive ablation studies on the impact of hyper-parameters like the scaling factor $\tau$ for CPA or the probability $p$ for class distribution prompts.

## Confidence

- **High Confidence:** The mechanism of decoupling domain alignment from task discrimination (LCDC + Discriminativity Prompts) is well-defined and its benefits are supported by the experimental results.
- **Medium Confidence:** The Class-aware Prompt Aggregation (CPA) method is logically sound, but its robustness to prompt polysemy and the sensitivity to the $\tau$ parameter require further validation.
- **Medium Confidence:** The improvement over state-of-the-art methods (2.51% and 2.90% on ImageNet-R and DomainNet) is statistically significant, but the exact implementation details for the baseline comparisons are not fully specified in the text.

## Next Checks
1. **Ablation Study on LCDC:** Train the model without the Local Class Distribution Compensation (LCDC) module to isolate and quantify its specific contribution to mitigating spatial forgetting.
2. **Hyperparameter Sensitivity Analysis:** Systematically vary the scaling factor $\tau$ in the CPA softmax (Eq. 10) and the probability $p$ for class distribution prompts to understand their impact on aggregation quality and overall performance.
3. **Visualization of Attention Maps:** Generate and analyze the attention maps from the prompts to verify that CPA results in the model attending to discriminative regions of the input rather than background noise or irrelevant features.