---
ver: rpa2
title: Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?
arxiv_id: '2512.13001'
source_url: https://arxiv.org/abs/2512.13001
tags:
- user
- tems
- llms
- recommendation
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates training-free cold-start recommendation
  (TFCSR), where no training data is available and new users have no interactions.
  The authors compare two approaches: large language models (LLMs) as direct rerankers
  and text embedding models (TEMs) that map texts into shared vector spaces.'
---

# Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?

## Quick Facts
- **arXiv ID:** 2512.13001
- **Source URL:** https://arxiv.org/abs/2512.13001
- **Reference count:** 40
- **Primary result:** Text embedding models (TEMs) consistently outperform LLM rerankers for training-free cold-start recommendation across multiple datasets and settings.

## Executive Summary
This study investigates training-free cold-start recommendation (TFCSR) where no user interaction data exists beyond profile text. The authors compare two approaches: LLM rerankers that directly generate ranked item lists versus TEMs that map texts into shared vector spaces. Through controlled experiments on three public datasets, they demonstrate that TEMs consistently achieve higher accuracy than LLM rerankers, with Qwen3-Embedding-8B achieving the highest performance. The findings challenge the assumption that LLMs are always superior in training-free environments, showing that TEMs provide a more scalable and effective foundation for TFCSR tasks.

## Method Summary
The study evaluates TFCSR using three public datasets (MovieLens-1M, Job Recommendation, Amazon Reviews) with standardized protocols. For TEMs, user profiles and item descriptions are embedded using models like Qwen3-Embedding-8B and gte-Qwen2-7B-instruct, then ranked by cosine similarity. For LLM rerankers, a zero-shot prompt instructs the model to select and order the top 10 items from a candidate set of 50. The evaluation distinguishes between narrow cold-start (m=0 interactions, profile only) and broad cold-start (m=1-5 interactions). Performance is measured using Recall@10 and nDCG@10 metrics.

## Key Results
- Qwen3-Embedding-8B achieved the highest accuracy with Recall@10 scores of 0.369-0.392 across datasets in narrow cold-start setting
- TEMs consistently outperformed LLM rerankers in both narrow and broad cold-start scenarios
- Query expansion techniques did not improve performance and sometimes degraded results on strong TEMs
- Even with increased user interactions or reduced candidate items, TEMs maintained their advantage over LLM rerankers

## Why This Works (Mechanism)

### Mechanism 1: Distillation Superiority in Semantic Matching
Text embedding models trained on synthetic data derived from LLMs capture semantic relevance more effectively for ranking than the generative LLMs themselves. Modern TEMs like Qwen3-Embedding distill LLM semantic knowledge into fixed vector spaces, removing generation overhead and prompt sensitivity. This results in more stable similarity scores for ranking. Evidence shows Qwen3-Embedding-8B surpassed GPT-4.1-mini and GPT-4.1 in accuracy, and its LLM counterpart Qwen3-8B achieved lower accuracy. Performance degrades to BM25 levels if TEMs are not trained with LLM supervision.

### Mechanism 2: Calibration Stability in Zero-Shot Ranking
TEMs provide better calibrated ordinal rankings (nDCG) than LLM rerankers, which struggle to translate relevance judgments into consistent ranking orders. TEMs rely on cosine similarity, a continuous metric that preserves relative preference strength, while LLM rerankers must convert implicit relevance into explicit list indices. Error analysis reveals TEMs outperform LLMs for over 75% of users in nDCG, suggesting a systemic ranking calibration failure in LLMs. The performance gap narrows with extremely small candidate sets (L=10) but TEMs generally maintain advantage.

### Mechanism 3: Query-Item Alignment Fidelity
Direct embedding of raw text (User Profile → Vector, Item → Vector) preserves alignment fidelity better than intermediate query expansion steps. Introducing an LLM to "expand" or "enrich" user profiles adds noise and potential hallucination that misaligns user representation from the item corpus. Raw embeddings maintain direct semantic correspondence. Evidence shows TEMs achieve higher accuracy when user and item texts are kept in original form without expansion, and query expansion often results in negative improvement for stronger TEMs.

## Foundational Learning

- **Concept: Narrow vs. Broad Cold-Start**
  - Why needed here: The paper defines "Narrow CS" as strictly zero interaction data (profile only) and "Broad CS" as few interactions. The mechanism of representation changes (Profile text vs. Averaged item history) depends on this distinction.
  - Quick check question: Does your target user have 0 interactions (Narrow) or 1-5 interactions (Broad)?

- **Concept: Cosine Similarity vs. Generative Ranking**
  - Why needed here: This is the architectural core of the comparison. Understanding that TEMs use vector geometry (v_u · v_i) while LLMs use probabilistic next-token prediction to output IDs is essential to understanding the tradeoff between calibration and generation.
  - Quick check question: Are you calculating a distance score or asking the model to "pick the best item"?

- **Concept: Zero-Shot Supervision (Training-Free)**
  - Why needed here: The paper strictly forbids training on domain-specific interaction data. The "learning" comes solely from the pre-trained knowledge of the TEM/LLM. One must distinguish between "no training at inference" and "no training ever" (pre-training is allowed).
  - Quick check question: Is your model allowed to see the interaction history of other users to learn patterns? (If yes, you are outside the scope of this paper).

## Architecture Onboarding

- **Component map:**
  Input: User Data D_u (Profile text t_u or History {t_{u,1} ... t_{u,m}}) + Candidate Items {t_1 ... t_{50}}
  Encoder: LLM-Supervised TEM (e.g., Qwen3-Embedding-8B or gte-Qwen2-7B-instruct)
  Aggregation: If Broad CS, compute average of historical item vectors → User Vector
  Retrieval: Cosine similarity between User Vector and Item Vectors
  Comparator (Baseline): LLM Reranker (Prompt: "Rank these 50 items based on profile...")

- **Critical path:**
  1. Selection of Encoder: Do not use legacy encoders (e.g., BERT-based) for Cold-Start; they often fail to beat BM25. Use LLM-supervised models.
  2. Text Formatting: Ensure proper prefixes (e.g., "Query: " vs "Passage: ") as per specific model requirements.
  3. History Handling: For Broad CS, aggregate history via averaging, not recursive summarization.

- **Design tradeoffs:**
  - Accuracy vs. Cost: LLM Rerankers (GPT-4.1) are expensive and slower but offer marginally better "relevance" detection in some contexts; however, they fail at ranking calibration (nDCG). TEMs are cheaper, faster, and higher accuracy.
  - Expansion vs. Raw: While tempting to use an LLM to "fix" sparse profiles, the evidence suggests keeping text raw is statistically safer.

- **Failure signatures:**
  - The "BM25 Floor" Failure: On specific domains (e.g., Job recommendations), non-LLM-supervised TEMs drop below BM25.
  - The Expansion Decay: Query expansion yields relative improvements only on weak models; on strong models (Qwen), it causes performance degradation.
  - The LLM Randomness: LLM rerankers show high variance in Recall but consistently poor nDCG, indicating they can find relevant items but cannot agree on which is best.

- **First 3 experiments:**
  1. Sanity Check: Run BM25, Qwen3-Embedding-8B, and gpt-4.1 on a sample of Narrow CS users. Verify that TEM > BM25 and TEM nDCG > LLM nDCG.
  2. Ablation on Interaction Count: Test m ∈ {0, 1, 3} interactions. Confirm performance scales with m for TEMs, while LLMs struggle to utilize the extra context efficiently.
  3. Query Expansion Audit: Attempt query expansion (generating search queries from user profile) and measure the delta against raw embedding. Expect a negative or near-zero delta for modern TEMs.

## Open Questions the Paper Calls Out

### Open Question 1
Can training methods that integrate structured features (profiles, item attributes) improve TEM performance for TFCSR beyond current LLM supervision focused on natural language? The paper states "recommendation tasks often involve structured inputs such as profiles or item attributes, while current LLM supervision focuses on natural language. Training methods that integrate structured features... will be necessary to advance this approach." This remains unresolved as current TEMs pre-trained on synthetic query-document pairs may not capture structured semantic relations needed for recommendation.

### Open Question 2
What scale and quality characteristics of synthetic training data yield optimal TEM performance for cold-start recommendation? The authors call for "systematic studies on the scale and quality of synthetic data" and note that "synthetic data to train TEMs may contain domain biases or hallucinated content that limit generalization." The relationship between synthetic data properties and downstream TFCSR performance remains unexplored.

### Open Question 3
Why do LLM rerankers fail to produce proper rankings despite being able to judge item relevance? The paper notes "LLM rerankers can judge item relevance but often fail to generate proper rankings" and shows gpt-4.1 achieves competitive Recall but significantly lower nDCG than TEMs. The error analysis shows TEMs outperform LLMs for most users, but the cognitive/architectural reasons for LLM ranking failure remain unclear.

### Open Question 4
Can replacing older embeddings in supervised cold-start methods with modern LLM-supervised TEMs improve performance in scenarios where training data is available? The authors note that existing supervised cold-start approaches "rely on relatively old embeddings such as TFIDF, Sentence2Vec, and multilingual-e5. Replacing them with modern TEMs trained under LLM supervision is expected to provide further improvements." This remains untested as the study focused on training-free settings.

## Limitations
- Performance comparisons are limited to candidate set size of 50 items, with uncertainty about gaps at smaller scales
- Query expansion showed mixed results with performance degradation on strong TEMs, suggesting method limitations
- Non-LLM-supervised TEMs can underperform BM25 baseline, indicating sensitivity to model choice

## Confidence
- **High Confidence:** TEMs consistently outperform LLM rerankers in nDCG@10 metrics across all tested conditions
- **Medium Confidence:** The mechanism explanation for why TEMs provide better ranking calibration than LLMs, based on vector similarity vs. generative ranking
- **Medium Confidence:** Query expansion typically degrades performance on strong TEMs, though the paper acknowledges this requires further investigation

## Next Checks
1. Test performance degradation when increasing candidate set size from L=50 to L=100 or L=200 to verify if the TEM advantage scales
2. Evaluate whether fine-tuning TEMs on synthetic interaction data from LLMs improves their ranking accuracy beyond the zero-shot results
3. Conduct ablation studies removing different components of the TEM pipeline (encoding, aggregation, similarity) to isolate the primary performance drivers