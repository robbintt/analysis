---
ver: rpa2
title: Evaluating Multimodal Large Language Models on Vertically Written Japanese
  Text
arxiv_id: '2511.15059'
source_url: https://arxiv.org/abs/2511.15059
tags:
- text
- japanese
- images
- written
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates the ability of Multimodal Large Language Models
  (MLLMs) to read vertically written Japanese text, a capability essential for understanding
  Japanese documents. To address the lack of vertical Japanese text evaluation data,
  the authors create two datasets: a synthetic Japanese OCR dataset (JSSODa) with
  horizontal and vertical text in multi-column layouts, and a real-world vertical
  Japanese OCR dataset (VJRODa) from PDF documents.'
---

# Evaluating Multimodal Large Language Models on Vertically Written Japanese Text

## Quick Facts
- arXiv ID: 2511.15059
- Source URL: https://arxiv.org/abs/2511.15059
- Reference count: 0
- Primary result: Fine-tuning MLLMs on synthetic vertical Japanese OCR data improves their ability to read vertically written Japanese text, addressing a significant performance gap compared to horizontal text.

## Executive Summary
This study evaluates Multimodal Large Language Models (MLLMs) on their ability to read vertically written Japanese text, a capability essential for processing Japanese documents. The authors create two datasets: a synthetic Japanese OCR dataset (JSSODa) with horizontal and vertical text in multi-column layouts, and a real-world vertical Japanese OCR dataset (VJRODa) from PDF documents. Experiments demonstrate that existing MLLMs perform significantly worse on vertically written Japanese text compared to horizontal text. Fine-tuning on the synthetic dataset improves the performance of models that initially struggle with vertical writing, demonstrating the dataset's effectiveness for enhancing vertical text reading capabilities in MLLMs.

## Method Summary
The study creates two datasets: JSSODa (synthetic, 22,493 images with 8 layout types) and VJRODa (100 real-world PDF images). Three MLLMs (Qwen2.5-VL-7B, InternVL3-8B, Gemma 3 12B) are fine-tuned on JSSODa train set using full parameter tuning with batch size 32, AdamW optimizer, learning rate 2e-5, and 1 epoch. Evaluation uses Character Error Rate (CER) and BLEU metrics with Unicode NFKC normalization and repetition removal via regex. The models are assessed on both synthetic and real-world test sets to measure vertical text reading capability.

## Key Results
- MLLMs show significantly higher CER on vertically written Japanese text (>50%) compared to horizontal text (<10%) in baseline tests
- Fine-tuning Qwen2.5-VL-7B on JSSODa corrects its horizontal reading bias, reducing vertical text CER from over 50% to near zero on synthetic data
- Models that improved on synthetic JSSODa test sets showed limited improvement on real-world VJRODa, indicating a synthetic-to-real transfer gap
- Repetitive generation loops were observed in some models, which were addressed through regex-based repetition removal during evaluation

## Why This Works (Mechanism)

### Mechanism 1: Reading Order Alignment via Instruction Tuning
Pre-trained MLLMs default to horizontal reading biases that can be corrected through fine-tuning on layout-specific data. The model learns to prioritize vertical reading order (top-to-bottom, right-to-left) over pre-training priors. This works when the vision encoder successfully extracts character features and the failure is primarily in sequential reasoning logic.

### Mechanism 2: Synthetic-to-Real Transfer Gap
While JSSODa teaches the "rule" of vertical reading, it fails to impart robustness for real-world visual noise. Clean synthetic data lacks the degradations, complex textures, and varying contrast found in real documents. Models improve on synthetic tests but stagnate on real data because they overfit to clean visual distribution, learning logic without visual robustness.

### Mechanism 3: Repetitive Generation Loops in Out-of-Distribution Contexts
When MLLMs face ambiguous or high-density vertical layouts, they are prone to repetitive generation loops, suggesting a failure in stopping criteria or attention stability. The attention mechanism may latch onto specific local features without finding a global "end" signal, causing the decoder to oscillate.

## Foundational Learning

- **Concept: Vision-Language Projection (The Adapter)**
  - Why needed here: Explains where fine-tuning happens (tuning all modules vs. freezing encoder)
  - Quick check question: If the vision encoder is frozen, can the model learn to "see" new visual features like vertical text, or only learn to "interpret" existing features?

- **Concept: Reading Order Semantics (Tategaki vs. Yokogiri)**
  - Why needed here: The core problem is not character recognition but the spatial syntax of Japanese documents
  - Quick check question: In a 2-column vertical layout, which column does the model process first? If it processes the left column first, it has failed the vertical reading order test.

- **Concept: Character Error Rate (CER)**
  - Why needed here: Primary metric measuring edit distance normalized by ground truth length
  - Quick check question: If a model reads all characters correctly but in the wrong order, would the CER be high or low? (Answer: High, because it constitutes complete sequence deletion/insertion)

## Architecture Onboarding

- **Component map:** Document Image -> Vision Encoder -> Projector -> LLM Backbone -> Transcribed Text
- **Critical path:**
  1. Data Curation: Generate JSSODa (Synthetic) and VJRODa (Real)
  2. Fine-Tuning: Full parameter tuning on JSSODa train set (18k images)
  3. Inference: Greedy decoding on test sets to generate transcription
  4. Evaluation: Calculate CER and BLEU between generated text and ground truth
- **Design tradeoffs:**
  - JSSODa vs. VJRODa: Synthetic dataset teaches layout rules but lacks visual diversity for real-world deployment
  - Full Fine-tuning vs. PEFT: Full parameter tuning yields strong results but is computationally expensive
- **Failure signatures:**
  - Horizontal Bias: Model reads vertical columns left-to-right instead of right-to-left
  - Token Loops: Model generates max tokens of repetitive characters
  - Semantic Hallucination: Model produces text following correct order but containing content not present in image
- **First 3 experiments:**
  1. Baseline Capability Check: Evaluate Qwen2.5-VL-7B and InternVL3 on VJRODa to confirm horizontal bias hypothesis
  2. Synthetic Alignment Training: Fine-tune Qwen2.5-VL-7B on JSSODa and verify reading order error drops to near zero on JSSODa test set
  3. Real-World Generalization Test: Take fine-tuned model and evaluate on VJRODa again to check if CER improves

## Open Questions the Paper Calls Out

### Open Question 1
How can the OCR capabilities of MLLMs be accurately evaluated on documents with multiple plausible reading orders without relying on a single ground truth annotation? The authors note that real-world documents may have multiple valid reading sequences, and current metrics penalize valid outputs that differ from the single annotated ground truth. Annotating all possible valid reading orders is described as financially and computationally prohibitive.

### Open Question 2
Does incorporating visual diversity, such as synthetic noise, embedded images, and tables, into pre-training data improve MLLM performance on real-world vertical Japanese documents? The authors state that JSSODa consists of clean, minimalist images without noise or figures, and suggest that "greater visual diversity" is necessary to better generalize to real-world documents.

### Open Question 3
Can fine-tuning on real-world vertical text datasets significantly improve the performance of models that already possess a basic understanding of vertical reading order? The authors observed that while fine-tuning on synthetic data helped Qwen2.5-VL-7B, it did not improve InternVL3 or Gemma 3 on the real-world VJRODa dataset, suggesting these models may require training with real-world OCR datasets.

## Limitations
- Synthetic-to-real transfer gap: JSSODa effectively teaches vertical reading order logic but lacks visual complexity (noise, textures) present in real-world documents
- Small real-world dataset: VJRODa contains only 100 real images, constraining statistical power for detecting fine-grained performance differences
- Repetitive generation loops: Observed in some models suggest underlying architectural instability when processing out-of-distribution layouts, though exact attention mechanism failure remains uncharacterized

## Confidence
- **High Confidence:** MLLMs perform significantly worse on vertically written Japanese text compared to horizontal text, as evidenced by baseline CER disparities (>50% vs <10%)
- **Medium Confidence:** The synthetic-to-real transfer gap hypothesis is supported by empirical evidence but requires more extensive real-world data to validate generalization
- **Low Confidence:** The claim that InternVL3's original architecture already understands vertical reading order is based on a single model's performance and lacks broader architectural analysis

## Next Checks
1. **Visual Robustness Test:** Create a synthetic dataset with controlled noise levels and evaluate whether models that improved on clean JSSODa maintain performance as noise increases, isolating visual vs logical learning components
2. **Architectural Dissection:** Compare attention weight distributions and attention head activations between models that successfully process vertical text versus those that fail, identifying specific architectural features that enable vertical reading
3. **Longitudinal Real-World Evaluation:** Expand VJRODa to 500+ real-world documents spanning multiple domains and conduct a year-long evaluation tracking model performance across document types, font variations, and layout complexities