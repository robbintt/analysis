---
ver: rpa2
title: Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source
  Models
arxiv_id: '2508.12461'
source_url: https://arxiv.org/abs/2508.12461
tags:
- arxiv
- evaluation
- performance
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We conducted a comprehensive evaluation of OpenAI''s GPT-OSS models
  (20B and 120B parameters) against six contemporary open source LLMs across ten benchmarks
  covering general knowledge, reasoning, code generation, multilingual understanding,
  and conversational ability. The evaluation revealed an unexpected inverse scaling
  phenomenon where the 20B model consistently outperformed the 120B model on multiple
  benchmarks (MMLU: 69% vs 66%, SCIQ: 87% vs 82%), challenging conventional scaling
  laws.'
---

# Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models

## Quick Facts
- arXiv ID: 2508.12461
- Source URL: https://arxiv.org/abs/2508.12461
- Reference count: 40
- Primary result: 20B MoE model outperforms 120B MoE variant on multiple benchmarks, challenging conventional scaling laws

## Executive Summary
This paper evaluates OpenAI's GPT-OSS models (20B and 120B parameters) against six contemporary open source LLMs across ten benchmarks covering general knowledge, reasoning, code generation, multilingual understanding, and conversational ability. The evaluation reveals an unexpected inverse scaling phenomenon where the 20B model consistently outperforms the 120B model on multiple benchmarks, challenging conventional scaling laws. GPT-OSS models demonstrate mid-tier overall performance with particular strength in code generation but notable weaknesses in multilingual tasks. The 20B variant offers superior efficiency, requiring 5× less GPU memory and 2.6× less energy per response while delivering higher throughput and maintaining comparable or better accuracy on several tasks.

## Method Summary
The evaluation compared 8 LLMs (GPT-OSS 20B/120B vs. 6 baselines including Qwen3, Llama-4, DeepSeek-R1) across 10 benchmarks using standardized inference settings. Models were evaluated via vLLM on 8x NVIDIA H100 (80GB) with consistent hyperparameters: temperature settings varying by task type (0.1 for factual, 0.7 for creative), top-p=0.95, top-k=50, repetition penalty=1.1, and max tokens=2000. Statistical significance was assessed using McNemar's test with Benjamini-Hochberg correction, and Cohen's d effect sizes were reported for benchmark differences.

## Key Results
- Inverse scaling observed: GPT-OSS 20B outperformed 120B on MMLU (69% vs 66%), SCIQ (87% vs 82%), and other benchmarks
- GPT-OSS models showed mid-tier performance with strength in code generation but weakness in multilingual tasks (C-Eval: 28% and 20% respectively)
- 20B variant offered superior efficiency: 5× less GPU memory, 2.6× less energy per response, 178 tokens/s vs 128 tokens/s throughput

## Why This Works (Mechanism)

### Mechanism 1: Sparse MoE Activation Reduces Compute Overhead
- Claim: MoE architectures achieve competitive performance with lower active parameter counts per token compared to dense models.
- Mechanism: Only a subset of experts (5.1B for 120B, 3.6B for 20B) activate per forward pass, reducing FLOPs while maintaining total parameter capacity for knowledge storage.
- Core assumption: Routing functions correctly assign tokens to relevant experts without significant misallocation.
- Evidence anchors: [abstract] mixture of experts architectures; [Table I] active parameters 5.1B (120B) vs 3.6B (20B); [corpus] related deployment analysis confirms approx. 3.61B active for 20B.

### Mechanism 2: Inverse Scaling Emerges from Suboptimal Expert Utilization
- Claim: Larger MoE models may underperform smaller variants when expert coordination or training data distribution creates routing inefficiencies.
- Mechanism: The 120B model's larger expert pool may experience dilution—more experts competing for similar token patterns, causing knowledge fragmentation or inconsistent routing decisions.
- Core assumption: Both models were trained with comparable data quality and training methodology.
- Evidence anchors: [abstract] 20B outperforms 120B on several benchmarks; [Section V.A] p<0.01, Cohen's d = 0.73; [corpus] weak direct evidence.

### Mechanism 3: Response Conciseness Reduces Inference Cost
- Claim: GPT-OSS models generate shorter outputs while maintaining task performance, directly reducing token-generation costs.
- Mechanism: Concise response generation reduces KV-cache memory growth during decoding and lowers total compute per response.
- Core assumption: Shorter outputs don't sacrifice correctness; quality metrics account for completeness.
- Evidence anchors: [Fig 6] mean output ~200 tokens vs 800-1000 for reasoning models; [Section V.I] 2.6× less energy per completed response; [corpus] no direct replication.

## Foundational Learning

- **Concept: Active vs. Total Parameters in MoE**
  - Why needed here: Understanding why a 120B MoE model requires only 5.1B active parameters per token is essential for memory and compute planning.
  - Quick check question: If an MoE model has 8 experts and activates 2 per token, what fraction of weights are computed during inference?

- **Concept: Scaling Laws and Their Limits**
  - Why needed here: The paper challenges Kaplan-style scaling assumptions; readers must understand what scaling laws predict to evaluate the inverse scaling anomaly.
  - Quick check question: According to Chinchilla scaling, what three factors should predictably improve with increased investment?

- **Concept: Statistical Significance in Benchmark Comparison**
  - Why needed here: The paper uses McNemar's test and effect sizes; understanding these determines whether reported differences are reliable.
  - Quick check question: Why is McNemar's test preferred over a simple accuracy difference for paired model evaluation?

## Architecture Onboarding

- **Component map:** Router network -> Expert layers (8 experts for 20B; more for 120B) -> Shared components (Attention layers) -> KV-cache
- **Critical path:** 1) Load model weights (234GB for 120B, 42GB for 20B in bf16) 2) Initialize router and expert weights on GPU(s) 3) For each token batch: router scores experts → top-k selection → expert computation → output aggregation 4) Monitor expert load balance
- **Design tradeoffs:** 120B variant: higher knowledge capacity but 5× memory, lower throughput (128 tok/s), unexpected performance regressions; 20B variant: 5× less memory, 39% higher throughput (178 tok/s), matches/exceeds 120B on MMLU and SciQ
- **Failure signatures:** Routing collapse (disproportionate expert usage), multilingual failure (C-Eval scores below 30%), excessive verbosity (outputs exceed ~3,000 characters)
- **First 3 experiments:** 1) Baseline throughput test: run identical prompts (n=100) through both variants; measure TTFT, tokens/second, peak VRAM (expect 178 tok/s for 20B vs 128 tok/s for 120B) 2) Routing balance analysis: log expert activation frequencies across diverse prompts; flag if any expert receives <5% or >40% of tokens 3) Task-specific scaling verification: replicate MMLU and SciQ evaluations on your data; confirm whether inverse scaling holds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural or training factors cause the observed inverse scaling in GPT-OSS mixture-of-experts (MoE) models?
- **Basis in paper:** [explicit] The authors note the 20B model outperforming the 120B model "suggests potential inefficiencies in the MoE routing mechanism or suboptimal training configuration" and highlights the need for "further investigation into optimisation strategies."
- **Why unresolved:** The study is a black-box empirical evaluation; it identifies the *existence* of the anomaly but lacks access to OpenAI's internal training data or architectural details to diagnose the root cause.
- **What evidence would resolve it:** Ablation studies varying expert counts and routing capacities, or transparency reports from the model developers detailing the loss curves and data composition used for both variants.

### Open Question 2
- **Question:** How do different prompting strategies and inference-time settings impact the performance and efficiency trade-offs in sparse MoE architectures?
- **Basis in paper:** [explicit] The conclusion states that the authors "did not exhaustively optimise prompting or decoding strategies for any single model" and calls for "systematic studies of prompting and inference time trade-offs."
- **Why unresolved:** The evaluation used standardized settings for fairness, leaving the potential performance ceiling (or efficiency gains) achievable through architecture-specific optimization unexplored.
- **What evidence would resolve it:** A comparative study applying advanced techniques like chain-of-thought or self-consistency prompting specifically tuned to the active parameter counts of the GPT-OSS variants.

### Open Question 3
- **Question:** How can evaluation frameworks fairly compare standard models against reasoning-optimized models that expose extensive internal "thinking" tokens?
- **Basis in paper:** [inferred] The case study highlights a comparison issue where reasoning models (like DeepSeek-R1) generated verbose outputs (26,000+ chars), leading the authors to suggest "a fair comparison would require... configuring all models to suppress detailed reasoning chains."
- **Why unresolved:** Current benchmarks often equate verbosity with reasoning steps or fail to penalize the massive computational overhead of exposing internal thoughts, lacking a standardized protocol for "hidden reasoning."
- **What evidence would resolve it:** The development and adoption of benchmarks that score based on "cost-adjusted accuracy" or provide standardized APIs to separate reasoning traces from final outputs.

## Limitations
- Model access and reproducibility barriers due to pre-release status of GPT-OSS and competing state-of-the-art systems
- Lack of transparency regarding training data composition, curriculum, and optimization strategies for the GPT-OSS variants
- Unspecified energy measurement methodology, making efficiency claims difficult to verify independently

## Confidence
- **High Confidence:** Relative performance ordering among available models on individual benchmarks; memory and throughput measurements
- **Medium Confidence:** Inverse scaling phenomenon (statistically supported but requires model access to verify); efficiency advantages of 20B variant
- **Low Confidence:** Architectural explanations for inverse scaling; generalization to other MoE architectures

## Next Checks
1. **Independent Performance Replication:** Replicate core benchmark results (MMLU, SciQ, HumanEval) using accessible MoE models with similar parameter counts and active parameter ratios
2. **Routing Analysis Verification:** Examine expert activation patterns during inference to test the routing inefficiency hypothesis; log per-expert token assignments across diverse prompts
3. **Multilingual Capability Investigation:** Conduct controlled experiments varying prompt languages and measuring performance degradation; test whether C-Eval weakness reflects training data scarcity or architectural limitations