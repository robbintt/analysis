---
ver: rpa2
title: Fast Training of Recurrent Neural Networks with Stationary State Feedbacks
arxiv_id: '2503.23104'
source_url: https://arxiv.org/abs/2503.23104
tags:
- bptt
- time
- gradient
- state
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck of training recurrent
  neural networks (RNNs) using backpropagation through time (BPTT), which is slow
  and memory-intensive. The authors propose Diagonal State Feedbacks (DSF), a method
  that approximates BPTT gradients using a fixed, diagonal feedback matrix inspired
  by state-space models.
---

# Fast Training of Recurrent Neural Networks with Stationary State Feedbacks

## Quick Facts
- arXiv ID: 2503.23104
- Source URL: https://arxiv.org/abs/2503.23104
- Reference count: 27
- One-line primary result: Diagonal State Feedbacks (DSF) achieves competitive perplexity scores close to standard BPTT while significantly reducing training costs.

## Executive Summary
This paper addresses the computational bottleneck of training recurrent neural networks (RNNs) using backpropagation through time (BPTT), which is slow and memory-intensive. The authors propose Diagonal State Feedbacks (DSF), a method that approximates BPTT gradients using a fixed, diagonal feedback matrix inspired by state-space models. This replaces the time-dependent Jacobian with a stationary one, allowing efficient gradient propagation via convolution. Experiments on language modeling tasks (Penn Treebank and Wikitext-103) show that DSF achieves competitive perplexity scores close to standard BPTT while significantly reducing training costs, outperforming simpler truncation methods and maintaining scalability with model size and depth.

## Method Summary
DSF approximates BPTT by replacing the time-dependent Jacobian matrix with a fixed, diagonal feedback matrix A. This transforms the gradient recurrence into a linear time-invariant (LTI) system, which can be computed as a convolution. The diagonal matrix is initialized randomly (uniform [0,1]) and kept fixed during training, removing the need to backpropagate through the feedback matrix. This approach significantly reduces computational complexity from O(d²T) to O(d) per timestep and enables efficient parallel implementations using FFT or prefix-sum algorithms.

## Key Results
- DSF achieves competitive perplexity scores on Penn Treebank and Wikitext-103 compared to standard BPTT
- Training costs are significantly reduced compared to BPTT and outperform fully truncated BPTT
- The method maintains scalability with model size and depth
- Experiments show robust performance across different sequence lengths

## Why This Works (Mechanism)

### Mechanism 1: Stationary Diagonal Feedback Approximation
The paper replaces the time-dependent Jacobian A_t in BPTT with a fixed, diagonal feedback matrix A. This drastically reduces computational complexity from O(d²T) to O(d) per timestep while preserving the network's ability to capture long-term dependencies. The core assumption is that gradient dynamics can be approximated by a stationary process captured by a simple diagonal structure.

### Mechanism 2: Efficient Gradient Computation via Convolution
With a fixed diagonal matrix A, the backward gradient equation becomes mathematically equivalent to a convolution of error signals with a kernel (I, A, A², ...). This allows efficient computation using Fast Fourier Transforms (FFT) or parallel prefix-sum algorithms, reducing sequential dependency to logarithmic complexity O(log T).

### Mechanism 3: Competitive Performance with Fixed Random Initialization
Inspired by Direct Feedback Alignment (DFA), the authors fix the diagonal elements of A at initialization and do not update them. This removes the need for backpropagating through the feedback matrix itself, based on the assumption that a fixed random projection can preserve enough information to convey useful error signals even without being tuned.

## Foundational Learning

- **Backpropagation Through Time (BPTT)**: The standard method for training RNNs that computes gradients by recursively multiplying time-varying Jacobian matrices. [Why needed: This is the baseline method whose computational bottleneck the paper addresses.]
- **State-Space Models (SSMs) and LTI Systems**: Mathematical frameworks where linear time-invariant systems can be represented as convolutions. [Why needed: The paper leverages LTI properties to reformulate gradient propagation.]
- **Feedback Alignment**: A technique that uses fixed random matrices for backpropagation instead of exact gradients. [Why needed: DSF draws direct inspiration from this approach.]

## Architecture Onboarding

- **Component map:** Recurrent Cell (e.g., GRU, LSTM) -> Error Computation -> DSF Gradient Module -> Parameter Update
- **Critical path:** The DSF module computes gradients g_t by convolving errors e_t with the kernel (I, A, A², ...). Efficient implementation using FFT or parallel scan is the key engineering challenge.
- **Design tradeoffs:** 
  - Approximation Quality vs. Speed: Diagonal matrix is faster but stronger approximation than full matrix
  - Initialization Strategy: Performance may be sensitive to choice of initialization
  - Parallelization Strategy: Choose between O(T) recurrence, O(log T) prefix-sum, or O(T log T) FFT based on hardware
- **Failure signatures:**
  - Performance matches FT-BPTT if gradient approximation fails
  - Training instability from poor initialization or very long sequences
  - No speedup from non-optimized implementation
- **First 3 experiments:**
  1. Implement DSF with GRU on Penn Treebank, compare perplexity and training time against BPTT and FT-BPTT
  2. Test different diagonal matrices (fixed random, identity, trainable) on PTB to evaluate contribution of random initialization
  3. Scale DSF to Wikitext-103 with longer sequences (1024, 2048) to verify logarithmic sequentiality and robustness

## Open Questions the Paper Calls Out
1. Can structured non-diagonal approximations (e.g., low-rank plus diagonal or orthogonal matrices) effectively bridge the performance gap between DSF and exact BPTT?
2. How can initialization strategies inspired by state-space models (e.g., HiPPO, S4) be adapted for DSF to overcome difficulties in preliminary experiments?
3. What are the theoretical convergence bounds and stability guarantees for training RNNs with fixed, stationary feedback matrices compared to exact gradient computation?

## Limitations
- The diagonal approximation may fail for tasks requiring precise credit assignment over long horizons or with highly non-linear dynamics
- Fixed random initialization trades optimization power for speed, potentially limiting performance on complex tasks
- The "Transformer-like" architecture for Wikitext-103 is not fully specified, creating ambiguity for exact reproduction

## Confidence
- **High Confidence:** DSF significantly reduces computational complexity from O(d²T) to O(d) per timestep
- **Medium Confidence:** DSF achieves competitive perplexity scores close to BPTT on Penn Treebank and Wikitext-103
- **Low Confidence:** Fixed random initialization of A is sufficient for learning across diverse tasks

## Next Checks
1. Implement DSF on PTB with three variants of diagonal matrix A (fixed random, identity, trainable) to test initialization sensitivity
2. Train DSF on Wikitext-103 with sequences of 1024 and 2048 tokens to verify logarithmic sequentiality gains
3. Apply DSF to a non-language modeling task (e.g., polyphonic music prediction) to test broader applicability