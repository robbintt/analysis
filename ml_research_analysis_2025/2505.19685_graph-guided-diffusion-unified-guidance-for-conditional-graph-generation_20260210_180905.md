---
ver: rpa2
title: 'Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation'
arxiv_id: '2505.19685'
source_url: https://arxiv.org/abs/2505.19685
tags:
- graph
- generation
- graphs
- diffusion
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of guiding graph diffusion models
  to satisfy arbitrary reward signals, including non-differentiable and combinatorial
  constraints. The authors propose Graph Guided Diffusion (GGDiff), a framework that
  reformulates conditional graph generation as a stochastic optimal control (SOC)
  problem.
---

# Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation

## Quick Facts
- **arXiv ID:** 2505.19685
- **Source URL:** https://arxiv.org/abs/2505.19685
- **Reference count:** 40
- **Key outcome:** Proposes Graph Guided Diffusion (GGDiff), a unified framework for guiding pre-trained graph diffusion models to satisfy arbitrary reward signals, including non-differentiable and combinatorial constraints.

## Executive Summary
Graph Guided Diffusion (GGDiff) addresses the challenge of guiding graph diffusion models to satisfy arbitrary reward signals, including non-differentiable and combinatorial constraints. The authors reformulate conditional graph generation as a stochastic optimal control (SOC) problem, providing a theoretical framework for optimal guidance. GGDiff unifies gradient-based, control-based, and zero-order approximation strategies into a single plug-and-play system that can guide pre-trained diffusion models without retraining. The method uses zeroth-order optimization techniques to approximate gradients for non-differentiable rewards, enabling flexible constraint satisfaction while preserving the underlying graph distribution.

## Method Summary
GGDiff interprets conditional graph generation as a stochastic optimal control problem, modifying the reverse diffusion process by adding a control variable $U_t$ to the drift term. The framework minimizes a cost function balancing control energy against terminal negative reward, yielding an optimal control policy via the Feynman-Kac formula. For differentiable rewards, GGDiff uses a greedy approximation with Tweedie's formula to compute gradients efficiently. For non-differentiable rewards, it employs zeroth-order optimization with multiple sample directions to approximate gradients. The method can be applied to any pre-trained diffusion model without retraining, making it a flexible plug-and-play system for conditional graph generation.

## Key Results
- Outperforms existing methods in both constraint adherence and preservation of the underlying graph distribution across structural constraints, fairness criteria, and link prediction tasks
- Achieves higher alignment with target rewards while maintaining diversity and fidelity compared to baselines like PRODIGY on constrained graph generation tasks
- Demonstrates effectiveness across multiple graph datasets including ego-small, community-small, enzymes, QM9, and ZINC250k

## Why This Works (Mechanism)

### Mechanism 1
Reformulating conditional graph generation as a stochastic optimal control (SOC) problem theoretically guarantees optimal guidance, provided the resulting Hamilton-Jacobi-Bellman (HJB) equation can be solved. The method modifies the reverse diffusion process by adding a control variable $U_t$ to the drift term, minimizing a cost function balancing control energy against terminal negative reward. Core assumption: the uncontrolled pre-trained diffusion model serves as a sufficiently accurate prior for the data distribution. Break condition: if the reward signal is extremely sparse or the diffusion prior is weak, the expectation estimation becomes unstable.

### Mechanism 2
For differentiable rewards, a greedy approximation using Tweedie's formula enables tractable, efficient guidance without simulating full trajectories. The method approximates the posterior $p(G_0|G_t)$ as a Dirac delta centered at the one-step denoised estimate $\hat{G}_0$, making the control signal the gradient of the reward evaluated at this estimate. Core assumption: the one-step denoised estimate $\hat{G}_0(G_t)$ is a sufficiently accurate proxy for the clean data $G_0$, particularly as $t \to 0$. Break condition: in early diffusion steps (high noise), $\hat{G}_0$ is a poor estimate, potentially causing misaligned gradient updates.

### Mechanism 3
Zeroth-order (ZO) optimization allows guidance via non-differentiable or black-box rewards by treating the reward landscape as a black-box function to be explored via random perturbations. The method samples $N$ candidate directions, evaluates the reward on denoised versions of these perturbed states, and aggregates results to approximate a "pseudo-gradient" direction. Core assumption: the reward function, while non-differentiable, is smooth enough on average for randomized smoothing to provide a useful directional signal. Break condition: high-dimensional graph spaces may require large numbers of samples ($N$) to reduce variance, potentially making the method computationally prohibitive.

## Foundational Learning

- **Concept: Stochastic Optimal Control (SOC)**
  - **Why needed here:** This is the theoretical lens through which the paper unifies guidance. Understanding SOC is necessary to grasp why the control signal $U_t$ is derived from a cost minimization problem rather than just a likelihood term.
  - **Quick check question:** How does the "control" variable modify the standard reverse SDE in this framework?

- **Concept: Tweedie's Formula (Denosing Estimator)**
  - **Why needed here:** This formula is the bridge between the noisy intermediate state $G_t$ and the clean reward domain. It justifies the "greedy" approximation used to calculate gradients at step $t$.
  - **Quick check question:** Why is the one-step denoiser $\hat{G}_0$ used instead of running the full diffusion chain to evaluate the reward?

- **Concept: Zeroth-Order (Gradient-Free) Optimization**
  - **Why needed here:** Essential for understanding how the system handles non-differentiable constraints. One must understand the trade-off between the variance of random gradient estimates and query complexity.
  - **Quick check question:** In the "Best-of-N" estimator, what is being maximized, and how does this differ from a standard gradient descent step?

## Architecture Onboarding

- **Component map:** Pre-trained GDSS score network $\epsilon_\theta$ -> GGDiff Controller (Reward Evaluation -> Control Signal Computation -> State Update) -> Diffusion Process
- **Critical path:** The **Reward Evaluation Loop** inside the ZO module. For every denoising step $t$, the model must perform $N$ denoising operations and reward evaluations. This is the primary computational bottleneck.
- **Design tradeoffs:**
  - **Sample Count ($N$) vs. Speed:** Higher $N$ in ZO optimization yields better gradient approximations but linearly increases inference time.
  - **Guidance Scale ($1/\lambda$) vs. Diversity:** Strong guidance (high scale) improves constraint satisfaction but risks "reward hacking" (collapsing graph quality/diversity).
- **Failure signatures:**
  - **Oscillation:** If the ZO smoothing parameter $\mu$ is too small or $N$ is too low, the control signal may become high-variance noise, failing to converge on the constraint.
  - **Distribution Collapse:** Excessive guidance might satisfy the constraint but result in trivial graphs.
- **First 3 experiments:**
  1. **Sanity Check (Differentiable):** Implement GGDiff-G on a simple constraint (e.g., Max Degree) to verify the gradient path matches DPS-style baselines.
  2. **Ablation on ZO Estimators:** Compare One-point, Two-point, and Best-of-N estimators on a non-differentiable task (e.g., Star Graph generation) to measure variance vs. accuracy.
  3. **Distribution Fidelity Test:** Generate graphs with hard constraints and compute $\Delta$MMD to ensure the "prior" distribution is preserved better than projection-based baselines.

## Open Questions the Paper Calls Out

- **Can GGDiff be effectively integrated with discrete diffusion models to better handle inherently discrete graph constraints?**
  - **Basis in paper:** "Future work includes integrating GGDiff with discrete diffusion models to better handle discrete constraints..."
  - **Why unresolved:** The current formulation operates in continuous domains using SDEs; discrete diffusion requires categorical transitions and different mathematical foundations for the SOC formulation.
  - **What evidence would resolve it:** Demonstration of GGDiff-style guidance on discrete diffusion models (e.g., DiGress) achieving comparable constraint satisfaction rates and sample quality.

- **Can adaptive or more efficient zeroth-order gradient estimators reduce the computational overhead and variance of GGDiff while maintaining guidance quality?**
  - **Basis in paper:** "ZO optimization adds computational overhead and may suffer from high variance in challenging settings" and "exploring more efficient or adaptive ZO gradient estimators to improve scalability and performance."
  - **Why unresolved:** Current estimators face a variance-dimensionality tradeoff (MSE scales as O(d) or O(d/N)), requiring N samples to reduce variance, which increases query complexity linearly.
  - **What evidence would resolve it:** Novel ZO estimators achieving lower variance without increased sample complexity, or adaptive schemes that reduce queries while matching GGDiff's constraint satisfaction performance.

- **Can non-greedy control approximations improve guidance quality by better accounting for the full diffusion trajectory?**
  - **Basis in paper:** Section 3.2 states the greedy approximation "does not fully account for the entire future trajectory, potentially leading to suboptimal choices, especially in the early stages of the reverse diffusion process."
  - **Why unresolved:** Computing the full expectation requires simulating numerous trajectories from each state, which is computationally prohibitive; no tractable non-greedy alternative has been proposed.
  - **What evidence would resolve it:** A tractable non-greedy controller achieving higher reward alignment or faster convergence than greedy GGDiff variants, particularly in early timesteps.

## Limitations

- The core theoretical framework assumes the uncontrolled diffusion model provides a sufficiently accurate prior, which may not hold for datasets with complex, multimodal structures
- The zero-order optimization approach suffers from high variance in gradient estimates, particularly in high-dimensional graph spaces, potentially requiring large sample counts that impact computational efficiency
- Empirical validation focuses primarily on graph structure and fairness constraints, with limited exploration of diverse reward landscapes or real-world deployment scenarios

## Confidence

- **High Confidence:** The SOC reformulation and its theoretical foundation (Feynman-Kac formula, HJB equation) are mathematically sound and well-established
- **Medium Confidence:** The practical approximations (Tweedie's formula, ZO estimators) are reasonable but rely on empirical performance rather than rigorous theoretical guarantees
- **Medium Confidence:** Empirical results show strong performance on tested benchmarks, but the evaluation scope is somewhat limited in diversity of tasks and datasets

## Next Checks

1. **Sample Efficiency Analysis:** Systematically vary N in the ZO estimators and measure the trade-off between constraint satisfaction quality and computational cost to identify optimal sample sizes for different graph dimensions
2. **Distribution Sensitivity Test:** Evaluate GGDiff's performance when the pre-trained diffusion model's prior is deliberately mismatched (e.g., train on Community-small, test on Enzymes) to assess robustness to prior quality
3. **Constraint Combination Stress Test:** Apply multiple, potentially conflicting constraints simultaneously (e.g., edge count + fairness + link prediction) to test the framework's ability to handle complex reward landscapes without compromising distribution fidelity