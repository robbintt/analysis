---
ver: rpa2
title: 'TPS-Bench: Evaluating AI Agents'' Tool Planning \& Scheduling Abilities in
  Compounding Tasks'
arxiv_id: '2511.01527'
source_url: https://arxiv.org/abs/2511.01527
tags:
- tool
- task
- tools
- completion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TPS-Bench, a benchmark designed to evaluate
  the tool planning and scheduling capabilities of large language model (LLM) agents
  in solving compounding tasks. TPS-Bench comprises 200 tasks of two difficulty levels,
  based on a diverse tool repository of hundreds of MCP tools.
---

# TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks

## Quick Facts
- **arXiv ID:** 2511.01527
- **Source URL:** https://arxiv.org/abs/2511.01527
- **Reference count:** 40
- **Primary result:** Introduces TPS-Bench with 200 tasks evaluating LLM agents' tool planning and scheduling, revealing trade-offs between sequential (higher accuracy) and parallel (higher efficiency) execution strategies.

## Executive Summary
This paper introduces TPS-Bench, a benchmark designed to evaluate the tool planning and scheduling capabilities of large language model (LLM) agents in solving compounding tasks. TPS-Bench comprises 200 tasks of two difficulty levels, based on a diverse tool repository of hundreds of MCP tools. The evaluation emphasizes both task completion rate and efficiency. Empirical studies on popular closed-source and open-source LLMs reveal that most models exhibit reasonable tool planning but differ significantly in scheduling. For example, GLM-4.5 achieves the highest task completion rate of 64.72% with extensive sequential tool calls but suffers from long execution times. In contrast, GPT-4o prioritizes parallel tool calls, achieving a task completion rate of 45.08% but with shorter execution times. The study also explores reinforcement learning to improve scheduling efficiency, showing that Qwen3-1.7B, trained on 100 samples, achieves a 6% improvement in task completion rate and a 14% reduction in execution time.

## Method Summary
The method involves a two-stage agent architecture: a tool selector that identifies relevant tools from a repository of 141 MCP tools (limited to 10), and a scheduler that decomposes tasks and executes tools with dependency analysis. The benchmark includes 200 tasks (Easy: ≤5 subtasks; Hard: ≤50 subtasks with dependencies) evaluated using LLM-as-a-judge with Gemini-2.5-Flash. For RL optimization, Qwen3-1.7B was fine-tuned using GRPO on 100 samples with a reward combining task completion and parallelism metrics.

## Key Results
- GLM-4.5 achieves 64.72% task completion via sequential scheduling, while GPT-4o achieves 45.08% via parallel scheduling
- Tool selection to 10 tools maintains performance while reducing context overflow (32% without selection → 12% with selection for Qwen3-1.7B)
- Qwen3-1.7B fine-tuned with GRPO on 100 samples shows 6% improvement in completion rate and 14% reduction in execution time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential tool scheduling improves task completion rates in compounding tasks at the expense of execution efficiency, whereas parallel scheduling optimizes for speed but risks failure due to unresolved dependencies.
- **Mechanism:** Sequential execution allows an LLM agent to utilize the output of one tool call (e.g., weather data) to ground the input of the next (e.g., clothing recommendation), reducing hallucination errors. Parallel execution attempts independent subtasks simultaneously to minimize latency, but current models struggle to perfectly distinguish independent vs. dependent subtasks, leading to cascading errors if dependencies are missed.
- **Core assumption:** The performance gap is driven primarily by the model's ability to resolve dependencies rather than simple tool invocation capability.
- **Evidence anchors:**
  - [abstract] "GLM-4.5 achieves... 64.72% with extensive sequential tool calls... GPT-4o prioritizes parallel... 45.08% completion rate."
  - [section 4.2] "GLM-4.5 tends to perform tool execution in a strictly sequential manner... GPT-4o tends to execute tool execution in parallel."
  - [corpus] CostBench (neighbor) highlights that current evaluations often overlook resource efficiency, aligning with the trade-off observed here.
- **Break condition:** If a model exhibits a reasoning capability leap that allows perfect dependency resolution, the accuracy penalty for parallel scheduling should disappear.

### Mechanism 2
- **Claim:** Restricting the tool context to a curated selection (top-10 tools) maintains task performance while reducing token consumption and execution latency.
- **Mechanism:** Providing the full repository (hundreds of tools) introduces noise and consumes context window, forcing the model to waste computation on retrieval. Filtering tools reduces the "search space," allowing the model to focus attention on relevant schemas, particularly for smaller models with limited context windows.
- **Core assumption:** The correct tools are successfully identified during the initial selection phase.
- **Evidence anchors:**
  - [section 4.3] "Different tool selection strategies do not lead to substantial differences in task completion rate. However, notable differences are observed in... tokens consumed."
  - [figure 6] "Without tool selection, 32% of cases of Qwen3-1.7B exceeded the context length."
  - [corpus] MCP-Bench (neighbor) confirms tool-use requires cross-tool coordination, implying context management is critical for multi-tool agents.
- **Break condition:** If the tool repository is small enough to fit entirely within the context window without degradation, selection mechanisms become redundant overhead.

### Mechanism 3
- **Claim:** Reinforcement Learning (specifically GRPO) can optimize the efficiency-effectiveness frontier by tuning small models to favor structured, parallel execution patterns over verbose, sequential reasoning.
- **Mechanism:** Standard SFT (Supervised Fine-Tuning) might mimic the sequential patterns of teacher models. GRPO, with a reward signal balancing "task completion" and "degree of parallelism," explicitly incentivizes the model to reduce the number of turns and output tokens.
- **Core assumption:** The reward definition (parallelism + completion) accurately captures the desired behavior for complex scheduling.
- **Evidence anchors:**
  - [abstract] "Qwen3-1.7B, trained on 100 samples... 6% improvement in task completion rate and a 14% reduction in execution time."
  - [section 5] "The model requires fewer tool call turns and generates fewer output tokens... suggesting that reinforcement learning can effectively guide the model toward more structured and parallel execution patterns."
  - [corpus] No direct corpus support for this specific RL result; this appears to be a novel finding in this paper.
- **Break condition:** If RL rewards over-optimize for parallelism, the model may start forcing parallelism on dependent tasks, causing a drop in completion rates.

## Foundational Learning

- **Concept: Model Context Protocol (MCP)**
  - **Why needed here:** The benchmark is built entirely on MCP tools (e.g., Amap, Tavily). Understanding that MCP standardizes how tools expose schemas to agents is necessary to interpret the "Tool Repository" architecture.
  - **Quick check question:** How does MCP differ from a standard REST API definition in the context of LLM context windows?

- **Concept: Dependency Graphs (DAGs) in Task Planning**
  - **Why needed here:** The core challenge of TPS-Bench is scheduling. You must understand Directed Acyclic Graphs to visualize why subtask A (check weather) must precede subtask B (pack clothes), while subtask C (check flight status) is parallel to A.
  - **Quick check question:** In a "compounding task," does an independent subtask always have a finish-to-start dependency with the final summary step?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The paper uses GRPO to improve scheduling. Unlike standard PPO, GRPO compares multiple roll-outs against each other to estimate advantages, which is critical for the "efficiency" tuning observed in the results.
  - **Quick check question:** Why is a group-based relative comparison useful when defining rewards for "efficiency" (time/turns) rather than just "correctness"?

## Architecture Onboarding

- **Component map:** Tool Repository (141 tools from 15 MCP servers) -> Selector Agent (outputs ≤10 tools) -> Scheduler Agent (dependency analysis + tool execution) -> Evaluator (Gemini-2.5-Flash as LLM-as-a-judge)

- **Critical path:**
  1. **Tool Selection:** Critical for small models to avoid context overflow (see Figure 6).
  2. **Dependency Analysis:** The main differentiator between "Easy" and "Hard" benchmarks; errors here cause the drop in GPT-4o's completion rate.
  3. **Parallel Execution:** The lever for efficiency; missing this step reverts the system to slow, sequential behavior.

- **Design tradeoffs:**
  - **Parallel vs. Safe:** GPT-4o prioritizes parallel (fast but risky 45% success), GLM-4.5 prioritizes sequential (slow but safe 65% success). System architects must choose based on latency SLAs vs. accuracy requirements.
  - **Rule-based vs. Self-selection:** Rule-based is faster/cheaper; Self-selection is smarter but adds token overhead.

- **Failure signatures:**
  - **Context Length Exceeded:** Common in small models (<7B) when Tool Selection is skipped.
  - **Hallucinated Dependencies:** Attempting parallel calls where sequential is required (e.g., searching attractions *before* knowing the city).

- **First 3 experiments:**
  1. **Baseline Establishment:** Run GPT-4o and GLM-4.5 on TPS-Bench-Hard to replicate the Sequential vs. Parallel trade-off curves (Time vs. Completion Rate).
  2. **Ablation on Context:** Disable the "Tool Selector" component for Qwen3-1.7B and measure the percentage of tasks failing due to context length vs. reasoning errors.
  3. **GRPO Fine-tuning Run:** Train Qwen3-1.7B on the TPS-100 dataset using the specified reward (Parallelism + Completion) to verify the 14% efficiency gain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can future model architectures be optimized to resolve the dichotomy between execution efficiency and task completion, achieving both high parallelism (like GPT-4o) and high reliability (like GLM-4.5) simultaneously?
- **Basis in paper:** [explicit] The authors contrast GLM-4.5’s high completion rate (64.72%) via sequential calls against GPT-4o’s low completion rate (45.08%) via parallel calls, stating models "differ in scheduling."
- **Why unresolved:** The empirical results show current models exist on a Pareto frontier where optimizing for speed reduces reasoning accuracy regarding dependencies.
- **What evidence would resolve it:** A model achieving >60% completion on TPS-Bench-Hard with an average execution time comparable to GPT-4o (<80s).

### Open Question 2
- **Question:** Does the reinforcement learning strategy (GRPO) used to enhance scheduling in Qwen3-1.7B generalize effectively to larger models, or does it primarily patch specific deficits in smaller architectures?
- **Basis in paper:** [explicit] The authors describe their RL work as an "initial study" on Qwen3-1.7B using "rarely 100 RL training samples."
- **Why unresolved:** The sample size is very small, and it is unclear if the resulting efficiency gains are specific to the small parameter count or transferable to state-of-the-art models.
- **What evidence would resolve it:** Successful application of the same GRPO training pipeline to a 70B+ parameter model, showing similar relative reductions in execution time without loss of task completion.

### Open Question 3
- **Question:** What specific failure modes in reasoning cause models like QwQ-32B to struggle with dependency analysis, leading them to invoke tools simultaneously even when sequential execution is required?
- **Basis in paper:** [explicit] The paper notes QwQ-32B "struggles to clearly distinguish dependencies among subtasks and often invokes multiple tools simultaneously," resulting in low completion rates.
- **Why unresolved:** The paper identifies the behavioral symptom (parallelizing dependent tasks) but does not investigate the underlying cognitive or attention mechanism failure.
- **What evidence would resolve it:** An interpretability analysis or ablation study mapping attention heads to the dependency graph generation process in reasoning models.

### Open Question 4
- **Question:** How robust is the "self-selection" tool retrieval strategy as the tool repository scales from hundreds to tens of thousands of tools?
- **Basis in paper:** [inferred] The paper limits tool selection to 10 and shows that "No selection" (using all tools) causes context overflow and massive latency, but it does not test the upper limits of the repository size the retrieval agent can handle.
- **Why unresolved:** Real-world agentic systems may have access to significantly larger tool sets than the hundreds defined in TPS-Bench, potentially degrading the Tool Selection Score.
- **What evidence would resolve it:** Evaluating the Tool Selection Score and context management efficiency of agents when the repository size is artificially inflated to 10,000+ tools.

## Limitations
- The benchmark tasks and training samples are not publicly released, preventing independent verification
- Results rely on LLM-as-a-judge evaluation, introducing potential subjectivity despite reported Pearson correlation of 0.75
- RL optimization is demonstrated only on Qwen3-1.7B with 100 samples, limiting generalizability claims

## Confidence
- **High Confidence:** The sequential vs. parallel scheduling trade-off mechanism (Mechanism 1) is well-supported by comparative results across multiple models and aligns with established software engineering principles about dependency resolution.
- **Medium Confidence:** The tool selection context reduction benefit (Mechanism 2) is empirically demonstrated but relies on specific model sizes and may not generalize to larger context windows becoming standard.
- **Medium Confidence:** The GRPO efficiency improvements (Mechanism 3) show measurable gains but are demonstrated on a single model-family and training dataset size, requiring replication for broader validation.

## Next Checks
1. **Replicate the sequential/parallel trade-off:** Run GPT-4o and GLM-4.5 on a subset of TPS-Bench-Hard tasks to verify the 45% vs. 65% completion rate differential and corresponding execution time differences.
2. **Test tool selection ablation:** Disable the tool selector for Qwen3-1.7B and measure the exact percentage of context-overflow failures versus successful task completions with selection enabled.
3. **GRPO generalization test:** Fine-tune a different small model (e.g., Llama-3.1-8B) using the same TPS-100 dataset and reward formulation to verify if the 6% completion improvement and 14% time reduction are model-specific or methodology-generalizable.