---
ver: rpa2
title: 'Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from
  Street View Imagery'
arxiv_id: '2511.00362'
source_url: https://arxiv.org/abs/2511.00362
tags:
- heritage
- image
- cultural
- such
- street
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Oitijjo-3D, a generative AI framework that
  rapidly reconstructs 3D models of Bangladeshi heritage sites from publicly available
  Google Street View imagery. The framework uses a two-stage pipeline: Gemini 2.5
  Flash Image generates isometric 2D architectural views from structured prompts,
  and Hexagen converts these into textured 3D meshes.'
---

# Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery

## Quick Facts
- **arXiv ID:** 2511.00362
- **Source URL:** https://arxiv.org/abs/2511.00362
- **Reference count:** 21
- **Primary result:** Generates 3D heritage models in ~45s using Street View imagery, 250x faster than photogrammetry

## Executive Summary
Oitijjo-3D is a generative AI pipeline that rapidly reconstructs 3D models of Bangladeshi heritage sites from publicly available Google Street View imagery. The system uses a two-stage approach: Gemini 2.5 Flash Image generates isometric 2D architectural views from structured prompts, and Hexagen converts these into textured 3D meshes. Tested on eight landmarks including Ahsan Manzil and Paharpur Buddhist Monastery, the framework produces photorealistic, metrically coherent models in about 45 seconds on average—over 250 times faster than conventional photogrammetry methods (4–8 hours). The approach achieves high visual fidelity without requiring specialized hardware or expert supervision, dramatically lowering economic and technical barriers to cultural heritage preservation.

## Method Summary
The Oitijjo-3D pipeline consists of five stages: (1) multi-view Street View acquisition from Google Maps, (2) Python-based prompt generation encoding architectural attributes like site name, type, material, and features, (3) Gemini 2.5 Flash Image synthesizes 1024×1024 isometric renders (~10–12s), (4) Hexagen performs image-to-3D conversion using latent diffusion priors (30–60s), and (5) web visualization via React/TypeScript frontend and Laravel backend. The system processes eight Bangladeshi landmarks, producing glTF 2.0 models with 50K–100K triangles and 69 MB memory footprint, achieving complete reconstruction in approximately 45 seconds versus 4–8 hours for traditional photogrammetry.

## Key Results
- Generates 3D heritage models in ~45 seconds versus 4–8 hours for traditional photogrammetry
- Produces glTF 2.0 models with 50K-100K triangles and 69 MB memory footprint
- Achieves high visual fidelity and metrically coherent geometry without requiring specialized hardware or expert supervision
- Successfully reconstructs eight Bangladeshi landmarks including Ahsan Manzil and Paharpur Buddhist Monastery

## Why This Works (Mechanism)

### Mechanism 1
Using a synthesized isometric 2D projection as an intermediate representation reduces depth ambiguity and improves geometric fidelity for the downstream 3D generator. The Gemini 2.5 Flash Image model transforms ground-level Street View images into 1024×1024 pixel, 45° top-down isometric renders, forcing the model to infer full 3D massing, roof geometry, and facade details. This "flattens" the 3D problem into a controlled 2D representation that is easier for subsequent 3D lifting.

### Mechanism 2
A neural image-to-3D generation module (Hexagen) infers complete, textured 3D meshes from synthesized isometric images by leveraging learned latent diffusion priors. The system encodes the 2D image and applies a generative process to infer full 3D geometry, filling occluded areas and assigning materials/textures based on training priors.

### Mechanism 3
A sequential, two-stage pipeline decouples semantic understanding from 3D geometry inference, allowing specialized state-of-the-art models for each sub-task. The first stage (Gemini) handles complex reasoning and visual synthesis, while the second stage (Hexagen) specializes in geometric lifting, avoiding forcing a single model to be expert at both tasks.

## Foundational Learning

- **Isometric Projection**
  - Why needed here: This is the core representational bridge in the pipeline. Understanding it's a parallel projection (vs perspective) clarifies why it "reduces depth ambiguity" for the 3D generator.
  - Quick check question: Can you explain why an isometric projection eliminates the "vanishing point" problem inherent in the original Street View perspective photos?

- **Structure-from-Motion (SfM)**
  - Why needed here: The paper frames results as a 250x speedup against this traditional baseline. Understanding SfM's requirements (multiple overlapping images, feature matching) clarifies the problem being solved.
  - Quick check question: What is the minimum data requirement for traditional SfM pipelines to reconstruct a 3D scene, and how does Oitijjo-3D's requirement differ?

- **Latent Diffusion Models**
  - Why needed here: Both pipeline stages rely on models built on this technology. Grasping the basic idea—learning to denoise data in compressed latent space—is necessary to understand how the system produces novel outputs.
  - Quick check question: How does a diffusion model generate a new image, starting from pure noise? (Briefly describe the iterative denoising process).

## Architecture Onboarding

- **Component map:** Prompt Engineering -> 2D Synthesis (Gemini) -> 3D Generation (Hexagen) -> Web Visualization
- **Critical path:** Sequential dependency: Prompt Engineering → 2D Synthesis (Gemini) → 3D Generation (Hexagen). Any failure or significant latency in Gemini or Hexagen API calls directly blocks the pipeline. Final 3D model quality depends on the quality of the synthesized 2D isometric image.
- **Design tradeoffs:** Uses proprietary APIs (Gemini, Hexagen) for speed and low resource use, but introduces costs and data privacy concerns; prioritizes rapid "good enough" reconstruction over sub-centimeter accuracy; generative approach infers unseen parts enabling complete models but trading off structural authenticity.
- **Failure signatures:** Misinterpreted landmarks producing factually incorrect models; topological errors like holes or intersecting faces; texture projection artifacts on complex geometries like curved domes.
- **First 3 experiments:**
  1. End-to-End Test with a Known Landmark: Run Taj Mahal through pipeline, compare against ground-truth photos and existing 3D models.
  2. Ablation on Prompt Engineering: Run same Street View image with minimal vs detailed prompts, compare resulting 2D isometric images.
  3. Component Benchmarking: Feed Hexagen real photograph vs synthetically generated isometric image of same building, compare resulting 3D models.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What is the quantitative geometric accuracy of the reconstructed models compared to ground truth?
**Basis in paper:** The paper claims models are "metrically coherent" and "structurally faithful" but evaluates primarily through visual fidelity and processing speed, omitting standard error metrics against LiDAR or photogrammetry data.
**Why unresolved:** Research focused on feasibility and accessibility in resource-constrained context, likely lacked access to precise ground truth data.
**What evidence would resolve it:** Comparative study calculating Chamfer distance or point-to-plane RMSE between AI-generated meshes and high-precision laser scans.

### Open Question 2
**Question:** Can open-source generative models achieve comparable fidelity to the proprietary APIs used?
**Basis in paper:** Conclusion states future work aims to transition to open-source implementations (Zero-1-to-3, OpenLRM) to reduce costs and improve scalability.
**Why unresolved:** Current implementation relied on proprietary APIs due to GPU resource limitations.
**What evidence would resolve it:** Benchmark running pipeline with open-source weights on local hardware, measuring drop in texture resolution or geometric consistency.

### Open Question 3
**Question:** Does hybrid photometric refinement effectively correct geometric imperfections in complex architectural features?
**Basis in paper:** Results acknowledge "minor imperfections appear in modeling curved domes, fine ornaments, and reflective materials," suggesting improvement through "future hybrid photometric refinement."
**Why unresolved:** Current pipeline is purely generative without optimization step to enforce multi-view consistency or correct geometric hallucinations.
**What evidence would resolve it:** Ablation studies showing reduction in surface artifacts after inputting generated mesh into photometric refinement loop.

## Limitations
- Evaluation relies on qualitative visual comparisons rather than quantitative geometric accuracy metrics
- Pipeline depends on proprietary APIs (Gemini, Hexagen) introducing cost and access barriers
- Claims about architectural accuracy lack systematic validation against ground truth measurements

## Confidence
- **High confidence**: Performance speed advantage over photogrammetry clearly demonstrated with specific time measurements (45s vs 4–8h)
- **Medium confidence**: Visual quality and general feasibility supported by example outputs but lack rigorous quantitative validation
- **Low confidence**: Claims about architectural accuracy and preservation of heritage details are largely qualitative with no systematic comparison against measurements

## Next Checks
1. **Geometric accuracy assessment**: Compare 3D outputs from Oitijjo-3D against laser-scanned reference models using quantitative metrics (point-to-surface distance, mesh similarity scores).
2. **Cross-architecture generalization**: Test pipeline on heritage sites from different architectural traditions (European Gothic cathedrals, Southeast Asian temples) to evaluate training generalization.
3. **Cost-benefit analysis over scale**: Calculate total cost of running pipeline across hundreds of sites using current API pricing versus one-time equipment cost of photogrammetry, including ongoing maintenance.