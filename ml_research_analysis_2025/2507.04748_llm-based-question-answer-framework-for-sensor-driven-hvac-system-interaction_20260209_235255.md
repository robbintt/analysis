---
ver: rpa2
title: LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction
arxiv_id: '2507.04748'
source_url: https://arxiv.org/abs/2507.04748
tags:
- data
- jarvis
- response
- user
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents JARVIS, a two-stage LLM-based QA framework
  for sensor-driven HVAC system interaction. The framework uses an Expert-LLM to translate
  user queries into structured execution instructions, followed by an Agent that performs
  SQL-based data retrieval, statistical processing, and response generation.
---

# LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction

## Quick Facts
- arXiv ID: 2507.04748
- Source URL: https://arxiv.org/abs/2507.04748
- Reference count: 40
- Key outcome: JARVIS framework consistently outperforms baseline models on response quality (>4.0 for cohesiveness and truthfulness) and query accuracy in real-world HVAC data.

## Executive Summary
This paper presents JARVIS, a two-stage LLM-based QA framework for sensor-driven HVAC system interaction. The framework uses an Expert-LLM to translate user queries into structured execution instructions, followed by an Agent that performs SQL-based data retrieval, statistical processing, and response generation. Key innovations include an adaptive context injection strategy, a parameterized SQL builder for robust data access, and a bottom-up planning scheme for coherent multi-stage responses. Evaluated on real-world HVAC data and expert-curated QA datasets, JARVIS consistently outperforms baseline models, achieving high response quality scores (>4.0 for cohesiveness and truthfulness) and accurate query execution.

## Method Summary
JARVIS employs a two-stage architecture: an Expert-LLM (fine-tuned LLaMA3.1-8B) translates natural language queries into structured JSON instructions using metadata context, and an Agent (EXAONE-3.5-7.8B) executes SQL retrieval, statistical processing via Pandas, and final response generation. The framework uses parameterized SQL builders with metadata mapping for robust data access, and a bottom-up planning approach via an Expectation component that defines the expected response format before planning retrieval steps. The system was evaluated on one year of 1-minute interval HVAC sensor data from 156 units, using 80 expert-curated QA pairs (53 training, 27 test) and LLM-as-a-Judge evaluation methodology.

## Key Results
- JARVIS achieves response quality scores >4.0 for cohesiveness and truthfulness, outperforming TAG baseline
- Query execution accuracy shows 0.74→0.92 precision and 0.70→0.84 recall improvements using modular sub-queries
- End-to-end latency dominated by Expert-LLM (8.86s for complex processing vs 5.47s for simple queries)
- User study shows w/o Expect variant conflates results, lowering cohesiveness/helpfulness scores

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Expert-LLM + Agent Architecture
- Claim: Decoupling semantic interpretation from data retrieval/processing improves response quality.
- Mechanism: The Expert-LLM (fine-tuned LLaMA3.1-8B) translates user-native queries into structured JSON instructions using metadata (taxonomy mappings, current time). The Agent (general-purpose EXAONE-3.5-7.8B) executes SQL retrieval and Python processing before response generation.
- Core assumption: Fine-tuning a smaller model for domain-specific instruction generation is more effective than using a general-purpose model for both understanding and execution.
- Evidence anchors:
  - [abstract] "JARVIS employs an Expert-LLM to translate high-level user queries into structured execution instructions, and an Agent that performs SQL-based data retrieval, statistical processing, and final response generation."
  - [section 5.3] JARVIS outperforms TAG baseline in cohesiveness (>4.0), truthfulness (>4.0), and helpfulness (~3.7).
  - [corpus] Related HVAC+LLM work (arXiv:2505.05796) uses similar domain-specific adaptation strategies.
- Break condition: If Expert-LLM training data doesn't cover deployment-specific terminology, instruction generation degrades.

### Mechanism 2: Parameterized SQL Builder with Metadata Mapping
- Claim: Abstracting SQL generation into high-level intents with rule-based expansion reduces query failures.
- Mechanism: Expert-LLM outputs user-native conditions (e.g., "Bedroom", "Recent7days"); the Query Builder maps these to database-native fields using metadata dictionaries, then constructs complete SQL including null-filtering boilerplate.
- Core assumption: Rule-based SQL construction is more reliable than end-to-end text-to-SQL generation for HVAC-specific patterns.
- Evidence anchors:
  - [section 2.3] Preliminary study shows 15% error rate for text-to-SQL on database-native inputs, with 43.2% failures from complex SQL constructs.
  - [section 3.4.1] "The Expert-LLM is only responsible for expressing high-level query intentions... while a rule-based SQL generation module converts these into complete, executable SQL statements."
  - [corpus] Limited direct corpus evidence; related text-to-SQL surveys show similar brittleness in complex domains.
- Break condition: If metadata mappings are incomplete or schema changes, query translation fails silently.

### Mechanism 3: Bottom-Up Planning via Expectation Component
- Claim: Defining the expected response format before planning retrieval/processing improves coherence.
- Mechanism: Expert-LLM generates an Expectation component (expected answer template) first, then plans backward to identify required data queries and processing steps (variable alignment).
- Core assumption: Explicitly constraining the output format reduces instruction misalignment.
- Evidence anchors:
  - [section 3.3.3] "By explicitly defining and training samples of these expectations, JARVIS performs bottom-up planning as it establishes the final output goal and works backward."
  - [section 5.5] User study shows w/o Expect variant conflates results (averaging across rooms instead of separate computations), lowering cohesiveness/helpfulness.
  - [corpus] PedagoSense (arXiv:2602.01169) uses similar two-stage strategy for contextual response generation.
- Break condition: If expectation templates over-constrain responses, naturalness degrades (rigid outputs like "larger by 0").

## Foundational Learning

- Concept: **Fine-tuning vs. Prompting for Domain Knowledge**
  - Why needed here: JARVIS uses fine-tuning for HVAC-common knowledge (stable, long-form) and prompting for deployment-specific metadata (dynamic, compact).
  - Quick check question: Can you explain why null-filtering logic belongs in the rule-based SQL builder rather than in LLM prompts?

- Concept: **Chain-of-Thought (CoT) for Structured Output Generation**
  - Why needed here: The Thinking component explicitly generates intermediate reasoning steps before JSON instruction generation.
  - Quick check question: What is the trade-off between CoT token overhead and query optimization latency (hint: see Figure 7)?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: Response quality metrics (cohesiveness, helpfulness, truthfulness) rely on GPT-4o/Gemini-2.5 as automated evaluators.
  - Quick check question: What validation step did the authors use to verify LLM-as-a-Judge scores correlate with human ratings?

## Architecture Onboarding

- Component map:
  Expert-LLM (fine-tuned LLaMA3.1-8B) -> Thinking -> Expectation -> Instructions (JSON) -> Agent -> Query Builder/Executor -> Data Processor (Pandas) -> Response Generator (EXAONE-3.5) -> TimescaleDB

- Critical path:
  1. User query → Expert-LLM with metadata context
  2. Expert-LLM generates Thinking → Expectation → Instructions
  3. Agent parses instructions, calls Query Builder with mappings
  4. Query Builder generates SQL, executes against TimescaleDB
  5. Data Processor runs Pandas scripts for statistical operations
  6. Response Generator LLM produces final answer using processed data + Expectation template

- Design tradeoffs:
  - Token length vs. latency: More detailed instructions increase Expert-LLM latency (Figure 11: ~0.03s per 100 tokens)
  - Response richness vs. stability: Expectation templates improve coherence but reduce naturalness (Section 5.3 limitation)
  - Query accuracy vs. recall: Modular sub-queries favor precision over recall (Figure 10: 0.74→0.92 precision, 0.70→0.84 recall)

- Failure signatures:
  - Malformed JSON from Expert-LLM (1/27 cases): missing/extra brackets → regex post-processing needed
  - Schema drift: metadata mismatches cause query translation failures (silent)
  - Context overflow: large query results exceed response generator input limits (TAG baseline failed 3/27 cases)

- First 3 experiments:
  1. Validate metadata coverage: Query database for all sensor modalities/spatial identifiers; verify metadata mappings exist.
  2. Test SQL builder robustness: Submit queries with implicit conditions (e.g., "my room") and verify null-filtering + taxonomy resolution.
  3. Profile latency breakdown: Measure Expert-LLM token output length vs. end-to-end latency; identify bottlenecks for queries requiring complex processing (Proc. O: 8.86s vs. Proc. X: 5.47s).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can structured decoding mechanisms eliminate JSON schema violations in the Expert-LLM more effectively than the current reactive regex-based patching?
- Basis: [Explicit] Section 6 states that an "interesting direction for future work is to incorporate structured decoding mechanisms" to address malformed JSON outputs that cause execution failures.
- Why unresolved: The current system relies on a brittle, reactive post-processing patch to fix syntax errors like missing brackets, rather than ensuring correctness by construction.
- Evidence: A comparative evaluation measuring the rate of successful instruction executions and schema errors between the current regex method and a constrained decoding approach.

### Open Question 2
- Question: How can the framework be optimized to balance the trade-off between flexible, conversational responses and stable, information-dense answers?
- Basis: [Explicit] Section 6 notes that the current design prioritizes factual stability over naturalness, and "balancing these competing objectives... is left for future work."
- Why unresolved: The reliance on strict expectation templates often results in rigid responses (e.g., stating a difference is "larger by 0" rather than "the same"), lacking contextual sensitivity.
- Evidence: A user study measuring satisfaction scores on "naturalness" versus "factuality" for responses generated with varying degrees of template constraint.

### Open Question 3
- Question: To what extent can model optimization techniques (e.g., quantization, distillation) reduce the Expert-LLM's high inference latency without degrading HVAC reasoning capabilities?
- Basis: [Explicit] Section 5.7 identifies the Expert-LLM as the primary latency bottleneck, and Section 6 suggests exploring "smaller, optimized model architectures" as a direction for future research.
- Why unresolved: The current implementation uses a standard 8B parameter model, and the impact of aggressive optimization on domain-specific reasoning quality remains untested.
- Evidence: Benchmarks comparing end-to-end latency and response quality scores (e.g., cohesiveness, truthfulness) between the current model and optimized variants (e.g., 4-bit quantization).

## Limitations
- Limited external validity due to small fine-tuning dataset (53 samples) and test set (27 samples)
- Metadata mapping completeness not independently verified
- LLM-as-a-Judge evaluation lacks direct human validation correlation data
- Modular sub-query approach may systematically favor precision over recall

## Confidence
- **High Confidence**: Two-stage architecture mechanism well-supported by ablation studies showing performance degradation when Expectation components are removed
- **Medium Confidence**: Response quality metrics show consistent superiority over TAG baseline, but automated evaluation methodology lacks human validation
- **Low Confidence**: Claims about bottom-up planning superiority rely primarily on user study comparisons without testing alternative strategies

## Next Checks
1. **Metadata Coverage Validation**: Query the database to enumerate all sensor modalities, spatial identifiers, and time ranges. Verify that the metadata mappings used by the SQL builder comprehensively cover these dimensions, and test implicit condition resolution (e.g., "my room") with multiple user personas.
2. **Schema Drift Robustness Test**: Simulate schema changes by modifying column names or adding/removing sensor types. Evaluate whether the parameterized SQL builder fails gracefully or silently, and assess the impact on query accuracy.
3. **Cross-Validation of LLM-as-a-Judge**: Conduct a small-scale human evaluation (n=10-20) on a subset of responses. Calculate correlation coefficients between automated and human scores for cohesiveness, helpfulness, and truthfulness to validate the evaluation methodology.