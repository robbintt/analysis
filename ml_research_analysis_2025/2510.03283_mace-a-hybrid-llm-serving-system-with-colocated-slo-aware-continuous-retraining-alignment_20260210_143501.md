---
ver: rpa2
title: 'MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining
  Alignment'
arxiv_id: '2510.03283'
source_url: https://arxiv.org/abs/2510.03283
tags:
- mace
- retraining
- inference
- memory
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACE is a GPU-local scheduling system that co-executes LLM inference
  and continuous retraining to maintain alignment accuracy under data drift. It uses
  iteration-level hybrid batching, dynamic prioritization based on latency and alignment
  rewards, and cache management (prefix sharing and KV cache pruning) to maximize
  GPU utilization while minimizing latency.
---

# MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment

## Quick Facts
- arXiv ID: 2510.03283
- Source URL: https://arxiv.org/abs/2510.03283
- Reference count: 32
- MACE is a GPU-local scheduling system that co-executes LLM inference and continuous retraining to maintain alignment accuracy under data drift.

## Executive Summary
MACE is a GPU-local scheduling system that co-executes LLM inference and continuous retraining to maintain alignment accuracy under data drift. It uses iteration-level hybrid batching, dynamic prioritization based on latency and alignment rewards, and cache management (prefix sharing and KV cache pruning) to maximize GPU utilization while minimizing latency. Compared to periodic and synchronous retraining baselines, MACE improves win rate by up to 1.03× and CLPD by up to 1.34× on real-world datasets, reduces inference latency by up to 63%, and sustains throughput under resource constraints, especially on edge devices like NVIDIA AGX Orin. The design effectively mitigates memory fragmentation and enables timely model adaptation without compromising SLOs.

## Method Summary
MACE implements iteration-level hybrid batching with a best-fit bin packing heuristic to co-locate LLM inference (prefill/decode) and continuous alignment retraining (DPO/LoRA) on a single GPU. The scheduler dynamically prioritizes tasks based on type (inference > training) and alignment utility (DPO loss), while cache management (prefix sharing and KV pruning) reduces memory pressure. The system is built on vLLM with PEFT/LoRA adapters and profiles task memory/latency characteristics offline to guide scheduling decisions.

## Key Results
- Improves win rate by up to 1.03× and CLPD by up to 1.34× compared to periodic and synchronous retraining baselines
- Reduces inference latency by up to 63% through cache management and efficient scheduling
- Sustains throughput under resource constraints, particularly on edge devices like NVIDIA AGX Orin
- Effectively mitigates memory fragmentation through iteration-level hybrid batching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iteration-level hybrid batching with best-fit bin packing reduces memory fragmentation and increases GPU utilization compared to periodic or synchronous scheduling.
- Mechanism: The scheduler packs heterogeneous tasks (prefill, decode, fine-tune) into single iteration batches using a score that balances memory fit and latency divergence, minimizing idle memory gaps.
- Core assumption: Workload memory and latency profiles can be accurately profiled offline and tasks are schedulable at iteration boundaries.
- Evidence anchors: [abstract] iteration-level hybrid batching maximizes utilization while minimizing latency; [section 4.2] Algorithm 1 uses best-fit score $f = \lambda_1 \cdot |\text{memory fit}| + \lambda_2 \cdot |\text{latency fit}|$.
- Break condition: Stale profiling data or drastic workload shifts can cause suboptimal batching, leading to OOM errors or SLO violations.

### Mechanism 2
- Claim: Fine-tuning tasks prioritized by alignment utility (DPO loss) adapt to data drift faster without requiring maximum retraining throughput.
- Mechanism: Dynamic priority assignment $P(t)$ incorporates reward term $\gamma \cdot L_{DPO}$, boosting priority for retraining samples with high misalignment.
- Core assumption: DPO loss is a valid proxy for sample importance in correcting model drift.
- Evidence anchors: [section 4.1] MACE integrates DPO loss into priority: $P_{ft}^{total}(t) = P_{ft}(t) + \gamma \cdot L_{DPO}$; [figure 7] shows task prioritization based on updated priorities.
- Break condition: Noisy or adversarial samples with high loss but low utility can waste resources.

### Mechanism 3
- Claim: Memory pressure reduction via cache management creates room for co-located fine-tuning tasks, reducing inference-throughput/update-frequency trade-off.
- Mechanism: Prefix sharing (Trie structure) reuses KV cache for common prompt prefixes; KV pruning evicts low-contribution cache entries during decoding.
- Core assumption: User prompts contain redundant prefixes and attention heads show exploitable sparsity.
- Evidence anchors: [section 4.3] Prefix sharing exploits input redundancy; [figure 8a] shows 36% prefix sharing ratio; [corpus] SuperInfer validates KV cache management importance.
- Break condition: Unique prompts or models requiring full attention density can degrade accuracy or fail to save memory.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: MACE relies on LoRA to perform fine-tuning on edge GPUs; standard full fine-tuning would likely OOM or stall inference entirely.
  - Quick check question: Can you explain why LoRA allows the "backward pass" to occur without rewriting the massive base model weights?

- **Service Level Objectives (SLOs) & TTFT/TBT**
  - Why needed here: System success is measured by adhering to latency constraints (SLOs), requiring distinction between Time To First Token (TTFT) and Time Between Tokens (TBT).
  - Quick check question: Which latency metric (TTFT or TBT) is most affected by memory fragmentation during the decode phase?

- **Direct Preference Optimization (DPO)**
  - Why needed here: MACE uses DPO loss as the "alignment reward" signal for prioritization, optimizing preferences directly from data pairs without a separate reward model.
  - Quick check question: In MACE, does a higher DPO loss value increase or decrease a retraining task's priority?

## Architecture Onboarding

- **Component map:** CPU Scheduler (Global Scheduler with Prioritization + Bin Packing) -> GPU Workers (Prefill with Prefix Engine, Decode with Cache Engine, Fine-tune with LoRA Adapters) -> Cache Engines (Prefix Engine with Trie, Cache Engine with Pruning) -> Feedback Loop (Compute Reward updates priorities post-execution)

- **Critical path:** 1) Request arrives and gets tagged with arrival time & workload type 2) Priority calculated based on wait time + type + (if FT) DPO loss 3) Bin Packing (Algorithm 1) tries to fit request into current iteration bin using best-fit score 4) GPU executes batch with active cache engines 5) CheckEnd: if not finished, task re-queued with updated priority

- **Design tradeoffs:** Scheduler adds ~15ms overhead (5x higher than baselines) but reduces GPU idle time; KV pruning frees memory for FT but risks dropping context; accuracy vs memory trade-off exists.

- **Failure signatures:** OOM on Edge (bin packing too aggressive, tau_mem threshold too high); Stale Model (FT tasks perpetually starved, gamma or delta_ft too low); High Latency Jitter (prefix sharing fails + high retrain rate creates memory churn).

- **First 3 experiments:** 1) Run Figure 11 analysis to verify TTFT and TBT remain stable as Retrain Rate increases from 10% to 50%; 2) Disable MACE/Prune and MACE/Prefix (Figure 12) to measure delta in SLO attainment; 3) Run trace-driven test (Figure 2) to confirm Continuous (MACE) recovers from data drift faster than Periodic (Ekya).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MACE be extended to support hierarchical or federated scheduling across distributed GPU clusters while maintaining cache coherence?
- Basis in paper: [explicit] Section 7 states, "Future extensions may explore hierarchical or federated variants of MACE that operate over distributed GPU clusters."
- Why unresolved: Current design assumes single-node architecture; extending to multiple edge nodes introduces unsolved challenges regarding model version synchronization and distributed cache consistency.
- What evidence would resolve it: Implementation in multi-node setup demonstrating cross-node retraining coordination without violating latency SLOs.

### Open Question 2
- Question: Can the reliance on offline profiling be replaced by an online-adaptive mechanism to handle dynamic workload changes?
- Basis in paper: [explicit] Section 7 notes, "Reducing this profiling overhead or making the scheduler online-adaptive remains an important direction for future work."
- Why unresolved: System currently depends on static, offline profiling to estimate prefill/decode latencies, which may not generalize to heterogeneous GPUs or shifting data distributions without re-profiling.
- What evidence would resolve it: Variant of MACE that dynamically updates latency and memory estimates during runtime, showing robustness to hardware changes or unseen batch sizes.

### Open Question 3
- Question: Can hybrid CPU-GPU execution be integrated into the MACE scheduler to improve energy efficiency?
- Basis in paper: [explicit] Section 7 suggests, "Integrating such hybrid CPU-GPU execution into MACE is a promising extension to reduce energy and cost."
- Why unresolved: Current system focuses exclusively on GPU execution to minimize latency; offloading tasks to CPU requires complex trade-off analysis between energy savings and potential SLO violations.
- What evidence would resolve it: Experiments showing that offloading specific lightweight inference or fine-tuning tasks to CPU reduces energy consumption while scheduler maintains acceptable throughput.

## Limitations

- The scheduling algorithm's effectiveness depends on precise offline profiling that may not remain stable under real-world workload shifts
- DPO-based reward mechanism assumes alignment loss is a reliable proxy for retraining utility, but this is not experimentally validated
- Cache optimizations are shown to work on specific datasets with high prefix overlap but may degrade on datasets with unique or adversarial prompts

## Confidence

**High Confidence:**
- The core design of iteration-level hybrid batching with dynamic prioritization is technically sound and aligns with established GPU scheduling principles
- The memory optimizations (prefix sharing, KV pruning) are well-established techniques with clearly specified integration into MACE

**Medium Confidence:**
- The reported improvements in win rate (1.03×) and CLPD (1.34×) are plausible but depend on untested hyperparameter choices
- The latency reductions (up to 63%) are consistent with known benefits of cache management and batching, though the interplay with continuous retraining is not fully isolated

**Low Confidence:**
- The generalizability of the scheduling algorithm to workloads with drastically different memory/latency profiles is unverified
- The robustness of DPO-based prioritization under noisy or adversarial loss signals is not tested

## Next Checks

1. **Profile Stability Under Shift:** Run the iteration-level scheduler with synthetic workloads that vary memory/latency profiles (e.g., sudden influx of large prefill requests) and measure whether bin packing efficiency degrades and SLOs are violated.

2. **Ablation of DPO Reward:** Disable the γ · L_DPO term in the priority function and compare alignment recovery speed under data drift to test whether the reward mechanism is essential or merely beneficial.

3. **Cache Effectiveness on Unique Prompts:** Construct a test dataset with minimal prefix overlap (<5%) and measure the impact on memory savings and SLO attainment to validate whether optimizations are robust to prompt diversity.