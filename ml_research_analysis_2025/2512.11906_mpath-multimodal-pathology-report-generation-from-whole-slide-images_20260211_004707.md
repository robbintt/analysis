---
ver: rpa2
title: 'MPath: Multimodal Pathology Report Generation from Whole Slide Images'
arxiv_id: '2512.11906'
source_url: https://arxiv.org/abs/2512.11906
tags:
- pathology
- mpath
- report
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MPath, a lightweight multimodal framework
  for automated pathology report generation from whole slide images (WSIs). The method
  leverages pretrained biomedical language model (BioBART) and injects WSI-derived
  visual embeddings via a learned visual-prefix prompting mechanism, avoiding costly
  end-to-end vision-language pretraining.
---

# MPath: Multimodal Pathology Report Generation from Whole Slide Images

## Quick Facts
- arXiv ID: 2512.11906
- Source URL: https://arxiv.org/abs/2512.11906
- Reference count: 0
- Primary result: 4th place ranking on REG 2025 Grand Challenge Test Phase 2 with score 0.8282

## Executive Summary
This paper introduces MPath, a lightweight multimodal framework for automated pathology report generation from whole slide images (WSIs). The method leverages a pretrained biomedical language model (BioBART) and injects WSI-derived visual embeddings via a learned visual-prefix prompting mechanism, avoiding costly end-to-end vision-language pretraining. MPath was evaluated on the REG 2025 Grand Challenge dataset, ranking 4th in Test Phase 2 despite limited tuning opportunities. The approach demonstrates that prompt-based multimodal conditioning is a scalable and interpretable strategy for pathology report generation, with results highlighting potential for clinically meaningful outputs.

## Method Summary
MPath generates pathology reports by combining foundation model WSI features (CONCH for patch-level, Titan for slide-level) with a frozen BioBART language model. Visual embeddings are projected through a learned MLP to form prefix tokens that are prepended to text embeddings, enabling the decoder to condition on image semantics. Only the visual prompt encoder and auxiliary heads are trained, preserving the language backbone for stability and data efficiency. The system was evaluated on the REG 2025 dataset using a composite score of ROUGE-L, BLEU-4, keyword overlap, and embedding similarity.

## Key Results
- Ranked 4th in REG 2025 Grand Challenge Test Phase 2 with score 0.8282
- Demonstrates hallucination of findings not in ground truth (e.g., "chronic granulomatous inflammation")
- Shows quantitative errors (Gleason grade, tumor volume) indicating precision limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual prefix prompting enables effective WSI-to-text generation without costly vision-language pretraining.
- Mechanism: WSI embeddings are projected through a learned MLP to form prefix tokens prepended to text embeddings, allowing BioBART's decoder to condition on image semantics while the language backbone remains frozen.
- Core assumption: Foundation model WSI features contain sufficient diagnostic information to guide text generation without further visual adaptation.
- Evidence anchors: Abstract states visual prefix prompting keeps language backbone frozen; Section 2.3 describes MLP projection and reshaping to prefix tokens.

### Mechanism 2
- Claim: Combining patch-level (CONCH) and slide-level (Titan) foundation model features provides complementary visual representations.
- Mechanism: CONCH encodes 1024-pixel tiles at 20× magnification for local morphology; Titan aggregates these into 768-dimensional global slide representations capturing tissue organization patterns.
- Core assumption: Titan's slide-level representations encode diagnostic-relevant spatial relationships that patch-level features alone miss.
- Evidence anchors: Abstract mentions foundation-model WSI features; Section 2.2 describes Titan's 768-dimensional global slide representations.

### Mechanism 3
- Claim: Freezing the language model preserves linguistic competence while enabling data-efficient multimodal adaptation.
- Mechanism: Only the visual prompt encoder, projection layers, and auxiliary heads receive gradient updates; BioBART's pretrained weights remain fixed, preventing catastrophic forgetting and maintaining biomedical text fluency.
- Core assumption: BioBART's pretrained biomedical knowledge is sufficiently general to generate pathology reports with only visual conditioning.
- Evidence anchors: Abstract states keeping language backbone frozen for stability; Section 2.4 describes freezing BioBART while training only prompt encoder and auxiliary heads.

## Foundational Learning

- **Visual Prefix Tuning / Prompt-Based Conditioning**
  - Why needed here: Understanding how continuous visual tokens can steer frozen LLMs without modifying their weights is core to MPath's architecture.
  - Quick check question: Can you explain how prefix tokens differ from standard text prompts and why they enable gradient-based learning?

- **Foundation Models in Computational Pathology (CONCH, Titan)**
  - Why needed here: MPath relies entirely on pretrained WSI encoders; understanding what they capture determines what the system can generate.
  - Quick check question: What types of visual information does a patch-level encoder (CONCH) capture versus a slide-level encoder (Titan)?

- **Seq2Seq Language Models (BART architecture)**
  - Why needed here: BioBART is an encoder-decoder model; understanding its denoising pretraining objective explains why freezing preserves linguistic capabilities.
  - Quick check question: How does BART's bidirectional encoder differ from autoregressive-only models like GPT, and why might this suit report generation?

## Architecture Onboarding

- **Component map:** WSI → Trident tiling + GrandQC → CONCH embeddings → Titan slide encoder → 768-dim f_WSI → Visual Prompt Encoder (MLP + reshape) → Prefix tokens + Text prompt ("Pathology report:") → BioBART tokenizer → BioBART encoder-decoder (FROZEN) → Generated pathology report

- **Critical path:** WSI → CONCH+Titan feature extraction → projection to prefix tokens → concatenation with text prompt → BioBART decoding. Any failure in feature quality or projection corrupts all downstream generation.

- **Design tradeoffs:**
  - Freezing vs. fine-tuning LLM: Stability and data efficiency vs. inability to adapt to institution-specific report styles
  - Global slide features vs. patch-level attention: Computational efficiency vs. fine-grained grounding (hallucination risk noted in Discussion)
  - Simple prompt vs. structured prompting: Flexibility vs. potential for more controllable generation

- **Failure signatures:**
  - Hallucinated findings: E.g., "Chronic granulomatous inflammation with foreign body reaction" added when not in ground truth → indicates weak visual grounding
  - Grade/tumor volume errors: Gleason 6 → 7, tumor 10% → 5% → suggests quantitative precision limitations
  - Metastasis misattribution: "Metastatic adenocarcinoma, from colon primary" for primary lung case → overgeneralization from pretrained text corpus

- **First 3 experiments:**
  1. Ablation: CONCH vs. Titan vs. CONCH+Titan features — Isolate contribution of patch-level vs. slide-level representations using held-out validation set; expect degraded performance if either is removed.
  2. Prefix token count sweep (L_p ∈ {1, 5, 10, 20}) — Determine optimal visual context length; too few tokens may undercondition, too many may dilute signal.
  3. Prompt robustness test: Compare fixed prompt ("Pathology report:") vs. empty prompt (0.2 probability in training) on validation hallucination rate — assess whether prompt dropout regularizes overgeneration.

## Open Questions the Paper Calls Out
None

## Limitations
- Hallucination of findings not in ground truth due to limited visual grounding from global slide representations
- Unspecified prefix token count (L_p) and hidden projection dimension (h) making faithful reproduction difficult
- Evaluation dataset not released, preventing independent verification of 4th-place ranking and clinical claims

## Confidence
- High confidence: Visual prefix prompting mechanism is technically sound and well-established; architectural description is sufficiently detailed
- Medium confidence: Claims about Titan+CONCH complementary representations are plausible but lack ablation evidence; clinical utility claims rest on single competition ranking
- Low confidence: Claims about "data efficiency" and "scalability" are stated but not empirically demonstrated with comparisons to fine-tuned models

## Next Checks
1. **Ablation study on WSI encoders**: Train MPath with only CONCH features, only Titan features, and both. Measure hallucination rate and report quality on a held-out validation set.
2. **Prefix token count sweep**: Systematically vary L_p from 1 to 20 tokens while monitoring validation loss and hallucination rate. Plot the tradeoff curve to identify optimal context length.
3. **Hallucination audit**: Generate reports on 100 validation WSIs and conduct a blind expert review comparing generated vs. reference reports. Quantify the frequency of hallucinated findings.