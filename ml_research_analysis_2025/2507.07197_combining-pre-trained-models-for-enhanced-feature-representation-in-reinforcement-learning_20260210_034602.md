---
ver: rpa2
title: Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement
  Learning
arxiv_id: '2507.07197'
source_url: https://arxiv.org/abs/2507.07197
tags:
- learning
- agents
- pre-trained
- performance
- breakout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Weight Sharing Attention (WSA), a novel architecture
  that combines embeddings from multiple pre-trained models to create enriched state
  representations for reinforcement learning. Unlike traditional end-to-end approaches,
  WSA uses attention mechanisms and parameter sharing to dynamically balance contributions
  from diverse pre-trained models, enabling scalability and interpretability.
---

# Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.07197
- Source URL: https://arxiv.org/abs/2507.07197
- Reference count: 40
- One-line primary result: WSA matches or exceeds end-to-end models on Atari games, achieving scores like 2530 on Ms.Pacman and 345 on Breakout, while demonstrating superior robustness to environmental changes.

## Executive Summary
This paper introduces Weight Sharing Attention (WSA), a novel architecture that combines embeddings from multiple pre-trained models to create enriched state representations for reinforcement learning. Unlike traditional end-to-end approaches, WSA uses attention mechanisms and parameter sharing to dynamically balance contributions from diverse pre-trained models, enabling scalability and interpretability. Extensive experiments across Atari games demonstrate that WSA matches or exceeds the performance of end-to-end models, achieving scores like 2530 on Ms.Pacman and 345 on Breakout. WSA also exhibits superior robustness to environmental changes, such as altered game colors or dynamics, and maintains strong performance when the number of pre-trained models changes over time. The method reduces the burden on RL agents to learn environmental representations from scratch, accelerating learning and improving adaptability.

## Method Summary
WSA combines multiple pre-trained model embeddings using a shared weight network that computes dynamic attention weights based on the current state context. Each pre-trained model is paired with an adapter layer that projects its output into a unified embedding space. The state encoder provides context, which the shared MLP uses to determine each model's contribution. The final representation is a weighted sum of all embeddings, passed to the policy network. Pre-trained model weights are frozen during training to maintain robustness, while only adapters, the shared MLP, and policy network are trained. The approach is evaluated on Atari games using PPO with 10M steps, showing comparable or better performance than end-to-end baselines.

## Key Results
- WSA achieves competitive scores on Atari games: 2530 on Ms.Pacman, 345 on Breakout, and 17 on Pong
- Demonstrates superior robustness to environmental changes, performing 3x better than end-to-end models under altered game colors
- Maintains strong performance when the number of pre-trained models changes, showing adaptability and scalability
- Reduces the burden on RL agents to learn representations from scratch, accelerating learning

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Gating via Shared Weight Network
- **Claim:** A shared MLP learns to assign attention weights to each pre-trained model's embedding based on the current state context, allowing the agent to emphasize the most relevant prior knowledge for any given situation.
- **Mechanism:** The State Encoder produces a context vector $C$ from the current observation. A shared MLP $f_\theta$ takes $C$ and each model's adapted embedding $E_i$, outputting a weight $w_i$. Weights are L1-normalized to form a probability distribution. The final representation $R = \sum w_i E_i$.
- **Core assumption:** The State Encoder produces a context representation that captures task-relevant features, and embeddings from different pre-trained models provide complementary information that the shared MLP can learn to arbitrate.
- **Evidence anchors:**
  - [abstract] "The key innovation is using a shared MLP network to compute dynamic attention weights over each pre-trained model's embedding, allowing adaptive combination based on the current state context."
  - [section 3] "A Multi-Layer Perceptron (MLP) $f_\theta$ (Shared Weight Network) receives as input the context and the single embedding of each pre-trained model, and outputs the model-specific weight $w_i$ to determine its contribution in the final representation."
  - [corpus] Corpus evidence is weak for this specific mechanism; no directly related papers on shared MLP gating for multi-model RL embedding combination were found in the neighbors.
- **Break condition:** Fails if (1) the State Encoder produces uninformative context vectors; (2) pre-trained models exhibit significant distribution shift from RL environment observations; (3) the shared MLP lacks capacity to model relationships between context and optimal weights.

### Mechanism 2: Feature Space Alignment via Trainable Adapters
- **Claim:** Trainable adapter layers project each pre-trained model's heterogeneous embedding into a shared, unified latent space, making diverse representations compatible for combination.
- **Mechanism:** Each pre-trained model $\psi_i$ is paired with an adapter $A_i$ (linear layer + non-linearity). The adapter transforms raw model output into a fixed-size embedding $E_i$. All $E_i$s share the same dimension. Adapters are trained alongside the policy; pre-trained model weights remain frozen.
- **Core assumption:** A linear transformation followed by a non-linearity is sufficient to map structurally and semantically diverse embeddings (keypoints, segmentation masks, state vectors) into a mutually informative shared space.
- **Evidence anchors:**
  - [abstract] "WSA as a method to combine multiple pre-trained models' embeddings into a unified representation for reinforcement learning agents."
  - [section 3] "Each pre-trained model is equipped with an adapter A - a linear layer followed by a non-linearity - that maps its latent representation into an embedding E of predefined size... Adapters' parameters are optimized during training."
  - [corpus] Neighbor paper "Codec2Vec" supports the general idea of transforming specialized representations but doesn't validate this specific adapter architecture.
- **Break condition:** Fails if (1) target shared embedding size is too small, causing information bottleneck; (2) linear+non-linear adapter is too simplistic for complex cross-modal mappings; (3) adapter training becomes unstable due to conflicting gradients.

### Mechanism 3: Robustness Through Frozen Feature Diversity
- **Claim:** Frozen embeddings from multiple pre-trained models trained on different auxiliary tasks provide more robust state representations to environmental changes than end-to-end learned features.
- **Mechanism:** Pre-trained models (VOS, State Representation, Keypoints) trained on different self-supervised objectives capture different invariances. When the RL environment changes, diverse features are less likely to all fail simultaneously compared to a single jointly optimized CNN.
- **Core assumption:** Diversity of pre-training objectives translates into complementary invariances that collectively cover a broader range of potential environmental variations.
- **Evidence anchors:**
  - [abstract] "Results show WSA achieves comparable or better performance... with advantages in robustness to environmental changes"
  - [section 4.4] "WSA is more robust and resilient to environmental changes compared to E2E in both scenarios... [WSA] performs more than 3x better than E2E in all configurations."
  - [corpus] Neighbor paper "Rethinking Cross-Modal Fine-Tuning" tangentially supports leveraging pre-existing robust features but doesn't provide direct evidence for this RL robustness claim.
- **Break condition:** Fails if (1) all pre-trained models share a common vulnerability to a specific environmental shift; (2) environmental change is so drastic that all frozen models produce uninformative embeddings; (3) WSA cannot adapt weights quickly enough to new model relevance distributions.

## Foundational Learning

- **Concept: Representation Learning / Feature Extraction**
  - **Why needed here:** The entire WSA architecture is a meta-feature extractor that combines existing learned representations. Understanding what makes a "good" representation (invariance, disentanglement, completeness) is crucial for selecting and debugging pre-trained models.
  - **Quick check question:** Can you explain why a pre-trained ImageNet classifier might produce poor features for an Atari game, even though both are visual tasks?

- **Concept: Attention Mechanisms**
  - **Why needed here:** WSA uses a learned, context-dependent attention weighting scheme. Grasping the core idea of attention—dynamically weighting information relevance based on a query/context—is essential for understanding how WSA adapts.
  - **Quick check question:** How does the attention mechanism in WSA differ from a simple learned linear combination (weighted sum with learned static weights)?

- **Concept: Transfer Learning & Fine-Tuning**
  - **Why needed here:** WSA is a form of transfer learning where knowledge from pre-trained models is transferred to an RL agent. The paper specifically freezes pre-trained weights and only trains adapters, a key design decision with trade-offs.
  - **Quick check question:** What are the potential benefits and drawbacks of freezing the weights of the pre-trained models versus fine-tuning them end-to-end with the RL agent?

## Architecture Onboarding

- **Component map:**
  - Pre-trained Models (Ψ): Frozen feature extractors (e.g., VOS, State Rep, Keypoints). Input: Observation O. Output: Raw embeddings.
  - Adapters (Ai): Trainable linear layers + non-linearity. Input: Raw embedding from model i. Output: Fixed-size embedding Ei.
  - State Encoder (E): Often an autoencoder. Input: Observation O. Output: Context vector C.
  - Shared Weight Network (fθ): Trainable MLP. Input: Context C and embedding Ei (for each model i). Output: Scalar weight wi.
  - Policy Network (FCN): Trainable Fully-Connected Network. Input: Final weighted representation R = Σ(wi × Ei). Output: Action distribution.

- **Critical path:**
  1. Observation O is passed to all pre-trained models ψi and the State Encoder E in parallel.
  2. Each ψi output is transformed by its adapter Ai to produce a unified embedding Ei.
  3. State Encoder produces context vector C.
  4. For each pre-trained model, the Shared Weight Network fθ takes (C, Ei) and outputs a raw weight score.
  5. All raw weights are concatenated and L1-normalized into a probability distribution.
  6. The final representation R is the weighted sum of all embeddings Ei using the normalized weights.
  7. R is passed to the Policy Network to produce an action.

- **Design tradeoffs:**
  - **Number/Type of Pre-trained Models:** More models provide more information but increase inference cost and risk of noisy/redundant features. Model diversity (different objectives) is likely more important than quantity.
  - **Adapter Capacity:** Larger adapters can model more complex transformations but risk overfitting. A simple linear layer may fail for complex cross-modal mappings.
  - **Shared Weight Network Depth:** A deeper MLP can model more complex weighting strategies but is harder to train and slower. A single layer may fail in complex control tasks (e.g., robotic manipulation required 3 layers).
  - **Freezing vs. Fine-tuning:** Freezing pre-trained models is faster and more robust, but prevents adaptation to domain-specific nuances. Fine-tuning is more powerful but computationally expensive and risks catastrophic forgetting.

- **Failure signatures:**
  - **Low performance from the start:** Check if pre-trained models are appropriate for the domain (e.g., an ImageNet model for a synthetic robotic task). Check adapter output dimensions.
  - **Training instability:** Check learning rates for adapters and Shared Weight Network. Inspect gradient norms. Ensure L1 normalization is stable.
  - **Poor generalization to new variations:** Check if pre-trained models cover the necessary invariances. Examine WSA weights—are they collapsing to a single model?
  - **Performance collapse when models are removed (scalability):** Indicates over-reliance on a single model. The weighting mechanism is not distributing importance effectively.

- **First 3 experiments:**
  1. **Ablation over combination methods:** Implement and compare WSA against simpler baselines (Mean Ensemble, Linear Projection, single best pre-trained model) on a simple benchmark (e.g., Pong) to verify the value of learned attention weights.
  2. **Sensitivity to pre-trained model quality:** Train WSA with different subsets of pre-trained models (e.g., all three, best two, worst two, single model) to understand how performance degrades and whether WSA can automatically de-weight poor models.
  3. **Robustness stress test:** Train WSA and an end-to-end baseline on the default environment, then evaluate both on a set of held-out environmental variations (e.g., different colors, textures) to directly test the robustness claim. Analyze the learned attention weights during these variations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can WSA effectively integrate richer forms of prior knowledge such as Large Language Models (LLMs) or Vision-Language Models (VLMs)?
- **Basis in paper:** [explicit] The conclusion explicitly states: "Looking ahead, we plan to extend our analysis to more diverse and complex scenarios. We also aim to scale our approach by incorporating richer forms of prior knowledge, e.g., Large Language Models (LLMs) or Vision-Language Models (VLMs)."
- **Why unresolved:** Current experiments only use vision-based models trained on game-specific data; architectural adaptations for language modalities remain unexplored.
- **What evidence would resolve it:** Demonstration of WSA successfully combining LLM/VLM embeddings with vision encoders, showing performance improvements on multi-modal RL tasks.

### Open Question 2
- **Question:** How can the distribution shift problem in pre-trained models be systematically addressed when encountering novel states during inference?
- **Basis in paper:** [explicit] In Section 4.2, the authors identify that Breakout underperformed due to distributional shift: "This poses a distributional shift problem between training and test data. Our initial training data, collected from random agents, lacks late-game scenarios with few blocks remaining." They also note similar issues in Space Invaders (Section 4.3).
- **Why unresolved:** The current workaround (collecting mixed random/expert data) is task-specific; a general solution for continuously updating pre-trained models without catastrophic forgetting remains open.
- **What evidence would resolve it:** A method that automatically detects and adapts to distribution shifts in pre-trained model embeddings without requiring manual data collection or suffering from forgetting.

### Open Question 3
- **Question:** Why does WSA underperform end-to-end baselines in certain games (Beam Rider, Qbert, Space Invaders) despite hyperparameter optimization?
- **Basis in paper:** [inferred] Table 2 shows WSA performs worse than E2E in three out of nine games. The authors hypothesize distribution shift issues but do not fully explain why optimization did not close the gap.
- **Why unresolved:** The underlying causes (model incompatibility, representation conflicts, or task-specific properties) are not diagnosed.
- **What evidence would resolve it:** Ablation studies analyzing per-model attention weights, embedding quality metrics, and failure mode characterization for underperforming games.

## Limitations

- **Adapter architecture ambiguity:** The paper specifies adapters as "linear layer followed by non-linearity" but does not provide exact dimensions or activation functions, which could significantly impact reproducibility and performance.
- **Limited scalability testing:** While WSA shows robustness to changes in pre-trained model count, the paper does not extensively test scenarios with very large numbers of models (e.g., 10+), leaving scalability limits unclear.
- **Narrow domain evaluation:** The evaluation is confined to three Atari games, and performance on more complex environments (continuous control, 3D navigation) remains untested.

## Confidence

- **High confidence:** The core mechanism of using shared MLP for context-dependent attention weighting is well-defined and supported by experimental results. The approach of freezing pre-trained models while training only adapters is a sound design choice.
- **Medium confidence:** The robustness claims are supported by specific ablation studies, but the environmental variations tested (color changes) are relatively simple. More challenging domain shifts would provide stronger validation.
- **Low confidence:** The paper does not adequately address potential negative transfer when combining pre-trained models with conflicting representations or when models are poorly suited to the task domain.

## Next Checks

1. **Ablation on adapter capacity:** Systematically vary adapter dimensions (e.g., 64, 128, 256) and activation functions to identify the minimal sufficient architecture that maintains performance, establishing a clearer understanding of the information bottleneck trade-off.

2. **Stress test environmental robustness:** Extend robustness evaluation beyond color changes to include more severe perturbations (e.g., random noise, occlusion, resolution changes, dynamic object removal) to test the limits of WSA's invariance properties.

3. **Scalability benchmark:** Evaluate WSA with increasing numbers of pre-trained models (2, 4, 8, 16) on a single game to empirically determine the point of diminishing returns and identify computational bottlenecks in the attention mechanism.