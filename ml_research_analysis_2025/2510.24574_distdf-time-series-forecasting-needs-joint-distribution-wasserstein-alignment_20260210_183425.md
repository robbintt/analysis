---
ver: rpa2
title: 'DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment'
arxiv_id: '2510.24574'
source_url: https://arxiv.org/abs/2510.24574
tags:
- forecast
- distdf
- discrepancy
- wang
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of biased likelihood estimation
  in time-series forecasting due to label autocorrelation. It proposes DistDF, a method
  that aligns conditional forecast and label distributions using a joint-distribution
  Wasserstein discrepancy, which provably upper bounds the conditional discrepancy
  of interest.
---

# DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment

## Quick Facts
- arXiv ID: 2510.24574
- Source URL: https://arxiv.org/abs/2510.24574
- Reference count: 40
- Primary result: Model-agnostic improvement in time-series forecasting via joint-distribution Wasserstein alignment

## Executive Summary
DistDF addresses the fundamental limitation of standard mean squared error (MSE) in time-series forecasting: it provides biased likelihood estimates when labels exhibit autocorrelation. The method introduces a novel approach that aligns conditional forecast and label distributions using a joint-distribution Wasserstein discrepancy, which provably upper bounds the conditional discrepancy of interest. This approach bypasses the limitations of both MSE and Fourier-based decorrelation methods, enabling unbiased alignment with tractable estimation from finite observations.

## Method Summary
DistDF is a model-agnostic enhancement for multi-step time-series forecasting that replaces or supplements standard MSE loss with a joint-distribution Wasserstein discrepancy. The method operates by concatenating historical input X with true labels Y to form joint distribution Z=[X,Y], and similarly concatenating X with predicted labels Ŷ to form Ẑ=[X,Ŷ]. It then computes the Bures-Wasserstein discrepancy between these joint distributions using empirical batch statistics. The final loss is a weighted combination of this distributional discrepancy and MSE: L_α = α · BW(μ_Z, μ_Ẑ, Σ_Z, Σ_Ẑ) + (1-α) · ∥Y - Ŷ∥²₂. The method is trained using Adam optimizer with batch size 128 and requires hyperparameter tuning for α, typically finding optimal values between 0.001-0.1.

## Key Results
- Consistent improvement across 8 diverse datasets (ETT, Electricity, Traffic, Weather, PEMS) with 6+ different model architectures
- State-of-the-art performance achieved on multiple benchmark datasets
- Robust performance across different forecast horizons (96, 192, 336, 720 timesteps)
- Model-agnostic gains demonstrated on both Transformer-based and non-Transformer models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSE produces biased likelihood estimates when label sequences exhibit autocorrelation.
- Mechanism: MSE computes point-wise squared errors independently across time steps, ignoring the conditional covariance structure Σ|X. When labels are autocorrelated (Σ|X ≠ I), the MSE divergence from the true conditional negative log-likelihood is given by Bias = ∥Y|X - Ŷ|X∥²_{Σ⁻¹|X} - ∥Y|X - Ŷ|X∥²₂, which vanishes only if Σ|X is diagonal.
- Core assumption: The label sequence given history X follows a multivariate Gaussian distribution with non-diagonal conditional covariance.
- Evidence anchors:
  - [abstract]: "standard direct forecast (DF) approach resorts to minimize the conditional negative log-likelihood... typically estimated using the mean squared error. However, this estimation proves to be biased in the presence of label autocorrelation."
  - [section 3.1]: Theorem 3.1 formalizes the autocorrelation bias formula.
  - [corpus]: Time-o1 (arXiv:2505.17847) identifies similar label autocorrelation bias but uses PCA-based label transformation rather than distributional discrepancy.
- Break condition: If label sequence is conditionally decorrelated (Σ|X ≈ I), bias vanishes and MSE becomes adequate.

### Mechanism 2
- Claim: Joint-distribution Wasserstein discrepancy provides a tractable upper bound on the intractable conditional-distribution discrepancy.
- Mechanism: Direct estimation of W_p(P_{Y|X}, P_{Ŷ|X}) fails because each X yields only one Y sample—insufficient for distributional estimation. By Lemma 3.3, the joint-distribution Wasserstein W_p(P_{X,Y}, P_{X,Ŷ}) upper-bounds the expected conditional discrepancy and can be estimated from the full dataset's empirical samples.
- Core assumption: Lemma 3.3 holds for p ≥ 1; equality requires p=1 or conditional term being constant in X.
- Evidence anchors:
  - [section 3.2]: Lemma 3.3 states ∫ W_p(P_{Y|X}, P_{Ŷ|X}) dP(X) ≤ W_p(P_{X,Y}, P_{X,Ŷ}).
  - [section 3.2]: "This proxy is advantageous for two reasons. First, it provides a provable upper bound... Second, it is readily estimable from finite time-series observations."
  - [corpus]: No direct corpus validation of this specific joint-to-conditional bounding strategy; this appears novel to this work.
- Break condition: If batch size is extremely small (e.g., B=1), joint-distribution estimation itself becomes unreliable.

### Mechanism 3
- Claim: Minimizing joint-distribution Wasserstein to zero guarantees conditional distribution alignment.
- Mechanism: Theorem 3.4 proves P_{Y|X} = P_{Ŷ|X} if W_p(P_{X,Y}, P_{X,Ŷ}) = 0. Under Gaussian assumptions, the squared 2-Wasserstein admits the closed-form Bures-Wasserstein formula (Lemma 3.5): BW = ∥μ_{X,Y} - μ_{X,Ŷ}∥²₂ + Tr(Σ_{X,Y} + Σ_{X,Ŷ} - 2√(Σ^{1/2}_{X,Y} Σ_{X,Ŷ} Σ^{1/2}_{X,Y})).
- Core assumption: Joint distributions P_{X,Y} and P_{X,Ŷ} obey Gaussian distributions for the closed-form BW computation.
- Evidence anchors:
  - [section 3.2]: Theorem 3.4 establishes alignment guarantee.
  - [section 3.3]: Algorithm 1 implements BW computation using empirical mean and covariance from batch.
  - [corpus]: Corpus papers use alternative distributional approaches (generative forecasting, contrastive learning) but do not validate the Bures-Wasserstein specific approach.
- Break condition: If joint distributions are highly non-Gaussian, the Bures-Wasserstein approximation may poorly estimate true Wasserstein distance.

## Foundational Learning

- Concept: **Conditional vs. Marginal Decorrelation**
  - Why needed here: The paper emphasizes that Fourier/PCA transformations achieve only *marginal* decorrelation (diagonal Σ) but not *conditional* decorrelation (diagonal Σ|X), leaving residual autocorrelation bias.
  - Quick check question: Given a transformation T, can you distinguish whether T(Y) is marginally decorrelated vs. conditionally decorrelated given X?

- Concept: **Optimal Transport / Wasserstein Distance**
  - Why needed here: The core contribution uses Wasserstein discrepancy as the distributional alignment metric; understanding the transport plan Π and cost matrix D is essential.
  - Quick check question: Why does Wasserstein distance remain meaningful for distributions with disjoint supports, unlike KL divergence?

- Concept: **Bures-Wasserstein Metric**
  - Why needed here: Provides differentiable, closed-form computation for 2-Wasserstein under Gaussian assumption—critical for gradient-based training.
  - Quick check question: What are the two terms in the Bures-Wasserstein formula, and what does each capture?

## Architecture Onboarding

- Component map: Input X → Base Model g → Ŷ → Concatenate [X, Y] → Z, [X, Ŷ] → Ẑ → Compute μ_Z, μ_Ẑ, Σ_Z, Σ_Ẑ → Bures-Wasserstein BW → Loss L_α

- Critical path:
  1. Forward pass through base model → Ŷ
  2. Concatenate X with Y and X with Ŷ → Z, Ẑ
  3. Compute batch-level mean and covariance for Z, Ẑ
  4. Compute Bures-Wasserstein term (trace operations on covariance matrices)
  5. Combine with MSE via hyperparameter α

- Design tradeoffs:
  - **α selection**: α=0 reverts to standard DF; α=1 removes point-wise supervision. Paper finds optimal α typically < 1 (Tables 5-6), suggesting MSE provides complementary pairing awareness.
  - **Batch size**: Larger batches improve covariance estimation quality but increase memory.
  - **Gaussian assumption**: Analytical convenience vs. distributional mismatch risk.

- Failure signatures:
  - **α too high (→1.0)**: Performance degrades (Table 5: α=1 gives MSE 0.381 vs. optimal 0.375 on ETTh2)—loss of point-wise alignment.
  - **Very small batches**: Covariance estimates become noisy/unstable.
  - **Non-Gaussian data with extreme α**: BW approximation may misrepresent true Wasserstein distance.

- First 3 experiments:
  1. **Sanity check**: Implement DistDF with α=0 (should match baseline MSE performance exactly) and α=0.01 on a single dataset to verify computation correctness and marginal improvement.
  2. **α sweep**: Run α ∈ {0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2} on validation set to find dataset-specific optimal α before test evaluation.
  3. **Cross-model validation**: Apply DistDF to at least two architecturally different base models (e.g., one Transformer-based, one linear) to confirm model-agnostic improvement claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DistDF framework be successfully extended to domains with correlated labels outside of time-series forecasting, such as speech synthesis and target recognition?
- Basis in paper: [explicit] The authors explicitly identify extending DistDF to speech synthesis and target recognition as an "interesting direction for future research" in the "Limitations & future works" section.
- Why unresolved: The current work focuses exclusively on time-series forecasting benchmarks, leaving the application to other sequential or structured data domains unverified.
- What evidence would resolve it: Demonstrating performance gains when applying DistDF to speech synthesis or object recognition tasks where label autocorrelation exists.

### Open Question 2
- Question: Does evaluating distributional discrepancy in transformed domains (e.g., frequency domain) yield better alignment than the proposed time-domain approach?
- Basis in paper: [explicit] The "Limitations & future works" section states that recent work suggests benefits to evaluating discrepancy in transformed domains, proposing it as a "promising direction."
- Why unresolved: The current method computes the joint-distribution Wasserstein discrepancy solely in the time domain.
- What evidence would resolve it: A comparative study evaluating a hybrid loss function that incorporates discrepancy measures from both time and frequency domains.

### Open Question 3
- Question: To what extent does the Gaussian assumption underlying the Bures-Wasserstein discrepancy limit the method's effectiveness on non-Gaussian data distributions?
- Basis in paper: [inferred] Lemma 3.5 derives the tractable discrepancy formula under the assumption that joint distributions are Gaussian; real-world data may violate this.
- Why unresolved: The paper relies on this assumption for tractability but does not empirically analyze the degradation of performance when this assumption is false.
- What evidence would resolve it: Performance benchmarks on synthetic datasets with controlled non-Gaussian noise profiles to observe the loss function's robustness.

## Limitations
- Method assumes Gaussian-distributed label sequences; performance on highly non-Gaussian data remains unclear
- Requires sufficient batch size for reliable covariance estimation; behavior in low-data regimes untested
- Sensitive hyperparameter α requires dataset-specific tuning, potentially limiting practical deployment

## Confidence
- **High Confidence**: Core theoretical contributions and experimental validation across 8 datasets with 6+ model architectures
- **Medium Confidence**: Model-agnostic claims supported by diverse architectures but all tested models are Transformer-based or similar capacity
- **Medium Confidence**: Computational efficiency claims plausible but precise runtime comparisons not provided

## Next Checks
1. **Non-Gaussian Stress Test**: Evaluate DistDF on datasets known to have heavy-tailed or non-Gaussian distributions (e.g., financial time-series with extreme events) to assess robustness of the Bures-Wasserstein approximation when Gaussian assumptions break down.

2. **Small Batch Performance**: Systematically test performance degradation as batch size decreases (e.g., 128 → 64 → 32 → 16) to identify the minimum viable batch size and understand failure modes in low-data regimes.

3. **Alternative Base Models**: Implement DistDF on fundamentally different forecasting architectures such as linear models, kernel methods, or classical statistical approaches to verify true model-agnostic performance rather than Transformer-specific improvements.