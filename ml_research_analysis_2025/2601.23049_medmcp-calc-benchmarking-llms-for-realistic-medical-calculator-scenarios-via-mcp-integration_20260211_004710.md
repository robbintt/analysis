---
ver: rpa2
title: 'MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios
  via MCP Integration'
arxiv_id: '2601.23049'
source_url: https://arxiv.org/abs/2601.23049
tags:
- text
- clinical
- name
- patient
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedMCP-Calc, the first benchmark for evaluating
  LLMs on realistic medical calculator workflows through Model Context Protocol (MCP)
  integration. Unlike existing benchmarks that focus on static single-step calculations,
  MedMCP-Calc simulates real clinical scenarios requiring fuzzy task descriptions,
  dynamic EHR database interaction, and multi-step decision making across 118 tasks
  spanning four clinical domains.
---

# MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration

## Quick Facts
- **arXiv ID:** 2601.23049
- **Source URL:** https://arxiv.org/abs/2601.23049
- **Reference count:** 40
- **Key result:** First benchmark for evaluating LLMs on realistic medical calculator workflows using MCP integration, exposing significant performance gaps in clinical tool usage

## Executive Summary
MedMCP-Calc introduces the first benchmark for evaluating large language models (LLMs) on realistic medical calculator workflows through Model Context Protocol (MCP) integration. Unlike existing benchmarks that focus on static single-step calculations, this benchmark simulates real clinical scenarios requiring fuzzy task descriptions, dynamic EHR database interaction, and multi-step decision making across 118 tasks spanning four clinical domains. The evaluation of 23 leading models reveals substantial performance gaps, with even top performers like Claude Opus 4.5 achieving only 33.97% calculation accuracy. The benchmark incorporates MCP servers for PostgreSQL database access, Google Search, and Python computation to mirror authentic physician workflows.

The study also develops CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation strategies that achieves state-of-the-art performance among open-source models. The benchmark exposes critical limitations in current LLMs, including struggles with calculator selection from ambiguous queries, iterative SQL-based database interactions, and reluctance to use external tools for numerical computation. Performance varies considerably across clinical domains, with complex scoring systems proving particularly challenging.

## Method Summary
The MedMCP-Calc benchmark evaluates LLMs through a comprehensive framework that simulates realistic clinical calculator workflows. The benchmark consists of 118 tasks across four clinical domains: acute cardiovascular care, preoperative assessment, chronic disease management, and trauma care. Each task requires multiple steps including calculator selection from ambiguous queries, dynamic database interaction through PostgreSQL MCP servers, and multi-step decision making. The evaluation framework integrates three MCP servers: PostgreSQL for EHR database access, Google Search for information retrieval, and Python for computation. Tasks are designed to mirror authentic physician workflows, requiring iterative SQL queries and tool orchestration. The benchmark measures accuracy across three key stages: calculator selection, database interaction, and final calculation results.

## Key Results
- Claude Opus 4.5 achieves only 33.97% calculation accuracy, revealing substantial performance gaps in realistic clinical scenarios
- Models average only 30% accuracy in selecting appropriate calculators from ambiguous clinical queries
- CalcMate, a fine-tuned model with scenario planning and tool augmentation, achieves state-of-the-art performance among open-source models

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic simulation of clinical workflows that require dynamic tool usage and decision-making. By integrating MCP servers for database access, search, and computation, the benchmark forces models to engage in authentic physician-like behavior rather than relying on static knowledge retrieval. The multi-step nature of tasks exposes models' limitations in tool orchestration and iterative problem-solving, while the ambiguous task descriptions test their ability to disambiguate clinical intent and select appropriate calculation pathways.

## Foundational Learning
- **Model Context Protocol (MCP):** Standard interface for tool integration that enables dynamic database queries and computation - needed for realistic clinical workflow simulation, quick check: can models successfully query and parse database responses
- **Clinical calculator workflows:** Multi-step decision processes from patient data retrieval to final scoring - needed to mirror authentic physician behavior, quick check: can models chain database queries with calculation steps
- **Fuzzy task description handling:** Ability to interpret ambiguous clinical queries and select appropriate tools - needed for realistic clinical scenarios, quick check: can models disambiguate intent from vague clinical descriptions
- **Iterative SQL interaction:** Dynamic database querying with context preservation - needed for realistic EHR navigation, quick check: can models maintain query context across multiple database interactions
- **Tool reluctance mitigation:** Training strategies to encourage appropriate tool usage for numerical computation - needed to overcome model hesitation, quick check: do models appropriately invoke computational tools for calculations
- **Scenario planning strategies:** Multi-step reasoning frameworks for clinical decision paths - needed for complex scoring systems, quick check: can models plan multi-step clinical workflows

## Architecture Onboarding

**Component Map:** Clinical Task Description -> Calculator Selection -> Database Query Planning -> MCP PostgreSQL Server -> Result Parsing -> Calculation Decision -> MCP Python Server -> Final Score Output

**Critical Path:** The critical execution path involves interpreting the clinical task, selecting the appropriate calculator, planning and executing iterative database queries through the PostgreSQL MCP server, parsing the results, deciding on calculation methodology, and executing the final computation through the Python MCP server.

**Design Tradeoffs:** The benchmark prioritizes realism over simplicity by requiring multi-step workflows with ambiguous inputs, which creates higher variance in model performance but better reflects actual clinical practice. The MCP integration adds technical complexity but enables authentic tool interaction patterns. The comprehensive task set across four domains provides broad coverage but requires significant evaluation infrastructure.

**Failure Signatures:** Models typically fail by either refusing to use computational tools for numerical work (tool reluctance), making incorrect calculator selections from ambiguous queries, failing to maintain context across iterative database queries, or producing incorrect SQL queries that cannot retrieve necessary patient data. Performance drops significantly on complex scoring systems requiring multiple data points.

**3 First Experiments:**
1. Evaluate a baseline model on a simple calculator selection task with clear input to establish minimal performance
2. Test tool invocation patterns by presenting a calculation task requiring numerical computation
3. Assess database interaction capabilities with a task requiring single-step patient data retrieval

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on simulated clinical scenarios that may not fully capture actual clinical practice complexity
- Performance evaluation focuses primarily on accuracy without comprehensive safety-critical factors assessment
- MCP server integration introduces potential technical variability affecting reproducibility across environments

## Confidence
- **High confidence:** Benchmark design effectiveness in exposing LLM limitations in medical calculator workflows
- **Medium confidence:** Comparative performance rankings across models in controlled evaluation environment
- **Medium confidence:** CalcMate's effectiveness, though external validation would strengthen claims

## Next Checks
1. Conduct real-world clinical workflow validation with practicing physicians to assess benchmark performance correlation with actual clinical utility and safety
2. Perform cross-institutional deployment testing of MCP servers and benchmark framework to evaluate robustness across different EHR systems
3. Implement comprehensive bias and safety analysis using the benchmark to examine performance gaps correlation with demographic variables or clinical scenario complexity