---
ver: rpa2
title: Dynamic Sparse Attention on Mobile SoCs
arxiv_id: '2508.16703'
source_url: https://arxiv.org/abs/2508.16703
tags:
- attention
- arxiv
- shadowattn
- mobile
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently running attention
  operations in on-device large language models (LLMs), which typically fall back
  to general-purpose CPU/GPU due to quantization sensitivity in specialized neural
  processing units (NPUs). To address this, the authors propose shadowAttn, a system-algorithm
  co-designed sparse attention module that minimizes reliance on CPU/GPU resources
  by offloading the estimation of important tokens to the NPU.
---

# Dynamic Sparse Attention on Mobile SoCs

## Quick Facts
- arXiv ID: 2508.16703
- Source URL: https://arxiv.org/abs/2508.16703
- Reference count: 40
- Primary result: Up to 6.9× speedup in attention kernel latency and up to 4.5× end-to-end speedup on mobile SoCs with minimal accuracy loss.

## Executive Summary
This paper tackles the problem of efficiently running attention operations in on-device large language models (LLMs), which typically fall back to general-purpose CPU/GPU due to quantization sensitivity in specialized neural processing units (NPUs). To address this, the authors propose shadowAttn, a system-algorithm co-designed sparse attention module that minimizes reliance on CPU/GPU resources by offloading the estimation of important tokens to the NPU. Key techniques include NPU compute graph bucketing to handle input fluctuations and head-wise NPU-CPU/GPU pipeline optimization. Evaluations on commercial smartphones with models like Qwen2 and PhoneLM show that shadowAttn achieves up to 6.9× speedup in attention kernel latency and up to 4.5× end-to-end speedup compared to state-of-the-art frameworks, with only 0.4 percentage points accuracy loss on average. It also reduces energy consumption by up to 7.7× while maintaining on-par or better performance with significantly fewer CPU/GPU resources.

## Method Summary
shadowAttn is a system-algorithm co-design that enables NPU-centric execution of attention in mobile LLMs. The method offloads the estimation of important tokens (top-k indices) to the NPU using INT8 quantization, leveraging the observation that token importance ranking is more quantization-resilient than full attention computation. To handle the NPU's inability to process dynamic input scales and shapes, the system pre-compiles a discrete set of static compute graphs ("buckets") covering a range of quantization scale factors. At runtime, inputs are routed to the graph instance that best matches the current scale. A head-wise heterogeneous pipeline overlaps NPU estimation with CPU/GPU sparse computation, minimizing latency. Offline profiling on calibration data determines per-head sparsity ratios and scale factor distributions, enabling fine-grained optimization.

## Key Results
- Up to 6.9× speedup in attention kernel latency compared to CPU/GPU execution
- Up to 4.5× end-to-end speedup on mobile devices for models like Qwen2 and PhoneLM
- Only 0.4 percentage points average accuracy loss versus dense attention
- Up to 7.7× reduction in energy consumption

## Why This Works (Mechanism)

### Mechanism 1: NPU-Offloaded Importance Estimation
The token importance estimation stage is significantly more resilient to low-precision (INT8) quantization error than the final attention calculation, allowing it to be offloaded to the integer-focused NPU. The system performs a dense Q·K matrix multiplication on the NPU using INT8 quantization to identify the top-k indices. It relies on the observation that identifying the *relative* order of attention scores (which tokens matter) does not require the *absolute* precision needed for the final softmax output. The core assumption is that the relative ordering of attention scores is preserved sufficiently under per-tensor quantization to achieve >99% recall of important tokens. Evidence includes a reported average recall rate of more than 99% for NPU-based estimation, in stark contrast to significant accuracy drops when using NPU to compute the full QKV. Break conditions include input distributions that shift such that INT8 precision clips or distorts the relative magnitude of scores, specifically for small, compact mobile LLMs.

### Mechanism 2: Compute Graph Bucketing
Pre-compiling a discrete set of static compute graphs ("buckets") mitigates the inability of NPUs to handle dynamic input scales and shapes dynamically. Instead of a single static graph, the system generates multiple graphs covering a range of quantization scale factors (λ) and tensor shapes. At runtime, inputs are routed to the graph instance that minimizes the Mean Squared Error (MSE) for the current input's scale. The core assumption is that a small, finite set of scale factor buckets (e.g., 9 buckets) covers the variance of activation scales seen during inference without excessive memory overhead. Evidence includes the system's use of bucketing to handle scale factor fluctuations and its offline generation of multiple graphs for online selection. Break conditions include online input scale factors that drift significantly from the offline profiled calibration data, where the "closest" bucket may introduce enough quantization error to break the estimation recall.

### Mechanism 3: Head-wise Heterogeneous Pipelining
Overlapping NPU estimation with CPU/GPU sparse computation minimizes the latency penalty of the two-stage sparse attention process. The system implements a fine-grained pipeline where NPU kernel execution for subsequent heads is fused and launched in parallel with the CPU/GPU top-k and sparse QKV computation of previous heads. It reorders execution to minimize pipeline bubbles. The core assumption is that the overhead of the runtime scheduling logic (greedy search) is negligible compared to the compute latency of the attention heads. Evidence includes the system's ability to overlap NPU estimation, CPU/GPU top-k operation, and CPU/GPU sparse attention, incurring negligible online overhead (e.g., < 1 ms). Break conditions include extreme compute variance between heads, where the static greedy schedule may fail to hide latency, resulting in pipeline stalls and performance no better than sequential execution.

## Foundational Learning

- **Concept: NPU Static Compute Graphs**
  - **Why needed here:** Unlike GPUs which support dynamic shapes and just-in-time compilation, mobile NPUs typically require fixed shapes and constants (like quantization scales) to be compiled offline. Understanding this is critical to grasping why "bucketing" is necessary.
  - **Quick check question:** Can a standard mobile NPU graph change its input tensor dimensions dynamically at runtime without recompilation?

- **Concept: Sensitivity of Attention to Quantization**
  - **Why needed here:** The paper hinges on the distinction that *finding* indices is hard quantization-resilient, while *computing* values is not. You must understand why activation outliers in Q, K, V tensors cause accuracy drops in full attention.
  - **Quick check question:** Why does per-tensor quantization (applying one scale to a whole tensor) fail specifically for attention activations compared to weights?

- **Concept: Sparse Attention Bottlenecks**
  - **Why needed here:** Standard sparse attention often fails to speed up inference because the "Estimation" phase (calculating scores to find what to prune) takes longer than the time saved by pruning.
  - **Quick check question:** In a sparse attention module, if sparsity is high (e.g., 80%), which stage typically dominates the latency: the sparse computation or the dense estimation?

## Architecture Onboarding

- **Component map:** Input -> NPU Graph Selection -> NPU Q·K Estimation -> Transfer Indices -> CPU/GPU Top-k -> CPU/GPU Sparse QKV
- **Critical path:** The execution flow moves from Input -> NPU Graph Selection -> NPU Q·K Estimation -> Transfer Indices -> CPU/GPU Top-k -> CPU/GPU Sparse QKV.
- **Design tradeoffs:**
  - **Bucket Granularity vs. Memory:** Increasing the number of scale factor buckets improves accuracy matching but increases storage and initialization time.
  - **Sparsity Ratio vs. Accuracy:** The default 20% sparsity is a "knee point"; tightening it saves compute but risks accuracy loss on compact models.
  - **CPU Core Allocation:** The system is designed to work on a *single* middle CPU core to avoid resource contention, trading raw peak performance for system stability.
- **Failure signatures:**
  - **Accuracy Collapse (18pp+):** Likely caused by forcing the NPU to execute the *full* attention (NPU-Full) rather than just the estimation, or mismatching scale buckets.
  - **No Speedup:** Occurs if the pipeline bubbles are not eliminated (scheduling failure) or if the estimation overhead on NPU is not effectively hidden by the CPU/GPU work.
- **First 3 experiments:**
  1. **Estimation Recall Validation:** Isolate the NPU estimation module. Feed float inputs, quantize to INT8, compute Q·K, and check if the top-k indices match the float32 ground truth (target >99% recall).
  2. **Bucket Sensitivity Sweep:** Run inference with varying numbers of scale factor buckets (e.g., 1, 5, 9) to visualize the accuracy vs. memory tradeoff curve on the target device.
  3. **Pipeline Efficiency Trace:** Instrument the runtime to measure the overlap between NPU estimation and CPU/GPU sparse attention. Verify that the tail latency is reduced compared to sequential execution.

## Open Questions the Paper Calls Out
- The paper explicitly states that the current design "directly employ[s] full attention on CPU/GPU for decoding" because that stage is primarily memory-bound, leaving the decoding phase unoptimized.
- The evaluation is restricted to small mobile models (0.5B–1.5B parameters), and the authors do not investigate how performance and accuracy scale to larger on-device models (e.g., 7B+ parameters).

## Limitations
- The central claim of robust NPU offloading rests on under-validated assumptions about the bucketing mechanism's ability to capture activation scale variance across diverse, real-world mobile workloads.
- Energy savings are benchmarked on synthetic workloads; the system's behavior under heavy multitasking or thermal throttling scenarios is unexplored.
- The 7.7× energy reduction figure is the most uncertain, as it is derived from end-to-end inference time multiplied by a power model, without direct measurement of NPU vs. CPU/GPU power consumption under varying workloads.

## Confidence
- **High Confidence:** The core mechanism of head-wise heterogeneous pipelining is well-supported by the ablation study (Fig. 8) and aligns with established principles of overlapping CPU/GPU and NPU execution.
- **Medium Confidence:** The claim of 0.4 percentage point average accuracy loss is credible given the reported >99% recall rate, but the narrow evaluation scope (limited to specific models and datasets) and the lack of ablation on the bucketing strategy introduce uncertainty.
- **Low Confidence:** The 7.7× energy reduction figure is the most uncertain, as it is derived from end-to-end inference time multiplied by a power model, without direct measurement of NPU vs. CPU/GPU power consumption under varying workloads.

## Next Checks
1. **Stress-Test the Bucketing System:** Run the shadowAttn pipeline on a diverse set of mobile user inputs (e.g., web queries, social media text, code) and log the frequency of scale factor mismatches between the online input and the closest offline bucket. Quantify the correlation between these mismatches and drops in Top-K recall rate or end-to-end latency.
2. **Characterize Accuracy Under Distribution Shift:** Systematically corrupt or perturb the calibration WikiText-2 data (e.g., inject out-of-vocabulary words, simulate typos) and measure the degradation in shadowAttn's recall rate and final output accuracy. Compare this robustness to a baseline CPU/GPU sparse attention system.
3. **Profile Energy Consumption Directly:** Use hardware performance counters or a power meter to measure the actual energy consumed by the NPU, CPU, and GPU during attention estimation and computation phases. Validate that the reported 7.7× savings holds under sustained, multi-head execution and not just in isolated benchmarks.