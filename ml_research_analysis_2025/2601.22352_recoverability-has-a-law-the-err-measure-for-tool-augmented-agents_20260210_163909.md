---
ver: rpa2
title: 'Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents'
arxiv_id: '2601.22352'
source_url: https://arxiv.org/abs/2601.22352
tags:
- recovery
- uni00000013
- uni00000011
- regret
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes the ERR\u2013ES law, showing that recovery\
  \ regret in tool-augmented language models is governed by a first-order relationship\
  \ between expected recovery regret (ERR) and an observable efficiency score (ES).\
  \ The law predicts that as efficiency increases, recovery regret decreases predictably,\
  \ independent of model architecture or scale."
---

# Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents

## Quick Facts
- arXiv ID: 2601.22352
- Source URL: https://arxiv.org/abs/2601.22352
- Reference count: 20
- Primary result: Recovery regret in tool-augmented LLMs is governed by a first-order relationship between Expected Recovery Regret (ERR) and Efficiency Score (ES)

## Executive Summary
This work establishes the ERR–ES law, showing that recovery regret in tool-augmented language models is governed by a first-order relationship between expected recovery regret (ERR) and an observable efficiency score (ES). The law predicts that as efficiency increases, recovery regret decreases predictably, independent of model architecture or scale. Across five benchmarks and four model families (8B–32B), predicted regret under ERR–ES closely matched observed outcomes (∆norm ≤ 0.05). Recovery trajectories aligned onto a shared efficiency–regret manifold, with scaling reducing variance and improving adherence.

## Method Summary
The study evaluates recovery regret using a supervised recovery prior (FORTIFY) trained on synthetic failure trajectories, combined with retrieval conditioning on semantically similar exemplars. The framework computes Expected Recovery Regret (ERR) and Efficiency Score (ES) across five benchmarks using four model families (8B–32B), with 200 Monte Carlo rollouts per configuration. The ERR–ES law is validated by comparing predicted ERR = (1-ES)/(1-γ) against observed outcomes, measuring deviation (∆norm) to assess predictive accuracy.

## Key Results
- ERR scales predictably with ES across diverse model families and benchmarks
- Predicted ERR closely matches observed outcomes (∆norm ≤ 0.05) across five benchmarks and four model families
- Scaling reduces variance and improves adherence to the ERR–ES manifold
- Retrieval conditioning reduces variance component of ERR without changing model weights

## Why This Works (Mechanism)

### Mechanism 1: First-Order Excess-Loss Linearization
The ERR–ES law is derived by linearizing excess loss around small perturbations. Per-step loss gap between a policy π and optimal π* is approximated as L_t - L*_t ≈ α·c_t·(1-s_t), where c_t is execution cost and s_t is success probability. Summing over discounted horizon yields ERR ∝ (1-ES)/(1-γ). This linearization is valid only for bounded per-step cost and small perturbations.

### Mechanism 2: Efficiency Score as the Unique Compatible Surrogate
Theorem 3.1 proves that ES = RR/(1 + λC) is the only scalar metric whose complement is affine-equivalent to ERR under first-order linearization assumptions. Any monotone surrogate S(RR, C) with (1-S) affine in ERR must take this rational form. Additive or separable surrogates cannot recover ERR ordering.

### Mechanism 3: Variance Suppression via Retrieval Conditioning
Retrieval-conditioned recovery reduces trajectory-level variance by aligning local decisions with observed failure structures. Conditioning on K semantically similar failure–recovery exemplars reduces the variance component of ERR without changing model weights, tightening adherence to the ERR–ES manifold.

## Foundational Learning

- **Discounted Cumulative Regret in MDPs**: ERR is defined as expected cumulative excess loss over a discounted horizon; understanding γ and regret bounds is essential. Quick check: If γ = 0.9 and per-step excess loss is constant at 0.5, what is the cumulative regret over infinite horizon? (Answer: 0.5/(1-0.9) = 5.0)

- **First-Order Linearization / Taylor Approximation**: The ERR–ES law is derived by linearizing excess loss around small perturbations. Quick check: What happens to a first-order approximation when variance of the underlying variable becomes large? (Answer: Higher-order terms dominate; approximation degrades or fails.)

- **Monte Carlo Rollout Estimation**: ERR is estimated empirically via 200 Monte Carlo rollouts per configuration. Quick check: Why use 200 rollouts rather than 20? (Answer: Reduced estimator variance; tighter confidence intervals.)

## Architecture Onboarding

- **Component map**: Failure Detector -> Tool Execution -> Failure Detection -> Retrieval Module -> Recovery Prior -> Tool Re-execution -> Loop until success or budget exhaustion

- **Critical path**: Tool call executes → perturbation δ_t sampled from F(s_t, a_t) → Failure detected → retrieval fetches K exemplars → Recovery prior conditions on exemplars → generates corrected action → Loop until success or cost budget exhausted → Aggregate RR, C → compute ES; compare to predicted ERR

- **Design tradeoffs**: Retrieval vs. Latency (retrieval adds 1.2–1.5× overhead); Budget vs. RR (higher cost budgets increase RR but may violate bounded-cost assumption); Model Scale vs. Variance (larger models reduce variance but increase inference cost)

- **Failure signatures**: High ∆ERR (>0.05) indicates non-stationary tool semantics or schema drift; ES ≈ RR suggests λ is too small or cost variance is extreme; ERR superlinear in variance indicates violation of bounded-cost assumption

- **First 3 experiments**: 1) Baseline RR/ES Measurement: Run vanilla policy, compute RR, ES, observed ERR, compare to predicted ERR = (1-ES)/(1-γ); verify ∆norm ≤ 0.05. 2) Retrieval Ablation: Disable retrieval conditioning, measure variance increase and deviation from law. 3) Controlled Variance Stress Test: Fix parameters, artificially inflate execution cost variance, plot predicted vs. observed ERR to identify transition regimes.

## Open Questions the Paper Calls Out

- **Can ERR–ES handle non-stationary tool semantics?** The law assumes stationary perturbation process F(s_t, a_t), but if APIs change schemas mid-trajectory, ERR grows while ES does not reflect this drift, systematically underestimating regret. A modified ERR formulation tracking schema drift signals could restore predictive validity.

- **How does ERR–ES behave in non-ergodic settings?** If recovery induces persistent state changes via caches, in-context memory, or adaptive patches, the interaction process becomes non-ergodic. ERR–ES remains locally predictive over short horizons but cannot govern global accumulation.

- **Does ERR–ES generalize beyond 8B–32B parameter range?** All experiments used four model families spanning only 8B–32B parameters. Whether the efficiency–regret manifold persists for frontier models (>100B) or resource-constrained models (<8B) remains untested.

## Limitations

- The ERR–ES law relies on first-order linearization, valid only for small execution variances (σ²_C ≤ 10⁻²)
- Theoretical uniqueness proof for ES assumes recovery loss is first-order linearizable
- The law assumes stationary perturbation processes, which may break with schema drift or API version changes
- The framework addresses execution-level recoverability but not upstream model reasoning failures

## Confidence

- **High confidence**: ERR–ES law's first-order relationship and empirical validation across five benchmarks and four model families (8B–32B) are well-supported by direct evidence showing ∆norm ≤ 0.05
- **Medium confidence**: Theoretical uniqueness proof for ES as compatible surrogate is rigorous but has limited direct validation in the corpus
- **Low confidence**: Variance suppression mechanism via retrieval conditioning is empirically observed but theoretical justification is weaker

## Next Checks

1. **Stress Test with Controlled Variance**: Replicate Appendix H, Figure 6 by systematically increasing execution cost variance while holding other factors constant. Measure the transition from linear to curvature to breakdown regimes to identify the exact variance threshold where the law fails.

2. **Non-Stationary Semantics Test**: Introduce deliberate API schema changes or tool behavior drift during evaluation runs. Measure how quickly and severely the ERR–ES law's predictions degrade, and quantify the impact on ∆norm.

3. **Adversarial Cost Distribution**: Design a synthetic perturbation process with heavy-tailed or adversarial cost distributions. Test whether first-order linearization assumptions break and if ES remains the unique compatible surrogate under these conditions.