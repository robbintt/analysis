---
ver: rpa2
title: Ability Transfer and Recovery via Modularized Parameters Localization
arxiv_id: '2601.09398'
source_url: https://arxiv.org/abs/2601.09398
tags:
- layer
- activation
- channels
- ability
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACT identifies ability-relevant model parameters by analyzing cross-model
  activation differences, finding that ability-specific activations are concentrated
  in sparse, disentangled channels (<5%) that remain stable under fine-tuning. It
  transfers only these channels via masked task-vector merging and applies lightweight
  post-transfer fine-tuning for compatibility.
---

# Ability Transfer and Recovery via Modularized Parameters Localization

## Quick Facts
- **arXiv ID:** 2601.09398
- **Source URL:** https://arxiv.org/abs/2601.09398
- **Reference count:** 40
- **Key outcome:** ACT recovers forgotten multilingual scientific reasoning while preserving math skills by transferring <5% of disentangled channels, outperforming baselines on Qwen2.5 models.

## Executive Summary
ACT (Ability Transfer and Recovery) introduces a parameter-efficient method for selectively transferring specific capabilities between large language models that share the same pretrained backbone. The method identifies ability-relevant parameters by analyzing cross-model activation differences, finding that ability-specific activations concentrate in sparse, disentangled channels (<5%). These channels are transferred via masked task-vector merging, followed by lightweight post-transfer fine-tuning for compatibility. Experiments on multilingual math and science reasoning demonstrate ACT's ability to recover forgotten abilities while preserving retained skills, and to merge multiple specialized models with minimal interference.

## Method Summary
ACT operates in three stages: first, it computes token-averaged activation differences per channel across model pairs to identify ability-relevant parameters; second, it selects top-p% channels, computes task vectors, and applies masked transfer using θ_merged = θ_target + λ·Δθ·mask; third, it optionally applies lightweight post-transfer fine-tuning with 1,500 samples per ability at learning rate 2e-5 for one epoch. The method uses sparse channel masks (typically 1-9% per ability) to transfer only ability-specific parameters while preserving base model capabilities. For multi-ability integration, ACT takes the union of masks from multiple source models, enabling efficient combination of specialized models.

## Key Results
- Recovers multilingual scientific reasoning (9 languages) in math-specialized Qwen2.5 models while maintaining strong math performance (>85% on English/Chinese)
- Transfers only 4.73% of parameters to recover multiple abilities, outperforming full-model merging approaches
- Successfully integrates English math, science, and code abilities from separate models, achieving best overall performance among tested merging methods

## Why This Works (Mechanism)
ACT leverages the observation that different abilities activate distinct, sparse sets of channels within shared model architectures. By computing activation differences between models with different specializations, ACT identifies channels that are uniquely important for specific abilities. The masked transfer approach ensures only these ability-relevant parameters are updated, while the lightweight fine-tuning stage resolves any compatibility issues between transferred parameters and the target model's existing weights. This modular approach allows selective capability enhancement without catastrophic forgetting of preserved abilities.

## Foundational Learning
- **Cross-model activation analysis**: Understanding how different abilities manifest as distinct activation patterns across model parameters. *Why needed:* Enables identification of ability-specific parameters without labeled capability data. *Quick check:* Verify activation differences correlate with ability performance gaps.
- **Channel masking for selective transfer**: Using binary masks to restrict parameter updates to specific subsets. *Why needed:* Prevents interference between transferred abilities and preserved capabilities. *Quick check:* Confirm mask sparsity matches reported <5% values.
- **Task-vector merging**: Computing parameter differences between specialized models and applying them as additive updates. *Why needed:* Provides a principled way to transfer ability-specific modifications. *Quick check:* Validate that Δθ represents meaningful capability differences.
- **Zero-shot evaluation methodology**: Assessing model capabilities without fine-tuning on target tasks. *Why needed:* Enables rapid evaluation of transferred abilities across multiple domains. *Quick check:* Ensure BenchMax evaluation uses consistent prompts and sampling.
- **Multilingual evaluation via translation**: Testing capabilities across languages using machine-translated datasets. *Why needed:* Validates generalization of transferred abilities beyond training languages. *Quick check:* Assess translation quality impact on performance metrics.

## Architecture Onboarding
- **Component map:** Qwen2.5-7B-Instruct -> ACT analysis -> Masked transfer -> Qwen2.5-Math-7B-Instruct (merged)
- **Critical path:** Activation extraction → Channel ranking → Mask generation → Task vector computation → Parameter merging → Evaluation
- **Design tradeoffs:** ACT prioritizes parameter efficiency (4.73% transfer) over complete ability recovery, accepting minor performance gaps filled by SFT. This contrasts with full-model merging which transfers all parameters but requires more compute.
- **Failure signatures:** Excessive transfer ratio causes base ability degradation; mismatched model configurations (Math-Instruct vs Instruct) destabilize merges at high λ values; translation quality issues may confound multilingual evaluation results.
- **First experiments:** 1) Extract and visualize activation differences between Qwen2.5-7B-Instruct and Qwen2.5-Math-7B-Instruct on MGSM samples; 2) Apply ACT transfer with p=1% and λ=0.4, evaluate zero-shot science performance; 3) Compare ACT performance against full-model merging and task arithmetic baselines.

## Open Questions the Paper Calls Out
None

## Limitations
- The proprietary nature of the activation-difference methodology means the specific normalization and aggregation techniques are underspecified, making exact reproduction challenging
- Multilingual evaluation relies entirely on GPT-4o-mini translations without validation of translation quality or potential bias introduction
- The transfer-only experiments show λ=0.4 works, but the SFT fine-tuning stage with λ=0.7 lacks ablation showing whether this higher ratio is necessary or optimal

## Confidence
- **High Confidence:** The basic feasibility of selective ability transfer using channel masking and task-vector merging
- **Medium Confidence:** The claims about parameter efficiency (4.73% vs full model merges) and the specific assertion that channels are "disentangled" and "sparse"
- **Low Confidence:** The generalizability of the approach to other model architectures and the robustness of the multilingual translations

## Next Checks
1. Replicate the activation difference computation using the same Qwen2.5-7B model pair on a subset of MGSM samples, verifying that the top channels identified match the paper's claims about sparsity (<5%) and that the ranking correlates with ability recovery when transferred
2. Evaluate the quality of the GPT-4o-mini translations for one language pair (e.g., English→Chinese) using human evaluation or established MT metrics on 100 samples to establish whether translation artifacts could explain performance differences
3. Perform paired t-tests or bootstrap confidence intervals on the zero-shot accuracy differences between ACT and baselines across all 9 languages to determine if reported performance advantages are statistically significant rather than random variation