---
ver: rpa2
title: A Benchmark for Incremental Micro-expression Recognition
arxiv_id: '2501.19111'
source_url: https://arxiv.org/abs/2501.19111
tags:
- learning
- incremental
- recognition
- micro-expression
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark for incremental micro-expression
  recognition (IMER), addressing the challenge of continuously adapting models to
  new micro-expression datasets while preserving previously learned knowledge. The
  authors formulate a composite class-domain incremental learning problem, organize
  sequential micro-expression datasets (CASME II, SAMM, MMEW, CAS(ME)3) by publication
  date, and propose two evaluation protocols (SLCV and ILCV) with fold binding to
  manage cross-subject evaluation complexity.
---

# A Benchmark for Incremental Micro-expression Recognition

## Quick Facts
- arXiv ID: 2501.19111
- Source URL: https://arxiv.org/abs/2501.19111
- Reference count: 16
- Introduces first benchmark for incremental micro-expression recognition (IMER)

## Executive Summary
This paper introduces the first benchmark for incremental micro-expression recognition (IMER), addressing the challenge of continuously adapting models to new micro-expression datasets while preserving previously learned knowledge. The authors formulate a composite class-domain incremental learning problem, organize sequential micro-expression datasets (CASME II, SAMM, MMEW, CAS(ME)3) by publication date, and propose two evaluation protocols (SLCV and ILCV) with fold binding to manage cross-subject evaluation complexity. A key contribution is the Remappable Classification Head (RCH) technique, which maintains redundant classification heads for each session and dynamically remaps them to handle both new emotion categories and domain shifts.

## Method Summary
The IMER benchmark uses four micro-expression datasets in chronological order: CASME II (2014), SAMM (2018), MMEW (2022), and CAS(ME)3 (2023). The core innovation is the Remappable Classification Head (RCH) mechanism that maintains session-specific classification heads and dynamically remaps them via weight summation for shared classes. The benchmark includes six baseline methods combining popular backbones (ResNet, ViT, SwinT) with incremental learning techniques (Finetune, Foster, DER, Ranpac, L2P, DualPrompt). Training uses ImageNet-pretrained models with 60 epochs for session 1 and 10 epochs for subsequent sessions, batch size 16, and learning rate 2e-5.

## Key Results
- Ranpac with ViT backbone achieves highest average accuracy: 48.44% (ILCV) and 41.64% (SLCV)
- SLCV protocol shows 7% accuracy gap compared to ILCV, indicating cross-subject generalization remains challenging
- RCH-based methods show 3-7% accuracy improvement over single-head expansion approaches
- Pre-trained model-based methods (Ranpac) outperform feature-updating approaches by 3-5% on average

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Remappable Classification Head (RCH) enables handling of composite class-domain incremental scenarios where new datasets may introduce both novel emotion categories and domain shifts for existing categories.
- **Mechanism:** RCH maintains session-specific classification heads (H^t ∈ R^|l_t|×d) for each session, then dynamically remaps them via summation: H^c_final = Σ_{t∈T_c} H^c_t, where T_c denotes sessions containing class c. This preserves session-specific representations while consolidating weights for shared classes across domains.
- **Core assumption:** The summation of classifier weights across sessions preserves discriminative information for shared emotion classes despite domain variations.
- **Evidence anchors:**
  - [abstract] "A key contribution is the Remappable Classification Head (RCH) technique, which maintains redundant classification heads for each session and dynamically remaps them to handle both new emotion categories and domain shifts."
  - [Section 3.4] "RCH enables the model to maintain session-specific representations while still provides a unified classification framework."
  - [corpus] No direct corpus evidence for RCH specifically; this appears novel to this benchmark.
- **Break condition:** If domain shifts cause feature representations to diverge significantly (inconsistent feature spaces), simple weight summation may degrade rather than improve classification.

### Mechanism 2
- **Claim:** Pre-trained model-based methods (specifically Ranpac) outperform other incremental learning approaches for IMER because freezing the backbone after initial training preserves generalizable features.
- **Mechanism:** Ranpac performs full fine-tuning during Session 1, then freezes the backbone for subsequent sessions, updating only the classifier. This architecture suits scenarios with high feature-sharing among classes—micro-expressions exhibit significant inter-class feature overlap.
- **Core assumption:** Pre-trained ImageNet features transfer sufficiently to micro-expression recognition, and backbone updates after Session 1 cause more harm (forgetting) than benefit.
- **Evidence anchors:**
  - [Section 4] "This advantage can be attributed to the design of pre-trained model-based methods that perform full fine-tuning of the backbone during the initial training stage and freeze the backbone in subsequent incremental stages."
  - [Table 2] Ranpac achieves highest average accuracies across all backbones: 48.44% (ILCV, ViT) and 41.64% (SLCV, ViT).
  - [corpus] Corpus neighbors on incremental learning do not specifically address micro-expression feature overlap; this finding is benchmark-specific.
- **Break condition:** If micro-expression classes require domain-specific fine-tuning (e.g., clinical settings with atypical presentations), frozen backbones may underfit.

### Mechanism 3
- **Claim:** Fold binding regulation enables computationally feasible cross-subject evaluation in incremental settings by constraining evaluation complexity from O(k^n) to O(n) trials.
- **Mechanism:** The same fold index τ is maintained across all sessions, so during trial τ, the model trains on D^{(t,τ)}_train and evaluates on test sets from all encountered sessions using the same fold assignment. This avoids combinatorial explosion while preserving cross-subject validity.
- **Core assumption:** Consistent fold assignment across sessions does not introduce systematic bias in performance estimation.
- **Evidence anchors:**
  - [Section 3.2] "Fold binding regulates cross-session evaluation, which maintains computational feasibility by controlling the total number of tests within O(n) complexity."
  - [Section 3.2] Example given: 10 × 12 × 15 = 1800 trials reduced to 5 (for k=5 folds).
  - [corpus] No corpus evidence on fold binding; appears to be a novel protocol contribution.
- **Break condition:** If certain subjects are systematically harder/easier across datasets, fixed fold binding may over- or under-estimate generalization.

## Foundational Learning

- **Concept: Class-Incremental Learning (CIL) vs. Domain-Incremental Learning (DIL)**
  - **Why needed here:** IMER is explicitly a "composite class-domain incremental learning problem." Understanding the distinction is prerequisite to grasping why standard CIL or DIL methods alone fail.
  - **Quick check question:** If a model learns "happiness" from CASME II, then sees "happiness" from SAMM with different imaging conditions, is this CIL or DIL?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper's core motivation is that sequential training risks overwriting previously learned emotion representations. All baseline methods (Finetune, Foster, DER, etc.) attempt to mitigate this.
  - **Quick check question:** Why does simply expanding the classification head for new classes "amplify concept drift across domains"?

- **Concept: Cross-Subject Evaluation in MER**
  - **Why needed here:** Subject invariance is critical for real-world deployment. The benchmark distinguishes SLCV (subject-level) from ILCV (instance-level), with SLCV consistently yielding lower accuracy (~7% gap for ViT-Ranpac).
  - **Quick check question:** What does the ILCV > SLCV performance gap imply about model generalization?

## Architecture Onboarding

- **Component map:**
  Input: Apex frames from video sequences
  Backbone (ResNet / ViT / SwinT) — ImageNet pretrained
  Feature vector (dim d)
  RCH: Session-specific heads H^t per session
  Dynamic remapping: Sum weights for shared classes
  Output: Softmax over cumulative label space L_t

- **Critical path:**
  1. Initialize backbone with ImageNet weights
  2. Session 1: Full fine-tuning on CASME II (60 epochs)
  3. Sessions 2-4: Incremental training (10 epochs each) with backbone frozen (for Ranpac) or updated per method
  4. At each session: Expand RCH with new head H^t, remap shared-class weights

- **Design tradeoffs:**
  - **Backbone choice:** ViT/SwinT outperform ResNet by 7-10% but require more compute.
  - **Freezing vs. updating:** Freezing backbone (Ranpac) yields best results but limits domain adaptation; dynamic expansion (DER, Foster) offers flexibility at cost of capacity growth.
  - **SLCV vs. ILCV:** SLCV is harder but more realistic; ILCV easier but may overestimate deployment performance.

- **Failure signatures:**
  - Finetune baseline: Catastrophic forgetting evident—accuracy drops from ~40% (Session 1) to ~22% (Session 4, ILCV/ResNet).
  - Inconsistent class label spaces across datasets: Requires careful label harmonization before training.
  - Low absolute accuracy (<50%): Expected given ME difficulty; not necessarily a failure mode.

- **First 3 experiments:**
  1. **Replicate Ranpac+ViT baseline** on the published dataset sequence (CASME II → SAMM → MMEW → CAS(ME)3) with both SLCV and ILCV protocols to establish reference performance.
  2. **Ablate RCH:** Replace remapped heads with single expanded classifier to quantify RCH contribution to handling composite class-domain shifts.
  3. **Vary backbone freezing point:** Test if freezing after Session 2 (post-SAMM) improves domain adaptation vs. freezing after Session 1, given SAMM's RGB/grayscale difference from CASME II.

## Open Questions the Paper Calls Out

- **Can the Remappable Classification Head (RCH) mechanism be effectively generalized to other computer vision tasks that exhibit composite class-domain incremental characteristics?**
  - Basis in paper: [explicit] The conclusion states that the proposed RCH technique "can potentially be extended to build incremental learning model for other similar tasks."
  - Why unresolved: The benchmark and experiments are restricted to micro-expression datasets; the generalizability of the redundant head summation strategy to other domains is not yet verified.
  - What evidence would resolve it: Application of the RCH module to other composite incremental benchmarks (e.g., face recognition or general object recognition) showing consistent performance improvements.

- **How do temporal, video-based architectures perform on the IMER benchmark compared to the static apex-frame baselines provided?**
  - Basis in paper: [inferred] The authors utilize apex frames for the baseline methods, acknowledging that while micro-expression recognition is "essentially a video classification task," they follow established practices using key frames.
  - Why unresolved: The current benchmark results rely entirely on 2D CNN and Transformer backbones processing static images, leaving the efficacy of temporal modeling (e.g., optical flow or 3D networks) in an incremental setting unexplored.
  - What evidence would resolve it: Evaluation of video-based incremental learning methods using the proposed IMER protocols to compare against the static-frame baselines.

- **How can the specific performance gap between Instance-Level (ILCV) and Subject-Level (SLCV) evaluation protocols be reduced?**
  - Basis in paper: [inferred] The results consistently show a significant accuracy drop in SLCV compared to ILCV (e.g., Ranpac+ViT drops from 48.44% to 41.64%), indicating that cross-subject generalization remains a distinct challenge.
  - Why unresolved: While the paper identifies this gap, the baseline methods do not implement specific strategies (such as domain adaptation or subject-invariant losses) to explicitly address subject variability in the incremental learning process.
  - What evidence would resolve it: Methods designed to handle subject-independent features in an incremental stream, resulting in higher SLCV accuracy relative to ILCV.

## Limitations

- The Remappable Classification Head (RCH) mechanism lacks ablation studies to quantify its specific contribution versus simpler approaches like direct classifier expansion.
- The benchmark assumes consistent fold binding across sessions, which may introduce systematic bias if certain subjects are consistently harder/easier across datasets.
- Absolute accuracy remains below 50%, limiting practical deployment confidence despite being expected for micro-expression recognition.

## Confidence

- **High confidence:** The problem formulation as composite class-domain incremental learning is well-established and correctly implemented.
- **Medium confidence:** The Ranpac method's superior performance is well-supported by results, but the mechanism (frozen backbone) may not generalize to all incremental scenarios.
- **Low confidence:** The fold binding protocol's impact on evaluation validity is insufficiently validated, particularly regarding potential subject-specific biases.

## Next Checks

1. **Ablation study:** Remove RCH and replace with standard incremental classifier expansion to measure its specific contribution to handling domain shifts.
2. **Cross-dataset generalization:** Train on sessions 1-3, test exclusively on session 4 (CAS(ME)3) to evaluate true zero-shot domain adaptation capability.
3. **Temporal robustness:** Shuffle dataset order (e.g., MMEW → SAMM → CASME II) to assess whether chronological sequence affects performance or if the method is truly order-agnostic.