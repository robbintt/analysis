---
ver: rpa2
title: 'ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language
  Modeling'
arxiv_id: '2507.15275'
source_url: https://arxiv.org/abs/2507.15275
tags:
- medical
- chinese
- data
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ChiMed 2.0, a large-scale Chinese medical dataset
  designed to support pre-training, supervised fine-tuning, and reinforcement learning
  from human feedback (RLHF) for large language models (LLMs). The dataset integrates
  traditional Chinese medicine classics with modern clinical content, totaling 204.4M
  Chinese characters across 164.8K documents, 351.6K question-answer pairs, and 41.7K
  preference data tuples.
---

# ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling

## Quick Facts
- arXiv ID: 2507.15275
- Source URL: https://arxiv.org/abs/2507.15275
- Reference count: 17
- This paper presents ChiMed 2.0, a large-scale Chinese medical dataset designed to support pre-training, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) for large language models (LLMs).

## Executive Summary
ChiMed 2.0 is a comprehensive Chinese medical dataset containing 204.4M Chinese characters across 164.8K documents, 351.6K question-answer pairs, and 41.7K preference data tuples. The dataset integrates traditional Chinese medicine classics with modern clinical content and was validated through experiments on Qwen-3 models of different scales (1.7B and 14B parameters). The results demonstrate consistent performance gains on medical benchmarks CMMLU and CEval, with absolute improvements of up to 4.09% on smaller models and 2.05% on larger models. The dataset aims to advance Chinese medical LLMs by providing high-quality, domain-specific training data across multiple learning paradigms.

## Method Summary
The ChiMed 2.0 dataset was constructed through a multi-stage processing pipeline that includes deduplication, noise filtering, sensitive content screening, automated translation of ancient Chinese texts, QA pair generation, and preference data construction. The dataset integrates traditional Chinese medicine classics with modern clinical content, totaling 204.4M Chinese characters across 164.8K documents, 351.6K question-answer pairs, and 41.7K preference data tuples. The construction process involved automated translation using GPT-4o followed by manual review, with quality assurance measures implemented throughout. The dataset was validated through experiments on Qwen-3 models of different scales (1.7B and 14B parameters), demonstrating consistent performance gains on medical benchmarks CMMLU and CEval.

## Key Results
- Absolute performance improvements of up to 4.09% on smaller Qwen-3 models (1.7B parameters)
- Consistent gains of 2.05% on larger Qwen-3 models (14B parameters)
- Demonstrated effectiveness across multiple learning paradigms including pre-training, supervised fine-tuning, and RLHF
- Validated on Chinese medical benchmarks CMMLU and CEval

## Why This Works (Mechanism)
ChiMed 2.0 works by providing domain-specific training data that bridges traditional Chinese medicine knowledge with modern clinical practices. The integration of ancient medical texts with contemporary medical information creates a comprehensive knowledge base that enables LLMs to reason across different medical paradigms. The multi-stage curation process ensures data quality through deduplication, noise filtering, and sensitive content screening. The automated translation pipeline for ancient Chinese texts, combined with manual review, makes classical medical knowledge accessible to modern language models. The dataset's diverse formats (documents, QA pairs, preference data) support different learning objectives, from general knowledge acquisition to alignment through RLHF.

## Foundational Learning
- **Deduplication**: Removes redundant content to ensure training efficiency and prevent model bias toward frequently occurring information. Quick check: Verify dataset size reduction after deduplication and confirm no loss of critical medical knowledge.
- **Noise filtering**: Eliminates irrelevant or low-quality content that could degrade model performance. Quick check: Analyze distribution of filtered content categories and their impact on downstream performance.
- **Automated translation of ancient texts**: Makes classical Chinese medical knowledge accessible to modern LLMs through GPT-4o translation followed by manual review. Quick check: Conduct expert evaluation of translation accuracy for sampled passages.
- **QA pair generation**: Creates structured training examples that help models learn to answer medical questions accurately. Quick check: Measure correlation between QA pair quality and model performance on benchmark tasks.
- **Preference data construction**: Supports RLHF by providing human judgments on model outputs. Quick check: Validate preference data distribution and ensure it covers diverse medical scenarios.
- **Sensitive content screening**: Ensures the dataset complies with medical ethics and privacy regulations. Quick check: Audit screening process to confirm coverage of all sensitive content categories.

## Architecture Onboarding

**Component Map**: Raw Data Sources -> Deduplication & Cleaning -> Automated Translation -> QA Generation -> Preference Data Construction -> Validation Pipeline -> Qwen-3 Models

**Critical Path**: The most critical path is Raw Data Sources -> Automated Translation -> QA Generation -> Model Training, as this sequence directly impacts the quality of medical knowledge acquisition and the model's ability to reason about medical concepts.

**Design Tradeoffs**: The paper prioritizes dataset comprehensiveness over absolute data quality in some areas, particularly in the automated translation of ancient texts. While this approach enables scaling to large volumes of classical medical literature, it introduces potential semantic drift that requires manual correction. The preference data construction is limited in scale (41.7K tuples) compared to the overall dataset size, representing a tradeoff between breadth of medical knowledge and depth of alignment training.

**Failure Signatures**: Key failure modes include: translation errors in ancient medical texts leading to semantic misunderstandings, insufficient preference data for effective RLHF, and potential overfitting to Chinese medical benchmarks without generalization to broader medical domains. Models trained on this dataset may also struggle with cross-cultural medical reasoning if the ancient text translations contain systematic biases.

**First Experiments**: 
1. Evaluate model performance on both Chinese medical benchmarks (CMMLU, CEval) and international medical standards (MedQA, PubMedQA) to assess cross-cultural generalizability.
2. Conduct blind human evaluation comparing model outputs trained with and without ChiMed 2.0 on clinical reasoning tasks to validate practical utility.
3. Perform error analysis on the automated translation pipeline using domain expert review of sampled passages to quantify translation quality and identify systematic issues.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance improvements of 2.05-4.09% are relatively modest given the extensive curation effort required
- Automated translation pipeline for ancient Chinese texts lacks transparent quality assurance metrics and may introduce semantic drift
- Preference data construction (41.7K tuples) appears limited compared to overall dataset scale, potentially under-resourcing RLHF component
- Evaluation focuses narrowly on Chinese medical benchmarks without external validation on international standards or clinical applicability testing

## Confidence
- Dataset construction methodology: High
- Performance improvement claims: Medium
- Translation quality assurance: Low
- Clinical relevance and generalizability: Low

## Next Checks
1. Conduct blind human evaluation comparing model outputs trained with and without ChiMed 2.0 on clinical reasoning tasks
2. Perform error analysis on the automated translation pipeline using domain expert review of sampled passages
3. Test model performance on international medical benchmarks (e.g., MedQA, PubMedQA) to assess cross-cultural generalizability