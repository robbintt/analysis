---
ver: rpa2
title: 'Memento 2: Learning by Stateful Reflective Memory'
arxiv_id: '2512.22716'
source_url: https://arxiv.org/abs/2512.22716
tags:
- memory
- learning
- policy
- episodic
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a theoretical framework for continual learning
  in LLM agents using episodic memory and reflection, without parameter fine-tuning.
  It introduces the Stateful Reflective Decision Process (SRDP), which extends the
  Markov Decision Process by incorporating episodic memory and a two-stage decision
  mechanism: retrieval from memory followed by LLM-based action generation.'
---

# Memento 2: Learning by Stateful Reflective Memory

## Quick Facts
- arXiv ID: 2512.22716
- Source URL: https://arxiv.org/abs/2512.22716
- Authors: Jun Wang
- Reference count: 40
- Primary result: Theoretical framework for continual learning in LLM agents using episodic memory and reflection without parameter fine-tuning, with convergence guarantees.

## Executive Summary
This paper introduces a theoretical framework for continual learning in LLM agents that operates without parameter updates. The framework extends the Markov Decision Process by incorporating episodic memory and a two-stage decision mechanism: retrieval from memory followed by LLM-based action generation. By augmenting the state with memory, the system is shown to be Markovian and can be reformulated as a Reflected MDP. The Read-Write Reflective Learning algorithm integrates Parzen-window-based retrieval with KL-regularized soft policy iteration, achieving asymptotic optimality as memory grows to cover the state space.

## Method Summary
The method augments the environment state with episodic memory to create a Markovian "Reflected MDP" framework. The agent operates through a two-stage decision process: first retrieving a relevant memory case using Parzen-window density estimation with a void case fallback, then generating actions via a fixed LLM conditioned on the current state and retrieved case. The Read-Write Reflective Learning algorithm iterates between policy evaluation (writing new experiences to memory) and policy improvement (updating the retrieval policy via KL-regularized soft policy iteration). Theoretical analysis proves convergence to optimal policies under bounded rewards and slow memory evolution relative to policy updates.

## Key Results
- The augmented state space (environment state + memory) is Markovian, enabling classical RL analysis
- KL-regularized soft policy iteration converges to optimal policies for the Reflected MDP
- Two-timescale analysis guarantees convergence when memory evolves slower than policy updates
- Asymptotic optimality: as memory coverage approaches the state space, the composite policy converges to the optimal MDP policy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting the state space with episodic memory restores the Markov property, allowing classical reinforcement learning analysis to apply to memory-dependent agents.
- Mechanism: The framework defines an augmented state x_t = (s_t, M_t) combining the environment state and the memory bank. While the environment state s_t alone is non-Markovian due to evolving memory, the joint space is Markovian (the "Reflected MDP"). This allows the agent to optimize a retrieval policy μ over the memory index rather than modifying LLM weights.
- Core assumption: The LLM kernel p_LLM(a|s,c) is fixed or slow-evolving, allowing it to be treated as part of the environment dynamics during policy optimization.
- Evidence anchors:
  - [Section 3.2.2]: "By augmenting the state with memory, we show this system is Markovian and can be reformulated as a Reflected MDP."
  - [Definition 4]: Defines the Reflected MDP tuple ⟨X, C, P_LLM, R_LLM, γ⟩.
  - [corpus]: Corpus evidence specifically addressing the "Reflected MDP" theoretical derivation is weak; related papers focus on empirical agent personalization.
- Break condition: If the LLM parameters change rapidly (breaking the "fixed kernel" assumption), the transition dynamics P_LLM become non-stationary, violating the MDP structure.

### Mechanism 2
- Claim: Continual learning is achieved by mapping memory retrieval to policy improvement (Read) and experience storage to policy evaluation (Write).
- Mechanism: The "Read" operation retrieves a case c to shape the LLM's action distribution, functioning as policy improvement. The "Write" operation stores trajectory tuples (s, a, r, s'), functioning as policy evaluation by updating the value estimates associated with memory cases. Iterating these steps constitutes a reflective policy iteration loop.
- Core assumption: Retrieval based on local state density (Parzen windows) successfully approximates the value gradient needed for policy improvement.
- Evidence anchors:
  - [Abstract]: "Writing stores interaction outcomes and plays the role of policy evaluation. Reading retrieves relevant past cases... and plays the role of policy improvement."
  - [Section 2]: Defines the reflection operator π_{t+1} = Read(Write(π_t)).
- Break condition: If the retrieval mechanism fails to surface high-value cases (e.g., due to poor state embeddings), the "policy improvement" step degrades into noise.

### Mechanism 3
- Claim: Integrating KL-regularization with Parzen-window retrieval ensures convergence to an optimal policy without parameter updates.
- Mechanism: The retrieval policy μ is updated via soft policy iteration (Eq. 11), maximizing a KL-regularized objective. This keeps the retrieval distribution close to a Parzen prior (similarity-based density) while shifting probability mass toward high-value cases. Theoretical analysis proves this converges if rewards are bounded and memory evolves slowly.
- Core assumption: Assumption 11 (LLM Local Consistency) holds, meaning the LLM can infer near-optimal actions even when the retrieved case is only an approximate match to the current state.
- Evidence anchors:
  - [Theorem 8]: "Iterations between (9) and (11)... converge to a fixed point (Q^*, μ^*) that is optimal for the KL-regularized objective."
  - [Section 4.1]: Derives the closed-form update μ^+(c|x) ∝ μ_0(c|x) exp(Q(x,c)/α).
- Break condition: If the temperature α is set incorrectly or the Parzen bandwidth h is too wide/narrow, the retrieval policy may oscillate or collapse to a sub-optimal deterministic strategy.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: The paper reformulates a complex memory system into a "Reflected MDP." Understanding standard MDPs (states, actions, transition kernels) is the prerequisite to understanding why adding memory to the state vector restores the Markov property required for convergence proofs.
  - Quick check question: Can you explain why a process that depends on a growing history of past events is typically non-Markovian?

- Concept: **Policy Iteration (Evaluation vs. Improvement)**
  - Why needed here: The core "Read-Write" loop is an analogy of Policy Iteration. You must distinguish between *evaluating* how good a current policy is (Write/Value Function) and *improving* that policy to select better actions (Read/Retrieval update).
  - Quick check question: In standard RL, what is the relationship between the Value Function update and the Policy update?

- Concept: **KL-Divergence Regularization**
  - Why needed here: The paper uses a "soft" policy update constrained by KL-divergence relative to a prior. This prevents the retrieval policy from changing too drastically in a single step, which is critical for stability in a non-parametric memory system.
  - Quick check question: Why might a "hard" policy update (greedy selection) be unstable when the policy is defined over a discrete, growing memory bank?

## Architecture Onboarding

- Component map:
  - LLM Kernel -> Memory Bank -> Retriever (Policy) -> State Encoder -> Environment

- Critical path:
  1. Observe state s_t; construct augmented state x_t = (s_t, M_t)
  2. **Retrieval (Read):** Compute Parzen prior, then adjust with Q-values to sample case c_t
  3. **Action:** LLM generates a_t ~ p_LLM(·|s_t, c_t)
  4. **Feedback:** Env returns r_t, s_{t+1}
  5. **Update (Write):** Add (s_t, a_t, r_t, s_{t+1}) to Memory; update Q-values for the retrieved case

- Design tradeoffs:
  - **Bandwidth h:** Controls locality of retrieval. Too small → sparse retrieval (void case triggered); too large → retrieval of irrelevant contexts
  - **Void Case Weight K_∅:** Controls reliance on internal LLM knowledge vs. memory. High weight encourages "discovery" (exploration); low weight forces reuse (exploitation)
  - **Timescale Separation ρ_t / η_t:** Memory must grow slower than policy convergence

- Failure signatures:
  - **Catastrophic Forgetting:** If the "slow timescale" constraint is violated (memory updates too fast), the policy cannot track the moving target, leading to oscillation
  - **Void Case Collapse:** The agent ignores memory entirely because the Parzen kernel density is too low or the void case weight is too high

- First 3 experiments:
  1. **Ablation on Timescales:** Verify that if you update memory at the same speed as the Q-function (ρ_t = η_t), convergence fails or slows significantly
  2. **Bandwidth Sensitivity:** Visualize the Parzen kernel weights. Does the effective retrieval set size shrink/grow appropriately as memory fills?
  3. **LLM Consistency Check:** Measure the performance gap when retrieving the "nearest" case vs. a random case to validate Assumption 11 (LLM Local Consistency)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can episodic memory be internalised within the LLM architecture itself (replacing external retrieval over a growing case base with an internal memory state updated online) while preserving the convergence guarantees of Read–Write Reflective Learning?
- Basis in paper: [explicit] Section 6.1: "A natural next step is to integrate the episodic memory and read–write reflection mechanism of SRDP directly into the LLM architecture, yielding a single neural system whose internal state corresponds to the augmented state (s,M)."
- Why unresolved: The current theoretical analysis assumes external, tabular-style episodic memory with explicit retrieval operations; extending proofs to neural internal memories requires new characterizations of memory coverage and approximation error.
- What evidence would resolve it: A convergence proof for a hybrid architecture where an internal memory module is updated online with provable two-timescale separation between fast policy iteration and slow memory evolution.

### Open Question 2
- Question: How does the framework scale computationally as memory size increases, and what mechanisms can maintain retrieval efficiency?
- Basis in paper: [explicit] Section 6: "It might face computational challenges as memory size increases."
- Why unresolved: The Parzen-window retrieval requires computing kernel similarities over all stored cases; the paper does not analyze or propose scalable approximations for large memory regimes.
- What evidence would resolve it: Theoretical bounds or empirical studies on retrieval latency/accuracy trade-offs with approximate nearest-neighbor methods, hierarchical indexing, or memory pruning strategies.

### Open Question 3
- Question: What is the quantitative impact of embedding quality (choice of ψ: S → R^d) on retrieval performance, convergence rate, and asymptotic optimality?
- Basis in paper: [explicit] Section 6: "The impact of embedding quality on performance is not fully addressed."
- Why unresolved: The theoretical analysis assumes a given embedding function, but poor embeddings could violate the LLM local consistency assumption (Assumption 11) by placing dissimilar states close together.
- What evidence would resolve it: Sensitivity analysis relating embedding distortion metrics to the coverage radius r_M and LLM approximation error ε_LLM(r), or empirical benchmarks across embedding choices.

### Open Question 4
- Question: When is reflection-based memory adaptation sufficient for continual learning versus when is parameter adaptation necessary, and how can both be combined without destabilizing long-horizon control?
- Basis in paper: [explicit] Section 6.1: "An important future direction is to recast SRDP within this nested-optimisation view...Such a unification could clarify when reflection-based memory updates are sufficient for continual learning, when parameter adaptation is necessary, and how to combine both without destabilising long-horizon control."
- Why unresolved: The current framework treats the LLM as frozen; the conditions under which memory-only updates fail and parameter updates become essential remain uncharacterized.
- What evidence would resolve it: Formal analysis identifying task properties (e.g., out-of-distribution shifts, novel reasoning patterns) that require parameter changes, with convergence guarantees for hybrid update schemes.

## Limitations
- The Parzen-window retrieval requires careful bandwidth tuning and may not scale to high-dimensional state spaces
- Assumption 11 (LLM Local Consistency) lacks empirical validation - the LLM must truly infer near-optimal actions from approximate cases
- The framework assumes a fixed LLM kernel, which may not hold for adaptive or online-trained models

## Confidence
- **High**: Theoretical MDP reformulation and convergence proofs (Mechanism 1)
- **Medium**: KL-regularized policy iteration framework (Mechanism 3)
- **Low**: Practical effectiveness of Parzen retrieval and Assumption 11 validation

## Next Checks
1. Test Assumption 11 empirically: measure action quality degradation when retrieving approximate vs. exact state matches across diverse environments
2. Validate timescale separation: systematically vary ρ_t/η_t ratios and measure convergence speed/stability
3. Benchmark Parzen bandwidth sensitivity: evaluate retrieval quality across different h values and embedding spaces to identify optimal scaling behavior