---
ver: rpa2
title: 'REPA Works Until It Doesn''t: Early-Stopped, Holistic Alignment Supercharges
  Diffusion Training'
arxiv_id: '2505.16792'
source_url: https://arxiv.org/abs/2505.16792
tags:
- haste
- alignment
- diffusion
- repa
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Diffusion Transformers (DiTs) deliver state-of-the-art image quality,\
  \ but their training remains slow due to the need to backpropagate through hundreds\
  \ of denoising steps. Recent representation alignment (REPA) accelerates early training\
  \ by aligning DiT features to those of a non-generative teacher, but its benefit\
  \ plateaus or degrades later due to a capacity mismatch: the teacher\u2019s embeddings\
  \ and attention patterns become a constraint rather than a guide once the generative\
  \ student matures."
---

# REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training

## Quick Facts
- **arXiv ID:** 2505.16792
- **Source URL:** https://arxiv.org/abs/2505.16792
- **Reference count:** 40
- **Primary result:** Holistic alignment + early termination speeds DiT training by 28×, reaching 50 epochs for SiT-XL/2 baseline FID vs. 1400 epochs vanilla.

## Executive Summary
Diffusion Transformers (DiTs) deliver state-of-the-art image quality, but their training remains slow due to the need to backpropagate through hundreds of denoising steps. Recent representation alignment (REPA) accelerates early training by aligning DiT features to those of a non-generative teacher, but its benefit plateaus or degrades later due to a capacity mismatch: the teacher's embeddings and attention patterns become a constraint rather than a guide once the generative student matures. To address this, we introduce HASTE, which combines holistic alignment (simultaneously distilling attention maps and feature projections from the teacher) with stage-wise termination. The alignment is disabled after a fixed iteration or gradient-angle trigger, freeing the DiT to focus on denoising. HASTE speeds up training of diverse DiTs without architectural changes. On ImageNet 256×256, it reaches the vanilla SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs, amounting to a 28× reduction in optimization steps. It also improves text-to-image DiTs on MS-COCO, demonstrating a simple yet principled recipe for efficient diffusion training across tasks.

## Method Summary
HASTE combines two techniques: holistic alignment (joint feature+attention distillation from a frozen DINOv2 teacher) and stage-wise termination (hard stop to alignment losses). The student aligns mid-layers [4,7] for attention and block 8 (XL/L) or 5 (B) for features, then drops all alignment at iteration τ (e.g., 250K for SiT-XL/2). This avoids capacity mismatch where the teacher's lower-dimensional embeddings constrain the maturing DiT. The method accelerates training across SiT, DiT, and MM-DiT variants without architectural changes.

## Key Results
- On ImageNet 256×256, HASTE achieves SiT-XL/2 vanilla baseline FID in just 50 epochs, vs. 1400 epochs vanilla training.
- Holistic alignment alone matches REPA's best FID in 500 epochs, a 28× reduction in optimization steps.
- For text-to-image MM-DiT on MS-COCO, HASTE improves FID and sFID vs. both vanilla and REPA baselines.
- The method generalizes across model sizes (B→XL) and tasks (class-conditional → text-to-image) without hyperparameter tuning.

## Why This Works (Mechanism)

### Mechanism 1: Holistic Distillation
Jointly distilling features and attention patterns provides superior structural priors compared to feature alignment alone. The teacher's attention maps encode spatial relationships (relational priors) that complement semantic content in feature embeddings, providing a more complete guidance signal. The core assumption is that attention patterns in vision encoders capture useful spatial reasoning that transfers to diffusion models. Evidence anchors include the abstract claim about holistic alignment distilling "attention maps (relational priors) and feature projections (semantic anchors)" and section 2.3's statement that "Two models can share identical patch embeddings but attend to them in entirely different patterns." The break condition is that this fails if the student architecture's attention mechanism is fundamentally different from the teacher's (e.g., linear vs. full attention), making transfer infeasible.

### Mechanism 2: Capacity-Mismatch Driven Termination
The alignment loss flips from beneficial to harmful as the student's generative capacity surpasses the teacher's representation capacity. Early in training, the teacher provides coarse semantic scaffolding. As the student matures and models the full joint distribution (including high-frequency details), the frozen teacher's lower-dimensional embeddings constrain the student, causing gradient conflict. The core assumption is that the gradient-angle between alignment and denoising objectives reliably signals the onset of capacity mismatch. Evidence anchors include the abstract claim that "the teacher's lower-dimensional embeddings and attention patterns become a straitjacket rather than a guide once the generative student matures" and section 2.2's observation that "For late steps (t ≤ 0.1)—responsible for textures and fine grain—they are near-orthogonal from the start." The break condition is that this fails if the teacher model is more expressive than the student's generative capacity (unlikely) or if the generative task doesn't require high-frequency detail learning.

### Mechanism 3: Mid-Layer Selective Alignment
Distilling to mid-level DiT layers maximizes benefit while avoiding interference with noise processing (shallow) and generation (deep) functions. Shallow layers handle noisy latents, making clean-image teacher supervision off-manifold. Deep layers are dedicated to final denoising and should remain unregularized. Mid-layers balance semantic receptivity with downstream impact. The core assumption is that DiT layers exhibit functional specialization, with distinct roles for noise handling vs. semantic integration vs. final denoising. Evidence anchors include section 2.3's statements that "Early DiT layers ingest Gaussian-noisy latents... Supervising those layers with pixel-space attention from a clean-image encoder is therefore off-manifold" and "The last blocks are responsible for translating high-level structure into precise generation update... should remain dedicated to the denoising objective, unregularized." The break condition is that this fails if the target diffusion model has a different layer-wise functional distribution (e.g., U-Nets with skip connections may distribute tasks differently).

## Foundational Learning

- **Concept: Gradient Cosine Similarity**
  - Why needed here: This metric quantifies when alignment and denoising objectives are cooperating (acute angle), neutral (orthogonal), or conflicting (obtuse). The paper uses it to time the termination switch.
  - Quick check question: Given two loss gradients g1 and g2, what does a negative cosine similarity indicate about their optimization directions?

- **Concept: Diffusion Timestep Functional Separation**
  - Why needed here: Different timesteps in the diffusion process handle different aspects of image generation. Early steps (high noise) determine global structure; later steps (low noise) refine textures. The paper shows alignment is useful for structure but conflicts with detail refinement.
  - Quick check question: At diffusion timesteps near t=0.1, is the model primarily establishing global object layout or refining fine-grained textures?

- **Concept: Knowledge Distillation vs. Representation Alignment**
  - Why needed here: This work combines feature-space alignment (REPA) with attention-map transfer. Understanding the distinction—distilling "what to look at" (embeddings) vs. "how to look" (attention)—is crucial for grasping the holistic approach.
  - Quick check question: If two models have identical attention maps but different feature embeddings, would their downstream behavior likely be similar or divergent?

## Architecture Onboarding

- **Component map:** Teacher Model (frozen DINOv2) -> Alignment Loss Manager -> Student Model (DiT/SiT) -> Denoising Loss Manager -> Total Loss

- **Critical path:**
  1. Setup forward passes for student and teacher; extract attention maps and embeddings from designated layers.
  2. Compute L_REPA (token-wise cosine similarity) and L_ATTA (token-wise cross-entropy on attention distributions).
  3. Combine with denoising loss, weight with λ_R and λ_A (default 0.5 each).
  4. Track training iteration; at trigger τ, zero out alignment losses for all subsequent steps.

- **Design tradeoffs:**
  - Fixed iteration τ vs. gradient-angle trigger: Fixed is simpler but may require per-dataset tuning. Gradient-based is more robust but adds compute overhead.
  - Mid-layer depth: Aligning deeper layers captures more abstract semantics but risks over-constraining generative capacity. Aligning shallower layers risks off-manifold supervision. The paper finds [4,5,6,7] optimal for SiT-XL/2.
  - Attention alignment weight (λ_A): Higher values (e.g., 1.5-3.0) can lead to over-regularization; 0.5 offers stable benefits across model sizes.

- **Failure signatures:**
  - FID plateaus or degrades after initial improvement: Alignment may be continuing past the capacity mismatch point. Check if termination trigger τ is too late.
  - Training instability / loss spikes in early epochs: Shallow layers may be receiving attention supervision. Verify alignment is restricted to mid-layers.
  - High-fidelity global structure but poor texture detail: Alignment may be too aggressive or λ_A too high, over-constraining the student. Reduce λ_A or trigger earlier termination.
  - Poor text-to-image alignment: In MM-DiT, ensure attention alignment uses only image-side QK^T; textual pathway should remain unsupervised.

- **First 3 experiments:**
  1. Establish baseline convergence curves with vanilla DiT/SiT training on ImageNet 256×256 for 1000+ epochs.
  2. Introduce REPA alignment alone; track FID and gradient-angle similarity over time to empirically identify when the plateau/conflict region begins (this determines candidate τ values).
  3. Add ATTA (attention alignment) to form holistic alignment; run ablation with τ set to the identified region vs. no termination. Compare final FID, sFID, and IS at matched iteration counts (e.g., 400K vs. 500K).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does HASTE maintain its efficiency gains when applied to pixel-level diffusion models or video generation tasks?
- Basis in paper: The authors explicitly state in the Limitations section: "Explorations of HASTE with pixel-level diffusion... or in video generation tasks... would be exciting directions for future work."
- Why unresolved: The paper currently restricts validation to latent-space Diffusion Transformers (DiTs) for image generation.
- What evidence would resolve it: Applying HASTE to pixel-based architectures (e.g., EDM) and video transformers to compare convergence speed against standard training baselines.

### Open Question 2
- Question: Can HASTE be successfully combined with other representation alignment techniques on non-Transformer architectures?
- Basis in paper: The authors suggest: "HASTE may also be incorporated with other methods... on different model architectures [45]" (referencing U-REPA for U-Nets).
- Why unresolved: The method is demonstrated on Transformer architectures (SiT, DiT, MM-DiT), but its interaction with CNN-based U-Nets remains untested.
- What evidence would resolve it: Integrating holistic alignment and early-stopping into a U-Net diffusion pipeline and measuring performance deltas.

### Open Question 3
- Question: Can the gradient-angle trigger fully automate the termination schedule without performance loss compared to manually tuned fixed iterations?
- Basis in paper: The paper notes that "A fixed τ works nearly as well but the gradient rule adds robustness," yet the best reported results (Table 1) rely on manually set fixed iterations (e.g., 100K vs. 250K) specific to model size.
- Why unresolved: It is unclear if the automated trigger eliminates the need for dataset-specific or model-specific hyperparameter tuning of the stopping point.
- What evidence would resolve it: A comparison of final FID scores where one group uses a fixed τ and the other uses the automated gradient-angle trigger across diverse model scales and datasets.

## Limitations
- The termination mechanism timing may be dataset-dependent and not universally optimal across tasks, modalities, or teacher-student pairs.
- The layer selection strategy assumes DiT layer-wise specialization that may not hold for all architectures (e.g., models with skip connections).
- The method has only been validated on Transformer architectures and latent-space diffusion models, not pixel-level diffusion or video generation.

## Confidence
- **High:** Holistic alignment mechanism, quantitative performance improvements on standard benchmarks
- **Medium:** Layer selection strategy, capacity-mismatch hypothesis timing
- **Low:** Termination trigger generalizability across datasets/tasks, attention transfer feasibility for architectures with fundamentally different attention mechanisms

## Next Checks
1. **Cross-dataset termination validation:** Train HASTE on multiple datasets (e.g., LSUN, CIFAR-10) with identical architecture to test whether the 250K-iteration termination point generalizes or requires per-dataset tuning. Compare against both fixed-iteration and gradient-angle-triggered termination strategies.

2. **Attention mechanism transfer robustness:** Replace the full-attention DINOv2 teacher with a linear-attention teacher (e.g., EDiT) to test whether the attention alignment component fails when student and teacher use fundamentally different attention mechanisms, as suggested by the break condition.

3. **Layer-wise ablation across architectures:** Systematically vary the alignment layer range ([0-3], [4-7], [8-11], [4-7,8-11]) on both SiT and alternative DiT architectures (e.g., U-Net-based DiTs with skip connections) to empirically validate the claimed functional separation and identify architecture-specific optimal ranges.