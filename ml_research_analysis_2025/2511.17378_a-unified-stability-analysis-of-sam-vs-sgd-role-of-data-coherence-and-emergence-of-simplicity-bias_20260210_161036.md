---
ver: rpa2
title: 'A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence
  of Simplicity Bias'
arxiv_id: '2511.17378'
source_url: https://arxiv.org/abs/2511.17378
tags:
- coherence
- stability
- data
- should
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified stability analysis of SGD, random
  perturbation, and SAM by introducing a coherence measure that quantifies alignment
  in curvature directions across data points. The authors show that high coherence
  leads to stable minima and biases optimization toward simpler solutions that use
  shared features.
---

# A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias

## Quick Facts
- arXiv ID: 2511.17378
- Source URL: https://arxiv.org/abs/2511.17378
- Reference count: 40
- This paper presents a unified stability analysis of SGD, random perturbation, and SAM by introducing a coherence measure that quantifies alignment in curvature directions across data points, showing SAM has stricter stability criteria and amplifies simplicity bias compared to SGD.

## Executive Summary
This paper provides a unified theoretical framework for understanding the stability of SGD, random perturbation, and Sharpness-Aware Minimization (SAM) through a coherence measure that quantifies alignment of gradient curvature directions across data points. The authors show that high coherence leads to stable minima and biases optimization toward simpler solutions that use shared features, while SAM enforces stricter stability criteria that amplify this simplicity bias. Through theoretical analysis and empirical validation, they demonstrate that SGD and SAM prefer generalizing solutions over memorizing ones, with SAM being more selective about the minima it converges to.

## Method Summary
The paper introduces a coherence measure that quantifies how gradient curvature directions align across data points, then analyzes stability of minima under SGD, random perturbation, and SAM by linearizing the dynamics near a minimum. For synthetic quadratic losses, they construct Hessians with controlled coherence properties and test divergence/convergence boundaries. For neural networks, they explicitly construct (C,r)-generalizing solutions using 2-layer ReLU networks and compare stability of memorizing versus generalizing solutions under different optimizers.

## Key Results
- High coherence leads to stable minima and biases optimization toward simpler solutions that use shared features
- SAM has a stricter stability criterion than SGD and amplifies the simplicity bias by effectively operating on a modified loss surface with amplified curvature
- SGD and SAM prefer coherent, generalizing solutions over memorizing ones, with SAM being more selective about convergence

## Why This Works (Mechanism)

### Mechanism 1: Data Coherence Governs Stability
- **Claim:** The stability of a minimum under SGD is determined by a "coherence measure" ($\sigma$), which quantifies the alignment of curvature (Hessian) directions across data points, rather than just global sharpness.
- **Mechanism:** High coherence implies that perturbations affect many data points similarly, creating a strong restorative gradient. The paper proves that for SGD, the divergence threshold depends on $\frac{\sigma}{\lambda_{max}}$; if curvature directions are misaligned (low coherence), the solution becomes unstable and is rejected.
- **Core assumption:** The loss landscape is locally quadratic near the minimum (linear stability regime).
- **Evidence anchors:**
  - [abstract] "Central to our analysis is a coherence measure that quantifies how gradient curvature aligns... revealing why certain minima are stable."
  - [section] Definition 1 (Coherence Measure) and Theorem 3.1 establish the precise divergence condition involving $\sigma$.
  - [corpus] Related work on "Transient learning dynamics" supports the view that dynamics, not just static geometry, drive solution selection.
- **Break condition:** If the data consists of purely i.i.d. noise or orthogonal features (memorization regime), coherence drops, and the mechanism fails to stabilize the solution.

### Mechanism 2: SAM as an Effective Curvature Amplifier
- **Claim:** Sharpness-Aware Minimization (SAM) enforces a stricter stability criterion than SGD by effectively operating on a modified loss surface with amplified curvature.
- **Mechanism:** The SAM update approximates an effective Hessian $H_{SAM} \approx H(I + \frac{\rho}{\alpha} H)$. This extra $H$-dependent term tightens the stability bound (Theorem 3.2), causing SAM to diverge from minima that SGD might tolerate, specifically targeting complex or fragile solutions.
- **Core assumption:** The SAM perturbation norm $||Hw_t||$ can be approximated by a fixed scalar $\alpha$ to yield tractable linear dynamics.
- **Evidence anchors:**
  - [abstract] "SAM has a stricter stability criterion and amplifies the simplicity bias compared to SGD."
  - [section] Theorem 3.2 derives the divergence criterion $\lambda_{max}(H) \geq \dots (1 + \frac{\rho}{\alpha}\lambda_{min})^{-1}$, showing stricter requirements than SGD.
  - [corpus] Literature on "Asynchronous SAM" and SAM variants supports the sensitivity of SAM to curvature and hyperparameters.
- **Break condition:** If the learning rate $\eta$ or SAM radius $\rho$ is too small relative to the noise, the amplification effect becomes negligible, reducing SAM to SGD behavior.

### Mechanism 3: Simplicity via Feature Sharing (Coherence Spectrum)
- **Claim:** The "simplicity bias" is a direct consequence of stability favoring solutions where features are shared across data points (high off-diagonal coherence).
- **Mechanism:** A "memorizing" solution (one neuron per sample) produces a diagonal coherence matrix (low stability). A "simple" solution (shared features) produces a dense coherence matrix with a dominant eigenvalue (high stability). Optimization naturally selects the latter.
- **Core assumption:** The model is a two-layer ReLU network where memorization vs. generalization can be explicitly constructed via $(C, r)$ solutions.
- **Evidence anchors:**
  - [abstract] "SGD and SAM prefer coherent, generalizing ones."
  - [section] Theorem 3.4 proves that memorizing solutions have diagonal coherence matrices, while Theorem 3.5 shows simple solutions have high $\lambda_{max}(S)$.
  - [corpus] Weak direct evidence in provided corpus; mechanism relies heavily on this paper's specific theoretical construction.
- **Break condition:** If the data requires complex, non-shared features to achieve zero loss (no simple solution exists), the model is forced into lower-coherence, less stable regions.

## Foundational Learning

- **Concept: Linear Stability Theory**
  - **Why needed here:** The entire framework relies on linearizing the update dynamics $w_{t+1} \approx (I - \eta H)w_t$ to judge if a minimum is an attractor.
  - **Quick check question:** Can you explain why the spectral radius of the update operator $I - \eta H$ determines convergence?

- **Concept: Hessian Geometry (Curvature)**
  - **Why needed here:** The paper links "flatness" (low curvature eigenvalues) and "coherence" (alignment of eigenvectors across samples) to generalization.
  - **Quick check question:** How does the trace or spectral norm of the Hessian relate to the "sharpness" of a minimum?

- **Concept: Implicit Regularization/Bias**
  - **Why needed here:** The paper argues that the algorithm's stability criteria implicitly select for "simple" solutions without explicit regularization terms.
  - **Quick check question:** Why would an algorithm naturally prefer a solution using fewer features if both solutions achieve zero training loss?

## Architecture Onboarding

- **Component map:**
  - Coherence Matrix ($S$) -> Stability Boundary -> Effective SAM Hessian

- **Critical path:**
  1. Identify solution type (Memorizing vs. Generalizing) $\to$ determine Coherence $\sigma$.
  2. Check Stability: Compare $\eta, \lambda_{max}(H), \sigma$ against the derived bounds.
  3. SAM Adjustment: If using SAM, recognize the effective curvature is higher; adjust $\eta$ or $\rho$ to maintain stability.

- **Design tradeoffs:**
  - **SAM vs. SGD:** SAM provides better bias toward simple/coherent solutions but requires stricter learning rate discipline (diverges easier).
  - **Random Perturbation:** Adds noise to escape sharp minima faster but does not change the *stability threshold* (unlike SAM).

- **Failure signatures:**
  - **Instability/Divergence:** Occurs if $\eta$ is too high relative to $\lambda_{max}$ AND coherence $\sigma$ is high (Theorem 3.1).
  - **Memorization:** Occurs if the model finds a diagonal coherence solution (Theorem 3.4), indicating a failure to find a shared-feature representation.

- **First 3 experiments:**
  1. **Boundary Validation:** Replicate Fig 1(b) by plotting divergence/convergence regions for SGD vs. SAM over varying batch size $B$ and noise $\sigma$ to verify the theoretical boundaries.
  2. **Coherence Tracking:** Train a 2-layer ReLU network and log the coherence measure $\sigma$ and rank of features over time (Table 1) to observe if the model actively reduces complexity.
  3. **Solution Construction:** Implement the $(C, r)$-generalizing solution construction (Appendix C.1) to explicitly verify that low-$C$ (simple) solutions are more stable than high-$C$ (complex) ones under the same sharpness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the coherence measure be efficiently approximated at scale to guide training of large modern architectures?
- **Basis in paper:** [explicit] "We see this as a motivation for future work on scalable surrogates or proxies."
- **Why unresolved:** Computing exact per-sample Hessian products is prohibitively expensive for large models, limiting practical deployment of coherence-based analysis.
- **What evidence would resolve it:** Demonstration of a computationally tractable coherence approximation that maintains predictive power for stability/generalization on large-scale models.

### Open Question 2
- **Question:** Does the coherence-driven stability framework extend to adaptive optimizers like Adam or momentum SGD?
- **Basis in paper:** [explicit] "Extending to optimizers like momentum SGD or Adam, which interact with curvature in subtle ways, is a promising direction."
- **Why unresolved:** Current theory covers plain SGD and SAM; adaptive methods have additional momentum/adaptation terms that may alter stability criteria.
- **What evidence would resolve it:** Derivation of analogous stability conditions for momentum-based optimizers linking coherence to convergence/divergence.

### Open Question 3
- **Question:** How does coherence-based stability behave under non-i.i.d. data sampling regimes?
- **Basis in paper:** [explicit] "Studying non-i.i.d. settings like curriculum or imbalanced sampling is a valuable direction for future work."
- **Why unresolved:** All theoretical results assume i.i.d. minibatch sampling; real-world training often violates this.
- **What evidence would resolve it:** Theoretical or empirical analysis showing how curriculum learning or class imbalance affects coherence and resulting solution stability.

### Open Question 4
- **Question:** Can coherence-aware adaptive learning rates or batch selection strategies improve optimization in practice?
- **Basis in paper:** [explicit] "One concrete idea is to adapt the learning rate based on coherence between mini-batches... While we have not explored these experimentally, we view them as promising directions."
- **Why unresolved:** Proposed as potential application but not empirically validated.
- **What evidence would resolve it:** Empirical demonstration that coherence-adaptive training schedules yield measurable generalization improvements over standard approaches.

## Limitations

- Theoretical framework relies heavily on linear stability assumptions and constructed (C,r) solution examples
- Empirical estimation of coherence measures requires expensive per-sample Hessian computations
- SAM's stricter stability criteria depend on the approximation that ||Hw_t|| can be replaced by a scalar α
- Weak direct empirical evidence from provided corpus suggests some claims may be more theoretical than experimentally validated

## Confidence

- **High confidence:** The linear stability analysis framework and coherence measure definitions (Theorem 3.1, 3.2)
- **Medium confidence:** The SAM curvature amplification mechanism and its stricter stability criterion
- **Medium confidence:** The connection between coherence and simplicity bias, though primarily supported by constructed solutions
- **Low confidence:** Direct empirical validation of all theoretical claims across diverse architectures and datasets

## Next Checks

1. **Coherence stability boundary validation:** Replicate the divergence/convergence plots (Fig 1b) across different batch sizes and noise levels to verify the theoretical stability thresholds for SGD vs SAM
2. **Coherence evolution tracking:** Monitor coherence measure σ and feature rank during training on real datasets to confirm the predicted reduction in complexity over time
3. **Solution construction verification:** Implement and test the (C,r)-generalizing solutions on synthetic data to empirically verify that high-coherence solutions are indeed more stable than memorizing ones under both SGD and SAM