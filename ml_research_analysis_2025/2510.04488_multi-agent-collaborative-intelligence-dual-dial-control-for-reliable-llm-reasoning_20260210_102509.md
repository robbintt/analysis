---
ver: rpa2
title: 'Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM
  Reasoning'
arxiv_id: '2510.04488'
source_url: https://arxiv.org/abs/2510.04488
tags:
- maci
- evidence
- crit
- debate
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACI introduces dual-dial control for multi-agent debate, using
  an information dial to gate evidence quality and a behavior dial to schedule contentiousness
  from exploration to consolidation. A moderator tracks disagreement, overlap, evidence
  quality, and argument quality, stopping when gains plateau.
---

# Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning

## Quick Facts
- arXiv ID: 2510.04488
- Source URL: https://arxiv.org/abs/2510.04488
- Authors: Edward Y. Chang; Ethan Y. Chang
- Reference count: 40
- Primary result: 3.9pp accuracy gain over majority vote in clinical diagnosis while reducing tokens by 19%

## Executive Summary
MACI introduces a dual-dial control framework for multi-agent LLM debate, decoupling information gating from behavioral scheduling. The system uses an information dial to filter evidence quality and a behavior dial to schedule contentiousness from exploration to consolidation. A moderator tracks disagreement, overlap, evidence quality, and argument quality, stopping when gains plateau. The approach improves clinical diagnosis accuracy by 3.9pp over majority vote while reducing computational tokens, and shows cross-domain robustness in news bias detection with 68% reduction in partisan gap.

## Method Summary
MACI orchestrates multi-agent debate through independent control of evidence admission (information dial) and interaction intensity (behavior dial). Agents present arguments that must pass dual gates based on evidence quality (Q) and CRIT-evaluated reasoning quality. A moderator monitors normalized information gain and Jensen-Shannon divergence, reducing contentiousness and tightening thresholds when progress plateaus. Final beliefs are aggregated via reliability-weighted averaging with gate enforcement to ensure monotonic dispersion reduction. The framework provides theory-lite convergence guarantees and includes budget-feasible scheduling for resource-constrained settings.

## Key Results
- 3.9pp accuracy improvement over majority vote on 1,500 clinical cases
- 19% reduction in computational tokens through efficient stopping
- 68% reduction in partisan gap in news bias classification (619 articles)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Dial Decoupling of Information and Behavior
- Claim: Independent control over evidence admission and interaction intensity improves convergence quality compared to single-dial or unmoderated debate.
- Mechanism: The information dial (τQ, τCRIT) gates which arguments enter the debate based on evidence and reasoning quality. The behavior dial (CL) schedules contentiousness from high (exploration) to low (consolidation). These operate orthogonally—high CL can explore alternatives while τ gates ensure only quality evidence informs beliefs.
- Core assumption: Evidence quality and behavioral stance are independently controllable dimensions that jointly determine debate productivity.
- Evidence anchors: [abstract] "two independent dials that decouple information from behavior"; [section 4.1, Table 3] Removing Q gate costs −5.2pp Acc@1 (+0.040 ECE); removing scheduling costs −3.9 to −6.0pp; uniform weights cost −3.0pp.

### Mechanism 2: Information-Theoretic Plateau Detection
- Claim: Stopping when normalized information gain (Î) and disagreement (DJS) plateau prevents wasted compute and improves calibration.
- Mechanism: Relative progress ratios r_I(t) and r_D(t) measure per-round improvement against remaining capacity. When both ratios fall below thresholds (ε_I, ε_D) for τ_stop consecutive rounds, the controller reduces CL and tightens τ. Stopping requires sustained plateaus plus sufficient evidence quality (Q ≥ τ_Q) and overlap (O ≥ τ_O).
- Core assumption: Plateaued signals indicate exhausted productive disagreement rather than temporary stalls; convergence correlates with downstream accuracy.
- Evidence anchors: [abstract] "information-theoretic stopping that detects convergence plateaus"; [section 3.3, Eq. 6-10] Formal definition of progress ratios and compound stop rule.

### Mechanism 3: Reliability-Weighted Gated Averaging
- Claim: Aggregating agent beliefs via CRIT-weighted mixture with gate enforcement yields monotonic dispersion reduction.
- Mechanism: Per-agent reliability weights ω_u(t) are exponential moving averages of CRIT scores (Eq. 4). The mixture p(t) = Σ_u ω_u(t) · p_u(t) weights higher-quality arguments more. The gate rejects updates that would increase dispersion D_KL(t), ensuring D_KL(t+1) ≤ D_KL(t) under bounded noise (Lemma 1).
- Core assumption: CRIT scores correlate with argument quality; cross-family judges reduce self-preference bias sufficiently for reliable weighting.
- Evidence anchors: [abstract] "dispersion is nonincreasing under gated averaging"; [Appendix M, Lemma 1] Formal proof of monotonicity under gated averaging with noise bounds.

## Foundational Learning

- **Jensen-Shannon Divergence (DJS)**
  - Why needed here: Primary disagreement signal; symmetric, bounded [0,1], finite under disjoint support—suitable for plateau detection.
  - Quick check question: Given two probability distributions p_A = [0.6, 0.3, 0.1] and p_B = [0.4, 0.4, 0.2], would DJS increase or decrease if p_A moved toward p_B?

- **Exponential Moving Average (EMA) for Reliability Tracking**
  - Why needed here: CRIT scores are noisy per-round; EMA (λ=0.8) smooths weights while allowing adaptation to sustained quality changes.
  - Quick check question: With λ=0.8, how much weight does the current round's CRIT score receive vs. historical average?

- **Upper Confidence Bound (UCB) for Budgeted Scheduling**
  - Why needed here: The learned scheduler (Appendix L) uses UCB over discrete (∆α, ∆γ) actions to balance exploration of schedules vs. exploitation of high-reward moves under token budget.
  - Quick check question: In a 10-action bandit with uniform priors, which action does UCB select after 5 rounds if action 3 has mean reward 0.4 (played twice) and action 7 has mean reward 0.35 (played once)?

## Architecture Onboarding

- **Component map:**
  [Query + Retrieved Context] → [Initialization Module] → CL(1), τ(1) from (Q_0, DJS_0) binning → [Debate Loop] → [Final Mixture] → [Precision RAG Plan]

- **Critical path:**
  1. **Initialization:** Compute Q_0 and DJS_0 from query + retrieval; bin to set CL(1)
  2. **Dual gating:** Every candidate argument must pass both Q and CRIT thresholds
  3. **Signal computation:** DJS, O, Q, Î drive all downstream decisions
  4. **Plateau detection:** Consecutive flags (π_I, π_D) trigger CL reduction and τ tightening
  5. **Termination:** Compound rule (Eq. 10) ensures quality + convergence before stop

- **Design tradeoffs:**
  - **K=3 vs K=5 judges:** More judges reduce variance (↑α) but increase latency and cost. Paper uses K=3 with α≈0.68 as acceptable.
  - **τ_stop=2 vs longer windows:** Shorter windows react faster but risk premature stopping on temporary stalls.
  - **Fixed schedule vs learned scheduler:** Fixed (α=0.2, γ=0.1) is reproducible; UCB scheduler (Appendix L) adapts but adds complexity. Main results use fixed for comparability.
  - **2 agents vs 3+ agents:** Paper validates M=2; scaling to 3-5 is noted as future work with expected token overhead.

- **Failure signatures:**
  - **Stall with high DJS:** Gate too strict (τ_Q, τ_CRIT too high) or judges poorly aligned—no arguments admitted, loop exhausts budget.
  - **Premature convergence:** O ≥ τ_O satisfied with superficial overlap; DJS low but Q poor—check Q(t) trend before stop.
  - **CRIT instability:** Winner-flip rate > 10% on judge swap—increase K, tighten thresholds, or audit judge alignment.
  - **Token budget violation:** Learned scheduler under-estimates costs—verify conservative cost estimator bct(k) ≥ E[ct(k)].

- **First 3 experiments:**
  1. **Reproduce G1 ablations on 100-case subset:** Run full MACI vs. no-Q-gate vs. no-schedule vs. uniform-weights. Verify Acc@1 drops match Table 3 (−3 to −6pp) before full-scale runs.
  2. **CRIT reliability check on your domain:** Sample 50 arguments, run cross-family judges (e.g., if agents are GPT-4o + Claude, use Gemini as judge). Compute α and winner-flip rate. If α < 0.6 or flip rate > 5%, increase K or re-tune τ_CRIT.
  3. **Plateau threshold sweep:** Vary ε_I ∈ {0.01, 0.02, 0.05} and ε_D ∈ {0.03, 0.05, 0.10} on dev set. Plot rounds-to-stop vs. Acc@1 to find domain-appropriate balance between early stopping and over-debating.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical scope limited to 1,500 clinical cases across 17 diagnoses and 619 news articles
- Theoretical convergence guarantees assume specific interaction models that may not hold under real-world noise
- CRIT evaluator reliability (α=0.68) is moderate but not strong
- Scaling to 3+ agents remains untested in the current framework

## Confidence
**High Confidence** (mechanisms grounded in formal proofs + ablation evidence):
- Dual-dial decoupling improves convergence vs. single-control or unmoderated debate (Table 3, −5.2 to −6.0pp losses when removed)
- Information-theoretic plateau detection prevents wasted compute while maintaining accuracy (ε_I=0.02, ε_D=0.05 thresholds work across domains)
- Reliability-weighted gated averaging achieves monotonic dispersion reduction (Lemma 1 proof + D_KL(t) nonincreasing in practice)

**Medium Confidence** (strong empirical results but limited scope):
- 3.9pp accuracy improvement over majority vote generalizes beyond MIMIC-III (cross-domain transfer to news bias shows 68% partisan gap reduction)
- K=3 CRIT judges provide sufficient reliability for weighting (α=0.68, 2-3% winner flips on judge swap, but moderate not strong)
- O(log(1/ε)) convergence under contraction applies to practical settings (theoretical assumption of bounded contraction factor not directly validated)

**Low Confidence** (claims extend beyond demonstrated evidence):
- Scaling to 3+ agents maintains MACI benefits (paper notes this as future work, no empirical validation)
- Budget-feasible scheduler achieves O(√KT) no-regret in practice (UCB scheduler in Appendix L, but main results use fixed schedule)
- Cross-family judge strategy eliminates all alignment bias (documented to reduce self-preference bias but no comprehensive bias audit)

## Next Checks
1. **Domain Transfer Robustness:** Apply MACI to three additional domains (legal reasoning, scientific literature review, and code review) with 200+ cases each. Compare accuracy improvements against baselines and measure CRIT reliability (α) in each domain to establish when the framework breaks down.

2. **Judge Alignment Audit:** For each domain, conduct a systematic audit of CRIT evaluator alignment by measuring inter-judge correlation when judging the same arguments, analyzing cases where judges disagree, and testing whether CRIT scores correlate with downstream accuracy. If α < 0.6 in any domain, implement judge training or increase K.

3. **Scaling Experiment:** Implement MACI with 3 and 5 agents (vs. the paper's 2 agents) on the same clinical dataset. Measure accuracy gains, token efficiency, and convergence behavior. If accuracy plateaus or tokens increase superlinearly, document the scaling limits and identify which components (gate thresholds, plateau detection) require adjustment.