---
ver: rpa2
title: 'GENMO: A GENeralist Model for Human MOtion'
arxiv_id: '2505.01425'
source_url: https://arxiv.org/abs/2505.01425
tags:
- motion
- generation
- estimation
- human
- genmo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GENMO introduces a unified generalist framework for human motion
  modeling that bridges the gap between motion estimation and generation tasks. The
  method formulates motion estimation as constrained motion generation, where outputs
  must satisfy observed conditioning signals.
---

# GENMO: A GENeralist Model for Human MOtion

## Quick Facts
- arXiv ID: 2505.01425
- Source URL: https://arxiv.org/abs/2505.01425
- Authors: Jiefeng Li; Jinkun Cao; Haotian Zhang; Davis Rempe; Jan Kautz; Umar Iqbal; Ye Yuan
- Reference count: 40
- Primary result: Achieves 202.1 mm W-MPJPE on EMDB for global motion estimation

## Executive Summary
GENMO introduces a unified generalist framework for human motion modeling that bridges the gap between motion estimation and generation tasks. The method formulates motion estimation as constrained motion generation, where outputs must satisfy observed conditioning signals. This unified approach leverages shared representations to enable synergistic benefits: generative priors enhance motion estimation under challenging conditions like occlusions, while diverse video data enriches generative capabilities.

The architecture employs a novel dual-mode training paradigm combining estimation mode (maximum likelihood estimation) and generation mode (diffusion-based generation), along with an estimation-guided training objective that effectively leverages in-the-wild videos with 2D annotations. The model handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals through a novel multi-text injection block and relative positional embeddings.

## Method Summary
GENMO proposes a unified framework that treats motion estimation as a special case of motion generation with constraints. The model uses a diffusion-based architecture with dual-mode training: estimation mode for maximizing likelihood of observed data and generation mode for unconstrained motion synthesis. A key innovation is the estimation-guided training objective that allows leveraging 2D annotations from in-the-wild videos. The architecture incorporates multi-text injection blocks and relative positional embeddings to handle variable-length motions and mixed multimodal conditions. This unified approach enables synergistic benefits where generative priors improve estimation performance and diverse video data enhances generation capabilities.

## Key Results
- Achieves 202.1 mm W-MPJPE on EMDB for global motion estimation, setting new state-of-the-art performance
- Music-to-dance generation reaches FIDk: 40.91 and PFC: 0.3702 on standard benchmarks
- Text-to-motion generation achieves R@3: 0.472 and FID: 0.207 on Motion-X
- Successfully handles multiple human motion tasks within a single model while maintaining generative diversity

## Why This Works (Mechanism)
GENMO works by unifying motion estimation and generation through a constrained generation framework. The key insight is that motion estimation can be viewed as generation conditioned on observed signals, allowing both tasks to share the same underlying generative model. The dual-mode training ensures the model learns both precise estimation capabilities through maximum likelihood objectives and creative generation capabilities through diffusion-based sampling. The estimation-guided training objective cleverly leverages abundant 2D annotations from in-the-wild videos that would otherwise be unusable for 3D motion estimation. This unified approach creates synergistic effects where generative priors help with challenging estimation scenarios (like occlusions) and diverse video data improves generation quality.

## Foundational Learning

1. **Diffusion-based generative models**
   - Why needed: Provide the foundation for both motion generation and the dual-mode training approach
   - Quick check: Understand the forward noising process and reverse denoising process in diffusion models

2. **Maximum likelihood estimation vs. diffusion generation**
   - Why needed: The dual training modes require understanding when to use MLE versus diffusion sampling
   - Quick check: Compare loss functions and training dynamics between MLE and diffusion objectives

3. **Motion representation and conditioning**
   - Why needed: Critical for understanding how observed signals constrain the generation process
   - Quick check: Examine how different modalities (text, audio, video) are encoded and used as conditions

4. **Relative positional embeddings**
   - Why needed: Enable handling of variable-length motions and temporal relationships
   - Quick check: Understand how relative positions differ from absolute positions in temporal modeling

5. **Multi-modal fusion techniques**
   - Why needed: Essential for combining text, audio, and video inputs at different time scales
   - Quick check: Review how different modalities are aligned and weighted in the fusion process

6. **Motion evaluation metrics**
   - Why needed: To properly interpret the reported results across different tasks
   - Quick check: Understand W-MPJPE, FID, PFC, and R@3 metrics and their significance

## Architecture Onboarding

**Component Map:**
Input modalities (text/audio/video) -> Multi-text injection block -> Relative positional embeddings -> Dual-mode diffusion backbone -> Output motion

**Critical Path:**
The critical path involves the multi-text injection block that processes and fuses multimodal inputs, followed by the diffusion backbone that handles both generation and estimation modes. The relative positional embeddings ensure temporal consistency across variable-length sequences.

**Design Tradeoffs:**
The unified architecture trades specialized optimization for individual tasks against the benefits of shared representations and synergistic training. The dual-mode approach requires careful balancing of estimation and generation objectives, while the use of 2D annotations introduces noise that must be handled through the estimation-guided objective.

**Failure Signatures:**
Poor performance on estimation tasks when generative priors dominate, mode collapse in generation tasks when estimation constraints are too strong, and temporal incoherence when relative positional embeddings are insufficient for complex motion patterns.

**First Experiments:**
1. Test the estimation mode alone on EMDB to verify baseline performance without generation capabilities
2. Evaluate the generation mode on unconditioned motion synthesis to assess baseline diffusion performance
3. Run ablations removing the multi-text injection block to measure its contribution to multimodal performance

## Open Questions the Paper Calls Out
None

## Limitations

- Performance on creative generation tasks relies heavily on metric-based evaluations without extensive qualitative validation or user studies
- Claims about handling "diverse" multimodal conditions lack systematic exploration of all possible modality combinations
- Synergistic benefits between estimation and generation modes are demonstrated but not fully quantified through detailed ablation studies

## Confidence

**High Confidence:**
- Architectural innovations are well-described with sufficient technical detail for reproduction
- Motion estimation results on established benchmarks demonstrate clear performance improvements
- Dual-mode training and estimation-guided objectives are technically sound

**Medium Confidence:**
- Synergistic benefits between estimation and generation modes need more comprehensive ablation studies
- Effectiveness of leveraging 2D annotations could benefit from more detailed analysis of annotation quality variations
- Technical implementation details support reproduction claims

**Low Confidence:**
- Claims about handling diverse multimodal conditions lack systematic exploration
- Qualitative aspects of creative generation tasks need user studies for validation
- Full generalization capabilities across all claimed multimodal combinations remain untested

## Next Checks

1. Conduct user studies comparing generated motions from GENMO against baselines for both music-to-dance and text-to-motion tasks, focusing on perceived naturalness, temporal coherence, and task-specific appropriateness.

2. Perform detailed ablation studies isolating the contributions of the dual-mode training objective, estimation-guided training, and multi-text injection block to quantify their individual impacts on performance.

3. Test the model's robustness and performance degradation when conditioned on noisy or partial multimodal inputs (e.g., corrupted audio signals, incomplete text descriptions, occluded video frames) to better understand its generalization capabilities.